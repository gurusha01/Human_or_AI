The paper introduces a Sparse Random Features algorithm for solving the \(\ell_1\)-regularized problem in a Hilbert space, offering a novel theoretical analysis. By interpreting the algorithm as Randomized Coordinate Descent (RCD) in an infinite-dimensional space, the authors demonstrate convergence to an \(\epsilon\)-precision solution using \(O(1/\epsilon)\) random features, improving upon the \(O(1/\epsilon^2)\) convergence rate of prior Monte Carlo analyses of Random Features. Theorem 2 and its corollaries are the paper's key technical contributions, providing rigorous convergence guarantees and bounds for the proposed method.
Strengths:
1. Theoretical Contributions: The paper provides a novel theoretical framework for analyzing \(\ell_1\)-regularized problems in an infinite-dimensional setting. Theorem 2 and its corollaries are significant advancements, offering insights into convergence rates and approximation guarantees.
2. Scalability: The Sparse Random Features algorithm addresses the scalability issues of kernel methods by reducing model size and memory requirements while maintaining comparable performance.
3. Comparison with Boosting: The paper highlights the advantages of the randomized approach over Boosting, particularly when exact greedy steps are infeasible, which is a practical and valuable insight.
4. Novelty: The reinterpretation of Random Features as RCD in a Hilbert space is a creative and original contribution.
Weaknesses:
1. Related Work: The paper does not adequately cite and summarize prior work on \(\ell_1\)-regularized problems in Hilbert spaces, such as [1*]. This omission weakens the contextualization of the contributions.
2. Empirical Section: The experimental results are underdeveloped. While the Sparse Random Features algorithm is compared to Random Features and kernel methods, the lack of direct comparisons with prior work like [1*] limits the empirical evaluation's depth.
3. Choice of \(\lambda\): The choice of the regularization parameter \(\lambda\) appears arbitrary. Employing cross-validation or presenting results across varying \(\lambda\) values would improve the robustness and comparability of the results.
4. Discussion of Results: The paper does not adequately discuss the observed differences in performance between regression and classification tasks. This analysis would provide deeper insights into the algorithm's behavior.
5. Simulations Section: The simulations are sparse and do not fully explore the algorithm's potential. For example, the impact of different kernel types or datasets with varying sizes and complexities could be explored further.
Recommendation:
While the paper makes strong theoretical contributions, the empirical section and contextualization of related work require significant improvement. The following steps are recommended:
- Include a detailed discussion of prior work, particularly [1*], to better situate the contributions.
- Expand the empirical section with comparisons to prior methods, a more systematic exploration of \(\lambda\), and additional datasets.
- Provide a more thorough discussion of the results, especially the differences between regression and classification tasks.
Arguments for Acceptance:
- The theoretical contributions are novel and significant.
- The reinterpretation of Random Features as RCD in a Hilbert space is an original and impactful idea.
- The algorithm addresses scalability issues in kernel methods, which is a critical challenge in machine learning.
Arguments Against Acceptance:
- The empirical evaluation is insufficient and lacks comparisons with prior work.
- The paper does not adequately discuss related work, limiting its contextual depth.
- The simulations and result discussions are underdeveloped.
In conclusion, the paper has strong theoretical merit but requires substantial revisions to its empirical and contextual components. I recommend acceptance conditional on addressing these issues.