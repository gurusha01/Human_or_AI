The paper introduces an alternative approach to solving large Markov Decision Processes (MDPs) by leveraging Difference of Convex (DC) programming to estimate the optimal state-action value function. The authors propose minimizing the Optimal Bellman Residual (OBR) as a proxy for optimal action-value function estimation, demonstrating that this approach is consistent in the Vapnik sense. They frame the minimization of the empirical norm of the OBR as a DC optimization problem, enabling the use of established DC algorithms (DCA). Experimental results on Garnet problems show that this method is competitive with Approximate Value Iteration (AVI) and Approximate Policy Iteration (API), with reduced variance in performance.
Strengths:
1. Novelty: The paper explores a relatively underexplored area in reinforcement learning (RL) by applying DC programming to the estimation of optimal action-value functions. This novel perspective could open new avenues for research in RL.
2. Theoretical Contributions: The authors provide theoretical justification for their approach, including consistency results for the empirical norm of the OBR and explicit DC decompositions for specific cases (e.g., p=1, p=2). These contributions are rigorous and grounded in optimization theory.
3. Experimental Validation: The computational experiments demonstrate that the proposed method performs comparably to established RL algorithms like AVI and API, with the added advantage of reduced variance in results.
4. Future Potential: The discussion of non-parametric extensions via boosting techniques and potential improvements to the DCA framework highlights the broader applicability and flexibility of the approach.
Weaknesses:
1. Computational Efficiency: DC programming is inherently computationally expensive, and the paper does not address this limitation in depth. This raises concerns about the scalability of the approach to high-dimensional or real-world RL problems.
2. Limited Scope of Consistency Results: The theoretical results in Section 3 are limited to deterministic finite MDPs, with only brief extensions to stochastic and continuous-state MDPs. This restricts the generalizability of the approach.
3. Simplistic Experimental Setup: The experiments are restricted to low-dimensional Garnet problems, which may not fully capture the challenges of real-world RL tasks. The naive implementation of DCA may also understate the potential of the method.
4. Clarity Issues: While the paper is generally well-written, there are minor typographical errors (e.g., "manly" instead of "mainly") and inconsistencies in notation (e.g., "+\inf"). Additionally, ambiguous phrasing such as "too important" in Section 4 should be clarified.
Arguments for Acceptance:
- The paper presents a novel and theoretically sound approach to RL, which could inspire further research.
- The experimental results, though limited, suggest that DC programming is a viable alternative to traditional RL methods.
- The exploration of non-parametric extensions and boosting techniques adds significant value to the work.
Arguments Against Acceptance:
- The computational inefficiency of DC programming limits its practical applicability.
- The theoretical and experimental results are constrained to simplistic settings, leaving its performance on complex, real-world problems untested.
- Minor clarity and presentation issues detract from the overall quality of the paper.
Recommendation:
This paper represents an early-stage contribution with promising theoretical insights but limited practical applicability at this stage. While it may not yet be ready for acceptance at a top-tier conference like NeurIPS, it could benefit from further refinement and expanded experiments. I recommend a weak reject, with encouragement to address scalability concerns and test the method on more complex RL benchmarks in future iterations.