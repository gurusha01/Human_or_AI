The paper introduces the Parallel Direction Method of Multipliers (PDMM), a novel algorithm for minimizing block-separable convex functions with linear constraints. PDMM extends the Alternating Direction Method of Multipliers (ADMM) to handle multiple blocks using Jacobian updates instead of Gauss-Seidel updates, overcoming ADMM's limitation to two blocks. The key innovation is the dual backward step, which compensates for limited information propagation in Jacobian updates by reducing the dual update step size. This ensures robust convergence under general conditions. The authors establish global convergence and O(1/T) iteration complexity for PDMM, aligning with ADMM's convergence rate. Additionally, they demonstrate that sADMM and PJADMM are special cases of PDMM, providing a unifying framework for these methods. Experimental results on robust principal component analysis (RPCA) and overlapping group lasso validate PDMM's effectiveness, though it is noted that Jacobian methods generally converge slower than Gauss-Seidel methods.
Strengths:
1. Technical Contribution: The dual backward step is a significant innovation, addressing a key limitation of Jacobian updates and ensuring convergence. The theoretical analysis is rigorous, with clear proofs of global convergence and iteration complexity.
2. Unification of Methods: By showing that sADMM and PJADMM are special cases of PDMM, the paper provides valuable insights into the relationships between existing decomposition methods.
3. Practical Relevance: The experimental results demonstrate PDMM's applicability to real-world problems like RPCA and overlapping group lasso, showing its potential as a robust alternative to existing methods.
4. Clarity of Presentation: The paper is well-organized, with a clear explanation of the algorithm, its theoretical properties, and experimental results.
Weaknesses:
1. Empirical Performance: While PDMM is theoretically sound, its empirical performance is mixed. The experiments show that PDMM converges slower than Gauss-Seidel-based methods like GSADMM in terms of iterations, though it compensates with parallelism.
2. Parameter Sensitivity: The effectiveness of PDMM depends on the choice of parameters (e.g., step sizes τi and νi), which may require careful tuning. This could limit its ease of use in practice.
3. Limited Scope of Applications: The experiments focus on two specific applications. Broader evaluations across diverse problem domains would strengthen the case for PDMM's general applicability.
4. Parallel Implementation: Although PDMM is designed for parallelism, the experiments are conducted sequentially, which does not fully showcase its advantages over sequential methods like GSADMM.
Pro and Con Arguments for Acceptance:
- Pro: The paper makes a significant theoretical contribution by extending ADMM to multiple blocks with Jacobian updates and introducing the dual backward step. The unification of sADMM and PJADMM under the PDMM framework is insightful, and the convergence guarantees are robust.
- Con: The empirical results, while promising, do not convincingly outperform existing methods like GSADMM in all aspects. Additionally, the reliance on parameter tuning and the lack of parallel implementation in experiments are limitations.
Recommendation: Accept with minor revisions. The paper presents a strong theoretical contribution and a novel algorithm with potential for distributed optimization. However, the authors should address the parameter sensitivity and provide additional experimental results, ideally with parallel implementations, to better demonstrate PDMM's practical advantages.