Review
This paper tackles the important problem of joint clustering and outlier detection, which has significant implications in unsupervised learning and optimization. The authors propose a novel approach by extending the facility location formulation and introducing a Lagrangian relaxation to solve the problem. The resulting subgradient-based algorithm is designed to be scalable, memory-efficient, and capable of automatically determining the number of clusters. Experimental results on synthetic and real datasets demonstrate the method's effectiveness compared to two baseline methods, k-means-- and APOC.
Strengths:
1. Novelty of Approach: The use of Lagrangian relaxation to address the joint clustering and outlier detection problem is a creative and well-motivated idea. This formulation allows the algorithm to balance clustering quality and outlier detection in a principled manner.
2. Practicality and Scalability: The proposed algorithm is simple to implement, computationally efficient, and memory-efficient, making it suitable for large-scale datasets. The ability to compute distances at runtime and avoid storing dense matrices is a significant advantage over APOC.
3. Experimental Validation: The authors provide thorough experimental comparisons on synthetic and MNIST datasets. The results demonstrate that the proposed method achieves competitive clustering quality and outlier detection performance while being more scalable than APOC and more accurate than k-means--.
4. Interpretability: The clusters and outliers identified by the algorithm are meaningful and interpretable, as illustrated by the MNIST results.
Weaknesses:
1. Theoretical Analysis: While the paper provides some analysis of the Lagrangian relaxation and subgradient method, the theoretical guarantees on convergence are insufficiently detailed. For example, the convergence rate analysis is limited and lacks rigorous empirical validation.
2. Lack of Recent References: The paper does not adequately situate its contributions within the broader context of recent work in computational statistics and robust clustering. For instance, more recent advances in robust clustering methods and scalable optimization techniques could have been cited and discussed.
3. Sensitivity to Parameters: The algorithm requires the user to specify the number of outliers (`l`) and the cluster creation cost scaling factor (`Î±`). While the experiments suggest robustness to misspecification, the paper does not provide a systematic analysis of how these parameters affect performance in diverse scenarios.
Pro and Con Arguments for Acceptance:
- Pros: The paper introduces an innovative and practical approach to a challenging problem. The algorithm is scalable, interpretable, and experimentally validated on both synthetic and real-world datasets.
- Cons: Theoretical depth is lacking, particularly regarding convergence guarantees. The omission of recent related work weakens the contextualization of the contribution.
Overall Recommendation:
This paper presents a novel and practical method for joint clustering and outlier detection, advancing the state of the art in this domain. However, the theoretical analysis and literature review need improvement. I recommend acceptance, provided the authors address the theoretical gaps and include more recent references in a revised version.