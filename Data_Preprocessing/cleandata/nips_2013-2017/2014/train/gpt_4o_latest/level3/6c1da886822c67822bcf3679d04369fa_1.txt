The paper proposes a novel privacy-preserving mechanism for matrix-factorization-based recommender systems by categorizing users into public and private groups. Public users share all their data, enabling accurate estimation of item features, while private users retain full privacy, sharing no direct information. The authors derive theoretical bounds on the estimation accuracy of item features and reconstruction error for private users, demonstrating the feasibility of their approach. Empirical validation on the Movielens 10M dataset shows promising results, with the proposed mechanism achieving competitive performance while addressing practical privacy concerns in untrusted recommender systems.
Strengths
1. Novelty and Practical Relevance: The paper addresses a critical privacy concern by assuming an untrusted recommender engine, which is a realistic and underexplored scenario. The proposed mechanism offers a practical alternative to differential privacy (DP) by leveraging public users and introducing a new privacy concept for private users.
2. Theoretical Rigor: The formalism is technically sound, with clear derivations and proofs. The bounds on estimation accuracy and reconstruction error provide strong theoretical guarantees.
3. Empirical Validation: The experiments on the Movielens 10M dataset convincingly demonstrate the effectiveness of the approach. The results highlight the trade-off between privacy and accuracy, showing that a small number of public users can suffice for reasonable performance.
4. Clarity and Organization: The paper is well-written and logically structured, making it accessible to readers. The theoretical and empirical contributions are clearly delineated.
Weaknesses
1. Theorem 3.5 Intuition: While the derivation is sound, Theorem 3.5 would benefit from additional intuition or examples to improve accessibility for readers unfamiliar with the technical details.
2. Experimental Clarifications: The meaning of "Percentage of Users" in the first experiment is unclear and should be explicitly defined. Additionally, the second experiment stops at 400 private users, leaving questions about scalability to larger datasets and comparisons to DP methods unanswered.
3. Real-World Applicability: The experiments focus on relatively synthetic settings. Results in more realistic scenarios, such as when private users consume fewer items, would strengthen the paper's conclusions.
4. Comparison with DP: While the paper positions itself as an alternative to DP, the comparisons to DP methods could be more comprehensive, particularly in terms of privacy guarantees and scalability.
Arguments for Acceptance
- The paper introduces a novel and practical approach to privacy-preserving recommender systems, addressing a significant gap in the literature.
- Theoretical contributions are robust, and empirical results are convincing.
- The work is well-aligned with the conference's focus on advancing the state of the art in machine learning and privacy.
Arguments Against Acceptance
- Some experimental details and results lack clarity and completeness, particularly regarding scalability and real-world applicability.
- The paper could better contextualize its contributions relative to existing DP methods, especially in terms of privacy-accuracy trade-offs.
Recommendation
Overall, this paper makes a meaningful contribution to privacy-preserving recommender systems and demonstrates both theoretical and empirical rigor. While there are areas for improvement, particularly in experimental clarity and scalability, the strengths outweigh the weaknesses. I recommend acceptance, with minor revisions to address the noted issues.