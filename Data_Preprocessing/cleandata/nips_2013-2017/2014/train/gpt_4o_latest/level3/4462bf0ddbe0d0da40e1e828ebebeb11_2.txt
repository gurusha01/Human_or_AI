The paper presents a novel Expectation-Maximization (EM) algorithm for learning Determinantal Point Processes (DPPs), a probabilistic model for subset selection that balances quality and diversity. Unlike prior work, which focused on restricted convex learning settings or parametric assumptions, the proposed method learns the full kernel matrix by leveraging eigendecomposition and Stiefel manifold optimization. The authors also introduce a log-likelihood lower-bounding approach using Jensen's inequality. This methodology represents a significant step forward in subset selection research, as it addresses the non-convexity of the log-likelihood function while avoiding the degeneracies observed in simpler approaches like projected gradient ascent.
Strengths:  
The proposed EM algorithm is both original and technically sound. By reparameterizing the kernel matrix in terms of eigenvalues and eigenvectors, the authors elegantly sidestep the challenges of maintaining positive semi-definiteness during optimization. The use of eigendecomposition and Stiefel manifold optimization is innovative and well-justified. The experimental results demonstrate that the EM algorithm outperforms the baseline "K-Ascent" (KA) method in terms of test log-likelihood, robustness to initialization, and computational efficiency. The approach is particularly advantageous in data-scarce scenarios, achieving up to 16.5% relative gains in test log-likelihood. These findings highlight the practical significance of the method, especially for applications like product recommendation, where modeling negative interactions is critical.
Weaknesses:  
The paper suffers from significant clarity issues. Key terms such as "marginal kernel," "V," and "weight" are left undefined, and the rationale behind certain steps is not adequately explained. The notation is dense and inconsistent, making it difficult for readers to follow the derivations without extensive prior knowledge. Additionally, the structure of the paper is disorganized, with critical details buried in supplementary material. The evaluation, while promising, is incomplete. The authors compare their method only to the KA baseline and omit comparisons with alternative subset selection methods, such as those based on parametric or Bayesian approaches. This limits the broader contextualization of their contributions.
Pro and Con Arguments for Acceptance:  
Pro:  
- The methodology is novel and addresses a challenging, NP-hard problem in subset selection.  
- The experimental results demonstrate clear improvements over the baseline in both performance and runtime.  
- The approach has practical significance for real-world applications like product recommendation.  
Con:  
- The paper is poorly written, with unclear notation and convoluted explanations.  
- The evaluation lacks breadth, as it does not compare against alternative methods beyond KA.  
- The reliance on external references for critical derivations reduces the paper's self-contained clarity.  
Recommendation:  
While the paper makes a meaningful and original contribution to subset selection research, the clarity and evaluation gaps are significant drawbacks. I recommend acceptance conditional on major revisions to improve the writing, define key terms, and expand the experimental comparisons. The proposed EM algorithm has the potential to advance the field, but the presentation must be significantly improved to make the work accessible and impactful.