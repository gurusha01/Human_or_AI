This paper explores the application of deep neural networks (DNNs) to the novel domain of discovering efficient mathematical identities, presenting an attribute grammar framework to represent symbolic expressions. The authors propose two learning-based search strategies—a simple n-gram model and a recursive neural network (RNN)—to guide the exploration of grammar trees, aiming to find computationally efficient versions of target expressions. While the paper is an interesting attempt to bridge machine learning with symbolic computation, it feels preliminary and raises several concerns regarding its quality, clarity, originality, and significance.
Strengths:
1. Novelty of Application: The application of machine learning, particularly RNNs, to symbolic computation and mathematical identity discovery is a creative and underexplored area. The use of attribute grammars and learned representations for symbolic expressions is an interesting contribution.
2. Potential for Broader Impact: The framework has potential applications in compiler optimization, symbolic mathematics, and machine learning, such as efficient computation of partition functions or dropout approximations.
3. Technical Implementation: The integration of continuous representations for symbolic expressions using RNNs is intriguing and could inspire future work in symbolic AI.
Weaknesses:
1. Preliminary Nature of Results: The paper's results are underwhelming. The proposed methods struggle to discover identities for more complex expressions, and the scalability of the approach to broader mathematical domains is unclear. For example, the RBM-2 family, which involves double exponential sums, shows no improvement over random strategies for \( k > 5 \).
2. Restricted Scope: The expressions considered are highly constrained (e.g., homogeneous polynomials with fixed degrees), limiting the generalizability of the approach. Simpler mathematical rules or brute-force methods could suffice for many of the presented examples, undermining the necessity of DNNs.
3. Unclear Performance Gains: The paper claims that the learning-based methods outperform random strategies, but the performance gains are modest and inconsistent. For instance, n-gram models outperform RNNs in many cases, raising questions about the utility of the more complex RNN-based approach.
4. Clarity Issues: While the paper is generally well-written, some sections (e.g., the RNN architecture and training details) are overly dense, making it difficult to reproduce the results. Additionally, the examples provided are not sufficiently diverse to demonstrate the robustness of the framework.
5. Lack of Theoretical Insights: The paper lacks a deeper theoretical analysis of why the proposed methods work (or fail) in certain cases. For example, the failure of RNNs to generalize to non-repetitive patterns is not adequately explained.
Recommendation:
While the paper introduces an interesting idea, its contributions are not yet mature enough for publication at NeurIPS. The limited scope, unimpressive results, and unclear scalability suggest that the work requires further refinement. Strengthening the experimental evaluation, expanding the scope of expressions, and providing deeper theoretical insights would significantly improve the paper's impact. At this stage, I recommend rejection but encourage the authors to continue developing their approach. 
Pro Arguments for Acceptance:
- Novel application of DNNs to symbolic computation.
- Potential for broader impact in AI and mathematics.
- Interesting use of RNNs for continuous representations of symbolic expressions.
Con Arguments for Acceptance:
- Preliminary results with limited scope and scalability.
- Modest performance gains that do not convincingly demonstrate the utility of the approach.
- Restricted problem setting where simpler methods may suffice.
- Lack of clarity and theoretical depth in some sections.