The paper introduces a novel approach to solving large Markov Decision Processes (MDPs) by directly minimizing the Optimal Bellman Residual (OBR) using Difference of Convex (DC) programming. This method diverges from traditional Approximate Dynamic Programming (ADP) techniques like Approximate Value Iteration (AVI) and Approximate Policy Iteration (API). The authors provide strong theoretical motivation by demonstrating that the OBR loss bound is often tighter than those of AVI and API, particularly for discount factors γ > 0.5. They also establish Vapnik-consistency for minimizing the empirical OBR, ensuring that the method is theoretically sound even when the MDP model is unknown. The key contribution lies in framing the OBR minimization as a DC optimization problem, enabling the use of DC algorithms (DCA) to find a local optimum. Theoretical results, including the explicit decomposition of OBR into convex components (Theorem 4), are technically robust and original.
The paper is well-written and organized, with clear explanations of the theoretical framework and proofs. However, the empirical evaluation is limited to a simple artificial MDP (stationary Garnet problems), which restricts the practical impact of the work. While the proposed method performs comparably to LSPI and Fitted-Q learning, it does not demonstrate a clear advantage in terms of performance. The authors highlight reduced variance in the results, which is a potential benefit, but this advantage is not explored in more complex or real-world scenarios. Furthermore, the computational cost of solving multiple convex optimization problems via DCA is not reported, raising concerns about the scalability of the approach.
Strengths:
1. Theoretical Contribution: The paper provides a novel perspective on solving MDPs by directly minimizing OBR, supported by rigorous proofs and a tighter loss bound compared to traditional methods.
2. Clarity: The writing is clear, and the theoretical results are well-explained, making the work accessible to readers with a background in reinforcement learning and optimization.
3. Originality: The framing of OBR minimization as a DC problem is novel and opens up new avenues for leveraging DC optimization techniques in reinforcement learning.
Weaknesses:
1. Empirical Evaluation: The experiments are limited to simple artificial MDPs and do not showcase the method's potential on real-world or more challenging benchmarks.
2. Scalability: The paper does not address the computational cost of DCA, which involves solving multiple convex optimization problems, potentially limiting its applicability to large-scale problems.
3. Practical Impact: While theoretically sound, the method does not outperform existing approaches like LSPI or Fitted-Q learning in terms of performance, reducing its immediate practical significance.
Suggestions for Improvement:
1. Expand the empirical evaluation to include more complex and realistic MDPs to better demonstrate the method's advantages.
2. Report computational costs and discuss scalability, especially for large state-action spaces.
3. Clarify the role of the distribution μ in the theoretical framework and remove redundant equations for conciseness.
4. Address typos and improve the presentation of experimental results to better highlight the method's strengths.
Recommendation:
This paper makes a strong theoretical contribution and introduces a novel approach to solving MDPs, warranting further exploration. However, its limited empirical evaluation and lack of scalability analysis reduce its practical impact. I recommend acceptance as a workshop paper, with the expectation that the authors address the empirical and computational concerns in future work.