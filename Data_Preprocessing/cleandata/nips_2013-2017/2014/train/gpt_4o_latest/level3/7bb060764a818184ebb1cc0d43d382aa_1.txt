The paper introduces a novel class of multi-class boosting algorithms that incorporate regularization based on Rademacher complexities, claiming theoretical and empirical improvements over existing methods. The authors extend prior work on DeepBoost to the multi-class setting, providing new generalization bounds and proposing several ensemble algorithms. Experimental results suggest that these algorithms outperform AdaBoost.MR and multinomial logistic regression, including their L1-regularized variants, on multiple datasets.
While the paper presents interesting ideas, there are significant issues that undermine its contributions. First, the title is misleading; the term "deep" implies the use of deep learning architectures, but the proposed methods rely on shallow decision trees, which could confuse readers. Second, the claim that boosting overfits in practice is based on outdated references and contradicts more recent findings that suggest boosting is robust to overfitting. This weakens the motivation for the proposed regularization approach.
The paper also fails to adequately situate its contributions within the context of prior work. Key state-of-the-art multi-class boosting algorithms, such as AOSO and ABC, are not cited, making it difficult to assess the novelty of the proposed methods. Furthermore, the experimental setup is flawedâ€”validation was performed on the test set, violating standard practices and rendering the results unreliable. When proper cross-validation was applied, the claimed benefits of regularization diminished, with error rates increasing significantly.
Despite these shortcomings, the paper has strengths. The tree-building procedure appears effective, though insufficient details are provided for reproducibility. The theoretical analysis, particularly the novel generalization bounds, is rigorous and could be of interest to the community. However, the dual focus on theoretical and practical aspects is not well-integrated, creating a "two papers in one" issue that dilutes the impact of both.
In conclusion, while the paper introduces promising ideas, the flawed experimental methodology, unsupported claims, and lack of engagement with prior work warrant rejection in its current form. To improve, the authors should revise the title, address the experimental flaws, provide more detailed algorithmic descriptions, and thoroughly compare their methods to existing multi-class boosting algorithms.