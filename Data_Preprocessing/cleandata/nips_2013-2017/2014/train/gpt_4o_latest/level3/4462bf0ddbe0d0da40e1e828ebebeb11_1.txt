The paper presents a novel expectation-maximization (EM) algorithm for learning kernel matrices in determinantal point processes (DPPs), addressing the non-convexity of the log-likelihood optimization problem. Unlike prior work that relied on restricted convex settings or parametric assumptions, the authors propose a method that leverages the eigendecomposition of the kernel matrix, alternating between updating a variational distribution over subsets (a k-DPP) in the E-step and taking gradient steps on eigenvalues and eigenvectors in the M-step. The proposed method outperforms the standard K-Ascent (KA) algorithm, particularly when initialized naively, and demonstrates robustness in data-poor settings. The authors validate their approach on a real-world product recommendation task, achieving up to 16.5% improvement in test log-likelihood.
Strengths:
1. Novelty and Contribution: The paper introduces a novel EM-based approach for learning full kernel matrices in DPPs, which is a significant step beyond prior methods that relied on restrictive assumptions. This addresses a challenging and important problem in subset selection tasks.
2. Empirical Performance: The proposed EM algorithm shows substantial improvements over KA, especially with naive initialization, and demonstrates robustness in scenarios with limited training data. Additionally, it is asymptotically faster than KA, with practical runtime improvements observed in experiments.
3. Theoretical Insights: The authors provide a detailed derivation of the EM algorithm, including efficient updates for eigenvalues and eigenvectors, and discuss the computational complexity of their method.
4. Practical Relevance: The application to product recommendation tasks highlights the practical utility of the method, particularly in scenarios where modeling diversity and negative interactions is critical.
Weaknesses:
1. Complexity of the M-step: While effective, the M-step involves several caveats, including the need for updates on a Stiefel manifold and careful handling of eigenvalue constraints. This makes the approach less elegant and potentially harder to implement compared to simpler alternatives.
2. Initialization Analysis: The paper does not thoroughly analyze the impact of initialization on the EM algorithm. For instance, it would be valuable to explore how initializing EM with KA results affects performance.
3. Metric Interpretability: The use of relative log-likelihood as the primary evaluation metric raises concerns about interpretability and practical significance. Alternative metrics, such as precision or recall in the recommendation task, could provide additional insights.
4. Writing Style: The paper occasionally engages in self-praise (e.g., claims of "elegance") and uses unnecessary bold text in the introduction, which detracts from the overall professionalism of the presentation.
Pro and Con Arguments for Acceptance:
- Pros: The paper addresses a challenging problem with a novel and well-motivated approach, demonstrates clear empirical improvements, and provides theoretical insights into the method's efficiency and robustness.
- Cons: The complexity of the M-step and the lack of a deeper analysis of initialization and evaluation metrics leave room for refinement.
Recommendation: Overall, the paper makes a meaningful contribution to the field of DPP learning and subset selection. While there are areas for improvement, the novelty, empirical results, and practical relevance justify its acceptance. I recommend acceptance with minor revisions to address the concerns raised.