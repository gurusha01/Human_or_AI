The paper presents a Bayesian extension of nonlinear Support Vector Machines (SVMs) using Gaussian processes and integrates this with factor models for joint feature learning and classification. The authors leverage a scaled mixture of normals to represent the hinge loss and propose inference methods based on Expectation Conditional Maximization (ECM) and Markov Chain Monte Carlo (MCMC). The paper also explores the use of the nonlinear Bayesian SVM as a module in larger hierarchical models, such as discriminative factor models, and evaluates the approach on benchmark datasets, gene expression data, and USPS digit classification.
Strengths:
1. Theoretical Contribution: The paper extends the Bayesian linear SVM framework to a nonlinear setting using Gaussian processes, which is a non-trivial and important step for improving the flexibility of Bayesian SVMs.
2. Integration with Factor Models: The integration of nonlinear SVMs into factor models for joint feature learning and classification is a valuable contribution, particularly for high-dimensional data.
3. Inference Methods: The authors provide multiple inference strategies (MCMC, ECM, and FITC approximation) that cater to different computational needs, demonstrating flexibility in implementation.
4. Biological Relevance: The application to gene expression data and the identification of biologically meaningful features (e.g., associations with GO terms) highlight the practical utility of the proposed model.
5. Probabilistic Outputs: The Bayesian formulation allows for the computation of class membership probabilities, which is an advantage over traditional SVMs.
Weaknesses:
1. Lack of Novelty: While the nonlinear extension and integration with factor models are useful, the techniques employed (e.g., kernel tricks, Gaussian processes) are well-established. The paper lacks significant methodological novelty.
2. Non-Convex Optimization: The proposed model introduces a complex non-convex optimization problem, which may limit its practical applicability.
3. Limited Experimental Scope: Experiments are restricted to Gaussian kernels, and the impact of other kernel functions is not explored. This limits the generalizability of the findings.
4. Small Datasets: The datasets used are relatively small, and the lack of large-scale experiments undermines the validation of the proposed approach.
5. Limited Comparisons: The experimental comparisons are restricted to SVM and Gaussian Process Classification (GPC), ignoring more advanced state-of-the-art methods for binary classification.
6. Insufficient Related Work: The discussion of related work, particularly in the context of discriminative feature learning and nonlinear classifiers, is inadequate.
7. Experimental Weaknesses: The experiments fail to convincingly demonstrate the efficacy of the proposed approach, as the performance improvements are marginal and not consistently significant.
Recommendation:
While the paper offers a theoretically sound extension of Bayesian SVMs and demonstrates its utility in specific applications, the lack of methodological novelty, limited experimental scope, and insufficient comparisons to state-of-the-art methods weaken its overall contribution. The paper would benefit from a more comprehensive exploration of kernel functions, larger-scale experiments, and a stronger emphasis on novelty. I recommend rejecting the paper in its current form but encourage the authors to address these issues for future submissions.
Arguments for Acceptance:
- Extends Bayesian SVMs to a nonlinear setting.
- Integrates nonlinear SVMs with factor models, which is useful for high-dimensional data.
- Provides probabilistic outputs and multiple inference methods.
Arguments for Rejection:
- Limited novelty in methodology.
- Experiments are restricted to small datasets and Gaussian kernels.
- Comparisons to state-of-the-art methods are insufficient.
- Results fail to convincingly demonstrate the superiority of the proposed approach.