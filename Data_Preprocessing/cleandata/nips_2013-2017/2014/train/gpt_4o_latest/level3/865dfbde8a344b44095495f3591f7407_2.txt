The paper presents a novel stochastic variational inference (SVI) algorithm tailored for hidden Markov models (HMMs), addressing the challenges posed by chain-based dependencies in time-series data. The authors propose a method that scales noisy subchain gradients and introduces buffering with additional observations to mitigate edge effects, ensuring unbiased gradient estimates. The algorithm, referred to as SVIHMM, is demonstrated to achieve comparable performance to batch variational Bayes (VB) on synthetic datasets and a large genomics dataset, while being computationally efficient.
Strengths:
1. Significant Contribution: The paper makes a meaningful extension of SVI to dependent data settings, which is a non-trivial and impactful advancement over existing methods designed for independent or exchangeable data.
2. Innovative Techniques: The buffering mechanism and adaptive subchain augmentation are well-motivated and provide a practical solution to edge effects in subchain sampling, ensuring convergence guarantees.
3. Empirical Validation: The algorithm is rigorously evaluated on synthetic datasets and a real-world genomics dataset, demonstrating its scalability and effectiveness in handling massive datasets where batch methods are computationally infeasible.
4. Clarity and Organization: The paper is well-written, with clear explanations of the algorithm, theoretical guarantees, and experimental results. The inclusion of synthetic experiments to analyze trade-offs (e.g., subchain length vs. minibatch size) adds depth to the evaluation.
5. Practical Relevance: The application to genomics data highlights the algorithm's utility in real-world scenarios, particularly for large-scale time-series problems.
Weaknesses:
1. Lack of Comparisons: The absence of a direct comparison with Johnson et al. (2014), a seminal work in SVI for HMMs, limits the contextualization of the proposed method's practical value and added complexity.
2. Runtime Evidence: While the paper claims significant computational gains over the EM algorithm, no explicit runtime comparisons are provided, which weakens the argument for efficiency.
3. Subchain Length Justification: The choice of subchain length (L) is critical to the algorithm's performance, yet the paper lacks a convincing theoretical or empirical justification for selecting optimal values of L.
4. Broader Context: The paper does not compare its method with prior SVI algorithms designed for independent time-series data, which could provide a broader perspective on its relative advantages.
5. Computational Complexity: While the complexity of the SVIHMM algorithm is discussed, a direct comparison of computational costs between batch VB and SVIHMM is missing, leaving the efficiency claims partially unsubstantiated.
Arguments for Acceptance:
- The paper addresses a challenging and important problem in Bayesian inference for time-series data.
- The proposed algorithm is innovative, theoretically grounded, and empirically validated on large-scale datasets.
- The work has potential applications in various domains requiring scalable inference for dependent data.
Arguments Against Acceptance:
- The lack of runtime comparisons and broader contextual evaluations limits the ability to assess the practical impact of the proposed method.
- Missing comparisons with Johnson et al. (2014) and other relevant SVI algorithms leave gaps in positioning the work within the existing literature.
Recommendation:
While the paper has some limitations, its contributions to SVI for HMMs are significant and well-executed. Addressing the noted weaknesses, particularly runtime evidence and comparisons with prior work, would strengthen the paper. I recommend acceptance with minor revisions.