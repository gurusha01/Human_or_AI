The paper proposes a novel approach to robust regression by introducing an automatic procedure for down-weighting regression points based on their influence, rather than discarding outliers outright. This method, termed Influence Weighted Subsampling (IWS-LS), is particularly relevant in large-scale corrupted linear regression problems where traditional least squares (LS) methods and their randomized approximations fail due to bias introduced by corrupted observations. The authors also present a randomized approximation of their method, aRWS-LS, which is computationally efficient and scales well with large datasets. The paper is supported by theoretical analysis, a useful theorem, and empirical evaluations on both simulated and real-world datasets.
Strengths:
1. Novelty and Significance: The paper introduces a fresh perspective on robust regression by leveraging influence diagnostics, which is a significant departure from traditional leverage-based subsampling methods. The claim of being the first to suggest weighting by 1/influence is noteworthy, though it requires more substantiation.
2. Theoretical Contributions: The authors provide a rigorous theoretical analysis, including error bounds for their estimator in the corrupted observation model. The results demonstrate that IWS-LS reduces both bias and variance compared to OLS and other randomized approximations.
3. Practical Relevance: The empirical evaluation, particularly on the airline delay dataset, highlights the practical utility of the proposed method in real-world scenarios where data is often corrupted.
4. Computational Efficiency: The randomized approximation (aRWS-LS) achieves scalability, making it suitable for large datasets, a critical requirement in modern machine learning applications.
Weaknesses:
1. Benchmarking: While the paper compares its results to LS and randomized approximations, it does not benchmark against the Maximum Likelihood Estimator (MLE) for the probabilistic model, which could provide a more meaningful comparison.
2. Connections to Classical Statistics: The connection between the weighting function and classical scoring functions (e.g., Cauchy or L1 regression) is not explored in depth. This could strengthen the theoretical foundation and contextualize the work within the broader literature.
3. Literature Engagement: The paper could benefit from a more extensive engagement with the robust regression and errors-in-variables literature. For instance, the suggested 1980s NBER paper appears relevant and should be discussed to clarify the novelty of the proposed estimator.
4. Model Dependency: The authors claim that the estimator is not tightly tied to the probabilistic model, yet the model serves as a key illustrative example. This duality could be clarified further.
Pro Acceptance Arguments:
- The paper addresses a critical gap in robust regression for large-scale corrupted data.
- It introduces a theoretically sound and computationally efficient method.
- The empirical results demonstrate clear improvements over existing methods.
Con Acceptance Arguments:
- The lack of benchmarking against MLE and insufficient exploration of connections to classical statistics weaken the evaluation.
- The novelty claim regarding weighting by 1/influence needs stronger substantiation.
- The paper could engage more deeply with related literature to contextualize its contributions.
Conclusion:
This paper makes a meaningful contribution to robust regression by proposing a novel influence-based subsampling approach. While there are areas for improvement, particularly in benchmarking and literature engagement, the theoretical and empirical results are compelling. I recommend acceptance, contingent on addressing the identified weaknesses in a revised version.