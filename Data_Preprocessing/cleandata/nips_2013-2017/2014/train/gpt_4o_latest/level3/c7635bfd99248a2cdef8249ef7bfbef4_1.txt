This paper presents a novel coarse-to-fine framework, "Inference by Learning" (IbyL), for accelerating optimization in graphical models, specifically targeting the Maximum A Posteriori (MAP) problem. The approach combines multi-scale label pruning with sequential optimization, leveraging a cascade of trained classifiers to progressively reduce the solution space. The authors demonstrate the method's efficacy on classic computer vision tasks such as stereo matching, image restoration, and optical flow estimation, achieving significant speed-ups while maintaining or even improving solution accuracy compared to direct optimization.
Strengths:
1. Originality and Motivation: The proposed method is innovative in its combination of coarse-to-fine optimization with learned label pruning. By training classifiers to intelligently prune labels, the approach avoids heuristic-based pruning, which is often suboptimal. This makes the framework generalizable across different MRF problems.
2. Technical Soundness: The method is well-supported by theoretical insights and experimental validation. The use of features derived from energy functions ensures the framework's applicability to a wide range of problems.
3. Experimental Results: The results are compelling, showing consistent speed-ups (up to 10x in some cases) with minimal loss in accuracy. The method even outperforms direct optimization in terms of energy minimization for certain settings, which is impressive.
4. Clarity: The paper is well-organized, with a clear explanation of the methodology, feature design, and experimental setup. The inclusion of pseudocode and detailed evaluations enhances reproducibility.
Weaknesses:
1. Applicability to Non-Grid Graphs: The experiments focus exclusively on regular grid graphs, which are common in computer vision but not representative of all graphical models. It is unclear how well the method generalizes to irregular or non-grid graph structures.
2. Integration with Learning Procedures: While the method relies on trained classifiers, its integration within a broader learning pipeline (e.g., end-to-end learning frameworks) is not explored. This limits its applicability in modern machine learning contexts.
3. Significance and Related Work: The paper provides limited discussion on how it advances the state-of-the-art relative to prior work. While the results are promising, the lack of a comprehensive comparison with recent methods (e.g., message-passing techniques) makes it difficult to assess the broader impact.
4. Feature Design: Although the features are generic, the reliance on manually designed features may limit scalability. Incorporating learned features or jointly training the classifiers could improve performance.
Recommendation:
- Pros for Acceptance: The method is original, technically sound, and demonstrates strong experimental results. It addresses a challenging problem in graphical model optimization and offers a general framework with potential for further extensions.
- Cons for Acceptance: The limited scope of experiments (grid graphs only) and the lack of integration with modern learning pipelines reduce the paper's immediate impact.
Suggestions for Improvement:
1. Extend the evaluation to non-grid graphs to demonstrate broader applicability.
2. Explore the integration of the method into end-to-end learning frameworks.
3. Provide a more detailed comparison with state-of-the-art methods, particularly message-passing approaches.
4. Consider jointly training the cascade of classifiers or incorporating learned features to enhance scalability.
In conclusion, this paper makes a valuable contribution to graphical model optimization, and with additional work to address its limitations, it has the potential to significantly impact the field. I recommend acceptance with minor revisions.