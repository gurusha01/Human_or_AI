The paper presents an extension of log-bilinear language models by introducing a third-order tensor-based framework, enabling three-way interactions between words, context, and attributes. The authors draw inspiration from factored Restricted Boltzmann Machines (RBMs), adapting the factorization approach for efficiency. While factored models are not new, their application to diverse language tasks such as sentiment classification, cross-lingual document classification, and blog authorship attribution is a novel contribution. The notion of conditional word similarity, where word meanings shift depending on the conditioned attributes, is particularly compelling and well-demonstrated through qualitative experiments.
Strengths:  
The paper is well-written, clearly structured, and engaging. It effectively communicates the potential of factored log-bilinear models for language tasks. The authors provide a solid theoretical foundation, extending the log-bilinear model with tensor factorization and demonstrating its applicability to a variety of tasks. The conditional word similarity experiment is a standout, showcasing the model's ability to capture nuanced contextual shifts in word meanings. Additionally, the blog authorship attribution experiment highlights the broader applicability of the proposed approach. The inclusion of t-SNE visualizations and qualitative examples adds depth to the analysis. The paper also draws meaningful parallels with prior work, such as factored RBMs and multiplicative neural networks, situating the contribution within the broader literature.
Weaknesses:  
Despite its conceptual novelty, the quantitative performance of the proposed model is underwhelming across tasks. While the authors suggest that hyper-parameter tuning could improve results, this claim is speculative and constrained by the challenges of small datasets and regularization. For instance, the sentiment classification results lag behind state-of-the-art methods like recursive neural networks and Paragraph Vectors. Similarly, in cross-lingual document classification, the model is outperformed by BiCVM+ and BAE-corr, which incorporate additional regularization or reconstruction terms. The reliance on parallel data for cross-lingual tasks may also limit scalability to low-resource settings. Furthermore, the experimental setup could benefit from more rigorous baselines and ablation studies to isolate the contributions of the tensor factorization.
Pro Acceptance:  
- Novel application of tensor factorization to diverse language tasks.  
- Well-executed conditional word similarity experiment.  
- Clear writing and engaging presentation.  
Con Acceptance:  
- Quantitative results are not state-of-the-art.  
- Speculative claims about performance improvements.  
- Limited scalability to larger datasets or low-resource settings.  
Recommendation:  
While the paper demonstrates conceptual novelty and provides interesting insights, the underwhelming quantitative results and speculative nature of some claims suggest that it is not yet ready for acceptance at a top-tier conference like NeurIPS. However, it is a promising direction, and with further refinement and stronger empirical results, it could make a significant contribution to the field. I recommend a weak rejection, encouraging the authors to address the noted limitations in future iterations.