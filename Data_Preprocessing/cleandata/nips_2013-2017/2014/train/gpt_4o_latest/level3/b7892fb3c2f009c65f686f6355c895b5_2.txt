This paper presents a novel Bayesian formulation for nonlinear Support Vector Machines (SVMs), leveraging Gaussian Processes (GPs) and extending prior work on Bayesian SVMs. The authors introduce a scalable mixture representation of the hinge loss function, enabling its integration into a GP framework. They further incorporate this nonlinear Bayesian SVM into a discriminative factor model, allowing joint feature learning and classifier design. The methodology is supported by a suite of inference algorithms, including Markov Chain Monte Carlo (MCMC), Expectation-Conditional Maximization (ECM), and scalable approximations like Fully Independent Training Conditional (FITC). Extensive experiments demonstrate the model's efficacy on benchmark datasets, gene expression data, and handwritten digit classification tasks.
Strengths:
1. Methodology: The paper introduces a clear and comprehensive extension of Bayesian SVMs to nonlinear settings, addressing limitations of prior linear models. The integration of a GP-based hinge loss function is both elegant and technically sound.
2. Algorithms: The development of multiple inference methods, including MCMC and ECM, showcases the versatility of the approach. The use of FITC for scalability is particularly commendable, as it addresses the computational challenges of GPs.
3. Quality: The work demonstrates a strong theoretical foundation, with well-supported claims through rigorous derivations and experiments. The integration of state-of-the-art methods results in a significant improvement over traditional Bayesian SVMs.
4. Clarity: The paper is well-organized and clearly written, with sufficient detail to allow reproducibility. The authors provide insightful explanations of the model's behavior and its advantages over existing approaches.
5. Originality: The combination of Bayesian SVMs with nonlinear GPs and their application in discriminative factor modeling is novel. The generalization of the hinge loss to a skewed Laplace distribution further enhances the originality of the work.
6. Significance: The proposed approach addresses a critical gap in the literature by enabling nonlinear classification within a Bayesian framework. The demonstrated improvements in accuracy and interpretability make this a highly impactful contribution.
Weaknesses:
1. Computational Complexity: While the authors address scalability using FITC, the cubic complexity of GPs remains a limitation for very large datasets. The paper could benefit from a more detailed discussion of trade-offs between accuracy and computational efficiency.
2. Empirical Comparisons: Although the experiments are extensive, the performance gains over baseline methods are modest in some cases. Additional comparisons with other state-of-the-art nonlinear classifiers could strengthen the empirical evaluation.
3. Practicality: The reliance on MCMC for some tasks may limit the approach's practicality in time-sensitive applications. While ECM and VB-EM offer alternatives, their performance relative to MCMC is not fully explored.
Recommendation:
I strongly recommend acceptance of this paper. Its contributions are both theoretically and practically significant, advancing the state of the art in Bayesian SVMs and nonlinear classification. The integration of GPs into a max-margin framework is innovative, and the results demonstrate clear advantages in both accuracy and interpretability. Addressing the computational challenges in future work could further enhance its impact. 
Arguments for Acceptance:
- Novel and rigorous methodology.
- Strong theoretical and empirical contributions.
- Clear writing and reproducibility of results.
- High relevance to the NIPS community.
Arguments Against Acceptance:
- Computational limitations for large-scale datasets.
- Modest performance gains in some experiments.
Overall, the paper represents a substantial and commendable contribution to the field.