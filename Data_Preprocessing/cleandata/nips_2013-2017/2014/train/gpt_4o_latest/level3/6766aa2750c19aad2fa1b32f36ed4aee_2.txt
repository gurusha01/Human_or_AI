This paper presents a hybrid policy search method that combines model-free and model-based techniques to optimize trajectory distributions under unknown dynamics. The key contribution is the use of iteratively refitted local linear models and a KL-divergence constraint to ensure stable convergence during trajectory optimization. The method is further integrated with guided policy search (GPS) to train neural network policies, enabling the learning of complex, parameterized policies for partially observed and contact-rich robotic tasks.
The paper is technically sound and well-supported by experimental results. The authors demonstrate the effectiveness of their approach on various robotics tasks, including peg insertion, octopus arm control, swimming, and walking. The results show faster convergence and superior performance compared to prior methods, even without relying on a global model or a Gaussian Mixture Model (GMM) prior. The use of a KL-divergence constraint instead of backtracking line search is a notable, albeit incremental, improvement to the iLQG framework. The experiments convincingly highlight the method's ability to handle discontinuous dynamics and high-dimensional state spaces, which are challenging for traditional model-based methods.
However, the paper has several limitations. First, while the GMM prior improves sample efficiency, its construction and scalability to high-dimensional tasks, such as 3D walking, are not thoroughly addressed. The reliance on example demonstrations for initializing walking tasks also raises concerns about the method's general applicability to more complex, real-world scenarios. Second, the paper lacks clarity on the contact model used for evaluations, which is critical for ensuring fair comparisons with other iLQG-based methods. Third, the novelty of using KL-divergence for line search is incremental, and the paper does not sufficiently differentiate itself from prior work in this regard. Finally, the scalability of the approach to realistic robots with complex dynamics and sensory inputs remains an open question.
Strengths:
1. Combines model-free and model-based techniques effectively, achieving faster convergence.
2. Demonstrates strong performance on challenging robotics tasks with discontinuous dynamics.
3. Integrates trajectory optimization with GPS to train expressive neural network policies.
4. Provides a well-written and organized presentation of the method and experiments.
Weaknesses:
1. Incremental novelty in the use of KL-divergence for line search.
2. Limited discussion on the scalability of the GMM prior and the approach to high-dimensional tasks.
3. Lack of clarity on the contact model used for evaluations.
4. Dependence on example demonstrations for walking tasks limits generalizability.
Recommendation:
While the paper provides valuable insights and demonstrates strong experimental results, the incremental nature of its contributions and concerns about scalability warrant further investigation. I recommend acceptance with minor revisions, provided the authors address the scalability of the GMM prior, clarify the contact model, and discuss the method's applicability to more complex, real-world scenarios.