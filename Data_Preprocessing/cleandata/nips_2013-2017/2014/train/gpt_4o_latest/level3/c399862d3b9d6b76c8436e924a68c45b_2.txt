The paper addresses the problem of Bandit Convex Optimization (BCO) with bandit feedback, focusing on strongly-convex and smooth loss functions. The authors propose an algorithm that achieves a regret bound of \(O(\sqrt{T} \log T)\), which is near-optimal up to logarithmic factors. The key contribution lies in employing a "shrinking exploration" scheme, enabled by the strong convexity of the losses, in contrast to prior works that used time-invariant exploration schemes. The authors also leverage self-concordant barriers for regularization and provide a detailed regret analysis using standard proof techniques in online convex optimization.
Strengths:
1. Improved Convergence Rate: The paper achieves a convergence rate of \(T^{1/2}\), which matches the known lower bound for this setting. This is a meaningful improvement over prior works that achieved \(T^{3/4}\) or \(T^{2/3}\) regret for related settings.
2. Clear Theoretical Framework: The authors provide a rigorous theoretical analysis of the algorithm, including proofs and bounds for exploration and exploitation terms. The use of self-concordant barriers and shrinking exploration is well-motivated and clearly explained.
3. Incremental Progress: While incremental, the paper refines existing techniques and provides a more efficient algorithm for a specific class of loss functions. The results are consistent with the state of the art and advance our understanding of regret bounds under strong convexity and smoothness assumptions.
Weaknesses:
1. Incremental Contribution: The algorithm is a minor adaptation of existing methods, particularly those using self-concordant barriers and gradient estimations. The proof techniques are nearly identical to prior work, with only slight modifications to account for strong convexity.
2. Limited Scope: The paper focuses on a highly specific setting (strongly-convex and smooth losses). The broader and more challenging question of achieving \(T^{1/2}\) regret under weaker assumptions, such as Lipschitz continuity or general convexity, remains unresolved.
3. Practical Impact: The practical significance of the work is unclear, as the assumptions (strong convexity and smoothness) may not hold in many real-world applications. Additionally, the algorithm's dependence on self-concordant barriers may limit its applicability.
Arguments for Acceptance:
- The paper achieves a theoretically significant result by matching the known lower bound for strongly-convex and smooth BCO.
- The analysis is rigorous, and the techniques, while incremental, are sound and well-executed.
- The work contributes to the ongoing exploration of optimal regret rates in bandit learning.
Arguments Against Acceptance:
- The contribution is incremental, with limited novelty in the algorithm and proof techniques.
- The paper does not address the more general and open question of achieving \(T^{1/2}\) regret under weaker assumptions, which would have broader implications.
- The practical relevance of the work is limited due to restrictive assumptions.
Recommendation:
While the paper is technically sound and provides a modest improvement in a specific setting, its incremental nature and narrow scope limit its impact. I recommend weak rejection, as the work does not sufficiently advance the state of the art to warrant publication at a top-tier conference like NeurIPS. However, with additional contributions addressing broader settings or practical applications, the work could be more impactful.