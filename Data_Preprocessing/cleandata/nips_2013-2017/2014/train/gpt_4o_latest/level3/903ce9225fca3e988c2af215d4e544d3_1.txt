This paper addresses the challenging problem of multi-armed bandits (MAB) with non-stationary mean rewards, where the total variation of the rewards over time is bounded by a variation budget \( V_T \). The authors provide a rigorous theoretical characterization of the regret complexity for this setting, establishing both upper and lower bounds of order \( O(T^{2/3}) \). Notably, the upper bound is achieved using a phased version of the EXP3 algorithm, which resets periodically based on the variation budget. This work bridges the gap between adversarial and stochastic MAB frameworks, offering insights into the exploration-exploitation tradeoff in non-stationary environments.
Strengths:
1. Technical Rigor: The paper provides a thorough theoretical analysis, including minimax lower bounds and a matching algorithmic upper bound, demonstrating the optimality of the proposed approach (up to logarithmic factors). The proofs are well-structured and align with the stated claims.
2. Novelty: The formulation of MAB with finite variation is a significant generalization of existing non-stationary bandit problems, capturing a broader class of temporal uncertainties than prior work.
3. Clarity: The paper is well-written and logically organized, making it accessible to readers familiar with MAB literature. The connection between adversarial and non-stationary stochastic settings is particularly insightful.
4. Impact: By quantifying the "price of non-stationarity" and characterizing regret as a function of \( V_T \), this work provides a foundation for future research on adaptive algorithms in dynamic environments.
Weaknesses:
1. State-of-the-Art Coverage: The paper does not adequately discuss related work in state-dependent ergodic bandits or cite recent contributions such as Ortner (2012) and Azar (2014). This omission limits the contextualization of the proposed results within the broader literature.
2. Assumption of Known \( VT \): The proposed algorithm requires prior knowledge of the variation budget \( VT \), which may not be realistic in practical scenarios. While the authors briefly discuss this limitation, they do not propose adaptive strategies for unknown or misestimated \( V_T \).
3. EXP3 Limitations: The use of EXP3, designed for adversarial settings, does not exploit the stochastic structure of the problem. The paper does not explore problem-dependent bounds or compare the performance of stochastic bandit algorithms like UCB in this setting, which could provide additional insights.
4. Practical Considerations: The paper focuses on theoretical guarantees but lacks empirical validation. Experiments comparing the proposed algorithm to other baselines in non-stationary environments would strengthen its practical relevance.
Recommendation:
I recommend acceptance with minor revisions. The paper makes a strong theoretical contribution by characterizing the regret complexity of MAB with finite variation and proposing a near-optimal algorithm. However, the authors should address the missing citations, discuss adaptive approaches for unknown \( V_T \), and consider extending their analysis to problem-dependent settings. Including experimental results would also enhance the paper's impact.
Arguments for Acceptance:
- Strong theoretical contributions with rigorous proofs.
- Novel problem formulation and insights into non-stationary bandits.
- Clear writing and logical presentation.
Arguments Against Acceptance:
- Limited discussion of related work and practical considerations.
- Dependence on known \( V_T \), which may not be realistic. 
Overall, this paper advances the state of the art in non-stationary MAB and provides a solid foundation for further research in this area.