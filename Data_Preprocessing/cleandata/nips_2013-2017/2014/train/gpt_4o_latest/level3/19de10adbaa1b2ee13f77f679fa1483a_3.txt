This paper introduces Deep Attention Selective Networks (dasNet), a novel convolutional neural network (CNN) architecture that incorporates attention-based feedback for top-down processing. By dynamically altering convolutional filter sensitivities during evaluation, dasNet achieves state-of-the-art results on the CIFAR-10 and CIFAR-100 datasets. The key innovation lies in its ability to iteratively refine classification decisions by leveraging both bottom-up and top-down signals, a departure from traditional feedforward CNNs. The use of Separable Natural Evolution Strategies (SNES) to train the attention policy in a high-dimensional parameter space is another notable contribution.
Strengths:
1. Novelty and Innovation: The integration of attention-based feedback into a CNN framework is a significant step forward, inspired by biological vision systems. This approach effectively models selective attention and demonstrates the potential of combining reinforcement learning with deep learning for vision tasks.
2. State-of-the-Art Results: The model achieves superior performance on CIFAR datasets, particularly in correcting misclassifications in ambiguous cases. This establishes dasNet as a strong contender in advancing the state of the art for unaugmented image classification.
3. Evaluation Across Time: The iterative processing of images and the use of gated signals to constrain representations are compelling features. The analysis of dynamics over multiple steps provides valuable insights into the model's behavior.
4. Visualization: The inclusion of visualizations, such as filter emphasis changes and class probability dynamics, enhances the interpretability of the model's internal mechanisms.
5. Biologically Inspired Design: The paper draws from neuroscience, aligning dasNet's architecture with principles of human visual processing, such as feedback connections and selective attention.
Weaknesses:
1. Scalability: While the results on CIFAR datasets are impressive, the paper does not address scalability to larger and more complex datasets like ILSVRC. Demonstrating performance on such datasets would significantly strengthen the paper's claims.
2. Failure Cases: The paper lacks a discussion of failure cases or limitations of the attention policy. Understanding where the model struggles would provide a more balanced evaluation.
3. Computational Cost: Training dasNet, particularly the reinforcement learning component, is computationally intensive. The paper could benefit from a discussion of efficiency and potential optimizations.
4. Broader Applicability: While the method is innovative, its practical applicability to real-world tasks beyond CIFAR datasets remains unclear. Experiments on larger datasets or more diverse tasks would enhance its relevance.
Recommendation:
I recommend acceptance of this paper, with the caveat that the authors address scalability and failure cases in a future revision or supplementary work. The combination of attention-based feedback and CNNs is a promising direction for advancing deep learning in vision tasks, and this paper provides a strong foundation for further exploration. Proposals for improving computational efficiency and extending the approach to larger datasets would also be valuable for broader impact.