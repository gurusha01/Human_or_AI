The paper introduces Universal Option Models (UOMs), a novel framework for modeling options in reinforcement learning (RL) that is independent of reward functions. By extending discounted state occupancy functions to options, the authors establish a theoretical connection between an option's discounted state occupancy and its return (Theorem 1). This reward-independent formulation enables efficient computation of option returns for dynamically specified reward functions, addressing a key limitation of traditional option models. The paper further extends UOMs to large state spaces using linear function approximation, with consistency and convergence guarantees provided through Theorems 2, 3, and 4. Experimental results in two domains—real-time strategy games and article recommendation—demonstrate the computational efficiency and accuracy of UOMs compared to prior methods, particularly the Linear Option Expectation Model (LOEM) proposed by Sorg & Singh (2010).
Strengths:
1. Novelty and Significance: The introduction of UOMs represents a significant advancement in abstract planning with options, particularly in settings with multiple dynamic reward functions. The ability to decouple option models from reward functions is a compelling contribution with practical implications for large-scale RL systems.
2. Theoretical Contributions: The paper provides rigorous theoretical results, including the universality of UOMs (Theorem 1) and their extension to linear function approximation (Theorems 2 and 3). These results are foundational and demonstrate the robustness of the proposed approach.
3. Empirical Validation: The experiments are well-designed and showcase the advantages of UOMs in both accuracy and computational efficiency. The comparison with LOEM highlights the practical benefits of UOMs in real-world applications like strategy games and article recommendation.
4. Clarity of Methodology: The experimental methodology is clearly explained, making it easy to follow the implementation and results.
Weaknesses:
1. Lack of Related Work: The paper surprisingly omits references to relevant RL literature from 2000–2010, aside from unrelated topics like PageRank. This oversight weakens the contextualization of UOMs within the broader RL landscape.
2. Ambiguity in Definitions: The notion of a "reward-less MDP," central to Theorems 1 and 2, is not formally defined, leading to potential confusion about its meaning and applicability.
3. Notation Issues: The use of inner product notation for \( u^o(s) \) and \( r^\pi \) is inconsistent with the definition of \( R^o(s) \), which may hinder readability. While this does not affect the core results, it detracts from the paper's clarity.
4. Incomplete Theoretical Proofs: Theorem 4 relies on the "Robbins-Monro conditions," which are neither defined nor cited. Additionally, the proof is only sketched, lacking the rigor expected for a theoretical contribution.
5. Theoretical Rigor: While the main ideas are novel, the theoretical aspects require better articulation and precision. For example, the convergence guarantees for the learning algorithm are not thoroughly justified.
Recommendation:
The paper makes a significant contribution to the field of RL, particularly in the domain of option modeling and planning with dynamic reward functions. Its theoretical and empirical results are promising, and the practical implications of UOMs are substantial. However, the lack of clarity in some definitions, incomplete proofs, and insufficient contextualization of related work detract from its overall quality. I recommend acceptance with major revisions, focusing on addressing the theoretical ambiguities, improving the rigor of proofs, and providing a more comprehensive discussion of related work.
Arguments for Acceptance:
- Novel and impactful contribution to option modeling.
- Strong empirical results demonstrating practical utility.
- Theoretical foundations, though incomplete, are promising.
Arguments against Acceptance:
- Ambiguity in key definitions (e.g., "reward-less MDP").
- Lack of rigor in the proof of Theorem 4.
- Insufficient references to prior RL literature.
Overall, the paper is a valuable contribution but requires revisions to fully meet the standards of the conference.