The paper presents a novel Bayesian formulation for nonlinear support vector machines (SVMs), extending prior work on linear Bayesian SVMs. By introducing a Gaussian process (GP) prior on the decision function and replacing the hinge loss with a skewed Laplace likelihood, the authors achieve a more flexible and theoretically grounded model. The work also addresses limitations of prior Bayesian SVM formulations by introducing a well-defined prior on the latent scale parameters, improving the model's interpretability and computational properties. Two optimization methods—Markov Chain Monte Carlo (MCMC) and Expectation Conditional Maximization (ECM)—are proposed for inference, with additional scalability achieved through the Fully Independent Training Conditional (FITC) approximation.
Strengths:
1. Technical Innovation: The extension from linear to nonlinear Bayesian SVMs using GP priors is a significant contribution. The use of a skewed Laplace likelihood instead of the hinge loss provides a valid probabilistic framework, addressing normalization issues in prior work.
2. Theoretical Rigor: The paper provides clear derivations, including the posterior distributions and computational strategies, showcasing a deep understanding of Bayesian inference.
3. Scalability: The inclusion of FITC and VB-EM approximations demonstrates the authors' awareness of computational challenges and their ability to address them effectively.
4. Empirical Results: The experiments on benchmark datasets, USPS digit classification, and gene expression data highlight the model's superior performance compared to traditional SVMs and GP classifiers. The biological relevance of the extracted features in the gene expression task adds practical significance.
5. Clarity: The manuscript is well-organized and provides sufficient background for readers to understand the contributions. The explanations of the skewed Laplace likelihood and its implications are particularly clear.
Weaknesses:
1. Predictive Distribution Derivation: The derivation of the predictive distribution in Eq. (11) is not detailed, which could hinder reproducibility for readers unfamiliar with GP-based models.
2. Computational Complexity: While the FITC approximation reduces complexity, the cubic scaling of the full GP model remains a limitation for large datasets. This is acknowledged but not fully addressed.
3. Comparison with Related Work: Although the paper compares its model to GP classifiers and traditional SVMs, a more detailed discussion of how it differs from other Bayesian hierarchical models (e.g., GP-LVM) would strengthen its positioning.
4. Limited Ablation Studies: The paper could benefit from additional ablation studies to isolate the contributions of the skewed Laplace likelihood, the GP prior, and the hierarchical structure.
Arguments for Acceptance:
- The paper introduces a novel and theoretically sound extension to Bayesian SVMs, addressing key limitations of prior work.
- The empirical results demonstrate the model's effectiveness across diverse tasks, with practical implications for feature learning and classification.
- The proposed methods are relevant to the NIPS community, advancing the state of the art in Bayesian machine learning and SVMs.
Arguments Against Acceptance:
- The lack of detailed derivations for the predictive distribution and limited discussion of computational trade-offs may hinder reproducibility and practical adoption.
- The paper could provide a more comprehensive comparison with related Bayesian models.
Recommendation:
Overall, this paper is a strong contribution to the field of Bayesian machine learning and is well-suited for the NIPS audience. While some areas could be improved, the technical novelty, empirical performance, and clarity of presentation outweigh the weaknesses. I recommend acceptance.