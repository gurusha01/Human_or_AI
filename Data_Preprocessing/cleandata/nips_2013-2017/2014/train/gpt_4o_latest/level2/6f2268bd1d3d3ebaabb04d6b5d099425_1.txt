The paper presents a novel approach to jointly address clustering and outlier detection using an extension of the facility location problem, formulated as an integer programming task. The authors propose a Lagrangian relaxation to derive a scalable subgradient-based algorithm, which automatically determines the number of clusters and identifies outliers. The paper claims that this joint approach leads to compact, semantically coherent clusters, robust clustering results, and interpretable outliers. The method is evaluated on synthetic and real-world datasets, including MNIST, and is compared against existing methods like k-means-- and APOC.
Strengths:
1. Novelty and Significance: The integration of clustering and outlier detection into a single framework is a significant contribution. The ability to infer the number of clusters directly from the data is a major advantage over traditional methods like k-means--, which require the number of clusters to be predefined.
2. Theoretical Rigor: The paper provides a thorough theoretical analysis of the Lagrangian relaxation, demonstrating its equivalence to the linear programming relaxation and analyzing the convergence properties of the subgradient method. The proof of total unimodularity for the constraint matrix is a strong theoretical contribution.
3. Scalability and Practicality: The proposed algorithm is computationally efficient, has a small memory footprint, and is amenable to large-scale datasets, as demonstrated by experiments on synthetic data and MNIST. The comparison of memory usage and runtime with APOC highlights the practical advantages of the method.
4. Experimental Validation: The authors provide extensive experimental results, including synthetic data to evaluate robustness and real-world data to demonstrate practical utility. Metrics like the Jaccard index, LOF ratio, and V-Measure are appropriate for assessing clustering and outlier detection quality.
Weaknesses:
1. Clarity: While the paper is generally well-written, some sections, particularly the theoretical analysis, are dense and could benefit from clearer explanations or illustrative examples. For instance, the proof of total unimodularity might be challenging for readers unfamiliar with integer programming.
2. Comparison with Baselines: Although the paper compares the proposed method with k-means-- and APOC, the choice of baselines could be expanded to include more recent or diverse clustering and outlier detection methods. Additionally, the authors do not discuss why APOC underperforms relative to LR in some cases.
3. Sensitivity to Parameters: The paper briefly discusses the impact of the cost scaling factor (Î±) and the number of outliers (`), but a more detailed sensitivity analysis would strengthen the claims about robustness.
4. Interpretability of Results: While the authors claim that the outliers are interpretable, the discussion of how this interpretability is achieved or measured is limited. For example, the MNIST results could include a more detailed qualitative analysis of the selected outliers.
Recommendation:
The paper makes a strong contribution to the field of clustering and outlier detection, offering a theoretically sound and practically scalable solution. However, the clarity of the theoretical sections and the breadth of baseline comparisons could be improved. I recommend acceptance with minor revisions to address these issues.
Pro and Con Arguments:
Pro:
- Novel formulation combining clustering and outlier detection.
- Strong theoretical guarantees and analysis.
- Scalable, memory-efficient, and practical for large datasets.
- Extensive experimental validation with meaningful metrics.
Con:
- Dense theoretical sections may hinder accessibility.
- Limited baseline comparisons and sensitivity analysis.
- Insufficient discussion of interpretability and qualitative results.
Overall, this paper is a valuable contribution to the field and aligns well with the goals of the conference.