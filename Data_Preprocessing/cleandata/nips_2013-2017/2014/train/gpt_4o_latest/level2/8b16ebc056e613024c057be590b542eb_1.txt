The paper presents a novel approach to infer neural network connectivity from Calcium imaging time series using inverse covariance estimation enhanced by a convolution filter. The authors propose a supervised optimization algorithm to learn signal processing parameters, significantly improving the accuracy and computational efficiency of inverse covariance estimation. The key claim is that their method achieves competitive AUC scores (comparable to the winning Kaggle Connectomics solution) while drastically reducing training and prediction times, making it more practical for real-world applications.
Strengths:
1. Novelty and Practicality: The proposed convolution-based preprocessing is a significant improvement over traditional thresholding methods. The supervised learning of signal processing parameters is innovative and addresses the limitations of brute force optimization methods like grid search and coordinate ascent.
2. Efficiency: The algorithm achieves competitive AUC scores in under two hours of training on a CPU and predicts new networks in under 15 minutes. This is a substantial improvement over the computationally intensive methods used in the Kaggle competition.
3. Reproducibility: The paper provides detailed descriptions of the methodology, including equations, optimization techniques, and implementation details, which enhance reproducibility.
4. Significance: The method has potential applications beyond neuroscience, such as credit risk contagion and social network analysis, broadening its impact.
5. Acknowledgment of Limitations: The authors discuss the limitations of their Gaussian model assumption and the lack of directionality in network estimation, demonstrating a balanced evaluation of their work.
Weaknesses:
1. Limited Benchmarking: While the method is compared to the Kaggle competition results, additional benchmarking against other state-of-the-art methods (e.g., Bayesian networks or Granger causality) would strengthen the evaluation.
2. Assumption of Gaussianity: The reliance on a Gaussian model for non-Gaussian neural firing data is acknowledged but not fully addressed. While the convolution filter compensates empirically, a more rigorous justification or alternative modeling approach would be beneficial.
3. Generality: The method's performance on networks with different sizes or parameters is not extensively tested, which raises questions about its generalizability.
4. Sparse Network Metrics: While AUC is the primary metric, the paper briefly mentions precision (PREC@k) but does not explore it in depth. For sparse networks, additional metrics like precision-recall curves could provide a more comprehensive evaluation.
Pro Acceptance Arguments:
- The method is innovative, computationally efficient, and achieves competitive results.
- The paper is well-written, with clear explanations and detailed methodology.
- The approach has potential applications beyond neuroscience.
Con Acceptance Arguments:
- Limited benchmarking against alternative methods.
- Assumptions about Gaussianity and lack of exploration of other loss functions for sparse networks.
Recommendation: Accept with minor revisions. The paper makes a strong contribution to the field of neural connectivity inference, but additional benchmarking and exploration of alternative metrics would enhance its impact.