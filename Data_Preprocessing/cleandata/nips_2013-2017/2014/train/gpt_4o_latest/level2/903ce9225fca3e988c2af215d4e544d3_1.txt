The paper addresses the multi-armed bandit (MAB) problem under non-stationary reward distributions, presenting a novel framework that bridges the gap between stochastic and adversarial MAB settings. The authors introduce a variation budget, \( VT \), to model temporal uncertainties in rewards, and fully characterize the regret complexity of this class of problems. The main contributions include a theoretical lower bound on regret, a near-optimal policy (Rexp3), and insights into the exploration-exploitation tradeoff in non-stationary environments. The regret is shown to scale as \( (K VT)^{1/3} T^{2/3} \), mapping the extent of reward variation to achievable performance.
Strengths
1. Novelty and Scope: The paper extends the classical MAB framework to a general non-stationary setting, introducing a flexible variation budget that captures a wide range of real-world scenarios. This is a significant departure from prior work, which often assumes finite or structured changes in rewards.
2. Theoretical Rigor: The authors provide tight lower and upper bounds on regret, demonstrating minimax optimality (up to logarithmic factors). The proofs are thorough and build on established techniques while adapting them to the non-stationary setting.
3. Practical Relevance: The proposed Rexp3 policy is computationally efficient and builds on existing adversarial MAB algorithms, making it adaptable for real-world applications. The discussion on the tradeoff between "remembering" and "forgetting" is insightful and relevant for practitioners.
4. Connections to Prior Work: The paper situates its contributions within the broader MAB literature, contrasting its results with stationary MAB settings and other non-stationary formulations (e.g., finite changes, Brownian motion). This contextualization enhances the paper's impact.
Weaknesses
1. Assumptions on \( VT \): The policy relies on prior knowledge of the variation budget \( VT \), which may not always be available in practice. While the authors briefly discuss this limitation, the exploration of adaptive policies that do not require \( V_T \) is left as future work.
2. Empirical Validation: The paper lacks experimental results to validate the theoretical findings. While the focus is on theoretical contributions, empirical evaluations would strengthen the practical relevance of the proposed policy.
3. Clarity and Accessibility: The paper is dense and highly technical, which may limit accessibility for non-experts. Some key ideas, such as the intuition behind the lower bound construction, could be better explained with illustrative examples or diagrams.
Pro and Con Arguments for Acceptance
Pro:
- The paper makes a significant theoretical contribution by characterizing the regret complexity of non-stationary MAB problems.
- The proposed Rexp3 policy is practical and achieves near-optimal performance.
- The work is well-grounded in the existing literature and advances the state of the art.
Con:
- The reliance on \( V_T \) limits the applicability of the results in scenarios where the variation budget is unknown.
- The lack of empirical validation weakens the paper's practical impact.
Recommendation
This paper is a strong theoretical contribution to the field of MABs and non-stationary optimization. While the absence of experiments and reliance on \( V_T \) are notable limitations, the novelty, rigor, and relevance of the work outweigh these concerns. I recommend acceptance, with a suggestion to include empirical results or further discussion on adaptive policies in future iterations.