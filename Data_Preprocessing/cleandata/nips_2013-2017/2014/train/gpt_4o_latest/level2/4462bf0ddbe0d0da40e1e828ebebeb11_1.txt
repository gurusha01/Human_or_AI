The paper presents a novel algorithm for learning the full kernel matrix of determinantal point processes (DPPs) by leveraging eigendecomposition and an expectation-maximization (EM) framework. The authors address the long-standing challenge of optimizing the non-convex log-likelihood function for DPPs, which has been conjectured to be NP-hard. Unlike prior approaches that impose restrictive parameterizations or partial learning of the kernel matrix, this work proposes a method that allows for unconstrained learning of the kernel matrix, preserving the diversity-seeking nature of DPPs. The algorithm is validated on a real-world product recommendation task, demonstrating significant improvements in test log-likelihood (up to 16.5%) compared to the naive projected gradient ascent approach.
Strengths:
1. Novelty and Contribution: The paper introduces a novel EM-based learning algorithm for DPPs, which is a significant step forward compared to prior restricted approaches. The use of eigendecomposition to parameterize the kernel matrix and avoid problematic projection steps is innovative and well-motivated.
2. Practical Relevance: The application to product recommendation tasks is compelling, as DPPs are well-suited for modeling diversity. The real-world dataset from Amazon baby registries adds credibility to the practical utility of the proposed method.
3. Empirical Validation: The experiments are thorough, comparing the proposed EM algorithm with the baseline projected gradient ascent method across multiple categories. The results convincingly demonstrate the advantages of the EM approach in terms of robustness, efficiency, and performance.
4. Theoretical Insights: The paper provides clear derivations for the EM updates, including efficient computation of eigenvalue and eigenvector updates. The asymptotic runtime analysis is detailed and supports the claim that the EM algorithm is faster than the baseline.
Weaknesses:
1. Clarity: While the technical content is rigorous, the paper is dense and may be challenging for readers unfamiliar with DPPs or matrix optimization techniques. Simplifying some explanations or including a high-level overview of the EM algorithm could improve accessibility.
2. Limited Scope of Experiments: The experiments are focused on a single application (product recommendation). While this application is relevant, additional experiments in other domains (e.g., document summarization or sensor placement) would strengthen the generalizability of the findings.
3. Initialization Sensitivity: The paper acknowledges that both EM and the baseline methods are sensitive to initialization. While the moments-matching initializer partially addresses this, further discussion on initialization strategies and their impact on convergence would be valuable.
4. Limitations Discussion: The paper does not explicitly discuss the limitations of the proposed method, such as potential scalability issues for extremely large datasets or the reliance on eigendecomposition, which may become computationally expensive for very large matrices.
Recommendation:
Overall, this paper makes a significant contribution to the field of DPP learning and is well-suited for presentation at NIPS. The proposed EM algorithm is novel, technically sound, and empirically validated. However, the authors should consider improving the clarity of the exposition and expanding the experimental scope in future work. I recommend acceptance with minor revisions to address the clarity and limitations discussion. 
Pro Arguments:
- Novel and effective algorithm for unconstrained DPP learning.
- Strong empirical results demonstrating practical utility.
- Theoretical rigor and detailed runtime analysis.
Con Arguments:
- Dense presentation may hinder accessibility.
- Experiments are limited to a single application domain.