The paper presents a novel Bayesian formulation for nonlinear support vector machines (SVMs), integrating Gaussian processes (GPs) with a scaled mixture of normals to extend the hinge loss function. The authors further embed this Bayesian nonlinear SVM into a factor model, enabling joint feature learning and nonlinear classifier design. This work addresses the limitations of prior Bayesian SVM approaches, which were restricted to linear classifiers, and demonstrates its utility through extensive experiments on benchmark datasets, USPS handwritten digits, and gene expression data.
Strengths:
1. Novelty: The paper introduces a significant extension of the Bayesian SVM framework to nonlinear classifiers, leveraging GPs. This innovation is both timely and impactful, addressing a gap in the literature.
2. Technical Rigor: The authors provide a thorough theoretical foundation, including a generalization of the SVM loss function to a skewed Laplace distribution. The use of MCMC and expectation conditional maximization (ECM) for inference is well-justified and detailed.
3. Practical Contributions: The integration of the nonlinear Bayesian SVM into discriminative factor models is a compelling contribution, particularly for high-dimensional datasets like gene expression data. The ability to compute class membership probabilities and the sparsity of support vectors are practical advantages over traditional GP classifiers.
4. Experimental Validation: The extensive experiments demonstrate the superiority of the proposed method over existing approaches (e.g., optimization-based SVMs and GP classifiers) in terms of accuracy and interpretability. The results on benchmark datasets and real-world applications, such as gene expression analysis, are convincing.
5. Scalability: The use of FITC approximations and VB-EM for scaling inference is a thoughtful addition, addressing the computational challenges of GP-based methods.
Weaknesses:
1. Clarity: While the paper is technically sound, the dense mathematical exposition may hinder accessibility for a broader audience. Some sections, such as the derivation of the skewed Laplace distribution, could benefit from additional explanation or visual aids.
2. Comparative Analysis: Although the proposed method outperforms alternatives in most cases, the performance gains are often incremental. A deeper discussion of why the method excels in certain datasets but not others would strengthen the narrative.
3. Limitations: The paper does not explicitly discuss potential limitations, such as the computational overhead of MCMC or the sensitivity of the model to hyperparameters like γ₀. Acknowledging these would provide a more balanced perspective.
4. Reproducibility: While the authors provide implementation details, the lack of publicly available code or pseudocode for key algorithms may hinder reproducibility.
Recommendation:
I recommend acceptance of this paper, as it presents a significant advancement in Bayesian SVMs and demonstrates its utility in both theoretical and practical contexts. The novelty, technical depth, and experimental validation make it a strong contribution to the field. However, the authors are encouraged to improve the clarity of the exposition and provide a more explicit discussion of limitations and reproducibility in the final version. 
Arguments for Acceptance:
- Significant extension of Bayesian SVMs to nonlinear classifiers.
- Strong experimental results demonstrating practical utility.
- Integration with factor models addresses real-world high-dimensional problems.
Arguments Against Acceptance:
- Dense presentation may limit accessibility.
- Lack of explicit discussion on limitations and reproducibility.
Overall, the paper is a valuable contribution to the intersection of Bayesian methods, SVMs, and GPs, and it aligns well with the goals of the conference.