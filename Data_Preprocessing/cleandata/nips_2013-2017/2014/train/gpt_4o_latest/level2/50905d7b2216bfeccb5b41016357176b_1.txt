The paper proposes a novel approach to solving large Markov Decision Processes (MDPs) by estimating the optimal state-action value function using Difference of Convex functions (DC) Programming. The authors argue that minimizing the Optimal Bellman Residual (OBR) serves as a proxy for estimating the optimal action-value function. They demonstrate that this minimization can be framed as a DC problem, enabling the use of established DC programming algorithms. The paper also establishes Vapnik-consistency for the empirical norm of the OBR, ensuring that minimizing the empirical norm leads to meaningful results. The authors conduct experiments comparing their approach to Approximate Value Iteration (AVI) and Approximate Policy Iteration (API), showing competitive performance and reduced variance.
Strengths
1. Novelty: The paper introduces a fresh perspective on reinforcement learning by framing the problem as a DC optimization task. This approach is innovative and leverages a well-established body of literature on DC programming.
2. Theoretical Contributions: The authors rigorously prove Vapnik-consistency for the empirical norm of the OBR and provide explicit DC decompositions for the optimization problem. These theoretical results are significant and well-explained.
3. Experimental Validation: The experiments demonstrate that the proposed method performs competitively with state-of-the-art AVI and API methods. The reduced variance observed in the results is a notable advantage.
4. Clarity of Exposition: The paper is well-organized and provides detailed explanations of the theoretical and experimental components. The inclusion of supplementary proofs enhances the completeness of the work.
Weaknesses
1. Limited Experimental Scope: The experiments are conducted on synthetic Garnet problems, which, while illustrative, may not fully represent the challenges of real-world MDPs. The practical applicability of the approach remains unclear.
2. Naive Implementation: The authors acknowledge that their implementation of the DC algorithm is basic, relying on canonical decompositions and sub-gradient descent. This limits the potential of the approach and may not reflect its true capabilities.
3. Computational Complexity: The DC programming approach involves solving a sequence of convex optimization problems, which may be computationally expensive for large-scale problems. This aspect is not thoroughly analyzed.
4. Batch Scenario Bias: In the batch setting, the estimation of the Bellman operator introduces an uncontrolled bias, which could impact the reliability of the results. While alternative techniques are mentioned, they are not explored in the experiments.
Suggestions for Improvement
1. Extend the experimental evaluation to real-world problems or standard RL benchmarks to better assess practical applicability.
2. Explore more sophisticated implementations of the DC algorithm, such as alternative decompositions or advanced convex solvers, to improve performance.
3. Provide a detailed analysis of the computational complexity and scalability of the proposed approach.
4. Investigate methods to mitigate the bias in the batch scenario, such as embedding techniques or weighted averaging methods.
Recommendation
The paper makes a significant theoretical contribution by introducing DC programming to reinforcement learning and proving its consistency. However, the experimental evaluation and implementation are limited, leaving room for improvement. I recommend acceptance with minor revisions, provided the authors address the practical applicability and computational aspects in more depth. This work has the potential to open a promising new direction in reinforcement learning research.