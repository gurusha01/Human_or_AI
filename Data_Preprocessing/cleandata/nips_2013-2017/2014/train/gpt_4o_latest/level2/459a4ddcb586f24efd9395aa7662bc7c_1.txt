The paper introduces the Sparse Random Features (Sparse-RF) algorithm, which addresses the scalability limitations of kernel methods by combining random feature approximations with ℓ1-regularization. The authors reinterpret Random Features as Randomized Coordinate Descent (RCD) in an infinite-dimensional Reproducing Kernel Hilbert Space (RKHS), providing theoretical guarantees for convergence to a solution within \( O(1/D) \)-precision of exact kernel methods. Sparse-RF achieves sparsity by selecting a compact subset of random features, reducing memory and prediction time while maintaining comparable performance on regression and classification tasks. The paper also compares Sparse-RF to Boosting, demonstrating better convergence guarantees in settings where greedy Boosting steps cannot be performed exactly.
Strengths:
1. Novelty and Contribution: The reinterpretation of Random Features as RCD in an infinite-dimensional space is innovative and provides a new perspective on kernel approximation methods. The theoretical guarantees for convergence and sparsity are significant contributions to the field.
2. Practical Usefulness: Sparse-RF addresses a critical challenge in kernel methods—scalability—by reducing model size without sacrificing predictive performance. This makes it highly relevant for large-scale machine learning applications.
3. Theoretical Rigor: The paper provides a thorough mathematical analysis of the Sparse-RF algorithm, including convergence proofs and bounds. The comparison to Boosting is particularly insightful, highlighting the advantages of the randomized approach in practical scenarios.
4. Experimental Validation: The experiments are well-designed, comparing Sparse-RF to standard Random Features, kernel methods, and Boosting across multiple datasets and kernel types. The results convincingly demonstrate the algorithm's efficiency and effectiveness.
Weaknesses:
1. Clarity: While the theoretical analysis is rigorous, the paper is dense and challenging to follow, particularly for readers unfamiliar with RKHS or coordinate descent methods. Simplifying some explanations or providing more intuitive insights would improve accessibility.
2. Limited Scope of Experiments: Although the experiments are comprehensive, they focus primarily on regression and classification tasks. It would be valuable to see Sparse-RF applied to other domains, such as structured prediction or feature extraction, to fully assess its versatility.
3. Comparison to Other Methods: The paper compares Sparse-RF to Boosting and standard Random Features but does not benchmark against other recent scalable kernel approximation methods (e.g., Nyström methods). Including such comparisons would strengthen the evaluation.
4. Hyperparameter Sensitivity: The choice of λ for Sparse-RF is critical for achieving comparable performance to other methods. The paper does not provide a detailed discussion of how λ is selected or its sensitivity to different datasets.
Recommendation:
Overall, the paper presents a significant advancement in scalable kernel methods, with strong theoretical and experimental support. While the clarity and scope of the experiments could be improved, the contributions are substantial and relevant to the NeurIPS audience. I recommend acceptance, with minor revisions to improve clarity and include broader comparisons.
Arguments for Acceptance:
- Novel and theoretically grounded approach to kernel approximation.
- Practical utility in reducing memory and computational costs for large-scale problems.
- Strong experimental results demonstrating the algorithm's effectiveness.
Arguments Against Acceptance:
- Dense presentation may limit accessibility to a broader audience.
- Lack of comparison to other scalable kernel approximation methods.
Suggested Improvements:
1. Simplify theoretical explanations and provide more intuitive insights.
2. Include comparisons to other scalable kernel approximation techniques.
3. Discuss hyperparameter selection and sensitivity in more detail.
In conclusion, Sparse-RF is a promising contribution to the field, and its acceptance would benefit the NeurIPS community.