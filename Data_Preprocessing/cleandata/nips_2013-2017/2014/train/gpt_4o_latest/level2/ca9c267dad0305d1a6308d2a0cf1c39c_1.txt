The paper introduces SHAMPO, a novel algorithm for online multi-task learning with a shared annotator, addressing the challenge of optimally allocating limited annotation resources across multiple tasks. The authors propose a framework where K learners share a single annotator, and a stochastic mechanism determines which task receives feedback in each round. The paper claims two main contributions: (1) the development of SHAMPO, which balances exploration and exploitation to optimize annotation allocation, and (2) its application to contextual bandits, specifically one-vs-rest and one-vs-one settings. Theoretical analysis provides mistake bounds, and empirical results demonstrate SHAMPO's superior performance compared to baseline methods.
Strengths:
1. Novelty: The paper introduces a unique framework for multi-task learning with a shared annotator, which is a practical and underexplored problem. The integration of exploration-exploitation trade-offs in this context is particularly innovative.
2. Theoretical Rigor: The authors provide a thorough theoretical analysis, including mistake bounds, which are well-supported by proofs in the supplementary material. The analysis highlights SHAMPO's ability to perform competitively with algorithms that observe all annotated data.
3. Empirical Validation: The experiments are comprehensive, spanning multiple datasets (e.g., OCR, document classification, and vowel prediction) and settings (one-vs-rest and one-vs-one). SHAMPO consistently outperforms baseline methods, demonstrating its effectiveness in reducing errors while optimizing annotation usage.
4. Practical Relevance: The framework has clear real-world applications, such as reducing annotation costs in multi-task systems and adapting models to individual users. The contextual bandit extensions further broaden its applicability.
Weaknesses:
1. Clarity: The paper is dense and could benefit from clearer explanations, particularly in the algorithm description and theoretical analysis. For instance, the role of parameters like `b` and `Î»` could be more intuitively explained for readers unfamiliar with the domain.
2. Limited Comparison: While SHAMPO is compared to baseline methods (uniform and exploit), the paper does not benchmark against more advanced multi-task learning algorithms or contextual bandit methods, which could strengthen its claims of superiority.
3. Prior Selection: The paper briefly discusses the use of priors to improve SHAMPO's performance but does not provide a systematic method for generating good priors. This limits the practical utility of the approach in scenarios where task difficulty is unknown.
4. Scalability: While the theoretical analysis suggests scalability to many tasks, the empirical evaluation is limited to problems with up to 45 tasks. It would be valuable to test SHAMPO on larger-scale problems to confirm its scalability.
Recommendation:
Overall, the paper makes a significant contribution to online multi-task learning and contextual bandits. Its novelty, theoretical soundness, and empirical results justify its acceptance. However, the authors should address clarity issues, expand comparisons to existing methods, and provide more guidance on prior selection in a revised version.
Pro Arguments:
- Novel and practical framework for shared annotator settings.
- Strong theoretical guarantees and empirical validation.
- Clear improvement over baseline methods.
Con Arguments:
- Dense presentation and limited clarity in some sections.
- Lack of comparison with more advanced methods.
- Limited discussion on scalability and prior selection.
Final Decision: Accept with minor revisions.