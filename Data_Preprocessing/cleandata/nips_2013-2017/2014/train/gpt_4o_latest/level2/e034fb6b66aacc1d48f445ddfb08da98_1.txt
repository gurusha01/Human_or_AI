The paper presents a novel framework, Attribute Tensor Decomposition (ATD), for learning distributed representations of attributes alongside word embeddings. By introducing a third-order multiplicative model, the authors enable conditional word similarity, where word meanings adapt based on contextual attributes. The paper demonstrates the utility of this approach across diverse tasks, including sentiment classification, cross-lingual document classification, and blog authorship attribution, while also exploring qualitative aspects like conditional word similarity and attribute-conditioned text generation.
Strengths:
1. Novelty and Scope: The proposed ATD framework is innovative, extending traditional word embeddings to incorporate attribute vectors, enabling a wide range of applications. The conditional word similarity concept is particularly compelling and well-demonstrated.
2. Experimental Rigor: The paper evaluates ATD on diverse tasks, showcasing its versatility. The sentiment classification and cross-lingual document classification experiments are especially robust, with results competitive with state-of-the-art methods.
3. Qualitative Insights: The qualitative analysis, such as attribute-conditioned text generation and conditional word similarity, provides valuable insights into the model's interpretability and practical utility.
4. Comprehensive Comparisons: The authors compare their method against strong baselines and related work, highlighting the advantages of ATD in both high-resource and low-resource settings.
5. Clarity of Presentation: The paper is well-organized, with detailed explanations of the methodology, including tensor factorization and attribute learning. The inclusion of mathematical formulations enhances reproducibility.
Weaknesses:
1. Limited State-of-the-Art Performance: While the results are competitive, ATD is outperformed by recent methods like Paragraph Vector and BiCVM+ on certain tasks. The paper could benefit from a deeper discussion of why these methods excel and how ATD might be improved.
2. Scalability Concerns: The use of a tensor representation and multiplicative interactions may raise scalability issues for larger vocabularies or datasets. The paper does not address computational efficiency or training time.
3. Sparse Analysis of Limitations: The authors briefly acknowledge limitations, such as the need for attribute availability during training, but do not explore these in depth. For example, the impact of noisy or incomplete attribute data on model performance remains unclear.
4. Insufficient Novelty in Some Aspects: While the framework generalizes prior work (e.g., multiplicative models for images), certain components, such as the use of attribute vectors, are incremental rather than groundbreaking.
Suggestions for Improvement:
- Provide a more detailed analysis of computational efficiency and scalability, particularly for real-world applications with large datasets.
- Explore additional tasks or datasets to further validate the generality of ATD.
- Discuss potential extensions, such as dynamic or hierarchical attributes, to address current limitations.
- Include ablation studies to better understand the contributions of individual components (e.g., tensor factorization, attribute inference).
Recommendation:
Overall, the paper presents a significant contribution to the field of natural language processing by introducing a flexible and interpretable framework for attribute-conditioned word embeddings. While some aspects could be refined, the work is well-executed and demonstrates strong potential for future research. I recommend acceptance with minor revisions to address scalability concerns and expand the discussion of limitations.