Review of the Paper
This paper presents a novel convex formulation for training deep learning models with multiple nonlinear layers, addressing key challenges in optimization and scalability. The authors propose a kernel-based approach that allows for the training of arbitrarily deep architectures to global optimality, overcoming traditional difficulties associated with non-convex objectives in deep learning. By introducing normalized kernels and leveraging convex relaxations, the paper extends prior work on kernel-based methods to support multi-layer architectures. The proposed framework is validated through synthetic and real-world experiments, demonstrating its effectiveness in capturing complex data structures.
Strengths
1. Novelty and Originality: The paper introduces a significant innovation in the form of normalized kernels and convex relaxations, enabling global optimization for deep architectures. This represents a meaningful advancement over prior work, such as [25], which was limited to single-layer adaptive kernels.
2. Technical Soundness: The authors provide a rigorous theoretical foundation for their approach, addressing key obstacles (nonlinear transfers, bilinear interactions, and joint input-output optimization) in a systematic manner. The use of normalized kernels to achieve joint convexity is particularly compelling.
3. Empirical Validation: The experiments on synthetic datasets (e.g., Parity and Inner Product problems) and real-world datasets (e.g., MNIST, CIFAR-100) convincingly demonstrate the advantages of deeper convex models (CVX3) over shallower ones (CVX2). The results highlight the representational power of the proposed method.
4. Clarity of Writing: Despite the technical complexity, the paper is well-organized and provides sufficient detail for expert readers to understand the methodology. The inclusion of pseudocode (Algorithm 1) and theoretical proofs enhances reproducibility.
Weaknesses
1. Scalability: While the authors acknowledge the computational challenges of their approach, the scalability of the method to very large datasets remains a concern. The reliance on matrix pseudo-inverses and spectral constraints may limit practical applicability in high-dimensional settings.
2. Limited Comparison: The paper primarily compares the proposed method to shallow kernel-based models and standard neural networks. A broader comparison with state-of-the-art deep learning methods (e.g., transformers, modern convolutional architectures) would strengthen the empirical evaluation.
3. Practical Usefulness: Although the theoretical contributions are significant, the practical utility of the approach in real-world applications is not fully demonstrated. The method's computational overhead may outweigh its benefits in scenarios where non-convex methods already perform well.
4. Assumptions and Relaxations: The convex relaxation of the normalized kernel domain (Postulate 2) and the design of the multi-label loss (Postulate 3) introduce approximations. While these are justified theoretically, their impact on the model's ability to generalize is not thoroughly explored.
Arguments for Acceptance
- The paper makes a substantial theoretical contribution by extending convex optimization to deep architectures, a longstanding challenge in machine learning.
- The empirical results provide strong evidence of the method's effectiveness in capturing complex data structures, particularly in synthetic settings.
- The work is highly relevant to the NeurIPS community, as it bridges the gap between kernel methods and deep learning.
Arguments Against Acceptance
- The scalability and computational feasibility of the proposed method for large-scale datasets are not adequately addressed.
- The practical impact of the work is limited by the lack of comparison with cutting-edge non-convex deep learning methods.
Recommendation
Overall, this paper presents a significant theoretical advancement in the field of deep learning and convex optimization. While there are concerns regarding scalability and practical utility, the novelty and rigor of the work make it a valuable contribution. I recommend acceptance with minor revisions, focusing on improving the discussion of scalability and including comparisons with modern deep learning architectures.