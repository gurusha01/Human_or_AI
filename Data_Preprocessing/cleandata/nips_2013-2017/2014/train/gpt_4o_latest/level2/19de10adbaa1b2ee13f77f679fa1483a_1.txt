The paper introduces Deep Attention Selective Networks (dasNet), a novel architecture that incorporates feedback mechanisms into convolutional neural networks (CNNs) to dynamically adjust filter sensitivities during classification. Unlike traditional feedforward CNNs, dasNet uses reinforcement learning, specifically Separable Natural Evolution Strategies (SNES), to train a policy that modulates internal attention iteratively. The approach is evaluated on CIFAR-10 and CIFAR-100 datasets, where dasNet achieves state-of-the-art performance on unaugmented data, demonstrating its ability to correct misclassifications through selective attention.
Strengths:
1. Novelty and Originality: The paper presents a significant innovation by introducing feedback mechanisms into CNNs, inspired by human visual processing. This is a notable departure from the static nature of traditional CNNs.
2. Technical Soundness: The use of SNES for training the feedback policy is well-motivated, addressing the challenge of optimizing a high-dimensional parameter space. The experimental setup is robust, with clear baselines and hyperparameter choices.
3. Significance: The results demonstrate a meaningful improvement over the state-of-the-art on CIFAR datasets, particularly in handling ambiguous classification cases. This advancement has the potential to inspire further research in dynamic and adaptive neural networks.
4. Clarity: The paper is well-organized, with detailed explanations of the architecture, training process, and experimental results. The inclusion of visualizations (e.g., filter activations and dynamics) aids in understanding the model's behavior.
5. Relevance: The work aligns with ongoing efforts in the field to make neural networks more biologically plausible and adaptive, addressing limitations of static feedforward architectures.
Weaknesses:
1. Limited Scope of Evaluation: While the results on CIFAR-10 and CIFAR-100 are compelling, the datasets are relatively small-scale and may not fully demonstrate dasNet's potential. Evaluation on larger and more diverse datasets, such as ImageNet, would strengthen the claims.
2. Computational Overhead: The iterative processing and use of SNES introduce significant computational costs, which may limit the model's practicality in real-world applications. A discussion of efficiency trade-offs is missing.
3. Interpretability Challenges: While the paper acknowledges the difficulty of qualitatively analyzing high-level filter changes, this remains a limitation. More effort to interpret the learned policies or visualize their impact would enhance the work's accessibility.
4. Comparison to Alternative Feedback Mechanisms: The paper does not compare dasNet to other feedback-based approaches in computer vision, such as recurrent or attention-based models. This omission makes it harder to contextualize the contribution.
Pro and Con Acceptance:
- Pro: The paper introduces a novel and biologically inspired approach, demonstrates state-of-the-art results, and provides a clear and detailed methodology.
- Con: The evaluation is limited to small-scale datasets, and the computational overhead may hinder practical adoption.
Recommendation: Accept with minor revisions. The paper makes a valuable contribution to the field, but addressing the computational efficiency and expanding the evaluation to larger datasets would strengthen its impact. Additionally, a more thorough comparison to related feedback-based methods is encouraged.