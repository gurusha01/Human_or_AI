This paper addresses the problem of low-rank approximation in the streaming model, specifically in the row-update setting, where the rows of a matrix arrive sequentially. The authors establish a tight lower bound of Ω(dk/ε) bits of space for any streaming algorithm that outputs a rank-k approximation matrix \( R \) satisfying \( \|A - AR^\dagger R\|F \leq (1 + \epsilon)\|A - Ak\|_F \) with constant probability. This result matches the upper bound of Liberty (2013) and Ghashami and Phillips (2014) up to word size, demonstrating that these algorithms are space-optimal. The authors leverage communication complexity techniques, particularly the Index problem, to prove their lower bound, overcoming technical challenges related to the limited information in the output matrix \( R \). The work also highlights the significance of the \( 1/\epsilon \) factor in the lower bound, which becomes critical for high-precision approximations.
Strengths
1. Theoretical Contribution: The paper makes a significant theoretical contribution by proving a tight lower bound for streaming low-rank approximation. This result resolves an open question about the optimality of existing algorithms and advances our understanding of the space complexity of streaming numerical linear algebra problems.
2. Novel Techniques: The use of communication complexity, particularly the reduction from the Index problem, is well-executed and demonstrates technical depth. The authors address challenges unique to the row-update model, such as the limited information in the output matrix \( R \), and provide a rigorous proof of their main theorem.
3. Relevance: The problem of low-rank approximation is central to many applications in machine learning and data science, particularly for large-scale datasets. The results are highly relevant to the NeurIPS community and contribute to the broader field of streaming algorithms.
4. Clarity of Results: The main theorem is clearly stated, and the implications for the optimality of existing algorithms are well-explained. The authors also provide intuitive insights into their proof techniques, such as the role of the Frobenius norm and the significance of the \( 1/\epsilon \) factor.
Weaknesses
1. Clarity of Presentation: While the theoretical results are strong, the paper is dense and challenging to follow, particularly for readers unfamiliar with communication complexity. The proof of the main theorem spans multiple pages and could benefit from additional high-level summaries or visual aids to improve accessibility.
2. Experimental Validation: The paper is purely theoretical and does not include any empirical evaluation. While this is not strictly necessary for a theoretical contribution, experimental results demonstrating the practical implications of the lower bound (e.g., comparing existing algorithms under memory constraints) would strengthen the paper.
3. Scope of Results: The paper focuses exclusively on the row-update model. While this is a natural setting for streaming algorithms, it would be interesting to discuss whether similar lower bounds hold in other streaming models, such as the entry-update model or multi-pass streaming.
Arguments for Acceptance
- The paper resolves an important open question and establishes a tight lower bound for a fundamental problem in streaming numerical linear algebra.
- The theoretical contributions are rigorous, novel, and relevant to the NeurIPS community.
- The results have implications for both theory and practice, particularly for designing memory-efficient algorithms for large-scale data.
Arguments Against Acceptance
- The dense and technical presentation may limit accessibility for a broader audience.
- The lack of experimental validation leaves open questions about the practical impact of the results.
- The scope is somewhat narrow, focusing only on the row-update model without exploring other streaming settings.
Recommendation
I recommend acceptance of this paper. Its theoretical contributions are significant, and the results will likely inspire further research in streaming algorithms and low-rank approximation. However, the authors should consider improving the clarity of the presentation and discussing broader implications in a revised version.