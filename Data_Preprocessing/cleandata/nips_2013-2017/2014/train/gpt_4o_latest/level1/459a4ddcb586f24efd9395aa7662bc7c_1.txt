The paper introduces a novel Sparse Random Features (Sparse-RF) algorithm that addresses the scalability issues of kernel methods by combining random feature approximations with l1-regularization. The authors frame the algorithm as Randomized Coordinate Descent (RCD) in an infinite-dimensional Hilbert space and demonstrate that it achieves an approximation error of O(1/D), improving upon the O(1/√D) convergence rate of traditional Random Features. The Sparse-RF algorithm produces sparse solutions, reducing memory and prediction time while maintaining comparable performance to kernel methods on regression and classification tasks. The paper also compares Sparse-RF to Boosting, showing that the randomized approach offers better convergence guarantees when exact greedy steps are infeasible.
Strengths
1. Technical Soundness and Novelty: The paper is technically rigorous and provides a novel interpretation of Random Features as RCD in a Hilbert space. The convergence analysis is thorough, and the results are supported by theoretical guarantees and empirical validation.
2. Scalability: Sparse-RF effectively addresses the scalability challenges of kernel methods by reducing model size and computational costs without sacrificing performance.
3. Comparison to Related Work: The paper situates its contributions well within the context of kernel approximation methods, including Random Features and Boosting. The theoretical and experimental comparisons to these methods are compelling.
4. Practical Relevance: The algorithm's ability to produce sparse models makes it highly relevant for large-scale machine learning applications, where memory and computational efficiency are critical.
5. Experimental Validation: The experiments are comprehensive, covering multiple datasets, kernel types, and tasks. The results convincingly demonstrate the advantages of Sparse-RF in terms of sparsity, efficiency, and performance.
Weaknesses
1. Clarity: While the paper is technically sound, certain sections, particularly the theoretical analysis, are dense and may be difficult for readers unfamiliar with coordinate descent or Hilbert space optimization. Simplifying or summarizing key results could improve accessibility.
2. Limited Scope of Experiments: Although the experiments are robust, they primarily focus on regression and classification tasks. It would be valuable to explore the algorithm's performance on other tasks, such as structured prediction or feature extraction, to highlight its versatility.
3. Dependence on λ Selection: The choice of the regularization parameter λ significantly impacts the performance of Sparse-RF. While the authors mention using a regularization path, a more detailed discussion or automated approach for selecting λ would enhance the paper's practical utility.
4. Comparison to Other Sparse Methods: The paper does not compare Sparse-RF to other sparse kernel approximation methods, such as those based on Nyström or greedy feature selection. Including such comparisons would provide a more comprehensive evaluation.
Arguments for Acceptance
- The paper presents a significant advancement in kernel approximation methods, with strong theoretical and empirical contributions.
- Sparse-RF addresses a critical scalability issue in machine learning, making it highly relevant for the NeurIPS audience.
- The theoretical analysis is rigorous, and the experimental results are convincing.
Arguments Against Acceptance
- The paper's clarity could be improved, particularly in the theoretical sections.
- The experiments could be expanded to include a broader range of tasks and comparisons to other sparse methods.
Recommendation
I recommend acceptance of this paper, as it provides a meaningful contribution to the field of scalable kernel methods. While there are areas for improvement in clarity and experimental scope, the strengths of the paper outweigh its weaknesses.