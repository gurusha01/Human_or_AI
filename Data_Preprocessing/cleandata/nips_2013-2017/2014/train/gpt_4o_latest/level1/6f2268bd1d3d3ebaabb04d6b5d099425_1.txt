The paper presents a novel approach to the joint problem of clustering and outlier detection using an extension of the facility location formulation. The authors argue that combining these tasks leads to compact, semantically coherent clusters that are robust to data perturbations and provide contextualized, interpretable outliers. The proposed method employs a Lagrangian relaxation of the integer programming formulation and solves it using a subgradient-based algorithm. Theoretical contributions include proofs of approximation properties and convergence guarantees. Empirical evaluations on synthetic and real datasets, including MNIST, demonstrate the method's scalability, robustness, and competitive performance compared to state-of-the-art algorithms like k-means-- and an affinity propagation-based method (APOC).
Strengths:
1. Technical Soundness: The paper is technically rigorous, providing clear theoretical guarantees for the approximation quality and convergence of the proposed method. The equivalence of the Lagrangian relaxation to the linear relaxation is a notable contribution.
2. Scalability: The algorithm is designed to handle large datasets efficiently, with a small memory footprint and the ability to process data incrementally. This is a significant advantage over methods like APOC, which require large memory allocations.
3. Interpretability: The method's ability to automatically determine the number of clusters and contextualize outliers is a practical and meaningful feature, particularly for real-world applications.
4. Empirical Validation: The experiments are thorough, with evaluations on both synthetic and real datasets. The use of multiple metrics (e.g., Jaccard index, LOF, V-Measure) provides a comprehensive assessment of the method's performance.
5. Comparison to Baselines: The paper compares the proposed method against strong baselines (k-means-- and APOC) and demonstrates superior or comparable performance in terms of clustering quality and outlier detection.
Weaknesses:
1. Clarity: While the paper is well-organized, some sections, particularly the mathematical derivations (e.g., Lagrangian relaxation and proof of total unimodularity), are dense and may be challenging for readers unfamiliar with optimization techniques. Simplifying or providing more intuition could improve accessibility.
2. Hyperparameter Sensitivity: The method requires setting parameters such as the cluster creation cost (Î±) and the number of outliers (`). While the authors explore the sensitivity of these parameters, further discussion on how to select them in practice would be valuable.
3. Comparison Scope: The comparison to k-means-- and APOC is insightful, but additional comparisons to other robust clustering methods (e.g., DBSCAN or spectral clustering with outlier handling) could strengthen the empirical evaluation.
4. Convergence Speed: While the method is scalable, the authors note that the subgradient algorithm requires more iterations than APOC. Exploring more advanced step-size schedules or optimization techniques could improve convergence speed.
Arguments for Acceptance:
- The paper addresses an important and underexplored problem by integrating clustering and outlier detection.
- It provides strong theoretical contributions and empirical evidence of effectiveness.
- The method's scalability and interpretability make it highly relevant for real-world applications.
Arguments Against Acceptance:
- The clarity of the mathematical exposition could be improved.
- The empirical comparison could be broadened to include additional baselines.
Recommendation:
Overall, this paper makes a significant contribution to the field of robust clustering and outlier detection. While there are minor issues with clarity and the scope of comparisons, the strengths outweigh the weaknesses. I recommend acceptance, with suggestions for improving clarity and expanding the empirical evaluation in future revisions.