The paper introduces SHAMPO (SHared Annotator for Multiple PrOblems), a novel framework for online multi-task learning where multiple learners share a single annotator with limited bandwidth. The authors propose an algorithm that judiciously selects which task receives feedback in each round, balancing exploration and exploitation based on the uncertainty of predictions. Theoretical analysis demonstrates that SHAMPO achieves competitive mistake bounds compared to algorithms that observe all labels. The framework is extended to contextual bandit settings, including one-vs-rest and one-vs-one reductions. Empirical results across diverse datasets, such as OCR, vowel prediction, and document classification, show that SHAMPO outperforms baseline methods, achieving higher accuracy for the same annotation effort.
Strengths:
1. Novelty and Scope: The problem of optimizing annotator bandwidth across multiple tasks is both practical and underexplored. SHAMPO introduces a fresh perspective by coupling task selection with model updates, which is a significant contribution to online multi-task learning.
2. Theoretical Rigor: The mistake-bound analysis is thorough, providing insights into the trade-offs between exploration and exploitation. The bounds are well-motivated and align with empirical observations.
3. Empirical Validation: The experiments are extensive, covering multiple datasets and task configurations. The results convincingly demonstrate SHAMPO's superiority over baselines like uniform and exploit strategies, particularly in reducing test error and focusing on harder tasks.
4. Versatility: The extension to contextual bandits broadens the applicability of the framework. The one-vs-one and one-vs-rest reductions are well-integrated, and the algorithm's ability to handle decoupled exploration and exploitation is a notable advancement.
5. Practical Implications: The framework is relevant for real-world scenarios, such as adaptive systems that interact with users, where annotation resources are limited.
Weaknesses:
1. Clarity: While the theoretical analysis is detailed, the presentation is dense and could benefit from clearer explanations and more intuitive examples, especially for readers unfamiliar with mistake-bound models.
2. Assumptions: The framework assumes no dependency between tasks, which may limit its applicability in scenarios where tasks are inherently related. Exploring task relationships could further improve performance.
3. Limited Comparison: The paper does not compare SHAMPO against other state-of-the-art multi-task learning algorithms that leverage task relationships or advanced regularization techniques. This omission leaves open questions about its relative performance in broader contexts.
4. Prior Selection: While the authors discuss the potential of using priors to improve performance, the heuristic for generating priors is underexplored. A more systematic approach to prior selection could enhance the algorithm's robustness.
5. Scalability: The computational complexity of SHAMPO, particularly in the one-vs-one setting, is not explicitly discussed. This could be a concern for large-scale problems with many tasks or classes.
Arguments for Acceptance:
- The paper addresses a practical and impactful problem with a novel approach.
- Theoretical and empirical results are strong and well-aligned.
- The framework is versatile, extending to contextual bandits and various feedback settings.
Arguments Against Acceptance:
- Clarity and presentation could be improved, particularly in the theoretical sections.
- The lack of comparison with related multi-task learning methods limits the broader contextual understanding of SHAMPO's contributions.
Recommendation:
Overall, the paper makes a significant contribution to online multi-task learning and contextual bandits. While there are areas for improvement, particularly in clarity and comparative analysis, the novelty, theoretical rigor, and empirical results justify its acceptance. I recommend acceptance with minor revisions to address clarity and expand the discussion on related work.