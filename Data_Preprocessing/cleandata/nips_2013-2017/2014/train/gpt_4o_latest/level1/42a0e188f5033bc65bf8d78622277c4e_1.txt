The paper proposes a novel optimization algorithm, Parallel Direction Method of Multipliers (PDMM), to address the problem of minimizing block-separable convex functions subject to linear constraints. PDMM generalizes the Alternating Direction Method of Multipliers (ADMM) to handle multi-block scenarios by introducing a randomized block coordinate approach. Unlike traditional ADMM, which struggles with convergence in multi-block settings, PDMM updates a random subset of blocks in parallel, incorporating a backward step in the dual update to ensure stability. The authors establish global convergence and derive an iteration complexity of \(O(1/T)\) for PDMM with a constant step size. They also demonstrate the algorithm's application to overlapping group lasso and robust principal component analysis (RPCA), showing superior empirical performance compared to state-of-the-art methods.
Strengths
1. Theoretical Contributions: The paper provides rigorous theoretical guarantees for PDMM, including global convergence and iteration complexity, which are significant given the challenges of multi-block ADMM.
2. Novelty: The introduction of randomized block updates and the backward step in the dual update is a creative extension of existing methods. The connection between PDMM, sADMM, and PJADMM offers valuable insights into the interplay between these approaches.
3. Practical Relevance: The experimental results on RPCA and overlapping group lasso demonstrate that PDMM outperforms existing methods in terms of runtime and convergence, highlighting its practical utility.
4. Clarity of Experimental Design: The experiments are well-structured, with comparisons to multiple baselines and detailed analysis of the effects of hyperparameters, such as the number of blocks \(K\).
Weaknesses
1. Clarity of Presentation: While the theoretical contributions are strong, the paper is dense and could benefit from clearer explanations, particularly in the derivation of step sizes (\(\taui, \nui\)) and the role of sparsity in \(A\). Simplifying the notation and providing more intuitive explanations would make the work accessible to a broader audience.
2. Limited Scope of Applications: Although RPCA and overlapping group lasso are compelling examples, the paper could explore additional applications to demonstrate the generality of PDMM.
3. Comparison to Non-ADMM Methods: While the paper focuses on ADMM-based baselines, it would be valuable to compare PDMM to other optimization frameworks, such as proximal gradient methods or stochastic optimization techniques, to contextualize its performance more broadly.
4. Parallel Implementation: The paper claims PDMM can run in parallel, but all experiments are conducted sequentially. Demonstrating parallel performance would strengthen the argument for its scalability.
Pro and Con Arguments for Acceptance
Pros:
- Strong theoretical guarantees and novel algorithmic contributions.
- Demonstrated empirical superiority over existing ADMM variants.
- Addresses a significant gap in multi-block optimization.
Cons:
- Dense and technical presentation limits accessibility.
- Limited exploration of broader applications and comparisons to non-ADMM methods.
- Lack of demonstrated parallel implementation, which is a key claim of the method.
Recommendation
Overall, the paper makes a solid contribution to the field of optimization and is relevant to the NIPS community. Despite some presentation issues and limited application scope, the novelty and rigor of the work outweigh these concerns. I recommend acceptance with minor revisions, particularly to improve clarity and discuss broader applications.