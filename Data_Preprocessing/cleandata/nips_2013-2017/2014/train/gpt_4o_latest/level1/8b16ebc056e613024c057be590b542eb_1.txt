The paper addresses the challenge of inferring direct neural network connections from Calcium imaging time series using inverse covariance estimation, a method that has shown promise in previous work, including the Kaggle Connectomics competition. The authors propose a novel approach that applies a convolution filter to preprocess the time series data, followed by supervised optimization of the filter and inverse covariance regularization parameters using a binomial log-likelihood loss function. This method significantly reduces training time (under 2 hours on a CPU) and achieves AUC scores competitive with the winning Kaggle solution, while also generalizing better to networks of varying sizes and parameters. The paper situates its contributions within the broader context of neuroimaging and network inference, referencing related work on methods like GLASSO, Bayesian networks, and transfer entropy.
Strengths
1. Technical Soundness: The paper is technically rigorous, with clear mathematical formulations and derivations for the proposed method. The use of L-BFGS for optimization is well-justified, and the authors provide detailed equations for gradient computation.
2. Efficiency: The proposed method is computationally efficient, reducing training time from days (as in the Kaggle competition) to hours, and prediction time to under 15 minutes.
3. Generality: Unlike prior methods that require extensive parameter tuning, the proposed approach generalizes well to networks with different sizes and parameters, addressing a key limitation of previous work.
4. Clarity: The paper is well-organized, with clear explanations of the methodology, results, and limitations. Figures and equations are used effectively to illustrate key points.
5. Significance: The method has practical implications for neuroimaging and other domains involving cross-exciting point processes, such as social networks and credit risk contagion.
Weaknesses
1. Assumptions: The method assumes Gaussianity in the data, despite the non-Gaussian nature of neural firing time series. While the convolution filter compensates for this, the mismatch between model assumptions and data could limit applicability in other contexts.
2. Limited Scope of Evaluation: The evaluation is restricted to the Kaggle dataset, which may not fully capture the diversity of real-world neural networks. Testing on additional datasets with different characteristics would strengthen the results.
3. Comparison to GLASSO: While the authors briefly discuss extending their method to GLASSO, the comparison between l2 and l1 penalization is limited. A more thorough exploration of trade-offs between AUC and precision metrics would be valuable.
4. Interpretability: The learned convolution filters are not deeply analyzed or interpreted in the context of neural dynamics, which could provide additional insights into the biological relevance of the method.
Arguments for Acceptance
- The paper presents a novel, efficient, and generalizable approach to a challenging problem in neural network inference.
- It demonstrates competitive performance with state-of-the-art methods while addressing key limitations, such as computational inefficiency and overfitting to specific datasets.
- The methodology is clearly described and has potential applications beyond neuroimaging.
Arguments Against Acceptance
- The reliance on Gaussian assumptions and limited evaluation on a single dataset raise questions about the broader applicability of the method.
- The comparison to existing methods, particularly GLASSO, could be more comprehensive.
Recommendation
I recommend acceptance with minor revisions. The paper makes a significant contribution to the field, but additional experiments on diverse datasets and a more detailed comparison to GLASSO would strengthen its impact.