This paper presents a comprehensive study of multi-armed bandit (MAB) problems under non-stationary reward distributions, addressing a critical gap in the literature where the assumption of stationary rewards often fails in real-world applications. The authors introduce a novel framework that bounds the temporal variation in rewards using a "variation budget" \( VT \), bridging the stochastic and adversarial MAB paradigms. The paper's main contributions include a sharp characterization of the regret complexity in this setting, demonstrating that the minimax regret scales as \( (K VT)^{1/3} T^{2/3} \), and the development of a near-optimal policy, Rexp3, which achieves this regret rate up to logarithmic factors.
Strengths:
1. Novelty and Relevance: The paper tackles a significant and underexplored problem in MAB literature by allowing for non-stationary reward structures. The introduction of the variation budget \( V_T \) provides a flexible and realistic way to model temporal uncertainties, making the work highly relevant for practical applications like online advertising and dynamic pricing.
2. Theoretical Rigor: The authors establish both lower and upper bounds on regret, ensuring a tight characterization of the problem's complexity. The proofs are detailed and build on established techniques while adapting them to the non-stationary setting.
3. Policy Design: The Rexp3 policy is well-motivated, leveraging insights from adversarial MAB algorithms like Exp3 while incorporating a "forgetting" mechanism to handle non-stationarity. The connection between adversarial and stochastic settings is particularly insightful.
4. Clarity of Results: The paper provides a clear mapping between the extent of reward variation (\( V_T \)) and the achievable regret, offering a spectrum of performance guarantees. This quantification of the "price of non-stationarity" is a valuable contribution.
Weaknesses:
1. Practical Evaluation: While the theoretical results are strong, the paper lacks empirical validation of the proposed Rexp3 policy. Simulations comparing Rexp3 to existing algorithms under various non-stationary scenarios would strengthen the paper's practical impact.
2. Assumption of Known \( VT \): The policy relies on prior knowledge of the variation budget \( VT \), which may not always be available in practice. Although the authors briefly discuss this limitation, a more detailed exploration of adaptive methods for estimating \( V_T \) would enhance the work's applicability.
3. Complexity of Analysis: The proofs, while rigorous, are dense and may be challenging for readers unfamiliar with advanced MAB theory. A more intuitive explanation of key results could improve accessibility.
Arguments for Acceptance:
- The paper addresses an important and timely problem, offering novel theoretical insights and a well-designed policy.
- The results are rigorous and advance the state of the art in non-stationary MAB problems.
- The work is likely to inspire further research on adaptive policies and practical applications.
Arguments Against Acceptance:
- The lack of empirical validation limits the paper's immediate practical relevance.
- The reliance on a known \( V_T \) may restrict the applicability of the proposed methods.
Recommendation:
I recommend acceptance of this paper, as its contributions to the theoretical understanding of non-stationary MAB problems are significant and well-executed. However, I strongly encourage the authors to include empirical results and explore adaptive approaches to estimating \( V_T \) in future iterations.