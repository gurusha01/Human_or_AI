This paper presents a novel learning algorithm for Mixture of Hidden Markov Models (MHMMs) based on the Method of Moments (MoM). The authors address the computational inefficiencies of traditional Expectation-Maximization (EM) algorithms, particularly for large datasets, by leveraging the spectral properties of a global transition matrix to resolve the permutation ambiguity inherent in MoM-based MHMM learning. The proposed approach is validated on synthetic and real-world datasets, demonstrating its computational efficiency and competitive accuracy compared to EM.
The paper builds on prior work on MoM algorithms for latent variable models (e.g., [1, 2, 3]) and extends their applicability to more complex models like MHMMs. The authors' insight that an MHMM can be represented as an HMM with a block-diagonal transition matrix is a key contribution, enabling the use of existing spectral learning techniques. The work also aligns with recent efforts in spectral learning for HMMs and mixtures of models, such as those presented at NIPS in the past decade. However, the paper goes further by addressing the critical challenge of de-permutation in the presence of noise, which has not been adequately explored in prior work.
Strengths:
1. Technical Novelty: The paper introduces a novel algorithm that combines MoM with spectral properties to efficiently learn MHMMs, addressing a significant computational bottleneck in EM-based methods.
2. Theoretical Contributions: The authors provide rigorous theoretical insights, including the equivalence of MHMMs to block-diagonal HMMs and the spectral properties of the global transition matrix.
3. Practical Utility: The proposed algorithm is computationally efficient and scalable, making it suitable for large datasets. The real-data experiment on the UCI handwritten character dataset demonstrates its practical applicability.
4. Clarity of Algorithm: The step-by-step description of the proposed algorithm (Algorithm 1) is clear and reproducible, enhancing the paper's utility for practitioners.
Weaknesses:
1. Empirical Evaluation: While the experiments demonstrate the algorithm's efficiency, the comparison to EM is limited to synthetic data and a single real-world dataset. A broader evaluation across diverse datasets and application domains would strengthen the paper.
2. Conjectures: The reliance on Conjecture 1 without formal proof weakens the theoretical rigor. While the authors justify this limitation, a more detailed empirical analysis of the conjecture's validity would be helpful.
3. Clustering Accuracy: The spectral algorithm occasionally underperforms EM in terms of clustering accuracy, particularly in low-data regimes. This limitation should be explored further, potentially with strategies to improve robustness.
4. Clarity of Writing: While the technical content is well-structured, certain sections (e.g., spectral properties and noise handling) are dense and may be challenging for readers unfamiliar with spectral methods.
Recommendation:
I recommend acceptance of this paper, as it offers a significant contribution to the field of scalable latent variable model learning. The algorithm is innovative, computationally efficient, and well-grounded in theory, with promising results on both synthetic and real data. However, the authors should address the weaknesses in empirical evaluation and clarity in the final version.
Arguments for Acceptance:
- Novel and computationally efficient algorithm for MHMM learning.
- Strong theoretical foundation and innovative use of spectral properties.
- Practical relevance demonstrated on real-world data.
Arguments Against Acceptance:
- Limited empirical evaluation across datasets.
- Reliance on unproven conjectures.
- Occasional underperformance in clustering accuracy compared to EM.
Overall, the paper advances the state of the art in MHMM learning and is a valuable contribution to the NIPS community.