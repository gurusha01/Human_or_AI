The paper presents a novel Bayesian formulation for nonlinear support vector machines (SVMs) using Gaussian processes (GPs) and a scaled mixture of normals to express the hinge loss. It extends prior work on Bayesian SVMs, which was limited to linear classifiers, by incorporating a kernel-based nonlinear decision function. This formulation is further integrated into a hierarchical factor model for joint feature learning and nonlinear classification, addressing a gap in previous work that predominantly assumed linear classifiers. The authors propose efficient inference methods, including Markov Chain Monte Carlo (MCMC), expectation-conditional maximization (ECM), and a GP approximation for scalability. Experimental results demonstrate the utility of the proposed nonlinear Bayesian SVM in improving classification accuracy and interpretability, particularly in high-dimensional datasets such as gene expression data.
Strengths:
1. Technical Novelty: The paper makes a significant contribution by extending the Bayesian SVM framework to nonlinear classifiers, leveraging Gaussian processes. This is a meaningful advancement over prior work, such as [5], which was restricted to linear SVMs.
2. Integration with Factor Models: The integration of the nonlinear Bayesian SVM into a discriminative factor model is innovative and addresses the need for joint feature learning and classification in complex datasets.
3. Scalable Inference: The use of ECM and FITC approximations for inference is well-motivated, offering practical solutions to the computational challenges of Gaussian processes.
4. Experimental Validation: The extensive experiments on benchmark datasets, USPS digits, and gene expression data provide strong evidence of the model's efficacy. The results demonstrate competitive or superior performance compared to traditional SVMs and GP classifiers.
5. Interpretability: The identification of support vectors and the biological relevance of features in gene expression data highlight the interpretability of the proposed approach, which is often lacking in conventional SVMs.
Weaknesses:
1. Clarity: While the technical content is rich, the paper is dense and could benefit from clearer explanations, particularly in the derivation of the mixture representation and the inference methods. Non-expert readers may find it challenging to follow.
2. Comparative Analysis: Although the paper compares its approach to GP classifiers and traditional SVMs, the discussion of why the nonlinear Bayesian SVM outperforms these methods could be more detailed. For example, the impact of the skewed Laplace loss function on performance is not fully explored.
3. Scalability: While FITC approximations are employed, the computational cost remains significant for large datasets. The authors briefly mention this but do not provide a detailed discussion of potential limitations or trade-offs.
4. Broader Impact: The paper focuses heavily on technical contributions but could better articulate the broader implications of the work, such as its potential applications in other domains or its impact on the interpretability of machine learning models.
Arguments for Acceptance:
- The paper addresses a clear gap in the literature by extending Bayesian SVMs to nonlinear settings and integrating them into hierarchical models.
- The experimental results are compelling and demonstrate the practical utility of the proposed approach.
- The use of Bayesian methods provides interpretability and uncertainty quantification, which are valuable for real-world applications.
Arguments Against Acceptance:
- The paper's clarity and accessibility could be improved, particularly for readers unfamiliar with Bayesian SVMs or Gaussian processes.
- The scalability of the approach, while partially addressed, remains a concern for very large datasets.
Recommendation:
Overall, this paper makes a strong scientific contribution and is well-suited for NIPS. Its technical novelty, experimental rigor, and practical relevance outweigh its weaknesses in clarity and scalability. I recommend acceptance, with minor revisions to improve clarity and provide a more detailed discussion of the method's limitations and broader impact.