The paper presents a novel Expectation-Maximization (EM) algorithm for learning the kernel matrix of Determinantal Point Processes (DPPs), which are probabilistic models balancing quality and diversity in subset selection tasks. Unlike prior work that either constrained the kernel matrix to fixed forms or relied on convex approximations, this approach directly learns the full kernel matrix by leveraging its eigendecomposition. The authors demonstrate that their EM algorithm avoids the pitfalls of naive projected gradient ascent (KA), such as poor local optima caused by projection steps, and achieves significant improvements in test log-likelihood on a real-world product recommendation dataset.
Strengths:
1. Technical Innovation: The paper introduces a novel EM algorithm that reformulates the DPP learning problem using eigenvalues and eigenvectors. This approach is computationally efficient and avoids the degeneracies associated with KA, such as learning near-diagonal matrices that fail to model diversity.
2. Empirical Results: The authors evaluate their method on a large-scale Amazon baby registry dataset, demonstrating up to 16.5% improvement in test log-likelihood over KA. The experiments are thorough, with comparisons across different initialization strategies and data regimes.
3. Theoretical Contributions: The paper provides detailed derivations for the EM algorithm, including efficient updates for eigenvalues and eigenvectors. The runtime analysis shows that the EM algorithm is asymptotically faster than KA, which is corroborated by empirical runtime comparisons.
4. Significance: The work addresses a challenging NP-hard problem in DPP learning and has practical implications for subset selection tasks in recommendation systems, document summarization, and beyond. The ability to model negative interactions between items is particularly impactful.
Weaknesses:
1. Clarity: While the paper is technically sound, the dense mathematical exposition may be difficult for readers unfamiliar with DPPs or eigendecomposition. The authors could improve accessibility by including more intuitive explanations or visualizations of key concepts.
2. Limited Scope of Applications: The experiments focus primarily on product recommendation. While the authors claim broader applicability, additional experiments in other domains (e.g., document summarization or sensor placement) would strengthen the paper's generalizability.
3. Initialization Sensitivity: The EM algorithm's reliance on non-trivial initializations (e.g., Wishart distribution or moment matching) raises concerns about its robustness in settings where such initializations are unavailable or poorly estimated.
Arguments for Acceptance:
- The paper makes a significant technical contribution by addressing a long-standing challenge in DPP learning with a novel algorithm that is both theoretically and empirically validated.
- The proposed EM algorithm is faster, more robust, and achieves better performance than the baseline KA method, making it a valuable addition to the field.
- The work has practical relevance for real-world applications requiring subset selection with diversity constraints.
Arguments Against Acceptance:
- The paper's clarity could be improved, particularly for readers outside the immediate DPP community.
- The experimental evaluation, while strong, is limited to a single domain, which may reduce the perceived generality of the approach.
Recommendation:
I recommend acceptance of this paper. Its contributions to DPP learning are both novel and impactful, and it advances the state of the art in a meaningful way. However, the authors should consider improving the clarity of exposition and expanding the experimental scope in future work.