The paper proposes a novel approach to solving large-scale Markov Decision Processes (MDPs) by framing the estimation of the optimal state-action value function as a Difference of Convex functions (DC) Programming problem. The authors focus on minimizing the Optimal Bellman Residual (OBR), which measures the deviation of a candidate action-value function from the optimal Bellman operator. They demonstrate that this minimization is consistent in the Vapnik sense, justifying its use with sampled transitions. The main contribution is the formulation of the OBR minimization as a DC problem, enabling the application of established DC Algorithms (DCA) to reinforcement learning (RL). The paper also includes experimental comparisons with Approximate Value Iteration (AVI) and Approximate Policy Iteration (API), showing that the proposed approach is competitive in terms of performance and exhibits lower variance.
Strengths
1. Novelty: The paper introduces a fresh perspective on solving MDPs by leveraging DC programming, which is relatively unexplored in the RL domain. This opens up new avenues for applying optimization techniques to RL problems.
2. Theoretical Contributions: The authors rigorously establish the consistency of minimizing the empirical norm of the OBR and provide explicit DC decompositions for different norms (e.g., p=1, p=2). This theoretical grounding is a significant strength.
3. Experimental Validation: The experiments, though limited to synthetic Garnet problems, demonstrate that the proposed approach performs comparably to state-of-the-art methods like LSPI and Fitted-Q while offering reduced variance.
4. Potential for Generalization: The authors highlight the possibility of extending their approach to non-parametric settings using boosting techniques, which could make the method more flexible and widely applicable.
Weaknesses
1. Limited Experimental Scope: The experiments are confined to synthetic Garnet problems, which may not fully represent the challenges of real-world RL tasks. The method's performance on more complex, high-dimensional, or continuous-state problems remains unclear.
2. Computational Complexity: While DC programming is theoretically appealing, the paper does not provide a detailed analysis of the computational overhead compared to AVI and API. The reliance on sub-gradient descent for intermediate convex problems may limit scalability.
3. Lack of Robustness in Batch Scenarios: The paper acknowledges that the batch scenario introduces uncontrolled bias in the estimation of the Bellman operator. While alternative techniques like RKHS embeddings are mentioned, they are not explored in the experiments.
4. Convergence Guarantees: The DCA algorithm does not guarantee convergence to the global optimum, and the paper does not analyze the gap between the local minima found by DCA and the true solution.
Arguments for Acceptance
- The paper provides a novel and theoretically sound approach to RL, leveraging DC programming to tackle large MDPs.
- It contributes to the literature by connecting RL with optimization techniques, potentially inspiring future research in this direction.
- The experimental results, though limited, show that the method is competitive with existing approaches.
Arguments Against Acceptance
- The experimental validation is narrow and does not convincingly demonstrate the method's applicability to real-world problems.
- The computational feasibility and scalability of the approach are not adequately addressed.
- The lack of robustness in batch scenarios and the absence of global convergence guarantees weaken the practical utility of the method.
Recommendation
I recommend weak acceptance. The paper makes a valuable theoretical contribution and proposes a novel approach, but its practical impact is limited by the narrow experimental scope and unresolved computational concerns. Expanding the experiments and addressing scalability issues in future work would significantly strengthen the paper.