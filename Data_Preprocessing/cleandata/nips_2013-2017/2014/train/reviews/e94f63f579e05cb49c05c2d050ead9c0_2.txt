This paper explores the application of Bayesian quadrature to Bayesian inference. This doubly Bayesian strategy was developed by Osborne et al. (NIPS 2012). The present work builds on those ideas and introduces some further innovations. The general approach is to build a probabilistic model of the posterior surface of some Bayesian inference problem, to allow ("meta") inference over quantities of interest, such as the partition function. The first innovation here is to use a squared Gaussian Process to model the likelihood (Osborne et al., used an exponentiated Gaussian Process) and the second innovation is to choose evaluate the posterior at points of high uncertainty. As before, various approximations must be made to make inference tractable.
These are valid innovations and the authors demonstrate their effectiveness. But since the obvious (and stated) precedent is that of Osborne et al., I would like to have seen "WSABI" experimentally compared against an exponentiated GP rather than just compared against the un-warped Bayesian Monte Carlo.
One pedantic point: some people find it grating to have the term "95% confidence interval" used to describe a region containing 95% of the posterior mass since it invites confusion with frequentist confidence intervals. You could consider avoiding the term. I also think the title of the paper is confusing and possibly misleading. You're using the term "sampling" in a very different sense from its usual meaning in the context of Bayesian inference. Something more explicit like "Squared Gaussian Processes for Fast Bayesian Quadrature" could be more appropriate.
 Some worthwhile innovations over recent work using Bayesian quadrature for Bayesian inference.