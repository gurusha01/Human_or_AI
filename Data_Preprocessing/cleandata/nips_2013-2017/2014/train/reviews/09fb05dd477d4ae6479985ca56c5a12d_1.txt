This paper addresses the issue of object detection, in particular the challenge of obtaining bounding boxes on a scale similar to which category labels exist for object categorization. The authors side-step this challenge by proposing to adapt object classifiers for the detection task. Their algorithm is fairly simple and straightforward, which is not a bad thing in itself. Their experimental protocol uses 100 categories for training (with both category labels and bounding boxes), and tests on 100 left-out categories. These left-out categories had only category labels. The results obtained were reasonably good, filling about 35% of the gap between an oracle and a baseline on the 100 left-out categories.
Quality and clarity: This is a good quality, clearly-written paper.
Originality: This paper combines two existing ideas related to adaptation: adapting object classifiers for detection, and adapting from one set of categories to another. To the best of this reviewer's knowledge, this is a novel and interesting idea -- but not ground-breaking.
Significance: The main contribution of this paper is the novel (but not ground-breaking) approach. Because results-wise, it is hard to judge the long-term significance, although they will likely be surpassed later by more sophisticated method. (This is to be expected). The main issue with the results is that they inevitably sacrifice performance (mAP) by using fewer bounding boxes, and there is no objective way to judge if this trade-off is worth it.
(As a side note, it is not clear if the issue being tackled is really an important long-term challenge. Yes, bounding boxes are more "expensive" to obtain than category labels. But who is to say that throwing resources at the problem won't fix it? Not too many years ago, having millions of images labeled with 20,000 object categories seemed almost impossible...)
Detailed issues that should be addressed:
- In Figure 3a, which methods do (or do not) use category labels to help with detection, other than their method and R-CNN? The authors should state this clearly, to be clear about an apples-to-apples comparison.
- My judgment of how good the proposed approach is, is based on the the blue bars in Figure 3b. DNN is about 1/3 between baseline and oracle. Do the authors agree that this is one of the main results? If yes, please highlight it, not the 78% (line 341) -- which is not quite the right number for 2 reasons: 1) should judge based only left-out categories, and 2) should not simply be a percentage of oracle performance (should take baseline into account). Analogous problem for the claim of 50% improvement (line 425).
- Authors should devote a table or a sub-section to clearly stating the differences from R-CNN, which seems to be the closest competing alternative. Currently, these statements seem to be scattered or not as prominent as they could be. I would suggest "sacrificing" Figure 6, which are simply anecdotal examples.
Minor issues (no need to respond)
- Is the proposed method called "DNN" or "DDA"???
- Typo on line 431 ("classi?ers") Overall, this is a reasonably good paper proposing a reasonably novel approach. I believe the paper deserves to be accepted, and to be seen by the NIPS community. While the results will almost certainly be surpassed soon, since the method is simple (fine, this being the first paper to take this approach), the long-term significance of the paper (and the approach in particular) remains to be seen.