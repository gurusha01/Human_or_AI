Summary: The authors present a model of auto-associative memory in a rate-based neural network subject to a battery of biological plausible constraints. Previous models of auto-associative memory have failed to include several key features of real biological networks, namely an adherence to Dale's Law that neurons have a strictly excitatory or inhibitory effect on their projections and the observation that networks can encode memories without relying on units that simply respond at their saturation rate or respond in a binary manner. Memories are encoded in the network via synaptic modifications based on a gradient descent procedure, constrained using a recently published method for ensuring that the linearization of the dynamics around a dynamical system's fixed point is stable. The authors illustrate the effectiveness of their training procedure with simulations, noting that the trained fixed points exhibit slow network dynamics (i.e. they are close to being, but are not exactly, fixed points) and are stable, as desired. The authors note two features of their trained network that are in line with experimentally observed features of cortical networks, namely a distribution of synaptic weights centered at zero with long tails and that an average network unit received approximately equal excitatory and inhibitory synaptic input. The authors identify these features as key components to the networks success in performing robust auto-associative memory when the network is perturbed from a fixed point, even in the presence of stochastic input noise. 
Review: This is a clearly-written, novel submission that will be of great interest to the community. The authors nicely review the history of training neural networks to perform auto-associative tasks, highlighting the main contributions and shortcomings of previous work. The network architecture and training procedure are both presented in a clear way and the figures are well-chosen to quickly illustrate the main results of the submission. The authors illustrate the networks performance limitations when network noise is included, which highlights the robust solution the training procedure finds. 
One recommendation would be to clarify the relationship between the chosen training procedure and the stated objectives. As the author's noted other methods have been employed in the past to enforce Dale's Law or achieve graded network memories, with the latter condition seemingly more difficult to enforce. The chosen training procedure achieves the desired goal of building an auto-associative network with non-saturing units, but nothing in the text indicates that the authors chose this method because they believed it was particularly well-suited to this task. Perhaps another network training procedure would achieve the same result? Can anything be said as to why this particular method achieves that goal so effectively?
Also (although probably beyond the scope of this paper) given a rich history of training networks to perform auto-associative tasks, a comparison of how the trained network performs against previous models in terms of network capacity (number of memories as a function of network units) would surely be of interest to the community. 1-2 sentences: Applying a recent method for ensuring stable fixed points in a dynamical system, the authors introduce a new model of auto-associative memory that conforms to several constraints of real neural networks, most notably the condition that network units are capable of encoding memories without saturating or binary responses.