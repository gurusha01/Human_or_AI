This paper presents a new neural network architecture for classifying videos of human actions. The model combines the predictions of two convolutional neural networks: one trained on single video frames and the other trained on short sequences of dense optical flow images. The model significantly outperforms other ConvNet-based approaches to action recognition and matches the current state-of-the-art on two standard video classification datasets.
While the paper essentially applies the standard ConvNet image classification pipeline to a new kind of data (dense optical flow frames) and combines this with a single frame ConvNet, the experiments are thorough and the results are impressive. It was good to see several different ways of training a ConvNet on optical flow compared with the results clearly showing the benefits of using optical flow as input.
I have only a few minor comments:
- It is not clear why the spatial network does so much better than the models from [13] given that the architectures are similar.
- It would be interesting to see a comparison of different sampling schemes for the frames fed into the spatial stream. This is a good empirical paper that will be of interest to anyone working on video classification with ConvNets.