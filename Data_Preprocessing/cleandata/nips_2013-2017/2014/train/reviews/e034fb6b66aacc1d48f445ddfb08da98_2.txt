The authors extend log-bilinear language models by replacing the traditional two-way interactions (matrix) with three-way interactions (tensor, which is then factorized). That is, the authors replace the energy with a factored energy. This is what is routinely done when transforming RBMs into a factored RBMs. 
The idea of using factored models has been widely explored in a number of tasks and applications. Nonetheless, the authors present several nice examples of the application of factored log-bilinear models to language tasks: context sensitive language models, sentiment classification, cross-lingual document classification, blog authorship attribution, and conditional word similarity.
What I found most interesting in these applications was the choice of attributes. I was disappointed with the performance of the models on quantitative tasks. Here the authors point out that with sophisticated hyper-parameter search the gap can narrow. This I believe is speculative. It is also conceivable that these are relatively small datasets and consequently models with more parameters will need better regularizers (or much more data).
The experiment on blog authorship makes a good case for a wider adoption of factored log-bilinear models in language tasks that use log-bilinear models. The experiment on conditional word similarity is very nice.
  The paper applies factored log-bilinear models to a wide range of language applications and demonstrates the value of context variables to some extent. The paper is well written, fun and clear.