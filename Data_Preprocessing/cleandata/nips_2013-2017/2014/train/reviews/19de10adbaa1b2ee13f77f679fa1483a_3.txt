The authors proposed a CNN that incorporates a form of top-down processing in the form of attention-based feedback. The results on the CIFAR datasets show state-of-the-art performances.
This paper is generally interesting to read and demonstrates novelty in terms of model design and application for the vision problem. It is also forward looking to consider both bottom-up and top-down signals for training a CNN. I particularly like the aspect of evaluating the model across time and using gated signal to constraint the representation. Since the quality of the representations learned hinges on the attention policy, it would more interesting if the authors can suggest or demonstrate cases where the algorithm may fail.
The experiments are interesting, with the authors demonstrating the model in various ways, such as visualization to help understand it better. However, since the method is more complex than the traditional feedforward CNN, I think the paper can be improved if there were results on ILSVRC, to truly demonstrate it's scalability to a larger dataset, with larger images. That will be my main (and possibly only) concern. The combination of attention-based feedback and and CNN makes for an interesting model, which is worth further study. More practical insights from experiments on larger data sets will be preferred.