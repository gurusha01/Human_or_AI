Summary of the paper:
The paper proposes a trajectory optimization method derived from iLQG, where instead of using the linearization of a known model, the linearized dynamics is acquired from samples. In order to ensure proper convergence in iLQG a backtracking line search can be implemented in the forward pass to ensure that the new trajectory does not differ too much from the previous one. In this paper, since the dynamics is unknown the line search is replaced by the addition of a constraint on the KL divergence between the previous and the new trajectory distribution during optimization to constrain the change in the new trajectory. A GMM model is used as a prior for the sampled unknown dynamics. Moreover, a parametrized policy is learned through guided policy search using the trajectory optimization framework with a neural network to approximate the policy. Experiments on 3 different simplified robotics problem as well as comparisons with other methods are provided to support the claims of the paper.
Comments:
The paper is interesting and well-written and in general it addresses an important topic for robotics research. In terms of the novelty of the approach, the main contribution of the paper is to show how iterative LQG can be used in a model-free context while ensuring convergence by replacing the backtracking line-search with a KL divergence constraint. While this is an interesting approach, one could argue that this technical improvement is rather incremental given knowledge on iLQG methods and linear Gaussian approximation of the dynamics. The guided policy search approach is also interesting but is also related to previous work, for example the work of Mordatch et al. (RSS 2014). One good aspect of the experiments is that it nicely shows that the proposed approach converges faster than other methods, even without the use of GMM as a prior model.
One comment with the use of a GMM prior is that this prior somehow already constitutes a model and therefore it is expected that the proposed approach performs close to model-based trajectory optimization methods. Indeed, it is mentioned in 3.2 that large mixtures that modeled the dynamics with high details produced the best results. This is to be expected since this basically already provide a good dynamic model. One question that arises is how is this GMM constructed? (i.e. what is the method to sample the potentially high dimensional space while ensuring safety on a real robot?). Additional comments on the likelihood that such an approach could scale to more realistic robot models where such GMM construction would be extremely challenging would also be useful. 
Another possible important limitation of the approach is that it already requires an example demonstration for the walking task, which is only a 2D walking task. It is well-known that 3D walking is much more difficult than 2D walking (where relatively simple feedback control solutions are known to exist for this type of walking, e.g. the work of M. Spong or R. Gregg or J. Grizzle on the topic). Having an example demonstration means that it is already possible to solve the problem in some sense. It would be interesting that the paper comments on this issue. It also raises the question on how this method can really scale to more complicated problems (e.g. 3D walking or swimming with more than 2 joints). Indeed, assuming that the GMM prior is precise, it is surprising that the optimizer does not find a walking solution since the iLQG method can find such solutions with 3D walking and many more DOFs.
Another comment concerns the comparison with iLQG for the tasks involving contacts. What is the contact model used in this evaluations and what is the contact model used by the iLQG algorithm? Tassa and colleagues have shown very good results with tasks involving contacts and many DOFs using iLQG, one of the reason residing in the choice of a contact model that is more "friendly" with iLQG optimization techniques. For a fair comparison, the paper should compare the tasks using this contact model for iLQG. Overall the paper is interesting and well written and the comparison with other learning approaches is very useful. My main concerns are related to 1) the use of KL divergence for line search which is a rather incremental change of iLQG and 2) scalability issues of the approach for more realistic scenarios.