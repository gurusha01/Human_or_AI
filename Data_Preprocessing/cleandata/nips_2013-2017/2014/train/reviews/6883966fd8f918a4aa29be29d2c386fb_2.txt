This paper proposes a new method for actively learning a linear regressor, under 
the usual least-squares loss function. The method works roughly as follows:
1. Let's say the data lie in R^d. First, a partition of the space is chosen.
Then, each data point and response value are reweighted in a simple manner that
is uniform within each cell of the partition and is specified by a single real
number within each cell. The authors show that this reweighting leaves the
squared loss of any linear function unchanged.
2. However, different weightings yield different rates of convergence for the
linear regressor. Here the authors are very insightful in the way they use
a recent generalization bound for linear regression.
3. Finding the optimal weighting, the one that leads to the best rate of convergence,
requires labels. The authors give a parsimonious, active way to do this, and along
the way, to estimate the linear regressor.
4. Steps (1)-(3) are all for a particular partition of space. The authors suggest
picking successively finer partitions as the number of points grows.
Label complexity bounds are given for steps (1)-(3).
Comments:
This paper has a lot of novel ideas and insights: of particular interest are the
reweighting method for the distribution and the way in which the new generalization
bound for regression is exploited. The paper also presents an algorithm that can 
be made reasonably practical. All in all, this is a significant advance in the state
of the art in active linear regression.
There are a few things that one could quibble about:
1. The analysis requires that the "label noise" for data point x be bounded by
O(||x||^2). It would be nice to do away with this. Still, previous work on
active regression has made far stronger assumptions on the noise.
2. No method is given for refining the partition, and there is no analysis of
the asymptotic rate that would be achieved. This doesn't bother me: in practice,
a reasonable partition could be obtained by hierarchical clustering, for instance.
 A novel and insightful paper that advances the state of the art in active learning forleast-squares linear regression.