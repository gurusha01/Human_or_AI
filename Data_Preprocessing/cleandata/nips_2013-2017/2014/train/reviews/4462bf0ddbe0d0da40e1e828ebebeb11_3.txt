Determinantal Point Processes is a distribution over a fixed ground set that assigns higher probability to diverse sets. DPPs can be parameterized by a positive semidefinite matrix (L). Since learning L is np-hard, only partial learning of L has been discussed as prior work. Learning a scalar vector for L compromises on the diversity dimension of DPP. Previous work have resorted to restricting the parametric form of L. In this paper, the authors propose a learning method that does not restrict parameterization of L. The learning method does not require a projection step as in gradient ascent that can lead to the similarity property being compromised, and compromise on the diversity property. Using EM on eigen-values and eigen-vectors overcomes this disadvantage. They explore some optimization algorithms to solve this without needing to project values. Using projected gradient ascent requires projection of both eigen-values and eigen-vectors. Exploiting the fact that V is full-rank, they avoid the projection of eiger-vectors. Jensen's inequality is used to lower bound the objective function and construct an EM procedure. 
The learning method is useful as it preserves the diversity property of the DPP as opposed to other previous work where only the quality property has been focused upon. The step-by-step derivation of the EM procedure is insightful. Mapping the constraint to a an optimization over Stiefel manifold to eliminate the projection of eigen-vector and showing that the inverse distribution is a DPP are solid contributions. They test the learning algorithm on both synthetic datasets and a product recommendation task. The experiments support the claim in the paper â€“the learning method retains the quality and diversity of the DPP, by comparing it with KA. While only two of the top 10 products are replaced by KA by products that are less likely, it still brings out the contribution of the paper and supports the claim made in the beginning. The conclusion is a bit hasty not bringing forward all the contributions of this paper, but overall the paper is written cohesively. The proofs are well-written and the step-by-step reduction of optimization to EM is interesting. 
 The paper is written coherently and proofs are well-written. The step-by-step reduction to EM is interesting and well explained. More experiments in real-world datasets can illustrate the importance of the "diversity" produced by this algorithm when compared to others in this area.