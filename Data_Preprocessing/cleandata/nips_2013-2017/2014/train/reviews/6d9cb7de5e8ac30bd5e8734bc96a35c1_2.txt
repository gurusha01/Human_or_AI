In this paper, the authors proposed a new norm, called scaled latent trace norm, to relax the convex condition for the tensor multilinear rank. Both theoretical and experimental results are presented to show that the advantage of the scaled latent trace norm especially when the multitask learning task has inhomogeneous dimensions and there is no priori knowledge about which mode is low rank.
Strengths about this paper:
1) This paper is well written.
2) The authors develop nice theorems that show the upper bound of the error between the empirical risk and the true risk in different scenarios in which overlapped trace norm, latent norm, and scaled latent norm are involved respectively.
3) The authors also list all sample complexity for matrix completion, multitask learning, and multilinear multitask learning to compare the results.
4) The authors provide corresponding experimental results to show that the scaled latent norm performs better when multitask learning involves inhomogeneous dimensions.
Some aspects to clarify/improve:
1) Latent trace norm is studied extensively in R. Tomioka and T.suzuki (2013), and the contribution of this paper is to develop "scaled" latent trace norm. The difference is not that big. 
2) From the results (Table 2), the sample complexity of the scaled latent trace norm is a little better than that of the latent trace norm. In tensor completion, it's hard to tell whether scaled latent trace norm is always better than that of the latent trace norm.
3) The upper bound of all scenarios involved in three norms are all proportional to the number of training samples to the power of negative 1/2, which makes them not essentially distinguished. In general a good paper with nice theoretical results.