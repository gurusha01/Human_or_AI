The paper introduces methodology to learn a semantic space for object categorization, where semantic entities like supercategories and attributes are used to constrain the resulting (discriminative) embedding. The main idea is to (sparsely) represent a category by means of its super-category + a combination of attributes (tiger=striped feline). An advantage of the method is the capacity to generate compact semantic descriptions for the learnt categories. The paper is clearly presented, the motivation is sound and the good results well-emphasize the superiority of the propose techniques. While some of the principles of designing cost functions for discriminative embeddings with good inter-class separation have been described elsewhere (e.g. [7], [14]), the authors here present novel ways to instantiate such ideas in order to connect categories, supercategories and attributes.
- Other Comments
The introduction is a bit confusing. Too many elements (concepts) float around without a clear buildup. On lines 077-079 it is unclear what the generative and the discriminative objectives are until later. The combination of precise terminology (e.g. generative/discriminative) and imprecise anchoring in the particular context makes the text more difficult to follow.
Line 145: S(z_i,…) not S(z,…)
The models are non-convex. How is the initialization performed?
 A compact semantic space model that learns a discriminative space for object categorization by leveraging constraints from supercategories and attributes. Improved results on AWA dataset with the additional advantage of a model that can generate a human-interpretable decomposition of categories.