The authors introduce a new model for multi-armed bandits with non stationary rewards based on the idea of imposing an upper bound on the total variation of changes in the expected rewards of the arms. They prove a lower bound and introduce a new algorithm (that is an extension of EXP3) with a matching upper bound in terms of the dependency on the time horizon.
The paper is well structured and written. The related work is properly cited and the connection with previous papers is reasonably covered.
I think the model is not sufficiently motivated. While setting a maximum bound on the variation of expected rewards is natural from a theoretical standpoint, it is not immediately obvious how one would test such a property in a practical setting. I think the authors should improve the presentation from this perspective. That being said, setting the problem in this way leads to an interesting intermediate setting between stochastic and adversarial bandit settings. 
Regarding the lower bound, I enjoyed the presentation of the sketch of the proof for Theorem 1. The ideas are nice and presented succinctly and clearly.
One key issue with the paper from my point of view is the definition of Rexp3. The algorithm is defined to have an optimal bound in the worst case (that from Theorem 1) in the sense of restarting an EXP3 instance at the beginning of every epoch. While it might be enough from a theory standpoint, it seems highly impractical to completely forget everything at the beginning of an epoch. Another reason of concern is that VT is an input to the algorithm. Why is it reasonable to assume VT is known or can be learned? I am not claiming it is not possible, but I think the authors should at least discuss this assumption.
I also have a question regarding Theorem 2: the proof holds for VT >= 1/K (as mentioned in the theorem statement). What happens with the algorithm when VT gets very close to 0 (the stationary case) and much smaller than 1/K? I understand the theorem doesn't cover this case, but it seems like a natural scenario to consider.
[Update] I have read the author feedback and I consider it satisfactory so I increase the score to 6: Marginally above the acceptance threshold.  The paper is well written and tackles an interesting and very relevant problem. At this point, due to the issues with the algorithm described above, I am inclined to recommend rejection.