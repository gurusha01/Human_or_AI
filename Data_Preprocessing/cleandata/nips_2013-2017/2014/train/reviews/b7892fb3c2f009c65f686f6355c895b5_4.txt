This paper proposes an extension of the previous Bayesian formulation of conventional Support Vector Machines (SVMs). The previous Bayesian formulation proposed a likelihood of the form of Gaussian mixture, and the authors extend the formulation using the priors on the scale-mean parameter. This formulation results in a skewed Laplace likelihood which is different from the previous hinge loss. In addition, the Gaussian process extension of the Gaussian prior on the weight vectors presents a nonlinear SVM formulation. Two different optimization method is proposed for optimization.
The paper shows a nice example of extending the conventional SVM framework to a Bayesian setting using a well-defined formulation with prior distributions. According to the authors, the previous work [4] showed a connection between an infinite Gaussian mixture formulation and a hinge loss likelihood producing the SVM solution, but the formulation was improper due to the flat prior. The authors show how the formulation can be extended and how the inference can be performed using a well-defined formulation. The results show how the likelihood changed from the hinge-type likelihood to the skewed Laplacian. The explanation is clear and I enjoyed reading the manuscript. One thing I did not understand is the derivation of the predictive distribution in Eq. (11). Does the supplementary material include the derivation? I want a detailed derivation of Eq. (11) either in the paper or in the supplementary if the paper is accepted.
 The paper is in general clear and the proposed formulation is correct. This paper can be a nice material for most of NIPS readers showing an example of Bayesian formulation of SVM.