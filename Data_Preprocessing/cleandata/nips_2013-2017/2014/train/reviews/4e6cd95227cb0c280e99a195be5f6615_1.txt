This paper presents a new Gibbs sampler algorithm for FHMMs. The idea is to add an auxillary variable, U, to the state of the Gibbs sampler. The value of U restricts the set of possible values that the hidden state X can take at the next step of the Gibbs sampler. As the number of possible values for X_i is small for each time point i, we can update X given U (and the data) using FFBS.
I think this is an original and clever approach to an important class of problems. The paper is written very clearly, and an empirical evaluation of the algorithm is done thoroughly. More importantly the intuition behind why (and when) this method would work well is presented. There are obvious extensions to this general idea -- which suggests the overall impact of the paper could be high.
E.g. one extension that may be possible is to define the set of values Xi could take given U in a way that depended on yi (i.e. used likelihood information to so that you run FFBS allowing for the most likely states of X_i at each time-step).
I noticed a couple of typos
p.3 "four bits" -> "for four bits"
p.3 could you define Hamming distance?
p.4 "U as auxiliary" -> "U as an auxiliary"
 A novel approach to MCMC for FHMM models: clearly presented and with a lot of scope to influence development of algorithms for this class of model.