Summary:
This paper proposes a regularization technique for neural nets where the model
is encouraged to reduce the variance of each hidden layer representation over
dropout noise being added to the layers below. This idea is generalized to
``pseudo-ensemble" models where other kinds of perturbations can be used.
The main contribution of this paper is the variance regularizer. Experiments
are done on MNIST (supervised and semi-supervised) and NIPS'11 transfer
learning dataset (CIFAR-100, TinyImages) using standard neural nets with
dropout perturbations. The authors also experiment with the Stanford Sentiment
Treebank dataset using Recursive Neural Tensor Nets with other kinds of
perturbations. The experiments show that this regularizer works the same or
better than using the perturbations alone.
Strengths-
- The model gets promising results in harsh situations where there is very
little labelled data.
- The experiments are well chosen to highlight the applicability of this method
to different models and datasets.
Weaknesses -
- Some parts of the paper seem somewhat superfluous. It's not clear what the
discussion about Baggy/Boosty PE adds to the paper (assuming that the main
point is the variance regularizer).
- Some crucial details about the experiments should be included. Those are
mentioned below.
The authors should mention / discuss -
(1) How many noise samples were used to compute the different variances ?
(2) Was back-prop done through each dropped-out model or just the clean one ?
(3) One of the major problems of dropout is that it slows down training. This
approach probably further exacerbates this problem by requiring that one must
do multiple forward and (back ?) props per gradient update (with/without noise,
or with different noise samples to compute the variance). It would be good to
analyze how much of a slow-down we get, if any, by making a plot of
training/test error vs time (as opposed to number of epochs).
(4) What was the stopping criterion for the semi-supervised MNIST experiments ?
The previous section mentions "We trained all networks for 1000 epochs with no
early-stopping." Does that also apply to the semi-supervised experiments ? If
yes, was it kept 1000 epochs even for 100 labelled cases ? It seems likely that
500-500 or 1000-1000 nets would overfit massively on the labelled data sizes
considered here, even with dropout, by the end of 1000 epochs for reasonable
learning rates. Is this true ? I think it is very important to know if early
stopping with a validation set was needed because in small data regimes, large
validation sets are hard to find.
(5) If multiple (say $n$) forward and backprops are done per gradient update in
PEV, would it be fair to compare models after running for a fixed number of
epochs ? Wouldn't each PEV update be equivalent to roughly $n$ regular SDE
updates ?
(6) For the semi-supervised case, did each mini-batch contain a mixture of
labelled and unlabelled training cases ? If yes, what fraction were labelled ?
(7) Consider comparing with SDE+ in Table 1 ?
(8) Was the same architecture and dropout rate used for SDE and PEV in Table 1
? If yes, is that a fair comparison ? May be it's possible that for SDE, a
smaller net or the same net with higher dropout would work better ? It is clear
that PEV is a ``stronger" regularizer, so we should probably also let SDE be
strong in its own way (by having higher dropout rate).
Quality:
The experiments are well-designed. Some more explanations and comparisons, as
asked for above, will add to the quality.
Clarity:
The paper is well-written barring minor typos.
Originality:
The variance regularizer is a novel contribution.
Significance:
This paper could have a strong impact on people working with small datasets.
  This paper proposes an interesting way of regularizing models. The experiments are convincing, but the paper can be improved by adding some more details and clarifications.