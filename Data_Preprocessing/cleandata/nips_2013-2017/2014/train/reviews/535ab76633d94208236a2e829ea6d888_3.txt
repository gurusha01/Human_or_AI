This paper looks at the problem of learning the true labels of objects in the context of crowd-sourcing systems and considers broader classes of adversarial voting strategies (and very powerful adversaries) than had been considered before. 
Here, the authors propose novel reputation-based algorithms based on user disagreements and the use of semi-matchings which identify adversarial users. For these algorithms, the authors were able to show that they improve vote aggregation quality for 3 widely used vote aggregation algorithms. In addition, the authors show that their definition of reputation is compatible with the notion or reliability of users and that the algorithm can detect adversarial users (even when 'smart' adversarial strategies are employed), under the assumption that non-adversarial users have high reliability.
Finally, the authors establish bounds on the minimum damage that may be caused by smart adversaries.
Overall, I found the paper to be both interesting and original in that it considers different classes of adversaries than I had seen previously. At the same time, the authors make a number of strong assumptions which may not be realistic, such as (a) the degree to which 'honest' users label objects correctly (see Assumption 1 in Section 3.2), (b) the knowledge of the complete honest user voting pattern by adversarial users and (c) the knowledge of the decision rule being employed by the adversaries. Given these, it is not clear if the results of Theorem 3 and 4 do really have much practical relevance.
The other point I was curious about was the way the authors motivate not giving any credit for agreement (Section 2), since they didn't want to give any incentive for adversaries to agree with honest users. In a scenario where all votes by honest users are known, this may be sensible (adversaries can simply agree with a set of items whose label they don't want to influence and which have a large number of honest votes already), but in practical scenarios where adversaries don't know the voting pattern, it's not at all clear that no credit should be given for agreement.
Detailed comments:
- It would be very interesting to see how the results in Section 4 depend on the threshold of 20% placed on the set of users that are being removed. 
 An original look at user-filtering in the context of crowd-sourcing. The authors show good practical results and have some interesting theory behind it, but require some strong assumptions for their theoretical results.