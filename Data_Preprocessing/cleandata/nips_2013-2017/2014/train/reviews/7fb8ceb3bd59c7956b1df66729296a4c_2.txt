This paper is refreshing in its scope and ambition; I've reviewed and read many NIPS papers that are just one tiny idea explored in excruciating detail. However, despite the paper's considerable breadth and the effort placed into evaluation, I feel this paper may walk the line too far to the side of broad and shallow, versus narrow and deep. I lay out specific concerns below that limit the impact of the results presented.
The area of algorithm selection / metareasoning is key in both AI and cog sci. 
I am surprised that more work hasn't been done in this important area. 
(I don't know the literature, but I am trusting the authors' review which 
mostly consists of decade-old work. They do fail to cite work in cognitive
architectures like SOAR and ACT-R which probably doesn't have a lot to
offer in terms of how people learn metastrategies.)
I am confused by the authors' decision to focus on the domain of sorting
algorthms because this domain doesn't lend itself well to the rational
metareasoning (RM) model. RM attempts to optimize a combined measure of
algorithm accuracy and algorithm time/opportunity cost. However, for sorting
algorithms, don't all algorithms achieve perfect accuracy? And as a result,
isn't the modeling of scoring (Equations 3, 8, 9, and 10) irrelevant?
I also wonder whether there is much benefit of a Bayesian linear regression
approach over simple ridge regression, given that only parameter means are
being utilized (Equation 12)? That said, the representation selected seems
very sensible (using the two features, their logs, and 2nd order polynomial
terms involving these features and their logs).
The work claims to contribute both to state-of-the-art AI approaches to
algorithm selection and to the psychological theory of metareasoning. I have
questions about its contributions to each, which I'll address separately.
With regard to AI, the authors compare their results to two existing models.
The Lagoudakis method doesn't appear to utilize presortedness. As a result,
it seems more like a straw man than a serious contender. With regard to
the comparison to Guo, I had some concern that (based on the text on line 160)
a different measure of presortedness was being used by Guo, but the authors
assure me in their rebuttal that they use the same representation as Guo.
With regard to the work's contribution to cognitive science: In human strategy
selection, the trade off between accuracy and opportunity cost is key, and the
opportunity cost involved is not only the run time of an algorithm, but the
cost of selecting an algorithm, as reflected in Simon's notion of satisficing.
Thus, I question selection of sorting algorithms as the most suitable domain
for studying human strategy selection. Although the experimental set up
in Section 5 is elaborate and impressive, the coarse performance statistics
(proportion of merge sort selection and overall quality of selection)
hardly make a compelling argument that the RM model is correct. All we
know from the experiment is that both the RM model and people are pretty 
good at selecting strategies, whereas the other models are not. This result 
gives us little insight as to whether the RM model is a good cognitive model.
I couldn't find much detail about training the models used in Section 5, but if
these are really meant to be cognitive models they should be trained on the
same data that people had available during practice, and the same total number
of trials. The author rebuttal assures me that the training data is identical.
MINOR COMMENTS:
[093]: A Gaussian distribution may be acceptable for sorting algorithm 
runtimes, but it's probably not the best choice for modeling human response
times. On a range of tasks from simple button presses to memory retrieval to
complex problem solving, reaction times tend to have a long-tailed asymmetric
distribution.
[091]: The standard deviation does not appear to be a polynomial in the
extended features (Equation 7)
[168]: The table caption should explain the performance measure. I believe
it is the percentage of runs in which the optimal sorting algorithm was
selected.
[295]: The text says that Equations 13 and 14 suggest the conditions
under which merge sort will be chosen over cocktail sort. However, the
coefficients on ncomparisons and nmoves in Equation 13 are both smaller
than the corresponding coefficients in Equation 14, so it seems to me
that cocktail sort should be chosen for all but the very shortest lists. The work addresses an important challenge to AI and to cognitive science. The authors try a straightforward approach to learning strategy selection and get sensible results in a limited domain (selecting a sorting algorithm).