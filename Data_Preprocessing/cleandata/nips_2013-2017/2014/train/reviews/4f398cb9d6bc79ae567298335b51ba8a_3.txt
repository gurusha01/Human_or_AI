This paper studies a a multi-layer conditional model for deep learning, where the input is sequentially multiplied by some weight matrices and then passed through nonlinear transfer functions. The aim is to choose the weight matrices so as to match the known multi-label target vectors of the training data and to keep some norms of the weight matrices small (regularization). Each non-terminal layer of the model produces unobserved outputs, which are treated as latent variables. If there are only two layers, it has been shown in reference [26] that the optimal weight matrices can be found approximately by solving a tractable convex optimization problem. This convex formulation is facilitated by the following tricks:
(1) The nonlinear transfer constraints linking the inputs and outputs of each layer are softly enforced by a nonlinear penalty function, which can be chosen to be linear in the layer's weight matrix.
(2) The bilinear coupling between the weight matrices and the latent variables is eliminated by a coordinate transformation whereby the problem is reformulated in terms of each layer's linear response and input kernel matrix.
If there is a third layer, one needs to find a (approximate) reformulation that is jointly convex in the inputs, latent variables and outputs of any intermediate layer. If the latent variables are Boolean-valued, [26] propose an exact reformulation under the additional assumptions that the penalty functions are convex in the propensity matrix and the output kernel and that the domain of the output kernel admits a tight convex relaxation.
This paper builds on the ideas from [26] to develop an approximate convex formulation for general models with intermediate layers. The key enabling mechanism is to introduce normalized kernels, which are defined in a similar way as the ordinary kernels but are normalized such that all of their eigenvalues are either 0 and 1. Moreover, the classical regularization is replaced by value regularization (penalizing the norm of the weight matrices multiplied with the outputs of the layer), and the penalty functions encoding the nonlinear relationship between weighted inputs and outputs are assumed to adopt a specific form (where the implicit linear transfer functions are step functions). Finally, a tight convex relaxation of the domain of the normalized output kernels is required (it is suggested to use simple spectral constraints).
The paper also suggests a nested optimization algorithm to optimize the training objective. The outer optimization is based on a conditional gradient algorithm and optimizes the best training objective as a function of the normalized kernels (i.e., essentially setting all propensities to their optimal values given the kernels). This algorithm involves a totally corrective update step, which is carried out efficiently via block coordinate descent.
The idea to use normalized kernels is a clever one that clearly deserves to be published. The numerical tests show that three-layer models can offer tangible advantages over two-layer architectures both in synthetic experiments as well as in experiments with real data.
It is shown that the loss function of the final formulation is jointly convex in all optimization variables (normalized kernels and propensities). However, the problem still contains non-standard constraints which required the propensities to be representable as matrix products involving the input and output kernels of the respective layer. These constraints are per se nonconvex. So unless I am missing something, this paper does not offer a fully convex model for training multi-layer models. If my understanding is correct, I feel that the authors overstate their contribution. Generally, it would be good to explicitly list all approximations that are made when passing from the desired model to the one that is actually solved.
I have the impression that the exposition could be improved by writing down the full 2 layer model from [26] and the full three- (or multi)-level model proposed here (including all constraints). In order to develop a better understanding of the new formulation with normalized kernels, it would also have been nice to discuss the difference between this approach and the approach from [26] when both are applied to a two-layer model. What are the differences between the two approaches in this special case?
It was also not clear to me whether the methods from [26] extend to multi-layer architectures whenever all latent variables are Boolean-valued. This point should be clarified.
On l352 it is stated that R is not jointly convex in its variables and that therefore an alternating optimization scheme can only produce a stationary point of R. In general, however, I disagree that the limit is a stationary point/local minimum. So the statement that a stationary point of R can be found may already be too optimistic. Here is an example of a convex maximization problem with linear constraints and a concave objective function for which an alternating optimization scheme gets stuck at a non-stationary point: max f(x,y) s.t. 0<=x<=1, 0<=y<=1, where f(x,y)= min{-x+2y, 2x-y} is jointly concave in x and y. If we start at point (1,0) and optimize w.r.t. x (fixing y=0), we will end up in point (0,0). Optimizing over y will not get us away from this point. That is, we are stuck at (0,0), which is not a local optimum of f. It is not clear to me why the proposed block coordinate descent algorithm described in lines 348/349 should not suffer from similar problems. If we are not certain to find a stationary point of R, then the relevance of Theorem 1, one of the central contributions of the paper, remains unclear.
Specific comments:
l212: Say explicitly that the squared Frobenius norms of the weight matrices serve as regularization terms.
Postulate 1: What is the meaning of the superscript "u" in L^u. "unnormalized"?
Postulate 2: It is stated that the domain of N=Theta'Theta can be "reasonably approximated". This seems too vague.
l194: It is stated that Theta has generally full row rank. If this is a necessary assumption, it needs to be made explicit.
Equation (7): I believe that the Phi in the leftmost term should be a Phi' (transposed). Please check.
Section 3.2: I found it very difficult to follow the reasoning in this section. In my opinion it would be useful to show more clearly why the proposed penalty function corresponds to a step-transfer-function
l396: The statement "It is well known that Parity is easiliy computable" should be backed by a reference.  The idea to use normalized kernels to cast deep learning models as optimization problems with nice convexity properties is nontrivial and innovative. The authors have made an effort to present the material well, but I feel that the contributions of the paper are somewhat overstated as the new approach involves several approximation steps. Also, the claim that block coordinate descent necessarily results in a stationary point seems to be false in general.