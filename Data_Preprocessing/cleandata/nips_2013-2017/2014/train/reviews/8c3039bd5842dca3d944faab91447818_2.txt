The paper proposes a Deep Gaussian Mixture model (Deep GMM), which generalizes Gaussian mixtures to multiple layers. The key idea is to stack multiple GMM layers on top of each other. One can view Deep GMMs as a generative model where a standard normal random variable is successively transformed through a path in a network of k layers, where a transformation (multiplication by a matrix and adding a bias) is performed at each layer of the network.
One can then construct an equivalent shallow GMM but with the exponential number of mixture components.
In general, this is quite an interesting idea and the authors provide various heuristics to speed up EM learning algorithm, including (1) using hard EM and (2) using a "folding" trick by folding all the layers above a current layer into a flat "shallow" GMM model (although this becomes expensive when considering bottom layers).
However, my main concern is that this work is very closely related to the following work on a deep mixture of factor analyzers:
Deep Mixtures of Factor Analysers (ICML 2012)
Yichuan Tang, Ruslan Salakhutdinov and Geoffrey Hinton
especially given the close connections between GMMs and mixture of factor analyzers. Similar to your construction, deep MFA can be "folded" into a shallow MFA and learning can be carried out using EM. One can also pretrain these models layer-by-layer.
I think it would be important to highlight similarities/differences between
your work and deep MFA work.
 In general, this is a well-written paper. But given its similarity to the previously publish work, the authors need to clarify their novel contributions.