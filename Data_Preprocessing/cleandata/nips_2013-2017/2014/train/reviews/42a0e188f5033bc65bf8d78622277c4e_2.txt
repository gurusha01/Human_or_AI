This paper proposes a parallel direction method of multipliers (PDMM) to minimize block-separable convex functions subject to linear constraints. In contrast with ADMM, which update primal variables using a Gauss-Seidel manner but limited to two blocks, this work proposes to update the primal blocks in the Jacobian manner with multiple blocks. The main result of the paper is the introduction of the dual backward step, which compensate the limited information propagation in the Jacobian updates by effectively reducing the step size for the dual updates. Similar to ADMM, O(1/T) type convergence results are established for PDMM in this paper.
It is shown (in supplementary materials) that two previous methods, sADMM and PJADMM are special cases of the proposed method, which give better understanding the relationship between different methods in the landscape of decomposition methods based on augmented Lagrangian. Experiments results demonstrated the effectiveness of the PDMM method as compared with ADMM and other variants. The results are a little counter intuitive for me, since in general Jacobian type methods would give slow convergence compared with Gauss-Seidel type of methods. It would be good to elaborate on this point.  The dual backward step in PDMM to ensure convergence seem to be new and key to obtain convergence under general conditions as ADMM. The proposed method gives more alternatives and possibilities for distributed optimization.