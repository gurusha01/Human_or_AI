This paper presents a multi-class extension to the recently proposed ensemble learning algorithm called DeepBoost. It provides new data-dependent bounds for convex ensembles in multi-class classification setup.
The paper reads well and technical derivations are correct Further it provides data-dependent bounds which are tighter than existing bounds due to the explicit dependency on the mixture weights in the convex combination. The special case of the bound derived in theorem 1, also leads to an important result as leads to a linear dependency on the number of classes. It improves the existing bounds from Koltchinskii and Panchenko, wherein this dependency is quadratic. In this sense, the bound is more useful, particularly for large number of classes. The authors also design optimization objectives and multi-class DeepBoosting algorithms which give good performance on UCI datasets as compared to the multi-class version of Adaboost and logistic regression.
However some parts of the paper need to be expanded and clarified especially the relation to the existing paper Deep Boosting in ICML, 2014 which makes the proposed approach somehow incremental as well as the experimental setup. Indeed, the proof techniques, design of the objective function, and developed algorithms are very similar in flavor to this existing work. This limits the novelty of this current work to certain extent. Another point which was not clear was that authors say in Line 143 (in section 3.1) that generalization error of $f$ and $f/\rho$ is same. In my opinion, $\rho$ has the interpretation of margin from definitions developed in equation (2), and hence it is not immediately clear why these two will admit the same generalization error. Moreover, it seems that the authors are using the labels on the test set to find the hyper-parameters of the algorithm. In the ICML paper on binary classification, the experimental setting does look different, in that paper, there is a separate validation set on which the parameter values are chosen, and not directly on the test set. The authors should clarify on this point as well.
 +The paper provides data-dependent bounds which are tighter than existing bounds due to the explicit dependency on the mixture weights in the convex combination.+ The special case of the bound derived in theorem 1, also leads to an important result as leads to a linear dependency on the number of classes.+ The authors also design optimization objectives and multi-class DeepBoosting algorithms which give good performance on UCI datasets- This work is somewhat incremental on the existing paper Deep Boosting in ICML, 2014- Some points need to be clarified (experimental setup, clear novelty with respect to Deep Boosting paper in ICML, 2014)