Summary: 
The authors present a scheme for semantic labeling and object detection. The former is accomplished by generating candidate segments and extracting features for these; and then regressing on the Intersection-over-Union overlap ratio between a candidate segment and pixel-accurate ground truth. In the case of object detection, only weaker labels in terms of bounding boxes are available and the precise spatial extent is used as latent variable. Regression is performed using a weighted SVM and a weighted transductive SVM, respectively. Results are on par with the state of the art in semantic labeling, and the best currently known for object detection on VOC 2007.
Originality:
Algorithmic novelty is limited, and the authors unfortunately do not cite pertinent previous work. In particular, the weighted SVM is well-known (e.g. [R1,R2,R4] below, not cited). The authors put a sum constraint on the weights, which may be novel (I did not conduct an extensive literature search here). The "latent e-SVM" that the authors propose is a weighted version of a transductive SVM (e.g. [R3], not cited). 
[R1] Suykens et al. Weighted least squares support vector machines: robustness and sparse approximation, Neurocomputing, 2002
[R2] X.Yang et al. Weighted Support Vector Machine for Data Classification, IJCNN, 2005
[R3] V. Vapnik, "Statistical Learning Theory", 1998
[R4] M.Lapin et al, Learning Using Privileged Information: SVM+ and Weighted SVM, Neural Networks, 2014
Significance:
Outperforming the recent CNN results by a "conventional" learning approach is a feat and of great interest to the community. 
Quality:
Crucial references to previous work are missing (see above). The object detection framework involves quite a bit of engineering, but not outside the norm. The default method for regression on the unit interval is (regularized) logistic regression, so these results should be reported as baseline to support the use of a sum-constrained weighted SVM. Authors should give more details on how they picked regularization coefficients \lambdaW and \lambdaR. The main strength of the paper are the good empirical results. 
Clarity:
The title and abstract do not summarize the core contribution of the paper well. The abstract also insinuates that regression targets from the unit interval are somehow less informative than mere binary class memberships. This is not true, because the latter can always be obtained from the former by thresholding. For a NIPS audience, the paper spends rather too much space (up to and including page 4) on small variations of well-known algorithms, and passes somewhat quickly over the all-important feature engineering details (lines 254ff). 
Minor comments 
Line 19: continues => continuous
Line 94: boxes => box
Line 98: AP: Abbreviation not introduced
Line 350: SVM => SVC
Line 350: with every different values => for all threshold values
-----
I have upped my quality score by one, assuming the authors will not be defensive about the relation to previous work, but rather comment extensively on it and connections to it.  Great results, deficient references to earlier work, little algorithmic novelty. A paper that should certainly be published, possibly after some rewriting, and possibly rather at a computer vision conference.