The goal of this study is to learn generative models of sensory signals in the attended region. The authors proposed a model that could successfully learn a generative model of frontal faces from large-size images that include faces among distractors.
The manuscript is well written overall. A wide range of researches are surveyed and combined, from neuroscience to computer vision to machine learning. I think the manuscript would be informative and useful to a broad audience.
The model itself is formulated in a generic way so that it could potentially be applied to a wide range of data. I think, however, the model is based on a couple of key assumptions that significantly limit its applicability. Namely, the model assumes "canonical image" (variable "v"), and also assumes similarity transformation. Therefore, the proposed approach could build a data generative model of "attended image regions", but not the underlying, three-dimensional "objects" whose observations by sensors involve more than just similarity transformation. I think this is the reason why the section 6: experiments only examined frontal faces, not those from different views, nor other objects. This manuscript can be improved if such limitations towards the grand goal are clearly discussed. An interesting approach to a challenging problem of "generative model of attended image regions", by combining several strands of prior researches from different areas. Thought-provoking paper, but I think the proposed model is still fairly limited.