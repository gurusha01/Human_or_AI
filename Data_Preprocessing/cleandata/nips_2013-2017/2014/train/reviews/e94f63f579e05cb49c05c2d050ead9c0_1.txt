Overview: this paper presents a fast alternative to MC methods for approximating intractable integrals. 
The main idea behind Bayesian quadrature is to exploit assumptions and regularities in the likelihood surface, something which pure Monte Carlo ignores. 
The authors in this paper model the square root of the integrand (the likelihood/prior) as a Gaussian Process. Samples are then drawn according to some criterion - in this case, samples are chosen to the location of the maximal expected posterior variance of the integrand. Intuitively, this is a location where the model knows the least about the value of the integrand, and stands to gain a lot of information. 
Importantly, they stress the computational benefits of their BQ active sampling method over standard Monte Carlo techniques. 
The authors then approximate integrals for a variety of problems, including marginal likelihood calculation for GP regression and GP classification marginal likelihoods. 
Quality - 
This paper is technically sound: the problem is well motivated, the method is well described and their approach does a good job when compared to other methods for numerical integration. 
Clarity- 
The paper is very well written and organized. The authors do a good job conveying all aspects of the analysis. They describe Bayesian Quadrature (and numerical integration in the first place), as well as existing approaches similar to theirs in a clear way. They differentiate their own method by very clearly laying out their contributions. They do a great job explaining their approach, and the process of going from problem to solution. 
Originality- 
They present their method as a way to improve (both speed and some accuracy) existing methods for Bayesian quadrature. They stress two contributions: the square root GP and 'fast active sampling'. 
The square root GP seems to be another way to model a positive function (the likelihood), and one that is typically less explored. The authors also do a great job describing two ways to cope with the intractability of inference given a non-linear transformation of a GP (linearization and moment matching). 
Significance-
The authors describe an alternative tool to compute a marginal likelihood - an extremely difficult and important task. The utility of such a tool is based on its speed, accuracy, and simplicity of implementation. This paper lays out an alternative solution to this common problem - one that is competitive in speed, accuracy, and simplicity. However, it remains unclear how significant this particular paper will be (or how much followup research it will inspire). What are some future directions of research made possible by this contribution? 
Questions and comments: 
- Figure 1: In this example, are the hyperparameters of the GP learned? If the covariance is something like a squared exponential, How does the length-scale cope with the crazy section?
- Line 208: how does the log transform compare to the square root transform. It seems somewhat clear that an unconstrained GP will more poorly, how well does another type of constrained GP perform? 
- Line 264: Why is the variance of the log transform worse (worse enough to make the whole scheme worse)? 
- Line 303: Is the task here just to integrate over the mixture of gaussians? 
- Fig 8+9: Maybe put the converged values in a table? It's hard to compare L, M and AIS here. 
 This is a good, technically sound paper describing a new method to perform Bayesian Quadrature.