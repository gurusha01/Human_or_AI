The paper proposes a modification to the SVM learning algorithm that deals with weakly supervised data. Specifically, a per-example weight (between 0 and 1) is assigned to each training example. Then the loss function of the SVM is modified to take into account this weight. The method is then extended to the case when the per-example weight is not observed (i.e., is a latent variable).
The method is well described and the experimental results indicate that, for the semantic segmentation and object detection tasks, there may be value is using such an approach over treating each example with equal weight. However, I fail to see the difference between the proposed method and existing "example dependent costs", which are widely known and used in the machine learning community. SVMlight, for example, provides such a mechanism.
The extension to the case when the per-example weight is not observed is interesting but straightforward. In this case the problem is non-convex and no theoretical analysis is given. Indeed a strong regularization term is used suggesting that the per-example weights do not deviate far from their initial values. Can the authors comment?
Minor comment: The claim on L050 that "it is also not a standard regression problem since the positiveness belongs to a bounded interval [0, 1]". Logistic regression is very much standard and is bounded to [0, 1]. 
Minor spelling mistake in the abstract: "continues" -> "continuous"
 The paper proposes a modified SVM learning algorithm in which the loss function is modified by a per-example weight. However, example dependent costs are already widely used in machine learning.