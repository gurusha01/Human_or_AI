The authors tackle here the difficult for proposing convex relaxations for learning deep architectures. The main difficulty in learning deep architectures lies in the fact that the model is built by iteratively performing composition of layers. For each layer, the key quantities are the weight matrix (parameterizing the inputs) and the output matrix (coding the output labels). An output matrix of one layer enters the input quantities of the next layer, which is the crux of the difficulty of posing the learning problem for learning deep architectures.
The authors propose here a new convex relaxation approach based on a previous relaxation that was made popular for maximum margin clustering. For a given single-layer learning problem such as maximum margin clustering, the convex relaxation used in maximum margin clustering consists in lifting the problem in a higher-dimensional space by parametrizing it using the equivalence matrix (Theta Theta') instead of the output matrix (Theta). In this paper, the authors propose instead to base the convex relaxation on the normalized quantity M = Theta' (Theta Theta')Theta, where denotes the pseudo-inverse and ' the transpose. This new convex relaxation has significant interesting outcomes. In particular, with appropriate modelling, the proposed approach based on this relaxation leads to constraints on M that are all spectral, whereas the un-normalized convex relaxation would lead to both spectral constraints and other non-spectral constraints. Owing to this elegant formulation, the approach can be efficiently implemented with a conditional gradient algorithm. Preliminary experimental results are presented on synthetic and real-world datasets. 
Detailed comments
The argument (lines 349-352) about the convergence to a stationary point is rather terse in the current version of the paper and should be significantly clarified in the final version of the paper. The exposition could be improved, by making extensive use of the supplemental material for the proofs and mathematical arguments and using the main part of the paper for clear explanations and illustrations.
I would like to praise the (scholar) effort of the authors to carefully define what the problem is, for designing a useful convex relaxation, by reviewing many possible options and highlighting their corresponding challenges. Putting aside the main contributions of the paper, these sections of the paper already represent a very valuable review material in order to tackle the current challenges of building theoretically-grounded deep learning approaches. I seldom read papers with such a significant and unusual effort. I'm actually rather impressed by such an effort for a conference paper. The paper is fresh and innovative. Designing useful convex relaxations of deep learning problems is probably one of the most daunting challenge in the ML community right now. This paper proposes a novel and interesting approach. A clear accept.