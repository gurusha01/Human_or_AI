This work studies the problem of active learning in linear regression.
Unlike classification, the objective in active regression is merely 
to improve the constant factors in the distribution-dependent rates 
of convergence compared to passive learning, since it is known that 
the asymptotic dependence on the number of labels typically cannot be 
improved compared to passive learning, and nor can the worst-case
values of the constant factor. The paper argues that there is a 
distribution-dependent constant factor in the rate of convergence 
of passive learning, which can sometimes be improved for active 
learning.
Specifically, they propose a rejection sampling scheme, which alters
the sampling distribution to a more-favorable one, without changing
the optimal solution. However, the rejection sampler requires a sort 
of scaling function \phi as a parameter, and obtaining good performance 
guarantees requires one to set this function carefully, and with some 
dependence on the joint distribution of (X,Y). Since this latter distribution 
is unknown, the algorithm attempts to optimize the choice of \phi among
piecewise constant functions, using an estimated linear function from an 
initial sample. They prove a risk bound for this method, which can be 
made to approach the ``oracle'' rate (where the optimal \phi is given),
and which is provably sometimes superior to the capabilities of passive 
learning methods.
Overall, this seems to be a solid contribution, which I suspect will have 
a wide audience.
My one main reservation is that I would have liked to see more discussion
of the dependence on K vs \rho*_A in Theorem 5.1. There are some terms
that are increasing in K, while we would like \rho*_A to decrease toward
\rho*, which presumably requires K to grow. Thus, the trade-off between
these two quantities can affect the rates. Some examples to illustrate 
how we should expect this trade-off to behave, for some reasonable 
distributions and sensible partition, would be helpful.
minor comments:
 
One citation that is missing here is Efromovich (2005): Sequential Design
and Estimation in Heteroscedastic Nonparametric Regression.
That work studies active regression as well (though for a nonparametric class),
and also finds improvements in constant factors based on the degree of 
heteroscedasticity.
I also want to echo a remark of one of the other reviewers, that the notation for 
P_\phi seems a little strange to me. I believe the intention is that, denoting 
by Q_\phi the measure having density \phi with respect to D, we define 
P_\phi as the distribution of (X/\sqrt{\phi(X)},Y/\sqrt{\phi(X)}), for 
(X,Y) \sim Q\phi. This P\phi should then be well-defined, and 
L(w,D)=L(w,P_\phi) would then follow via the law of the unconscious statistician.
Was this the intended meaning of P_\phi?
 A solid paper on improving the distribution-dependent constant factors in linear regression via active learning.