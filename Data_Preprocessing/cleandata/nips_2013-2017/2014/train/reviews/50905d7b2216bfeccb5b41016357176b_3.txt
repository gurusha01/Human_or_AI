Summary:
The paper is focused on the application of Difference of Convex Functions algorithms to minimize the norm of the Optimal Bellman Residual. In order to support such an application, the author(s) establish that the norm of the OBR can be represented as a difference of convex functions, and that this loss function has desirable theoretical benefits to the error bounds. 
The decomposition that proves the OBR can be written as a difference of convex functions is straightforward enough to follow without further proof and establishes an interesting connection. I have not seen this shown before, and so despite seeming obvious in hindsight provides a novel contribution. 
A small empirical study comparing the use of DCA for minimizing the norm of the OBR with other reinforcement learning algorithms (LSPI and Fitted-Q). These results are over a collection of randomly generated finite-state MDPs and show comparable results, but with lower variance. 
Discussion:
The paper presents an interesting application of algorithms for non-convex optimization to the reinforcement learning problem. In particular, the authors do a good job of motivating the use of the norm of the OBR and for its decomposition into a difference of convex functions. This is an interesting new direction to consider and these initial theoretical and empirical results suggest it is worth further exploration. 
Although I was left with many questions about this approach, especially about the details of the empirical study, the paper as a whole is a good contribution to the field. 
Recommendations:
The empirical study is very limited. At the least I would like to see another application domain and a thorough discussion of the methodology behind the study. Specifically, the algorithms for DCA, LSPI, and Fitted-Q all have tunable parameters and the author(s) do not provide any information about the values or how they were chosen. There are a number of typos in the math and writing. Although these do not greatly detract from the paper, the authors should be sure to fix them. 
 Gives a novel approach to value-function based reinforcement learning. Could be improved, especially in the empirical study, but as it is this work provides an interesting contribution.