This paper combines a coarse-to-fine cascade with pruning classifiers to speed up the optimization of the MRF. 
Although the greedy method that is used to prune the solution space seems interesting, the paper does not have sufficient theoretical and empirical evidence to support the argument that the heuristic works. Also, the type of MRFs which may benefit from the method is limited to the ones whose solutions are piecewise smooth. 
Some part of the paper is not clear enough. For example, in the description of the pruning matrix, it is not clear enough what "active" means. The pruning matrix updating of the algorithm is also not very clear. I would suggest the author provide a toy example to illustrate the algorithm better. On the other hand, the proposed approach part of sec 1 is too long and some contents may be redundant. Figure 2 is also not very informative. 
The experiment is interesting but also not clear enough. First of all, there are three algorithms which are compared, but there is only one curve shown in fig 1 (Since there are three algorithms in comparison, only showing the ratio is not sufficient). Secondly, since the energy ratio is between the current energy and the lowest computed energy by any strategy, why is it less than 1 in many cases? 
 After author feedback: 
The authors clarify some of the confusions in the original paper. I expect the authors to correct all the typos and address all the concerns by the reviewers. I move my rating from 4 to 5, because there is still no theoretical justification for the pruning method. 
A few more comments about the experiments: 
- In column(d) of fig 1, both two curves in the 3rd row have less than 96% agreement, while in the paper it says "the agreement is never worse than 96%". 
- Consider moving fig 2 to appendix. Add more experiments or analysis on hyperparameter choosing.  Interesting heuristics. Lack of clarity. Weak in theoretical and empirical support.