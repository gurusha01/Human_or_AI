The paper describes a method for estimating sparse connectivity graphs of firing neurons. An L_2 norm is used to obtain an penalised inverse covariance matrix as it improves the cost function. Furthermore, a previously published hard thresholding is replaced by a soft one. A cost function is formulated as the X-entropy and optimised using BFGS. 
The paper is self-contained and the references are extensive. The text is very easy to read albeit the structure of the paper at times emerges from the text rather than being explicitly presented to the reader. The result is that the paper reads as if it is chronological and incremental rather than theoretically and scientifically motivated. 
The paper compares favourably to other published methods, namely those who took part in the Kaggle Connectnomics competition. However, the heavily repeated reference to the competition makes the paper read like a late submission which, with the benefit of hindsight, is the best. This style also adds to the incremental feel of the publication. 
Comments following author rebuttal:
I have decided to increase my score. What convinced me the the case of improved computational speed ("parametrized in a differentiable way with a very simple, easy-to-implement formula") and a better justification for the choices made, e.g. filter order and chi2 value.  The paper is well written and theoretically complete. Nevertheless, it reads like an belated contribution to the Kaggle competition showing an (albeit not insignificant) improvement using mainly logistic regression for preprocessing and an X-entropy formulation of the cost that allows the use efficient off-the-shelf solvers.