This paper is an incremental work on kernel approximation using Random Features. The proposed algorithm (Sparse Random Features) aims to introduce l1-regularization in the Random Feature algorithm so that the model doesn't grow linearly with the number of features. The authors also show that the proposed algorithm can be seen as a Randomized Coordinate Descent in Hilbert Space.
Overall the paper is easy to read and clear. The work appears to be significant and will allow for solving practical large scale problems more efficiently than current kernel methods. 
However, I will suggest to make some efforts for a better presentation (analysis) of the experiments and results. There is a noticeable drop in accuracy of the proposed algorithm with Laplacian and Perceptron using the Covtype data, which could be further discussed. In addition to that, it is not clear whether the tables come from cross validation (preferable) or a simple split of the data.
Although the narrative is easy to follow and understand, I have the impression that some sentences need proofreading. This is an interesting piece of work with direct impact on machine learning applications. The paper is well written, however, the authors require to improve the experiments section, and perhaps, adding conclusions and future work directions.