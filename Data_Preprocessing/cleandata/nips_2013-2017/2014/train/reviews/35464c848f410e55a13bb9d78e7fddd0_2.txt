The authors present an interesting new method for supervised and semi-supervised learning using Fredholm kernels. Many aspects of the method are discussed in detail. Overall, I find the paper very interesting.
Nevertheless, I still have a few suggestions and questions:
- section 2 (from eq 1 to eq on line 99):
Also in the Nystrom method one considers an approximation to an integral equation
(in that case kernel PCA). I think it would be good if the authors could mention this and also explain in what sense the approach is similar or different.
- section 2 eq 2:
In the area of support vector machines and kernel methods also approaches with operator equations have been presented in literature. It would be good to mention this work and explain in which sense eq 2 is a special case of it (or explain how it differs).
- section 2 line 128 representer theorem:
Eq 2 is less common in learning theory. The authors assume that a representer theorem exists. They should prove a representer theorem here or otherwise give a precise reference and explain how eq 2 is a special case of it.
- section 4 eq 8:
The interpretation as soft-thresholding PCA is nice and interesting.
How does it differ from other methods like Fisher kernels and probability product kernels?
- typos:
line 165 Not that
line 221 achived
line 341 chossing
line 672 Table ??
 New approach to supervised and unsupervised learning using Fredholm kernels, which are data-dependent.