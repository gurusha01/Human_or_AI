This paper proposes a randomized parallel version of ADMM, which can handle palatalization with multiple blocks. In each iteration, PADMM picks K random blocks, updates the block primal vector, and then updates the dual vector via a backward step. The authors illustrate the importance of using a backward step, as it makes the dual update more conservative, enabling global convergence. The algorithm also allows adding a proximal term to the primal update, making a part of optimization problems easier to solve. The authors give theoretical analysis on the algorithm, establishing its global convergence and its iteration complexity. When there are totally J blocks, and when the PADMM algorithm randomly picks K blocks at one time, the convergence rate of the algorithm is O(J/(TK)) after T iterations of update.
The main advantage of the PADMM algorithm is (1) it allows full parallelization for the primal step and (2) it allows relatively large update stepsize comparing to other methods. In particular, PADMM can be faster than sADMM as it allows greater stepsizes. It is more flexible than PJADMM, and more parallelizable than GSADMM. The authors have evaluated the algorithm on robust principal component analysis and
overlapping group lasso. The experimental results are quite promising. The PADMM algorithm achieves the desired accuracy with less computation time. Nevertheless, the authors haven't reported the performance of PADMM with parallel implementation. Comparing to traditional ADMM, it is less convenient to tune the three parameters of PADMM.
Overall, this is an interesting contribution to solving the ADMM-type problems, where multiple constraints are provided. The algorithm has the same convergence rate as ADMM, but it allows parallelization and it exhibits promising practical performance.  Overall, this is an interesting contribution to solving the ADMM-type problems. The paper is well written. Both the theoretical and empirical parts are solid.