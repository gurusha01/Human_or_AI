This paper consider the problem of estimating the mean of a d-dimensional normal distribution with unit covariance, given nm samples available at m machines. The authors study the trade-off between communication complexity and dimensionality.
Several results are derived:
- Communication complexity cannot be reduced by processing jointly the various dimensions.
- It can be reduced for sparse means.
- Improved upper bound in the scalar case.
- Improved lower bound under simultaneous protocols.
I have a few minor suggestions:
1) At the beginning \theta is not a random variable, but it becomes a random variable after Definition 1. This is a standard device in minimax theory, but it is worth emphasizing the passage. Also, before definition 1, the authors introduce the conditional mutual information given theta. Strictly speaking makes sense only for theta a random variable (although taking a constant value with probability 1).
2) In Corollary 3.1. The minimax rate for the stated problem is not d\sigma^2/(nm). Thing of the case in which \sigma^2 = 1000000, n = m = d = 1. Then the estimator \hat{\theta} = 0 can do better than that. The statement must be qualified.
3) In Protocol 2, last two lines. I believe that Yi in the argument of \hat{\theta}i should not have subscript.
 A very nice papers with several interesting ideas.