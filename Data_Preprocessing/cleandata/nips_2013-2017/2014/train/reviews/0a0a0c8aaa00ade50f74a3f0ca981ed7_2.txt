In this manuscript, the authors design a rate-based neuronal network which is trained to hold a number of stable activity patterns, i.e. memories. The goal is to overcome several shortcomings of previous attractor models, namely violation of Dale's law and saturation of activities in the memory states. For training, the authors use a gradient descent on a cost function which minimizes activity changes in the desired attractors and the Frobenius norm of the weight matrix, and maximizes the stability of attractors. The main results are a weight distribution close to experimental findings and a balance of excitation and inhibition in the attractor states.
The writing of the manuscript is good. I found the model and the training procedure easy to understand, as well as the presentation of the results. The combination of biological constraints going into the model and the resulting features after training make this paper an interesting contribution to the understanding of attractor neural networks. The authors train a rate-based neuronal network to hold graded and non-saturated attractors in the synaptic weights and inhibitory activities. Constraints imposed on the synapses and neurons for biological plausability lead to a balance of excitation and inhibition on the network level.