This paper provides a new insight on sparsity inducing regularizations. The authors imported the notion of group-majorization and showed that several well-known regularizations are recovered by properly introducing a group G and a seed vector v. The authors also provided gradient-based optimization methods and a regularization path heuristic.
Overall, the paper is well-written and the main idea is clear. Characterization of sparsity inducing regularizations is important since sparsity is fundamental in recent machine learning, and this work would help us for deeper understanding.
The use of the orbitope would be an unique point of this research. However, the discussion on its utility seems not sufficient. As stated in Corollary 4, the orbitope and the atomic norm are relevant, and the premutahedra and sorted l1-norms discussed in the paper are both atomic norms. The orbitope and the atomic norm may be different in general, but how does this difference brings us to a new insight? In particular, can we find practically useful new regularizations with a help of the orbitope, in which any existing studies could not? Further discussion on this point will be beneficial. I think the regularization path heuristic in Section 6 would be one advantage of the orbitope. This heuristic allows us to adaptively tune the regularization, which will not be available with the atomic norm. This research provides a new way to interpret sparsity inducing regularizations using the orbitope. The content is interesting, although the advantage of the use of the orbitope is yet unclear.