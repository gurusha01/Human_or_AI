Summary:
The paper presents an influence reweighted sampling method (IWS-LS) (as well as a residual weighted sampling method, i.e., RWS-LS) for learning large-scale least squares, which is robust with respect to some data corruption, e.g., the particular sub-Gaussian additive noise. Existing approximation methods are adopted to compute the OLS estimate and the leverage scores. Estimation error is analyzed theoretically for IWS-LS. Finally, empirical results are reported on both synthetic and real-world data sets.
Comments:
Overall, the paper is well written. The influence reweighted subsampling method is new. The theoretical and empirical results appear to be sound. 
For the experiments, the dataset with 100,000 samples in a p=500 space is not huge (the real-world Airline delay dataset is even smaller). The computation of the OLS estimate as well as the leverage scores should be feasible (with O(n p^2) complexity), even though it may take minutes or hours, on a standard desktop; and thus the results of exact methods should be included as a baseline. It is also helpful to include the experiment environment. Furthermore, as time efficiency is one very important aspect of large-scale learning, running time should be included. 
For data nosing/corruption, this paper focuses on developing estimators that are robust to the resulting outliers. It would be useful if the authors can discuss on the work that leverages data nosing to actually improve classification/regression. Some recent work includes dropout training for deep neural networks [1,2], learning with marginalized corrupted features [3,4], and etc.
References:
[1] G. Hinton et al., Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580v1, preprint. 
[2] S. Wager, et al., Dropout training as adaptive regularization. NIPS, 2014.
[3] L. van der Maaten, et al., Learning with marginalized corrupted features. ICML, 2013.
[4] N. Chen, et al. Dropout Training for Support Vector Machines, AAAI, 2014.
Finally, some typos should be corrected. For example, line 42 "more more realistic"; line 254 "ideas from $4 and $4". 
 The paper is well written. The influence reweighted subsampling method is new. The theoretical and empirical results appear to be sound.