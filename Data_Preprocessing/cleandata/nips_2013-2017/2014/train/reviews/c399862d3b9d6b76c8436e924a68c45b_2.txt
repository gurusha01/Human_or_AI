The authors consider the online convex optimization with bandit feedback. Until now we know rates of convergence in T^{2/3} when the loss functions are convex and either smooth or strongly convex . The authors make both assumption and prove convergence in T^{1/2}.
The paper is incremental. The algorithm is a mere adaptation of existing ones (Abernethy, Hazan & Rakhlin or Saha & Tewari) and the techniques of proof are almost exactly the same (basically, one inequality is improved using the strong convexity assumption).
The true interesting question would be whether we could remove any assumption (apart from Lipschitz) and still get T^{1/2}  This is another paper on online convex optimization with several assumptions on the loss functions. I find it is rather incremental.