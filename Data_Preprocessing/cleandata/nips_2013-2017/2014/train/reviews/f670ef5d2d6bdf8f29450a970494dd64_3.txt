The paper investigates a group-theoretic view of penalized likelihood functions in vector spaces. It is shown that many commonly used regularization terms, such as the L1-, L2-, and L_infty-norm, are specific instantiations of what is here called "orbit regularization". The generic view is shown to suggest also new reasonable forms of regularization, which admit nice properties when it comes to optimization using conditional and projected gradient algorithms.
Quality:
The work is of very good quality. The relevant literature, both old and recent, is well cited. 
Clarity:
The presentation is excellent. The notation is carefully chosen and adheres the usual conventions. The definitions are clear. The paper is pleasant and relatively easy to read, even if the subject is quite abstract. 
The paper does not adhere to the NIPS section headings and referencing styles.
Line 035: "klowledge". In Def 5, Prop 6, etc., consider placing the period (in bold) outside the parentheses, or just remove it.
Originality:
It seems that the proposed view is new to the machine learning community. However, much of the underlying mathematics does not look that new: for example, Propositions 11 and 12 are attributed to earlier works as old as Eaton (1984) and Hardy et al. (1952). The paper could better crystallize the its contributions. 
Significance:
The significance of the work is unclear. In particular, it is not clear how the work advances the state of the art when it comes to the practice of machine learning. In general, it is nice to have several computationally efficient forms of regularization available. On the other hard, there is no objective way to select "the best" among them for a given learning problem. So, just generating new forms will not lead to very significant advancements in the field. 
 Well presented, carefully typed, view that may contribute to better understanding of likelihood regularization in "linearly flavored" machine learning problems.