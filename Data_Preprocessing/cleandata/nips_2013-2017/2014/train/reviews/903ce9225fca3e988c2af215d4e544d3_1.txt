The paper aims at addressing an important problem in the literature of multi-armed bandit, namely whether it is possible to prove sub-linear regret bounds in the case of non-stationary mean rewards with finite variation? It provides a definitive answer for these questions by proving matching lower and upper bound of order O(T^(2/3) ) under the assumption that the total variation of mean-rewards are bounded by some VT\geq 0 for T rounds. The upper bound is proven for a phased version of EXP3 algorithm which resets the algorithm after every O((T/Vt)^(2/3)) rounds. 
 General:
The paper is very well-written and easy to follow. I checked the main results and technical proofs (those presented in the main paper) and could not find any error. Also the key passages of the technical proof have been well explained. Overall I believe the paper includes enough contribution for a conference submission. 
Few technical comments:
 
1- A closely related setting to the non-stationary reward scenario is the state-dependent ergodic bandits (e.g., restless bandits) as it is recognized in the paper. Although some of the earlier works in ergodic bandits have been discussed, the paper does not completely cover the-state-of-the-art of this field and misses some of the new works (e.g. see resteless bandit of (Ortner 12) for regret bounds for restless bandits with unknown dynamics and the bandit for correlated feedback of (Azar 14) for history-dependent reward bandits). 
2- Rexp3 requires to receive VT as an input. This may turn out to be a restrictive requirement in problems with unknown variation of rewards. I wonder whether one can relax this requirement and still achieve a same regret? I would like to see some discussion regarding those cases in which VT under-estimated.
3- The proposed algorithm utilizes a variant of EXP3, which is designed for adversarial bandit, to deal with a problem which is stochastic (albeit time varying) in nature. Although this seems to be sufficient for achieving the minimax rate in the worst-case scenario, it does not exploit the stochastic structure and therefore might not be the best problem dependent strategy. I wonder whether one can achieve problem dependent bounds, which can exploit the structure, e.g., gap between best arm and others by extending stochastic bandit algorithms to this setting? maybe, some extension of UCB which can 'forget'?
 Nice results. Technically strong and rigorous. The case of unknown variation bound needs to be discussed.