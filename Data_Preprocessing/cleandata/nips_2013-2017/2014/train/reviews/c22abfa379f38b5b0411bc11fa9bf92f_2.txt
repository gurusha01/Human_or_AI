The paper proposes a generative model for images that uses an attentional mechanism to focus on items (objects) of interest within an image, and devoting the object model just to these attended objects, as opposed to the entire image (as in most deep network approaches). Latent variable representations for the object and its pose (position, size and orientation) are inferred from the image using hamiltonian monte carlo. Performance is demonstrated on the Caltech and CMU-PIE face datasets, showing good performance of the model.
Overall I love this paper. It proposes something that is totally sensible and long overdue in both discriminative and generative models of images - i.e., employing an attentional mechanism. I feel that this is an important advance, and although many aspects of the work could probably be expanded and improved, I think it will generate strong interest at NIPS. 
One area where I think this work could be improved is in learning the transformation model rather than assuming affine translation, scale and rotation. For example 2D projections of 3D objects will generate a richer set of warps/transformations. Also how to deal with occlusion?
 Great paper - a very sensible approach with good results.