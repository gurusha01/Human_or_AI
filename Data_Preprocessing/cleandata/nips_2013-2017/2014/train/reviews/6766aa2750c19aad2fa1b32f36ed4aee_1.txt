Summary:
The paper presents a sample-efficient policy search algorithm for large, continuous reinforcement learning problems. In contrast to existing model-based policy search algorithms, the approach presented in this paper tries to learn local models in form of linear Gaussian controllers. Given the information (rollouts) from these linear local models, a global, nonlinear policy can then be learned using an arbitrary parametrization scheme. The so-called Guided Policy Search approach alternates between (local) trajectory optimization and (global) policy search in an iterative fashion. In their experiments, the authors show that the approach outperforms various state-of-the-art Policy Search methods, e.g., REPS, PILCO etc. Experiments where conducted in (mostly 2D) dynamics simulations involving the continuous control of multi-linked agents. 
Quality:
This is a well-written paper that addresses a topic which is relevant to the NIPS. Policy search algorithms have gained significant popularity in recent years and the work presented in this paper adds various important and interesting insights. Training a global model from locally linear models in a supervised fashion is an excellent idea! The results presented in the experiments section show that the approach can outperform well-known PS methods. However, it is at times difficult to judge the quality of the method since the proposed experiments are new and (to the best of my knowledge) have not been used by other researchers. Deisenroth et al. showed that PILCO can learn the "Cart-Pole Swing-up" task within a few trials. Can GPS do the same? I also wonder whether the local models can influence each other during dynamics fitting. For this not to happen, you need to ensure that the rollouts are only generated in the timeframe in which a particular local model is the expert. After reading the paper several times, it was still unclear to me how this is achieved. Especially when training Neural Networks, conflicts between several local datasets can have an extremely negative effect. That being said, I really like the fact that different types of policy representations can be used. While they have been out of fashion for controls, this paper clearly shows that (trained the right way) neural networks can we powerful tools for continuous problems.
Clarity:
All sections of the paper are clear and understandable. The authors did a very good job explaining the reasons behind the each choice made in the development of the algorithm. I particularly like that information about practical/empirical choices (e.g. parameter values, how to speed up learning) is shared with the reader. 
Originality:
The paper bears several similarities with well-known PS algorithms, in particular REPS. The use of KL divergence when learning policies was made popular by the REPS algorithm. Using supervised learning bears some resemblance to model-based RL. However, I think that these similarities are mostly superficial. At its core, the GPS approach differs significantly from exisiting PS approaches. In constract to model-based RL, the approach here learns the Policy in a supervised learning fashion rather than a model of the environment. Using local models has various beneficial effects, e.g. simpler training, stability of the system, convergence etc. 
Significance:
I think that the paper makes a significant and valuable contribution to the Policy Search and Reinforcement Learning community. While the approach builds upon similar, existing methods, it has various interesting new ideas. I recommend the publication of this paper.  This is a well-written paper that introduces a method for sample-efficient learning of policies in unknown environments. Turning policy search into a supervised learning problem is a very interesting new research direction and will hopefully lead to more insights into this problem.