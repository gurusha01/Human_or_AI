The authors formalize the notion of learning with pseudo-ensemble, under which several existing learning methods can be explained, e.g., dropout in deep neural networks. The authors further present the PEV regularization, which encourages the robustness of learned model (activations of hidden units in different layers in the case of deep layered neural networks) under perturbation of the model space. Since no label information is used in the PEV regularization, it naturally generalizes to semi-supervised learning setup. The authors compare the method to several baseline methods under different learning scenarios, supervised, semi-supervised and transfer learning and show improvement. 
The Pseudo-Ensemble notion generalizes previous work on robust learning w.r.t. perturbation in the input space (Burge & Scholkopf 1997, Chapelle et. al. 2000, Maaten et. al. 2013, Wager et. al. 2013). It is a straight-forwarding extension. The PEV regularization is interesting by itself, and is able to match the performance of dropout under supervised learning setup, and significantly outperform in the scenarios of semi-supervised and transfer learning. 
The authors try to connect the PEV regularization to dropout in section 4.1. The pseudo-ensemble objective introduced in eq.(1) explains dropout in the limiting case. However, it is less clear from the writings that learning with the PEV regularization actually approximates dropout. It is not very convincing to draw the conclusion that discouraging co-adaption is the reason of success for both PEV regularization and dropout simply by verifying the performance (accuracy numbers) of the two model is similar. 
Starting from eq. (1) , a more natural formulation for the regularization would be to penalize the variance of distribution of the output layer rather than summing over all different layers from 2 to d as shown in eq. (3). Could the authors explain why it is done in the current formulation and how much performance deterioration would be observed if we drop the penalties on the hidden layers. 
Small comment:
1. Figure 1 is not explained in the text. 
2. Based on Table 2, even with the PEV regularization, DAE pre-training (PEV+) was still able to significantly improve the performance of the learned model, which seems to suggest that the PEV regularization is less effective in terms of utilizing unlabeled data.  The work introduces an interesting regularization for learning with multi-layer neural networks, motivated by dropout and pseudo-ensemble learning. The regularization can also be applied to other classes of models. The authors try to connect directly the PEV regularization to dropout in section 4.1, but the connection is not clear from the writing. Though a straight-forward method, the PEV regularization offers satisfying performance. It could be of interest to a subset of the community to learn about it.