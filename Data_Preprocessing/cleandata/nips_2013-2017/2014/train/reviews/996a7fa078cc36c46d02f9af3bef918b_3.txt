The paper describes a new technique for handling "options" (abstract, high-level actions) in reinforcement learning. The paper argues that the new approach is more efficient, and has the advantage of producing models that are independent of the reward function. In addtion, it is shown that the approach can be extended to linear function approximation. Experiments are conducted in two domains.
These results appear to be a significant advance in the handling of options, which seem to be a natural and potentially powerful approach for dealing with complex environments. The paper is very clearly written. The research is solid and high quality.
I would suggest that the authors also provide a complexity analysis of their approach with regard to its time and space requirements. The paper would also benefit from further discussion of any disadvantages of the approach, or situations in which it might not be appropriate.
 Significant results, very clearly written.