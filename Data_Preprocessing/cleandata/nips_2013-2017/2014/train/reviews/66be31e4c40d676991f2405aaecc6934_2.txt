In essence, this paper considers the generalization of dropout to other classes of models by perturbing the parameters of the corresponding source model. For instance, in a the simple case of a mixture model, one could add appropriate noise to the means of the components or to their covariance matrices. The paper introduces a regularizer to ensure robustness of the pseudo-ensemble of child models with respect to the noise process used to perturb the parent model and generate the child models. The general topic of the paper is of significant interest to the NIPS community and in my opinion worth presenting, although overall it is a somewhat straightforward generalization of dropout. The ideas are supported by several informative simulations on standard benchmark datasets.
Some specific comments:
1)The first two sentences are too vague to make sense, especially in relation to the role of the variables x and y. For instance, is one trying to derive approximations of p(x,y) that are conditioned on x?
2) The statement "Many useful methods could be developed....by generating perturbations beyond the iid masking noise that has been considered for neural networks" is not correct. Even in the original paper by the Hinton group the noise was not iid since the dropout probability used in the input layer (data) was lower (e.g. 0.2) than in the other layers (e.g. 0.5). Furthermore, examples of non-iid cases were analyzed in reference [1] which should be augmented with, or replaced by, its more extended version (Artificial Intelligence, 210, 78â€“122, 2014). This reference considers also other forms of noise, such as adding Gaussian noise to the activity of the units, and shows how they the fall under the same framework. This is highly related to the theme of this paper.
3) This is a minor point but the author may want to consider changing the terminology. The term "pseudo-ensemble" is perhaps not ideal since "pseudo" has a slightly negative connotation, whereas the point to convey is that this is a new approach to learning in its own right.
4) The remark "While dropout is well supported empirically, its mode of action is not well understood outside the limited context of linear models" is not entirely correct. In the non-linear case, the ensemble properties of dropout in deep non-linear neural networks are reasonably well understood, and so are its regularization properties, as described in the reference given above.
5) This is another minor point but there are a few typos. The paper should be run through a spell-checker. See, for instance, the last line of page 6 ("northe images in the target domain").
6) The paper is well supported by several informative experiments on different benchmark datasets.
 In essence this paper presents an incremental generalization of dropout. This is currently a hot topic for the NIPS audience. It is supported by a set of interesting simulations.