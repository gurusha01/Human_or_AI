The authors adapt SVI of Hoffman, Blei, Wang and Paisley (2013) to hidden
markov models (HMM).
Previous applications of SVI consider minibatches of complete data items.
SVIHMM differs in that minibatches consist of minibatches of subsequences.
This is an interesting idea, where some external observations are used to
seed the messages on each side of the subsequence.
The authors propose a heuristic for determining how many observations need to be added
on each side of the subsequence. It is a bit surprising that this works since
one might imagine there might be long term dependence in the messages.
Doesn't GrowBuf terminate immediately if S^new = S^old?
Is there a relationship between the optimal tau and the second largest
eigenvalue of A?
The paper quality itself is low, unfortunately: there are missing figures and tables from
experiments (e.g., table 4 line 355 and the timing experiments).
The introduction and review is rather long: it is not until page 4 that we get
to the material of the paper. Consequently, too much is placed in the
supplement. GrowBuf would appear key to understanding the paper, but is only
presented in the supplement.
I find it hard to assess the FBR rate quoted: DBN has a FDR of 0.999038, whilst
this method yields 0.999026. This difference appears really rather slight,
but perhaps SVIHMM is faster than the DBN result? No results were presented.
Also, how much noise is there in the estimate of the FDR? A mostly straightforward adaptation of SVI to HMMs with a heuristic for training on subsequences. Presentation is incomplete.