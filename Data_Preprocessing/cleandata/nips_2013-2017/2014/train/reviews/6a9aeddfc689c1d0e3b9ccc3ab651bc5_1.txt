This paper looks at alternatives to LS for solving linear regression
problems. Classical statistics will consider a score function as a
way of weighting observations. But, these would only capture an
additive error which is different than Gaussian. This paper instead,
also considers the influence of a point. Statisticians for years have
thrown out influential outliers--but as	far as I can find, this	is the
first automatic	procedure that	uses influence as a way	of
down-weighting a	regression.
The probabilistic model	presented here is not the driving force	of the
paper. 	In other words,	it is pretty easy to identify which points are
contaminated and which are clean. (This is very easy for large p
since the two distributions won't overlap.) So	if the estimator
presented was tied to this model, I would be unexcited about the
results. But, the estimator is	fairly natural.	 So the	model is only
being used as an example.
1) Put a line where the accuracy of the MLE for your probabilistic
model would be. You shouldn't end up being close to this line since
you aren't assuming your model. But it still is a more useful
comparison point than the LS line. If you don't want to bother	with
doing the de-convolution problem, you can assume that the ML has
access to the random variables U. Then	it will	be a lower bound on
how well it will perform. As p	--> infinity, this lower bound will
become tight.
2) Make the connection to some of the literature on the use of scoring
functions (Since this goes back to the early 1970's I'm not going to
suggest any literature). For example, the aRWS can be thought of as
down weighting points by a 1/epsilon^2. If one were to down weight by
1/epsilon, that would correspond to doing a L-1 regression. So it
would be similar to assuming a double exponential distribution for the
errors. Your weighting function is even more draconian. So what
distribution does it correspond to if you think of it as a score?
(I'm guessing it looks like a Cauchy distribution--but only
approximately.) (Note: you are sampling, whereas I'm more used to
thinking of weights. So it might be that your 1/epsilon^2 sampling is
related to a 1/epsilon weight. If so, then it is approximating a L-1
regression. This would be very nice to point out if it were true.)
3) Are you claiming to be the first people to suggest weighting by
1/influence? This might be true--but it is a strong claim.
4) I think what you are doing is using a different regression function
for "large observations" than you use for "small observations." One
way to test this and provide a better comparison for LS would be to
define tilde-l as you do and then interact this variable with all your
X's in your regression. This will allow the LS methods to have access
to tilde-l. Hence LS should be able to end up with a better estimator
since it now can have different slopes for the pure X observations
than it has for the X+W observations.
5) There is extensive literature on both errors in variables and
robust regressions. At least a few of these 1000+ papers should have
something useful to say. Put in at least some effort into finding
something that connects. That will help the implied claim that your
methods are in fact new.
6) PLease look at this 1980's NBER paper:
 http://www.nber.org/chapters/c11698.pdf
 In particular, equations (15) and (16). This is very close to the estimator you are using. It would be nice if you were to chase down some of the modern references and see if there are any connections to your work.
 This paper provides a new alternative to robust regression. Namely, it down weights by the influence of a point. A useful theorem is provided and good alternatives are considered in the empirical section.