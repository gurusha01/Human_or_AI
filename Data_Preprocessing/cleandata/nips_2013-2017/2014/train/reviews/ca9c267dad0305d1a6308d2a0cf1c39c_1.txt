The paper presents a method for learning multiple tasks in parallel where at each round a sample is given per each task, but only a single task can have its sample annotated. The authors formulate their method using a trade-off between exploitation and exploration. Th1 provides an upper bound on the expected cumulative number of mistakes. The algorithm is compared to 2 different approaches for choosing the single sample/task to be annotated.
I like the paper. It is well written and provides good theoretical as well as experimental results. 
I'm not sure the strict synchronic assumption on choice of annotation is really important. It seems more natural just to assume a total budget on annotation among all tasks in a learning system. It would be nice if the authors could add more real life motivation for this specific setting.
I also feel the experimental results could be strengthen by adding two more comparisons:
a. The case were annotation is cheap and all samples can be annotated. Obviously the results could be better but this will show the trade-off between being cheap on annotation or regarding the annotation as cheap. 
b. An active learning approach applied to each task separately, while controlling the number of annotations among all tasks to be equal to the number of learning rounds. 
This will provide a comparison to a method which "pays" the same on annotation but considers a weaker constraint among all tasks. Only the total number of annotation would have to be the same but each round several tasks could be annotated (or non).
 I liked the paper, it presents an interesting new problem while providing nice theoretical and experimental guarantees for the proposed solution. I would like to see more motivation into the specific choice of parallel task learning together with experimental evidence to its justification (see suggestions above).