The paper presents a new model called DeepGMM, which is a MoG with an exponential number of components and tied parameters. Each component in the mixture correspond to a path through a fully interconnected multilayer network of affine transformations (where the matrix is the Cholesky of a positive-definite matrix)applied on a identity-covariance multivariate Gaussian. The weights of each component are not tied, although a factorisation is suggested in the paper for very deep networks.
As the authors comment, their hypothesis is that by tying the parameters of the many components a bigger number of components can be used while avoiding overfitting.
The authors also propose a hard-EM based training algorithm. For the expectation phase, coordinate descent and several heuristics are recommended to decrease the computational load. For the maximization phase the authors present three options: 
A batch GD method suitable for DeepGMMs with a small number of paths. Unfortunately, the authors give no specific figures.
A batch GD method suitable for DeepGMMs modelling data of sufficiently small dimensionality
A SGD method for bigger DeepGMMs
Experimental results on two well-known dataset are presented. All experiments use the second of the aforementioned optimisation techniques.
Quality:
Pros: The paper is technically sound and it main hypothesis supported by experimental results. Figure 4 shows by tying parameters it is possible to train a DeepMoG with 2500 effective components that offers superior performance than a untied MoG using 300 (which has more parameters), and using more components in an untied mixture offers no improvement.
Cons: The paper only show results on natural images. The inductive bias of a DeepGMM could be specially advantageous on this kind of data.
Clarity
The paper is well structured and reads well, but there are some typos (parrallizable -> parallelizable, netwerk -> network).
In Figure 4 the maximum value achieved is about 154.5 while in Table 1 it is reported as 156.2. Are these different experiments using more data instead of 500 thousand patches? If so it should be specified.
It would be of interest to report training times, does it take hours, days or weeks to train a model of 8x8 patches?
In Figure 4, if the goal is to show, the ability to train a MoG with many components without overfitting, it would be more interesting to show the effective number of components instead of the number of components in the top layer.
If the authors find space it could be interesting to show some samples from the model.
Was the tinyimages dataset resized to 8x8 pixels, if not the likelihods should not be compared to those obtained on BSDS300 as is done on line 397.
Originality:
The particular technique for parameter tying in a GMM presented in this paper is new. Also the training algorithms presented (including heuristics) are of interest.
Significance:
The results are important. Although training and evaluation of densities at test time will have high computational cost, sampling should be very efficient. Also the idea is interesting and can be further built upon.
 The paper presents a new way of tying the parameters of a MoG that allows the authors to obtain state-of-the-art results on patches of natural images. The paper is interesting and easy to read.