This paper considers an important problem in unsupervised machine learning and optimization, clustering with outlier detection. Clustering has a long history in both theoretical and practical areas. People worked on many types of clustering problems before, such as k-means, k-medians and k-centers in geometric background, and correlation clustering, spectral clustering in graph theory. However, one critical issue for clustering is outlier detection, which could influence the final result significantly. 
The authors present a gradient descent algorithm for clustering with outlier detection. They slightly modify the integer programing model from [8], through adding the outlier part to the constraint. Then they relax the model into a sequence of Lagrange relaxations, and solve it via a gradient descent strategy. 
The experiment considers both of synthetic and real data, and shows the advantages over other two methods. 
Positive points:
1. Using Lagrange relaxation is a new idea for outlier detection. 
2. The algorithm is clean, and easy to implement, which makes it practical. 
Negative points:
1. The theoretical analysis is not enough. For example, in section 4.2, the authors should provide more details for the convergence.
2. More references are needed. In computational statistics, there are many new techniques on trimming outlier for regression and clustering, such as David Mount et al, ``A practical approximation algorithm for the LMS line estimator" and ``On the least trimmed squares estimator".
 This paper provides a Lagrange relaxation approach for a hard problem in clustering area. The technique is new, but needs more theoretical analysis.