This paper is concerned with learning kernel matrices to be used by determinantal point processes (DPPs). The main contribution of the paper is a reformulation of the objective that outperforms the standard approach of differentiating the logprob with respect to kernel parameters and doing projected gradient ascent (this baseline approach is called K-Ascent (KA) in the paper). The idea is to reformulate the problem within an EM framework. This works by observing that any DPP can be represented as a mixture over "elementary DPPs", where there is one elementary DPP for each subset of elements. A variational distribution is instantiated over these subsets, and the algorithm alternates between updating the distribution over these subsets, which is represented as a k-DPP, and taking single gradient steps in the M step. Results show that the EM method outperforms the KA method by quite a bit when KA is initialized naively, and by a little bit in some settings when KA is initialized more intelligently. 
Overall, I find the approach interesting, and it seems like the EM method does yield some improvement. The M-step is fairly involved, and the caveats required to make it work are a bit unsatisfying, but there is something to be said for getting it to work.
Experimentally, I would have liked to have seen a bit more analysis on the effect of initialization on EM. For example, if the EM algorithm were initialized with the result of KA, would it improve upon the solution?
I'm also confused by the use of relative log likelihood as a measure to report. Log prob differences reflect ratios of probabilities. Why take ratios of log probabilities? Why not just differences in log probs? I have a hard time interpreting what these numbers mean.
Also a minor note (sorry for the crankiness!): I find some of the writing style to be annoying. I would prefer the authors don't call their own algorithm elegant, particularly when the elegance is debatable due to the caveat and approximation required in the M step. I also find it unnecessary to bold the text in the introduction. Interesting new approach for learning kernels in DPPs. Experiments could be improved a bit, and the improvements over the basic projected gradient algorithm aren't huge, but overall a pretty good paper.