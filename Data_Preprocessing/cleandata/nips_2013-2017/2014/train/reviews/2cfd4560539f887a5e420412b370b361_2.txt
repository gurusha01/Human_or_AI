This exciting paper introduces a new deep recursive neural architecture that obtains state of the art performance on a hard sentiment classificationd dataset.
The results from their new architecture are impressive and the exposition is very clear!
The changes they propose to previous RNN models are clever, well motivated and experimentally demonstrated to work well:
- untying leaf nodes and nonterminal nodes
- using rectified linear units in the recursive setting
- using large unsupervised word vectors but smaller hidden nonterminal nodes
- using a deep architecture which is not simply replacing a single neural network RNN layer with a deep layer but introducing connections between the layers and tree nodes. -- it would have been an interesting comparison if the outputs of the last hidden layer at each node was the only input to the next parent, i.e. to drop the connection from matrix V?
Table 1 (a) results should have been on the dev set, not the final test set. you're tuning ont he final test set here!
typos:
- comprise a class of architecture This exciting paper introduces a new deep recursive neural architecture that obtains state of the art performance on a hard sentiment classificationd dataset.The results from their new architecture are impressive and the exposition is very clear!