This paper proposes a two-stream ConvNet architecture for action recognition in videos, which incorporates spatial and temporal networks to capture complementary information on appearance and motion. The authors demonstrate that a ConvNet trained on multi-frame dense optical flow achieves good performance despite limited training data, and that multi-task learning can increase the amount of training data and improve performance on both datasets. The architecture is evaluated on the UCF-101 and HMDB-51 benchmarks, where it is competitive with the state of the art.
The strengths of the paper include:
* The proposed two-stream architecture is novel and effective, allowing for the capture of both spatial and temporal information in videos.
* The use of multi-frame dense optical flow as input to the temporal ConvNet is a good way to represent motion information, and the authors demonstrate that it outperforms other input representations.
* The authors provide a thorough evaluation of their architecture on two benchmark datasets, and demonstrate that it is competitive with the state of the art.
The weaknesses of the paper include:
* The authors do not provide a detailed analysis of the computational cost of their architecture, which could be a limitation for real-time applications.
* The authors do not explore the use of other types of input representations, such as 3D convolutional neural networks, which could potentially capture more information about the video.
* The authors do not provide a detailed comparison with other state-of-the-art methods, which could help to better understand the strengths and weaknesses of their architecture.
Arguments for acceptance:
* The paper proposes a novel and effective architecture for action recognition in videos.
* The authors provide a thorough evaluation of their architecture on two benchmark datasets.
* The paper is well-written and easy to follow, with clear explanations of the architecture and the experiments.
Arguments against acceptance:
* The paper does not provide a detailed analysis of the computational cost of the architecture.
* The authors do not explore the use of other types of input representations.
* The paper could benefit from a more detailed comparison with other state-of-the-art methods.
Overall, I would recommend accepting this paper, as it proposes a novel and effective architecture for action recognition in videos, and provides a thorough evaluation on two benchmark datasets. However, the authors could improve the paper by providing a more detailed analysis of the computational cost and exploring the use of other types of input representations. 
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10 
I would suggest the authors to revise the paper to address the weaknesses mentioned above, and to provide a more detailed comparison with other state-of-the-art methods. Additionally, the authors could consider exploring the use of other types of input representations, such as 3D convolutional neural networks, to capture more information about the video.