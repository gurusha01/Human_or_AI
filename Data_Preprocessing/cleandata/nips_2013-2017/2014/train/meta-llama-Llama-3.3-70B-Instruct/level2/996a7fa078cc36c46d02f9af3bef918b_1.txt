This paper introduces the concept of Universal Option Models (UOMs) for real-time abstract planning in reinforcement learning. The main claim of the paper is that UOMs can efficiently compute option returns and value functions of policies over options, even when the reward function changes. The authors support this claim through theoretical analysis and empirical results in two domains: a real-time strategy game and article recommendation.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and their proposed solution. The technical sections are thorough and well-organized, making it easy to follow the authors' reasoning. The empirical results demonstrate the effectiveness of UOMs in both domains, showing that they outperform existing methods in terms of computational efficiency and accuracy.
One of the strengths of the paper is its ability to unify the study of multiple reward functions within the setting of real-time abstract planning. The authors provide a comprehensive review of the relevant literature and clearly explain how their work builds upon existing research. The use of linear function approximation to scale UOMs to large state spaces is also a significant contribution.
The paper has several weaknesses, however. One of the main limitations is that the authors assume that the reward function can be specified at any time, which may not always be the case in practice. Additionally, the authors do not provide a clear comparison of UOMs with other methods in terms of sample complexity, which is an important consideration in reinforcement learning.
Overall, I would argue in favor of accepting this paper. The authors present a novel and well-motivated solution to an important problem in reinforcement learning, and their empirical results demonstrate the effectiveness of their approach. While there are some limitations to the work, the authors provide a clear and thorough analysis of their method, and their contributions have the potential to make a significant impact in the field.
Arguments pro acceptance:
* The paper presents a novel and well-motivated solution to an important problem in reinforcement learning.
* The authors provide a clear and thorough analysis of their method, including theoretical guarantees and empirical results.
* The empirical results demonstrate the effectiveness of UOMs in two domains, showing that they outperform existing methods in terms of computational efficiency and accuracy.
Arguments con acceptance:
* The authors assume that the reward function can be specified at any time, which may not always be the case in practice.
* The authors do not provide a clear comparison of UOMs with other methods in terms of sample complexity.
* The paper could benefit from additional analysis of the limitations and potential applications of UOMs.