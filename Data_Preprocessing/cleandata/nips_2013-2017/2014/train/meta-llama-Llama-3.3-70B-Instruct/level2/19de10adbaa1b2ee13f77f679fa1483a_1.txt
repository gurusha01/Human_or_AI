This paper introduces Deep Attention Selective Networks (dasNet), a novel architecture that incorporates feedback connections to enable selective internal attention in deep neural networks. The main claim of the paper is that dasNet can improve classification performance by allowing the network to iteratively focus its internal attention on the most discriminative features in an image. The authors support this claim through experiments on the CIFAR-10 and CIFAR-100 datasets, where dasNet outperforms the previous state-of-the-art model on unaugmented datasets.
The paper is well-written, and the authors provide a clear and concise explanation of the dasNet architecture and its components. The use of reinforcement learning to learn the feedback connections is a novel and interesting approach. The experiments are well-designed, and the results are impressive, with dasNet achieving a relative improvement of 6% with respect to the vanilla CNN.
The paper has several strengths, including its originality, clarity, and significance. The idea of incorporating feedback connections to enable selective internal attention is a new and exciting direction in deep learning. The authors provide a thorough analysis of the related work, and their experiments are well-designed and comprehensive.
However, there are some limitations to the paper. One potential weakness is that the dasNet architecture is complex and may be difficult to train and optimize. The authors use a variant of Natural Evolution Strategies (NES) to evolve the policy, which may not be the most efficient or effective method. Additionally, the paper could benefit from more analysis and visualization of the learned feedback connections and their impact on the network's behavior.
Overall, I would argue in favor of accepting this paper. The ideas presented are novel and significant, and the experiments are well-designed and comprehensive. While there are some limitations to the paper, the authors provide a clear and concise explanation of their approach, and the results are impressive.
Arguments pro acceptance:
* The paper presents a novel and original architecture that incorporates feedback connections to enable selective internal attention.
* The experiments are well-designed and comprehensive, and the results are impressive.
* The paper provides a thorough analysis of the related work and clearly explains the dasNet architecture and its components.
Arguments con acceptance:
* The dasNet architecture is complex and may be difficult to train and optimize.
* The use of NES to evolve the policy may not be the most efficient or effective method.
* The paper could benefit from more analysis and visualization of the learned feedback connections and their impact on the network's behavior.