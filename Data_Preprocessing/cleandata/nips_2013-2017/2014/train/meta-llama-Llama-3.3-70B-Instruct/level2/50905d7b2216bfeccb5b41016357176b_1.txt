This paper presents an alternative approach to solving large Markov Decision Processes (MDPs) by estimating the optimal action-value function using Difference of Convex (DC) programming. The main claim of the paper is that minimizing a norm of the Optimal Bellman Residual (OBR) is a viable approach to estimating the optimal action-value function. The authors support this claim by providing a theoretical analysis of the OBR minimization approach and demonstrating its consistency in the Vapnik sense.
The paper is well-structured and clearly written, making it easy to follow the authors' arguments. The introduction provides a good background on MDPs and the challenges of solving large MDPs. The authors also provide a clear explanation of the OBR minimization approach and its relationship to DC programming.
The theoretical analysis in Section 3 is thorough and provides a solid foundation for the OBR minimization approach. The authors prove that minimizing the empirical norm of the OBR is consistent in the Vapnik sense, which is a key result in the paper. The analysis is rigorous and well-supported by mathematical proofs.
The experiment in Section 5 provides a good illustration of the OBR minimization approach in practice. The authors compare their approach to state-of-the-art Reinforcement Learning algorithms, such as LSPI and Fitted-Q, and show that their approach is competitive. The results are promising, and the authors provide a good discussion of the implications of their findings.
The paper has several strengths, including:
* A clear and well-structured presentation of the OBR minimization approach
* A thorough theoretical analysis of the approach, including consistency in the Vapnik sense
* A good illustration of the approach in practice, with promising results
However, the paper also has some weaknesses, including:
* The experiment is limited to a specific type of MDP, and it is unclear how the approach will perform on more complex MDPs
* The authors do not provide a detailed comparison of their approach to other DC programming-based approaches
* The paper could benefit from a more detailed discussion of the implications of the results and the potential applications of the OBR minimization approach
Overall, the paper presents a promising approach to solving large MDPs using DC programming. The theoretical analysis is thorough, and the experiment provides a good illustration of the approach in practice. However, further research is needed to fully explore the potential of the OBR minimization approach and to address the limitations of the current paper.
Arguments pro acceptance:
* The paper presents a novel approach to solving large MDPs using DC programming
* The theoretical analysis is thorough and provides a solid foundation for the approach
* The experiment provides a good illustration of the approach in practice, with promising results
Arguments con acceptance:
* The experiment is limited to a specific type of MDP, and it is unclear how the approach will perform on more complex MDPs
* The authors do not provide a detailed comparison of their approach to other DC programming-based approaches
* The paper could benefit from a more detailed discussion of the implications of the results and the potential applications of the OBR minimization approach.