This paper presents a novel approach to building real-time Atari game playing agents by combining model-free reinforcement learning with deep learning. The authors propose using slow, off-line Monte Carlo tree search planning methods to generate training data for a deep-learned classifier capable of state-of-the-art real-time play. The main claim of the paper is that their new agents, which use the slow planning-based agents to provide training data, outperform the current state-of-the-art real-time agent, DQN.
The paper is well-supported by experimental results, which show that the proposed agents achieve significant performance gains over DQN in most of the 7 games evaluated. The authors also provide a detailed analysis of the learned features and policies, which suggests that the CNN learns relevant patterns useful for game playing.
The paper is well-written and clearly organized, making it easy to follow and understand. The authors provide a thorough background on reinforcement learning and deep learning, as well as a clear explanation of the challenges of perception in Atari games. The experimental results are thoroughly evaluated, and the authors provide a detailed discussion of the strengths and weaknesses of their approach.
One of the strengths of the paper is its ability to bridge the gap between planning-based approaches and model-free reinforcement learning. The authors show that by using the slow planning-based agents to provide training data, they can achieve significant performance gains over DQN while still maintaining real-time playability.
However, one potential limitation of the paper is that the proposed approach requires a significant amount of computational resources to generate the training data. The authors acknowledge this limitation and propose a method to interleave training and data collection, which shows promising results.
Overall, the paper presents a significant contribution to the field of reinforcement learning and deep learning, and the results have the potential to be widely applicable to other domains with significant perception and policy selection challenges.
Arguments pro acceptance:
* The paper presents a novel approach to building real-time Atari game playing agents that achieves significant performance gains over the current state-of-the-art.
* The authors provide a thorough analysis of the learned features and policies, which suggests that the CNN learns relevant patterns useful for game playing.
* The paper is well-written and clearly organized, making it easy to follow and understand.
Arguments con acceptance:
* The proposed approach requires a significant amount of computational resources to generate the training data.
* The authors do not provide a thorough comparison with other planning-based approaches, which could provide additional context for the results.
* The paper could benefit from additional analysis of the limitations of the approach and potential avenues for future work.