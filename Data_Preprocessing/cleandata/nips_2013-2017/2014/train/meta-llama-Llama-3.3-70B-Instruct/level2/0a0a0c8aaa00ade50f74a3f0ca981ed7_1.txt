This paper proposes a novel control-theoretic framework for building functioning attractor networks that satisfy key physiological constraints, including Dale's law, sparse and recurrent synaptic connections, and graded neuronal activity. The authors optimize network parameters to force sets of arbitrary analog patterns to become stable fixed points of the dynamics, resulting in networks that operate in the balanced regime and are robust to corruptions of the memory cue and ongoing noise.
The main claims of the paper are well-supported by theoretical analysis and numerical simulations. The authors demonstrate that their framework can embed multiple analog memories as stable fixed points of the dynamics, and that these memories are stable in the face of ongoing noise and corruption of the recall cues. The results are significant, as they provide a step forward in our understanding of the neural substrate of memory.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand the technical details. The authors provide a thorough review of previous work in the field, highlighting the limitations of existing models and the advantages of their approach. The use of a rate-based formulation of the circuit dynamics raises questions about the applicability of the method to understanding spiking memory networks, but the authors acknowledge this limitation and suggest potential avenues for future research.
The paper's strengths include its novel approach to building attractor networks, its ability to satisfy key physiological constraints, and its robustness to noise and corruption of the recall cues. The authors also provide a detailed analysis of the network's dynamics, including the evolution of the synaptic weights and the balance of excitation and inhibition.
One potential limitation of the paper is that it relies on a simplified model of neuronal activity, which may not capture the full complexity of real neuronal networks. Additionally, the authors' use of a global optimization algorithm to optimize network parameters may not be biologically plausible, and it is unclear how local synaptic learning rules could achieve the same stabilization objective.
Overall, the paper makes a significant contribution to our understanding of the neural substrate of memory, and its results have important implications for the development of artificial neural networks and other applications. The authors' approach is novel and well-supported, and their analysis is thorough and well-organized. I would recommend accepting this paper for publication, with minor revisions to address the limitations and potential avenues for future research.
Arguments pro acceptance:
* The paper proposes a novel and well-supported approach to building attractor networks that satisfy key physiological constraints.
* The results are significant and provide a step forward in our understanding of the neural substrate of memory.
* The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand the technical details.
Arguments con acceptance:
* The paper relies on a simplified model of neuronal activity, which may not capture the full complexity of real neuronal networks.
* The authors' use of a global optimization algorithm to optimize network parameters may not be biologically plausible.
* The paper could benefit from additional analysis and discussion of the potential limitations and avenues for future research.