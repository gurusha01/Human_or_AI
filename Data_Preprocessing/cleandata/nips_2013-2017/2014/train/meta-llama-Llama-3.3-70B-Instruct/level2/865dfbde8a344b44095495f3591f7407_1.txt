This paper proposes a stochastic variational inference (SVI) algorithm for hidden Markov models (HMMs) in time-dependent data settings. The main claim of the paper is that the proposed SVIHMM algorithm can efficiently learn the parameters of HMMs in large datasets by harnessing the memory decay of the chain to adaptively bound errors arising from edge effects.
The support for this claim comes from a combination of theoretical analysis and empirical experiments. The authors provide a detailed derivation of the SVIHMM algorithm, including the global update and local update steps, and prove that the algorithm converges to a local mode of the batch objective. They also demonstrate the effectiveness of the algorithm on synthetic experiments and a large genomics dataset, where it achieves comparable performance to batch variational Bayes (VB) in significantly less time.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments. The use of mathematical notation is consistent and well-defined, and the authors provide a clear explanation of the key concepts and techniques used in the paper.
One of the strengths of the paper is its ability to address the challenges of applying SVI to HMMs in time-dependent data settings. The authors identify the two potential issues that arise when sampling subchains as minibatches - the subsequences are not mutually independent, and updating the latent variables in the subchain ignores the data outside of the subchain - and propose an approximate message-passing scheme to address these issues.
The paper also provides a thorough evaluation of the SVIHMM algorithm, including a comparison to batch VB on synthetic experiments and a large genomics dataset. The results show that SVIHMM achieves comparable performance to batch VB in significantly less time, making it a promising approach for large-scale HMM inference.
However, there are some limitations to the paper. One potential limitation is that the SVIHMM algorithm is designed for HMMs with a fixed number of states, and it is not clear how the algorithm would perform on HMMs with a large or unknown number of states. Additionally, the paper assumes that the underlying chain is irreducible and aperiodic, which may not always be the case in practice.
Overall, I would rate this paper as a strong accept. The paper makes a significant contribution to the field of Bayesian inference, and the proposed SVIHMM algorithm has the potential to be widely used in practice. The paper is well-written and clearly organized, and the authors provide a thorough evaluation of the algorithm.
Arguments for acceptance:
* The paper proposes a novel and efficient algorithm for HMM inference in large datasets.
* The algorithm is well-motivated and clearly explained, with a thorough derivation and proof of convergence.
* The paper provides a thorough evaluation of the algorithm, including a comparison to batch VB on synthetic experiments and a large genomics dataset.
* The results show that SVIHMM achieves comparable performance to batch VB in significantly less time, making it a promising approach for large-scale HMM inference.
Arguments against acceptance:
* The algorithm is designed for HMMs with a fixed number of states, and it is not clear how the algorithm would perform on HMMs with a large or unknown number of states.
* The paper assumes that the underlying chain is irreducible and aperiodic, which may not always be the case in practice.
* The paper could benefit from additional experiments and evaluations to further demonstrate the effectiveness of the SVIHMM algorithm.