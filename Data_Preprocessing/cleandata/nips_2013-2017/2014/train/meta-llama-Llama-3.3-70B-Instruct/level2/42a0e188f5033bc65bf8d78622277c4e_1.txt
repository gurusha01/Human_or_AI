This paper proposes a parallel randomized block coordinate method, named Parallel Direction Method of Multipliers (PDMM), to solve optimization problems with multi-block linear constraints. The main claim of the paper is that PDMM achieves global convergence and O(1/T) iteration complexity with constant step size, outperforming existing methods such as ADMM and its variants.
The support for this claim is provided through theoretical analysis and experimental results. The authors establish the convergence results for PDMM under fairly simple assumptions, including the existence of a KKT point and the convexity of the objective function. The experimental results demonstrate the effectiveness of PDMM in solving robust principal component analysis and overlapping group lasso problems, showing that it converges faster than existing methods.
The usefulness of PDMM lies in its ability to solve a wide range of optimization problems with linear constraints, including those with overlapping blocks. The method is also parallelizable, making it suitable for large-scale problems. The authors provide a detailed analysis of the method, including its connection to existing methods such as ADMM and its variants.
The paper reflects common knowledge in the field, citing relevant literature and using standard terminology. The authors also provide a clear and concise explanation of the method, making it easy to understand and implement.
The novelty of PDMM lies in its ability to handle multi-block linear constraints and its parallelizable nature. The method is also more efficient than existing methods, achieving faster convergence rates and better performance in practice.
The completeness of the paper is evident in the detailed analysis and experimental results provided. The authors also discuss the limitations of the method, including the choice of step size and the number of blocks to be updated.
The limitations of the paper are acknowledged by the authors, who note that the method may not be suitable for all types of optimization problems. However, the authors provide suggestions for future work, including the extension of PDMM to handle non-convex objective functions and the development of more efficient algorithms for solving large-scale problems.
Overall, the paper presents a significant contribution to the field of optimization, providing a new and efficient method for solving optimization problems with multi-block linear constraints. The results are well-supported by theoretical analysis and experimental results, and the paper is well-written and easy to follow.
Arguments pro acceptance:
* The paper presents a novel and efficient method for solving optimization problems with multi-block linear constraints.
* The method achieves global convergence and O(1/T) iteration complexity with constant step size.
* The experimental results demonstrate the effectiveness of the method in solving robust principal component analysis and overlapping group lasso problems.
* The paper provides a detailed analysis of the method, including its connection to existing methods and its limitations.
Arguments con acceptance:
* The method may not be suitable for all types of optimization problems.
* The choice of step size and the number of blocks to be updated may require careful tuning.
* The paper does not provide a comparison with other state-of-the-art methods for solving optimization problems with linear constraints.