This paper proposes a novel algorithm for learning the full kernel matrix of a determinantal point process (DPP) by exploiting its eigendecomposition. The authors develop an expectation-maximization (EM) style optimization algorithm that negates the need for the problematic projection step required for naive gradient ascent to maintain positive semi-definiteness of the kernel matrix. The paper is well-written, and the authors provide a clear and concise introduction to DPPs and their applications.
The main claim of the paper is that the proposed EM algorithm is more robust and efficient than the naive approach of maximizing likelihood by projected gradient ascent. The authors support this claim with experiments on a real-world product recommendation task, where they achieve relative gains of up to 16.5% in test log-likelihood compared to the naive approach.
The paper has several strengths. Firstly, the authors provide a thorough review of previous work on DPPs and their limitations, which motivates the need for a new approach. Secondly, the proposed EM algorithm is well-derived and easy to follow, with a clear explanation of the expectation and maximization steps. Thirdly, the experiments are well-designed and provide convincing evidence of the effectiveness of the proposed algorithm.
However, there are some limitations to the paper. Firstly, the authors assume that the kernel matrix can be eigendecomposed, which may not always be the case in practice. Secondly, the proposed algorithm requires the computation of the eigendecomposition of the kernel matrix, which can be computationally expensive for large matrices. Thirdly, the authors do not provide a thorough analysis of the computational complexity of the proposed algorithm, which makes it difficult to understand its scalability.
In terms of novelty, the paper presents a significant improvement over previous work on DPPs. The proposed EM algorithm is a new and innovative approach to learning the kernel matrix of a DPP, and the authors provide a clear and concise explanation of its derivation and implementation.
In terms of significance, the paper has the potential to impact a wide range of applications, including product recommendation, document summarization, sensor placement, and image search. The authors demonstrate the effectiveness of the proposed algorithm on a real-world product recommendation task, which suggests that it can be applied to other domains where modeling negative interactions between items is important.
Overall, I would recommend accepting this paper for publication. The authors provide a clear and concise introduction to DPPs and their applications, and the proposed EM algorithm is well-derived and easy to follow. The experiments are well-designed and provide convincing evidence of the effectiveness of the proposed algorithm, and the paper has the potential to impact a wide range of applications.
Arguments for acceptance:
* The paper presents a novel and innovative approach to learning the kernel matrix of a DPP.
* The proposed EM algorithm is well-derived and easy to follow.
* The experiments are well-designed and provide convincing evidence of the effectiveness of the proposed algorithm.
* The paper has the potential to impact a wide range of applications.
Arguments against acceptance:
* The authors assume that the kernel matrix can be eigendecomposed, which may not always be the case in practice.
* The proposed algorithm requires the computation of the eigendecomposition of the kernel matrix, which can be computationally expensive for large matrices.
* The authors do not provide a thorough analysis of the computational complexity of the proposed algorithm.