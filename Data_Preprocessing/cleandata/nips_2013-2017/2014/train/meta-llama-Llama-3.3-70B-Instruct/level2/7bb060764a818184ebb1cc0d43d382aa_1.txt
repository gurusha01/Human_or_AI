This paper presents new ensemble learning algorithms for multi-class classification, which can use complex base classifier sets, such as deep decision trees, and still benefit from strong generalization guarantees. The authors introduce a novel data-dependent learning bound for convex ensembles in the multi-class setting, which depends on the Rademacher complexity of the sub-families composing the base classifier set and the mixture weight assigned to each sub-family. This bound is finer than existing ones, with an improved dependency on the number of classes and a more favorable complexity term.
The paper also presents several new multi-class ensemble algorithms, including MDeepBoostSum, MDeepBoostMaxSum, and MDeepBoostCompSum, which are derived from the learning guarantees. The authors prove positive results for the H-consistency and convergence of these algorithms and report the results of experiments showing that their performance compares favorably with that of multi-class versions of AdaBoost and Logistic Regression and their L1-regularized counterparts.
The strengths of the paper include the introduction of a novel learning bound, the derivation of new ensemble algorithms, and the provision of theoretical guarantees for these algorithms. The experiments demonstrate the effectiveness of the proposed algorithms in practice. The paper is well-written, and the authors provide a clear and detailed explanation of their methods and results.
However, there are some limitations to the paper. The authors assume that the base classifier set is composed of increasingly complex sub-families, which may not always be the case in practice. Additionally, the paper focuses on the multi-class setting, and it is not clear how the results would generalize to other settings, such as binary classification or regression.
Overall, the paper presents a significant contribution to the field of ensemble learning, and the proposed algorithms have the potential to be useful in practice. The authors provide a thorough analysis of their methods and results, and the paper is well-suited for publication in a top-tier conference.
Arguments for acceptance:
* The paper introduces a novel learning bound for convex ensembles in the multi-class setting.
* The authors derive new ensemble algorithms, including MDeepBoostSum, MDeepBoostMaxSum, and MDeepBoostCompSum.
* The paper provides theoretical guarantees for the proposed algorithms, including H-consistency and convergence results.
* The experiments demonstrate the effectiveness of the proposed algorithms in practice.
Arguments against acceptance:
* The paper assumes that the base classifier set is composed of increasingly complex sub-families, which may not always be the case in practice.
* The paper focuses on the multi-class setting, and it is not clear how the results would generalize to other settings.
* The authors could provide more discussion on the potential applications and limitations of their methods.