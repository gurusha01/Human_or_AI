This paper introduces the concept of Universal Option Models (UOMs) for real-time abstract planning in reinforcement learning. The authors propose a new model for options that is independent of any reward function, allowing for efficient computation of option-conditional returns and value functions of policies over options. The paper provides a thorough theoretical foundation for UOMs, including proofs of their universality and consistency, as well as empirical evaluations in two domains: a real-time strategy game and article recommendation.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of real-time abstract planning and the limitations of existing methods. The technical contributions of the paper are significant, and the authors provide a detailed analysis of the strengths and weaknesses of their approach.
One of the main strengths of the paper is its ability to handle multiple reward functions and compute option returns efficiently. The authors demonstrate that UOMs outperform existing methods in two domains, and the empirical results are convincing. The paper also provides a thorough analysis of the computational complexity of UOMs and compares it to existing methods.
However, there are some potential weaknesses of the paper. One of the limitations of UOMs is that they require a large amount of data to learn the option models, which can be a challenge in practice. Additionally, the paper assumes that the reward functions are linear, which may not always be the case in practice.
Arguments for acceptance:
* The paper introduces a new and significant concept in reinforcement learning, which has the potential to impact the field.
* The technical contributions of the paper are sound, and the authors provide a thorough analysis of the strengths and weaknesses of their approach.
* The empirical evaluations are convincing, and the authors demonstrate that UOMs outperform existing methods in two domains.
Arguments against acceptance:
* The paper assumes that the reward functions are linear, which may not always be the case in practice.
* The paper requires a large amount of data to learn the option models, which can be a challenge in practice.
* The paper does not provide a clear comparison to other existing methods, such as deep reinforcement learning approaches.
Overall, I believe that the paper is well-written, and the authors provide a significant contribution to the field of reinforcement learning. While there are some potential weaknesses, the strengths of the paper outweigh the weaknesses, and I recommend acceptance.
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 9/10
Recommendation: Accept with minor revisions. The authors should address the potential weaknesses of the paper, such as the assumption of linear reward functions and the requirement of a large amount of data. Additionally, the authors should provide a clear comparison to other existing methods.