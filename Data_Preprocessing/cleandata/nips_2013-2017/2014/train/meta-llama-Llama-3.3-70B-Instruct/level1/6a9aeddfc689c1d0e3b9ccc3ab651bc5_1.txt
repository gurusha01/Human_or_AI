This paper proposes a novel subsampling algorithm for large-scale corrupted linear regression, which limits the influence of data points with high influence. The algorithm, called influence weighted subsampling (IWS-LS), is based on the concept of influence, which measures the effective impact of an individual datapoint on the overall estimate. The authors show that IWS-LS improves over the current state-of-the-art approximation schemes for ordinary least squares (OLS) in terms of bias and variance.
The paper is well-written and clearly organized, with a thorough introduction to the problem of corrupted observations in linear regression and a detailed explanation of the proposed algorithm. The authors provide a theoretical analysis of the algorithm, including an upper bound on the estimation error, and demonstrate its effectiveness through extensive experimental evaluations on simulated and real datasets.
The strengths of the paper include:
* The proposal of a novel algorithm that addresses the problem of corrupted observations in linear regression, which is a common issue in real-world datasets.
* A thorough theoretical analysis of the algorithm, including an upper bound on the estimation error.
* Extensive experimental evaluations on simulated and real datasets, which demonstrate the effectiveness of the algorithm.
The weaknesses of the paper include:
* The computational complexity of the algorithm, which is O(np^2) for the exact version and o(np^2) for the approximate versions. While the authors provide fast approximation algorithms, the computational cost may still be high for very large datasets.
* The assumption that the corruption model is sub-Gaussian, which may not always be the case in practice.
Overall, the paper makes a significant contribution to the field of linear regression and provides a useful algorithm for addressing the problem of corrupted observations. The authors demonstrate the effectiveness of the algorithm through thorough theoretical analysis and experimental evaluations, and provide fast approximation algorithms to reduce the computational cost.
Arguments pro acceptance:
* The paper proposes a novel algorithm that addresses a common problem in linear regression.
* The theoretical analysis is thorough and provides an upper bound on the estimation error.
* The experimental evaluations demonstrate the effectiveness of the algorithm on simulated and real datasets.
Arguments con acceptance:
* The computational complexity of the algorithm may be high for very large datasets.
* The assumption of a sub-Gaussian corruption model may not always be valid in practice.
In terms of the four criteria for evaluation, the paper scores well on:
* Quality: The paper is well-written and clearly organized, with a thorough introduction to the problem and a detailed explanation of the proposed algorithm.
* Clarity: The paper is easy to follow, with clear explanations of the theoretical analysis and experimental evaluations.
* Originality: The paper proposes a novel algorithm that addresses a common problem in linear regression.
* Significance: The paper makes a significant contribution to the field of linear regression and provides a useful algorithm for addressing the problem of corrupted observations.