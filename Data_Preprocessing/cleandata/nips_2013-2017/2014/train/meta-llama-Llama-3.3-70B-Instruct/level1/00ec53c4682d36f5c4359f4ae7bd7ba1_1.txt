This paper proposes a two-stream ConvNet architecture for action recognition in videos, which incorporates spatial and temporal networks to capture complementary information on appearance and motion. The spatial stream operates on individual video frames, while the temporal stream is trained on multi-frame dense optical flow. The authors also introduce a multi-task learning framework to increase the amount of training data and improve performance on both UCF-101 and HMDB-51 datasets.
The paper is well-written, and the authors provide a clear overview of the related work in action recognition. The proposed architecture is novel and well-motivated, and the experimental results demonstrate its effectiveness in achieving competitive performance with state-of-the-art methods.
The strengths of the paper include:
* The proposal of a novel two-stream architecture that effectively combines spatial and temporal information for action recognition.
* The use of multi-task learning to increase the amount of training data and improve performance on both datasets.
* The thorough evaluation of the proposed architecture on two challenging datasets, UCF-101 and HMDB-51.
The weaknesses of the paper include:
* The reliance on pre-computed optical flow, which may not be feasible for large-scale video datasets.
* The lack of explicit handling of camera motion, which may affect the performance of the temporal stream.
* The limited analysis of the learned features and the convolutional filters in the temporal network.
Arguments pro acceptance:
* The paper proposes a novel and effective architecture for action recognition in videos.
* The experimental results demonstrate competitive performance with state-of-the-art methods.
* The use of multi-task learning is a significant contribution to the field.
Arguments con acceptance:
* The reliance on pre-computed optical flow may limit the applicability of the method to large-scale video datasets.
* The lack of explicit handling of camera motion may affect the performance of the temporal stream.
* The paper could benefit from a more detailed analysis of the learned features and the convolutional filters in the temporal network.
Overall, I believe that the paper is well-written, and the proposed architecture is novel and effective. The experimental results demonstrate competitive performance with state-of-the-art methods, and the use of multi-task learning is a significant contribution to the field. However, the paper could benefit from a more detailed analysis of the learned features and the convolutional filters in the temporal network, as well as a more explicit handling of camera motion. 
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10