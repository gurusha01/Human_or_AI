This paper proposes a novel algorithm called Sparse Random Features, which learns a sparse non-linear predictor by minimizing an l1-regularized objective function over the Hilbert Space induced from a kernel function. The authors interpret the algorithm as Randomized Coordinate Descent in an infinite-dimensional space and show that it converges to a solution within ε-precision of that using an exact kernel method, by drawing O(1/ε) random features.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of kernel approximation and the motivation behind their approach. The technical sections are also well-organized, and the authors provide a thorough analysis of the convergence properties of their algorithm.
The strengths of the paper include:
* The authors propose a novel algorithm that combines the benefits of random features and sparse learning, which can lead to more efficient and effective kernel approximation.
* The paper provides a thorough analysis of the convergence properties of the algorithm, including a detailed proof of the convergence rate.
* The authors demonstrate the effectiveness of their algorithm on several benchmark datasets, including regression and classification tasks.
The weaknesses of the paper include:
* The paper assumes that the kernel function can be decomposed into a set of basis functions, which may not always be the case in practice.
* The authors do not provide a clear comparison with other state-of-the-art kernel approximation methods, such as Nyström approximation or fastfood.
* The paper could benefit from more detailed experiments, including a comparison with other sparse learning algorithms and a more thorough analysis of the effect of the regularization parameter on the performance of the algorithm.
Overall, the paper is well-written and provides a significant contribution to the field of kernel approximation and sparse learning. The proposed algorithm has the potential to be more efficient and effective than existing methods, and the authors provide a thorough analysis of its convergence properties.
Arguments pro acceptance:
* The paper proposes a novel algorithm that combines the benefits of random features and sparse learning.
* The paper provides a thorough analysis of the convergence properties of the algorithm.
* The authors demonstrate the effectiveness of their algorithm on several benchmark datasets.
Arguments con acceptance:
* The paper assumes that the kernel function can be decomposed into a set of basis functions, which may not always be the case in practice.
* The authors do not provide a clear comparison with other state-of-the-art kernel approximation methods.
* The paper could benefit from more detailed experiments and a more thorough analysis of the effect of the regularization parameter on the performance of the algorithm.
Rating: 8/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above, including providing a clear comparison with other state-of-the-art kernel approximation methods and more detailed experiments. Additionally, the authors should consider providing more insight into the choice of the regularization parameter and its effect on the performance of the algorithm.