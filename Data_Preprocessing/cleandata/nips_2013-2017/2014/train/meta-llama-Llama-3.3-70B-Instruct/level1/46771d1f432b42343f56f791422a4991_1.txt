This paper explores the connection between dimensionality and communication cost in distributed learning problems, specifically in estimating the mean of a d-dimensional Gaussian distribution. The authors provide a comprehensive analysis of the problem, including lower bounds for the general case and an upper bound for the sparse case.
The paper's main contribution is the "direct-sum" theorem, which states that the communication cost for a d-dimensional problem is at least d times the communication cost for a one-dimensional problem. This theorem is used to derive lower bounds for the communication cost in the general case, including a bound of Ω(md/log(m)) for the interactive setting and Ω(md) for the simultaneous setting.
The authors also provide an interactive protocol that achieves the minimax squared loss with O(md) bits of communication, improving upon the simple simultaneous protocol by a logarithmic factor. Additionally, they initiate the study of distributed parameter estimation problems with structured parameters, specifically considering the case where the parameter is promised to be s-sparse. They provide a simple thresholding-based protocol that achieves the same squared loss while saving a d/s factor of communication.
The paper is well-written, and the authors provide clear explanations of their results and techniques. The proof sketches are helpful in understanding the intuition behind the theorems, and the authors provide complete proofs in the supplementary material.
The strengths of the paper include:
* The direct-sum theorem, which provides a powerful tool for deriving lower bounds for communication cost in distributed learning problems.
* The improved upper bound for the interactive protocol, which demonstrates the potential for interactive communication to reduce communication cost.
* The initiation of the study of distributed parameter estimation problems with structured parameters, which is an important area of research.
The weaknesses of the paper include:
* The lower bounds for the general case are not tight, and the authors acknowledge that there may be a gap between the lower and upper bounds.
* The protocol for the sparse case is simple, but the authors conjecture that the tradeoff between communication cost and squared loss may be optimal, which requires further investigation.
Overall, the paper makes significant contributions to the field of distributed learning and provides a foundation for further research in this area.
Arguments for acceptance:
* The paper provides a comprehensive analysis of the problem, including lower bounds and an upper bound.
* The direct-sum theorem is a powerful tool that can be applied to other statistical problems.
* The initiation of the study of distributed parameter estimation problems with structured parameters is an important area of research.
Arguments against acceptance:
* The lower bounds for the general case are not tight, which may limit the paper's impact.
* The protocol for the sparse case is simple, and the conjecture about the optimality of the tradeoff requires further investigation.
Rating: 8/10
Recommendation: Accept with minor revisions. The authors should consider addressing the gap between the lower and upper bounds for the general case and providing further evidence for the conjecture about the optimality of the tradeoff for the sparse case.