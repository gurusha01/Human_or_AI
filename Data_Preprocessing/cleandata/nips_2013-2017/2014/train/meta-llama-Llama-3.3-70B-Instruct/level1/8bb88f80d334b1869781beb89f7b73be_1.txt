This paper presents a novel approach to building real-time Atari game playing agents by combining model-free reinforcement learning with deep learning. The authors propose using slow, off-line Monte Carlo tree search planning methods to generate training data for a deep-learned classifier capable of state-of-the-art real-time play. The paper builds upon previous work on Deep Q-Networks (DQN) and introduces three new methods for combining UCT-based planning with deep learning: UCTtoRegression, UCTtoClassification, and UCTtoClassification-Interleaved.
The paper is well-written, and the authors provide a clear and concise introduction to the background and motivation of the work. The experimental results are thorough and demonstrate the effectiveness of the proposed methods, with the UCTtoClassification-Interleaved agent achieving the best performance in most games. The visualization of the learned features and policy provides valuable insights into what the CNN learns and how it makes decisions.
The strengths of the paper include:
* The proposal of a novel approach to combining model-free reinforcement learning with deep learning
* The thorough experimental evaluation of the proposed methods
* The visualization of the learned features and policy, which provides valuable insights into the decision-making process of the agent
The weaknesses of the paper include:
* The reliance on a specific planning method (UCT) and a specific deep learning architecture (CNN)
* The lack of comparison to other planning methods and deep learning architectures
* The limited analysis of the trade-offs between the different methods and the sensitivity of the results to hyperparameters
Arguments pro acceptance:
* The paper presents a novel and effective approach to building real-time Atari game playing agents
* The experimental results demonstrate the effectiveness of the proposed methods
* The visualization of the learned features and policy provides valuable insights into the decision-making process of the agent
Arguments con acceptance:
* The paper relies on a specific planning method and deep learning architecture, which may limit the generality of the results
* The lack of comparison to other planning methods and deep learning architectures may limit the impact of the paper
* The limited analysis of the trade-offs between the different methods and the sensitivity of the results to hyperparameters may limit the usefulness of the paper for practitioners.
Overall, the paper is well-written, and the proposed methods are effective. However, the reliance on a specific planning method and deep learning architecture, and the limited analysis of the trade-offs between the different methods, may limit the impact and usefulness of the paper. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by experimental results. However, the reliance on a specific planning method and deep learning architecture may limit the generality of the results.
Clarity: 9/10
The paper is well-written, and the authors provide a clear and concise introduction to the background and motivation of the work. The experimental results are thorough and easy to follow.
Originality: 8/10
The paper presents a novel approach to combining model-free reinforcement learning with deep learning. However, the reliance on a specific planning method and deep learning architecture may limit the originality of the paper.
Significance: 8/10
The paper demonstrates the effectiveness of the proposed methods in building real-time Atari game playing agents. However, the limited analysis of the trade-offs between the different methods and the sensitivity of the results to hyperparameters may limit the impact of the paper.