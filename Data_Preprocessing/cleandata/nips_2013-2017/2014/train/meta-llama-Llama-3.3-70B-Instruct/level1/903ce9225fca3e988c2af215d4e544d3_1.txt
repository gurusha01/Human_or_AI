This paper presents a significant contribution to the field of multi-armed bandits (MAB) by fully characterizing the regret complexity of a broad class of MAB problems with non-stationary reward structures. The authors establish a direct link between the extent of allowable reward variation and the minimal achievable regret, providing a spectrum of orders of the minimax regret ranging between order T 2/3 and order T.
The paper's main strengths lie in its thorough analysis of the problem, including a lower bound on the best achievable performance (Theorem 1) and the development of a near-optimal policy (Rexp3) that achieves a regret of order (KVT) 1/3 T 2/3. The authors also provide a detailed discussion of the tradeoffs between exploration and exploitation, as well as the impact of non-stationarity on the regret.
The paper is well-written, and the authors provide a clear and concise explanation of the problem formulation, main results, and proofs. The use of examples and illustrations helps to clarify the concepts and make the paper more accessible to readers.
One potential weakness of the paper is the assumption of a known variation budget (VT). The authors acknowledge this limitation and discuss the potential for adaptive policies that do not require prior knowledge of VT. However, this remains an open problem, and further research is needed to address this challenge.
In terms of originality, the paper builds upon existing work in the field of MAB, but the authors' contribution lies in their ability to characterize the regret complexity of a broad class of non-stationary MAB problems. The paper's significance is evident in its ability to provide a framework for understanding the impact of non-stationarity on the regret and to develop policies that can adapt to changing reward structures.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of MAB, providing a thorough analysis of the regret complexity of non-stationary MAB problems.
* The authors develop a near-optimal policy (Rexp3) that achieves a regret of order (KVT) 1/3 T 2/3.
* The paper provides a clear and concise explanation of the problem formulation, main results, and proofs.
* The authors discuss the potential for adaptive policies that do not require prior knowledge of VT, highlighting an important area for future research.
Arguments con acceptance:
* The paper assumes a known variation budget (VT), which may not be realistic in all scenarios.
* The authors do not provide a complete solution to the problem of adaptive policies that do not require prior knowledge of VT.
* The paper's results may not be directly applicable to all MAB problems, as the authors focus on a specific class of non-stationary MAB problems.
Overall, I recommend accepting this paper, as it presents a significant contribution to the field of MAB and provides a thorough analysis of the regret complexity of non-stationary MAB problems. The authors' development of a near-optimal policy and their discussion of the potential for adaptive policies highlight important areas for future research.