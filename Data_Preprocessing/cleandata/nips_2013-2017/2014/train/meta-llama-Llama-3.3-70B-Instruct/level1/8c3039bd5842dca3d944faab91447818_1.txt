This paper proposes a novel deep generative model for images, called the Deep Gaussian Mixture Model (Deep GMM), which is a generalization of Gaussian Mixture Models to multiple layers. The model is designed to efficiently capture products of variations in natural images and can be trained using an Expectation Maximization (EM) algorithm that scales well to large datasets. The paper demonstrates the effectiveness of the Deep GMM in density estimation experiments, showing that it generalizes better than shallow Gaussian Mixture Models and achieves results comparable to the state of the art.
The strengths of the paper include:
* The proposal of a novel deep generative model that addresses the scalability issues of traditional deep unsupervised learning methods.
* The development of an efficient EM-based algorithm for training the Deep GMM, which can be easily distributed over multiple machines.
* The demonstration of the model's effectiveness in density estimation experiments, with results comparable to the state of the art.
The weaknesses of the paper include:
* The model's limitation to continuous real-valued data, which restricts its applicability to other types of data.
* The need for further research to extend the model to larger images and other high-dimensional data, as well as to discrete data.
* The comparison to other deep unsupervised learning techniques is limited, and further experiments are needed to fully evaluate the model's performance.
Arguments pro acceptance:
* The paper proposes a novel and scalable deep generative model that addresses the limitations of traditional deep unsupervised learning methods.
* The model demonstrates strong performance in density estimation experiments, with results comparable to the state of the art.
* The paper provides a clear and well-organized presentation of the model and its training algorithm.
Arguments con acceptance:
* The model's limitation to continuous real-valued data restricts its applicability to other types of data.
* The need for further research to extend the model to larger images and other high-dimensional data, as well as to discrete data, may limit its immediate impact.
* The comparison to other deep unsupervised learning techniques is limited, and further experiments are needed to fully evaluate the model's performance.
Overall, the paper presents a significant contribution to the field of deep generative models, and its strengths outweigh its weaknesses. With some further research to address the limitations, the Deep GMM has the potential to become a widely used and effective model for unsupervised learning. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The model is a complete piece of work, and the authors are careful about evaluating both the strengths and weaknesses of the work.
Clarity: 9/10
The paper is clearly written, well-organized, and provides enough information for the expert reader to reproduce the results.
Originality: 8/10
The paper proposes a novel deep generative model, which is a significant contribution to the field. However, the idea of generalizing Gaussian Mixture Models to multiple layers is not entirely new, and the paper builds upon existing work.
Significance: 8/10
The paper addresses a difficult problem in unsupervised learning and provides a novel solution that has the potential to advance the state of the art. The results are significant, and the model has the potential to be widely used in the field.