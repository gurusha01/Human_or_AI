This paper proposes a novel architecture called deep recursive neural networks (deep RNNs), which stacks multiple recursive layers to capture hierarchical representations in structured input data. The authors apply this architecture to fine-grained sentiment classification using binary parse trees and demonstrate that deep RNNs outperform shallow recursive nets and previous baselines, achieving state-of-the-art performance.
The paper is well-written, and the authors provide a clear explanation of the proposed architecture and its components. The experimental evaluation is thorough, and the results are impressive. The authors also provide qualitative analyses of the learned representations, which offer insights into the operation of different layers and their contribution to the overall performance.
The strengths of the paper include:
* The proposal of a novel architecture that combines the strengths of recursive neural networks and deep learning
* The thorough experimental evaluation, which demonstrates the effectiveness of the proposed architecture
* The qualitative analyses, which provide insights into the learned representations and their properties
The weaknesses of the paper include:
* The lack of a clear comparison to other deep learning architectures, such as convolutional neural networks or long short-term memory networks
* The limited exploration of the hyperparameter space, which might affect the generalizability of the results
* The reliance on pre-trained word vectors, which might not be available for all languages or domains
Arguments for acceptance:
* The paper proposes a novel architecture that addresses a significant problem in natural language processing
* The experimental evaluation is thorough and demonstrates the effectiveness of the proposed architecture
* The qualitative analyses provide insights into the learned representations and their properties
Arguments against acceptance:
* The paper might benefit from a more comprehensive comparison to other deep learning architectures
* The limited exploration of the hyperparameter space might affect the generalizability of the results
* The reliance on pre-trained word vectors might limit the applicability of the proposed architecture to other languages or domains
Overall, the paper is well-written, and the proposed architecture is novel and effective. The experimental evaluation is thorough, and the qualitative analyses provide insights into the learned representations. While there are some weaknesses, the paper is a significant contribution to the field of natural language processing, and its strengths outweigh its weaknesses. Therefore, I recommend accepting the paper. 
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 9/10