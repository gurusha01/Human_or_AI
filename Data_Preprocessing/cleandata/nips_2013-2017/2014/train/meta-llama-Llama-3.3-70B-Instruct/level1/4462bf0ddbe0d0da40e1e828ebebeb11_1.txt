This paper proposes a novel algorithm for learning the full kernel matrix of a determinantal point process (DPP) by changing the kernel parameterization from matrix entries to eigenvalues and eigenvectors and then lower-bounding the likelihood in the manner of expectation-maximization algorithms. The authors demonstrate the effectiveness of their method on a real-world product recommendation task, achieving relative gains of up to 16.5% in test log-likelihood compared to the naive approach of maximizing likelihood by projected gradient ascent on the entries of the kernel matrix.
The paper is well-written, and the authors provide a clear and concise introduction to DPPs and their applications. The proposed algorithm is well-motivated, and the authors provide a detailed analysis of its computational complexity and runtime. The experimental results are impressive, and the authors provide a thorough analysis of the advantages and limitations of their method.
The strengths of the paper include:
* The proposal of a novel algorithm for learning the full kernel matrix of a DPP, which addresses the limitations of previous methods that assume fixed values or restrictive parametric forms for the kernel.
* The demonstration of the effectiveness of the proposed algorithm on a real-world product recommendation task, with significant gains in test log-likelihood compared to the naive approach.
* The provision of a detailed analysis of the computational complexity and runtime of the proposed algorithm, which shows that it is asymptotically faster than the naive approach.
The weaknesses of the paper include:
* The assumption that the kernel matrix is positive semi-definite, which may not always be the case in practice.
* The use of a simplified initialization method, which may not always provide a good starting point for the algorithm.
* The lack of comparison with other state-of-the-art methods for learning DPPs, which would provide a more comprehensive evaluation of the proposed algorithm.
Arguments pro acceptance:
* The paper proposes a novel and effective algorithm for learning the full kernel matrix of a DPP, which addresses the limitations of previous methods.
* The experimental results demonstrate significant gains in test log-likelihood compared to the naive approach, which shows the practical effectiveness of the proposed algorithm.
* The paper provides a detailed analysis of the computational complexity and runtime of the proposed algorithm, which shows that it is asymptotically faster than the naive approach.
Arguments con acceptance:
* The paper assumes that the kernel matrix is positive semi-definite, which may not always be the case in practice.
* The initialization method used in the paper is simplified and may not always provide a good starting point for the algorithm.
* The paper lacks comparison with other state-of-the-art methods for learning DPPs, which would provide a more comprehensive evaluation of the proposed algorithm.
Overall, the paper is well-written, and the proposed algorithm is novel and effective. The experimental results demonstrate significant gains in test log-likelihood compared to the naive approach, and the paper provides a detailed analysis of the computational complexity and runtime of the proposed algorithm. However, the paper could be improved by addressing the limitations mentioned above, such as assuming positive semi-definiteness of the kernel matrix and using a simplified initialization method. With these improvements, the paper would be even stronger and more convincing.