This paper proposes a stochastic variational inference (SVI) algorithm for learning the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The authors address the challenge of applying SVI to HMMs by developing an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. The paper provides a clear and well-organized presentation of the proposed algorithm, SVIHMM, and demonstrates its effectiveness on synthetic experiments and a large genomics dataset.
The paper's strengths include:
1. Technical soundness: The paper provides a thorough derivation of the SVIHMM algorithm, including the global and local update steps, and proves its convergence to a local mode.
2. Originality: The paper proposes a novel approach to applying SVI to HMMs, addressing the challenges of dependent observations and edge effects.
3. Significance: The paper demonstrates the effectiveness of SVIHMM on a large genomics dataset, where batch inference is computationally infeasible, and shows that it achieves comparable performance to batch VB.
4. Clarity: The paper is well-written, with clear explanations of the proposed algorithm and its components.
The paper's weaknesses include:
1. Computational complexity: While the paper demonstrates the efficiency of SVIHMM, the computational complexity of the algorithm is still O(K2(L+2τ)M), which may be a limitation for very large datasets.
2. Choice of hyperparameters: The paper does not provide a clear guideline for choosing the hyperparameters, such as the subchain length L, the number of subchains per minibatch M, and the buffer length τ.
3. Comparison to other methods: While the paper compares SVIHMM to batch VB, it would be useful to compare it to other stochastic inference methods, such as stochastic gradient descent, to assess its performance.
Arguments for acceptance:
1. The paper proposes a novel and technically sound algorithm for applying SVI to HMMs.
2. The paper demonstrates the effectiveness of SVIHMM on a large genomics dataset.
3. The paper provides a clear and well-organized presentation of the proposed algorithm.
Arguments against acceptance:
1. The computational complexity of the algorithm may be a limitation for very large datasets.
2. The paper does not provide a clear guideline for choosing the hyperparameters.
3. The paper could benefit from a more comprehensive comparison to other stochastic inference methods.
Overall, I recommend accepting the paper, as it proposes a novel and technically sound algorithm for applying SVI to HMMs, and demonstrates its effectiveness on a large genomics dataset. However, the authors should address the limitations and provide a clearer guideline for choosing the hyperparameters.