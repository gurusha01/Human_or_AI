This paper presents a novel approach to solving large Markov Decision Processes (MDPs) by estimating the optimal action-value function using Difference of Convex (DC) programming. The authors show that minimizing a norm of the Optimal Bellman Residual (OBR) is an interesting alternative to traditional Approximate Dynamic Programming methods. They prove that the empirical norm of the OBR is consistent in the Vapnik sense and frame the minimization of the empirical norm as a DC minimization problem.
The paper is well-structured and clearly written, making it easy to follow the authors' arguments. The introduction provides a good background on MDPs and the challenges of solving large ones. The authors also provide a thorough review of related work, including Approximate Value Iteration (AVI) and Approximate Policy Iteration (API).
The technical contributions of the paper are significant. The authors prove that minimizing a norm of the OBR is equivalent to finding the optimal action-value function and provide a bound on the error of the estimated action-value function. They also show that the empirical norm of the OBR is consistent in the Vapnik sense, which justifies the use of sampled transitions to estimate the OBR.
The authors frame the minimization of the empirical norm as a DC minimization problem, which allows them to rely on the existing literature on DC programming. They provide two polyhedral explicit decompositions of the DC function, which can be used to solve the minimization problem using the Difference of Convex functions Algorithm (DCA).
The experimental results show that the proposed approach is competitive with state-of-the-art Reinforcement Learning algorithms, including LSPI and Fitted-Q. The authors also demonstrate that the DCA algorithms have less variance than the other algorithms, which is an advantage.
The strengths of the paper include:
* A novel approach to solving large MDPs using DC programming
* A thorough review of related work and clear explanations of the technical contributions
* Significant technical contributions, including the proof of consistency and the framing of the minimization problem as a DC minimization problem
* Competitive experimental results with state-of-the-art algorithms
The weaknesses of the paper include:
* The use of a basic setting for DCA, which may not be optimal
* The lack of a non-parametric version of the algorithm, which could be an interesting direction for future work
Overall, the paper is well-written and presents a significant contribution to the field of Reinforcement Learning. The authors provide a clear and thorough explanation of their approach and demonstrate its competitiveness with state-of-the-art algorithms.
Arguments pro acceptance:
* The paper presents a novel approach to solving large MDPs using DC programming
* The technical contributions are significant and well-explained
* The experimental results are competitive with state-of-the-art algorithms
* The paper is well-structured and clearly written
Arguments con acceptance:
* The use of a basic setting for DCA may not be optimal
* The lack of a non-parametric version of the algorithm may limit its applicability
* The paper may benefit from additional experimental results or comparisons with other algorithms.