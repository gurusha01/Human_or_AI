This paper presents a novel approach to stochastic variational inference for hidden Markov models (HMMs) in time-dependent data settings. The authors develop an algorithm, SVIHMM, that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects when subsampling dependent observations. The paper provides a clear and well-organized presentation of the methodology, including a detailed description of the algorithm and its components.
The key strengths of the paper include:
* The authors address a significant challenge in applying stochastic optimization to HMMs, which is the dependence between observations.
* The proposed algorithm, SVIHMM, is shown to converge to a local mode of the batch objective, and empirical results demonstrate similar performance to batch VB in significantly less time.
* The paper provides a thorough analysis of the trade-off between the choice of subchain length L and the number of subchains per minibatch M.
* The authors demonstrate the effectiveness of their algorithm on a large genomics dataset, where batch inference is computationally infeasible.
However, there are some areas that require clarification or improvement:
* The transition of θ from a non-random to a random variable, particularly around Definition 1 and the introduction of conditional mutual information, could be clarified.
* The minimax rate stated in Corollary 3.1 may not hold in scenarios where σ^2 is very large and n = m = d = 1, and this should be qualified.
* A minor correction is proposed for Protocol 2, regarding the subscript of Yi in the argument of \hat{\theta}i.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The presentation is clear and well-organized, making it easy to follow. The approach is novel and extends previous work on stochastic variational inference to handle time-dependent data settings. The results are significant, demonstrating the effectiveness of the algorithm on a large genomics dataset.
Arguments for acceptance:
* The paper presents a novel approach to stochastic variational inference for HMMs, addressing a significant challenge in the field.
* The algorithm is shown to converge to a local mode of the batch objective, and empirical results demonstrate similar performance to batch VB in significantly less time.
* The paper provides a thorough analysis of the trade-off between the choice of subchain length L and the number of subchains per minibatch M.
* The authors demonstrate the effectiveness of their algorithm on a large genomics dataset, where batch inference is computationally infeasible.
Arguments against acceptance:
* The paper may benefit from additional clarification on the transition of θ from a non-random to a random variable.
* The minimax rate stated in Corollary 3.1 may not hold in certain scenarios, and this should be qualified.
* A minor correction is proposed for Protocol 2, which should be addressed.
Overall, the paper presents a significant contribution to the field of stochastic variational inference, and with some minor revisions, it has the potential to be a strong acceptance.