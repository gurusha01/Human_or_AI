This paper introduces a new framework for online multi-task learning with a shared annotator, where multiple learners share a single annotator with limited bandwidth. The authors propose an algorithm, SHAMPO, which extends the Perceptron algorithm to this setting and provides a bound on the expected cumulative number of mistakes. The algorithm is shown to outperform other algorithms, including uniform allocation and exploit, in various tasks and settings.
The paper is clearly written, and the idea is both simple and good, making it an overall likable paper. The experimental results are impressive, demonstrating the effectiveness of SHAMPO in reducing the test error of each problem by more than 40% compared to full exploration. The authors also provide a good analysis of the algorithm, including a bound on the expected cumulative number of mistakes.
However, there are some areas that could be improved. The experimental results are somewhat limited, as they are only presented for a few datasets and tasks. It would be beneficial to see more extensive experiments, including comparisons with other algorithms and evaluations on a wider range of tasks and datasets. Additionally, the authors could provide more discussion on the choice of hyperparameters, such as the tradeoff parameter b and the prior ai.
One potential issue with the algorithm is that it may not be suitable for high-dimensional data, as the number of parameters increases quadratically with the number of tasks. The authors could consider using techniques such as dimensionality reduction or sparse coding to mitigate this issue.
In terms of related work, the authors provide a good overview of the existing literature on multi-task learning and contextual bandits. However, they could provide more discussion on how their work differs from and builds upon existing approaches.
Overall, this is a strong paper that makes a significant contribution to the field of online multi-task learning. With some additional experiments and discussion, it has the potential to be even more impactful.
Arguments for acceptance:
* The paper introduces a new and interesting framework for online multi-task learning with a shared annotator.
* The proposed algorithm, SHAMPO, is simple and effective, and provides a bound on the expected cumulative number of mistakes.
* The experimental results are impressive, demonstrating the effectiveness of SHAMPO in reducing the test error of each problem.
* The paper provides a good analysis of the algorithm and its limitations.
Arguments against acceptance:
* The experimental results are somewhat limited, and more extensive experiments would be beneficial.
* The algorithm may not be suitable for high-dimensional data, and techniques such as dimensionality reduction or sparse coding may be necessary.
* The paper could provide more discussion on the choice of hyperparameters and how they affect the performance of the algorithm.
* The related work section could be more comprehensive, and provide more discussion on how the authors' work differs from and builds upon existing approaches.