This paper presents a novel approach to active learning in linear regression, aiming to improve the constant factors in distribution-dependent rates of convergence compared to passive learning. The authors propose a rejection sampling scheme that alters the sampling distribution without changing the optimal solution and prove a risk bound for this method that can approach the "oracle" rate. The algorithm requires careful setting of a scaling function parameter, which is optimized using an estimated linear function from an initial sample, and can be provably superior to passive learning methods.
The paper is well-written, and the authors provide a clear and concise explanation of their approach. The theoretical analysis is sound, and the experimental results demonstrate the effectiveness of the proposed method. The paper makes a solid contribution to the field of active learning and has the potential to be of interest to a wide audience.
One of the strengths of the paper is its ability to provide a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. The authors show that the classic GLM can be viewed as a special case of their conductance-based model, which allows for more realistic and flexible modeling of neural responses. The paper also provides a clear and detailed explanation of the conductance-based model and its relationship to the GLM.
However, one of the weaknesses of the paper is the lack of discussion on the dependence between certain terms in Theorem 5.1, which can affect the trade-off between rates and requires further illustration with examples. Additionally, the paper could benefit from additional citations, such as Efromovich (2005), and clarification on notation, particularly for P_\phi.
Overall, the paper is well-structured, and the authors provide a clear and concise explanation of their approach. The theoretical analysis is sound, and the experimental results demonstrate the effectiveness of the proposed method. The paper makes a solid contribution to the field of active learning and has the potential to be of interest to a wide audience.
Arguments pro acceptance:
* The paper presents a novel approach to active learning in linear regression.
* The authors provide a clear and concise explanation of their approach.
* The theoretical analysis is sound, and the experimental results demonstrate the effectiveness of the proposed method.
* The paper makes a solid contribution to the field of active learning.
Arguments con acceptance:
* The paper lacks discussion on the dependence between certain terms in Theorem 5.1.
* The paper could benefit from additional citations and clarification on notation.
* The paper may not be of interest to a narrow audience, but it has the potential to be of interest to a wide audience.