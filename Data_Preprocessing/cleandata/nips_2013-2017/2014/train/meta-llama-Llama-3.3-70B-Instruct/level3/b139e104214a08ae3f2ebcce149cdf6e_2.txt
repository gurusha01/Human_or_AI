This paper presents a stochastic variational inference (SVI) algorithm for hidden Markov models (HMMs) in a time-dependent data setting. The authors extend the SVI algorithm to handle dependencies between observations, which is a significant challenge in applying stochastic optimization to HMMs. The proposed algorithm, SVIHMM, uses a subsampling approach to break the dependencies between observations and adaptively bounds errors arising from edge effects.
The paper is well-written and provides a clear explanation of the technical details. The authors demonstrate the effectiveness of their algorithm on synthetic experiments and a large genomics dataset, where batch inference is computationally infeasible. The results show that SVIHMM achieves comparable performance to batch variational Bayes (VB) in terms of predictive log-probability and transition matrix estimation.
The strengths of the paper include its ability to handle long-time series data and its scalability to large datasets. The authors also provide a detailed analysis of the algorithm's convergence and provide guarantees for its performance. The use of a buffering scheme to mitigate edge effects is a novel contribution and provides robustness to the algorithm.
However, the paper's originality is somewhat limited, as it builds heavily on earlier work on SVI and HMMs. The authors could have provided more insight into how their work differs from existing approaches and how it advances the state of the art. Additionally, the paper could benefit from more discussion on the choice of hyperparameters, such as the subchain length and the number of subchains per minibatch.
In terms of significance, the paper addresses an important problem in Bayesian inference for time-series data. The proposed algorithm has the potential to be applied to a wide range of applications, including genomics, finance, and signal processing. The authors demonstrate the effectiveness of their algorithm on a large genomics dataset, which is a significant contribution to the field.
Overall, the paper is well-written, and the authors provide a clear explanation of the technical details. The proposed algorithm is a significant contribution to the field of Bayesian inference for time-series data, and its scalability and effectiveness make it a valuable tool for practitioners.
Arguments for acceptance:
* The paper presents a novel algorithm for stochastic variational inference in HMMs, which is a significant contribution to the field.
* The authors provide a detailed analysis of the algorithm's convergence and provide guarantees for its performance.
* The paper demonstrates the effectiveness of the algorithm on synthetic experiments and a large genomics dataset.
* The proposed algorithm has the potential to be applied to a wide range of applications, including genomics, finance, and signal processing.
Arguments against acceptance:
* The paper's originality is somewhat limited, as it builds heavily on earlier work on SVI and HMMs.
* The authors could have provided more insight into how their work differs from existing approaches and how it advances the state of the art.
* The paper could benefit from more discussion on the choice of hyperparameters, such as the subchain length and the number of subchains per minibatch.