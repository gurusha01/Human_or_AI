This paper presents a significant contribution to the field of online bandit optimization, specifically in the context of smooth strongly convex losses. The authors propose a meta-algorithm that constructs an unbiased approximation of the gradient through careful sampling strategies and supplies it to an optimization routine. The theoretical analysis of the approach yields a bound of O(T^{1/2}), which improves upon existing approaches.
The paper is well-written, clear, and precise, with straightforward and easy-to-follow proofs that do not contain obvious errors. The authors provide a comprehensive review of prior work, highlighting the key challenges and open questions in the field. The proposed algorithm is efficient and near-optimal, making it a valuable contribution to the field.
The strengths of the paper include:
* The proposed algorithm achieves a near-optimal regret bound of O(T^{1/2}), which improves upon existing approaches.
* The paper provides a comprehensive review of prior work, highlighting the key challenges and open questions in the field.
* The authors provide a clear and precise explanation of the proposed algorithm and its theoretical analysis.
The weaknesses of the paper include:
* The paper assumes that the loss functions are strongly convex and smooth, which may not be the case in all real-world applications.
* The paper does not provide experimental results to validate the performance of the proposed algorithm.
Arguments for acceptance:
* The paper presents a significant contribution to the field of online bandit optimization.
* The proposed algorithm achieves a near-optimal regret bound, which is a significant improvement upon existing approaches.
* The paper is well-written, clear, and precise, with straightforward and easy-to-follow proofs.
Arguments against acceptance:
* The paper assumes that the loss functions are strongly convex and smooth, which may not be the case in all real-world applications.
* The paper does not provide experimental results to validate the performance of the proposed algorithm.
Overall, I recommend accepting this paper for publication. The proposed algorithm is efficient and near-optimal, and the paper provides a comprehensive review of prior work and a clear explanation of the theoretical analysis. While the paper has some weaknesses, the strengths outweigh the weaknesses, and the paper makes a significant contribution to the field of online bandit optimization. 
Quality: 9/10
The paper is technically sound, and the claims are well-supported by theoretical analysis. The paper is a complete piece of work, and the authors are careful and honest about evaluating both the strengths and weaknesses of the work.
Clarity: 9/10
The paper is clearly written, well-organized, and provides enough information for the expert reader to reproduce the results.
Originality: 8/10
The paper presents a novel combination of familiar techniques, and the approach is new. However, the problem of bandit convex optimization is not new, and the paper builds upon existing work.
Significance: 9/10
The results are important, and the paper addresses a difficult problem in a better way than previous research. The paper advances the state of the art in a demonstrable way and provides a unique theoretical approach.