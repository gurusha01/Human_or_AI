This paper introduces a novel framework for online multi-task learning with a shared annotator, where multiple learners share a single annotator with limited bandwidth. The authors propose an algorithm, SHAMPO, which learns to select tasks to query and update models based on the received feedback. The algorithm is analyzed in the mistake-bound model, and the results show that it outperforms other algorithms in sparse recovery problems with structured sparsity.
The paper is well-written, and the authors provide a clear explanation of the problem setting, the proposed algorithm, and the analysis. The use of notation is generally good, although there are some minor errors and suggestions for improvement. The addition of real data examples would strengthen the paper and demonstrate the proposed method's impact.
The method has several strengths, including its ability to handle multiple tasks with a shared annotator, its exploration-exploitation strategy, and its aggressive update rule. The algorithm's performance is evaluated on several datasets, including OCR, vowel prediction, and document classification, and the results show that it outperforms other algorithms.
However, there are some weaknesses and areas for improvement. The paper could benefit from a more detailed discussion of the attenuation of correlation of sparse variables with respect to the correlation of parameters gamma. Additionally, the paper assumes that the tasks are independent, which may not always be the case in practice. The authors could also provide more insight into how to generate good priors for the algorithm.
Overall, the paper makes a valuable contribution to the field of online multi-task learning and provides a novel framework for handling multiple tasks with a shared annotator. The proposed algorithm, SHAMPO, is well-designed and performs well in practice. With some minor revisions to address the weaknesses and areas for improvement, the paper has the potential to be a strong contribution to the field.
Arguments for acceptance:
* The paper introduces a novel framework for online multi-task learning with a shared annotator.
* The proposed algorithm, SHAMPO, is well-designed and performs well in practice.
* The paper provides a clear explanation of the problem setting, the proposed algorithm, and the analysis.
* The results show that the algorithm outperforms other algorithms in sparse recovery problems with structured sparsity.
Arguments against acceptance:
* The paper could benefit from a more detailed discussion of the attenuation of correlation of sparse variables with respect to the correlation of parameters gamma.
* The paper assumes that the tasks are independent, which may not always be the case in practice.
* The authors could provide more insight into how to generate good priors for the algorithm.
* The paper could benefit from additional real data examples to demonstrate the proposed method's impact.