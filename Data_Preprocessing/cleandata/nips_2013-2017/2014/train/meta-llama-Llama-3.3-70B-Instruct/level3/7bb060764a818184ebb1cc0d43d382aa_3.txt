This paper proposes a Sparse Random Features algorithm, which learns a sparse non-linear predictor by minimizing an l1-regularized objective function over the Hilbert Space induced from a kernel function. The authors interpret the algorithm as Randomized Coordinate Descent in an infinite-dimensional space and show that it converges to a solution within ε-precision of that using an exact kernel method, by drawing O(1/ε) random features.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments. The introduction provides a good background on kernel methods and the limitations of existing Random Features approaches. The problem setup is clearly defined, and the authors provide a thorough analysis of the convergence behavior of the proposed algorithm.
The strengths of the paper include:
* The proposed algorithm is novel and addresses the limitation of existing Random Features approaches, which can result in large model sizes.
* The authors provide a thorough analysis of the convergence behavior of the algorithm, including a proof of the descent amount and convergence rate.
* The experimental results demonstrate the effectiveness of the proposed algorithm in achieving sparse solutions with comparable performance to existing methods.
However, there are some weaknesses and areas for improvement:
* The paper builds upon existing work on Random Features and Boosting, and the authors could provide more context on how their work differs from and improves upon existing methods.
* Some of the technical aspects, such as the generalization error of $f$ and $f/\rho$, could be clarified further.
* The experimental setup could be more thoroughly described, including the choice of hyperparameters and the specific implementation details.
In terms of the conference guidelines, the paper addresses the criteria of quality, clarity, originality, and significance. The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The writing is clear, and the organization is logical and easy to follow. The proposed algorithm is novel and addresses an important problem in machine learning. The results have the potential to impact the field, as they provide a new approach to achieving sparse solutions with comparable performance to existing methods.
Arguments for acceptance:
* The paper proposes a novel algorithm that addresses an important problem in machine learning.
* The authors provide a thorough analysis of the convergence behavior of the algorithm.
* The experimental results demonstrate the effectiveness of the proposed algorithm.
Arguments against acceptance:
* The paper builds upon existing work, and the authors could provide more context on how their work differs from and improves upon existing methods.
* Some of the technical aspects could be clarified further.
* The experimental setup could be more thoroughly described.
Overall, I would recommend accepting the paper, as it proposes a novel algorithm with a thorough analysis and demonstrates its effectiveness in achieving sparse solutions with comparable performance to existing methods. However, the authors should address the areas for improvement mentioned above to strengthen the paper.