This paper presents a novel framework for online multi-task learning with a shared annotator, where multiple learners share a single annotator with limited bandwidth. The proposed algorithm, SHAMPO, learns multiple tasks in parallel with a single annotated sample per round, using a trade-off between exploitation and exploration to minimize mistakes. The paper provides a thorough analysis of the algorithm, including an upper bound on the expected cumulative number of mistakes, and demonstrates its effectiveness through empirical evaluations on various datasets.
The paper is well-written, and the authors provide a clear motivation for the problem and a detailed description of the algorithm and its analysis. The experimental results show that SHAMPO outperforms other algorithms, including uniform allocation and exploit, in terms of average test error and score. The paper also explores the application of SHAMPO to contextual bandits settings, demonstrating its flexibility and potential for adaptation to different problem domains.
One of the strengths of the paper is its ability to balance exploration and exploitation, allowing the algorithm to focus on harder instances and improve overall performance. The use of a prior distribution over tasks is also a notable aspect, as it enables the algorithm to adapt to different task difficulties and improve results.
However, there are some areas that could be improved. The paper assumes a strict synchronic assumption on annotation choice, which may not always be realistic. The authors could consider relaxing this assumption and exploring alternative scenarios, such as assuming a total budget on annotation among all tasks. Additionally, the paper could benefit from more comparisons to other algorithms, including active learning approaches applied to each task separately with a controlled number of annotations.
Overall, the paper presents a significant contribution to the field of multi-task learning and online learning, and its results have the potential to impact various applications, including natural language processing, computer vision, and recommender systems. With some revisions to address the areas mentioned above, the paper could be even stronger and more convincing.
Arguments pro acceptance:
* The paper presents a novel and well-motivated framework for online multi-task learning with a shared annotator.
* The proposed algorithm, SHAMPO, is thoroughly analyzed and demonstrated to be effective through empirical evaluations.
* The paper explores the application of SHAMPO to contextual bandits settings, demonstrating its flexibility and potential for adaptation to different problem domains.
* The use of a prior distribution over tasks is a notable aspect, enabling the algorithm to adapt to different task difficulties and improve results.
Arguments con acceptance:
* The paper assumes a strict synchronic assumption on annotation choice, which may not always be realistic.
* The paper could benefit from more comparisons to other algorithms, including active learning approaches applied to each task separately with a controlled number of annotations.
* Some areas of the paper, such as the analysis and experimental results, could be further clarified and detailed.