This paper introduces a novel application of stochastic variational inference (SVI) to hidden Markov models (HMMs) in a time-dependent data setting. The authors propose an algorithm, SVIHMM, which adaptively bounds errors arising from edge effects in the subchain sampling process. The paper demonstrates the effectiveness of SVIHMM on synthetic experiments and a large genomics dataset, where batch inference is computationally infeasible.
The paper's strengths include its originality, as it extends SVI to HMMs with time-dependent data, and its potential impact on scalable Bayesian inference for large datasets. The authors provide a clear and well-structured presentation of their algorithm and its theoretical guarantees. The experimental results demonstrate the effectiveness of SVIHMM in achieving comparable performance to batch VB while significantly reducing computational time.
However, there are some weaknesses and areas for improvement. The paper lacks a constraint for expressions with lower computational complexity, which could limit the search space to more efficient solutions. The restrictions mentioned in Section 2 are not fully justified, and the authors could provide more empirical or theoretical evidence to support these restrictions. Additionally, the paper's structure is sometimes difficult to follow, with stages not clearly explained, making it hard to understand the methodology.
The paper also contains several typos that need to be corrected. Furthermore, there is a concern about the constraint for the size of the vector/matrix, with potential issues for certain values of k and n.
In terms of the conference guidelines, the paper meets the criteria for quality, as it is technically sound and provides a complete piece of work. The claims are well-supported by theoretical analysis and experimental results. The paper also demonstrates clarity, as it is well-organized and provides enough information for the expert reader to reproduce the results.
The paper's originality is also a strong point, as it introduces a novel application of SVI to HMMs. The significance of the paper is evident, as it addresses a difficult problem in scalable Bayesian inference and provides a unique solution. The paper provides a unique theoretical or pragmatic approach, and its results have the potential to be used by practitioners or researchers in the field.
Arguments for acceptance:
* The paper introduces a novel application of SVI to HMMs, which has the potential to impact scalable Bayesian inference for large datasets.
* The authors provide a clear and well-structured presentation of their algorithm and its theoretical guarantees.
* The experimental results demonstrate the effectiveness of SVIHMM in achieving comparable performance to batch VB while significantly reducing computational time.
Arguments against acceptance:
* The paper lacks a constraint for expressions with lower computational complexity, which could limit the search space to more efficient solutions.
* The restrictions mentioned in Section 2 are not fully justified, and the authors could provide more empirical or theoretical evidence to support these restrictions.
* The paper contains several typos that need to be corrected, and there is a concern about the constraint for the size of the vector/matrix.
Overall, I recommend accepting the paper, as its strengths outweigh its weaknesses. The paper provides a novel and significant contribution to the field of scalable Bayesian inference, and its results have the potential to be used by practitioners or researchers in the field.