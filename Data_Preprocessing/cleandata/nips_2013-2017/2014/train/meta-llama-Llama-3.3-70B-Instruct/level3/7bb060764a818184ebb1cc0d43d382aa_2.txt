This paper proposes a multi-class extension of the Deep Boosting framework, which improves upon the standard multi-class generalization bound of Koltchinskii and Panchenko. The authors introduce a novel data-dependent learning guarantee for convex ensembles in the multi-class setting, which depends on the Rademacher complexity of the hypothesis classes. This bound is finer than existing ones, with an improved dependency on the number of classes and a more favorable complexity term.
The proposed algorithm is a coordinate-descent style algorithm that minimizes an upper bound of the generalization error. The optimization problem is well-motivated by the improved generalization bound, and its formulation naturally implies a coordinate-descent algorithm to solve it. The experimental results show that the proposed method outperforms standard multi-class boosting algorithms, including AdaBoost.MR and multinomial logistic regression, as well as their L1-regularized variants.
The strengths of the paper include the theoretical contribution, which is sufficient for the NIPS community, and the new experimental results, which demonstrate the effectiveness of the proposed method. The paper is well-written, and the authors provide a clear and detailed explanation of the proposed algorithm and its theoretical guarantees.
However, there are some weaknesses to the paper. The formulation of the algorithm is "primal", and exploring a "dual" view of boosting could deepen the understanding of the algorithm and lead to further investigations. Additionally, using validation sets to optimize parameters could further improve the results.
Overall, the paper makes a significant contribution to the field of ensemble learning, and the proposed algorithm has the potential to be widely adopted. The experimental results are promising, and the theoretical guarantees provide a solid foundation for the method.
Arguments pro acceptance:
* The paper proposes a novel and significant extension of the Deep Boosting framework to the multi-class setting.
* The theoretical contribution is sufficient for the NIPS community, and the experimental results demonstrate the effectiveness of the proposed method.
* The paper is well-written, and the authors provide a clear and detailed explanation of the proposed algorithm and its theoretical guarantees.
Arguments con acceptance:
* The formulation of the algorithm is "primal", and exploring a "dual" view of boosting could deepen the understanding of the algorithm and lead to further investigations.
* Using validation sets to optimize parameters could further improve the results.
* The paper could benefit from additional experimental results and comparisons to other state-of-the-art methods.