This paper proposes a novel approach to policy search in reinforcement learning, which combines the strengths of model-based and model-free methods. The authors introduce a hybrid method that fits local, time-varying linear dynamics models to optimize trajectory distributions, and then uses guided policy search to learn general parameterized policies. The approach is shown to be effective in learning complex neural network policies that can solve challenging tasks, such as robotic manipulation and locomotion, in partially observed environments.
The paper is well-written and clearly explains the technical details of the approach. The authors provide a thorough analysis of the method, including its strengths and weaknesses, and demonstrate its effectiveness through extensive experiments. The results show that the proposed method outperforms prior model-free and model-based methods in terms of sample efficiency and policy quality.
One of the key strengths of the paper is its ability to handle complex, discontinuous dynamics, which is a common challenge in robotics and other domains. The authors demonstrate that their method can learn effective policies even in the presence of contacts and discontinuities, which is a significant advantage over prior methods.
The paper also makes a significant contribution to the field of guided policy search, which is a popular approach to policy learning. The authors show that their method can be used to learn complex policies with thousands of parameters, which is a challenging task for prior methods.
However, there are some limitations to the paper. One of the main limitations is that the method relies on a strong assumption about the local linearity of the dynamics, which may not always hold in practice. Additionally, the method requires a significant amount of computation and memory to store and update the local linear models, which can be a challenge for large-scale problems.
Overall, the paper is well-written and makes a significant contribution to the field of reinforcement learning. The proposed method is effective in learning complex policies and handling challenging dynamics, and has the potential to be applied to a wide range of domains.
Quality: 8/10
The paper is technically sound and well-written, with clear explanations of the approach and its strengths and weaknesses. The experiments are extensive and demonstrate the effectiveness of the method.
Clarity: 9/10
The paper is well-organized and easy to follow, with clear headings and concise language.
Originality: 8/10
The paper proposes a novel approach to policy search, which combines the strengths of model-based and model-free methods. While the idea of using local linear models is not new, the authors' approach to guided policy search is innovative and effective.
Significance: 9/10
The paper makes a significant contribution to the field of reinforcement learning, with potential applications to a wide range of domains, including robotics, autonomous vehicles, and healthcare.
Arguments for acceptance:
* The paper proposes a novel and effective approach to policy search, which combines the strengths of model-based and model-free methods.
* The method is shown to be effective in learning complex policies and handling challenging dynamics.
* The paper makes a significant contribution to the field of guided policy search.
Arguments for rejection:
* The method relies on a strong assumption about the local linearity of the dynamics, which may not always hold in practice.
* The method requires a significant amount of computation and memory to store and update the local linear models, which can be a challenge for large-scale problems.