This paper proposes a novel approach to policy search by combining trajectory optimization with guided policy search, allowing for the learning of arbitrary parameterized policies under unknown dynamics. The algorithm iteratively fits local linear dynamics models and uses a background dynamics distribution as a prior to reduce sample complexity. The method is evaluated on several simulated robotic tasks, including peg insertion, octopus arm control, swimming, and walking, and demonstrates improved performance over prior model-free and model-based methods.
The paper is well-written and clearly explains the algorithm and its components. The use of a KL-divergence constraint during trajectory optimization is a key innovation, allowing the method to handle complex and discontinuous dynamics. The evaluation demonstrates the effectiveness of the approach in learning complex neural network policies that can act intelligently in partially observed environments.
The strengths of the paper include its novelty, clarity, and thorough evaluation. The algorithm is well-motivated and clearly explained, and the experiments demonstrate its effectiveness in a range of tasks. The use of a background dynamics distribution as a prior is a clever idea, and the evaluation shows that it can significantly reduce sample complexity.
The weaknesses of the paper include the fact that the method is designed specifically for linear-Gaussian controllers, which may limit its applicability to other types of policies. Additionally, the evaluation could be more comprehensive, with more comparison to other state-of-the-art methods and more analysis of the method's limitations.
Overall, the paper is of high quality and makes a significant contribution to the field of policy search. The algorithm is novel and well-motivated, and the evaluation demonstrates its effectiveness in a range of tasks. The paper is well-written and clearly explains the algorithm and its components, making it easy to follow and understand.
Arguments for acceptance:
* The paper proposes a novel approach to policy search that combines trajectory optimization with guided policy search.
* The algorithm is well-motivated and clearly explained, and the evaluation demonstrates its effectiveness in a range of tasks.
* The use of a background dynamics distribution as a prior is a clever idea, and the evaluation shows that it can significantly reduce sample complexity.
Arguments against acceptance:
* The method is designed specifically for linear-Gaussian controllers, which may limit its applicability to other types of policies.
* The evaluation could be more comprehensive, with more comparison to other state-of-the-art methods and more analysis of the method's limitations.
Recommendation: Accept. The paper makes a significant contribution to the field of policy search, and the algorithm is novel and well-motivated. The evaluation demonstrates its effectiveness in a range of tasks, and the paper is well-written and clearly explains the algorithm and its components. While there are some limitations to the method, the paper is of high quality and deserves to be accepted.