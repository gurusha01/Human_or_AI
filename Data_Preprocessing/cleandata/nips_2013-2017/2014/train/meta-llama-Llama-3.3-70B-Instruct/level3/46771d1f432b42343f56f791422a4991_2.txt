This paper explores the application of stochastic variational inference (SVI) to hidden Markov models (HMMs) in a time-dependent data setting. The authors propose an algorithm, SVIHMM, which extends SVI to handle the dependencies between observations in HMMs. The key idea is to use subchains of consecutive observations as the basic sampling unit, rather than individual observations, to capture the transition structure of the HMM.
The paper is well-written and clearly explains the challenges of applying SVI to HMMs, including the need to break dependencies between observations and the introduction of error in the local step. The authors propose a scheme to mitigate this error by adaptively buffering the subchains with extra observations, which is related to splash belief propagation. The algorithm is shown to converge to a local mode of the batch objective, and empirical results demonstrate its effectiveness on synthetic experiments and a large genomics dataset.
The strengths of the paper include its clear explanation of the challenges and limitations of applying SVI to HMMs, its proposal of a novel algorithm to address these challenges, and its empirical evaluation on a range of datasets. The paper also provides a thorough discussion of the related work and the potential extensions of the algorithm to other settings.
However, there are some weaknesses to the paper. The assumption of a high level of prior knowledge from readers may make the paper difficult to follow for those without a strong background in variational inference and HMMs. Additionally, the paper could benefit from more detailed explanations of some of the key concepts, such as the batch factor and the GrowBuf routine.
In terms of originality, the paper makes a significant contribution to the field by proposing a novel algorithm for applying SVI to HMMs. The idea of using subchains as the basic sampling unit and adaptively buffering them to mitigate error is new and interesting. The paper also provides a thorough evaluation of the algorithm on a range of datasets, which demonstrates its effectiveness and potential for application to real-world problems.
The significance of the paper lies in its potential to enable the application of SVI to large-scale time-dependent data, which is a common challenge in many fields. The algorithm proposed in the paper could be used in a range of applications, including speech recognition, natural language processing, and bioinformatics.
Overall, I would recommend accepting this paper for publication. It makes a significant contribution to the field, is well-written and clearly explained, and has the potential to enable the application of SVI to large-scale time-dependent data.
Arguments for acceptance:
* The paper proposes a novel algorithm for applying SVI to HMMs, which addresses the challenges of breaking dependencies between observations and introducing error in the local step.
* The algorithm is shown to converge to a local mode of the batch objective, and empirical results demonstrate its effectiveness on a range of datasets.
* The paper provides a thorough discussion of the related work and the potential extensions of the algorithm to other settings.
* The algorithm has the potential to enable the application of SVI to large-scale time-dependent data, which is a common challenge in many fields.
Arguments against acceptance:
* The paper assumes a high level of prior knowledge from readers, which may make it difficult to follow for those without a strong background in variational inference and HMMs.
* The paper could benefit from more detailed explanations of some of the key concepts, such as the batch factor and the GrowBuf routine.
* The empirical evaluation of the algorithm could be more comprehensive, including a comparison to other state-of-the-art methods and a more detailed analysis of the results.