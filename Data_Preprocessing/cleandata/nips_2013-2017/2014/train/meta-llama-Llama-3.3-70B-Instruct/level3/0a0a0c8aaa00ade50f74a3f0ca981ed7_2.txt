This paper proposes a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints, overcoming the limitations of previous models. The authors design a rate-based neuronal network to hold stable activity patterns, or memories, by optimizing network parameters to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The network operates in the balanced regime, is robust to corruptions of the memory cue as well as to ongoing noise, and explains the reduction of trial-to-trial variability following stimulus onset.
The paper is well-written, making the model, training procedure, and results easy to understand. The combination of biological constraints and resulting features makes it an interesting contribution to the field of attractor neural networks. The authors provide a clear and detailed explanation of the optimization procedure, including the use of the Smoothed Spectral Abscissa (SSA) to perform robust stability optimization.
The results show that the network can store and recall multiple analog memories, with a high degree of robustness to noise and corruption of the recall cues. The network also exhibits a balanced state, where excitation and inhibition are tightly balanced, allowing neurons to fire at relatively low rates during recall. The authors also demonstrate that the network can reduce trial-to-trial variability following stimulus onset, a phenomenon observed in sensory and motor cortical areas.
The strengths of the paper include its ability to overcome the limitations of previous models, its robustness to noise and corruption, and its ability to explain experimental phenomena. The weaknesses of the paper include the fact that it is a rate-based model, which may not be directly applicable to spiking networks, and the need for further research on how local synaptic learning rules can achieve the stabilization objective.
In terms of the conference guidelines, the paper scores well on quality, clarity, and significance. The paper is technically sound, with well-supported claims and a clear explanation of the methods and results. The paper is also well-organized and easy to follow, making it accessible to a wide range of readers. The significance of the paper lies in its ability to provide a novel framework for understanding attractor neural networks, and its potential to explain experimental phenomena in a more realistic and robust way.
Arguments pro acceptance:
* The paper proposes a novel and robust framework for building attractor neural networks that satisfy physiological constraints.
* The paper provides a clear and detailed explanation of the optimization procedure and the results.
* The paper demonstrates the ability of the network to store and recall multiple analog memories, with a high degree of robustness to noise and corruption.
* The paper explains experimental phenomena, such as the reduction of trial-to-trial variability following stimulus onset.
Arguments con acceptance:
* The paper is a rate-based model, which may not be directly applicable to spiking networks.
* The paper requires further research on how local synaptic learning rules can achieve the stabilization objective.
* The paper may not be directly comparable to other models in the field, due to its novel framework and approach.