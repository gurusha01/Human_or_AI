This paper introduces the concept of pseudo-ensembles, a collection of child models spawned from a parent model by perturbing it with some noise process. The authors propose a novel regularizer, Pseudo-Ensemble Agreement (PEA) regularization, which encourages the robustness of learned models under perturbation of the model space. The PEA regularization generalizes to semi-supervised learning setup and shows improvement over baseline methods in supervised, semi-supervised, and transfer learning scenarios.
The paper is well-written, and the authors provide a clear explanation of the pseudo-ensemble framework and its relationship to standard ensemble methods and existing notions of robustness. The experiments demonstrate the effectiveness of PEA regularization in various settings, including fully-supervised MNIST, semi-supervised MNIST, and transfer learning on a dataset from the NIPS 2011 Workshop on Challenges in Learning Hierarchical Models.
The strengths of the paper include:
* The introduction of a novel concept, pseudo-ensembles, which provides a unified framework for understanding various regularization techniques, including dropout.
* The proposal of a new regularizer, PEA regularization, which is shown to be effective in various settings.
* The thorough experimentation and comparison with baseline methods, which demonstrates the effectiveness of PEA regularization.
However, there are some weaknesses and areas for improvement:
* The connection between PEA regularization and dropout is not clearly explained, and the reasoning behind discouraging co-adaptation as the reason for success is not convincing.
* The formulation of the PEA regularization could be improved by penalizing the variance of the output layer distribution instead of summing over all layers.
* The effectiveness of the PEA regularization in utilizing unlabeled data is questionable, as DAE pre-training still significantly improves performance even with PEA regularization.
Overall, the paper is well-written, and the authors provide a clear explanation of the pseudo-ensemble framework and its relationship to standard ensemble methods and existing notions of robustness. The experiments demonstrate the effectiveness of PEA regularization in various settings, and the paper provides a significant contribution to the field of machine learning.
Arguments pro acceptance:
* The paper introduces a novel concept, pseudo-ensembles, which provides a unified framework for understanding various regularization techniques.
* The proposal of a new regularizer, PEA regularization, which is shown to be effective in various settings.
* The thorough experimentation and comparison with baseline methods, which demonstrates the effectiveness of PEA regularization.
Arguments con acceptance:
* The connection between PEA regularization and dropout is not clearly explained.
* The formulation of the PEA regularization could be improved.
* The effectiveness of the PEA regularization in utilizing unlabeled data is questionable.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-written, and provides a clear explanation of the pseudo-ensemble framework and its relationship to standard ensemble methods and existing notions of robustness. The experiments demonstrate the effectiveness of PEA regularization in various settings, and the paper provides a significant contribution to the field of machine learning.