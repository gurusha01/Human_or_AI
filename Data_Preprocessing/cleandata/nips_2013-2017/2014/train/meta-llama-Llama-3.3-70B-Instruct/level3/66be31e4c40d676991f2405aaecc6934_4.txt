This paper proposes a novel regularization technique for neural networks that reduces the variance of hidden layer representations over dropout noise. The technique is generalized to "pseudo-ensemble" models, where other types of perturbations can be used, and experiments show that it works as well as or better than using perturbations alone. The paper's strengths include its promising results in low-labeled data situations and well-chosen experiments that highlight its applicability to different models and datasets.
The paper's weaknesses include the presence of superfluous sections and a lack of crucial details about the experiments, such as the noise samples used and the back-propagation methods employed. The authors should discuss and clarify several aspects, including the computation of variances, back-propagation, training speed, and stopping criteria for semi-supervised experiments.
The paper's quality, clarity, and originality are good, but can be improved with additional explanations and comparisons. The significance of the paper lies in its potential impact on people working with small datasets. The proposed technique has the potential to improve the performance of neural networks in low-data regimes, which is a significant challenge in many real-world applications.
Arguments for acceptance include:
* The paper proposes a novel and promising technique for reducing variance in neural networks
* The technique is generalized to "pseudo-ensemble" models, which increases its applicability
* The experiments are well-chosen and demonstrate the effectiveness of the technique
* The paper has the potential to impact people working with small datasets
Arguments against acceptance include:
* The paper lacks crucial details about the experiments, which makes it difficult to reproduce the results
* The presence of superfluous sections detracts from the overall quality of the paper
* The paper could benefit from additional explanations and comparisons to improve its clarity and originality
Overall, I recommend accepting the paper, but with revisions to address the weaknesses mentioned above. The authors should provide more details about the experiments, remove superfluous sections, and clarify several aspects of the technique to improve the paper's quality, clarity, and originality.