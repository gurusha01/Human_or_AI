This paper proposes a novel algorithm, Sparse Random Features, which learns a sparse non-linear predictor by minimizing an l1-regularized objective function over the Hilbert Space induced from a kernel function. The algorithm can be seen as a Randomized Coordinate Descent in an infinite-dimensional space, allowing for more efficient solutions to large-scale problems. The paper is well-written and easy to read, with significant work that will enable practical large-scale problem-solving more efficiently than current kernel methods.
The strengths of the paper include its originality, as it introduces a new interpretation of Random Features that justifies its usage with l1-regularization. The algorithm is also shown to converge to a solution within ε-precision of that using an exact kernel method, by drawing O(1/ε) random features. The experiments demonstrate the effectiveness of the Sparse Random Features algorithm in obtaining a sparse solution that requires less memory and prediction time, while maintaining comparable performance on regression and classification tasks.
However, there are some weaknesses in the paper. The presentation and analysis of experiments and results need improvement, including clarification on cross-validation and discussion of noticeable drops in accuracy. Additionally, the paper requires proofreading to correct some sentences and would benefit from adding conclusions and future work directions to enhance its overall quality.
In terms of quality, the paper is technically sound, with claims well-supported by theoretical analysis and experimental results. The algorithm is carefully designed, and the convergence analysis is thorough. The paper is also clear and well-organized, making it easy to follow.
The originality of the paper is high, as it introduces a new algorithm and provides a novel interpretation of Random Features. The significance of the paper is also high, as it addresses a difficult problem in kernel methods and provides a more efficient solution.
Overall, I would recommend accepting this paper, as it makes a significant contribution to the field of kernel methods and has the potential to impact practical large-scale problem-solving. However, I would suggest that the authors address the weaknesses mentioned above to improve the overall quality of the paper.
Arguments pro acceptance:
* The paper introduces a novel algorithm that addresses a difficult problem in kernel methods.
* The algorithm is shown to converge to a solution within ε-precision of that using an exact kernel method.
* The experiments demonstrate the effectiveness of the Sparse Random Features algorithm in obtaining a sparse solution that requires less memory and prediction time.
* The paper is well-written and easy to read.
Arguments con acceptance:
* The presentation and analysis of experiments and results need improvement.
* The paper requires proofreading to correct some sentences.
* The paper would benefit from adding conclusions and future work directions to enhance its overall quality.