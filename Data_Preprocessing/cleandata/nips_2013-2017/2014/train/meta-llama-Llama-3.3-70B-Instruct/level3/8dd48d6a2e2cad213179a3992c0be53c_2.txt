This paper proposes a novel approach to policy search in reinforcement learning, combining the strengths of model-based and model-free methods. The authors introduce a hybrid method that fits local, time-varying linear dynamics models to optimize trajectory distributions, which can be used within the framework of guided policy search to learn policies with arbitrary parameterization. The key idea is to restrict the change in the trajectory distribution at each iteration, ensuring that the time-varying linear model remains valid under the new distribution.
The paper is well-structured, and the authors provide a clear and concise introduction to the problem, related work, and their approach. The technical sections are detailed and easy to follow, with a good balance between mathematical derivations and intuitive explanations. The experimental evaluation is thorough, covering a range of tasks, including simulated robotic manipulation and locomotion, and demonstrating the effectiveness of the proposed method in learning complex policies.
The strengths of the paper include:
* The proposed method combines the sample efficiency of model-based methods with the versatility of model-free techniques, allowing it to handle complex and discontinuous dynamics.
* The use of local linear models enables the method to outperform model-free policy search methods, while the learned models are highly local and time-varying, in contrast to model-based methods that rely on learning an effective global model.
* The experimental evaluation demonstrates the effectiveness of the proposed method in learning complex policies, including neural network policies that can act intelligently in partially observed environments.
However, there are some weaknesses and areas for improvement:
* The tuning of regularization strength and margin parameters is a key issue, and the current ad hoc approach may not be optimal. A more systematic approach, such as cross-validated tuning, may be necessary.
* The choice of parameter settings, particularly for topic modeling, seems informed but unclear, and should be clarified or better accounted for to ensure the approach's performance is not highly dependent on these choices.
* Minor issues include unclear figure notation, typographical errors, and grammatical corrections that need to be addressed for clarity and readability.
Overall, the paper presents a significant contribution to the field of reinforcement learning, and the proposed method has the potential to be widely applicable to a range of tasks. With some revisions to address the weaknesses and areas for improvement, the paper could be even stronger.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of reinforcement learning.
* The proposed method combines the strengths of model-based and model-free methods, allowing it to handle complex and discontinuous dynamics.
* The experimental evaluation is thorough and demonstrates the effectiveness of the proposed method in learning complex policies.
Arguments con acceptance:
* The tuning of regularization strength and margin parameters is a key issue, and the current ad hoc approach may not be optimal.
* The choice of parameter settings, particularly for topic modeling, seems informed but unclear, and should be clarified or better accounted for.
* Minor issues, such as unclear figure notation and typographical errors, need to be addressed for clarity and readability.