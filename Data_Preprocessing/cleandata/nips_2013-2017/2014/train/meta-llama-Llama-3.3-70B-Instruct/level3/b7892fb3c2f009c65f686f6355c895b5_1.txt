This paper proposes a novel control-theoretic framework for building functioning attractor networks that satisfy key physiological constraints, including Dale's law and the presence of recurrent and sparse synaptic connections. The authors optimize network parameters to embed multiple analog memories as stable fixed points of the dynamics, using a combination of fixed point and stability conditions. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue and ongoing noise, and explain the reduction of trial-to-trial variability following stimulus onset.
The paper is well-written and clearly explains the background, methods, and results. The authors provide a detailed description of the optimization procedure, including the use of the Smoothed Spectral Abscissa (SSA) to perform robust stability optimization. The results are impressive, with the network demonstrating successful recall of multiple analog memories despite ongoing noise and corruption of the recall cues.
One of the strengths of the paper is its ability to overcome the limitations of previous models, including the violation of Dale's law and the restriction of memories to a binary format. The authors also provide a clear explanation of the differences between their model and previous work, including the use of a rate-based formulation of the circuit dynamics and the incorporation of inhibitory neurons.
However, there are some areas where the paper could be improved. For example, the authors could provide more empirical assessment and intuition beyond prediction errors and timing, to better understand the model's properties and differences to related models. Additionally, the paper could benefit from more experimental validation, particularly in terms of the model's ability to generalize to new, unseen data.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and originality. The paper is technically sound, with well-supported claims and a clear explanation of the methods and results. The writing is clear and well-organized, making it easy to follow and understand. The paper also presents a novel combination of familiar techniques, including the use of control theory and optimization methods to build attractor networks.
The significance of the paper is also high, as it addresses a difficult problem in a better way than previous research. The paper provides a unique theoretical approach to understanding the neural substrate of memory, and the results have important implications for our understanding of how memories are stored and retrieved in the brain.
Overall, I would recommend accepting this paper for publication, with some minor revisions to address the areas mentioned above. The paper makes a significant contribution to the field and has the potential to advance our understanding of the neural mechanisms of memory.
Arguments pro acceptance:
* The paper presents a novel and innovative approach to building attractor networks that satisfy key physiological constraints.
* The results are impressive, with the network demonstrating successful recall of multiple analog memories despite ongoing noise and corruption of the recall cues.
* The paper provides a clear explanation of the differences between the model and previous work, and the authors take care to relate their work to previous research in the field.
* The paper meets the criteria for quality, clarity, and originality, and has the potential to advance our understanding of the neural mechanisms of memory.
Arguments con acceptance:
* The paper could benefit from more empirical assessment and intuition beyond prediction errors and timing, to better understand the model's properties and differences to related models.
* The paper could benefit from more experimental validation, particularly in terms of the model's ability to generalize to new, unseen data.
* Some of the technical details, such as the use of the SSA, may be difficult for non-experts to follow, and could be clarified or simplified for a broader audience.