This paper proposes a stochastic variational inference (SVI) algorithm for learning the parameters of hidden Markov models (HMMs) in a time-dependent data setting. The algorithm, referred to as SVIHMM, extends the traditional SVI approach to handle dependent observations by considering subchains of consecutive observations as the basic sampling unit. The authors address the challenges of applying SVI to HMMs, including the need to break dependencies between observations and the introduction of error due to edge effects.
The paper provides a clear and well-structured presentation of the SVIHMM algorithm, including the derivation of the global and local updates, as well as the introduction of a buffering scheme to mitigate edge effects. The authors also provide a thorough analysis of the algorithm's convergence properties and demonstrate its effectiveness on synthetic experiments and a large genomics dataset.
The strengths of the paper include:
* The proposal of a novel SVI algorithm for HMMs that can handle dependent observations and large datasets.
* The introduction of a buffering scheme to mitigate edge effects, which is shown to improve the algorithm's performance.
* The provision of a thorough analysis of the algorithm's convergence properties and empirical evaluation on synthetic and real-world datasets.
However, there are some weaknesses and areas for improvement:
* The paper assumes a product distribution of each coordinate and a known covariance matrix, which may limit its applicability to more complex models.
* The notation is not always consistent between the main paper and the supplementary file, which can make it difficult to follow.
* The introduction is lengthy and could be condensed to provide a clearer overview of the paper's contributions.
* The paper could benefit from a more detailed comparison with existing methods, such as batch variational Bayes and other stochastic inference algorithms.
In terms of the conference guidelines, the paper addresses the key criteria of quality, clarity, originality, and significance. The paper is technically sound, well-written, and provides a clear presentation of the algorithm and its analysis. The proposal of a novel SVI algorithm for HMMs and the introduction of a buffering scheme to mitigate edge effects demonstrate originality and significance. However, the paper could benefit from some revisions to address the areas for improvement mentioned above.
Arguments for acceptance:
* The paper proposes a novel and effective SVI algorithm for HMMs that can handle dependent observations and large datasets.
* The introduction of a buffering scheme to mitigate edge effects is a significant contribution to the field.
* The paper provides a thorough analysis of the algorithm's convergence properties and empirical evaluation on synthetic and real-world datasets.
Arguments against acceptance:
* The paper assumes a product distribution of each coordinate and a known covariance matrix, which may limit its applicability to more complex models.
* The notation is not always consistent between the main paper and the supplementary file, which can make it difficult to follow.
* The introduction is lengthy and could be condensed to provide a clearer overview of the paper's contributions.
Overall, I recommend accepting the paper with minor revisions to address the areas for improvement mentioned above.