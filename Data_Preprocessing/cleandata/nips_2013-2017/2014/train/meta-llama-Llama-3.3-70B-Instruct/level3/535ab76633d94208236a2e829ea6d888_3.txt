This paper proposes a novel Sparse Random Features algorithm for learning sparse non-linear predictors in kernel methods. The algorithm is based on a randomized coordinate descent approach in the Hilbert space induced by a kernel function, and it is shown to converge to a solution within Îµ-precision of the exact kernel method. The paper also establishes bounds on the minimum damage caused by smart adversaries and provides a comparison with the Boosting approach.
The strengths of the paper include:
* The proposal of a novel algorithm that combines the benefits of random features and sparse learning
* The provision of theoretical guarantees for the algorithm's convergence and performance
* The comparison with existing methods, including the Boosting approach, and the demonstration of the algorithm's advantages
However, there are also some weaknesses:
* The paper relies on strong assumptions, such as the honesty of users and the knowledge of voting patterns, which may not be realistic in practice
* The algorithm's performance may be limited by the choice of kernel and the number of random features used
* The paper could benefit from more extensive experimental evaluations and comparisons with other state-of-the-art methods
In terms of quality, the paper is well-written and clearly presents the proposed algorithm and its theoretical guarantees. The experiments are also well-designed and provide useful insights into the algorithm's performance. However, the paper could benefit from more detailed discussions of the limitations and potential extensions of the proposed approach.
The clarity of the paper is good, with clear explanations of the proposed algorithm and its components. The organization is also logical, with a clear introduction, problem setup, and presentation of the algorithm and its theoretical guarantees.
The originality of the paper is high, as it proposes a novel algorithm that combines random features and sparse learning. The significance of the paper is also high, as it provides a new approach to learning sparse non-linear predictors in kernel methods.
Overall, I would recommend accepting this paper, as it provides a novel and well-motivated approach to learning sparse non-linear predictors in kernel methods. However, the authors should address the limitations and potential extensions of the proposed approach in future work.
Arguments for acceptance:
* The paper proposes a novel and well-motivated algorithm that combines random features and sparse learning
* The paper provides theoretical guarantees for the algorithm's convergence and performance
* The experiments demonstrate the algorithm's advantages over existing methods
Arguments against acceptance:
* The paper relies on strong assumptions that may not be realistic in practice
* The algorithm's performance may be limited by the choice of kernel and the number of random features used
* The paper could benefit from more extensive experimental evaluations and comparisons with other state-of-the-art methods.