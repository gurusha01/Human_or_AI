This paper presents new ensemble learning algorithms for multi-class classification, building upon the work of Cortes, Mohri, and Syed (2014) in the binary setting. The authors introduce a novel data-dependent learning guarantee for convex ensembles in the multi-class setting, which generalizes the previous result and provides a finer analysis related to Rademacher complexity. The paper also discusses several new multi-class ensemble algorithms, including DeepBoost, and proves positive results for their H-consistency and convergence.
The technical quality of the paper is high, with clear and interpretable results. The authors provide a thorough analysis of the learning guarantees and algorithms, and the experiments demonstrate the effectiveness of the proposed methods. The paper is well-organized, and the writing is generally clear, although some sections, such as Section 3, are dense and require careful reading.
One of the main strengths of the paper is its ability to provide a unified framework for multi-class classification, which can handle complex base classifier sets, including deep decision trees. The authors also provide a detailed analysis of the connections between their algorithms and other boosting methods, such as AdaBoost.MR and logistic regression.
However, the paper has some weaknesses. Some results, such as the learning guarantees, are incremental improvements over previous work, and it is not entirely clear how they contribute to a bigger picture. Additionally, the notation could be improved, as the use of the same capital 'V' to denote different objects can be confusing. The paper could also benefit from a more general approach, working out what is going on in the general case and coming up with a complete criterion for identifiability.
There are also some errors and inconsistencies in the paper, such as incorrect language, undefined notation, and missing definitions. Furthermore, the paper has some formatting issues, such as the use of incorrect symbols and unclear subscripts.
In terms of significance, the paper addresses an important problem in machine learning, and the results have the potential to be useful in practice. The authors demonstrate the effectiveness of their methods on several datasets, and the results are competitive with state-of-the-art algorithms.
Overall, I would recommend accepting this paper, as it presents a significant contribution to the field of machine learning, and the results have the potential to be useful in practice. However, the authors should address the weaknesses and errors mentioned above to improve the clarity and quality of the paper.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of machine learning.
* The results have the potential to be useful in practice.
* The authors provide a thorough analysis of the learning guarantees and algorithms.
* The experiments demonstrate the effectiveness of the proposed methods.
Arguments con acceptance:
* Some results are incremental improvements over previous work.
* The notation could be improved.
* The paper has some errors and inconsistencies.
* The paper could benefit from a more general approach.