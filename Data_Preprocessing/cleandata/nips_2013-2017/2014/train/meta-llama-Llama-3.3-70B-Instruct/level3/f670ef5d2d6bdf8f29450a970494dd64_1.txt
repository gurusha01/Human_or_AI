This paper introduces a novel framework for online multi-task learning with a shared annotator, where multiple learners share a single annotator with limited bandwidth. The authors propose an algorithm, SHAMPO, which learns to select a task to query its label and updates its models accordingly. The algorithm is analyzed in the mistake-bound model, and the authors show that it can perform well compared to methods that observe all annotated data.
The paper is well-written and clear, with a good introduction to the problem setting and a thorough explanation of the algorithm and its analysis. The authors also provide empirical results, which demonstrate the effectiveness of their approach in various tasks and settings.
One of the strengths of the paper is its ability to decouple exploration and exploitation in the contextual bandits setting, which is a novel contribution. The authors also show how their algorithm can be used to solve two bandits problems: contextual bandits and dueling bandits with context.
However, there are some weaknesses in the paper. The numerical results do not clearly show the significance of the algorithm's performance, and the authors could have provided more detailed comparisons with other algorithms. Additionally, the paper could benefit from more discussion on the choice of hyperparameters, such as the tradeoff parameter b and the prior ai.
The paper is technically sound, and the authors provide a thorough analysis of their algorithm. The idea of re-explaining regularization using orbitopes is new, and the continuation algorithm is a novel contribution. However, the paper could benefit from more discussion on the limitations of the approach and potential future directions.
Overall, I would recommend accepting this paper, as it presents a novel and interesting approach to online multi-task learning with a shared annotator. The paper is well-written, and the authors provide a thorough analysis of their algorithm. However, the authors could have provided more detailed comparisons with other algorithms and more discussion on the choice of hyperparameters.
Arguments pro acceptance:
* The paper presents a novel and interesting approach to online multi-task learning with a shared annotator.
* The algorithm is analyzed in the mistake-bound model, and the authors show that it can perform well compared to methods that observe all annotated data.
* The paper provides empirical results, which demonstrate the effectiveness of the approach in various tasks and settings.
* The authors show how their algorithm can be used to solve two bandits problems: contextual bandits and dueling bandits with context.
Arguments con acceptance:
* The numerical results do not clearly show the significance of the algorithm's performance.
* The paper could benefit from more discussion on the choice of hyperparameters, such as the tradeoff parameter b and the prior ai.
* The paper could benefit from more detailed comparisons with other algorithms.
* The authors could have provided more discussion on the limitations of the approach and potential future directions.