This paper presents a novel stochastic variational inference (SVI) algorithm for hidden Markov models (HMMs) in time-dependent data settings. The authors address the challenge of applying SVI to HMMs by proposing an algorithm that harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. The paper is well-written and clearly presents interesting and novel results, with small-scale simulations demonstrating the estimator's performance.
The key strengths of the paper are:
* The authors derive a stochastic variational inference algorithm for HMMs, which is a significant contribution to the field of Bayesian inference.
* The algorithm is shown to converge to a local mode of the batch objective, and empirical results demonstrate similar performance to batch VB in significantly less time on synthetic datasets.
* The paper provides a thorough analysis of the algorithm's performance, including a discussion of the trade-off between subchain length and the number of subchains per minibatch.
However, there are some limitations to the paper:
* The authors do not propose a practical encoding method to achieve their bound, nor do they evaluate existing methods, which limits the paper's practical application.
* The paper's focus on HMMs may limit its applicability to other models, although the authors suggest that their ideas can be generalized to other settings.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is clearly written, and the authors provide sufficient background information to make the paper accessible to a broad audience.
The originality of the paper is high, as it presents a novel algorithm for SVI in HMMs. The paper builds on existing work in SVI and HMMs, but the authors' contribution is significant, and the paper provides a new perspective on the problem.
The significance of the paper is also high, as it addresses a important problem in Bayesian inference and provides a scalable solution for large datasets. The paper has the potential to impact the field of Bayesian inference and machine learning, and the authors' results are likely to be of interest to researchers and practitioners in the field.
Overall, I would recommend accepting this paper for publication, as it presents a significant contribution to the field of Bayesian inference and provides a well-written and clearly presented analysis of the algorithm's performance.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of Bayesian inference.
* The algorithm is shown to converge to a local mode of the batch objective, and empirical results demonstrate similar performance to batch VB in significantly less time on synthetic datasets.
* The paper provides a thorough analysis of the algorithm's performance, including a discussion of the trade-off between subchain length and the number of subchains per minibatch.
Arguments against acceptance:
* The paper's focus on HMMs may limit its applicability to other models.
* The authors do not propose a practical encoding method to achieve their bound, nor do they evaluate existing methods, which limits the paper's practical application.