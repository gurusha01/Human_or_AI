This paper presents a novel approach to robust learning, introducing the concept of pseudo-ensembles, which are collections of child models spawned from a parent model by perturbing it with some noise process. The authors develop a regularizer, Pseudo-Ensemble Agreement (PEA), that minimizes variation in the output of a model when it is subject to noise on its inputs and internal state. The paper provides a thorough analysis of the relationship between pseudo-ensembles and standard ensemble methods, as well as existing notions of robustness.
The strengths of the paper include its sound theoretical foundation, empirical results on both synthetic and real-world datasets, and the introduction of a novel regularizer that outperforms standard dropout in some settings. The experiments demonstrate the effectiveness of PEA regularization in supervised and semi-supervised learning, including state-of-the-art results on the MNIST dataset and a competitive sentiment analysis benchmark.
However, there are some areas for improvement. The experiments could be strengthened by including results of exact methods as a baseline and providing more details on the experiment environment and running time. Additionally, the paper focuses primarily on developing estimators robust to outliers, but could be enhanced by discussing work that leverages data nosing to improve classification or regression. Some typos and errors, such as "more more realistic" and "ideas from $4 and $4", need to be corrected to improve the paper's overall quality.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-written, and well-organized, making it easy to follow and understand. The introduction of pseudo-ensembles and PEA regularization is a novel contribution that advances the state of the art in robust learning. The paper provides sufficient information for the expert reader to reproduce the results, and the experiments demonstrate the significance of the approach.
Arguments pro acceptance:
* The paper introduces a novel concept, pseudo-ensembles, which unifies several rapidly evolving lines of research.
* The PEA regularizer outperforms standard dropout in some settings, demonstrating its effectiveness.
* The paper provides a thorough analysis of the relationship between pseudo-ensembles and standard ensemble methods.
* The experiments demonstrate the significance of the approach, including state-of-the-art results on the MNIST dataset.
Arguments con acceptance:
* The experiments could be strengthened by including results of exact methods as a baseline.
* The paper focuses primarily on developing estimators robust to outliers, but could be enhanced by discussing work that leverages data nosing to improve classification or regression.
* Some typos and errors need to be corrected to improve the paper's overall quality.
Overall, the paper is well-written, technically sound, and makes a significant contribution to the field of robust learning. With some minor revisions to address the areas for improvement, the paper is ready for acceptance.