This paper proposes a novel approach to policy search in reinforcement learning, combining the strengths of model-based and model-free methods. The authors introduce a hybrid method that fits local, time-varying linear dynamics models to optimize trajectory distributions, which can then be used to learn general parameterized policies through guided policy search. The key innovation is the use of a KL-divergence constraint to limit the change in the trajectory distribution at each iteration, allowing the method to handle complex, discontinuous dynamics.
The paper is well-organized and clearly written, making it easy to follow the authors' arguments and understand the technical details. The introduction provides a thorough overview of the background and motivation, while the methodology section presents a clear and concise explanation of the proposed approach. The experimental evaluation is comprehensive, covering a range of tasks, including simulated robotic manipulation and locomotion, and demonstrating the effectiveness of the method in learning complex policies.
The strengths of the paper include:
* The proposed method combines the sample efficiency of model-based methods with the versatility of model-free techniques, allowing it to handle complex, high-dimensional tasks.
* The use of a KL-divergence constraint provides a principled way to limit the change in the trajectory distribution, enabling the method to handle discontinuous dynamics.
* The experimental evaluation is thorough and well-designed, demonstrating the effectiveness of the method in a range of tasks.
However, there are some weaknesses and areas for improvement:
* The method relies on a number of assumptions, such as the use of time-varying linear-Gaussian controllers, which may not always be valid in practice.
* The computational cost of the method is not thoroughly evaluated, which could be a concern for large-scale applications.
* The comparison to other methods is not always fair, as the authors use different numbers of samples and iterations for different methods.
Overall, the paper presents a significant contribution to the field of reinforcement learning, and the proposed method has the potential to be widely applicable in a range of domains. However, further work is needed to fully evaluate the strengths and limitations of the method, and to explore its potential applications in practice.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of reinforcement learning.
* The proposed method combines the strengths of model-based and model-free methods, allowing it to handle complex, high-dimensional tasks.
* The experimental evaluation is comprehensive and well-designed, demonstrating the effectiveness of the method in a range of tasks.
Arguments against acceptance:
* The method relies on a number of assumptions, which may not always be valid in practice.
* The computational cost of the method is not thoroughly evaluated, which could be a concern for large-scale applications.
* The comparison to other methods is not always fair, as the authors use different numbers of samples and iterations for different methods.