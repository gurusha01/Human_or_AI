This paper addresses the problem of proving sub-linear regret bounds in multi-armed bandit with non-stationary mean rewards and finite variation. The authors provide a definitive answer with matching lower and upper bounds, which is a significant contribution to the field. The paper is well-written, easy to follow, and includes enough contribution for a conference submission. The main results and technical proofs are accurate and well-explained.
The proposed algorithm, Rexp3, is a variant of EXP3, which may not be the best problem-dependent strategy. However, the authors demonstrate that Rexp3 can achieve good performance in practice. One potential limitation of Rexp3 is that it requires knowledge of the total variation of mean-rewards, which may be a restrictive requirement. It is unclear if the same regret can be achieved without this knowledge.
The paper lacks a comprehensive discussion of state-dependent ergodic bandits, missing some recent works in the field, such as restless bandits with unknown dynamics and history-dependent reward bandits. This omission may limit the applicability of the results to more general settings.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and provides enough information for the expert reader to reproduce the results.
The originality of the paper lies in its ability to provide a definitive answer to the problem of proving sub-linear regret bounds in multi-armed bandit with non-stationary mean rewards and finite variation. The paper also introduces a new algorithm, Rexp3, which is a variant of EXP3. The significance of the paper lies in its ability to advance the state of the art in a demonstrable way, providing a new perspective on the problem and opening up new avenues for research.
Arguments pro acceptance:
* The paper provides a definitive answer to an important problem in the field.
* The proposed algorithm, Rexp3, is a significant contribution to the field.
* The paper is well-written, easy to follow, and includes enough contribution for a conference submission.
Arguments con acceptance:
* The paper lacks a comprehensive discussion of state-dependent ergodic bandits.
* The algorithm requires knowledge of the total variation of mean-rewards, which may be a restrictive requirement.
* The paper may not be directly applicable to more general settings, such as restless bandits with unknown dynamics and history-dependent reward bandits.