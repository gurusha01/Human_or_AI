This paper proposes a stochastic variational inference algorithm for Hidden Markov Models (HMMs), called SVIHMM, which adapts stochastic variational inference (SVI) to time-dependent data settings. The authors address the challenge of applying SVI to HMMs by using minibatches of subsequences instead of complete data items and propose an approximate message-passing scheme to account for the memory decay of the chain.
The paper's main contribution is the development of SVIHMM, which harnesses the memory decay of the chain to adaptively bound errors arising from edge effects. The algorithm is shown to converge to a local mode of the batch objective, and empirical results demonstrate similar performance to batch VB in significantly less time on synthetic datasets. The authors also apply SVIHMM to a large genomics dataset, where batch inference is computationally infeasible, and achieve comparable performance to a dynamic Bayesian network model.
However, the paper's quality is compromised by missing figures and tables, and a lengthy introduction that delays the presentation of the paper's material. The presentation of key components, such as GrowBuf, is incomplete and relegated to the supplement, making it hard to understand the paper. The results are difficult to assess due to a lack of comparison with other methods, such as DBN, and insufficient information about the estimate of the False Discovery Rate (FDR).
The paper's strengths include its originality, as it extends SVI to HMMs, and its significance, as it enables scalable Bayesian inference on large datasets. The authors provide a clear and detailed explanation of the algorithm and its components, and the empirical results demonstrate the effectiveness of SVIHMM.
Arguments for acceptance:
* The paper proposes a novel algorithm that extends SVI to HMMs, which is a significant contribution to the field.
* The algorithm is shown to converge to a local mode of the batch objective, and empirical results demonstrate its effectiveness.
* The paper addresses a important problem in Bayesian inference, which is the scalability of HMMs to large datasets.
Arguments against acceptance:
* The paper's quality is compromised by missing figures and tables, and a lengthy introduction.
* The presentation of key components is incomplete and relegated to the supplement.
* The results are difficult to assess due to a lack of comparison with other methods and insufficient information about the estimate of the FDR.
Overall, I would recommend accepting the paper, but with revisions to address the issues mentioned above. The authors should provide a complete and clear presentation of the algorithm and its components, and include more detailed empirical results and comparisons with other methods. Additionally, the authors should provide more information about the estimate of the FDR and address the issues with the paper's quality. 
Quality: 6
Clarity: 7
Originality: 8
Significance: 8
Recommendation: Accept with revisions.