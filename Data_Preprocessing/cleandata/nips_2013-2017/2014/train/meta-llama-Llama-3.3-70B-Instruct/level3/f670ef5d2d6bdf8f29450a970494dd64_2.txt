This paper introduces a novel framework for online multi-task learning with a shared annotator, where multiple learners share a single annotator with limited bandwidth. The authors propose an algorithm, SHAMPO, which learns to adaptively allocate the annotator's bandwidth to different tasks. The algorithm is based on a gradient-based optimization method and a regularization path heuristic, allowing for adaptive tuning of regularization.
The paper provides new insights on sparsity-inducing regularizations by introducing the notion of group-majorization and recovering well-known regularizations through a group G and a seed vector v. The use of the orbitope is a distinctive aspect of the research, although its utility and difference from the atomic norm need further discussion to understand its benefits and potential to bring new insights.
The paper is well-written, and the characterization of sparsity-inducing regularizations is important for a deeper understanding of machine learning. The authors demonstrate the effectiveness of their algorithm through empirical studies on various datasets, including OCR, vowel prediction, and document classification. The results show that SHAMPO outperforms other algorithms, including those that use uniform allocation, and achieves better accuracy for the same amount of annotator labor.
The paper's strengths include its novel framework, effective algorithm, and thorough empirical evaluation. However, there are some weaknesses, such as the need for further discussion on the orbitope and its relationship to the atomic norm. Additionally, the paper could benefit from more detailed analysis of the algorithm's computational complexity and scalability.
Arguments for acceptance:
* The paper introduces a novel framework for online multi-task learning with a shared annotator, which is a significant contribution to the field.
* The proposed algorithm, SHAMPO, is effective and outperforms other algorithms in empirical studies.
* The paper provides a thorough empirical evaluation on various datasets, demonstrating the algorithm's effectiveness in different settings.
Arguments against acceptance:
* The paper could benefit from more detailed analysis of the algorithm's computational complexity and scalability.
* The discussion on the orbitope and its relationship to the atomic norm needs further clarification.
* The paper assumes a limited bandwidth annotator, which may not be realistic in all scenarios.
Overall, the paper is well-written, and the proposed algorithm is effective. With some revisions to address the weaknesses, the paper has the potential to make a significant contribution to the field of machine learning. I recommend acceptance with minor revisions.