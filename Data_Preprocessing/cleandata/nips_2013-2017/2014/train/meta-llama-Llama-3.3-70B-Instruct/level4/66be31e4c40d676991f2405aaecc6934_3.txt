This paper introduces the concept of a pseudo-ensemble, which refers to the combination of dependent child models, providing a unified framework for various related techniques, including dropout. The authors propose the pseudo-ensemble variance regularizer and demonstrate its effectiveness through empirical results. The technique appears to be more easily applicable to multi-layer, semi-supervised settings compared to the method presented in [23].
However, some concerns arise regarding the novelty of the pseudo-ensemble concept, as it can be seen as an expectation, which is already discussed in references [5] and [23]. The generalization to multi-layer settings may not be a significant contribution. The authors' effort to provide a deeper understanding by exploring the boosty and baggy forms is appreciated, but the connection to boosting remains vague, and these forms are not fully utilized. Furthermore, the introduction of the regularizer, while empirically successful, lacks a clear theoretical motivation or connection to existing concepts.
The paper's structure suggests a general framework is being established, but the introduction of the new regularizer in section 4 seems disconnected from the preceding sections. A more thorough explanation of the regularizer's derivation or approximation would be beneficial, as would a clearer connection to existing methods, such as [23]. The empirical strength and simplicity of the proposed regularizer are notable, but the overly complex presentation detracts from the paper's clarity and insight.
Detailed comments include the observation that the formalism of f(x; ξ) is more general than necessary, and the notation ξ could be more clearly related to subsampling child models, such as dropout. Additionally, there are discrepancies in the equations, such as the placement of the loss L in relation to the expectation over noise. The baggy PE equation is unclear, and the connection to boosting is intuitive but not rigorously established. The comparison to existing approaches in lines 129-140 may not be entirely accurate, as some methods do deal with model noise and have explicit, interpretable regularizers.
The notation for variance in Eq 3 could be simplified, and the term "scale-invariant variance penalty" might be more descriptive to avoid confusion with actual variance. The motivation behind the regularization method is unclear, and a derivation or approximation of the regularizer, potentially showing its relationship to existing methods like [23], would be beneficial. Overall, while the paper presents a promising technique with convincing empirical results, the lack of theoretical motivation and unclear connections to existing concepts detract from its impact.