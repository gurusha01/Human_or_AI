This manuscript presents a refreshing and ambitious scope, offering a departure from the typical NIPS papers that delve into a single idea in exhaustive detail. However, upon closer examination, it appears that the paper may lean too far towards breadth at the expense of depth, potentially compromising the impact of the results. The following concerns are outlined to substantiate this assertion.
The domain of algorithm selection and metareasoning is a crucial area of research in both artificial intelligence (AI) and cognitive science, surprisingly underrepresented in the literature. The authors' review of existing work is largely based on decade-old studies, with a notable omission of research on cognitive architectures such as SOAR and ACT-R, which may not significantly contribute to the understanding of metastrategy learning.
The choice of sorting algorithms as the primary domain of study is perplexing, as it may not be the most suitable for the rational metareasoning (RM) model. The RM model aims to optimize a combined measure of algorithm accuracy and time/opportunity cost, but sorting algorithms typically achieve perfect accuracy, rendering the scoring model (Equations 3, 8, 9, and 10) potentially irrelevant.
Furthermore, the benefits of employing a Bayesian linear regression approach over simple ridge regression are unclear, given that only parameter means are utilized (Equation 12). Nevertheless, the selected representation appears sensible, incorporating features such as logs and 2nd-order polynomial terms.
The manuscript claims to contribute to both state-of-the-art AI approaches to algorithm selection and psychological theories of metareasoning. However, questions arise regarding its contributions to each field, which are addressed separately.
In terms of AI, the authors compare their results to two existing models, but the Lagoudakis method seems to be a straw man due to its lack of consideration for presortedness. The comparison to Guo's method is more substantial, although concerns were raised regarding the measure of presortedness used, which was later clarified by the authors.
Regarding the contribution to cognitive science, the trade-off between accuracy and opportunity cost is a crucial aspect of human strategy selection. However, the opportunity cost in this context encompasses not only the runtime of an algorithm but also the cost of selecting an algorithm, as reflected in Simon's notion of satisficing. Therefore, the choice of sorting algorithms as the primary domain of study may not be the most suitable for investigating human strategy selection. Although the experimental setup in Section 5 is elaborate, the coarse performance statistics (proportion of merge sort selection and overall quality of selection) do not provide compelling evidence for the RM model's validity. The experiment merely demonstrates that both the RM model and humans are proficient in selecting strategies, whereas other models are not, offering limited insight into the RM model's cognitive plausibility.
The training procedures for the models used in Section 5 are not thoroughly detailed, but if these models are intended to be cognitive models, they should be trained on the same data and number of trials as humans. The authors' rebuttal assures that the training data is identical.
Minor comments include:
* [093]: A Gaussian distribution may not be the most suitable choice for modeling human response times, which tend to exhibit long-tailed asymmetric distributions.
* [091]: The standard deviation does not appear to be a polynomial in the extended features (Equation 7).
* [168]: The table caption should provide an explanation for the performance measure, which is believed to be the percentage of runs in which the optimal sorting algorithm was selected.
* [295]: The text suggests that Equations 13 and 14 indicate the conditions under which merge sort will be chosen over cocktail sort, but the coefficients on ncomparisons and nmoves in Equation 13 are smaller than those in Equation 14, implying that cocktail sort should be chosen for all but the shortest lists.
Overall, the manuscript addresses a significant challenge in AI and cognitive science, presenting a straightforward approach to learning strategy selection and yielding sensible results in a limited domain (sorting algorithm selection).