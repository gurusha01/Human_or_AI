The authors present a novel regression approach based on locally weighted regression (LWR), which is linked to Gaussian process (GP) regression to form a probabilistic model. Initially, the model appears to suffer from scalability issues similar to those of GP regression (cubic in N), but a variational scheme is employed to rescue the method. The updates in the variational scheme are analogous to the LWR procedure, thereby recovering the algorithm's scalability.
Clarity
--
The paper is exceptionally well-written, with a clear and consistent notation system, making it easy to follow. The authors provide sufficient detail without overwhelming the reader. The connection between LWR and GP regression is particularly noteworthy, offering a fresh perspective on interpreting the entire methodology as a GP. However, section 4 is somewhat unclear and anecdotal compared to the rest of the paper, although the presentation of the algorithm is helpful.
Quality and significance
--
The paper is technically sound, of interest to a significant portion of the NIPS community, and features solid experiments that represent interesting challenges. The proposed method demonstrates a notable improvement in terms of speed while maintaining accuracy. It would be beneficial to mention the availability of the implementation to further enhance the paper. One notable omission is the lack of exploration of the algorithm's probabilistic nature in the experiments. Including the average log-density of held-out data alongside the MSE would provide a more comprehensive evaluation, as the proposed method and SSGPR can provide probabilistic estimates. In a robotics environment, where decision-making under uncertainty is crucial, log p(y*) is a more informative measure than MSE.
Queries
--
To render the variational approximation tractable, uncertainties are introduced via the parameters beta. It would be interesting to know what values these parameters converged to in practice and whether this slight change in the model has a significant impact. The variational updates for local models are independent, but the beta parameters are global, which may make the mode computationally costly. Do the authors interlace fewer of these updates with the local updates? Additionally, the comparison between SSGPR and LWGPR in Table 2 raises questions about the complexity of the models, as SSGPR was pre-trained with 200 features, while LWGPR used around 500 local models. Is it fair to say that both models have the same complexity, and would SSGPR not benefit from more features?
Summary
--
The paper is well-presented and enjoyable to read, with a relevant topic and good experiments. However, the lack of probabilistic quantities in the results is a notable drawback. The authors' responses to the technical questions and concerns will be crucial in determining the overall score.