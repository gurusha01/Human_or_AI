This manuscript presents a novel rate-based neuronal network architecture, trained to maintain multiple stable activity patterns, or memories, with the aim of addressing key limitations of prior attractor models, including non-compliance with Dale's law and activity saturation in memory states. The training process employs gradient descent on a cost function that concurrently minimizes changes in desired attractor activities and the Frobenius norm of the weight matrix, while maximizing attractor stability. The outcomes include a weight distribution that aligns with experimental observations and a balanced excitation and inhibition within attractor states.
The manuscript is well-written, with clear explanations of the model, training procedure, and results. The integration of biological constraints into the model and the emergent features following training render this work a compelling addition to the field of attractor neural networks. By training a rate-based neuronal network to support graded, non-saturated attractors in both synaptic weights and inhibitory activities, the authors demonstrate that biologically plausible constraints on synapses and neurons yield a balanced excitation and inhibition at the network level, making this paper a significant contribution to understanding attractor neural networks.