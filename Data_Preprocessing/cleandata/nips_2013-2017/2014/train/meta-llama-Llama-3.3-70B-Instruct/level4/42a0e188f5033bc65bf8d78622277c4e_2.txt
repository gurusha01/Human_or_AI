This manuscript presents a parallel direction method of multipliers (PDMM) for minimizing block-separable convex functions under linear constraints. Unlike the alternating direction method of multipliers (ADMM), which updates primal variables sequentially in a Gauss-Seidel manner limited to two blocks, the proposed approach updates primal blocks in a Jacobian manner, accommodating multiple blocks. The key contribution of this work is the introduction of a dual backward step, which mitigates the limited information propagation inherent in Jacobian updates by adaptively reducing the step size for dual updates. The paper establishes an O(1/T) convergence rate for PDMM, similar to ADMM.
The supplementary materials reveal that previous methods, including sADMM and PJADMM, are special cases of the proposed PDMM, providing valuable insights into the relationships between various decomposition methods based on the augmented Lagrangian. Experimental results demonstrate the efficacy of PDMM compared to ADMM and its variants. However, the results seem counterintuitive, as Jacobian-type methods typically exhibit slower convergence than Gauss-Seidel-type methods. Further clarification on this aspect would be beneficial. The dual backward step in PDMM, crucial for ensuring convergence under general conditions, appears to be a novel contribution, analogous to ADMM. The proposed method offers additional alternatives and possibilities for distributed optimization, expanding the existing landscape of decomposition methods.