This paper investigates the relationship between dimensionality and communication cost in distributed learning problems, where samples from an unknown distribution are dispersed across m machines. The objective is to estimate the mean at the optimal minimax rate while minimizing the number of bits communicated. The authors demonstrate that, in this context, the communication cost increases linearly with the number of dimensions. By applying this result to existing lower bounds for one dimension in interactive and simultaneous settings, the paper establishes new lower bounds of Ω(md/log(m)) and Ω(md) for the bits of communication required to achieve the minimax squared loss. Furthermore, an interactive protocol is presented that achieves the minimax squared loss with O(md) bits of communication. Given the strong lower bounds in the general setting, the paper initiates the study of distributed parameter estimation problems with structured parameters, showing that when the parameter is s-sparse, a protocol can achieve the minimax squared loss with high probability and a communication cost proportional to s rather than the ambient space dimension d.
Quality: The authors should provide more background information, as the current presentation assumes readers are familiar with the problem in [4], which may not be the case.
Clarity: Several definitions are missing or unclear in Section 2, including:
0. The definition of s-sparse is not provided.
1. The definition of R(θ̂,θ) lacks clarity, particularly regarding the expectation over θ̂, X, and θ, and how the mean θ̂(Y) of Y is obtained. Additionally, the reason for Y being in ℵ^n and an example of a transcript would be helpful.
2. The explanation of Private/Public Randomness is confusing, and definitions for private and public randomness, as well as what constitutes a protocol, are needed. Specifically:
a. Clarification on the distinction between private and public randomness and their roles in a protocol.
b. An explanation for why public randomness can be shared among machines before the protocol begins.
c. A justification for why the protocol performs well on average over all public randomness.
d. A description of how private randomness enables machines to hide information from each other in a protocol.
e. The definition "(Π,θ̂) solves T(d,m,n,σ^2,ℵ_θ^d) with C and R" requires further explanation.
Originality: The derivation of new lower and upper bounds for interactive and simultaneous settings, as well as the negative result, appears to be original. It is reasonable that the authors highlight the difficulty of deriving these results from [4].
Significance: The finding that "the communication cost scales linearly in the number of dimensions" may seem intuitive, but the authors should explain why this result is noteworthy and how it differs from their initial expectations. Assuming readers are unfamiliar with the material, additional introductory explanations would be beneficial. While the idea is novel, the authors should provide more context to appreciate its significance.