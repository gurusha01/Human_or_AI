This study investigates the issue of identifying and eliminating adversarial workers in crowdsourced classification tasks. A worker is deemed adversarial if their voting strategy deviates from the standard random process, where they vote correctly with probability p and incorrectly with probability 1-p, with p representing the worker's reliability. To address this problem, the authors propose assigning reputation values to workers, which are calculated based on the extent to which their votes align with those of other workers, with a focus on penalizing disagreements rather than rewarding agreements. The paper presents two variants of the penalty algorithm and provides theoretical analysis of their properties under different adversarial worker voting strategies, supplemented by experiments on both synthetic and real datasets.
In my opinion, this paper tackles a fascinating problem, as most existing research on label aggregation assumes that workers' votes are independent and identically distributed samples from a given distribution. The current study considers various voting strategies, moving towards a more realistic setting. However, the theoretical analysis of the hard penalty algorithm is not entirely satisfactory due to two concerns: first, the results are difficult to interpret, and second, the authors make a strong assumption that honest workers are perfect. I also have reservations regarding the experimental results, which are elaborated upon below.
Some specific comments include:
- The analysis of the hard penalty assignment algorithm (and the accompanying simulation) assumes that honest workers are perfect, voting correctly with probability 1. This assumption appears to be quite strong, and it is unclear whether it is necessary for the analysis. The authors may want to consider relaxing this assumption and provide intuition on how to do so.
- The results of Theorem 3 and 4 are presented in terms of the workers' voting graph B_H. To facilitate reader understanding, it would be helpful to provide examples that illustrate the bounds.
- The experimental results in Table 2 show varying numbers of users being filtered out across different settings. For completeness, it would be beneficial to present all the results, with the number of filtered users ranging from 1 to 10, as the optimal parameters may not be known in real-world applications.
- The experiments demonstrate that the proposed algorithm significantly improves the performance of ITER (introduced by Karger et al. in 2011). Nevertheless, ITER assumes that all workers complete the same number of tasks. To apply ITER to datasets without this property, it is essential to modify the algorithm by normalizing the messages at each step during the message passing process. Failure to do so may result in the algorithm's performance being dominated by the accuracies of workers who complete the most tasks, rather than effectively filtering out adversarial workers.
Following the rebuttal, I appreciate the authors' response to my concern about the ITER implementation, which has led me to update my evaluation scores. If the paper is accepted, I recommend reporting the updated results.
Overall, I believe this is a well-written paper that addresses a more realistic setting, although the contribution is limited by the strong assumption of perfect workers. While the empirical evaluations mitigate this shortcoming to some extent, I still have concerns about the experiments. In my assessment, this is a borderline paper, and I do not have a strong preference for either acceptance or rejection.