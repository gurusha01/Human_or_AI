This manuscript focuses on the development of kernel matrices for determinantal point processes (DPPs) through a novel approach. The primary contribution lies in the reformulation of the optimization objective, which surpasses the conventional method of differentiating the log probability with respect to kernel parameters and applying projected gradient ascent, referred to as K-Ascent (KA) in the paper. By framing the problem within an Expectation-Maximization (EM) framework, the authors leverage the fact that any DPP can be represented as a mixture of "elementary DPPs," each corresponding to a subset of elements. A variational distribution is defined over these subsets, and the algorithm iterates between updating the distribution over subsets (represented as a k-DPP) and performing single gradient steps in the M-step. The results indicate that the EM method significantly outperforms the KA method when the latter is initialized naively and shows modest improvements in certain settings with more informed initialization.
In general, the proposed approach is intriguing, and the EM method appears to yield some enhancements. Although the M-step is complex and the necessary caveats to ensure its functionality are somewhat unsatisfying, the achievement of making it operational is noteworthy.
From an experimental standpoint, a more in-depth analysis of the impact of initialization on the EM algorithm would be beneficial. For instance, initializing the EM algorithm with the outcome of KA could potentially lead to further improvements.
Furthermore, the use of relative log likelihood as an evaluation metric is perplexing. Since log probability differences represent probability ratios, it is unclear why ratios of log probabilities are taken instead of simply differences in log probabilities, making the interpretation of these numbers challenging.
On a minor note, certain aspects of the writing style are found to be irritating. The authors' description of their algorithm as "elegant" is debatable due to the required caveats and approximations in the M-step. Additionally, the bolding of text in the introduction seems unnecessary. The paper presents an interesting novel approach for learning kernels in DPPs. While the experiments could be enhanced, and the improvements over the basic projected gradient algorithm are not substantial, the manuscript is generally well-written.