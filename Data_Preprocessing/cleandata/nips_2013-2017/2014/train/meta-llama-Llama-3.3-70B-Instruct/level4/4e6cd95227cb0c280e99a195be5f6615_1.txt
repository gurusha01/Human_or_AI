This manuscript introduces a novel Gibbs sampler algorithm for Finite Hidden Markov Models (FHMMs) by incorporating an auxiliary variable, U, into the state of the Gibbs sampler, which constrains the potential values of the hidden state X at each subsequent step. Given that the number of possible values for X_i is limited at each time point i, the hidden state X can be updated conditioned on U (and the observed data) using the Forward-Backward algorithm (FFBS).
The proposed approach is deemed original and ingenious, addressing a significant class of problems. The paper is well-structured and clearly written, with a thorough empirical evaluation of the algorithm. Moreover, the authors provide insightful explanations for the method's effectiveness and its potential applications. The concept lends itself to obvious extensions, suggesting a substantial impact of this work.
For instance, a potential extension could involve defining the set of possible values for Xi given U in a manner that incorporates the observed data yi, utilizing likelihood information to inform the FFBS algorithm and focus on the most probable states of X_i at each time step.
Upon reviewing the manuscript, a few minor errors were noted, including typos on page 3 ("four bits" should be "for four bits") and a missing definition of Hamming distance. Additionally, on page 4, "U as auxiliary" should be revised to "U as an auxiliary." Overall, this work presents an innovative approach to Markov Chain Monte Carlo (MCMC) for FHMM models, with a clear and well-organized presentation, and significant potential to influence the development of algorithms for this class of models.