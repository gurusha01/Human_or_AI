This paper proposes a methodology for learning a semantic space for object categorization, utilizing semantic entities such as supercategories and attributes to constrain the resulting discriminative embedding. The core concept involves representing a category sparsely through its supercategory and a combination of attributes, as exemplified by the representation of a tiger as a striped feline. A key benefit of this approach is its ability to generate compact semantic descriptions for the learned categories. The paper is well-presented, with a sound motivation and results that effectively demonstrate the superiority of the proposed techniques. Although some principles for designing cost functions for discriminative embeddings with good inter-class separation have been previously discussed (e.g., [7], [14]), the authors introduce novel methods to connect categories, supercategories, and attributes.
- Other Comments
The introduction could be improved for clarity, as multiple concepts are introduced without a clear progression. Specifically, lines 077-079 are unclear regarding the generative and discriminative objectives until later in the text. The combination of precise terminology, such as generative and discriminative, with imprecise context-specific explanations, makes the text more challenging to follow.
On line 145, the notation should be corrected to S(z_i,…) instead of S(z,…).
Given that the models are non-convex, it would be beneficial to provide details on the initialization process.
Overall, the proposed model offers a compact semantic space that learns a discriminative space for object categorization by leveraging constraints from supercategories and attributes, achieving improved results on the AWA dataset. An additional advantage of the model is its capability to generate human-interpretable decompositions of categories.