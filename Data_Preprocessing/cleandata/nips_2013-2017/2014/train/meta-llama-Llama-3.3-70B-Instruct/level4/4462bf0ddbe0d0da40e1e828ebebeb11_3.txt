This paper presents a novel approach to learning Determinantal Point Processes (DPPs), a distribution that favors diverse sets over a fixed ground set. DPPs are typically parameterized by a positive semidefinite matrix (L), but learning L is NP-hard, leading to previous work focusing on partial learning or restricting the parametric form of L. In contrast, the authors propose a learning method that does not impose such restrictions. By utilizing Expectation-Maximization (EM) on eigenvalues and eigenvectors, they avoid the need for a projection step, which can compromise the diversity property of DPPs. This approach exploits the fact that the matrix V is full-rank, eliminating the need to project eigenvectors. The use of Jensen's inequality to lower bound the objective function and construct an EM procedure is a key contribution. 
The proposed learning method is significant as it preserves the diversity property of DPPs, unlike previous work that has primarily focused on the quality property. The step-by-step derivation of the EM procedure is insightful, and the mapping of the constraint to an optimization over the Stiefel manifold to eliminate eigenvector projection is a solid contribution. The authors demonstrate the effectiveness of their learning algorithm on both synthetic datasets and a product recommendation task, showing that it retains the quality and diversity of DPPs. While the conclusion could be more comprehensive in highlighting the paper's contributions, the overall writing is cohesive, and the proofs are well-written. The reduction of optimization to EM is interesting and well-explained. However, additional experiments on real-world datasets would further illustrate the importance of the diversity produced by this algorithm compared to others in the field.