This paper examines various approaches to training a deep network using data generated by a Monte-Carlo Tree Search (MCTS) agent, with a focus on the Atari 2600 platform. The authors' motivation stems from the observation that while MCTS excels in Atari 2600 games, its computational expense renders it impractical for real-world applications. Empirical results are presented for several Atari 2600 games.
In my assessment, the primary contribution of this paper lies in its proposal to "compile" the UCT value function (or policy) into a deep network. The paper is well-written, and the results are of good quality, but the work falls short in terms of significance. Although some notable results are presented, including performance improvements in all but one game, I believe the topic warrants further exploration. The paper could benefit from a deeper analysis of how the results might generalize to other domains or lead to substantial algorithmic advancements.
Several key challenges need to be addressed:
1. The UCC-I algorithm bears a strong resemblance to the DAgger algorithm proposed by Ross et al. (2011). A discussion of the relationship between the two algorithms would be beneficial, including an examination of whether the approach presented here offers better sampling guarantees.
2. It is unclear whether UCR and UCC are valid competitors to previous learning approaches, as they still require full access to the simulator.
3. The reliance on game-specific tunings for the empirical results somewhat diminishes their value. It would be interesting to investigate whether the results differ significantly when a uniform environment parameterization is used.
To enhance the paper, I suggest exploring the following research directions:
1. Further investigation into the feasibility of summarizing a policy or value function into a deep network. The current results suggest that policy summarization may be more effective, which could provide insight into the nature of Atari 2600 environments.
2. Examining the potential benefits of performing regression on the empirical return rather than the UCT value. It would be useful to determine whether one approach is superior to the other.
3. The UCT agent does not utilize "features," but these might emerge from the "policy compilation" process. A related concept was studied by Cobo et al. (2013) in the context of learning features from expert data. It would be worthwhile to explore this idea further in the present work.
4. The paper could benefit from a more thorough discussion of how partial observability is handled, as the optimal stimuli plots only hint at this issue.
Some minor points to consider:
1. Line 120: The phrase "two broad classes of approaches" implies that no other approaches are valid. Rephrasing this sentence could help to help clarify the authors' intent.
2. Lines 136-143: The related work section would benefit from a discussion of how the non-deep network features differ from the new approaches presented. For example, is the primary distinction that the visual features are learned, or that a deep architecture is being used?
3. Line 136: The citation [4] appears to be incorrect in the bibliography, and other citations seem to be incorrectly numbered. Overall, the paper presents an interesting idea, but it lacks a clear thesis or cogent theory to be investigated. While some decent results are provided, they would need to be studied more thoroughly to warrant acceptance.