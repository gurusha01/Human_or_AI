The submission presents a convex deep learning formulation that incorporates several key concepts. Initially, a training objective is proposed that treats the outputs of hidden layers as variables to be inferred through optimization, linking them to linear responses via a loss function. The overall objective is the sum of these loss functions across layers, along with regularization terms. The authors then apply a series of changes of variables to reparameterize the objective into a convex form, heavily relying on the representer theorem and value regularization. This results in a convex objective in terms of three matrices per layer to optimize, including a nonparametric 'normalized output kernel' matrix that replaces direct optimization of hidden layer outputs. However, this approach leads to a transductive method requiring simultaneous optimization for training and test inputs. A relaxation is also necessary to achieve a truly convex formulation, as the set of valid kernel matrices is generally non-convex.
The technical aspects of the submission appear sound, although verifying many aspects of the derivation was challenging. The authors' use of sophisticated techniques, assumptions, and observations to achieve a convex deep learning formulation is impressive. However, the resulting method seems more like an academic curiosity than a practical approach due to several reasons. The derivation relies on critical assumptions and relaxations, such as relaxing the set of output kernels to a convex set, which may not capture the essence of traditional deep networks. The original motivation for the objective, resembling a Boltzmann-machine-style model, is also compromised by the convex relaxation of the output kernel. Furthermore, the transductive nature of the algorithm and the replacement of a saturating loss with a hinge loss raise concerns about scalability and interpretation.
Several steps in the derivation were difficult to comprehend, and technical doubts remain, such as the convexity of the set induced by the reparameterizations of the objective. The experiments are weak and do not alleviate concerns about the method's practicality, using small datasets and only comparing to a two-layer version of the method. A comparison to traditional deep nets or shallow methods would be more informative.
The submission lacks clarity, with the authors' mental model and rationalization for their approach not being clearly conveyed. The connection to neural-net-like Bayes nets or Boltzmann machines is not well-motivated, and critical points in the derivation, such as the vicinity of Eq. 5-7, are not adequately explained. The originality of the work lies in the generalization to arbitrary nesting, and the proposed optimization algorithm seems novel. However, the significance of the work is limited by the complicated and inflexible formulation, which scales poorly and has limited practical impact. While the work may stimulate further research, its immediate practical implications are likely to be low.