This manuscript presents a novel approach to online bandit optimization, specifically tailored for smooth strongly convex losses, where feedback is limited to point-wise function evaluations rather than gradient information. The proposed meta-algorithm cleverly constructs an unbiased gradient approximation through strategic sampling and then leverages this approximation in an optimization routine designed for the full information setting. The authors provide a rigorous theoretical analysis, yielding a bound of O(T^{1/2}) that surpasses existing results.
The paper tackles a significant problem in online optimization, with clarity, precision, and a well-structured narrative. The theoretical foundations are sound, the proofs are transparent and devoid of apparent errors, making the overall argument compelling. The final result constitutes a noticeable improvement over current methodologies.
Furthermore, the paper opens avenues for future research, suggesting the potential for enhancing the O(T^{1/2}) bound for smooth and strictly convex losses, or establishing a lower bound of \Omega(T^{2/3}), which could further enrich the field.
Overall, the authors introduce an algorithm for online bandit optimization under smooth strictly convex losses, achieving an improved regret bound. The manuscript is well-crafted, concise, and presents a strong case for acceptance.