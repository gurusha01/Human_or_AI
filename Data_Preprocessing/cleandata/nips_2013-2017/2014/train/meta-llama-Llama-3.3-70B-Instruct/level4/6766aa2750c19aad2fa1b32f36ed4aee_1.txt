Review- Summary:
This paper proposes a novel, sample-efficient policy search algorithm for tackling large, continuous reinforcement learning problems. Unlike existing model-based approaches, it learns local models in the form of linear Gaussian controllers and utilizes the information from these models to learn a global, nonlinear policy through an arbitrary parametrization scheme. The Guided Policy Search approach iteratively alternates between local trajectory optimization and global policy search. The authors demonstrate the effectiveness of their approach by outperforming state-of-the-art policy search methods, such as REPS and PILCO, in experiments involving continuous control of multi-linked agents in 2D dynamics simulations.
Quality:
The paper is well-written and addresses a relevant topic in the NIPS community. The idea of training a global model from locally linear models in a supervised manner is innovative and adds significant value to the field of policy search algorithms. The experimental results show promising performance, but it is challenging to fully assess the quality of the method due to the novelty of the proposed experiments. For instance, it would be interesting to see how the approach compares to PILCO in tasks like the "Cart-Pole Swing-up" task. Additionally, the potential influence of local models on each other during dynamics fitting needs clarification, particularly in ensuring that rollouts are generated within the timeframe of a specific local model's expertise. The paper's ability to accommodate different policy representations, including neural networks, is a notable strength.
Clarity:
The paper is well-structured and clear, with the authors providing a thorough explanation of the algorithm's development and the reasoning behind their choices. The inclusion of practical and empirical details, such as parameter values and techniques for accelerating learning, is appreciated.
Originality:
While the paper shares similarities with existing policy search algorithms, such as REPS, and model-based RL, its core approach is distinct. The use of supervised learning to learn policies, rather than models of the environment, sets it apart from model-based RL. The employment of local models offers several benefits, including simplified training, stability, and convergence.
Significance:
The paper makes a substantial contribution to the policy search and reinforcement learning community, introducing a novel approach that builds upon existing methods while offering innovative ideas. The transformation of policy search into a supervised learning problem is a promising research direction that may lead to further insights. Overall, the paper is well-written and deserves publication, offering a valuable addition to the field.