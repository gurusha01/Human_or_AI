Summary of the paper:
This paper presents a novel trajectory optimization approach based on iterative Linear Quadratic Gaussian (iLQG) methods, where the linearized dynamics are learned from samples rather than relying on a known model. To ensure convergence, the authors replace the traditional backtracking line search with a constraint on the Kullback-Leibler (KL) divergence between the previous and new trajectory distributions. A Gaussian Mixture Model (GMM) is utilized as a prior for the unknown dynamics, and a parametrized policy is learned through guided policy search using a neural network. The paper provides experimental results on three simplified robotics problems, as well as comparisons with other methods, to support its claims.
Comments:
The paper is well-written and addresses a significant topic in robotics research. The main contribution lies in adapting iLQG to a model-free context by replacing the backtracking line search with a KL divergence constraint, ensuring convergence. Although this approach is interesting, it can be argued that the technical improvement is somewhat incremental, given existing knowledge on iLQG methods and linear Gaussian approximations of dynamics. The guided policy search approach is also related to previous work, such as that of Mordatch et al. (RSS 2014). The experiments demonstrate that the proposed approach converges faster than other methods, even without using a GMM as a prior.
One concern regarding the use of a GMM prior is that it already constitutes a model, which may explain why the proposed approach performs similarly to model-based trajectory optimization methods. The paper mentions that large mixtures that modeled the dynamics with high detail produced the best results, which is expected since this provides a good dynamic model. However, questions arise regarding the construction of the GMM, such as the method used to sample the potentially high-dimensional space while ensuring safety on a real robot. Additional comments on the scalability of this approach to more realistic robot models, where GMM construction would be extremely challenging, would be useful.
Another potential limitation of the approach is that it requires an example demonstration for the walking task, which is only a 2D walking task. Given that 3D walking is significantly more difficult than 2D walking, and that relatively simple feedback control solutions exist for 2D walking, it is surprising that the optimizer does not find a walking solution. The paper should comment on this issue and address how this method can scale to more complicated problems, such as 3D walking or swimming with multiple joints.
Furthermore, the comparison with iLQG for tasks involving contacts raises questions about the contact models used in the evaluations. Tassa and colleagues have demonstrated excellent results with tasks involving contacts and many degrees of freedom using iLQG, which can be attributed to the choice of a contact model that is more compatible with iLQG optimization techniques. For a fair comparison, the paper should use this contact model for iLQG. Overall, the paper is interesting and well-written, and the comparison with other learning approaches is useful. The main concerns are related to the use of KL divergence for line search, which is a relatively incremental change to iLQG, and the scalability issues of the approach for more realistic scenarios.