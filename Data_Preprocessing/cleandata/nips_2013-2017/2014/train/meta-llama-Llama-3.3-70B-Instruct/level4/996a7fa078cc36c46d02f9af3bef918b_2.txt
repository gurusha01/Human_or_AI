This manuscript introduces "Universal Option Models", an extension of discounted occupancy functions (Ng & Russell, 2000) to "options" (Sutton et al., 1999), which are high-level actions comprising a policy and a state-dependent termination probability function. The authors establish a connection between an option's discounted state occupancy function and its rewards (Theorem 1), and then explore the application of options to learning and planning in large state spaces using linear function approximation. The paper provides consistency and convergence results (Theorems 2, 3, and 4) before demonstrating the efficacy of the technique compared to an earlier option-based technique on two domains.
Overall, the paper is well-motivated, and the main ideas are presented in a logical and clear manner. The experimental methodology is well-explained, and the results show a significant improvement over the existing method by Sorg & Singh (2010). Although I am not an expert in the reinforcement learning literature, this appears to be a strong aspect of the paper. However, I notice a gap in the references, with little citation of related reinforcement learning work between 2000 and 2010, aside from work on PageRank (2002) and citation networks (2008).
My primary concerns lie with the theoretical results in the paper. The concept of a "reward-less MDP" appears in Theorems 1 and 2 but lacks a formal definition. This omission makes me apprehensive, as it is unclear what the theorem applies to. A formal definition and brief discussion of this concept would be beneficial to clarify its meaning.
Furthermore, the use of an inner product between the vectors $u^o(s)$ and $r^\pi$ at line 151 is confusing. Given that $u^o(s)$ is defined as a vector indexed by states over the second argument of $u^o$, and $r^\pi$ is also a vector indexed by states, my interpretation of the inner product $(u^o(s))^\top r^\pi$ would be $\sum_{s'} u^o(s, s') r^\pi(s')$, which differs from the sum in (4) for $R^o(s)$. The proof of Theorem 1 in the appendix does not utilize the inner product notation, instead directly showing that the return of option $o$ satisfies (4). As the vector form of $R^o$ is not relied upon later in the paper, I do not believe this has significant implications for the rest of the paper.
Theorem 4 references the "Robbins-Monro conditions" without providing a definition or reference. Even if these conditions are common knowledge in reinforcement learning, it is essential to be precise when stating theorems. Moreover, the "proof" of this result is only a sketch, and given the lack of page limits for appendices, a full proof should be provided. While the main idea, analysis, and experimental results appear novel, interesting, and significant, some of the theoretical results are poorly expressed, and the proof of Theorem 4 lacks the necessary rigor.