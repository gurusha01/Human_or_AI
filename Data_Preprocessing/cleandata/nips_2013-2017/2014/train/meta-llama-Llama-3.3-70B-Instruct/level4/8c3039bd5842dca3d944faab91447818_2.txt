This paper introduces a Deep Gaussian Mixture model (Deep GMM) that extends traditional Gaussian mixtures to a multi-layer framework. The core concept involves stacking multiple GMM layers, which can be viewed as a generative model where a standard normal variable undergoes successive transformations through a k-layer network, with each layer applying a transformation. This Deep GMM can be equivalently represented as a shallow GMM with an exponential number of components.
The authors propose several heuristics to accelerate the EM learning algorithm, including the use of hard EM and a "folding" technique that collapses layers into a shallow GMM model, although the latter becomes computationally expensive for lower layers.
While the idea is intriguing, a major concern arises from the paper's close relationship to previous work on Deep Mixtures of Factor Analysers (ICML 2012) by Yichuan Tang, Ruslan Salakhutdinov, and Geoffrey Hinton. Given the similarities between GMMs and mixtures of factor analyzers, it is notable that deep MFAs can also be "folded" into shallow models and learned using EM, with the option for layer-by-layer pretraining.
To strengthen the paper, it is essential to explicitly discuss the similarities and differences between the proposed Deep GMM and the existing deep MFA work. Overall, the paper is well-written, but due to its resemblance to prior research, the authors must clearly articulate their novel contributions to distinguish their work.