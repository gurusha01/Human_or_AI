The authors propose an extension of log-bilinear language models by substituting traditional two-way interaction matrices with three-way interaction tensors, which are subsequently factorized, effectively replacing the energy function with a factored counterpart. This approach is analogous to the transformation of Restricted Boltzmann Machines (RBMs) into factored RBMs, a common practice in the field.
Although the concept of factored models has been extensively explored in various tasks and applications, the authors provide several compelling examples of applying factored log-bilinear models to language tasks, including context-sensitive language models, sentiment classification, cross-lingual document classification, blog authorship attribution, and conditional word similarity.
One aspect that caught my attention was the selection of attributes in these applications. However, I was underwhelmed by the models' performance on quantitative tasks. The authors suggest that a thorough hyper-parameter search could narrow the gap, but I consider this claim speculative. Alternatively, it is possible that the relatively small dataset sizes require models with more parameters to be paired with more effective regularizers or substantially larger datasets.
The experiment on blog authorship attribution presents a strong case for the broader adoption of factored log-bilinear models in language tasks that currently utilize log-bilinear models. The conditional word similarity experiment is also noteworthy.
Overall, the paper successfully applies factored log-bilinear models to a diverse range of language applications, highlighting the benefits of incorporating context variables to a certain extent. The paper is well-written, engaging, and clear, making it a pleasure to read.