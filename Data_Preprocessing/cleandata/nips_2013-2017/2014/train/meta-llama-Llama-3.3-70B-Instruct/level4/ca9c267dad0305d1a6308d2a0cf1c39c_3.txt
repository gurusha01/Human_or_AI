This paper examines the issue of online multi-task learning, where a single annotator with limited capacity is shared across multiple tasks, and proposes an online algorithm to address this problem, analyzing its performance in the process.
Major Strengths:
The topic of online multi-task learning is of significant importance, and drawing inspiration from the field of multi-armed bandits, which has a rich history, is likely to lead to the development of improved techniques. This is particularly relevant given that some labels may not be observable in multi-task learning scenarios. Furthermore, this area of research is highly pertinent to the NIPS community.
Major Weaknesses:
A crucial aspect of multi-task learning is the existence of dependencies between tasks, allowing for joint learning and knowledge transfer. However, the work under review assumes no such dependency, which appears to be a substantial deviation from the core multi-task learning literature without clear justification. As a result, the approach seems more akin to a multi-armed bandit analog in multi-task learning. For instance, the restriction that only one feature can be annotated at a time, despite all features being annotatable, is puzzling. In applications utilizing Mechanical Turk, workers may not annotate labels for all features with equal proficiency, making the setting where only a subset of features can be annotated more relevant. Although the article presents this as a new multi-task framework, thus not bound by traditional assumptions, more motivation and justification are needed to explain the significance of these differences.
A notable exception is the work by Romera-Paredes et al., "Exploiting Unrelated Tasks in Multi-Task Learning," presented at AISTATS 2012, where the knowledge of unrelated tasks is leveraged to prevent overfitting by utilizing the same features. In contrast, the work under review does not exploit the lack of dependence between tasks.
A less critical point is that, to fully understand the implications of Theorem 1 (lines 153-154), it would have been interesting to see a comparison of SHAMPO's performance against a scenario where labels are available for all tasks in the data analysis.
Minor notes:
- [25] should read "both allow"
- [209] contains a typo: "outputs multicalss"
- [347] should have the comma removed. The paper introduces a novel problem in multi-task learning and provides a detailed analysis.