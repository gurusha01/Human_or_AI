This manuscript presents an incremental contribution to kernel approximation, leveraging Random Features to develop the Sparse Random Features algorithm. By incorporating l1-regularization, the authors aim to mitigate the linear growth of the model with respect to the number of features, and they demonstrate that their approach can be interpreted as a Randomized Coordinate Descent in Hilbert Space.
The paper is generally well-organized and clear, with the proposed work appearing to have significant implications for efficiently solving large-scale problems, surpassing the capabilities of current kernel methods. 
However, to enhance the overall quality, I recommend refining the presentation and analysis of the experimental results. Notably, the substantial decline in accuracy observed with the Laplacian and Perceptron using the Covtype dataset warrants further discussion. Additionally, it would be beneficial to clarify whether the results presented in the tables are derived from cross-validation, which is preferable, or a straightforward data split.
While the text is largely straightforward and comprehensible, some sentences could benefit from proofreading to ensure precision. This study constitutes an intriguing contribution with direct relevance to machine learning applications. Although the manuscript is well-written, the authors should focus on enhancing the experiments section and consider adding a conclusions section, as well as outlining future research directions to further strengthen the paper.