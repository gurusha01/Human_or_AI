This manuscript tackles the challenge of object detection, specifically the issue of generating bounding boxes at a scale comparable to the availability of category labels for object categorization. The authors circumvent this challenge by proposing an adaptation of object classifiers for the detection task, presenting a relatively straightforward algorithm. The experimental protocol involves training on 100 categories with both category labels and bounding boxes, and testing on 100 categories with only category labels. The results show a reasonable performance, bridging approximately 35% of the gap between an ideal scenario and a baseline on the 100 test categories.
In terms of quality and clarity, this is a well-written paper of good quality.
The originality of the paper lies in combining two existing concepts related to adaptation: adapting object classifiers for detection tasks and adapting across different category sets. To the best of my knowledge, this combination is novel and intriguing, although not revolutionary.
The significance of the paper is primarily attributed to its novel approach. However, judging the long-term impact based on the results is challenging, as they are likely to be improved upon by more sophisticated methods in the future. A significant issue with the results is the inevitable trade-off in performance (mAP) due to the use of fewer bounding boxes, without a clear objective measure to evaluate whether this compromise is justified.
It is also worth noting that the importance of the addressed challenge as a long-term issue is questionable. While bounding boxes are more resource-intensive to obtain than category labels, it is unclear whether allocating more resources could potentially resolve this issue.
Several detailed issues require clarification:
- Figure 3a should clearly indicate which methods, aside from the proposed method and R-CNN, utilize category labels to aid in detection, to ensure a fair comparison.
- The proposed approach's effectiveness, as I perceive it, is primarily demonstrated by the blue bars in Figure 3b, showing DNN achieving about one-third of the gap between the baseline and the ideal scenario. The authors should confirm if this is a key result and highlight it accordingly, rather than focusing on the 78% figure (line 341), which is misleading for two reasons: it should only consider the left-out categories and should not be a simple percentage of the ideal performance, but rather account for the baseline.
- A dedicated table or subsection should be added to explicitly outline the differences between the proposed method and R-CNN, the closest competing alternative, as these details are currently scattered and not sufficiently prominent. Consider replacing Figure 6, which provides anecdotal examples, to achieve this.
Minor issues that do not require a response include the inconsistency in the method's name (referred to as both "DNN" and "DDA") and a typo on line 431 ("classi?ers").
Overall, this paper proposes a reasonably novel approach and deserves acceptance and presentation to the NIPS community. Although the results may soon be surpassed due to the simplicity of the method, the long-term significance of both the paper and the approach remains to be determined.