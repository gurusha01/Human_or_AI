This manuscript primarily builds upon the Bayesian linear SVM presented in [5] by developing a nonlinear variant, which is then integrated with factor models. 
The transition from a linear to a nonlinear model is relatively straightforward, achieved through the application of standard kernel techniques. However, this nonlinear extension introduces a more complex inference problem, as it also involves learning the parameters of the kernel function.
The integration with factor models is accomplished by combining the two objectives, with kernels being generated based on factor representations.
In terms of model extension and combination strategies, the paper does not offer significant novelty. The overall learning problem is a complex, non-convex optimization issue.
Within the Bayesian framework, the authors propose inference procedures for learning, but they do not provide an analysis of the complexity of these inference processes.
The experimental section is limited, using only Gaussian kernels. It would be beneficial to explore the use of other nonlinear kernel types and assess their impact on the inference algorithm.
Furthermore, the datasets utilized in the experiments (as shown in Table 1) are excessively small. There is a need for large-scale experimental validation.
Additionally, the comparison of the proposed method is restricted to SVM and GPC approaches. Given that the tasks addressed are basic binary classification problems, it would be more comprehensive to compare the method to more advanced, state-of-the-art techniques.
The authors' motivation for this work stems from discriminative feature-learning models, a broad and significant topic. However, the discussion of related works in this area seems insufficient in the related work section.
The paper's contribution lies in extending previous linear models to nonlinear ones. Nevertheless, the experimental evidence provided is inadequate to fully demonstrate the effectiveness of the proposed approach.