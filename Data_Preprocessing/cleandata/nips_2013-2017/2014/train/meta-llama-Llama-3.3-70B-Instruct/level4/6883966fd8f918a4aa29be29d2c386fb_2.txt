This manuscript presents a novel approach to active learning of a linear regressor under the standard least-squares loss function. The methodology can be outlined as follows:
1. The process begins with the selection of a partition of the R^d space. Each data point and its corresponding response value are then reweighted using a straightforward method that is uniform within each partition cell, defined by a single real number per cell. The authors demonstrate that this reweighting technique does not alter the squared loss of any linear function.
2. However, the choice of weighting significantly impacts the convergence rate of the linear regressor. The authors leverage a recent generalization bound for linear regression in an innovative manner to address this aspect.
3. Determining the optimal weighting, which yields the best convergence rate, necessitates knowledge of the labels. The authors propose a frugal, active method for achieving this, concurrently estimating the linear regressor.
4. The aforementioned steps (1)-(3) are applied to a specific partition of the space. The authors suggest progressively refining the partition as the dataset grows.
The manuscript provides label complexity bounds for steps (1)-(3).
Overall, this paper introduces several innovative concepts and insights, notably the reweighting method for the distribution and the exploitation of the new generalization bound for regression. The proposed algorithm has the potential to be reasonably practical. This work represents a significant advancement in the state of the art for active linear regression.
Some potential areas for improvement include:
1. The analysis relies on the assumption that the "label noise" for a data point x is bounded by O(||x||^2). Relaxing this assumption would be beneficial, although it is worth noting that previous research on active regression has made more stringent assumptions regarding noise.
2. The manuscript does not provide a method for refining the partition or analyze the asymptotic rate that would be achieved. However, in practice, a suitable partition could be obtained through techniques such as hierarchical clustering. 
This novel and insightful paper advances the state of the art in active learning for least-squares linear regression.