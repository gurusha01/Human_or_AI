This manuscript introduces a novel neural network framework for categorizing human action videos, which integrates the predictions of two convolutional neural networks: one trained on individual video frames and the other on brief sequences of dense optical flow images. The proposed model achieves superior performance compared to other ConvNet-based methods for action recognition, matching the current state-of-the-art on two widely-used video classification benchmarks.
Although the paper largely adapts the conventional ConvNet image classification pipeline to a new data type (dense optical flow frames) and combines it with a single-frame ConvNet, the experimental evaluation is comprehensive and the results are noteworthy. The comparison of various methods for training a ConvNet on optical flow, along with the clear demonstration of the benefits of utilizing optical flow as input, is a valuable contribution.
I have a few minor suggestions for improvement:
- The significant outperformance of the spatial network over the models from [13], despite architectural similarities, warrants further clarification.
- A comparison of different frame sampling strategies for the spatial stream would be a worthwhile addition. Overall, this is a strong empirical paper that will appeal to researchers working on video classification using ConvNets.