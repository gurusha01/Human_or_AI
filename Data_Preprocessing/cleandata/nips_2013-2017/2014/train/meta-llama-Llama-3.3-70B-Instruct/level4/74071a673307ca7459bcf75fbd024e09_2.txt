This manuscript explores the challenge of performing Bayesian inference under structural priors that constrain the distribution's support. 
The authors commence by justifying the incorporation of support constraints using the maximum entropy principle, which aims to minimize deviation from a base distribution, as measured by relative entropy, while enforcing the specified constraint. The resulting optimal distribution is referred to as the information projection of the base distribution onto the constraint set. The paper demonstrates that this principle yields intuitive outcomes when applied to support restrictions, where the information projection of a distribution p onto a support set A is given by 1{x \in A} p(x) / \int_{x \in A} p(x), and that this concept can be utilized to describe Bayesian inference, where the posterior distribution arises from projecting the joint distribution onto the support set defined by the observations.
When projecting onto the intersection of multiple sets, the authors show that this can be achieved by sequentially projecting onto each set in any order, yielding unchanged results. 
This leads to the development of an inference procedure for the specific case of sparsity restrictions, which forms the basis of the paper's primary algorithm. The algorithm structures inference as an inner and outer optimization problem, where the inner optimization computes a posterior distribution with a fixed set of k nonzero entries, and the outer optimization determines the optimal set of nonzero entries. The authors formulate this optimization problem as a submodular optimization problem, prompting the proposal of a greedy forward selection strategy for choosing nonzero dimensions. Experimental results demonstrate the effectiveness of the method against various baselines.
The paper's key strengths include:
- clear and well-justified writing, with technically sound arguments
- a novel and insightful formulation of constraint imposition using information projections, providing a deeper understanding of the implications of adding constraints to models
- the algorithm's empirical success
However, a significant drawback is that the technical development, although thorough, leads to results that are not particularly surprising. From a practical perspective, it appears that Section 2 could be omitted, and the algorithm in Section 3 could be derived more directly by recognizing that sparsity constraints can be viewed as a union of simple constraint sets, leading naturally to the objective function used in Section 3. The submodularity of J(s) is also readily apparent. While the paper is well-executed and engaging, it ultimately fails to provide new tools for incorporating prior knowledge into models, leaving the reader somewhat disappointed in terms of the methods presented for imposing structural constraints. Overall, the paper is interesting and well-written, but its impact is somewhat limited by the lack of novel methodology for incorporating structural constraints into models.