This manuscript introduces a novel approach to solving Markov Decision Processes (MDPs) by directly minimizing the Optimal Bellman Residual (OBR), offering an alternative to approximate policy and value iteration methods. The authors commence by motivating their approach, demonstrating that the loss bound associated with OBR is often tighter than that of policy or value iteration, a result supported by existing literature [9,15]. They further establish that an empirical estimate of OBR is consistent in the Vapnik sense, implying that minimizing the empirical OBR equates to minimizing an upper bound on the true, yet unknown, OBR in cases where the MDP model is unknown. A key contribution lies in the decomposition of OBR into a difference of two convex functions, enabling the use of standard Difference of Convex Functions (DC) optimization methods to find a local optimum. The authors argue that this local optimum is typically close to the global optimum. Empirical results show that the performance of their method is comparable to standard techniques like LSPI and Fitted-Q learning, with the added benefit of smaller variance in the obtained values.
The manuscript is commendable for its clarity, organization, and technical soundness. The proofs presented are original and non-trivial, with Theorem 4, which deals with the decomposition of OBR into a difference of convex functions, being particularly noteworthy for its practical implications. The motivation based on loss bounds (Section 2.2) and the proof of consistency for empirical risk minimization when using basis functions for value function approximation are also appreciated.
However, the empirical evaluation falls short, limited as it is to a simple artificial MDP, where the method does not outperform LSPI or Fitted-Q learning. This raises questions about the contribution's significance if OBR minimization does not offer improvements over existing methods. Additionally, the computational effort required for solving multiple convex optimization problems is not reported, which could be a significant drawback if the DC decomposition is not efficiently handled.
Despite these concerns, the paper's main contribution—introducing a new approach to solving MDPs via OBR minimization—holds intrigue. Further exploration, such as relating DC iterations to policy/value iterations, could yield valuable insights.
Several points require clarification or correction:
1. The definition and distinction of \(\mu\) in line 120 from the state-action distribution \(v\) should be introduced prior to Equation (1).
2. Equation (3) largely repeats Equation (2), suggesting redundancy.
3. Typos are noted in lines 158 ("better that", "manly") and 313 ("is currently is").
4. The proof of Theorem 3 is straightforward and could be omitted, with Equation (3) removed to make room for the more significant proof of Theorem 4 in the main paper.
5. The computational cost comparison between DC programming and dynamic programming in the experiment would provide valuable context.
In conclusion, this is a strong and well-written paper, with the decomposition of the optimal Bellman residual into a difference of convex functions being an original contribution. While the empirical evaluation is weak, it does not detract from the paper's overall merit, given the novelty and potential of the proposed method.