This paper explores a multi-layer conditional model for deep learning, where inputs are sequentially multiplied by weight matrices and passed through nonlinear transfer functions. The goal is to select weight matrices that match known multi-label target vectors in the training data while keeping the norms of the weight matrices small through regularization. Each non-terminal layer produces unobserved outputs, treated as latent variables. 
For two-layer models, reference [26] has shown that optimal weight matrices can be approximated by solving a tractable convex optimization problem, facilitated by two key strategies: (1) softly enforcing nonlinear transfer constraints using a nonlinear penalty function that is linear in the layer's weight matrix, and (2) eliminating bilinear coupling between weight matrices and latent variables through a coordinate transformation that reformulates the problem in terms of each layer's linear response and input kernel matrix.
When extending to a third layer, finding a jointly convex reformulation in the inputs, latent variables, and outputs of any intermediate layer is necessary. For Boolean-valued latent variables, [26] proposes an exact reformulation under specific assumptions, including convex penalty functions and a tight convex relaxation of the output kernel domain.
This paper builds upon [26] to develop an approximate convex formulation for general models with intermediate layers. The approach introduces normalized kernels, defined similarly to ordinary kernels but normalized so that all eigenvalues are either 0 or 1. It also replaces classical regularization with value regularization and assumes specific penalty functions that encode nonlinear relationships. A tight convex relaxation of the normalized output kernels' domain is required, with a suggestion to use simple spectral constraints.
A nested optimization algorithm is proposed to optimize the training objective, with an outer optimization based on a conditional gradient algorithm that optimizes the best training objective as a function of the normalized kernels. This involves a totally corrective update step carried out efficiently via block coordinate descent.
The concept of using normalized kernels is innovative and deserving of publication. Numerical tests demonstrate that three-layer models can offer advantages over two-layer architectures in both synthetic and real-data experiments. However, the loss function's joint convexity in all optimization variables (normalized kernels and propensities) is noted, but the problem contains non-standard constraints that are non-convex, suggesting that the paper may not offer a fully convex model for training multi-layer models as claimed.
The exposition could be improved by explicitly listing all approximations made from the desired model to the solved model and by providing the full model formulations for better understanding. Discussing the differences between the new formulation and the approach from [26] when applied to a two-layer model would also be beneficial. Clarification is needed on whether the methods from [26] extend to multi-layer architectures with Boolean-valued latent variables.
There are concerns regarding the claim that the block coordinate descent algorithm necessarily results in a stationary point, as this may not always be the case. An example is provided to illustrate how an alternating optimization scheme can get stuck at a non-stationary point in a convex maximization problem. The relevance of Theorem 1, a central contribution, remains unclear if a stationary point of the objective function is not guaranteed.
Specific comments include the need for explicit statements on regularization terms, clarification of notation (e.g., the superscript "u" in L^u), and precision in statements about domain approximations and necessary assumptions (e.g., Theta having full row rank). Additionally, there are suggestions for corrections (e.g., Equation (7)) and requests for references to support certain claims (e.g., the computability of Parity). The idea of using normalized kernels is innovative, but the contributions may be somewhat overstated due to the involvement of several approximation steps and the potential issues with the optimization algorithm's convergence to a stationary point.