This paper presents a innovative multi-task framework that utilizes a single annotator shared among multiple online learners, with a proposed algorithm resembling a perceptron that incorporates an exploit-exploration strategy for task selection. The algorithm's analysis provides bounds on expected errors, and it is also applicable to solving two distinct bandit problems. Empirical evaluations on standard machine learning datasets demonstrate superior performance over weak baselines using naive query selection strategies within the same framework. The paper's clarity is commendable. However, a notable limitation is that the proposed algorithm fails to effectively exploit the inter-task relationships to enhance learning, making it less compelling compared to other multi-task learning algorithms. Furthermore, the paper lacks an analysis of the sampling complexity of the proposed algorithm, a crucial aspect in this context, similar to what is found in selective sampling literature.
Detailed comments and suggestions:
1. Including results from a fully informative supervised learning baseline would provide valuable insight into the task's difficulty, enhancing the reader's understanding.
2. Discussing the advantages of learning tasks jointly under the proposed framework versus learning them separately with K selective sampling algorithms would add depth to the paper. Empirical results supporting these points would further strengthen the argument.
The paper proposes a novel multi-task framework and a perceptron-like algorithm with theoretical guarantees. A significant shortcoming of the algorithm is its inability to leverage inter-task relationships to facilitate joint learning, a key aspect of multi-task learning.