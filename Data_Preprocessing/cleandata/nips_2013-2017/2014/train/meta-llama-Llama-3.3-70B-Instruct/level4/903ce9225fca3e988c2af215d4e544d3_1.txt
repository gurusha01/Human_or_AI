This paper tackles a significant problem in the multi-armed bandit literature, specifically investigating the possibility of achieving sub-linear regret bounds in non-stationary environments with finite variation in mean rewards. The authors provide a conclusive answer by establishing matching lower and upper bounds of order O(T^(2/3)) under the assumption that the total variation of mean rewards is bounded by VT â‰¥ 0 for T rounds. This is accomplished by proving an upper bound for a phased version of the EXP3 algorithm, which resets every O((T/VT)^(2/3)) rounds.
The paper is well-structured and clear, making it easy to follow. Upon reviewing the main results and technical proofs presented in the paper, no errors were found. The key aspects of the technical proof are well-explained, and the paper contributes sufficiently to warrant a conference submission.
Several technical points are noteworthy:
1. The setting of non-stationary rewards is closely related to state-dependent ergodic bandits, such as restless bandits. Although the paper acknowledges earlier works in ergodic bandits, it does not comprehensively cover the current state of the field, omitting recent studies like the restless bandit work by Ortner (2012) on regret bounds for restless bandits with unknown dynamics and the bandit for correlated feedback by Azar (2014) for history-dependent reward bandits.
2. The Rexp3 algorithm requires VT as input, which may be restrictive in scenarios with unknown reward variation. It would be beneficial to discuss whether this requirement can be relaxed while maintaining the same regret bounds, particularly in cases where VT is underestimated.
3. The proposed algorithm adapts a variant of EXP3, originally designed for adversarial bandits, to a stochastic problem with time-varying characteristics. While this approach achieves the minimax rate in the worst-case scenario, it does not leverage the stochastic structure of the problem. Exploring problem-dependent bounds that exploit the gap between the best arm and others, potentially by extending stochastic bandit algorithms like UCB to 'forget' in this setting, could yield more efficient strategies.
Overall, the paper presents nice results that are technically strong and rigorous. However, discussing the case of unknown variation bounds is essential for completeness.