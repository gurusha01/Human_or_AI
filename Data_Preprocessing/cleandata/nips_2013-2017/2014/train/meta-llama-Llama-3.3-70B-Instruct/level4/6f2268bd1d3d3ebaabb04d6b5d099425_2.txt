This manuscript addresses a significant challenge in unsupervised machine learning and optimization, namely clustering with outlier detection, a problem that has been extensively explored in both theoretical and practical contexts. Clustering has a rich history, with various types of clustering problems having been investigated, including k-means, k-medians, and k-centers in geometric settings, as well as correlation clustering and spectral clustering in graph theory. However, outlier detection remains a critical issue that can substantially impact the final clustering results.
The authors propose a gradient descent algorithm for clustering with outlier detection, which builds upon the integer programming model presented in [8] by incorporating an outlier component into the constraint. This modified model is then relaxed into a series of Lagrange relaxations, solved using a gradient descent strategy.
The experimental evaluation considers both synthetic and real datasets, demonstrating the advantages of the proposed method over two existing approaches.
The strengths of this paper include:
1. The innovative application of Lagrange relaxation to outlier detection, a novel approach in this context.
2. The algorithm's simplicity and ease of implementation, rendering it a practical solution.
However, there are also some weaknesses:
1. The theoretical analysis is insufficient, particularly in Section 4.2, where more detailed explanations of convergence are needed.
2. The manuscript would benefit from additional references, particularly in computational statistics, where recent techniques such as those described by David Mount et al. in "A practical approximation algorithm for the LMS line estimator" and "On the least trimmed squares estimator" have been developed for trimming outliers in regression and clustering.
Overall, this paper introduces a new Lagrange relaxation approach to a challenging problem in clustering, although further theoretical analysis is required to strengthen the technique.