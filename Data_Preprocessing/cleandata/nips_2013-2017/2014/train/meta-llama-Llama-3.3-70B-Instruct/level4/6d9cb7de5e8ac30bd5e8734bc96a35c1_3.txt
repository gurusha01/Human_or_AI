After reconsidering the authors' feedback, I realize that I may have misinterpreted the "mode" in the experiment, which was the aspect I was asked to focus on, and I have adjusted my evaluation accordingly.
This paper explores an approach to multi-task learning by representing task models as a tensor and imposing a low-rank structure to capture task relatedness, facilitating knowledge transfer among tasks. Unlike existing methods such as latent trace norm, which struggle with heterogeneous task dimensions, the authors propose an enhanced version of latent trace norm that normalizes along each dimension. They derive error bounds for two existing tensor norms and their proposed norm, relating them to the expected dual norm and demonstrating the expected dual norm for all three norms. The unified comparison of different norms presented in the paper is particularly noteworthy. The authors also provide experimental results on both synthetic and real-world data. Overall, this is a solid paper with intriguing theoretical analysis, and the following are my detailed comments.
My primary concern with this paper is the practicality of tensor-based multi-task learning. While it's possible to model "features X aspects X customers" to explore relatedness, the question remains whether it's necessary to extend beyond two-dimensional cases. Any tensor model can be collapsed into a "features X task" matrix and enforce a low rank to capture task relatedness. Although some might argue that this approach misses certain information captured by tensor formulations, it's rare to see significant performance improvements from using tensors. Even if there are minor gains, it's unclear whether the additional computational time would be justified. The lack of comparison to simple matrix trace norm in the experiments exacerbates this concern.
The core innovation of this paper lies in the scaled version of traditional latent trace norm, achieved by introducing a scaling factor to the unfolding along each mode. This implies that each task (and its unfolding) should be treated differently due to specific reasons. This concept bears some resemblance to another paper, "Multivariate Regression with Calibration" by Liu et al., and the authors may find it beneficial to explore connections between the two works.
I would like to inquire about the algorithm used to solve the formulation with the new norm. Is it identical to the one employed in the original latent tensor norm? What is the algorithm's complexity? In cases where the scaled latent trace norm outperforms the original version, what is the convergence time?
As previously mentioned, for multi-task learning, any method using tensors can also be achieved with flat trace norm. The authors should, at the very least, compare their approach to these "flat" multi-task learning methods, such as trace norm, in terms of performance and efficiency. Furthermore, it's unusual to see the authors using a different evaluation metric (explained variance) for the school data, whereas many multi-task learning papers report MSE/NMSE for this dataset. Using MSE for the school data would enhance consistency throughout the paper.
Some minor observations include:
1. The notation "W_{(k)}^{(k)}" in equation (2) and subsequent sections is confusing, as it has not been defined prior to its use.
2. There is a typo, "Turing" instead of "Turning", on page 5, 10 lines from the bottom.
In conclusion, the authors propose a scaled latent trace norm for multi-task learning, accompanied by theoretical analysis and empirical evaluation. The paper appears to be well-structured, but I have some reservations regarding its impact.