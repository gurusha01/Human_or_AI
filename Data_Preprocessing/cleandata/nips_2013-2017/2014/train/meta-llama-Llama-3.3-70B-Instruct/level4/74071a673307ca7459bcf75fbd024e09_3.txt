This paper presents a novel framework for incorporating prior constraints on parameters by leveraging KL-divergence projections, with a specific application to sparse learning where prior knowledge of the parameter support cardinality is available.
* Although the paper introduces an innovative perspective on enforcing parameter constraints through KL-divergence projections, it fails to yield groundbreaking theoretical results or novel algorithms, which somewhat limits its impact.
* The presentation of the proposed algorithm lacks clarity and could benefit from a structured outline, potentially in the form of pseudocode, to enhance readability and understanding.
* The experimental results indicate that the proposed Sparse-G method outperforms other approaches, but the underlying reasons for this superiority are not adequately explained. Furthermore, the significant underperformance of Spike-and-slab compared to Lasso is inconsistent with established literature, suggesting a potential issue with hyper-parameter settings.
* The overall writing is clear, with a few exceptions: 
 - The term "structure" requires more detailed explanation to ensure comprehension.
 - The statement on line 310, asserting that spike-and-slab fails to produce sparse estimates, is misleading since spike-and-slab is recognized as an effective prior for sparse learning. 
The introduction of a framework for learning under prior parameter constraints via KL-divergence projection is intriguing but appears to lack substantial theoretical or algorithmic contributions.