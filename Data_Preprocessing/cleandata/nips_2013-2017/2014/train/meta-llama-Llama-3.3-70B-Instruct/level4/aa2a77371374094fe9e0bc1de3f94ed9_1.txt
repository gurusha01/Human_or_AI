The authors tackle the challenge of interpolation (cokriging) and forecasting on spatio-temporal tensor data, formulating both tasks as a non-convex low-rank tensor optimization problem. They propose a learning method that integrates a forward greedy selection with an optional orthogonal projection step. The experimental evaluation encompasses simulated data, two climatological datasets, and a dataset of FourSquare checkins.
Although I am not an expert in this specific domain and thus cannot comment on the novelty or validity of the proofs, I find the paper to be well-structured and clearly written. The presented greedy approach to low-rank learning appears to be a sensible method, and the experimental results demonstrate significant improvements over other methods. Overall, the paper seems to be robust with no apparent weaknesses.
Minor Points:
- The explanations in sections 2.1 and 2.2 are straightforward, but the equivalence between equations (4) and (1), equations (5) and (3), and the connection between equations (4), (5), and (6) in section 2.3 are not immediately clear. Providing a reference or a more detailed explanation in the supplementary material would be beneficial.
- The discussion of Fig 1c states that the runtime of ADMM increases rapidly with data size, while the greedy algorithm remains steady. However, the runtime appears to increase for all algorithms. If the longest run duration is only 1000 seconds, it would be worthwhile for the authors to further assess the scalability of their algorithm with longer runs.
- It is unclear whether the RMSE used to evaluate cokriging and forecasting performance is normalized. If it is, the authors could provide insight into why their algorithm yields relatively large gains in the CCDS dataset, whereas all algorithms perform well on the Foursquare dataset.
- Line 34 should be revised to "From a machine learning perspective" for grammatical correctness.
- Line 305 should be revised to "five-fold cross-validation" for consistency.
The greedy approach to low-rank learning presented in this paper appears reasonable, and the experiments demonstrate clear improvements over other methods. Overall, this is a solid, well-written paper with no obvious flaws.