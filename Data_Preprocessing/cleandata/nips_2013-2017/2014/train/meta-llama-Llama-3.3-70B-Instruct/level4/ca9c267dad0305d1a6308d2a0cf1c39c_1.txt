This paper introduces a novel approach to parallel multi-task learning, where a single sample is provided for each task at each round, but only one task's sample can be annotated. The authors' method balances exploitation and exploration, with Theorem 1 establishing an upper bound on the expected cumulative mistake count. The algorithm is evaluated against two alternative methods for selecting the sample to annotate.
Overall, the paper is well-written and presents compelling theoretical and experimental results. However, the strict assumption that annotation choices are synchronized across tasks may not be crucial, and a more relaxed assumption of a total annotation budget across all tasks might be more natural. Additional real-world motivation for this specific setting would be beneficial.
To further strengthen the experimental results, two additional comparisons are suggested: 
a. A scenario where annotation is inexpensive and all samples can be annotated, highlighting the trade-off between annotation cost and performance.
b. An active learning approach applied to each task separately, with a controlled total number of annotations equal to the number of learning rounds, allowing for a comparison with a method that has the same annotation budget but a weaker constraint across tasks.
The paper presents an intriguing new problem and provides solid theoretical and experimental guarantees for the proposed solution. To enhance the paper's impact, more justification and experimental evidence for the parallel task learning setting would be valuable, as outlined in the suggested comparisons.