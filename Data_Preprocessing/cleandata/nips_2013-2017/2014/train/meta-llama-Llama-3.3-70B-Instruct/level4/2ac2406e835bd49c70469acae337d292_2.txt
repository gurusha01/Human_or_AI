This manuscript proposes a spectral approach to learning a mixture of hidden Markov models (MHMMs) by leveraging an existing spectral algorithm for hidden Markov models (HMMs) and formulating MHMMs as a special case of HMMs. The authors introduce a stabilization technique to address parameter permutation issues by assuming a unique eigenvalue of 1 for each HMM's transition matrix.
However, there are some notation inconsistencies and a lack of references to related work, such as the use of spectral estimates to initialize expectation-maximization (EM) algorithms, which has been explored in previous studies (e.g., Chaganty and Liang, 2013). The experimental results are somewhat basic and would benefit from being presented in a tabular format, as the current graphical representation in Figure 4 is difficult to interpret.
On the other hand, the proposed method for mitigating parameter permutation issues is innovative and intuitive. The authors also provide an analysis of the algorithm's performance under estimation noise, which leads to an interesting application of eigenvalues (Equation (7)). 
Some specific suggestions for improvement include:
- Clarifying the notation used in Corollary 1, such as defining the term 1J^T and elaborating on the structure of lim{e->inf} bar{A}^e.
- Providing explicit definitions and dimensions for vectors and matrices upon their introduction to enhance the presentation.
- Replacing Figure 4 with tables to improve readability and including the best accuracy result for the EM algorithm, as is standard practice.
- Adopting established notation conventions, such as those used in Hsu et al. (2009), to facilitate understanding.
The paper's core contribution lies in its algorithm for learning MHMMs, which combines an existing spectral HMM learning algorithm with a novel stabilization step to resolve parameter permutation issues.