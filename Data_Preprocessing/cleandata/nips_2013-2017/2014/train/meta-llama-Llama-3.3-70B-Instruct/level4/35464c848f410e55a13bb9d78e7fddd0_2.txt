The authors introduce a novel approach to supervised and semi-supervised learning, leveraging Fredholm kernels, which presents a fascinating methodology. The paper provides an in-depth examination of various aspects of the method. Overall, the submission is found to be highly intriguing.
However, several suggestions and inquiries remain:
- Section 2 (equations 1 through 99):
The Nystrom method also involves an approximation of an integral equation, specifically in the context of kernel PCA. It would be beneficial for the authors to acknowledge this connection and elucidate the similarities and differences between the two approaches.
- Section 2, equation 2:
The literature on support vector machines and kernel methods includes approaches based on operator equations. The authors should cite this relevant work and clarify how equation 2 relates to it, either as a special case or by highlighting the distinctions.
- Section 2, line 128, representer theorem:
As equation 2 is less commonly encountered in learning theory, the authors' assumption of a representer theorem warrants further justification. Either a proof of the representer theorem should be provided or a precise reference should be given, along with an explanation of how equation 2 fits into the broader context.
- Section 4, equation 8:
The interpretation of the method as soft-thresholding PCA is a compelling and interesting contribution. A comparison with other methods, such as Fisher kernels and probability product kernels, would be useful to understand the distinctions and relative merits.
- Typographical errors:
Line 165: "Not that" should be revised.
Line 221: "achived" should be corrected to "achieved".
Line 341: "chossing" should be corrected to "choosing".
Line 672: The table reference is unclear and should be revised.
The proposed approach to supervised and unsupervised learning using data-dependent Fredholm kernels offers a new perspective on the field.