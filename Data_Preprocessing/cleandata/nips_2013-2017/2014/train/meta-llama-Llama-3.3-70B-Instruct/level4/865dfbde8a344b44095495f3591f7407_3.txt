This manuscript presents a novel approach to solving Hidden Markov Models (HMMs) using Stochastic Variational Inference (SVI), offering a significant improvement over existing methods. The proposed algorithm effectively samples subchains from sequence data while addressing the critical issue of dependency breaking at subchain edges. This is achieved by introducing a buffer around the target subchain and iteratively expanding its scope until convergence of subchain posteriors across varying buffer lengths is attained. The experimental results demonstrate the superior efficiency and applicability of this method to real-world data, providing valuable insights into the application of online learning algorithms to time-dependent models.
Minor comments:
In Equation (1), the parameter phi is notably absent from p(y1|x1).
On Line 355, the reference to "Table 4" should be corrected to "Table 1".
Regarding Table 1, several key details are unclear: the predictive log-likelihood value, the percentage of held-out data, and the specific settings of hyperparameters used in the experiment. Furthermore, the decision not to compare ||A - A0||_F in this context makes it challenging to interpret the error in the log-predictive case.
Throughout the experiments, the authors only specify the hyperparameter setting for k, without providing details on other hyperparameters.
On Page 8, in the context of the human chromatin segmentation experiment, the runtime for Dynamic Bayesian Networks (DBN) is not reported. This work represents a substantial advancement in tackling large-scale HMM inference challenges.