Summary: The authors address the problem of learning a mixture of Hidden Markov Models, proposing a spectral learning algorithm to estimate parameters for a hidden Markov model, followed by a method to resolve permutation ambiguity in the transition matrix and recover its underlying block-diagonal structure.
Major Comments:
1. While the paper is generally well-written, the experimental results section requires expansion, particularly in terms of providing more detailed insights into the experiments conducted.
2. The approach relies on expressing a mixture of Hidden Markov Models as a single HMM, leveraging the fact that the mixture's block diagonal structure is sparse. However, the algorithm's requirement for accurately estimating full HMM parameters before reversing the permutation may not be feasible for large problems with limited training data. The authors should discuss the appropriateness and limitations of their algorithm in real-world scenarios.
3. The recovered transition matrix may not retain sparsity in practice. It is essential to investigate whether the spectral algorithms can guarantee a sparse or near-sparse transition matrix, considering the potential existence of full transition matrices close to the target sparse model.
4. The experimental results may not accurately represent the impact of noise in the transition matrix. Instead of permuting a transition matrix with noise sampled from a Dirichlet distribution, the authors should generate observation data from a mixture of HMMs and learn parameters from the observed data to better assess the algorithm's performance in the presence of noise.
5. The experiment in Section 4.2 is a step in the right direction, but lacks crucial details about the mixture of HMMs used, such as the parameters of the transition and observation matrices. Providing this information, potentially in an appendix, would facilitate a more thorough evaluation of the experiment.
6. The results in Table 1, where EM initialized with spectral learning performs worse than spectral learning alone, are counterintuitive and require explanation, as EM should theoretically improve the results.
Minor Comments:
1. In the proof in Section 3.2.1, the subscript of Î› in the bottom right corner of the first matrix should be K, not 2.
2. The text and numbers in the figures are too small and should be enlarged for better readability.
The paper presents interesting ideas and sound theoretical results, but its practical applicability to real datasets is a concern.