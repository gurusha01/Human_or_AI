This paper examines the estimation of the mean of a d-dimensional normal distribution with unit covariance, utilizing nm samples distributed across m machines, and investigates the interplay between communication complexity and dimensionality. The authors present several key findings, including:
- The inability to reduce communication complexity through joint processing of multiple dimensions,
- The potential for reduction in communication complexity when dealing with sparse means,
- An improved upper bound in the scalar case, and
- An enhanced lower bound under simultaneous protocols.
I offer a few minor suggestions for improvement:
1) It would be beneficial to highlight the transition of \theta from a non-random variable to a random variable following Definition 1, a common technique in minimax theory. Additionally, the introduction of conditional mutual information given \theta prior to Definition 1 should be clarified, as it strictly applies to \theta as a random variable, despite being a constant with probability 1.
2) Corollary 3.1 requires a qualification, as the stated minimax rate of d\sigma^2/(nm) does not hold universally. For instance, in a scenario with \sigma^2 = 1000000, n = m = d = 1, the estimator \hat{\theta} = 0 can achieve better performance, indicating the need for a more nuanced statement.
3) In the description of Protocol 2, the last two lines contain a subscripted Yi in the argument of \hat{\theta}i, which appears to be an error and should be corrected to remove the subscript. Overall, this is a well-crafted paper presenting several intriguing ideas.