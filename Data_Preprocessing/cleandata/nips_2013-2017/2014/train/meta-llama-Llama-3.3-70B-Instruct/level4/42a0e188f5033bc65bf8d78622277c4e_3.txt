This manuscript presents a novel randomized parallel variant of the Alternating Direction Method of Multipliers (ADMM), termed PADMM, which is capable of handling problems with multiple blocks through a palatalization approach. In each iteration, PADMM randomly selects K blocks, updates the corresponding block primal vector, and subsequently updates the dual vector using a backward step. The incorporation of this backward step is crucial, as it renders the dual update more conservative, thereby facilitating global convergence. Furthermore, the algorithm allows for the addition of a proximal term to the primal update, which simplifies the solution of certain optimization problems. The authors provide a rigorous theoretical analysis of the algorithm, establishing its global convergence and iteration complexity. Specifically, when dealing with J blocks and randomly selecting K blocks at each iteration, the algorithm's convergence rate is O(J/(TK)) after T iterations.
The primary advantages of the PADMM algorithm are twofold: (1) it enables full parallelization of the primal step, and (2) it permits relatively large update step sizes compared to other methods. Notably, PADMM can outperform the sequential ADMM (sADMM) due to its ability to accommodate greater step sizes. Additionally, PADMM offers greater flexibility than the Parallel Jacobian ADMM (PJADMM) and superior parallelizability compared to the Gauss-Seidel ADMM (GSADMM). The authors have demonstrated the algorithm's efficacy through experiments on robust principal component analysis and overlapping group lasso, yielding promising results. The PADMM algorithm achieves the desired accuracy with reduced computational time. However, the authors have not reported on the performance of PADMM with parallel implementation. Compared to traditional ADMM, PADMM's three parameters can be less convenient to tune.
In summary, this work constitutes an interesting contribution to the solution of ADMM-type problems, particularly in scenarios involving multiple constraints. The algorithm exhibits the same convergence rate as ADMM while offering parallelization capabilities and demonstrating promising practical performance. Overall, this is a notable contribution to the field, with the paper being well-written and both the theoretical and empirical components being solid.