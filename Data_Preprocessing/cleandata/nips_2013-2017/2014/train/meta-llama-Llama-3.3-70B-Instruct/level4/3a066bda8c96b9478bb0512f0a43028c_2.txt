This paper presents a revised SVM learning algorithm designed to handle weakly supervised data by introducing a per-example weight, ranging from 0 to 1, for each training example, which is then incorporated into the SVM's loss function. The approach is further generalized to scenarios where these per-example weights are latent variables.
The methodology is clearly outlined, and experimental results suggest potential benefits of this weighted approach over uniform weighting for tasks like semantic segmentation and object detection. Nevertheless, the distinction between this proposed method and existing techniques utilizing example-dependent costs, a well-established concept in machine learning, is not clearly articulated. For instance, SVMlight already implements a similar mechanism.
The extension to handle unobserved per-example weights is noteworthy yet straightforward, leading to a non-convex problem without accompanying theoretical analysis. The use of a strong regularization term implies that the per-example weights are constrained to remain close to their initial values, prompting the question of whether the authors can provide further insight into this aspect.
On a minor note, the assertion on line 50 that the problem is not a standard regression due to the bounded interval [0, 1] overlooks the fact that logistic regression, which is indeed standard, operates within this very interval. Additionally, there's a minor spelling error in the abstract, where "continues" should be corrected to "continuous". Overall, while the paper proposes modifying the SVM algorithm with per-example weights, it bears a strong resemblance to existing example-dependent cost functions widely recognized in the machine learning community.