This study aims to develop generative models of sensory signals in the focused region, and the authors have proposed a model that effectively learns to generate frontal faces from large images containing faces amidst distractors.
The manuscript is generally well-structured and provides a comprehensive overview of relevant research, drawing from neuroscience, computer vision, and machine learning. Its broad scope makes it a valuable resource for a wide range of readers.
The proposed model is formulated in a versatile manner, allowing for potential application to diverse datasets. However, its applicability is constrained by key assumptions, notably the concept of a "canonical image" (represented by variable "v") and the assumption of similarity transformation. Consequently, the model is suited for generating data models of "attended image regions" but not the underlying three-dimensional objects, whose sensor observations involve more complex transformations beyond similarity. This limitation may explain why the experiments in section 6 only explored frontal faces and did not consider faces from different viewpoints or other objects. A clearer discussion of these limitations in relation to the overarching goal would enhance the manuscript. The paper presents an intriguing approach to the challenging problem of generating models of attended image regions by integrating prior research from various fields. Although thought-provoking, the proposed model still appears to have significant limitations.