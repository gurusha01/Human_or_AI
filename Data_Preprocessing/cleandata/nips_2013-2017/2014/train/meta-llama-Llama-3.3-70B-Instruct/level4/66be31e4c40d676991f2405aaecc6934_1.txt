The authors have introduced a formal framework for learning with pseudo-ensembles, which provides a unified explanation for various existing learning methods, including dropout in deep neural networks. Furthermore, they propose the PEV regularization technique, designed to enhance the robustness of learned models, specifically the activations of hidden units across different layers in deep neural networks, under model space perturbations. Notably, since PEV regularization does not rely on label information, it seamlessly extends to semi-supervised learning scenarios. Through comparisons with several baseline methods across supervised, semi-supervised, and transfer learning settings, the authors demonstrate the effectiveness of their approach.
The concept of pseudo-ensembles builds upon and generalizes earlier work focused on robust learning under input space perturbations, as seen in the contributions of Burge & Scholkopf, Chapelle et al., Maaten et al., and Wager et al. The PEV regularization, in itself, is an intriguing concept that not only matches the performance of dropout in supervised learning but also outperforms it in semi-supervised and transfer learning scenarios.
In their attempt to link PEV regularization with dropout in Section 4.1, the authors utilize the pseudo-ensemble objective defined in equation (1) to explain dropout under limiting conditions. However, the explanation falls short in convincingly demonstrating that learning with PEV regularization approximates dropout. The reasoning that both methods' success can be attributed to discouraging co-adaptation, based solely on similar performance metrics, lacks depth.
Starting with equation (1), an alternative, potentially more natural formulation for the regularization could involve penalizing the variance of the output layer's distribution, rather than summing penalties across all layers from 2 to d, as presented in equation (3). It would be beneficial for the authors to clarify the rationale behind the current formulation and to quantify the potential performance degradation if penalties on hidden layers were to be omitted.
Minor observations include the lack of explanation for Figure 1 in the text and the notable improvement in performance observed with DAE pre-training (PEV+) in Table 2, even with PEV regularization. This suggests that PEV regularization might be less effective in leveraging unlabeled data. The work presents an interesting regularization technique for multi-layer neural networks, drawing motivation from dropout and pseudo-ensemble learning, with potential applications to other model classes. Although the direct connection to dropout in Section 4.1 is not clearly articulated, the PEV regularization demonstrates satisfactory performance, making it a worthwhile contribution for a specific segment of the community.