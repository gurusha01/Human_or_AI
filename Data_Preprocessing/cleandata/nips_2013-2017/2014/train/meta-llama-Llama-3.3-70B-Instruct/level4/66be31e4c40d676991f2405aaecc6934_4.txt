This manuscript presents a novel regularization technique for neural networks, which encourages the model to minimize the variance of each hidden layer representation when subjected to dropout noise or other perturbations in the underlying layers. The concept is further extended to "pseudo-ensemble" models, where various types of perturbations can be applied.
The primary contribution of this work lies in the introduction of the variance regularizer. The authors conduct experiments on several datasets, including MNIST (under supervised and semi-supervised settings) and the NIPS'11 transfer learning dataset (comprising CIFAR-100 and TinyImages), using standard neural networks with dropout perturbations. Additionally, they experiment with the Stanford Sentiment Treebank dataset using Recursive Neural Tensor Networks and different types of perturbations. The results demonstrate that this regularizer performs equally well or better than using perturbations alone.
The strengths of this paper include:
- The model yields promising results in challenging scenarios with limited labeled data.
- The experiments are carefully chosen to showcase the applicability of this method to various models and datasets.
However, there are some weaknesses:
- Certain sections of the paper appear redundant, such as the discussion on Baggy/Boosty PE, which does not seem to add significant value to the manuscript, assuming the primary focus is the variance regularizer.
- Some crucial experimental details are missing, which are outlined below.
The authors should provide clarification or discussion on the following points:
(1) The number of noise samples used to compute the variances.
(2) Whether back-propagation was performed through each dropped-out model or only the clean one.
(3) One of the significant drawbacks of dropout is that it increases training time. This approach likely exacerbates this issue by requiring multiple forward and backward propagations per gradient update (with or without noise, or with different noise samples to compute the variance). It would be beneficial to analyze the slow-down by plotting training and test error versus time (instead of the number of epochs).
(4) The stopping criterion for the semi-supervised MNIST experiments is unclear. The previous section mentions training all networks for 1000 epochs without early stopping. It is essential to know if this applies to the semi-supervised experiments and if the same number of epochs was used even with only 100 labeled cases. It is likely that the models would overfit severely on the labeled data sizes considered, even with dropout, by the end of 1000 epochs for reasonable learning rates.
(5) If multiple (say $n$) forward and backward propagations are performed per gradient update in PEV, would it be fair to compare models after running for a fixed number of epochs? Each PEV update might be equivalent to roughly $n$ regular SDE updates.
(6) For the semi-supervised case, did each mini-batch contain a mixture of labeled and unlabeled training cases? If so, what fraction were labeled?
(7) Consider comparing with SDE+ in Table 1.
(8) Was the same architecture and dropout rate used for SDE and PEV in Table 1? If yes, is this a fair comparison? Perhaps SDE could be made stronger by using a smaller network or the same network with a higher dropout rate.
Quality:
The experiments are well-designed, but additional explanations and comparisons, as requested above, would enhance the quality.
Clarity:
The paper is well-written, with minor typos.
Originality:
The variance regularizer is a novel contribution.
Significance:
This paper has the potential to significantly impact researchers working with small datasets, as it presents an interesting approach to regularizing models. The experiments are convincing, but the manuscript can be improved by adding more details and clarifications.