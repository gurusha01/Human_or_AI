This manuscript presents an extension of the traditional Bayesian formulation of conventional Support Vector Machines (SVMs), building upon a previous formulation that utilized a Gaussian mixture likelihood. The authors enhance this formulation by introducing priors on the scale-mean parameter, resulting in a skewed Laplace likelihood that deviates from the conventional hinge loss. Furthermore, the extension of the Gaussian prior on the weight vectors to a Gaussian process yields a nonlinear SVM formulation. The authors propose two distinct optimization methods for this purpose.
The paper provides a compelling example of how the conventional SVM framework can be effectively extended to a Bayesian setting through a well-defined formulation incorporating prior distributions. As noted by the authors, previous work [4] established a connection between an infinite Gaussian mixture formulation and a hinge loss likelihood that yields the SVM solution, albeit with an improper formulation due to a flat prior. The current work demonstrates how this formulation can be extended and how inference can be performed using a well-defined approach. The results illustrate the transition of the likelihood from a hinge-type to a skewed Laplacian, with the explanation being clear and engaging. However, the derivation of the predictive distribution in Eq. (11) is not entirely clear to me. I would appreciate it if the supplementary material included this derivation or if a detailed derivation could be provided in the paper or supplementary material if the paper is accepted.
Overall, the paper is well-structured, and the proposed formulation appears to be correct. It has the potential to serve as a valuable resource for NIPS readers, showcasing a Bayesian formulation of SVMs. The clarity of the paper makes it accessible to a broad audience, and the technical content is sound, making it a worthwhile contribution to the field.