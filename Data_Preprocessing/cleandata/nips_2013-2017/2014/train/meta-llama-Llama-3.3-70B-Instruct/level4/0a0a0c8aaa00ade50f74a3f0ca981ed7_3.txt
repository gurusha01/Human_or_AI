This paper offers a refreshing perspective on the long-standing issue of associative memory in neural networks, providing significant new insights into a well-established problem. The authors note that previous research has often overlooked crucial biological aspects, such as Dale's law, and has relied on binary memory representations in rate-based models, as well as non-sparse representations for memories. Although these issues have been addressed individually in other studies, the authors develop a comprehensive strategy that simultaneously tackles all these concerns. Specifically, they utilize a recent control-theoretic approach based on spectral bounding techniques to construct networks that address these issues. This approach involves a cost function that balances three terms to achieve the desired outcome through gradient descent. The first term contributes to the fixed-point nature of the desired memories, the second term ensures the stability of these fixed points, and the third term promotes small weights, preventing saturation. By directly incorporating Dale's principle into the weight parameterization, the authors effectively address the shortcomings of previous work, resulting in a network with naturally balanced excitation and inhibition. Notably, although they do not explicitly enforce saturation in single neuron firing, this emerges naturally from the solution to the optimization problem. The authors focus on using excitatory neurons to implement memory, consistent with the fact that these neurons typically send output to other regions, and treat inhibitory neurons as variables to be optimized. The results are validated through numerical simulations, demonstrating the network's noise robustness. In addition to showcasing the network's good associative memory properties, the authors explain experimental findings that trial-to-trial variability decreases following stimulus onset, attributing this to convergence to the closest attractor. 
In summary, this is a well-structured and clearly written paper that tackles a classic problem in neural computation using innovative control-theoretic tools, demonstrating how a biologically plausible associative memory may function. Although the authors do not propose a biologically plausible learning mechanism at this stage, they hint at recent proposals that may lead to such rules. The utilization of inhibitory neurons as resources is an interesting and beneficial aspect. Overall, the paper presents a clear, concise, and important contribution to neural computation.
A few minor issues require attention:
- The choice of \gamma=0.04 in equation (3) should be discussed in terms of its sensitivity, although it is expected to be minimal.
- The proposal of a log-normal distribution for memory states on line 115 needs motivation.
- The potential for the cost function (4) to yield solutions that violate the exact fixed-point nature of memories should be quantified and discussed.
- The statements on lines 254-256 are unclear and require clarification.
- The authors should address capacity issues, even if only preliminary, as this is a crucial computational aspect.
- Discussing the computational benefits of Dale's law, specifically whether it enhances computational power under certain constraints, would be beneficial. This could relate to evolutionary and/or biophysical constraints, providing a more comprehensive understanding. 
Overall, the paper is clear, well-written, and tackles a classic problem in neural computation with novel tools, demonstrating a biologically plausible associative memory.