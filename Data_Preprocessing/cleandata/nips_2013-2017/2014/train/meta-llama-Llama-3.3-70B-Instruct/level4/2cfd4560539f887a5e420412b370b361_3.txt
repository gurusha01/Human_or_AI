This manuscript presents a novel approach to utilizing 'deep' recursive networks by introducing depth across space, drawing inspiration from the successes achieved with recurrent networks. Each layer in the proposed architecture possesses its own set of parameters, which are shared across different heights within that layer. The authors also incorporate dropout and ReLUs into their model, while utilizing pre-trained embeddings for word representations. The experimentation is conducted on the sentiment treebank dataset introduced by Socher et al.
The proposed methodology is innovative, essentially generalizing the approach used with recurrent networks. The authors demonstrate strong performance on the sentiment treebank dataset, with notable performance gains achieved through the use of dropout and ReLUs. However, some of the design choices regarding the model and experiments are questionable or lack sufficient justification, and additional experimentation is necessary to fully support the recommendation for acceptance.
The paper is generally well-written and provides sufficient detail for readers to replicate the results. 
Detailed comments and questions include:
- Line 138: The definitions of $W^{\eta}$ and $W^{xh}$ are unclear, as they have not been previously defined. It would be beneficial to clarify whether these refer to $W^{\eta}L$ and $W^{\eta}R$. Consider presenting two separate equations for the inputs and hidden layers, and provide a summary of the parameter space, including the dimensionalities of each matrix.
- Lines 148-154: This discussion appears speculative and lacks empirical evidence. If this is based on the authors' own results, it should be explicitly stated. The concern raised is not entirely convincing and requires further justification.
- Baseline: The authors should include a comparison with the results from "A convolutional neural network for modelling sentences" by Kalchbrenner et al, presented at ACL this year.
- The decision not to fine-tune the word vectors is puzzling, as fine-tuning can enable the word embeddings to capture sentiment. This seems counterintuitive, given that the phrase and sentence vectors are discriminative of sentiment. It is possible that the improvements achieved with deep RNNs are due to the lack of fine-tuning, and a single-layer RNN with fine-tuned embeddings might perform better. This variable should be controlled for in the experiment.
- Line 251: The motivation behind using shared dropout units is unclear and requires further explanation.
- Lines 255-256: As an alternative to the current approach, the authors could consider constraining the norms of the weights or truncating the gradients, as is sometimes done with recurrent nets.
- Binary classification: This should be conducted as a separate experiment, as the current protocol deviates from the standard approach by including neutral class examples during training. Consequently, the results are not directly comparable to existing methods.
- The analogy with deep recurrent nets is reasonable when making predictions at each node in the tree, as in the treebank experiments. However, it would be beneficial to investigate whether deep RNNs offer an advantage when only a single, root label exists. Would the authors still expect to see an improvement over a single-layer RNN? The use of deep RNNs appears promising, but additional experiments are necessary to fully demonstrate its effectiveness. It is recommended that the authors redo the binary classification experiment following the proper protocol, study the effect of fine-tuning word embeddings, and include one or two additional datasets that only use a global label. These additional experiments would significantly strengthen the paper and provide more robust evidence for the usefulness of depth across space.