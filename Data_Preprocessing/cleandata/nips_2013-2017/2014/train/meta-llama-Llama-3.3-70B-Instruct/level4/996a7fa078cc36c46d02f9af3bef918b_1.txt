This manuscript presents a novel framework for reinforcement learning that incorporates options, which are policies with a probability of terminating at specific states. The authors propose the concept of an "option policy," a high-level policy enabling multi-step state transitions. They demonstrate how to render the option model reward-agnostic and provide a TD-style algorithm for learning with these models.
The proposed universal option model appears to be a significant contribution, offering a theoretically sound and intriguing approach. It seems to effectively address the "memoryless" limitation of Markov Decision Processes (MDPs) while preserving their simplicity. Although I am not a specialist in reinforcement learning and therefore cannot assess the novelty of this research, I find the model to be elegant and useful. Consequently, I support accepting this paper, as it presents an interesting reinforcement learning model using options, which I believe warrants further consideration despite my inability to evaluate its novelty.