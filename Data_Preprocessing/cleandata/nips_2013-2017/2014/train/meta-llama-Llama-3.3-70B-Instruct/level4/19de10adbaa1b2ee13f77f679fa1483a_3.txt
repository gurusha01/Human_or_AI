The authors have presented a convolutional neural network (CNN) that integrates top-down processing through attention-based feedback, yielding state-of-the-art results on the CIFAR datasets. 
This manuscript is engaging and exhibits innovation in its model architecture and application to vision tasks, showcasing a forward-thinking approach by considering both bottom-up and top-down signals in CNN training. A notable aspect is the evaluation of the model over time and the utilization of gated signals to constrain representation, which is commendable. Given that the quality of learned representations is heavily dependent on the attention policy, it would be enlightening for the authors to explore or demonstrate scenarios where the algorithm might fail, providing a more comprehensive understanding.
The experimental section is intriguing, with the authors employing various demonstration methods, including visualization, to facilitate a deeper understanding of the model. However, considering the complexity of the method compared to traditional feedforward CNNs, the inclusion of results from the ILSVRC dataset would significantly enhance the paper by demonstrating its scalability to larger datasets with larger images, which is my primary concern. The synergy between attention-based feedback and CNNs results in a compelling model worthy of further investigation. Additional practical insights derived from experiments on more extensive datasets would be highly valued, offering a more robust validation of the proposed approach.