The authors propose a novel framework for multi-armed bandits with non-stationary rewards, leveraging the concept of constraining the total variation of expected reward changes across arms. They establish a lower bound and present an extended version of the EXP3 algorithm, demonstrating a matching upper bound in relation to the time horizon dependency.
The paper's organization and writing style are commendable, with thorough citations of related work and a reasonable connection to preceding studies.
However, the motivation behind the model could be more convincing. Although theoretically sound, the practical applicability of imposing a maximum bound on expected reward variation is not immediately clear, as verifying such a property in real-world scenarios may be challenging. The authors should enhance the presentation to address this aspect. Nevertheless, this formulation yields an intriguing intermediate setting between stochastic and adversarial bandit environments.
The presentation of the proof sketch for Theorem 1 is engaging, with concise and lucid exposition of the underlying ideas.
A significant concern with the paper is the definition of Rexp3. While the algorithm achieves an optimal bound in the worst case by restarting an EXP3 instance at each epoch, this approach seems impractical due to the complete loss of accumulated knowledge. Furthermore, the assumption that V_T is known or can be learned warrants discussion, as it is unclear why this should be a reasonable expectation.
Regarding Theorem 2, it is notable that the proof holds for VT >= 1/K. The behavior of the algorithm when VT approaches 0 (i.e., the stationary case) and becomes significantly smaller than 1/K is unclear, as this scenario is not covered by the theorem. Although this case is not explicitly addressed, it appears to be a natural and relevant consideration.
After reviewing the author feedback, which I find satisfactory, I have increased the score to 6, placing it marginally above the acceptance threshold. The paper is well-written and tackles a relevant problem, but due to the aforementioned concerns with the algorithm, I am inclined to recommend rejection.