The authors extend the Stochastic Variational Inference (SVI) framework of Hoffman et al. (2013) to Hidden Markov Models (HMMs), proposing a novel approach termed SVIHMM. Unlike previous SVI applications that utilize minibatches of complete data items, SVIHMM employs minibatches of subsequences, leveraging external observations to initialize messages on either side of the subsequence. This concept is intriguing, particularly given the introduction of a heuristic method to determine the requisite number of observations for subsequence initialization. The efficacy of this heuristic is somewhat surprising, as one might anticipate issues with long-term dependence in message passing. Furthermore, questions arise regarding the GrowBuf algorithm, such as whether it terminates immediately when S^new equals S^old, and the potential relationship between the optimal tau value and the second-largest eigenvalue of matrix A.
Unfortunately, the overall quality of the paper is compromised by several issues. Notably, key figures and tables from experimental results are missing, including table 4 and timing experiments. The introduction and literature review are excessively lengthy, delaying the presentation of the paper's core content until page 4 and resulting in an overreliance on supplementary materials. Crucially, the GrowBuf algorithm, essential for understanding the paper, is relegated to the supplement. The reported False Discovery Rate (FDR) of 0.999026, compared to the DBN's FDR of 0.999038, raises questions about the significance of this difference and the potential trade-offs in computational efficiency, as no comparative results are provided. Additionally, the estimate's noise level and its impact on the FDR assessment remain unclear. In summary, while the paper presents a straightforward adaptation of SVI to HMMs with a heuristic for subsequence training, its incomplete presentation and lack of crucial details hinder a comprehensive evaluation.