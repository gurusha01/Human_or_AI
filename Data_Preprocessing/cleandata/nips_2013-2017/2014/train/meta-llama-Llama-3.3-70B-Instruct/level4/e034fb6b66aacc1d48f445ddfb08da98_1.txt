This manuscript presents a novel approach to enhancing word embeddings in vector space by integrating side information through an "attribute vector" that adjusts the word-projection matrices, essentially creating word-projection tensors with factorized representations, where the attribute vector serves as the loadings for the tensor slices. Although this concept is explored within the context of log-bilinear language models, its applicability extends to other word embedding research. 
The theoretical aspects of the paper are well-articulated and provide a solid foundation. However, the experimental section lacks clarity and depth. A more focused approach with fewer, yet more in-depth experiments would be preferable to the extensive, yet somewhat superficial, analysis presented. The initial experiments in section 3, as reflected in Tables 2 and 3, are not particularly illuminating, as the capability to generate text is not unique to the proposed model and can be achieved by simpler models like n-gram language models. Furthermore, crucial details such as the initial learning rate, decay factor, momentum, and momentum increase factor used in these experiments are not provided.
In section 3.1, the definition and rationale behind the choice of attributes are unclear. It is essential to specify whether the attributes are derived from sentence vectors averaged over all sub-phrases or another method and to justify this selection. 
Section 3.2 requires notation adjustments for clarity, specifically distinguishing that S originates from l, while S' and Ck come from l', and considering the use of vl instead of v. Additionally, the nature of the attributes (x) in this context needs explanation. The reference to a 1-hot attribute vector in Figure 1 is misleading, as it actually pertains to the language-id vector. The significant distance between "Germany" and "Deutschland" compared to the closeness of other country translations warrants further investigation, potentially revealing interesting aspects of the model's learning process, such as a distinction between concepts based on language.
The dimensionality of the attribute vector used in section 3.3 seems arbitrary, given the presence of 234 unique attributes. A 100-dimensional vector is utilized without clear justification. Overall, while the paper innovatively introduces the concept of attribute vectors to modulate word embeddings, the theoretical presentation is strong, but the experimental analysis requires refinement for greater impact and clarity.