This paper introduces DeepGMM, a novel model that extends the traditional Mixture of Gaussians (MoG) by incorporating an exponential number of components with tied parameters. Each component in the mixture corresponds to a unique path through a fully connected multilayer network of affine transformations, applied to a multivariate Gaussian with identity covariance. Notably, while the parameters within each component are tied, the weights associated with each component are not, although the authors suggest a factorization approach for very deep networks to mitigate this issue.
The authors' central hypothesis is that by tying parameters across the multitude of components, it becomes feasible to utilize a larger number of components without succumbing to overfitting. To train DeepGMM, they propose a hard Expectation-Maximization (EM) algorithm. The expectation phase leverages coordinate descent and several heuristics to reduce computational complexity. For the maximization phase, three optimization methods are presented: a batch Gradient Descent (GD) method suitable for DeepGMMs with a limited number of paths, another batch GD method tailored for DeepGMMs applied to data of sufficiently small dimensionality, and a Stochastic Gradient Descent (SGD) method for larger DeepGMMs.
Experimental evaluations are conducted on two well-established datasets, with all experiments employing the second optimization technique mentioned. The results demonstrate the efficacy of DeepGMM, particularly in its ability to train models with a large number of effective components without overfitting, outperforming untied MoG models with fewer but untied parameters.
Quality-wise, the paper is technically sound, with its main hypothesis well-supported by experimental evidence. For instance, Figure 4 illustrates that tying parameters enables the training of a DeepMoG with 2500 effective components, which surpasses the performance of an untied MoG with 300 components, despite the latter having more parameters. However, the experiments are limited to natural images, which might uniquely benefit from the inductive bias of DeepGMM.
In terms of clarity, the paper is well-structured and readable, albeit with minor typos. There are also discrepancies, such as the maximum value reported in Figure 4 and Table 1, which could indicate different experimental setups. Reporting training times would provide valuable insight into the model's practicality. Furthermore, showing the effective number of components in Figure 4 instead of the number in the top layer could better illustrate the model's capability to avoid overfitting with many components. Including samples from the model and clarifying the preprocessing of the tinyimages dataset would also enhance the paper.
The technique presented for parameter tying in a GMM is novel, and the proposed training algorithms, including the heuristics for computational efficiency, are of significant interest. The results are important, offering state-of-the-art performance on natural image patches. Although the model's computational cost for training and density evaluation at test time is high, the efficiency of sampling from the model and the novelty of the idea make it a valuable contribution. Overall, the paper presents an innovative approach to MoG parameter tying, achieving state-of-the-art results on natural image patches and offering an interesting foundation for future research.