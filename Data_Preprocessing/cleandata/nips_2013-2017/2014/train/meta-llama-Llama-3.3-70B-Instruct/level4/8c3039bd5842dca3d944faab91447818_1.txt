This manuscript presents a novel concept of deep Gaussian mixture models, where a Gaussian mixture model can be viewed as a single isotropic unit norm Gaussian transformed by various linear transformations. The authors extend this idea to a multilayer network, where each node represents a linear transformation and each path through the network corresponds to a sequence of transformations, resulting in the number of mixture components being equivalent to the number of paths through the network.
The paper is well-written, and the idea is both straightforward and effective. Overall, the paper is enjoyable to read. However, the experimental results are somewhat disappointing, particularly given the emphasis on making the algorithm computationally tractable. The low dimensionality of the results is unexpected.
It is also worth noting that the proposed algorithm bears a strong resemblance to the work of Tang, Yichuan, Ruslan Salakhutdinov, and Geoffrey Hinton, "Deep mixtures of factor analysers" (International Conference on Machine Learning, 2012), which deserves citation and discussion.
Releasing the source code for the experimental results as supplemental material would be highly beneficial, as it would make the paper more convincing, increase citation counts, and promote a culture of reproducible science.
More detailed comments are as follows:
39 - The phrase "in function" should be replaced with "as a function."
73 - The notation "A3,1" should be corrected to "A1,3."
89 - While the authors provide compelling arguments for the parallelizability of their algorithm, it is unclear whether EM is more parallelizable than stochastic gradient descent in general. A more specific statement may be warranted.
139 - The exponential number of paths through the network necessitates factorization to assign probabilities, as assigning probabilities without factorization would become intractable for even relatively small networks.
199 - The MAP approximation is commonly used, but an alternative approach could involve sampling a single nonzero gamma with probability proportional to pi, which would provide an unbiased estimate of the learning gradient and potentially lead to a higher log likelihood model.
230 - The phrase "in function of" should be replaced with "as a function of."
235 - The plot does not appear to converge to the optimum.
251 - The phrase "and scalable" should be rephrased to "and in a scalable way."
307, 341 - For quasi-Newton optimization with minibatches and without hyperparameters, the authors may consider using the Sum-of-Functions-Optimizer (https://github.com/Sohl-Dickstein/Sum-of-Functions-Optimizer).
312-318 - The benefits of EM are well-supported by the provided arguments.
331 - The phrase "have to construction" should be corrected to "have to construct."
337 - The phrase "with using" should be replaced with "using."
344 - The observed effect may be a consequence of the MAP approximation to the posterior rather than incomplete optimization during the M step. As EM maximizes a lower bound on the log likelihood, the lower bound increases even if the M step is not run to convergence.
Figure 4 - The x-axis may not be the most suitable choice for the Deep GMMs, as the number of components in the top layer is not particularly significant. The number of parameters might be a more appropriate x-axis.
436 - The author's name is listed as "anonymous."
The idea presented in the paper is simple, effective, and clearly presented. While the paper is enjoyable to read, the experimental results are somewhat disappointing.