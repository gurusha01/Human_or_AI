This paper investigates the active learning problem in the context of linear regression, where the goal is to enhance the constant factors in distribution-dependent convergence rates compared to passive learning. Unlike classification, active regression aims to improve these constant factors, as the asymptotic dependence on the number of labels and worst-case constant factors typically cannot be improved. The authors argue that there exists a distribution-dependent constant factor in the convergence rate of passive learning that can be improved through active learning.
The proposed approach involves a rejection sampling scheme that modifies the sampling distribution to a more favorable one without altering the optimal solution. This scheme relies on a scaling function φ, which requires careful setting and depends on the joint distribution of (X, Y). Since this distribution is unknown, the algorithm optimizes the choice of φ among piecewise constant functions using an estimated linear function from an initial sample. The authors provide a risk bound for this method, which can approach the "oracle" rate and is sometimes superior to passive learning methods.
Overall, this contribution appears to be solid and is likely to appeal to a broad audience. However, one notable reservation is the lack of discussion on the dependence between K and ρ*_A in Theorem 5.1. The trade-off between these quantities affects the rates, and examples illustrating this trade-off for reasonable distributions and sensible partitions would be beneficial.
Additionally, a minor comment is that the citation of Efromovich (2005) is missing, which also explores active regression in a nonparametric class and finds improvements in constant factors based on heteroscedasticity. Furthermore, the notation for Pφ seems unusual, and it is believed that the intention is to define Pφ as the distribution of (X/√φ(X), Y/√φ(X)) for (X, Y) ∼ Qφ, where Qφ is the measure with density φ with respect to D. This would make Pφ well-defined, and the equation L(w, D) = L(w, Pφ) would follow via the law of the unconscious statistician. 
The paper presents a valuable contribution to improving distribution-dependent constant factors in linear regression through active learning, and its findings are likely to resonate with a wide audience.