This paper proposes an asynchronous variant of SAGA, a stochastic gradient method for finite sum convex optimization (Defazio et al, 2014). To my understanding, the paper addresses two challenges: asynchronicity and proximity. The authors achieve three improvements over the original SAGA. First, they propose a sparse update based on block coordinate wise using extended support for the gradients when the regularization is decomposable. Second, they design an asynchronous variant of SAGA where the delay quantity can be up to sqrt(n)/10. Finally, they can deal with nonsmooth regularizers via proximal operators. In terms of convergence results, the still achieve a linear convergence rate under the strong convexity of the overall sum function f, and individual Lipschitz gradient of fi. Although the step-size is slightly smaller than the one of SAGA, the convergence factor remains comparable. This is probably due to different assumption. 
In fact, the paper combines several advanced ideas from existing works such as SAGA with variance reduction, sparse updates, Hogwild, asynchorinization such as Arock, etc, to improve over all these techniques. The proof is quite clean and well organized.
In my opinion, such improvements are significant and are important in practice due to obvious reasons. The numerical results also strongly support their theoretical contribution. These experiments are carried out on three large-scale data sets, and empirically show a linear speed up of the new algorithm.
Overall, this paper has significant contribution both in terms of theory and experiments. It merits to be accepted for NIPS.
Minor comments.
Some concepts and notation should be defined. For example, support (supp), \Omega(\cdot), inconsistence read.
The phrase "step size" should be consistent on all text. For example, on line 46 and line 142 they are not consistent.
Line 152, delete "a" or "the".
Line 361, the y should be in bold.
Line 127: remove one "a".