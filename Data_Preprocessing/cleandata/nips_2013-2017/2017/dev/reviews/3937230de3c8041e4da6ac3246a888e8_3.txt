This paper proposes a memory-augmented generative model that performs stochastic, discrete memory addressing, by interpreting the memory as a non-parametric conditional mixture distribution. This variational memory addressing model can combine discrete memory addressing variables with continuous latent variables, to generate samples only with few samples in the memory, which is useful for few-shot learning. The authors implement a VAE version of their model and validate it for the few-shot recognition tasks on the Omniglot dataset, on which it significantly outperforms the Generative Matching Networks, which is an existing memory-augmented network model. Further analysis shows that the proposed model accesses relevant part of the memory even with hundreds of unseen instances in the memory. 
Pros:
- Performing discrete, stochastic memory addressing for memory-augmented generative model is a novel idea which makes sense. Also, the authors have done a good job in motivating why this model is superior to soft attention approach.
- The proposed variational addressing scheme is shown to work well in case of few-shot learning, even in case where existing soft-attention model fails to work. 
- The proposed scheme of interpreting the memory usage with KL divergence seems useful.
Cons
-Not much, except that the experimental study only considers character data, although they are standard datasets. It would be better if the paper provides experimental results on other types of data, such as images, and compared against (or coupled with) recent generative models (such as GANs)
Overall, this is a good paper that presents a novel, working idea. It has effectively solved the problem with existing soft-attention memory addressing model, and is shown to work well for few-shot learning. Thus I vote for accepting the paper. 
- Some typos:
Line 104: unconditioneal -> unconditional
Line 135: "of" is missing between context and supervised.
Line 185: eachieving -> achieving
Table 1 is missing a label "number of shots" for the numbers 1,2,3,4,5,10 and 19.