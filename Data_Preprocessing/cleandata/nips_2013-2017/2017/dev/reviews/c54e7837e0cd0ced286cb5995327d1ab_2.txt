This is an interesting paper, nice and neat. Batch normalization has proven to be an effective technique for dealing with internal covariate shift and has been widely used these days in the training of neural networks with a deep structure. It is known that BN has issues with small-size mini-batches. First of all, it gives rise to unreliable estimates of mean and variance of each mini-batch. Second, the mean and variance of the whole population, which is used in classification or inference, is computed by a moving average of mini-batches during training. This is a mismatch. This paper proposes a simple way to cope with the two issues in the conventional BN. It introduces another affine transform to correct the bias between the local and global normalization and this so-called renormalization makes the normalization in training and inference matched. The idea is simple but it seems to be fairly effective from the reported experimental results. 
I have no real criticism to lay out regarding the paper. However, I think the experiments can be more convincing if more results can be reported on mini-batches with a variety of sizes to give a full picture on the behavior of this batch renormalization technique. I am particularly curious about the case where the size of a mini-batch is down to one. In this case, the conventional batch normalization doesn't work any more but this renormalization can still be applied.