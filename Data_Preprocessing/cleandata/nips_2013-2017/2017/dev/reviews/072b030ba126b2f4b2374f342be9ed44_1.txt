This paper considers solving the finite sum optimization via the SAGA framework. This work extends from Leblond et al. 2017 by considering the composite optimization, where the additional part is the non-smooth separable regularization. The key improvement lies on how to deal with the non-smooth regularization while obey the principle of sparse update. The proposed trick to split the non-smooth regularization looks interesting to me. The analysis basically follow the framework in Leblond et al. 2017, the proof for the asynchronous variant follows the idea in Mania et al. 2015.
Major comments:
- Authors need to explain the "inconsistent read" more clear. In particular, what is hat{x}_k and how to decide the index of k. Is it the same as Lian et al. 2016 and Liu and Wright 2015? What is the key difference?
- It is unclear to me how to obtain the neat representation of the difference between hat{x}t and xt.
- To obtain the linear convergence rate and speedup, this paper makes several implicit assumptions. Authors should explicitly indicate the assumption used in this paper. 
Minor comments / typos:
- The definition of \Delta in line 217 was incorrect.
Missing reference:
The following is related to asynchronous greedy SDCA.
- Asynchronous parallel greedy coordinate descent, NIPS, 2016.