Recently there has been a lot of interest in the Algorithmic Game Theory community in designing approximately revenue optimal auctions given a small number of samples of the buyer's valuations. A pertinent question is what is the minimum number of samples required to design a truthful auction that provides an additive epsilon approximation to the optimal revenue. A bound on this quantity is usually obtained by relating it to learning theory concepts like VC dimension and pseudo-dimension. This paper proposes a new notion of split-sample complexity and applies to obtain bound on the sample complexity for common auction scenarios. 
The notion of split-sample complexity is as follows: Given a set of m samples, for any given subset of size m/2, we can identify an optimal hypothesis. Denote by \hat{H}S the set of optimal hypotheses over all size m/2 subsets of S. The split sample growth rate is how large the set \hat{H}S can get over all sets of size S. 
The authors show that given m samples, the expected revenue obtained from the optimal hypothesis over these m samples, can be related to the optimal revenue for the given distribution and an additive error term. This additive error term relates to the split-sample complexity as a function of m. Then for specific classes of auctions sample complexity bounds can be obtained by bounding the split-sample complexity.
This new approach allows the author to strengthen previous bounds for bundle and item pricing mechanisms by Morgernstern and Roughgarden. 
Overall this paper is well written and provides a new technique for tackling an ongoing research agenda.