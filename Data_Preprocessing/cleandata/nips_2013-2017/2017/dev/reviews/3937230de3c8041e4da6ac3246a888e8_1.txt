The authors propose variational memory addressing. It augments
generative models with external memory and hard attention, and
interestingly, derives read and write mechanisms that mimick more
classical probabilistic graphical models than the more sophisticated
mechanisms as in, e.g., neural Turing machines.
In their formulation, external memory acts much like a global variable
in topic models and mixture models, whether they sample a "membership"
given by the hard attention and proceed to generate the local variable
z and data x conditional on memory indexed by this membership. I found
this a particularly useful way of understanding memory in the context
of latent variable models, where writing corresponds to inference.
As the authors note in, e.g., L175-186, it seems the algorithm does
not scale well with respect to the external memory size. This can be
justified mathematically as the the variance of the black box
gradients with respect q(a) parameters increases with the size of a.
It is unlikely that VIMCO can help much in this regard. That said, I'm
impressed that the authors were able to get interesting results with
|M| up to 1024.