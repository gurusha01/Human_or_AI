In this paper, the authors have proposed to estimate m weighted automata based on spectral learning algorithms applied to the associated Hankel tensor. Specifically, they apply rank factorization via SVD to the Hankel Tensor. They also how this factorization can then be used to learn individual functions. I have the following comments: 
Is the factorization technique proposed here for Hankel tensor a novel contribution of this paper? 
In Definition 2, the authors introduce a measure of relatedness tau. Is there any specific reason for this specific definition? I wasn't able to relate it to other quantities in the paper; more detail on the purpose of defining this quantity would be very helpful. Would it dictate the choice R - the common rank?
I was hoping that the benefit of estimating multiple functions together would be apparent through dependence of error measures on tau in Theorem 5 (maybe in a more general version of it) or computational complexity. 
Specifically, it seems the computational complexity is worse than when each individual WFAs are estimated separately. Can the relatedness not be leveraged in any way? I certainly feel that it would be very interesting and illuminating to see a more general version of Theorem 5. If the WFAs are minimally related - would the performance be worse than doing it individually? Specially, in simulation what if dS were 0 or at least less than dT. It seemed concerning that even when there is commonality - if dS and dT are same MT-SL seems to offer no benefit compare to simple SL.
In showing the benefit of multiple learning at least for maximally related WFAs, the authors note that the estimation error for `enough` tasks would be O(T^2) compared to O(T) if done individually - where T is || E ||F/sR(H). I couldn't follow why O(T^{2}) would be better that O(T); is T <= 1?