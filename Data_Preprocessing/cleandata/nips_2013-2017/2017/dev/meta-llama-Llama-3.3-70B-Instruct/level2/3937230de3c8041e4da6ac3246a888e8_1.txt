This paper proposes a novel approach to augmenting generative models with external memory, interpreting the output of a memory module with stochastic addressing as a conditional mixture distribution. The authors apply variational inference to memory addressing, enabling effective training of the memory module using target information to guide memory lookups. The model is evaluated on the tasks of generative few-shot learning and unconditional density modeling, demonstrating its ability to identify and access relevant memory contents even with hundreds of unseen Omniglot characters in memory.
The paper's main claims are well-supported by theoretical analysis and experimental results. The authors provide a clear and concise explanation of the model's components, including the memory buffer, addressing variable, and variational posterior distribution. The use of VIMCO to estimate gradients for the discrete latent variable is a key contribution, allowing for effective training of the model.
The paper is well-written and easy to follow, with clear explanations of the model's architecture and training procedure. The authors provide a thorough discussion of related work, highlighting the differences between their approach and existing methods. The experimental results are impressive, demonstrating the model's ability to perform reliable approximate inference over memory locations and scale to large memory sizes.
The paper's strengths include its novel approach to memory-augmented generative models, its ability to scale to large memory sizes, and its impressive experimental results. The weaknesses include the potential computational cost of using VIMCO to estimate gradients and the need for further evaluation on more complex datasets.
Arguments for acceptance include:
* The paper proposes a novel and well-motivated approach to memory-augmented generative models
* The model is evaluated on a range of tasks, including generative few-shot learning and unconditional density modeling
* The experimental results are impressive, demonstrating the model's ability to perform reliable approximate inference over memory locations and scale to large memory sizes
* The paper is well-written and easy to follow, with clear explanations of the model's architecture and training procedure
Arguments against acceptance include:
* The potential computational cost of using VIMCO to estimate gradients
* The need for further evaluation on more complex datasets
* The lack of comparison to other state-of-the-art methods for generative few-shot learning and unconditional density modeling
Overall, I believe that this paper makes a significant contribution to the field of generative models and memory-augmented neural networks, and I recommend acceptance.