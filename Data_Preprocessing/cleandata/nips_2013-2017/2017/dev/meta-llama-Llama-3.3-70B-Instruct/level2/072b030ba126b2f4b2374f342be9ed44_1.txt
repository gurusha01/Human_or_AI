This paper proposes PROXASAGA, a novel asynchronous parallel variant of the SAGA algorithm, designed to solve large-scale composite optimization problems with sparse gradients and block-separable penalties. The main claims of the paper are: (1) PROXASAGA achieves a theoretical linear speedup with respect to its sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term, and (2) empirical benchmarks demonstrate that PROXASAGA significantly outperforms state-of-the-art alternatives on large sparse datasets.
The support for these claims is provided through a combination of theoretical analysis and empirical experiments. The theoretical analysis establishes the convergence guarantee of PROXASAGA, showing that it converges geometrically in expectation with a rate factor that matches the sequential SAGA algorithm. The empirical experiments demonstrate the practical speedups achieved by PROXASAGA on several large-scale datasets, with speedups of up to 12x on a 20-core machine.
The paper is well-written, and the ideas are clearly presented. The authors provide a thorough analysis of the algorithm, including a discussion of the difficulties in extending the SAGA algorithm to the composite setting and the key insights that enable the development of PROXASAGA. The experimental results are convincing, and the comparison with other state-of-the-art methods is thorough.
The usefulness of the ideas presented in the paper is high, as PROXASAGA has the potential to significantly accelerate the solution of large-scale composite optimization problems, which are common in many areas of machine learning and signal processing. The paper also provides a detailed analysis of the algorithm's properties, including its convergence guarantee and speedup, which will be useful for practitioners and researchers alike.
The paper demonstrates a good understanding of the field, citing relevant previous work and providing a clear discussion of the relationships between PROXASAGA and other algorithms. The novelty of the paper is high, as PROXASAGA is a new algorithm that addresses a significant gap in the existing literature.
The completeness of the paper is good, with a clear presentation of the algorithm, its analysis, and the experimental results. The limitations of the paper are acknowledged, including the assumption of sparsity in the gradients and the block-separability of the proximal term. The authors also provide suggestions for future work, including the extension of PROXASAGA to other proximal incremental schemes and the exploration of convergence guarantees without sparsity assumptions.
Overall, I would recommend accepting this paper, as it presents a significant contribution to the field of optimization and machine learning. The paper is well-written, and the ideas are clearly presented, making it a valuable resource for researchers and practitioners alike.
Arguments pro acceptance:
* The paper presents a novel algorithm that addresses a significant gap in the existing literature.
* The theoretical analysis is thorough, and the convergence guarantee is established.
* The empirical experiments demonstrate significant speedups over state-of-the-art alternatives.
* The paper provides a clear discussion of the relationships between PROXASAGA and other algorithms.
* The authors acknowledge the limitations of the paper and provide suggestions for future work.
Arguments con acceptance:
* The assumption of sparsity in the gradients and the block-separability of the proximal term may limit the applicability of PROXASAGA.
* The paper could benefit from a more detailed comparison with other asynchronous optimization algorithms.
* The experimental results could be strengthened by including more datasets and comparing with additional state-of-the-art methods.