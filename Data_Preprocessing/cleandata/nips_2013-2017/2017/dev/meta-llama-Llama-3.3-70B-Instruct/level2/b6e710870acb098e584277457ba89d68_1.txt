This paper proposes a new stochastic variance-reduced algorithm, Breg-SVRG, for solving saddle-point optimization problems. The algorithm is designed to adapt to the underlying geometry of the problem by using a suitable Bregman divergence, which can lead to a significant reduction in the condition number and improved convergence rates. The authors demonstrate the effectiveness of Breg-SVRG on two example applications: adversarial prediction and LPboosting.
The paper makes several key contributions. First, it extends the notion of Bregman divergence to saddle functions and proves a new Pythagorean theorem that is used to analyze the convergence of the algorithm. Second, it proposes a new stochastic variance-reduced algorithm, Breg-SVRG, which is designed to work with any Bregman divergence. The algorithm is shown to have a linear rate of convergence, which is a significant improvement over existing algorithms that have a sublinear rate of convergence. Third, the authors demonstrate the effectiveness of Breg-SVRG on two example applications, including adversarial prediction and LPboosting.
The paper is well-written and provides a clear and concise explanation of the proposed algorithm and its analysis. The authors provide a detailed proof of the convergence of the algorithm and demonstrate its effectiveness on several example problems. The paper also provides a thorough discussion of the related work and highlights the key contributions of the paper.
One of the strengths of the paper is its ability to adapt to the underlying geometry of the problem. By using a suitable Bregman divergence, the algorithm can take advantage of the structure of the problem and achieve improved convergence rates. This is particularly important for problems with a large number of variables, where the condition number can be very large.
Another strength of the paper is its ability to handle non-Euclidean norms. Many existing algorithms are designed to work with Euclidean norms, but the proposed algorithm can handle any norm that is induced by a Bregman divergence. This makes the algorithm more flexible and able to handle a wider range of problems.
The paper also provides a thorough discussion of the related work and highlights the key contributions of the paper. The authors provide a clear and concise explanation of the proposed algorithm and its analysis, and demonstrate its effectiveness on several example problems.
One potential weakness of the paper is that the algorithm requires the computation of a proximal update, which can be computationally expensive. However, the authors provide a detailed analysis of the computational complexity of the algorithm and demonstrate that it is comparable to existing algorithms.
Overall, the paper makes a significant contribution to the field of optimization and provides a new and effective algorithm for solving saddle-point optimization problems. The algorithm is well-suited for problems with a large number of variables and can take advantage of the structure of the problem to achieve improved convergence rates.
Arguments for acceptance:
* The paper proposes a new and effective algorithm for solving saddle-point optimization problems.
* The algorithm is designed to adapt to the underlying geometry of the problem and can take advantage of the structure of the problem to achieve improved convergence rates.
* The paper provides a thorough discussion of the related work and highlights the key contributions of the paper.
* The authors provide a clear and concise explanation of the proposed algorithm and its analysis, and demonstrate its effectiveness on several example problems.
Arguments against acceptance:
* The algorithm requires the computation of a proximal update, which can be computationally expensive.
* The paper does not provide a thorough comparison with existing algorithms, which makes it difficult to evaluate the effectiveness of the proposed algorithm.
However, overall, the paper makes a significant contribution to the field of optimization and provides a new and effective algorithm for solving saddle-point optimization problems. Therefore, I recommend acceptance.