This paper explores the solution of finite sum optimization problems using the SAGA framework, building upon the work of Leblond et al. (2017) by incorporating composite optimization with non-smooth separable regularization. The primary innovation lies in the approach to handling non-smooth regularization while adhering to the principle of sparse updates. The proposed method of splitting the non-smooth regularization appears to be a notable contribution. The analysis largely follows the framework established by Leblond et al. (2017), while the proof for the asynchronous variant aligns with the ideas presented in Mania et al. (2015).
Major comments:
- The authors should provide a clearer explanation of the "inconsistent read", specifically defining \hat{x}_k and the process for determining the index k. It would be beneficial to compare and contrast this with the approaches outlined in Lian et al. (2016) and Liu and Wright (2015), highlighting the key differences.
- The derivation of the concise representation of the difference between \hat{x}t and xt is not immediately clear and requires further clarification.
- To achieve linear convergence rates and speedup, this paper relies on several implicit assumptions that should be explicitly stated to ensure transparency and reproducibility.
Minor comments and typos:
- The definition of \Delta on line 217 is incorrect and should be revised.
Missing reference:
- A relevant work on asynchronous greedy SDCA is the paper "Asynchronous parallel greedy coordinate descent" presented at NIPS in 2016.