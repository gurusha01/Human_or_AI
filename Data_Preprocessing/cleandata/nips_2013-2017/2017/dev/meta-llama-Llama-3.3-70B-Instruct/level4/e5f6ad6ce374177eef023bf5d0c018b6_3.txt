This paper presents a novel architecture for next frame video prediction, leveraging a convolutional LSTM approach that integrates spatial and temporal variations into a unified memory pool, distinguishing it from prior efforts.
Comments
The paper is well-structured and proposes a promising new approach, demonstrating effectiveness on two relatively simple datasets. However, including generated videos with the submission would enhance the presentation.
While the results appear favorable, evaluating the proposed approach on real, natural images would provide more compelling evidence of its efficacy. Notably, the KTH dataset, described as comprising "natural image sequences," is relatively simplistic due to its low resolution and uniform foreground and background, which should not pose a challenge given the claimed memory efficiency of the approach. Additionally, providing an estimate of the training time would be beneficial.
On lines 84-86, the authors assert the applications of video prediction as factual across numerous domains without citing references, leaving the reader wondering if these applications are currently in use or potential future uses. To clarify, the authors should either provide references or rephrase the statement (from "finds" to "could find").
The introduction of the deconvolution operator on lines 105-106 is unclear, as it seems unused in the subsequent equations and the rest of the paper.
Minor issues include:
- Line 47: Missing comma.
- Lines 57-58: "they ... always" should be "they always," and "the next one" should be "the next."
- Line 135: Missing comma.
- Line 141: "in in" should be "in."
- Lines 135-137: The sentence is unclear.
- Line 154: "We" should be "we."
- Line 242: "ignores" should be "ignore."
Furthermore, references [9] {LSTM} ICML 15, [15] ICLR workshop 16, [19] ICLR 16, [23] {SVM}, and [24] ICLR 15 should be verified, and other references should also be checked for accuracy.