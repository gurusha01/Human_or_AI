Recently, the Algorithmic Game Theory community has seen a surge of interest in developing approximately revenue-optimal auctions based on a limited number of samples of buyer valuations. A key question in this context is determining the minimum number of samples needed to design a truthful auction that achieves an additive epsilon approximation of the optimal revenue. Typically, bounds on this quantity are established by drawing connections to concepts from learning theory, such as VC dimension and pseudo-dimension. This paper introduces a novel concept called split-sample complexity and applies it to derive bounds on sample complexity for common auction scenarios.
The concept of split-sample complexity is defined as follows: Given a set of m samples, for any subset of size m/2, an optimal hypothesis can be identified. The set of optimal hypotheses over all subsets of size m/2 is denoted by \hat{H}S. The split-sample growth rate refers to the maximum size of the set \hat{H}S across all sets of size S. 
The authors demonstrate that the expected revenue obtained from the optimal hypothesis based on m samples can be related to the optimal revenue for the underlying distribution, along with an additive error term. This error term is linked to the split-sample complexity as a function of m. By bounding the split-sample complexity, the authors derive sample complexity bounds for specific classes of auctions.
This innovative approach enables the authors to improve upon previous bounds for bundle and item pricing mechanisms established by Morgernstern and Roughgarden. 
Overall, the paper is well-written and presents a new technique for addressing an ongoing research agenda, providing a valuable contribution to the field.