Summary
This paper presents an algorithm designed to handle a mixed online and stochastic setting, where the objective function consists of a sequence of arbitrary convex functions with bounded subgradients, subject to stochastic convex constraints also with bounded subgradients. The authors conduct a static regret analysis, providing both expected and high-probability bounds. The proposed method is illustrated through a real-world experiment on job allocation across servers to minimize electricity costs, demonstrating its benefits.
Assessment:
+ The paper has an overall clear structure and presentation.
+ The technical content appears sound and novel.
+ The regret guarantees provided are good.
- However, the connection to "Deterministic constrained convex optimization" is confusing.
- There is some confusion regarding unreferenced previous work, specifically [A].
- The experimental section lacks detail.
Further details are provided below.
Further comments
- In Section 2.2, the algorithm's rationale is explained, including the choice of x(t+1) to minimize the "drift-plus-penalty" expression. It would be beneficial to provide intuition on how the importance of drift and penalty are balanced, particularly why a simple sum without a tuning weight is sufficient.
- Regarding Lemma 5, it is unclear whether it represents a new drift analysis lemma for stochastic processes, as the reviewer lacks expertise in stochastic process literature.
- Potential improvements could be explored by making Î± and V dependent on t.
- For Lemma 9, it seems that the authors could leverage results from Proposition 34 in [B] for simplification.
- The comparison with "OCO with long term constraints" reveals that [A], which is not referenced, already provides O(sqrt(T)) guarantees using similar techniques, suggesting that this related work should be discussed.
- The comparison with "Stochastic constrained convex optimization" raises questions about whether [16] is the only relevant reference.
- The acceptance of non-feasible solutions in the stochastic setting, as opposed to the deterministic setting where feasible solutions are expected, is confusing and warrants explanation, especially considering the constraint violation in O(1/sqrt(T)).
- Experiments:
  - More details on baselines, implementation, and starting points are necessary for reproducibility and should be included in the main article.
  - Clarification on the set \mathcal{X_0} and whether problem (2) is solved with box constraints is needed.
  - A log-scale for unserved jobs (Figure d) might enhance clarity.
  - Larger figures would improve readability.
  - An assessment of result variability, such as through repetitions to display standard errors and means, is missing.
  - To accommodate more experimental details, Section 4 could be condensed and moved to supplementary material.
- Extending the analysis to the dynamic regret setting could be a valuable addition.
Minor
- Line 52 contains a typo, where "min" should be "argmin." The assumptions under which the argmin of the problem reduces to the single x^* are not clear.
- Line 204 (a) appears to be an inequality rather than an equality.
[A] Yu, H. & Neely, M. J. A Low Complexity Algorithm with O(sqrt(T)) Regret and Constraint Violations for Online Convex Optimization with Long Term Constraints preprint arXiv:1604.02218, 2016
[B] Tao, T.; Vu, V. Random matrices: universality of local spectral statistics of non-Hermitian matrices The Annals of Probability, 2015, 43, 782-874
==================
post-rebuttal comments
==================
The authors' rebuttal is acknowledged, although the discussion about [A] was not addressed. After reviewing other comments, the score remains unchanged. However, it is emphasized that the authors should clarify the discussion regarding the stochastic and deterministic constrained convex optimization cases, as partially addressed in the rebuttal.