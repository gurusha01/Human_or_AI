The authors introduce a novel approach called variational memory addressing, which enhances generative models by incorporating external memory and hard attention. Notably, this method derives read and write mechanisms that resemble those found in traditional probabilistic graphical models, rather than the more complex mechanisms employed in models like neural Turing machines.
In this framework, external memory functions similarly to a global variable in topic models and mixture models. The hard attention samples a "membership" and then generates local variables z and data x conditioned on the memory indexed by this membership. This perspective on memory in latent variable models, where writing is analogous to inference, is a valuable insight.
As acknowledged by the authors, particularly in lines 175-186, the algorithm's scalability is limited by the size of the external memory. This limitation can be mathematically attributed to the increasing variance of the black box gradients with respect to the q(a) parameters as the size of a grows. Although VIMCO may not significantly mitigate this issue, the authors have nonetheless achieved impressive results with external memory sizes up to 1024, which is a notable accomplishment.