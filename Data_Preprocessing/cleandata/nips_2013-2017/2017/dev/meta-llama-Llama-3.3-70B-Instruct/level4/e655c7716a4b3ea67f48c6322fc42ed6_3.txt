SUMMARY
This paper investigates the problem of multitask learning for Weighted Finite Automata (WFAs) by introducing a concept of task relatedness and proposing a novel algorithm. The algorithm combines Hankel matrices from multiple tasks and applies an adapted spectral learning method to obtain a vector-valued WFA with a unified state representation. A post-processing step is also introduced to reduce the dimensionality of the WFA for each task, thereby minimizing noise. The proposed algorithm is evaluated against a baseline that involves learning each task separately, using both synthetic and real-world datasets.
COMMENTS
Overall, the paper is well-written, but there are some concerns regarding the experimental section. Specifically, it would be beneficial to compare the proposed algorithm to a baseline where data from all tasks are aggregated and treated as a single task. This approach should theoretically outperform other methods when all tasks are identical, as it leverages the entire dataset. However, when tasks are unrelated, this method may lead to asymptotic approximation errors. It would be interesting to investigate the magnitude of this error in practice, particularly on the datasets used in the experiment. If the error is negligible, it may indicate that the datasets are not sufficiently challenging, as any algorithm that aggregates data would likely show improvement over single-task learning. Preliminary results on this comparison would be appreciated, if possible, during the rebuttal phase.
Additionally, comparing the proposed algorithm to its variant without the projection step would help quantify the improvement brought by this post-processing procedure. 
The paper's presentation could be enhanced by discussing the application scenarios of multitask learning for WFAs. For instance, in natural language modeling tasks, predictions need to be made in different contexts (e.g., online chat vs. newspaper articles), and datasets are available for each context. In such cases, it is natural to expect that basic grammar is shared across datasets and can be learned jointly. While aggregating all datasets into a single large dataset and building a unified model is possible, this approach neglects the context information available during prediction.
Two further suggestions are:
- The current algorithm implicitly assumes equal weights for all tasks, which may not be suitable when dataset sizes vary significantly across tasks. In such cases, a weighted approach, as described in Kulesza et al.'s work on Low-Rank Spectral Learning with Weighted Loss Functions, might be more appropriate.
- Another motivation for the projection step is to eliminate redundancy in the model. When tasks are unrelated and each requires a separate set of states, single-task learning would result in a much larger model size compared to the proposed multitask learning approach. The projection step helps mitigate this issue.
MINOR ISSUE
On line 93, it is mentioned that the empty string is included in prefixes or suffixes. However, according to the PSR literature, this is not a requirement. The authors may want to verify this.
==============================
The rebuttal and additional results are appreciated, and there are no further complaints. The argument for acceptance will be maintained.