This manuscript demonstrates that specific adversarial prediction problems involving multivariate losses can now be resolved more efficiently. The approach relies on two primary concepts: (1) simplifying the general saddle function optimization problem outlined in equation (9) to reduce complexity from exponential to quadratic in relation to sample size, and (2) leveraging an extended version of the stochastic variance reduction gradient (SVRG) method, adapted for Bregman divergences, to solve the simplified optimization problem with added regularization.
The paper's central theme is accelerating the solution of adversarial problems, although it concentrates on the F-score loss for its key simplification. This raises questions about the potential to broaden the applicability of the proposed method to other types of losses. Notably, the extension of SVRG to accommodate Bregman divergences appears to be a more universally applicable result, suggesting that the manuscript could have alternatively focused on introducing Breg-SVRG, with the adversarial optimization using the F-score serving as a specific example.
Overall, I find the paper to be technically sound and sufficiently engaging to warrant acceptance.