The authors suggest utilizing a low-rank approximation of second-order pooling features to enhance action recognition. They demonstrate the efficacy of their approach on three prominent vision benchmarks, showcasing improvements over the baseline.
Strengths:
(+) The proposed method is straightforward and can be integrated with any network architecture
(+) It yields a clear improvement over the baseline
Weaknesses:
(-) The novelty of the approach is unclear in comparison to established second-order pooling methods
(-) A fair comparison with other attention-based models is lacking
I am uncertain about the novelty of the proposed approach compared to other low-rank second-order pooling methods, which are commonly used for fine-grained recognition (e.g., Lin et al, ICCV2015) and semantic segmentation. Although the authors provide a clear explanation of the linear algebra and interpretation of the variables used, the final approach essentially boils down to a classifier applied to second-order features.
Furthermore, the authors make design choices without providing clear justifications. For instance, in multi-class recognition, they opt for a class-agnostic 'b' but a class-specific 'a' without explaining the rationale behind this decision. Additional experiments are necessary to demonstrate the effectiveness of the proposed design.
The authors claim to eliminate the box detection step present in other approaches, such as R*CNN or Mallya & Lazebnik. However, they fail to discuss how their approach would generalize to instance-specific action recognition, where an image may contain multiple individuals performing different actions, and action labels are assigned at the instance level rather than the frame level. The authors should address how their approach can handle this more general setup of instance-level labels, which is currently a limitation of their method.
The authors also fail to provide a fair comparison with competing approaches. They use a ResNet101 or Inception-V2 architecture as their base network and demonstrate improvement over their own non-attentive baseline using the same architecture. However, competing approaches, such as R*CNN or Mallya & Lazebnik, employ a VGG16 network, making the comparison unfair and inconclusive on the MPII and HICO datasets. On the HMDB dataset, the improvement over the TSN BN-Inception method is marginal, and comparisons with other methods are unclear due to the varying underlying base architectures.