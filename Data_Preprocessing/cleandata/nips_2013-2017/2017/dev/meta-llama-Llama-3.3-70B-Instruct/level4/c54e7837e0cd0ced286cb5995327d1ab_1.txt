This paper introduces the Batch Renormalization technique as a solution to the issues associated with batch normalization when dealing with small or non-i.i.d minibatches, a crucial consideration in applications where large neural network models are trained with limited GPU memory. The proposed method is straightforward and easy to implement. Experimental results demonstrate its effectiveness with non-i.i.d minibatches and its ability to improve outcomes for small minibatches compared to traditional batch normalization.
The authors commence by providing a comprehensive overview of batch normalization, highlighting its key limitations: the inconsistency between the mean and variance used during training and inference, and the instability encountered with small minibatches. A potential initial approach might involve using moving averages for normalization, but this could lead to model instability. Therefore, the authors suggest a simple yet effective batch renormalization method that combines minibatch mean and variance with moving averages. In essence, Batch Renormalization gradually transitions from using the original batch normalization (with minibatch mean and variance) to a version that relies almost exclusively on moving averages. This approach allows the model to leverage the advantages of moving averages, facilitating successful convergence.
Several questions arise from this paper:
(1) It is unclear why Batch Renormalization does not offer an advantage over batch normalization when using large minibatch sizes (e.g., 32). The consistency of mean and variance between training and inference seems to provide minimal benefit in such cases.
(2) Given that experiments show worse results for small minibatch sizes (e.g., batch size = 4) compared to larger ones (e.g., batch size = 32), it would be interesting to explore the use of multiple moving averages (for mean and variance) with different update rates (e.g., 0.1 and 0.01). A smaller update rate could help address the inconsistency issue, while a larger rate might mitigate the problems associated with small minibatch sizes.
(3) The paper lacks an analysis of how the parameters rmax and dmax influence performance, suggesting a need for extensive parameter tuning.
Overall, this work is commendable, and it will be interesting to see if future research can provide a more refined solution to the challenges posed by batch normalization.