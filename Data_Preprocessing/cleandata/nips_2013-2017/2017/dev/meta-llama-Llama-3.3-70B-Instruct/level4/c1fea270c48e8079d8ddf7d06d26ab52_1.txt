This manuscript investigates the minimization of decomposable submodular functions, a crucial problem in machine learning that has been extensively studied, with existing exact algorithms suffering from high polynomial time complexity, rendering them impractical. To circumvent this limitation, the focus is often on submodular functions that can be expressed as a sum of submodular functions over a significantly smaller support, known as DSFM, which frequently occur in practical scenarios.
The paper presents an improved analysis of the fastest algorithms for DSFM, achieving a speedup by a factor equivalent to the number of functions in the decomposition. Additionally, it introduces an experimental framework that categorizes algorithms into "level 0" and "level 1", where level 0 algorithms serve as subroutines for quadratic minimization, and level 1 algorithms utilize level 0 as a black box to minimize the function. This framework enables a more meaningful comparison of different algorithms using the same level 0 subroutines, revealing a tradeoff between discrete algorithms that require more level 0 subroutine calls and gradient methods that have weaker level 0 requirements but incur more level 1 computational costs.
The analysis is intricate, leveraging both discrete and continuous optimization techniques to substantially improve the running time of an important problem with expensive computational complexity. The experimental results highlight an interesting tradeoff, suggesting that the choice of algorithm for DSFM minimization should be context-dependent. 
However, a notable drawback of the paper is its dense writing style, which can be challenging to follow at times. Further discussion on the parameters kappa and l, as well as more detailed bounds in terms of these parameters, would have been beneficial. Moreover, a comparison of the theoretical bounds between RCDM, ACDM, and IBFS would have provided additional insight, enhancing the overall clarity and impact of the manuscript.