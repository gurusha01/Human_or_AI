This paper proposes a novel algorithm, Breg-SVRG, for solving saddle-point optimization problems with a linear rate of convergence. The algorithm extends the stochastic variance-reduced gradient (SVRG) method to accommodate Bregman divergences, allowing for more flexibility in adapting to the underlying geometry of the problem. The authors demonstrate the effectiveness of Breg-SVRG on two example applications: adversarial prediction and LPboosting.
The paper is well-written, and the authors provide a clear and concise introduction to the problem of saddle-point optimization and the motivation behind their work. The technical sections are thorough and well-organized, with a clear explanation of the algorithm and its convergence guarantees. The experimental results are also well-presented and demonstrate the superiority of Breg-SVRG over existing methods.
The strengths of the paper include:
* The proposal of a novel algorithm, Breg-SVRG, which extends the SVRG method to accommodate Bregman divergences.
* The provision of a clear and concise introduction to the problem of saddle-point optimization and the motivation behind the work.
* The thorough and well-organized technical sections, with a clear explanation of the algorithm and its convergence guarantees.
* The well-presented experimental results, which demonstrate the superiority of Breg-SVRG over existing methods.
The weaknesses of the paper include:
* The paper assumes a certain level of familiarity with saddle-point optimization and Bregman divergences, which may make it difficult for non-experts to follow.
* The algorithm and its convergence guarantees are presented in a general form, which may make it difficult to understand the specific applications and examples presented in the paper.
* The experimental results are limited to two example applications, and it would be beneficial to see more extensive experiments and comparisons with other methods.
Overall, the paper is well-written and provides a significant contribution to the field of machine learning. The proposal of Breg-SVRG and its application to adversarial prediction and LPboosting demonstrate the potential of this algorithm for solving complex optimization problems.
Arguments for acceptance:
* The paper proposes a novel algorithm, Breg-SVRG, which extends the SVRG method to accommodate Bregman divergences.
* The paper provides a clear and concise introduction to the problem of saddle-point optimization and the motivation behind the work.
* The technical sections are thorough and well-organized, with a clear explanation of the algorithm and its convergence guarantees.
* The experimental results demonstrate the superiority of Breg-SVRG over existing methods.
Arguments against acceptance:
* The paper assumes a certain level of familiarity with saddle-point optimization and Bregman divergences, which may make it difficult for non-experts to follow.
* The algorithm and its convergence guarantees are presented in a general form, which may make it difficult to understand the specific applications and examples presented in the paper.
* The experimental results are limited to two example applications, and it would be beneficial to see more extensive experiments and comparisons with other methods.
Quality: 8/10
Clarity: 8/10
Originality: 9/10
Significance: 9/10
Overall, I would recommend accepting this paper, as it provides a significant contribution to the field of machine learning and demonstrates the potential of Breg-SVRG for solving complex optimization problems. However, I would suggest that the authors provide more extensive experiments and comparisons with other methods to further demonstrate the effectiveness of their algorithm.