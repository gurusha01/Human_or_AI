This paper proposes PROXASAGA, a novel asynchronous parallel variant of the SAGA algorithm, designed to solve large-scale composite optimization problems with nonsmooth objectives. The authors introduce a sparse proximal SAGA algorithm that leverages sparsity in the partial gradients, reducing the cost per iteration. They then extend this algorithm to the asynchronous parallel setting, allowing multiple cores to update a central parameter vector without consistent reads.
The paper provides a thorough analysis of the proposed algorithm, including convergence guarantees and speedup results. The authors show that PROXASAGA achieves a theoretical linear speedup with respect to its sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks demonstrate that PROXASAGA significantly outperforms state-of-the-art alternatives on large sparse datasets, achieving practical speedups of up to 12x on a 20-core machine.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed algorithm and its analysis. The related work section is comprehensive, and the authors provide a detailed comparison with existing methods.
Strengths:
* The paper proposes a novel algorithm that addresses the limitation of existing parallel asynchronous variants of stochastic gradient descent, which are limited to smooth objectives.
* The authors provide a thorough analysis of the proposed algorithm, including convergence guarantees and speedup results.
* The empirical benchmarks demonstrate the effectiveness of the proposed algorithm on large sparse datasets.
Weaknesses:
* The paper assumes that the proximal term is block-separable, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of the proposed algorithm, which could be an important consideration in practice.
* The paper does not explore the extension of the proposed algorithm to other proximal incremental schemes, such as SGD or ProxSVRG, which could be an interesting direction for future work.
Arguments pro acceptance:
* The paper proposes a novel algorithm that addresses an important limitation of existing parallel asynchronous variants of stochastic gradient descent.
* The authors provide a thorough analysis of the proposed algorithm, including convergence guarantees and speedup results.
* The empirical benchmarks demonstrate the effectiveness of the proposed algorithm on large sparse datasets.
Arguments con acceptance:
* The paper assumes that the proximal term is block-separable, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of the proposed algorithm, which could be an important consideration in practice.
* The paper does not explore the extension of the proposed algorithm to other proximal incremental schemes, such as SGD or ProxSVRG, which could be an interesting direction for future work.
Overall, I believe that the paper makes a significant contribution to the field of optimization and machine learning, and I recommend acceptance. However, I suggest that the authors address the weaknesses mentioned above in a revised version of the paper.