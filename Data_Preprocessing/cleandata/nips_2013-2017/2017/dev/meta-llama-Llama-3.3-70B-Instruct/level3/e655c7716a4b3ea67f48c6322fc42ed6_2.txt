This paper proposes a novel approach to generative modeling by augmenting variational autoencoders (VAEs) with external memory. The key idea is to interpret memory-read operations as a conditional mixture distribution and use amortized variational inference for training. The model uses a discrete latent variable to select a memory entry, which is then used as a context for a continuous latent variable. The authors demonstrate the effectiveness of their approach on several datasets, including MNIST and Omniglot, and show that it outperforms baseline models, including those with soft attention.
The paper is well-written and clearly explains the technical details of the proposed approach. The experiments are thorough and demonstrate the benefits of the proposed model, particularly in the context of generative few-shot learning. The authors also provide a detailed comparison with existing work and highlight the advantages of their approach.
However, there are some concerns regarding the novelty of the factorization technique proposed for Hankel tensor, which is not clearly explained in the paper. Additionally, the introduction of the measure of relatedness tau in Definition 2 lacks clarity and purpose, and its relation to other quantities in the paper is unclear. The benefit of estimating multiple functions together is not apparent in the error measures or computational complexity, and it would be useful to explore a more general version of Theorem 5.
The computational complexity of the proposed method seems worse than estimating individual WFAs separately, and it is unclear whether the relatedness can be leveraged to improve performance. Furthermore, even when there is commonality between WFAs, the proposed method offers no benefit compared to simple SL if dS and dT are the same. The comparison of estimation error for multiple learning and individual learning is unclear, and it is questionable why O(T^2) would be better than O(T), especially if T is less than or equal to 1.
Overall, the paper presents a novel approach to generative modeling with external memory, but there are some concerns regarding the technical details and the benefits of the proposed method. The authors should address these concerns and provide more clarity on the novelty and advantages of their approach.
Arguments pro acceptance:
* The paper proposes a novel approach to generative modeling with external memory.
* The experiments demonstrate the effectiveness of the proposed model on several datasets.
* The paper provides a detailed comparison with existing work and highlights the advantages of the proposed approach.
Arguments con acceptance:
* The novelty of the factorization technique proposed for Hankel tensor is unclear.
* The introduction of the measure of relatedness tau lacks clarity and purpose.
* The benefit of estimating multiple functions together is not apparent in the error measures or computational complexity.
* The computational complexity of the proposed method seems worse than estimating individual WFAs separately.
* The comparison of estimation error for multiple learning and individual learning is unclear.