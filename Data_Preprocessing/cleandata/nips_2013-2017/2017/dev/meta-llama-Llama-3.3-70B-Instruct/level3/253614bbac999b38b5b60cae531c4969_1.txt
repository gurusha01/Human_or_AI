This paper proposes a novel approach to augmenting generative models with external memory, interpreting the output of a memory module with stochastic addressing as a conditional mixture distribution. The authors apply variational inference to memory addressing, enabling effective training of the memory module using target information to guide memory lookups. The model is evaluated on the tasks of generative few-shot learning and unconditional density modeling, demonstrating promising results.
The paper is well-written, and the authors provide a clear explanation of their approach and its relation to previous work. The experiments are thorough, and the results are impressive, particularly in the few-shot learning setting. The use of stochastic addressing and variational inference is a key contribution, allowing the model to perform precise memory lookups during inference and training.
However, there are some areas that require clarification and further explanation. For instance, the authors' 1-bit mechanism for mean estimation needs more details on its difference from the basic randomizer in [3] to establish its efficiency. Additionally, the utility of the proposed protocol in Theorem 2 should be compared with existing histogram protocols in terms of n and d to provide a more comprehensive understanding of its advantages.
The privacy guarantee after alpha-point rounding is also unclear and needs further explanation, particularly when a data collector reports data multiple times. This is a crucial aspect, as it directly affects the model's ability to maintain privacy and security.
The paper's strengths include its novel approach to memory-augmented generative models, its ability to perform precise memory lookups, and its promising results in few-shot learning. However, the weaknesses include the need for clarification on certain aspects, such as the 1-bit mechanism and privacy guarantee.
Arguments for acceptance:
* The paper proposes a novel and promising approach to memory-augmented generative models.
* The experiments demonstrate impressive results, particularly in the few-shot learning setting.
* The use of stochastic addressing and variational inference is a key contribution.
Arguments against acceptance:
* The paper requires clarification on certain aspects, such as the 1-bit mechanism and privacy guarantee.
* The comparison with existing histogram protocols is limited, and more comprehensive analysis is needed.
* The paper could benefit from more detailed explanations and justifications for certain design choices.
Overall, the paper is well-written, and the authors demonstrate a good understanding of the topic. With some revisions to address the areas mentioned above, the paper has the potential to make a significant contribution to the field of generative models and memory-augmented neural networks.