This paper proposes a novel approach to multitask learning of Weighted Finite Automata (WFAs) by exploiting relatedness among tasks and making vector-valued predictions with a unified state representation. The algorithm is compared to a baseline where each task is learned separately, but a more comprehensive comparison would be to a baseline where all data from different tasks are combined and treated as a single task. 
The paper's strengths include its ability to effectively train the memory module using variational inference and its application to generative few-shot learning tasks. The experiments demonstrate the model's capability to identify and access relevant memory contents even with hundreds of unseen Omniglot characters in memory. However, the paper could be improved by discussing potential application scenarios of multi-task learning of WFAs, such as natural language modeling tasks.
One potential weakness of the algorithm is its assumption of equal weights among tasks. The authors may want to consider a weighted approach for tasks with significantly different dataset sizes. Additionally, the projection step, while useful for eliminating redundancy in the model, may not be effective in all cases, particularly when tasks are unrelated and require different numbers of states.
To further evaluate the effectiveness of the algorithm, it would be beneficial to compare its performance with and without the post-processing projection step. This would provide insight into the importance of this step in the overall performance of the model. Furthermore, the authors may want to consider addressing the minor issue with the definition of prefixes and suffixes, specifically regarding the inclusion of the empty string.
Overall, the paper presents a novel and promising approach to multitask learning of WFAs, and with some additional comparisons and discussions, it has the potential to make a significant contribution to the field. 
Arguments pro acceptance:
- The paper proposes a novel approach to multitask learning of WFAs.
- The algorithm is effective in training the memory module using variational inference.
- The model demonstrates strong performance in generative few-shot learning tasks.
Arguments con acceptance:
- The comparison to the baseline could be more comprehensive.
- The assumption of equal weights among tasks may not be suitable for all scenarios.
- The projection step may not be effective in all cases.