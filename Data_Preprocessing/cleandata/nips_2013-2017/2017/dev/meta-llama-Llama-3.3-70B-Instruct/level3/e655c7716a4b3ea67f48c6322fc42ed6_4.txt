This paper presents a novel approach to augmenting generative models with external memory, interpreting the output of a memory module with stochastic addressing as a conditional mixture distribution. The authors propose a variational inference method for training the memory module, which enables effective use of target information to guide memory lookups. The paper is well-written, clear, and easy to read, making it accessible to readers.
The key contribution of this paper is the introduction of a stochastic perspective on memory addressing, allowing for the application of variational inference to memory lookups. This approach is particularly well-suited for generative models, as it naturally encourages multimodality and enables the quantification of information gained with a memory lookup. The authors demonstrate the effectiveness of their approach through experiments on the MNIST and Omniglot datasets, showing that their model can learn to use memory to model highly multi-modal input data and perform reliable approximate inference over memory locations.
The paper has several strengths, including its novelty, clarity, and empirical results. The authors provide a thorough discussion of related work, highlighting the differences between their approach and existing methods. The experiments are well-designed and demonstrate the effectiveness of the proposed approach in various settings.
However, there are some potential weaknesses and areas for improvement. One concern is that the proposed notion of relatedness may be too strong for real learning problems, and the authors should discuss ways to relax this notion for more practical applications. Additionally, the paper could benefit from a more detailed analysis of the computational complexity of the proposed approach and its scalability to larger memory sizes.
In terms of the conference guidelines, this paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, with well-supported claims and a clear presentation of the methodology and results. The writing is clear and concise, making it easy to follow and understand. The approach is novel and represents a significant contribution to the field of generative models and external memory. The results are important and have the potential to impact the development of future generative models.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of generative models and external memory.
* The approach is well-motivated and clearly presented, with a thorough discussion of related work.
* The experiments are well-designed and demonstrate the effectiveness of the proposed approach in various settings.
* The paper is well-written and easy to read, making it accessible to a wide range of readers.
Arguments con acceptance:
* The proposed notion of relatedness may be too strong for real learning problems, and the authors should discuss ways to relax this notion for more practical applications.
* The paper could benefit from a more detailed analysis of the computational complexity of the proposed approach and its scalability to larger memory sizes.
* Some readers may find the paper to be slightly dense and requiring a significant amount of background knowledge in generative models and external memory.