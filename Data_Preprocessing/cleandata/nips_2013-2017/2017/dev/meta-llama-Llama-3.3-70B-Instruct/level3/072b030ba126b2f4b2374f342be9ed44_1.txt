This paper proposes a novel approach to augmenting generative models with external memory, interpreting memory-read operations as conditional mixture distributions and using amortized variational inference for training. The model combines discrete memory addressing variables with continuous latent variables, allowing for powerful models for generative few-shot learning that scale gracefully with the number of items in memory.
The paper's key strengths lie in its ability to handle non-smooth regularization and its novel approach to memory addressing. The authors demonstrate the effectiveness of their approach through experiments on MNIST and Omniglot datasets, showing that their model can learn to use the top-level memory to model highly multi-modal input data and perform reliable approximate inference over memory locations.
However, there are some areas that require clarification and improvement. The concept of "inconsistent read" is not clearly defined, and the authors should provide more details on how to obtain the neat representation of the difference between the estimated and true values. Additionally, the authors make several implicit assumptions to obtain linear convergence rate and speedup, which should be explicitly stated in the paper.
The paper also lacks a clear comparison with existing works, such as Lian et al. (2016) and Liu and Wright (2015). The authors should provide a more detailed analysis of the differences between their approach and existing methods, and demonstrate the advantages of their approach through experiments.
In terms of quality, the paper is well-written and well-organized, but there are some areas that require improvement. The authors should provide more details on the experimental setup and the hyperparameters used in the experiments. Additionally, the paper could benefit from more visualizations and plots to illustrate the results and the behavior of the model.
Overall, the paper presents a novel and interesting approach to augmenting generative models with external memory, and demonstrates its effectiveness through experiments. However, there are some areas that require clarification and improvement, and the authors should provide more details on the experimental setup and the hyperparameters used in the experiments.
Arguments pro acceptance:
* The paper proposes a novel approach to augmenting generative models with external memory
* The authors demonstrate the effectiveness of their approach through experiments on MNIST and Omniglot datasets
* The paper is well-written and well-organized
Arguments con acceptance:
* The concept of "inconsistent read" is not clearly defined
* The authors make several implicit assumptions to obtain linear convergence rate and speedup
* The paper lacks a clear comparison with existing works
* The authors should provide more details on the experimental setup and the hyperparameters used in the experiments.