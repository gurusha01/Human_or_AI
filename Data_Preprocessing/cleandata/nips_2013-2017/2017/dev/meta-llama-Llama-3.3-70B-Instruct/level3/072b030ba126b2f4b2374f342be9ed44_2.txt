This paper proposes a novel approach to generative modeling by augmenting variational autoencoders (VAEs) with external memory. The key idea is to interpret memory-read operations as a conditional mixture distribution, allowing for the application of variational inference to memory addressing. This enables effective training of the memory module using target information to guide memory lookups. The authors demonstrate the advantages of this approach by incorporating it into a VAE and applying the resulting model to generative few-shot learning.
The paper is well-written, and the ideas are clearly presented. The authors provide a thorough overview of the related work and position their contribution within the existing literature. The technical content is sound, and the experiments are well-designed to demonstrate the effectiveness of the proposed approach. The results show that the model is able to identify and access relevant memory contents even with hundreds of unseen Omniglot characters in memory.
The strengths of the paper include:
* The proposal of a novel approach to generative modeling that combines VAEs with external memory
* The application of variational inference to memory addressing, which enables effective training of the memory module
* The demonstration of the model's ability to perform reliable approximate inference over memory locations
* The comparison with existing approaches, such as soft-attention baselines, which shows the advantages of the proposed approach
The weaknesses of the paper include:
* The use of a simple VAE architecture, which may not be the most effective choice for complex datasets
* The reliance on a learned prior over the memory addresses, which may not always be effective
* The lack of exploration of other gradient estimators for discrete latent variable models, which may provide alternative approaches to VIMCO
Overall, the paper makes a significant contribution to the field of generative modeling and provides a novel approach to combining VAEs with external memory. The experiments demonstrate the effectiveness of the proposed approach, and the results are well-presented. However, there are some areas for improvement, such as exploring more complex VAE architectures and alternative gradient estimators.
Arguments for acceptance:
* The paper proposes a novel approach to generative modeling that combines VAEs with external memory
* The application of variational inference to memory addressing is a significant contribution to the field
* The experiments demonstrate the effectiveness of the proposed approach and provide a thorough comparison with existing approaches
Arguments against acceptance:
* The use of a simple VAE architecture may not be the most effective choice for complex datasets
* The reliance on a learned prior over the memory addresses may not always be effective
* The lack of exploration of other gradient estimators for discrete latent variable models may limit the applicability of the proposed approach.