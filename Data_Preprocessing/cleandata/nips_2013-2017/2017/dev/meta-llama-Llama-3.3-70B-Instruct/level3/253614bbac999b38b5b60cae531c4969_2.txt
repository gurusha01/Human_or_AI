This paper proposes a novel approach to augmenting generative models with external memory, interpreting memory-read operations as conditional mixture distributions and using amortized variational inference for training. The authors demonstrate the effectiveness of their approach in generative few-shot learning tasks, particularly on the Omniglot dataset. The paper is well-written, and the ideas are clearly presented, making it easy to follow.
The strengths of the paper include its originality, as it introduces a new perspective on memory addressing and its application to generative models. The authors provide a thorough analysis of the model's behavior, including the interplay between the prior and posterior distributions over the memory addresses. The experimental results are convincing, demonstrating the model's ability to perform precise memory lookups and its robustness to increasing memory sizes.
However, there are some weaknesses to the paper. The presentation of the alpha-rounding part is partially unclear and requires further elaboration from the authors on the claimed guarantees. Additionally, the experimental study raises questions about reproducibility due to the use of real user and company data. Providing alternative solutions, such as simulated data, would be beneficial for follow-up work.
In terms of the conference guidelines, the paper meets the criteria for quality, as it is technically sound and well-supported by theoretical analysis and experimental results. The paper is also clear and well-organized, making it easy to understand. The originality of the paper is high, as it introduces a new approach to memory addressing and its application to generative models. The significance of the paper is also high, as it demonstrates the effectiveness of the proposed approach in generative few-shot learning tasks and provides a new perspective on memory addressing.
Arguments pro acceptance:
* The paper introduces a novel approach to augmenting generative models with external memory.
* The authors provide a thorough analysis of the model's behavior and demonstrate its effectiveness in generative few-shot learning tasks.
* The paper is well-written and easy to follow.
Arguments con acceptance:
* The presentation of the alpha-rounding part is partially unclear and requires further elaboration.
* The experimental study raises questions about reproducibility due to the use of real user and company data.
Overall, I recommend accepting the paper, as its strengths outweigh its weaknesses. The paper provides a significant contribution to the field of generative models and memory addressing, and its originality and significance make it a valuable addition to the conference program.