The paper addresses the computational challenges of adversarial prediction under multivariate losses, presenting a novel algorithm, Breg-SVRG, which extends stochastic variance-reduced gradient (SVRG) methods to saddle-point problems using Bregman divergences. The authors demonstrate that by leveraging sufficient statistics, the dimensionality of the optimization problem can be reduced exponentially, and by adapting Breg-SVRG to non-Euclidean geometries, they achieve significant improvements in convergence rates. Theoretical results are supported by experiments on adversarial prediction and LPBoosting, showing empirical speedups over Euclidean-based alternatives.
Strengths:
1. Technical Novelty: The extension of SVRG to saddle-point problems with Bregman divergences is a significant contribution. The introduction of a new Pythagorean theorem for saddle functions and the innovative proof techniques are noteworthy.
2. Dimensionality Reduction: The reformulation of the adversarial prediction problem to reduce the optimization variable from \(2^n\) to \(n^2\) is a major advancement, making the problem computationally feasible.
3. Theoretical Guarantees: The authors provide rigorous proofs of linear convergence for Breg-SVRG, addressing challenges posed by asymmetry in Bregman divergences.
4. Empirical Validation: Extensive experiments on adversarial prediction and LPBoosting validate the theoretical claims, showing significant speedups and better adaptation to problem geometry.
5. Broader Applicability: The authors suggest that Breg-SVRG can be applied to other saddle-point problems, indicating the potential for broader impact.
Weaknesses:
1. Clarity: While the mathematical rigor is commendable, the paper is dense and may be difficult for non-experts to follow. The notation is complex, and some derivations could benefit from additional explanation or visual aids.
2. Experimental Scope: The experiments, while thorough, are limited to two applications. Demonstrating the algorithm's effectiveness on a wider range of adversarial machine learning problems would strengthen the paper's impact.
3. Practical Considerations: The computational cost of the proximal update (e.g., \(O(n^2 \log^2(1/\epsilon))\)) may still be prohibitive for very large datasets. A discussion on scalability and potential optimizations would be valuable.
4. Comparison with Alternatives: While the paper compares Breg-SVRG to Euclidean-SVRG, it does not benchmark against other state-of-the-art methods for saddle-point problems, such as primal-dual hybrid gradient methods or other mirror descent variants.
Arguments for Acceptance:
- The paper makes a strong theoretical contribution by extending SVRG to non-Euclidean geometries for saddle-point problems.
- The dimensionality reduction and improved convergence rates are significant advancements for adversarial prediction.
- The experimental results convincingly demonstrate the algorithm's advantages over Euclidean-based methods.
Arguments Against Acceptance:
- The paper's clarity could be improved, making it more accessible to a broader audience.
- The experimental evaluation, while promising, is somewhat narrow in scope.
- Practical scalability for very large datasets remains an open question.
Recommendation:
I recommend acceptance of this paper, as it provides a novel and theoretically sound contribution to the field of optimization for adversarial machine learning. However, the authors should consider improving the clarity of their presentation and expanding the experimental evaluation in future work.