This paper introduces a novel attention module for action recognition tasks, proposing a simple yet effective method to integrate attention into deep networks. The authors demonstrate that their attention mechanism, which can be trained with or without additional supervision, significantly improves performance on three standard benchmarks: MPII, HICO, and HMDB51. Notably, the proposed method achieves a 12.5% relative improvement on the MPII dataset, setting a new state of the art. The paper also provides a theoretical perspective, framing the attention mechanism as a low-rank approximation of second-order pooling, which connects action recognition to fine-grained classification problems. This dual empirical and analytical approach is a key strength of the work.
Strengths:
1. Technical Soundness and Quality: The paper is technically rigorous, with a solid derivation of the attention mechanism as a low-rank approximation of second-order pooling. The experiments are extensive, spanning multiple datasets and baselines, and the results are compelling, showing consistent improvements across tasks.
2. Clarity: The paper is well-written and organized, with clear explanations of the proposed method, its theoretical underpinnings, and its implementation. The derivation of the attention mechanism is particularly well-presented, making the approach accessible to readers.
3. Originality: The idea of framing attention as low-rank second-order pooling is novel and provides a fresh perspective on attention mechanisms. The integration of bottom-up saliency and top-down attention is also a unique contribution, inspired by neuroscience literature.
4. Significance: The results demonstrate that the proposed attention module is a practical and impactful contribution to action recognition. Its ability to improve performance with minimal computational overhead makes it a valuable addition to existing architectures.
Weaknesses:
1. Limited Exploration of Alternatives: While the paper discusses full-rank pooling and higher-rank approximations, the exploration of these alternatives is somewhat limited. A more detailed analysis of when higher-rank approximations might be beneficial would strengthen the work.
2. Overfitting in Small Datasets: The paper notes overfitting issues when using class-specific attention maps on smaller datasets like MPII. While this is acknowledged, further discussion on how to mitigate this limitation would be helpful.
3. Comparison with Sequential Attention Models: The paper positions its method as an alternative to sequential attention models (e.g., LSTMs), but direct comparisons with such models are absent. Including these comparisons would provide a more comprehensive evaluation.
Arguments for Acceptance:
- The paper introduces a novel and theoretically grounded approach to attention in action recognition.
- The results are strong, with state-of-the-art performance on multiple benchmarks.
- The method is simple to implement and computationally efficient, making it broadly applicable.
Arguments Against Acceptance:
- The exploration of higher-rank approximations and alternatives could be more thorough.
- The lack of direct comparisons with sequential attention models leaves a gap in the evaluation.
Recommendation:
I recommend accepting this paper. Its contributions are significant, both in terms of theoretical insights and practical impact, and it addresses an important problem in computer vision. While there are areas for improvement, the strengths of the work outweigh its weaknesses.