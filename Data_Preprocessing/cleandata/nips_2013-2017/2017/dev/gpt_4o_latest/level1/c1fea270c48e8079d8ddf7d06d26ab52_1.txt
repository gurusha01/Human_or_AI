This paper explores the connections between discrete and continuous approaches for decomposable submodular function minimization (DSFM), providing both theoretical advancements and systematic experimental comparisons. The authors improve the worst-case complexity bounds for continuous optimization methods by leveraging combinatorial insights, simplifying prior proofs, and achieving a factor of \( r \) improvement in the bounds. They also propose a clean separation between level-0 and level-1 algorithms, enabling a fair comparison of discrete and continuous methods under consistent conditions. The experimental results reveal tradeoffs between these approaches, with discrete algorithms excelling for small supports and continuous methods performing better for larger supports or when approximate solutions suffice.
Strengths:
1. Theoretical Contributions: The paper provides significant theoretical improvements, including tighter bounds on condition numbers (\( \kappa^ \) and \( \ell^ \)), which directly translate to improved convergence guarantees for continuous methods. The use of combinatorial arguments to simplify prior algebraic proofs is a notable achievement.
2. Systematic Experimental Design: By isolating level-0 and level-1 algorithms, the authors ensure a fair comparison of discrete and continuous methods. This approach highlights the practical tradeoffs between these methods, such as the robustness of continuous algorithms to approximate level-0 solutions.
3. Practical Relevance: The focus on DSFM, a problem with applications in image segmentation and other domains, ensures the paper's relevance to both theoretical and applied machine learning communities.
4. Novel Insights: The experimental results provide actionable insights, such as the suitability of gradient methods with warm-started Fujishige-Wolfe algorithms for large-scale problems, where discrete methods struggle.
Weaknesses:
1. Clarity: While the paper is well-organized, some sections, particularly the theoretical proofs and algorithmic descriptions, are dense and may be challenging for readers unfamiliar with submodular optimization. Simplifying or summarizing key ideas could improve accessibility.
2. Experimental Scope: The experiments focus on specific types of potentials and datasets. While these are standard in the literature, broader evaluations on diverse real-world datasets or other submodular applications could strengthen the empirical claims.
3. Approximation Tradeoffs: The paper emphasizes the practicality of continuous methods with approximate level-0 solutions but does not fully quantify the impact of these approximations on solution quality. A more detailed analysis of this tradeoff would be valuable.
Arguments for Acceptance:
- The paper makes a clear theoretical contribution by improving complexity bounds and simplifying proofs, advancing the state of the art in DSFM.
- The experimental framework and results provide practical guidance for choosing between discrete and continuous methods, addressing a critical gap in the literature.
- The work is well-aligned with NIPS's focus on optimization and machine learning applications.
Arguments Against Acceptance:
- The paper's dense presentation may limit its accessibility to a broader audience.
- The experimental evaluation, while systematic, could be expanded to include more diverse datasets and applications.
Recommendation:
I recommend acceptance of this paper. Its contributions to both theory and practice in DSFM are significant, and the insights provided are likely to influence future research in submodular optimization and its applications. Minor revisions to improve clarity and expand the experimental scope would further enhance the paper's impact.