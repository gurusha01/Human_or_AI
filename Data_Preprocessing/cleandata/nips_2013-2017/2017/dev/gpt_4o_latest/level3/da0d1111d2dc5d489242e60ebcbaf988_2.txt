This paper addresses the challenging problem of online convex optimization (OCO) with stochastic constraints, a generalization of Zinkevich's OCO framework. By considering constraints that are stochastically generated and i.i.d. over time, the authors bypass the linear regret lower bound associated with adaptively chosen constraints, achieving significant theoretical and practical advancements. The proposed algorithm achieves \(O(\sqrt{T})\) expected regret and constraint violations, and \(O(\sqrt{T} \log(T))\) high-probability bounds, which are state-of-the-art for this problem setting.
Strengths:
1. Novel Contributions: The paper makes a strong contribution by addressing a previously intractable problem setting. The use of a time-varying regularization parameter ("virtual queue") to manage constraints is innovative and well-motivated.
2. Algorithm Design: The proposed algorithm is elegant, leveraging a first-order optimization approach with instantaneous penalty and constraint functions. The equivalence to a projected gradient descent (PGD)-style update is a key insight.
3. Theoretical Guarantees: The theoretical analysis is rigorous, with the drift analysis and its adaptation to virtual queue terms being particularly novel. The results improve upon existing work in related domains, such as OCO with long-term constraints.
4. Experimental Validation: The experiments on a distributed job scheduling problem demonstrate the algorithm's practical utility, showing comparable throughput to REACT while reducing costs by ~10%. This highlights the algorithm's potential for real-world applications.
Weaknesses:
1. Limited Experimental Scope: While the results on job scheduling are promising, the experimental evaluation is limited to a single application. Expanding to other domains would strengthen the empirical claims.
2. Clarity of Motivation: The paper could provide more intuition behind the design of the virtual queue dynamics (e.g., Equation 3). This would help readers better understand the algorithm's core mechanisms.
3. Challenges in Extending PGD: The paper does not sufficiently discuss the challenges of extending PGD to time-varying constraints, which could provide valuable insights for future research.
4. Connections to Online Mirrored Descent: Comments on how the results could be extended to online mirrored descent would enhance the paper's generality and relevance.
5. Broader Context: While the related work is well-referenced, the paper could better contextualize its contributions within the broader landscape of stochastic optimization and constrained learning.
Recommendation:
Accept with Minor Revisions. The paper makes a significant theoretical and practical contribution to OCO with stochastic constraints, a topic of high relevance to the community. While the experimental evaluation and clarity of certain aspects could be improved, the strengths far outweigh the weaknesses. The paper is technically sound, original, and advances the state of the art, making it a valuable addition to the conference.
Arguments for Acceptance:
- Strong theoretical guarantees with novel drift analysis.
- Significant improvement over existing methods for related problems.
- Promising experimental results demonstrating practical impact.
Arguments Against Acceptance:
- Limited experimental scope.
- Insufficient discussion of challenges and broader implications.
With minor revisions to address the noted weaknesses, this paper would be a strong contribution to the field.