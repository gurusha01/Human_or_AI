Review of the Paper
This paper proposes a novel attention module for action recognition tasks, formulated as a low-rank approximation of second-order pooling. The method is simple, computationally efficient, and applicable to any CNN architecture. It demonstrates significant improvements on three standard benchmarks (MPII, HICO, and HMDB51), achieving state-of-the-art results on MPII and competitive performance on the others. The authors also provide a mathematical analysis of their approach, connecting attention mechanisms to second-order pooling, and perform extensive empirical evaluations, including a pose-regularized variant of their model.
Strengths:  
1. Simplicity and Generalizability: The proposed attention module is straightforward to implement and can be integrated into existing architectures with minimal computational overhead. This makes it a practical contribution to the field.  
2. Performance Gains: The method achieves clear improvements over baseline architectures and even surpasses state-of-the-art methods on some datasets. The results on MPII are particularly impressive, with a 12.5% relative improvement.  
3. Theoretical Insights: The connection between attention mechanisms and second-order pooling is novel and provides a fresh perspective on the design of attention modules.  
4. Extensive Analysis: The paper includes a thorough evaluation of the method across datasets, architectural variants, and design choices, which strengthens its claims.  
Weaknesses:  
1. Novelty Concerns: While the connection to second-order pooling is interesting, the core idea of using low-rank approximations for attention is not entirely new. The paper does not sufficiently differentiate its approach from existing second-order pooling methods.  
2. Design Justification: Some design choices, such as the use of class-agnostic versus class-specific parameters, are not well justified. Additional ablation studies could clarify their impact.  
3. Instance-Level Recognition: The method does not explicitly address instance-specific action recognition, which is a limitation compared to methods like R*CNN. This omission reduces the scope of applicability.  
4. Fairness of Comparisons: The comparisons with competing methods are not entirely fair, as they use different base architectures. This makes it difficult to conclusively attribute performance gains to the proposed attention module.  
5. Marginal Gains in Some Cases: The improvements over certain baselines, such as TSN BN-Inception, are relatively small, raising questions about the practical significance of the method in those scenarios.  
Arguments for Acceptance:  
- The method is simple, effective, and broadly applicable.  
- It achieves state-of-the-art results on MPII and competitive results on other benchmarks.  
- The theoretical connection to second-order pooling is a valuable contribution.  
Arguments Against Acceptance:  
- The novelty of the approach is limited, and its differentiation from prior work is unclear.  
- The lack of instance-level recognition limits its applicability.  
- Unfair comparisons and small gains in some cases weaken the empirical evidence.  
Conclusion:  
Overall, this paper makes a meaningful contribution to the field of action recognition by proposing a simple and effective attention mechanism. However, concerns about novelty, design justification, and fairness of comparisons need to be addressed. I recommend acceptance, provided the authors clarify these issues and strengthen the experimental analysis.