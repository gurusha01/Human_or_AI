The paper presents PROXASAGA, an asynchronous parallel algorithm that extends the SAGA framework to handle composite optimization problems with non-smooth separable regularization. This work is significant as it addresses a key limitation of existing asynchronous stochastic gradient methods, which are typically restricted to smooth objectives. The authors propose a novel sparse variant of the proximal SAGA algorithm and demonstrate its effectiveness both theoretically and empirically.
Strengths:
1. Novelty and Contribution: The paper introduces a clever mechanism to handle non-smooth regularization while maintaining sparse updates. This is a meaningful extension of the SAGA framework, as it enables efficient optimization for problems like Lasso and group Lasso, which are widely used in machine learning.
2. Theoretical Analysis: The authors provide rigorous convergence guarantees, showing that PROXASAGA achieves linear speedup under certain sparsity and delay assumptions. The analysis builds on prior work by Leblond et al. (2017) and incorporates asynchronous proof techniques from Mania et al. (2015), demonstrating a solid theoretical foundation.
3. Empirical Validation: The experimental results are compelling, showing that PROXASAGA significantly outperforms state-of-the-art methods like ASYSPCD and FISTA on large-scale sparse datasets. The observed speedups (up to 12x on a 20-core machine) align well with the theoretical predictions.
4. Practical Relevance: The algorithm is easy to implement and well-suited for modern multi-core architectures, making it a practical contribution to the field.
Weaknesses:
1. Clarity Issues: 
   - The concept of "inconsistent read" and the indexing of \( \hat{x}_k \) require further clarification. This is critical for understanding the asynchronous framework.
   - The derivation of the difference between \( \hat{x}t \) and \( xt \) is not sufficiently detailed, making it difficult to follow the analysis.
2. Implicit Assumptions: The paper relies on certain sparsity and bounded delay assumptions for linear convergence and speedup but does not explicitly state these assumptions upfront. This could hinder reproducibility and generalizability.
3. Missing Reference: The paper omits a citation to "Asynchronous parallel greedy coordinate descent, NIPS, 2016," which is relevant to the discussion of related work.
4. Typographical Error: A typo was noted in the definition of \( \Delta \) on line 217, which should be corrected for clarity.
Arguments for Acceptance:
- The paper addresses an important problem in asynchronous optimization and provides a novel, well-motivated solution.
- The theoretical and empirical results are strong and demonstrate clear advantages over existing methods.
- The work is relevant to the NIPS community, particularly researchers working on optimization and large-scale machine learning.
Arguments Against Acceptance:
- The clarity issues and missing details in the analysis could limit the accessibility of the paper to a broader audience.
- The reliance on sparsity assumptions may restrict the applicability of the method to certain types of problems.
Recommendation:
I recommend acceptance of this paper, contingent on the authors addressing the clarity issues and explicitly stating the assumptions for convergence and speedup. The contributions are significant, and the results advance the state of the art in asynchronous optimization for composite objectives.