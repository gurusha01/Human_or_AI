This paper presents a novel stochastic variance-reduced algorithm, Breg-SVRG, for saddle-point optimization problems, with a focus on adversarial prediction under multivariate losses and LP boosting with entropy regularization. The authors extend the stochastic variance-reduction framework to accommodate Bregman divergences, achieving a linear convergence rate while adapting to the underlying problem geometry. The paper also introduces a reformulation of adversarial prediction problems that reduces the dimensionality of the optimization variables, making them computationally feasible. Empirical results demonstrate significant improvements in convergence speed and test accuracy when using Breg-SVRG with entropy-based regularization compared to Euclidean-based methods.
Strengths:
1. Technical Contribution: The extension of stochastic variance-reduction methods to Bregman divergences is a substantial theoretical advancement. The introduction of a new Pythagorean theorem and a novel proof technique to handle asymmetry in Bregman divergences is noteworthy.
2. Practical Impact: The reformulation of adversarial prediction problems to reduce dimensionality is a significant contribution, enabling efficient optimization for problems that were previously computationally intractable.
3. Empirical Validation: The experiments convincingly demonstrate the advantages of Breg-SVRG with entropy regularization over Euclidean-based methods in terms of primal gap reduction and test accuracy. The results are consistent across multiple datasets and applications (adversarial prediction and LP boosting).
4. Clarity in Algorithm Design: The paper provides detailed algorithmic steps, making it easier for practitioners to implement the proposed method.
Weaknesses:
1. Disjointed Sections: Sections 3 and 4 feel disconnected, with Section 3 focusing on the reformulation of adversarial prediction and Section 4 diving into the Breg-SVRG algorithm. Reordering these sections or providing a clearer transition would improve the paper's coherence.
2. Abrupt Introduction of Regularizer: The regularizer \( ||\theta||^2 \) in Equation (10) is introduced without sufficient explanation or justification. A discussion on its role and impact on the optimization problem is necessary for clarity.
3. Comparison with Prior Work: While the paper references prior methods, the discussion on LP boosting with entropy regularization could be expanded. Specifically, a more detailed comparison with methods from the original LP boosting paper would strengthen the paper's positioning.
4. Proximal Update Complexity: Although the proximal update is discussed in detail, its computational complexity for large-scale problems remains a concern. A more explicit discussion on scalability would be beneficial.
Pro and Con Arguments for Acceptance:
Pro:
- The paper addresses a challenging and important problem in machine learning, advancing both theory and practice.
- The proposed method achieves significant speedups and improved performance in adversarial prediction tasks.
- The theoretical contributions, particularly the extension to Bregman divergences, are novel and impactful.
Con:
- The paper's organization could be improved for better readability and coherence.
- Certain technical aspects, such as the role of the regularizer and scalability of the proximal update, are insufficiently addressed.
- The discussion on related work, particularly for LP boosting, is limited.
Recommendation:
Overall, this paper makes a strong contribution to the field of saddle-point optimization and adversarial learning. While there are areas for improvement in clarity and discussion, the strengths outweigh the weaknesses. I recommend acceptance, provided the authors address the noted concerns, particularly the coherence between Sections 3 and 4 and the justification for the regularizer in Equation (10).