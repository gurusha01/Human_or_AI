This paper addresses the challenging problem of adversarial prediction under multivariate losses by proposing a novel approach that significantly improves computational efficiency. The authors reduce the complexity of a general saddle-point optimization problem from exponential to quadratic in sample size, leveraging sufficient statistics to simplify the problem. They then extend the Stochastic Variance Reduced Gradient (SVRG) method to Bregman divergences, introducing the Breg-SVRG algorithm, which achieves a linear convergence rate while adapting to the underlying geometry of the problem. The paper demonstrates the efficacy of this approach with applications to adversarial prediction (focused on the F-score) and LPBoosting, achieving substantial speedups over Euclidean-based alternatives.
Strengths:
1. Technical Contributions: The reduction of problem size and the extension of SVRG to Bregman divergences are significant contributions. The authors provide a rigorous theoretical analysis, including a new Pythagorean theorem for saddle Bregman divergences, and demonstrate the linear convergence of Breg-SVRG.
2. Practical Impact: The proposed method achieves a notable speedup (by a factor of n) over Euclidean-based methods, as validated both theoretically and experimentally. This makes it highly relevant for large-scale adversarial prediction problems.
3. Clarity and Organization: The paper is well-written and provides sufficient detail for reproducibility. The experimental results are thorough, comparing the proposed method against baselines and demonstrating its advantages in terms of both primal gap reduction and test accuracy.
4. Broader Applicability: While the experiments focus on the F-score, the authors highlight the potential applicability of Breg-SVRG to other saddle-point problems, suggesting broader relevance.
Weaknesses:
1. Limited Generalization Across Losses: The simplification of the adversarial prediction problem is specific to the F-score. It is unclear how this approach would generalize to other multivariate losses, such as DCG or precision@k, which the authors mention as future work. This limits the immediate applicability of the method.
2. Focus on Breg-SVRG: While the extension of SVRG to Bregman divergences is a valuable contribution, it could have been the paper's primary focus. The adversarial prediction problem, while interesting, somewhat overshadows the broader utility of Breg-SVRG.
3. Proximal Update Complexity: The proximal updates required by Breg-SVRG can be computationally intensive, particularly for large-scale problems. While the authors propose efficient algorithms for specific cases, a more general solution would enhance the method's usability.
Arguments for Acceptance:
- The paper is technically sound, with well-supported claims and rigorous theoretical analysis.
- It addresses a challenging and relevant problem in adversarial prediction, achieving significant computational improvements.
- The extension of SVRG to Bregman divergences is a novel and broadly applicable contribution.
Arguments Against Acceptance:
- The method's applicability to losses beyond the F-score remains unclear, limiting its generalizability.
- The computational cost of the proximal updates could hinder practical adoption in some scenarios.
Recommendation:
I recommend acceptance of this paper. Its contributions to both adversarial prediction and stochastic optimization are substantial, and the results are likely to inspire further research in the field. However, the authors are encouraged to clarify the method's applicability to other multivariate losses and explore more efficient proximal update strategies in future work.