This paper addresses a critical gap in the literature by extending asynchronous stochastic optimization methods to nonsmooth composite objectives, a class of problems frequently encountered in machine learning. The authors propose PROXASAGA, a sparse, lock-free asynchronous variant of the SAGA algorithm, and provide theoretical guarantees for its convergence and speedup. The contributions are significant, particularly in proving a convergence rate for nonsmooth objectives in asynchronous settings (Theorem 2), which had not been rigorously established before.
Strengths:
1. Theoretical Contributions: The paper provides two key theoretical results. Theorem 1 establishes convergence for Sparse Proximal SAGA, while Theorem 2 extends this to the asynchronous setting, offering a novel convergence rate. These results slightly improve upon prior work and represent a meaningful advancement in the field.
2. Practical Relevance: The block separability assumption allows for efficient, lock-free updates, making the algorithm well-suited for large-scale, sparse datasets. The empirical results demonstrate significant speedups (up to 12x on a 20-core machine) and orders-of-magnitude improvements over state-of-the-art methods.
3. Clarity of Contributions: The authors clearly position their work relative to existing methods, such as ASAGA and ASYSPCD, and highlight the unique challenges of extending asynchronous methods to nonsmooth objectives.
4. Open Problems: The paper acknowledges its reliance on sparsity assumptions and identifies asynchrony without sparsity for nonsmooth functions as an open problem, which is a valuable direction for future research.
Weaknesses:
1. Borrowed Proof Techniques: Theorem 2 heavily borrows from Leblond et al. (2017), which assumes smooth \( g(x) \). It is unclear whether this assumption is fully avoided in the current proof, raising concerns about the novelty of the theoretical analysis.
2. Minor Errors: The paper contains typos (e.g., "a a" on line 127) and formatting issues (e.g., semicolon after "sparse" on line 61). Additionally, there are errors in equations (e.g., eq. (14) is backwards) and unclear notations (e.g., lines 425 and 447), which hinder readability.
3. Loose Inequalities: Some inequalities in the proofs (e.g., Lemma 7) appear loose, which could affect the tightness of the convergence guarantees.
4. Limited Exploration of Proximal Decomposition: While the block separability assumption is practical, the paper does not explore element-wise proximal decomposition, which could further broaden the applicability of the method.
Recommendation:
The paper makes strong theoretical and practical contributions to asynchronous optimization for nonsmooth objectives, addressing a previously unproven problem. However, the reliance on prior work for key proofs and the presence of minor errors detract from its overall quality. I recommend acceptance, contingent on addressing the proof-related concerns and improving the manuscript's clarity.
Arguments for Acceptance:
- Novel convergence proof for nonsmooth objectives in asynchronous settings.
- Significant empirical improvements over state-of-the-art methods.
- Clear identification of open problems and future directions.
Arguments Against Acceptance:
- Heavy reliance on prior work for Theorem 2.
- Minor errors and unclear notations reduce clarity.
- Limited exploration of alternative proximal decomposition methods. 
Overall, the paper is a valuable contribution to the field and merits inclusion in the conference.