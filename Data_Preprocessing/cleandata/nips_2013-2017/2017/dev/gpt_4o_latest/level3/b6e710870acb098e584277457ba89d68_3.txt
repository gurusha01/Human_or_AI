Review of the Paper
This paper presents an extension of the Stochastic Variance-Reduced Gradient (SVRG) algorithm to incorporate entropy-based regularization using Bregman divergences, rather than the traditional Euclidean norm. The authors propose a new algorithm, Breg-SVRG, which is applied to adversarial prediction tasks and LPBoosting. The paper demonstrates that the proposed method achieves a linear convergence rate while adapting to the underlying geometry of the problem, leading to significant computational speedups. Theoretical results are supported by experimental evaluations on synthetic and real-world datasets.
Strengths
1. Novelty and Originality: The extension of SVRG to handle entropy regularization via Bregman divergences is both novel and significant. The work bridges a gap in the literature by addressing the challenge of adapting variance-reduced methods to non-Euclidean geometries, which has not been adequately explored in prior work.
2. Theoretical Contributions: The authors provide rigorous theoretical analysis, including a new Pythagorean theorem for saddle Bregman divergences and a proof of linear convergence for Breg-SVRG. These contributions are likely to be of independent interest to the optimization and machine learning communities.
3. Practical Impact: The reformulation of adversarial prediction problems to reduce dimensionality and the efficient proximal updates for entropy-regularized objectives make this work highly practical. The demonstrated speedup (by a factor of \(n\)) over Euclidean alternatives is compelling.
4. Experimental Validation: The experimental results convincingly show that Breg-SVRG outperforms Euclidean-SVRG in terms of both primal gap reduction and test accuracy, especially for problems with entropic regularizers and simplex constraints. The inclusion of multiple datasets and tasks strengthens the empirical claims.
Weaknesses
1. Clarity and Presentation: The paper is densely written, with heavy reliance on technical jargon and mathematical notation. While the content is technically sound, the presentation is challenging to follow, even for an expert audience. For example, the derivation of the reformulated adversarial prediction problem and the detailed algorithmic steps could benefit from more intuitive explanations or visual aids.
2. Experimental Scope: Although the experiments are thorough, they are limited to adversarial prediction and LPBoosting. It would be valuable to see the applicability of Breg-SVRG to a broader range of saddle-point problems or other machine learning tasks.
3. Proximal Update Complexity: The proximal update step, while efficient for the chosen problems, may be challenging to generalize to other settings. The authors acknowledge this limitation but do not provide concrete directions for addressing it.
Arguments for Acceptance
- Pro: The paper makes a novel and well-supported contribution to the field of stochastic optimization, extending SVRG to a broader class of problems with entropy regularization.
- Pro: Theoretical guarantees and empirical results demonstrate the effectiveness and efficiency of the proposed method.
- Pro: The work addresses a challenging and relevant problem in adversarial prediction and optimization, with potential for broader impact.
Arguments Against Acceptance
- Con: The dense presentation may limit accessibility and reproducibility for a broader audience.
- Con: The scope of experiments could be expanded to demonstrate generalizability beyond the chosen tasks.
Recommendation
I recommend acceptance of this paper, as its contributions are both novel and significant, advancing the state of the art in stochastic optimization and adversarial machine learning. However, I strongly encourage the authors to revise the manuscript for improved clarity and accessibility, particularly in the presentation of the algorithm and theoretical results.