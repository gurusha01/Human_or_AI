The paper introduces PROXASAGA, an asynchronous variant of the SAGA algorithm tailored for finite-sum convex optimization with nonsmooth regularizers. This work addresses key challenges in asynchrony and proximity, extending the applicability of variance-reduced methods to composite optimization problems. By leveraging sparse updates and block coordinate-wise gradients, the authors propose a novel algorithm that achieves linear convergence under strong convexity and Lipschitz gradient assumptions, albeit with a slightly smaller step size compared to SAGA. The theoretical framework is complemented by strong empirical results, demonstrating significant speedups on large-scale datasets.
Strengths:
1. Novelty and Scope: The paper makes a meaningful contribution by extending asynchronous optimization techniques to nonsmooth objectives, a gap in existing literature. The integration of variance reduction, sparse updates, and asynchronization is innovative and well-motivated.
2. Theoretical Rigor: The convergence proof is clean and well-organized, addressing the complexities introduced by asynchrony and sparsity. The bounded delay assumption (up to \( \sqrt{n}/10 \)) is reasonable and aligns with practical hardware constraints.
3. Practical Relevance: The empirical results are compelling, showcasing up to 12x speedups on multi-core architectures. The method outperforms state-of-the-art alternatives like ASYSPCD and FISTA by a large margin, particularly on sparse datasets.
4. Clarity of Contributions: The paper clearly delineates its contributions, including the development of Sparse Proximal SAGA, its asynchronous extension (PROXASAGA), and the analysis of convergence and speedup regimes.
5. Reproducibility: The inclusion of implementation details and a reference to the open-source code enhances reproducibility and practical adoption.
Weaknesses:
1. Clarity Issues: While the theoretical sections are rigorous, the paper occasionally suffers from inconsistent notation and undefined terms, which may hinder accessibility for non-expert readers. For example, the distinction between "virtual iterates" and "inconsistent reads" could be clarified earlier in the text.
2. Limited Generalization: The method relies on sparsity assumptions for its speedup guarantees. While this is reasonable for many machine learning problems, it limits the algorithm's applicability to dense datasets or problems without inherent sparsity.
3. Experimental Scope: Although the experiments are thorough, they focus primarily on \( \ell1 + \ell2 \)-regularized logistic regression. Additional benchmarks on other composite objectives (e.g., group lasso or constrained optimization) would strengthen the paper's generalizability claims.
4. Minor Textual Errors: There are small typographical and formatting issues, such as inconsistent use of symbols and references to undefined variables (e.g., \( \alpha \) in some equations).
Recommendation:
The paper makes significant theoretical and experimental contributions to asynchronous optimization, addressing an important gap in the literature. Its combination of variance reduction, sparse updates, and asynchronization is both novel and impactful. Despite minor clarity and scope issues, the strengths of the work far outweigh its weaknesses. The results are robust, the proofs are rigorous, and the practical implications are substantial.
Arguments for Acceptance:
- Advances the state of the art in asynchronous optimization for nonsmooth objectives.
- Provides a clean theoretical framework with strong empirical validation.
- Offers practical utility for large-scale machine learning problems on multi-core architectures.
Arguments Against Acceptance:
- Relies on sparsity assumptions, limiting general applicability.
- Minor clarity and notation issues could be improved.
Final Verdict: Accept. This paper is a high-quality contribution to the field and is well-suited for presentation at the conference.