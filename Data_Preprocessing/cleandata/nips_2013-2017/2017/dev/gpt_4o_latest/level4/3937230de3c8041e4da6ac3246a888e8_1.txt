The authors introduce variational memory addressing, which enhances generative models by incorporating external memory and hard attention. Notably, they derive read and write mechanisms that resemble classical probabilistic graphical models more closely than the more advanced mechanisms found in approaches like neural Turing machines.
In their framework, external memory functions similarly to a global variable in topic models and mixture models, where a "membership" is sampled via hard attention, followed by the generation of the local variable z and data x conditioned on the memory indexed by this membership. I found this perspective particularly insightful for interpreting memory within the context of latent variable models, where the writing process aligns with inference.
As highlighted by the authors in, for instance, L175-186, the algorithm appears to face scalability challenges as the size of the external memory increases. This can be mathematically explained by the rising variance of the black-box gradients with respect to the q(a) parameters as the size of a grows. It seems unlikely that VIMCO would significantly alleviate this issue. Nonetheless, I am impressed that the authors achieved compelling results with memory sizes (|M|) as large as 1024.