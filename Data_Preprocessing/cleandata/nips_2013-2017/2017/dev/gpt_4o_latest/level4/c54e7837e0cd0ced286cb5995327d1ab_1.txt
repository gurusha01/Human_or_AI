In this paper, the authors introduce the Batch Renormalization technique to address the limitations of batch normalization (batchnorm) when applied to small or non-i.i.d. minibatches. Reducing the reliance on large minibatch sizes is crucial for many applications, particularly when training large neural network models with constrained GPU memory. The proposed method is both straightforward to understand and easy to implement. Experimental results demonstrate that Batch Renormalization performs effectively with non-i.i.d. minibatches and enhances the performance of small minibatches compared to batchnorm.
The authors begin by providing a concise review of batchnorm, highlighting its key limitations: the inconsistency of mean and variance between training and inference, and its instability when applied to small minibatches. While using moving averages for normalization might seem like an intuitive solution, this approach can cause the model to diverge. To address this, the authors propose a simple Batch Renormalization method that combines the minibatch mean and variance with moving averages. In my view, Batch Renormalization essentially transitions from the original batchnorm (which normalizes using the minibatch mean and variance) to a version of batchnorm that relies almost entirely on moving averages. This gradual transition allows the model to leverage some of the benefits of moving averages while maintaining stable convergence.
However, I have a few questions regarding this work:  
(1) When using a large minibatch size (e.g., 32), why does Batch Renormalization offer no significant advantage over batchnorm? It appears that the consistency of mean and variance between training and inference does not provide much benefit in this scenario.  
(2) The experiments indicate that the performance with a small minibatch size (batch size = 4) is still inferior to that with a large minibatch size (batch size = 32). Would employing two (or multiple) moving averages for mean and variance, each with different update rates (e.g., one with 0.1 and another with 0.01), help address this issue? A smaller update rate could mitigate the inconsistency problem, while a larger update rate might address the challenges associated with small minibatch sizes.  
(3) The paper does not provide results on how the hyperparameters \( r{\text{max}} \) and \( d{\text{max}} \) influence performance. It seems that significant parameter tuning may be required.
Overall, this is a solid piece of work, and I look forward to seeing a more refined and elegant solution to this problem in the future.