The authors introduce a novel approach for performing inference in Gaussian processes (GPs) under linear equality constraints. Unlike prior methods, which often rely on techniques such as data augmentation with artificial observations, the proposed approach directly integrates the linear constraints into the GP kernel, ensuring that all realizations of the GP inherently satisfy the constraints. This represents an elegant and effective solution to the problem of linearly constrained GP regression.
The main limitation of the proposed method appears to lie in the non-trivial challenge of identifying an operator \( Gx \) that spans the nullspace of \( Fx \). While Algorithm 1 outlines an iterative procedure for constructing such an operator, the paper provides limited guidance on the critical step of selecting a set of scalar operators (\( \xig \)). For the illustrative example, the choice of differential operators seems intuitive given the structure of \( Fx \), but it remains unclear how this selection might be generalized to other cases. Furthermore, is the existence of such an operator always guaranteed?
Minor comments:
- Section 3.1 feels somewhat misplaced. It would fit more naturally under "Related Work," which is currently positioned at the end. I would recommend moving "Related Work" to precede Section 3.
- The explanation of "interpreting \( Fx \) and \( Gx \) as matrices" and "viewing \( F_x[f] \) as matrix-vector multiplications" is somewhat informal and could benefit from greater rigor. As it stands, readers may be uncertain about the conditions under which this interpretation is valid.