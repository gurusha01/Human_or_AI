SUMMARY
The paper addresses the challenge of multitask learning for Weighted Finite Automata (WFAs). It introduces a concept of task relatedness and proposes a novel algorithm that leverages this relatedness. Specifically, the algorithm stacks Hankel matrices from multiple tasks and applies a modified spectral learning technique, resulting in a vector-valued WFA (vv-WFA) capable of producing vector-valued predictions with a shared state representation. Additionally, a post-processing step is proposed to reduce the dimensionality of the WFA for individual tasks, aiming to mitigate noise. The algorithm is evaluated against the baseline of learning each task independently, using both synthetic and real-world datasets.
COMMENTS
This is a well-written paper overall. However, I have a concern regarding the experimental evaluation: it is crucial to compare the proposed approach to a baseline where data from all tasks are pooled together and treated as if they originate from a single task. When all tasks are identical, this baseline should theoretically outperform others since it fully utilizes all available data. Conversely, when tasks are unrelated, such an approach may introduce asymptotic approximation errors. The practical magnitude of this error, however, remains unclear. If the datasets used in the experimental section exhibit minimal error in such a scenario, they may not be particularly compelling, as any algorithm that aggregates data would likely outperform single-task learning. I would appreciate seeing some results (even preliminary ones) for this comparison during the rebuttal phase.
It would also be beneficial to compare the performance of Algorithm 1 with and without the projection step to assess the contribution of this post-processing procedure.
The paper's presentation could be enhanced by elaborating on potential application scenarios for multitask learning of WFAs. For instance, one could consider natural language modeling tasks where predictions are required in different contexts (e.g., online chat versus newspaper articles) with access to corresponding datasets. In such cases, it is reasonable to assume that fundamental grammar is shared across datasets and can be jointly learned. Of course, one could aggregate all datasets into a single one and train a unified model (corresponding to the baseline mentioned earlier), but this approach would fail to utilize the contextual information available during prediction.
Two additional suggestions:
- The current algorithm assumes equal weighting across tasks. While this assumption is reasonable when dataset sizes are comparable, it may lead to suboptimal performance when dataset sizes vary significantly. In such cases, a weighted approach could be considered; see Kulesza et al., Low-Rank Spectral Learning with Weighted Loss Functions.
- Another justification for the projection step is the scenario where the m tasks are entirely unrelated and each requires n states. Single-task learning would require n*m² parameters per character in the alphabet, whereas the multitask approach would involve a model of size (nm)². The projection step helps eliminate this redundancy.
MINOR ISSUE
Line 93: To my knowledge, it is not strictly necessary for the empty string to be included in prefixes or suffixes (at least in the Predictive State Representation (PSR) literature, which I am more familiar with). The authors may want to verify this.
---
POST-REBUTTAL COMMENTS
Thank you for the rebuttal and the additional results. I have no further concerns and will continue to advocate for acceptance.