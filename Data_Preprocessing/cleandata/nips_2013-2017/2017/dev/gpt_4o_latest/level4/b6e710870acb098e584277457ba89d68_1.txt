This paper demonstrates that "certain" adversarial prediction problems under multivariate losses can now be solved "much faster than before." The work is built on two central ideas: (1) the general saddle function optimization problem formulated in eq. (9) can be reduced from exponential to quadratic complexity (with respect to the sample size), and (2) the resulting simplified optimization problem, when regularized appropriately, can be addressed using an extension of the SVRG (stochastic variance reduction gradient) method adapted to Bregman divergences.
The paper primarily focuses on achieving a faster solution to the adversarial problem. However, the key simplification is specifically applied to the F-score loss, raising the question of whether the proposed method's advantages could generalize to other types of losses. While the extension of SVRG is a more broadly applicable result, the paper seems to emphasize adversarial optimization with the F-score, which could alternatively have been presented as a specific application of the proposed Breg-SVRG framework.
Overall, I find the paper to be technically sound and sufficiently compelling to warrant acceptance.