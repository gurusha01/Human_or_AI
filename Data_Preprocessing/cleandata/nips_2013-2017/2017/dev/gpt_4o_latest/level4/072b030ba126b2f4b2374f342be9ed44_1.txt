This paper addresses the problem of finite sum optimization using the SAGA framework. It builds upon the work of Leblond et al. (2017) by incorporating composite optimization, specifically focusing on the inclusion of non-smooth separable regularization. The main contribution lies in the method proposed to handle the non-smooth regularization while adhering to the principle of sparse updates. I find the proposed technique for splitting the non-smooth regularization particularly interesting. The analysis largely follows the framework established in Leblond et al. (2017), while the proof for the asynchronous variant draws upon the approach outlined in Mania et al. (2015).
Major Comments:
- The explanation of "inconsistent read" needs to be clarified. Specifically, what does \(\hat{x}_k\) represent, and how is the index \(k\) determined? Is this approach similar to that of Lian et al. (2016) and Liu and Wright (2015)? What are the key differences?
- The derivation of the concise representation for the difference between \(\hat{x}t\) and \(xt\) is unclear.
- To achieve the linear convergence rate and speedup, the paper relies on several implicit assumptions. The authors should explicitly state these assumptions.
Minor Comments / Typos:
- The definition of \(\Delta\) on line 217 is incorrect.
Missing Reference:
The following work on asynchronous greedy SDCA is relevant and should be cited:
- Asynchronous Parallel Greedy Coordinate Descent, NIPS, 2016.