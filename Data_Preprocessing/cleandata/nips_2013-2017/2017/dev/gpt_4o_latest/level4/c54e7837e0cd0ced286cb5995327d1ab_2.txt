This paper is engaging, well-structured, and concise. Batch normalization has demonstrated its effectiveness in addressing internal covariate shift and has become a widely adopted technique in training deep neural networks. However, it is well-documented that BN encounters challenges when dealing with small-sized mini-batches. Specifically, it leads to unreliable estimates of the mean and variance for each mini-batch. Additionally, the population mean and variance, which are used during classification or inference, are computed as a moving average of mini-batches during training, resulting in a mismatch. This paper introduces a straightforward approach to address these two issues in conventional BN. By incorporating an additional affine transformation to correct the bias between local and global normalization, the proposed renormalization ensures consistency between normalization during training and inference. The concept is simple, yet the experimental results presented suggest it is quite effective.
I do not have significant criticisms of the paper. However, I believe the experimental evaluation could be more compelling if additional results were provided for mini-batches of varying sizes to offer a comprehensive understanding of the behavior of this batch renormalization technique. In particular, I am intrigued by the scenario where the mini-batch size is reduced to one. In such cases, conventional batch normalization fails, but this renormalization method remains applicable.