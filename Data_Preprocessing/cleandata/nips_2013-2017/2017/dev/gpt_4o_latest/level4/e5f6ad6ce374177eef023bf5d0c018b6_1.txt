The paper presents a novel architecture for conditional video generation, leveraging convolutional LSTMs with the key insight that the hierarchical structure of prior architectures, where each layer represents increasingly abstract scene properties, may not be optimal for video generation. Unlike classification tasks, generative models must retain precise spatial information about objects even at the output layer. The proposed model, PredRNN, extends convolutional LSTMs by incorporating two memory cells: one that flows temporally within the same layer (as in the original ConvLSTM) and another that flows vertically across layers. The authors evaluate their model on two datasets—moving MNIST digits and the KTH action recognition dataset—demonstrating superior MSE performance in video prediction compared to prior approaches. The paper is well-written and appropriately cites relevant prior work.
The proposed architecture is both novel and compelling. However, the comparison with prior work could be improved to better highlight the empirical contributions. For instance, Kalchbrenner et al. (2016) claim near lower-bound performance on the moving MNIST task, yet their method is not included as a baseline. Additionally, prior work on moving MNIST (e.g., Kalchbrenner et al., 2016; Srivastava et al., 2016) reports results using cross-entropy loss, whereas the authors here focus on maximum likelihood output. Including likelihood metrics would facilitate a more direct comparison with these earlier studies.
Furthermore, while the authors mention computational efficiency as an advantage of their model, they do not provide a detailed analysis or comparison of computational complexity with prior methods, making it difficult to assess this claim quantitatively.
Minor notational suggestion:
To improve clarity, consider using "M" instead of "C" for the cell state in Equation 3, as this would make its connection to Equation 4 more intuitive for readers.
[After reviewing the authors' response, I believe the paper has been strengthened by the inclusion of additional comparisons with prior work, which I assume will be incorporated into the final version. However, I remain uncertain about the evaluation methodology—if the model is trained using MSE while other approaches use different loss functions, wouldn't this inherently favor the proposed model when evaluating on MSE?]