This paper investigates online convex optimization with time-varying constraints. The objective is to minimize regret, akin to the standard online convex optimization framework, while simultaneously ensuring that the average violation of stochastic constraints remains well-bounded. The authors tackle the problem in both fixed and stochastic settings and, under mild assumptions on the constraints, achieve optimal \(\sqrt{T} \log T\) bounds for both regret and constraint violation. This represents a notable improvement over the previously best-known \(T^{3/4}\) bound for constraint violations. Preliminary experiments on a stochastic job scheduler are presented, which support the theoretical findings.
The problem addressed in the paper is compelling and well-motivated, though it has been explored in prior work to some extent. However, this paper demonstrates a significant advancement by improving the existing bounds on both regret and constraint violations. The presentation is generally clear, and the claimed contributions are contextualized within the framework of existing literature. The paper appropriately surveys related work. In terms of writing quality, the paper is reasonably well-written, with good structure and language. It is technically sound, and the proofs appear correct based on the checks I performed (though I did not have sufficient time to thoroughly examine the Appendix; I plan to review it carefully during the rebuttal phase).
In summary, the paper offers a novel perspective on online linear optimization with constraints, necessitating a fresh analytical approach. While the exact setup of the feedback could benefit from stronger motivation, the overall concept of focusing on long-term constraint violations instead of relying on costly (and sometimes infeasible) projections in online learning is both interesting and well-justified.