Recently, there has been significant interest within the Algorithmic Game Theory community in developing approximately revenue-optimal auctions using a limited number of samples of buyers' valuations. A key question in this area is determining the minimum number of samples required to design a truthful auction that achieves an additive epsilon approximation to the optimal revenue. Typically, this quantity is bounded by leveraging learning theory concepts such as VC dimension and pseudo-dimension. This paper introduces a novel concept called split-sample complexity and uses it to derive sample complexity bounds for common auction settings.
The concept of split-sample complexity is defined as follows: Given a set of m samples, for any subset of size m/2, one can identify an optimal hypothesis. Let \hat{H}S represent the set of optimal hypotheses derived from all subsets of size m/2 within S. The split-sample growth rate measures the maximum size of the set \hat{H}S across all sets of size S.
The authors demonstrate that, given m samples, the expected revenue achieved by the optimal hypothesis derived from these m samples can be expressed in terms of the optimal revenue for the underlying distribution, along with an additive error term. This error term is shown to depend on the split-sample complexity as a function of m. By bounding the split-sample complexity for specific auction classes, the authors derive sample complexity bounds for these settings.
This novel framework enables the authors to improve upon prior bounds for bundle and item pricing mechanisms established by Morgenstern and Roughgarden.
In summary, this paper is well-written and introduces a fresh technique to address an active line of research.