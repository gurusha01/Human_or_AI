This paper addresses the problem of minimizing a decomposable submodular function. Submodular minimization is a well-established and significant problem in machine learning, with existing algorithms capable of solving it exactly. However, these algorithms often have high polynomial running times, making them impractical in many scenarios. To mitigate this issue, decomposable submodular functions (DSFM), which can be expressed as a sum of submodular functions over a much smaller support, are frequently considered due to their prevalence in practical applications.
The paper enhances the analysis of the fastest algorithms for DSFM by a factor corresponding to the number of functions in the decomposition. Additionally, it introduces an experimental framework that distinguishes between "level 0" algorithms, which serve as subroutines for quadratic minimization, and "level 1" algorithms, which minimize the function by treating level 0 as a black box. This framework enables more meaningful comparisons by standardizing the use of level 0 algorithms across different methods. The experiments reveal a tradeoff: discrete algorithms require more calls to the level 0 subroutines, while gradient-based methods impose weaker demands on level 0 but involve more computation at level 1.
The analysis is intricate, combining discrete and continuous optimization techniques to achieve a significant improvement in the running time for an important problem with high computational complexity. The experimental results also highlight a compelling tradeoff, suggesting that the choice of algorithm should depend on the specific context and computational requirements for DSFM.
One limitation of the paper is that the writing is dense and occasionally difficult to follow. A more detailed discussion of the parameters κ and ℓ, as well as the precise bounds in terms of these parameters, would have been beneficial. Furthermore, a theoretical comparison of the bounds for RCDM, ACDM, and IBFS would have added value to the paper.