Summary  
This paper introduces an algorithm designed to tackle a mixed online/stochastic scenario, where the objective function consists of a sequence of arbitrary convex functions (with bounded subgradients) under stochastic convex constraints (also with bounded subgradients). The authors provide a static regret analysis, both in expectation and with high probability. Additionally, a real-world experiment focused on job allocation across servers (aiming to minimize electricity costs) demonstrates the advantages of the proposed approach.
Assessment  
+ The structure and presentation are clear and well-organized.  
+ The work is technically robust and appears novel.  
+ The regret guarantees are strong.  
- The connection to "Deterministic constrained convex optimization" is unclear.  
- There is ambiguity regarding unreferenced prior work [A].  
- The experimental section requires more details.  
Further elaboration is provided below.
Further Comments  
- In Section 2.2, the algorithm's rationale is outlined, particularly the choice of \( x(t+1) \) to minimize the "drift-plus-penalty" term. Could the authors provide some intuition on how the drift and penalty terms are balanced (e.g., why a simple sum without a tuning weight suffices)?  
- Regarding Lemma 5, I lack the expertise to confidently evaluate whether it constitutes "a new drift analysis lemma for stochastic processes." My familiarity with the stochastic process literature is limited.  
- Could the performance be improved by allowing \( \alpha \) and \( V \) to vary with \( t \)?  
- For Lemma 9, it seems possible that the authors could leverage results from Proposition 34 in [B].  
- Comparison with "OCO with long-term constraints": It seems that [A] (which is not cited) already provides \( O(\sqrt{T}) \) guarantees for both regret and constraint violations using a similar algorithmic approach. This related work should be discussed.  
- Comparison with "Stochastic constrained convex optimization": Is [16] the only relevant reference in this context?  
- Clarification regarding "Deterministic constrained convex optimization": In deterministic constrained settings, optimization algorithms are typically expected to produce feasible solutions. Why is it acceptable here to allow constraint violations (e.g., \( O(1/\sqrt{T}) \))? Could the authors clarify why this makes sense in the stochastic setting?  
- Experiments:  
  - Additional details about the baselines and implementation (e.g., starting points) should be included in the main paper to ensure reproducibility.  
  - Is the set \( \mathcal{X}0 = [x1^{\text{min}}, x1^{\text{max}}] \times \dots \times [x{100}^{\text{min}}, x_{100}^{\text{max}}] \)? If so, this implies that problem (2) involves box constraints. More details would be helpful.  
  - Using a log scale for unserved jobs in Figure (d) might improve clarity.  
  - Enlarging the figures would enhance readability.  
  - An assessment of result variability is missing. Repeated experiments with standard errors and means would help establish the significance of the findings.  
  - To allocate more space for the experimental section, Section 4 could be condensed and moved to the supplementary material.  
- Could the analysis be extended to the dynamic regret setting?  
Minor Comments  
- Line 52: Typo — "min" should be "argmin." Additionally, under what assumptions does the argmin reduce to a unique \( x^* \)?  
- Line 204 (a): Should this be an inequality rather than an equality?  
[A] Yu, H. & Neely, M. J. A Low Complexity Algorithm with \( O(\sqrt{T}) \) Regret and Constraint Violations for Online Convex Optimization with Long Term Constraints. Preprint arXiv:1604.02218, 2016.  
[B] Tao, T. & Vu, V. Random Matrices: Universality of Local Spectral Statistics of Non-Hermitian Matrices. The Annals of Probability, 2015, 43, 782–874.  
==================  
Post-Rebuttal Comments  
==================  
I appreciate the authors' rebuttal (though the discussion of [A] was not addressed). After reviewing the other evaluations, I stand by my initial score. However, I strongly encourage the authors to clarify the discussion regarding the stochastic versus deterministic constrained convex optimization case, as outlined in their rebuttal.