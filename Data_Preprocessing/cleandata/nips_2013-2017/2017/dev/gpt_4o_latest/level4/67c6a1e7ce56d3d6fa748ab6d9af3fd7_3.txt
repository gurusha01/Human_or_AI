The authors present a method for action recognition that leverages a low-rank approximation of second-order pooling features. They evaluate their approach on three prominent vision benchmarks and demonstrate improvements over their baseline model.
Pros:
(+) The method is straightforward and can be integrated with any network architecture.  
(+) Demonstrates clear performance gains over the baseline.  
Cons:
(-) Limited novelty compared to existing second-order pooling techniques.  
(-) Lacks a fair comparison with other attention-based models.  
The novelty of the proposed approach relative to established low-rank second-order pooling methods, such as those used in fine-grained recognition (e.g., Lin et al., ICCV 2015) and semantic segmentation, is not evident. While the authors provide a reasonable explanation of the underlying linear algebra and the variables involved, the final method essentially reduces to a classifier applied to second-order features.
Moreover, some design choices in the proposed method are not adequately justified. For instance, in the multi-class recognition setup, the authors opt for a class-agnostic parameter \( b \) but a class-specific parameter \( a \). The rationale behind this choice is unclear, and additional experiments are needed to validate the effectiveness of this design decision.
The authors also claim that their approach eliminates the need for the box detection step required by methods like RCNN or Mallya & Lazebnik. However, they do not address how their method would generalize to instance-specific action recognition scenarios. In such cases, an image may contain multiple individuals performing different actions, and action labels are assigned at the instance level rather than the frame level. RCNN, for example, incorporates person box detection to handle this instance-level recognition task. The authors should discuss how their approach could be extended to this more general setup, as this currently appears to be a limitation rather than an advantage, contrary to their claims in the Related Work section.
Finally, the authors do not provide a fair comparison with competing methods. Their experiments use ResNet101 or Inception-V2 as the base network, and while they show improvements over their own non-attentive baseline using the same architecture, competing methods like R*CNN and Mallya & Lazebnik rely on VGG16. This discrepancy in base architectures makes the comparisons on the MPII and HICO datasets unfair and inconclusive. Additionally, on the HMDB dataset, the improvement over the TSN BN-Inception baseline is marginal, and comparisons with other methods remain unclear due to the variability in underlying architectures.