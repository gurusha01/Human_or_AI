This paper represents a significant advancement in enhancing the SGD training process for Neural Networks.
A comment I would like to offer:  
The renormalizing moving-average affine transformation A(r,d) and the output affine transformation A(beta,gamma) together constitute a composite affine transformation (refer to Algorithm 1, page 4). Consequently, it is possible to "absorb" the (r,d) transformation into a redefined (beta, gamma). As a result, the renormalization effectively translates into an adjustment of the (beta, gamma) training procedure.