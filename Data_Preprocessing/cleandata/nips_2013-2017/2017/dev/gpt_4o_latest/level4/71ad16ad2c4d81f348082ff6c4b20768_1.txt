This paper explores an intriguing topic: encoding symmetries or invariances into kernels. The primary application is Bayesian nonparametric regression, where constraints can be incorporated as prior information, even when the regression function's form is not fully specified. A notable example of such a constraint is modeling fields of the divergence type, ensuring conservation of a physical quantity. Prior work in this domain includes ANOVA kernels and Stein's method-based kernels, both constrained to have vanishing integrals. The current work distinguishes itself by attempting to address more general forms of constraints. This is a commendable effort, as generic kernels are still widely employed in machine learning without strong physical underpinnings.
While I believe the proposed method lacks generality and there is no theoretical guarantee it will succeed beyond specific cases, it remains an interesting contribution and could reasonably be included in the NIPS proceedings.
- The authors should discuss their work in the context of "Learning with Algebraic Invariances, and the Invariant Kernel Trick" by Franz J. Király, Andreas Ziehe, and Klaus-Robert Müller.
- On page 4, the claim that the prior for \( f \) will "inherit" the properties of the prior for \( g \) is incorrect. Specifically, if \( g \) has a prior \( \text{GP}(0, k) \), where \( k \) generates a Sobolev space of order \( b \), and \( \mathcal{F} \) is a differential operator of order \( c \), then the prior on \( f \) will correspond to a Sobolev space of order \( b - c \). Thus, the smoothness properties of \( f \) and \( g \) differ.
- On page 5, the equation \( fi = \Phii \xi f \) is presented as though it is without loss of generality, but it is, in fact, an ansatz. This should be made clearer by explicitly labeling it (and the equation \( g = \Gamma \xi^g \)) as an ansatz.
- The limited generality of the proposed method should be more explicitly acknowledged. Including an example where the method fails would help illustrate its limitations.