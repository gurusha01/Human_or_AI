This paper presents a novel convolutional LSTM-based architecture for next-frame video prediction. The key distinction from prior work lies in the integration of spatial and temporal variations into a unified memory pool.
---
Comments
The paper is generally well-written and introduces a new approach that shows promise on two relatively simple datasets.
Including generated videos as part of the submission would enhance the paper's presentation.
While the results appear promising, testing the proposed method on real natural image datasets would make the evaluation more compelling.
The KTH dataset is referred to as a collection of "natural image sequences," but it remains quite simplistic: it features very low resolution, uniform foregrounds, and backgrounds. Given that the proposed approach is claimed to be memory-efficient, handling more complex datasets should not pose a challenge.
Could the authors provide an estimate of the training time?
Lines 84–86: The authors discuss the applications of video prediction as established facts across various domains but fail to provide references. If these applications are already in use, the authors should include citations. If they are speculative, the phrasing should be adjusted (e.g., "finds" should be replaced with "could find").
Lines 105–106: The introduction of the deconvolution operator is unclear, as it does not appear to be utilized in the equations or elsewhere in the paper.
---
Minor Comments
- Line 47: Remove the comma.
- Lines 57–58: "they ... always" → "they always."
- Line 57: "the next one" → "the next."
- Line 135: Remove the comma.
- Line 141: "in in" → "in."
- Lines 135–137: This sentence is unclear and needs revision.
- Line 154: "We" → "we."
- Line 242: "ignores" → "ignore."
- Reference [9]: {LSTM} ICML 15.
- Reference [15]: ICLR workshop 16.
- Reference [19]: ICLR 16.
- Reference [23]: {SVM}.
- Reference [24]: ICLR 15.
- Please verify the accuracy of other references.