The paper introduces PROXASAGA, an asynchronous, variance-reduced optimization algorithm designed for composite objective functions, extending the SAGA framework to nonsmooth, large-scale problems. The authors claim two main contributions: (1) a sparse proximal variant of SAGA that leverages sparsity in gradients and block-separable penalties to reduce computational cost, and (2) an asynchronous, lock-free parallel implementation of this algorithm, achieving theoretical linear speedup under certain assumptions. Empirical results demonstrate significant performance improvements over state-of-the-art methods on large, sparse datasets.
Strengths:
1. Novelty and Contribution: The paper addresses a critical gap in the literature by extending variance-reduced methods like SAGA to nonsmooth composite objectives in an asynchronous setting. This is a significant advancement, as existing methods are either limited to smooth objectives or synchronous implementations.
2. Theoretical Rigor: The authors provide detailed convergence guarantees and speedup analysis, grounded in the "perturbed iterate" framework. The theoretical results are well-supported and align with the empirical findings.
3. Practical Usefulness: The algorithm is highly relevant for modern machine learning tasks, such as Lasso and group Lasso, where nonsmooth penalties and large-scale datasets are common. The demonstrated speedups (up to 12x on a 20-core machine) and orders-of-magnitude runtime improvements over competitors highlight its practical utility.
4. Empirical Validation: The experiments are thorough, comparing PROXASAGA against strong baselines (e.g., ASYSPCD and FISTA) on real-world datasets. The results convincingly show that PROXASAGA is faster and scales well with the number of cores, particularly for sparse datasets.
5. Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical analysis, and experimental setup. The inclusion of practical implementation details (e.g., memory compression) adds to its reproducibility.
Weaknesses:
1. Sparsity Assumption: The algorithm's performance heavily relies on the sparsity of gradients and penalties. While this is common in many applications, the method's applicability to dense datasets or penalties is limited. The authors acknowledge this but do not explore potential extensions to handle dense scenarios.
2. Memory Access Overhead: The empirical speedups are sublinear due to memory access overhead, which is not accounted for in the theoretical model. A discussion on mitigating this bottleneck (e.g., through hardware-specific optimizations) would strengthen the paper.
3. Comparison with Related Work: While the paper provides a detailed theoretical comparison with ASYSPCD and other methods, the empirical comparison could include additional baselines, such as ProxSVRG, to provide a more comprehensive evaluation.
4. Scalability Beyond 20 Cores: The experiments are limited to a 20-core machine. It is unclear how the algorithm scales on architectures with significantly more cores, which could be relevant for cloud-based or distributed systems.
Recommendation:
I recommend acceptance of this paper. The proposed method is a substantial contribution to the field of optimization for machine learning, addressing a key limitation of existing asynchronous methods. The theoretical and empirical results are robust, and the paper is well-written and organized. While the reliance on sparsity and memory overhead are limitations, they do not detract significantly from the overall impact of the work.
Arguments for Acceptance:
- Novel and impactful extension of SAGA to nonsmooth, asynchronous settings.
- Strong theoretical guarantees and practical relevance.
- Significant empirical improvements over state-of-the-art methods.
Arguments Against Acceptance:
- Limited applicability to dense datasets.
- Sublinear empirical speedups due to memory overhead.
Overall, the paper advances the state of the art in asynchronous optimization and is likely to inspire further research in this area.