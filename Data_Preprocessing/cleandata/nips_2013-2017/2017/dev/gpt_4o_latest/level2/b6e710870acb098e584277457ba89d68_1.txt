The paper presents a novel stochastic variance-reduced algorithm, Breg-SVRG, designed to optimize saddle-point problems, particularly in adversarial machine learning settings. The authors extend the stochastic variance-reduction framework to accommodate Bregman divergences, enabling better adaptation to the underlying geometry of optimization problems. The key contributions include a reformulation of adversarial prediction problems to reduce dimensionality, a new proof technique to handle the asymmetry of Bregman divergences, and empirical validation of the algorithm's efficiency on adversarial prediction and LPBoosting tasks.
Strengths:
1. Novelty and Theoretical Contributions: The extension of stochastic variance-reduced methods to Bregman divergences is a significant innovation. The authors introduce a new Pythagorean theorem for saddle functions and provide a rigorous convergence analysis, demonstrating a linear rate of convergence. This work fills a gap in the literature, as prior methods relied heavily on Euclidean norms.
   
2. Practical Impact: The reformulation of adversarial prediction problems to reduce dimensionality from \(2^n\) to \(n^2\) is a substantial improvement, making these problems computationally tractable. The demonstrated speedup (by a factor of \(n\)) over Euclidean-based alternatives highlights the practical utility of the proposed approach.
3. Empirical Validation: The experiments on adversarial prediction and entropy-regularized LPBoosting effectively showcase the advantages of Breg-SVRG, particularly its ability to adapt to problem geometry and achieve faster convergence. The comparison with Euclidean-SVRG and convex optimization methods is thorough and convincing.
4. Clarity of Results: The paper provides detailed experimental results, including primal gap reduction, test accuracy, and F-score comparisons, which clearly illustrate the superiority of the proposed method in relevant scenarios.
Weaknesses:
1. Complexity of Presentation: While the theoretical contributions are significant, the dense mathematical exposition may hinder accessibility for readers unfamiliar with stochastic variance-reduction or Bregman divergences. Simplifying some sections or providing more intuitive explanations could improve clarity.
2. Limited Scope of Applications: Although the paper demonstrates the effectiveness of Breg-SVRG on adversarial prediction and LPBoosting, it would benefit from additional experiments on other saddle-point problems, such as GANs or reinforcement learning, to broaden its impact.
3. Proximal Update Complexity: The proximal update step in Breg-SVRG, while theoretically justified, may pose practical challenges for large-scale problems. The authors acknowledge this limitation but do not provide a concrete strategy for relaxing this requirement.
4. Comparison with State-of-the-Art: The paper primarily compares Breg-SVRG with Euclidean-SVRG and convex optimization methods. A comparison with other recent saddle-point optimization algorithms, such as those based on adaptive gradient methods, would strengthen the evaluation.
Recommendation:
This paper makes a strong contribution to the field of saddle-point optimization and adversarial machine learning. The theoretical advancements and practical speedups demonstrated by Breg-SVRG are compelling. However, the paper would benefit from broader application scenarios and improved accessibility for a wider audience. I recommend acceptance, contingent on minor revisions to improve clarity and include additional comparisons with state-of-the-art methods.
Pro and Con Arguments for Acceptance:
Pros:
- Significant theoretical innovation with rigorous analysis.
- Practical speedups and dimensionality reduction for adversarial prediction.
- Empirical results validate the proposed method's effectiveness.
Cons:
- Dense presentation may limit accessibility.
- Limited application scope and comparisons with other recent methods.
Overall, the paper is a valuable contribution to the field and is well-suited for presentation at NIPS.