This paper introduces a novel approach to training feed-forward deep neural networks (DNNs) by lifting the ReLU activation function into a higher-dimensional space, enabling a smooth multi-convex formulation. The authors propose a block coordinate descent (BCD) algorithm that guarantees global convergence to stationary points with an R-linear convergence rate of order one. The approach is validated empirically on the MNIST dataset, where the proposed method outperforms stochastic gradient descent (SGD) variants in terms of test-set error rates, computational efficiency, and sparsity of learned networks.
Strengths:
1. Novelty and Originality: The paper presents a unique reformulation of the DNN training problem using Tikhonov regularization and multi-convex optimization. This is a significant departure from traditional SGD-based methods and addresses key challenges such as vanishing gradients and poor convergence properties in deep networks.
2. Theoretical Contributions: The authors provide rigorous theoretical guarantees for the convergence of their BCD algorithm, including global convergence to stationary points and an R-linear convergence rate. This is a notable advancement over existing methods like SGD and ADMM, which lack such guarantees in the context of DNN training.
3. Empirical Validation: The experiments on MNIST demonstrate the effectiveness of the proposed method, showing superior test-set error rates and faster convergence compared to SGD-based solvers. The ability to learn sparse networks without significant performance degradation is particularly impressive and has practical implications for resource-constrained environments.
4. Clarity in Problem Formulation: The reinterpretation of ReLU as a projection onto a convex set and its integration into the Tikhonov regularization framework is well-articulated and mathematically sound.
Weaknesses:
1. Scope of Experiments: While the MNIST results are promising, the evaluation is limited to a single dataset. Testing on more diverse and complex datasets (e.g., CIFAR-10, ImageNet) would strengthen the claims of generalizability and scalability.
2. Computational Complexity: The authors acknowledge that their algorithm involves solving a sequence of quadratic programs (QPs), which can be computationally expensive. Although the paper claims faster training times compared to SGD, the scalability of the method to very large datasets and deeper networks remains unclear.
3. Comparison with Related Work: The paper provides a detailed discussion of related work but could benefit from more direct empirical comparisons with state-of-the-art methods like ADMM-based solvers or other recent advancements in optimization for deep learning.
4. Practical Usability: The reliance on a linear SVM for classification at test time may limit the practical applicability of the method in end-to-end learning scenarios. This aspect could be further explored or justified.
Arguments for Acceptance:
- The paper makes a strong theoretical and empirical contribution to the optimization of DNNs, addressing fundamental issues like vanishing gradients and convergence guarantees.
- The proposed method shows potential for learning sparse networks, which is valuable for applications in embedded systems and resource-constrained environments.
- The work is well-grounded in mathematical theory and provides a clear path for future research.
Arguments Against Acceptance:
- The experimental validation is limited in scope, and the computational complexity of the method may hinder its applicability to large-scale problems.
- The reliance on external classifiers at test time detracts from the end-to-end nature of modern deep learning pipelines.
Recommendation:
Overall, this paper presents a significant and well-founded contribution to the field of optimization for deep learning. While there are limitations in experimental scope and practical usability, the theoretical advancements and empirical results justify its acceptance. I recommend acceptance with minor revisions to address the scalability and applicability concerns.