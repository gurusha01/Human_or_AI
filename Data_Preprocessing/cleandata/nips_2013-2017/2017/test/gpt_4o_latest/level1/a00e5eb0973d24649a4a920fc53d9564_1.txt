Review
This paper investigates the theoretical limitations of variance-reduction and acceleration techniques for finite sum optimization problems, focusing on the interplay between algorithmic assumptions and achievable iteration complexity bounds. The authors present three key results: (1) the finite sum structure alone is insufficient to achieve the complexity bound \( \tilde{O}((n + L/\mu) \ln(1/\epsilon)) \) unless the oracle explicitly identifies the individual function being queried, (2) accelerated rates of \( \tilde{O}((n + \sqrt{nL/\mu}) \ln(1/\epsilon)) \) are unattainable without explicit knowledge of the strong convexity parameter, and (3) for smooth and convex finite sums, oblivious algorithms with fixed update rules are fundamentally limited to a lower bound of \( \Omega(n + L/\epsilon) \). These results are supported by rigorous lower-bound proofs and reductions that leverage information-theoretic arguments and the framework of Canonical Linear Iterative (CLI) algorithms.
The paper builds on prior work in variance-reduction methods (e.g., SAG, SVRG, SDCA) and acceleration techniques (e.g., Nesterov's AGD), while addressing open questions about their theoretical limitations. The authors extend results from foundational works such as [3], [4], and [16], and provide new insights into the role of oracle assumptions and algorithmic design in determining iteration complexity. The use of restarting schemes to analyze smooth and convex settings is particularly noteworthy, as it bridges the gap between strongly convex and non-strongly convex cases.
Strengths:
1. Theoretical Rigor: The paper provides detailed and well-constructed proofs, leveraging advanced techniques from information theory and polynomial approximation to establish tight lower bounds.
2. Novel Contributions: The results clarify important limitations of variance-reduction and acceleration methods, particularly in scenarios where oracle information or strong convexity parameters are unavailable.
3. Breadth of Analysis: The paper covers a wide range of settings, including smooth and strongly convex, smooth and convex, and oblivious algorithms, offering a comprehensive perspective on the problem.
4. Clarity of Results: The main contributions are clearly stated, and the implications for practical algorithm design (e.g., the necessity of explicit enumeration or strong convexity knowledge) are well-articulated.
Weaknesses:
1. Practical Implications: While the theoretical results are significant, the paper does not provide much discussion on how these limitations translate to real-world applications or how practitioners might address them.
2. Complexity of Presentation: The paper is dense and assumes a high level of familiarity with optimization theory, variance-reduction methods, and CLI frameworks. This may limit accessibility for a broader audience.
3. Experimental Validation: The paper is purely theoretical, and while this is appropriate for its scope, some empirical demonstrations of the practical impact of these limitations would strengthen the work.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution by identifying fundamental limitations of widely used optimization techniques.
- The results are novel, rigorous, and extend the state of the art in finite sum optimization theory.
- The work is highly relevant to the NeurIPS community, given the widespread use of variance-reduction and acceleration methods in machine learning.
Arguments Against Acceptance:
- The paper's focus on theoretical results without practical validation may limit its immediate impact on practitioners.
- The dense presentation and reliance on advanced mathematical tools may make the paper less accessible to a general audience.
Recommendation:
I recommend acceptance of this paper. Its theoretical contributions are substantial and address important questions in the field of optimization. However, I encourage the authors to include a discussion of potential practical implications and strategies to mitigate the identified limitations in future revisions.