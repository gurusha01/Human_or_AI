This paper introduces a novel approach for training feed-forward deep neural networks (DNNs) by reformulating the optimization problem as a multi-convex objective using Tikhonov regularization. The authors propose a Block Coordinate Descent (BCD) algorithm to solve this formulation, offering theoretical guarantees of global convergence to stationary points with R-linear convergence rates. Experimental results on the MNIST dataset demonstrate that the proposed method outperforms traditional stochastic gradient descent (SGD) variants in terms of test-set error rates, training speed, and sparsity of learned networks.
Strengths:
1. Novelty: The paper presents a fresh perspective on training DNNs by lifting the ReLU function into a higher-dimensional space and leveraging Tikhonov regularization. This approach is innovative and addresses key challenges in deep learning, such as vanishing gradients and poor convergence properties of SGD.
2. Theoretical Contributions: The authors provide rigorous theoretical analysis, proving global convergence and R-linear convergence rates for the proposed BCD algorithm. This is a significant improvement over the weak convergence guarantees of SGD.
3. Empirical Validation: The experiments on MNIST convincingly demonstrate the effectiveness of the proposed method. The BCD algorithm achieves better test-set accuracy and faster convergence compared to six widely used SGD variants.
4. Sparsity: The sparse version of the algorithm (BCD-S) is particularly noteworthy, as it learns highly sparse networks with minimal loss in accuracy. This has practical implications for resource-constrained environments.
5. Clarity: The paper is well-organized, with clear explanations of the problem setup, methodology, and experimental results. The inclusion of detailed mathematical proofs enhances its credibility.
Weaknesses:
1. Scope of Experiments: While the MNIST dataset is a good starting point, the evaluation is limited to a single dataset and relatively simple architectures. Testing on more complex datasets (e.g., CIFAR-10, ImageNet) and deeper networks would strengthen the claims of generalizability.
2. Computational Complexity: The authors acknowledge that the BCD algorithm involves solving quadratic programs, which can be computationally expensive. While the paper emphasizes theoretical development, practical scalability to larger datasets and networks is not fully addressed.
3. Comparison with State-of-the-Art: The paper compares the proposed method with SGD variants but does not benchmark against other advanced optimization techniques, such as AdamW or second-order methods like L-BFGS.
4. Reproducibility: Although the theoretical framework is detailed, the lack of implementation details (e.g., hyperparameter tuning, code availability) may hinder reproducibility.
Arguments for Acceptance:
- The paper introduces a novel and theoretically sound approach to DNN training, addressing fundamental limitations of SGD.
- The empirical results, though limited in scope, convincingly demonstrate the advantages of the proposed method.
- The work is well-aligned with the conference's focus on advancing the state of the art in neural information processing.
Arguments Against Acceptance:
- The experimental evaluation is narrow, limiting the generalizability of the findings.
- The computational overhead of the BCD algorithm may pose challenges for large-scale applications.
Recommendation:
I recommend acceptance with minor revisions. The theoretical contributions and promising empirical results make this paper a valuable addition to the field. However, expanding the experimental evaluation and discussing practical scalability would significantly enhance its impact.