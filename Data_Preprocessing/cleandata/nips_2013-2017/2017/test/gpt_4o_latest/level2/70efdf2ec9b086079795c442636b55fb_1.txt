The paper introduces the Tensorized LSTM (tLSTM), a novel approach to improving the capacity of Long Short-Term Memory (LSTM) networks by leveraging tensorized hidden states and cross-layer convolutions. The main claims are that tLSTM can widen and deepen LSTMs efficiently without significantly increasing the number of parameters or runtime, and that it achieves state-of-the-art or competitive performance on a variety of sequence learning tasks. The authors also propose a memory cell convolution mechanism to capture long-range dependencies and introduce channel normalization (CN) to improve training stability.
Strengths:
1. Novelty and Innovation: The paper presents a unique approach to scaling LSTMs using tensorized hidden states and cross-layer convolutions. The idea of merging deep computations with temporal computations is particularly innovative and addresses runtime bottlenecks in deep LSTMs.
2. Comprehensive Experiments: The authors evaluate tLSTM on diverse tasks, including language modeling, algorithmic tasks, and sequential image classification. The results consistently demonstrate the model's effectiveness, particularly in handling long-range dependencies and achieving competitive performance with fewer parameters and faster runtime.
3. Practical Significance: The proposed method is highly practical, as it reduces computational overhead while improving model capacity. The ability to scale LSTMs without significant parameter growth is a valuable contribution for real-world applications.
4. Clarity of Results: The experimental results are well-organized, with clear comparisons to baseline and state-of-the-art methods. The inclusion of runtime analysis and visualizations of memory cell behavior provides additional insights into the model's workings.
5. Theoretical Contributions: The introduction of memory cell convolution and channel normalization adds depth to the theoretical framework, and the paper provides detailed derivations and explanations.
Weaknesses:
1. Limited Discussion of Limitations: While the paper acknowledges that tLSTM cannot parallelize deep computations during autoregressive sequence generation, other potential limitations, such as scalability to extremely large datasets or tasks with highly irregular temporal patterns, are not discussed in detail.
2. Complexity of Implementation: The proposed model introduces several new components (e.g., memory cell convolution, dynamic kernel generation), which may make implementation challenging for practitioners. A more detailed discussion of implementation trade-offs would be helpful.
3. Comparison to Broader Methods: While the paper compares tLSTM to state-of-the-art LSTM variants, it does not extensively discuss how it performs relative to non-LSTM-based architectures like Transformers, which dominate many sequence modeling tasks.
4. Reproducibility: Although the paper provides detailed equations, the lack of publicly available code or explicit reproducibility guidelines may hinder adoption by the community.
Recommendation:
The paper is a strong contribution to the field of sequence modeling and is well-suited for presentation at NIPS. Its novel approach to scaling LSTMs, combined with rigorous experimental validation, makes it a valuable addition to the literature. However, the authors should consider addressing the reproducibility and implementation challenges in a future revision. Additionally, a more thorough discussion of limitations and comparisons to non-LSTM architectures would strengthen the paper further.
Arguments for Acceptance:
- Novel and practical contributions to LSTM scalability.
- Strong experimental results across diverse tasks.
- Clear theoretical framework and detailed analysis.
Arguments Against Acceptance:
- Limited discussion of broader limitations and comparisons.
- Potential implementation complexity for practitioners.
Final Decision: Accept with minor revisions.