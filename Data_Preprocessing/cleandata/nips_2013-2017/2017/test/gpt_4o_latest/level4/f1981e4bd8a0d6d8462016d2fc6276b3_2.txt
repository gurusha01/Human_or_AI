This paper addresses the challenge of learning a sequence classifier without relying on labeled data by leveraging sequential output statistics. The motivation stems from scenarios where obtaining labels is expensive, but sequential output statistics are readily accessible. A notable example is OCR, where language models serve as the output statistics. The paper introduces an unsupervised learning cost function that is conceptually straightforward: it treats the mapping from input sequences to output sequences as a transformation of the input distribution into a distribution in the output domain, with the cost defined as the cross-entropy between the output statistics and the transformed distribution.
The authors highlight several appealing properties of the proposed cost function. While it is non-convex, it possesses a coverage-seeking property that makes it more favorable for optimization compared to existing approaches. Additionally, the paper reformulates the cost into its primal-dual form and develops a stochastic gradient method for optimization. Experimental results on two real-world applications demonstrate the effectiveness of the proposed method, achieving performance comparable to supervised learning, whereas alternative methods perform poorly.
The paper is well-written and clearly presented. It provides detailed comparisons with related works, though I lack sufficient expertise in this area to confirm whether the coverage is exhaustive.
In summary, the proposed method is intuitive, and the paper thoroughly discusses its advantages over existing approaches. The experimental results strongly validate the method's effectiveness.
Minor comment:
--Line 283: rate rate â†’ rate