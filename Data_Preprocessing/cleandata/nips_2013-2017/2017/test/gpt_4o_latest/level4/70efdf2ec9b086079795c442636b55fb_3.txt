This paper introduces Tensorized LSTMs as a novel approach for efficient sequence learning. The method represents hidden layers as tensors and incorporates cross-layer memory cell convolution to enhance both efficiency and effectiveness. The model is well-articulated, and experimental results demonstrate the utility of the proposed approach.
While the paper is generally well-written, I have several questions and points of confusion that I would like the authors to address. My final decision may be reconsidered if these concerns are adequately clarified in the rebuttal.
1. My primary confusion arises from Section 2.1, where the authors describe widening the network using convolution (lines 65–73). The text states that "P is akin to the number of stacked hidden layers" and that the model "locally-connects" along the P direction to share parameters. However, this seems more like a strategy to deepen the network rather than widen it, as increasing P (the number of hidden layers) does not introduce additional parameters in the convolution. Similarly, in lines 103–104, the authors claim that tRNN can be "widened without additional parameters by increasing the tensor size P." This assertion is unclear, as increasing P conceptually corresponds to increasing the number of hidden layers in sRNN, which would deepen the network rather than widen it.
2. The authors assert that the network can be deepened using delayed outputs (Section 2.2), with the parameter L controlling the network depth. However, as shown in Eq. 9, L is determined by P and K, which implies that the network depth cannot be freely adjusted as an independent parameter. In practice, it seems that P and K would be pre-set before conducting experiments, and L would then be derived from Eq. 9. This makes the claim in lines 6–10, which suggests that both the width and depth of the network can be freely configured, appear overstated.
3. The authors claim that the proposed memory cell convolution can prevent gradient vanishing or exploding (line 36). However, this claim is neither theoretically justified nor empirically validated in the paper. Furthermore, the terms "gradient vanishing" and "gradient exploding" are not mentioned or discussed in the subsequent sections.
4. In the experiments, the authors evaluate tLSTM variants across dimensions such as tensor shape (2D or 3D), normalization (none, LN, CN), memory cell convolution (enabled or disabled), and feedback connections (enabled or disabled). This results in 2×3×2×2=24 possible combinations. However, only six combinations are selected for evaluation (lines 166–171). While I understand that testing all combinations could become unwieldy, the rationale for selecting these specific six combinations is unclear. Additionally, some potentially interesting variants, such as 2D tLSTM with CN, are omitted. It might also be helpful to organize the experiments into groups, such as one focusing on normalization strategies, another on memory cell convolution, and another on feedback connections, to provide more structured insights.