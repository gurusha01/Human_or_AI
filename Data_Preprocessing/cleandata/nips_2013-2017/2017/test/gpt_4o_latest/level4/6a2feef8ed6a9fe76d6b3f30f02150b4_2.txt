The paper consolidates several significant recent ideas on optimizing deep neural networks as an alternative to widely used SGD variants. The derivation of the Tikhonov regularized problem in Eqs. (4) and (5) from the recursive objective in Eq. (2), achieved by relaxing the ReLU outputs as a convex projection, is presented in a clear and compelling manner. The decomposition into three components—an inverse problem (activations), a least-squares problem (weights), and a final classification problem (soft-max weights)—is particularly insightful. This formulation offers a fresh perspective on deep learning and opens up numerous intriguing possibilities, such as generalizing ReLUs, enabling more flexible connectivity patterns through shortcuts (as demonstrated in the chosen architecture), and promoting sparsity in network connectivity.
Regarding optimization, the paper highlights the challenges in guaranteeing convergence for a straightforward alternating optimization approach (e.g., ADMM) and instead adopts a more tractable block-coordinate descent algorithm, detailed in Algorithm 1 and analyzed in Section 4. On the positive side, the authors successfully derive a convergent algorithm. However, on the downside, the work appears to represent an initial, somewhat preliminary exploration of the potential benefits of the proposed problem formulation, leaving substantial room for further investigation.
A proof of concept is provided using MNIST data in Section 3. However, the experimental section is relatively underdeveloped, featuring only a single dataset, MATLAB versus Python code comparisons, and minimal analysis. A more comprehensive quantitative (e.g., run-time, solution quality) and qualitative investigation is needed. Additionally, Figure 4, which presents a percentage pie chart, is quite confusing. For the final version, a more thorough evaluation would significantly enhance the overall quality of the paper.