This paper introduces a straightforward and effective block coordinate descent (BCD) algorithm incorporating a novel Tikhonov regularization technique for training both dense and sparse deep neural networks (DNNs) with ReLU activations. The authors demonstrate that the proposed BCD algorithm achieves global convergence to a stationary point with an R-linear convergence rate of order one and outperforms various stochastic gradient descent (SGD) variants in experimental evaluations. However, the rationale behind employing Tikhonov regularization and block coordinate descent is not sufficiently explained. Additionally, the technical exposition lacks clarity due to the omission of critical details. The experimental results fall short of state-of-the-art performance, raising doubts about the applicability of the proposed method to practical "DNNs." My detailed comments are as follows:
---
Specific Comments:
1. The motivation for using Tikhonov regularization requires further elaboration. Why was Tikhonov regularization chosen over alternative regularization methods? What specific advantages does it offer compared to other approaches? In the second paragraph of Section I, the authors highlight several challenges in DNNs, such as "highly non-convex" landscapes, "saddle points," and "local extrema," followed by the introduction of Tikhonov regularization. Does this imply that the proposed Tikhonov regularization addresses all these challenges?
2. The rationale for decomposing the problem into three sub-problems is unclear. The authors should clarify why this decomposition avoids issues such as vanishing gradients.
3. The derivation from Eqn. (4) to Eqn. (5) is difficult to follow. Specifically, on line 133, what is the explicit formula for the matrix \( Q(\tilde{\mathcal{A}}) \)? How is it ensured that \( Q(\tilde{\mathcal{A}}) \) is positive semidefinite?
4. A precise mathematical definition of the Tikhonov regularized inverse problem should be provided.
5. The statement that "the inverse sub-problem resolves the vanishing gradient issue in traditional deep learning because it seeks the optimal solution for the output features of each hidden layer, which are interdependent through the Tikhonov matrix" is a strong claim. However, there is no theoretical justification or empirical evidence provided to substantiate this assertion.
6. Additional details are needed to explain why the "optimal" output features behave similarly to target propagation.
7. On line 246, the definition of \( \mathcal{P} \) is missing and should be clarified.
8. In Fig. 5, while BCD-S is shown to learn much sparser networks, the weight matrix of the 4th layer remains dense. The authors should explain this discrepancy.
9. On line 252, the meaning of the prefix "Q" is unclear and should be defined.
10. On line 284, there is a missing word: "dense."
11. The formatting of the references is inconsistent and needs to be corrected.
12. The choice of the DNN structure in Figure 1 is unclear. The network used in the experiments contains only three hidden layers, which is not sufficiently "deep." Should skip connections be incorporated? Furthermore, the experimental results are not competitive with state-of-the-art methods, raising concerns about the practical applicability of the proposed approach to real-world "DNNs."