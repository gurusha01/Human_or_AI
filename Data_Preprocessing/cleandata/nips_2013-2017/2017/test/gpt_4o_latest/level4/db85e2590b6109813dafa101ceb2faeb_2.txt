This paper introduces a low-rank regularizer aimed at compressing deep models during the training phase. Overall, the paper is well-structured, and the motivation is clearly articulated. However, I have the following comments:
1. The novelty of the work appears to be somewhat limited, as the technical contributions are closely aligned with prior research.  
2. The experimental evaluation could be further strengthened:  
   (1) Parameter sensitivity: As shown in Fig. 1, the performance of the proposed method (\tau = 1, \lambda ≠ 0) is comparable to [2] (\tau = 0, \lambda ≠ 0). For other values of \tau, the compression rate improves, but this comes at the cost of reduced accuracy.  
   (2) Results on larger models: A direct comparison with [2] on larger models is necessary to better demonstrate the method's effectiveness. Additionally, it would be valuable to include comparisons with other state-of-the-art compression techniques, such as [18].