This paper presents TrajGRU, an extension of convolutional LSTM/GRU architectures. Unlike convLSTM/GRU, TrajGRU is designed to learn location-dependent filter support for each hidden state location. It achieves this by generating a flow field from the current input and the previous hidden state, subsequently warping the previous hidden states using bilinear sampling guided by this flow field.
The authors evaluate their proposed model on video generation tasks using two datasets: MovingMNIST with three digits simultaneously and the HKO-7 nowcasting dataset. In these experiments, TrajGRU demonstrates superior performance compared to its convolutional counterparts.
Specific questions/remarks:
- Did you compare TrajGRU with ConvGRU models that use larger filter supports than the standard 5x5 kernels?
- Does TrajGRU incur additional computational overhead compared to ConvGRU due to the warping operation? It would be helpful to include details on the total number of parameters, computational complexity, and runtime for the models evaluated in the experiments section.
- Why was the model trained for a fixed number of epochs instead of employing early stopping? Could the performance of certain models improve if training were terminated earlier?
Quality  
The paper appears to be technically sound.
Clarity  
The paper is generally clear. However, it would be beneficial to specify the warping method used to provide a more comprehensive understanding of TrajGRU. Additionally, the number of examples and the training/validation/test splits for the HKO-7 dataset are not clearly stated.
Originality  
While a few prior works have explored the use of warping in video modeling (e.g., "Spatio-temporal video autoencoder with differentiable memory"), this paper takes a novel approach. It would be valuable to compare and contrast TrajGRU with such methods.
Significance/Conclusion  
Developing models that effectively learn video representations remains an open research challenge. This paper introduces a novel approach by proposing a model that learns both the filter support and filter weights, representing an interesting advancement in video modeling.  
However, the evaluation is currently limited to one synthetic dataset (MovingMNIST) and one specialized nowcasting dataset. It would strengthen the paper to demonstrate whether this model improves video representations for more general tasks, such as human action classification on generic video datasets.