This paper introduces a novel aggregation rule for gradient vectors obtained from various workers in distributed stochastic gradient descent, aimed at mitigating Byzantine failures. The authors demonstrate that standard aggregation methods, such as averaging, are incapable of tolerating even a single Byzantine failure. To address this, the paper introduces the concept of Byzantine resilience to quantify the tolerance of such failures and proves that the proposed aggregation function satisfies this property. Experimental results indicate that the proposed method significantly outperforms the baseline as the number of Byzantine workers increases. However, the approach introduces computational overhead even in the absence of Byzantine workers, highlighting the need to balance robustness and efficiency.
While I am not deeply familiar with distributed stochastic gradient descent, my main question pertains to the machine learning tasks themselves. Specifically, in Figure 4, it appears that Krum achieves a worse learning error compared to simple averaging. I am curious about the extent of this difference in errorâ€”can it be bounded? Additionally, what is the performance cost incurred to ensure robustness against Byzantine failures when using Krum?