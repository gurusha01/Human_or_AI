In this work, the authors introduce an accelerated version of the Min-Sum message-passing protocol tailored for addressing consensus problems in distributed optimization. Leveraging the reparametrization techniques proposed in [Ruozzi and Tatikonda, 2013], the authors derive convergence rates for the Min-Sum Splitting algorithm specifically for consensus problems with quadratic objective functions. A key element of their analysis is the formulation of an auxiliary linear process that serves to track the progression of the Min-Sum Splitting algorithm.
The primary contributions of the paper are as follows: (i) the development of a novel proof technique for analyzing the Min-Sum Splitting algorithm, centered on the introduction of an auxiliary process, (ii) the design of a Min-Sum protocol for consensus problems that demonstrates improved convergence rates compared to prior results, and (iii) the establishment of connections between the proposed method and concepts such as lifted Markov chains and multi-step methods in convex optimization.
The paper's motivation and contributions are well-articulated. The writing is generally clear and easy to follow, though it contains multiple typos and grammatical errors (detailed below). The proofs for Propositions 1 and 2, as well as Theorem 1, appear to be correct upon review.
Typos and grammatical errors:
- Line 34: "…with theirs neighbors…" → "…with their neighbors…"
- Line 174: "double-stochastic" → "doubly-stochastic"
- Line 183: "… can be casted as…" → "… can be cast as…"
- Line 192: "…class of graph with…" → "…class of graphs with…"
- Line 197: "…which seems to…" → "…which seem to…"
- Line 206: "…additional overheads…" → "…additional overhead…"
- Line 225: "…pugging…" → "…plugging…"
- Line 238: "…are seen to…" → "…are able to…"
- Line 240: "…both type of…" → "…both types of…"
- Line 248: "…also seen to…" → "…also shown to…"
- Lines 279–280: "…to convergence to…" → "…to converge to…"
- Line 300: "…,which scales like…" → "…,which scale like…"
- Line 302: "…for the cycle,…" → "…for cycle graphs,…"
Additional minor comments:
- Lines 220 and 221: Did the authors intend to refer to "Lagrangian" and "Lagrange multipliers" rather than "Laplacian" and "Laplace multipliers"?
- The paper introduces three algorithms, but the variables involved are not consistently defined. For instance, in Algorithm 2, the quantities \(R{vw}\) and \(r{vw}\) are not explained, and the initialization of \(\hat{R}^0\) and \(\hat{r}^0\) is missing. Furthermore, since the auxiliary linear process is central to the analysis, the authors should explicitly identify which variables correspond to this process in Algorithm 3.
The paper also lacks several relevant references. Specifically:
- Lines 41 and 43: References to (sub)gradient methods for consensus optimization are missing. The following works could be cited:
  -- Bertsekas and Tsitsiklis, Parallel and distributed computation: numerical methods, 1989
  -- Sundhar Ram Srinivasan et al., Incremental stochastic subgradient algorithms for convex optimization, 2009
  -- Wei Shi et al., Extra: An exact first-order algorithm for decentralized consensus optimization, 2015
  (and potentially others)
- Line 170: "The original literature…" lacks citations.
- Line 229: Missing reference to Polyak's work on the heavy-ball method.
- Line 232: Missing reference to Nesterov's work.
The following questions and suggestions could further strengthen the paper:
- Although the paper is theoretical, the authors should discuss the practical applicability of their method and clarify scenarios where it would be preferred over other distributed consensus optimization methods.
- What are the limitations of the Min-Sum Splitting method?
- Can the authors provide intuition for the role of the auxiliary process in the Min-Sum Splitting method?
- The results focus on consensus problems with quadratic objective functions. Could this framework be extended to address more general consensus problems, such as those encountered in Machine Learning?
- The authors should explicitly explain why this approach is relevant and valuable for the Machine Learning community.
In conclusion, this paper is a theoretical contribution that establishes convergence rates using a novel proof technique and highlights connections between the proposed method and established approaches in the literature. While the ideas presented are compelling, the paper would benefit from additional discussion on the method's practicality, intuition behind the results, and its significance for the Machine Learning community.