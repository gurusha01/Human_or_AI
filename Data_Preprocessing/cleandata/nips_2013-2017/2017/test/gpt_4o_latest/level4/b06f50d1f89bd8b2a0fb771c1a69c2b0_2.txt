This paper presents a multitask bandit learning framework, building on two key prior works: Valko et al. (2013) on kernelized contextual bandits and Evgeniou and Pontil (2004) on regularized multitask learning. A notable contribution of this paper is its method for estimating task similarities when such information is unavailable, a crucial aspect for handling real-world datasets.
Strengths of the paper:
- The problem of multitask contextual bandit learning is highly relevant to numerous practical applications (e.g., recommendation systems).
- The mathematical analysis appears to be sound and accurate based on my verification.
- The numerical simulation results are compelling and support the proposed approach.
Weaknesses of the paper:
- The contribution feels incremental, and the novelty could be better emphasized. For example, much of the work seems closely related to Valko et al. (2013). If multitask kernels were incorporated into that work, would the resulting algorithm differ significantly from the one proposed here?
- The computational complexity arising from the expanding kernel matrix \( K_{t-1} \) is not addressed, and this aspect warrants discussion.
- The proof of regret is unsatisfying as it references two algorithms, SupKMTL-UCB and BaseKMTL-UCB, which are only detailed in the supplementary material. The authors should include at least the key components of these algorithms in the main text to make Theorem 4.1 comprehensible.
In summary, while the paper tackles an important problem, there are notable concerns regarding 1) the incremental nature of the contribution, 2) algorithmic aspects (e.g., the growing kernel matrix), and 3) the clarity of Theorem 4.1's presentation.