The paper introduces a human image generator conditioned on both appearance and human pose. The proposed method leverages an adversarial training framework, employing a two-step generative network that produces high-resolution images, which are then evaluated by a discriminator. In the generator module, the first network generates a coarse image using a U-shaped architecture based on the input appearance and pose map. The second network refines this coarse image by predicting a residual map, using the original appearance as additional input. The approach is evaluated on the DeepFashion dataset.
The paper presents several noteworthy contributions:
* Task novelty: Introducing a framework that conditions human image generation on appearance and pose maps.
* Techniques: A stacked architecture that predicts a residual (difference) map instead of directly upsampling, along with a carefully designed loss function.
However, the paper could be improved in the following areas to make a stronger impact:
* Image quality: The generated results still require significant improvement.
* Significance: The work risks being perceived as another variation of GAN-based architectures.
* Generalizability: While the application is relevant to vision/graphics, the approach may not easily extend to other learning domains.
The paper is well-structured and effectively communicates the key aspects of the proposed architecture. By conditioning on appearance and pose information, the generator employs a coarse-to-fine strategy with two stacked networks. The method is well-suited to the problem of human image generation in the context of virtual dressing. The use of a difference map for refining images is a small but notable contribution toward improving high-resolution image generation.
The primary limitation of the paper lies in the quality of the generated images, which exhibit visible artifacts that hinder practical application. For instance, patterns in ID346 of Fig. 4 result in black dots in the final output. Although the second generator reduces the blurriness of the first generator's output, the model still struggles to recover high-frequency details in the target appearance.
Another, albeit less critical, concern is that the proposed method might be viewed as a straightforward application of conditional GANs. Conditioning and stacking generators for adversarial training have been explored in prior works, such as the following arXiv papers:
* Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, Dimitris Metaxas, "StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks," arXiv:1612.03242.
* Xun Huang, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie, "Stacked Generative Adversarial Networks," arXiv:1612.04357.
While the paper addresses application-specific challenges, its appeal may be limited to a narrower audience.
Overall, the paper provides a viable solution to the pose-conditioned image generation problem and includes a proper evaluation. The proposed approach demonstrates sufficient technical novelty. Despite the need for further improvements in image quality, the model generates visually consistent results. My initial rating is accept.