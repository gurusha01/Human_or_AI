This paper introduces an algorithm for learning a sequence classifier without relying on labeled data, leveraging sequential output statistics (language model). The proposed formulation is challenging to optimize in its functional form, and a stochastic primal-dual gradient method is designed to solve this problem efficiently. Compared to prior work, this approach is less prone to converging to trivial solutions and does not require a generative model. Experimental results on two real-world datasets demonstrate that the proposed method achieves a lower error rate compared to baseline methods.
Below are a few comments:
1. In Figure 2(a), what do the two axes represent (lambdad and lambdap)?
2. The core idea of the paper and its stochastic primal-dual gradient formulation appear convincing. However, it would strengthen the work to include comparisons with additional baselines, such as those in [11] and [30]. Furthermore, incorporating more datasets would enhance the robustness of the evaluation.
3. It would be helpful to explicitly provide the gradient formulas (step 5 of Algorithm 1).