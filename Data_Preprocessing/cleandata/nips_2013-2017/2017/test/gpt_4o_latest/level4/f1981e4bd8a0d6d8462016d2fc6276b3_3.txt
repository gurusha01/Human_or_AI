This paper introduces a method for predicting a sequence of labels without requiring a labeled training set. The approach leverages a prior probability distribution over possible output sequences of labels, aiming to find a linear classifier such that the predicted sequence distribution closely aligns (in the KL divergence sense) with the given prior. The authors propose a cost function, an optimization algorithm, and demonstrate the method's effectiveness through experiments on real-world data for two tasks: OCR and spell correction.
The paper is technically robust and generally well-written. The proposed idea is novel (with relevant prior work cited) and intriguing. My primary concern, however, lies in the challenge of obtaining a reliable prior model \( p_{LM} \). Such models often face data sparsity issues, particularly when the vocabulary is large, leading to scenarios where some test samples have zero probability under the prior/LM derived from a training set. While this issue is well-documented in the literature and existing solutions are acknowledged, I would like the authors to explicitly address its impact on their method. A related concern is the scalability of the algorithm to larger vocabularies and/or longer sub-sequences (parameter \( N \) in the paper). The experiments presented use a relatively small vocabulary (29 characters) and short sub-sequences (N-grams with \( N=2,3 \)). However, in NLP tasks at the word level, vocabularies are orders of magnitude larger, raising questions about the method's scalability.
Additional comments:
- I would encourage the authors to discuss the applicability of their method to non-NLP domains.
- The inclusion of the supervised solution in the 2D cost function figures raises a question: the red dot (representing the supervised solution) consistently appears to be in a local minimum of the unsupervised cost. I am unclear why this should necessarily be the case.
- Given the assumption of structure in the output space of sequences, why not incorporate this structure into the classifier? Specifically, why rely on a point prediction \( p(yt|xt) \) for a single label independently of its neighbors?
Specific comments:
- (*) Lines 117-118: The term "negative cross entropy" seems incorrect. The formula provided appears to represent the cross entropy itself, not its negative.
- () Line 119: This sentence is somewhat unclear. If I understand correctly, \( \bar{p} \) represents the expected frequency of a specific sequence \( \{i1, \ldots, in\} \), rather than the frequency of all* sequences.
- (*) Lines 163-166: Here as well, the sign of the cross entropy appears to be reversed. For instance, when \( p_{LM} \) approaches 0, \(-\log(0)\) should yield \( +\infty \), not \( -\infty \) as stated in the paper. This aligns with the sentence in line 167, where one would expect a penalizing term in a minimization problem to approach \( +\infty \), not \( -\infty \).
Typos:
- (*) Lines 173-174: The phrase "that \( p{LM} \)" seems incorrect. Did the authors intend to write "for which \( p{LM} \)"?
- (*) Line 283: The word "rate" is repeated.