This paper introduces a non-parametric approach for caching previously encountered contexts in language modelling. The core concept involves retrieving the k nearest-neighbour states from prior contexts at each step and applying a kernel density estimation technique to produce a probability distribution over an open vocabulary. Unlike approaches such as pointer networks or continuous caches, this caching mechanism is unbounded. Experimental results highlight the effectiveness of the method in handling language modelling tasks with temporal and topical drift, outperforming standard RNN-based language models.
The paper is well-written, and the concept of an unbounded cache is both innovative and intuitively compelling. I believe the proposed approach has the potential to be applied to a variety of tasks beyond language modelling.
However, I would have appreciated a direct comparison with parametric or local caching methods, such as the pointer-generator network. Additionally, I am curious about the computational efficiency of the proposed model during inference. For instance, querying 1024 nearest neighbours to compute p_{cache} seems computationally intensive and could pose a bottleneck.