This manuscript introduces a modification to the feed-forward neural network optimization problem when the activation function is the ReLU function.
My primary concern lies with the convergence proof for the block coordinate descent algorithm. Specifically, Theorem 1 is flawed for the following reasons:  
A. It assumes that the sequence generated by the algorithm has a limit point.  
However, this assumption may not hold since the set \( U \) is not compact (it is closed but unbounded). Consequently, the sequence may fail to converge to a finite point.  
B. It claims that the sequence generated by the algorithm has a unique limit point (line 263: "the" limit point).  
Even if the algorithm produces limit points, they may not be unique. For instance, consider the sequence \( x_i = (-1)^n - 1/n \), which clearly has two limit points: \( +1 \) and \( -1 \).
For the remaining parts of the paper, my comments are as follows:  
1. The proposed formulation is intriguing and could be discussed independently from the block coordinate descent algorithm. It appears that the primary novelty lies in the new formulation rather than the algorithm itself.  
However, a significant concern with the new formulation is the substantial increase in the number of variables, which could lead to spatial infeasibility for larger datasets. This limitation restricts the formulation's applicability to large-scale problems, thereby reducing its practical utility.  
2. I am not convinced that the problem in line 149 is always convex with respect to \( W \). This point requires further clarification.  
3. The introduction dedicates considerable effort to discussing the issue of saddle points. However, this manuscript does not address that problem. I recommend removing the related discussion.  
4. The formatting of references for papers and equations is unconventional. Typically, [1] is used for paper references, and (1) is used for equation references.  
5. The comparison with Caffe solvers in terms of runtime is not meaningful, as the implementation platforms differ significantly. The statement "using MATLAB still run significantly faster" is unclear, particularly the use of the word "still," since MATLAB is generally expected to be faster than Python.  
6. The comparison of objective values in Figure 3(a) is problematic, as it compares two methods solving fundamentally different problems, akin to comparing apples and oranges.
=== After feedback ===  
I noticed that in Line 100, the authors mention that \( U, V, W \) are compact sets. However, as indicated in Line 124, \( U \) is defined as the nonnegative half-space, which is unbounded and therefore non-compact.  
Thus, while the convergence analysis may hold true when \( U \) is compact (as this ensures the existence of at least one limit point), this is not the case for the problems (4) or (5) as currently defined.
Regarding the proof, I disagree with the notion of "the" limit point when \( t \to \infty \). By its nature, such a unique limit point does not necessarily exist.  
As I stated in my original review, there may be multiple limit points, and by definition, all limit points are associated with \( t \to \infty \), albeit in different ways.  
The authors argue in their rebuttal that their algorithm converges, and therefore "the" limit point exists. However, this reasoning relies on the assumption being true to prove the assumption itself, which is not a valid approach to proof.
Additionally, consider an extreme case where the sets \( U, V, W \) each consist of a single point, say \( u, v, w \), and the gradient at \( (u, v, w) \) is nonzero. In this scenario, there is exactly one limit point, \( (u, v, w) \), but it is not a stationary point since the gradient is nonzero.
For these reasons, I will maintain the same score.