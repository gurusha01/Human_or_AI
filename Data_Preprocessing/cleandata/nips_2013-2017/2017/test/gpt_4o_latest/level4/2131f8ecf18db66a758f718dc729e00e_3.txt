This paper presents an interesting study, and I found it enjoyable to read. The authors tackle the problem of clustering data points through crowdsourcing, focusing on recovering the original labels of data points based on responses to similarity cluster queries. They derive bounds on the number of queries required for label recovery, accounting for both noisy and noiseless query responses. The primary contribution of the paper is theoretical, supported by empirical simulations. The paper is well-written, making the theoretical results accessible to a broad machine learning audience. In particular, the connection to Information Theory, used to derive the bounds, is clearly and effectively explained.
I have the following concerns that could help improve the paper:
(1) While the concept of AND queries is theoretically intriguing, it appears to conflict with the original motivation for crowdsourcing. Specifically, in an AND query, the cluster label seems to be implicitly provided as part of the response, rather than simply indicating whether the data points belong to the same cluster. If my interpretation is correct, this raises the question: why not directly ask for the label of a data point instead of relying on similarity queries? This is my primary concern regarding the contributions of the paper.
(2) Can the expressions for the bounds in Theorems 1, 2, 3, and 4 be simplified without losing their core essence? Simplified expressions could enhance the clarity and accessibility of the results.
(3) The experiments appear to focus exclusively on AND queries. If this is the case, the validity of the experimental study hinges on the clarification of point (1) above.
I am satisfied with the authors' responses to my concerns and questions. Therefore, I recommend the acceptance of this paper.