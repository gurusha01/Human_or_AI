The paper presents an intriguing approach. The authors propose training neural networks using a cost function that explicitly encourages the development of networks more amenable to compression via truncated SVD. This is achieved by formulating the regularization as a penalty on the nuclear norm of the weight matrices, which they implement through the soft thresholding of singular values as the proximal operator applied after each epoch. I found the concept compelling, and the experimental section provided a clear breakdown of their results and the behavior of the proposed method. However, I would have appreciated additional comparative analyses, such as evaluating their network's performance against other compression techniques targeting the same parameter budget on the given datasets. Overall, it is a solid paper with an innovative idea and well-executed methodology, though the experimental evaluation could be more comprehensive.