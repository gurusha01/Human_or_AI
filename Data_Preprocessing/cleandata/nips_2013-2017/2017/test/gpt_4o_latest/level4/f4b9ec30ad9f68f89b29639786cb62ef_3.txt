Review - Introduction:
The paper presents a novel algorithm, termed Krum, which aggregates partially computed gradients in a Byzantine fault-tolerant manner. This algorithm is designed to facilitate distributed training of machine learning models on extremely large datasets.
I am a machine learning practitioner but not an expert in distributed computing. Therefore, my review reflects a perspective rooted in the practical needs of machine learning.
---
Strengths:
- The research question is compelling. As datasets grow exponentially in size, distributed techniques are becoming increasingly critical in machine learning.
- The paper is well-written, employing sophisticated academic language and a clear, logical structure.
- The proposed algorithm is underpinned by rigorous theoretical analysis.
---
Weaknesses:
- The results presented in Figure 4 are unconvincing. In the absence of Byzantine failures, the proposed algorithm performs significantly worse than simple averaging. Its advantages only become evident when one-third of the workers are Byzantine, which represents an unusually high failure rate. In realistic scenarios with lower failure rates, simple averaging remains highly competitive, raising questions about the practical motivation for this approach.
- Although the paper addresses challenges associated with very large datasets, the experiments are conducted on relatively small datasets. MNIST contains 60,000 instances, and Spambase has only 4,601. The divergence between the true gradient and stochastic gradient typically becomes significant only with much larger datasets. With current hardware, a modest workstation equipped with a reasonably priced GPU can handle millions of data points without requiring protocols like Krum. Additionally, the necessity of training on datasets exceeding a few million instances is highly application-dependent. In many cases, the performance difference between 10 million and 100 million data points is negligible. For machine learning practitioners to adopt a slower algorithm for the sake of distributed safety, the authors must identify specific applications where such a need exists and provide experimental results for those scenarios.
- The algorithm focuses on updating global parameters using stochastic gradients computed on minibatches. While this is foundational to many machine learning models, the primary challenge in contemporary distributed techniques lies elsewhere. Deep neural networks often require distributed computing to parallelize operations across "neurons" rather than minibatches. The inability of the proposed algorithm to generalize to this use case significantly limits its broader impact.
Minor Point: The paper would benefit from citing earlier foundational work on distributed machine learning. For example:  
C.T. Chu et al., Map-Reduce for Machine Learning on Multicore, NIPS, 2007.
---
Preliminary Evaluation:
Although the theoretical contributions of this paper are commendable and distributed machine learning is undoubtedly a critical area, the experimental results cast doubt on the practical utility of the proposed approach.
---
Final Evaluation:
I recognize that a 33% Byzantine failure rate is a standard benchmark for distributed computing tasks. However, in the context of training machine learning models, the dynamics differ significantly. Here, the primary concern is achieving high accuracy rather than merely processing large datasets. According to Figure 4, Krum significantly compromises model accuracy in the absence of attacks. This implies that a machine learning practitioner would only consider using Krum under the following highly specific conditions: (i) a dataset of 10–20 million instances is insufficient to achieve satisfactory accuracy, necessitating distributed computing, and (ii) at least 33% of the nodes are expected to behave in a Byzantine manner, necessitating Krum. As a machine learning practitioner, I struggle to identify a realistic scenario where both conditions are met. It is the authors' responsibility—not the readers'—to highlight such use cases, but this is absent from both the paper and the rebuttal. Consequently, I maintain my initial negative recommendation.