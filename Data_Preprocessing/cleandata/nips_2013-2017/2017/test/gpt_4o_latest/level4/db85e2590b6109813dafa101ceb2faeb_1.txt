The authors propose a regularization term designed to encourage weight matrices to exhibit low rank during network training. This approach enhances network compression and facilitates explicit rank reduction during post-processing, thereby decreasing the computational cost of inference. Overall, the paper is well-written and addresses an interesting topic. However, I have several concerns: first, the authors do not discuss variational inference, which explicitly compresses networks by reducing their description length in bits and enables weight pruning after trainingâ€”see 'Practical Variational Inference for Neural Networks' for reference. More broadly, most regularizers inherently promote a form of implicit 'compression-aware training' (as simpler models tend to generalize better), and they can often be leveraged for post-hoc pruning. For instance, networks trained with l1 or l2 regularization typically result in many near-zero weights, which can be pruned with minimal impact on performance. Clarifying this distinction is important, particularly since the authors incorporate an l2 term alongside their proposed regularizer during training. Additionally, the paper does not seem to compare the effectiveness of prior low-rank post-processing techniques with and without the proposed regularizer, nor does it benchmark against other regularizers from existing literature. Addressing these gaps by including more baseline results in the experimental section would strengthen the paper, particularly by demonstrating that the proposed regularizer achieves a superior accuracy/compression tradeoff compared to alternative methods.
Furthermore, I found the presentation of results somewhat difficult to interpret, which may have caused me to overlook key insights. A more informative visualization would be a set of curves plotting accuracy against compression ratio (e.g., in terms of parameter count or number of MACs), rather than accuracy against the regularization strength. Such a graph would enable direct comparisons between the proposed approach and prior regularizers or compression techniques.