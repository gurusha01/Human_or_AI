This paper introduces a formal condition for Byzantine fault-tolerant stochastic gradient aggregation and proposes an algorithm (Krum) that satisfies this condition. Based on the provided definition, it is evident that aggregation methods relying on linear combinations (such as averaging) are not Byzantine fault-tolerant. Additionally, the paper provides evidence that Krum is not only theoretically robust but also performs reasonably well in practice.
I found the paper to be clear, well-structured, self-contained, and fairly comprehensive. The motivation is straightforward: while distributed learning research typically assumes statistical regularity and cooperative participants, this work addresses the question of what can be achieved under highly adversarial conditions. To the best of my knowledge, this is the first paper to explore this perspective on fault tolerance, which has been studied in other domains. I did not independently verify the analysis.
A suggestion regarding exposition: The draft includes an unusually detailed background on SGD, illustrated with a cartoon figure (Figure 1) to explain unbiased stochastic gradients. While this is a nice touch, it may not be necessary for an audience familiar with stochastic optimization, such as those at NIPS. That said, it is ultimately the authors' decision whether to retain this or reallocate the space to more technical content.
My remaining comments pertain to the experiments:
1. The experiments are not well-aligned with the paper's primary setting and could be significantly improved by incorporating more modern datasets and models. The current experiments use small-scale ("single-machine") datasets, where distributed learning is not particularly relevant. As such, the experiments in the draft function more as simulations or benchmarks. While they do reveal interesting phenomena, it is unclear whether these results generalize to scenarios where the algorithm would be more applicable. For example, since the paper uses an MLP for MNIST, a logical next step would be to evaluate a deep convolutional network on a data-augmented CIFAR-10 dataset. This would involve a training process long enough to benefit from a distributed setup.
2. Why are the correct workers assigned a mini-batch size of 3? Without further explanation, this choice seems arbitrary.
3. In the "cost of resilience" experiment (Figure 5), reporting the error at round 500 appears to capture an early stage of training. Indeed, the accuracies shown are not particularly high for the task at hand. With a batch size of 100 (on the larger end), 500 rounds correspond to 50,000 examples, which do not constitute a full pass through the training set. The experiment would be more complete if it included a second plot (or at least a remark) describing the behavior closer to convergence.
4. In the "multi-Krum" experiment (Figure 6), setting the parameter m to n - f (the number of non-Byzantine machines) seems like an overly omniscient choiceâ€”how would one know the number of adversaries f in practice? The experiment would be more comprehensive if it included results for other values of m (for the same f). Since the text suggests that m provides a trade-off, such a plot would effectively illustrate the trade-off curve.