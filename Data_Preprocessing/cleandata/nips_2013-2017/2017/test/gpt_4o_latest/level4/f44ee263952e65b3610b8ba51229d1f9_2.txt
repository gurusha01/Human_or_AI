This paper introduces a modification to recurrent networks (such as Elman networks, LSTMs, or gated recurrent units) for language modeling, enabling them to adapt to shifts in data distribution. This is achieved by incorporating an unbounded cache designed to handle unseen or rare words during training. While similar approaches exist in the literature (e.g., [21], [50], [41]), this work distinguishes itself by storing all previously encountered words rather than just the most recent ones. It does so by estimating the probability distribution of observed words using a kernel density estimator, combined with an approximate kNN method to ensure scalability. From a theoretical perspective, the paper is compelling and tackles a challenge prevalent in many NLP applications. In the results section, the authors demonstrate that the inclusion of the cache is crucial for achieving strong performance. During the review process, they provided substantial additional empirical validation, which further supports the claim that their method advances the state of the art.
Opinion on the paper:  
+ The paper is well-written (though the authors should consider using an English language editor).  
+ The topic is highly relevant and significant for NLP practitioners.  
- The paper lacks sufficient numerical evaluation (despite the authors presenting extensive numerical results during the rebuttal process, these cannot be included in the final version and are therefore excluded from the evaluation).