The paper introduces an accelerated Min-Sum message-passing protocol, termed Min-Sum Splitting, for solving consensus problems in distributed optimization. The authors address the limitations of the ordinary Min-Sum algorithm, which fails to converge in graphs with cycles, by employing reparametrization techniques. They establish convergence rates for the Min-Sum Splitting algorithm, particularly for quadratic objectives, and demonstrate its accelerated performance compared to classical diffusive methods. The analysis leverages an auxiliary linear process to track the algorithm's evolution, providing a novel proof technique. The paper also draws connections between Min-Sum Splitting and lifted Markov chains as well as multi-step convex optimization methods, highlighting its theoretical significance.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with proofs of key propositions and theorems appearing correct. The introduction of an auxiliary linear process to analyze the algorithm's evolution is innovative and provides a strong foundation for the convergence analysis.
2. Novelty: The work establishes the first formal connection between Min-Sum schemes, lifted Markov chains, and multi-step methods in convex optimization. This is a significant contribution to the literature on distributed optimization and consensus algorithms.
3. Improved Convergence Rates: The authors demonstrate that Min-Sum Splitting achieves subdiffusive convergence rates, offering a square-root improvement over classical methods. This result is particularly relevant for graphs with geometric structures, such as cycles and tori.
4. Clarity of Theoretical Contributions: The paper is well-organized, with clear delineation of the main contributions, including the novel proof technique and the design of a Min-Sum protocol that outperforms prior methods.
Weaknesses:
1. Clarity Issues: While the paper is generally well-written, several algorithm descriptions are unclear, and some variables are not properly initialized. This could hinder reproducibility for readers unfamiliar with the topic.
2. Typos and Grammatical Errors: The manuscript contains multiple typographical and grammatical errors, which detract from its overall readability.
3. Insufficient References: The paper does not adequately reference prior work, particularly recent advances in distributed optimization and machine learning. This limits its contextualization within the broader research landscape.
4. Practicality and Applicability: The authors do not sufficiently address the practicality, limitations, and broader applicability of their method to general consensus problems or machine learning tasks. The relevance of the work to the NeurIPS community, which emphasizes machine learning applications, is not well-justified.
Arguments for Acceptance:
- The paper makes a strong theoretical contribution by improving convergence rates and connecting Min-Sum Splitting to established techniques like lifted Markov chains and multi-step methods.
- The novel proof technique and auxiliary process analysis are valuable additions to the field of distributed optimization.
Arguments Against Acceptance:
- The lack of clear practical relevance to machine learning tasks and insufficient discussion of broader applicability may limit the paper's impact on the NeurIPS audience.
- Clarity issues, typos, and missing references reduce the paper's accessibility and polish.
Recommendation: Weak Accept. While the paper is theoretically strong and makes novel contributions, addressing the clarity, practical relevance, and broader applicability would significantly enhance its impact.