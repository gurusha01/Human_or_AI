This paper provides a comprehensive theoretical analysis of the limitations of variance-reduced stochastic algorithms (e.g., SAG, SDCA, SAGA, SVRG) for finite-sum optimization problems. The authors identify several critical insights that challenge existing assumptions about these methods. First, they demonstrate that achieving linear convergence for strongly convex problems requires explicit knowledge of which individual function is returned by the oracle, a result that has significant implications for the use of stochastic data augmentation. Specifically, they show that variance-reduced methods are incompatible with stochastic data augmentation unless the augmented samples form a finite set. This finding is particularly relevant for modern machine learning applications where data augmentation is widely used.
The paper also establishes that "oblivious" algorithms—those with fixed update rules independent of problem parameters—cannot achieve Nesterov-style acceleration (reducing the dependence on the condition number from \( \kappa \) to \( \sqrt{\kappa} \)) without explicit knowledge of the strong convexity parameter. Furthermore, the authors analyze the use of restarts to address the non-adaptive behavior of oblivious algorithms under unknown local conditioning, showing that stationary algorithms are suboptimal for smooth and convex problems.
Strengths:  
1. Theoretical Rigor: The paper is technically sound, with well-supported claims backed by rigorous proofs. The use of lower-bound arguments and reductions to information-theoretic problems is particularly compelling.  
2. Clarity: The manuscript is well-written and pedagogically clear, making complex theoretical results accessible to readers. The structured presentation of contributions and detailed proofs enhance its readability.  
3. Originality: The work provides novel insights into the limitations of variance-reduction and acceleration schemes, particularly in the context of stochastic oracles and oblivious algorithms. These findings extend and refine existing results in the literature.  
4. Significance: The results have practical implications for algorithm design, especially in large-scale machine learning where finite-sum problems and data augmentation are prevalent. The paper advances our understanding of the theoretical boundaries of optimization algorithms.
Weaknesses:  
1. Practical Relevance: While the theoretical contributions are significant, the paper does not provide empirical validation or practical recommendations for addressing the identified limitations.  
2. Minor Issues: There are minor typographical errors (e.g., a typo in L224) and premature use of CLI terminology before its formal introduction, which could confuse readers.  
Arguments for Acceptance:  
- The paper addresses an important theoretical gap and provides novel insights with rigorous proofs.  
- The results are significant for both theoretical research and practical algorithm design.  
- The manuscript is well-organized and clear, making it accessible to a broad audience.
Arguments Against Acceptance:  
- The lack of empirical validation limits the immediate practical impact of the findings.  
- The paper focuses primarily on theoretical limitations without proposing concrete solutions to overcome them.
Recommendation: Accept with minor revisions. The theoretical contributions are strong and relevant, but addressing the minor issues and providing some discussion on potential practical implications or future directions would enhance the paper's impact.