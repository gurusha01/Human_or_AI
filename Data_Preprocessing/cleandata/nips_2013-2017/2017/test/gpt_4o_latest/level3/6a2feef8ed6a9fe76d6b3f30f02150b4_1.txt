The paper introduces a Block Coordinate Descent (BCD) algorithm with Tikhonov regularization for training dense and sparse deep neural networks (DNNs) with ReLU activation. The authors claim that their method addresses key challenges in DNN optimization, such as vanishing gradients, by decomposing the training objective into three convex sub-problems. They provide theoretical guarantees of global convergence to a stationary point with an R-linear convergence rate. Empirical results on MNIST demonstrate that the proposed BCD algorithm outperforms traditional stochastic gradient descent (SGD) variants in terms of test-set error rates and computational efficiency.
Strengths:
1. Theoretical Contributions: The paper provides a rigorous theoretical analysis of the BCD algorithm, including proofs of global convergence and R-linear convergence rates. This is a significant contribution, as convergence guarantees for non-convex optimization in DNNs are rare.
2. Novel Formulation: The use of Tikhonov regularization to reformulate the ReLU activation as a smooth optimization problem is innovative. The decomposition into three sub-problems is well-motivated from a convex optimization perspective.
3. Empirical Validation: The experiments on MNIST show that the BCD algorithm achieves better test-set error rates and faster convergence compared to SGD variants. The inclusion of sparse network training (BCD-S) is a useful extension, particularly for resource-constrained applications.
4. Efficiency: The reported computational speedup over SGD-based solvers is promising and highlights the practical utility of the proposed method.
Weaknesses:
1. Unclear Motivation: The rationale for choosing Tikhonov regularization over other regularization techniques is not sufficiently explained. While the authors mention its connection to ReLU, they fail to provide a compelling argument for its advantages over alternatives like dropout or weight decay.
2. Technical Clarity: The mathematical exposition is difficult to follow, with several undefined terms (e.g., \(\mathcal{P}\), \(Q(\tilde{\mathcal{A}})\)) and missing explanations for key concepts like the Tikhonov regularized inverse problem. This lack of clarity hinders reproducibility.
3. Limited Applicability: The experiments are restricted to a shallow DNN architecture on MNIST, which does not reflect the complexity of modern deep learning tasks. The results are far from state-of-the-art, raising concerns about the method's scalability and relevance to real-world applications.
4. Vanishing Gradient Claim: The claim that the inverse sub-problem resolves the vanishing gradient issue lacks theoretical or empirical justification. This is a critical gap, as addressing vanishing gradients is a major motivation for the proposed approach.
5. Sparse Network Results: While the paper highlights the ability to learn sparse networks, the observation that the 4th layer remains dense (Fig. 5) is unexplained. This undermines the claim of effective sparsity.
Suggestions for Improvement:
1. Provide a clearer and more detailed explanation of Tikhonov regularization, including its advantages over other regularization methods.
2. Define all mathematical terms and symbols explicitly, and include a glossary for reference.
3. Justify the decomposition into three sub-problems, particularly with respect to its handling of vanishing gradients.
4. Extend experiments to deeper networks and more challenging datasets to demonstrate scalability and relevance.
5. Address the issue of dense layers in sparse networks and provide insights into the observed behavior.
Recommendation:
While the paper makes valuable theoretical contributions and proposes an innovative approach, its unclear motivation, technical ambiguities, and limited empirical validation weaken its impact. I recommend major revisions to address these issues before the paper can be considered for acceptance.