The paper introduces Tensorized LSTMs (tLSTMs), a novel approach to enhancing the capacity of LSTMs by tensorizing hidden states and employing cross-layer memory cell convolution. This method allows for efficient widening and deepening of the network without significantly increasing parameters or runtime. The authors validate their approach on five sequence learning tasks, demonstrating its potential over baseline and state-of-the-art methods.
Strengths:
1. Proposed Method: The introduction of tensorized hidden states and memory cell convolution is innovative. By leveraging tensors, the model achieves efficient parameter sharing and computational scalability. The merging of deep computations into temporal computations is particularly compelling, offering a practical solution to runtime challenges in deep LSTMs.
2. Clarity: The paper is well-structured and clearly written. The mathematical formulation of the tLSTM is detailed and accessible, and the experimental results are presented systematically.
3. Experimental Validation: The authors conduct experiments across diverse tasks (e.g., language modeling, algorithmic tasks, MNIST classification), showcasing the model's versatility. The visualization of memory cell behavior provides valuable insights into the model's internal workings.
4. Significance: The proposed method addresses key challenges in scaling LSTMs, such as parameter efficiency and runtime, making it a meaningful contribution to the field of sequence modeling.
Weaknesses:
1. Widening vs. Deepening: The claim that increasing the tensor size (P) widens the network is conceptually misleading. As the paper acknowledges that P is akin to the number of stacked hidden layers, this operation effectively deepens the network rather than widening it. This discrepancy should be clarified.
2. Network Depth Parameterization: The depth (L) is not an independent parameter but is derived from P and K. This contradicts the claim that both width and depth can be freely adjusted. The authors should explicitly address this limitation.
3. Gradient Vanishing/Exploding: The assertion that memory cell convolution prevents gradient issues is unsubstantiated. The paper lacks both theoretical justification and empirical evidence to support this claim.
4. Experimental Design: While the experiments are comprehensive, the grouping of configurations (e.g., normalization, memory cell convolution, feedback connections) could be more systematic. Additionally, the limited exploration of parameter combinations (e.g., varying K) restricts the generalizability of the findings.
Recommendation:
The paper presents a novel and promising approach to improving LSTMs, with strong empirical results and clear exposition. However, the conceptual confusion around widening vs. deepening, the lack of theoretical validation for gradient claims, and the experimental limitations weaken its overall impact. Addressing these issues would significantly strengthen the paper.
Arguments for Acceptance:
- Innovative method with practical implications for scaling LSTMs.
- Strong empirical performance across diverse tasks.
- Clear and well-organized presentation.
Arguments Against Acceptance:
- Conceptual ambiguity regarding network widening and deepening.
- Lack of theoretical and empirical support for gradient-related claims.
- Limited exploration of experimental configurations.
Overall, this paper makes a meaningful contribution to the field, but revisions are needed to address the identified weaknesses.