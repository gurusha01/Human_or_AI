The paper introduces Krum, a novel Byzantine failure-tolerant algorithm for distributed stochastic gradient descent (SGD), addressing the critical problem of robustness in distributed machine learning systems. The authors highlight the vulnerability of existing gradient aggregation techniques, such as averaging, to Byzantine failures and propose Krum as a theoretically grounded solution. The algorithm selects gradients based on a non-linear distance-based criterion, ensuring convergence even in adversarial settings. While the theoretical contributions are significant, the practical utility of Krum is limited by several shortcomings.
Strengths:
1. Relevance: The paper addresses a timely and important problem in distributed machine learning, where Byzantine failures can arise due to software bugs, adversarial attacks, or data biases. This is particularly relevant given the increasing scale of distributed systems.
2. Theoretical Rigor: The authors provide a detailed theoretical analysis, proving that Krum satisfies a defined (α, f)-Byzantine resilience property and guarantees convergence under certain conditions. This is a notable contribution to the field of robust distributed optimization.
3. Clarity and Presentation: The paper is well-organized and clearly written, with a logical flow from problem formulation to theoretical analysis and experimental validation. The inclusion of supplementary material for proofs is commendable.
Weaknesses:
1. Performance in Non-Byzantine Scenarios: A major limitation of Krum is its poor performance compared to simple averaging when there are no Byzantine failures. The algorithm introduces significant computational overhead and slows down convergence, making it impractical for typical distributed training scenarios.
2. Experimental Validation: The experiments are conducted on small datasets (e.g., Spambase and MNIST), which fail to demonstrate the algorithm's scalability and relevance for large-scale machine learning tasks. This undermines the claim that Krum is suitable for industry-grade distributed systems.
3. Limited Applicability: Krum is designed for minibatch-based updates and does not generalize to neuron-level distributed operations, which are critical for training deep neural networks. This restricts its applicability to a narrow subset of distributed learning problems.
4. Lack of Motivation and Practical Examples: The paper does not provide compelling real-world scenarios or applications where Krum's resilience to Byzantine failures would be essential. This raises questions about its practical utility.
5. Missing Citations: Foundational works, such as Chu et al.'s "Map-Reduce for Machine Learning on Multicore" (NIPS 2007), are not cited, which limits the contextualization of the proposed approach within the broader literature.
Evaluation:
While the paper makes a valuable theoretical contribution by introducing Krum and proving its resilience to Byzantine failures, its practical impact is limited. The algorithm's utility is confined to rare scenarios with extremely high failure rates (≥33%), which are not convincingly justified by the authors. The lack of scalability experiments and generalizability to deep learning further weakens its case. 
Recommendation:
Given the limited practical applicability and the lack of compelling experimental evidence, I recommend rejecting the paper. However, the theoretical insights are valuable, and I encourage the authors to address the identified weaknesses in future work, particularly by demonstrating scalability on large datasets and exploring extensions to deep learning frameworks.