This paper introduces a novel approach to training deep neural networks (DNNs) by leveraging a Tikhonov regularized multi-convex formulation and a block coordinate descent (BCD) algorithm. The authors reinterpret the ReLU activation function as a convex projection problem, enabling a clean and theoretically sound derivation of the regularized objective. The decomposition of the training process into three sub-problems—inverse problem (activations), least-squares regression (weights), and classification (softmax weights)—is insightful and provides a fresh perspective on DNN optimization. Additionally, the proposed framework offers novel contributions to deep learning, including insights into ReLU generalization, shortcut connectivity, and sparsity in network design.
The paper is technically sound in its theoretical contributions. The multi-convex formulation and the convergence guarantees of the BCD algorithm are rigorously derived, with clear connections to proximal point methods. The R-linear convergence rate of the proposed algorithm is a significant theoretical result, especially in the context of non-convex optimization for DNNs, where such guarantees are rare. However, the experimental section is a notable weakness. The evaluation is limited to the MNIST dataset, which is relatively simple and insufficient to demonstrate the broader applicability of the proposed method. Furthermore, the visualizations, such as Figure 4, are unclear and fail to provide meaningful insights into the algorithm's performance.
The clarity of the paper is generally good, with well-organized sections and a logical flow of ideas. However, the experimental results could benefit from more detailed explanations and better visual representation. The originality of the work is commendable, as it combines concepts from convex optimization and deep learning in a novel way. The reinterpretation of ReLU and the introduction of the Tikhonov matrix as a unifying framework for architecture and parameterization are particularly innovative.
In terms of significance, the paper has potential but falls short due to its limited empirical validation. While the theoretical contributions are strong, the lack of extensive experiments on more challenging datasets and architectures limits the practical impact of the work. A more comprehensive evaluation, including comparisons with state-of-the-art methods on diverse datasets, would significantly enhance the paper's contribution to the field.
Pros:
1. Rigorous theoretical foundation with convergence guarantees.
2. Novel decomposition of the training process into interpretable sub-problems.
3. Innovative reinterpretation of ReLU and introduction of the Tikhonov matrix.
Cons:
1. Limited experimental validation (only MNIST dataset).
2. Unclear visualizations and insufficient qualitative analysis.
3. Lack of comparison with modern large-scale datasets and architectures.
Recommendation:
The paper presents a strong theoretical contribution but lacks sufficient empirical evidence to support its claims. I recommend major revisions to address the experimental shortcomings, including evaluations on more diverse datasets, improved visualizations, and comparisons with state-of-the-art methods. With these improvements, the paper could make a significant impact on the field.