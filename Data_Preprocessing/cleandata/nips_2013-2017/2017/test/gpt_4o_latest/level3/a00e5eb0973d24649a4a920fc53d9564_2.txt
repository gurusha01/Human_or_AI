This paper makes a significant contribution to the theoretical understanding of lower bounds for finite sum optimization problems, an area of growing importance in machine learning and optimization. The authors provide a rigorous analysis of the conditions under which variance reduction and acceleration techniques can be applied, offering new insights into the limitations of these methods. The results are particularly relevant for researchers developing optimization algorithms for large-scale machine learning problems, where finite sum structures are ubiquitous.
The paper is well-written and polished, with a clear exposition of the main results and their implications. The authors effectively situate their work within the broader literature, referencing key contributions such as SAG, SDCA, and SVRG, and extending prior results in meaningful ways. However, there are several areas where the manuscript could be improved for clarity and precision.
1. Technical Soundness and Quality: While the theoretical results appear robust, clarification is needed on the use of Fano's inequality in Equation (10). The derivation of the lower bound using information-theoretic arguments is compelling, but the connection between the inequality and the optimization setting should be more explicitly explained. Additionally, there seems to be a possible error in the equation between lines 134-135, where the term \( n^2/2 \) is mentioned. The authors should verify this calculation.
2. Clarity: The paper is generally well-organized, but some definitions and notations could be more precise. For instance, the definition of \( t^* \) in Equation (13) is missing, which could confuse readers unfamiliar with the context. Similarly, the inconsistent use of the tilde with big-Omega notation detracts from the presentation's rigor. Definition 2 could also explicitly state that \( \theta \) typically contains the function index, which would improve clarity.
3. Originality and Significance: The results are novel and extend the state of the art in finite sum optimization. The authors demonstrate that knowing the strong convexity parameter is crucial for achieving accelerated rates and establish tight lower bounds for smooth and convex finite sums. However, the relationship between Section 3.2 and Theorem 2 in reference [4] requires further clarification, particularly regarding the additional \( n \) term. The introduction should also more explicitly highlight the exact improvement over reference [4].
4. Broader Impact: The theoretical insights provided by this paper have practical implications for algorithm design, particularly in scenarios involving large-scale data and stochastic oracles. The results are likely to influence future work on adaptive and accelerated optimization methods.
Pros:
- Rigorous theoretical contributions with tight lower bounds.
- Well-written and polished manuscript.
- Extends prior work and provides new insights into variance reduction and acceleration.
Cons:
- Missing definitions and inconsistent notation.
- Potential error in a key equation.
- Some connections to prior work are insufficiently explained.
Recommendation:
I recommend acceptance with minor revisions. Addressing the noted issues will further strengthen the paper and enhance its accessibility to a broader audience.