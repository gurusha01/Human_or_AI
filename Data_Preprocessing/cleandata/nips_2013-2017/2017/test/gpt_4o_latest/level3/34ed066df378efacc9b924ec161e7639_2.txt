This paper introduces the Pose Guided Person Generation Network (PG2), a novel approach to synthesizing person images in arbitrary poses by conditioning on both a reference image and a target pose. The authors propose a two-stage generative framework: the first stage integrates pose and appearance information using a U-Net-like architecture to produce a coarse image, while the second stage refines this result using a conditional GAN to generate sharper and more detailed images. The use of a difference map prediction in the refinement stage, rather than direct upsampling, is a notable technical innovation. The method is evaluated on the DeepFashion and Market-1501 datasets, demonstrating its ability to generate pose-consistent and visually plausible images.
Strengths:
The paper addresses a specific and challenging problem in human image generation, introducing a novel conditioning framework that combines pose and appearance. The two-stage coarse-to-fine strategy is well-motivated and effectively handles the complexity of generating both global structure and fine details. The use of a pose mask loss to suppress background influence is a thoughtful addition that aligns with the task's goals. The qualitative results, particularly on the DeepFashion dataset, are visually compelling, and the quantitative metrics (e.g., SSIM and user studies) support the effectiveness of the approach. The paper is well-organized, with clear explanations of the methodology, experiments, and contributions. It also builds on prior work, such as StackGAN and DCGAN, while addressing application-specific challenges.
Weaknesses:
Despite its strengths, the paper has some limitations. The generated images often exhibit visible artifacts and lack high-frequency details, particularly in more complex or cluttered scenarios like the Market-1501 dataset. This limits the practical applicability of the method in real-world settings. Additionally, while the task is novel, the approach could be perceived as another conditional GAN variant with limited generalizability beyond vision/graphics tasks. The paper could benefit from a more thorough discussion of its broader applicability and potential extensions. Furthermore, the reliance on pose estimation introduces potential errors, which are not deeply analyzed in the paper. Finally, while the authors compare their method to a related work, the comparison is limited, and additional baselines would strengthen the evaluation.
Arguments for Acceptance:
- Novel task formulation and technical contributions, including the two-stage framework and difference map prediction.
- Strong experimental results on a challenging problem, with clear improvements over related work.
- Well-written and organized, making the methodology accessible to readers.
Arguments Against Acceptance:
- Image quality issues, including artifacts and lack of fine details, limit practical utility.
- Perceived incremental nature of the approach as a conditional GAN variant.
- Limited discussion of generalizability and broader impact.
Overall Evaluation:
The paper presents a technically sound and innovative approach to a novel task, with promising results and clear contributions to the field of generative image modeling. While there are quality and generalizability concerns, the strengths outweigh the weaknesses, and the paper is a valuable addition to the conference. I recommend acceptance, with the suggestion that the authors address the identified limitations in future work.