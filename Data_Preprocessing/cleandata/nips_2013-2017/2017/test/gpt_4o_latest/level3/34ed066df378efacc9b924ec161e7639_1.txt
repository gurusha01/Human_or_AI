This paper introduces the Pose Guided Person Generation Network (PG2), a novel framework for synthesizing person images in arbitrary poses by conditioning on a reference image and a target pose. The proposed two-stage coarse-to-fine architecture effectively addresses the challenges of pose transfer by first generating a coarse image capturing the global structure and then refining it to add appearance details through adversarial training. The authors also propose a pose mask loss to focus on human body features while suppressing background interference. Extensive experiments on the DeepFashion and Market-1501 datasets demonstrate the effectiveness of the approach, with qualitative and quantitative results showing its ability to generate photo-realistic and pose-consistent images.
Strengths:
1. Novelty: The paper tackles a new and challenging task of pose-guided person image generation, which has potential applications in movie production, pose estimation, and data augmentation. The two-stage architecture is innovative and well-motivated, dividing the problem into manageable sub-tasks.
2. Writing Quality: The paper is clearly written, well-organized, and easy to follow. The methodology is described in detail, and the experimental setup is thorough.
3. Evaluation: The authors conduct extensive evaluations, including ablation studies on pose embeddings, loss functions, and the two-stage architecture. These analyses provide valuable insights into the design decisions.
4. Results: The qualitative results are visually compelling, and the generated images exhibit convincing details. The use of a pose mask loss is particularly effective in improving the focus on the human body.
5. Comparison to Related Work: The paper positions itself well within the literature, comparing its approach to existing methods like VariGAN. However, the inclusion of recent iterative refinement works would further strengthen the related work section.
Weaknesses:
1. Quantitative Evaluation: The paper acknowledges the lack of suitable metrics for this task, which weakens the quantitative evaluation. While SSIM and Inception Score are used, they may not fully capture the perceptual quality of the generated images.
2. Artifacts in Results: Some generated images exhibit artifacts, especially in challenging cases with complex poses or textures. This limitation is noted but not deeply analyzed.
3. User Study: The preliminary user study on Amazon Mechanical Turk is unconvincing due to its limited scope and lack of detailed analysis. The reported scores do not strongly support the qualitative claims.
4. Clarification Needed: The impact of residual blocks on performance is mentioned but not elaborated upon. Additionally, the reviewer is curious about the performance of the Stage 2 network when trained adversarially with Stage 1 inputs, which is not explored.
Recommendation:
I confidently recommend this paper for acceptance. Its novelty, clear presentation, and promising results make it a valuable contribution to the field of generative modeling. While there are some weaknesses, particularly in quantitative evaluation and user study, these are understandable given the nascent nature of the task. Addressing the suggested clarifications and including iterative refinement works would further strengthen the paper.