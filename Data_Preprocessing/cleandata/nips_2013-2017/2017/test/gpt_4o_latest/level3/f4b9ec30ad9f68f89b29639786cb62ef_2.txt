The paper addresses the critical challenge of Byzantine fault tolerance in distributed implementations of Stochastic Gradient Descent (SGD). It introduces a formal resilience condition for gradient aggregation and proposes a novel algorithm, Krum, that satisfies this condition. The authors demonstrate that traditional linear combination-based aggregation methods, such as averaging, are inherently vulnerable to Byzantine failures. Krum, by contrast, employs a non-linear, distance-based aggregation rule to mitigate the influence of Byzantine workers, ensuring convergence under adversarial conditions. The paper provides both theoretical guarantees and experimental evidence to support its claims.
Strengths:
1. Novelty and Originality: The paper introduces a fresh perspective on fault tolerance in distributed learning, addressing a significant gap in the literature. The formulation of the (Î±, f)-Byzantine resilience property and the design of Krum are innovative contributions.
2. Theoretical Rigor: The authors provide a thorough theoretical analysis, proving Krum's resilience and convergence properties under well-defined conditions. The mathematical rigor enhances the paper's credibility.
3. Clarity and Organization: The paper is well-written, self-contained, and logically structured. The introduction and problem formulation are clear, and the proposed solution is systematically developed.
4. Practical Implications: The work has potential real-world applications, particularly in federated learning and distributed machine learning systems, where robustness to adversarial behavior is critical.
Weaknesses:
1. Experimental Limitations: The experiments are conducted on small-scale datasets (e.g., spambase and MNIST) and relatively simple models (e.g., MLPs). This does not align with the large-scale distributed learning scenarios that the paper aims to address. Testing on modern datasets like CIFAR-10 with deep convolutional networks would better demonstrate Krum's practical utility.
2. Arbitrary Mini-Batch Size: The choice of a mini-batch size of 3 for correct workers is unexplained and appears arbitrary. A justification or sensitivity analysis of this parameter is necessary.
3. Incomplete Experimental Insights: The "cost of resilience" experiment (Figure 5) only reports results from an early training phase, leaving questions about Krum's performance closer to convergence. Additionally, the "multi-Krum" experiment (Figure 6) uses an omniscient parameter choice for `m`, and exploring other values would provide a more comprehensive understanding of the trade-offs.
4. Background Material: While the paper is self-contained, the extensive background on SGD (e.g., Figure 1) might be unnecessary for the NIPS audience, who are likely familiar with this material.
Recommendation:
The paper makes a significant theoretical contribution to the field of distributed machine learning by addressing Byzantine fault tolerance in gradient aggregation. However, the experimental evaluation needs to be strengthened to fully validate the practical applicability of Krum. Testing on larger datasets and models, along with a more detailed exploration of parameter choices and convergence behavior, would enhance the paper's impact. Despite these limitations, the novelty and rigor of the work make it a valuable contribution to the conference. I recommend acceptance with minor revisions.
Arguments for Acceptance:
- Novel and theoretically sound approach to Byzantine fault tolerance.
- Clear and well-organized presentation.
- Potentially impactful in real-world distributed learning settings.
Arguments Against Acceptance:
- Limited experimental validation on small-scale datasets and models.
- Unexplained parameter choices and incomplete experimental insights.