The paper addresses the challenging problem of sequence classification without labeled data by leveraging sequential output statistics, proposing a novel unsupervised learning cost function called Empirical Output Distribution Match (Empirical-ODM). This cost function avoids trivial solutions and eliminates the need for a generative model, improving upon prior works such as [7] and [30]. To optimize the non-convex functional form of the proposed cost, the authors develop a Stochastic Primal-Dual Gradient (SPDG) method, which effectively reduces optimization barriers. Experimental results on two real-world tasks—unsupervised OCR and spelling correction—demonstrate that the proposed method achieves significantly lower error rates compared to baseline methods, with performance approaching that of supervised learning.
Strengths:
1. Novelty and Contribution: The paper introduces a new cost function that emphasizes coverage-seeking behavior, addressing limitations in prior works like [7]. The avoidance of trivial solutions and the elimination of reliance on generative models are significant advancements.
2. Algorithmic Innovation: The SPDG method is well-motivated and demonstrates clear advantages over standard stochastic gradient descent (SGD) and prior methods. The authors provide theoretical insights into why SPDG works effectively.
3. Experimental Results: The method achieves impressive results, with error rates only about twice those of fully supervised learning, a notable achievement in unsupervised sequence classification. The robustness of the method to out-of-domain language models is also commendable.
4. Clarity of Analysis: The paper provides detailed comparisons with prior works, theoretical justifications, and empirical evaluations, making the contributions clear and well-supported.
Weaknesses:
1. Limited Baseline Comparisons: While the paper compares its method to [7], it does not include results from [11] and [30], which are referenced as related works. Including these baselines would strengthen the evaluation.
2. Dataset Scope: The experiments are limited to two datasets. Testing on additional datasets, especially from diverse domains, would enhance the generalizability of the results.
3. Figure 2(a) Clarity: The meaning of the axes (λd and λp) in Figure 2(a) is unclear. A detailed explanation is necessary for readers to interpret the visualization effectively.
4. Algorithm Details: The gradient formulas for Step 5 of Algorithm 1 are not explicitly provided, which could hinder reproducibility.
Pro and Con Arguments:
- Pro: The paper addresses an important problem, introduces a novel cost function, and proposes an effective optimization method. The results are strong and demonstrate significant improvements over prior works.
- Con: The lack of broader baseline comparisons and additional datasets limits the scope of the evaluation. Some details, such as Figure 2(a) and gradient formulas, are insufficiently explained.
Recommendation:
I recommend acceptance with minor revisions. The paper makes a significant contribution to unsupervised learning and sequence classification, and its strengths outweigh the weaknesses. Addressing the reviewer's suggestions—adding baseline comparisons, testing on more datasets, clarifying Figure 2(a), and including gradient formulas—would further strengthen the paper.