The manuscript proposes a novel approach to training feed-forward neural networks with ReLU activation by formulating the problem as a multi-convex optimization task and solving it using a block coordinate descent (BCD) algorithm. While the idea of lifting ReLU into a higher-dimensional space and employing Tikhonov regularization is intriguing, the paper suffers from several technical and conceptual issues that undermine its contributions.
Strengths:
1. Novelty: The proposed multi-convex formulation and the use of Tikhonov regularization to address the vanishing gradient problem are innovative. The idea of decomposing the optimization into convex sub-problems is theoretically appealing.
2. Empirical Results: The experimental results on MNIST demonstrate that the BCD algorithm can outperform traditional stochastic gradient descent (SGD) solvers in terms of test-set error rates and computational time.
3. Sparse Learning: The ability to learn sparse networks (BCD-S) is a valuable feature, particularly for resource-constrained applications.
Weaknesses:
1. Convergence Proof Issues: Theorem 1, which claims global convergence to a stationary point, is flawed. It incorrectly assumes the existence of a limit point without addressing the non-compactness of the set \( U \). Additionally, the claim of a unique limit point is invalid, as sequences in non-compact spaces can have multiple limit points, as demonstrated by counterexamples.
2. Scalability: The proposed formulation introduces a large number of auxiliary variables, which limits its scalability to larger datasets and deeper networks. The cubic complexity of solving the quadratic programs further exacerbates this issue.
3. Unaddressed Saddle Points: While the introduction discusses the challenges posed by saddle points, the proposed approach does not explicitly address them, making this discussion irrelevant.
4. Convexity Ambiguity: The convexity of the problem with respect to \( W \) (line 149) is unclear and requires further justification.
5. Experimental Comparisons: The time comparison with Caffe solvers is invalid due to differences in implementation platforms (MATLAB vs. Python). Additionally, the claim about MATLAB's speed advantage is unsubstantiated. The comparison of objective values in Figure 3(a) is inappropriate, as it compares different optimization problems.
6. Formatting Issues: The manuscript does not adhere to standard formatting for paper and equation references, which detracts from its readability.
7. Rebuttal Shortcomings: The rebuttal fails to address key concerns, such as the non-compactness of \( U \) and the flawed convergence assumptions. It also overlooks the fact that the existence of a limit point does not guarantee stationarity, as gradients may still be non-zero.
Recommendation:
While the paper presents an interesting idea, the unresolved theoretical issues, scalability concerns, and experimental shortcomings significantly weaken its contributions. The authors should address the convergence proof rigorously, clarify the convexity assumptions, and provide fair experimental comparisons. Additionally, the manuscript would benefit from improved formatting and a more focused discussion of its contributions relative to existing work.
Arguments for Acceptance:
- Novel approach to training DNNs with a theoretically motivated BCD algorithm.
- Promising empirical results on MNIST, particularly for sparse networks.
Arguments Against Acceptance:
- Flawed convergence proof and theoretical assumptions.
- Limited scalability and practical applicability to large-scale problems.
- Invalid experimental comparisons and unclear claims about computational efficiency.
- Poorly addressed reviewer concerns in the rebuttal.
Final Score: 4/10 (Reject). The paper requires substantial revisions to address the theoretical and practical limitations before it can be considered a strong contribution to the field.