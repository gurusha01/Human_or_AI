This paper proposes a low-rank regularizer for deep neural network compression during training, aiming to produce compact models without significant accuracy loss. By incorporating a nuclear norm-based regularizer into the training loss, the method encourages the parameter matrices of each layer to exhibit low rank, facilitating post-training compression via singular value decomposition (SVD). The authors further extend their approach by combining the low-rank regularizer with a sparse group Lasso regularizer, enabling the removal of entire units to achieve even higher compression rates. The paper reports experimental results on ImageNet and ICDAR datasets, demonstrating compression rates exceeding 90% with minimal accuracy degradation.
Strengths:
1. Practical Relevance: The focus on training compact networks from scratch rather than relying on post-hoc compression aligns well with the growing demand for efficient deployment of deep models on resource-constrained devices.
2. Comprehensive Evaluation: The authors provide detailed experiments, including parameter sensitivity analysis, comparisons with state-of-the-art methods, and results on multiple architectures (e.g., ResNet-50 and DecomposeMe). The analysis of multiply-accumulate operations (MACs) and inference time adds practical value.
3. Combination of Regularizers: The integration of low-rank and group-sparsity regularizers is novel and demonstrates improved compression rates compared to using either technique alone.
4. Training Efficiency: The paper highlights additional benefits during training, such as reduced training time and memory requirements when pruning is applied early.
Weaknesses:
1. Limited Novelty: While the proposed approach is well-motivated, it builds heavily on existing methods, such as low-rank approximations and group sparsity. The novelty lies primarily in combining these techniques, which may limit its impact.
2. Incomplete Comparisons: The paper lacks a thorough comparison with state-of-the-art compression methods on larger, more diverse models and datasets. For instance, results on architectures like EfficientNet or MobileNet, which are already optimized for efficiency, would strengthen the claims.
3. Parameter Sensitivity: Although parameter sensitivity is analyzed, the trade-offs between compression rate and accuracy are not fully explored across a wider range of hyperparameters. This could help practitioners better understand the method's robustness.
4. Inference Time Limitations: While the method reduces MACs significantly, the lack of substantial inference time improvements on modern hardware limits its practical applicability. The authors acknowledge this but do not propose solutions for optimizing hardware utilization.
Pro Acceptance Arguments:
- Addresses a critical problem in deep learning: efficient model compression.
- Demonstrates strong empirical results with high compression rates and minimal accuracy loss.
- Provides a solid foundation for future work on compression-aware training.
Con Acceptance Arguments:
- Limited novelty compared to prior work.
- Results on larger models and comparisons with more diverse state-of-the-art methods are needed.
- Practical benefits in real-world deployment scenarios (e.g., inference time) remain unclear.
Recommendation: While the paper makes a meaningful contribution to the field of model compression, its limited novelty and incomplete evaluation on larger models temper its impact. I recommend acceptance if the authors address the need for broader comparisons and provide additional insights into inference-time improvements. Otherwise, it may be better suited for a more specialized venue.