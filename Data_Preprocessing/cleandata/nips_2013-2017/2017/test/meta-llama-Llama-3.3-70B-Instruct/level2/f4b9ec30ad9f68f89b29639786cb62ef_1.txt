This paper presents a novel approach to achieving Byzantine resilience in distributed stochastic gradient descent (SGD) algorithms. The authors propose a new aggregation rule, called Krum, which is designed to tolerate Byzantine failures in a distributed system. The main claim of the paper is that Krum is the first provably Byzantine-resilient algorithm for distributed SGD.
The paper provides a thorough analysis of the problem, including a proof that no linear combination of the updates proposed by the workers can tolerate a single Byzantine worker. The authors then introduce the concept of (Î±, f)-Byzantine resilience, which captures the basic requirements for an aggregation rule to guarantee convergence despite f Byzantine workers. They show that Krum satisfies this property and provide a convergence analysis of the SGD algorithm using Krum.
The experimental evaluation of Krum demonstrates its effectiveness in tolerating Byzantine failures and achieving convergence in the presence of such failures. The authors also compare Krum with classical averaging and show that Krum is more resilient to Byzantine attacks. Additionally, they propose a variant of Krum, called Multi-Krum, which allows for a trade-off between convergence speed and resilience to Byzantine workers.
The paper is well-written, and the authors provide a clear and concise explanation of the problem, the proposed solution, and the experimental results. The analysis is thorough, and the proofs are provided in the supplementary material.
The strengths of the paper include:
* The proposal of a novel aggregation rule, Krum, which is provably Byzantine-resilient
* A thorough analysis of the problem, including a proof that no linear combination of the updates proposed by the workers can tolerate a single Byzantine worker
* A convergence analysis of the SGD algorithm using Krum
* Experimental evaluation of Krum, demonstrating its effectiveness in tolerating Byzantine failures and achieving convergence
The weaknesses of the paper include:
* The assumption that the parameter server is reliable, which may not be the case in practice
* The lack of comparison with other Byzantine-resilient algorithms, such as those proposed in [28, 29]
* The limited experimental evaluation, which only considers a few scenarios and datasets
Overall, the paper presents a significant contribution to the field of distributed machine learning, and the proposed Krum algorithm has the potential to be widely adopted in practice. However, further research is needed to address the limitations of the paper and to explore the applicability of Krum in different scenarios.
Arguments pro acceptance:
* The paper presents a novel and provably Byzantine-resilient algorithm for distributed SGD
* The analysis is thorough, and the proofs are provided in the supplementary material
* The experimental evaluation demonstrates the effectiveness of Krum in tolerating Byzantine failures and achieving convergence
Arguments con acceptance:
* The assumption that the parameter server is reliable may not be realistic in practice
* The lack of comparison with other Byzantine-resilient algorithms may limit the impact of the paper
* The limited experimental evaluation may not be sufficient to demonstrate the applicability of Krum in different scenarios.