This paper proposes an extension to recurrent neural network language models, called the unbounded cache model, which can adapt to local changes in the data distribution by storing all the hidden activations seen in the past. The model leverages recent advances in approximate nearest neighbor search and quantization algorithms to store millions of representations while searching them efficiently. The authors demonstrate the effectiveness of their approach through extensive experiments on various language modeling tasks, showing significant improvements in perplexity over pre-trained language models on new distributions.
The paper is well-written, and the authors provide a clear and concise overview of the background and related work in language modeling and non-parametric approaches. The proposed model is well-motivated, and the use of approximate nearest neighbor search and quantization algorithms is a clever solution to the problem of scaling the cache to large datasets.
The experiments are thorough and well-designed, covering various scenarios, including near and far domain adaptation. The results show that the unbounded cache model outperforms the static model and other baseline models, including the local cache model, in most cases. The authors also provide a detailed analysis of the performance of their model with respect to the number of nearest neighbors and the size of the cache.
The strengths of the paper include:
* The proposal of a novel and effective approach to adapting language models to changing data distributions
* The use of approximate nearest neighbor search and quantization algorithms to scale the cache to large datasets
* The thorough and well-designed experiments that demonstrate the effectiveness of the approach
* The clear and concise writing style
The weaknesses of the paper include:
* The lack of a detailed analysis of the computational complexity of the proposed model
* The limited discussion of the potential applications of the proposed model beyond language modeling
* The absence of a comparison with other state-of-the-art models that use non-parametric approaches for language modeling
Overall, the paper is well-written, and the proposed approach is novel and effective. The experiments are thorough, and the results are convincing. The paper has the potential to make a significant contribution to the field of natural language processing, and I would recommend it for acceptance.
Arguments pro acceptance:
* The paper proposes a novel and effective approach to adapting language models to changing data distributions
* The experiments are thorough and well-designed, demonstrating the effectiveness of the approach
* The paper has the potential to make a significant contribution to the field of natural language processing
Arguments con acceptance:
* The lack of a detailed analysis of the computational complexity of the proposed model
* The limited discussion of the potential applications of the proposed model beyond language modeling
* The absence of a comparison with other state-of-the-art models that use non-parametric approaches for language modeling
Recommendation: Accept with minor revisions to address the weaknesses mentioned above.