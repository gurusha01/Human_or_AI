This paper proposes a novel approach to compressing deep neural networks by explicitly accounting for compression during the training process. The authors introduce a regularizer that encourages the parameter matrix of each layer to have low rank, which allows for more compact models to be learned. The approach is demonstrated to achieve higher compression rates than state-of-the-art methods, with minimal loss in prediction accuracy.
The paper is well-written and clearly explains the motivation and methodology behind the proposed approach. The authors provide a thorough review of related work and demonstrate the effectiveness of their approach on several deep architectures, including the 8-layers DecomposeMe network and the 50-layers ResNet.
The strengths of the paper include:
* The proposal of a novel approach to compressing deep neural networks that takes into account the compression stage during training.
* The demonstration of the effectiveness of the approach on several deep architectures, including the 8-layers DecomposeMe network and the 50-layers ResNet.
* The provision of a thorough review of related work and a clear explanation of the methodology behind the proposed approach.
The weaknesses of the paper include:
* The lack of a clear comparison to other state-of-the-art compression methods, such as quantization and pruning.
* The limited evaluation of the approach on only two datasets, ImageNet and ICDAR.
* The need for further experimentation to fully understand the trade-offs between compression rate, accuracy, and inference time.
Overall, the paper presents a promising approach to compressing deep neural networks and demonstrates its effectiveness on several deep architectures. However, further experimentation and comparison to other state-of-the-art compression methods are needed to fully understand the potential of the proposed approach.
Arguments for acceptance:
* The paper proposes a novel approach to compressing deep neural networks that takes into account the compression stage during training.
* The approach is demonstrated to achieve higher compression rates than state-of-the-art methods, with minimal loss in prediction accuracy.
* The paper provides a thorough review of related work and a clear explanation of the methodology behind the proposed approach.
Arguments against acceptance:
* The lack of a clear comparison to other state-of-the-art compression methods, such as quantization and pruning.
* The limited evaluation of the approach on only two datasets, ImageNet and ICDAR.
* The need for further experimentation to fully understand the trade-offs between compression rate, accuracy, and inference time.
Recommendation: Accept with minor revisions. The paper presents a promising approach to compressing deep neural networks and demonstrates its effectiveness on several deep architectures. However, further experimentation and comparison to other state-of-the-art compression methods are needed to fully understand the potential of the proposed approach. The authors should provide a clear comparison to other state-of-the-art compression methods and evaluate the approach on a wider range of datasets. Additionally, the authors should provide more details on the trade-offs between compression rate, accuracy, and inference time.