This paper proposes a novel approach to increasing the capacity of Long Short-Term Memory (LSTM) networks, called the Tensorized LSTM (tLSTM). The tLSTM represents hidden states as tensors and updates them via a cross-layer convolution, allowing for efficient widening and deepening of the network without significantly increasing the parameter number or runtime. The authors also introduce a memory cell convolution mechanism to capture long-range dependencies from multiple directions.
The paper is well-written and clearly explains the motivation, methodology, and experimental results. The authors provide a thorough analysis of the tLSTM's performance on various sequence learning tasks, including language modeling, algorithmic tasks, and image classification. The results show that the tLSTM outperforms other popular approaches, such as Stacked LSTMs and Convolutional LSTMs, in many cases.
The strengths of the paper include:
* The introduction of a novel and efficient approach to increasing the capacity of LSTMs
* A thorough analysis of the tLSTM's performance on various tasks
* A clear and well-organized presentation of the methodology and results
The weaknesses of the paper include:
* The complexity of the tLSTM architecture may make it difficult to implement and train
* The memory cell convolution mechanism may require careful tuning of hyperparameters
* The paper could benefit from a more detailed comparison with other state-of-the-art methods
Arguments for acceptance:
* The paper introduces a novel and efficient approach to increasing the capacity of LSTMs
* The experimental results show that the tLSTM outperforms other popular approaches in many cases
* The paper is well-written and clearly explains the methodology and results
Arguments against acceptance:
* The complexity of the tLSTM architecture may make it difficult to implement and train
* The paper could benefit from a more detailed comparison with other state-of-the-art methods
* The memory cell convolution mechanism may require careful tuning of hyperparameters
Overall, I believe that the paper is well-written and presents a novel and efficient approach to increasing the capacity of LSTMs. The experimental results are promising, and the paper provides a thorough analysis of the tLSTM's performance on various tasks. However, the complexity of the tLSTM architecture and the need for careful tuning of hyperparameters may be limitations of the approach. I recommend acceptance of the paper, but suggest that the authors address the weaknesses and provide more detailed comparisons with other state-of-the-art methods in future work.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10
Recommendation: Accept with minor revisions.