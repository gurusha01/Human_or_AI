This paper proposes a novel Tikhonov regularization for training deep neural networks (DNNs) with ReLU activation functions. The authors reformulate the network training as a block multi-convex minimization problem and propose a block coordinate descent (BCD) based algorithm to solve it. The algorithm is proven to converge globally to stationary points with R-linear convergence rate of order one.
The paper is well-written and provides a clear overview of the proposed method. The authors relate their work to previous research in the field, including stochastic regularization, ADMM, and BCD. The experimental results demonstrate the effectiveness of the proposed algorithm, showing that it can achieve better test-set error rates than traditional SGD-based solvers.
The strengths of the paper include:
* The proposal of a novel Tikhonov regularization for DNNs, which can be used to learn both dense and sparse networks.
* The development of a BCD-based algorithm that can guarantee global convergence to stationary points with R-linear convergence rate.
* The provision of a clear and detailed convergence analysis, including a proof of the algorithm's global convergence and R-linear convergence rate.
The weaknesses of the paper include:
* The computational complexity of the proposed algorithm, which may be high due to the need to solve a sequence of quadratic programs.
* The lack of comparison with other state-of-the-art methods, such as Entropy-SGD and ADMM-based algorithms.
* The limited experimental evaluation, which only includes results on the MNIST dataset.
Arguments for acceptance:
* The paper proposes a novel and interesting approach to training DNNs, which can be used to learn both dense and sparse networks.
* The algorithm is proven to converge globally to stationary points with R-linear convergence rate, which is a desirable property.
* The experimental results demonstrate the effectiveness of the proposed algorithm, showing that it can achieve better test-set error rates than traditional SGD-based solvers.
Arguments against acceptance:
* The computational complexity of the proposed algorithm may be high, which could limit its practical applicability.
* The lack of comparison with other state-of-the-art methods may make it difficult to evaluate the significance of the proposed approach.
* The limited experimental evaluation may not be sufficient to demonstrate the effectiveness of the proposed algorithm in a variety of settings.
Overall, I believe that the paper is well-written and proposes a novel and interesting approach to training DNNs. While there are some weaknesses, the strengths of the paper outweigh them, and I recommend acceptance.