This paper proposes an extension to recurrent neural network language models, called the unbounded cache model, which can adapt to local changes in the data distribution by storing all the hidden activations seen in the past. The model leverages recent advances in approximate nearest neighbor search and quantization algorithms to efficiently store and search millions of representations. The authors demonstrate the effectiveness of their approach through extensive experiments on various language modeling tasks, showing significant improvements in perplexity over pre-trained language models on new distributions.
The paper relates to previous work on cache models, which have been used in speech recognition and language modeling to adapt to local changes in the data distribution. However, these models are limited to capturing local context and cannot scale to larger contexts. The proposed unbounded cache model overcomes this limitation by using a non-parametric memory component that can store millions of representations.
The strengths of the paper include its clear and well-organized presentation, as well as its thorough experimental evaluation. The authors provide a detailed description of their approach, including the use of approximate nearest neighbor search and quantization algorithms, and demonstrate its effectiveness on various datasets. The paper also provides a good overview of related work in the area, including cache models and large-scale retrieval methods.
The weaknesses of the paper include the fact that the proposed approach requires significant computational resources to store and search millions of representations. Additionally, the authors do not provide a detailed analysis of the computational complexity of their approach, which could be an important consideration for practical applications.
Arguments pro acceptance:
* The paper proposes a novel extension to recurrent neural network language models that can adapt to local changes in the data distribution.
* The authors demonstrate the effectiveness of their approach through extensive experiments on various language modeling tasks.
* The paper provides a clear and well-organized presentation, as well as a thorough overview of related work in the area.
Arguments con acceptance:
* The proposed approach requires significant computational resources to store and search millions of representations.
* The authors do not provide a detailed analysis of the computational complexity of their approach.
* The paper could benefit from a more detailed discussion of the potential limitations and challenges of the proposed approach.
Overall, the paper makes a significant contribution to the area of language modeling and demonstrates the effectiveness of the proposed unbounded cache model. However, the authors could provide more detail on the computational complexity of their approach and discuss potential limitations and challenges. 
Quality: The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work.
Clarity: The paper is clearly written, well-organized, and adequately informs the reader. The authors provide a detailed description of their approach and demonstrate its effectiveness on various datasets.
Originality: The paper proposes a novel extension to recurrent neural network language models, which is a significant contribution to the area. The authors leverage recent advances in approximate nearest neighbor search and quantization algorithms to efficiently store and search millions of representations.
Significance: The results are important, and other people (practitioners or researchers) are likely to use these ideas or build on them. The paper addresses a difficult problem in a better way than previous research and advances the state of the art in a demonstrable way.