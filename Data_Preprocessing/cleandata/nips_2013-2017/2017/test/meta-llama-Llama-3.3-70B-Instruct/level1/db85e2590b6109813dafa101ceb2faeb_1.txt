This paper proposes a novel approach to compressing deep neural networks by explicitly accounting for compression during the training process. The authors introduce a regularizer that encourages the parameter matrix of each layer to have low rank, which allows for more compact models to be learned. The approach is demonstrated to achieve higher compression rates than state-of-the-art methods, with minimal loss in prediction accuracy.
The paper is well-written and clearly explains the motivation and methodology behind the proposed approach. The authors provide a thorough review of related work in the field of neural network compression, highlighting the limitations of existing methods and the benefits of their approach. The experimental results are impressive, with compression rates of over 90% achieved on several deep architectures, including the 8-layers DecomposeMe network and the 50-layers ResNet.
The strengths of the paper include:
* The proposal of a novel and effective approach to compressing deep neural networks
* A thorough review of related work and clear explanation of the methodology
* Impressive experimental results demonstrating the effectiveness of the approach
* The potential for significant reductions in computational cost and memory usage
The weaknesses of the paper include:
* The approach may not be applicable to all types of neural networks or compression strategies
* The choice of hyperparameters, such as the regularization strength and energy percentage, may require careful tuning
* The paper could benefit from a more detailed analysis of the trade-offs between compression rate, accuracy, and computational cost
Arguments for acceptance:
* The paper proposes a novel and effective approach to compressing deep neural networks
* The experimental results are impressive and demonstrate the potential for significant reductions in computational cost and memory usage
* The approach has the potential to be widely applicable and to make a significant impact in the field of deep learning
Arguments against acceptance:
* The approach may not be applicable to all types of neural networks or compression strategies
* The paper could benefit from a more detailed analysis of the trade-offs between compression rate, accuracy, and computational cost
* The choice of hyperparameters may require careful tuning, which could limit the applicability of the approach.
Overall, I believe that the paper makes a significant contribution to the field of deep learning and neural network compression, and I recommend acceptance. The approach is novel, effective, and has the potential to make a significant impact in the field. While there are some limitations and potential weaknesses, these can be addressed through further research and development.