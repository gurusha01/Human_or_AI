This paper introduces a novel aggregation rule for gradient vectors in distributed stochastic gradient descent, designed to mitigate Byzantine failures. The authors demonstrate that conventional aggregation methods, such as averaging, are vulnerable to even a single Byzantine failure. To address this, they define the concept of Byzantine resilience, which measures the tolerance to such failures, and prove that their proposed aggregation function satisfies this criterion. Experimental results indicate that the proposed method outperforms the approach without it as the number of Byzantine workers increases. However, this comes at the cost of computational efficiency, even in the absence of Byzantine workers, highlighting the need for a trade-off between robustness and efficiency.
As someone less familiar with distributed stochastic descent, my inquiry focuses on the machine learning aspects. Notably, Figure 4 suggests that Krum also yields a higher learning error compared to averaging. I am interested in understanding the magnitude of this error difference. Is there a bound on this difference? What is the performance cost of using Krum to achieve robustness against Byzantine failures, and how significant is this cost in terms of machine learning performance?