This manuscript presents TrajGRU, a novel extension of convolutional LSTM/GRU architectures. Unlike traditional convLSTM/GRU, TrajGRU focuses on learning location-dependent filter support for each hidden state location. The TrajGRU model generates a flow field from the current input and previous hidden state, and then applies bilinear sampling to warp the previous hidden states according to this flow field.
The authors evaluate their proposed model on video generation tasks using two datasets: MovingMNIST, which features three digits simultaneously, and the HKO-7 nowcasting dataset. The results demonstrate that TrajGRU outperforms its convolutional counterparts.
Several specific questions and remarks arise:
Did the authors compare TrajGRU to ConvGRU with a larger kernel support than the standard 5x5?
Does the warping operation in TrajGRU increase computational costs compared to convGRU? Providing detailed information on model parameters, operations, and running times for the experimental evaluations would be informative.
Why did the authors opt for a fixed number of training epochs instead of implementing early stopping, and could some models' performance be improved by terminating training earlier?
- Quality: The paper appears to be technically sound.
- Clarity: The paper is generally clear, but specifying the warp method used would provide a more comprehensive understanding of TrajGRU. Additionally, the number of examples and training/validation/test splits for the HKO-7 dataset are not clearly stated.
- Originality: While few studies have explored warping for video modeling, such as "Spatio-temporal video autoencoder with differentiable memory," comparing and contrasting TrajGRU to these approaches would be beneficial.
- Significance/Conclusion: Developing models that learn effective video representations remains an ongoing research challenge. This paper proposes a novel model that learns filter support in addition to filter weights, representing an interesting step towards improved video modeling. However, the approach has only been tested on a synthetic dataset (MovingMNIST) and a specialized nowcasting dataset. It would be valuable to investigate whether this model yields better video representations for more traditional video tasks, such as human action classification using generic videos.