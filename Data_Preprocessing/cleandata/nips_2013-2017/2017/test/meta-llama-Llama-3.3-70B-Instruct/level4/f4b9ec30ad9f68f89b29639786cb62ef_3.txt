The manuscript presents a novel algorithm, referred to as Krum, designed to combine partially calculated gradients in a manner that is tolerant to Byzantine failures. This approach is intended to be applicable in the distributed training of machine learning models on exceptionally large datasets.
As a machine learning expert without specialized knowledge in distributed computing, my evaluation of the manuscript is from the perspective of a potential user.
---
Strengths:
* The research question addressed is highly intriguing, given the increasing importance of distributed techniques in machine learning as dataset sizes expand exponentially.
* The paper is crafted in a sophisticated academic tone and boasts a clear, organized structure.
* The proposed algorithm is substantiated by rigorous theoretical arguments.
---
Weaknesses:
* The results depicted in Figure 4 are underwhelming. In the absence of Byzantine failures, the proposed algorithm significantly lags behind simple averaging, only demonstrating its utility when over a third of the workers are Byzantine. This failure rate seems excessively high, suggesting that averaging will remain highly competitive at realistically low failure rates, thereby questioning the motivation behind the proposed approach.
* Although the paper introduces an idea that becomes relevant with dramatically large datasets, the experiments reported are conducted on exceedingly small datasets. For instance, MNIST contains 60,000 instances, and spambase contains 4,601. The divergence between true and stochastic gradients becomes noticeable with sufficiently large datasets. Given current hardware capabilities, a modest workstation equipped with a moderately priced GPU can easily be trained on millions of data points without necessitating protocols like Krum. Furthermore, the need for more than a few million data points depends on the application, and in many cases, the performance difference between 10 million and 100 million data points is negligible. To convince machine learners to compromise on model speed for the sake of distributed safety, the authors should identify specific applications where this need is genuine and report results from those applications.
* The proposed algorithm is designed for updating global parameters from stochastic gradients calculated on minibatches, a fundamental approach in many machine learning models. However, contemporary techniques face a different central challenge, particularly in deep neural networks, which require distributed computing to distribute operations across neurons rather than minibatches. The inability to generalize the proposed algorithm to this scenario significantly reduces its potential impact.
* A minor point is that the paper would benefit from citing earlier pioneering work on distributed machine learning, such as C.T. Chu et al.'s "Map-Reduce for Machine Learning on Multicore" presented at NIPS in 2007.
---
Preliminary Evaluation:
While appreciating the theoretical groundwork of this paper and the importance of distributed machine learning, the reported results raise doubts about the usefulness of the proposed approach.
---
Final Evaluation:
I acknowledge that a 33% Byzantine failure rate is a standard test case in general distributed computing tasks. However, the context here is training machine learning models, where dynamics differ significantly from merely processing large data batches. The primary concern is accuracy, not data size. According to Figure 4, Krum substantially compromises model accuracy in the absence of an attack. This implies that a machine learner would only consider using Krum if they are certain that a subset of 10-20 million data points would be insufficient for satisfactory accuracy (necessitating distributed computing) and that at least 33% of the nodes would act Byzantine (necessitating Krum). As a machine learner, I find it challenging to identify such a scenario. Essentially, it is the authors' responsibility, not the readers', to highlight these cases. Unfortunately, both the paper and the rebuttal fail to do so, leading me to maintain my initial negative vote.