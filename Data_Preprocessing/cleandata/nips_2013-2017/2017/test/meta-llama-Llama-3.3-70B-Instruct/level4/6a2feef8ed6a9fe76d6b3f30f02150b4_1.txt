This manuscript presents a straightforward and efficient block coordinate descent (BCD) algorithm, incorporating a novel Tikhonov regularization technique for the training of both dense and sparse deep neural networks (DNNs) utilizing the ReLU activation function. The authors demonstrate that their proposed BCD algorithm achieves global convergence to a stationary point, exhibiting an R-linear convergence rate of order one, and outperforms various stochastic gradient descent (SGD) variants in experimental evaluations. However, the rationale behind the selection of Tikhonov regularization and the block coordinate descent approach is not adequately explained. The technical aspects of the manuscript are challenging to comprehend due to the lack of detailed information. Furthermore, the reported results fall short of the current state-of-the-art, raising concerns about the applicability of the proposed method to actual DNNs. My detailed comments are provided below.
Specific comments:
1. The authors should provide a clearer justification for the use of Tikhonov regularization. What advantages does it offer over other regularization techniques? In the second paragraph of Section I, the authors discuss several challenges associated with DNNs, including non-convexity, saddle points, and local extrema, followed by the introduction of Tikhonov regularization. Is it implied that the proposed Tikhonov regularization can effectively address these issues?
2. The motivation behind decomposing the problem into three sub-problems should be explicitly stated. The authors should explain why this decomposition does not suffer from the vanishing gradient problem.
3. The transition from Eqn. (4) to Eqn. (5) is unclear. On line 133, the specific formula for the matrix Q(\tilde {\mathcal A}) is not provided. How can it be confirmed that the matrix Q(\tilde {\mathcal A}) is positive semidefinite?
4. A precise mathematical definition of the Tikhonov regularized inverse problem should be given.
5. The claim that the inverse sub-problem resolves the vanishing gradient issue in traditional deep learning lacks theoretical justification and empirical evidence. The authors should provide supporting evidence to verify this assertion.
6. Further details are needed to explain why the "optimal" output features work similarly to target propagation.
7. On line 246, the definition of \mathcal P is not provided.
8. In Fig. 5, BCD-S is capable of learning much sparser networks; however, the weight matrix of the 4th layer remains dense. This discrepancy should be addressed.
9. On line 252, the meaning of the prefix "Q" is unclear.
10. On line 284, a word appears to be missing: "dense".
11. The reference format is confusing and should be revised.
12. The choice of DNN structure in Figure 1 is unclear. The network used in the experiments has only 3 hidden layers, which may not be considered "deep". The use of skip connections should be considered. Moreover, the presented results are not competitive with the current state-of-the-art, which raises doubts about the applicability of the proposed method to real-world DNNs.