This manuscript presents a novel adaptation of recurrent neural networks, including Elman networks, LSTMs, and gated recurrent units, for language modeling tasks, enabling the model to accommodate shifts in data distribution through the incorporation of an unbounded cache. This cache is designed to handle out-of-vocabulary words during training and rare terms by estimating the probability distribution of encountered words using a kernel density estimator, scaled efficiently with an approximate k-nearest neighbors approach. Unlike similar studies ([21], [50], [41]), this approach uniquely stores all previously seen words, rather than just the most recent ones. Theoretically, the paper addresses a significant challenge in NLP applications, making it an interesting contribution. The results demonstrate the importance of the cache in achieving good performance, and the authors have substantially enhanced the empirical validation during the review process, showing improvements over the current state of the art.
Opinion on the paper:
+ The manuscript is well-structured and clearly written, although it would benefit from a professional English language edit.
+ The topic holds considerable significance for NLP practitioners.
- A notable limitation is the lack of comprehensive numerical evaluation in the original submission, despite the authors providing additional numerical results during the rebuttal phase, which, due to the review process constraints, could not be included in the final manuscript and thus were not considered in this evaluation.