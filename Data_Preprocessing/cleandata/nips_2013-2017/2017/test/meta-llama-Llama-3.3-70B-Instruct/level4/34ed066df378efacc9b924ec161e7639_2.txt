The manuscript presents a novel approach to human image generation, conditioned on both appearance and pose, leveraging an adversarial training framework. This framework employs a two-stage generative network to produce high-resolution images, which are then evaluated by a discriminator. The generative process involves an initial coarse image generation using a U-shaped network based on appearance and pose maps, followed by a refinement stage where a second generator predicts residual information to enhance the coarse image. The DeepFashion dataset is utilized for evaluation purposes.
Several key contributions are highlighted in the paper, including:
* The introduction of a novel task: generating human images conditioned on appearance and pose maps.
* The proposal of a stacked architecture that predicts difference maps rather than relying on direct upsampling, alongside a tailored loss function design.
To further distinguish itself, the paper could address the following areas:
* Quality improvement: the generated images still exhibit noticeable artifacts.
* Significance: the work may be perceived as another iteration of GAN architecture, lacking a broader impact.
* Generalizability: while the application to vision and graphics is clear, its relevance to other learning problems is less apparent.
The manuscript is well-structured, effectively conveying the core aspects of the proposed architecture. The generator's coarse-to-fine strategy, conditioned on appearance and pose information, is a notable approach. The application of this strategy to the dressing problem demonstrates its potential. The technique of generating difference maps is also a valuable contribution to producing higher resolution images.
However, a significant concern is the presence of visible artifacts in the generated results, indicating a need for substantial improvement for practical applications. For instance, the patterns in ID346 of Fig 4 result in undesirable black dots in the final image. Although the second generator improves upon the blurry images produced by the first, it appears insufficient in recovering high-frequency components of the target appearance.
Another concern, albeit less severe, is that the proposed approach might be viewed as an application of existing conditional GAN methodologies. Previous works, such as those by Han Zhang et al. and Xun Huang et al., have explored similar concepts of conditioning and stacking generators for adversarial training. While the paper addresses specific application challenges, its appeal might be limited to a narrower audience.
In conclusion, the paper successfully addresses the pose-conditioned image generation problem and conducts a thorough evaluation. It demonstrates technical novelty, particularly in its approach to generating visually consistent images. Although quality improvements are necessary, the proposed model shows promise. Based on this assessment, the initial recommendation is to accept the paper.