This manuscript proposes a novel approach for sequence label prediction without relying on labeled training data. The method involves incorporating a prior probability distribution over possible output sequences, aiming to find a linear classifier that yields a predicted sequence distribution close to the given prior in the Kullback-Leibler sense. The authors introduce a cost function, an optimization algorithm, and present experimental results on real-world data for character prediction tasks in optical character recognition (OCR) and spell correction.
The paper is technically sound and well-structured, presenting an interesting and novel idea that builds upon existing work cited by the authors. However, my primary concern influencing the score is the challenge of obtaining a useful prior model, denoted as p_LM. Such models often suffer from data sparsity, particularly with larger vocabularies, leading to zero probability for some test samples under the learned prior. Although this issue is well-known and solutions are available in the literature, I believe the authors should address its impact on their proposed method. Furthermore, the scalability of the algorithm to large vocabularies and longer sub-sequences (parameter N) is a concern, given that the experiments were conducted with a relatively small vocabulary (29 characters) and short sub-sequences (N-grams with N=2,3), which may not reflect the complexity of natural language processing (NLP) data at the word level, where vocabularies are significantly larger.
Additional comments include:
- The authors should consider discussing the applicability of their method to domains beyond NLP.
- The inclusion of the supervised solution in the 2D cost figures raises questions, as the supervised solution appears to always be in a local minimum of the unsupervised cost, which is not clearly explained.
- Given the assumption of structure in the output sequence space, it would be beneficial to incorporate this structure into the classifier, rather than using independent point predictions for each label.
Specific comments:
- Lines 117-118 refer to 'negative cross entropy'; however, the formula provided seems to represent cross entropy itself, not its negative.
- Line 119 is somewhat confusing, as \bar p appears to represent the expected frequency of a given sequence, not all sequences.
- Lines 163-166 seem to have an incorrect sign for the cross entropy, which is also inconsistent in the text, potentially leading to confusion regarding the behavior of the function as p_LM approaches 0.
Typos and minor errors:
- Lines 173 and 174 contain the phrase "that pLM," which might be a mistake, potentially meant to be "for which pLM."
- Line 283 contains a repeated word, "rate," which should be corrected for clarity.