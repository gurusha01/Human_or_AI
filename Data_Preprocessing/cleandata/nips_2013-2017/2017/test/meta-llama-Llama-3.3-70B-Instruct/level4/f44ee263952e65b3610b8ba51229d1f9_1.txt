This manuscript presents a novel non-parametric approach to caching previously encountered contexts for language modelling purposes. The core concept involves retrieving the k nearest-neighbour states from prior contexts at each point and utilizing a kernel density estimation technique to produce a probability distribution across an open vocabulary, thereby rendering the cache model unbounded - a distinction from methods like pointer networks or continuous caches. The results showcase the efficacy of this approach in language modelling tasks characterized by time and topic drift, outperforming standard RNN language models.
The paper is well-structured and clearly written, with the concept of an unbounded cache being particularly intriguing. The proposed methodology has the potential to be adapted to a wide range of tasks beyond language modelling.
However, a comparative analysis against parametric or local cache methods, such as the pointer-generator network, would have strengthened the paper. Additionally, an assessment of the model's inference time efficiency would be beneficial, as querying 1024 nearest neighbours to estimate p_{cache} appears to be computationally intensive, potentially impacting the model's practical applicability.