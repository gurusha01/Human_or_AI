The paper under review explores the concept of contextual bandits with N arms, where in each round, the learner is presented with a context $x{ti}$ for each arm, selects an arm to pull, and subsequently receives a reward $r{ti}$. A key question addressed in this work concerns the structure imposed on the rewards, with the authors noting two common choices: $E[r{ti}] = < x{ti}, \theta >$ and $E[r{ti}] = < x{ti}, \theta_i >$. The former facilitates faster learning but has limited capacity, while the latter offers more capacity at the cost of slower learning. This paper seeks to find a middle ground, which is achieved through kernelization.
The primary idea presented involves augmenting the context space so that the learner observes $(z{ti}, x{ti})$, where $z_{ti}$ belongs to an additional space Z. A kernel defined on this augmented space measures the similarity between contexts and determines the degree of sharing between arms.
The main contribution of this paper appears to be the concept of augmenting the context space in this manner. The regret analysis utilizes standard techniques. While there is some novelty in this approach, it is not significantly new, as the analysis by Valko et al. could potentially apply directly to the augmented contexts with similar guarantees. Thus, the true innovation lies in the augmentation idea itself.
Assessing the impact of this paper is challenging. From a theoretical standpoint, there is not much that is new. The practical experiments are appreciated but not overwhelming, particularly in the absence of comparisons, such as with linear Thompson sampling.
The correctness of the paper seems plausible, based on a cursory review of the proofs in the supplementary material. However, the paper's overall contribution is borderline. A stronger argument from the authors that the theoretical results offer more than the analysis in Valko et al.'s cited paper could potentially increase the score.
Other notable comments include the unjustified assumption on $n_{a,t} = t/N$ after time t, which undermines the conclusions drawn from it. The inability to analyze Algorithm 1 is also a pity, and comparing the regret of the "sup-" algorithms to Algorithm 1 in a figure could be useful.
Minor suggestions for improvement include capitalizing random variables like $At$, correcting grammatical errors such as "one estimate" to "one estimates", specifying the domain of $za$ as $za \in \mathcal Z$, and clarifying the notation $ta$ and the use of primes. Additionally, a noise assumption on the rewards seems necessary but was not explicitly stated. The notation for the augmented context should also be consistent, such as using $(a, x_a)$ throughout.