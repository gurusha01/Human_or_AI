This paper introduces a rigorous condition for achieving Byzantine fault-tolerant stochastic gradient aggregation and proposes an algorithm, Krum, that fulfills this condition. It is evident from the definition that aggregation methods based on linear combinations, including averaging, are not Byzantine tolerant. The paper also provides empirical evidence demonstrating that Krum is not only theoretically sound but also practically viable.
The paper is well-structured, clear, and self-contained, offering a thorough exploration of the topic. The primary motivation is well-articulated, addressing the gap in the literature on distributed learning, which has traditionally focused on statistical assumptions and cooperative actors, by investigating the scenario under very pessimistic conditions. To the best of my knowledge, this is the first paper to explore fault-tolerance from this perspective, a concept already established in other contexts. However, I did not verify the analysis presented.
Regarding the presentation, the paper provides an extensive background on stochastic gradient descent (SGD), including an illustration of unbiased stochastic gradients via a cartoon figure (Figure 1). While this background is nice, it might be unnecessary for an audience familiar with stochastic optimization at NIPS, and the authors may consider reallocating this space to more central technical content.
My additional comments pertain to the experimental section:
1. The experiments could be improved by utilizing more contemporary datasets and models, as the current setup, focusing on small-scale, single-machine datasets, does not fully align with the paper's primary setting. Distributed learning is not typically applied to such datasets, making the experiments more of a simulation. Although they do reveal interesting phenomena, it is unclear whether these findings generalize to scenarios where distributed learning is more applicable. A potential next step could involve using a deep convolutional network for data-augmented CIFAR-10, which would likely benefit from a distributed setup due to its longer training time on a single machine.
2. The choice of a mini-batch size of 3 for correct workers appears arbitrary and lacks explanation.
3. In the "cost of resilience" experiment (Figure 5), reporting the error at round 500 seems to capture an early phase of training, given that the accuracies are not yet optimal for the task. With a batch size of 100, 500 rounds correspond to 50,000 examples, which do not constitute a full pass through the training set. The experiment would benefit from an additional plot or a remark describing the behavior closer to convergence.
4. In the "multi-Krum" experiment (Figure 6), setting the parameter m to be n - f, where n is the total number of machines and f is the number of Byzantine machines, seems like an overly informed choice, as it assumes knowledge of the number of adversaries. The experiment would be more comprehensive if it included results for other values of m (for the same f), as this would illustrate the trade-off curve suggested by the writing.