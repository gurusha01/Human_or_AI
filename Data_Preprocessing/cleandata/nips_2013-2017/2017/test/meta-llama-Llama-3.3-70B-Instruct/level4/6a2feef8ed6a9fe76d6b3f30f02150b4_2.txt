This manuscript effectively integrates several key recent concepts for optimizing deep neural networks, presenting a compelling alternative to prevalent SGD variants. The authors' derivation of the Tikhonov regularized problem, as outlined in equations (4) and (5), from the recursive objective in equation (2) by relaxing ReLU outputs as a convex projection, is both lucid and persuasive. Furthermore, the decomposition of the problem into three distinct components - an inverse problem concerning activations, a least-squares problem related to weights, and a final classification problem involving soft-max weights - offers a fresh and enlightening perspective on deep learning. This novel formulation opens up several intriguing research pathways, including the generalization of ReLUs, exploration of more complex connectivity patterns through shortcuts (as hinted at by the utilized architecture), and the introduction of sparsity in network connectivity.
Regarding optimization, the authors astutely note the challenges inherent in guaranteeing the convergence of a straightforward alternating optimization scheme, such as ADMM, and instead opt for a block-coordinate descent algorithm, which is both easier to analyze and presented in Algorithm 1, with a detailed analysis in Section 4. While the derivation of a convergent algorithm is a significant positive aspect, it also underscores that this work may be viewed as an initial, somewhat preliminary step in fully leveraging the potential of the formulated problem. 
The paper includes a proof of concept using MNIST data in Section 3. However, the experimental section is somewhat underwhelming, relying on a single dataset and lacking a comprehensive comparison between different implementation environments (e.g., MATLAB vs. Python). The analysis is also limited, which necessitates a more thorough quantitative (in terms of runtime and solution quality) and qualitative investigation to bolster the manuscript's conclusions. Specifically, Figure 4, which presents a percentage pie chart, is perplexing and could be revised for clarity. To enhance the overall quality of the paper, a more extensive evaluation would be highly beneficial in the final version.