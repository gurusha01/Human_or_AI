The authors propose a regularization technique that promotes low-rank weight matrices during network training, thereby enhancing compression and enabling explicit rank reduction in post-processing, which in turn decreases the number of operations required for inference. The paper appears to be well-written and explores an intriguing topic. However, I have several concerns: firstly, the authors fail to mention variational inference, which also achieves explicit network compression by reducing the description length and can be utilized to prune weights post-training, as detailed in 'Practical Variational Inference for Neural Networks'. More broadly, virtually any regularizer implements a form of implicit 'compression-aware training', as they facilitate simpler models that generalize better and can often be used for post-hoc network pruning. For instance, networks trained with l1 or l2 regularization typically yield many weights close to 0, which can be removed without significantly impacting performance. It is essential to clarify this point, particularly since the authors employ an l2 term alongside their custom regularizer during training. Furthermore, they do not seem to compare the effectiveness of their regularizer with previous low-rank post-processing methods or other regularizers used in prior work. These concerns could be addressed by providing additional baseline results in the experimental section, demonstrating that training with this specific regularizer leads to a superior accuracy-compression tradeoff compared to other approaches. I found the results somewhat challenging to interpret and may have overlooked certain aspects; the graph I would have liked to see is a set of curves illustrating accuracy versus compression ratio (in terms of parameters or MACs) rather than accuracy against the regularization term's strength. This graph would enable a direct comparison of the proposed approach with previous regularizers and compressors.