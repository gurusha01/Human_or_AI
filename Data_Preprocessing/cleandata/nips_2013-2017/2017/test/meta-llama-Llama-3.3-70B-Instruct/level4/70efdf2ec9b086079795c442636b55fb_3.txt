This paper presents a novel approach to sequence learning using Tensorized LSTMs, which represent hidden layers as tensors and utilize cross-layer memory cell convolution to achieve efficiency and effectiveness. The model formulation is clear and well-defined. The experimental results demonstrate the usefulness of the proposed method.
However, despite the paper being well-written, I have several concerns and questions that need to be addressed. If the authors can provide clarification on these points in their rebuttal, I would reconsider my final decision.
1. I am confused by the description in Section 2.1, where the authors explain how to expand the network using convolution (lines 65-73). The text states that "P is akin to the number of stacked hidden layers," and the model "locally-connects" along the P direction to share parameters. In my understanding, this approach seems to deepen the network rather than widen it, as increasing P (the number of hidden layers) does not incur additional parameters in the convolution. Similarly, in lines 103-104, it is mentioned that tRNN can be "widened without additional parameters by increasing the tensor size P," which appears to be conceptually equivalent to increasing the number of hidden layers in sRNN, thereby deepening the network, not widening it.
2. The authors claim to deepen the network using delayed outputs (Section 2.2) by setting up the network depth using the parameter L. However, as shown in Equation 9, L is determined by P and K, which means that the network depth cannot be set as a free parameter. In practice, it is likely that P and K would be pre-set before experiments, and L would be derived from Equation 9. Therefore, it seems like an overstatement in lines 6-10, which suggests that the width and depth of the network can be freely set up.
3. The authors assert that the proposed memory cell convolution can prevent gradient vanishing or exploding (line 36), but this claim is not theoretically or empirically verified. Furthermore, the terms "gradient vanishing" and "exploding" are not mentioned again in the text, which raises questions about the validity of this claim.
4. In the experiments, the authors compared various tLSTM variants across several dimensions, including tensor shape (2D or 3D), normalization (no normalization, LN, or CN), memory cell convolution (yes or no), and feedback connections (yes or no), resulting in a total of 24 possible combinations. However, only six combinations are selected for comparison (lines 166-171). While it is understandable that comparing too many methods can be cumbersome, it would be interesting to explore other variants, such as 2D tLSTM+CN. Additionally, organizing the experiments into groups, such as one for normalization strategies, one for memory cell convolution, and one for feedback connections, might provide more insights and clarity.