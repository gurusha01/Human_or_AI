This paper presents an engaging discussion on the topic of clustering data points through crowdsourcing, where the goal is to recover the original labels of data points based on responses to similar cluster queries. The authors propose theoretical bounds on the number of queries required to achieve this, considering both noisy and noiseless responses. The paper's primary contribution lies in its theoretical foundations, supplemented by empirical simulations. The writing is clear and accessible, effectively conveying the theoretical concepts to a broad machine learning audience. Notably, the authors provide a compelling explanation of the connection to information theory in deriving these bounds.
However, several points warrant further consideration to enhance the paper's quality:
(1) The concept of AND queries, while theoretically intriguing, appears to contradict the fundamental premise of crowdsourcing. Specifically, AND queries seem to provide the cluster label as part of the response, rather than merely indicating whether the data points belong to the same cluster. This raises questions about the necessity of such queries, as directly asking for a data point's label could be more efficient. Clarification on this aspect is essential to fully appreciate the paper's contributions.
(2) It would be beneficial to simplify the expressions for the bounds outlined in Theorems 1 through 4, if possible, without compromising their essence. This could enhance the readability and understanding of the theoretical results.
(3) The experimental section appears to focus exclusively on AND queries. If this is the case, the validity of the experimental study hinges on the resolution of the concerns raised in point (1). Otherwise, the experimental findings may not fully support the paper's claims.
Given the authors' satisfactory response to these concerns, I am inclined to vote for the acceptance of this paper.