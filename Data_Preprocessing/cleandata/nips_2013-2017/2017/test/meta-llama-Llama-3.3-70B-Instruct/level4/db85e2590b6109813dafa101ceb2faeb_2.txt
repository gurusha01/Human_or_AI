This paper presents a novel approach to deep model compression via a low-rank regularizer during the training phase. The manuscript is well-structured and effectively conveys its motivation. Nevertheless, several aspects warrant further consideration:
1. The originality of the work is somewhat constrained, as the technical components bear a strong resemblance to existing research in the field.
2. The experimental evaluation could be enhanced in the following ways:
(1) Sensitivity analysis of parameters: As illustrated in Figure 1, the proposed method's performance (with τ = 1 and λ ≠ 0) closely aligns with that of [2] (where τ = 0 and λ ≠ 0). For alternative τ settings, while the compression rate improves, the accuracy decreases.
(2) Evaluation on larger models: A comparative analysis with [2] would be beneficial to demonstrate the proposed method's efficacy. Additionally, comparing the results with other state-of-the-art model compression techniques, such as [18], would provide valuable insights.