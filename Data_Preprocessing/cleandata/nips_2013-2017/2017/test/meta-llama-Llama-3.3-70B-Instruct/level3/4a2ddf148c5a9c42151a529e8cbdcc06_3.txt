This paper proposes a novel Tikhonov regularization approach for training deep neural networks (DNNs) with ReLU activation functions. The authors develop a block coordinate descent (BCD) algorithm to solve the resulting block multi-convex minimization problem, which is proven to converge globally to stationary points with R-linear convergence rate of order one. The paper provides a thorough analysis of the algorithm's convergence properties and demonstrates its effectiveness through experiments on the MNIST dataset.
The technical contribution of the paper appears sound, with a clear and well-structured presentation of the proposed approach. The authors provide a detailed analysis of the algorithm's convergence properties, including a proof of global convergence and R-linear convergence rate. The experimental results demonstrate the effectiveness of the proposed algorithm, showing that it can achieve better test-set error rates than traditional SGD-based solvers.
However, the paper could benefit from some improvements in presentation and clarity. There are several typos and grammatical errors throughout the paper, which can make it difficult to follow at times. Additionally, the abstract could be improved to provide a clearer summary of the paper's contributions and main results. Specifically, the definition of the condition number and the use of Îº could be clarified for better understanding.
In terms of originality, the paper's approach is novel and distinct from other works in the field. The use of Tikhonov regularization and BCD algorithm is a unique combination that has not been explored before in the context of DNN training. However, the paper could benefit from a more detailed comparison with other related works, such as [4], to clarify the advantages and limitations of the proposed approach.
Overall, the paper is well-written and provides a significant contribution to the field of deep learning. With some improvements in presentation and clarity, it has the potential to be a strong candidate for acceptance at NIPS.
Arguments pro acceptance:
* The paper proposes a novel and effective approach for training DNNs with ReLU activation functions.
* The algorithm's convergence properties are thoroughly analyzed and proven.
* The experimental results demonstrate the effectiveness of the proposed algorithm.
Arguments con acceptance:
* The paper could benefit from improvements in presentation and clarity.
* The comparison with other related works could be more detailed and comprehensive.
* The abstract could be improved to provide a clearer summary of the paper's contributions and main results.