This paper proposes a novel Tikhonov regularization for training deep neural networks (DNNs) with ReLU activation functions. The authors reformulate the network training as a block multi-convex minimization problem and propose a block coordinate descent (BCD) based algorithm to solve it. The algorithm is proven to converge globally to stationary points with R-linear convergence rate of order one.
The paper is well-structured and easy to follow, with clear explanations of the proposed method and its theoretical foundations. The authors provide a thorough analysis of the convergence properties of the algorithm and demonstrate its effectiveness through experiments on the MNIST dataset.
However, there are several areas that require clarification and improvement. Firstly, the motivation behind using Tikhonov regularization in the proposed algorithm is unclear, and its advantages over other regularizations need to be explicitly stated. Additionally, the decomposition of the problem into three sub-problems lacks explanation and justification, particularly on why it won't suffer from vanishing gradient issues.
The derivation from Eqn. (4) to Eqn. (5) is also unclear, specifically the formula and properties of matrix Q, which needs further explanation. A clear mathematical definition of the Tikhonov regularized inverse problem is missing and should be provided. The claim that the inverse sub-problem resolves the vanishing gradient issue lacks theoretical justification and empirical evidence.
Furthermore, the similarity between the "optimal" output features and target propagation needs more detailed explanation and justification. The definition of \mathcal P on line 246 is unclear and requires clarification. The results in Fig. 5 are inconsistent, as the weight matrix of the 4th layer remains dense despite the algorithm learning much sparser networks.
The prefix "Q" on line 252 is undefined and needs explanation. There is a missing word "dense" on line 284, which affects the clarity of the text. The reference format is confusing and should be revised for better readability.
The choice of DNN structure in Figure 1 is unclear, and the experimental results are not state-of-the-art, raising doubts about the applicability of the proposed method to real DNNs. Overall, while the paper presents an interesting approach to training DNNs, it requires significant clarification and improvement to demonstrate its effectiveness and robustness.
Arguments for acceptance:
* The paper proposes a novel Tikhonov regularization for training DNNs, which is a significant contribution to the field.
* The authors provide a thorough analysis of the convergence properties of the algorithm, which is a strong theoretical foundation.
* The experimental results demonstrate the effectiveness of the algorithm in learning both dense and sparse networks.
Arguments against acceptance:
* The motivation behind using Tikhonov regularization is unclear, and its advantages over other regularizations need to be explicitly stated.
* The decomposition of the problem into three sub-problems lacks explanation and justification, particularly on why it won't suffer from vanishing gradient issues.
* The experimental results are not state-of-the-art, raising doubts about the applicability of the proposed method to real DNNs.
* The paper requires significant clarification and improvement to demonstrate its effectiveness and robustness.