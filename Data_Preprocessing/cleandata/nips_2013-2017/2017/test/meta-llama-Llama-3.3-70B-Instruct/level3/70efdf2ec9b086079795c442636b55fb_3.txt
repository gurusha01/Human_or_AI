This paper proposes a novel approach to increasing the capacity of Long Short-Term Memory (LSTM) networks, called Tensorized LSTM (tLSTM). The authors introduce a way to widen and deepen the LSTM network without significantly increasing the parameter number or runtime. The key idea is to represent the hidden states as tensors and update them via a cross-layer convolution, allowing for more flexible parameter sharing and efficient widening of the network.
The paper is well-written and provides a clear explanation of the proposed method. The authors also provide a thorough analysis of the related work, highlighting the advantages of tLSTM over other approaches such as Convolutional LSTMs and Deep LSTMs. The experimental results demonstrate the effectiveness of tLSTM on various sequence learning tasks, including language modeling, algorithmic tasks, and image classification.
However, there are some concerns and questions that need to be addressed. Firstly, the concept of widening the network with convolution is not entirely clear, as increasing the number of hidden layers does not necessarily incur more parameters. It seems that the authors are deepening the network instead of widening it. Secondly, the claim that the proposed memory cell convolution prevents gradient vanishing/exploding is not theoretically or empirically verified, and further explanation is needed to support this claim.
Additionally, the selection of only six combinations of tLSTM variants for comparison in experiments seems limited, and exploring more combinations, such as 2D tLSTM+CN, could be beneficial. The authors also mention that the runtime of tLSTM is largely unaffected by the depth, but it would be interesting to see a more detailed analysis of the computational complexity of tLSTM.
In terms of quality, the paper is well-organized and easy to follow, but some of the notations and equations could be better explained. The originality of the paper is high, as it proposes a novel approach to increasing the capacity of LSTM networks. The significance of the paper is also high, as it demonstrates the potential of tLSTM on various sequence learning tasks.
Overall, this paper presents a promising approach to improving the capacity of LSTM networks, but some concerns and questions need to be addressed to further verify the effectiveness of the proposed method. 
Arguments pro acceptance:
- The paper proposes a novel approach to increasing the capacity of LSTM networks.
- The experimental results demonstrate the effectiveness of tLSTM on various sequence learning tasks.
- The paper provides a thorough analysis of the related work and highlights the advantages of tLSTM.
Arguments con acceptance:
- The concept of widening the network with convolution is not entirely clear.
- The claim that the proposed memory cell convolution prevents gradient vanishing/exploding is not theoretically or empirically verified.
- The selection of tLSTM variants for comparison in experiments seems limited.