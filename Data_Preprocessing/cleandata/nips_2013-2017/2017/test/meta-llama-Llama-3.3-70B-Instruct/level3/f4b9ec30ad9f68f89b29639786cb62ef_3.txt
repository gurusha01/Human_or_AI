This paper introduces a novel screening rule for `1-regularized Ising model estimation, which is a necessary and sufficient condition for exact blockwise structure recovery. The screening rule is simple and closed-form, making it easily applicable to large-scale problems. The authors demonstrate the effectiveness of the screening rule through experiments on both synthetic and real-world data, showing that it can provide significant speedups when combined with various optimization algorithms.
The paper is well-written and structured, with rigorous theoretical argumentations. The authors provide a clear explanation of the screening rule and its implications, as well as a detailed analysis of its performance in different scenarios. The experimental results are convincing, demonstrating the potential of the screening rule to improve the efficiency of learning in large-scale problems.
However, there are some limitations to the paper. The screening rule is limited to updating global parameters from stochastic gradients calculated on minibatches and cannot be generalized to distribute operations across neurons in deep neural nets. Additionally, the experiments are conducted on small data sets, which may not accurately represent the real effect of the algorithm. The authors should report results on larger data sets or specific applications where the algorithm is necessary.
The paper also lacks citation of earlier pioneer work on distributed machine learning, and the authors should provide more convincing examples of the algorithm's usefulness in real-world applications. The reported results shed doubt on the usefulness of the proposed approach, and the authors have not provided sufficient evidence to justify the use of the screening rule in machine learning models.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and adequately informs the reader.
In terms of originality, the paper presents a novel screening rule for `1-regularized Ising model estimation, which is a significant contribution to the field. The approach is new, and the authors demonstrate its effectiveness in improving the efficiency of learning in large-scale problems.
In terms of significance, the paper addresses a difficult problem in a better way than previous research. The screening rule has the potential to improve the efficiency of learning in large-scale problems, making it a significant contribution to the field. However, the authors should provide more convincing examples of the algorithm's usefulness in real-world applications to demonstrate its significance.
Overall, the paper is well-written, and the authors demonstrate the effectiveness of the screening rule in improving the efficiency of learning in large-scale problems. However, there are some limitations to the paper, and the authors should address these limitations to strengthen their claims. 
Arguments pro acceptance:
- The paper introduces a novel screening rule for `1-regularized Ising model estimation.
- The screening rule is simple and closed-form, making it easily applicable to large-scale problems.
- The authors demonstrate the effectiveness of the screening rule through experiments on both synthetic and real-world data.
Arguments con acceptance:
- The screening rule is limited to updating global parameters from stochastic gradients calculated on minibatches.
- The experiments are conducted on small data sets, which may not accurately represent the real effect of the algorithm.
- The paper lacks citation of earlier pioneer work on distributed machine learning.
- The reported results shed doubt on the usefulness of the proposed approach.