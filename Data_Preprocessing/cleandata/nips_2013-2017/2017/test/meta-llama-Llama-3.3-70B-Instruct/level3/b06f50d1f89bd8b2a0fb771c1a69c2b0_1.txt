This paper proposes a novel approach to training deep neural networks (DNNs) using a Tikhonov regularization technique. The authors reformulate the traditional DNN training objective as a block multi-convex minimization problem, which can be solved using a block coordinate descent (BCD) algorithm. The BCD algorithm is proven to converge globally to stationary points with R-linear convergence rate of order one.
The paper is well-organized and clearly written, with a thorough introduction to the background and motivation of the work. The authors provide a detailed description of the proposed approach, including the Tikhonov regularization technique and the BCD algorithm. The theoretical analysis of the algorithm's convergence is also well-presented.
The experimental results demonstrate the effectiveness of the proposed approach, showing that it can outperform traditional stochastic gradient descent (SGD) based solvers in terms of test-set error rates. The authors also provide a comparison with other state-of-the-art methods, such as ADMM, and show that their approach can achieve better results.
However, there are some concerns and areas for improvement. Firstly, the paper assumes that the Tikhonov matrix can be computed efficiently, which may not be the case for large-scale networks. Secondly, the authors do not provide a clear analysis of the computational complexity of the BCD algorithm, which is an important consideration for practical applications. Finally, the paper could benefit from more extensive experimental evaluations, including comparisons with other optimization methods and a more detailed analysis of the hyperparameter settings.
In terms of the conference guidelines, the paper meets most of the criteria. The paper is well-written, and the authors provide a clear and concise summary of the main ideas. The paper is also well-organized, with a logical flow of ideas and a clear structure. However, the paper could benefit from more attention to the clarity and readability of the mathematical notation and derivations.
Overall, I would rate this paper as a solid contribution to the field of deep learning, with a well-presented and well-motivated approach to training DNNs. While there are some areas for improvement, the paper demonstrates a good understanding of the underlying theory and provides a clear and concise presentation of the results.
Arguments for acceptance:
* The paper proposes a novel approach to training DNNs using Tikhonov regularization, which is a well-motivated and well-presented idea.
* The authors provide a thorough theoretical analysis of the algorithm's convergence, which is an important consideration for practical applications.
* The experimental results demonstrate the effectiveness of the proposed approach, showing that it can outperform traditional SGD based solvers.
Arguments against acceptance:
* The paper assumes that the Tikhonov matrix can be computed efficiently, which may not be the case for large-scale networks.
* The authors do not provide a clear analysis of the computational complexity of the BCD algorithm, which is an important consideration for practical applications.
* The paper could benefit from more extensive experimental evaluations, including comparisons with other optimization methods and a more detailed analysis of the hyperparameter settings.