This paper proposes a novel Tikhonov regularized multi-convex formulation for training deep neural networks (DNNs) with ReLU activation functions. The authors develop a block coordinate descent (BCD) algorithm to solve this formulation, which is proven to converge globally to stationary points with R-linear convergence rate of order one. The paper also presents empirical results on the MNIST dataset, demonstrating the effectiveness and efficiency of the proposed algorithm.
The strengths of this paper include its novel formulation of the DNN training problem, which allows for the development of a convergent BCD algorithm. The authors provide a thorough theoretical analysis of the algorithm's convergence properties, including a proof of global convergence and R-linear convergence rate. The empirical results are also promising, showing that the proposed algorithm can outperform traditional SGD-based solvers in terms of test-set error rates.
However, there are some weaknesses and limitations to the paper. The description of previous results is sometimes confusing, with inaccuracies in referencing prior work. The simulation is limited to s-sparse signals, which may not fully test the aspects of the results that go beyond existing work. Additionally, the dependence on the condition number of the matrix is not clearly tested. There are also some minor comments on grammar, terminology, and clarity, including issues with the title, property names, and algorithm description.
In terms of the conference guidelines, this paper meets some of the criteria for a good scientific contribution. The paper is technically sound, with a clear and well-organized presentation of the theoretical results. The authors provide a thorough analysis of the algorithm's convergence properties, which is a significant contribution to the field. However, the paper could be improved in terms of clarity and presentation, with some sections being difficult to follow due to notation and terminology issues.
Overall, I would rate this paper as "marginally above threshold" for acceptance, pending revisions to address the minor comments and clarify the presentation. The authors have addressed some points in their rebuttal, but the impact of the contribution is still considered borderline. With revisions, this paper has the potential to make a significant contribution to the field of deep learning.
Arguments pro acceptance:
* Novel formulation of the DNN training problem
* Convergent BCD algorithm with proven global convergence and R-linear convergence rate
* Promising empirical results on the MNIST dataset
Arguments con acceptance:
* Confusing description of previous results
* Limited simulation to s-sparse signals
* Dependence on condition number of the matrix not clearly tested
* Minor comments on grammar, terminology, and clarity.