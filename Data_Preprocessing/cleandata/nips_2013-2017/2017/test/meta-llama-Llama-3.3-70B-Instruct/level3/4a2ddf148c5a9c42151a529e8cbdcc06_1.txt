This paper proposes a novel Tikhonov regularization approach for training deep neural networks (DNNs) with ReLU activation functions. The authors derive a block multi-convex formulation for the training objective and develop a block coordinate descent (BCD) algorithm to solve it. The algorithm is proven to converge globally to stationary points with R-linear convergence rate of order one.
The paper's main strengths lie in its theoretical contributions, particularly the derivation of the Tikhonov regularization matrix and the proof of convergence for the BCD algorithm. The authors also provide empirical results demonstrating the effectiveness of their approach on the MNIST dataset, showing that their algorithm can outperform traditional SGD-based solvers.
However, there are some weaknesses in the paper. Firstly, the presentation of problem examples is brief and lacks concrete arguments and specific details, making it difficult to fully understand the implications of the proposed approach. Additionally, the description of simulations is too brief, making it hard to relate simulation setups to theoretical results and understand the relevance of parameters like function F(x) and condition numbers.
The paper's originality is moderate, as it extends existing arguments on support recovery via hard thresholding to partial hard thresholding, a non-trivial contribution to the field. The theoretical support-recovery guarantees for the partial hard thresholding algorithm are significant, as they encompass special cases like conventional hard thresholding and orthogonal matching pursuit with replacement.
Some minor errors and suggestions for improvement include corrections to grammar, punctuation, and consistency in notation. Additionally, the paper could benefit from more detailed explanations of the problem setup and the Tikhonov regularization approach, as well as more comprehensive empirical results to demonstrate the effectiveness of the proposed algorithm.
Arguments for acceptance:
* The paper makes a significant theoretical contribution to the field of deep learning, particularly in the development of a novel Tikhonov regularization approach.
* The authors provide a proof of convergence for the BCD algorithm, which is a notable achievement.
* The empirical results demonstrate the effectiveness of the proposed approach on the MNIST dataset.
Arguments against acceptance:
* The presentation of problem examples is brief and lacks concrete arguments and specific details.
* The description of simulations is too brief, making it hard to relate simulation setups to theoretical results.
* The paper's originality is moderate, as it builds upon existing work in the field.
Overall, I would recommend accepting this paper, as its theoretical contributions and empirical results demonstrate its significance and potential impact on the field of deep learning. However, the authors should address the minor errors and suggestions for improvement to enhance the clarity and comprehensiveness of the paper.