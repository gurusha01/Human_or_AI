This paper proposes a novel Tikhonov regularization approach for training deep neural networks (DNNs) with ReLU activation functions. The authors reformulate the network training as a block multi-convex minimization problem and develop a block coordinate descent (BCD) algorithm to solve it. The algorithm is proven to converge globally to stationary points with R-linear convergence rate of order one.
The paper is well-structured and easy to follow, with clear explanations of the proposed approach and its theoretical foundations. The authors provide a thorough review of related work, highlighting the differences between their approach and existing methods. The experimental results demonstrate the effectiveness of the proposed algorithm, showing that it can outperform traditional SGD-based solvers in terms of convergence and test-time performance.
One of the strengths of the paper is its clear and concise presentation of the theoretical results, including the proof of global convergence and R-linear convergence rate. The authors also provide a detailed analysis of the algorithm's computational complexity and its suitability for parallel computing.
However, there are some limitations and areas for improvement. The experimental results are limited to a single dataset (MNIST) and a specific network architecture, which may not be representative of more complex real-world scenarios. Additionally, the authors do not provide a detailed comparison with other state-of-the-art methods, such as ADMM-based approaches, which could provide a more comprehensive understanding of the proposed algorithm's strengths and weaknesses.
Another potential limitation is the lack of discussion on the choice of regularization parameters, which can significantly impact the performance of the algorithm. The authors simply set the regularization parameters to a fixed value (Î³n = 0.1) without providing any justification or exploration of the impact of different values on the results.
In terms of originality, the paper proposes a novel approach to training DNNs, which combines Tikhonov regularization with block coordinate descent. While the individual components of the approach are not new, their combination and application to DNN training is innovative and contributes to the existing literature.
Overall, the paper is well-written, and the proposed approach shows promise for improving the training of DNNs. However, further experimentation and comparison with other state-of-the-art methods are necessary to fully evaluate the algorithm's effectiveness and potential impact on the field.
Arguments pro acceptance:
* The paper proposes a novel approach to training DNNs, which combines Tikhonov regularization with block coordinate descent.
* The algorithm is proven to converge globally to stationary points with R-linear convergence rate of order one.
* The experimental results demonstrate the effectiveness of the proposed algorithm, showing that it can outperform traditional SGD-based solvers.
Arguments con acceptance:
* The experimental results are limited to a single dataset and a specific network architecture.
* The authors do not provide a detailed comparison with other state-of-the-art methods.
* The choice of regularization parameters is not thoroughly discussed or explored.
Recommendation: Accept with minor revisions, pending additional experimentation and comparison with other state-of-the-art methods.