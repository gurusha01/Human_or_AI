This paper presents a significant contribution to the field of statistical learning with sparsity, particularly in the context of `1-regularized Ising model estimation. The authors propose a simple closed-form screening rule that is a necessary and sufficient condition for exact blockwise structure recovery of the solution with a given regularization parameter. This screening rule is especially valuable for large-scale exploratory data analysis, where the number of variables can be thousands, and we are only interested in the relationship among a handful of variables within moderate-size clusters for interpretability.
The paper is well-organized, and the background on Ising models and graphical interpretation is extensive. The authors provide a clear motivation to explore fault-tolerance in distributed learning under pessimistic conditions. The experiments demonstrate the efficiency and insights gained from the introduction of the screening rule, particularly when combined with various optimization algorithms.
However, there are some areas that require improvement. The background on SGD is extensive but may be unnecessary for the target audience and could be replaced with more technical content. The experiments are not well-matched to the paper's main setting and could be strengthened by using more modern datasets and models, such as deep convolutional nets for CIFAR-10. The choice of mini-batch size for correct workers appears arbitrary and requires explanation.
The "cost of resilience" experiment is incomplete, as it only reports error at an early phase of training and does not show behavior closer to convergence. The "multi-Krum" experiment makes a strongly omniscient choice of parameter m and would be more complete with a plot showing the trade-off curve for different values of m.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and adequately informs the reader.
The originality of the paper lies in the proposal of a screening rule for `1-regularized Ising model estimation, which is a novel combination of familiar techniques. The related work is adequately referenced, and the authors demonstrate how their work differs from previous contributions.
The significance of the paper lies in its potential to provide drastic speedups for learning when combined with various optimization algorithms. The results are important, and other people, both practitioners and researchers, are likely to use these ideas or build on them. The paper addresses a difficult problem in a better way than previous research and advances the state of the art in a demonstrable way.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of statistical learning with sparsity.
* The screening rule is a necessary and sufficient condition for exact blockwise structure recovery, which is a valuable result.
* The experiments demonstrate the efficiency and insights gained from the introduction of the screening rule.
* The paper is well-organized, clearly written, and adequately informs the reader.
Arguments against acceptance:
* The background on SGD is extensive but may be unnecessary for the target audience.
* The experiments are not well-matched to the paper's main setting and could be strengthened.
* The choice of mini-batch size for correct workers appears arbitrary and requires explanation.
* The "cost of resilience" experiment is incomplete, and the "multi-Krum" experiment makes a strongly omniscient choice of parameter m.