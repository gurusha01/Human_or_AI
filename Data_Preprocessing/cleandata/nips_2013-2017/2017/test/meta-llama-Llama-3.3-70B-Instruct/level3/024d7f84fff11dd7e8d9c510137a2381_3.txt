This paper presents an accelerated variant of the Min-Sum message-passing protocol for solving consensus problems in distributed optimization. The authors establish rates of convergence for the Min-Sum Splitting algorithm and show connections to lifted Markov chains and multi-step methods in convex optimization. The paper is well-written, but there are several typos and grammatical mistakes that need to be corrected.
The main contributions of the paper include providing analysis for the Min-Sum splitting algorithm, designing a Min-Sum protocol for consensus problems, and showing connections to lifted Markov chains and multi-step methods in convex optimization. The authors prove that a proper choice of the tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated convergence rates, matching the rates obtained by shift-register methods.
However, the paper lacks justification for the importance of the result for the Machine Learning community and should clearly state why the approach is of interest in this context. Additionally, the authors should provide intuition behind using the auxiliary process in the Min-Sum Splitting method and discuss the potential to extend the framework to solve more general consensus problems in Machine Learning.
The proofs of Propositions 1 and 2, and Theorem 1 appear to be correct, but some minor comments are made regarding the use of terms like "Laplacian" and "Laplace multipliers". The paper is missing several references, including works on (sub)gradient methods for consensus optimization.
Overall, the paper presents a significant contribution to the field of distributed optimization, but could benefit from additional context, clarification, and references to make it more accessible to a broader audience.
Arguments pro acceptance:
- The paper presents a novel accelerated variant of the Min-Sum message-passing protocol for solving consensus problems in distributed optimization.
- The authors establish rates of convergence for the Min-Sum Splitting algorithm and show connections to lifted Markov chains and multi-step methods in convex optimization.
- The paper provides a thorough analysis of the Min-Sum Splitting algorithm and its connections to other methods in the field.
Arguments con acceptance:
- The paper lacks justification for the importance of the result for the Machine Learning community.
- The authors should provide intuition behind using the auxiliary process in the Min-Sum Splitting method and discuss the potential to extend the framework to solve more general consensus problems in Machine Learning.
- The paper is missing several references, including works on (sub)gradient methods for consensus optimization.