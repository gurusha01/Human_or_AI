This paper proposes a novel Tikhonov regularization for training deep neural networks (DNNs) with ReLU activation functions, which leads to a block multi-convex minimization problem. The authors then develop a block coordinate descent (BCD) algorithm to solve this problem, proving its global convergence to stationary points with R-linear convergence rate of order one. The paper also presents empirical results on the MNIST dataset, demonstrating the effectiveness and efficiency of the proposed algorithm.
However, there are several concerns with the paper. Firstly, the convergence proof for the BCD algorithm is flawed due to the assumption of a limit point and the possibility of non-unique limit points. The authors' rebuttal does not address this concern, and the proof is invalid due to circular reasoning. Furthermore, the algorithm's convergence to a single limit point is not guaranteed.
Additionally, the new formulation of the feed-forward neural network optimization problem is interesting, but its usefulness is limited due to the increased number of variables, which can lead to spatial unfeasibility for large-scale problems. The problem in line 149 may not be convex with respect to W, requiring further elaboration to clarify this point.
The discussion of saddle points in the introduction is not relevant to the manuscript, and the authors should consider removing it. The reference format used in the paper is non-standard, and the comparison with Caffe solvers in terms of time is meaningless due to differences in implementation platforms. The comparison of objective values in Figure 3(a) is not valid, as the two methods are solving different problems.
The assumption of compact sets U, V, and W is incorrect, as U is defined as the nonnegative half-space, which is unbounded and non-compact. The concept of "the" limit point when t = âˆž is flawed, and the authors' proof is invalid due to circular reasoning.
In terms of the conference guidelines, the paper does not meet the criteria for quality, clarity, originality, and significance. The paper lacks technical soundness, and the claims are not well-supported by theoretical analysis or experimental results. The paper is not clearly written, and the organization is poor. The ideas presented are not novel, and the paper does not advance the state of the art in a demonstrable way.
Arguments for acceptance:
* The paper proposes a novel Tikhonov regularization for training DNNs, which is an interesting idea.
* The empirical results on the MNIST dataset demonstrate the effectiveness and efficiency of the proposed algorithm.
Arguments against acceptance:
* The convergence proof for the BCD algorithm is flawed, and the algorithm's convergence to a single limit point is not guaranteed.
* The new formulation of the feed-forward neural network optimization problem is limited due to the increased number of variables.
* The paper lacks technical soundness, and the claims are not well-supported by theoretical analysis or experimental results.
* The paper is not clearly written, and the organization is poor.
* The ideas presented are not novel, and the paper does not advance the state of the art in a demonstrable way.