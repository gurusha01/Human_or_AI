This paper presents a novel approach to optimizing deep neural networks (DNNs) by formulating the training objective as a Tikhonov regularized problem and decomposing it into inverse, least-squares, and classification problems. The authors propose a block-coordinate descent (BCD) algorithm for optimization, which is proven to converge globally to stationary points with R-linear convergence rate of order one. The paper provides a thorough theoretical analysis of the algorithm's convergence properties and demonstrates its effectiveness through experiments on the MNIST dataset.
The strengths of the paper include its novel formulation of the DNN training problem, which provides a fresh perspective on deep learning. The BCD algorithm is well-motivated and its convergence properties are thoroughly analyzed. The experimental results demonstrate the algorithm's effectiveness in learning both dense and sparse networks, and its potential to outperform traditional stochastic gradient descent (SGD) based solvers.
However, the paper also has some weaknesses. The experimental section is limited to a single dataset (MNIST) and a limited analysis of the results. The authors acknowledge that the algorithm's computational complexity is cubic in the input dimension, which may limit its scalability to larger datasets. Additionally, the paper could benefit from a more extensive comparison with other state-of-the-art optimization algorithms for DNNs.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, and originality. The paper is technically sound, well-written, and provides a clear explanation of the proposed approach. The authors demonstrate a good understanding of the related work and provide a thorough analysis of the algorithm's convergence properties. The paper's significance is also evident, as it provides a novel perspective on deep learning and has the potential to improve the optimization of DNNs.
Arguments for acceptance:
* The paper presents a novel and well-motivated approach to optimizing DNNs.
* The BCD algorithm is thoroughly analyzed and its convergence properties are proven.
* The experimental results demonstrate the algorithm's effectiveness in learning both dense and sparse networks.
* The paper provides a fresh perspective on deep learning and has the potential to improve the optimization of DNNs.
Arguments against acceptance:
* The experimental section is limited to a single dataset and a limited analysis of the results.
* The algorithm's computational complexity is cubic in the input dimension, which may limit its scalability to larger datasets.
* The paper could benefit from a more extensive comparison with other state-of-the-art optimization algorithms for DNNs.
Overall, I recommend accepting the paper, as its strengths outweigh its weaknesses. The paper provides a novel and well-motivated approach to optimizing DNNs, and its experimental results demonstrate its effectiveness. With some revisions to address the limitations of the experimental section and provide a more extensive comparison with other optimization algorithms, the paper has the potential to make a significant contribution to the field of deep learning.