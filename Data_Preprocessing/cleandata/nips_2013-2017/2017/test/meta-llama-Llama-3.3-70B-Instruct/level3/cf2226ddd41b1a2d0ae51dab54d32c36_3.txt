This paper presents a novel approach to increasing the capacity of Long Short-Term Memory (LSTM) networks, called the Tensorized LSTM (tLSTM). The key idea is to represent the hidden states as higher-dimensional tensors, which allows for more flexible parameter sharing and efficient widening of the network without increasing the number of parameters. The authors also propose a cross-layer convolution to perform deep computations, which enables the network to be deepened with little additional runtime.
The paper is well-written, and the authors provide a clear and detailed explanation of the proposed approach. The experimental results demonstrate the effectiveness of the tLSTM on several challenging sequence learning tasks, including language modeling, algorithmic tasks, and image classification. The results show that the tLSTM outperforms other popular approaches, such as stacked LSTMs and convolutional LSTMs, in terms of performance and runtime.
The strengths of the paper include:
* The proposal of a novel and efficient approach to increasing the capacity of LSTMs, which has the potential to improve the state-of-the-art in sequence learning tasks.
* The thorough experimental evaluation of the tLSTM on several tasks, which demonstrates its effectiveness and efficiency.
* The clear and detailed explanation of the proposed approach, which makes it easy to understand and implement.
The weaknesses of the paper include:
* The lack of a detailed analysis of the computational complexity of the tLSTM, which would be helpful in understanding its efficiency.
* The limited comparison with other approaches, such as attention-based models and graph-based models, which would be helpful in understanding the strengths and weaknesses of the tLSTM.
Overall, I recommend accepting this paper due to its interesting problem, theoretical results, and well-supported algorithm. The paper has the potential to make a significant contribution to the field of sequence learning and neural networks.
Arguments pro acceptance:
* The paper proposes a novel and efficient approach to increasing the capacity of LSTMs.
* The experimental results demonstrate the effectiveness of the tLSTM on several challenging sequence learning tasks.
* The paper is well-written, and the authors provide a clear and detailed explanation of the proposed approach.
Arguments con acceptance:
* The lack of a detailed analysis of the computational complexity of the tLSTM.
* The limited comparison with other approaches, such as attention-based models and graph-based models.
Quality: 9/10
Clarity: 9/10
Originality: 8/10
Significance: 9/10
Overall score: 8.5/10
Recommendation: Accept.