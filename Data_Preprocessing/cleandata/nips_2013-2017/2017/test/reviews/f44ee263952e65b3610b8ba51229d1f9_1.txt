This paper proposes a non-parametric method to cache previously seen contexts for language modelling. The basic idea is that at each point, the k nearest-neighbour states from previously seen contexts are retrieved, and a kernel density estimation method applied to generate a probability distribution over an open vocabulary. Thus, the cache model is unbounded, unlike methods such as pointer networks or continuous caches. Results demonstrate the performance of this method on language modelling with time and topic drift, over standard RNN language models.
This was a well-written paper, and the unbounded cache idea is intuitively appealing. I think that the proposed method could become useful for many tasks besides language modelling. 
I would have liked to see a comparison of this method against parametric or local cache methods such as the pointer-generator network. Also, how much slower is the proposed model at inference time? Querying 1024 nearest neighbours in order to estimate p_{cache} looks like it may be expensive.