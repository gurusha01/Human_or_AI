This is an interesting paper. I enjoyed reading it. In the paper, the authors consider the task of clustering data points via crowdsourcing wherein the problem is of recovering original labels of data points based on the answers to similar cluster queries. The authors propose bounds on the number of queries required to recover the labels considering noisy as well as noiseless answers for the queries. The main contribution of the paper is theoretical, along with some empirical simulations. The paper is well written, making it easy to understand the theoretical contributions for a general machine learning audience. Especially, the connection w.r.t. Information theory for deriving the bounds is explained nicely. 
I have the following concerns, in regards to improving the paper.
(1) While the idea of AND queries is interesting theoretically, it seems to conflict with the original motivation for crowdsourcing. In the case of an AND query, effectively, the label of the cluster is also given as part of the answer rather than just answering if the data points of interest belong to same cluster or not. If this is the case indeed, as I interpret from the paper, why would one even need to ask those similarity queries ? Why not ask the label for a data point itself ? This is my primary confusion w.r.t. the contributions in this paper.
(2) Is it possible to simplify the expressions for the bounds in Theorems 1,2,3,4, while keeping the essence.
(3) In the experiments, it seems that only AND queries are considered. If so, the experimental study is in question unless the above point (1) is clarified.
I am satisfied with the response of the authors for my questions above. So, voting for the acceptance of this paper.