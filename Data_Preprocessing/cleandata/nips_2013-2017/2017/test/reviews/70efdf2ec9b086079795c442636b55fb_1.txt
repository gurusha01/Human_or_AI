The paper explores a new way to share parameters in an RNN by using convolution inside an LSTM cell for an unstructured input sequence, and using tensors as convolution kernels and feature maps. The method also adds depth to the model by delaying the output target for a specified number of steps.
The idea is quite interesting, and is novel as far as I can tell. The authors provide clear formulation (although a rather complicated one) and provide some experimental results. 
On a real world dataset (wikipedia LM) the method seems very close to SOA, with about half the parameters.
The problems I see with this approach are:
- I find it hard to believe that meaningful high dimensional feature maps can be created for most problems, thus scaling to high dimensional tensors is questionable (The authors only try up to dimension 3)
- Using "depth in time" introduces a delay and is not suitable for streaming applications (e.g. speech)
- For high dimensional tensors the number of hyper parameters can become quite large.
Minor nit:
- Line 242, it says "Fig.3" and should be "Table 3"