This paper proposes a simple and efficient block coordinate descent (BCD) algorithm with a novel Tikhonov regularization for training both dense and sparse DNNs with ReLU. They show that the proposed BCD algorithm converges globally to a stationary point with an R-linear convergence rate of order one and performs better than all the SGD variants in experiments. However, the motivations of using Tikhonov regularization and block coordinate descent are not very clear. The technical parts are hard to follow due to the absence of many details. The presented results are far from state-of-the-art. In this sense, I am not sure whether the proposed method can be applied to real "DNNs". My detailed comments are as follows. 
Specific comments: 
1. The motivation of Tikhonov Regularization should be clarified. Why do we need to introduce Tikhonov Regularization instead of others? What is the main advantage of Tikhonov Regularization compared with other Regularizations? In the second paragraph of Section I, the authors mentioned several issues of DNN, such as the "highly non-convex", "saddle points" and "local extrema", followed by the Tikhonov Regularization. Does it mean that the proposed Tikhonov Regularizationcan address all these issues?
 
2. What is the motivation of the decomposition into three sub-problems? The authors should explain why such decomposition will not suffer from vanishing gradient.
3. From Eqn. (4) to Eqn. (5), it is hard to follow. On line 133, what is the specific formula of matrix Q(\tilde {\mathcal A})? How to confirm matrix Q(\tilde {\mathcal A}) is positive semidefinite?
4. A mathematical definition of Tikhonov regularized inverse problem should be given clearly.
5. There is a quite strong argument: "In fact, the inverse sub-problem resolve the vanishing gradient issue in traditional deep learning, because it tries to obtain the optimal solution for the output feature of each hidden layer, which are dependent on each other through the Tikhonov matrix." There are no theoretical justifications or empirical backing to verify that such inverse sub-problem can resolve the vanishing gradient issue.
6. The authors should give more details for why the "optimal" output features work similarly as target propagation.
7. On line 246, what is the definition of \mathcal P?
8. In Fig. 5, BCD-S can learn much sparser networks, why weight matrix of the 4th layer is still dense.
9. On line 252, what the prefix "Q" means?
10. On line 284, there is a missing word: "dense".
11. The format of the reference is confusing.
12. It is unclear why using the DNN structure in Figure 1. The network studied in the experiments has only 3 hidden layers, which is actually not "deep". Should we use skip connections here? Moreover, the presented results are far from state-of-the-art. In this sense, I am not sure whether the proposed method can be applied to real "DNNs".