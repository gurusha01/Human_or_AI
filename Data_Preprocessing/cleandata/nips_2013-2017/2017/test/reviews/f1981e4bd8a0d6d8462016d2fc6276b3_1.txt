This paper proposes an algorithm to learn a sequence classifier without labeled data by using sequential output statistics (language model). The proposed formulation is hard to optimize in its functional form and a stochastic primal dual gradient method is developed to efficiently solve this problem. Compared to earlier work this formulation is less inclined to get stuck at a trivial solution and doesn't require a generative model. Experimental results on two real world datasets show that the proposed method results in lower error rate compared to its base line methods.
Here are few comments.
1. In figure 2(a), what does the two axis represent (lambdad and lambdap)?
2. The basic idea of the paper and its primal dual stochastic gradient formulation seems convincing. It would be nice however, to include other baseline comparisons, namely from [11] and [30]. Also it would be convincing to add more datasets.
3. It would be nice to provide explicit gradient formulas (step 5 of algorithm 1).