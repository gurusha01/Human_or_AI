This paper studies the problem of learning a sequence classifier without labeled data by using sequential output statistics. 
This is motivated by applications where labels are costly to obtain, while sequential output statistics can be obtained easily. One typical example is OCR, where the output statistics are the language models. It proposes an unsupervised learning cost function, which is intuitive: view the mapping from the input sequence to the output sequence as transforming the input distribution to a distribution in the output domain, and the cost is the cross entropy between the output statistics and the transformed distribution. 
The authors discuss some nice properties of the cost: though it's non-convex, compared to existing works it has the property of coverage seeking that is better for optimization. It also transforms the cost to its primal-dual form and proposes a stochastic gradient method. Experiments on two real applications show that this approach works well (gets results close to that of supervised learning), while the alternatives fail miserably.
The presentation is very clear. Detailed comparisons are given to related works, though I'm not familiar enough with this direction to see if it covers all. 
Overall, the proposed approach is intuitive and the paper gives detailed discussions about advantages over existing works. Experimental results provide strong support for the effectiveness of the method. 
minor:
--Line 283: rate rate --> rate