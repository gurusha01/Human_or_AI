This paper proposes a low-rank regularizer for deep model compression during training. Overall, this paper is well-written, and the motivation is clear. However, here are some comments as follows.
1 The novelty is relatively limited, as the technical parts are strongly relevant to the previous works.
2 The experiments should be further improved. 
(1) Parameter sensitivity: From Fig 1, the performance of the proposed method (\tau is 1,\lambda is not 0) is similar to [2] (\tau is 0,\lambda is not 0). For other settings of \tau, the compression rate is improved while the accuracy is reduced.
(2) Results on larger models: the comparison with [2] should be performed to show the effectiveness. Furthermore, it would be interesting to compare with other state-of-the-art compression approaches, such as [18].