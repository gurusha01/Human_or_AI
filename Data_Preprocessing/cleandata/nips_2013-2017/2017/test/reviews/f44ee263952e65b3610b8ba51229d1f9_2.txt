This paper proposes a modification of recurrent networks (Elman network, LSTM or gated recurrent unit) for language modelling that is able to adapt to changes in the data distribution, by introducing an unbounded cache defined to deal with unseen words during training or uncommon words. Although there are other similar works in the literature (see [21],[50],[41]), this one allows to store all the previously seen words, instead of just the most recent ones by estimating the probability distribution of the words seen using a kernel density estimator (and an approximate knn to scale the process). From a theoretical point of view, the paper is interesting and addresses an issue that is present in many NLP applications. In the result sections, the authors show that using the cache is important to obtain good results. During the reviewing process they have added substantial empirical validation to the paper, showing that it improves on the state of the art. 
Opinion on the paper: 
+ the paper is very well written (although the authors should pass an english corrector) 
+ the topic is of great importance for every NLP practitioner
- the paper lacks proper numerical evaluation (although in the rebuttal process the authors provided a wide range of numerical results, they cannot be included in the final paper, thus not taken into account in the evaluation)