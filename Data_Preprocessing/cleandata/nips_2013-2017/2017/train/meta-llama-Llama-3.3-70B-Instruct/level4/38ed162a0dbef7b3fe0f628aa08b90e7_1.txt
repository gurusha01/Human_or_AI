This paper examines the desirable characteristics of deep convolutional kernel networks (CKNs), specifically their near-invariance to transformation groups and stability under diffeomorphisms, provided that the patch extraction operator, kernel mapping operator, and pooling operator are meticulously designed. The authors also demonstrate that, under certain conditions, these beneficial properties of CKNs can be extended to convolutional neural networks (CNNs).
Overall, the paper is well-structured and technically robust. However, a notable limitation is the absence of empirical experiments to corroborate the theoretical findings. It would be beneficial to include experimental results that illustrate the invariance of CKNs to transformations and diffeomorphisms when appropriately configured.
Furthermore, an interesting avenue for exploration is whether alternative approaches, such as random projection or explicit kernel mapping using a finite number of Fourier series, could serve as viable solutions for kernel approximation. If so, it would be valuable to investigate the impact of these methods on stability, providing a more comprehensive understanding of the subject matter.