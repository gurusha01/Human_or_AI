Post-rebuttal comments: I appreciate the authors' response and have subsequently increased my overall rating score.
Summary: In my view, this paper presents a promising research direction for applying deterministic quadrature rules to kernel approximation, which I find intriguing. However, the experimental section falls short, lacking comprehensive comparisons of computational time and benchmarks against QMC-based methods, such as those proposed by Yang et al. (ICML 2014). Without these, the advantages of the proposed approach remain ambiguous.
- Limitations of the results:
The authors' assumption that the kernel's spectrum is sub-Gaussian is reasonable, given that popular Gaussian kernels belong to this class. Nevertheless, this excludes other widely used kernels like Matern kernels, whose spectra decay polynomially. Consequently, the paper's findings may be somewhat restrictive.
- Equation (3):
The definition of $e_l$ is unclear.
Corollaries 1, 2, and 3, and Theorem 4:
These results exhibit an exponential dependence on the domain diameter $M$, leading to an exponential increase in the required feature size as $M$ grows. Although this factor does not increase with decreasing error $\varepsilon$, the dependence on $M$ affects the constant factor in the required feature size. Figure 1 illustrates that performance degrades more rapidly than standard random features, potentially highlighting a weakness in the proposed approach or its theoretical underpinnings.
- The equation in Line 170:
The variable $e_i$ is not defined.
- Subsampled dense grid:
While the authors employed this method in Section 5, it appears to lack theoretical guarantees. The approaches with theoretical guarantees seem to be impractical.
- Reweighted grid quadrature:
(i) There is no apparent theoretical guarantee for this method.
(ii) This approach bears resemblance to Bayesian quadrature, which determines weights by minimizing the worst-case error in an RKHS unit ball. A comparison with this method would be insightful.
(iii) Is it possible to derive a time complexity for this approach?
(iv) How was the regularization parameter $\lambda$ chosen in the $\ell_1$ approach?
- Experiments in Section 5:
(i) The computation time results are briefly reported (320 seconds vs. 384 seconds for 28,800 features in MNIST and a statement about quadrature-based features being twice as fast as random Fourier features in TIMIT). However, this is insufficient; the authors should present the results in a more detailed format, such as tables, varying the number of features.
(ii) A comparison with the QMC-based methods of Yang et al. (ICML 2014, JMLR 2016) is necessary to establish the advantage of the proposed method.
(iii) The experimental settings for the MNIST and TIMIT classification tasks should be clarified, including the classifiers used and how their hyperparameters were determined. At the very least, this information should be included in the appendix.