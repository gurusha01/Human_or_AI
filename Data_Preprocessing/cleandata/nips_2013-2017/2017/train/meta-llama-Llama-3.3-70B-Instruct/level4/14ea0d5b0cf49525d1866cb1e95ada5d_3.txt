This manuscript proposes a novel approach to extreme classification, a multiclass classification problem where the output space comprises up to 100,000 distinct classes. 
Many existing methods for extreme classification rely on either inferring a hierarchical structure among the output labels, such as hierarchical softmax, which reduces the number of required binary predictions to logarithmic, or employing a label embedding approach that can be efficiently learned through least-squares or sampling. However, the authors highlight the limitations of these methods, including the potential suboptimality of the inferred latent tree, which can lead to cascading errors, and the label embedding approach, which may result in prediction errors when the true label matrix is not low-rank.
As an alternative, the authors adopt a framework that reduces extreme classification to pairwise binary classification. They present a scalable, sampling-based method with theoretical guarantees ensuring consistency with the original multiclass empirical risk minimization (ERM) formulation.
Experimental evaluations on five text datasets, comparing the proposed method to several competing approaches, including standard one-vs-all classification where computationally feasible, demonstrate that the method generally outperforms others in terms of accuracy and F1 score when the number of classes exceeds 30,000. Additionally, it trains faster and uses significantly less memory, although it suffers slightly in terms of prediction speed.
The proposed Aggressive Double Sampling reduction is controlled by two key parameters: one that determines the sampling frequency of each class, inversely proportional to the empirical class frequency to prevent missing classes in the long tail, and another that sets the number of adversarial examples drawn uniformly. This process is repeated to train the final dyadic classifier.
During prediction, generating pairwise features for all possible classes is infeasible. To address this, a heuristic is employed to identify candidate classes by creating an input space centroid representation for each class, averaging the vectors within the class. Prediction is then carried out through a series of pairwise classifications with only these candidate classes.
Theoretical analysis of the reduction is complex due to the change from a sum of independent random variables to a sum of dependent random variables with a specific structure among the dyadic examples. Furthermore, oversampling rare classes shifts the empirical distribution, adding complexity. The proof involves decomposing the sum of dependent examples into sums of independent variables based on the graph structure induced by the example construction and utilizing concentration inequalities for partially dependent variables introduced by Janson. Although the finite-sample risk bound is biased by the ratios between true class probabilities and oversampled ones, these biases decrease linearly with the size of the sampled and re-sampled training sets. The analysis appears sound, although a detailed verification of the proof has not been conducted.
In summary, this paper presents a robust method for extreme classification, characterized by low memory and computational requirements and superior performance, especially on the largest datasets. The binary reduction algorithm is theoretically sound, interesting, and potentially useful for practitioners, offering a strong approach for extreme classification tasks.