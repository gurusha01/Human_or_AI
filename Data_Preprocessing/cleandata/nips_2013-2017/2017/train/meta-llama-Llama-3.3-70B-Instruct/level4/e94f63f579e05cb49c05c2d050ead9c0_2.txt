The paper presents a neural network-based supervised hashing algorithm, which generates binary codes by minimizing classification error loss. To optimize two losses simultaneously, the proposed approach employs an alternating strategy. Experimental results on two datasets demonstrate competitive performance compared to existing hashing methods, including those utilizing deep learning frameworks.
Strengths:
The paper is well-structured and easy to understand, with a clearly motivated idea and a sound algorithmic approach. The proposed optimization strategy is effective, and the overall methodology is well-explained.
Weaknesses:
Several relevant references are missing, notably [1], which shares similar concepts and should be discussed in-depth to highlight the contributions of this work. A thorough performance evaluation comparing this method to [1] is necessary.
[1] Zhang et al., Efficient Training of Very Deep Neural Networks for Supervised Hashing, CVPR16
[2] Liu et al., Deep Supervised Hashing for Fast Image Retrieval, CVPR16
The paper lacks detailed information about network training, such as hyper-parameter settings, which may hinder accurate reproduction of the results. Releasing the code upon acceptance would be beneficial.
Given the alternating optimization approach, it would be informative to visualize the loss curves and demonstrate the convergence effectiveness. 
The study in Figure 1 is intriguing, but the results and explanations (Ln 229-233) are inconsistent. For instance, DSDH-B does not always outperform DSDH-A, and the reasoning behind DSDH-C being only slightly better than DSDH-A is unclear (Ln 231-232). This discrepancy may be attributed to different binarization strategies and requires further experimentation to validate.
It is also unclear why DSDH-B surpasses DSDH in Figure 1(a) when using more bits.