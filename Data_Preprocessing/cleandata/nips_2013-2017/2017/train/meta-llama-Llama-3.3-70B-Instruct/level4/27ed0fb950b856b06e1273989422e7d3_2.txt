The authors present a novel approach to capturing complex long-time dependencies in count vectors by utilizing recurrent neural networks to represent implicit distributions, specifically through the gamma distribution. The shape parameters of these gamma distributions, which encapsulate time dependence, are approximated by neural networks that aim to integrate information from both prior (higher layer) and likelihood (lower layer) sources. The neural network parameters are updated based on the expectation of latent variables, a strategy that helps mitigate variance introduced by sampling and enhances computational efficiency. This model offers an innovative and intelligent solution to addressing the long dependency issue in dynamic models, which is commendable. However, the manuscript appears to be somewhat rushed, prompting several comments outlined below:
1. Notable typographical errors are present, such as in equation 10, which require correction.
2. The decision to treat the scale parameter as fixed while only updating the shape parameter in the gamma distributions warrants detailed discussion and analysis to justify this approach.
3. The experimental section is limited by only demonstrating the performance of the proposed model with two layers, without comparisons to models featuring a single layer or multiple layers. This omission makes it challenging to fully understand the impact of layer count on performance. Furthermore, other parameters, such as window size and the number of factors, should be discussed.
4. To facilitate better comprehension, it is recommended that the authors provide detailed graphical illustrations of the entire model. For reference, figure 1 in [1] could serve as a useful template.
5. The introduction would benefit from a discussion on recently proposed deep Poisson factor analysis models, including those presented in [2] and [3], to contextualize the current work within the broader field.
[1] Chung J, Kastner K, Dinh L, et al. A Recurrent Latent Variable Model for Sequential Data. NIPS, 2015.
[2] Mingyuan Zhou, Yulai Cong, and Bo Chen, Augmentable gamma belief networks, Journal of Machine Learning Research,17(163), 1-44, 2016.
[3] Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou, Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC, ICML 2017.