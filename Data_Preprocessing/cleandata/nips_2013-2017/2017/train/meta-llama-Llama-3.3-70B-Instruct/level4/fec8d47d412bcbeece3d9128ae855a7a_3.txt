This paper presents a methodology for learning a parametric function that maps a set of features describing a hypothesis to a per-hypothesis significance threshold. Theoretically, if this function is learned accurately, it would enable prioritizing hypotheses based on their feature representations, potentially leading to increased statistical power. The function that generates per-hypothesis rejection regions is trained to control the False Discovery Proportion (FDP) with high probability, under certain restrictive assumptions.
Multiple hypothesis testing, although not a dominant topic in machine learning currently, remains crucial in fields like computational biology and healthcare, where multiple comparisons are commonplace. A notable trend in this area involves designing methods that can incorporate prior knowledge to adjust the weight of hypotheses. The approach outlined in this paper aligns with this trend.
However, despite the relevance of the topic, several concerns arise regarding the practical applicability of the proposed method and other aspects of the paper.
 Major Points 
(1) The paper relies heavily on assumptions that are not valid for the motivating applications described and experimented with. 
Firstly, the method assumes that the triplets (P{i}, \mathbf{X}{i}, H_{i}) are independent and identically distributed (i.i.d.). This assumption is critical because its violation would render cross-validation inappropriate for validating decision rules learned from a training set. Unfortunately, in bioinformatics, independence cannot be assumed due to phenomena like linkage disequilibrium in genome-wide association studies and complex co-expression networks in RNA-Seq analysis. Unobserved confounding variables can also introduce statistical dependencies. There are extensions of the Benjamini-Hochberg procedure that guarantee FDR control under general dependence, suggesting that the authors should adapt their approach to handle arbitrary dependencies between hypotheses. The identical distribution assumption is also challenging to justify in practice.
Another assumption that raises concerns is the requirement that the alternative distribution $f_{1}(p \mid \mathbf{x})$ must be non-increasing. The authors claim this is a standard assumption, but recent work (e.g., [1]) has modeled the alternative distribution as a beta distribution with learnable mean and precision, counter-exemplifying this claim.
(2) The paper places significant emphasis on the use of neural networks, yet they play a relatively minor role in the approach. Essentially, any parametric function capable of mapping the input feature space to a significance threshold in the unit interval could suffice. Given that the motivating examples are low-dimensional (1D to 5D), it is likely that various parametric functions with sufficient capacity would perform adequately.
The authors should conduct comprehensive experiments to compare the performance of different function classes in this task. This could demonstrate the necessity of neural networks in their approach and might reveal that simpler, more interpretable functions perform equally well, potentially offering a preferable alternative.
(3) While the paper is largely free of grammatical errors, the quality of exposition could be improved.
Section 3 lacks detailed description of Algorithm 1, and the use of cross-validation is confusing, as it applies to subsets of features (hypotheses) rather than samples used to compute P-values. The variables $\gamma_{i}$ are introduced without adequate discussion. Adopting the notation from Equation (8) in Supplementary Section 2 for Equation (3) could clarify that the true objective function is being optimized through a smooth approximation.
The description of the implementation and training procedure in Supplementary Section 2 is insufficient, lacking justification for hyperparameter choices (e.g., network architecture, $\lambda$ values, optimization scheme) and results on the robustness of the approach to these hyperparameters. This information is crucial for practitioners in computational biology and statistical genetics who may not be familiar with deep learning techniques.
 Minor Points 
The use of cross-validation introduces randomness into the results, an undesirable property in statistical association testing. Experiments should be conducted to demonstrate the stability of results across different random partitions and to assess potential issues with stratification of features in the cross-validation process.
The proposed approach does not explicitly account for the number of hypotheses (n) being tested, relying instead on the cross-validation and test sets containing the same number of hypotheses and assuming i.i.d. hypotheses. It would be beneficial to relax this requirement by explicitly accounting for the number of tests.
Several claims in the paper are incorrect or imprecise, although they do not significantly impact the overall contribution. For example, the definition of a P-value is incorrect, and the statement about the popularity of FDP and FDR overlooks the Family-Wise Error Rate, which has a longer history and remains popular. Definition 1 is mathematically imprecise, and the discussion on non-parametric modelings is misleading, as kernel-based methods can be applied via inducing variables.
The paper would benefit from proofreading to correct frequent typos and minor errors in equations and references.
References:
[1] Li, Y., & Kellis, M. (2016). Joint Bayesian inference of risk variants and tissue-specific epigenomic enrichments across multiple complex human diseases. Nucleic acids research, 44(18), e144-e144.
[2] Dustin Tran, Rajesh Ranganath, David M. Blei - The variational Gaussian process. A powerful variational model that can universally approximate any posterior. International Conference on Learning Representations, 2016.