Multiple hypothesis testing is a crucial component of various high-throughput analyses, and leveraging available features for each tested hypothesis can enhance statistical power through independent hypothesis weighting. This involves learning the discovery threshold on P-values as a function of hypothesis features. The authors of this paper introduce neuralFDR, a novel method utilizing a neural network to model this relationship. Through simulations and two real-data applications, they demonstrate that neuralFDR can increase power while maintaining false discovery rate control and outperform existing weighting methods, particularly when considering multiple hypothesis features. The results are compelling, the paper is well-written, and the method shows promise for high-throughput hypothesis testing problems.
However, two key points warrant discussion: 
1. The computational demands of neuralFDR and its comparison to other methods are not addressed, which is crucial for potential users choosing a correction method.
2. In the GTeX dataset application, as the number of considered features increases, neuralFDR's discoveries rise while those of IHW (a competing method) decrease. With a 4.5% increase in discoveries (37,195 vs 35,598) when using three features, it is puzzling that the authors did not explore incorporating additional features to further boost discoveries, given the abundance of variant or gene features available in eQTL mapping.
Minor suggestions include:
- At Line 215, reporting the percent increase alongside "800 more discoveries" would enhance clarity.
- Table 2 would benefit from including absolute and percent increases to improve readability.