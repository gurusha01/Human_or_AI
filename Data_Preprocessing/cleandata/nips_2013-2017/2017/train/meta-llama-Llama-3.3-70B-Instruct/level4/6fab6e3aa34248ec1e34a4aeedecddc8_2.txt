The paper presents a novel approach called conditional batch normalization (CBN), which is designed to be integrated into existing visual question answering (VQA) models. This method aims to modulate visual processing with language information from the question at an early stage. Specifically, the parameters of the batch normalization layer in a pre-trained convolutional neural network (CNN) are updated using the VQA loss, conditioned on the long short-term memory (LSTM) embedding of the input question.
The effectiveness of CBN is evaluated on two VQA datasets: the VQA dataset from Antol et al. (ICCV15) and the GuessWhat?! dataset from Vries et al. (CVPR17). The experimental results demonstrate that CBN significantly improves VQA performance. Furthermore, the paper investigates the impact of adding CBN to different layers and finds that applying it to the last two layers of the CNN yields the most substantial improvements. The authors also provide quantitative evidence that the performance gains are not solely due to fine-tuning the CNN, but rather the result of modulating visual processing with language information.
The strengths of the paper include:
1. The idea of modulating early visual processing with language is novel and intriguing for the VQA task.
2. The proposed CBN method can be easily integrated into any existing VQA model, making it widely applicable.
3. The ablation studies provide valuable insights into the effectiveness of early language modulation.
4. The paper provides detailed hyperparameter settings, ensuring the reproducibility of the results.
However, there are some weaknesses:
1. Although CBN improves VQA performance, the results do not advance the state-of-the-art on the VQA dataset, possibly due to the use of a non-optimal VQA model. To address this, the authors should conduct experiments using more recent, high-performing VQA models, such as MCB (Fukui et al., EMNLP16) or HieCoAtt (Lu et al., NIPS16).
2. The performance difference resulting from using different image sizes and ResNet variations is unclear (L170).
3. The results on the VQA dataset are reported on the test-dev split, whereas the guidelines recommend using the test-standard split to avoid overfitting.
4. The application of CBN to layer 2, in addition to layers 3 and 4, deteriorates performance on the GuessWhat?! dataset, which requires further explanation.
5. The visualization in Figure 4 could be improved by comparing the results with the fine-tuned batch normalization ResNet.
6. Minor errors and typos, such as repetition of words, missing references, and grammatical errors, should be corrected.
Post-rebuttal comments:
The new results applying CBN to the MRN model are convincing, demonstrating the effectiveness of CBN on fairly developed VQA models. However, some concerns remain:
1. There is still confusion regarding the test-standard and test-dev splits of the VQA dataset, which needs to be clarified.
2. The reproduced performance on the MRN model should be compared to the original results, taking into account the training data used (train only or train + val).
3. The citation for the MRN model is incorrect and should be updated.
4. It would be interesting to investigate whether the findings from ResNet generalize to other CNN architectures, such as VGGNet.