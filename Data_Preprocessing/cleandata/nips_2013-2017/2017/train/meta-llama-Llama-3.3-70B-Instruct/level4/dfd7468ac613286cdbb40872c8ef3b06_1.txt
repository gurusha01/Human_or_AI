This manuscript introduces the MMD-GAN, a novel generative framework that leverages the strengths of both generative adversarial networks (GANs) and generative moment matching networks (GMMNs). The proposed architecture relies on Maximum Mean Discrepancy (MMD) but incorporates a kernel function that is learned by an adversarial component aiming to maximize MMD. Theoretical foundations are provided to ensure non-degeneracy, and experimental results demonstrate that the generated samples are comparable to those produced by W-GANs.
In my opinion, this is a robust paper that presents a straightforward concept and executes it effectively. The proposed methodology is intuitively appealing, with well-motivated modeling choices, clear writing, and a solid theoretical justification. The relationships between the proposed method and other existing approaches are also clearly elucidated. The experimental evaluation compares the proposed method against strong baselines on challenging datasets, showcasing competitive performance. Notably, bridging the performance gap between GMMNs and W-GANs is a significant achievement, given the substantial difference in their performance prior to this work.
Although the results indicate a substantial improvement over the baselines, further analysis is necessary to understand the source of this enhancement. Specifically, it would be beneficial to investigate how the MMD-GAN alleviates the need for large mini-batches, as the samples within each mini-batch seemingly still need to cover the entire space to achieve low MMD. Additionally, an explanation for the outperformance of the WGAN would provide valuable insights.
Some minor suggestions for improvement include:
- In Section 3, the method is described in relation to GANs, but a more direct comparison to W-GANs might be more appropriate, given that the MMD-GAN can be viewed as a kernelized version of the W-GAN.
- The statement "Also, the gradient has to be bounded, which can be done by clipping phi" raises questions about the connection to the Lipschitz constraint in WGANs. It might be worthwhile to consider using the regularization approach from the "improved WGAN" paper instead of the original method.
- To substantiate the claim that the proposed method prevents mode dropping, estimating log-likelihoods could provide a more quantitative evaluation, as demonstrated in Wu et al. (2017).
- The experiment in Section 5.3 appears to not directly assess stability, which is typically understood as the avoidance of degenerate solutions commonly encountered in regular GANs.