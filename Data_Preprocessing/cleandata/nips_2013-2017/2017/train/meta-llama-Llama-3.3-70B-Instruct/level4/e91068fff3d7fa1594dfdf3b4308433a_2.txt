This submission presents a methodology that frequently decreases the variance of stochastic gradients in variational inference. By utilizing the reparameterization trick, the score function is incorporated into the stochastic gradients, albeit often at the cost of increased variance, as it can act as a control variate. Omitting the score function does not introduce bias into the stochastic gradients and commonly yields superior empirical outcomes.
The subject matter addressed in the paper is of significant importance, as the reparameterization trick has substantially broadened the applicability of variational inference. However, a comprehensive evaluation of the various methods for computing stochastic gradients with this trick has been lacking.
In addition to tackling a crucial topic, the paper reveals unexpected findings: 1) a Monte Carlo approximation of the entropy term can result in lower overall variance compared to using the exact entropy term, and 2) the inclusion of the score function unnecessarily increases the variance of the stochastic gradients.
The paper's argument does not rely on novel mathematical derivations or extensive mathematical content. While the empirical results are valuable, the observed improvements appear moderate. Overall, the paper is well-written.
It is assumed that equations 5-7 are correct, but further clarification would be beneficial. The differentiation of only the integrand in equation 1, without considering the impact of variational parameters on the measure, warrants explanation. Additionally, the derivation of equality 7 could be elaborated upon, potentially through the application of the chain rule or alternative notational approaches that enhance clarity. The notational complexities may have contributed to the oversight of the score function's role in introducing unnecessary variance to the gradients, which has gone unnoticed until now.