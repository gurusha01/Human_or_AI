Overall, I consider the paper to be well-structured and the concept to be both intuitive and well-founded. Nevertheless, I have several technical concerns and questions regarding the methodology employed in the paper.
* The architecture of the encoder (Fig. 4) raises a question about the necessity of explicitly generating K posterior mean/variances before combining them. Would it not be more efficient to have the encoder network directly generate the posterior mean/variances, and what benefits does the current approach provide in comparison?
* Given that the ultimate goal is conditional caption generation, which does not inherently require posterior inference, I wonder if the use of VAE is overly complex. An alternative approach could involve directly learning the decoder through maximum likelihood, where z can still be sampled from p(z|c) and marginalized to approximate the maximization of p(x|c) = ∑[p(zi|c)p(x|zi, c)]. This could potentially yield a stronger LSTM baseline while still incorporating z.
* The computation of KL-divergence in GMM-CVAE is not clearly explained and warrants further clarification.
* The standard deviations σk in the prior play a crucial role in balancing KL-divergence and reconstruction error during training. I am interested in knowing whether the authors explored different values for σk during training, as they did during testing. It seems reasonable to maintain consistency in σ_k values between training and testing phases.
Additionally, a minor typo was noticed: on line 36, "maximize (an upper bound on) the likelihood" should likely be "minimize (a lower bound on) the likelihood" or simply "lower bound" for accuracy.