Summary of the Paper
====================
This paper presents a geodesic extension of Nesterov's accelerated gradient descent (AGD) algorithm, tailored for Riemannian spaces, with two distinct versions: one for geodesic convex cases and another for geodesic strongly convex smooth cases. The proposed algorithms are applied to Karcher mean problems, demonstrating superior performance compared to existing algorithms (RGD, RSGD) in the same setting, using randomized data.
Evaluation
==========
From a theoretical perspective, the generalization of the momentum term to maintain the same convergence rate across any Riemannian space is a novel and intriguing contribution. However, from a practical standpoint, it is unclear when the overall running time is reduced, as the implementation of the momentum term can be costly, despite requiring significantly fewer iterations. The algorithm's broad applicability makes it an attractive general mechanism, potentially encouraging further development. The paper is well-structured and relatively easy to follow.
General Comments
================
- The paper heavily relies on differential geometrical notions and definitions that may be unfamiliar to the typical NIPS reader. It is suggested that the definition section be made more concise and clear, with specific definitions for terms such as star-concave, star-convex, grad f(x), intrinsic inner-product, diameter of X, conic geometric optimization, and retractions.
- Equations 4 and 5 appear without intuitive explanations for their derivation, seeming somewhat abrupt, and the accompanying figure could be better utilized to clarify these equations in the text.
Minor Comments
==============
L52 - The redundant word "The" can be removed.
L52+L101 - The statement that the proximal interpretation is the main interpretation of AGD may be misleading, as there is a significant body of work providing alternative, more satisfying interpretations.
L59 - Further elaboration on the computational complexity of implementing the exponent function and the nonlinear momentum operator S would be beneficial.
L70 - The linearization of gradient-like updates is not explicitly stated as a contribution.
L72 - The term "classes" is used without clear definition.
L90 + L93 - The sentence "we denote" is unclear and requires rephrasing.
L113 - The sentence "In addition, different.." could be rephrased for better clarity.
L126 - Besides the constraint on alpha, what other factors should be considered for optimal setting of its value?
L139 + L144 - The font used for "Exp" appears to be incorrect.
L151 - The wording in Lemma 3 could be clarified.
L155 - In Theorem 1, consider rephrasing the value of beta for better understanding.
L165 - How does the upper bound depend on D, and what considerations are necessary for setting alpha optimally?
L177 - The term "geometrically" could be replaced with "geodesically" for precision.
L180 - In Section 5, the instantiation of the definition of S for this specific case is not entirely clear.
L181 - The reference to "the accelerated scheme in (4)" should be clarified, potentially as "algorithm 1".
L183 - The notation "Yk" or "yk" should be consistently used.
L193 - The phrase "with the geometry" is unclear and requires further explanation.
L201 - The redundant word "The" can be removed, and the reference "(2)" should be "(3)".
L219 - A more rigorous explanation or relevant pointers are needed to justify why RGD is a suitable proxy for the algorithms mentioned.
Page 8 - Providing the actual time, in addition to the benchmark, would offer a more comprehensive comparison, as the per-iteration cost is not accounted for.
L225 - The explicit definition of C could be justified or explained further.