The authors present a novel approach to testing conditional independence among three continuous variables, eschewing strong parametric assumptions in favor of a method that leverages samples from the joint distribution to approximate the conditional product distribution in terms of total variation distance. This is achieved by generating samples that closely resemble those from the conditional product distribution, which are then utilized to train a binary classifier, such as a deep neural network. The classifier's predictive error is subsequently employed to either accept or reject the null hypothesis of conditional independence. The proposed method is benchmarked against existing approaches in the literature, with comparisons drawn through both descriptive analyses and experimental results on synthetic and real-world data.
The significance of conditional independence testing in modern machine learning, particularly in causal inference where assumptions of unconfoundedness are frequently scrutinized, renders this proposal noteworthy. The method appears sufficiently straightforward and powerful for application in various causal inference scenarios.
The paper's clarity is commendable, with the authors successfully summarizing complex theoretical results and proofs, making the methodology accessible to practitioners. However, the theoretical underpinnings and their proofs can be challenging to follow in detail.
The theoretical results are arguably the most impressive aspect of the paper, representing a logical extension of the independence testing method outlined in "Revisiting Classifier Two-Sample Tests," with the incorporation of a third variable and a neighbor-matching approach based on this variable. The theoretical guarantees provided are particularly noteworthy.
Several additional considerations arise:
- The potential for a discrete analog is intriguing, especially given the enhanced power of the bootstrap when the variable Z is discrete.
- An examination of the test's sensitivity to the choice of classification method and its hyperparameters would be valuable, as variations in these parameters could potentially lead to differing outcomes in terms of acceptance or rejection of the null hypothesis.
- Section 1.1 of the main contributions could be streamlined, as the approach is summarized multiple times, rendering some of the repetition unnecessary.
- The explicit writing out of Algorithm 2 may be redundant, as the process could be described more concisely and has already been outlined in the paper.
- Minor typographical errors, such as the omission of "classifier" in line 157 and "graphs" in line 280, should be corrected.