Update after rebuttal: I recommend accepting the paper as a poster presentation, provided that the authors refine their writing to more effectively emphasize their contributions, motivations, and design decisions, which would make their work more distinctive and memorable, rather than simply another hybrid generative model.
The paper introduces the PixelGAN autocencoder, a generative model that combines an adversarial autoencoder with a PixelCNN autoencoder, supported by a theoretical justification based on the decomposition of the variational evidence lower bound (ELBO). The authors present qualitative results using different priors on the hidden distribution and quantitative results on semi-supervised learning tasks for MNIST, SVHN, and NORB datasets.
This work is closely related to Adversarial Autoencoders (Makhzani et al., ICLR 2016 Workshop), which, despite only being published on arXiv and as a workshop contribution to ICLR, is well-known and widely cited. The novelty of the current submission is significantly influenced by whether Adversarial Autoencoders are considered an existing approach or not, a judgment call that area chairs and program chairs are better equipped to make.
Moving on to more detailed comments:
Strengths:
1. The paper achieves good results in semi-supervised learning on MNIST, SVHN, and NORB, as well as unsupervised clustering on MNIST. Although it's challenging to determine if these results are state-of-the-art due to missing baseline numbers, they are at least comparable to current state-of-the-art performances.
2. The discussion on ELBO decomposition and its relation to architectural choices is clear and well-presented.
Weaknesses:
1. If Adversarial Autoencoders are considered prior work, the novelty of this paper may be limited, as it essentially combines two existing generative models. The authors should clarify what makes this specific combination particularly noteworthy.
2. The lack of results on actual image generation is notable. Including likelihood bounds (if computable), Inception scores, and generated images, even in the appendix, would be beneficial for a more comprehensive evaluation.
3. The paper confusingly uses two versions of the approach (with location-dependent and location-independent biases) interchangeably in experiments without directly comparing them. A more in-depth analysis would be valuable.
4. A more detailed comparison with existing approaches like VLAE and PixelVAE (both published at ICLR 2017) would be helpful. The current discussion, while somewhat informative, does not clearly highlight the differences, strengths, and weaknesses of these approaches.
5. Certain formulations are unclear, such as the distinction between "limited stochasticity" and "powerful decoder," and the statement regarding the optimization of KL divergence seems too abstract and requires clarification.
6. The bibliography often overlooks the ICLR conference, listing officially published papers as arXiv submissions.
7. Placing an entire section on cross-domain relations in the appendix is not ideal. Authors should organize their paper to ensure all significant contributions are included in the main text.
Overall, my assessment is borderline. While the results are commendable, the perceived novelty is limited, which leads me to suggest acceptance as a poster to encourage further refinement and clarity in presenting the work's unique value.