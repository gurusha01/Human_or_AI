This paper proposes a method to decrease the memory requirements of embedding parameters by utilizing hashed embeddings. The approach is straightforward, involving the learning of importance parameters for each component vector, resulting in two smaller trainable matrices: one for importance parameters and one for component vectors, both of which are significantly smaller than a standard embedding matrix.
To generate an embedding for a given word, the method first hashes the token ID to a row in the importance parameters matrix, and then each component of these importance parameters is hashed to obtain the corresponding component vector embedding.
The experimental results demonstrate that the ensemble of hash embeddings outperforms previous bag-of-words models on classification tasks, even when using simple models. This technique offers a simple yet effective means of reducing model parameters and can also act as a regularizer. 
However, the authors should provide additional information regarding the computational time required for inference, specifically comparing the efficiency of hashing operations to embedding gather operations on a GPU, to offer a comprehensive understanding of the method's practicality.