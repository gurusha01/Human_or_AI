This paper proposes a two-step decoding model for Neural Machine Translation (NMT) and summarization, presenting a well-explained concept with experimental results that demonstrate notable improvements over a realistic baseline.
The authors employ Monte Carlo approximation for marginalizing intermediate hypotheses, which is a viable approach. However, crucial details such as the number of samples used and the impact of this hyperparameter on performance are not provided. Additionally, it is unclear whether beam search is utilized instead of simple sampling, and these technical aspects warrant more precise description.
The paper's contribution is interesting, and the experimental section is well-founded. Nevertheless, the introduction comes across as overly pretentious, dedicating two paragraphs to a "cognitive" rationale that essentially reinvents existing concepts. Multi-step decoding has been explored for years, and while proposing a solution for end-to-end neural models is commendable, it is unnecessary to dismiss prior work or justify it with populist cognitive considerations, especially when "cognitive science" implies a scientific foundation.
On Page 2, the sentence at line 73 appears redundant and could be removed.
At Line 78, the statement about end-to-end neural models is accurate but overlooks the extensive history of multi-pass decoding in speech recognition. This process involves generating a word lattice with initial "weak" models, followed by iterative refinement using more sophisticated models. Thus, claiming that sequence prediction has not been explored in this context is inaccurate. It is not necessary to disregard previous work merely to introduce a novel terminology.
On Page 8, Line 277, the phrase "In this work, inspired by human cognitive process" could be omitted to avoid an overly pretentious tone.