The paper presents an adversarial autoencoder model that incorporates an autoregressive decoder, allowing the model to leverage both its latent representation and autoregressive connections to capture data characteristics. The architecture can be modified to control which components of the model learn specific aspects of the data by adjusting the prior distribution used for the latents and the model's structure.
The experiment illustrated in Figure 2 effectively demonstrates the advantages of combining autoregressive and latent variable-based modeling approaches. Additionally, Figure 3 shows that the model learns to encode digit labels despite the continuity of the prior, which is a notable outcome.
However, it is unclear to what extent the different decompositions (e.g., global vs. local structure and digit identity vs. details) are solely attributed to the choice of priors. The model with a categorical prior also employs a different biasing mechanism and has a deeper architecture (as mentioned at the end of Section 2.1). Therefore, it seems premature to conclude that the change in prior is the primary reason for the learning of different decompositions. An experiment where the model with the categorical prior is used, but only the prior is changed back to Gaussian, would be insightful. Based on intuition, it is likely that the architectural changes have a more significant influence on the decomposition than the choice of prior, and this could be an interesting direction to explore.
The experiment in Figure 5 is also noteworthy, as it clearly illustrates the "discretizing" effect of the GAN loss on the latent code. The semi-supervised classification experiment is another valuable addition to the paper.
In general, the proposed model represents a natural extension of both adversarial autoencoders and autoregressive models, and the paper provides a nice follow-up to previous studies on VAEs with autoregressive decoders, such as VLAE and PixelVAE. The experimental section is well-designed, and the results are convincing, although the contribution of the prior choice to the learned decompositions may be overstated.
Some specific points to consider include:
- The statement on Line 45, "the global latent code no longer has to model all the irrelevant and fine details," may be too broad, as its usefulness depends on the specific task. For instance, in a texture classification task, this might not be beneficial.
- In Lines 187-188, it is mentioned that the adversarial net receives continuous inputs from the softmax layer. However, it would be helpful to explicitly state whether the decoder receives discrete or continuous inputs for clarity.
- Although scaling up is not the primary focus of the paper, conducting an experiment on a larger dataset would be valuable. Intuitively, this model may scale better than regular PixelCNNs due to its ability to effectively utilize latents to capture global structure.