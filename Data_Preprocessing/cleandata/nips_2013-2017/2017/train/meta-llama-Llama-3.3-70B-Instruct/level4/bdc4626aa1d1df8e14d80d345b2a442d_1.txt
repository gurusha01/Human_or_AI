This paper examines a variant of contextual linear bandits subject to a conservative constraint, which stipulates that the cumulative reward earned by the player at any time t must be at least (1-alpha) times the cumulative reward of a predefined baseline policy, given a specified alpha value. The authors investigate two scenarios: one where the baseline rewards are known and another where they are unknown. For each scenario, they develop a UCB-type algorithm, building upon an existing algorithm for contextual linear bandits, and derive an upper bound on the regret, comprising the regret inherent to the base algorithm and the additional regret incurred by adhering to the conservative constraint. The authors also conduct simulations for the algorithm with known baseline rewards, verifying that the algorithm indeed satisfies the conservative constraint.
The introduction of the conservative constraint, the proposed algorithms, and the corresponding regret bounds appear to be well-motivated and logical. However, Figure 1(a) would benefit from revision to better illustrate the initial conservative phases of CLUCB. Furthermore, it is recommended that the same experimental evaluation be conducted for CLUCB2, as it operates in a more realistic setting, to provide a more comprehensive understanding of its performance.