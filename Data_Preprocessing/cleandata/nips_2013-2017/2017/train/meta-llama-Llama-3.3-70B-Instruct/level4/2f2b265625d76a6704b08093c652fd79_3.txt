The authors pose several intriguing questions related to GANs, including the implications of solving the GAN problem on convergence in parameter space, the nature of the generator's fit when convergence occurs, and the selection of the discriminator's output activation function to ensure proper compositeness of the loss function. 
They address these questions within the framework of deformed exponential distributions, yielding theoretical results such as a variational generalization of a known theorem that connects f-divergence between distributions to Bregman divergence between parameters. This provides an interesting link between the information-theoretic and information-geometric perspectives on measuring dissimilarity. 
Under a reversibility assumption, they show that deep generative networks can be factored into escorts of deformed exponential distributions, suggesting that careful selection of the generator's hidden activation functions and the discriminator's output activation function could improve GAN performance. They also touch upon an alternative interpretation of the GAN game in the context of expected utility theory.
Overall, the authors present interesting theoretical findings, but I have concerns regarding the practical relevance and usefulness of these results in their current form.
Questions and concerns:
Firstly, many of the theorems and insights are specific to the case where data and model distributions P and Q belong to the deformed exponential family. Can the authors justify this assumption and discuss whether similar results, such as Theorems 4 and 6, could be derived without it?
One of the main contributions, the information-geometric f-GAN identity in Eq.(7), relates the variational f-divergence formulation over distributions to an information-geometric optimization problem over parameters. What benefits does this parameter-based perspective offer, and can GANs be implemented using this approach? I would have liked to see experimental comparisons between the two optimization approaches. Notably, the authors do not utilize the right-hand side of this identity beyond asserting that solving the GAN game implies convergence in parameter space (given a small residual J(Q)). Why is this implication not obvious, and can the authors provide a scenario where convergence in the variational f-divergence formulation does not imply convergence in parameter space?
The authors' theoretical investigation has a practical implication: the choice of the output activation function gf matters for the GAN loss to have the desirable proper compositeness property. I found this result interesting and would have liked to see an experimental comparison between their theoretically derived gf and the heuristic choice in Ref.[34].
Consequences for deep learning and experimental results: Assuming a reversible generator network, the authors show that there exists an activation function v such that the generator's hidden layers factor exactly as escorts for the deformed exponential family. In Table 2, they compare generator architectures against different activation functions. However, the "theoretically superior" μ-ReLU performs worse than the baseline ReLU. I would expect the invertible μ-ReLU to perform better than the non-invertible baseline. Can the authors explain this discrepancy? Can they comment on whether the DCGAN architecture satisfies the reversibility assumption and how these results compare to the Wasserstein GAN, which is not based on f-divergences? Furthermore, the results on the performance of different activations and link functions are within error bars, which I do not think provides sufficient support for the practical relevance of their theoretical results.
Minor concerns:
Many results are derived within the framework of deformed exponential densities, which may be unfamiliar to the general ML community. I would appreciate more intuition on what a deformation (or signature) and an escort are. 
In Theorem 8, the authors derive upper bounds for J(Q). How do these bounds compare, and what are the implications for choosing activation functions? They mention that this upper bound decreases with the normalization parameter of the escort Z. Can they elaborate on why this is beneficial and how it can be leveraged?
In light of these comments, I believe the insights from the theoretical analysis are not substantial enough from a GAN practitioner's perspective.