This paper proposes the deep dynamic Poisson factorization model, which extends traditional Poisson factorization (PF) to accommodate temporal dependencies by utilizing a simplified recurrent neural network (RNN) structure. Unlike previous dynamic PF approaches, this model leverages the RNN to capture long-term dependencies. The inference process is conducted through variational inference, incorporating an additional step to optimize the neural network parameters. Experimental results are presented across five real-world datasets.
While I find the concept intriguing, I believe the paper requires refinement in its presentation to meet the standards of a conference like NIPS. My detailed comments are outlined below.
+The formulation presented in Equation 4 does not align with a traditional RNN structure because \(h_t^{(n)}\) is dependent solely on the top-layer memory vectors and does not account for previous time steps within the same layer, as is characteristic of RNN setups. Could the authors provide a rationale for this simplified structure?
+Section 2 lacks completeness in its model equations. For instance, the parameterization of the Gamma distribution in Equation 4 and the distribution generating \(\theta_t\) in Equation 3 are not clearly defined.
+I disagree with the discussion regarding "implicit distributions" in lines 110-118. Equations 9 and 10 explicitly define Gamma distributions with specified shapes and rates. Given that the distribution of \(\theta_{tk}\) (Equation 9) can be computed in closed form conditioned on \(h^{(0)}\), and similarly for \(h\) in Equation 10, these cannot be considered implicit distributions.
+The sentence in lines 30-31, concerning the method's potential weaknesses in analyzing data with different long-time dependence patterns, such as fanatical or disaster data, is unclear. Could the authors elaborate on this point?
+The subsection spanning lines 133-155 is not transparent, particularly regarding the definition of the "loss function" in Equation 15.
+For models akin to Poisson factorization, metrics like MSE/PMSE are typically insufficient. The results should be supplemented with predictive log-likelihood values.
+The chosen value of \(K\) in the experiments (line 207) appears too small. I suggest the authors consider larger values, such as \(K=100\), for more robust outcomes.
+The discussion in lines 224-232 does not clearly relate to Figure 3, necessitating further clarification.
+The update equations for the variational inference procedure (Equations 11-14) could be relocated to the supplementary materials without affecting the paper's clarity.
+Figure 4's caption is difficult to read and could be improved for better understanding.
+I strongly advise adhering to general writing guidelines, particularly concerning the use of the passive voice, which is especially pertinent for the abstract.
+The overall writing quality is subpar, with numerous typos and grammatical errors. Examples include:
 -Line 15, "high-dimensional" is correctly used but in other contexts, precision is needed.
 -Line 22, the term "factorize" should be used correctly.
 -Line 24, the phrase "property of the" is incomplete.
 -Lines 29 and 34, "Dirichlet" is mentioned without clear context.
 -Line 34, "nested" could be more appropriately used.
 -Figure 1, "visual representation" and "transmission" require clearer definitions.
 -Incomplete sentences, such as line 49, and missing articles (e.g., "Although the Dirichlet distribution is often used as a prior distribution") detract from the paper's readability.