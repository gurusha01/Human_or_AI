Review Summary:
This work presents a distinctive approach to fusing language and vision modalities by modulating visual features using text, unlike most existing methods that fuse modalities after extracting independent representations. The authors propose 'Conditional Batch Normalization' (CBN), which involves learning the batch normalization parameters of ResNet, typically used for visual representation extraction, conditioned on the text, resulting in the ModRNet network. Experimental results on two tasks demonstrate the effectiveness of this approach.
Strengths:
(a) The idea of fusing modalities as early as the visual feature extraction stage is novel and well-motivated, drawing parallels to neuroscience findings on the influence of language on visual stimulus response in the human brain.
(b) Utilizing batch normalization parameters for modality fusion is a suitable choice, as it scales with the number of channels, limiting the additional parameters to be learned, and the simplicity of the approach is exemplified by freezing other network parameters.
(c) The paper includes thorough ablation studies and analyzes fine-tuning ResNet on the task versus ModRNet, with enjoyable experiments and discussions.
Weaknesses:
(a) The experiments are limited to the ResNet architecture for feature extraction, raising questions about the generalizability of CBN to other networks, such as batch-normalized VGG, and whether this is a property specific to ResNet due to residual connections.
(b) The authors should carefully review the manuscript to resolve typos and grammatical issues that hinder the reading and understanding of the proposed approach, with several errors listed.
Comments:
(a) The sentences in L13-16 do not flow well, as the initial sentences suggest the capability to model tasks, while the later sentences state that these remain a long-standing challenge.
(b) The task of the oracle is not described with clear input-outputs until L185, making it difficult to understand the specifics that follow.
(c) It is suggested to retrain the model with the same visual features on same-sized images as the current work for a perfect comparison, as mentioned in L212.
Typos:
(a) L4 - inputs
(b) L17 - towards
(c) L42 - Missing reference or typo
(d) L150 - LaTeX typos
(e) L190 - ReLU
(f) L218, L267 - typos
Post Discussion:
The authors have done a good job with the rebuttal, and after reviewing other comments and the rebuttal, it is clear that the paper has novel contributions that bring new insights for future works to build upon. However, there is still curiosity about the generalization to other networks, and showing evidence on other networks, such as batch-normalized VGG, would strengthen the paper, even if the findings suggest limited usefulness for other architectures. The authors mention the lack of pre-trained batch norm VGG models on TensorFlow, but using PyTorch pre-trained models in time for the camera-ready version would be beneficial.