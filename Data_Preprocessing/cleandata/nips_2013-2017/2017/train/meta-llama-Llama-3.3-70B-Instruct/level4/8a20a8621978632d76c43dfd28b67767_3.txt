Review Summary:
Recent endeavors have focused on providing interpretability to complex models, including techniques like LIME, DeepLift, and Layer-wise Relevance Propagation. In complex models, changes in input features contribute to output changes in a nonlinear manner, prompting efforts to fairly distribute feature contributions. This work highlights the Shapley value from game theory as the unique solution that satisfies desired properties, providing a unified framework for interpretable methods through additive feature attribution methods and SHAP as a measurement for feature importance. The author introduces a SHAP value estimation method, supported by user studies and comparisons to related works on benchmark datasets.
Quality:
The paper demonstrates technical soundness with detailed proofs and supportive experiments. However, a more in-depth discussion on the trade-off between Shapley value estimation accuracy and computational efficiency would be beneficial.
Clarity:
The paper is well-organized, providing sufficient background and motivations. Nevertheless, the implementation of model-specific approximations in Section 4.2 could be clarified with an algorithm flow, illustrating how to modify existing methods like DeepLift to satisfy SHAP values. Minor issues include:
* In Equation (10) on page 4, the definition of \bar{S} is unclear, and the last symbol should be x_{\bar{S}} instead of x.
* The citation style in line 207 on page 6 appears inconsistent.
Originality:
This paper uniquely identifies the Shapley value from game theory as the proper distribution of feature contributions, linking it to existing methods through proofs. The explanation of enhanced performance in the original vs current DeepLift due to better Shapley value approximation is noteworthy. To further strengthen the work, the author could explore connections between the Shapley value and gradient-based approaches, such as those presented in [1] and [2].
[1]: Simonyan et al. (2013): "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", http://arxiv.org/abs/1312.6034
[2]: Springenberg et al. (2015): "Striving for Simplicity - The All Convolutional Net", http://arxiv.org/abs/1412.6806
Significance:
This work provides a theoretical framework for feature importance methods, with the potential for the SHAP metric to guide feature importance evaluation in the field of interpretable methods. The paper's contributions have significant implications for the development of more effective and interpretable models.