This paper presents an accelerated first-order approach for geodesically convex optimization on Riemannian manifolds, demonstrating linear convergence of O((1-\sqrt{mu/L})^k) for mu-strongly G-convex and L-smooth functions, and O(1/k^2) for G-L-smooth functions, thus extending the Euclidean Nesterov's accelerated method. The numerical experiments compare the proposed method's performance to that of RGD and RSGD.
However, a major concern lies in the application of the proposed method, as the experimental results are not persuasive. Specifically, for the Karcher mean on SPD matrices, where the cost function is smooth and the Hessian has a favorable condition number, methods leveraging higher-order information exhibit superior performance. The authors' claim that Bini's method, Riemannian GD, and limited-memory Riemannian BFGS have comparable performance is misleading, particularly when using BB step size in the line search algorithm instead of a constant step size. Typically, 30 passes can achieve an objective gap reduction of more than 10^10, far surpassing the 10^3 reduction reported in this paper.
This application appears to be an inadequate showcase for the proposed method, as it does not effectively utilize the method's ability to handle non-C^2 functions. A more suitable application might involve a C^1 cost function, which would better demonstrate the proposed method's capabilities.