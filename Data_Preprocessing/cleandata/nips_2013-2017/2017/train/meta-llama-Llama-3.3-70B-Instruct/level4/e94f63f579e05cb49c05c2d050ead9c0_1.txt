The paper presents a deep hashing method that operates under a supervised setting with class-labeled training data. The model's parameters are optimized using a loss function that balances pairwise similarities and class separability, assuming a linear model. A key innovation lies in the direct optimization of binary embeddings during training without resorting to relaxation techniques, achieved through an alternating minimization approach. The results demonstrate superior performance compared to numerous baseline methods across two datasets, accompanied by an exhaustive examination of various options. The clarity of the writing is commendable. However, as a non-expert in this domain, my primary concern revolves around the novelty of the approach. Given that similar optimization techniques have been explored in previous works ([9, 17, 21]), the distinguishing factor appears to be the incorporation of a linear classification component within the loss function. Furthermore, the overall methodology seems intricate and potentially computationally intensive, raising questions about its training efficiency and the associated computational cost.