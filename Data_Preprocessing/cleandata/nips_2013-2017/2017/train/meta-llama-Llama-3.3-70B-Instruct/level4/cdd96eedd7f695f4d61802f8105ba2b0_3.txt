This manuscript investigates the network embedding problem, emphasizing the preservation of proximity and global ranking. The authors propose an unsupervised Siamese neural network architecture that learns node embeddings in a generative manner. They assert that their model possesses four key characteristics: scalability, asymmetry, unity, and simplicity.
The Siamese neural network is formulated as a multi-task learning framework, where shared hidden layers are utilized to embed vectors of each pair of connected nodes. Specifically, the node ranking hidden layer encodes global ranking information, while the proximity hidden layer preserves local proximity information. Consequently, this generative framework updates the input embedding vectors using gradients propagated from the output layers, based on an objective function comprising two reward components, each corresponding to a task. The network's design appears straightforward and robust, featuring only one hidden layer, which reduces the number of hyperparameters to be tuned. Furthermore, the authors provide detailed proofs for second-order proximity preservation and global ranking preservation, with an upper bound of PageRank.
The paper is well-structured and easy to follow. Figure 1 is particularly effective, offering a concise visual representation of the model. The experimental section is well-designed, and the results demonstrate that the proposed embedding model outperforms its competitors in tasks such as rank prediction, classification, regression, and link prediction. Additionally, the authors conduct extra experiments to showcase the robustness of their method to noisy data, comparing favorably to state-of-the-art approaches.
Upon reviewing the manuscript, I did not identify any significant flaws. The presentation is clear, and the technical quality is high, making the paper a strong contribution to the field.