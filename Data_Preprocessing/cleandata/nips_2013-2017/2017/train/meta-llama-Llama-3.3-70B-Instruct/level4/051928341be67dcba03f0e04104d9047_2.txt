The manuscript introduces a novel approach to modeling attention for images and questions in Visual Question Answering (VQA) tasks. The key innovation lies in a generic attention formulation that incorporates unary, pairwise, and ternary potentials across three modalities: image, question, and answer. This method is evaluated on the VQA dataset, demonstrating superior performance to existing models when ternary potentials are utilized.
Strengths:
1. The proposed generic attention formulation for multimodal tasks offers versatility, as it is not limited to a specific task and has the potential to generalize across multiple tasks.
2. The ablation study, which assesses performance without ternary potentials, provides valuable insights into the contribution of ternary potentials to the overall performance.
3. The paper is well-structured and clearly written, facilitating easy comprehension of the complex concepts presented.
Weaknesses:
1. The concept of attention over answers, particularly when answers are single words or treated as such even when multiword, lacks clarity. The absence of visualizations for attention over answers further complicates understanding. Therefore, clarification from the authors on this aspect is necessary.
2. The results in Table 1 suggest that the primary improvement in the proposed model stems from the inclusion of ternary potentials. Without these potentials, the model's performance is not superior to existing models in a two-modality setup, except in comparison to HieCoAtt. The authors should provide further insight into this observation.
3. Given the significant impact of ternary potentials on the proposed model's performance, a comparison with existing models that also utilize answers as inputs, such as the Revisiting Visual Question Answering Baselines by Jabri et al. (ECCV16), would be beneficial.
4. The paper does not discuss failure cases of the proposed model, which would be invaluable for guiding future research by highlighting areas for improvement.
5. Notable errors and typos include:
   a. Line 38: "mechanism" should be pluralized to "mechanisms."
   b. Line 237 mentions evaluation on the validation set, yet Table 1 reports results on the test-dev and test-std sets, indicating a discrepancy.
Post-rebuttal comments:
Although the authors' response to concerns about the model's performance in a two-modality setup without ternary potentials was not entirely satisfactory due to the lack of quantitative evidence, the paper's contribution of a generic attention framework for multiple modalities and its quantitative results showing superior performance with three-modality attention justify a recommendation for acceptance. The additional information provided in the rebuttal regarding the quantitative evaluation of the proposed model's attention maps against human attention maps is promising, indicating a higher correlation with human maps than existing models. However, the absence of a correlation value for state-of-the-art (SOTA) models like MCB leaves room for further comparison.
A question arises from one of the authors' responses:
> Authors' response -- "MCB vs. MCT": MCT is a generic extension of MCB for n-modalities. Specifically for VQA, the 3-MCT setup yields 68.4% on test-dev, whereas a 2-layer MCB yields 69.4%. The authors tested other combinations of more than 2-modalities MCB and found them to yield inferior results.
It appears there might be a mistake in the numbers provided. Shouldn't the 69.4% accuracy be associated with the 3-MCT setup? Furthermore, the overall performance of MCB in Table 1 is reported as 68.6%, which does not align with the 68.4% mentioned in the authors' response, prompting confusion about which specific number is being referenced.