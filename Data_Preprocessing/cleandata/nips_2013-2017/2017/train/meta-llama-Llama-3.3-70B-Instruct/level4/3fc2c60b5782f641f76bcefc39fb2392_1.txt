This paper presents a novel approach to training predictive models, wherein performance is evaluated based on an external task rather than traditional likelihood objectives. To achieve this, model parameters are optimized to minimize loss on the external task, which may involve solving a sub-optimization problem dependent on the model parameters. The authors provide synthetic and real-data experiments that effectively demonstrate the proposed approach's usefulness.
The introduction is well-motivated, and the overall exposition is clear. The paper is technically sound, building upon established foundations, and no obvious flaws are apparent.
The paper provides a compelling motivation for the proposed approach, and I consider this work a worthwhile and well-motivated application of existing technical contributions. Although the key technical component enabling this approach, differentiation through an argmax, has been previously presented in [Amos 2016], this work applies it in a relevant and well-motivated formulation of task-based learning, which would likely be of interest to the machine learning community.
A notable aspect of the proposed approach is that the model is not independent of the task, which could be both beneficial and problematic. It would be valuable to investigate how much the model may overfit to the specific task and its ability to generalize to slightly different tasks.
The authors may want to consider referencing related work on meta-learning, such as "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" by Finn et al, as it also involves differentiation through an optimization procedure, providing another relevant example in this context.