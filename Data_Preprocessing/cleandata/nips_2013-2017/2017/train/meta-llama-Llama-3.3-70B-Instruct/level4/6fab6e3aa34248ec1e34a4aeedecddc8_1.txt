Overall Impression:
This paper presents a novel and intriguing concept that has the potential to inspire future research on multi-modal early-fusion methods. Nevertheless, the presentation and writing require further refinement. Although the experiments demonstrate the approach's effectiveness across multiple tasks, they are somewhat limited in scope, which restricts the justification of the proposed method beyond the vision + language application domain. To strengthen this submission, I recommend revising the text and conducting additional experiments with diverse model architectures or different types of multi-modal data.
Strengths:
+ The neurological motivations underlying the CBN approach are well-founded, and its simplicity is appreciated.
+ The comparison between fine-tuning batch norm parameters (Ft BN) and question-conditioned batch norm predictions provides a valuable ablation study. Notably, adapting to new image statistics (Ft BN) yields significant improvements (~1% VQA, 2% Crop GuessWhich), which are further doubled when conditioned on the question (~2% VQA, 4% Crop GuessWhich).
+ The promise of publicly available code to reproduce the experimental results is a significant advantage.
+ The tSNE plots are insightful, illustrating the substantial impact of language conditional modulation on visual features.
Weaknesses:
- The inclusion of Section 2.1 is unclear, as both Batch Normalization and the proposed Conditional Batch Normalization (CBN) are general techniques. The description of the proposed methodology appears to be independent of the model choice, and the time spent describing the ResNet architecture could be better utilized to provide more motivation and intuition for the CBN approach.
- While the neurological motivation for early vision benefiting from language modulation is understandable, the argument for modulating normalization parameters is less convincing, particularly in Section 3. The introduction mentions that the proposed approach reduces over-fitting compared to fine-tuning but fails to discuss CBN in the context of alternative early-fusion strategies.
- Given that CBN is a general method, demonstrating improvements in performance across multiple model architectures for vision + language tasks would be more convincing. For instance, CBN seems directly applicable to the MCB architecture, although memory concerns due to backpropagation through the CNN may be limiting.
- It is surprising that applying CBN to Stage 4 (the highest level stage) accounts for the majority of the improvement in both the VQA and GuessWhat tasks. Additional discussion in this section would be beneficial. The supplementary figures are also interesting, showing that question-conditioned separations in image space only occur after later stages.
- Figures 2 and 3 appear to be somewhat redundant.
Minor things:
- Visualizing how different questions change the feature representation of a single image using a gradient visualization method would be interesting.
- Adding a space before citation brackets would improve readability.
- The bolding of baseline models is inconsistent.
- Equation 2 contains a gammaj instead of gammac.
- Line 34 should be revised to "to let the question attend" instead of "to let the question to attend".
- Line 42 is missing a citation.
- The first discussion of batch norm on Line 53 is missing a citation.
- Line 58 should be revised to "which we refer to as" instead of "to which we refer as".
- Line 89 should be revised to "is achieved through a" instead of "is achieved a".