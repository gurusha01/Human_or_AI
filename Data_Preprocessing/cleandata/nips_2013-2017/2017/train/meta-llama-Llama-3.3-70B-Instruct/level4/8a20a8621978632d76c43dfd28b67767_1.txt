The authors demonstrate that various methods in the literature for explaining individual model predictions can be categorized as "additive feature attribution" methods. They propose a novel additive feature attribution method based on the concept of Shapley values, resulting in explanations referred to as SHAP values. Additionally, they introduce a new kernel, the Shapley kernel, which can be used to compute SHAP values via linear regression, a method they term kernel SHAP. The authors also discuss how other methods, such as DeepLIFT, can be improved by better approximating Shapley values.
Summary of review:
Positives:
(1) The paper presents a novel and sound theoretical framework for approaching model explanations, addressing a significant gap in the field, as most existing methods were developed ad-hoc.
(2) Early versions of this paper have already been cited to justify improvements in other methods, specifically New DeepLIFT.
(3) Kernel SHAP is a significantly superior method for approximating Shapley values compared to classical Shapley sampling, exhibiting much lower variance versus the number of model evaluations, as illustrated in Figure 3.
Negatives:
(1) The algorithm for Max SHAP is incorrect.
(2) The proof for Kernel SHAP included in the supplement is incomplete and hastily written.
(3) The paper contains significant typos, including discrepancies in the runtime of Max SHAP and errors in equations.
(4) The argument that SHAP values are a superior form of model explanation is contentious, with case studies in Figure 4 potentially being cherry-picked to favor SHAP, and the paper lacks discussion on the runtime for kernel SHAP or the recommended number of function evaluations needed for good performance.
Detailed comments:
The adaptation of Shapley values to model explanations is an excellent insight, and the development of kernel SHAP is a significant contribution due to its lower variance compared to Shapley sampling. However, there are several issues with the paper. The algorithm for Max SHAP is flawed, assuming that including a new input contributes to the output based on its difference from the maximum reference value of previously included inputs, without considering the reference values of inputs not yet included. This can lead to incorrect SHAP values, as demonstrated by a concrete example.
The proof for Kernel SHAP in the supplement is incomplete and poorly written, making it difficult to understand. The computational proof provided appears to compare the final Shapely values produced by kernel SHAP to those produced by the classic Shapely value computation for a specific model, rather than comparing the coefficients used in the two methods. The code for the computational proof is also incomplete, and several methods called in the code are not provided.
The claim that SHAP values are a superior form of model explanation is debatable, as they may not always align with human intuition. For instance, in a convolutional neural network with a ReLU activation and negative bias, SHAP values may assign nonzero importance to input pixels even when the input is white noise. The comparison with LIME in Figure 5 is also lacking, and the authors' argument for favoring kernel SHAP over LIME would be more compelling with a direct comparison.
The section on Deep SHAP is problematic, as the authors only discuss two analytical solutions, Linear SHAP and Max SHAP, and the algorithm for Max SHAP is incorrect. The mention of Low-order SHAP is also unclear, as it may not be suitable for many neural networks, and its implementation on a GPU may significantly slow down backpropagation.
Despite these issues, the adaptation of Shapley values to model interpretation and the development of kernel SHAP are substantial contributions, and the paper has been revised to address the concerns and correct the issues.