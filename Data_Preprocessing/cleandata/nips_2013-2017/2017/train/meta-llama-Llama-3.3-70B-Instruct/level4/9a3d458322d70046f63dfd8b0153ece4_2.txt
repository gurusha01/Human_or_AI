This paper presents a fascinating interpretation of batch normalization and ReLU in the context of generalized Hamming distance networks, and introduces variations that yield improvements. The revealed connection, particularly illustrated in Figure 2, is both surprising and intriguing.
The established correspondence between generalized Hamming distance (GHD) and batch normalization (BN) is noteworthy. However, the underlying reasoning behind this equivalence is not entirely clear. It appears that the paper suggests the estimated bias equals the sum of weights and inputs in equation (3), effectively making the weighted sum plus bias equivalent to GHD. Yet, it is unclear whether this equivalence holds on average across all nodes within a single layer or varies across different layers. Furthermore, the primary role of GHD seems to be preventing information loss during the transformation from input to weight. The rationale behind stacking multiple layers of GHD is somewhat difficult to comprehend, and additional clarification on this aspect would be beneficial.
A minor correction is needed: there is a typo in the critical box on line 80.
Overall, this is an excellent paper.
----------------------
After author rebuttal:
I appreciate the authors' rebuttal. My evaluation remains unchanged. Thank you for submitting this outstanding paper!