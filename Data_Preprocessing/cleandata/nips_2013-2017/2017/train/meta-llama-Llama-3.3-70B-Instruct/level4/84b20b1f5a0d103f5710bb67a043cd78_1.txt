Review- Paper Summary:
This paper proposes an improved Greedy Coordinate Descent (GCD) algorithm, termed Accelerated Stochastic Greedy Coordinate Descent (ASGCD), which leverages the advantages of Nesterov's acceleration method and Stochastic Gradient Descent (SGD) to solve high-dimensional sparse and dense optimization problems. The algorithm first reformulates the $l_1$-square-regularized approximate optimization problem using a greedy rule, and then solves it as an exact problem using the SOTOPO algorithm. The solution is further improved by combining the convergence rate advantage of Nesterov's method with the reduced complexity of SGD, resulting in an algorithm with a convergence rate of $O(\sqrt{1/\epsilon})$ and reduced complexity compared to the vanilla GCD.
Originality of the paper:
The SOTOPO algorithm is a novel contribution, as it utilizes the $l_1$ regularization term to investigate potential sub-gradient directions, sorting them to find the optimal direction without requiring the full gradient calculation beforehand. Although the combination of Nesterov's advantage with SGC and GCD advantages is not entirely new, the authors deserve credit for creating an efficient and rigorous algorithm by integrating multiple components.
Contribution:
The paper makes several significant contributions, including:
- Reducing complexity and increasing convergence rate for large-scale, dense, convex optimization problems with sparse solutions,
- Combining existing results to generate a more efficient algorithm,
- Proposing a criterion to reduce complexity by identifying non-zero descent directions and sorting them to find the optimal direction,
- Eliminating the need for full gradient computation beforehand,
- However, the paper lacks a theoretical framework for choosing the regularization parameter $\lambda$ as a function of batch size, which affects the ASGCD's performance in both batch choice cases.
Technical Soundness: 
The paper provides rigorous proofs for all lemmas, corollaries, theorems, and propositions in the supplementary material. The derivations are solid, although referencing basic optimization theorems or lemmas could enhance clarity for non-optimization researchers.
Implementation of Idea: 
The algorithm, particularly the SOTOPO part, is complex to implement.
Clarity of presentation: 
While the paper is detailed, it can be challenging for readers to maintain a clear understanding of the overall picture. Adding reminders of the goal and purpose of each step throughout the paper could improve clarity. The application order of different algorithms or parts could be more transparent with a diagram or pseudo-code. Notation issues, such as unclear explanation of $g$ in equation 3 and typos in Algorithm 1, detract from the presentation. Despite the challenges of writing a mathematically incremental paper, the clarity is decent.
Theoretical basis: 
The paper thoroughly proves all lemmas and transformations in the supplementary material. However, some literature results related to convergence rates or complexities of known algorithms are not referenced, and Remark 1 lacks justification. A comparison of the theoretical solution accuracy with other pre-existing methods would be interesting. A minor error is found in the supplementary material, where a $d \theta_t$ is missing from one of the integrals.
Empirical/Experimental basis: 
The experimental results validate the proposed algorithm's performance compared to chosen algorithms. Consistency in data sets used between algorithms supports a valid experimental analysis. The choice of smoothing constant $T_1$ is provided, but its justification could be clearer. The proposed method underperforms compared to Katyusha for small regularization and specific test cases, suggesting room for improvement or exploration of a threshold in sparsity. The connection between batch size and regularization value, which affects ASGCD's performance, is not discussed.
Interest to NIPS audience [YES]: This paper compares the proposed algorithm with well-established algorithms, making it interesting to the NIPS audience. The discussion may focus on simplifying the algorithm without compromising its performance.