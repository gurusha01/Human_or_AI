This paper examines the impact of eigenvalue decay on function approximation using Gradient Descent (GD), highlighting that GD may require a large number of iterations when approximating non-smooth functions with infinitely differentiable kernels. To address this, the paper suggests using second-order methods or explicit regularization, although the former can be computationally expensive and the latter introduces bias. As a solution, the paper proposes Eigen-Pro, which achieves implicit regularization.
The findings that GD is ineffective for approximating non-smooth functions using infinitely differentiable kernels, and the link to the decay of the kernel's eigenstructure, are both novel and insightful. The introduction of Eigen-Pro, which uses randomization to overcome computational challenges, is a noteworthy alternative with potential practical significance.
However, there are two notable drawbacks. Firstly, the implications of the derived results are unclear, particularly regarding speed-up. The comparisons with PEGASOS show only marginal improvements, and Eigen-Pro is often slower. While it is commendable that the method can match the performance of state-of-the-art algorithms using kernel methods, more explicit comparisons are needed to understand the trade-offs between time and accuracy. It would be beneficial to include results from training a deep network, as used in state-of-the-art approaches, and comparing it with kernel approximation in a synthetic setting to strengthen the understanding of these trade-offs.
Secondly, the longer version of the paper is well-written, but the submitted version is not easily readable. Overall, the paper is well-conceived but falls short in explaining the experimental results and readability.