After reconsidering the paper in light of the rebuttal, I have revised my score to 7. The paper presents an intriguing concept, and although the theoretical aspects are highlighted, I find the empirical results to be more compelling. However, the theoretical framework, including the distinction between input and feature spaces, as well as the convexity assumption, is not entirely convincing.
To strengthen the paper, I suggest emphasizing the connection to classical semi-supervised learning and the cluster assumption, particularly the low-density assumption on the boundary, as discussed in the work by Olivier Chapelle and Alexander Zien, "Semi-Supervised Classification by Low Density Separation". By situating their contribution within the context of established semi-supervised learning research, the authors can better highlight the significance of their work, specifically the idea that the boundary of separation should lie in low-density regions.
The paper provides an analysis of how Generative Adversarial Networks (GANs) contribute to semi-supervised learning in the "K+1" setting. It outlines assumptions under which a complement generator is necessary to enhance the accuracy of the supervised task, implying that the generator should target low-density areas in the input space. An algorithm is proposed, combining feature matching criteria with density estimation using a pixel CNN, which enables the generator to target low-density areas of a fixed estimated model, given a threshold of log probability. Additional entropy regularizers are incorporated to promote diversity in the generator, and positive empirical results are reported.
However, the paper has several limitations:
- The analysis is conducted in the feature space under numerous assumptions, whereas the proposed method operates in the input space, creating a significant mismatch between the analysis and the method. The convexity assumption used throughout the proofs is no longer valid in this context.
- The KL expression (line 238) is incorrect, as the term assumed to be constant is not constant. Instead, it equals '-log(C) P(x ~ p_g, p(x) <= epsilon)', which should also be optimized. An alternative approach to deriving the objective function is needed, potentially involving the minimization of the cross-entropy term with added cross-entropy regularization.
- The paper suggests that the generator should provide samples outside the manifold to reinforce the boundaries of the classifier. However, a more nuanced approach is required, where the generator supplies a portion of samples on the manifold to reinforce positive examples and another portion outside to reinforce negative examples. A probabilistic setup, where assumptions hold with probability 1-delta off the manifold and delta on the manifold, may be more realistic, although corollary 1 is not proven and seems unrealistic.
- An ablation study, including the performance of the method for various epsilon values, would illustrate the discussion above and should be reported. Justification for the choice of the 10th quantile is also necessary.
- The use of density estimation (Pixel CNN) and entropic regularizers with another estimator seems ad hoc. I wonder if the authors have considered alternative approaches to encourage 'low-density sampling' using implicit modeling or hybrid implicit/prescribed modeling while maintaining end-to-end training.
Minor comments include:
- The loss function is Lsupervised + Lunsupervised; have the authors experimented with balancing these terms using a penalty term lambda, and how does this balancing interact with the threshold epsilon?
- The comment in lines 162 and 163 regarding the connection to "the low-density boundary assumption" in Laplacian regularization is unclear and requires further elaboration.
- A typo is present in line 178: 'the a bounded'.
- The proof for corollary 1 is not provided, and it is unclear how the claims hold uniformly over all X, or what it means for |G| to go to infinity; stronger assumptions may be necessary.