Summary: 
The authors introduce a probabilistic data transformation as a preprocessing step to mitigate algorithmic bias in machine learning models by ensuring that the treatment of individuals based on protected attributes such as race and gender is fair. This transformation aims to create a new data distribution that is as close as possible to the original distribution while adhering to two key constraints: 
- Distortion control, which ensures that the representation of each individual does not change significantly, and 
- Discrimination control, which seeks to make the outcome distributions conditioned on protected attributes as similar as possible.
Overall, the proposed method appears to be technically sound and reasonable. However, the experimental results do not convincingly demonstrate its superiority over existing preprocessing techniques, such as Learning Fair Representations (LFR), nor do they provide comparisons with post-processing or in-processing methods. This lack of comparison makes it challenging to determine the practical usefulness and applicability of this new method.
Detailed Comments:
Related Work and Experimental Comparisons:
- Although the appeal of preprocessing over other methods is understood, experimental comparisons with in-processing and post-processing techniques would be beneficial. Such comparisons would help practitioners decide whether investing in more complex procedures is worthwhile.
Methods:
- The approach of defining a unique distortion metric for each new dataset may be challenging. It would be valuable to investigate the sensitivity of the results to the choices made in determining these metrics.
- The distortion constraint, currently formulated in expectation, may lead to significant feature shifts for a small number of individuals, potentially resulting in unfair treatment. This could occur when an individual's features are drastically altered, despite the low probability of such events.
- The discrimination constraint is defined in terms of label distributions and acknowledges the need for conditioning to avoid Simpson's paradox. Discussing other fairness notions, such as calibration, which requires conditioning on predictive scores, would be beneficial. Although the current work may not align with the calibration definition of fairness, an explicit discussion would be valuable.
- The two constraints (distortion and discrimination) in the optimization problem are conflicting, which may lead to settings with no feasible solution. In such cases, practitioners would need to relax their thresholds and retry, a process that seems less than ideal. This situation may indicate something about the dataset, particularly if it requires excessive distortion to achieve fairness.
- The authors assume that a small KL-divergence between the original and target distributions will preserve the utility of the classifier. However, it is unclear whether this assumption is provable, and there may be cases where a small KL-divergence corresponds to a significant change in utility. This could depend on the hypothesis class from which classifiers are chosen.
Results:
- On the COMPAS dataset, LFR appears to outperform the proposed approach, achieving a higher AUC with less discrimination. For the Adult dataset, the operating characteristics differ, making comparisons challenging. It would be useful to explore a range of parameters for LFR to understand its performance on the Adult dataset. Overall, the experimental results do not provide a clear picture of the proposed method's performance relative to LFR. However, the proposed method offers an explicit way to set the discrimination constraint, which is an advantage.
- The losses in AUC resulting from the proposed approach are substantial on both datasets, which raises concerns about the practicality of applying this method in real-world applications. The differences in AUC are more significant than those between different machine learning models, such as log-linear and random forest models.