I appreciate the detailed response provided, which has addressed two of my primary concerns: the weakness of the baseline and the lack of comparison with automatic post-editing. As a result, I have revised my evaluation, with the expectation that these findings will be incorporated into the final version of the paper.
Regarding the examples presented, my previous query about "cherry-picked" examples was motivated by the absence of information on the selection process. It would be beneficial to clarify whether these examples were chosen randomly or through an unbiased method. While it is acceptable to select representative examples, acknowledging this process would enhance the clarity of the paper. Furthermore, a quantitative analysis of the specific parts of the sentence that demonstrate improvement would be valuable, as I mentioned earlier.
The explanation provided on the challenges faced by beam search in recovering from initial search errors is accurate. However, it is essential to note that neural machine translation (NMT) models optimize the full-sentence likelihood, which treats errors equally regardless of their position in the sentence. I believe it is crucial to accurately represent this aspect, as it is a fundamental premise of this work.
This paper proposes a method for training a network to refine the output of a first-pass decoder in neural sequence-to-sequence models. The approach involves training a second-level model using a sampling-based method, where outputs are sampled from the first-pass model. While the idea presented in the paper is potentially interesting, I have several concerns regarding the validity and analysis of the results. It would be beneficial to contextualize the content within the framework of prior work.
Initially, I found the reasoning behind the proposed method to be unclear. The example in the introduction, where a poet uses the final word of a sentence to determine the first word, is noteworthy. However, it is essential to consider that standard neural sequence-to-sequence models, despite decoding from left to right, calculate the joint probability over the entire sentence and indirectly account for later words in earlier decisions. This is because the conditional probability of later words is lower if the model makes poor decisions for earlier words. While this may not hold for greedy decoding, beam search can potentially recover from such errors. A careful discussion of this aspect is necessary.
Regarding the validity of the results, the baselines used in the paper appear to be outdated and not representative of the current state of the art for the respective tasks. Although it is not expected that every paper will surpass the current state of the art, the use of the original Bahdanau et al. attentional model (RNNSearch) for machine translation is likely to omit several years' worth of improvements in neural MT. Consequently, it is challenging to determine how the proposed results will generalize to a stronger model. For instance, the paper "Deep Neural Machine Translation with Linear Associative Unit" by Mingxuan Wang et al. (ACL 2017) achieves single-model BLEU scores on the NIST04-06 datasets that are 4-7 BLEU points higher than the proposed model.
The paper provides a limited qualitative evaluation of examples, which I previously questioned as potentially "cherry-picked." However, there is a lack of analysis on why the proposed method performs better. It would be beneficial to conduct an analysis to demonstrate whether the improvements are indeed due to the network's access to future target words in a quantitative manner. For example, it would be useful to investigate whether the proposed method excels at generating sentence beginnings, where standard models often struggle due to the more information-impoverished setting.
Finally, there is a significant body of prior work that employs similar methodologies or addresses similar problems. Notable examples include automatic post-editing or pre-translation, such as the work by Junczys-Dowmunt and Grundkiewicz (WMT2016) and Niehues et al. (COLING2016). Although these approaches are not jointly trained with the first-pass decoder, they have still demonstrated usefulness. Additionally, the paper by Hoang et al. (arXiv 2017) attempts to perform globally consistent decoding and introduces bilingual or bidirectional models that can address similar challenges.