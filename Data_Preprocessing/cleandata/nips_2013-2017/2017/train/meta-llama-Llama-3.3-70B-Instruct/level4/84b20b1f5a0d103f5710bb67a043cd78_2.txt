This paper proposes a solution to the L1 regularized Empirical Risk Minimization (ERM) problem by integrating several successful techniques, including greedy coordinate descent, Stochastic Variance Reduced Gradient (SVRG), and acceleration. The resulting algorithm, which combines the strengths of Katyusha (an accelerated SVRG method) and greedy coordinate descent, is surprisingly simple and effective. 
The authors' approach is based on the observation that, in the absence of L1 regularization and in the batch setting, greedy coordinate descent can be expressed as a gradient descent step using the L1 norm instead of the standard L2 norm. Although this property is lost when L1 regularization is added, the authors develop a variant of greedy coordinate descent that can be combined with Katyusha to produce an accelerated, stochastic, and greedy method. This new method has the potential to outperform Katyusha in certain settings.
A crucial component of the proposed algorithm is the efficient solution of the key subproblem (3) using a novel method called SOTOPO. This method involves a variational reformulation of the squared L1 norm as a convex minimization problem over the unit simplex, which is then solved as a min-min problem. The SOTOPO method is not only efficient but also of independent interest.
The paper is well-written and presents interesting novel ideas, leading to an efficient method that works in practice across various regimes, including both large and small sample sizes relative to the number of features. The authors also demonstrate improved complexity for the L1 regularized ERM problem in certain regimes.
Some suggestions for improvement include acknowledging earlier contributions to accelerated randomized coordinate descent, such as those by Nesterov, Lee and Sidford, and Fercoq and Richtarik. Additionally, the authors may want to explore connections with similar syntheses of methods, like the combination of SVRG and randomized coordinate descent by Konecny, Qu, and Richtarik. 
In terms of experiments, it would be beneficial to include the APPROX method of Fercoq and Richtarik for comparison, particularly in the high-dimensional and low-sample-size regime. The authors should also provide more insight into the effect of the parameter η on the sparsity level in subproblem (3), even when λ = 0, and discuss the implications for the choice between L1 and L2 norms.
Minor corrections and clarifications are also suggested, including explaining the notation (???) in Step 1 of Algorithm 1, correcting grammatical errors, and providing more precise language in certain sections. 
Overall, the paper presents a significant contribution to the field, and with the suggested improvements, it has the potential to make an even more substantial impact.