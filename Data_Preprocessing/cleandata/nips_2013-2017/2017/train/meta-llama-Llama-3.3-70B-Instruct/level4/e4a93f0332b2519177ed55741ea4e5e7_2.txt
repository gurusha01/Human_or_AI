This manuscript proposes a novel RNN framework that integrates the benefits of stacked multiscale RNNs for capturing long-term dependencies with deep transition RNNs for modeling complex dynamics, enabling rapid adaptation to input changes. The proposed architecture typically comprises four fast RNN cells (specifically LSTMs, as used in the paper) in the lower deep transition layer, and a single slow RNN cell in the upper layer, which receives input from the first fast cell and updates the state of the second fast cell.
The model's performance is assessed on the PTB and enwiki8 datasets, where it achieves the lowest character-based perplexity compared to similarly-sized architectures (in terms of the number of parameters or cells). The provided analysis of vanishing gradients and cell updates offers valuable insights.
One minor suggestion: it appears that Figure 1 should be labeled as $h{t-1}^{Fk}$ for clarity.