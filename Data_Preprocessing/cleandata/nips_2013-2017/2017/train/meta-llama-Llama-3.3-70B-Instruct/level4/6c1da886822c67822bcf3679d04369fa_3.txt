This manuscript is well-structured and engaging to read. It presents a novel density model, termed masked autoregressive flow (MAF), which constructs a normalizing flow by stacking multiple MADE layers. While the techniques employed may seem somewhat incremental, given their prior exploration in IAF and MADE, the paper excels in providing a comprehensive overview of various generative modeling approaches and offering practical guidance on their applications. Additionally, it establishes a connection between MAF and IAF. The following points require further clarification:
* A more detailed motivation for the advantages of density models would be beneficial. What specific applications or downstream tasks render density models more suitable than alternative approaches, such as VAEs or GANs? For instance, does the proposed density model facilitate efficient algorithms for marginalization or other inference tasks?
* When multiple transformations are applied within the normalizing flow, it appears that the conditionals may not necessarily be Gaussian, as indicated in equation 2, since the density is calculated based on the density of u. Given that u can be transformed from lower-level random vectors, which may be non-Gaussian, is this interpretation correct?
* The base density of an MAF, mentioned on line 225, refers to either the density of u in equation 3 or the density of p(xi|x{1:i-1}) in equation 1. It is assumed to be the former; however, confirmation is needed.
* The comparison of results from MAF (5), MAF (10), and MAF MoG (5) on the POWER and MNIST datasets shows that MAF MoG outperforms the other two. It is expected that using multiple normalizing flow layers, each comprising multiple non-linear MADE layers, would make MAF universal without requiring MoG. The authors' perspective on this matter would be appreciated.