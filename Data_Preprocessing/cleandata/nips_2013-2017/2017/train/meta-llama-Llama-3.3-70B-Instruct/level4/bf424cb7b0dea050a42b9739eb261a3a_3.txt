This manuscript examines the convergence challenges of kernel methods optimized via gradient descent, specifically the lengthy time required to reach convergence. The authors introduce the concept of computational reach of gradient descent after a specified number of iterations, as well as the scenario where gradient descent fails to converge to an optimum within a certain epsilon neighborhood after a given number of iterations. They provide illustrative examples of simple function settings with binary labels, demonstrating the slow convergence of gradient descent to the optimal solution. Additionally, they note that while regularization can improve the condition number of the eigenspectrum, potentially leading to better convergence, it can also result in overregularization.
The authors propose the EigenPro method, which involves pre-multiplying the data with a pre-computed preconditioner matrix to accelerate convergence. This approach modifies the eigenvalues, making the smaller ones closer to the largest, and reduces the cost per iteration. By performing a randomized SVD of the data matrix or kernel operator to obtain the eigenvalues, they generate the preconditioning matrix using the ratio of the eigenvalues. The results show that the per-iteration time is comparable to that of standard kernel methods, and experiments demonstrate that EigenPro minimizes errors more effectively, using less overall GPU time. The acceleration provided by EigenPro is substantial for certain kernels.
Although the paper is densely presented, it is well-written and relatively easy to follow. However, additional details could be provided to elucidate the influence of the preconditioning matrix on gradient descent in general and to discuss the scenarios in which it should be applied to data matrices when training linear models. The authors argue that reducing the ratio of smaller eigenvalues to larger ones makes the problem more conducive to convergence. A geometric description or motivation for how the method improves convergence using gradient descent would be beneficial. Furthermore, it would be interesting to investigate whether other faster algorithms, such as proximal methods and momentum-based methods, can also benefit from preconditioning and improve the convergence rates of kernel methods.