Summary: This paper presents a novel gradient boosting approach, referred to as CEGB, designed for predictive modeling under budget constraints. The CEGB methodology involves minimizing empirical loss in conjunction with a cost penalty term during the training phase, incorporating three distinct types of costs: feature cost per example, feature cost across all examples, and evaluation cost. The algorithm iteratively seeks the optimal split that minimizes the second-order approximation of the loss objective in a greedy manner. Experimental evaluations on select datasets demonstrate improved performance of CEGB over current state-of-the-art methods.
Significance: The proposed CEGB algorithm bears a close resemblance to the GreedyMiser approach (Xu et al., 2012), with the primary distinction lying in the construction of weak regressors. A key contribution of CEGB is its ability to introduce new splits at any existing leaf node, rather than adhering to a predefined structure. To mitigate computational complexity associated with identifying the optimal leaf node for splitting at each iteration, CEGB utilizes a second-order approximation of the loss function. Although concepts such as best-first tree learning and second-order loss approximation are well-established in the gradient boosting literature, their application to cost-efficient settings represents a novel contribution. Thus, CEGB can be viewed as a refinement of the GreedyMiser algorithm.
Results: The evaluation of CEGB is limited to comparisons with state-of-the-art methods, specifically BudgetPrune and GreedyMiser, across two datasets: Yahoo and MiniBooNE. Notably, the authors omitted the Forest dataset, which was utilized in the BudgetPrune study (Nan et al., 2016). In the reviewer's experience, gradient boosting-based methods often underperform compared to random forest-based methods, such as BudgetPrune, on the Forest dataset. The inclusion of additional datasets for comparative analysis would strengthen the paper. Furthermore, the exceptionally favorable results for CEGB without tree cost, as depicted in Figure 2(b), seem surprising and warrant further clarification. Specifically, details regarding the number of trees employed and the average number of nodes per tree would provide valuable insight into the achievement of such performance.
Others: The authors' assertion that BudgetPrune assumes feature computation to be significantly more expensive than evaluation costs, thereby rendering the latter irrelevant in several experiments, is inaccurate. The original formulation of BudgetPrune by Nan et al. (2016) does indeed account for evaluation costs and allows for trade-offs with feature costs.
Overall: In the reviewer's opinion, this paper offers a refinement of the GreedyMiser algorithm, with experimental results indicating promise. The incorporation of additional datasets and comparisons would further enhance the paper's strength. Following discussions with fellow reviewers, while the novelty of this work may be somewhat diminished by the existence of XGBoost and GreedyMiser, the demonstrated improvement over state-of-the-art methods is noteworthy. The authors' commitment to including additional datasets in the final version is appreciated, and as such, the reviewer is inclined to recommend acceptance of this paper.