This manuscript introduces a novel neural network architecture, termed SphereNet, which substitutes traditional dot product operations with geodesic distance-based convolutional operators and fully-connected layers. Additionally, SphereNet imposes a regularization constraint on the softmax weights to maintain a norm of 1, facilitating angular softmax. The experimental results demonstrate that SphereNet achieves enhanced performance in terms of accuracy and convergence rate, while also alleviating the issues of vanishing and exploding gradients in deep neural networks.
Originality: The concept of replacing dot product similarity with angular similarity is not new in the deep learning literature, with previous works primarily focusing on its application to softmax or loss functions. However, this paper's introduction of spherical operations for convolutional layers is a novel contribution, as far as I am aware.
Impact: The spherical operations presented in this paper yield faster convergence rates and improved accuracy performance, while also addressing the long-standing issue of vanishing and exploding gradients caused by dot product operations. This is likely to be of significant interest to the machine learning community.
Suggestions for Improvement:
- The removal of bias from the angular softmax equation may limit its calibration capabilities, particularly for imbalanced datasets. It would be beneficial for the authors to reinstate the bias term or evaluate the current model's performance on an imbalanced dataset to assess its effects.
- The initial convergence rates of baseline methods, such as standard CNNs, appear to be faster in the first two subfigures of Figure 4, but are followed by a significant accuracy drop. The underlying cause of this phenomenon is not explained in the paper and warrants further investigation.
- Although the experimental results demonstrate faster convergence rates in terms of accuracy versus iteration, it is unclear whether this trend holds when considering accuracy versus time. Traditional dot product operators can leverage optimized matrix multiplication libraries for accelerated computation, whereas angular operators have limited support, which may impact their performance.
Minor grammatical corrections:
- Line 216: "this problem will automatically solved" should be revised to "be solved".
- Line 319: "The task can also viewed as a" should be revised to "be viewed".