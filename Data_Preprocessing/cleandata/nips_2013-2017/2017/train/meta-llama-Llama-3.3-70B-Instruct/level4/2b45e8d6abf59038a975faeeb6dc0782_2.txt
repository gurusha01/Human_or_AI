This paper introduces the population matching discrepancy (PMD) as a superior alternative to Maximum Mean Discrepancy (MMD) for distribution matching tasks. It is demonstrated that PMD is essentially a sampled version of the Wasserstein metric, also known as earth mover's distance, offering several advantages over MMD, including more robust gradients, the ability to utilize smaller mini-batch sizes, and potentially fewer hyperparameters.
For the purpose of training generative models, at least, MMD is known to be hindered by weak gradients and the necessity for large mini-batches, making the proposals outlined in this paper a welcome solution to these issues. The claim regarding smaller mini-batch sizes is convincingly supported by the empirical results. However, the verification of the claim regarding stronger gradients is less conclusive, as the performance of MMD is dependent on the scale parameter sigma, making it crucial to consider either the optimal sigma or a range of sigmas when making such comparisons.
Regarding the assertion that PMD has fewer hyperparameters, it appears to be less substantiated, as PMD relies on a distance metric that may itself introduce additional hyperparameters, similar to MMD. Furthermore, accurately determining a reliable distance metric in high-dimensional spaces can be challenging, which means PMD may face similar limitations as MMD in this regard. It is also worth noting that established heuristics exist for selecting the bandwidth parameter in MMD, which could be used to make MMD effectively hyperparameter-free, and such heuristics should be considered for comparison.
In summary, the proposed PMD method has the desirable property of allowing for small mini-batch sizes, facilitating faster training, and appears to offer a legitimate improvement over MMD methods that require large batches. Nevertheless, PMD's reliance on a distance metric may restrict its effectiveness in modeling high-dimensional data, a limitation that warrants further consideration.