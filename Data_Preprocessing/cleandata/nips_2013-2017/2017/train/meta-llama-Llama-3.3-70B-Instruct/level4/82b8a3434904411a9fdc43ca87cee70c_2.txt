The paper presents a novel approach to optimization problems commonly encountered in machine learning, where the loss function to be minimized is a sum of smooth-convex functions paired with a convex regularization potential. This method is specifically designed to handle perturbations in the data, which introduce a stochastic component that necessitates modifications to Stochastic Gradient Descent (SGD) to reduce gradient variance, as highlighted in references [14,28]. The authors argue that existing modifications to SGD for variance reduction are limited in their applicability to inline training due to requirements such as storing full gradients or dual variables. They propose a variant of SGD, termed S-MIMO, which overcomes these limitations while preserving the accelerated characteristics of SCG variants for batch processing.
A primary concern with the paper is the proliferation of SGD variants in recent years, making it challenging to assess the fairness of the presented comparisons. For instance, an experiment not included in the paper could involve utilizing the SVRG method [14], which requires computing the full gradient after a specified number of stochastic iterations, assuming all data is available. The authors dismiss SVRG for inline training based on this requirement. However, considering the case of finite data, where each perturbed data point can be viewed as new data, it would be intriguing to explore SVRG's performance in an epoch-based strategy, where different perturbations are used in each epoch, and compare it with S-MIMO. Similarly, SVRG could be applied in batch processing by updating the full gradient after each batch.
If the proposed idea of using SVRG in optimization problems of the form (2) proves viable, it raises the question of whether this would justify a new paper proposing the "new method." In my opinion, and likely in the view of NIPS, the answer is no, as this would contribute to the saturation of the literature with minor variants.
In summary, the authors should strive to compare their algorithm with as many variants as possible, including those that might require trivial modifications, such as the one outlined. 
Note: Without thoroughly checking the algebra of all proofs in the supplementary material, the text appears to be correct, and the reasoning follows logically.