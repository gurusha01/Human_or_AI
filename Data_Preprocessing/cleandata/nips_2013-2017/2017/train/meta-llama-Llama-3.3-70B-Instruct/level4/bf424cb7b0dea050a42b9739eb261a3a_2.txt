This manuscript provides an in-depth examination of gradient descent as a learning strategy for kernel methods, yielding a key finding that highlights the limitations of gradient descent in effectively approximating certain functions. Specifically, the concept of 'computational reach' is introduced, demonstrating that for smooth kernels, gradient descent's 'computational reach' encompasses only a limited subset of the function space. To address this constraint, the authors propose a novel approach termed "EigenPro", which employs a preconditioning strategy to enhance the method's capabilities. The implementation and evaluation of EigenPro on benchmark datasets are efficiently executed, with the results indicating consistent improvements over existing methods. However, the statistical significance of these improvements is not assessed. 
Overall, the paper is well-structured and clearly conveys its ideas, with a convincing experimental setup and results. Although a detailed verification of the mathematical claims was not conducted, they appear to be theoretically sound. In conclusion, this work offers valuable insights into the functioning and potential enhancements of large-scale kernel learning, making it a notable contribution to the field.