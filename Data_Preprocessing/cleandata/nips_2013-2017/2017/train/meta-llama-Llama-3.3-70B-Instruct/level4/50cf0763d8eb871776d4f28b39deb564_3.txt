The paper presents a comparative analysis of generative and discriminative models within the context of general exponential families, including high-dimensional settings. It introduces a concept of separability for general loss functions and establishes a framework for deriving $l{\infty}$ convergence rates for M-estimators. This theoretical framework is then applied to examine the convergence rates of both generative and discriminative models in $l{\infty}$ and $l_2$ norms.
Although I encountered difficulties in following the paper, which may be attributed to my limited expertise in high-dimensional estimation, the introduction and background sections (Section 2) are clear and accessible. However, the subsequent sections become increasingly dense with technical results and equations, sometimes presented without sufficient guidance for the reader. Certain author comments, such as the comparison between Corollary 4 and 3 in lines 215-218, led to confusion. Specifically, it is stated that the generative approach achieves the same convergence rate as the discriminative approach but with only a logarithmic dependence on the dimension, whereas the dimensional dependence in both corollaries appears to be identical, expressed as $\sqrt{\log p}$.
The central idea of the paper revolves around the notion of loss separability. The introductory example illustrates how, under a conditional independence assumption, the log-likelihood loss in a generative model can be fully separated into components that can be estimated independently. In contrast, the loss in the corresponding discriminative model (e.g., logistic regression) exhibits less separability, motivating the introduction of a more flexible separability concept in Section 3. It is found that losses with higher separability require fewer samples to achieve convergence.
However, the definition of separability (Definition 1 and 2) as the upper bound of the error in the first-order approximation of the gradient by the error in the parameters lacks clear motivation in relation to the intuitive notion of loss separability into multiple components. Enhancing the paper's readability would require the authors to clarify why this specific definition is used and its connection to the intuitive understanding of separability.
Starting with Corollary 3, the notation "$\succsim$" is used without explanation, leaving its meaning unclear.
Given my limited familiarity with prior work in this area (aside from Ng & Jordan's contributions), it is challenging for me to assess the paper's significance. Nonetheless, I found the results intriguing, albeit in need of clearer presentation. The theoretical contribution is evaluated as strong, but a clear guideline on when to prefer generative over discriminative methods in high-dimensional settings (akin to the message conveyed by Ng & Jordan) is missing.
Minor corrections and suggestions include:
- Line 36: "could be shown to separable" should be "could be shown to be separable".
- Line 65: "n" should be replaced with "in".
- Line 87: The subscript should include "and j".
- Line 259: "bayes" should be capitalized as "Bayes".
- Lines 254-265: The "Notation and Assumptions" paragraph defines the Bayes classifier after its error has already been mentioned, which could be rearranged for clarity.