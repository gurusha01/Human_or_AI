This paper introduces a novel reduction technique for converting multi-class classification problems with a large number of classes into binary classification problems. The proposed method involves mapping input-class pairs to scores, with an underlying loss function that measures the proportion of incorrect classes assigned higher scores than the true class. Specifically, the authors utilize a feature transformation, denoted as phi, to map input-class pairs into a p-dimensional feature space, and subsequently learn a scoring function from this feature space.
The reduction builds upon the work of Joshi et al. (2015), which generates K-1 transformed data points for each input data point, effectively comparing the true label with all possible incorrect labels. However, the resulting transformed dataset exhibits correlated training examples, rendering many standard generalization bounds inapplicable. The authors address this limitation by deriving improved generalization bounds using new estimates of the fractional Rademacher complexity.
A key contribution of this paper is the incorporation of sampling techniques to balance class distributions and reduce the size of the transformed training dataset. By sampling each class with replacement, the authors ensure roughly equal representation of all classes in the transformed data. Furthermore, they demonstrate that using a random sample of classes for comparisons, rather than all K-1 possible comparisons, does not significantly impact the generalization bounds. The empirical evaluation of this new procedure yields competitive results against several strong baseline methods.
The paper presents an intriguing reduction from multi-class to binary classification, along with corresponding generalization bounds for handling correlated data in the reduced dataset. The novelty of the proposed reduction lies in the sampling technique employed for rebalancing class distributions and reducing dataset size, whereas the basic reduction methodology has been previously explored by Joshi et al. The empirical comparisons with existing algorithms are robust and convincing.
One notable omission in the paper is a theoretical analysis of the prediction algorithm outlined in Section 2.3, which restricts comparisons to the k classes with centroids closest to a given input point. Additionally, a more in-depth discussion of the sample complexity bounds, potentially comparing them to generalization bounds obtained for non-transformed multi-class problems, would have been beneficial.