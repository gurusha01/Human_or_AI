The authors employ the concept of generalized Hamming distance to provide insight into the effectiveness of Batch normalization and ReLU units. 
Upon reviewing the paper, I remain unclear about its core contribution. The authors assert that generalized Hamming distance offers a superior understanding of batch normalization and ReLUs, elaborating on this in two paragraphs on pages 4 and 5. Their explanation of batch normalization is encapsulated in the statement: 
"It turns out BN is indeed attempting to compensate for deficiencies in neuron outputs with respect to GHD. This surprising observation indeed adheres to our conjecture that an optimized neuron should faithfully measure the GHD between inputs and weights."
However, I fail to comprehend how this explanation sheds light on the performance or effects of batch normalization.
The authors subsequently propose a generalized Hamming network, suggesting that it "demystified and confirmed effectiveness of practical techniques such as batch normalization and ReLU". 
In my assessment, this paper is poorly written, lacking significant technical contribution or novelty, and fails to provide theoretical insights into the effectiveness of BN or ReLUs. Beyond the unclear novelty and technical contribution, the paper is marred by numerous typos, grammatical, and syntactical errors, as exemplified in the abstract and introduction.
This warrants a clear rejection.
Notable errors include:
—— abstract —— 
- "generalized hamming network (GNN)" should be "generalized hamming network (GHN)"
- "GHN not only lends itself to rigiour analysis" should be "GHN not only lends itself to rigorous analysis"
- "but also demonstrates superior performances" should be "but also demonstrates superior performance"
—— intro —— 
- "computational neutral networks" should be "computational neural networks"
- "has given birth" should be "have given birth"
- "to rectifying misunderstanding of neural computing" is unclear
- "Once the appropriate rectification is applied ," should be "Once the appropriate rectification is applied,"
- "the ill effects of internal covariate shift is automatically eradicated" should be "the ill effects of internal covariate shift are automatically eradicated"
- "The resulted learning process" should be "The resulting learning process"
- "lends itself to rigiour analysis" should be "lends itself to rigorous analysis"
- "the flexaible knowledge" should be "the flexible knowledge"
- "are equivalent and convertible with other" should be "are equivalent and convertible with others" or "other architectures"
- "successful applications of FNN" should be "successful applications of FNNs"