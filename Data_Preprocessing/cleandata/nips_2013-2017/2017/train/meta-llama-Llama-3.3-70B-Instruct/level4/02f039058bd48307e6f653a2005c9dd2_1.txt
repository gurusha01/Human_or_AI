This paper presents a model-powered approach for conducting conditional independence tests on iid data, leveraging nearest neighbor bootstrap to generate samples that closely follow the conditional independence distribution, denoted as $f^{CI}$. A classifier is then trained and tested to determine if it can distinguish between the observation distribution and the bootstrapped distribution. If the classifier's performance is comparable to a random guess, the null hypothesis that the data follows conditional independence is not rejected; otherwise, it is accepted.
The authors establish bounds on the closeness of the nearest neighbor bootstrapped distribution to $f^{CI}$ in terms of total variation distance and bounds on empirical risks under ideal classification settings and for near-independent samples. The paper tackles an important problem and is presented clearly.
Detailed comments are as follows:
Major:
1. The methodology appears to be separable into two primary components: generating samples that mimic $f_{CI}$ and using a classifier for determination. It is worth exploring whether either step can be substituted with existing solutions from the literature. For instance, combining the permutation-based method from [7] with the classification-based approach, or using nearest neighbor bootstrap with the kernel two-sample test, could yield interesting performance comparisons.
2. In the finite sample size setting, the nearest neighbor bootstrap distribution approximates $f_{CI}$. If the ground truth indicates that $x$ and $y$ are weakly dependent given $z$, it is unclear how the proposed method would differentiate this scenario.
3. The method's symmetry with respect to $x$ and $y$ is questionable. From a causal perspective, $x$ and $y$ can be dependent given $z$ due to different causal models (e.g., $x$ causes $y$ or $y$ causes $x$). In such cases, determining which variable to sample becomes crucial, a consideration not addressed by the authors.
4. The selection of the parameter $\tau$ in Algorithms 2 and 3 raises significant concerns.
Minor:
In Algorithms 2 and 3, the empirical risk should be normalized by dividing it by the sample size to ensure accurate calculations.