This paper presents an approach to enhance the performance of the generative moment matching network (GMMN) by learning the kernel of the maximum mean discrepancy (MMD). The resulting optimization process resembles a min-max game, analogous to generative adversarial networks, where the adversarial kernel is trained to maximize the MMD distance between the model and data distributions, while the generator is simultaneously trained to deceive the MMD. The MMD kernel must be characteristic and is composed of Gaussian kernels with an injective function learned using an autoencoder.
I have several inquiries for the authors:
In Algorithm 1, it appears that the decoder of the autoencoder, f_dec, is not learned. Is it simply the transpose of the encoder, and if so, what is its architecture and what type of up-sampling does it employ?
Is the encoder of the autoencoder, f_Ï†, identical to the discriminator of the DCGAN, and if so, how does it maintain injectivity given the presence of max-pooling layers in the DCGAN discriminator? Furthermore, what is the reconstruction quality of the autoencoder, considering the low-dimensional latent space (approximately 100 dimensions) and the use of max-pooling?
Is autoencoder training truly necessary for the algorithm to function, or is it primarily required to ensure the injectivity of the function, as this aspect is not present in similar algorithms like W-GAN?
The proposed algorithm can be viewed as generative moment matching in the code space of an autoencoder, where the autoencoder's features are also trained to maximize the MMD distance. The algorithm bears similarities to WGAN, with the key difference being the matching of high-order moments via the kernel trick, rather than just the means. While I do not anticipate this algorithm to significantly outperform other GAN variants, I find the underlying intuition and its connections to GMMN and W-GAN to be intriguing.