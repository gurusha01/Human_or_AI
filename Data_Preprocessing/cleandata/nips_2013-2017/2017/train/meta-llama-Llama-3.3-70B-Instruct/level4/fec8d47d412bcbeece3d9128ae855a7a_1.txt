The paper presents a novel approach to controlling the false discovery rate (FDR) for p-values by incorporating additional information. Each hypothesis is associated with a p-value, pi, and a feature vector, Xi. The method learns an optimal threshold for each hypothesis as a function of X_i, which appears to be an interesting and innovative concept. The paper is generally well-explained, and the authors demonstrate the effectiveness of their method in simulated and real-data examples, showing that it can increase the number of rejections for a given FDR control threshold by leveraging the additional information.
A crucial aspect to consider is that the feature vectors Xi should not be used in calculating the p-values, pi, to avoid circularity issues. Although the authors touch on this point in an example, a formal discussion on the probabilistic relationship between Xi and pi is lacking. It is likely that both pi and Xi depend on the validity of the null versus alternative hypothesis, but if Xi has already been used to calculate pi, it should not improve the decision boundary.
The authors' claim that non-parametric methods are ruled out due to the multidimensionality of X is not entirely convincing. For instance, nearest-neighbors regression can adapt to the intrinsic dimension of the regression function, even when the dimension of X is large.
The robustness of the proposed cross-validation procedure to overfitting is unclear. This setting deviates from standard supervised learning, as the true "label" FDR is unknown and replaced by an estimator. The authors should emphasize and elaborate on this point, as the noisy estimation of FDR may result in higher FDR for the test set. Although Theorem 1 provides a bound that grows with the number of folds in cross-validation, this issue warrants further discussion.
The mirror estimator proposed by the authors may exhibit low bias but high variance when t(x) is small, as few p-values will fall between 1-t(x) and 1. This concern is reminiscent of Story's approach, where the lambda parameter is chosen to balance variance and bias by estimating the null proportion using the interval [lambda, 1].
Minor corrections:
======
Line 136: 'this' should be replaced with 'these'.