This manuscript introduces a new framework, the Fast Slow Recurrent Neural Network (FS-RNN), which seeks to combine the advantages of multiscale RNNs and deep transition RNNs. The authors conducted thorough empirical evaluations against various state-of-the-art RNN architectures. 
They have also made their implementation openly available, along with publicly accessible datasets, thereby facilitating the reproducibility of the described experiments. However, it would be beneficial if the authors were to compare their methodology to other cutting-edge systems, such as Tree-LSTM, which is capable of capturing hierarchical long-term dependencies, to further contextualize their approach.