This paper presents a two-stage decoding approach for Seq2Seq models, where an initial output sequence is generated, followed by the creation of an improved target sequence using attention on both the source and the initially generated target sequence. The underlying motivation is that this process allows for consideration of the entire target sequence when making decisions about word choice.
While the authors demonstrate some improvement in BLEU scores using this method, the overall approach seems somewhat unconventional. Seq2Seq models have superseded traditional statistical machine translation (SMT) methods, such as phrase-based models (PBSMT), due to their end-to-end framework and global training objective, which replaces the need for multiple individually trained models combined in a log-linear framework.
Essentially, the proposed method involves training a second neural machine translation (NMT) model to translate from an initial target hypothesis to an improved one, bearing resemblance to statistical post-editing (SPE). However, this comes at a significant computational cost, with a 50% increase in training time and a twofold decrease in decoding speed.
It is unclear whether beam search is employed for both decoder networks. Furthermore, the baseline model used for comparison is somewhat outdated, as RNNSearch was one of the first NMT models. Given the advancements in architecture since then, it would be beneficial to demonstrate that the proposed two-stage approach yields improvements over more contemporary systems, including those with deeper encoder and decoder architectures, which the authors themselves mention.
A potential alternative baseline, which is relatively straightforward to implement, would involve performing a forward and backward decode (from last to first word) and combining the results of both decodes.