This paper introduces the concept of Population Matching Discrepancy between two distributions, defined as the Wasserstein distance between minibatches from the respective distributions, which can be computed using either an exact algorithm with a time complexity of O(N^3) or an approximate algorithm with a time complexity of O(N^2).
The strengths of the paper include:
- The experimentation with the computation of the Wasserstein distance is noteworthy, although the quality of the generated images does not surpass those produced by the Wasserstein GAN.
However, several weaknesses are identified:
- The proposed distance measurement may require a large N to accurately estimate the Wasserstein distance between two diverse multimodal distributions, potentially leading to issues, such as those that could arise when attempting to match a mixture of Gaussians, including the learning of variances.
- If N is insufficiently large, the optimization process may converge to a global minimum that differs from the true distribution, potentially resulting in a learned distribution with lower entropy, as suggested by the limited diversity of SVHN samples in Figure 4, where digit 8 appears with high frequency.
Minor errors include:
- A typo in Line 141, where "usally" should be corrected to "usually".
Following the rebuttal, additional experiments were provided, which are appreciated. However, the authors should further clarify the limitations of both Population Matching Discrepancy (PMD) and Maximum Mean Discrepancy (MMD). Notably, MMD can be effectively used with a batch size as small as 2 and can be trained using SGD if the unbiased estimator of MMD from the original "A Kernel Two-Sample Test" paper is utilized, allowing MMD to converge to the correct distribution even with small minibatches. In contrast, PMD lacks a known unbiased estimator of the gradient, highlighting a significant difference in the capabilities of these two methods.