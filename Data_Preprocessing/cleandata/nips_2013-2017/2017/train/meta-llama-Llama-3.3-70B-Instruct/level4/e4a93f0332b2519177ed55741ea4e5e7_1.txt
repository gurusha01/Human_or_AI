This paper introduces a novel RNN architecture, termed Fast-slow RNN, which demonstrates enhanced performance on several language modeling datasets.
Strengths:
1. The proposed algorithm effectively integrates the benefits of a deeper transition matrix (fast RNN) and a shorter gradient path (slow RNN), resulting in improved performance.
2. The algorithm's simplicity and versatility allow it to be applied to various RNN cells, making it a widely applicable solution.
Weaknesses:
1. The initial sections of the paper are challenging to follow due to the dense presentation of previous approaches without clear explanations for each method. Specifically:
(1) The conversion of the stacked LSTM in Fig 2(a) to the sequential LSTM in Fig 2(b) is unclear, particularly with regards to the handling of $h{t-1}^{1..5}$ and $h{t-1}$.
(2) The sentences in lines 96, regarding the lower hierarchical layers and their relation to time, are difficult to comprehend.
2. The multi-scale concept appears to be somewhat misleading, as the slow and fast RNNs operate on logical time scales when the stacks are sequentialized, rather than distinct physical time scales. Consequently, the primary advantage seems to be the reduction of the gradient path via the slow RNN.
3. Alternative approaches, such as utilizing Residual Units or fully connecting stacked cells, could potentially achieve similar gradient path reduction with greater simplicity. However, these methods are not discussed or compared in the paper.
4. The experimental results lack standard deviations, making it difficult to assess the statistical significance of the reported improvements.