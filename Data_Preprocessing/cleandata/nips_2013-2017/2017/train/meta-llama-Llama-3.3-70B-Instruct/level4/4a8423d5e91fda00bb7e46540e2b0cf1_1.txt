This paper presents a pragmatic approach to integrating selective classification capabilities into an existing neural network. The methodology comprises three key steps:
1. Selecting a score function to quantify the network's confidence in its predictions, with an examination of MC-dropout scores for dropout-trained networks and the maximum softmax score for softmax output networks, the latter of which yields better empirical performance.
2. Specifying the desired confidence level and error rate.
3. Employing a binomial search to determine a score threshold, ensuring that the classifier achieves an error rate below the specified level with the desired confidence, leveraging an existing bound on the true error rate (Lemma 3.1) and incorporating a Bonferroni correction on the confidence level (Algorithm 1).
The experimental results substantiate the efficacy of this approach, demonstrating a strong correlation between the algorithm's input parameters (desired error rate) and the observed empirical error rates on a test set.
The paper's strengths lie in its practical applicability, particularly with the softmax response score function, which enables seamless integration with any pre-trained neural network, and the straightforward specification of the algorithm's desired confidence level and error rate, inspired by reference [5]. Although the paper builds upon established concepts, the meticulous validation of these concepts significantly enhances its value.
However, the paper would benefit from the inclusion of simple baselines to underscore the importance of utilizing the binomial search and the bound on the classifier's error rate. Specifically, it would be informative to investigate the outcome of selecting the score threshold as the lowest value for which the error rate on a given tuning set falls below a specified value, and to compare these results with those obtained using the bound from Lemma 3.1. The inclusion of such a baseline would provide a more compelling motivation for the advanced techniques employed in the paper and would likely increase its overall score.
One minor issue is the use of an uninitialized variable r* in Algorithm 1.