The paper explores kernel regression in high-dimensional settings, initially considering the "additive model" which assumes a form of f = sump fp(X_p), thereby neglecting correlations between different dimensions. Recognizing the limitation of ignoring such correlations, the authors propose the "group additive model", where dimensions are grouped to capture correlations between them. They provide a formal framework for this setting and introduce algorithms, albeit somewhat ad hoc, for determining optimal groupings. The results are demonstrated on small synthetic and real-world datasets.
A significant concern with the paper is its inadequate discussion of related work, making it challenging to assess the novelty of the contributions. The task of finding optimal groups in data overlaps with areas like "structured sparsity" and learning graphical models/Bayesian networks, yet these are not mentioned. Similarly, the problem can be viewed as a feature selection instance, but this connection is also omitted. The related work section primarily references a single KDD-2015 paper [8], which is insufficient for evaluating the paper's novelty.
The theoretical approach, however, appears novel. The authors define a complexity measure for groupings that favors many small groups, derived as an upper bound on the covering number. While this measure is sensible, its optimization is not straightforward, leading the authors to propose either an exhaustive search, which scales poorly, or a greedy method akin to forward feature selection. Although optimizing discrete functions is generally challenging, it is somewhat disappointing that a more efficiently optimizable measure was not developed. The authors mention investigating different measures but do not provide further insights, making it difficult to assess the suitability of their chosen measure.
Experimentally, the paper presents a simple synthetic validation and an example using the Boston Housing data, lacking baseline comparisons with other methods. This omission makes it difficult to evaluate the quality of the results.
Following the rebuttal, some concerns were addressed, but the experimental section remains underdeveloped, and the relation to broader machine learning concepts is still inadequately discussed. While the paper has valid contributions, and the score has been improved, it is strongly recommended that the authors enhance their discussion of related work in the published version.