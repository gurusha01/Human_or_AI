This paper proposes a framework for integrating human feedback into reinforcement learning (RL) and demonstrates its application to image captioning. The approach involves collecting human feedback on machine-generated captions, which includes scoring and correction, and then training a feedback network to mimic human feedback assignment for phrase correctness. The captioning model is subsequently trained using RL, with rewards derived from both automated metrics, such as weighted BLEU, and simulated human feedback based on the feedback network. Experiments were conducted on the COCO dataset to assess the model's effectiveness.
Although the task is intriguing, there are areas for improvement, including:
1. The paper's novelty is not entirely clear. While the concept of incorporating human feedback into RL training is interesting, the implementation is a two-stage batch process rather than an online human-in-the-loop setting, necessitating the use of a simulator, such as the feedback network, to estimate human feedback during RL training, a common approach in many RL settings. Furthermore, using RL to optimize non-differentiable metrics, as discussed in the related work, is not a new concept.
2. The major results reported on the COCO dataset in Table 6 show that the baseline performance is significantly lower than the current state-of-the-art (e.g., BLEU-4 scores are typically around 30%, whereas the baseline result in Table 6 is around 20%). Moreover, the improvement achieved by incorporating feedback over the baseline is less than 0.5% in terms of BLEU-4, which is generally not considered statistically significant.
3. The paper lacks a detailed analysis and examples illustrating how feedback contributes to the RL framework. For instance, given that the RL baseline (RLB) and the model with feedback (RLF) yield similar scores, it is unclear whether they are making similar errors or if the feedback leads to distinctly different predictions.