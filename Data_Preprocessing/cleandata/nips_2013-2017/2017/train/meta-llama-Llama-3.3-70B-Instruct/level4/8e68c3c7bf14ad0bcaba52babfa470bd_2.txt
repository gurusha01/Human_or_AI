The manuscript proposes a novel approach to automatic image captioning, where the model leverages both ground truth captions and human-provided natural language feedback during training. This is achieved through a reinforcement learning framework, which fine-tunes a phrase-based captioning model initially trained using maximum likelihood estimation. The reward function is a weighted sum of BLEU scores with respect to ground truth captions and human feedback, as well as phrase-level rewards derived from human feedback.
The model is trained and evaluated on the MSCOCO image caption dataset and compared to two baselines: a supervised learning model and a reinforcement learning model without human feedback. The results show that the proposed model significantly outperforms the supervised learning model and slightly outperforms the reinforcement learning model.
The strengths of the paper include its well-motivated idea of incorporating human feedback into image captioning models, the use of reasonable baselines, and the potential significance of the work if the improvements over the reinforcement learning baseline are substantial. The additional experiment comparing the use of one ground truth caption versus one feedback caption is also insightful.
However, there are several weaknesses that need to be addressed. Firstly, the paper's motivation is based on using natural language feedback, but the proposed feedback network also utilizes additional information, such as the incorrect phrase, the correct phrase, and the type of mistake. This should be clarified in the introduction. Secondly, the improvements over the reinforcement learning baseline are not substantial, and the authors should verify whether these improvements are statistically significant. Thirdly, it is unclear how much the additional information about incorrect phrases and mistake types contributes to the feedback network's performance, and the authors should provide ablation studies to investigate this.
Furthermore, there are several areas where the paper requires clarification or improvement. The use of cross-entropy loss for the first few phrases in the feedback network is not well-justified, and the authors should explain the rationale behind this choice. The performance of the model when using reinforcement learning for all phrases should also be reported. Additionally, the authors should clarify why the official test set of MSCOCO is not used for reporting results.
The results in Table 5 show that the performance degrades when using additional information about missing, wrong, or redundant phrases, and the authors should provide an explanation for this. The MLEC accuracy using ROUGE-L in Table 6 is also surprisingly low, and the authors should verify whether this is a typo. The authors should also discuss the failure cases of the proposed model to guide future research.
Finally, there are several minor errors and typos that need to be corrected, including incorrect phrasing, missing words, and punctuation errors.
In the post-rebuttal comments, the authors acknowledge the importance of proper evaluation and agree to verify that the baseline results are comparable and that the proposed model is adding value on top of the baseline. Based on this, the rating is revised to marginally below the acceptance threshold.