Review Summary:
The authors present a novel approach to image captioning by integrating human feedback in natural language, which involves capturing a snapshot of a phrase-based caption model, generating captions for a subset of images, collecting annotations to identify model errors, training a feedback network, and utilizing reinforcement learning to refine the model based on this feedback. Experimental results demonstrate that incorporating human feedback leads to improved performance as measured by automatic evaluation metrics for captioning.
Strengths:
(a) The paper tackles a crucial aspect of learning, namely human feedback, and demonstrates that such feedback is more valuable than additional caption annotations, which is an intuitive and positive finding.
(b) The authors have successfully collected human feedback in a manner that minimizes ambiguity, facilitating the fine-tuning of the model; their choice of a phrase-based captioning model aids in this collection process.
(c) The experiments are comprehensive, featuring various ablations that effectively demonstrate the efficacy of human feedback.
Weaknesses:
(a) Conducting human evaluations on caption quality would have provided a more accurate assessment of performance; even on a smaller scale, such evaluations offer more insight than automatic correlations, which have been shown to correlate poorly with human judgment.
Comments:
(a) L69: [3] does not employ reinforcement learning, whereas [34] appears to utilize reinforcement learning for visual dialog.
(b) Table 6: The Rouge-L value in the second row seems incorrect, as it does not align with the values reported for other models.
(c) Figure 2 is incomplete, as it lacks proper labeling (e.g., ct, ht, h_{t-1}), making it unclear how the image is processed to generate caption phrases.
(d) Sec 3.3: The overall setup for reinforcement learning fine-tuning using feedback is not discussed until this point, resulting in confusion; for instance, the description of the feedback network assumes a prior understanding of the setup, and the mention of "newly sampled caption" at L164 lacks context. It is recommended to rephrase the text to establish the overall training procedure.
(e) L245: The sentence is unclear and requires clarification.
(f) The decoding method used during evaluation (e.g., beam search, sampling, or greedy policy) is not specified.
(g) L167: There is an inconsistency in the superscript.
(h) L222: The testing dataset is relatively small, comprising almost 1/20th of the training dataset, which raises questions about the representativeness of the results.
Typos:
(a) L28-29: "reinforcement learning" should be abbreviated as "RL".
(b) L64: The phrase "it exploits" is unclear and may require rephrasing.
(c) L74: "Policy Gradients" should be written in lowercase as "policy gradients".
(d) L200: There appears to be a typo in the equation specifying the range for Î».
References:
[34] Das, Abhishek, et al. "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning." arXiv preprint arXiv:1703.06585 (2017).