Here is a paraphrased version of the review:
Summary of the Paper:
This paper presents a novel learning approach for tackling two-stage stochastic programming problems, which aim to minimize the function f(x,y,z) with respect to z. The core idea is to learn a predictive model p(y|x;θ) that directly optimizes the task's objective function f, differing from traditional methods that focus on minimizing prediction errors without considering f. The key technical challenge addressed in the paper is solving a sub-optimization problem involving argmin with respect to z, which the proposed method efficiently handles by assuming convexity in z. Experimental evaluations on two problems demonstrate the method's superiority over traditional approaches.
Major Comments:
The concept of applying end-to-end learning to two-stage stochastic programming is intriguing. However, a significant concern with the proposed method is the absence of convergence guarantees. Given the assumption of convexity in z, the obtained solution z*(x;θ) should be the true optimal solution if the data is drawn from the true distribution p(x,y). Nevertheless, a solution derived using the predictive model p(y|x;θ) is unlikely to be truly optimal unless p(y|x;θ) accurately represents the true conditional distribution p(y|x), a issue akin to model bias in model-based reinforcement learning. Since the proposed method lacks theoretical guarantees that p(y|x;θ) converges to p(y|x) even when the model hypothesis is correct, it is likely that the method may only yield a sub-optimal solution, even for convex optimization problems. Therefore, establishing convergence guarantees or error bounds for the predictive model or the obtained solution is crucial for theoretically justifying the method and would significantly enhance the paper's contribution.
Questions:
1. The requirement for mini-batch training in Algorithm 1 is unclear, as Line 7 only checks the constraint for a single sample.
2. In the first experiment, it is unclear why the performance of the end-to-end policy optimization method depends on the model hypothesis, given that it does not rely on a predictive model.
Minor Suggestions:
1. The argument in Line 154 that the model-free approach requires a rich policy class and is data-inefficient overlooks the fact that the model-based approach also requires a rich model class and can suffer from model bias, whereas the model-free approach does not.
2. The proposed method's applicability is limited, as solving sub-optimization problems with argmin is non-trivial and the convexity assumption is helpful but not always realistic. Developing a variant of the method that can handle non-convex or unknown objective functions would increase its appeal.
3. The last term of Eq.(4) should include an expectation over the density of x.
Comments after Author's Response:
After reviewing the author's response, I have a more positive impression of the paper and intend to increase my score. Nevertheless, I remain unconvinced about the method's usefulness outside domains with convex objectives without empirical evidence.