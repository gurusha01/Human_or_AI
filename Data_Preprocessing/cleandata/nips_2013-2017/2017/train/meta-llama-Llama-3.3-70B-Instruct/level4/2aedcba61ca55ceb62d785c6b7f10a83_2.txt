This study examines group-additive nonparametric regression models, where the regression function is additive across groups of predictor variables, defined by a specific partitioning. This approach bridges the gap between fully nonparametric models, which are challenging to fit, and additive models, which can be overly restrictive. The paper focuses on the scenario where the group structure is unknown and must be inferred from the data. To address this, a novel penalty function is proposed, based on the covering numbers of Reproducing Kernel Hilbert Space (RKHS) balls, which is then incorporated into the kernel ridge regression objective. This yields an objective function that can be optimized jointly over both the group structure, which determines a function space via the direct sum of RKHSs over each group of variables, and the regression estimate within each group. Two algorithms are presented to approximate the solution to this complex optimization problem. Theoretical results are provided, demonstrating the convergence rate of the empirical risk of the estimate to the true risk of the optimal solution, as well as the consistency of the group structure estimate, where the probability of correctly identifying the true group structure approaches 1 as the sample size increases. Experimental results on both synthetic and real data are also presented.
The primary contribution of the paper lies in its recognition that the complexity of a group structure can be quantified using the covering numbers of the direct sum space. The paper is comprehensive, featuring a well-motivated and novel methodology, along with solid theoretical and empirical results. Assuming the discussion in Lines 31-50 is accurate and comprehensive, this work appears to be a significant advancement in the field, particularly in addressing the challenge of fitting models that fall between additive and nonparametric models. However, as noted in the Discussion section, a major limitation of the method is the difficulty in solving the optimization problem over the group structure when dealing with a large number of variables. The paper is generally well-written, although it contains numerous typos.
Several minor points and questions were noted. Firstly, it would be interesting to explore if there exists a simple characterization of "interaction" between two variables that does not require specifying the entire model as in Equation (1). Additionally, the use of "almost surely" in Line 169 was unclear; it is presumed that this refers to the limit as the sample size approaches infinity. Equation (3) appears to be missing a summation over the index i. Furthermore, the term "translation invariant kernel" might be more commonly used in the literature than "convolutional kernel," as mentioned in Line 205.
Several typos were identified: in Line 145, "on RHS" should be corrected to "on the RHS"; in Line 152, "not only can the bias of \hat f{\lambda,G} reduces" should be corrected to "not only can the bias of \hat f{\lambda,G} reduce"; in Line 160, "G^ exists and unique" should be corrected to "G^ exists and is unique"; in Line 247, "turning parameters" should be corrected to "tuning parameters"; in Algorithm 2, Line 1, "State with" should be corrected to "start with"; and in Line 251, "by compare" should be corrected to "by comparing".