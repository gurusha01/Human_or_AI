This manuscript delves into the concept of generalized Hamming distance within the framework of fuzzy neural networks, revealing that neural computations at each neuron can be interpreted as calculating the generalized Hamming distance between the respective weight vector and input vector. This perspective is particularly useful as it inherently provides candidate bias parameters for each neuron, eliminating the need for learning additional parameters through batch normalization during training. Moreover, this viewpoint enables the authors to elucidate the role of rectified linear units and propose a double-thresholding scheme, which enhances the speed and stability of the training process. Building on these insights, the authors introduce generalized Hamming networks (GHN), where each neuron precisely computes the generalized Hamming distance, and these networks also incorporate non-linear activations.
The paper proceeds to present simulation results that demonstrate the efficacy of the proposed GHN in image classification and auto-encoder design tasks. The results indicate that GHN exhibit rapid learning and achieve superior performance on the tasks at hand. Notably, the simulations suggest that GHN do not necessarily require max-pooling to function effectively.
The manuscript is well-structured and clearly conveys the primary concepts. The simulation results are also persuasive. However, there appears to be some incompleteness in the plots, such as Fig. 4.2, which lacks a curve for a learning rate of 0.0001. Although a more exhaustive evaluation, both theoretically and practically, would have been preferable, the concept of GHN is found to be intriguing overall.
Minor comments include:
1) The abstract should be corrected to reflect '(GHN)' instead of '(GNN)'.
2) On page 2, line 55, the typo 'felxaible' should be corrected to 'flexible'.