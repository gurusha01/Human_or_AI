Summary: This paper explores the online Isotonic regression problem within the random permutation model, where an adversary selects a set of instances beforehand and reveals them in a random order. At each round, the algorithm predicts an output based on the revealed instance, and then the true output is disclosed, resulting in a squared loss. The objective is to minimize regret.
Previous research has shown that with arbitrary sequences of instances, regret can be linear in the number of rounds. Additionally, under the fixed design model where the sequence of instances is predetermined and known to the algorithm, an optimal regret of approximately T^(1/3) has been achieved.
The authors of this paper extend these findings to the more general random permutation model, presenting an algorithm that achieves an optimal T^(1/3) regret, albeit with high computational complexity. They also demonstrate that a broad class of efficient algorithms, known as Forward Algorithms, attain a T^(1/2) regret in the general case but achieve an optimal T^(1/3) regret when the revealed labels are isotonic.
In my opinion, the online Isotonic regression problem is intriguing, and the authors' results are noteworthy, particularly given the hopeless nature of the completely adversarial model. Although the efficient algorithms presented do not achieve optimal bounds, the authors make a significant contribution by elucidating the behavior of a large class of algorithms with good regret bounds and conjecturing the existence of an efficient optimal algorithm. I believe the techniques employed in this paper will be essential for future progress.
I recommend accepting this paper. However, I do have one concern: the authors should provide a clearer explanation for why the chosen notion of regret is natural, considering that the algorithm is not constrained to produce isotonic outputs. This is important because the current notion of regret could potentially yield negative values.
After reviewing the authors' feedback, my assessment remains unchanged.