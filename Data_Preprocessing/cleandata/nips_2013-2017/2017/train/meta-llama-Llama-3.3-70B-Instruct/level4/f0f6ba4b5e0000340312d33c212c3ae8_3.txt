This paper presents a novel approach to generating vector representations of words through the utilization of hash embeddings, leveraging hashing techniques to map words to a limited set of shared embedding vectors. This methodology offers a significant reduction in the cumulative number of parameters and mitigates the challenges inherent in conventional word embedding models. The proposed model is distinguished by its simplicity, ease of implementation, and its achievement of state-of-the-art results in experimental settings.
Initially, a primary concern arises regarding the complexity of training hash embeddings, as this process involves the simultaneous training of importance weights for hash functions and shared embeddings. Intuitively, this interdependence could complicate the attainment of a reasonable local minimum during training. It would be beneficial for the authors to demonstrate the robustness of this model concerning initialization points and hyperparameters, providing insight into the strategies for identifying a suitable local minimum.
Furthermore, an inquiry arises regarding the relationship between "discourse atoms" [1] and the shared embeddings derived from hash embeddings. Given that "discourse atoms" aim to reconstruct word embeddings through a linear combination of atom vectors, certain atoms may capture higher-level concepts related to words, such as ontological relationships. It is plausible that these characteristics could also be encapsulated within the shared embeddings of hash embeddings. The authors' perspectives on the connection between these "atoms" and the hashed representations would be invaluable, offering a deeper understanding of the model's capabilities and potential applications.
[1] Linear Algebraic Structure of Word Senses, with Applications to Polysemy https://arxiv.org/abs/1601.03764