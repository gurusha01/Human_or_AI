The authors introduce PMD, a population-based measure of divergence between probability distributions, and demonstrate its consistency as an estimator of the Wasserstein distance. 
The proposed estimator is conceptually straightforward and differentiable, making it suitable for training neural network-based models. 
A comprehensive comparison is made between PMD and MMD, the most widely used population-based divergence in machine learning. 
The authors acknowledge the limitations of their approach, noting that exact computation has cubic complexity, but they propose an approximation with quadratic complexity and show that it does not significantly compromise statistical performance in their empirical results. 
The paper is well-organized, clearly written, and appropriately references prior work. 
The theoretical findings appear to be accurate. 
The experimental analysis is sufficient, with comparisons between PMD and MMD, as well as other methods, for domain adaptation and generative modeling. 
However, it would be interesting to see the application of this method to generative modeling in multimodal domains, and further investigation is needed to determine its effectiveness when the sample size is smaller than the number of modes. 
Overall, this is a strong paper that presents a potentially highly useful method for comparing distributions, and its contributions are noteworthy.