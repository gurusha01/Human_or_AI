This paper proposes two approaches to encode invariance to additive symmetric noise in distances between distributions. The first approach measures the degree of asymmetry in paired sample differences, while the second approach compares the phase functions of the corresponding samples. The paper provides a thorough overview of the background and setup, including the definition of symmetric positive definite (SPD) components and decomposable random vectors.
The authors introduce the concept of phase discrepancy (PhD) as a weighted L2-distance between phase functions, which is invariant to additive SPD noise components. They also construct an approximate explicit feature map for kernel K, referred to as phase features, which can be used for learning on distributions. The paper provides theoretical results, including propositions and proofs, to support the proposed methods.
The experimental results demonstrate the effectiveness of the proposed methods, including the Symmetric Mean Embedding (SME) test and the PhD test, on both synthetic and real-world data. The results show that the SME test controls the Type I error even for large differences in noise levels, while the PhD test has better power than the SME test for small noise levels. The paper also demonstrates the applicability of phase features in learning on distributions, including distribution regression and multi-instance learning.
The strengths of the paper include:
* The proposal of two novel approaches to encode invariance to additive symmetric noise in distances between distributions
* The provision of theoretical results and proofs to support the proposed methods
* The demonstration of the effectiveness of the proposed methods on both synthetic and real-world data
* The applicability of phase features in learning on distributions
The weaknesses of the paper include:
* The assumption of indecomposable underlying distributions, which may not always hold in practice
* The potential sensitivity of the PhD test to large noise levels, which may lead to inflated Type I error rates
* The need for further research to fully understand the properties and limitations of the proposed methods
Overall, the paper makes a significant contribution to the field of machine learning and statistics, and the proposed methods have the potential to be widely applicable in various domains.
Arguments pro acceptance:
* The paper proposes novel and innovative approaches to encode invariance to additive symmetric noise in distances between distributions
* The paper provides thorough theoretical results and proofs to support the proposed methods
* The experimental results demonstrate the effectiveness of the proposed methods on both synthetic and real-world data
Arguments con acceptance:
* The assumption of indecomposable underlying distributions may not always hold in practice
* The PhD test may be sensitive to large noise levels, which may lead to inflated Type I error rates
* Further research is needed to fully understand the properties and limitations of the proposed methods.