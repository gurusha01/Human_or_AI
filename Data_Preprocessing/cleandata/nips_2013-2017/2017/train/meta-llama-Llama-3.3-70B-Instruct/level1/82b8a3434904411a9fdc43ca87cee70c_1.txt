This paper introduces a novel stochastic optimization algorithm, called Stochastic MISO, which is designed to minimize a composite and strongly convex objective function with stochastic perturbations of input data. The algorithm is an extension of the MISO/Finito algorithms, which are incremental methods based on variance reduction techniques. The authors provide a detailed analysis of the convergence properties of the algorithm, including a recursion on a Lyapunov function and a convergence rate of O(1/t) on the expected suboptimality.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and the proposed algorithm. The related work section is thorough, and the authors provide a good overview of the existing literature on stochastic optimization and variance reduction techniques.
The strengths of the paper include:
* The proposed algorithm, Stochastic MISO, is a significant contribution to the field of stochastic optimization, as it is the first algorithm that can efficiently tackle the hybrid case of finite-sum problems with stochastic perturbations.
* The authors provide a detailed analysis of the convergence properties of the algorithm, which is essential for understanding the behavior of the algorithm in practice.
* The paper includes a comprehensive set of experiments, which demonstrate the effectiveness of the proposed algorithm in various scenarios, including image classification, gene expression data, and movie review sentiment analysis.
The weaknesses of the paper include:
* The algorithm requires storing the vectors (zti)i=1,...,n, which can be a significant memory requirement, especially for large datasets.
* The authors do not provide a clear comparison with other state-of-the-art algorithms, such as SVRG and SAGA, in terms of computational complexity and memory requirements.
* The paper could benefit from a more detailed discussion of the limitations and potential extensions of the proposed algorithm.
Arguments pro acceptance:
* The paper presents a significant contribution to the field of stochastic optimization, with a novel algorithm that can efficiently tackle the hybrid case of finite-sum problems with stochastic perturbations.
* The authors provide a thorough analysis of the convergence properties of the algorithm, which is essential for understanding the behavior of the algorithm in practice.
* The paper includes a comprehensive set of experiments, which demonstrate the effectiveness of the proposed algorithm in various scenarios.
Arguments con acceptance:
* The algorithm requires significant memory storage, which can be a limitation for large datasets.
* The paper could benefit from a more detailed comparison with other state-of-the-art algorithms, such as SVRG and SAGA, in terms of computational complexity and memory requirements.
* The paper could benefit from a more detailed discussion of the limitations and potential extensions of the proposed algorithm.
Overall, I recommend accepting the paper, as it presents a significant contribution to the field of stochastic optimization, with a novel algorithm that can efficiently tackle the hybrid case of finite-sum problems with stochastic perturbations. However, I suggest that the authors address the weaknesses mentioned above, such as providing a more detailed comparison with other state-of-the-art algorithms and discussing the limitations and potential extensions of the proposed algorithm.