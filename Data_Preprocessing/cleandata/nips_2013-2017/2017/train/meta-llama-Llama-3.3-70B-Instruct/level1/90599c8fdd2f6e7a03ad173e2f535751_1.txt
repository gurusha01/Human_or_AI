This paper presents a novel algorithm, FREE-SUCRL (FSUCRL), for learning in Markov Decision Processes (MDPs) with options, which integrates temporal abstraction into the reinforcement learning model. The authors address the limitation of previous works, such as RMAX-SMDP and UCRL-SMDP, which require prior knowledge of the distributions of cumulative rewards and durations of each option. FSUCRL combines the Semi-Markov Decision Process (SMDP) view with the inner Markov structure of options to estimate the stationary distribution of an associated irreducible Markov Chain (MC), allowing for the computation of the optimistic policy at each episode.
The paper is well-structured, and the authors provide a clear introduction to the problem, related work, and the proposed algorithm. The technical sections are detailed and well-explained, with appropriate references to previous work. The authors also provide a theoretical analysis of the algorithm, including a regret bound that matches the bound of SUCRL up to an additive term.
The strengths of the paper include:
* The authors address a significant limitation of previous works, making the algorithm more practical and applicable to real-world problems.
* The paper provides a clear and detailed explanation of the algorithm and its components.
* The theoretical analysis is thorough, and the authors provide a regret bound that is comparable to SUCRL.
The weaknesses of the paper include:
* The algorithm's performance is sensitive to the choice of options and their structure, which may require careful design and tuning.
* The authors assume that the options are well-defined and satisfy certain properties, which may not always be the case in practice.
* The paper could benefit from more extensive empirical evaluations, including comparisons with other state-of-the-art algorithms and more complex domains.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of reinforcement learning with options.
* The algorithm addresses a critical limitation of previous works, making it more practical and applicable to real-world problems.
* The theoretical analysis is thorough, and the authors provide a regret bound that is comparable to SUCRL.
Arguments against acceptance:
* The algorithm's performance may be sensitive to the choice of options and their structure, which may require careful design and tuning.
* The authors assume that the options are well-defined and satisfy certain properties, which may not always be the case in practice.
* The paper could benefit from more extensive empirical evaluations, including comparisons with other state-of-the-art algorithms and more complex domains.
Overall, I recommend accepting the paper, as it presents a significant contribution to the field of reinforcement learning with options and addresses a critical limitation of previous works. However, the authors should be encouraged to provide more extensive empirical evaluations and to address the potential limitations of the algorithm.