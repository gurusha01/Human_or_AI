This paper proposes an accelerated stochastic greedy coordinate descent (ASGCD) algorithm to solve `1-regularized problems. The authors introduce a new rule for greedy selection based on an `1-norm square approximation, which is nontrivial to solve but convex. They then propose an efficient algorithm called SOft ThreshOlding PrOjection (SOTOPO) to exactly solve the `1-regularized `1-norm square approximation problem. The SOTOPO algorithm has a cost of O(d+ |Q| log |Q|), which is better than the O(d log d) cost of its counterpart SOPOPO.
The ASGCD algorithm combines the new rule and SOTOPO algorithm with Nesterov's acceleration and stochastic optimization strategies. The authors show that ASGCD has an optimal convergence rate of O(√CL1‖x∗‖1√) and reduces the iteration complexity of greedy selection by a factor of the sample size. The experimental results demonstrate that ASGCD has better performance than the state-of-the-art algorithms for high-dimensional and dense problems with sparse solutions.
The paper is well-written, and the authors provide a clear and detailed explanation of the proposed algorithm and its theoretical analysis. The experimental results are also well-presented and demonstrate the effectiveness of the proposed algorithm.
Here are the arguments pro and con acceptance:
Pro:
* The paper proposes a new and efficient algorithm for solving `1-regularized problems.
* The authors provide a detailed theoretical analysis of the proposed algorithm, including its convergence rate and iteration complexity.
* The experimental results demonstrate the effectiveness of the proposed algorithm in comparison to state-of-the-art algorithms.
Con:
* The paper assumes that the objective function is convex and smooth, which may not be the case in all applications.
* The proposed algorithm requires the computation of the full gradient beforehand, which can be computationally expensive for large datasets.
* The paper does not provide a comparison with other stochastic optimization algorithms, such as stochastic gradient descent (SGD) and its variants.
Overall, I believe that the paper is well-written and provides a significant contribution to the field of optimization. The proposed algorithm has the potential to be useful in a wide range of applications, and the experimental results demonstrate its effectiveness. Therefore, I recommend accepting the paper.
Quality: 9/10
The paper is well-written, and the authors provide a clear and detailed explanation of the proposed algorithm and its theoretical analysis. The experimental results are also well-presented and demonstrate the effectiveness of the proposed algorithm.
Clarity: 9/10
The paper is well-organized, and the authors provide a clear and concise explanation of the proposed algorithm and its theoretical analysis. The notation is also clear and consistent throughout the paper.
Originality: 8/10
The paper proposes a new algorithm for solving `1-regularized problems, which is a significant contribution to the field of optimization. However, the idea of combining Nesterov's acceleration and stochastic optimization strategies is not new and has been explored in other papers.
Significance: 9/10
The paper provides a significant contribution to the field of optimization, and the proposed algorithm has the potential to be useful in a wide range of applications. The experimental results demonstrate the effectiveness of the proposed algorithm, and the paper provides a detailed theoretical analysis of the algorithm's convergence rate and iteration complexity.