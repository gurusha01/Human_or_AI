This paper proposes a budget-aware strategy based on deep boosted regression trees, which aims to learn classifiers or regressors that are both accurate and cheap to evaluate. The authors address the challenge of constructing an ensemble of trees that is both accurate and yet cheap to evaluate, by adapting the gradient boosting framework to take into account prediction cost penalties.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand their contributions. The authors provide a thorough review of related work, highlighting the limitations of existing approaches and motivating the need for their proposed method.
The proposed method, CEGB, is a significant improvement over existing state-of-the-art algorithms, such as GREEDYMISER and BUDGETPRUNE. The authors demonstrate the effectiveness of CEGB through extensive experiments on several datasets, showing that it outperforms existing methods in terms of accuracy and prediction cost.
The strengths of the paper include:
* The authors propose a novel adaptation of gradient boosting that takes into account prediction cost penalties, which is a significant contribution to the field.
* The paper provides a thorough review of related work, highlighting the limitations of existing approaches and motivating the need for the proposed method.
* The authors demonstrate the effectiveness of CEGB through extensive experiments on several datasets, showing that it outperforms existing methods in terms of accuracy and prediction cost.
* The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand their contributions.
The weaknesses of the paper include:
* The paper assumes that the prediction cost function is known and fixed, which may not be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of CEGB, which could be an important consideration for large-scale applications.
* The paper could benefit from more discussion on the interpretability of the learned models and the trade-offs between accuracy and prediction cost.
Arguments pro acceptance:
* The paper proposes a novel and significant contribution to the field of machine learning.
* The authors demonstrate the effectiveness of CEGB through extensive experiments on several datasets.
* The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand their contributions.
Arguments con acceptance:
* The paper assumes that the prediction cost function is known and fixed, which may not be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of CEGB, which could be an important consideration for large-scale applications.
* The paper could benefit from more discussion on the interpretability of the learned models and the trade-offs between accuracy and prediction cost.
Overall, I believe that the paper is a significant contribution to the field of machine learning and deserves to be accepted. The authors propose a novel adaptation of gradient boosting that takes into account prediction cost penalties, and demonstrate its effectiveness through extensive experiments on several datasets. While there are some weaknesses to the paper, they do not outweigh the strengths, and I believe that the paper makes a valuable contribution to the field.