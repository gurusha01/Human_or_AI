This paper proposes a novel adaptive importance sampling scheme for coordinate descent (CD) and stochastic gradient descent (SGD) algorithms. The authors argue that optimal gradient-based sampling is theoretically well-justified, but its computation is often infeasible. To address this, they rely on safe lower and upper bounds on the gradient and formulate the problem of picking the optimal sampling distribution as a convex optimization problem.
The paper's main contributions are: (1) a generic and efficient adaptive importance sampling strategy that can be applied to CD and SGD methods, (2) a proof that the proposed sampling distribution is provably better than uniform or fixed importance sampling, and (3) an efficient algorithm to compute the solution.
The paper is well-written, and the authors provide a clear and detailed explanation of their approach. The theoretical analysis is thorough, and the experimental results demonstrate the effectiveness of the proposed scheme.
Strengths:
* The paper proposes a novel and efficient adaptive importance sampling scheme that can be applied to CD and SGD methods.
* The authors provide a thorough theoretical analysis of the proposed scheme, including proofs of its optimality and efficiency.
* The experimental results demonstrate the effectiveness of the proposed scheme in practice.
Weaknesses:
* The paper assumes that safe lower and upper bounds on the gradient can be efficiently computed, which may not always be the case.
* The computational cost of the proposed scheme is O(n log n) per iteration, which may be a bottleneck for very large datasets.
Arguments pro acceptance:
* The paper proposes a novel and efficient adaptive importance sampling scheme that can be applied to CD and SGD methods.
* The theoretical analysis is thorough, and the experimental results demonstrate the effectiveness of the proposed scheme.
* The paper addresses an important problem in machine learning, and the proposed scheme has the potential to improve the efficiency of CD and SGD algorithms.
Arguments con acceptance:
* The paper assumes that safe lower and upper bounds on the gradient can be efficiently computed, which may not always be the case.
* The computational cost of the proposed scheme may be a bottleneck for very large datasets.
Overall, I believe that the paper is well-written, and the proposed scheme is novel and efficient. The theoretical analysis is thorough, and the experimental results demonstrate the effectiveness of the proposed scheme. However, the paper could be improved by addressing the potential limitations of the proposed scheme, such as the assumption that safe lower and upper bounds on the gradient can be efficiently computed. 
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10
Recommendation: Accept with minor revisions. The authors should address the potential limitations of the proposed scheme and provide more discussion on the computational cost and the assumption that safe lower and upper bounds on the gradient can be efficiently computed.