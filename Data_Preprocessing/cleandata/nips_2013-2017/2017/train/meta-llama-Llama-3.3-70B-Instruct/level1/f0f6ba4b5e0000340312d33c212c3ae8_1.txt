This paper introduces hash embeddings, a novel method for representing words in a continuous vector form. The authors propose an efficient hybrid between standard word embeddings and feature hashing, which allows for a significant reduction in the number of parameters required to represent large vocabularies. The key idea is to use multiple hash functions to select component vectors from a shared pool, and then combine these vectors using trainable importance parameters.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed method. The related work section is thorough, and the authors demonstrate a good understanding of the existing literature on word embeddings and feature hashing. The experimental evaluation is comprehensive, and the results show that hash embeddings can achieve state-of-the-art performance on a range of text classification tasks, while requiring significantly fewer parameters than standard embeddings.
The strengths of the paper include:
* The proposed method is novel and addresses a significant problem in natural language processing, namely the need to represent large vocabularies in a compact and efficient manner.
* The authors provide a thorough analysis of the theoretical properties of hash embeddings, including the probability of collisions and the expected number of tokens in collision.
* The experimental evaluation is comprehensive and demonstrates the effectiveness of hash embeddings on a range of tasks.
The weaknesses of the paper include:
* The authors could provide more insight into the choice of hyperparameters, such as the number of hash functions and the size of the component vector pool.
* The paper could benefit from a more detailed analysis of the computational complexity of hash embeddings, particularly in comparison to standard embeddings.
* Some of the experimental results, such as the comparison to standard embeddings without a dictionary, could be more thoroughly discussed and analyzed.
Overall, I believe that this paper makes a significant contribution to the field of natural language processing, and the proposed method has the potential to be widely adopted. The authors demonstrate a good understanding of the existing literature and provide a thorough evaluation of the proposed method.
Arguments for acceptance:
* The paper proposes a novel and efficient method for representing large vocabularies in a compact and continuous vector form.
* The experimental evaluation is comprehensive and demonstrates the effectiveness of hash embeddings on a range of tasks.
* The authors provide a thorough analysis of the theoretical properties of hash embeddings.
Arguments against acceptance:
* The paper could benefit from a more detailed analysis of the computational complexity of hash embeddings.
* Some of the experimental results could be more thoroughly discussed and analyzed.
* The authors could provide more insight into the choice of hyperparameters.
Quality: 8/10
Clarity: 9/10
Originality: 9/10
Significance: 9/10
Overall score: 8.5/10
Recommendation: Accept with minor revisions. The authors should address the weaknesses mentioned above, particularly providing more insight into the choice of hyperparameters and a more detailed analysis of the computational complexity of hash embeddings.