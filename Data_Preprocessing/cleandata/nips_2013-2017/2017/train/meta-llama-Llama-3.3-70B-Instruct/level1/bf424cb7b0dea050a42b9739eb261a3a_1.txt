This paper presents a thorough analysis of the limitations of gradient descent-based optimization methods when used in conjunction with smooth kernels, and proposes a novel preconditioning scheme, EigenPro, to alleviate these limitations. The authors demonstrate that the computational reach of gradient descent is limited to very smooth functions, and that a large number of iterations are required to approximate functions with less smoothness. They also show that EigenPro can significantly improve the convergence of gradient descent, achieving acceleration factors of up to 35 times without loss of accuracy.
The paper is well-written, and the authors provide a clear and detailed explanation of their analysis and methodology. The experimental results are impressive, demonstrating the effectiveness of EigenPro on several large datasets. The comparison to state-of-the-art results in the literature is also convincing, showing that EigenPro can improve or match performance at a much lower computational budget.
The strengths of the paper include:
* A thorough analysis of the limitations of gradient descent-based optimization methods with smooth kernels
* A novel and effective preconditioning scheme, EigenPro, that can significantly improve the convergence of gradient descent
* Impressive experimental results demonstrating the effectiveness of EigenPro on several large datasets
* A clear and detailed explanation of the analysis and methodology
The weaknesses of the paper include:
* The paper assumes a significant amount of background knowledge in machine learning and optimization, which may make it difficult for non-experts to follow
* Some of the notation and terminology may be unfamiliar to readers without a strong background in the field
* The paper could benefit from a more detailed discussion of the potential applications and implications of the results
Overall, I would recommend accepting this paper for publication. The results are significant, and the paper provides a valuable contribution to the field of machine learning and optimization.
Arguments pro acceptance:
* The paper presents a thorough and well-written analysis of the limitations of gradient descent-based optimization methods with smooth kernels
* The proposed preconditioning scheme, EigenPro, is novel and effective, and has the potential to significantly improve the convergence of gradient descent
* The experimental results are impressive, demonstrating the effectiveness of EigenPro on several large datasets
* The paper provides a clear and detailed explanation of the analysis and methodology
Arguments con acceptance:
* The paper assumes a significant amount of background knowledge in machine learning and optimization, which may make it difficult for non-experts to follow
* Some of the notation and terminology may be unfamiliar to readers without a strong background in the field
* The paper could benefit from a more detailed discussion of the potential applications and implications of the results
Quality: 9/10
Clarity: 8.5/10
Originality: 9/10
Significance: 9/10
Overall score: 8.9/10