This paper proposes a novel approach to fuse language and vision in visual question answering (VQA) tasks by modulating the entire visual processing pipeline using a linguistic input. The authors introduce Conditional Batch Normalization (CBN), a mechanism that conditions the batch normalization parameters of a pre-trained ResNet on a language embedding. This approach, called MODERN, is evaluated on two VQA datasets, VQAv1 and GuessWhat?!.
The paper is well-written, and the authors provide a clear motivation for their approach, discussing the limitations of current VQA models that process visual and linguistic inputs independently. The technical details of CBN and MODERN are well-explained, and the authors provide a thorough analysis of the results, including an ablation study and visualizations of the learned representations.
The strengths of the paper include:
* The proposal of a novel fusion mechanism that modulates the visual processing pipeline using a linguistic input, which is a significant departure from current approaches.
* The demonstration of significant improvements over strong baselines on two VQA datasets.
* The provision of a thorough analysis of the results, including an ablation study and visualizations of the learned representations.
The weaknesses of the paper include:
* The reliance on a pre-trained ResNet, which may limit the applicability of the approach to other architectures.
* The lack of a detailed comparison with other state-of-the-art VQA models that use different fusion mechanisms.
* The limited evaluation of the approach on other tasks beyond VQA, although the authors suggest that MODERN can be extended to other modalities and tasks.
Arguments pro acceptance:
* The paper proposes a novel and significant contribution to the field of VQA.
* The approach is well-motivated, and the technical details are well-explained.
* The results demonstrate significant improvements over strong baselines.
Arguments con acceptance:
* The reliance on a pre-trained ResNet may limit the applicability of the approach.
* The lack of a detailed comparison with other state-of-the-art VQA models may limit the understanding of the approach's strengths and weaknesses.
* The limited evaluation of the approach on other tasks beyond VQA may limit its potential impact.
Overall, I believe that the paper makes a significant contribution to the field of VQA and deserves to be accepted. However, I suggest that the authors address the weaknesses mentioned above, particularly by providing a more detailed comparison with other state-of-the-art VQA models and evaluating the approach on other tasks beyond VQA.