This paper presents a novel approach to modeling human question asking abilities in computational terms. The authors propose a probabilistic model that treats questions as formal programs that, when executed on the state of the world, output an answer. The model specifies a probability distribution over a complex, compositional space of programs, favoring concise programs that help the agent learn in the current context. The authors evaluate their approach by modeling the types of open-ended questions generated by humans in an information-search game, and find that their model predicts what questions people will ask and can creatively produce novel questions that were not present in the training set.
The paper is well-written, clearly organized, and provides a thorough introduction to the problem of question asking in humans and machines. The authors provide a detailed description of their model, including the grammar and probabilistic generative model, and evaluate its performance using a data set of human questions. The results show that the model is able to predict the distribution of human questions and generate novel, human-like questions that are useful in their respective contexts.
The strengths of the paper include its originality, clarity, and significance. The authors tackle a difficult problem in a novel way, and their approach has the potential to advance the state of the art in active machine learning. The paper is well-organized and easy to follow, making it accessible to a broad audience. The results are significant, as they demonstrate the ability of the model to predict and generate human-like questions.
The weaknesses of the paper include its reliance on a specific domain (the Battleship game) and the need for further evaluation and extension to other domains. Additionally, the model operates on semantic representations rather than natural language text directly, which may limit its applicability to real-world scenarios.
Arguments pro acceptance:
* The paper presents a novel and original approach to modeling human question asking abilities.
* The model is well-evaluated using a data set of human questions, and the results are significant.
* The paper is well-written, clearly organized, and provides a thorough introduction to the problem.
* The approach has the potential to advance the state of the art in active machine learning.
Arguments con acceptance:
* The model is limited to a specific domain (the Battleship game) and may not generalize to other domains.
* The model operates on semantic representations rather than natural language text directly, which may limit its applicability.
* Further evaluation and extension to other domains are needed to fully demonstrate the model's capabilities.
Overall, I believe that the paper is a strong contribution to the field of active machine learning and cognitive science, and I recommend acceptance. However, I also suggest that the authors consider addressing the limitations of their approach and extending their model to other domains in future work.