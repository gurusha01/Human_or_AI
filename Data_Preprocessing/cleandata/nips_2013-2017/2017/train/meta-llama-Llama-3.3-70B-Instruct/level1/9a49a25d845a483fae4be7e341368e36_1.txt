This paper introduces a novel probabilistic formulation for reducing discrimination in algorithmic decision making through data pre-processing. The authors propose a convex optimization framework that balances discrimination control, individual distortion, and utility preservation. The approach is applied to two real-world datasets, demonstrating its effectiveness in reducing discrimination while maintaining a reasonable level of accuracy.
The paper relates to previous work on fairness and discrimination in machine learning, including the concepts of statistical parity, group fairness, and individual fairness. The authors cite relevant studies, such as those by Feldman et al. (2015), Dwork et al. (2012), and Zemel et al. (2013), and position their work as a contribution to the ongoing research in this area.
The strengths of the paper include:
1. Novel formulation: The authors introduce a new probabilistic framework for discrimination-preventing pre-processing, which is a significant contribution to the field.
2. Convex optimization: The proposed optimization problem is convex, making it solvable to optimality, which is a desirable property in machine learning.
3. Theoretical guarantees: The authors provide theoretical guarantees for the approach, including bounds on the convergence rate of the optimization problem.
4. Empirical evaluation: The paper includes an empirical evaluation on two real-world datasets, demonstrating the effectiveness of the approach in reducing discrimination.
The weaknesses of the paper include:
1. Complexity: The proposed framework and optimization problem may be challenging to understand and implement for practitioners without a strong background in optimization and probability theory.
2. Limited comparison: The authors compare their approach to only one other method (LFR), and the comparison is limited to a specific setting.
3. Parameter tuning: The choice of parameters, such as the distortion metric and the level of discrimination control, may require careful tuning, which can be time-consuming and require expertise.
Arguments pro acceptance:
1. Original contribution: The paper introduces a novel formulation and approach to reducing discrimination in algorithmic decision making.
2. Theoretical guarantees: The authors provide theoretical guarantees for the approach, which is essential for establishing trust in the method.
3. Empirical evaluation: The paper includes an empirical evaluation on real-world datasets, demonstrating the effectiveness of the approach.
Arguments con acceptance:
1. Complexity: The proposed framework and optimization problem may be challenging to understand and implement for practitioners.
2. Limited comparison: The authors compare their approach to only one other method, which may not provide a comprehensive understanding of the approach's strengths and weaknesses.
3. Parameter tuning: The choice of parameters may require careful tuning, which can be time-consuming and require expertise.
Overall, the paper presents a significant contribution to the field of fairness and discrimination in machine learning, and the authors provide a thorough theoretical and empirical evaluation of their approach. While there are some limitations and challenges associated with the proposed framework, the paper demonstrates the potential of the approach to reduce discrimination in algorithmic decision making.