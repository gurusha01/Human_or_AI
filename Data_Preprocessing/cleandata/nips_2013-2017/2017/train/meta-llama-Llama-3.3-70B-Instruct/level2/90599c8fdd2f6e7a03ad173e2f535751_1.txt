This paper presents a novel algorithm, FREE-SUCRL (FSUCRL), for learning in Markov Decision Processes (MDPs) with options, which integrates temporal abstraction into the reinforcement learning model. The main claim of the paper is that FSUCRL removes the limitation of prior knowledge about options, specifically the distributions of cumulative rewards and durations, and achieves a regret performance comparable to that of UCRL-SMDP.
The paper provides a clear and well-structured introduction to the problem, including a thorough review of previous work on options in reinforcement learning. The authors motivate the need for a parameter-free algorithm, highlighting the limitations of existing approaches that rely on prior knowledge about options.
The technical contributions of the paper are sound, and the authors provide a detailed analysis of the algorithm's regret bound. The introduction of irreducible Markov chains associated with options is a key innovation, allowing the authors to estimate the stationary distribution of options without requiring prior knowledge about their parameters.
The experimental results demonstrate the effectiveness of FSUCRL in comparison to UCRL and SUCRL, particularly in scenarios where the options have a high degree of overlap. The authors provide a thorough discussion of the results, highlighting the advantages of FSUCRL in terms of temporal abstraction and the reduction of the state-action space.
The paper is well-written, and the authors provide a clear and concise presentation of the technical material. The use of notation is consistent throughout the paper, and the authors provide a comprehensive list of references to related work.
In terms of the review criteria, the paper scores well on:
* Quality: The paper presents a technically sound and well-motivated algorithm, with a clear and thorough analysis of its regret bound.
* Clarity: The paper is well-written, with a clear and concise presentation of the technical material.
* Originality: The introduction of irreducible Markov chains associated with options is a novel contribution, and the authors provide a thorough analysis of the algorithm's regret bound.
* Significance: The paper demonstrates the effectiveness of FSUCRL in comparison to existing algorithms, particularly in scenarios where the options have a high degree of overlap.
However, the paper could be improved in terms of:
* Completeness: The authors could provide more details on the implementation of the algorithm, particularly in terms of the computational complexity and the choice of hyperparameters.
* Limitations: The authors could provide a more thorough discussion of the limitations of the algorithm, particularly in terms of the assumptions made about the options and the MDP.
Overall, the paper presents a significant contribution to the field of reinforcement learning, and the authors demonstrate the effectiveness of FSUCRL in a range of scenarios. With some minor revisions to address the limitations and completeness of the paper, it has the potential to be a strong contribution to the NIPS conference.
Arguments pro acceptance:
* The paper presents a novel and technically sound algorithm for learning in MDPs with options.
* The authors provide a thorough analysis of the algorithm's regret bound and demonstrate its effectiveness in comparison to existing algorithms.
* The paper is well-written, with a clear and concise presentation of the technical material.
Arguments con acceptance:
* The paper could benefit from more details on the implementation of the algorithm, particularly in terms of the computational complexity and the choice of hyperparameters.
* The authors could provide a more thorough discussion of the limitations of the algorithm, particularly in terms of the assumptions made about the options and the MDP.