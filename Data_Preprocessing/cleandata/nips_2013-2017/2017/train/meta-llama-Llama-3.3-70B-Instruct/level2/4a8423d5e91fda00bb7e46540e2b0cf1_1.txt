This paper proposes a method for constructing a selective classifier given a trained neural network, allowing users to set a desired risk level and reject instances at test time to guarantee the desired risk with high probability. The authors introduce a selection with guaranteed risk (SGR) algorithm, which outputs a selective classifier and a risk bound. The algorithm is based on a confidence-rate function, which can be either the softmax response (SR) or the Monte-Carlo dropout (MC-dropout) technique.
The paper's main claims are that the proposed SGR algorithm can provide a selective classifier with guaranteed risk control and that the algorithm is effective in reducing the error rate while keeping coverage high. The authors support these claims through empirical results on CIFAR-10, CIFAR-100, and ImageNet datasets, demonstrating that the SGR algorithm can achieve state-of-the-art results with high coverage.
The paper is well-written, and the authors provide a clear explanation of the proposed algorithm and its theoretical foundations. The empirical results are convincing, and the authors demonstrate the effectiveness of the SGR algorithm in various settings.
The paper's strengths include:
* The proposal of a novel algorithm for selective classification with guaranteed risk control
* The provision of theoretical foundations for the algorithm
* The demonstration of the algorithm's effectiveness through empirical results on various datasets
The paper's weaknesses include:
* The assumption that the classifier f is already trained, which may not always be the case in practice
* The limitation of the algorithm to classification tasks with 0/1 loss, which may not be applicable to other loss functions or regression tasks
* The lack of comparison to other state-of-the-art methods for selective classification
Overall, the paper is well-written, and the proposed algorithm is novel and effective. However, the authors could improve the paper by addressing the limitations and providing more comparisons to other state-of-the-art methods.
Arguments pro acceptance:
* The paper proposes a novel algorithm for selective classification with guaranteed risk control
* The algorithm is effective in reducing the error rate while keeping coverage high
* The empirical results are convincing and demonstrate the algorithm's effectiveness on various datasets
Arguments con acceptance:
* The assumption that the classifier f is already trained may not always be the case in practice
* The limitation of the algorithm to classification tasks with 0/1 loss may not be applicable to other loss functions or regression tasks
* The lack of comparison to other state-of-the-art methods for selective classification may limit the paper's impact.