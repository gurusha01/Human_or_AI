This paper presents a novel approach to modeling human question asking abilities in computational terms. The authors propose a probabilistic model that treats questions as formal programs that, when executed on the state of the world, output an answer. The model is evaluated using a data set of natural language questions asked by human participants in an information-search game. The results show that the model can predict what questions human learners will ask and can also synthesize novel human-like questions that were not present in the training set.
The paper's main claims are that the proposed model can capture the compositionality and computability of human questions, and that it can provide a predictive model of open-ended human question asking. The authors support these claims through a combination of theoretical analysis, experimental results, and comparisons with alternative models.
The strengths of the paper include its novel approach to modeling human question asking, its ability to capture the compositionality and computability of questions, and its impressive results in predicting and generating human-like questions. The paper also provides a thorough evaluation of the model, including comparisons with alternative models and an analysis of the model's limitations.
However, there are also some weaknesses to the paper. One limitation is that the model operates on semantic representations rather than on natural language text directly, which may limit its applicability to real-world scenarios. Additionally, some aspects of the grammar are specific to the Battleship domain, which may require significant knowledge engineering to extend to new domains.
Overall, I would argue that the paper's strengths outweigh its weaknesses, and that it presents a significant contribution to the field of artificial intelligence and cognitive science. The paper's results have important implications for the development of more capable active learning machines, and its approach to modeling human question asking has the potential to be applied to a wide range of domains.
Arguments pro acceptance:
* The paper presents a novel and innovative approach to modeling human question asking abilities.
* The model is able to capture the compositionality and computability of human questions, which is a significant achievement.
* The results show that the model can predict and generate human-like questions, which has important implications for the development of more capable active learning machines.
* The paper provides a thorough evaluation of the model, including comparisons with alternative models and an analysis of the model's limitations.
Arguments con acceptance:
* The model operates on semantic representations rather than on natural language text directly, which may limit its applicability to real-world scenarios.
* Some aspects of the grammar are specific to the Battleship domain, which may require significant knowledge engineering to extend to new domains.
* The paper could benefit from more discussion of the potential applications and implications of the model, as well as more analysis of the model's limitations and potential biases.