This paper proposes an accelerated stochastic greedy coordinate descent (ASGCD) algorithm to solve `1-regularized problems. The main claims of the paper are: (1) the authors propose a new rule for greedy selection based on an `1-norm square approximation, which is nontrivial to solve but convex; (2) they propose an efficient algorithm called SOft ThreshOlding PrOjection (SOTOPO) to exactly solve the `1-regularized `1-norm square approximation problem; and (3) the ASGCD algorithm, which combines the new rule and SOTOPO with Nesterov's acceleration and stochastic optimization, achieves the optimal convergence rate O(√1/).
The support for these claims is provided through theoretical analysis and experimental results. The authors show that the SOTOPO algorithm has a cost of O(d + |Q| log |Q|), which is better than the O(d log d) cost of its counterpart SOPOPO. They also provide a convergence rate analysis for the ASGCD algorithm, which shows that it achieves an -additive error using at most O(√CL1‖x∗‖1√) iterations. The experimental results demonstrate the effectiveness of the ASGCD algorithm on several datasets, showing that it outperforms other state-of-the-art algorithms in some cases.
The usefulness of the ideas presented in the paper is evident, as the ASGCD algorithm has the potential to be used in a variety of applications, such as machine learning and signal processing. The paper also reflects common knowledge in the field, as it builds upon existing work on coordinate descent and stochastic optimization.
The novelty of the paper lies in the proposal of the new rule for greedy selection and the SOTOPO algorithm, which allows for the efficient solution of the `1-regularized `1-norm square approximation problem. The authors also provide a comprehensive analysis of the ASGCD algorithm, including its convergence rate and experimental results.
The completeness of the paper is good, as it provides all the necessary details for the reproduction of the results. However, some of the proofs and derivations are omitted due to space limitations.
The limitations of the paper are acknowledged by the authors, who note that the log d factor in the bound of ASGCD may not be necessary and deserves further research.
In conclusion, the paper presents a significant contribution to the field of optimization, and the ASGCD algorithm has the potential to be a useful tool in a variety of applications. The strengths of the paper include its novelty, completeness, and experimental results, while its limitations are acknowledged and provide opportunities for future research.
Arguments pro acceptance:
* The paper presents a novel and efficient algorithm for solving `1-regularized problems.
* The theoretical analysis provides a comprehensive understanding of the ASGCD algorithm.
* The experimental results demonstrate the effectiveness of the ASGCD algorithm.
* The paper reflects common knowledge in the field and builds upon existing work.
Arguments con acceptance:
* The log d factor in the bound of ASGCD may not be necessary and deserves further research.
* Some of the proofs and derivations are omitted due to space limitations.
* The paper may benefit from additional experimental results and comparisons with other state-of-the-art algorithms.