This paper presents a novel approach to constructing feature maps for kernel machines, which are used to scale up kernel methods to large datasets. The authors propose a deterministic method for approximating the kernel's Fourier transform integral, using Gaussian quadrature and sparse grid quadrature, as an alternative to the widely used random Fourier features method. The main claim of the paper is that deterministic feature maps can achieve better scaling in the desired accuracy compared to random Fourier features, with a sample complexity that increases asymptotically slower than any negative power of the desired error.
The paper provides a thorough evaluation of the proposed method, including theoretical analysis and experimental results on several datasets, such as MNIST and TIMIT. The authors show that their method can produce comparable accuracy to state-of-the-art methods based on random Fourier features, while requiring fewer samples and being faster to compute. The paper also highlights the potential benefits of using deterministic feature maps, including improved statistical performance and systems benefits such as reduced multiplication requirements.
The strengths of the paper include its clear and well-organized presentation, its thorough evaluation of the proposed method, and its potential impact on the field of kernel methods. The authors provide a detailed analysis of the theoretical foundations of their method, including the use of Gaussian quadrature and sparse grid quadrature, and demonstrate its effectiveness through experiments on several datasets.
However, there are some limitations to the paper. One potential weakness is that the authors assume that the kernel's spectrum is subgaussian, which may not always be the case in practice. Additionally, the paper focuses primarily on sparse ANOVA kernels, which may not be representative of all types of kernels used in machine learning. Furthermore, the authors do not provide a direct comparison to other methods, such as quasi-Monte Carlo estimation, which may be relevant to the problem of approximating kernel functions.
Overall, the paper presents a significant contribution to the field of kernel methods, and its results have the potential to impact a wide range of applications. The authors' use of deterministic feature maps provides a promising alternative to random Fourier features, and their thorough evaluation and analysis demonstrate the effectiveness of their approach.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of kernel methods.
* The authors provide a thorough evaluation of their method, including theoretical analysis and experimental results.
* The paper has the potential to impact a wide range of applications, including speech recognition and image classification.
* The authors' use of deterministic feature maps provides a promising alternative to random Fourier features.
Arguments against acceptance:
* The paper assumes that the kernel's spectrum is subgaussian, which may not always be the case in practice.
* The paper focuses primarily on sparse ANOVA kernels, which may not be representative of all types of kernels used in machine learning.
* The authors do not provide a direct comparison to other methods, such as quasi-Monte Carlo estimation.