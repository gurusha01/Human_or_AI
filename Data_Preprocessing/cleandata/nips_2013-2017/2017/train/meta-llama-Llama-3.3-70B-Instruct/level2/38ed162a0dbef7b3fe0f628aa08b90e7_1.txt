This paper presents a theoretical framework for understanding the properties of deep neural networks, specifically convolutional neural networks (CNNs), in terms of their stability and invariance to transformations. The authors introduce a kernel-based approach to study the geometry of the functional space associated with CNNs, which allows them to derive stability results for the representation of signals under the action of diffeomorphisms.
The main claims of the paper are: (1) the proposed kernel representation is stable to small deformations and provides a notion of translation invariance, (2) the representation can be made invariant to any group of transformations by defining adapted patch extraction and pooling operators, and (3) the stability results apply to a class of CNNs with smooth homogeneous activations.
The support for these claims is provided through a series of theoretical results, including the derivation of bounds on the stability of the representation under diffeomorphisms and the characterization of the geometry of the functional space associated with the kernel. The authors also provide a connection between the kernel representation and CNNs, showing that the RKHS obtained from the kernel construction contains a set of CNNs with certain types of smooth homogeneous activations.
The usefulness of the ideas presented in the paper lies in their potential to provide a better understanding of the properties of deep neural networks and to guide the design of more robust and generalizable models. The results on stability and invariance can be used to develop new architectures and training methods that take into account the geometric properties of the data.
The paper demonstrates a good understanding of the field, with references to relevant literature and a clear explanation of the technical concepts. The notation and mathematical tools used are standard in the field, and the proofs are provided in the appendix.
The originality of the paper lies in its novel approach to studying the properties of deep neural networks using kernel methods and functional analysis. The connection between the kernel representation and CNNs is also a new contribution, which sheds light on the links between generalization and stability in deep learning.
The significance of the results is high, as they have the potential to impact the design of deep learning models and their applications in various fields. The paper is well-written, and the arguments are clear and well-organized.
Arguments pro acceptance:
* The paper presents a novel and original approach to studying the properties of deep neural networks.
* The results on stability and invariance have the potential to impact the design of deep learning models and their applications.
* The connection between the kernel representation and CNNs is a new contribution that sheds light on the links between generalization and stability in deep learning.
Arguments con acceptance:
* The paper is highly technical, and some readers may find it difficult to follow.
* The results are theoretical, and it is not clear how they will be applied in practice.
* The paper could benefit from more numerical experiments and examples to illustrate the results and their significance.