This paper introduces a novel stochastic optimization algorithm, called Stochastic MISO, which is designed to minimize composite and strongly convex objectives with stochastic perturbations of input data. The algorithm is an extension of the MISO/Finito algorithms, which are incremental methods based on variance reduction techniques. The authors provide a thorough analysis of the convergence properties of the algorithm, including a recursion on a Lyapunov function and a convergence rate of O(1/t) on the expected suboptimality.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and the proposed algorithm. The theoretical analysis is rigorous and well-presented, with a clear explanation of the assumptions and the proof techniques used. The experimental results are also well-presented, with a clear comparison of the proposed algorithm with other state-of-the-art methods.
The strengths of the paper include:
* The proposed algorithm is novel and addresses a specific problem in stochastic optimization, which is the minimization of composite and strongly convex objectives with stochastic perturbations of input data.
* The theoretical analysis is rigorous and provides a clear understanding of the convergence properties of the algorithm.
* The experimental results are well-presented and demonstrate the effectiveness of the proposed algorithm in practice.
The weaknesses of the paper include:
* The algorithm requires storing the vectors (zti)i=1,...,n, which takes the same amount of memory as the original dataset. This may be a limitation in practice, especially for large datasets.
* The algorithm is designed for non-huge datasets, and the authors note that SGD should be preferred for huge-scale settings.
Overall, the paper is well-written, and the proposed algorithm is novel and effective. The theoretical analysis is rigorous, and the experimental results are well-presented. The paper provides a significant contribution to the field of stochastic optimization and is suitable for publication in a top-tier conference.
Arguments pro acceptance:
* The paper proposes a novel algorithm that addresses a specific problem in stochastic optimization.
* The theoretical analysis is rigorous and provides a clear understanding of the convergence properties of the algorithm.
* The experimental results are well-presented and demonstrate the effectiveness of the proposed algorithm in practice.
Arguments con acceptance:
* The algorithm requires storing the vectors (zti)i=1,...,n, which takes the same amount of memory as the original dataset.
* The algorithm is designed for non-huge datasets, and the authors note that SGD should be preferred for huge-scale settings.
Recommendation: Accept. The paper provides a significant contribution to the field of stochastic optimization, and the proposed algorithm is novel and effective. The theoretical analysis is rigorous, and the experimental results are well-presented. While the algorithm has some limitations, the paper provides a clear understanding of the trade-offs and the potential applications of the proposed algorithm.