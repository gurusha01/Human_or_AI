This paper proposes a budget-aware strategy based on deep boosted regression trees, which aims to learn classifiers or regressors that are both accurate and cheap to evaluate. The main contribution of this work is the development of a cost-aware impurity function that takes into account both feature acquisition cost and tree evaluation cost. The authors demonstrate the effectiveness of their approach through extensive experiments on several datasets, showing that their method outperforms state-of-the-art algorithms, including GREEDYMISER and BUDGETPRUNE.
The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach. The related work section is comprehensive, and the authors provide a thorough discussion of the limitations of existing methods. The proposed method is well-motivated, and the authors provide a detailed description of their cost-aware impurity function and tree growing algorithm.
The experimental results are impressive, and the authors demonstrate the effectiveness of their approach in various settings, including feature acquisition cost and tree evaluation cost. The results show that the proposed method can learn deep trees that are cheap to evaluate on average, which is a key advantage over existing methods.
The paper has several strengths, including:
* The proposed method is easy to implement and can be used with any gradient boosting library.
* The authors provide a thorough discussion of the limitations of existing methods and demonstrate the effectiveness of their approach in various settings.
* The experimental results are impressive, and the authors demonstrate the significance of their approach in terms of accuracy and cost.
However, the paper also has some limitations, including:
* The authors assume that the feature costs are known, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of their method, which could be an important consideration in practice.
Overall, this paper presents a significant contribution to the field of machine learning, and the proposed method has the potential to be highly valuable for many applications. The authors demonstrate the effectiveness of their approach through extensive experiments, and the paper is well-written and easy to follow.
Arguments pro acceptance:
* The proposed method is novel and significant, and the authors demonstrate its effectiveness through extensive experiments.
* The paper is well-written, and the authors provide a clear and concise introduction to the problem and their approach.
* The authors provide a thorough discussion of the limitations of existing methods and demonstrate the advantages of their approach.
Arguments con acceptance:
* The authors assume that the feature costs are known, which may not always be the case in practice.
* The authors do not provide a detailed analysis of the computational complexity of their method, which could be an important consideration in practice.
Recommendation: Accept. The paper presents a significant contribution to the field of machine learning, and the proposed method has the potential to be highly valuable for many applications. The authors demonstrate the effectiveness of their approach through extensive experiments, and the paper is well-written and easy to follow.