This paper proposes a novel adaptive importance sampling scheme for coordinate descent (CD) and stochastic gradient descent (SGD) algorithms. The authors argue that optimal gradient-based sampling is theoretically well-justified, but its computation is often infeasible. To address this, they rely on safe lower and upper bounds on the gradient and formulate the problem of picking the optimal sampling distribution as a convex optimization problem.
The paper's main claims are: (1) the proposed sampling scheme is provably better than uniform or fixed importance sampling, (2) it can be efficiently computed in every iteration, and (3) it recovers the gradient-based sampling in the full-information setting. The authors also provide a geometric interpretation of the optimization problem and propose an efficient algorithm to compute the solution.
The paper's strengths include its clear and well-organized presentation, its thorough analysis of the proposed scheme's properties, and its extensive numerical evaluation on real datasets. The authors also provide a detailed discussion of the computational complexity and the overhead introduced by the proposed scheme.
However, there are some limitations and potential areas for improvement. For example, the paper assumes that safe lower and upper bounds on the gradient can be efficiently computed, which may not always be the case. Additionally, the proposed scheme's performance may depend on the quality of these bounds, and the authors could provide more insight into how to choose or improve these bounds.
The paper's contributions are significant, as they provide a novel and efficient adaptive importance sampling scheme that can be applied to a wide range of machine learning problems. The authors' use of convex optimization to formulate the problem of picking the optimal sampling distribution is also a notable contribution.
In terms of novelty, the paper's approach is distinct from previous work on adaptive importance sampling, which often relies on heuristics or approximations. The authors' use of safe bounds on the gradient and their formulation of the optimization problem are also new and interesting contributions.
The paper's significance is high, as it has the potential to improve the performance of CD and SGD algorithms in a wide range of applications. The authors' extensive numerical evaluation and their discussion of the computational complexity and overhead introduced by the proposed scheme also demonstrate its practical relevance.
Overall, I would recommend accepting this paper, as it makes a significant contribution to the field of machine learning and provides a novel and efficient adaptive importance sampling scheme. However, I would suggest that the authors provide more insight into how to choose or improve the safe bounds on the gradient and explore the potential applications of their scheme in more detail.
Arguments pro acceptance:
* The paper proposes a novel and efficient adaptive importance sampling scheme that can be applied to a wide range of machine learning problems.
* The authors provide a thorough analysis of the proposed scheme's properties and its computational complexity.
* The paper's extensive numerical evaluation demonstrates the scheme's practical relevance and effectiveness.
* The authors' use of convex optimization to formulate the problem of picking the optimal sampling distribution is a notable contribution.
Arguments con acceptance:
* The paper assumes that safe lower and upper bounds on the gradient can be efficiently computed, which may not always be the case.
* The proposed scheme's performance may depend on the quality of these bounds, and the authors could provide more insight into how to choose or improve these bounds.
* The paper could benefit from a more detailed discussion of the potential applications of the proposed scheme and its limitations.