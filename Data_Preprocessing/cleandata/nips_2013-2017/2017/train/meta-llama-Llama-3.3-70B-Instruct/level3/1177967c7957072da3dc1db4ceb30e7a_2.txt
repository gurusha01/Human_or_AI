This paper provides a thorough analysis of the limitations of gradient descent-based optimization methods when used in conjunction with smooth kernels, and proposes a novel preconditioning scheme, EigenPro, to alleviate these limitations. The authors demonstrate that the computational reach of gradient descent is limited to very smooth functions, and that a large number of iterations may be required to approximate less smooth functions. They also show that EigenPro can significantly improve the convergence of gradient descent, especially for large datasets.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand the technical details. The introduction provides a clear motivation for the work, and the related work section provides a thorough overview of existing methods. The experimental results are impressive, demonstrating the effectiveness of EigenPro in improving the convergence of gradient descent on several large datasets.
One of the strengths of the paper is its ability to provide a clear and concise analysis of the limitations of gradient descent, and to propose a novel solution that addresses these limitations. The authors also provide a thorough evaluation of their method, including a comparison to state-of-the-art methods.
However, there are a few areas where the paper could be improved. For example, the figures are too small and lack visibility, making it difficult to read the labels and understand the results. It would be helpful to move some of the figures to the appendix and provide larger versions of the remaining figures.
Additionally, the paper could benefit from a more detailed comparison of the new sampling method with uniform sampling, similar to Conclusion 2 in Theorem 3.2. This would provide further insight into the effectiveness of the proposed method.
Finally, the authors should remark on the impact of the new sampling method on the total complexity of RCD or SGD, including the time complexity for updating parameters to find an epsilon-optimal solution. This would provide a more complete understanding of the trade-offs involved in using the proposed method.
Overall, this is a strong paper that makes a significant contribution to the field of machine learning. With some minor revisions to address the areas mentioned above, it has the potential to be an excellent paper.
Arguments pro acceptance:
* The paper provides a clear and concise analysis of the limitations of gradient descent
* The proposed method, EigenPro, is novel and effective in improving the convergence of gradient descent
* The experimental results are impressive and demonstrate the effectiveness of EigenPro on several large datasets
* The paper is well-written and clearly organized
Arguments con acceptance:
* The figures are too small and lack visibility
* The paper could benefit from a more detailed comparison of the new sampling method with uniform sampling
* The authors should remark on the impact of the new sampling method on the total complexity of RCD or SGD. 
Quality: 8/10
The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. However, there are a few areas where the paper could be improved, such as providing larger figures and a more detailed comparison of the new sampling method with uniform sampling.
Clarity: 9/10
The paper is well-written and clearly organized, making it easy to follow the authors' arguments and understand the technical details.
Originality: 8/10
The proposed method, EigenPro, is novel and effective in improving the convergence of gradient descent. However, the paper builds on existing work in the field, and the authors could provide more context and comparison to related methods.
Significance: 9/10
The paper makes a significant contribution to the field of machine learning, providing a clear analysis of the limitations of gradient descent and proposing a novel solution that addresses these limitations. The experimental results are impressive, and the paper has the potential to be an excellent paper with some minor revisions.