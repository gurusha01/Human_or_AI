This paper proposes a deterministic approach to constructing feature maps for kernel machines, which is an important contribution to the field of machine learning. The authors introduce a framework for approximating the Fourier transform integral of a kernel using Gaussian quadrature, which allows for the construction of deterministic feature maps. The paper also explores various methods for constructing these feature maps, including polynomially-exact quadrature, dense grid construction, sparse grid construction, and reweighted grid construction.
The strengths of this paper include its thorough analysis of the theoretical foundations of the proposed approach, as well as its experimental evaluation on several datasets. The authors provide a clear and detailed explanation of their methods, and their results demonstrate the effectiveness of their approach in achieving better scaling in the desired accuracy compared to the state-of-the-art method, random Fourier features.
One of the weaknesses of the paper is that it assumes a subgaussian distribution for the kernel's spectrum, which may not always be the case in practice. Additionally, the construction of polynomially-exact quadrature rules can be computationally expensive, and the authors do not provide a clear solution to this problem.
The paper is well-written and easy to follow, with clear explanations of the technical concepts and a well-organized structure. The authors provide a thorough review of related work and clearly explain how their approach differs from existing methods.
In terms of originality, the paper proposes a novel approach to constructing feature maps for kernel machines, which is a significant contribution to the field. The authors also provide a thorough analysis of the theoretical foundations of their approach, which demonstrates a deep understanding of the underlying concepts.
The significance of the paper lies in its potential to improve the accuracy and efficiency of kernel machines, which are widely used in many applications. The authors demonstrate the effectiveness of their approach on several datasets, including MNIST and TIMIT, and show that it can achieve comparable accuracy to state-of-the-art methods based on random Fourier features.
Overall, I would recommend accepting this paper for publication, as it makes a significant contribution to the field of machine learning and demonstrates a thorough understanding of the underlying concepts. However, I would suggest that the authors address the weaknesses mentioned above, such as providing a clear solution to the computational expense of constructing polynomially-exact quadrature rules, and exploring the applicability of their approach to non-subgaussian distributions.
Arguments pro acceptance:
* The paper proposes a novel approach to constructing feature maps for kernel machines, which is a significant contribution to the field.
* The authors provide a thorough analysis of the theoretical foundations of their approach, which demonstrates a deep understanding of the underlying concepts.
* The paper is well-written and easy to follow, with clear explanations of the technical concepts and a well-organized structure.
* The authors demonstrate the effectiveness of their approach on several datasets, including MNIST and TIMIT, and show that it can achieve comparable accuracy to state-of-the-art methods based on random Fourier features.
Arguments con acceptance:
* The paper assumes a subgaussian distribution for the kernel's spectrum, which may not always be the case in practice.
* The construction of polynomially-exact quadrature rules can be computationally expensive, and the authors do not provide a clear solution to this problem.
* The paper could benefit from a more detailed analysis of the computational complexity of the proposed approach, and a comparison with other methods in terms of computational efficiency.