This paper presents a novel method for representing words in a continuous vector form, called hash embeddings. The authors extend previous work on word embeddings by introducing a hashing-based approach that reduces the number of parameters required to represent large vocabularies. The key idea is to use multiple hash functions to map each token to a set of component vectors, which are then combined using trainable importance parameters.
The paper is well-motivated, and the authors provide a clear explanation of the limitations of existing word embedding methods. The proposed hash embedding method is technically sound, and the authors provide a thorough analysis of its properties and advantages. The experimental results demonstrate that hash embeddings can achieve comparable or better performance than standard embeddings on a range of text classification tasks, while requiring significantly fewer parameters.
However, the presentation of the paper could be improved. The writing is sometimes haphazard, and the authors could benefit from reorganizing the sections to improve clarity. Additionally, some of the notation and terminology could be more clearly defined.
One of the strengths of the paper is its ability to handle large vocabularies without requiring a dictionary or vocabulary pruning. The authors also demonstrate that hash embeddings can be used with or without a dictionary, making them a flexible and practical solution for a range of applications.
The paper could be improved by discussing the proposed links to incoherence and developing analogous notions of incoherence for non-linear settings. Additionally, the authors could compare their work to Ren et al.'s research on asymptotic normality and optimalities in estimation of large Gaussian graphical models.
The text following Corollary 3 is unclear, particularly regarding the connection between generative and discriminative models under isotropic Gaussian design. The authors could benefit from providing more context and explanation to help readers understand the significance of this result.
The authors' remarks comparing sample requirements between methods could be softened, as there are no established lower bounds to support such claims. However, the paper provides a valuable contribution to the field of natural language processing, and the proposed hash embedding method has the potential to be widely adopted.
Overall, the paper is well-written, and the authors provide a clear and concise explanation of their method. The experimental results are convincing, and the paper makes a significant contribution to the field. With some revisions to address the areas mentioned above, the paper has the potential to be even stronger.
Arguments pro acceptance:
* The paper presents a novel and technically sound method for representing words in a continuous vector form.
* The authors provide a thorough analysis of the properties and advantages of the proposed hash embedding method.
* The experimental results demonstrate that hash embeddings can achieve comparable or better performance than standard embeddings on a range of text classification tasks.
* The paper makes a significant contribution to the field of natural language processing.
Arguments con acceptance:
* The presentation of the paper could be improved, with some sections feeling haphazard or unclear.
* The authors could benefit from providing more context and explanation for some of the results, such as the connection between generative and discriminative models under isotropic Gaussian design.
* The remarks comparing sample requirements between methods could be softened, as there are no established lower bounds to support such claims.