This paper studies the limitations of gradient descent-based optimization methods when used in conjunction with smooth kernels, and proposes a novel preconditioning scheme called EigenPro to alleviate these limitations. The authors demonstrate that gradient descent with smooth kernels can only approximate very smooth functions, and that the number of iterations required to achieve a certain accuracy can be exponential in the dimensionality of the data.
The paper is well-organized and clearly written, with a good balance between theoretical analysis and experimental results. The authors provide a thorough discussion of the limitations of gradient descent with smooth kernels, and demonstrate the effectiveness of their proposed EigenPro method in improving the convergence rate of gradient descent.
One of the strengths of the paper is its ability to provide a clear and concise explanation of the underlying theory, making it accessible to a wide range of readers. The authors also provide a thorough analysis of the computational complexity of their proposed method, which is essential for understanding its practical implications.
However, there are some areas where the paper could be improved. For example, the authors could provide more motivation for why the random permutation model is used, and how it relates to real-world applications. Additionally, the paper could benefit from more detailed comparisons with other related work, such as other preconditioning schemes or optimization methods.
In terms of the conference guidelines, the paper meets most of the criteria. The paper is well-written, and the authors provide a clear and concise explanation of the underlying theory. The paper also provides a thorough analysis of the computational complexity of the proposed method, which is essential for understanding its practical implications.
However, the paper could be improved in terms of providing more motivation for why the random permutation model is used, and how it relates to real-world applications. Additionally, the paper could benefit from more detailed comparisons with other related work, such as other preconditioning schemes or optimization methods.
Overall, the paper is well-written, and the authors provide a clear and concise explanation of the underlying theory. The paper also provides a thorough analysis of the computational complexity of the proposed method, which is essential for understanding its practical implications. With some minor improvements, the paper has the potential to make a significant contribution to the field of machine learning.
Arguments for acceptance:
* The paper provides a clear and concise explanation of the underlying theory, making it accessible to a wide range of readers.
* The authors provide a thorough analysis of the computational complexity of their proposed method, which is essential for understanding its practical implications.
* The paper demonstrates the effectiveness of the proposed EigenPro method in improving the convergence rate of gradient descent.
Arguments against acceptance:
* The paper could benefit from more motivation for why the random permutation model is used, and how it relates to real-world applications.
* The paper could benefit from more detailed comparisons with other related work, such as other preconditioning schemes or optimization methods.
* Some of the experimental results could be improved, such as providing more detailed comparisons with other methods or using more realistic datasets.