This paper proposes a novel method to reduce the variance of stochastic gradients for variational inference by excluding the score function from the reparameterized gradient estimator. The authors address an important topic in variational inference, evaluating different ways of computing stochastic gradients with the reparameterization trick. The paper presents surprising results, including that a Monte Carlo approximation to the entropy term can lead to lower variance than using the exact term, and the score function adds unnecessary variance to stochastic gradients.
The paper's empirical results show modest improvements, and while well-written, could benefit from clarification on certain equations and notational changes for better understanding. The use of the reparameterization trick and its impact on stochastic gradients is a significant area of study, with the paper providing valuable insights into optimizing its application.
The strengths of the paper include its novel contribution to the field of variational inference, its well-structured and easy-to-follow presentation, and its empirical validation on benchmark datasets. The weaknesses of the paper include the modest improvements in empirical results and the potential for overfitting.
Arguments pro acceptance:
* The paper proposes a novel and general method for reducing variance in stochastic gradients for variational inference.
* The paper provides a clear and well-structured presentation of the method and its empirical validation.
* The paper addresses an important topic in variational inference and provides valuable insights into optimizing the application of the reparameterization trick.
Arguments con acceptance:
* The empirical results show only modest improvements, which may not be significant enough to warrant acceptance.
* The paper could benefit from clarification on certain equations and notational changes for better understanding.
* The potential for overfitting is not fully addressed in the paper.
Overall, I would recommend acceptance of the paper, as it provides a novel and valuable contribution to the field of variational inference, and its strengths outweigh its weaknesses. However, I would suggest that the authors address the potential for overfitting and provide clarification on certain equations and notational changes to improve the paper's clarity and readability.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The paper is a complete piece of work, and the authors are careful and honest about evaluating both the strengths and weaknesses of the work.
In terms of clarity, the paper is well-written, but could benefit from clarification on certain equations and notational changes for better understanding. The paper is well-organized, and the authors provide enough information for the expert reader to reproduce the results.
In terms of originality, the paper proposes a novel method for reducing variance in stochastic gradients for variational inference, which is a significant contribution to the field. The paper is a novel combination of familiar techniques, and it is clear how this work differs from previous contributions.
In terms of significance, the paper addresses an important topic in variational inference, and the results are important and likely to be used by other researchers and practitioners. The paper advances the state of the art in a demonstrable way, and it provides unique insights into optimizing the application of the reparameterization trick.