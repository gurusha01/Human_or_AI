This paper proposes a novel algorithm that learns ensembles of deep decision trees to optimize both prediction accuracy and feature cost, resulting in an adaptive prediction system. The algorithm is an iterative process that extends gradient tree boosting with a cost-sensitive objective function, adding decision trees to the ensemble while optimizing the cost of the entire ensemble. A cost-aware impurity function is introduced to facilitate individual tree learning with cost constraints, given the existing ensemble.
The paper is well-written, with interesting and novel algorithm derivations, addressing a highly relevant problem in many domains. The authors provide a clear and concise explanation of the proposed algorithm, and the experimental results demonstrate its effectiveness in outperforming existing methods in cost-aware ensemble learning on several real datasets.
The strengths of the paper include its novelty, technical soundness, and clarity. The algorithm is well-motivated, and the authors provide a thorough analysis of its properties and behavior. The experimental results are convincing, and the authors provide a detailed comparison with existing methods.
However, there are some weaknesses to the paper. One potential limitation is that the algorithm may not be suitable for very large datasets, as the iterative process of adding decision trees to the ensemble may become computationally expensive. Additionally, the authors could provide more insight into the choice of hyperparameters and the sensitivity of the algorithm to these choices.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The paper is technically sound, well-written, and provides a novel contribution to the field. The results are important, and the paper addresses a difficult problem in a better way than previous research.
Arguments pro acceptance:
* The paper proposes a novel and effective algorithm for cost-aware ensemble learning.
* The algorithm is well-motivated, and the authors provide a thorough analysis of its properties and behavior.
* The experimental results demonstrate the effectiveness of the algorithm in outperforming existing methods on several real datasets.
Arguments con acceptance:
* The algorithm may not be suitable for very large datasets due to its iterative nature.
* The authors could provide more insight into the choice of hyperparameters and the sensitivity of the algorithm to these choices.
Overall, I recommend accepting the paper, as it provides a significant contribution to the field of machine learning and addresses a highly relevant problem in many domains.