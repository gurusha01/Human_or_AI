This paper discusses the limitations of gradient descent-based optimization methods when used with smooth kernels in shallow learning methods. The authors identify a fundamental limitation in the computational reach of gradient descent, which is that only very smooth functions can be approximated after a polynomial number of iterations. This limitation is purely algorithmic and persists even in the limit of infinite data. To address this shortcoming, the authors propose EigenPro iteration, a simple and direct preconditioning scheme that uses a small number of approximately computed eigenvectors to modify the spectrum of the kernel matrix.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments. The theoretical analysis is thorough and well-supported by experimental results. The authors provide a detailed comparison of EigenPro with other state-of-the-art kernel methods, demonstrating its effectiveness in improving convergence and reducing computational costs.
One of the strengths of the paper is its ability to provide a clear and concise explanation of the underlying issues with gradient descent-based optimization methods. The authors do an excellent job of motivating the need for a new approach and providing a detailed analysis of the limitations of existing methods. The proposed EigenPro iteration is also well-motivated and easy to understand, making it a compelling solution to the problems identified in the paper.
However, there are a few areas where the paper could be improved. One issue is with the figures in Section 5, which are too small and have illegible labels, making them difficult to read and understand. Additionally, some of the experimental results could be more thoroughly explained, and the authors could provide more context about the datasets and metrics used in the experiments.
Overall, the paper makes a significant contribution to the field of machine learning, providing a new perspective on the limitations of gradient descent-based optimization methods and proposing a effective solution to address these limitations. The paper is well-written, well-organized, and provides a thorough analysis of the underlying issues and proposed solution.
Arguments pro acceptance:
* The paper provides a clear and concise explanation of the underlying issues with gradient descent-based optimization methods.
* The proposed EigenPro iteration is well-motivated and easy to understand.
* The paper provides a thorough analysis of the limitations of existing methods and demonstrates the effectiveness of EigenPro in improving convergence and reducing computational costs.
* The experimental results are impressive and demonstrate the potential of EigenPro in real-world applications.
Arguments con acceptance:
* The figures in Section 5 are too small and have illegible labels, making them difficult to read and understand.
* Some of the experimental results could be more thoroughly explained, and the authors could provide more context about the datasets and metrics used in the experiments.
* The paper could benefit from a more detailed comparison with other related work in the field.
Quality: 9/10
The paper is well-written, well-organized, and provides a thorough analysis of the underlying issues and proposed solution. The experimental results are impressive and demonstrate the potential of EigenPro in real-world applications.
Clarity: 8.5/10
The paper is clearly written, but some of the figures and experimental results could be more thoroughly explained.
Originality: 9/10
The paper provides a new perspective on the limitations of gradient descent-based optimization methods and proposes a effective solution to address these limitations.
Significance: 9/10
The paper makes a significant contribution to the field of machine learning, providing a new perspective on the limitations of gradient descent-based optimization methods and proposing a effective solution to address these limitations.