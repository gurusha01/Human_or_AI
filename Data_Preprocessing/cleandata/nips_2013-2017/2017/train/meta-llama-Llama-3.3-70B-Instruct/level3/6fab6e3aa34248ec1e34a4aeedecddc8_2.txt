This paper proposes a novel method called hash embeddings for representing words in a continuous vector form, which is an extension and improvement of standard word embeddings. The key idea is to use multiple hash functions to map each token to a set of component vectors, and then combine these vectors using trainable importance parameters. This approach allows for efficient handling of large vocabularies and reduces the number of parameters required.
The paper provides a clear and well-organized presentation of the method, including a detailed description of the hash embedding construction and the experimental setup. The experiments demonstrate that hash embeddings achieve comparable or better performance than standard embeddings across a range of text classification tasks, while requiring significantly fewer parameters.
The strengths of the paper include its novel idea, wide applicability, and meaningful ablation studies. The use of hash embeddings shows promise in reducing the parameter space and improving model performance, especially in scenarios where vocabulary size is a concern. The paper also provides a thorough analysis of the method's properties and behavior, including the effect of collisions and the importance of parameter tuning.
However, there are some weaknesses and areas for improvement. The paper could benefit from more comprehensive comparisons to state-of-the-art methods, as well as more detailed analysis of the hash embedding's behavior in different scenarios. Additionally, some technical details, such as the choice of hash functions and the optimization of importance parameters, could be further clarified.
Overall, the paper presents a well-motivated and well-executed contribution to the field of natural language processing, and the proposed hash embedding method has the potential to be a useful tool in a variety of applications. With some revisions to address the mentioned weaknesses, the paper could be even stronger. I recommend acceptance of the paper, and I am interested in seeing how the findings carry over to other architectures and tasks in the future.