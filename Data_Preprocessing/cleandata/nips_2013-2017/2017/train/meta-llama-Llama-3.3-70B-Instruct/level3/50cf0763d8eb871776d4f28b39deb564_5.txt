This paper presents a novel method for representing words in a continuous vector form, called hash embeddings. The authors propose an efficient hybrid between standard word embeddings and feature hashing, which allows for a significant reduction in the number of parameters required to represent large vocabularies. The key idea is to use multiple hash functions to select component vectors from a shared pool, and then combine these vectors using trainable importance parameters.
The paper is well-written, and the authors provide a clear and concise explanation of the proposed method. The experiments demonstrate the effectiveness of hash embeddings in various text classification tasks, showing that they can achieve comparable or better performance than standard embeddings while requiring significantly fewer parameters. The authors also provide a theoretical analysis of the convergence of maximum likelihood estimates for generative and discriminative models, which adds to the paper's technical soundness.
One of the strengths of the paper is its ability to handle huge vocabularies without the need for creating a dictionary beforehand or performing vocabulary pruning after training. The authors also demonstrate the flexibility of hash embeddings by showing that they can be used both with and without a dictionary. The experimental results suggest that hash embeddings have an inherent regularizing effect on performance, which is an interesting finding that warrants further investigation.
However, there are some areas where the paper could be improved. For example, the authors could provide more detailed analysis of the trade-offs between the number of hash functions, buckets, and importance parameters. Additionally, the paper could benefit from more comparisons with other state-of-the-art methods, such as wordCNN or character-based models.
In terms of originality, the paper presents a novel combination of familiar techniques, which is a significant contribution to the field. The authors also provide a clear explanation of how their method differs from previous contributions, which demonstrates a good understanding of the related work.
Overall, I would recommend accepting this paper, as it presents a well-motivated and technically sound contribution to the field of natural language processing. The paper has the potential to make a significant impact on real-world problems, particularly in applications where large vocabularies are common. With some minor revisions to address the areas mentioned above, the paper could be even stronger.
Arguments pro acceptance:
* The paper presents a novel and efficient method for representing words in a continuous vector form.
* The experiments demonstrate the effectiveness of hash embeddings in various text classification tasks.
* The authors provide a clear and concise explanation of the proposed method.
* The paper has the potential to make a significant impact on real-world problems.
Arguments con acceptance:
* The paper could benefit from more detailed analysis of the trade-offs between the number of hash functions, buckets, and importance parameters.
* The paper could benefit from more comparisons with other state-of-the-art methods.
* Some minor revisions are needed to address the areas mentioned above.