This paper proposes a novel approach to constructing feature maps for kernel machines, leveraging deterministic quadrature rules to approximate the kernel's Fourier transform integral. The authors introduce several methods, including polynomially-exact quadrature, dense grid construction, sparse grid construction, and reweighted grid construction, to achieve better scaling in the desired accuracy compared to the state-of-the-art method, random Fourier features.
The paper is well-written, and the authors provide a clear and concise overview of the background and related work. The technical contributions are significant, and the experimental results demonstrate the effectiveness of the proposed methods, particularly for sparse ANOVA kernels. The authors also provide a thorough analysis of the sample complexity of their methods, which is a crucial aspect of kernel machines.
One of the strengths of the paper is its ability to bridge the gap between kernel methods and deep learning, by showing that deterministic feature maps can be used to improve the performance of kernel machines, making them more competitive with deep neural networks. The authors also highlight the potential applications of their work in various domains, such as speech recognition and image classification.
However, there are some areas that could be improved. For example, the authors could provide more detailed comparisons with other kernel methods, such as the Nystr√∂m method, and explore the connections between their work and other areas of machine learning, such as Gaussian processes and Bayesian neural networks.
In terms of the conference guidelines, the paper meets the criteria for quality, clarity, originality, and significance. The technical contributions are sound, and the experimental results are well-presented and convincing. The paper is well-organized, and the authors provide sufficient background and context for the reader to understand the contributions.
Here is a list of arguments pro and con acceptance:
Pros:
* The paper proposes a novel and significant contribution to the field of kernel machines.
* The technical contributions are sound, and the experimental results demonstrate the effectiveness of the proposed methods.
* The paper is well-written, and the authors provide a clear and concise overview of the background and related work.
* The potential applications of the work are significant, and the authors highlight the potential for kernel machines to be used in various domains.
Cons:
* The paper could benefit from more detailed comparisons with other kernel methods.
* The authors could explore the connections between their work and other areas of machine learning, such as Gaussian processes and Bayesian neural networks.
* Some of the technical details, such as the construction of the quadrature rules, could be clarified or expanded upon.
Overall, I would recommend accepting this paper, as it makes a significant contribution to the field of kernel machines and has the potential to impact various areas of machine learning.