This paper presents a novel approach to word embeddings, called hash embeddings, which combines the benefits of standard word embeddings and feature hashing. The authors propose a method that uses multiple hash functions to map words to a set of component vectors, and then combines these vectors using trainable importance parameters. This approach allows for efficient representation of large vocabularies, eliminates the need for creating a dictionary beforehand, and provides a mechanism for implicit vocabulary pruning.
The paper is well-written, and the authors provide a clear explanation of the proposed method, along with theoretical analysis and experimental results. The experiments demonstrate that hash embeddings achieve comparable or better performance than standard embeddings on a range of text classification tasks, while requiring significantly fewer parameters.
One of the strengths of the paper is its ability to handle large vocabularies efficiently. The authors show that hash embeddings can deal with huge vocabularies consisting of millions of tokens, without the need for vocabulary pruning or creating a dictionary beforehand. This makes it particularly useful for problems such as online learning, where a dictionary cannot be constructed before training.
The paper also provides a good analysis of the trade-offs between different approaches to word embeddings, including standard embeddings, feature hashing, and hash embeddings. The authors discuss the advantages and disadvantages of each approach and provide a clear comparison of their proposed method with existing methods.
However, there are some areas where the paper could be improved. For example, the authors could provide more detailed analysis of the effect of the number of hash functions and the size of the component vectors on the performance of the model. Additionally, the paper could benefit from more discussion on the interpretability of the importance parameters and how they relate to the semantics of the words.
In terms of originality, the paper presents a novel approach to word embeddings that combines the benefits of standard word embeddings and feature hashing. The use of multiple hash functions and trainable importance parameters is a new contribution, and the authors provide a clear explanation of how this approach works.
Overall, the paper is well-written, and the proposed method has the potential to make a significant impact in the field of natural language processing. The experimental results are convincing, and the authors provide a good analysis of the trade-offs between different approaches to word embeddings.
Arguments pro acceptance:
* The paper presents a novel approach to word embeddings that combines the benefits of standard word embeddings and feature hashing.
* The proposed method is efficient and can handle large vocabularies without the need for vocabulary pruning or creating a dictionary beforehand.
* The experimental results demonstrate that hash embeddings achieve comparable or better performance than standard embeddings on a range of text classification tasks.
* The paper provides a good analysis of the trade-offs between different approaches to word embeddings.
Arguments con acceptance:
* The paper could benefit from more detailed analysis of the effect of the number of hash functions and the size of the component vectors on the performance of the model.
* The paper could provide more discussion on the interpretability of the importance parameters and how they relate to the semantics of the words.
* The paper assumes that the importance parameters are learned during training, but it is not clear how these parameters are initialized or updated during training. 
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 8/10