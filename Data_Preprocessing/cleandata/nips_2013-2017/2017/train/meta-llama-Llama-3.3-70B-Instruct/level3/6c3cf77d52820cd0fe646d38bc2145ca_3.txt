This paper presents a unique approach to constructing deterministic feature maps for kernel machines, which is a significant contribution to the field of machine learning. The authors propose a method for approximating the kernel's Fourier transform integral using Gaussian quadrature, which allows for the construction of deterministic feature maps with better scaling in the desired accuracy compared to the state-of-the-art method, random Fourier features.
The paper is well-structured, and the authors provide sufficient figures to support their experiments. The probabilistic analysis in the paper is notable and provides thought-provoking ideas. The authors also demonstrate the effectiveness of their method on real datasets, including MNIST and TIMIT, and show that deterministic feature maps can produce comparable accuracy to the state-of-the-art methods based on random Fourier features.
The strengths of the paper include:
* The proposal of a novel method for constructing deterministic feature maps, which has the potential to improve the accuracy and efficiency of kernel machines.
* The provision of a thorough analysis of the method, including theoretical bounds on the sample complexity and experimental results on real datasets.
* The demonstration of the effectiveness of the method on sparse ANOVA kernels, which is an important class of kernels in machine learning.
The weaknesses of the paper include:
* The method may not be applicable to all types of kernels, and the authors assume that the kernel is subgaussian, which may not always be the case.
* The construction of the feature maps requires the solution of a non-negative least squares problem, which can be computationally expensive.
* The authors do not provide a detailed comparison with other methods, such as quasi-Monte Carlo estimation, which may be relevant to the problem.
Overall, I recommend acceptance of the paper, but I have some doubts about its relevance and potential impact on the NIPS community. The paper is well-written, and the authors provide a clear and concise presentation of their method and results. However, the method may not be widely applicable, and the authors should consider providing more context and comparison with other methods to demonstrate the significance of their contribution.
Arguments pro acceptance:
* The paper presents a novel and significant contribution to the field of machine learning.
* The authors provide a thorough analysis of the method, including theoretical bounds and experimental results.
* The method has the potential to improve the accuracy and efficiency of kernel machines.
Arguments con acceptance:
* The method may not be widely applicable, and the authors assume that the kernel is subgaussian.
* The construction of the feature maps requires the solution of a non-negative least squares problem, which can be computationally expensive.
* The authors do not provide a detailed comparison with other methods, which may be relevant to the problem.