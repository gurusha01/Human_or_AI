This paper introduces a novel approach to word embeddings, called hash embeddings, which can efficiently handle large vocabularies. The key idea is to represent each token as a weighted sum of component vectors, where the weights are learned during training. This approach is an extension of feature hashing, but with a trainable mechanism to handle collisions. The authors demonstrate that hash embeddings can achieve comparable or better performance than standard embeddings on a range of text classification tasks, while requiring significantly fewer parameters.
The paper is well-structured and easy to follow, with clear explanations of the proposed approach and its advantages. The experiments are thorough and well-designed, covering various scenarios, including the use of dictionaries and ensemble methods. The results show that hash embeddings can be a viable alternative to standard embeddings, especially in situations where vocabulary size is a concern.
One of the strengths of the paper is its ability to spark future experimentation. The idea of using hash embeddings as a regularizer is particularly interesting, as it offers a new perspective on how to control model complexity. The authors also provide a clear and concise explanation of the hashing theory, which is essential for understanding the proposed approach.
However, there are some areas that need improvement. The presentation and writing could be more polished, with some sections feeling a bit disjointed. The experiments, while thorough, are narrow in scope, and it would be beneficial to see more comparisons with other state-of-the-art methods. Additionally, the argument for why language modulation should be done through normalization parameters is not well-argued, and the paper lacks discussion of hash embeddings in the context of alternative early-fusion strategies.
The inclusion of Section 2.1 is unclear, and the description of the proposed methodology seems independent of the choice of model. The improvement in performance is mostly accounted for by applying hash embeddings to the highest level stage, which is surprising and needs further discussion. Furthermore, the paper lacks experiments with other model architectures or different types of multi-modal data to strengthen the submission.
Minor issues with formatting, citations, and grammar also need to be addressed. Nevertheless, the paper has the potential to make a significant contribution to the field, and with some revisions, it could be even stronger. Overall, I would recommend accepting this paper, but with some revisions to address the mentioned weaknesses. 
Arguments pro acceptance:
- Novel approach to word embeddings
- Efficient handling of large vocabularies
- Comparable or better performance than standard embeddings
- Interesting idea of using hash embeddings as a regularizer
Arguments con acceptance:
- Narrow scope of experiments
- Lack of discussion on alternative early-fusion strategies
- Unclear inclusion of Section 2.1
- Minor issues with formatting, citations, and grammar.