This paper presents a thorough analysis of the limitations of gradient descent as a learning strategy for kernel methods, introducing the concept of "computational reach" to demonstrate its restricted scope, particularly for smooth kernels. The authors propose a novel method, EigenPro, which utilizes a preconditioning strategy to overcome these limitations, and provide experimental results showcasing systematic improvements over existing methods.
The paper is well-written, clearly presenting the ideas, experimental setup, and results, contributing valuable insights into large-scale kernel learning and its potential improvements. The introduction of EigenPro, a simple and direct preconditioning scheme, is a significant contribution, as it can be efficiently implemented and is compatible with stochastic gradient descent.
The experimental results demonstrate the effectiveness of EigenPro, achieving acceleration factors of up to 35 times in terms of the number of epochs required to reach the error of the optimal kernel classifier, without any loss of accuracy. The comparisons to state-of-the-art methods on large datasets, such as MNIST, TIMIT, and SUSY, show that EigenPro improves or matches performance at a significantly lower computational budget.
The strengths of the paper include:
* A thorough analysis of the limitations of gradient descent for kernel methods
* The introduction of a novel and effective preconditioning scheme, EigenPro
* Experimental results demonstrating significant improvements over existing methods
* Comparisons to state-of-the-art methods on large datasets
The weaknesses of the paper include:
* The statistical significance of the improvements is not assessed
* The paper could benefit from a more detailed discussion of the relationship between EigenPro and other related work
Arguments for acceptance:
* The paper presents a significant contribution to the field of kernel learning, addressing a crucial limitation of gradient descent
* The proposed method, EigenPro, is simple, efficient, and effective
* The experimental results demonstrate the potential of EigenPro to improve performance on large-scale datasets
Arguments against acceptance:
* The paper could benefit from a more detailed analysis of the statistical significance of the results
* The relationship between EigenPro and other related work could be more thoroughly discussed
Overall, the paper presents a valuable contribution to the field of kernel learning, and the proposed method, EigenPro, has the potential to improve performance on large-scale datasets. With some minor revisions to address the weaknesses, the paper is a strong candidate for acceptance.