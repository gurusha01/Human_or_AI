This paper presents a novel interpretation of batch normalization and ReLU in terms of generalized Hamming distance networks, providing a fresh perspective on neural computing. The authors demonstrate that batch normalization can be seen as approximating the rightful bias induced by the generalized Hamming distance, and that ReLU can be viewed as setting a minimal Hamming distance threshold between network inputs and weights. The proposed generalized Hamming network (GHN) shows promising results, with fast learning speed, well-controlled behavior, and state-of-the-art performances on various learning tasks.
The paper's strengths include its originality, as it provides a new perspective on neural computing and demystifies some effective techniques such as batch normalization and ReLU. The authors also provide a clear and well-organized presentation of their ideas, making it easy to follow their reasoning. The experimental results are also impressive, demonstrating the effectiveness of the proposed GHN architecture.
However, there are some weaknesses that need to be addressed. The connection between generalized Hamming distance and batch normalization, although interesting, is not entirely clear and requires further explanation. The authors could provide more insight into how this connection works across nodes and layers. Additionally, the paper could benefit from more clarification on how the generalized Hamming distance serves to prevent information loss and why layers of GHN are stacked together.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of their work. The paper is also well-written, and the organization is clear and easy to follow.
The originality of the paper is one of its strongest aspects, as it provides a novel combination of familiar techniques and introduces a new perspective on neural computing. The significance of the results is also high, as the proposed GHN architecture demonstrates state-of-the-art performances on various learning tasks.
Overall, I would recommend accepting this paper, as it provides a significant contribution to the field of neural computing and has the potential to inspire future research. The minor typo found in the paper does not affect the overall quality, and the author's rebuttal was appreciated, although it did not change my score.
Arguments pro acceptance:
* Originality and novelty of the approach
* Clear and well-organized presentation
* Impressive experimental results
* Potential to inspire future research
Arguments con acceptance:
* Lack of clarity on the connection between generalized Hamming distance and batch normalization
* Need for more explanation on how the generalized Hamming distance prevents information loss and why layers of GHN are stacked together
* Minor typo in the paper
Score: 8/10
Recommendation: Accept with minor revisions.