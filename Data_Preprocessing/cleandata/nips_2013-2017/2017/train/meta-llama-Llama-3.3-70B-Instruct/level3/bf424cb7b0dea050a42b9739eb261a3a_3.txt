This paper presents a thorough analysis of the limitations of gradient descent-based optimization methods when applied to kernel methods, particularly in the context of large datasets. The authors introduce the concept of "computational reach" and demonstrate that gradient descent can only approximate very smooth functions, even with an infinite number of iterations. To address this limitation, they propose EigenPro, a preconditioning scheme that modifies the spectrum of the kernel matrix to improve convergence.
The paper is well-written, and the authors provide a clear and concise explanation of their methodology and results. The experimental evaluation demonstrates the effectiveness of EigenPro in accelerating convergence and improving accuracy on several large-scale datasets.
Strengths:
* The paper provides a thorough analysis of the limitations of gradient descent-based optimization methods for kernel methods.
* The introduction of EigenPro, a preconditioning scheme that modifies the spectrum of the kernel matrix, is a novel and effective approach to improving convergence.
* The experimental evaluation demonstrates the effectiveness of EigenPro in accelerating convergence and improving accuracy on several large-scale datasets.
Weaknesses:
* The paper assumes a strong background in kernel methods and optimization, which may make it difficult for non-experts to follow.
* The authors could provide more discussion on the relationship between EigenPro and other preconditioning schemes, such as those used in second-order optimization methods.
* The paper could benefit from more detailed analysis of the computational complexity of EigenPro and its scalability to very large datasets.
Arguments for acceptance:
* The paper presents a novel and effective approach to improving convergence of kernel methods, which is a significant contribution to the field.
* The experimental evaluation demonstrates the effectiveness of EigenPro in accelerating convergence and improving accuracy on several large-scale datasets.
* The paper provides a thorough analysis of the limitations of gradient descent-based optimization methods for kernel methods, which is an important contribution to the understanding of these methods.
Arguments against acceptance:
* The paper assumes a strong background in kernel methods and optimization, which may limit its accessibility to a broader audience.
* The authors could provide more discussion on the relationship between EigenPro and other preconditioning schemes, which may be seen as a limitation of the paper.
* The paper could benefit from more detailed analysis of the computational complexity of EigenPro and its scalability to very large datasets, which may be seen as a limitation of the paper.
Overall, I believe that the paper presents a significant contribution to the field of kernel methods and optimization, and I recommend acceptance. However, I suggest that the authors address the weaknesses mentioned above to improve the clarity and completeness of the paper. 
Quality: 9/10
The paper is well-written, and the authors provide a clear and concise explanation of their methodology and results. The experimental evaluation is thorough and demonstrates the effectiveness of EigenPro.
Clarity: 8.5/10
The paper assumes a strong background in kernel methods and optimization, which may make it difficult for non-experts to follow. However, the authors provide a clear and concise explanation of their methodology and results.
Originality: 9/10
The introduction of EigenPro, a preconditioning scheme that modifies the spectrum of the kernel matrix, is a novel and effective approach to improving convergence.
Significance: 9/10
The paper presents a significant contribution to the field of kernel methods and optimization, and the experimental evaluation demonstrates the effectiveness of EigenPro in accelerating convergence and improving accuracy on several large-scale datasets.