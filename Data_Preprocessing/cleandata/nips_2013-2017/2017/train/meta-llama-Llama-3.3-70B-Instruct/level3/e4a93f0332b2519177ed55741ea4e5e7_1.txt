This paper proposes a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN), which combines the strengths of multiscale RNNs and deep transition RNNs. The FS-RNN architecture consists of a lower hierarchical layer with multiple sequentially connected RNN cells (Fast cells) and a higher hierarchical layer with a single RNN cell (Slow cell). The Fast cells update their hidden states multiple times per time step, while the Slow cell updates its hidden state only once per time step. This architecture allows the model to learn complex transition functions from one time step to the next and to capture long-term dependencies efficiently.
The paper presents empirical results on two character-level language modeling datasets, Penn Treebank and Hutter Prize Wikipedia, where the FS-RNN architecture achieves state-of-the-art results. The authors also provide an empirical analysis of the learning and network dynamics of the FS-RNN, which shows that the Slow cell enables the network to learn long-term dependencies, while the Fast cells enable the network to quickly adapt to unexpected inputs and learn complex transition functions.
The strengths of this paper include the proposal of a novel and versatile RNN architecture, the achievement of state-of-the-art results on two benchmark datasets, and the provision of a detailed empirical analysis of the model's dynamics. However, there are also some weaknesses, such as the unclear introduction to previous approaches, the misleading statement about the multi-scale property of the model, and the lack of comparison with simpler approaches like Residual Units or fully connected stacked cells. Additionally, the experimental results are incomplete, as they do not include standard deviations, making it difficult to judge the significance of the results.
Overall, I would argue that the paper is well-written and provides a significant contribution to the field of RNNs. However, there are some areas that need improvement, such as clarifying the introduction and providing more comprehensive experimental results. The paper's quality, clarity, originality, and significance are all high, but the lack of completeness in the experimental results and the unclear introduction are drawbacks that need to be addressed.
Arguments pro acceptance:
* The paper proposes a novel and versatile RNN architecture that achieves state-of-the-art results on two benchmark datasets.
* The empirical analysis of the model's dynamics provides valuable insights into the strengths and weaknesses of the architecture.
* The paper is well-written and easy to follow, making it accessible to a wide range of readers.
Arguments con acceptance:
* The introduction to previous approaches is unclear and confusing, making it hard to understand the methodology.
* The statement about the multi-scale property of the model is misleading, as it operates on a logical time scale, not a physical time scale.
* The lack of comparison with simpler approaches like Residual Units or fully connected stacked cells is a significant drawback.
* The experimental results are incomplete, as they do not include standard deviations, making it difficult to judge the significance of the results.