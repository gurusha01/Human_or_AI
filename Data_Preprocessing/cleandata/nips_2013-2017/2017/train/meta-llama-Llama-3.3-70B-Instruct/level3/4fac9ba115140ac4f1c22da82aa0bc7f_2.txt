This paper proposes a novel approach to neural networks, called the Generalized Hamming Network (GHN), which is based on the concept of generalized hamming distance. The authors claim that this approach provides a theoretically justified framework for re-interpreting many useful neural network techniques in terms of fuzzy logic. The paper is well-written and provides a clear explanation of the proposed approach, as well as its relationship to previous work in the field.
The strengths of the paper include its originality and significance. The authors provide a new perspective on neural computing, which has the potential to advance the state of the art in the field. The experimental results are also promising, demonstrating fast learning speed, well-controlled behavior, and state-of-the-art performances on several learning tasks.
However, there are some weaknesses to the paper. The authors' claim that batch normalization assumes feature computation is much more expensive than evaluation cost is incorrect, as batch normalization's formulation does take into account evaluation costs. Additionally, the paper could benefit from more datasets and comparisons to other methods to make the results more convincing. The surprisingly good results in Figure 2(b) also require more information on the number of trees and average number of nodes used to achieve such performance.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of their work. The paper is also well-organized and clearly written, making it easy to follow and understand.
The originality of the paper is one of its strongest aspects. The authors provide a novel combination of familiar techniques, which has the potential to advance the state of the art in the field. The paper is also significant, as it addresses a difficult problem in a better way than previous research and provides a unique perspective on neural computing.
Overall, I believe that the paper provides a refinement of previous work and the experimental results are promising. However, the novelty of the paper is limited, and additional datasets and comparisons would make it stronger. I would recommend accepting the paper, but with revisions to address the weaknesses mentioned above. 
Arguments pro acceptance:
- The paper provides a novel approach to neural networks, which has the potential to advance the state of the art in the field.
- The experimental results are promising, demonstrating fast learning speed, well-controlled behavior, and state-of-the-art performances on several learning tasks.
- The paper is well-written and provides a clear explanation of the proposed approach, as well as its relationship to previous work in the field.
Arguments con acceptance:
- The authors' claim that batch normalization assumes feature computation is much more expensive than evaluation cost is incorrect.
- The paper could benefit from more datasets and comparisons to other methods to make the results more convincing.
- The surprisingly good results in Figure 2(b) require more information on the number of trees and average number of nodes used to achieve such performance.