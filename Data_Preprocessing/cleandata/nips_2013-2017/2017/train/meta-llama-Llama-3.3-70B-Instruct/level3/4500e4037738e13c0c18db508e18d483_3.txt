This paper addresses the limitations of gradient descent-based optimization methods when used in conjunction with smooth kernels, a common setting in machine learning. The authors identify a fundamental limitation in the computational reach of gradient descent, which can only approximate very smooth functions in a polynomial number of iterations. To alleviate this issue, they propose EigenPro iteration, a preconditioned Richardson iteration that uses approximate second-order information to accelerate convergence.
The paper is well-written and clearly organized, making it easy to follow the authors' arguments. The introduction provides a thorough background on the limitations of gradient descent and the importance of understanding the interplay between architecture and optimization in machine learning. The authors also provide a detailed analysis of the computational reach of gradient descent and its limitations, which is a significant contribution to the field.
The proposed EigenPro iteration is a simple yet effective method for accelerating convergence. The authors provide a thorough analysis of the method, including its convergence properties and computational complexity. The experimental results demonstrate the effectiveness of EigenPro in improving the convergence of gradient descent on several large-scale datasets.
One of the strengths of the paper is its clarity and readability. The authors provide a clear and concise explanation of the technical details, making it accessible to a broad audience. The paper also provides a thorough analysis of related work, which helps to situate the contributions of the paper in the context of existing research.
However, there are some areas where the paper could be improved. For example, the authors could provide more details on the choice of hyperparameters, such as the number of eigen-directions and the damping factor. Additionally, the paper could benefit from more extensive experimental results, including comparisons with other optimization methods and a more detailed analysis of the computational complexity of EigenPro.
Overall, this paper makes a significant contribution to the field of machine learning by identifying a fundamental limitation of gradient descent and proposing an effective method for alleviating it. The paper is well-written, clearly organized, and provides a thorough analysis of the technical details. With some minor improvements, this paper has the potential to be a seminal work in the field.
Arguments for acceptance:
* The paper identifies a fundamental limitation of gradient descent and proposes an effective method for alleviating it.
* The paper provides a thorough analysis of the computational reach of gradient descent and its limitations.
* The proposed EigenPro iteration is a simple yet effective method for accelerating convergence.
* The experimental results demonstrate the effectiveness of EigenPro in improving the convergence of gradient descent on several large-scale datasets.
Arguments against acceptance:
* The paper could benefit from more extensive experimental results, including comparisons with other optimization methods.
* The paper could provide more details on the choice of hyperparameters, such as the number of eigen-directions and the damping factor.
* The paper assumes a quadratic loss function, which may not be applicable to all machine learning problems.
Quality: 8/10
Clarity: 9/10
Originality: 8/10
Significance: 9/10
Overall, I would recommend accepting this paper with minor revisions to address the areas mentioned above.