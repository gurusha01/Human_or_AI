This paper presents a comprehensive analysis of the convergence rates of generative and discriminative models in high-dimensional settings. The authors develop a novel notion of separability for loss functions, which allows them to derive `1 and `2 convergence rates for general M-estimators. They then instantiate these results for both generative and discriminative models, providing insights into their nuanced behaviors in high-dimensions.
The paper's approach relies heavily on assumptions that are often false in real-world applications, such as independence and identical distribution of hypothesis triplets. The method's validity is questionable due to its inability to handle arbitrary dependencies between hypotheses, which is a common issue in bioinformatics and other fields. Furthermore, the use of neural networks is overly emphasized, and the approach may work just as well with simpler, more interpretable functions.
The paper's exposition and notation could be improved, particularly in Section 3, to clarify the description of Algorithm 1 and the use of cross-validation. The implementation and training procedure lack sufficient detail, including justification of hyperparameter choices and experimental results on robustness to hyperparameters. The use of cross-validation introduces randomness to the results, which is undesirable in statistical association testing, and experiments should be conducted to demonstrate stability across different random partitions.
The approach does not explicitly account for the number of hypotheses being tested, which is a crucial factor in multiple hypothesis testing, and should be relaxed to allow for arbitrary numbers of tests. The paper contains several incorrect or imprecise claims, including definitions of P-value, FDP, and FDR, which should be corrected to ensure accuracy. Finally, the paper would benefit from proof-reading to address frequent typos and formatting issues.
Arguments for acceptance:
* The paper presents a novel and comprehensive analysis of the convergence rates of generative and discriminative models in high-dimensional settings.
* The authors develop a new notion of separability for loss functions, which allows them to derive `1 and `2 convergence rates for general M-estimators.
* The paper provides insights into the nuanced behaviors of generative and discriminative models in high-dimensions, which is an important area of research.
Arguments against acceptance:
* The paper's approach relies heavily on assumptions that are often false in real-world applications.
* The method's validity is questionable due to its inability to handle arbitrary dependencies between hypotheses.
* The use of neural networks is overly emphasized, and the approach may work just as well with simpler, more interpretable functions.
* The paper lacks sufficient detail in its implementation and training procedure, and the use of cross-validation introduces randomness to the results.
* The approach does not explicitly account for the number of hypotheses being tested, and the paper contains several incorrect or imprecise claims.
Overall, while the paper presents a novel and comprehensive analysis of the convergence rates of generative and discriminative models, its limitations and flaws outweigh its contributions, and it requires significant revisions to address these issues.