This paper proposes a generalized hamming network (GHN) that revisits fuzzy neural networks with a cornerstone notion of generalized hamming distance. The authors claim that this framework provides a novel and theoretically justified perspective on neural computing, allowing for the re-interpretation of techniques such as batch normalization and ReLU units. However, I find the explanation unclear and unconvincing.
The paper's main contribution is the proposal of a GHN, which is claimed to demystify and confirm the effectiveness of batch normalization and ReLU. However, I see no major technical contribution or novelty in this work. The paper's writing is also poor, with numerous typos, grammar, and syntax mistakes, making it difficult to understand and take seriously.
In terms of quality, the paper lacks theoretical insights on the effectiveness of batch normalization and ReLU, which is a significant weakness. The authors' claims about the GHN's ability to provide a rigorous analysis and interpretation within the fuzzy logic theory are not well-supported by the paper's content. The experimental results presented are limited and do not demonstrate a significant improvement over existing methods.
Regarding clarity, the paper is poorly written, and the authors' use of terminology and notation is often confusing. The paper's organization is also unclear, making it difficult to follow the authors' arguments. The lack of clear and concise explanations of the GHN's architecture and its components is a significant issue.
In terms of originality, the paper's proposal of a GHN is not particularly novel, and the authors' claims about its ability to demystify batch normalization and ReLU are not well-supported. The paper's use of fuzzy logic and generalized hamming distance is not new, and the authors do not provide a clear explanation of how their work differs from previous contributions.
Finally, regarding significance, the paper's results are not significant, and the authors' claims about the GHN's ability to provide state-of-the-art performances on a variety of learning tasks are not well-supported. The paper's lack of theoretical insights and poor writing make it difficult to take seriously, and I do not believe that it makes a significant contribution to the field.
Arguments pro acceptance:
* The paper proposes a novel framework for neural computing based on generalized hamming distance.
* The authors provide some experimental results that demonstrate the effectiveness of the GHN.
Arguments con acceptance:
* The paper's explanation of the GHN is unclear and unconvincing.
* The paper lacks theoretical insights on the effectiveness of batch normalization and ReLU.
* The paper's writing is poor, with numerous typos, grammar, and syntax mistakes.
* The paper's results are not significant, and the authors' claims are not well-supported.
* The paper's lack of clarity and poor writing make it difficult to take seriously.
Overall, I do not recommend accepting this paper due to its poor writing, lack of theoretical insights, and limited experimental results.