This paper presents a thorough analysis of the limitations of gradient descent-based optimization methods when used in conjunction with smooth kernels, particularly in the context of kernel methods. The authors demonstrate that the fast spectral decay of smooth kernels can lead to slow convergence and over-regularization, making it challenging to approximate non-smooth functions. To address this issue, they propose EigenPro, a preconditioning scheme that uses approximate second-order information to accelerate convergence.
The paper's main contributions are:
1. Analysis of gradient descent's limitations: The authors provide a detailed analysis of the limitations of gradient descent when used with smooth kernels, highlighting the issues of slow convergence and over-regularization.
2. Introduction of EigenPro: The authors propose EigenPro, a preconditioning scheme that uses approximate second-order information to accelerate convergence. EigenPro is shown to be effective in improving the convergence rate of gradient descent.
3. Theoretical guarantees: The authors provide theoretical guarantees for the convergence of EigenPro, including bounds on the step size and the acceleration factor.
The paper's strengths include:
1. Clear and concise writing: The paper is well-written, and the authors provide a clear and concise explanation of the technical details.
2. Thorough analysis: The authors provide a thorough analysis of the limitations of gradient descent and the benefits of EigenPro.
3. Extensive experiments: The authors provide extensive experimental results to demonstrate the effectiveness of EigenPro.
However, the paper also has some weaknesses:
1. Readability: While the paper is well-written, some sections may be challenging to follow for readers without a strong background in optimization and kernel methods.
2. Comparison to existing methods: The authors could provide more detailed comparisons to existing methods, such as PEGASOS, to better understand the advantages and limitations of EigenPro.
3. Implications of the results: The authors could provide more discussion on the implications of their results, including potential applications and future directions.
Overall, the paper presents a significant contribution to the field of optimization and kernel methods, and the authors provide a clear and concise explanation of the technical details. However, some sections may be challenging to follow, and more detailed comparisons to existing methods and discussion on the implications of the results would be beneficial.
Arguments for acceptance:
1. The paper presents a thorough analysis of the limitations of gradient descent and proposes a novel preconditioning scheme, EigenPro, to address these limitations.
2. The authors provide theoretical guarantees for the convergence of EigenPro and demonstrate its effectiveness through extensive experiments.
3. The paper has the potential to make a significant impact in the field of optimization and kernel methods.
Arguments against acceptance:
1. The paper may be challenging to follow for readers without a strong background in optimization and kernel methods.
2. The authors could provide more detailed comparisons to existing methods and discussion on the implications of their results.
3. Some sections of the paper may require additional clarification or explanation to ensure that the results are accessible to a broader audience.