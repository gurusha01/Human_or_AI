This paper proposes a novel recurrent neural network (RNN) architecture, the Fast-Slow RNN (FS-RNN), which combines the strengths of both multiscale RNNs and deep transition RNNs. The FS-RNN architecture consists of a lower hierarchical layer with multiple RNN cells that update their hidden states multiple times per time step, and a higher hierarchical layer with a single RNN cell that updates its hidden state only once per time step. The authors evaluate the FS-RNN on two character-level language modeling datasets, Penn Treebank and Hutter Prize Wikipedia, and achieve state-of-the-art results.
The paper is well-written and provides a clear explanation of the proposed architecture and its motivations. The authors also provide a thorough analysis of the network dynamics of the FS-RNN and compare it to other RNN architectures, such as stacked LSTMs and sequential LSTMs. The results show that the FS-RNN is able to learn long-term dependencies and adapt quickly to unexpected inputs.
One of the strengths of the paper is its ability to provide a general framework for connecting RNN cells, allowing for flexibility in applying the architecture to different tasks. The authors also provide a clear explanation of the update rules for the FS-RNN architecture and provide a detailed analysis of the network dynamics.
However, there are some areas that could be improved. For example, the connection between Section 5 and the rest of the paper is not entirely clear, as it seems to explain the distribution that a deep neural network can represent without being a direct consequence of the f-GAN optimization results. Additionally, the authors could provide more justification for why the phi_l can be viewed as "deep sufficient statistics" in Theorem 6, and equation (13) does not appear to be of the form of a deformed exponential family.
Furthermore, some paragraphs, such as the utility theory part, could be moved or deleted to free up space to explain the main results and make the paper more coherent. The experiments could also be more directly related to the main points claimed in the paper, and the results could be presented without relying on the IG view of f-GAN.
In terms of quality, the paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results. The authors are careful and honest about evaluating both the strengths and weaknesses of the work. The paper is clearly written, well-organized, and provides enough information for the expert reader to reproduce the results.
The originality of the paper is high, as it proposes a novel RNN architecture that combines the strengths of both multiscale RNNs and deep transition RNNs. The paper is significant, as it achieves state-of-the-art results on two character-level language modeling datasets and provides a general framework for connecting RNN cells.
Overall, I would recommend accepting this paper, as it provides a significant contribution to the field of recurrent neural networks and language modeling. However, the authors should address the minor issues mentioned above to improve the clarity and coherence of the paper. 
Arguments pro acceptance:
- The paper proposes a novel RNN architecture that combines the strengths of both multiscale RNNs and deep transition RNNs.
- The paper achieves state-of-the-art results on two character-level language modeling datasets.
- The paper provides a general framework for connecting RNN cells, allowing for flexibility in applying the architecture to different tasks.
- The paper is technically sound, and the claims are well-supported by theoretical analysis and experimental results.
Arguments con acceptance:
- The connection between Section 5 and the rest of the paper is not entirely clear.
- The authors could provide more justification for why the phi_l can be viewed as "deep sufficient statistics" in Theorem 6.
- Some paragraphs could be moved or deleted to free up space to explain the main results and make the paper more coherent.
- The experiments could be more directly related to the main points claimed in the paper.