This paper proposes a novel approach to word embeddings, called hash embeddings, which reduces the number of parameters required to represent words in a continuous vector form. The authors demonstrate that hash embeddings can achieve state-of-the-art results on various text classification tasks, while requiring significantly fewer parameters than traditional word embedding models.
The key idea behind hash embeddings is to represent each word as a weighted sum of component vectors, which are selected from a shared pool of vectors using multiple hash functions. This approach allows for a significant reduction in the number of parameters, as the same component vectors can be shared across multiple words. The authors also introduce importance parameters, which are learned during training and allow the model to selectively weight the component vectors for each word.
The paper is well-written and clearly explains the motivation and technical details of the proposed approach. The experiments demonstrate the effectiveness of hash embeddings on several benchmark datasets, and the authors provide a thorough analysis of the results. The paper also discusses the relationship between hash embeddings and other related work, such as feature hashing and word embeddings.
One potential weakness of the paper is the lack of discussion on the difficulty of training hash embeddings, particularly with respect to the dual parameters of importance weights and shared embeddings. The authors could provide more insight into how the model handles collisions between words, and how the importance parameters are learned during training. Additionally, it would be interesting to see a more detailed analysis of the relationship between the hash embeddings and the "discourse atoms" mentioned in the paper, and whether the shared embeddings can capture higher-level concepts such as ontology.
Overall, the paper presents a significant contribution to the field of natural language processing, and the proposed approach has the potential to be widely adopted in practice. The strengths of the paper include its clear presentation, thorough experimentation, and significant reduction in parameters required for word embeddings. The weaknesses are minor and primarily relate to the need for further discussion on the training process and the relationship between hash embeddings and other related concepts.
Arguments for acceptance:
* The paper presents a novel and significant contribution to the field of natural language processing.
* The proposed approach achieves state-of-the-art results on several benchmark datasets.
* The paper is well-written and clearly explains the technical details of the proposed approach.
* The experiments are thorough and demonstrate the effectiveness of the proposed approach.
Arguments against acceptance:
* The paper could benefit from a more detailed discussion on the difficulty of training hash embeddings.
* The relationship between hash embeddings and "discourse atoms" could be explored in more detail.
* The paper could provide more insight into how the importance parameters are learned during training.