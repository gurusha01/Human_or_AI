This paper proposes a novel approach to representing words in a continuous vector form, called hash embeddings. The method is an extension of feature hashing, where multiple hash functions are used to select component vectors for each token, and importance parameters are learned to combine these vectors. The approach is well-motivated, drawing parallels to findings in neuroscience on how language influences visual stimulus response in the human brain. 
The use of hash embeddings has several advantages, including the ability to handle huge vocabularies, implicit vocabulary pruning, and a reduction in the number of parameters required. The paper contains good ablation studies and experiments, including a comparison with standard embeddings and an analysis of the effect of using different numbers of hash functions and buckets. The results show that hash embeddings perform at least as well as standard embeddings across a range of tasks, and in some cases, better.
However, there are some limitations to the paper. The experiments are limited to a specific architecture, and it is unclear how well the approach will generalize to other networks. Additionally, the paper contains several typos and grammatical issues that need to be resolved to improve readability and understanding. 
The authors' approach to addressing the problem of large vocabularies is novel and has the potential to be useful in a variety of applications. The use of hash embeddings could be particularly useful in online learning scenarios where a dictionary cannot be constructed before training. The paper also suggests that hash embeddings have an inherent regularizing effect on performance, which could be beneficial in preventing overfitting.
In terms of quality, the paper is technically sound, and the claims are well-supported by experimental results. The paper is clearly written, although there are some areas where the language could be improved. The approach is original and differs from previous contributions in the field. The related work is adequately referenced, and the paper provides a good overview of the current state of the art in word embeddings.
Overall, the paper is a good scientific contribution to the field, and the approach has the potential to be useful in a variety of applications. With some revisions to address the limitations and improve readability, the paper could be even stronger. 
Arguments pro acceptance:
- Novel approach to representing words in a continuous vector form
- Well-motivated and draws parallels to findings in neuroscience
- Good ablation studies and experiments
- Potential to be useful in a variety of applications, particularly online learning scenarios
- Inherent regularizing effect on performance
Arguments con acceptance:
- Limited to a specific architecture
- Typos and grammatical issues need to be resolved
- Unclear how well the approach will generalize to other networks
- Could benefit from additional experiments and analysis to fully demonstrate the effectiveness of the approach.