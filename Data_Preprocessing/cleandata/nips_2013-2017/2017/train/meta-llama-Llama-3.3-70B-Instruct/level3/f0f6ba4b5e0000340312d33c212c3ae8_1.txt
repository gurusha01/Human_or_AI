This paper presents a novel approach to word embeddings, called hash embeddings, which combines the benefits of standard word embeddings and feature hashing. The authors propose a method that uses a weighted combination of vectors to represent each word, allowing for efficient handling of large vocabularies and reducing the number of embedding parameters. The approach is shown to be effective in various text classification tasks, achieving performance comparable to or better than standard embeddings.
The paper is well-written, and the authors provide a clear explanation of the methodology and its advantages. The use of hash embeddings is demonstrated to be particularly useful for problems with large vocabularies, such as online learning, where creating a dictionary beforehand is not feasible. The authors also show that hash embeddings can be used with or without a dictionary, making them a versatile tool for various applications.
The experimental results are impressive, with hash embeddings achieving state-of-the-art performance in several datasets. The authors also demonstrate the effectiveness of using an ensemble of hash embeddings, which can further improve performance. The paper is well-organized, and the authors provide a thorough discussion of related work and the advantages of their approach.
However, there are some areas that could be improved. For example, tables 2 and 3 could be enhanced by adding vocabulary sizes, parameter reduction, and a joint state-of-the-art model for the DBPedia dataset. Additionally, the claim that the ensemble trains in the same time as a single large model is questionable, as the number of non-embedding weights in each network is the same, which could increase training time.
To further improve the paper, I recommend splitting table 3 to compare embedding-only approaches with RNN/CNN approaches and using these embeddings in more context-sensitive models. Minor comments include correcting typos, improving sentence structures, and providing clarification on specific points, such as the meaning of "patience" and the calculation of importance weights in table 4.
Overall, the paper presents a significant contribution to the field of natural language processing, and the proposed hash embeddings approach has the potential to be widely adopted. With some minor revisions to address the suggested improvements, the paper could be even stronger. 
Arguments pro acceptance:
- The paper presents a novel and effective approach to word embeddings.
- The authors provide a clear explanation of the methodology and its advantages.
- The experimental results are impressive, with hash embeddings achieving state-of-the-art performance in several datasets.
- The paper is well-organized, and the authors provide a thorough discussion of related work and the advantages of their approach.
Arguments con acceptance:
- The claim that the ensemble trains in the same time as a single large model is questionable.
- Some tables could be enhanced with additional information, such as vocabulary sizes and parameter reduction.
- Minor revisions are needed to correct typos, improve sentence structures, and provide clarification on specific points.