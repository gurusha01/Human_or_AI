The paper presents a novel model-based approach for non-parametric Conditional Independence (CI) testing, leveraging nearest neighbor bootstrap and binary classification. By reframing the CI testing problem as a classification task, the authors utilize powerful classifiers such as gradient-boosted trees and deep neural networks, enabling the method to handle high-dimensional data effectively. The key innovation lies in the nearest neighbor bootstrap procedure, which generates samples approximating the conditionally independent distribution \( f^{CI} \). Theoretical contributions include bounds on the closeness of the bootstrapped distribution to \( f^{CI} \) and generalization error bounds for classification under near-independent samples. Empirical results demonstrate the method's superiority over state-of-the-art kernel-based CI tests in both synthetic and real-world datasets.
Strengths:
1. Novelty and Theoretical Contributions: The paper introduces a fresh perspective by reducing CI testing to a classification problem, which is a significant departure from traditional kernel-based methods. The theoretical guarantees on the bootstrap procedure and classification generalization bounds under non-i.i.d. settings are rigorous and valuable.
2. Empirical Performance: The proposed method outperforms existing approaches (KCIT, RCIT) in both synthetic experiments and real-world applications, particularly in high-dimensional settings, which is a critical advantage.
3. Clarity: The paper is well-written and clearly explains the methodology, theoretical results, and experimental setup. The modular nature of the approach is intuitive and allows for flexibility in classifier choice.
Weaknesses:
1. Component Substitution: The method's modularity raises the question of whether existing techniques from the literature could replace one or both components (nearest neighbor bootstrap and classification) for potentially better performance. This possibility is not explored in the paper.
2. Handling Weak Dependencies: The method's robustness to weak dependencies between \( x \) and \( y \) given \( z \) in finite sample settings is not thoroughly evaluated. This is a critical limitation, as weak dependencies are common in real-world datasets.
3. Symmetry in Causal Scenarios: The approach assumes symmetry between \( x \) and \( y \), which may not align with causal inference scenarios where one variable causes the other. This limitation could restrict the method's applicability in causal discovery tasks.
4. Parameter Sensitivity: The choice of the parameter \( \tau \) in Algorithms 2 and 3 is not well-justified, and its impact on performance is unclear. A sensitivity analysis would strengthen the paper.
5. Minor Issue: The empirical risk in Algorithms 2 and 3 should be normalized by the sample size for consistency.
Recommendation:
The paper makes a strong contribution to the field of CI testing by introducing a novel classification-based framework and providing solid theoretical and empirical support. However, the concerns regarding component substitution, weak dependency handling, symmetry in causal scenarios, and parameter sensitivity need to be addressed to enhance the method's robustness and applicability. I recommend acceptance, provided the authors address these issues in the final version.
Arguments for Acceptance:
- Novel and theoretically grounded approach to CI testing.
- Demonstrated empirical superiority over state-of-the-art methods.
- Clear and well-organized presentation.
Arguments Against Acceptance:
- Lack of exploration of alternative components for the method.
- Unclear handling of weak dependencies and causal asymmetry.
- Insufficient justification for parameter choices.
Overall, the paper is a valuable contribution to the field and warrants acceptance with revisions.