Review of the Paper
This paper proposes an accelerated stochastic greedy coordinate descent (ASGCD) algorithm that combines Nesterov's acceleration and stochastic optimization techniques to improve the performance of greedy coordinate descent (GCD) for high-dimensional sparse and dense optimization problems. The authors introduce a novel rule based on an $l_1$-norm square approximation, which is solved using the proposed SOft ThreshOlding PrOjection (SOTOPO) algorithm. ASGCD achieves a convergence rate of $O(\sqrt{1/\epsilon})$, with reduced complexity compared to vanilla GCD, and demonstrates superior performance in certain cases compared to state-of-the-art methods like Katyusha and AFG.
Strengths:
1. Originality and Contribution: The paper introduces a novel $l_1$-norm square approximation rule and the SOTOPO algorithm, which efficiently identifies optimal sub-gradient directions without requiring full gradient computation. The integration of Nesterov's acceleration and stochastic optimization into GCD is a thoughtful and rigorous contribution, addressing a known bottleneck in GCD.
2. Theoretical Soundness: The authors provide detailed proofs for convergence and complexity results, demonstrating the theoretical validity of their claims. The use of $l_1$-norm-based guarantees is particularly relevant for sparse optimization problems.
3. Empirical Validation: The experimental results validate the theoretical claims, showing that ASGCD outperforms existing methods in specific scenarios, particularly for high-dimensional, dense problems with sparse solutions.
4. Significance: The proposed method addresses a challenging problem in large-scale optimization and has the potential to impact both theoretical research and practical applications in machine learning.
Weaknesses:
1. Clarity and Presentation: While the paper is mathematically rigorous, it suffers from organizational issues. The lack of high-level overviews and visual aids (e.g., diagrams or pseudo-code) makes it difficult for readers to follow the algorithm's flow. Notation inconsistencies (e.g., unclear $g$ in Equation 3) and typos in Algorithm 1 further detract from readability.
2. Implementation Complexity: The SOTOPO algorithm, while theoretically efficient, is complex to implement. This may limit its adoption by practitioners.
3. Regularization Parameter Selection: The paper does not provide theoretical guidance on selecting the regularization parameter $\lambda$, which is crucial for practical applications.
4. Empirical Limitations: The experiments lack exploration of the relationship between batch size, regularization, and performance. Additionally, ASGCD underperforms for small regularization parameters and batch sizes compared to Katyusha, which raises questions about its robustness across different scenarios.
Pro and Con Arguments for Acceptance:
Pros:
- Novel combination of techniques with rigorous theoretical backing.
- Significant improvement in complexity and convergence for specific problem settings.
- Empirical results validate the theoretical claims in relevant cases.
Cons:
- Clarity and presentation issues hinder accessibility.
- Complexity of implementation may limit practical impact.
- Lack of theoretical guidance on parameter selection and limited exploration of empirical trade-offs.
Recommendation:
This paper makes a meaningful contribution to the field of optimization by advancing GCD with acceleration and stochastic techniques. However, the clarity and implementation challenges, along with some gaps in empirical exploration, suggest room for improvement. I recommend acceptance with minor revisions, focusing on improving clarity, addressing notation issues, and providing more practical insights into parameter selection and empirical trade-offs.