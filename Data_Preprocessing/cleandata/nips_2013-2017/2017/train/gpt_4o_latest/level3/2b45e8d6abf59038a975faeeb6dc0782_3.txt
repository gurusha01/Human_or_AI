The paper introduces Population Matching Discrepancy (PMD), a novel population-based divergence that serves as a consistent estimator of the first Wasserstein distance. PMD is conceptually simple, differentiable, and designed to address limitations of Maximum Mean Discrepancy (MMD), the current leading method for sample-based distribution comparison. Specifically, PMD avoids MMD's reliance on kernel bandwidth hyper-parameters, provides stronger gradients, and supports smaller mini-batch sizes during training. The authors also propose a quadratic complexity approximation for PMD to mitigate its cubic complexity in exact computation, demonstrating minimal performance degradation. The method is evaluated on domain adaptation and generative modeling tasks, where it consistently outperforms MMD in both performance and convergence speed.
Strengths
1. Technical Soundness: The theoretical results are well-founded, with PMD shown to be a strongly consistent estimator of the Wasserstein metric. The paper also provides a clear connection between PMD and MMD, highlighting its advantages.
2. Practical Contributions: PMD addresses key limitations of MMD, such as sensitivity to hyper-parameters and weak gradients. The proposed quadratic approximation makes PMD computationally feasible for larger datasets.
3. Experimental Validation: The experiments are thorough, covering both domain adaptation and generative modeling tasks. PMD demonstrates superior performance and faster convergence compared to MMD, validating its practical utility.
4. Clarity and Structure: The paper is well-written and organized, with clear explanations of the methodology, theoretical results, and experimental setup. Prior work is appropriately cited, situating PMD within the context of existing research.
5. Significance: The work addresses an important problem in machine learning—estimating distributional distances—and provides a method that could be widely adopted in tasks such as domain adaptation and generative modeling.
Weaknesses
1. Cubic Complexity: While the authors propose a quadratic approximation, the exact computation of PMD remains cubic, which may limit its scalability for very large datasets.
2. Limited Evaluation: The experiments focus on standard tasks and datasets, but additional evaluations on more challenging settings, such as multi-modal generative modeling or scenarios with smaller sample sizes relative to the number of modes, would strengthen the paper.
3. Blurry Generative Outputs: The generated images in the experiments, particularly for natural image datasets, appear blurry. This suggests that the L1 distance used in PMD may not be optimal for such tasks, and further exploration of alternative distance metrics could improve results.
Recommendation
Pros:
- Strong theoretical foundation and practical contributions.
- Clear advantages over MMD in terms of gradient strength, hyper-parameter tuning, and mini-batch size.
- Thorough experimental validation demonstrating superior performance.
Cons:
- Scalability concerns due to cubic complexity in exact computation.
- Limited exploration of PMD's performance in more challenging or diverse settings.
Overall, the paper presents a significant contribution to the field of machine learning. PMD is a promising method that advances the state of the art in distribution comparison and has the potential for broad applicability. I recommend acceptance, with a suggestion to explore PMD's performance in multi-modal generative modeling and under more constrained sample size conditions in future work.