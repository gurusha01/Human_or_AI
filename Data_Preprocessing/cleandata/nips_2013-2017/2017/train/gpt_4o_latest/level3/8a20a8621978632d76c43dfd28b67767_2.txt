This paper introduces SHAP (SHapley Additive exPlanations), a unified framework for interpreting model predictions under the additive feature importance paradigm. The authors propose a novel theoretical foundation that unifies six existing methods, demonstrating that SHAP values uniquely satisfy three desirable properties: local accuracy, missingness, and consistency. The paper also introduces efficient approximation methods, including Kernel SHAP and Deep SHAP, to compute SHAP values for complex models. The work addresses a critical and timely problem in machine learning, where the trade-off between model accuracy and interpretability has become increasingly pronounced. By leveraging Shapley values from cooperative game theory, the authors provide a principled approach to feature attribution that is both theoretically grounded and practically applicable.
Strengths:
1. Novelty and Unification: The paper makes a significant contribution by unifying six existing methods under a single framework. The use of Shapley values to derive a unique solution with desirable properties is both innovative and impactful.
2. Clarity and Organization: The paper is well-written and logically structured, making it accessible to readers with varying levels of expertise. The theoretical results are clearly presented, and the connection between SHAP and existing methods is well-articulated.
3. Practical Relevance: The proposed methods, particularly Kernel SHAP and Deep SHAP, address computational challenges, making SHAP values more practical for real-world applications. The user studies and experiments further validate the utility of SHAP in aligning with human intuition.
4. Significance: The work addresses a critical problem in explainable AI and has the potential to influence both research and practice. The unified framework and efficient algorithms are likely to be widely adopted.
Weaknesses:
1. Local Accuracy Criterion: While the local accuracy property is theoretically appealing, it may be overly restrictive in some practical scenarios. The authors could discuss potential trade-offs or relaxations of this criterion.
2. Sanity Check Issue: Corollary 1 suggests that \(E[x_j]\) is nonzero, which raises concerns when the original model is interpretable. This point requires further clarification to ensure consistency across different model types.
3. Computational Cost: While the authors propose efficient methods, more details on the computational cost of Deep SHAP, particularly for large-scale models, would enhance the paper's practical relevance.
4. Limited Discussion of Assumptions: The reliance on assumptions like feature independence and model linearity for certain approximations could limit the generalizability of the results. A more detailed discussion of these assumptions and their implications would strengthen the paper.
Recommendation:
This paper is a strong candidate for acceptance due to its theoretical rigor, practical relevance, and clarity. However, addressing the concerns regarding the local accuracy criterion, computational cost, and assumptions would further enhance its impact. The work represents a significant step forward in explainable AI and is likely to inspire future research in this area.
Arguments for Acceptance:
- The paper introduces a novel and theoretically sound framework.
- It addresses a critical problem with significant practical implications.
- The experiments and user studies provide strong empirical validation.
Arguments Against Acceptance:
- The local accuracy criterion may be overly restrictive.
- Computational cost details for Deep SHAP are insufficient.
- Certain assumptions may limit generalizability.
Overall, the paper is a valuable contribution to the field and merits inclusion in the conference proceedings.