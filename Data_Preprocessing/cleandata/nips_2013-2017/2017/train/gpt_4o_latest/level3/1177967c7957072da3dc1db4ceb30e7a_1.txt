The paper introduces a novel "safe" adaptive importance sampling strategy for coordinate descent (CD) and stochastic gradient descent (SGD) methods, leveraging gradient bounds to efficiently compute sampling distributions. The proposed method addresses a significant challenge in adaptive importance sampling: balancing computational feasibility with theoretical guarantees. By formulating the sampling problem as a convex optimization task, the authors present an efficient algorithm that ensures the sampling distribution is provably better than uniform and fixed importance sampling, while incurring negligible computational overhead.
Strengths:
1. Novelty and Contribution: The paper makes a valuable contribution to importance sampling techniques by introducing a theoretically sound and computationally efficient adaptive sampling strategy. The convex optimization formulation for selecting the optimal sampling distribution is innovative and well-justified.
2. Theoretical Guarantees: The proposed method is rigorously analyzed, with proofs demonstrating its superiority over fixed importance sampling. The authors also provide guarantees that the method is always better than static sampling, a significant improvement over prior work.
3. Practicality: The algorithm is computationally efficient, with an overhead of \(O(n \log n)\) per iteration, making it feasible for large-scale applications. The integration into existing CD and SGD frameworks is straightforward.
4. Clarity and Writing: The paper is well-written, with clear explanations of the methodology, theoretical results, and experimental setup. The authors provide sufficient detail for reproducibility.
5. Empirical Validation: The numerical experiments demonstrate the effectiveness of the proposed method, showing significant speed-ups in CD and moderate improvements in SGD. The results align well with the theoretical predictions.
Weaknesses:
1. Proof Error: There is a missing factor of 2 in the proof of Lemma 2.1, which should be corrected for consistency and accuracy.
2. Unclear Example: Example 3.1 lacks clarity in illustrating the suboptimality of na√Øve approaches compared to uniform sampling. A more detailed explanation or additional context would strengthen the argument.
3. Algorithmic Issues: Algorithm 4 has potential issues:
   - Line 4's condition may never be satisfied, which could lead to inefficiencies or incorrect behavior.
   - Line 7 appears to have a typo, which should be clarified.
4. Experimental Limitations: 
   - The numerical results lack comparisons to other adaptive sampling schemes and fixed importance sampling with non-uniform distributions, which would provide a more comprehensive evaluation.
   - Timing results for SGD are missing, leaving an incomplete picture of the method's practical performance in this setting.
5. References: Reference [14] has an incorrect title, and an important related work ("Importance Sampling for Minibatches," Csiba and Richtarik, 2016) is missing from the citations.
Recommendation:
Despite the noted weaknesses, the paper makes a significant contribution to the field of adaptive importance sampling. The proposed method is theoretically sound, computationally efficient, and practically relevant. Post-rebuttal, the authors have clarified several concerns, and the remaining issues are minor and addressable. Therefore, I recommend the paper for acceptance, contingent on the authors addressing the proof error, algorithmic issues, and experimental gaps in the final version.
Arguments for Acceptance:
- Novel and theoretically grounded approach to adaptive importance sampling.
- Efficient algorithm with negligible computational overhead.
- Strong empirical results demonstrating practical utility.
Arguments Against Acceptance:
- Minor theoretical and algorithmic inconsistencies.
- Limited experimental comparisons and missing timing results for SGD.
Overall, the paper advances the state of the art in importance sampling and is a valuable contribution to the conference.