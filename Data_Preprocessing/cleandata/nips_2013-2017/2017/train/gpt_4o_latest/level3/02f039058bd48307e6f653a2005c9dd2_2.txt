Review
This paper addresses the problem of non-parametric Conditional Independence (CI) testing for continuous random variables by reframing it as a binary classification problem. The authors propose a novel nearest-neighbor bootstrap procedure to generate samples that approximate the conditionally independent distribution \( f_{CI} \), enabling the use of powerful classifiers such as gradient-boosted trees and deep neural networks. The paper also provides theoretical guarantees on the bootstrap procedure and generalization bounds for classification under near-independent samples. Empirical results demonstrate the superiority of the proposed method over existing kernel-based CI testing approaches, particularly in high-dimensional settings.
Strengths:
1. Novel Assumption and Theoretical Insights: The paper introduces a novel smoothness assumption on the conditional density, which is used to bound the total variation distance between the null hypothesis density and the bootstrap density. This assumption is also leveraged to bound the error of the optimal classifier on the training set. Theorem 1, which integrates multiple assumptions (smoothness, classifier error, and error difference), is a significant theoretical contribution and provides non-trivial insights into the problem.
   
2. Reduction to Classification: The reduction of CI testing to a binary classification problem is both elegant and practical. This approach allows the use of state-of-the-art classifiers, making the method scalable and adaptable to high-dimensional data.
3. Empirical Validation: The empirical results are compelling, demonstrating that the proposed method outperforms existing kernel-based methods (KCIT, RCIT) in both synthetic and real-world datasets. The robustness of the method in high-dimensional settings is particularly noteworthy.
4. Generalization Bounds: The analysis of Rademacher complexity and generalization bounds under near-independent samples is novel and of independent interest, extending the scope of learning theory to non-i.i.d. settings.
Weaknesses:
1. Clarity and Accessibility: The paper is difficult to read and lacks sufficient interpretation of its results. For instance, the intuition behind key inequalities, such as those in Section 1.1(iii), is not adequately explained, particularly in scenarios where the classifier's expressiveness is limited. This makes it challenging for readers unfamiliar with the topic to fully grasp the contributions.
2. Limited Intuition for Theoretical Results: While the theoretical results are rigorous, they are presented in a dense and technical manner. For example, the implications of Theorem 1 and the smoothness assumptions could benefit from more intuitive explanations or visualizations.
3. Practical Considerations: Although the proposed method demonstrates strong empirical performance, the computational cost of the nearest-neighbor bootstrap procedure and the reliance on powerful classifiers are not discussed in detail. This could be a limitation in resource-constrained environments.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution by introducing a novel assumption and providing rigorous guarantees for CI testing under non-i.i.d. settings.
- The reduction of CI testing to binary classification is innovative and enables the use of modern supervised learning tools.
- Empirical results convincingly demonstrate the method's superiority over existing approaches, particularly in high-dimensional scenarios.
Arguments Against Acceptance:
- The paper's clarity and accessibility are lacking, which may hinder its impact and reproducibility.
- Practical considerations, such as computational complexity and sensitivity to classifier choice, are not thoroughly addressed.
Recommendation:
Overall, this paper represents a strong contribution to the field of CI testing and statistical learning. While the clarity of presentation could be improved, the novelty and significance of the theoretical and empirical contributions outweigh the weaknesses. I recommend acceptance, provided the authors address the clarity issues and offer more intuitive explanations for their results.