The paper introduces phase discrepancy (PhD) and phase features as novel tools for encoding invariance to additive symmetric positive definite (SPD) noise in two-sample testing and learning on distributions. The authors argue that traditional methods like Maximum Mean Discrepancy (MMD) are sensitive to irrelevant sources of variability, such as noise, and propose PhD as a shift-invariant alternative. The PhD is mathematically similar to MMD but focuses on phase functions, which are invariant to SPD noise. This allows the derivation of phase features using random Fourier features, enabling robust learning and hypothesis testing.
The paper is technically sound and provides strong experimental evidence for the effectiveness of the proposed methods. The authors demonstrate that the PhD test and Symmetric Mean Embedding (SME) test outperform traditional methods in scenarios with additive SPD noise, as seen in synthetic and real-world datasets like the Higgs and Aerosol datasets. The robustness of phase features to noise is further validated in learning tasks, where they show superior performance under covariate shift compared to traditional Fourier features.
Strengths:
1. Novelty and Relevance: The introduction of phase discrepancy and phase features addresses a critical gap in nonparametric testing and learning by explicitly encoding invariance to SPD noise. This is a significant contribution to the field.
2. Theoretical Rigor: The mathematical formulation of PhD and its connection to RKHS properties is well-founded, with clear derivations and proofs provided in the appendices.
3. Experimental Validation: The experiments are comprehensive, spanning synthetic and real-world datasets, and convincingly demonstrate the advantages of the proposed methods.
4. Practical Impact: The methods are shown to be robust to noise, making them highly applicable in real-world scenarios where data collection processes often introduce variability.
Weaknesses:
1. Subset of Indecomposable Measures: The paper assumes a subset of indecomposable probability measures for PhD but provides limited justification for this restriction. The implications of this assumption on the generalizability of the method are unclear.
2. RKHS Connection: While the authors mention the similarity of PhD to MMD, a deeper exploration of its relationship to RKHS properties would strengthen the theoretical foundation and help situate the work within the broader kernel methods literature.
3. Type I Error Inflation: The PhD test exhibits inflated Type I error rates under high noise levels, which could limit its reliability in certain scenarios.
Arguments for Acceptance:
- The paper addresses an important and underexplored problem in nonparametric testing and learning.
- The methods are novel, theoretically grounded, and experimentally validated.
- The proposed techniques have practical implications for noise-robust machine learning.
Arguments Against Acceptance:
- The justification for the subset of indecomposable measures is insufficient.
- The inflated Type I error rates for the PhD test under high noise levels raise concerns about its robustness.
Recommendation:
Overall, this paper makes a significant contribution to the field by introducing methods that are both theoretically innovative and practically impactful. While there are some areas for improvement, particularly in theoretical justification and robustness under extreme noise conditions, the strengths outweigh the weaknesses. I recommend acceptance with minor revisions to address the concerns raised.