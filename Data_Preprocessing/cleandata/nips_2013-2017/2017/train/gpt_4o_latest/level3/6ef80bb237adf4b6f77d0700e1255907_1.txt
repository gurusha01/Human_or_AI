This paper presents a significant theoretical advancement by extending Nesterov's accelerated gradient method from Euclidean spaces to Riemannian manifolds, addressing geodesically convex optimization problems. The authors propose two novel algorithms tailored for geodesically strongly convex and general geodesically convex problems, achieving convergence rates of \(O((1-\sqrt{\mu/L})^k)\) and \(O(1/k^2)\), respectively. These rates match the optimal convergence rates of Nesterov's method in Euclidean spaces, marking a substantial improvement over prior Riemannian gradient descent methods. Additionally, the paper applies the proposed framework to the matrix Karcher mean problem, demonstrating both theoretical and empirical efficacy.
Strengths:
1. Theoretical Contribution: The paper tackles a long-standing open problem by generalizing Nesterov's method to nonlinear Riemannian spaces. The derivation of two nonlinear operators to replace the linear extrapolation step is particularly innovative and addresses the geometric challenges inherent to Riemannian spaces.
2. Convergence Analysis: The authors rigorously analyze the global convergence properties of their methods, providing clear theoretical guarantees. The improvement in convergence rates over existing methods (e.g., Riemannian gradient descent) is well-documented and significant.
3. Application to Karcher Mean: The specific iterative scheme for the matrix Karcher mean problem is a compelling application. The experimental results validate the theoretical claims, showing faster convergence compared to state-of-the-art methods like RGD and LRBFGS.
4. Clarity and Organization: The paper is well-structured, with clear explanations of the mathematical framework, algorithms, and experimental setup. The inclusion of geometric interpretations and detailed proofs in supplementary materials enhances its accessibility to readers familiar with Riemannian optimization.
Weaknesses:
1. Practical Impact: While the theoretical contributions are robust, the practical impact of the proposed methods could be better contextualized. For instance, the paper could discuss more real-world applications beyond the matrix Karcher mean problem.
2. Computational Complexity: The paper does not explicitly compare the computational overhead of the proposed methods to existing approaches. Although the experiments show faster convergence in terms of runtime, a deeper analysis of computational costs (e.g., per iteration complexity) would strengthen the results.
3. Experimental Scope: The experiments are limited to synthetic data for the Karcher mean problem. Testing the methods on a broader range of geodesically convex problems or real-world datasets would enhance the empirical validation.
Pro and Con Arguments for Acceptance:
Pros:
- Significant theoretical advancement in Riemannian optimization.
- Rigorous convergence analysis with improved rates.
- Clear and well-organized presentation.
- Promising experimental results on a challenging problem.
Cons:
- Limited discussion of broader practical applications.
- Lack of detailed computational complexity analysis.
- Narrow experimental scope.
Recommendation:
This paper represents a high-quality contribution to the field of optimization on Riemannian manifolds. Its theoretical insights and methodological innovations are likely to inspire further research and applications in this area. While there are some limitations in practical scope and experimental diversity, these do not detract significantly from the paper's overall merit. I recommend acceptance, with minor revisions to address the computational complexity discussion and expand on potential applications.