Review
This paper explores deterministic quadrature rules for kernel approximation as an alternative to the widely used random Fourier features. The authors propose several deterministic schemes, including polynomially-exact quadrature, dense grids, sparse grids, and reweighted grids, and demonstrate their applicability to sparse ANOVA kernels. The theoretical contributions claim better scaling with respect to the desired approximation error compared to random Fourier features, and the experimental results validate the approach on MNIST and TIMIT datasets. While the direction is promising, the paper has several limitations that need to be addressed for it to make a stronger contribution to the field.
Strengths:
1. Novelty: The paper addresses an important problem in kernel approximation by investigating deterministic alternatives to random Fourier features. The focus on sparse ANOVA kernels is particularly interesting, as these kernels have shown strong performance in structured data tasks.
2. Theoretical Contributions: The authors provide theoretical bounds (e.g., Corollaries 1-3, Theorem 4) on the sample complexity of deterministic quadrature rules, which are insightful for understanding their behavior in high-dimensional settings.
3. Experimental Validation: The experiments on MNIST and TIMIT datasets demonstrate that deterministic feature maps can achieve comparable or slightly better performance than random Fourier features in terms of classification accuracy and kernel approximation error.
4. Efficiency: The paper highlights the computational benefits of deterministic methods, such as faster feature generation, which could be advantageous in resource-constrained settings.
Weaknesses:
1. Restrictive Assumptions: The theoretical results assume sub-Gaussian kernel spectra, which exclude popular kernels like Matern. This significantly limits the general applicability of the proposed methods.
2. Lack of Comparisons: The experimental results do not include comparisons with quasi-Monte Carlo (QMC)-based methods, which are a natural baseline for deterministic kernel approximation. Additionally, the time complexity of the proposed methods is not thoroughly analyzed.
3. Theoretical Limitations: The exponential dependence on the domain diameter \( M \) in the theoretical bounds suggests that the proposed methods may perform worse than random Fourier features in large domains, which is not adequately discussed.
4. Experimental Clarity: The experimental settings are insufficiently detailed. For example, the classifiers used and hyperparameter tuning strategies for MNIST and TIMIT tasks are not described, making it difficult to reproduce the results.
5. Methodological Gaps: The subsampled dense grid and reweighted grid methods lack theoretical guarantees, which weakens the rigor of the proposed approaches. Their practical utility for guaranteed performance is unclear.
Arguments for Acceptance:
- The paper introduces a novel perspective on kernel approximation and provides theoretical insights that could inspire future research.
- The focus on sparse ANOVA kernels is timely and relevant, given their structural similarity to convolutional neural networks.
Arguments Against Acceptance:
- The restrictive assumptions and lack of comprehensive experimental comparisons limit the generalizability and impact of the work.
- Theoretical results are not uniformly favorable, as the exponential dependence on \( M \) raises concerns about scalability.
- The experimental section lacks sufficient detail and breadth, particularly in terms of baselines and computational analysis.
Recommendation: While the paper presents an interesting direction, the limitations in theoretical generality, experimental rigor, and methodological guarantees make it unsuitable for acceptance in its current form. A more comprehensive evaluation, including comparisons with QMC methods, detailed experimental setups, and broader kernel applicability, would significantly strengthen the paper.