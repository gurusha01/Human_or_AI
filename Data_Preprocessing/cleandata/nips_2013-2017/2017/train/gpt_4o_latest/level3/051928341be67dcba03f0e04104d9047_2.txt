Review of the Paper
Summary:
This paper presents a novel attention mechanism for Visual Question Answering (VQA) that models high-order correlations across multiple modalities (image, question, and answer). The proposed approach introduces a generic attention framework that incorporates unary, pairwise, and ternary potentials, enabling flexible and scalable attention modeling. The authors demonstrate the effectiveness of their method through extensive experiments, achieving state-of-the-art results on the VQA dataset. Notably, the inclusion of ternary potentials significantly improves performance, particularly in the 3-modality setup. The paper also provides qualitative visualizations of attention maps, showcasing their alignment with human attention.
Strengths:
1. Generic Attention Framework: The proposed method is versatile and can generalize to tasks involving multiple modalities. This is a notable contribution to the field, as many existing attention mechanisms are task-specific.
2. Performance Gains: The inclusion of ternary potentials leads to substantial improvements in the 3-modality setup, achieving state-of-the-art results on the VQA dataset while using fewer parameters compared to competing models.
3. Comprehensive Ablation Study: The ablation study effectively highlights the contribution of each component, particularly the importance of ternary potentials.
4. Clarity and Organization: The paper is well-written, with clear explanations of the proposed method, experimental setup, and results. The visualizations of attention maps are particularly helpful in understanding the model's behavior.
Weaknesses:
1. Limited Analysis of "Attention over Answers": While the paper models attention over answers, it lacks sufficient clarity and visualizations to justify its effectiveness, especially since answers are often single words.
2. 2-Modality Performance: The model shows limited gains in the 2-modality setup, which raises questions about its general applicability when fewer modalities are involved.
3. Comparison with Relevant Baselines: The paper does not compare its approach with existing models that also incorporate answers as inputs, such as "Revisiting Visual Question Answering Baselines." This omission makes it difficult to contextualize the improvements.
4. Failure Case Analysis: The paper does not adequately discuss failure cases, which could provide valuable insights for future research.
5. Minor Issues: There are minor inconsistencies in the reported evaluation details (e.g., validation vs. test sets) and some typographical errors.
Post-Rebuttal Comments:
The authors provided unsatisfactory responses regarding the limited gains in the 2-modality setup and the lack of comparisons with relevant baselines. However, the paper's contributions, particularly the generic attention framework and strong 3-modality results, justify its acceptance. The attention maps demonstrate better alignment with human attention compared to existing models, although exact comparisons with state-of-the-art methods remain unclear. Additionally, the potential inconsistency in reported performance metrics (e.g., MCB and MCT results) raises concerns about the reliability of the experimental results.
Recommendation:
Accept with Minor Revisions. The paper makes a significant contribution to multimodal attention modeling and achieves state-of-the-art results in VQA. However, the authors should address the issues related to "attention over answers," provide a more thorough comparison with relevant baselines, and clarify inconsistencies in the reported results. Despite these shortcomings, the proposed method's novelty and performance gains make it a valuable addition to the field.