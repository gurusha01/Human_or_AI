The paper presents a theoretical study of the stability of convolutional kernel networks (CKNs) under \(C^1\) diffeomorphisms, extending the understanding of deep signal representations. The authors demonstrate that CKNs, when designed with norm-preserving, non-expansive kernels and appropriate patch sizes, exhibit stability properties similar to the scattering transform, while preserving signal information. This stability is further analyzed in the context of locally compact groups, offering a broader perspective on invariance and robustness. The work bridges kernel methods and deep learning, providing insights into the geometry of functional spaces associated with convolutional architectures.
Strengths:
1. Theoretical Contribution: The paper makes a significant theoretical contribution by extending stability results to CKNs and connecting them to convolutional neural networks (CNNs). The analysis of stability under diffeomorphisms and the exploration of global invariance to group actions are particularly valuable.
2. Clarity and Organization: The manuscript is well-organized and clearly written, making complex mathematical concepts accessible. The combination of kernel methods and deep learning is presented in a coherent manner.
3. Relevance: The stability results have practical implications for robustness in machine learning, particularly in adversarial settings. The connection to CNNs and the RKHS framework is of interest to both theoretical and applied researchers.
4. Novelty: The extension of stability analysis from \((\mathbb{R}^d, +)\) to locally compact groups is a novel contribution, broadening the applicability of the results.
Weaknesses:
1. Motivation: The paper lacks sufficient motivation for the stability analysis. While robustness to adversarial examples is briefly mentioned, a more detailed discussion of practical implications would strengthen the paper.
2. Notation Overload: The overloading of the \(\kappa\) notation in (A3) is confusing, as it also denotes a function defining the kernel \(K\) in Eq. (10). This could be clarified to avoid ambiguity.
3. Redundancy: The second part of the displayed equation between lines 384-385 is redundant due to the symmetry of kernel \(k\). Removing this redundancy would improve clarity.
4. Missing Definitions: The definition of \(\phi\) is missing in Eq. (10), which could hinder understanding for readers unfamiliar with the context.
5. Technical Errors: The inequality in lines 427-428 appears to hold with equality, and the term \(| ||z|| - ||z|| |^2\) should be corrected to \(| ||z|| - ||z'|| |^2\).
6. References: Several references lack page information ([3,6,8,13,14,18,25-27,30,31]), and others ([9], [17], [19], [32]) require updates or corrections for citation accuracy.
Arguments for Acceptance:
- The paper provides a rigorous theoretical framework for understanding stability in CKNs and its implications for CNNs.
- The extension to locally compact groups and the connection to RKHS theory are novel and relevant to the machine learning community.
- The manuscript is well-written and organized, making it accessible to a broad audience.
Arguments Against Acceptance:
- The lack of sufficient motivation and practical context weakens the impact of the theoretical results.
- The technical errors and missing definitions detract from the paper's clarity and precision.
- The incomplete and inaccurate references reduce the overall quality of the work.
Recommendation:
Overall, the paper is a strong theoretical contribution that advances the understanding of stability in convolutional architectures. However, addressing the identified weaknesses, particularly the motivation and technical errors, would significantly enhance its impact. I recommend acceptance with minor revisions to address these issues.