The paper presents a cognitive model for predicting human question-asking behavior by treating questions as compositional programs evaluated over possible world states. The model incorporates factors like informativeness, complexity, answer type, and relevance, and is tested in the Battleship domain, where it predicts human questions about ambiguous game board configurations. The authors demonstrate that their full model outperforms lesioned versions in log-likelihood evaluations, though Expected Information Gain (EIG)-less models achieve similar correlations with human judgments. The work contributes to the understanding of human question generation and offers a promising compositional framework for future AI applications.
Strengths:
1. Technical Soundness: The paper is technically rigorous, presenting a well-defined probabilistic model and a clear optimization framework. The use of a grammar-based approach to represent questions as programs is innovative and aligns with prior work in computational cognitive science.
2. Reproducibility: The methodology is well-documented, with sufficient detail for reproduction. The inclusion of supplementary materials, such as grammar rules and question examples, enhances clarity.
3. Novelty: The compositional approach to question generation is a significant advancement over existing active learning and question-generation systems, which often rely on simpler or more constrained query types.
4. Future Potential: The model's ability to generate novel, human-like questions beyond the training set is a compelling feature, suggesting applications in both cognitive science and AI, such as adaptive tutoring systems or conversational agents.
Weaknesses:
1. Sparse and Noisy Human Data: While the model quantitatively outperforms alternatives, the human data is sparse, with many questions asked only once. This limits the robustness of the evaluation and weakens claims about how humans balance informativeness and complexity.
2. Overlooked Related Work: The paper fails to cite relevant prior studies, particularly Hawkins (2015) and Roberts (1996), which address similar themes around questions and information gain. This omission reduces the paper's situational awareness within the field.
3. Domain-Specific Limitations: While the grammar includes general-purpose constructs, some elements are tailored to the Battleship domain. The scalability of the approach to other domains remains unclear and requires further validation.
4. Unconvincing Cognitive Claims: The paper's cognitive science contributions are underdeveloped. The authors claim to model how humans balance informativeness and complexity, but the evidence is limited and does not convincingly support these claims.
Arguments for Acceptance:
- The paper introduces a novel and technically sound framework for modeling human question-asking behavior.
- The compositional approach has significant potential for advancing both cognitive science and AI applications.
- The methodology is clear and reproducible, with promising results in generating human-like questions.
Arguments Against Acceptance:
- The cognitive science claims are weak, relying on sparse human data and lacking robust evidence.
- The omission of key related work undermines the paper's originality and contextual grounding.
- The domain-specific nature of the model raises concerns about its generalizability.
Recommendation:
I recommend conditional acceptance. While the technical contributions and future potential are strong, the authors should address the overlooked related work and clarify the cognitive implications of their findings. Additionally, discussing how the model could generalize to other domains would strengthen its impact.