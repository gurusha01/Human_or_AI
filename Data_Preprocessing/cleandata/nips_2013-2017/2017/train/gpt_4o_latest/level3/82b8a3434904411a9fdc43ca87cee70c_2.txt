The paper introduces S-MIMO, a novel stochastic optimization algorithm designed to address the problem of minimizing a sum of smooth-convex functions with convex regularization under data perturbations. This is a significant contribution, as existing variance reduction techniques for stochastic gradient descent (SGD) fail to handle data perturbations effectively. The authors critique current SGD variance reduction methods for their reliance on storing full gradients or dual variables, which makes them unsuitable for inline training. S-MIMO overcomes these limitations while retaining the accelerated convergence properties of batch optimization methods like SCG.
Strengths:
1. Novelty and Motivation: The paper tackles a relevant and underexplored problem in stochastic optimization, particularly in scenarios involving data augmentation or perturbations. The motivation is well-grounded, with practical applications in machine learning tasks such as feature selection, privacy-aware learning, and data augmentation for generalization improvement.
2. Theoretical Contributions: The authors provide a rigorous theoretical analysis of S-MIMO, demonstrating its faster convergence rate compared to SGD. The analysis is logically consistent, and the introduction of a Lyapunov function to study convergence is a notable contribution.
3. Practical Relevance: The experiments span diverse scenarios, including image classification with data augmentation, Dropout regularization on gene expression data, and text classification. These demonstrate the algorithm's broad applicability and its superiority over SGD and N-SAGA in terms of suboptimality.
4. Clarity: The paper is well-organized, with clear explanations of the algorithm, its theoretical underpinnings, and its practical implications.
Weaknesses:
1. Comparative Analysis: While the paper compares S-MIMO with SGD and N-SAGA, it does not include other prominent SGD variants, such as SVRG with epoch-based strategies. This limits the scope of the evaluation and raises concerns about the fairness of the comparisons.
2. Incremental Contribution: The paper builds on existing variance reduction techniques, and while S-MIMO addresses specific limitations, the novelty may be perceived as incremental. The reviewer questions whether minor modifications to existing methods, such as the sketched SVRG approach, justify a standalone publication.
3. Memory Requirements: S-MIMO requires storing vectors proportional to the dataset size, which may not be feasible for large-scale datasets. While the authors acknowledge this limitation, alternative approaches to reduce memory overhead are not explored.
4. Empirical Validation: The experiments are thorough but could benefit from additional baselines and ablation studies to isolate the impact of specific components of S-MIMO.
Recommendations:
To strengthen the paper, the authors should:
1. Include comparisons with more SGD variants, even if trivial modifications are required.
2. Address the concern of incremental novelty by emphasizing the unique aspects of S-MIMO and its broader implications.
3. Explore strategies to reduce memory requirements, such as trading off storage for computation.
4. Incorporate an experiment using SVRG with an epoch-based strategy or batch processing to provide a more comprehensive evaluation.
Conclusion:
The paper makes a meaningful contribution to stochastic optimization under data perturbations, with strong theoretical and empirical support. However, the limited comparative analysis and concerns about incremental novelty warrant further investigation. Pending additional experiments and clarifications, I lean toward recommending acceptance, as the work addresses a practical and challenging problem with potential for significant impact.