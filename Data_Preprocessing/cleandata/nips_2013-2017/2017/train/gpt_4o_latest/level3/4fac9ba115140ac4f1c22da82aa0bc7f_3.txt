This paper introduces a novel algorithm for learning ensembles of deep decision or regression trees that balance prediction accuracy with feature cost, addressing a critical challenge in cost-sensitive machine learning. The proposed approach extends gradient tree boosting by incorporating a cost-sensitive objective function and a cost-aware impurity function, enabling the construction of deep trees that are computationally efficient on average. The algorithm is adaptive, dynamically acquiring different feature sets for different inputs to minimize cost while maintaining high predictive accuracy. Experimental results demonstrate that the method outperforms state-of-the-art algorithms such as GREEDYMISER and BUDGETPRUNE across several datasets, including Yahoo! LTR, MiniBooNE, and HEPMASS, under various cost settings. The paper is well-written, with clear derivations and a strong experimental section, and addresses a problem of significant relevance across domains.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with well-supported claims through both theoretical derivations and extensive experimental results. The cost-aware impurity function and iterative tree construction are novel and effectively address the problem of balancing accuracy and cost.
2. Clarity: The paper is well-organized and clearly written, with sufficient detail to allow reproducibility. The inclusion of source code further enhances its accessibility.
3. Originality: The approach is a novel extension of gradient boosting, introducing cost-sensitive optimization and deep tree construction that are not present in prior methods like GREEDYMISER or BUDGETPRUNE. The authors also provide a detailed comparison with related work, highlighting the unique contributions of their method.
4. Significance: The results are highly significant, demonstrating substantial improvements over existing methods in terms of both accuracy and cost-efficiency. The ability to construct deep trees with low average evaluation costs is a meaningful advancement for applications requiring cost-sensitive predictions.
Weaknesses:
1. Generality of Cost Settings: While the paper explores various cost settings, it assumes uniform feature costs in some datasets (e.g., MiniBooNE and HEPMASS), which may not reflect real-world scenarios. A discussion on how the method generalizes to non-uniform cost distributions would strengthen the work.
2. Scalability: Although the authors claim that the algorithm does not significantly slow down training, the computational overhead of best-first tree construction compared to traditional breadth-first methods could be discussed in more detail, particularly for very large datasets.
3. Broader Impact: The paper focuses primarily on technical and empirical contributions but could benefit from a discussion on the broader implications of cost-sensitive learning, such as ethical considerations in domains like healthcare or resource-constrained environments.
Arguments for Acceptance:
- The paper addresses an important and practical problem in machine learning with a novel and effective solution.
- The method is well-supported by both theoretical insights and empirical results, demonstrating clear improvements over existing approaches.
- The work is highly relevant to the NIPS community, given its focus on cost-sensitive learning and its potential applications across domains.
Arguments Against Acceptance:
- The paper could provide more discussion on the generalizability of the method to diverse cost settings and its scalability to extremely large datasets.
- Broader implications and potential limitations of the approach are not deeply explored.
Recommendation:
I recommend accepting this paper. Its contributions are significant, the methodology is sound and novel, and the results advance the state of the art in cost-sensitive learning. The minor weaknesses identified do not detract from the overall quality and impact of the work.