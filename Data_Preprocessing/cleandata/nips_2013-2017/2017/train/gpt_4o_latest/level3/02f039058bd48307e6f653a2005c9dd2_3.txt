The paper presents a novel approach to non-parametric Conditional Independence (CI) testing for continuous random variables by reframing the problem as a binary classification task. The authors propose a nearest-neighbor bootstrap method to generate samples approximating the conditional product distribution, leveraging total variation distance for theoretical guarantees. The predictive error of a trained binary classifier is then used to accept or reject the null hypothesis of conditional independence. The method is empirically validated on synthetic and real-world datasets, demonstrating superior performance compared to state-of-the-art kernel-based approaches like KCIT and RCIT, particularly in high-dimensional settings.
Strengths:
1. Innovative Approach: The reduction of CI testing to binary classification is a significant conceptual contribution, enabling the use of powerful supervised learning models such as gradient-boosted trees and deep neural networks.
2. Theoretical Guarantees: The paper provides rigorous theoretical results, including bounds on the total variation distance for the bootstrap samples and generalization guarantees for classification under near-independent samples.
3. Empirical Validation: Extensive experiments on synthetic and real-world datasets, including the flow cytometry dataset, show that the proposed method outperforms existing techniques, particularly in high-dimensional scenarios.
4. Practical Utility: The modularity of the approach allows practitioners to select classifiers based on domain knowledge, making the method adaptable to various applications in causal inference and Bayesian network discovery.
5. Clarity: The paper is well-written, with clear explanations of the methodology and its implications for practitioners.
Weaknesses:
1. Complex Theoretical Proofs: While the theoretical guarantees are impressive, some proofs are challenging to follow, potentially limiting accessibility for a broader audience.
2. Algorithm Redundancy: Algorithm 2 could be made more concise, as its description overlaps with the main text.
3. Limited Exploration of Alternatives: The paper does not explore a discrete analog of the method, which could broaden its applicability. Additionally, sensitivity to different classification models is not thoroughly analyzed.
4. Redundancy in Section 1.1: The contributions section could be streamlined to avoid repetition.
5. Minor Typos: Errors like "classifier" on line 157 and "graphs" on line 280 should be corrected.
Arguments for Acceptance:
- The paper addresses a critical problem in statistical testing with significant implications for causal inference.
- The proposed method is both innovative and practical, advancing the state of the art in CI testing.
- Theoretical guarantees and empirical results are robust and well-supported.
Arguments Against Acceptance:
- The complexity of theoretical proofs may hinder accessibility.
- The lack of exploration of discrete analogs and sensitivity analysis limits the scope of the contribution.
Suggestions for Improvement:
1. Explore the possibility of a discrete analog to extend the method's applicability.
2. Analyze the sensitivity of the method to different classification models and hyperparameters.
3. Streamline redundant sections, particularly in Section 1.1 and Algorithm 2.
4. Address minor typographical errors for clarity.
Conclusion:
The paper makes a significant contribution to the field of CI testing by introducing a novel, classifier-based approach with strong theoretical and empirical support. Despite minor weaknesses, the strengths of the work outweigh its limitations, and it is likely to have a meaningful impact on both research and practice. I recommend acceptance with minor revisions.