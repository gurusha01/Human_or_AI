The paper introduces hash embeddings, a novel approach to reduce the memory footprint of embedding parameters while maintaining or improving performance on text classification tasks. The method interpolates between standard word embeddings and embeddings created using the hashing trick. Each token is represented by multiple component vectors and a weight vector, which are combined to form the final embedding. By leveraging multiple hash functions and a shared pool of embedding vectors, the method reduces parameter size significantly compared to traditional embeddings. The authors demonstrate that hash embeddings outperform standard bag-of-words models on various classification tasks, with an inherent regularization effect and the ability to handle large vocabularies without pre-training dictionaries.
Strengths:
1. Innovation and Originality: The paper presents a novel extension to feature hashing by introducing trainable importance parameters and multiple hash functions. This approach effectively balances the trade-off between parameter reduction and model performance, distinguishing it from prior work.
2. Significant Parameter Reduction: The method achieves substantial memory savings, reducing parameters by several orders of magnitude compared to standard embeddings. This is particularly valuable for large-scale applications with massive vocabularies.
3. Performance: Empirical results across seven datasets show that hash embeddings consistently perform at least as well as standard embeddings and often better, particularly when used in ensemble models.
4. Flexibility: The method works both with and without dictionaries, making it suitable for tasks like online learning where vocabulary construction is impractical.
5. Regularization Effect: The inherent regularization provided by hash embeddings is a compelling feature, as it avoids adding unnecessary parameters while maintaining performance.
Weaknesses:
1. Inference Compute Time: While the paper claims negligible computational overhead during training, it does not provide a detailed analysis of inference compute time. A comparison of hashing costs versus embedding lookups on GPUs would strengthen the evaluation.
2. Clarity: While the methodology is well-explained, certain sections, such as the hashing mechanism and its theoretical guarantees, could benefit from additional clarity and examples to aid understanding for non-expert readers.
3. Limited Comparison to State-of-the-Art: Although hash embeddings outperform bag-of-words models, the paper does not extensively compare its approach to more recent contextual embedding methods (e.g., transformer-based models), which could provide a broader perspective on its significance.
4. Preliminary Future Work: The proposed extensions, such as using different hash functions for importance parameters or pre-training hash vectors, are intriguing but remain unexplored.
Pro and Con Arguments:
Pro: The method is simple, effective, and addresses a critical challenge in embedding large vocabularies. Its parameter efficiency and regularization properties make it a valuable contribution to the field.  
Con: The lack of detailed analysis of computational trade-offs during inference and limited comparison to modern embedding techniques slightly detract from its impact.
Recommendation:
The paper is a strong contribution to the field, addressing a practical and significant problem with a novel and effective solution. While some areas could be improved, the strengths outweigh the weaknesses. I recommend acceptance, with minor revisions to clarify inference costs and expand comparisons to state-of-the-art methods.