The paper introduces a novel greedy coordinate descent (GCD) algorithm that incorporates a new Gauss-Southwell coordinate selection rule based on minimizing an `1-norm squared convex problem. This is coupled with an efficient algorithm, SOTOPO, to solve the induced subproblem. Additionally, the authors integrate Nesterov's acceleration and stochastic optimization techniques into the GCD framework, resulting in the Accelerated Stochastic Greedy Coordinate Descent (ASGCD) algorithm. Theoretical analysis demonstrates that ASGCD achieves an optimal convergence rate of \(O(\sqrt{1/\epsilon})\), with reduced iteration complexity for greedy selection. Numerical experiments validate the algorithm's performance, particularly for high-dimensional, dense problems with sparse solutions.
Strengths:
1. Novelty and Practicality: The proposed Gauss-Southwell rule using an `1-norm squared approximation is a significant contribution. It addresses the computational bottleneck of traditional GCD methods by introducing a convex formulation that is efficiently solvable using the SOTOPO algorithm.
2. Acceleration and Stochastic Optimization: The integration of Nesterov's acceleration and stochastic optimization into GCD is innovative and aligns with current trends in optimization research. The resulting ASGCD algorithm is theoretically sound and achieves a convergence rate comparable to state-of-the-art methods like Katyusha.
3. Theoretical and Empirical Validation: The paper provides rigorous theoretical guarantees for the ASGCD algorithm and demonstrates its empirical effectiveness on real-world datasets. The experiments highlight the advantages of ASGCD, particularly in scenarios where `1-norm-based guarantees are more relevant than `2-norm-based ones.
Weaknesses:
1. Limited Experimental Scope: The numerical experiments are restricted to small-scale problems, which limits the generalizability of the results. Testing on larger-scale datasets would strengthen the empirical claims.
2. Clarity and Organization: While the technical content is solid, the paper suffers from clarity issues due to numerous typos and suboptimal English. This detracts from the readability and accessibility of the work.
3. Comparison with Related Work: Although the paper references state-of-the-art methods like Katyusha and AFG, the discussion of related work could be more comprehensive. For example, a deeper comparison with other greedy selection strategies or stochastic GCD methods would provide better context for the contributions.
4. Logarithmic Factor: The theoretical analysis includes a log(d) factor in the convergence bound, which is not fully justified. Further exploration of whether this factor is necessary would improve the theoretical contribution.
Recommendation:
Pros for Acceptance:
- The paper tackles a relevant and challenging problem in optimization, offering a novel and practical solution.
- The combination of theoretical rigor and empirical validation makes it a valuable contribution to the field.
- The topic is timely and likely to attract interest from the community.
Cons for Acceptance:
- The limited experimental scope and clarity issues reduce the overall impact of the work.
- The paper could benefit from a more thorough discussion of related work and additional large-scale experiments.
Final Decision:
I recommend acceptance with minor revisions. The paper presents a meaningful contribution to the field, but the authors should address the clarity issues, expand the experimental evaluation, and provide a more thorough discussion of related work to maximize its impact.