The paper presents a novel framework for analyzing \( l\infty \) and \( l2 \) convergence rates of generative and discriminative models in high-dimensional settings, introducing the concept of local separability for loss functions. This work builds on classical results comparing generative and discriminative models, extending them to exponential families and high-dimensional regimes. The authors provide a unified analysis of \( l_\infty \) convergence rates for general M-estimators, with applications to logistic regression and Gaussian generative models, and explore the nuanced behaviors of these models under different sparsity and separability conditions.
Strengths:
1. Novelty and Generalization: The introduction of local separability as a flexible notion for analyzing loss functions is a significant contribution. This generalization allows the derivation of \( l\infty \) rates for a broad class of models, addressing gaps in prior work that focused on \( l2 \) rates.
2. Theoretical Insights: The paper provides a rigorous theoretical framework, deriving convergence rates for both generative and discriminative models. The results highlight the impact of separability on sample complexity, offering nuanced insights into when each approach is preferable.
3. Practical Relevance: The analysis is instantiated for specific models (logistic regression, isotropic Gaussian, and Gaussian graphical models), making the results applicable to real-world problems. The use of soft-thresholding for generative models to improve \( l_2 \) rates is a practical and effective methodological contribution.
4. Experimental Validation: The experiments corroborate the theoretical findings, particularly the advantages of generative models in low-sparsity settings and their independence from differential sparsity in sample complexity.
Weaknesses:
1. Clarity Issues: The paper suffers from some clarity problems. The terminology inconsistency between "empirical loss function" and "empirical risk function" (line 127) is confusing. Additionally, the notation \( \succsim \) in Corollary 3 is not defined, and the condition \( n \gg \log p \) in Corollary 4 could benefit from further explanation.
2. Dependence on \( n \): The dependence of beta and gamma on the number of samples \( n \) (lines 181-182) is unclear. Similarly, the role of separability parameters changing with \( n \) is not well-articulated, leaving gaps in understanding the practical implications of the results.
3. Explicit \( l\infty \) Rates: While Theorems 1 and 2 are central to the paper, the explicit derivation of \( l\infty \) convergence rates is not immediately clear. The connection between the separability parameters and these rates could be better elucidated.
4. Generative vs. Discriminative Comparison: The paper notes that for isotropic Gaussian models, both approaches achieve the same convergence rate, but the generative approach has logarithmic dependence on dimension. However, the implications for logistic regression are less clear and could use additional discussion.
Recommendation:
The paper is a strong theoretical contribution to the field, offering novel insights into high-dimensional learning. However, the clarity issues and gaps in explanation slightly detract from its accessibility. I recommend acceptance with minor revisions, focusing on improving the clarity of the notation, addressing the dependence of separability parameters on \( n \), and explicitly connecting the theoretical results to practical implications.
Arguments for Acceptance:
- Novel and generalizable theoretical framework.
- Rigorous analysis with practical applications.
- Experimental results validate theoretical claims.
Arguments Against Acceptance:
- Clarity issues in terminology and notation.
- Some theoretical dependencies (e.g., on \( n \)) are insufficiently explained.
- Explicit derivation of \( l_\infty \) rates could be clearer.
In summary, this paper makes a valuable contribution to understanding the trade-offs between generative and discriminative models in high dimensions, but minor revisions are needed to enhance its clarity and accessibility.