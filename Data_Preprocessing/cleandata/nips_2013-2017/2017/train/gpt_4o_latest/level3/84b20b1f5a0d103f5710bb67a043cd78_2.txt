The paper presents a novel approach to solving the L1-regularized empirical risk minimization (ERM) problem by integrating techniques such as greedy coordinate descent (GCD), stochastic variance-reduced gradient (SVRG), and Nesterov's acceleration (Katyusha). The authors introduce an efficient subproblem solver, SOft ThreshOlding PrOjection (SOTOPO), and propose the Accelerated Stochastic Greedy Coordinate Descent (ASGCD) algorithm. This work is a significant contribution to the optimization literature, particularly for high-dimensional problems with sparse solutions.
Strengths:
1. Theoretical Contributions: The paper provides a rigorous theoretical analysis of the proposed method. It demonstrates that ASGCD achieves the optimal convergence rate \( O(\sqrt{1/\epsilon}) \) while reducing the iteration complexity of greedy selection by a factor of the sample size. The novel insight that GCD without L1 regularization can be interpreted as gradient descent with an L1 norm upper bound is particularly noteworthy.
2. Novel Subproblem Solver: The introduction of SOTOPO is a highlight. By reformulating the squared L1 norm problem, the authors achieve an efficient solution with complexity \( O(d + |Q| \log |Q|) \), which is an improvement over existing methods.
3. Practical Relevance: The proposed ASGCD algorithm is shown to be effective in both \( n \gg d \) and \( n \ll d \) regimes, making it versatile for various applications. The experiments demonstrate competitive performance compared to state-of-the-art methods like Katyusha and AFG.
4. Clarity of Experiments: The experimental results are well-presented, with clear comparisons to baseline methods. The use of both stochastic and deterministic settings adds depth to the evaluation.
Weaknesses:
1. Limited Experimental Scope: While the experiments are thorough, the inclusion of additional baseline methods, such as APPROX, would strengthen the empirical validation, especially in high-dimensional, low-sample-size scenarios.
2. Parameter Analysis: The effect of the parameter \( \eta \) on sparsity in subproblem (3) and its implications for L1 vs. L2 regularization are not discussed in detail. This could provide further insights into the method's behavior.
3. Clarity of Writing: While the paper is generally well-organized, there are minor grammatical and phrasing issues that could be improved for better readability. Additionally, some technical explanations, such as the derivation of the SOTOPO algorithm, are dense and may benefit from further simplification or illustrative examples.
Pro and Con Arguments for Acceptance:
Pros:
- Significant theoretical and practical contributions to L1-regularized optimization.
- Novel and efficient subproblem solver (SOTOPO).
- Competitive performance in experiments, with clear advantages in specific regimes.
Cons:
- Limited experimental comparison with certain baseline methods.
- Insufficient discussion on parameter sensitivity and sparsity implications.
- Minor clarity issues in technical sections.
Decision:
This paper is a strong contribution to the field of optimization and machine learning. Its combination of theoretical rigor, practical relevance, and novel algorithmic design makes it suitable for acceptance. However, addressing the suggested weaknesses, particularly the inclusion of additional baselines and parameter analysis, would further enhance its impact.