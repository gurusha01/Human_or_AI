Review of the Paper
This paper introduces a method for selective classification in deep neural networks (DNNs), allowing users to set a desired risk level and enabling the classifier to reject uncertain predictions to meet this risk threshold. The authors demonstrate the effectiveness of their approach on CIFAR-10, CIFAR-100, and ImageNet datasets, achieving impressive results in terms of guaranteed risk control and coverage. The work is motivated by mission-critical applications such as autonomous driving and medical diagnostics, where controlled error rates are essential.
Strengths:
1. Novelty and Relevance: The paper addresses an important and underexplored area of selective classification in the context of DNNs, which has significant implications for real-world applications. The proposed method is practical and well-motivated, making it relevant to both researchers and practitioners.
2. Empirical Validation: The experiments demonstrate the viability of the method across multiple datasets and architectures (e.g., VGG-16 and RESNET-50). The results show that the approach can achieve tight risk bounds with high coverage, outperforming existing baselines.
3. Clarity of Empirical Results: The risk-coverage curves and detailed tables provide a clear picture of the method's performance. The comparison between SR and MC-dropout confidence-rate functions is particularly insightful.
4. Potential Impact: The proposed method could be transformative for applications requiring high reliability, such as medical diagnostics and autonomous systems.
Weaknesses:
1. Unclear Task Definition: The paper does not clearly differentiate between the ultimate goal of selective classification and the simpler problem the algorithm actually solves. For example, while the method guarantees risk control for a given classifier, it does not address the joint optimization of the classifier and selection function, which is a more ambitious goal.
2. Algorithmic Limitations: Algorithm 1 may return trivial solutions when the desired risk \( r^ \) is set below a specific bound \( B^ \). This limitation is not adequately discussed, and the authors should clarify the conditions under which the algorithm is effective.
3. Lack of Baseline Comparison: The experiments do not include a comparison to a simple baseline, such as directly using empirical risk \( \hat{r}_i \). This omission makes it difficult to assess the practical advantages of the proposed method over simpler alternatives.
4. Numerical Challenges: The paper does not discuss the computational challenges associated with solving equation (4), particularly for large combinatorial coefficients. This omission could hinder reproducibility and practical adoption.
5. Clarity and Writing: While the paper is generally well-organized, there are minor typos (e.g., line 80 "(f,g)", line 116 "B^*(\hat{r},\delta,S_m)", and line 221 "mageNet") that should be corrected. Additionally, some technical details, such as the derivation of the confidence-rate functions, could be better explained.
Arguments for Acceptance:
- The paper addresses a critical problem in machine learning with significant practical implications.
- The proposed method is novel and demonstrates strong empirical performance.
- The work opens up new avenues for research in selective classification and risk control.
Arguments Against Acceptance:
- The task definition and algorithmic limitations are not sufficiently clarified.
- The lack of baseline comparisons and discussion of numerical challenges weakens the experimental and methodological rigor.
- Minor issues with clarity and presentation detract from the overall quality.
Recommendation:
While the paper has notable strengths, the weaknesses—particularly the unclear task definition, lack of baseline comparisons, and insufficient discussion of numerical challenges—must be addressed before acceptance. I recommend a weak reject at this stage, with encouragement to revise and resubmit after addressing these concerns.