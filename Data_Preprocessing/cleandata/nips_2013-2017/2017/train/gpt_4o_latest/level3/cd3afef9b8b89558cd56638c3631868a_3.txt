This paper investigates the problem of online isotonic regression, focusing on the random permutation model, and introduces algorithms with provable regret bounds. The authors address the limitations of prior work in the fixed design model by proposing a more practical setting where data arrives in a random order. The main contributions include a class of "forward algorithms" achieving \( \sqrt{T} \) regret, a reduction from the fixed design model to the random permutation model, and a discussion of computationally efficient approaches. The paper also provides theoretical lower bounds and explores extensions to different settings and loss functions.
The study builds on prior work in isotonic regression and online learning, such as the fixed design model analyzed in [14], and connects to broader themes in nonparametric regression and cross-validation. The authors' use of the leave-one-out loss as a proxy for regret is a novel and insightful approach, enabling a unified analysis of algorithms in the random permutation model. However, the paper could benefit from additional motivation for studying the random permutation model, particularly in terms of its practical relevance and how it compares to existing models like adversarial or fixed design. While the rebuttal clarified some of these points, further elaboration on the challenges specific to this model would strengthen the paper.
The role of Theorems 3.1 and 3.2, which establish the connection between fixed design and random permutation models, is somewhat unclear in the broader narrative. While these results are theoretically significant, their practical implications and computational feasibility, especially for the estimator in Section 3, warrant more discussion. Additionally, the paper could explore how variance decreases when sampling multiple data points or permutations, as this could provide further insights into the behavior of the proposed algorithms.
The paper is well-written and mathematically rigorous, though certain sections are overly dense with technical details. A more intuitive, less math-driven presentation in parts would improve accessibility for a broader audience. Despite some gaps, the paper makes meaningful contributions to the field, particularly in advancing the understanding of online isotonic regression under more realistic assumptions.
Strengths:
- Novel extension of isotonic regression to the random permutation model.
- Rigorous theoretical analysis with matching upper and lower bounds.
- Introduction of forward algorithms, which are computationally efficient and practical.
Weaknesses:
- Limited discussion of the practical motivation and challenges of the random permutation model.
- Computational feasibility of the estimator in Section 3 is not fully addressed.
- Some sections are overly technical and could benefit from clearer exposition.
Pro Acceptance:
- Advances the state of the art in online isotonic regression.
- Provides a strong theoretical foundation and explores multiple avenues.
Con Acceptance:
- Practical significance of the random permutation model is not fully justified.
- Certain results (e.g., Theorems 3.1 and 3.2) lack clear integration into the broader narrative.
Overall, this paper is a valuable contribution to the field of online learning and isotonic regression, and I recommend acceptance with minor revisions to improve clarity and practical motivation.