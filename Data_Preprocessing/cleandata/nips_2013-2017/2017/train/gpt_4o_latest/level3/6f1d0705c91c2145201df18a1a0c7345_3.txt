The paper introduces Hierarchical Implicit Models (HIMs), a novel class of Bayesian hierarchical models that combine implicit densities with hierarchical Bayesian modeling, and proposes a new inference algorithm, Likelihood-Free Variational Inference (LFVI). HIMs enable the modeling of complex latent structures in implicit models, while LFVI extends variational inference to implicit models by using implicit variational families and density ratio estimation. The paper evaluates the proposed methods on diverse tasks, including ecological simulations, Bayesian GANs for classification, and implicit models for sequence generation. While the work builds on prior research in approximate Bayesian computation (ABC), GANs, and expressive variational approximations, it aims to generalize these approaches to broader applications.
Strengths:  
1. Novelty: The paper presents a creative integration of hierarchical Bayesian modeling and implicit densities, which is a significant advancement over traditional GANs and ABC methods. The introduction of LFVI is particularly innovative, as it enables scalable inference for implicit models, addressing challenges in high-dimensional data and complex latent structures.  
2. Diverse Applications: The experiments demonstrate the versatility of the proposed methods, spanning ecological simulations, Bayesian GANs, and sequence generation. This breadth highlights the potential impact of HIMs and LFVI across multiple domains.  
3. Scalability: The use of stochastic optimization and implicit variational families makes LFVI scalable to large datasets, which is a notable improvement over traditional ABC methods.  
Weaknesses:  
1. Clarity: The relationship between LFVI and existing methods like Adversarial Variational Bayes (AVB) and Adversarially Learned Inference (ALI) is not sufficiently clarified. This omission may confuse readers unfamiliar with these prior works.  
2. High-Dimensional Analysis: The stability of LFVI is only demonstrated on a low-dimensional toy example, leaving its performance in high-dimensional settings unaddressed. This is a critical limitation, as many real-world applications involve high-dimensional data.  
3. Hinge Loss Rationale: The paper lacks an explanation for using hinge loss in the ratio estimation step, which raises questions about its theoretical justification and empirical advantages.  
4. Scalability for Discrete Data: While the paper claims that Bayesian GANs can handle discrete data, scalability concerns arise due to the high dimensionality of the input to the discriminator network. This issue is not adequately addressed.  
5. Evaluation Rigor: The absence of quantitative results for sequence generation weakens the evaluation of the proposed methods. A more thorough analysis would strengthen the paper's claims.  
Arguments for Acceptance:  
- The paper introduces a novel and flexible modeling framework with significant potential for advancing Bayesian analysis in implicit models.  
- The proposed LFVI algorithm is a meaningful contribution to scalable inference methods.  
- The diverse applications demonstrate the generality and utility of the approach.  
Arguments Against Acceptance:  
- The lack of high-dimensional analysis and quantitative results for sequence generation undermines the evaluation rigor.  
- Key theoretical and practical aspects, such as the hinge loss rationale and scalability for discrete data, are insufficiently addressed.  
- The relationship to prior methods like AVB and ALI is unclear, limiting the accessibility of the work to a broader audience.  
Conclusion:  
The paper makes a significant contribution to the field by proposing HIMs and LFVI, but it falls short in providing a comprehensive evaluation and addressing key theoretical concerns. I consider the paper borderline and recommend acceptance only if the authors address the issues related to high-dimensional stability, hinge loss justification, and evaluation rigor.