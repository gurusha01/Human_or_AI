This paper introduces Population Matching Discrepancy (PMD), a novel approach to estimating the distance between two distributions based on samples, as an alternative to the widely used Maximum Mean Discrepancy (MMD). PMD is framed as a sampled approximation of the Wasserstein metric, leveraging minimum weight matching to compute the discrepancy. The authors argue that PMD addresses key limitations of MMD, including weak gradients, sensitivity to kernel bandwidth hyperparameters, and the need for large mini-batch sizes. The paper demonstrates PMD's application in domain adaptation and generative modeling, showing empirical improvements over MMD in terms of performance and convergence speed.
Strengths:
1. Technical Novelty: PMD is a strongly consistent estimator of the first Wasserstein metric, offering a theoretically sound alternative to MMD. This connection to optimal transport theory is well-established and significant.
2. Practical Advantages: The paper highlights PMD's benefits, including stronger gradients and the ability to work with smaller mini-batch sizes, which are crucial for efficient training in deep learning tasks.
3. Empirical Validation: The experiments on domain adaptation and generative modeling tasks demonstrate PMD's superior performance and faster convergence compared to MMD. The results are compelling, particularly in the domain adaptation tasks where PMD achieves better or comparable accuracy.
4. Clarity of Presentation: The paper is well-organized, with clear explanations of PMD's formulation, theoretical properties, and practical implementation. The inclusion of pseudocode and detailed experimental setups enhances reproducibility.
Weaknesses:
1. Gradient Strength Validation: While the claim of stronger gradients is supported by empirical evidence, the analysis is limited to specific settings. A more comprehensive evaluation across different distance metrics (e.g., L2) and sigma values for MMD would strengthen the argument.
2. Hyperparameter Claim: The claim that PMD involves fewer hyperparameters is somewhat overstated. While PMD avoids kernel bandwidth tuning, its reliance on a distance metric introduces implicit hyperparameters, such as the choice of L1 or L2 distance.
3. Scalability in High Dimensions: PMD's dependence on reliable distance metrics may pose challenges in high-dimensional spaces, akin to MMD. This limitation is acknowledged but not thoroughly explored in the experiments.
4. Computational Complexity: The O(NÂ³) complexity of the Hungarian algorithm for exact matching, while mitigated by approximate methods, could become a bottleneck for large-scale datasets. A deeper discussion of computational trade-offs would be beneficial.
Pro and Con Arguments for Acceptance:
- Pro: PMD is a theoretically grounded and practically useful contribution that addresses key limitations of MMD. Its empirical results demonstrate clear advantages in important applications like domain adaptation and generative modeling.
- Con: The scalability and robustness of PMD in high-dimensional settings remain uncertain, and the claim of fewer hyperparameters could be more rigorously justified.
Recommendation: I recommend acceptance of this paper, as it provides a meaningful contribution to the field of distribution matching and advances the state of the art in both theory and practice. However, further exploration of PMD's limitations in high-dimensional spaces and its computational efficiency would strengthen its impact.