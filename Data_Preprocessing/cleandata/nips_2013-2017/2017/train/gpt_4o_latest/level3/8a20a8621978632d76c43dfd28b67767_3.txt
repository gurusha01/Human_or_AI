This paper presents a unified framework for interpreting model predictions, introducing SHAP (SHapley Additive exPlanations) as a feature attribution method grounded in cooperative game theory. The authors identify a new class of additive feature attribution methods and demonstrate that SHAP values uniquely satisfy three desirable properties: local accuracy, missingness, and consistency. By unifying six existing methods, including LIME, DeepLIFT, and layer-wise relevance propagation, the paper provides a theoretical foundation that clarifies their relationships and limitations. The authors propose novel SHAP value estimation methods, including Kernel SHAP and Deep SHAP, which improve computational efficiency and alignment with human intuition. Experimental results, including user studies and benchmark comparisons, validate the effectiveness of SHAP in providing consistent and interpretable feature attributions.
Strengths:
The paper is technically sound and provides a rigorous theoretical foundation for SHAP values, supported by detailed proofs and experiments. The unification of existing methods under a single framework is a significant contribution, offering clarity to a fragmented field. The authors effectively demonstrate the practical utility of SHAP through computational experiments and user studies, showing its superiority over alternative methods in terms of consistency with human intuition and computational efficiency. The introduction of new estimation methods, such as Kernel SHAP and Deep SHAP, further enhances the framework's applicability to various model types, including deep learning models. The work is well-organized, with clear motivations and a logical progression of ideas.
Weaknesses:
While the paper is comprehensive, it could benefit from a more detailed discussion of the trade-offs between Shapley value estimation accuracy and computational efficiency. For instance, the computational complexity of Kernel SHAP and its scalability to large datasets or high-dimensional models is not fully explored. Additionally, the implementation details for adapting DeepLIFT to approximate SHAP values (Deep SHAP) could be clarified, as this is a key contribution. Minor clarity issues, such as undefined symbols in equation (10) and inconsistent citation styles, detract slightly from the overall readability. The paper also misses an opportunity to explore connections between SHAP values and gradient-based approaches, which could broaden its applicability.
Originality and Significance:
The paper makes a novel contribution by identifying SHAP values as the unique solution within the class of additive feature attribution methods that satisfies desirable properties. This theoretical insight is both original and impactful, as it provides a unifying perspective that could guide future research in model interpretability. The proposed estimation methods and their demonstrated effectiveness further enhance the paper's significance. By addressing the growing need for interpretable machine learning, the work advances the state of the art and has the potential to influence both research and practice.
Recommendation:
I recommend accepting this paper, as it provides a well-founded, original, and significant contribution to the field of model interpretability. However, addressing the minor clarity issues and expanding the discussion on computational trade-offs and connections to gradient-based methods would further strengthen the work.