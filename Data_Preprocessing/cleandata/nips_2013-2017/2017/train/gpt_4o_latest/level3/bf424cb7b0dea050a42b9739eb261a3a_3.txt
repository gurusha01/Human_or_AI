This paper addresses a critical limitation of gradient descent (GD) when applied to kernel methods, particularly its slow convergence due to the fast spectral decay of smooth kernels. The authors introduce the concept of "computational reach" to quantify the fraction of the function space accessible to GD after a given number of iterations. They demonstrate that GD struggles to approximate less smooth functions, even in simple binary classification tasks, leading to over-regularization. To address this, the paper proposes EigenPro, a novel preconditioning approach that modifies the eigenspectrum of the kernel matrix to accelerate convergence without introducing bias.
Strengths:
1. Technical Soundness: The paper provides a rigorous theoretical analysis of the limitations of GD in kernel methods, supported by clear mathematical derivations and experiments. The introduction of computational reach is a novel and insightful contribution.
2. Practical Impact: EigenPro is computationally efficient, leveraging randomized SVD to compute a low-rank preconditioner. It achieves significant acceleration (up to 30-fold) in convergence compared to standard kernel methods, as demonstrated across multiple datasets.
3. Compatibility: EigenPro integrates seamlessly with stochastic gradient descent (SGD), making it scalable for large datasets. The authors also provide a theoretically sound step-size selection strategy.
4. Experimental Validation: The experiments show that EigenPro outperforms state-of-the-art methods in terms of both convergence speed and computational efficiency, with reduced GPU time and competitive accuracy.
Weaknesses:
1. Broader Applicability: While EigenPro is effective for kernel methods, the paper does not explore its potential applicability to other optimization algorithms, such as momentum-based or proximal methods. This could limit its perceived generality.
2. Geometric Motivation: The authors argue that reducing the ratio of smaller to larger eigenvalues aids convergence but do not provide a geometric intuition for why this modification is effective. This could enhance the reader's understanding of the method.
3. Clarity: Although the paper is dense and well-written, it could benefit from a more detailed discussion of the broader implications of preconditioning matrices in other machine learning contexts.
4. Overregularization Risks: The paper acknowledges that regularization can lead to overregularization but does not explore strategies to mitigate this risk when using EigenPro.
Pro Acceptance Arguments:
- The paper addresses a critical and well-defined problem in kernel methods, offering a novel and practical solution.
- Theoretical contributions, such as computational reach and spectral analysis, are significant and advance the understanding of GD in kernel methods.
- The experimental results are compelling, demonstrating both theoretical and practical impact.
Con Acceptance Arguments:
- The lack of exploration into broader applicability and geometric motivation limits the paper's scope.
- The dense presentation may hinder accessibility for a broader audience.
Recommendation: Accept with minor revisions. The paper makes a strong scientific contribution to the field, particularly in improving the scalability of kernel methods. Addressing the weaknesses mentioned above would further strengthen its impact.