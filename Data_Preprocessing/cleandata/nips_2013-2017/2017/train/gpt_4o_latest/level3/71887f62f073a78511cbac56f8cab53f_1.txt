This paper presents a novel randomized coordinate descent algorithm, SMART-CD, for solving a general convex optimization problem of the form \( F(x) = f(x) + g(x) + h(Ax) \), where \( f \) is smooth, \( g \) is separable and non-smooth, and \( h \) is non-smooth. The authors combine four key techniques—smoothing, acceleration, homotopy, and non-uniform coordinate sampling—into a primal-dual framework to achieve the best-known convergence rate of \( O(n/k) \) for this class of problems. The algorithm is also extended to handle constrained optimization problems and special cases such as support vector machines and regularized regression. Numerical experiments demonstrate the algorithm's superior performance compared to state-of-the-art methods.
Strengths:
1. Technical Contributions: The paper makes a significant theoretical contribution by providing the first rigorous convergence rate guarantees for randomized coordinate descent methods on the three-composite convex optimization problem. The use of homotopy to dynamically adjust the smoothing parameter is particularly innovative and reduces complexity compared to fixed-parameter methods.
2. Algorithm Design: The integration of smoothing, acceleration, and non-uniform sampling is well-motivated and effectively addresses the challenges posed by the non-smooth term \( h(Ax) \). The efficient implementation of SMART-CD further enhances its practical applicability.
3. Numerical Validation: The numerical experiments are comprehensive, covering diverse applications such as support vector machines, brain imaging, and degenerate linear programs. The results convincingly demonstrate the algorithm's advantages in terms of convergence rate and scalability.
4. Clarity of Presentation: The paper is well-structured, with clear explanations of the algorithm, its theoretical properties, and its practical implementation. The inclusion of special cases and restart strategies adds depth to the discussion.
Weaknesses:
1. Missing Reference: Reference [14] is missing in line 165, which hinders the reader's ability to verify certain claims. This oversight should be addressed in the final version.
2. Limited Discussion of Limitations: While the paper highlights the strengths of SMART-CD, it does not adequately discuss its limitations, such as potential challenges in tuning parameters like the smoothing parameter \( \beta \) or the probability distribution \( q \).
3. Comparison with Strongly Convex Settings: The paper briefly mentions that incorporating strong convexity could lead to faster rates (e.g., \( O(1/k^2) \)) but does not explore this direction. A discussion or preliminary results in this setting would strengthen the paper's impact.
4. Reproducibility: While the algorithm is described in detail, the numerical experiments lack sufficient information about hyperparameter tuning and computational resources, which may hinder reproducibility.
Recommendation:
The paper is technically sound, well-written, and addresses an important problem in large-scale optimization. Its contributions are novel and significant, particularly the combination of smoothing, acceleration, and homotopy in a randomized coordinate descent framework. The numerical results are compelling and demonstrate practical relevance. However, the missing reference and limited discussion of limitations slightly detract from the overall quality.
Arguments for Acceptance:
- Novel theoretical contributions with rigorous convergence guarantees.
- Strong empirical results demonstrating practical utility.
- Clear and well-organized presentation.
Arguments Against Acceptance:
- Missing reference and insufficient discussion of limitations.
- Lack of exploration of strongly convex settings.
Overall, I recommend acceptance with minor revisions to address the missing reference and improve the discussion of limitations and reproducibility. This paper is a valuable contribution to the field of optimization and machine learning.