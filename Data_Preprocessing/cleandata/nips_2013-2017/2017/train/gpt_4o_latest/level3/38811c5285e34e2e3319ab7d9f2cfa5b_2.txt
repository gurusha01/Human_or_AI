The paper presents a novel parallelization scheme for machine learning algorithms, leveraging Radon points instead of traditional averaging. The authors propose the "Radon machine," which achieves polylogarithmic runtime on quasi-polynomially many processors while maintaining theoretical performance guarantees. This approach is significant as it addresses the challenge of efficiently parallelizing learning algorithms, particularly in high-confidence domains where errors are costly. The empirical results demonstrate that the Radon machine achieves comparable or superior accuracy to existing methods, such as Spark-based algorithms and averaging-based parallelization, while significantly reducing runtime.
Strengths
1. Novelty: The use of Radon points for aggregation is a key innovation. Unlike averaging, Radon points provide doubly exponential confidence improvement with the height of the aggregation tree, which is theoretically and practically advantageous.
2. Theoretical Contributions: The paper rigorously proves that the Radon machine achieves polylogarithmic runtime under specific conditions and provides guarantees on the quality of the hypotheses. This addresses an open question in parallel machine learning related to Nick's Class (NC).
3. Empirical Validation: The experiments are comprehensive, comparing the Radon machine against baselines (e.g., no parallelization, averaging, and Spark-based methods) on multiple datasets. The results consistently show significant speed-ups (up to 700x) with comparable or better accuracy.
4. Generality: The scheme is a black-box parallelization method applicable to a wide range of learning algorithms, requiring no modifications to the underlying algorithm.
Weaknesses
1. Related Work Overlap: The theoretical foundation overlaps with the paper "Parallelizing Randomized Convex Optimization," which is not cited. The authors must clarify how their contributions differ and ensure proper attribution.
2. Scalability Concerns: While the Radon machine demonstrates impressive results, its reliance on a quasi-polynomial number of processors may limit practical applicability in resource-constrained environments. The discussion on deparallelization is helpful but could be expanded.
3. High-Dimensional Data: The scheme's applicability to high-dimensional or non-linear models is limited without preprocessing steps like random projections or low-rank approximations. This constraint should be more explicitly acknowledged.
4. Clarity: While the paper is generally well-written, some sections (e.g., proofs and theoretical derivations) are dense and may benefit from additional explanations or examples to aid reader comprehension.
Arguments for Acceptance
- The paper addresses a fundamental problem in parallel machine learning, providing both theoretical and empirical contributions.
- The use of Radon points is innovative and demonstrates clear advantages over traditional averaging.
- The empirical results are robust, showing significant runtime improvements without sacrificing accuracy.
Arguments Against Acceptance
- The overlap with prior work raises concerns about originality, which must be clarified.
- The scalability to fewer processors and high-dimensional data is not fully addressed, potentially limiting practical impact.
Recommendation
The paper makes a strong contribution to the field of parallel machine learning, particularly with its novel use of Radon points. However, the authors must address the related work overlap and provide clearer distinctions from prior contributions. Additionally, expanding the discussion on scalability and high-dimensional data would strengthen the paper. Pending these revisions, I recommend acceptance with minor revisions.