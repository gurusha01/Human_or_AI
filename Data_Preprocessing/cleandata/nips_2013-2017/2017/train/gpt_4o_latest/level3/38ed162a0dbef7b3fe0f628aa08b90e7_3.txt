This paper provides a rigorous theoretical framework for understanding the functional spaces implemented by deep convolutional neural networks (DCNNs) through the lens of reproducing kernel Hilbert spaces (RKHS). By generalizing multilayer kernels, the authors demonstrate how DCNNs achieve near-invariance to group transformations and stability to diffeomorphic transformations while preserving signal information. The work builds on prior efforts in convolutional kernel networks (CKNs) and scattering transforms, extending these ideas to more general CNN architectures. The paper also links these theoretical results to practical implementations, including discretization and kernel approximations, and establishes connections to classical CNNs with smooth homogeneous activation functions.
Strengths:
1. Theoretical Rigor: The paper provides a solid mathematical foundation for understanding invariance and stability in DCNNs, leveraging tools from harmonic analysis and kernel theory. The results are well-supported by proofs and detailed derivations.
2. Novelty: Reformulating DCNNs in the context of RKHS is an interesting and potentially impactful contribution. It bridges theoretical insights from kernel methods with practical deep learning architectures.
3. Clarity of Stability Analysis: The authors convincingly show how stability to diffeomorphic transformations and translation invariance can be achieved and controlled through kernel design and pooling operations.
4. Connections to Prior Work: The paper builds on and extends prior work on scattering transforms and convolutional kernel networks, situating its contributions within a broader research context.
Weaknesses:
1. Practical Utility: While the theoretical insights are valuable, the practical implications of the RKHS reformulation are not entirely clear. The authors should elaborate on how this framework can directly benefit practitioners or lead to improved architectures.
2. Preservation of Signal Information: The paper emphasizes the importance of preserving signal information but does not sufficiently justify why this is critical for task-specific applications like image classification, where some information loss (e.g., through pooling) is often acceptable.
3. Connection to Existing Methods: The relationship between the proposed group invariance approach and Hans Burkhardt's invariant kernel methods is not explicitly clarified. This connection could strengthen the theoretical grounding of the work.
4. Equivariance with Other Operators: The paper raises an interesting question about whether other operators commuting with group actions could ensure equivariance in DCNNs but does not explore this direction further.
Suggestions for Improvement:
1. Provide concrete examples or experiments to demonstrate the practical utility of the RKHS reformulation, particularly in scenarios where traditional CNNs face limitations.
2. Clarify the significance of preserving signal information in the context of specific tasks or applications.
3. Discuss the relationship to Burkhardt's invariant kernel methods in more detail, possibly with citations or comparisons.
4. Explore or at least hypothesize about the potential of other operators for achieving equivariance, as this could open new avenues for research.
Recommendation:
The paper is a strong theoretical contribution that advances our understanding of DCNNs from a functional perspective. However, its practical relevance and novelty could be better articulated. I recommend acceptance with minor revisions to address the highlighted weaknesses and provide more clarity on the practical implications of the work.