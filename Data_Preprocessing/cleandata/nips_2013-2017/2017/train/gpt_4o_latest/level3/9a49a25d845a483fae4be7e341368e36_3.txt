Review
The paper introduces a novel probabilistic framework for pre-processing data to reduce algorithmic discrimination while balancing utility and individual fairness. The authors formulate a convex optimization problem that enforces statistical parity and individual fairness through constraints, ensuring that discrimination is minimized without significantly distorting the original data distribution. The proposed method is applicable to both training and unseen test data, making it versatile in real-world applications. Theoretical contributions include a robust characterization of the optimization problem, its convexity under certain conditions, and the impact of limited sample sizes on discrimination and utility guarantees. Experimental results on the COMPAS and UCI Adult datasets demonstrate the method's effectiveness, although some limitations are noted.
Strengths:
1. Technical Soundness: The paper is technically rigorous, with well-grounded theoretical claims. The convexity of the optimization problem and its robustness to sample size variations are significant contributions. The inclusion of different distance/similarity measures into the constraints adds flexibility.
2. Comprehensive Positioning: The paper is well-situated within the discrimination-aware machine learning literature. It provides a thorough overview of related work and highlights its novelty compared to existing methods, such as Learning Fair Representations (LFR).
3. Practical Applicability: The method's ability to transform both training and test data while preserving the original data distribution is a practical advantage. The explicit control of individual fairness through distortion constraints is a unique and valuable feature.
4. Supplementary Material: The inclusion of proofs, dataset discussions, and a novel utility bound when probabilities are estimated via maximum likelihood enhances the paper's completeness and reproducibility.
Weaknesses:
1. Choice of Similarity Measure: The probability ratio measure for distribution similarity is poorly motivated and may be unnecessarily restrictive, particularly when probabilities are low. A more robust justification or alternative metric would strengthen the paper.
2. Experimental Limitations: While the method performs well on the COMPAS dataset, its results on the UCI Adult dataset underperform compared to LFR. Additionally, the experiments are limited to only two datasets, which restricts the generalizability of the findings. More diverse case studies would bolster the experimental section.
3. Distortion Measure Explanation: The distortion metrics used in the experiments are insufficiently explained, leaving readers unclear about their selection and impact on the results.
4. Timing Measurements: The lack of timing measurements for solving the optimization problem is a notable omission. This limits the evaluation of the method's scalability and practical feasibility for large datasets.
Arguments for Acceptance:
- The paper makes a significant contribution to discrimination-aware machine learning by proposing a flexible and theoretically grounded framework.
- It advances the state of the art by explicitly addressing individual fairness and enabling multivariate, non-binary protected variables.
- The method's flexibility and potential for extensions make it a valuable addition to the field.
Arguments Against Acceptance:
- The experimental evaluation is limited in scope and diversity, and the method underperforms on the UCI Adult dataset compared to LFR.
- The choice of similarity and distortion measures requires better justification and explanation.
- The absence of timing measurements leaves questions about the method's scalability.
Recommendation: Accept with minor revisions. While the paper has some limitations, its theoretical contributions and practical implications make it a valuable addition to the conference. Addressing the experimental and methodological critiques would further strengthen the work.