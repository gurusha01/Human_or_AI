The paper addresses a novel and important problem in sequential hypothesis testing, focusing on controlling the False Discovery Rate (FDR) and Modified FDR (mFDR) while maximizing the Best Discovery Rate (BDR). The authors propose an innovative framework that integrates pure-exploration Multi-Armed Bandit (MAB) methods with online FDR control techniques. This combination allows for adaptive sampling strategies that significantly reduce sample complexity while maintaining rigorous statistical guarantees. The paper is well-written, clearly organized, and highly relevant to the NeurIPS community, given its focus on adaptive algorithms and statistical rigor in sequential decision-making.
The key contribution of the paper lies in its meta-algorithm, which merges MAB techniques for best-arm identification with online FDR control procedures. The authors define a novel null hypothesis for MAB instances and propose a method for deriving always-valid p-values, enabling continuous monitoring. They also demonstrate that setting MAB confidence levels to the rejection thresholds of online FDR algorithms ensures simultaneous control of FDR and BDR. The theoretical guarantees provided are robust, and the experimental results, including simulations and real-world data from the New Yorker Cartoon Caption contest, validate the framework's effectiveness in terms of power, sample efficiency, and FDR control.
Strengths:
1. Technical Soundness: The paper provides rigorous theoretical guarantees for both FDR control and power, supported by detailed proofs and extensive simulations.
2. Innovation: The integration of MAB methods with online FDR control is novel and addresses a significant gap in the literature.
3. Clarity: The paper is well-organized, with clear explanations of the problem, methodology, and results. The use of illustrative figures and examples enhances understanding.
4. Significance: The proposed framework has broad applicability in domains like A/B testing, clinical trials, and adaptive experimentation, where sequential decision-making and statistical rigor are critical.
Weaknesses:
1. Complexity: While the framework is theoretically sound, its practical implementation may be challenging for practitioners unfamiliar with both MAB and FDR techniques. Simplified guidelines or code examples could improve accessibility.
2. Comparison Baseline: Although the paper compares its method to AB-FDR and MAB-IND, additional baselines, such as other adaptive sampling techniques or offline FDR control methods, could strengthen the evaluation.
3. Generality: The framework assumes sub-Gaussian rewards and specific MAB algorithms. Extending the approach to other reward distributions or more general MAB settings could broaden its applicability.
Arguments for Acceptance:
- The paper addresses a timely and important problem with a novel and well-justified approach.
- The theoretical contributions and experimental results are strong and demonstrate the framework's potential impact.
Arguments Against Acceptance:
- The practical complexity of the framework may limit its immediate adoption.
- The evaluation could benefit from additional baselines and broader applicability.
Overall, this paper makes a significant contribution to the field of sequential hypothesis testing and adaptive experimentation. It is a strong candidate for acceptance at NeurIPS, provided the authors address the practical implementation challenges and expand the evaluation in future work.