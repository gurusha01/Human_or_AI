The paper presents a novel approach to reducing the variance of stochastic gradient estimators in variational inference by removing the score function term from the reparameterized gradient estimator. This modification, termed the "path derivative estimator," maintains unbiasedness while achieving variance reduction, particularly as the variational posterior approaches the true posterior. The authors provide theoretical analysis and empirical validation on benchmark datasets (MNIST and Omniglot), demonstrating modest but consistent improvements in performance. The method is simple to implement, requiring only a minor modification to the computation graph in automatic differentiation frameworks, and is generalized to complex variational families such as mixtures and importance-weighted posteriors.
Strengths:
1. Relevance and Importance: The paper addresses an important gap in the field of variational inference, where systematic evaluation of stochastic gradient computation with the reparameterization trick has been limited. The proposed method is particularly relevant for practitioners working with flexible variational families.
2. Technical Soundness: The theoretical analysis is well-grounded, and the authors convincingly argue that removing the score function does not bias the gradient estimator. The surprising finding that Monte Carlo approximations to the entropy term can outperform exact calculations is thought-provoking and warrants further exploration.
3. Empirical Validation: The experiments on MNIST and Omniglot datasets provide evidence of the efficacy of the proposed method, with consistent improvements in negative log-likelihood (NLL) scores across most scenarios. The method's simplicity and compatibility with existing frameworks make it accessible to practitioners.
4. Clarity: The paper is generally well-written, with clear explanations of the problem and proposed solution. The inclusion of algorithms and implementation details enhances reproducibility.
Weaknesses:
1. Modest Empirical Gains: While the proposed method consistently improves performance, the gains are relatively small and may not justify adoption in all scenarios, particularly when computational resources are limited.
2. Lack of Novel Mathematical Contributions: The paper relies more on empirical improvements than on introducing fundamentally new mathematical insights. The variance reduction technique, while effective, is conceptually straightforward.
3. Notational Ambiguity: Equations 5–7 require clarification, particularly regarding the differentiation and the role of the score function. The notational challenges may have obscured the issue of unnecessary variance from the score function in prior work.
4. Limited Scope of Extensions: While the method is generalized to mixtures and importance-weighted posteriors, its application to flow-based variational families remains incomplete, limiting its broader applicability.
Recommendation:
I recommend accepting this paper, as it provides a meaningful contribution to the field of variational inference by addressing a practical challenge with a simple and effective solution. However, the authors should address the notational ambiguities in Equations 5–7 and provide additional discussion on scenarios where the path derivative estimator may underperform (e.g., when the score function acts as a control variate). The modest empirical improvements should also be contextualized, emphasizing the method's utility in complex variational families.
Arguments for Acceptance:
- Addresses a significant problem in stochastic gradient estimation for variational inference.
- Provides a simple, generalizable, and easy-to-implement solution.
- Demonstrates consistent empirical improvements on benchmark datasets.
- Opens avenues for further exploration in related fields (e.g., reinforcement learning).
Arguments Against Acceptance:
- Limited novelty in mathematical formulation.
- Modest empirical gains may not justify adoption in all cases.
- Incomplete generalization to flow-based posteriors.
In summary, the paper offers a valuable contribution to the field, particularly for practitioners working with complex variational families, and merits inclusion in the conference proceedings.