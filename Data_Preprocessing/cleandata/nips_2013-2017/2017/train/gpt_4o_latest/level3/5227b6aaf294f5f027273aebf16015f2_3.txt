This paper introduces a novel hyperspherical convolution framework, SphereConv, which operates on angular representations rather than traditional inner products. The authors propose SphereNet, a deep hyperspherical convolutional network, and demonstrate its potential advantages, including improved training stability, faster convergence, and comparable or better classification accuracy. The work also introduces SphereNorm as a normalization method and a learnable SphereConv operator, adding flexibility to the framework. Experiments conducted on datasets like CIFAR-10, CIFAR-100, and ImageNet-2012 validate the proposed method's effectiveness, though the performance improvement over traditional convolutional networks appears limited in some cases.
Strengths:
1. Novelty and Motivation: The paper presents a unique perspective by projecting convolutional learning onto hyperspheres, which is a significant departure from traditional convolutional frameworks. The theoretical insights into the advantages of angular representations are compelling and well-argued.
2. Comprehensive Experiments: The authors provide extensive experimental results, including ablation studies, exploratory experiments, and evaluations on multiple datasets. The inclusion of comparisons across different network architectures and loss functions strengthens the empirical validation.
3. Training Stability and Convergence: SphereNet demonstrates faster and more stable convergence than traditional CNNs, which is particularly valuable for training ultra-deep networks.
4. Flexibility: The introduction of learnable SphereConv and SphereNorm showcases the adaptability of the framework, with potential applications beyond CNNs, such as in fully connected layers or other neural network paradigms.
Weaknesses:
1. Limited Performance Gains: While SphereNet offers faster convergence and training stability, the overall performance improvement in terms of classification accuracy is modest, especially when compared to state-of-the-art CNNs.
2. Complexity Concerns: The computational complexity of SphereConv is higher than traditional convolution, but the paper does not provide a detailed comparison of computational costs, leaving questions about its scalability in large-scale applications.
3. Operator Selection: The process for selecting the appropriate SphereConv operator (linear, cosine, or sigmoid) and the hyperparameter \(k\) for Sigmoid SphereConv is unclear. While cross-validation is suggested, this may not be practical for all use cases.
4. Limited Generalization: The framework's dependence on network width for significant performance gains raises concerns about its applicability to narrower architectures or resource-constrained scenarios.
5. Prefixed Design: Despite the introduction of learnable SphereConv, most operators remain prefixed, which may limit the framework's adaptability to diverse tasks.
Recommendation:
The paper is a valuable contribution to the field, offering a fresh perspective on convolutional learning. However, the modest performance gains and lack of clarity in operator selection and complexity analysis temper its impact. I recommend acceptance, provided the authors address the concerns regarding computational complexity and provide clearer guidance on operator selection. The work opens up an interesting research direction and has the potential to inspire further advancements in hyperspherical learning. 
Arguments for Acceptance:
- Novel and theoretically motivated framework.
- Extensive experimental validation.
- Faster convergence and training stability.
Arguments Against Acceptance:
- Limited performance improvement.
- Unclear complexity analysis and operator selection process.
- Dependency on network width for significant gains.