The paper introduces a double sampling strategy for multi-class to binary reduction in extreme classification problems, addressing challenges in scenarios with a large number of classes and long-tailed distributions. The proposed method employs an aggressive double sampling scheme to balance class distributions and reduce the size of dyadic examples, with theoretical guarantees provided through generalization bounds based on local Rademacher complexities. The empirical evaluation on large-scale text datasets, such as DMOZ and Wikipedia, demonstrates the method's effectiveness in terms of predictive performance, training time, and memory efficiency compared to state-of-the-art approaches.
Strengths:
1. Significance: The paper tackles a critical problem in extreme classification, which has practical applications in domains like recommendation systems and text classification. The proposed method demonstrates scalability to datasets with up to 100,000 classes, making it relevant for real-world applications.
2. Empirical Validation: The experiments are comprehensive, covering multiple datasets and comparing the proposed approach to several baselines. The results highlight the method's competitive performance in terms of accuracy, macro F1 score, and resource efficiency.
3. Theoretical Contributions: The derivation of generalization bounds using local Rademacher complexities is a valuable theoretical contribution, ensuring the consistency of the empirical risk minimization principle under the proposed sampling strategy.
4. Efficiency: The double sampling strategy significantly reduces training time and memory usage, making the approach suitable for large-scale problems, where traditional methods like OVA and M-SVM become computationally infeasible.
Weaknesses:
1. Limited Novelty: The method builds heavily on prior work ([16]), with the main contributions being the double sampling strategy and the associated theoretical analysis. While these are valuable, the novelty of the approach is somewhat incremental.
2. Prediction Time: A notable drawback is the longer prediction time compared to other methods, which limits its applicability in latency-sensitive scenarios. The candidate selection strategy partially mitigates this issue but does not fully address it.
3. Clarity of Comparisons: The interpretation and comparison of the proposed generalization bounds with those of existing methods are unclear. This could be improved by providing more explicit discussions and numerical insights.
4. Experimental Design: The mix of batch and online learning algorithms in the experiments leads to differing memory usage, complicating direct comparisons. A more uniform experimental setup would strengthen the results.
Arguments for Acceptance:
- The paper addresses a significant problem in extreme classification and demonstrates strong empirical results on large-scale datasets.
- The theoretical analysis adds rigor and ensures the soundness of the proposed approach.
- The method achieves a good balance between predictive performance and computational efficiency, making it a practical contribution.
Arguments Against Acceptance:
- The novelty is limited, as the work primarily extends existing methods with incremental improvements.
- The longer prediction time and unclear comparisons of theoretical bounds are practical and conceptual drawbacks.
Recommendation: Overall, the paper makes a meaningful contribution to the field of extreme classification, particularly in terms of scalability and efficiency. While the novelty is limited, the strong empirical results and theoretical guarantees justify its acceptance, provided the authors address the clarity issues and discuss prediction time trade-offs more thoroughly.