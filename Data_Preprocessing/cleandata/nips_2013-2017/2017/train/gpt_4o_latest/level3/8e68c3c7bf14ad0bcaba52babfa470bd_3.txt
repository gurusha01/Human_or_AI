Review
Summary:  
This paper proposes a novel approach to improving image captioning models by incorporating natural language feedback from humans into the training process. The authors argue that descriptive feedback provides a stronger learning signal than numeric rewards, as it identifies specific mistakes and suggests corrections. The proposed method involves a hierarchical phrase-based captioning model trained with reinforcement learning (RL), where a feedback network (FBN) conditions on human feedback to refine the model. Experiments on the MS-COCO dataset demonstrate that the approach improves captioning performance, as measured by automatic evaluation metrics, and reduces the need for extensive ground-truth annotations.
---
Strengths:  
1. Novelty and Practicality: The paper introduces a creative and practical way to integrate human feedback into RL for image captioning, addressing the challenge of sparse rewards and credit assignment in RL. The use of natural language feedback is particularly compelling, as it aligns with how non-experts might interact with AI systems in real-world scenarios.  
2. Well-Executed Feedback Collection: The authors designed a robust feedback collection process via crowd-sourcing, ensuring minimal ambiguity. Their hierarchical phrase-based model is well-suited for incorporating feedback at the phrase level, making the approach more interpretable and actionable.  
3. Comprehensive Experiments: The paper includes thorough experimental validation, including ablations, comparisons with baselines, and analysis of the feedback network's performance. The results demonstrate consistent improvements over standard RL and MLE baselines.  
4. Open Science: The authors commit to releasing their code and data, which will facilitate further research in this area.
---
Weaknesses:  
1. Lack of Human Evaluation: While the paper relies on automatic metrics like BLEU and ROUGE-L, it does not include a detailed human evaluation of caption quality. Human judgments would provide more reliable insights into the practical utility of the generated captions.  
2. Small Testing Dataset: The testing dataset is only 1/20th the size of the training dataset, raising concerns about the robustness and generalizability of the evaluation. Larger-scale testing would strengthen the claims.  
3. Clarity Issues: Some sections of the paper are difficult to follow due to insufficient explanation (e.g., RL fine-tuning in Section 3.3). Figure 2 is poorly labeled, and certain terms (e.g., "newly sampled caption") are ambiguous.  
4. Citation Errors and Typos: There are notable citation errors (e.g., misattribution of reinforcement learning in [3] vs. [34]) and minor typos (e.g., L28-29, L64, L74, L200). These detract from the overall quality of the paper.  
5. Decoding Method Unspecified: The paper does not specify the decoding method (e.g., beam search or sampling) used during evaluation, which is critical for reproducibility.  
6. Notation Inconsistencies: There are inconsistencies in notation (e.g., L167 superscript), which could confuse readers.
---
Arguments for Acceptance:  
- The paper addresses an important problem in RL and image captioning, proposing a novel and practical solution.  
- The integration of natural language feedback is a significant contribution to human-in-the-loop learning.  
- The experimental results demonstrate clear improvements over baselines, and the open-source commitment adds value to the research community.  
Arguments Against Acceptance:  
- The lack of human evaluation limits the reliability of the results.  
- Clarity and presentation issues, including ambiguous terms, citation errors, and typos, reduce the paper's accessibility.  
- The small testing dataset raises concerns about the robustness of the findings.
---
Recommendation:  
I recommend conditional acceptance of this paper, provided the authors address the clarity issues, correct the citation errors and typos, and include a human evaluation of caption quality. The proposed approach is innovative and has the potential to advance the state of the art in image captioning and human-in-the-loop learning. However, the presentation and evaluation need refinement to fully realize its impact.