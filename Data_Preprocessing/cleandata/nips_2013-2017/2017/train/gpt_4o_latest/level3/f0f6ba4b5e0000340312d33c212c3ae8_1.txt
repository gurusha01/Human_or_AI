The paper presents a novel weighted hash embedding technique that effectively reduces the number of embedding parameters while maintaining competitive performance across various tasks. The approach is elegant, combining the benefits of standard word embeddings and the hashing trick, and introduces a mechanism to handle collisions through trainable importance parameters. This method is versatile, as it can utilize a predefined dictionary or be applied dynamically during online training, making it particularly useful for tasks with large or evolving vocabularies. The authors demonstrate that hash embeddings act as an effective regularizer, reducing overfitting and achieving comparable or superior performance to standard embeddings on multiple datasets.
Strengths:  
The proposed method addresses a critical challenge in natural language processing—scaling embeddings for massive vocabularies—by significantly reducing the parameter count without sacrificing performance. The theoretical foundation is sound, and the experiments are well-designed, covering diverse datasets and tasks. The paper also highlights the practical advantages of hash embeddings, such as their ability to handle dynamic vocabularies and their inherent regularization properties. The inclusion of ensemble models further underscores the flexibility of the approach. Additionally, the method is computationally efficient, with negligible overhead compared to standard embeddings, making it accessible for real-world applications.
Weaknesses:  
While the paper is strong overall, there are areas where clarity and completeness could be improved. For instance, Table 2 should include final vocabulary sizes and parameter reduction details to provide a clearer picture of the method's efficiency. Table 3 lacks a comparison with a joint state-of-the-art model for the DBPedia dataset, which would strengthen the evaluation. The claim that ensemble training time matches that of a single large model is questionable, as the non-embedding weights remain unchanged, and this aspect requires further justification. Additionally, separating embedding-only methods from RNN/CNN approaches in Table 3 would enhance interpretability, and testing hash embeddings in context-sensitive models could provide a more comprehensive evaluation. Minor issues, such as typos, awkward phrasing, and unclear details in specific lines and Table 4, detract slightly from the paper's readability.
Pro and Con Arguments for Acceptance:  
- Pros:  
  - Novel and efficient embedding technique with strong theoretical and empirical support.  
  - Versatility in handling both predefined and dynamic vocabularies.  
  - Demonstrated parameter efficiency and regularization benefits.  
  - Broad applicability across tasks and datasets.  
- Cons:  
  - Missing details in key tables and lack of comparison with some state-of-the-art models.  
  - Questionable claim regarding ensemble training time.  
  - Minor clarity and presentation issues.  
Recommendation:  
I recommend acceptance of this paper, contingent on addressing the identified weaknesses. The proposed method is a significant contribution to the field, offering a scalable and efficient alternative to standard embeddings, with potential for widespread adoption in both research and practical applications.