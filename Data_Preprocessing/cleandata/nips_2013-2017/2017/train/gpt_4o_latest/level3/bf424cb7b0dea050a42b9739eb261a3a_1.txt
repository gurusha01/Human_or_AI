Review of the Paper
Summary
This paper investigates the limitations of gradient descent (GD) in approximating non-smooth functions when used with smooth (infinitely differentiable) kernels. The authors identify that the fast eigenvalue decay of such kernels restricts the computational reach of GD, requiring a super-polynomial number of iterations to approximate less smooth functions. To address this, the paper introduces EigenPro, a preconditioning scheme that uses approximate second-order information to accelerate convergence without introducing bias. EigenPro achieves this by modifying the spectrum of kernel matrices through randomization, offering a computationally efficient alternative to traditional second-order methods. The authors provide theoretical analysis and experimental results to demonstrate that EigenPro significantly accelerates convergence compared to standard GD and Pegasos, particularly for large datasets. However, the paper notes that the speed-ups over PEGASOS are sometimes marginal, and the implications of the results remain unclear in certain scenarios.
Strengths
1. Novel Insight: The paper provides a clear and theoretically grounded explanation of why GD struggles with smooth kernels, linking inefficiency to the eigenvalue decay of kernel functions. This insight is valuable for understanding the limitations of shallow methods.
2. Practical Contribution: EigenPro is a computationally efficient and scalable method that addresses the identified limitations. Its compatibility with stochastic gradient descent (SGD) and low overhead per iteration make it practical for large-scale problems.
3. Theoretical Rigor: The analysis of EigenPro is thorough, including step size selection and convergence guarantees. The authors also connect their work to related literature, situating EigenPro as a hybrid method that balances first- and second-order optimization.
4. Experimental Validation: The paper includes extensive experiments on large datasets, demonstrating that EigenPro achieves significant acceleration over standard GD and often matches or improves state-of-the-art kernel methods at a fraction of the computational cost.
Weaknesses
1. Unclear Implications: While the theoretical and experimental results are promising, the broader implications of the findings remain unclear. For example, the paper does not explore how EigenPro compares to deep learning methods in synthetic or real-world settings, which could provide valuable context for its practical utility.
2. Marginal Speed-Ups in Some Cases: The experimental results show only marginal speed-ups over PEGASOS in some scenarios, and EigenPro is occasionally slower. This raises questions about the consistency of its advantages.
3. Readability: The submitted version of the paper is less readable compared to the longer version on arXiv. The dense presentation of theoretical results and lack of intuitive explanations may hinder accessibility for a broader audience.
4. Limited Comparisons: The paper does not explicitly compare EigenPro to deep networks, which would help clarify its strengths and weaknesses in modern machine learning contexts. Additionally, the choice of datasets could be expanded to include more diverse benchmarks.
Arguments for Acceptance
1. The paper provides a novel and theoretically sound contribution to the understanding of GD inefficiencies with smooth kernels.
2. EigenPro is a practical and scalable method with demonstrated acceleration on large datasets, making it relevant to the machine learning community.
3. The work addresses a challenging and important problem, bridging the gap between theoretical insights and practical improvements for shallow methods.
Arguments Against Acceptance
1. The implications of the results are not fully explored, particularly in comparison to deep learning methods.
2. The experimental results show inconsistent speed-ups, and the practical significance of the improvements is not always clear.
3. The paper's readability and organization could be improved to make the contributions more accessible.
Recommendation
I recommend acceptance with minor revisions. The paper makes a valuable contribution to the field, but the authors should improve the clarity of their experimental results, provide more intuitive explanations of their theoretical findings, and explore comparisons with deep networks to better contextualize their work. Additionally, improving the readability of the paper would enhance its impact and accessibility.