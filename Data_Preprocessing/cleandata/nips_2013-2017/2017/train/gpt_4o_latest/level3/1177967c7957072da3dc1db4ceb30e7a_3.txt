This paper addresses the problem of safe adaptive importance sampling, a topic of growing interest in the AI community due to its potential to accelerate optimization algorithms like coordinate descent (CD) and stochastic gradient descent (SGD). The authors propose a novel sampling scheme that approximates gradient-based sampling using safe bounds on the gradient. This method is theoretically proven to outperform uniform and fixed importance sampling while being computationally efficient. The proposed approach is generic and can be integrated into existing algorithms, offering significant speed-ups in optimization tasks. The paper is supported by rigorous theoretical analysis and extensive empirical validation on real-world datasets.
Strengths:  
1. Quality: The paper is technically sound, with well-supported claims through both theoretical proofs and practical experiments. The authors provide clear mathematical formulations and demonstrate the advantages of their approach over existing methods. The inclusion of safe bounds ensures that the proposed sampling is robust and reliable, addressing a key limitation of prior adaptive strategies.  
2. Originality: The work introduces a novel way of leveraging safe gradient bounds to compute adaptive sampling distributions efficiently. Unlike previous methods, the proposed approach guarantees that the sampling is never worse than fixed importance sampling, which is a significant theoretical contribution.  
3. Significance: The results have practical implications for large-scale machine learning tasks, where computational efficiency is critical. The ability to integrate the method into CD and SGD makes it broadly applicable, and the demonstrated speed-ups in both iterations and computation time are compelling.  
4. Clarity: The paper is generally well-written and organized, with a logical flow from problem formulation to theoretical analysis and empirical evaluation. The authors provide sufficient details for reproducibility, including algorithmic descriptions and dataset information.
Weaknesses:  
1. Figures: The figures in Section 5 are difficult to read due to their small size and unclear labels. Enlarging these figures and improving their readability would enhance the presentation of experimental results.  
2. Empirical Results for SGD: While the proposed method shows significant improvements for CD, its impact on SGD is less pronounced. This discrepancy could be better analyzed and discussed to provide a more balanced evaluation.  
3. Gradient Bound Initialization: Although the authors claim that no initialization of gradient bounds is needed, further discussion on how this affects early iterations of optimization would strengthen the paper.
Arguments for Acceptance:  
- The paper addresses a relevant and challenging problem with a novel and theoretically grounded solution.  
- The proposed method is generic, efficient, and demonstrates strong empirical performance, particularly for CD.  
- The theoretical guarantees and practical applicability make it a valuable contribution to the field.
Arguments Against Acceptance:  
- The figures in Section 5 hinder the clarity of experimental results.  
- The limited improvement observed for SGD raises questions about the generalizability of the approach.  
- The paper could benefit from a deeper analysis of the impact of gradient bound initialization.
Recommendation: Accept with minor revisions. The paper makes a significant contribution to the field of optimization in machine learning, and addressing the clarity of figures and providing additional analysis on SGD performance would further strengthen its impact.