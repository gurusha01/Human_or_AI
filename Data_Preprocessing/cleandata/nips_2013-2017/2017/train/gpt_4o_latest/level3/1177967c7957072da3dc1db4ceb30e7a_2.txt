This paper presents a novel adaptive importance sampling method for Coordinate Descent (CD) and Stochastic Gradient Descent (SGD) algorithms, addressing the inefficiency of static sampling strategies and the computational infeasibility of full gradient-based adaptive sampling. The authors propose a safe approximation of gradient-based sampling using upper and lower bounds on the gradient, which is computationally efficient and provably better than fixed importance sampling. The theoretical guarantees, particularly Theorem 3.2, demonstrate the superiority of the proposed sampling over Li-based sampling, and the empirical results validate its performance on generalized linear models (GLMs) with L1 and L2 regularization.
Strengths:
1. Theoretical Contribution: The paper provides a rigorous theoretical foundation for the proposed sampling method. Theorem 3.2 and its associated guarantees are particularly compelling, as they establish the method's superiority over fixed sampling strategies.
2. Efficiency: The proposed algorithm achieves an \(O(n \log n)\) complexity per iteration, which is comparable to the cost of evaluating a single gradient component, making it practical for large-scale applications.
3. Empirical Validation: Extensive numerical experiments on real-world datasets demonstrate the method's effectiveness, particularly for CD, where it significantly outperforms fixed sampling in both iterations and computation time.
4. Generality: The method is generic and can be integrated into existing CD and SGD frameworks with minimal modifications.
Weaknesses:
1. Clarity Issues: The figures are too small to be easily interpretable. Moving some figures to the appendix and enlarging the remaining ones would improve readability.
2. Notation Inconsistencies: Line 85 incorrectly uses \(x\) instead of \(xk\) in the denominator of \(p^*i\), and Line 135 should remove the subscript \(k\) for \(\hat{p}\) and \(\hat{c}\) to maintain consistency.
3. Limited SGD Impact: While the method shows significant improvements for CD, its impact on SGD is less pronounced, as the optimal sampling performs only slightly better than uniform sampling in the experiments.
4. Missing Complexity Analysis: The paper does not explicitly discuss how the new sampling method affects the total complexity of CD or SGD, including the \(O(n)\) cost for updating \(lk\) and \(uk\). Adding remarks on this would strengthen the practical applicability of the method.
Suggestions for Improvement:
1. Add a comparison of the new sampling method with uniform sampling (\(p = 1/n\)) in the same vein as Conclusion 2 in Theorem 3.2.
2. Provide explicit remarks on how the proposed sampling impacts the total complexity of CD and SGD, including the cost of maintaining gradient bounds.
3. Improve figure clarity by enlarging key plots and moving less critical ones to the appendix.
4. Address the noted notation inconsistencies for better readability.
Recommendation:
This paper makes a significant theoretical and practical contribution to the field of optimization for machine learning. Despite minor clarity and notation issues, its strengths in advancing adaptive sampling methods outweigh its weaknesses. I recommend acceptance with minor revisions to address the clarity and notation concerns.