The paper presents a computational framework for modeling human-like question generation, specifically in the context of a Battleship-style game. The authors propose a probabilistic model that treats questions as compositional programs, evaluated for informativeness, complexity, and answer type. The model employs a log-linear approach with features such as Expected Information Gain (EIG), question complexity, and board-relatedness. The authors demonstrate the model's ability to predict human question frequencies and generate novel, semantically coherent questions not present in the training data. While the work is grounded in active learning, it also draws on cognitive science and linguistics to formalize question generation as a program synthesis problem.
Strengths:  
The paper is technically sound and well-organized, providing a clear explanation of the proposed model and its components. The use of a compositional grammar to represent questions is innovative, allowing for the generation of novel, human-like queries. The evaluation is thorough, employing both quantitative (log-likelihood and correlation metrics) and qualitative (novel question generation) analyses. The inclusion of alternative model variants highlights the importance of features like complexity and informativeness in human question generation. Furthermore, the authors situate their work within the broader context of active learning and cognitive science, referencing relevant prior research.
Weaknesses:  
Despite its technical rigor, the paper lacks novelty in its core motivation. The task—modeling question generation in a constrained, unambiguous setting—is relatively simple and domain-specific. While the Battleship game provides a controlled environment, it limits the generalizability of the findings to more complex, real-world scenarios. Additionally, the model relies heavily on predefined domain knowledge and a hand-crafted grammar, raising questions about scalability to other domains. The paper does not address how the approach could be extended to handle natural language input directly, which is a critical limitation for practical applications. Lastly, while the authors emphasize informativeness, the generated questions often prioritize syntactic coherence over deeper semantic creativity.
Significance and Scope:  
The paper's contribution is incremental rather than groundbreaking. While it advances the understanding of human-like question generation, its constrained scope and reliance on a toy domain make it less relevant to the broader NeurIPS community. The ideas, though well-executed, are not sufficiently novel or impactful to significantly advance the state of the art in active learning or AI question generation.
Recommendation:  
Reject. While the paper is technically solid and provides interesting insights into question generation, its limited scope, lack of novelty, and constrained domain reduce its potential impact. The work may be better suited for a specialized conference in cognitive modeling or human-computer interaction rather than NeurIPS. 
Pros for Acceptance:  
- Clear and technically sound methodology.  
- Novel use of compositional grammars for question synthesis.  
- Thorough evaluation of model variants.  
Cons for Acceptance:  
- Limited novelty in motivation and task.  
- Constrained, domain-specific application.  
- Lack of scalability to natural language or complex domains.  
- Unlikely to interest the broader NeurIPS audience.