The paper introduces a framework for incorporating human feedback into reinforcement learning (RL) and applies it to the task of image captioning. The authors propose a hierarchical phrase-based RNN captioning model that integrates human feedback, which is simulated using a feedback network (FBN). The feedback includes scoring machine-generated captions and providing corrections, which are used to guide the RL-based optimization process. The authors claim that natural language feedback provides a richer learning signal than numerical rewards, addressing issues like credit assignment and exploration in RL. Experiments on the MS-COCO dataset demonstrate the approach, with results showing modest improvements in BLEU-4 and other metrics.
Strengths  
1. Motivated Problem: The paper tackles an important challenge in AIâ€”enabling non-experts to guide learning agents through natural language feedback. This is a relevant and timely topic, particularly for applications like household robots and personal assistants.  
2. Integration of Feedback: The hierarchical phrase-based RNN model is well-suited for incorporating feedback at a granular level, such as individual phrases. The feedback network (FBN) is a thoughtful addition that leverages detailed human corrections to refine the RL process.  
3. Crowdsourcing Feedback: The use of a web interface to collect human feedback at scale is a practical contribution that could benefit future research. The authors also provide their code and data, which supports reproducibility.  
Weaknesses  
1. Limited Novelty: While the integration of human feedback into RL is an interesting application, the novelty is limited. The two-stage batch mode feedback collection and reliance on a feedback simulator are standard practices in RL. The approach does not significantly advance the state of the art in human-in-the-loop RL.  
2. Performance: The reported improvements over baseline models are minimal (<0.5% in BLEU-4), and the authors do not provide statistical significance tests to validate these gains. The performance remains far from state-of-the-art on the MS-COCO benchmark.  
3. Lack of Analysis: The paper lacks detailed qualitative analysis or examples to illustrate how feedback improves the model or addresses specific error patterns. This makes it difficult to assess the practical impact of the proposed framework.  
4. Clarity: The paper is dense and could benefit from better organization. For instance, the experimental results are difficult to parse, and the connection between the feedback network and the RL optimization process could be more clearly explained.  
Pro and Con Arguments for Acceptance  
- Pro: The paper addresses a relevant problem, proposes a practical framework, and provides open-source resources.  
- Con: The novelty and performance improvements are limited, and the lack of detailed analysis weakens the scientific contribution.  
Recommendation  
While the paper presents a practical application of human feedback in RL, the limited novelty, modest performance improvements, and lack of in-depth analysis make it a borderline submission. I recommend rejection but encourage the authors to refine their approach, conduct more thorough analysis, and explore ways to achieve more significant improvements.