The paper presents a unified framework for model interpretability, SHAP (SHapley Additive exPlanations), which leverages Shapley values from cooperative game theory to provide a theoretically grounded approach to feature importance attribution. By identifying a new class of additive feature attribution methods, the authors unify six existing methods and demonstrate that SHAP values are the unique solution in this class satisfying local accuracy, missingness, and consistency properties. The introduction of Kernel SHAP, a novel method for efficiently approximating SHAP values using weighted linear regression, is a significant contribution, as it improves computational efficiency and reduces variance compared to classical Shapley sampling. The paper also explores model-specific approximations, such as Deep SHAP, which adapts DeepLIFT to better align with Shapley values.
Strengths:
The paper makes a novel contribution by formalizing a theoretical framework that unifies existing methods and introduces SHAP values as a principled solution. This is a meaningful step forward in the field of model interpretability, addressing the ad-hoc nature of prior methods. The proposed Kernel SHAP method is both innovative and practical, offering improved computational efficiency and accuracy. The authors also demonstrate the impact of their work by influencing improvements in methods like DeepLIFT. The experiments, including user studies and computational benchmarks, provide empirical evidence supporting SHAP's superiority in terms of consistency with human intuition and sample efficiency. The paper's focus on desirable properties (local accuracy, missingness, and consistency) is well-motivated and aligns with the broader goals of interpretable machine learning.
Weaknesses:
Despite its strengths, the paper has several technical and presentation issues. The Max SHAP algorithm is flawed and tested in limited scenarios, raising concerns about its correctness and generalizability. Additionally, the proofs for Kernel SHAP in the supplementary material are incomplete and lack clarity, which undermines the theoretical rigor of the work. The manuscript contains numerous typos and inconsistencies, such as conflicting runtime claims for Max SHAP and missing parentheses in equations, which detract from its readability. The claim that SHAP values are a universally superior form of explanation is contentious, as the examples provided are biased toward SHAP, and counterexamples are not discussed. The lack of a direct comparison with LIME in Figure 5 weakens the argument for Kernel SHAP's superiority. Furthermore, the paper does not adequately address the runtime trade-offs or provide guidance on the recommended number of function evaluations for Kernel SHAP, which is critical for practical adoption.
Recommendation:
The paper makes a significant contribution to the field by adapting Shapley values for model interpretation and introducing Kernel SHAP. However, the issues with incomplete proofs, algorithmic errors, and presentation flaws need to be addressed. I recommend acceptance with major revisions, contingent on the authors providing a corrected Max SHAP algorithm, completing the proofs for Kernel SHAP, and improving the clarity and consistency of the manuscript. A more balanced discussion of SHAP's limitations and a direct comparison with LIME would also strengthen the paper. 
Pros for acceptance:
- Novel theoretical framework unifying existing methods.
- Introduction of Kernel SHAP with improved computational efficiency.
- Significant impact on existing methods like DeepLIFT.
- Strong empirical results demonstrating SHAP's advantages.
Cons for acceptance:
- Flawed Max SHAP algorithm and incomplete proofs.
- Typos, inconsistencies, and unclear explanations.
- Contentious claims and lack of balanced comparisons.
- Missing runtime analysis for practical applicability.