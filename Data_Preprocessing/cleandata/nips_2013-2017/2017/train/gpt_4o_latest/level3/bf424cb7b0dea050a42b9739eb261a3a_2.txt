This paper investigates the limitations of gradient descent (GD) as a learning strategy for kernel methods and proposes a novel preconditioning method, EigenPro, to address these shortcomings. The authors introduce the concept of "computational reach" to demonstrate that GD with smooth kernels can only approximate a small fraction of the function space, leading to over-regularization. EigenPro leverages approximate second-order information through a low-rank preconditioning strategy to accelerate convergence while maintaining compatibility with stochastic gradient descent (SGD). The method is evaluated on standard datasets, showing systematic improvements in computational efficiency and accuracy compared to existing kernel methods. However, the paper does not evaluate the statistical significance of these performance improvements.
Strengths:
1. Novelty and Insight: The paper provides a fresh perspective on the limitations of GD for kernel methods by introducing the concept of computational reach. This is a significant theoretical contribution that deepens our understanding of the interplay between optimization and kernel smoothness.
2. Practical Contribution: EigenPro is a practical and computationally efficient solution to the identified limitations. Its compatibility with SGD and low computational overhead make it highly relevant for large-scale kernel learning.
3. Experimental Validation: The experiments are well-designed and demonstrate substantial acceleration (up to 35x) and improved accuracy across multiple datasets. The comparison with state-of-the-art methods highlights the practical utility of EigenPro.
4. Clarity: The paper is well-written, with clear explanations of both theoretical and practical aspects. The inclusion of algorithmic details and open-source code enhances reproducibility.
Weaknesses:
1. Lack of Statistical Significance Analysis: While the experimental results are promising, the paper does not provide statistical significance testing to validate the observed performance improvements. This omission weakens the empirical claims.
2. Limited Discussion of Trade-offs: The paper briefly mentions the computational overhead of EigenPro but does not thoroughly analyze the trade-offs between preconditioning cost and overall efficiency gains.
3. Scope of Evaluation: The datasets used for evaluation are standard benchmarks, but additional experiments on more diverse or domain-specific datasets could strengthen the generalizability of the results.
Arguments for Acceptance:
- The paper addresses a critical limitation of GD for kernel methods and provides a well-motivated, theoretically sound, and practically effective solution.
- The proposed EigenPro method is novel and demonstrates significant improvements over state-of-the-art methods.
- The work is well-written and provides valuable insights into large-scale kernel learning.
Arguments Against Acceptance:
- The lack of statistical significance testing raises questions about the robustness of the reported improvements.
- The evaluation could be more comprehensive, particularly in terms of datasets and trade-off analysis.
Recommendation:
Overall, this paper makes a strong theoretical and practical contribution to the field of kernel learning. While the lack of statistical significance testing is a notable weakness, it does not overshadow the paper's strengths. I recommend acceptance, with a suggestion to include statistical significance analysis and a more detailed discussion of trade-offs in the final version.