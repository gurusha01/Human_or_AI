Review
This paper proposes a novel approach to image captioning by incorporating human natural language feedback into the training process, alongside traditional ground-truth captions. The authors combine supervised learning with reinforcement learning (RL), leveraging a hierarchical phrase-based RNN model and a feedback network (FBN) to process and integrate human-provided feedback. The model is evaluated on the MSCOCO dataset, demonstrating improvements over supervised learning and marginal gains over RL without feedback.
Strengths:
1. Motivation and Novelty: The idea of using human-in-the-loop training for image captioning is well-motivated and addresses a critical challenge in enabling non-expert users to guide learning agents. The use of natural language feedback, rather than scalar rewards, is a significant step toward more intuitive human-AI interaction.
2. Experimental Setup: The paper includes reasonable baselines and insightful experiments, comparing feedback captions against ground-truth captions. The hierarchical phrase-based RNN model is well-suited for integrating feedback, and the feedback network is a creative addition to the RL framework.
3. Potential Impact: If the approach can be further refined, it has the potential to significantly advance human-in-the-loop learning paradigms and improve the practicality of image captioning systems in real-world scenarios.
4. Dataset and Code: The authors provide a detailed description of their crowd-sourced feedback collection process and commit to releasing their data and code, which will facilitate reproducibility and further research.
Weaknesses:
1. Feedback Definition and Clarity: The feedback includes structured information (e.g., mistake types, corrections) that goes beyond natural language. This deviates from the claim of using purely natural language feedback and requires clearer explanation and justification.
2. Marginal Improvements: The improvements over RL without feedback are relatively small (e.g., 0.5â€“1.1 BLEU points), raising concerns about the statistical significance of the results. A more robust analysis of the gains is needed.
3. Ablation Study: The paper lacks an ablation study to disentangle the contributions of structured feedback components (e.g., mistake types) versus natural language feedback alone. This is critical to understanding the true impact of the proposed method.
4. Clarity Issues: The integration of feedback into the model is not clearly described, with ambiguous equations and insufficient rationale for certain design choices. For example, the reward weighting scheme and the role of the feedback network could be better justified.
5. Dataset Usage: The authors do not use the official MSCOCO test set for reporting results, which may limit the comparability of their findings with prior work.
6. Performance Degradation: The observed degradation in performance when additional feedback information (e.g., redundant phrases) is used needs further explanation.
7. Metric Anomaly: The low ROUGE-L score in Table 6 is unexplained and may indicate a typo or an inconsistency in the evaluation.
8. Failure Cases: The paper does not discuss failure cases or limitations of the proposed model, which would provide valuable insights for future research.
9. Typos and Errors: Multiple minor errors and typos detract from the overall clarity and polish of the paper.
Recommendation:
While the paper presents an interesting and novel approach, the marginal improvements, lack of clarity in feedback integration, and missing ablation studies weaken its overall contribution. The idea of using natural language feedback is compelling, but the reliance on structured feedback components and the limited analysis of their impact raise questions about the generalizability of the approach. Additionally, the lack of statistical significance tests and the absence of failure case discussions make it difficult to fully assess the robustness of the method.
Arguments for Acceptance:
- Novel and well-motivated idea with potential for significant impact.
- Reasonable baselines and insightful comparisons.
- Promises to release data and code, aiding reproducibility.
Arguments Against Acceptance:
- Marginal improvements over RL without feedback.
- Lack of clarity and missing ablation studies.
- Concerns about statistical significance and dataset usage.
Final Rating:
Marginally below the acceptance threshold. The paper would benefit from clearer explanations, additional experiments, and a stronger demonstration of the significance of its contributions.