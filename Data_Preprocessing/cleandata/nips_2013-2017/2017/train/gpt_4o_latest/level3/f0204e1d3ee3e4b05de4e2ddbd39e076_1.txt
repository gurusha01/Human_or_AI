The paper presents a novel framework for addressing the limitations of traditional A/B testing by integrating multi-armed bandit (MAB) algorithms with online false discovery rate (FDR) control. This approach is particularly relevant in scenarios where sequential experiments are conducted, such as clinical trials or website optimization, and where controlling false alarms while maintaining statistical power is critical. The authors propose a "doubly-sequential" framework that leverages adaptive sampling for efficient hypothesis testing and introduces always-valid p-values for continuous monitoring. The paper also provides theoretical guarantees for FDR control and power, supported by extensive simulations and real-world experiments.
Strengths:
1. Significance and Novelty: The problem of balancing adaptive experimentation with FDR control is a well-known challenge in statistical literature. The proposed framework is innovative and addresses this gap effectively, combining two previously disparate methodologies.
2. Theoretical Rigor: The paper provides clear theoretical guarantees for both FDR control and power, which are critical for establishing the validity of the proposed framework.
3. Practical Relevance: The application of the framework to real-world data, such as the New Yorker Cartoon Caption contest, demonstrates its practical utility and adaptability.
4. Efficiency: The use of MAB algorithms significantly reduces sample complexity compared to traditional A/B testing, as shown in the experimental results.
5. Potential for Future Research: The framework is modular, allowing for improvements in either MAB algorithms or FDR procedures to enhance overall performance, thus opening avenues for further exploration.
Weaknesses:
1. Clarity: While the paper is technically sound, certain concepts, such as "truncation time" and its impact on results, require clearer explanation. Additionally, the motivation in Lines 288-291 should be moved to the introduction for better contextual alignment.
2. Citations: The paper lacks citations for some well-known issues, such as those discussed in the Villar, Bowden, and Wason survey. Including these would strengthen the connection to existing literature.
3. Typographical Errors: Several minor errors, such as "testing multiple literature" instead of "multiple testing literature," "and and," "samplesas," and "are ran," detract from the overall polish of the manuscript.
4. Comparison with Platform Trials: The authors should clarify how their framework differs from "platform trials" in biometrics, as this distinction is not immediately clear.
5. LUCB Reference: A detailed reference and description of the LUCB algorithm are missing, which may hinder understanding for readers unfamiliar with this method.
Recommendation:
This paper is a strong candidate for acceptance, given its significant contribution to the field and its potential to inspire future research. However, the authors should address the weaknesses outlined above to improve the clarity and completeness of the manuscript. Specifically, they should:
1. Add citations to relevant literature, including biometrics references.
2. Correct typographical errors and improve the explanation of key terms like "truncation time."
3. Clarify the distinction from platform trials and provide a detailed reference for LUCB.
Arguments for Acceptance:
- Innovative and significant contribution to adaptive testing and FDR control.
- Strong theoretical and experimental support for the proposed framework.
- High practical relevance and potential for future research.
Arguments Against Acceptance:
- Minor clarity issues and typographical errors.
- Missing citations and insufficient differentiation from related methodologies.
Overall, the paper is a high-quality scientific contribution and aligns well with the conference's scope. With minor revisions, it will be an excellent addition to the proceedings.