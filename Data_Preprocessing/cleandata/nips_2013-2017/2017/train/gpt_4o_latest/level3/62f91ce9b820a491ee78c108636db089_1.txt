This paper presents a novel approach to deterministic feature maps for kernel machines, offering an alternative to the widely used random Fourier features (RFF). The authors leverage deterministic quadrature methods, such as Gaussian quadrature and sparse grid quadrature, to approximate kernels in the frequency domain. Their theoretical contributions include bounds on sample complexity for deterministic maps, particularly for sparse ANOVA kernels, which are shown to outperform RFF in terms of scaling with the desired error. Experimental results on MNIST and TIMIT datasets validate the practical utility of the proposed methods, demonstrating comparable or improved performance over RFF while reducing feature generation time.
Strengths:
1. Theoretical Contributions: The paper provides strong theoretical results, demonstrating that deterministic feature maps can achieve better scaling in terms of error compared to RFF. The analysis of sample complexity, particularly for sparse ANOVA kernels, is thorough and insightful.
2. Practical Relevance: The experiments on MNIST and TIMIT datasets show that deterministic methods can match or outperform RFF in accuracy while being computationally more efficient. The use of sparse ANOVA kernels, which resemble convolutional layers in CNNs, is particularly compelling.
3. Clarity and Organization: The paper is well-structured, with a clear progression from theoretical results to practical implementations. The authors provide sufficient background on kernels, quadrature methods, and related work, making the paper accessible to a broad audience.
4. Novelty: The deterministic approach to kernel approximation is a significant departure from the probabilistic guarantees of RFF, offering a new perspective on kernel methods.
Weaknesses:
1. Incomplete Implementation in Experiments: The experiments do not fully implement the proposed deterministic methods for which theoretical results are provided. For instance, higher-order QMC methods, which could address some of the pessimistic assessments on page 2, are not explored.
2. Overlooked Definitions: The Cartesian basis \( e_i \) is used without prior definition, which could confuse readers unfamiliar with the notation. Similarly, \( \tilde{\Omega} \) in Section 4 should be explicitly defined for clarity.
3. Comparison Caveats: The comparison to RFF on page 4 should be tempered with a caveat about the additional assumptions made for the novel deterministic results. This would provide a more balanced evaluation.
4. LaTeX Notation: The authors should use `\top` for transpose in LaTeX to maintain consistency and readability.
Suggestions for Improvement:
- Address the incomplete implementation of the proposed methods in experiments, particularly by exploring higher-order QMC methods.
- Define all symbols and notations before their usage, including \( e_i \) and \( \tilde{\Omega} \).
- Add a caveat to the comparison with RFF, acknowledging the additional assumptions required for deterministic methods.
- Revise the LaTeX formatting to use `\top` for transpose.
Recommendation:
This paper makes a significant contribution to the field of kernel methods by introducing deterministic feature maps with strong theoretical guarantees and practical benefits. However, the incomplete experimental implementation and minor clarity issues slightly detract from its overall impact. I recommend acceptance with minor revisions to address the noted weaknesses.