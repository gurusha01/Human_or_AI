The paper introduces a probabilistic pre-processing framework to mitigate algorithmic discrimination while balancing individual distortion and utility preservation. The authors formulate a convex optimization problem that explicitly incorporates constraints for discrimination control, distortion control, and utility preservation. This novel approach is benchmarked on two datasets, COMPAS and UCI Adult, and compared against baseline methods and the Learning Fair Representations (LFR) algorithm.
Strengths:
1. Novelty: The paper presents a principled probabilistic framework for pre-processing, which is a significant departure from existing heuristic-based methods. The explicit incorporation of individual fairness through distortion constraints is a commendable contribution.
2. Flexibility: The framework supports multivariate and non-binary protected attributes, making it adaptable to diverse real-world scenarios.
3. Theoretical Rigor: The authors provide a detailed theoretical characterization, including convexity conditions, generalizability, and robustness to mismatched prior distributions.
4. Explicit Discrimination Control: The ability to directly set discrimination constraints is a key advantage over competing methods like LFR, which rely on indirect parameter tuning.
5. Practical Relevance: The paper addresses a critical issue in algorithmic fairness, and the proposed method is applicable across different classifiers and datasets.
Weaknesses:
1. Experimental Comparisons: The paper does not compare its method with in-processing or post-processing approaches, limiting the assessment of its practical utility relative to the broader landscape of fairness interventions.
2. Performance Trade-offs: The proposed method shows significant AUC losses on both datasets, raising concerns about its applicability in scenarios where predictive performance is critical.
3. Distortion Metric Sensitivity: The results are highly dependent on the choice of distortion metrics, which are dataset-specific and may require domain expertise to define effectively.
4. Extreme Shifts: The distortion constraint in expectation could lead to extreme feature shifts for certain individuals, potentially undermining fairness at the individual level.
5. Limited Fairness Notions: The focus on label distribution parity overlooks other fairness definitions, such as calibration or equalized odds, which should be discussed for a more comprehensive evaluation.
6. Constraint Feasibility: The interplay between distortion and discrimination constraints may lead to infeasible solutions, necessitating threshold relaxation, which is not explored in depth.
Arguments for Acceptance:
- The paper addresses a critical problem in algorithmic fairness with a novel and theoretically grounded approach.
- It provides explicit control over fairness and distortion, which is a unique contribution compared to existing methods.
- The flexibility of the framework allows for extensions and adaptations to various fairness scenarios.
Arguments against Acceptance:
- The significant AUC losses and lack of comparisons with in-processing and post-processing methods limit the practical impact of the proposed approach.
- The reliance on dataset-specific distortion metrics and potential for extreme feature shifts raise concerns about generalizability and fairness at the individual level.
- The omission of discussions on broader fairness notions and constraint feasibility weakens the comprehensiveness of the work.
Recommendation:
While the paper makes a valuable theoretical contribution, its practical utility is undermined by performance trade-offs and limited experimental comparisons. I recommend acceptance only if the authors address these concerns, particularly by including comparisons with in-processing and post-processing methods and providing a more detailed discussion on fairness notions and constraint feasibility.