This paper presents a novel deep supervised discrete hashing (DSDH) algorithm that integrates pairwise similarity and classification information into a single-stream framework for learning binary hash codes. The authors propose a unique loss function (Eq. 6) that combines these two components, aiming to optimize both retrieval and classification performance. The method directly constrains the outputs of the final layer to binary codes, addressing the quantization challenge inherent in hashing. An alternating minimization approach is employed to optimize the discrete objective function, ensuring the binary nature of the codes is preserved. Experimental results demonstrate the superiority of the proposed method over state-of-the-art approaches on benchmark datasets like CIFAR-10 and NUS-WIDE.
Strengths:
1. Novelty: The paper introduces a unique approach by combining pairwise similarity and classification information in a single-stream framework, which is a significant departure from the traditional two-stream methods. This integration is well-motivated and supported by the hypothesis that binary codes should be ideal for classification.
2. Comprehensive Experiments: The authors conduct extensive experiments on two benchmark datasets, exploring the impact of label information and comparing the proposed method with both traditional and deep hashing methods. The results consistently demonstrate the superiority of the proposed approach.
3. Technical Soundness: The alternating minimization strategy for optimizing the discrete objective function is well-justified, and the derivation of gradients and updates is detailed and clear.
4. Significance: The proposed method achieves state-of-the-art performance, particularly on CIFAR-10, and demonstrates robustness under different experimental settings, including scenarios with limited training data.
Weaknesses:
1. Comparison with Related Work: While the paper compares its method with numerous baselines, a detailed comparison with K. Lin et al.'s work (which also employs deep hashing) in terms of network structure and performance is missing. This would provide a clearer context for the contributions of this work.
2. Classifier Design: The use of a linear classifier in Eq. (4) may limit the expressiveness of the model. Exploring a softmax classifier could potentially enhance the classification performance and align better with standard deep learning practices.
3. Clarity: While the technical content is rigorous, the paper could benefit from improved organization and clearer explanations in some sections, particularly in the optimization and experimental setup. For instance, the rationale behind specific parameter choices (e.g., µ, ν, η) is not thoroughly discussed.
4. Limited Multi-Label Analysis: The performance on the NUS-WIDE dataset, which involves multi-label classification, is less impressive compared to CIFAR-10. The authors acknowledge this limitation but do not propose potential solutions or future directions to address it.
Recommendation:
Overall, this paper makes a strong contribution to the field of deep hashing for image retrieval, introducing a novel framework and demonstrating its effectiveness through extensive experiments. However, addressing the aforementioned weaknesses, particularly the comparison with related work and the exploration of a softmax classifier, would further strengthen the paper. I recommend acceptance with minor revisions. 
Pros:
- Novel loss function combining pairwise similarity and classification.
- Strong experimental results and comprehensive evaluation.
- Technical rigor in optimization and discrete constraints.
Cons:
- Missing detailed comparison with K. Lin et al.
- Potential limitations of the linear classifier.
- Limited clarity in some sections.