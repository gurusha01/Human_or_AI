This paper introduces a novel supervised hashing algorithm, Deep Supervised Discrete Hashing (DSDH), which leverages a neural network to generate binary hash codes while minimizing classification error. The authors propose a one-stream framework that integrates pairwise label information and classification information, a notable departure from the two-stream frameworks commonly used in prior work. The binary nature of hash codes is preserved during optimization, and an alternating minimization strategy is employed to optimize the objective function. The method is evaluated on two benchmark datasets, CIFAR-10 and NUS-WIDE, demonstrating superior performance compared to both traditional and deep learning-based hashing methods.
Strengths:
1. Technical Soundness: The paper is technically solid, presenting a clear and well-motivated objective function that combines pairwise similarity and classification loss. The alternating minimization strategy is appropriate for handling the discrete nature of hash codes.
2. Novelty: The integration of pairwise label information and classification information into a single-stream framework is innovative and addresses limitations in prior methods that separate these components.
3. Experimental Results: The method achieves state-of-the-art performance on two datasets, with significant improvements over traditional and deep hashing methods. The results convincingly demonstrate the effectiveness of the proposed approach.
4. Clarity: The paper is well-written and organized, with a thorough explanation of the methodology and experimental setup.
Weaknesses:
1. Missing References: The paper fails to discuss and compare its approach with [1], which is cited but not elaborated upon. This omission weakens the contextualization of the work within the broader literature.
2. Reproducibility: The lack of training details, such as hyperparameter settings, and the absence of publicly available code hinder reproducibility. Providing these details or releasing the code would strengthen the paper's contribution.
3. Convergence Analysis: Given the alternating optimization strategy, the authors should include loss curves to demonstrate convergence effectiveness. This would provide additional confidence in the robustness of the optimization process.
4. Figure 1 Inconsistencies: The results and explanations in Figure 1 (Ln 229-233) are inconsistent and require clarification. Additional experiments or analysis are needed to validate the observed trends.
5. Performance Differences Among Variants: The performance differences between DSDH variants (e.g., DSDH-B vs. DSDH-A/C) are not adequately explained. For instance, the superior performance of DSDH-B with more bits in Figure 1(a) is unclear and warrants further investigation.
Recommendation:
This paper makes a strong contribution to the field of deep hashing for image retrieval, with a novel approach and compelling results. However, the missing references, lack of training details, and unclear explanations of certain results are significant weaknesses. I recommend acceptance with minor revisions, contingent on addressing the issues of missing references, reproducibility, and clarification of experimental results. These improvements would enhance the paper's impact and utility for the research community.
Pros:
- Innovative one-stream framework.
- Strong empirical performance.
- Clear and well-structured presentation.
Cons:
- Missing references and comparisons.
- Insufficient training details for reproducibility.
- Lack of convergence analysis and unclear experimental results.