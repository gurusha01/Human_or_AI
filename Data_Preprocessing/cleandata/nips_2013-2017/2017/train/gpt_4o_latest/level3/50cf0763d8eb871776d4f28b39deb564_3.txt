This paper presents a theoretical analysis of generative and discriminative models in high-dimensional settings, focusing on their convergence rates and introducing a novel notion of separability for loss functions. While the work builds on classical analyses, such as those by Ng and Jordan (2001), it extends the scope to exponential families and high-dimensional regimes, which are increasingly relevant in modern machine learning. The authors' framework for obtaining \( l_{\infty} \) convergence rates for M-estimators is a notable contribution, as is the exploration of how separability influences sample complexity.
Strengths:
1. Theoretical Contributions: The paper provides a robust theoretical framework for analyzing \( l{\infty} \) and \( l2 \) convergence rates, offering insights into the nuanced behaviors of generative and discriminative models in high dimensions. The introduction of separability as a flexible notion is innovative and has the potential to influence future work in this area.
2. Clarity in Low-Dimensional Results: The analysis of isotropic Gaussian models and Gaussian MRFs is clear and well-supported, with the results aligning with intuition about the advantages of generative models under certain conditions.
3. Practical Implications: The findings on sample complexity and the role of separability provide actionable insights for practitioners deciding between generative and discriminative approaches in high-dimensional settings.
4. Experimental Validation: The experiments corroborate the theoretical findings, particularly the advantages of generative models in terms of sample complexity and robustness to sparsity.
Weaknesses:
1. Clarity and Accessibility: Beyond the introduction and background sections, the paper becomes dense and difficult to follow. The definitions of separability (Definitions 1 and 2) lack clear motivation and intuitive explanations, making it challenging for readers to grasp their significance.
2. Notation Issues: Unexplained notation, such as "\(\succsim\)" starting from Corollary 3, hinders readability. A glossary or more detailed explanations would improve accessibility.
3. Dimensional Dependence Confusion: The comparison of Corollaries 3 and 4 is confusing due to seemingly inconsistent claims about dimensional dependence. This discrepancy needs clarification to avoid misinterpretation.
4. Lack of Practical Takeaways: While the theoretical contributions are strong, the paper does not provide clear guidance on when to prefer generative over discriminative models in practice, especially in non-idealized settings.
5. Minor Issues: Typographical and clarity issues (e.g., lines 36, 65, 87, 259, and 254-265) detract from the overall presentation.
Pro and Con Arguments for Acceptance:
- Pros:
  - Strong theoretical contributions with novel insights into separability and convergence rates.
  - Relevance to high-dimensional machine learning problems.
  - Experimental validation supports theoretical claims.
- Cons:
  - Dense and inaccessible presentation limits its impact.
  - Lack of clear practical takeaways.
  - Confusion in dimensional dependence analysis.
Recommendation:
While the paper makes significant theoretical contributions, its clarity and practical relevance need improvement. I recommend conditional acceptance provided the authors address the clarity issues, improve the motivation for separability, and resolve the confusion in dimensional dependence claims. This work has the potential to be impactful, but its current form limits its accessibility to a broader audience.