This paper presents a significant extension to the theoretical understanding of reinforcement learning (RL) with options by addressing key limitations in prior work, particularly Fruit & Lazaric (2017). By relaxing assumptions on primitive options and the distributions of sojourn times and cumulative returns, the authors introduce a novel parameter-free algorithm, FSUCRL, which leverages the intrinsic Markov Decision Process (MDP) structure within options rather than relying solely on the semi-MDP (SMDP) abstraction. This approach models each option as an irreducible Markov chain with a random horizon defined by its termination function, enabling the computation of optimistic policies through stationary distributions and their confidence bounds.
The paper is technically rigorous, with regret bounds derived for FSUCRL that match SUCRL's bounds up to an additive term. The authors provide theoretical insights into the challenges of learning long or complex options and demonstrate how sparse transition models can improve learning efficiency. The empirical results, though preliminary, support the theoretical findings, showing that FSUCRL retains the advantages of temporal abstraction while removing the need for strong prior knowledge about options. The paper is well-written and accessible, making it a valuable resource for researchers with varying levels of expertise in RL.
Strengths:
1. Novelty: The paper introduces a fresh perspective by combining the SMDP and MDP views, addressing a critical gap in prior work.
2. Theoretical Contributions: The regret analysis is thorough, and the introduction of pseudo-diameter and stationary distribution-based confidence bounds is innovative.
3. Practical Relevance: By removing the need for prior knowledge about option parameters, FSUCRL is more applicable to real-world scenarios, especially in settings with automatically generated options.
4. Clarity: Despite the technical depth, the paper is well-organized and accessible, with clear explanations of assumptions, algorithms, and theoretical results.
Weaknesses:
1. Discounted Reward Setting: The analysis does not explicitly address the discounted reward setting, which is more common in practice. Extending the framework to this setting would enhance its applicability.
2. Compositionality of Assumptions: Assumption 1's compositionality and general applicability are not fully explored, raising questions about the robustness of the framework in diverse environments.
3. Correlation Between Options: The discussion on the correlation between options (line 251) is underdeveloped, leaving a gap in understanding how overlapping options influence learning efficiency.
Arguments for Acceptance:
- The paper makes a substantial theoretical contribution to RL with options, advancing the state of the art.
- It addresses practical limitations of prior algorithms, making it relevant for both theoretical and applied researchers.
- The empirical results, while limited, are promising and align with the theoretical claims.
Arguments Against Acceptance:
- The lack of analysis for the discounted reward setting limits the paper's practical scope.
- Some assumptions and statements (e.g., compositionality and correlation) require further elaboration to strengthen the paper's claims.
Conclusion:
This paper is a strong candidate for acceptance due to its theoretical contributions, practical relevance, and clear exposition. Addressing the highlighted weaknesses in future work would further solidify its impact on the RL community.