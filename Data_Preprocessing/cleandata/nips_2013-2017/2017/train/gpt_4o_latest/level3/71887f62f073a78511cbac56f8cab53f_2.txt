The paper introduces a novel smooth primal-dual randomized coordinate descent method (SMART-CD) to solve a three-composite convex optimization problem involving smooth, non-smooth, and general non-smooth functions. This work generalizes prior methods, such as those by Fercoq, Richtarik, and Qu, by addressing the additional complexity of non-smooth functions composed with a linear operator. The authors achieve the best-known convergence rate of \(O(n/k)\), which is a significant theoretical contribution to the field of coordinate descent methods.
Strengths:
The paper is technically sound and provides rigorous theoretical analysis, including convergence guarantees for the proposed method. The authors effectively combine several advanced techniques—smoothing, acceleration, homotopy, and non-uniform sampling—into a cohesive framework. The inclusion of restart schemes to enhance practical performance is a thoughtful addition, and the numerical experiments demonstrate the method's applicability to real-world problems, such as total variation regularization, support vector machines, and degenerate linear programs. The paper is well-written, with clear organization and detailed explanations of the algorithm and its implementation. Its broad applicability to various machine learning and optimization problems makes it a valuable contribution to the field.
Weaknesses:
While the paper excels in many areas, some aspects require clarification. The homotopy and acceleration techniques, though mentioned, are not explained in sufficient detail, leaving gaps in understanding their practical implementation. Additionally, the origin of certain formulas (e.g., \(\tau\) in line 9 and line 153) and initialization values (\(\hat{x}, \tilde{x}, \bar{x}, \dot{y}\)) is unclear and should be elaborated upon. The presence of an unknown reference in line 165 is a minor issue but should be addressed for completeness. Lastly, while the numerical experiments are comprehensive, the paper could benefit from a more detailed comparison with state-of-the-art methods in terms of computational efficiency and scalability.
Recommendation:
The paper makes a significant theoretical and practical contribution to the field of convex optimization and randomized coordinate descent methods. Despite some minor issues with clarity and missing details, the strengths of the work outweigh its weaknesses. The convergence rate guarantees, efficient implementation, and broad applicability of the proposed method make it a valuable addition to the literature. Post-rebuttal, the authors have addressed some of the concerns raised, further solidifying the case for acceptance.
Arguments for Acceptance:
- Novel and generalizable method with rigorous theoretical guarantees.
- Best-known convergence rate for the problem class.
- Broad applicability to machine learning and optimization problems.
- Well-written and organized presentation of the algorithm and experiments.
Arguments Against Acceptance:
- Insufficient explanation of homotopy and acceleration techniques.
- Missing details on certain formulas and initialization values.
- Minor issues with referencing and clarity.
Overall, I recommend acceptance of this paper, as it advances the state of the art in randomized coordinate descent methods and provides a solid foundation for future research.