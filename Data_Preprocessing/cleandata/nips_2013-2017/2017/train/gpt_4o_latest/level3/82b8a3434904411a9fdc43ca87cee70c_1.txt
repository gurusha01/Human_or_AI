This paper introduces a novel stochastic optimization algorithm, termed Stochastic MISO (S-MISO), designed to address variance reduction in scenarios where input data is randomly perturbed (e.g., through rotations, scalings, or noise). The work extends variance reduction techniques, which have been traditionally limited to finite-sum problems, to the more general setting of composite and strongly convex objectives with stochastic perturbations. The authors build on the MISO/Finito framework by leveraging moving averages of approximate quadratic lower bounds, achieving a convergence rate that depends only on the variance due to perturbations (\(\sigma_{\rho}\)) rather than the overall variance, as in SGD. This distinction is significant, as it enables faster convergence in scenarios where perturbation-induced variance is small compared to sampling noise.
Strengths
1. Theoretical Contribution: The paper provides a rigorous theoretical analysis of S-MISO, including a convergence proof and complexity analysis. The results demonstrate that the algorithm achieves a faster convergence rate than SGD, particularly in low-variance perturbation settings.
2. Practical Relevance: The algorithm addresses an important problem in machine learning, where data augmentation and perturbations are widely used to improve generalization and robustness. The proposed method is intuitive and bridges the gap between finite-sum variance reduction methods and stochastic approximation techniques.
3. Experimental Validation: The experimental results convincingly validate the theoretical claims. S-MISO consistently outperforms SGD and N-SAGA across diverse tasks, including image classification with data augmentation and Dropout regularization on gene expression and text datasets. The experiments also highlight the algorithm's adaptability to different perturbation scenarios.
4. Incremental but Impactful: While the algorithm is an incremental extension of MISO/Finito, it addresses a previously unexplored setting, making it a valuable contribution to the field.
Weaknesses
1. Plot Readability: The experimental plots suffer from poor readability due to small font sizes, low contrast, and reliance on color alone for curve differentiation. This makes it challenging to interpret the results, particularly for readers with visual impairments or when printed in grayscale.
2. Memory Requirements: The algorithm requires storing additional vectors proportional to the dataset size, which may limit its applicability to very large-scale datasets. This limitation is acknowledged but could have been explored further with alternative strategies (e.g., memory-efficient approximations).
3. Limited Exploration of Alternatives: While the paper discusses related methods like SVRG and SAGA, it does not thoroughly investigate whether these methods could be adapted to the stochastic perturbation setting with appropriate modifications.
Recommendation
I recommend acceptance of this paper. Its strong theoretical foundation, practical relevance, and convincing experimental results make it a valuable contribution to the field of stochastic optimization. However, the authors should address the readability issues in the plots and consider discussing potential extensions to mitigate memory constraints.
Arguments for Acceptance
- Strong theoretical contributions with clear advancements over SGD.
- Practical relevance to widely used machine learning techniques like data augmentation and Dropout.
- Convincing experimental validation across diverse datasets and tasks.
Arguments Against Acceptance
- Poor plot readability detracts from the clarity of experimental results.
- Limited scalability to extremely large datasets due to memory requirements.
In summary, this paper advances the state of the art in variance reduction for stochastic optimization and is likely to inspire future research in this area.