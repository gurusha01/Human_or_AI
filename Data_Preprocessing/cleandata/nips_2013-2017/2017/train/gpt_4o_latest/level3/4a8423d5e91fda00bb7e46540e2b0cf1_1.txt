The paper presents a novel method for adding selective classification capabilities to pre-trained deep neural networks (DNNs), enabling risk-controlled predictions by rejecting uncertain instances. The approach leverages confidence score thresholds, specifically analyzing two score functions: MC-dropout scores and maximum softmax scores (SR). Empirical results demonstrate the superiority of SR in most cases, particularly on large-scale datasets like ImageNet. The proposed method employs a binomial search algorithm (SGR) to identify thresholds that guarantee a desired error rate with high confidence, using a theoretical bound on true error rates (Lemma 3.1) and Bonferroni correction. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate the method's effectiveness, achieving tight alignment between desired and observed error rates while maintaining significant coverage.
Strengths:
1. Practical Applicability: The method is designed to work with pre-trained networks, making it highly practical for real-world applications without requiring retraining of the base model.
2. Theoretical Guarantees: The use of Lemma 3.1 and Bonferroni correction ensures robust error rate guarantees, which is critical for mission-critical applications like autonomous driving and medical diagnosis.
3. Empirical Validation: Comprehensive experiments on diverse datasets (CIFAR-10, CIFAR-100, ImageNet) demonstrate the method's effectiveness, with SR achieving superior performance in most scenarios.
4. Ease of Use: The ability to specify a desired error rate and confidence level provides users with intuitive control over the trade-off between accuracy and coverage.
5. Significance: The work addresses an important gap in selective classification for DNNs, with potential applications in high-stakes domains requiring controlled risk.
Weaknesses:
1. Baseline Comparisons: The paper lacks comparisons with simpler baselines beyond a rudimentary thresholding method. Including additional baselines, such as cost-sensitive rejection models or ensemble-based approaches, would strengthen the justification for the proposed method.
2. Algorithmic Clarity: Algorithm 1 references an uninitialized variable (r*), which could confuse readers and should be corrected for clarity and reproducibility.
3. Limited Exploration of MC-Dropout: While SR outperforms MC-dropout empirically, the paper does not deeply analyze why MC-dropout underperforms, particularly on ImageNet. A more detailed discussion could provide insights for future improvements.
4. Broader Applicability: The method is limited to classification tasks with 0/1 loss. Extending the approach to other loss functions, regression tasks, or specific error metrics (e.g., false positives/negatives) would enhance its utility.
Recommendation:
This paper makes a significant contribution to the field of selective classification for DNNs, offering a theoretically sound and empirically validated method with practical relevance. However, the lack of baseline comparisons and minor clarity issues slightly detract from its overall impact. I recommend acceptance with minor revisions, particularly addressing the uninitialized variable in Algorithm 1 and adding baseline comparisons to contextualize the method's performance.