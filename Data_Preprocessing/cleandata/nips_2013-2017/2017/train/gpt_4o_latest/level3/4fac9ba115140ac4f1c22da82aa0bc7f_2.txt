Review
This paper introduces Cost-Efficient Gradient Boosting (CEGB), a novel adaptation of gradient boosting that incorporates prediction cost penalties into the learning process. The proposed method is designed to construct deep regression trees that are computationally efficient on average, addressing the dual challenges of feature acquisition cost and tree evaluation cost. CEGB is evaluated against state-of-the-art methods such as GREEDYMISER and BUDGETPRUNE on multiple datasets, demonstrating superior performance in terms of accuracy-cost tradeoff. The authors provide source code, enhancing the reproducibility and practical utility of their approach.
Strengths  
1. Significance and Novelty: CEGB refines GREEDYMISER by allowing flexible splits at any leaf and incorporating second-order loss approximations. This flexibility enables the construction of deep, cost-efficient trees, which is a novel contribution in the cost-aware learning domain. The ability to optimize both feature acquisition and evaluation costs simultaneously is a meaningful advancement over prior methods.
2. Performance: The experimental results convincingly demonstrate that CEGB outperforms both GREEDYMISER and BUDGETPRUNE across several datasets, achieving better accuracy-cost tradeoffs. The method is particularly effective in scenarios where deep trees with low average evaluation costs are advantageous.
3. Implementation and Reproducibility: The algorithm is easy to implement using existing gradient boosting libraries, and the authors provide publicly available source code, which is commendable for fostering further research and adoption.
Weaknesses  
1. Dataset Scope: While CEGB shows strong results on the Yahoo! LTR and MiniBooNE datasets, the lack of evaluation on the Forest Covertype dataset raises concerns about the generalizability of the method. Including results on this dataset would strengthen the paper's claims.
2. Precision Improvement in Figure 2(b): The significant precision improvement observed in Figure 2(b) warrants further explanation. Specifically, the relationship between the number of trees, tree depth, and node structure should be clarified to ensure the results are interpretable and reproducible.
3. Misrepresentation of Related Work: The paper incorrectly claims that BUDGETPRUNE ignores evaluation costs, which contradicts its original formulation. This misrepresentation undermines the thoroughness of the related work discussion and should be corrected.
4. Limited Novelty: While CEGB introduces meaningful refinements to existing methods, the core approach is an incremental improvement over GREEDYMISER rather than a fundamentally new paradigm.
Arguments for Acceptance  
- CEGB offers a practical and effective solution to a significant problem in cost-aware learning, with demonstrated improvements over state-of-the-art methods.  
- The method is well-motivated, technically sound, and supported by extensive experimental results.  
- The availability of source code and ease of implementation make CEGB a valuable contribution to the field.
Arguments Against Acceptance  
- The paper's evaluation lacks breadth, as it omits the Forest dataset and does not explore additional diverse datasets to establish generalizability.  
- Some claims about related work are inaccurate, which could mislead readers.  
- The novelty is somewhat limited, as the approach builds incrementally on existing methods.
Recommendation: Accept with Minor Revisions  
The paper presents a solid contribution to the field of cost-efficient learning. However, the authors should address the concerns regarding dataset coverage, clarify the precision improvement in Figure 2(b), and correct the misrepresentation of related work. These revisions would enhance the paper's clarity, rigor, and impact.