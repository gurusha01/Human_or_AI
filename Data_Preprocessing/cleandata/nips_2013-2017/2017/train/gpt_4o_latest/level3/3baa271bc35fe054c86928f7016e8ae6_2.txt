The paper introduces a novel two-stage procedure for robust submodular maximization under a cardinality constraint, combining the robust streaming algorithm STAR-T and a simple greedy method (STAR-T-GREEDY). STAR-T employs a partitioning structure and an exponentially decreasing thresholding rule, achieving a constant-factor approximation guarantee as \( k \to \infty \). The authors provide theoretical proofs (Lemmas 4.2–4.3) to demonstrate STAR-T's robustness against element loss, and experimental results validate its effectiveness in influence maximization and personalized movie recommendation tasks. Notably, this is the first streaming algorithm for robust submodular maximization, with potential for broad applications.
Strengths:
1. Originality and Novelty: The paper addresses a challenging and practical problem in large-scale machine learning—robust submodular maximization in a streaming setting. The proposed algorithm, STAR-T, is novel and extends existing techniques like SIEVE-STREAMING with a robust partitioning structure and thresholding rule.
2. Theoretical Contributions: The authors rigorously prove the robustness of STAR-T and its approximation guarantees. The analysis is detailed and well-supported, with clear connections to prior work.
3. Practical Relevance: The algorithm is evaluated on real-world tasks (influence maximization and personalized movie recommendation), demonstrating its applicability to diverse domains.
4. Ease of Use: STAR-T's hyperparameters (\(\tau\) and \(w\)) are shown to be easy to approximate, with \(w=1\) performing well in experiments, simplifying its deployment.
5. Efficiency: The algorithm is memory-efficient, requiring \(O((k + m \log k) \log^2 k)\) space, and operates in a single pass over the data, making it suitable for large-scale applications.
Weaknesses:
1. Experimental Limitations: While STAR-T performs comparably to SIEVE-STREAMING in most cases, it does not consistently outperform it, especially when elements are removed adversarially. This raises questions about its practical advantage over existing methods.
2. Objective Function Discrepancy: The influence maximization task uses a different objective function than a referenced standard, which might limit the generalizability of the results to standard spread models.
3. Approximation Factor: The constant-factor approximation guarantee (\(c = 0.149\)) is relatively low, which could be a limitation in scenarios requiring higher accuracy.
4. Clarity of Presentation: While the theoretical analysis is thorough, the paper could benefit from a clearer explanation of the intuition behind the partitioning and thresholding mechanisms for readers unfamiliar with submodular optimization.
Pro and Con Arguments for Acceptance:
- Pro: The paper makes a significant theoretical and practical contribution to the field of submodular optimization, introducing the first robust streaming algorithm for this problem. Its potential applications in large-scale machine learning are compelling.
- Con: The experimental results, while promising, do not consistently demonstrate a clear advantage over existing methods like SIEVE-STREAMING. Additionally, the low approximation factor may limit its impact in certain applications.
Recommendation: Accept with minor revisions. The paper is a strong contribution to the field, but the authors should address the concerns regarding experimental comparisons and clarify the practical implications of the low approximation factor.