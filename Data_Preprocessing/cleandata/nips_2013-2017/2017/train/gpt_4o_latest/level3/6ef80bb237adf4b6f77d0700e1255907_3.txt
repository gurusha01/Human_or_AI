This paper presents a novel accelerated first-order method for geodesically convex optimization on Riemannian manifolds, extending Nesterov's accelerated gradient method from Euclidean to nonlinear Riemannian spaces. The authors provide theoretical guarantees for the proposed method, demonstrating linear convergence for μ-strongly G-convex functions and an improved O(1/k²) convergence rate for G-L-smooth functions. The paper also includes numerical experiments on the matrix Karcher mean problem to validate the method's efficiency.
Strengths:
1. Theoretical Contribution: The paper addresses a long-standing open question about the generalization of Nesterov's acceleration to Riemannian spaces. The authors derive two nonlinear operators to replace the linear extrapolation step in Euclidean space, providing rigorous global convergence analysis for both strongly G-convex and general G-convex cases.
2. Improved Convergence Rates: The proposed method achieves convergence rates comparable to the Euclidean counterpart, outperforming Riemannian gradient descent (RGD) in theory and experiments.
3. Clarity in Theoretical Development: The paper is well-organized, with clear derivations of the proposed algorithms and convergence proofs. The use of geometric interpretations (e.g., Figures 1a and 1b) aids understanding.
4. Potential for Generalization: The method is applicable to a wide range of geodesically convex problems, and the authors suggest future extensions, such as stochastic settings and non-smooth regularizations.
Weaknesses:
1. Choice of Application: The experimental results on the matrix Karcher mean problem are underwhelming. While the method outperforms RGD, it is significantly outpaced by higher-order methods like limited-memory Riemannian BFGS (LRBFGS) in terms of objective gap reduction (10³ vs. 10¹⁰ in 30 passes). This raises concerns about the practical utility of the method for well-conditioned smooth problems.
2. Experimental Validation: The experiments are limited to a single application (matrix Karcher mean), which is not ideal given the method's broader theoretical scope. A more compelling application, such as one involving a C¹ cost function, could better showcase the method's advantages, especially since it does not require C² smoothness.
3. Disputed Claims: The assertion that Bini's method, Riemannian GD, and limited-memory Riemannian BFGS perform similarly is questionable, particularly when using the Barzilai-Borwein (BB) step size in line search. This discrepancy warrants further investigation and clarification.
4. Computational Efficiency: Although the method converges faster than RGD in terms of iterations, its computational overhead per iteration (due to solving nonlinear equations) may limit its scalability for large-scale problems.
Recommendation:
While the paper makes a significant theoretical contribution by extending Nesterov's acceleration to Riemannian spaces, its practical impact is less convincing due to the choice of application and experimental results. To strengthen the paper, the authors should include additional experiments on diverse applications and address the performance gap with higher-order methods. Despite these limitations, the paper is a valuable contribution to the field of Riemannian optimization and merits acceptance, provided the authors address the concerns raised.
Arguments for Acceptance:
- Novel theoretical framework with rigorous convergence analysis.
- Advances the state of the art in Riemannian optimization.
- Clear and well-structured presentation.
Arguments against Acceptance:
- Limited and unconvincing experimental validation.
- Practical performance lags behind higher-order methods.
- Questionable claims about comparative performance with existing methods.
Overall, the paper is a strong theoretical contribution but requires additional work to solidify its practical relevance.