Review of the Paper
This paper presents a novel extension of Nesterov's accelerated gradient descent (AGD) to Riemannian spaces, addressing both geodesically strongly convex and general geodesically convex optimization problems. The authors propose two algorithms that replace the linear extrapolation step in Euclidean AGD with nonlinear operators tailored to Riemannian geometry. They provide theoretical convergence guarantees, achieving rates of \(O((1-\sqrt{\mu/L})^k)\) for strongly convex problems and \(O(1/k^2)\) for general convex problems, matching the optimal rates in Euclidean settings. The paper also demonstrates the practical utility of the proposed methods on the matrix Karcher mean problem, showing faster convergence compared to Riemannian gradient descent (RGD) and Riemannian stochastic gradient descent (RSGD).
Strengths  
1. Theoretical Contribution: The paper makes a significant theoretical contribution by generalizing the momentum term in AGD to Riemannian spaces, a longstanding open problem. The authors successfully maintain the optimal convergence rates, which is a non-trivial accomplishment given the geometric complexities of Riemannian manifolds.  
2. Practical Relevance: The application to the matrix Karcher mean problem demonstrates the practical utility of the proposed algorithms. The empirical results validate the theoretical claims, showing superior performance over existing methods in terms of convergence speed.  
3. Clarity: The paper is generally well-written and organized, with a clear presentation of the problem, methodology, and results. The inclusion of geometric interpretations and detailed proofs in the supplementary material enhances the paper's rigor.  
Weaknesses  
1. Computational Complexity: While the proposed algorithms reduce the number of iterations, the computational cost of the nonlinear momentum term and the exponential map is high. The authors should provide a more detailed discussion of the trade-offs between iteration count and per-iteration cost.  
2. Definitions and Intuition: Key terms like "star-concave" and "intrinsic inner-product" are not defined, limiting accessibility for a broader audience. Additionally, Equations (4) and (5) lack intuitive explanations, making it difficult to grasp their geometric significance.  
3. Experimental Evaluation: The experiments focus on iteration counts rather than actual runtime benchmarks. Given the high per-iteration cost, runtime comparisons would provide a more complete picture of the algorithm's efficiency.  
4. Parameter Sensitivity: The paper does not adequately discuss the choice of parameters (e.g., \(\alpha\), \(\beta\)) or their dependence on problem-specific characteristics like the diameter \(D\). This omission could hinder reproducibility and practical adoption.  
Minor Issues  
- Typographical errors (e.g., redundant "The" at L52, inconsistent font for "Exp" at L139 and L144).  
- Misleading interpretation of AGD as primarily proximal (L52, L101).  
- Ambiguities in notation (e.g., \(Yk\) vs \(yk\) at L183).  
Arguments for Acceptance  
- The paper addresses a fundamental and challenging problem in optimization, making a significant theoretical contribution.  
- The proposed algorithms have broad applicability and potential for further exploration in Riemannian optimization.  
- The experimental results validate the theoretical claims and demonstrate practical utility.  
Arguments Against Acceptance  
- High computational cost of the momentum term may limit the practical scalability of the algorithms.  
- Lack of clarity in definitions and intuition for key equations could hinder understanding and reproducibility.  
- Insufficient discussion of runtime performance and parameter sensitivity.  
Recommendation: Accept with Minor Revisions  
The paper makes a strong theoretical contribution and demonstrates promising empirical results. However, the authors should address the computational complexity concerns, provide runtime benchmarks, and improve the clarity of definitions and explanations to make the work more accessible and impactful.