The paper introduces S-MISO, a novel stochastic optimization algorithm that extends variance reduction techniques like MISO/Finito to finite-sum problems with random perturbations. This is a significant contribution, as existing variance reduction methods are not well-suited for such settings, leaving stochastic gradient descent (SGD) as the primary alternative. The authors provide theoretical convergence analysis and validate their approach with experiments on tasks involving data augmentation and Dropout regularization.
Strengths:
1. Algorithmic Innovation: S-MISO is a well-motivated extension of variance reduction methods to stochastic settings. It effectively bridges the gap between finite-sum optimization and stochastic approximation, outperforming SGD and N-SAGA both theoretically and empirically.
2. Practical Relevance: The algorithm is particularly suited for machine learning tasks involving data augmentation or perturbations, such as image and text classification. This is a highly relevant domain with broad applications.
3. Empirical Validation: The experiments demonstrate clear advantages of S-MISO over competing methods, especially in scenarios with low perturbation variance. The results align with the theoretical predictions, showcasing the algorithm's practical utility.
4. Memory Efficiency: While S-MISO requires additional memory for storing auxiliary variables, this is manageable for non-huge datasets, which are the primary focus of the paper.
Weaknesses:
1. Convergence Analysis: The theoretical analysis, while insightful, is loosely presented. The unconventional use of parameters like epsilon bar and the lack of standard O-notation make the proofs harder to follow.
2. Parameter Tuning: S-MISO requires careful tuning of parameters like step-sizes and gamma, which is non-trivial and not well-elaborated in the paper.
3. Experimental Scope: The experiments are limited to 2-layer networks, despite claims of applicability to multi-layer networks. This raises concerns about the algorithm's scalability to deeper architectures.
4. Clarity Issues: Certain terms, such as "data radius" and "simple," are not well-defined. Additionally, some paragraphs and sentences appear misplaced, affecting the paper's overall organization.
5. Formatting and Typographical Errors: Minor issues like missing thousand separators detract from readability.
Arguments for Acceptance:
- The paper addresses an important and underexplored problem, extending variance reduction techniques to stochastic settings.
- Both theoretical and empirical results demonstrate significant improvements over existing methods.
- The algorithm has practical relevance for common machine learning tasks involving data augmentation.
Arguments Against Acceptance:
- The convergence analysis lacks clarity and rigor, with unconventional parameter usage and missing standard notations.
- The experimental evaluation is incomplete, failing to address multi-layer networks as promised.
- The paper requires additional polishing in terms of clarity, organization, and formatting.
Recommendation:
I recommend conditional acceptance, provided the authors address the clarity issues in the convergence analysis, expand the experimental evaluation to include multi-layer networks, and improve the paper's organization and formatting. The contributions are significant, but the presentation needs refinement to meet the standards of the conference.