The paper investigates kernel regression in high-dimensional settings by extending the traditional additive model to a group additive model (GAM), which accounts for correlations between predictor variables. The authors rigorously define the intrinsic group additive structure, propose a complexity measure for groupings based on the covering number of RKHSs, and develop algorithms for identifying optimal groupings. Theoretical results demonstrate the consistency of the proposed method, and experiments on synthetic and real-world datasets are used to validate the approach.
Strengths:
1. Novelty of Theoretical Contribution: The introduction of a complexity measure for group additive structures is a notable theoretical advancement. By leveraging the covering number of RKHSs, the authors provide a mathematically grounded penalty term to identify intrinsic group structures.
2. Addressing Nonparametric Challenges: The paper tackles a critical limitation of the additive model—its inability to capture interactions between predictors—by introducing GAM. This positions the work as a meaningful extension of existing nonparametric regression techniques.
3. Consistency Proofs: The authors provide rigorous proofs demonstrating the consistency of their method in identifying the true group structure, which strengthens the theoretical foundation of the work.
4. Potential Practical Utility: The method's ability to balance interpretability and flexibility in high-dimensional regression is appealing, particularly for applications where interactions between variables are important but difficult to model.
Weaknesses:
1. Related Work: The discussion of related work is inadequate. The paper fails to connect GAM with structured sparsity, graphical models, Bayesian networks, or feature selection. These omissions limit the contextualization of the contribution within the broader literature.
2. Algorithmic Limitations: The proposed optimization methods—exhaustive search and a greedy stepwise approach—are computationally expensive and lack innovation. This could hinder the scalability of the method to very high-dimensional datasets.
3. Experimental Design: The experiments are underwhelming. The synthetic data validation is simplistic, and the Boston Housing dataset example lacks comparisons with baseline methods or alternative approaches. This makes it difficult to assess the practical advantages of the proposed method.
4. Clarity and Accessibility: While the theoretical sections are rigorous, the paper is dense and may be challenging for readers unfamiliar with RKHS theory or covering numbers. Additionally, the lack of implementation details for the algorithms hinders reproducibility.
Arguments for Acceptance:
- The theoretical contributions are significant and address a critical gap in high-dimensional nonparametric regression.
- The method has potential for practical use in domains requiring interpretable models with moderate interactions between predictors.
Arguments Against Acceptance:
- The lack of a comprehensive discussion of related work and the absence of experimental comparisons with baselines weaken the paper's impact.
- The computational inefficiency of the proposed algorithms limits their applicability to large-scale problems.
- The experimental results are insufficiently robust to demonstrate the method's effectiveness.
Recommendation: Weak Reject. While the theoretical contributions are valuable, the paper falls short in experimental validation, related work discussion, and algorithmic innovation. Strengthening these aspects in a future revision could make the paper a strong candidate for acceptance.