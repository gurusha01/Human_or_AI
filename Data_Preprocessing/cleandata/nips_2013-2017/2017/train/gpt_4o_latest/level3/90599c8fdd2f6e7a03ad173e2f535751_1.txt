This paper introduces two novel variants of SMDP-UCRL (SUCRL), termed FSUCRLv1 and FSUCRLv2, which address a significant limitation of SUCRL by estimating option-related parameters from data, eliminating the need for prior knowledge. The key distinction between the two variants lies in their parameter estimation methods: FSUCRLv1 employs explicit estimation with confidence intervals, while FSUCRLv2 utilizes a nested implicit procedure. The authors derive regret bounds for both variants, demonstrating that they are close to SUCRL's bounds, with an additional cost for parameter estimation. Empirical results are provided to validate the theoretical findings, showcasing the competitiveness of the proposed algorithms.
The paper makes a notable contribution by leveraging the irreducible Markov chain representation of options to estimate parameters. This approach is innovative and addresses a practical challenge in reinforcement learning with options, where prior knowledge of cumulative reward and duration distributions is often unavailable. Theorem 1, which establishes the regret bounds for FSUCRL, is the primary theoretical contribution. However, the proof is relegated entirely to the appendix, and the main text lacks an intuitive explanation, which could hinder accessibility for readers unfamiliar with the technical details.
Strengths of the paper include its rigorous theoretical analysis and the introduction of a parameter-free approach to learning with options. The empirical validation is well-executed, demonstrating that FSUCRL retains the benefits of temporal abstraction while being competitive with SUCRL. Additionally, the nested structure of FSUCRLv2 aligns well with the hierarchical nature of reinforcement learning, offering a principled approach to leveraging the inner structure of options.
However, the paper has some weaknesses. While the theoretical results are robust, the presentation is dense, particularly in sections detailing the derivation of regret bounds and the nested value iteration algorithm. This could be improved with more intuitive explanations and visual aids. Additionally, the practical implications of the pseudo-diameter term in the regret bound are not fully explored, leaving readers with limited insight into how it might affect performance in real-world scenarios. Finally, while the empirical results are promising, they are limited to relatively simple domains, and the scalability of FSUCRL to more complex environments remains unclear.
Arguments for Acceptance:
1. The paper addresses a critical limitation of SUCRL by removing the need for prior knowledge of option parameters.
2. Theoretical and empirical results demonstrate that FSUCRL is competitive with SUCRL while retaining the benefits of temporal abstraction.
3. The irreducible Markov chain representation is a novel and insightful contribution.
Arguments Against Acceptance:
1. The presentation of the theoretical contributions is overly dense, with insufficient intuition provided in the main text.
2. The empirical validation is limited to simple domains, leaving questions about scalability to more complex environments.
3. The practical implications of the additional regret term (pseudo-diameter) are not thoroughly analyzed.
In conclusion, the paper makes a significant theoretical and algorithmic contribution to reinforcement learning with options. While there are some clarity and scope limitations, the novelty and rigor of the work make it a strong candidate for acceptance.