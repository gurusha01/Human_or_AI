This paper presents a computational framework for modeling human question-asking behavior in a "Battleship" domain, representing questions as lambda calculus programs. The authors propose a probabilistic context-free grammar (PCFG) to generate questions and evaluate them based on informativeness, complexity, and answer type. Their results demonstrate that complexity, rather than informativeness alone, plays a dominant role in predicting human-like questions, offering a nuanced perspective on the cognitive processes underlying question formulation.
Strengths:
The paper is well-written and effectively bridges cognitive science and computational linguistics, providing a novel approach to understanding human question-asking. The use of lambda calculus and PCFG to model questions as programs is a creative and rigorous formalization, enabling the generation of semantically coherent and contextually relevant questions. The authors' inclusion of human question data from prior research lends empirical validity to their model, and the leave-one-out cross-validation demonstrates the robustness of their approach. The study also makes a significant contribution by addressing why informativeness alone fails to predict human question-asking, emphasizing the role of complexity and question space size. Furthermore, the paper successfully synthesizes novel, human-like questions, showcasing the model's generative capabilities.
Weaknesses:
The primary limitation of the study lies in its scalability and generalizability. The reliance on domain-specific knowledge (e.g., the Battleship game) raises concerns about the applicability of the proposed framework to broader, real-world domains. While the authors acknowledge this limitation, the paper would benefit from a more detailed discussion of how the model could be extended to other contexts. Additionally, the computational efficiency of the approach is not thoroughly addressed, particularly given the large hypothesis spaces involved. Another limitation is the lack of nested model comparisons to assess the statistical significance of the lesioned models, which would strengthen the claims about the relative importance of different features.
Suggestions for Improvement:
1. Perform nested model comparisons to statistically validate the contributions of informativeness, complexity, and answer type.
2. Explore integration with formal semantics and pragmatics literature to enhance the theoretical grounding of the model.
3. Investigate scalability by applying the framework to more diverse domains and reducing reliance on domain-specific grammars.
4. Reference relevant work from the "Questions under Discussion" literature to situate the study within broader linguistic and cognitive research.
Recommendation:
This paper makes a valuable contribution to the field by advancing our understanding of human question-asking and its computational modeling. While the scalability and domain-specificity issues limit its immediate applicability, the methodological innovation and insights into the cognitive trade-offs between informativeness and complexity are significant. I recommend acceptance with minor revisions to address the outlined limitations and suggestions.