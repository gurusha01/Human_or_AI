This paper presents a novel approach for high-dimensional nonparametric regression by addressing the limitations of traditional additive models, which neglect interactions between predictor variables. The authors rigorously define the intrinsic group additive structure (GAS) and propose a structure-penalized kernel method to simultaneously identify this structure and estimate nonparametric functions. The paper introduces a complexity measure for GAS based on the covering number of Reproducing Kernel Hilbert Spaces (RKHSs) and demonstrates the consistency of the proposed method in identifying the intrinsic GAS. The authors also develop two algorithms—exhaustive search and backward stepwise—for practical implementation, supported by simulation studies and real data applications.
Strengths:
1. Novelty and Originality: The paper addresses a significant gap in nonparametric regression by introducing the concept of intrinsic GAS, which balances interpretability and flexibility. This is a meaningful contribution to the field, as it lies between simple additive models and more complex interaction models.
2. Theoretical Rigor: The authors provide a solid theoretical foundation, including proofs of consistency for their method. The introduction of a complexity measure based on RKHS covering numbers is well-motivated and innovative.
3. Practical Applicability: The proposed algorithms (exhaustive search and backward stepwise) make the method computationally feasible for different problem scales. The application to real-world data, such as the Boston Housing dataset, demonstrates the utility of the approach.
4. Empirical Validation: Extensive simulations show that the method successfully identifies the intrinsic GAS under various settings. The results are promising, particularly for models with small to moderate numbers of predictors.
Weaknesses:
1. Computational Scalability: While the backward stepwise algorithm reduces computational cost, it remains unclear how the method scales to very high-dimensional datasets (e.g., thousands of predictors). The exhaustive search algorithm is computationally infeasible for large p.
2. Sensitivity to Tuning Parameters: The method relies on multiple tuning parameters (e.g., λ, µ, α), and the selection process appears ad hoc. A more systematic approach to parameter tuning would enhance robustness.
3. Limited Real-World Applications: Although the Boston Housing dataset is a standard benchmark, the paper would benefit from additional real-world applications in diverse domains to demonstrate broader applicability.
4. Algorithmic Greediness: The backward stepwise algorithm, while practical, is inherently greedy and may fail to identify the true GAS in some cases, as noted in the simulation results for Model 4. This limitation warrants further exploration.
Pro and Con Arguments for Acceptance:
Pro:
- The paper makes a substantial theoretical and methodological contribution to high-dimensional nonparametric regression.
- The proposed method is novel, well-motivated, and addresses a critical limitation of existing additive models.
- Theoretical guarantees (e.g., consistency) and empirical results support the validity of the approach.
Con:
- Computational scalability and sensitivity to parameter tuning are practical concerns that limit the method's applicability to very large datasets.
- The backward stepwise algorithm's performance is inconsistent in some scenarios, indicating room for improvement.
Recommendation:
I recommend acceptance with minor revisions. The paper is a strong contribution to the field, but addressing scalability and parameter tuning issues would significantly enhance its impact. Additionally, expanding the range of real-world applications would strengthen the empirical validation.