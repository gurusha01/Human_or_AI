This paper addresses the challenge of enabling reinforcement learning (RL) agents to learn from non-expert human feedback, specifically in the domain of image captioning. The authors propose a novel hierarchical phrase-based captioning model that integrates natural language feedback from human teachers. Unlike prior approaches that rely on scalar rewards or binary feedback, this work leverages descriptive sentences to provide richer learning signals, which can pinpoint specific errors and suggest corrections. The proposed framework incorporates a feedback network (FBN) to evaluate the correctness of phrases in generated captions based on human feedback and integrates this into policy gradient optimization. The authors demonstrate that their approach outperforms baseline RL models trained with traditional ground-truth captions, achieving improvements in BLEU and ROUGE metrics. The paper also highlights the efficiency of natural language feedback in reducing human interaction effort compared to traditional methods.
Strengths:
1. Novelty: The paper introduces a unique approach to incorporating natural language feedback into RL for image captioning, which has not been extensively explored. The hierarchical phrase-based RNN is well-suited for integrating feedback at a granular level.
2. Practicality: The focus on enabling non-expert users to guide learning agents aligns with real-world applications, such as household robots or personal assistants.
3. Comprehensive Evaluation: The authors conduct thorough experiments on the MS-COCO dataset, demonstrating the effectiveness of their method in improving caption quality. They also include human evaluations to validate the practical utility of their approach.
4. Open-Source Contribution: The promise to release code and data fosters reproducibility and encourages further research in this area.
5. Clarity of Feedback Integration: The feedback network is well-designed, with clear mechanisms for incorporating human corrections into the RL framework.
Weaknesses:
1. Limited Scope of Feedback: While the paper demonstrates the utility of natural language feedback, the experiments are restricted to image captioning. It would be beneficial to explore generalizability to other RL tasks with larger action spaces.
2. Baseline Comparisons: The paper primarily compares its approach to standard RL and MLE baselines. Including comparisons with other methods that incorporate human feedback, such as TAMER or policy shaping, would strengthen the claims.
3. Feedback Network Dependence: The FBN relies heavily on annotated feedback, which may not scale well to tasks requiring more complex or domain-specific corrections.
4. Human Effort Analysis: While the paper mentions reduced human interaction, a more detailed analysis of the trade-off between feedback quality and annotation effort would provide deeper insights.
Recommendation:
This paper makes a significant contribution to the field of human-in-the-loop RL by demonstrating the potential of natural language feedback in improving agent performance. The proposed method is technically sound, well-motivated, and addresses a practical problem. However, the scope of the work could be broadened, and comparisons with alternative feedback-based RL methods would enhance its impact. I recommend acceptance, with minor revisions to address the weaknesses mentioned above.
Arguments for Acceptance:
- Novel and practical approach to integrating natural language feedback into RL.
- Strong experimental results and comprehensive evaluation.
- Clear writing and well-structured methodology.
Arguments against Acceptance:
- Limited exploration of generalizability to other RL tasks.
- Lack of comparison with other human feedback-based RL methods.