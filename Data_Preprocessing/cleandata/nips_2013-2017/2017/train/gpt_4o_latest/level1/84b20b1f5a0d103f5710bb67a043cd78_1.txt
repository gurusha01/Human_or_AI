The paper proposes an accelerated stochastic greedy coordinate descent (ASGCD) algorithm for solving `1-regularized problems, combining Nesterov's acceleration and stochastic optimization techniques. The authors introduce a novel greedy selection rule based on an `1-norm square approximation, which is convex but challenging to solve. To address this, they propose the SOft ThreshOlding PrOjection (SOTOPO) algorithm, which efficiently solves the induced subproblem with a complexity of \(O(d + |Q| \log |Q|)\). By integrating SOTOPO into the greedy coordinate descent (GCD) framework, the authors achieve an accelerated convergence rate of \(O(\sqrt{1/\epsilon})\), while reducing the iteration complexity of greedy selection by a factor proportional to the sample size. Theoretical analysis and empirical results demonstrate that ASGCD outperforms existing methods, particularly for high-dimensional and dense problems with sparse solutions.
Strengths:
1. Novelty and Originality: The paper introduces a new greedy selection rule based on `1-norm square approximation, which is a significant departure from traditional quadratic approximations. The SOTOPO algorithm is a novel contribution, extending the applicability of GCD.
2. Theoretical Rigor: The authors provide thorough theoretical analysis, including convergence guarantees and complexity bounds. The results are well-supported by mathematical proofs and align with the state-of-the-art.
3. Practical Significance: The proposed ASGCD algorithm reduces the computational bottleneck of GCD by leveraging stochastic optimization, making it more practical for large-scale problems. This is particularly relevant for high-dimensional datasets.
4. Empirical Validation: The experiments are comprehensive, comparing ASGCD with multiple baselines (e.g., Katyusha, CGD, SVRG) on real-world datasets. The results corroborate the theoretical claims, demonstrating superior performance in specific scenarios.
Weaknesses:
1. Clarity: While the paper is mathematically rigorous, it is dense and challenging to follow, particularly for readers unfamiliar with the topic. The derivation of SOTOPO and its integration into ASGCD could benefit from clearer explanations and visual aids (e.g., flowcharts or diagrams).
2. Limited Scope of Experiments: The experiments focus on a narrow set of datasets and problem types (e.g., Lasso). It would be beneficial to evaluate ASGCD on a broader range of applications, such as logistic regression or other `1-regularized tasks.
3. Comparison with Related Work: While the paper compares ASGCD with several baselines, it does not include some recent advancements in coordinate descent or stochastic optimization (e.g., APCG or accelerated SDCA). A more exhaustive comparison would strengthen the claims.
4. Logarithmic Factor: The theoretical bound for ASGCD includes a \(\log d\) factor, which the authors acknowledge but do not fully address. Further discussion or experiments to evaluate the impact of this factor would be valuable.
Arguments for Acceptance:
- The paper makes a significant theoretical and practical contribution to the field of optimization, particularly for `1-regularized problems.
- The proposed algorithm is novel, well-motivated, and supported by rigorous analysis and empirical results.
- The integration of Nesterov's acceleration and stochastic optimization into GCD is a meaningful advancement.
Arguments Against Acceptance:
- The clarity and accessibility of the paper could be improved, particularly for a broader audience.
- The experimental evaluation, while solid, is somewhat limited in scope and does not include all relevant baselines.
- The necessity of the \(\log d\) factor in the convergence bound remains an open question.
Recommendation:
I recommend acceptance with minor revisions. The paper presents a novel and impactful algorithm with strong theoretical and empirical support, but it would benefit from improved clarity and a broader experimental evaluation.