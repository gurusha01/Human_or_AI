This paper presents a novel accelerated first-order method for geodesically convex optimization, extending Nesterov's accelerated gradient descent from Euclidean to Riemannian spaces. The authors derive two nonlinear operators to replace the linear extrapolation step in Euclidean space, achieving improved convergence rates for geodesically strongly-convex problems (from \(O((1-\mu/L)^k)\) to \(O((1-\sqrt{\mu/L})^k)\)) and geodesically convex problems (from \(O(1/k)\) to \(O(1/k^2)\)). The paper also includes a specific application to matrix Karcher mean problems, validating the theoretical results through experiments.
Strengths:
1. Novelty and Originality: The paper addresses an open problem in the literature by generalizing Nesterov's acceleration to nonlinear Riemannian spaces, a significant contribution to the field of optimization on manifolds.
2. Theoretical Contributions: The authors rigorously analyze the global convergence properties of their method, providing theoretical guarantees that match the optimal rates in Euclidean settings. This is a meaningful advancement over prior work such as Zhang and Sra (2017), which lacked accelerated convergence rates.
3. Practical Relevance: The application to matrix Karcher mean problems demonstrates the method's utility in real-world scenarios, such as medical imaging and radar signal processing.
4. Experimental Validation: The experiments convincingly show that the proposed method outperforms Riemannian gradient descent (RGD) in terms of convergence speed and is competitive with limited-memory Riemannian BFGS (LRBFGS) in runtime efficiency.
Weaknesses:
1. Clarity: While the paper is mathematically rigorous, some sections, particularly the derivation of nonlinear operators and the geometric interpretation, are dense and may be challenging for readers unfamiliar with Riemannian geometry. Additional diagrams or simplified explanations could improve accessibility.
2. Scope of Experiments: The experiments focus primarily on synthetic data for matrix Karcher mean problems. It would strengthen the paper to include results on a broader range of geodesically convex problems or real-world datasets.
3. Computational Complexity: The paper does not provide a detailed analysis of the computational overhead introduced by the nonlinear operators compared to standard RGD. This could be a concern for large-scale problems.
4. Comparison with Stochastic Methods: The paper briefly mentions the potential for extending the method to stochastic settings but does not explore or compare against existing stochastic Riemannian optimization techniques.
Arguments for Acceptance:
- The paper addresses a significant gap in the literature by extending acceleration techniques to Riemannian spaces.
- The theoretical contributions are robust, with clear improvements over existing methods.
- The experimental results validate the practical utility of the proposed approach.
Arguments Against Acceptance:
- The paper's clarity could be improved, particularly for readers less familiar with Riemannian geometry.
- The experimental evaluation is somewhat narrow in scope, limiting the generalizability of the results.
- A deeper discussion of computational trade-offs and scalability is needed.
Recommendation:
I recommend acceptance, as the paper makes a strong theoretical contribution to the field of optimization on Riemannian manifolds and demonstrates promising empirical results. However, the authors should consider improving the clarity of their presentation and broadening the experimental evaluation in future revisions.