This paper presents SHAP (SHapley Additive exPlanations), a unified framework for interpreting predictions of complex machine learning models. The authors identify a new class of additive feature attribution methods and demonstrate that there is a unique solution in this class that satisfies three desirable properties: local accuracy, missingness, and consistency. By unifying six existing methods (e.g., LIME, DeepLIFT, and Shapley value-based approaches), the paper provides a theoretical foundation for comparing and improving interpretability methods. The authors also propose novel estimation methods, such as Kernel SHAP and Deep SHAP, which improve computational efficiency and alignment with human intuition. Experimental results demonstrate SHAP's advantages in terms of computational efficiency, consistency with human intuition, and ability to explain class differences in deep learning models.
Strengths
1. Theoretical Contribution: The paper makes a significant theoretical contribution by unifying six existing methods under a single framework and proving the uniqueness of SHAP values within the class of additive feature attribution methods. This clarity is valuable for the interpretability community.
2. Practical Utility: The proposed SHAP values are not only theoretically grounded but also practical, as evidenced by the introduction of efficient approximation methods (e.g., Kernel SHAP and Deep SHAP) that make SHAP values computationally feasible for large models.
3. Experimental Validation: The authors conduct thorough experiments, including computational efficiency comparisons, user studies to validate consistency with human intuition, and applications to deep learning models. These experiments convincingly demonstrate the advantages of SHAP over existing methods.
4. Clarity and Organization: The paper is well-written and logically organized, with clear definitions, proofs, and explanations of the proposed methods. The inclusion of user studies and practical examples enhances the paper's accessibility and impact.
Weaknesses
1. Computational Complexity: While the authors propose approximation methods to make SHAP values computationally feasible, the exact computation of SHAP values remains intractable for high-dimensional models. This limitation could restrict the framework's applicability in real-world scenarios with large datasets and complex models.
2. Assumptions: Some of the proposed methods rely on assumptions such as feature independence or model linearity, which may not hold in practice. The authors acknowledge this but could provide more discussion on the implications of these assumptions and potential ways to relax them.
3. Scope of Evaluation: While the experiments are comprehensive, the evaluation is primarily focused on specific tasks (e.g., image classification with MNIST). Additional experiments on diverse datasets and tasks (e.g., natural language processing or time-series data) would strengthen the paper's generalizability claims.
Arguments for Acceptance
- The paper addresses a critical problem in machine learning interpretability and provides a unified framework that advances the state of the art.
- The theoretical contributions are novel and well-supported by proofs, and the practical methods are validated through experiments.
- The framework has the potential to become a standard for feature attribution methods, benefiting both researchers and practitioners.
Arguments Against Acceptance
- The computational complexity of exact SHAP value computation limits its scalability.
- The reliance on certain assumptions (e.g., feature independence) could reduce the robustness of the proposed methods in some scenarios.
Recommendation
I recommend acceptance of this paper. Its theoretical and practical contributions are significant, and it addresses an important challenge in machine learning interpretability. While there are some limitations, they are outweighed by the paper's strengths and potential impact on the field.