The paper presents NeuralFDR, a novel algorithm for multiple hypothesis testing that leverages hypothesis-specific features using neural networks. Unlike traditional methods such as Benjamini-Hochberg (BH) or Independent Hypothesis Weighting (IHW), which either ignore features or assume simple feature structures, NeuralFDR models a flexible, feature-dependent discovery threshold using a multilayer perceptron (MLP). This approach enables the algorithm to handle multi-dimensional, continuous, and discrete features while maintaining strong false discovery rate (FDR) control. The authors provide theoretical guarantees for FDR control under both independence and weak dependence assumptions and demonstrate the algorithm's superior performance on synthetic and real-world datasets, including RNA-Seq and eQTL studies.
Strengths
1. Technical Novelty: NeuralFDR introduces a significant advancement by parametrizing the discovery threshold as a neural network, enabling it to model complex, high-dimensional feature spaces. This is a clear improvement over existing methods like IHW, which rely on binning and are less effective for multi-dimensional features.
2. Theoretical Guarantees: The paper rigorously proves FDR control under independence and provides asymptotic guarantees under weak dependence. The use of cross-validation to prevent overfitting is a thoughtful addition.
3. Empirical Performance: NeuralFDR consistently outperforms state-of-the-art methods (e.g., IHW) in terms of the number of discoveries while maintaining FDR control. The results are robust across diverse datasets, including synthetic examples with varying feature structures and real-world genomic datasets.
4. Interpretability: The learned discovery thresholds are interpretable and align with domain knowledge, such as the dependency of eQTL discoveries on SNP-gene distance and gene expression levels.
5. Scalability: The algorithm handles large-scale datasets (e.g., millions of hypotheses in the GTEx study) and demonstrates its utility in real-world scenarios.
Weaknesses
1. Dependence on Large Datasets: NeuralFDR performs best when the number of hypotheses and the alternative proportion are large. Its performance on smaller datasets or datasets with low signal-to-noise ratios is less clear and warrants further investigation.
2. Model Complexity: The use of a 10-layer MLP raises questions about the computational cost and the potential for overfitting, especially for smaller datasets. A more systematic exploration of the optimal network architecture would strengthen the paper.
3. Limited Comparison: While NeuralFDR is compared against BH, Storey's BH, and IHW, it would be beneficial to include comparisons with other recent FDR control methods, such as AdaPT or SABHA, to provide a more comprehensive evaluation.
4. Assumptions on Independence: Although the paper extends to weak dependence, the practical implications of this assumption in highly correlated datasets (e.g., linkage disequilibrium in genomics) are not fully explored.
Arguments for Acceptance
- The paper addresses a critical limitation in multiple hypothesis testing by incorporating hypothesis-specific features in a principled way.
- It provides both theoretical and empirical evidence of its superiority over existing methods.
- The proposed method has practical significance, particularly in genomics, where feature-rich datasets are common.
Arguments Against Acceptance
- The reliance on large datasets may limit its applicability in scenarios with fewer hypotheses or low alternative proportions.
- The computational complexity and choice of neural network architecture could be better justified.
Recommendation
I recommend acceptance of this paper. NeuralFDR is a significant contribution to the field of multiple hypothesis testing, offering both theoretical rigor and practical utility. Addressing the identified weaknesses in future work could further enhance its impact.