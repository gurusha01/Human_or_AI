The paper addresses the challenging problem of non-parametric Conditional Independence (CI) testing for continuous random variables, proposing a novel model-driven approach that reduces CI testing to a binary classification problem. The authors leverage powerful classifiers, such as gradient-boosted trees and deep neural networks, to distinguish between the joint distribution \(f(x, y, z)\) and the conditional independence distribution \(f{CI}(x, y, z)\). A key contribution is the development of a nearest-neighbor bootstrap procedure to generate samples approximating \(f{CI}\), with theoretical guarantees on their closeness in total variation distance. The paper also provides generalization bounds for classification under near-independent samples and demonstrates the empirical superiority of the proposed method over state-of-the-art kernel-based CI tests (KCIT, RCIT) on synthetic and real-world datasets.
Strengths:
1. Novelty and Originality: The paper introduces a novel reduction of CI testing to binary classification, which is a significant departure from traditional kernel-based methods. This approach enables the use of modern supervised learning tools, making it highly relevant in high-dimensional settings.
2. Theoretical Contributions: The authors provide rigorous theoretical guarantees, including bounds on the total variation distance of the bootstrapped samples and generalization risk for classifiers trained on near-independent samples. The analysis of Rademacher complexity in this context is particularly noteworthy.
3. Empirical Validation: The proposed method (CCIT) demonstrates superior performance compared to KCIT and RCIT in both synthetic experiments and real-world applications, such as the flow-cytometry dataset. The scalability of CCIT with respect to the dimensionality of \(Z\) is a clear advantage.
4. Flexibility: The modular design allows the use of different classifiers, enabling domain-specific adaptations and improvements as classification techniques evolve.
Weaknesses:
1. Computational Complexity: While the method is scalable in terms of dimensionality, the nearest-neighbor bootstrap and repeated classification steps may still be computationally expensive, especially for very large datasets. A more detailed discussion of runtime trade-offs would strengthen the paper.
2. Bias Correction: The bias correction mechanism in Algorithm 3, while effective, introduces additional complexity. The empirical evaluation does not clearly quantify the impact of this correction on performance.
3. Limited Real-World Evaluation: Although the flow-cytometry dataset is a well-known benchmark, additional real-world datasets from diverse domains would enhance the generalizability of the results.
4. Comparison with Other Approaches: The paper primarily compares CCIT with kernel-based methods. Including comparisons with other recent CI testing methods, such as those based on mutual information or entropy estimation, would provide a more comprehensive evaluation.
Arguments for Acceptance:
- The paper presents a significant methodological innovation by reframing CI testing as a classification problem.
- Theoretical guarantees and empirical results demonstrate the robustness and effectiveness of the approach.
- The flexibility and scalability of the method make it a valuable contribution to the field.
Arguments Against Acceptance:
- The computational overhead of the nearest-neighbor bootstrap and classification steps may limit practical applicability in some scenarios.
- The evaluation could benefit from additional datasets and comparisons with a broader range of methods.
Recommendation:
Overall, this paper makes a strong contribution to the field of CI testing and statistical learning. While there are areas for improvement, the novelty, theoretical rigor, and empirical performance justify its acceptance. I recommend acceptance with minor revisions to address computational concerns and expand the evaluation.