This paper introduces novel greedy optimization algorithms for optimization over the convex cone, a domain parameterized as the conic hull of a generic atom set. The authors extend the Matching Pursuit (MP) framework to address this intermediate case between the linear span and convex hull parameterizations, which has been underexplored in prior work. They propose Non-Negative Matching Pursuit (NNMP) algorithms and their corrective variants, providing explicit convergence guarantees. The paper demonstrates sublinear convergence for general smooth convex objectives and linear convergence for strongly convex objectives, supported by theoretical analyses and empirical results. The authors also establish connections between their algorithms and existing MP and Frank-Wolfe (FW) methods, highlighting the generality and applicability of their approach across diverse learning settings.
Strengths:
1. Novelty and Originality: The paper addresses a significant gap in the literature by introducing principled MP algorithms for conic hull optimization, a domain not adequately covered by prior MP or FW methods. The proposed algorithms are novel and extend the applicability of greedy optimization techniques.
2. Theoretical Contributions: The authors provide rigorous convergence guarantees, including sublinear and linear rates, which are supported by detailed proofs. The introduction of geometric complexity measures like Cone Width is particularly insightful.
3. Generality: The algorithms are applicable to general smooth convex functions and do not rely on restrictive assumptions about the atom set, making them versatile for various applications.
4. Empirical Validation: The experimental results demonstrate the practical utility of the proposed methods across synthetic and real-world tasks, including non-negative matrix factorization and logistic regression. The corrective variants show competitive or superior performance compared to established baselines.
5. Clarity of Related Work: The paper situates its contributions well within the context of existing MP and FW literature, providing a clear comparison and highlighting its advancements.
Weaknesses:
1. Complexity of Presentation: While the theoretical contributions are strong, the paper is dense and may be challenging for readers unfamiliar with MP and FW frameworks. Simplifying some sections or providing more intuitive explanations could improve accessibility.
2. Limited Empirical Scope: Although the experiments are illustrative, they are somewhat limited in scope. Additional benchmarks, particularly on large-scale datasets or more diverse applications, would strengthen the empirical claims.
3. Computational Overhead: The corrective variants (e.g., FCMP) introduce additional computational complexity due to repeated linear minimization oracle (LMO) queries. A more detailed discussion of scalability and runtime trade-offs would be beneficial.
4. Practical Implications: While the theoretical guarantees are robust, the paper could better articulate the practical implications of the proposed methods, particularly in comparison to simpler alternatives like projected gradient descent.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution by addressing a previously unexplored optimization domain and providing rigorous convergence guarantees.
- The proposed methods are general and applicable to a wide range of problems, with demonstrated empirical utility.
- The work advances the state of the art in greedy optimization and offers insights that could inspire future research.
Arguments Against Acceptance:
- The dense presentation and limited empirical scope may hinder accessibility and practical impact.
- The computational overhead of the corrective variants could limit scalability for large-scale problems.
Recommendation:
Overall, this paper represents a valuable contribution to the field of optimization and machine learning. While there are areas for improvement, particularly in presentation and empirical breadth, the novelty and rigor of the work justify its acceptance. I recommend acceptance with minor revisions to improve clarity and address scalability concerns.