This paper introduces deterministic feature maps for kernel methods, aiming to improve upon the widely-used random Fourier features (RFF) approach. The authors propose leveraging Gaussian quadrature and sparse ANOVA kernels to construct deterministic feature maps that achieve better scaling in terms of approximation error and computational efficiency. The paper demonstrates that deterministic maps can achieve error bounds with sample complexity \(O(e^\gamma + \epsilon^{-1/\gamma})\), which is asymptotically better than the \(O(\epsilon^{-2})\) complexity of RFF. The work is validated on MNIST and TIMIT datasets, showing comparable or improved accuracy and faster feature generation compared to RFF.
Strengths:
1. Novelty and Originality: The paper addresses a fundamental limitation of RFF by removing its reliance on randomness, which leads to probabilistic guarantees. Deterministic feature maps, particularly using Gaussian quadrature, represent a novel contribution to kernel methods.
2. Theoretical Contributions: The authors provide rigorous theoretical analysis, including bounds on approximation error and sample complexity for various deterministic schemes (e.g., polynomially-exact rules, sparse grids). These results are well-grounded in numerical integration literature.
3. Practical Relevance: The focus on sparse ANOVA kernels, which mimic convolutional layers in CNNs, is timely and relevant given the increasing interest in structured kernels for large-scale machine learning tasks. The experimental results demonstrate that deterministic maps can match or outperform RFF in real-world applications.
4. Experimental Validation: The experiments on MNIST and TIMIT datasets are comprehensive, comparing deterministic methods to RFF and other techniques like quasi-Monte Carlo. The results show that deterministic maps achieve lower kernel approximation error and comparable classification accuracy while reducing feature generation time.
Weaknesses:
1. Scalability in High Dimensions: While deterministic methods outperform RFF in low-dimensional settings, the curse of dimensionality remains a significant challenge. The dense and sparse grid quadrature methods suffer from exponential growth in sample complexity with increasing dimensions, as acknowledged by the authors.
2. Limited Applicability: The methods rely on specific kernel structures (e.g., subgaussian kernels, sparse ANOVA kernels), which may limit their general applicability to other kernel types or tasks.
3. Experimental Scope: Although the experiments are thorough, the paper could benefit from additional benchmarks on larger and more diverse datasets to better demonstrate the scalability and robustness of the proposed methods.
4. Complexity of Implementation: Some of the proposed methods, such as reweighted grid quadrature, require solving non-negative least squares problems, which may introduce additional computational overhead and complexity in practice.
Arguments for Acceptance:
- The paper makes a significant theoretical and practical contribution to kernel methods by introducing deterministic feature maps with provable guarantees.
- The experimental results validate the utility of the proposed methods, particularly for structured kernels like sparse ANOVA kernels.
- The work is well-positioned to inspire further research into deterministic approximations and their applications in machine learning.
Arguments Against Acceptance:
- The scalability of the proposed methods in high-dimensional settings remains a concern, as the sample complexity grows exponentially with dimension.
- The paper's focus on specific kernel types may limit its impact on the broader kernel methods community.
Recommendation:
Overall, this paper provides a compelling contribution to the field of kernel methods, addressing a critical limitation of RFF and offering a novel deterministic alternative. While scalability and generalizability remain open challenges, the strengths of the work outweigh its weaknesses. I recommend acceptance, with the suggestion that the authors expand the experimental scope and discuss potential extensions to other kernel types in future work.