Review of the Paper
This paper introduces a novel parallelisation scheme, termed the "Radon Machine," which enables efficient parallelisation of a broad class of consistent and efficient learning algorithms. The key contribution is the ability to reduce the runtime of learning algorithms from polynomial to polylogarithmic time while maintaining theoretical guarantees on confidence and error bounds. The scheme leverages the concept of Radon points for aggregating hypotheses, which enhances confidence exponentially with the height of the aggregation tree. The authors provide both theoretical analysis and empirical validation, demonstrating significant speed-ups and comparable predictive performance to state-of-the-art parallel learning algorithms, such as those in Spark's MLlib.
The work builds on prior research in parallel machine learning, particularly in the context of Nick's Class (NC) and the efficient parallelisation of polynomial-time algorithms. It addresses an open problem posed by Long and Servedio (2006) regarding the feasibility of parallel learning in polylogarithmic time. Unlike averaging-based parallelisation methods, the Radon Machine achieves better confidence guarantees and runtime reductions. The authors also position their work within the broader context of PAC learning, stochastic gradient descent, and distributed optimization, providing a comprehensive comparison to existing methods.
Strengths:
1. Theoretical Contributions: The paper provides rigorous theoretical guarantees, including bounds on regret and runtime complexity, and demonstrates the doubly exponential reduction in error probability with the Radon Machine.
2. Generality: The proposed scheme is a black-box approach, applicable to a wide range of learning algorithms without requiring modifications to their implementation.
3. Empirical Validation: The experiments convincingly demonstrate substantial speed-ups (up to 700x) and comparable or superior predictive performance compared to Spark MLlib and averaging-based baselines.
4. Clarity of Presentation: The paper is well-organized, with detailed explanations of the algorithm, theoretical proofs, and experimental setup.
Weaknesses:
1. Sample Complexity: The increased sample complexity required for the Radon Machine is a notable trade-off. While the authors acknowledge this, a deeper exploration of its practical implications (e.g., in data-scarce scenarios) would strengthen the paper.
2. Scalability to High Dimensions: The method assumes a finite Radon number, which may limit its applicability to high-dimensional or non-linear models. While the authors suggest potential solutions (e.g., random projections), these are not empirically validated.
3. Communication Overhead: Although the authors claim low communication complexity, a more detailed analysis of communication costs in distributed settings would be beneficial.
4. Limited Diversity of Base Learners: The experiments primarily focus on linear models. Extending the evaluation to non-linear models or more complex learning tasks would enhance the generality claims.
Arguments for Acceptance:
- The paper addresses a fundamental open problem in parallel machine learning and provides a significant theoretical and practical contribution.
- The proposed method is novel, broadly applicable, and empirically validated on large datasets.
- The clarity of exposition and thoroughness of the analysis make the paper accessible and impactful.
Arguments Against Acceptance:
- The reliance on increased sample complexity and the limited empirical evaluation for high-dimensional and non-linear models raise concerns about the general applicability of the method.
- The communication complexity and scalability in real-world distributed systems are not fully explored.
Recommendation: Accept with Minor Revisions  
This paper makes a strong contribution to the field of parallel machine learning, advancing both theoretical understanding and practical implementation. Addressing the concerns about sample complexity and scalability in high-dimensional settings would further strengthen the work.