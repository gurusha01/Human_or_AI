The paper introduces a novel adaptive importance sampling scheme for optimization algorithms, specifically Coordinate Descent (CD) and Stochastic Gradient Descent (SGD), which addresses the computational inefficiencies of existing gradient-based sampling methods. The authors propose a safe approximation of gradient-based sampling using upper and lower bounds on the gradient, which ensures provable performance improvements over uniform and fixed importance sampling. This scheme is computationally efficient, with a complexity of \(O(n \log n)\) per iteration, and integrates seamlessly into existing CD and SGD frameworks. The theoretical contributions are supported by extensive numerical experiments on real-world datasets, demonstrating significant speed-ups in convergence.
Strengths:
1. Theoretical Rigor: The paper provides strong theoretical guarantees for the proposed sampling scheme, proving that it is always better than fixed importance sampling and can approach the performance of optimal gradient-based sampling. The authors also derive bounds on the competitive ratio of their method.
2. Computational Efficiency: The proposed method introduces minimal overhead (\(O(n \log n)\)), making it practical for large-scale applications. The authors demonstrate how safe gradient bounds can be efficiently maintained, particularly for generalized linear models (GLMs).
3. Generality: The scheme is generic and can be applied to both CD and SGD methods, making it broadly applicable across optimization problems.
4. Empirical Validation: The extensive experiments validate the theoretical claims, showing significant improvements in convergence speed and computational efficiency. The results also highlight the robustness of the method, even with loose gradient bounds.
5. Clarity of Contributions: The paper clearly outlines its contributions, including the derivation of the sampling distribution, the efficient algorithm for its computation, and the empirical evaluation.
Weaknesses:
1. Limited Scope of Empirical Evaluation: While the experiments are thorough for GLMs, the paper does not explore the applicability of the method to more complex machine learning models, such as deep neural networks or non-convex objectives.
2. SGD Performance: The proposed sampling scheme shows less pronounced improvements for SGD compared to CD. The authors acknowledge this but do not provide a detailed analysis of why the benefits are less significant in the SGD setting.
3. Dependence on Gradient Bounds: The method relies on the availability of safe gradient bounds, which may not always be straightforward to compute or maintain for more complex models. While the authors propose simple methods for GLMs, further exploration of this limitation is warranted.
4. Clarity of Presentation: While the paper is well-organized, some sections, particularly the mathematical derivations, are dense and may be challenging for readers unfamiliar with the topic.
Arguments for Acceptance:
- The paper addresses a significant problem in optimization for large-scale machine learning, offering a practical and theoretically sound solution.
- The proposed method is computationally efficient and demonstrates strong empirical performance on real-world datasets.
- The theoretical contributions advance the understanding of adaptive importance sampling and provide guarantees not established in prior work.
Arguments Against Acceptance:
- The limited scope of empirical evaluation and weaker performance gains for SGD reduce the generality of the claims.
- The reliance on gradient bounds may limit the applicability of the method to more complex or non-convex problems.
Recommendation:
Overall, the paper makes a strong contribution to the field of optimization in machine learning, particularly for CD methods. While there are some limitations, the strengths outweigh the weaknesses. I recommend acceptance with minor revisions to address the clarity of presentation and to discuss the limitations of the method in more detail.