The paper presents a novel budget-aware strategy for learning accurate yet computationally efficient classifiers and regressors using deep boosted regression trees. The authors extend the gradient boosting framework to incorporate prediction cost penalties, addressing both feature acquisition and model evaluation costs. Unlike prior methods such as GREEDYMISER and BUDGETPRUNE, which often construct shallow or constrained trees, the proposed method allows for the creation of deep trees that are computationally cheap on average. The authors demonstrate the superiority of their approach through extensive experiments on multiple datasets, including Yahoo! LTR, MiniBooNE, and HEPMASS, where their method consistently outperforms existing state-of-the-art algorithms in terms of accuracy-cost tradeoff. The paper also highlights the flexibility of the proposed method in handling different cost scenarios, such as feature-dominated or evaluation-dominated settings, and provides open-source code for reproducibility.
Strengths
1. Technical Soundness: The paper is technically rigorous, with well-defined mathematical formulations for incorporating prediction cost penalties into gradient boosting. The derivation of the cost-aware impurity function and the best-first tree-growing strategy is particularly compelling.
2. Significant Contribution: The proposed method advances the state of the art by addressing key limitations of GREEDYMISER and BUDGETPRUNE. The ability to construct deep, cost-efficient trees is a notable improvement.
3. Comprehensive Evaluation: The authors evaluate their method on diverse datasets and scenarios, demonstrating its robustness and generalizability. The experiments convincingly show that the proposed method achieves better accuracy-cost tradeoffs than existing approaches.
4. Practical Relevance: The algorithm is computationally efficient, easy to implement, and compatible with existing gradient boosting libraries like LightGBM. This makes it highly applicable for real-world use cases.
5. Clarity and Reproducibility: The paper is well-organized, with clear explanations of the problem setup, methodology, and experimental results. The availability of open-source code further enhances its reproducibility.
Weaknesses
1. Limited Theoretical Analysis: While the empirical results are strong, the paper lacks a deeper theoretical analysis of the conditions under which the proposed method guarantees optimality or convergence.
2. Comparison with Neural Networks: The paper briefly mentions neural networks but does not provide a direct experimental comparison. This would have strengthened the argument for using boosted trees in cost-sensitive scenarios.
3. Scalability: Although the method is computationally efficient, the paper does not explicitly discuss its scalability to extremely large datasets or high-dimensional feature spaces beyond the provided experiments.
4. Broader Impact: The paper could benefit from a discussion on the broader implications of cost-aware learning, such as its potential impact on fairness or interpretability in machine learning models.
Arguments for Acceptance
- The paper addresses an important and practical problem in machine learning, advancing the state of the art in cost-aware learning.
- The proposed method is novel, technically sound, and demonstrates significant improvements over existing approaches.
- The experiments are thorough, and the results are compelling, with clear evidence of the method's effectiveness.
Arguments Against Acceptance
- The lack of theoretical guarantees and limited discussion of scalability may leave some questions unanswered.
- A direct comparison with neural networks, which are also widely used in cost-sensitive applications, is missing.
Recommendation
I recommend acceptance of this paper. Its contributions are significant, and the proposed method is both novel and practical. While there are areas for improvement, the strengths of the paper far outweigh its weaknesses. This work is likely to be of high interest to the NeurIPS community and practitioners in cost-sensitive machine learning.