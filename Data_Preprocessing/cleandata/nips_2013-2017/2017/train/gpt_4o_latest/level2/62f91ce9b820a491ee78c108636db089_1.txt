This paper introduces deterministic feature maps for kernel machines, offering an alternative to the widely used random Fourier features (RFF) for kernel approximation. The authors propose leveraging numerical quadrature techniques, such as Gaussian quadrature and sparse grids, to deterministically approximate the Fourier transform integral of shift-invariant kernels. The paper focuses on sparse ANOVA kernels, which are inspired by convolutional layers in CNNs, and demonstrates that deterministic feature maps achieve comparable or superior performance to RFF in terms of accuracy and computational efficiency. Experimental validation on MNIST and TIMIT datasets highlights the practical utility of the proposed methods.
Strengths:
1. Novelty and Contribution: The paper presents a significant innovation by introducing deterministic feature maps for kernel approximation, addressing the probabilistic limitations of RFF. The use of Gaussian quadrature and sparse grids is well-motivated and theoretically grounded.
2. Theoretical Insights: The authors provide rigorous theoretical analysis, including error bounds and sample complexity comparisons. The results show that deterministic methods scale better with the desired approximation error compared to RFF.
3. Practical Relevance: The focus on sparse ANOVA kernels, which mimic the structure of convolutional layers, is timely and relevant, given the success of CNNs in various domains. The experimental results demonstrate that deterministic methods can match or outperform RFF in real-world tasks.
4. Efficiency: The proposed methods reduce feature generation time and computational complexity, making them attractive for large-scale applications.
5. Comprehensive Experiments: The paper validates its claims with experiments on diverse datasets (MNIST and TIMIT), showing both statistical and computational advantages of deterministic feature maps.
Weaknesses:
1. Scalability in High Dimensions: While the paper addresses the curse of dimensionality using sparse grids, the sample complexity still grows exponentially with dimension in some cases. This limitation is acknowledged but could be explored further.
2. Limited Comparison: The experimental evaluation focuses primarily on RFF. Including comparisons with other kernel approximation methods, such as the Nystr√∂m method or quasi-Monte Carlo techniques, would strengthen the paper.
3. Practical Implementation Details: The paper could provide more details on the computational overhead of constructing deterministic quadrature rules, especially for high-dimensional data.
4. Dataset Scope: While MNIST and TIMIT are standard benchmarks, additional experiments on more diverse datasets (e.g., large-scale image or text datasets) would enhance the generalizability of the results.
Arguments for Acceptance:
- The paper addresses an important problem in kernel approximation and provides a novel, theoretically sound solution.
- Deterministic feature maps offer practical advantages in terms of accuracy, efficiency, and reproducibility, making them a valuable contribution to the field.
- The connection between sparse ANOVA kernels and CNNs opens up exciting possibilities for applying kernel methods in new domains.
Arguments Against Acceptance:
- The scalability of the proposed methods in very high-dimensional settings remains a concern.
- The experimental evaluation could be more comprehensive, particularly in terms of dataset diversity and comparison with alternative methods.
Recommendation:
Overall, this paper makes a strong contribution to the field of kernel methods and feature approximation. Its theoretical rigor, practical relevance, and experimental validation make it a valuable addition to the conference. I recommend acceptance, with minor revisions to address scalability concerns and expand the experimental comparisons.