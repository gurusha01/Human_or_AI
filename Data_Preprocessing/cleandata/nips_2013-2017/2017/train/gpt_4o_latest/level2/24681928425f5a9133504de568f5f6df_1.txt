This paper presents a novel computational framework for modeling human-like question generation, treating questions as programs that, when executed on a world state, yield answers. The authors propose a probabilistic generative model that balances informativeness and complexity to predict and synthesize human-like questions. The model is evaluated on a dataset of 605 natural language questions collected from participants in a Battleship-like game, demonstrating its ability to predict human question frequencies and generate novel, contextually relevant questions.
Strengths:
1. Novelty and Contribution: The paper introduces a unique perspective by framing question generation as program synthesis, leveraging compositionality and computability. This approach is innovative and significantly extends the capabilities of active machine learning beyond simple label queries.
2. Evaluation and Results: The model is rigorously evaluated using both quantitative (log-likelihood of human questions) and qualitative (novel question synthesis) metrics. The results demonstrate strong alignment with human question-asking behavior and highlight the importance of balancing informativeness and complexity.
3. Generative Capability: The ability of the model to generate novel, human-like questions that were not present in the training set is a notable achievement, showcasing its creative potential.
4. Clarity and Organization: The paper is well-written and logically structured, with clear explanations of the grammar, probabilistic model, and evaluation methods. The inclusion of supplementary materials enhances reproducibility.
Weaknesses:
1. Domain-Specific Grammar: While the authors acknowledge this limitation, the grammar used in the model is partially tailored to the Battleship domain. Extending the approach to other domains may require significant manual engineering, which limits its generalizability.
2. Natural Language Gap: The model operates on semantic representations rather than raw natural language, which may hinder its applicability to real-world scenarios without additional integration with natural language processing techniques.
3. Sparse Human Data: The dataset is relatively small, and many questions were asked only once, which may limit the robustness of the model's predictions. Additionally, the evaluation relies heavily on this dataset, raising concerns about overfitting.
4. Complexity Feature Dominance: The results suggest that complexity plays a disproportionately large role in predicting human questions. While this aligns with human preferences for concise questions, it may overshadow other important factors like creativity or contextual nuance.
Arguments for Acceptance:
- The paper addresses a challenging and underexplored problem in AI and cognitive science, offering a novel and promising approach.
- The results are compelling, with strong quantitative and qualitative evidence supporting the model's effectiveness.
- The work has potential applications in active learning, AI dialogue systems, and cognitive modeling.
Arguments Against Acceptance:
- The domain-specific nature of the grammar and the reliance on semantic representations limit the model's scalability and applicability to broader contexts.
- The evaluation dataset is small and domain-specific, which may restrict the generalizability of the findings.
Recommendation: Accept with minor revisions. While the paper has some limitations in generalizability and scalability, its contributions to the field of question generation are significant and innovative. Addressing the natural language gap and demonstrating the model's applicability to other domains would strengthen the work further.