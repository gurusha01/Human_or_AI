This paper investigates the limitations of gradient descent-based optimization methods when applied to kernel methods and proposes a novel preconditioning scheme, EigenPro, to address these challenges. The authors identify a fundamental issue: gradient descent with smooth kernels can only approximate a vanishingly small fraction of the function space after a polynomial number of iterations, leading to over-regularization and suboptimal performance. EigenPro mitigates this by introducing approximate second-order information through a low-rank preconditioner, significantly improving convergence rates and computational efficiency. The authors demonstrate that EigenPro achieves state-of-the-art performance on large datasets while requiring a fraction of the computational budget.
Strengths:
1. Novel Contribution: The identification of the computational reach limitation of gradient descent with smooth kernels is a significant theoretical insight. The EigenPro method is a practical and elegant solution to this problem, combining low-rank approximations with first-order methods.
2. Strong Empirical Results: The experimental results convincingly demonstrate EigenPro's acceleration (6-35x) over baseline methods like Pegasos and SGD with random Fourier features. The method achieves competitive or superior performance on large-scale datasets with reduced computational costs.
3. Practical Relevance: The proposed method is highly practical, as it is compatible with stochastic gradient descent and scalable to large datasets. The use of randomized SVD for preconditioner computation is efficient and well-suited for modern hardware.
4. Clarity of Presentation: The paper is well-organized, with a clear explanation of the theoretical limitations of gradient descent and the design of EigenPro. The inclusion of step size selection and computational cost analysis adds depth to the discussion.
Weaknesses:
1. Limited Theoretical Analysis of EigenPro: While the paper provides strong empirical evidence, the theoretical guarantees for EigenPro's performance, especially in the stochastic setting, are less developed. A more rigorous analysis of convergence rates and robustness to approximate eigenvector computation would strengthen the contribution.
2. Overhead of Preconditioning: Although the authors claim low overhead, the additional cost of computing and applying the preconditioner is not fully quantified in comparison to other second-order methods. This could be clarified further.
3. Limited Discussion of Limitations: The paper does not explicitly discuss potential drawbacks of EigenPro, such as its reliance on accurate eigenvector estimation or its applicability to non-smooth kernels.
Pro and Con Arguments for Acceptance:
Pro:
- The paper addresses a fundamental limitation of gradient descent for kernel methods and provides a novel, practical solution.
- The empirical results are robust and demonstrate significant improvements over existing methods.
- The work is highly relevant to the NIPS community, advancing the state of the art in scalable kernel methods and optimization.
Con:
- The theoretical analysis of EigenPro is less comprehensive, particularly in stochastic settings.
- The paper could better quantify the trade-offs between preconditioning overhead and acceleration.
Recommendation:
I recommend acceptance of this paper. It makes a significant contribution to the field by addressing a critical limitation of gradient descent for kernel methods and proposing a practical, scalable solution. While the theoretical analysis could be expanded, the empirical results and practical utility of EigenPro make it a valuable addition to the conference.