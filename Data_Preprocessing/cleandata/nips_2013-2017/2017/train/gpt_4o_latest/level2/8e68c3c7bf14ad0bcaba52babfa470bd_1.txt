This paper addresses the challenge of incorporating natural language feedback from non-expert users into reinforcement learning (RL) agents, with a focus on image captioning. The authors propose a hierarchical phrase-based RNN model that integrates human feedback to improve captioning quality. By leveraging descriptive feedback rather than numeric rewards, the approach aims to provide stronger learning signals, facilitating better credit assignment and exploration. The authors also design a feedback network (FBN) to process human-provided feedback and integrate it into policy gradient optimization. Experiments on the MS-COCO dataset demonstrate that the proposed method outperforms baselines, including RL agents trained with ground-truth captions alone.
Strengths:
1. Novelty: The paper introduces a unique approach to integrating natural language feedback into RL for image captioning. While prior work has explored numeric feedback, this paper's use of descriptive feedback is a significant innovation.
2. Technical Contributions: The hierarchical phrase-based RNN model is well-designed for incorporating feedback, and the feedback network (FBN) effectively translates human annotations into actionable rewards for RL optimization.
3. Practical Relevance: The work addresses a critical problem in human-AI interaction, particularly for non-expert users, making it highly relevant for real-world applications like household robots.
4. Experimental Validation: The authors conduct extensive experiments on the MS-COCO dataset, showing consistent improvements over baselines. The inclusion of human evaluation adds credibility to the results.
5. Reproducibility: The authors promise to release their code and data, which is commendable and supports reproducibility.
Weaknesses:
1. Limited Scope of Feedback: While the paper focuses on image captioning, it does not explore how the proposed method generalizes to other RL tasks with different modalities or domains.
2. Annotation Noise: The paper acknowledges noise in human annotations but does not propose robust strategies to mitigate its impact on training.
3. Baseline Comparisons: Although the paper compares its method to several baselines, it would benefit from more comparisons to state-of-the-art captioning models that do not rely on RL.
4. Scalability: The reliance on human feedback raises questions about scalability for large datasets or real-time applications. The authors could discuss strategies to reduce human effort, such as semi-supervised or active learning approaches.
Suggestions for Improvement:
1. Extend the discussion on generalizing the approach to other RL tasks beyond image captioning.
2. Explore methods to handle noisy annotations, such as confidence weighting or filtering unreliable feedback.
3. Include comparisons with more advanced non-RL captioning models to better contextualize the performance gains.
4. Discuss potential applications and limitations in real-world scenarios, particularly regarding the cost of human feedback.
Recommendation:
This paper makes a meaningful contribution to the field of human-in-the-loop reinforcement learning by introducing a novel way to leverage natural language feedback. While there are some limitations, the strengths of the work outweigh the weaknesses. I recommend acceptance, provided the authors address the scalability and generalization concerns in the final version.