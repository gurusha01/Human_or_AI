The paper investigates deep signal representations that are near-invariant to transformations and stable to diffeomorphisms, while preserving signal information. By generalizing convolutional kernel networks (CKNs) and analyzing the geometry of their corresponding reproducing kernel Hilbert space (RKHS), the authors establish theoretical stability guarantees for these representations. They demonstrate that a broad class of convolutional neural networks (CNNs) with smooth homogeneous activation functions can inherit these stability properties. The paper also explores the practical implications of the theoretical framework, including discretization, kernel approximations, and connections to standard CNN architectures.
Strengths:
1. Theoretical Contribution: The paper provides a rigorous mathematical framework for understanding stability and invariance properties of deep signal representations. The use of RKHS to analyze CNNs is novel and bridges kernel methods with deep learning.
2. Generality: The proposed framework encompasses both CKNs and a large class of CNNs, making the results broadly applicable.
3. Stability Analysis: The stability results, particularly under diffeomorphisms, are well-formulated and extend prior work on scattering transforms. The explicit bounds for stability and invariance are valuable.
4. Practical Relevance: The discussion on discretization and kernel approximations ensures that the theoretical results are applicable to real-world scenarios. The connection to existing CNN architectures enhances the paper's impact.
5. Clarity in Mathematical Derivations: The mathematical rigor and detailed proofs (albeit in the appendix) are commendable, providing confidence in the validity of the claims.
Weaknesses:
1. Limited Empirical Validation: While the theoretical contributions are strong, the paper lacks sufficient experimental evidence to validate the practical utility of the proposed framework. For example, comparisons with standard CNNs or scattering networks on benchmark datasets would strengthen the claims.
2. Complexity of Presentation: The paper is dense and assumes a high level of familiarity with RKHS, kernel methods, and harmonic analysis. This may limit accessibility for a broader audience at the conference.
3. Novelty in Practical Impact: While the theoretical results are significant, the practical implications for improving CNN performance (e.g., in terms of accuracy or robustness) remain unclear. The paper could benefit from more concrete examples or applications.
4. Limited Discussion of Limitations: The paper does not sufficiently address potential drawbacks, such as computational overhead introduced by kernel approximations or the scalability of the proposed framework to very deep architectures.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution by extending stability and invariance results to a broader class of CNNs using RKHS.
- It provides a novel perspective on the geometry of deep signal representations, which could inspire future research in both kernel methods and deep learning.
- The connection between stability, invariance, and generalization is well-articulated and aligns with ongoing efforts to understand the theoretical foundations of deep learning.
Arguments Against Acceptance:
- The lack of empirical validation limits the practical impact of the work.
- The dense and technical presentation may make it challenging for a broader audience to fully appreciate the contributions.
- The novelty in terms of practical improvements over existing methods (e.g., scattering networks or standard CNNs) is not convincingly demonstrated.
Recommendation:
I recommend acceptance with minor revisions. While the paper excels in theoretical rigor and novelty, adding empirical results and clarifying the practical implications would significantly enhance its impact. The authors should also consider simplifying some of the mathematical exposition to improve accessibility.