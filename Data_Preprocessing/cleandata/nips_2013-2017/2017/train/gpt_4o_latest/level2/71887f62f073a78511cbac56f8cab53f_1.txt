This paper introduces a novel randomized coordinate descent method for solving a broad class of composite convex optimization problems. The authors combine four key techniques—smoothing, acceleration, homotopy, and non-uniform coordinate sampling—within a primal-dual gap-based framework. Their primary contribution is the development of a method that achieves the best-known convergence rate of \(O(n/k)\) for this problem class, which is rigorously proven for the first time. The paper also provides numerical evidence to validate the theoretical results, comparing the proposed method (SMART-CD) with state-of-the-art algorithms across several applications, including support vector machines and brain imaging.
Strengths:
1. Technical Novelty: The paper extends existing randomized coordinate descent methods to a more general three-composite convex optimization setting, filling a significant gap in the literature. The combination of smoothing and homotopy strategies is particularly innovative.
2. Theoretical Contributions: The authors provide rigorous convergence rate guarantees, which are a notable improvement over prior work. The reduction from \(O(n^2/k)\) to \(O(n/k)\) using a homotopy strategy is a key advancement.
3. Practical Relevance: The method is applicable to large-scale machine learning problems, such as support vector machines and total variation-regularized regression, which are highly relevant to the NIPS community.
4. Numerical Validation: The experiments are comprehensive and demonstrate the practical efficiency of SMART-CD compared to existing methods. The inclusion of real-world datasets (e.g., fMRI data and SVM benchmarks) strengthens the paper's impact.
5. Clarity of Presentation: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup.
Weaknesses:
1. Limited Discussion of Limitations: While the authors acknowledge the potential for incorporating strong convexity to achieve faster rates, they do not discuss other limitations of their approach, such as scalability to extremely high-dimensional problems or sensitivity to parameter tuning.
2. Complexity of Algorithm Description: The technical details, while thorough, may be difficult for non-expert readers to follow. Simplified explanations or visual aids (e.g., flowcharts) could improve accessibility.
3. Comparative Analysis: Although the paper compares SMART-CD with several baselines, some state-of-the-art methods (e.g., stochastic gradient-based approaches) are not included. Additionally, the performance of SMART-CD in non-smooth constrained settings could be explored further.
4. Reproducibility: While the theoretical contributions are robust, the paper does not provide sufficient implementation details (e.g., code or pseudocode for efficient variants) to ensure reproducibility.
Arguments for Acceptance:
- The paper makes a significant theoretical and practical contribution to randomized coordinate descent methods.
- The proposed algorithm is rigorously analyzed and empirically validated on relevant applications.
- The work addresses a challenging and important problem in convex optimization, with clear implications for large-scale machine learning.
Arguments Against Acceptance:
- The technical complexity may limit accessibility for a broader audience.
- The paper could benefit from a more thorough discussion of limitations and additional comparisons with alternative methods.
Recommendation: Accept with minor revisions. The paper is a strong contribution to the field, but addressing the weaknesses outlined above (e.g., reproducibility and broader comparisons) would enhance its impact.