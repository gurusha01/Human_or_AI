The paper introduces Stochastic MISO (S-MISO), a novel variance reduction algorithm designed for stochastic optimization problems with random perturbations in input data, a common scenario in machine learning tasks involving data augmentation. The authors claim that S-MISO achieves faster convergence rates than stochastic gradient descent (SGD) by reducing gradient variance due to data sampling, while maintaining robustness to perturbations. This work addresses a gap in existing variance reduction methods, which are typically incompatible with stochastic formulations involving random perturbations.
Strengths
1. Novelty and Contribution: The paper proposes the first variance reduction algorithm tailored to hybrid settings where the objective combines finite-sum and stochastic components. This is a significant extension of existing methods like MISO and SVRG.
2. Theoretical Guarantees: The authors provide rigorous convergence analysis, demonstrating that S-MISO achieves a faster convergence rate than SGD, with gains proportional to the ratio of total gradient variance to perturbation-induced variance. The theoretical results are well-supported by empirical evidence.
3. Practical Relevance: The algorithm is applicable to real-world scenarios such as data augmentation in image classification and Dropout regularization in sparse datasets. These applications are convincingly demonstrated through experiments.
4. Comprehensive Experiments: The experiments span diverse tasks (image classification, gene expression analysis, sentiment analysis) and include comparisons with SGD and N-SAGA. The results consistently show S-MISO's superiority in terms of convergence speed and suboptimality.
Weaknesses
1. Memory Requirements: S-MISO requires storing auxiliary variables for each data point, which may be infeasible for very large datasets. While the authors acknowledge this limitation, alternative strategies to mitigate memory usage (e.g., trade-offs with computation) are not explored in depth.
2. Limited Comparison with Alternatives: While the paper compares S-MISO with SGD and N-SAGA, it does not benchmark against other recent stochastic optimization methods, such as Adam or AdaGrad, which are widely used in practice.
3. Clarity of Presentation: The paper is dense and highly technical, which may hinder accessibility for non-expert readers. For instance, the explanation of the Lyapunov function and step-size strategies could benefit from additional intuition or visual aids.
4. Focus on Strong Convexity: The algorithm and analysis are restricted to strongly convex objectives, limiting its applicability to non-convex problems, which are prevalent in modern deep learning.
Pro and Con Arguments for Acceptance
Pros:
- Novel and theoretically sound contribution to variance reduction in stochastic settings.
- Strong empirical results demonstrating practical utility.
- Addresses a relevant and underexplored problem in machine learning.
Cons:
- High memory requirements limit scalability.
- Lack of comparison with broader optimization baselines.
- Focus on strongly convex objectives may reduce impact in non-convex optimization.
Recommendation
I recommend acceptance of this paper, as it presents a significant advancement in stochastic optimization with variance reduction, supported by strong theoretical and empirical results. However, addressing the scalability and applicability to non-convex settings in future work would enhance its impact.