The paper presents a novel approach to measuring distances between distributions while encoding invariance to additive symmetric noise, addressing a critical issue in nonparametric two-sample testing and learning on distributions. The authors propose two methods: (1) a nonparametric measure of asymmetry in paired sample differences, termed the Symmetric Mean Embedding (SME) test, and (2) a weighted distance between empirical phase functions, termed the Phase Discrepancy (PhD). These methods aim to disentangle signal differences from noise differences, providing robust tools for hypothesis testing and learning on distributions.
Strengths
The paper makes significant contributions to the field of kernel methods and distributional learning. The SME test is a practical and theoretically grounded extension of the Maximum Mean Discrepancy (MMD) test, offering robustness to irrelevant noise. The PhD framework is particularly innovative, leveraging phase functions to construct invariant features that are robust to symmetric additive noise. The authors provide a thorough theoretical foundation, including proofs and propositions, and demonstrate the utility of their methods through extensive experiments on synthetic and real-world datasets, such as the Higgs, Aerosol, and Dark Matter datasets. The experiments convincingly show that the proposed methods outperform traditional approaches in scenarios with noise, highlighting their practical significance.
The paper is well-organized, with a clear structure that guides the reader through the theoretical background, methodology, and experimental results. The inclusion of practical considerations, such as the trade-off between noise invariance and signal retention, adds depth to the analysis. The authors also address limitations, such as the PhD test's sensitivity to large noise levels, and propose combining SME and PhD for exploratory analysis.
Weaknesses
While the paper is technically sound, the clarity of some sections could be improved. For instance, the mathematical exposition in the derivation of phase features (Section 3) may be challenging for readers unfamiliar with characteristic functions and Fourier analysis. Additionally, while the authors acknowledge the limitations of the PhD test under high noise levels, the discussion on mitigating these issues could be expanded. The reliance on permutation tests for the PhD test's null distribution is practical but may not scale well to large datasets. Finally, the paper could benefit from a more detailed comparison with recent work in invariant representation learning and robust kernel methods, beyond the cited references.
Evaluation
- Quality: The paper is technically rigorous, with well-supported claims through theoretical analysis and experiments. However, the PhD test's limitations under high noise levels warrant further exploration.
- Clarity: The paper is generally clear but could improve accessibility for non-experts in kernel methods and Fourier analysis.
- Originality: The work is highly original, introducing novel methods for noise-invariant distributional learning and testing.
- Significance: The results are significant, addressing a practical and theoretical gap in the field, with potential applications in noisy real-world datasets.
Recommendation
I recommend accepting the paper, as it makes a substantial contribution to the field of kernel methods and distributional learning. However, the authors should consider revising the manuscript to improve clarity and address the scalability of the PhD test. Overall, the proposed methods are innovative, practical, and likely to inspire further research in robust learning and testing frameworks.