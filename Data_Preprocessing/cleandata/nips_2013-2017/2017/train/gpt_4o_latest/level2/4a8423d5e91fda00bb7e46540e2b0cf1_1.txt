The paper presents a novel approach to selective classification in the context of deep neural networks (DNNs), addressing a significant gap in the literature. The authors propose a method to construct a selective classifier by combining a trained DNN with a rejection function, allowing users to set a desired risk level. The classifier abstains from predictions when necessary to ensure the specified risk is not exceeded, with high probability. The empirical results on CIFAR-10, CIFAR-100, and ImageNet datasets demonstrate the effectiveness of the proposed method, achieving unprecedented performance guarantees, such as a 2% error in top-5 ImageNet classification with 99.9% confidence and 60% test coverage.
Strengths:
1. Novelty and Relevance: The paper introduces selective classification to DNNs, a domain where it has been underexplored. The work is timely and relevant, especially for mission-critical applications like autonomous driving and medical diagnostics.
2. Empirical Validation: The experiments are thorough, spanning multiple datasets (CIFAR-10, CIFAR-100, ImageNet) and architectures (VGG-16, RESNET-50). The results convincingly demonstrate the method's viability and superiority over existing baselines.
3. Practical Utility: The ability to guarantee risk levels with high confidence is a significant contribution, making the method highly applicable in real-world scenarios where reliability is paramount.
4. Clarity of Presentation: The paper is well-organized, with clear explanations of the problem setting, methodology, and experimental results. The inclusion of risk-coverage curves and detailed tables enhances understanding.
Weaknesses:
1. Theoretical Depth: While the empirical results are strong, the theoretical analysis could be more comprehensive. For instance, the paper relies heavily on existing bounds (e.g., Lemma 3.1) without exploring their limitations or potential improvements in the context of DNNs.
2. Comparison with Related Work: Although the paper references prior work on selective classification and uncertainty estimation (e.g., MC-dropout), the discussion of how the proposed method compares to these approaches could be more detailed. For example, the authors could provide a deeper analysis of why SR outperforms MC-dropout on ImageNet.
3. Scalability: The method's computational efficiency, particularly for large-scale datasets like ImageNet, is not discussed in detail. The binary search in the SGR algorithm and the reliance on confidence-rate functions like SR and MC-dropout may introduce overheads that could limit scalability.
4. Limitations: While the authors acknowledge that the rejection function is trained separately from the classifier, they do not explore the potential trade-offs or limitations of this approach in depth. Joint training of the classifier and rejection function is mentioned as future work but is not addressed experimentally.
Arguments for Acceptance:
- The paper addresses an important and underexplored problem in DNNs with a novel and practical solution.
- The empirical results are compelling, demonstrating significant improvements in risk control and coverage.
- The method has broad applicability in critical domains, making it a valuable contribution to the field.
Arguments Against Acceptance:
- The theoretical contributions are limited, and the scalability of the method is not fully addressed.
- The comparison with related work could be more rigorous, particularly in explaining why the proposed method outperforms alternatives.
Recommendation:
Overall, this paper makes a strong case for the adoption of selective classification in DNNs and provides a practical framework for achieving guaranteed risk control. While there are areas for improvement, particularly in theoretical analysis and scalability, the paper's contributions are significant and well-supported by empirical evidence. I recommend acceptance, with minor revisions to address the noted weaknesses.