The paper revisits the classical analysis of generative versus discriminative models, extending it to general exponential families and high-dimensional settings. The authors introduce a novel framework based on a "soft" notion of separability for loss functions, enabling the derivation of `1 convergence rates for general M-estimators. This framework is then applied to analyze the nuanced behaviors of generative and discriminative models in high-dimensional regimes, with a focus on differential parameter estimation. The theoretical results are instantiated for isotropic Gaussian and Gaussian Markov Random Field (GMRF) models, and validated through simulations.
Strengths:
1. Novelty and Contribution: The paper provides a significant extension of prior work by analyzing generative and discriminative models in high-dimensional settings. The introduction of a soft separability notion is innovative and has potential applications beyond the scope of this paper.
2. Theoretical Rigor: The authors present a well-structured theoretical framework, deriving `1 and `2 convergence rates for both generative and discriminative models. The results are robustly supported by mathematical proofs and detailed derivations.
3. Practical Insights: The paper highlights the trade-offs between generative and discriminative models in terms of sample complexity and separability, offering actionable insights for practitioners working in high-dimensional data settings.
4. Empirical Validation: The simulations corroborate the theoretical findings, demonstrating the practical relevance of the proposed framework. The experiments effectively illustrate the differences in sample complexity and classification performance between the two approaches.
Weaknesses:
1. Clarity: While the paper is mathematically rigorous, it is dense and difficult to follow for readers without a strong background in high-dimensional statistics or exponential families. The introduction of the separability notion, while novel, could benefit from more intuitive explanations and examples.
2. Limited Scope of Experiments: The experiments focus primarily on isotropic Gaussian and GMRF models. While these are illustrative, the generalizability of the results to other exponential family distributions or real-world datasets is not fully explored.
3. Practical Applicability: The paper assumes access to well-specified generative models, which may not always hold in practice. The authors acknowledge this limitation but do not provide sufficient discussion on how their framework might perform under model misspecification.
4. Comparisons to Prior Work: While the authors contrast their results with prior work, such as Li et al. (2017), the discussion could be more comprehensive. For instance, it would be helpful to explicitly quantify the improvements in sample complexity achieved by their framework.
Arguments for Acceptance:
- The paper makes a significant theoretical contribution by extending the analysis of generative and discriminative models to high-dimensional settings.
- The introduction of the separability framework is novel and has potential for broader applications.
- The results are well-supported by rigorous proofs and validated through simulations.
Arguments Against Acceptance:
- The paper's clarity and accessibility could be improved, particularly for a broader audience at NeurIPS.
- The experimental validation is somewhat narrow in scope, limiting the generalizability of the findings.
- The practical implications of the framework under real-world conditions (e.g., model misspecification) are not fully addressed.
Recommendation:
I recommend acceptance with minor revisions. The paper presents a strong theoretical contribution and provides valuable insights into the trade-offs between generative and discriminative models in high-dimensional settings. However, the authors should improve the clarity of their exposition and broaden the scope of their experiments to enhance the paper's accessibility and practical relevance.