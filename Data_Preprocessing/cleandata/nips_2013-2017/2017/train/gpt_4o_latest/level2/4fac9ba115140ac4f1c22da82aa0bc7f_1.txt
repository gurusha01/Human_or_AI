Review
This paper presents a novel budget-aware adaptation of gradient boosting, termed CEGB (Cost-Efficient Gradient Boosting), which incorporates prediction cost penalties into the learning process. The authors propose a method to construct deep regression trees that are computationally cheap on average while maintaining high accuracy. The key contributions include a cost-aware tree-growing algorithm that optimizes both feature acquisition and evaluation costs, as well as a demonstration of significant performance improvements over state-of-the-art methods like GREEDYMISER and BUDGETPRUNE across multiple datasets. The authors also provide open-source code, enhancing the reproducibility and practical utility of their approach.
Strengths:
1. Novelty and Significance: The paper addresses an important and underexplored problem of balancing prediction accuracy with computational cost, which is critical for real-world applications. The proposed method advances the state of the art by enabling the construction of deep, cost-efficient trees, a feature not supported by existing methods like GREEDYMISER and BUDGETPRUNE.
   
2. Empirical Validation: The experimental results are comprehensive, spanning diverse datasets (e.g., Yahoo! LTR, MiniBooNE, HEPMASS) and scenarios (e.g., feature-dominated costs, evaluation-dominated costs). The method consistently outperforms baselines, demonstrating its robustness and generalizability.
3. Practical Utility: The algorithm is easy to implement using existing gradient boosting libraries and does not significantly increase training time. The availability of open-source code further enhances its accessibility to practitioners.
4. Theoretical Rigor: The paper provides a detailed mathematical formulation of the cost penalties and their integration into the gradient boosting framework. The derivation of the cost-aware impurity function and the use of best-first tree growth are well-justified and innovative.
5. Clarity: The paper is well-structured, with clear explanations of the problem setup, related work, and the proposed method. The figures (e.g., comparisons of tree structures) effectively illustrate the advantages of the approach.
Weaknesses:
1. Limited Discussion of Limitations: While the authors acknowledge the NP-hard nature of tree optimization, the paper does not thoroughly discuss potential limitations of the proposed method, such as scalability to extremely large datasets or sensitivity to hyperparameters like Î».
2. Comparative Analysis: Although the paper demonstrates superior performance over GREEDYMISER and BUDGETPRUNE, it would benefit from a deeper analysis of why these methods fail in specific scenarios. For example, a more detailed breakdown of cost savings (e.g., feature vs. evaluation costs) would provide additional insights.
3. Generality of Cost Functions: The method assumes predefined feature and evaluation costs, which may not always be available or straightforward to estimate in real-world applications. A discussion on how to handle unknown or dynamic cost settings would strengthen the paper.
4. Broader Impact: While the paper focuses on machine learning applications, it does not explore potential extensions to other domains, such as reinforcement learning or online decision-making, where cost-aware predictions are also crucial.
Arguments for Acceptance:
- The paper introduces a significant innovation in cost-aware learning, with strong theoretical and empirical support.
- The method is practical, reproducible, and broadly applicable to various machine learning tasks.
- The results demonstrate clear improvements over existing methods, advancing the state of the art.
Arguments Against Acceptance:
- The paper could provide a more thorough discussion of its limitations and broader applicability.
- The reliance on predefined cost functions may limit its generalizability to some real-world scenarios.
Recommendation: Strong Accept. The paper makes a substantial contribution to cost-aware machine learning and is likely to have a significant impact on both research and practice. Minor revisions to address the noted weaknesses would further enhance its quality.