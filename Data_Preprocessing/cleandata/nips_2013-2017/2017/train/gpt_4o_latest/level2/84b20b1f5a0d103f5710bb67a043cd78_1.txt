The paper proposes an innovative approach to improve the Greedy Coordinate Descent (GCD) algorithm for solving `1-regularized problems by incorporating Nesterov's acceleration and stochastic optimization strategies. The authors introduce a novel greedy selection rule based on an `1-norm square approximation, which is convex but challenging to solve. To address this, they propose the SOft ThreshOlding PrOjection (SOTOPO) algorithm, which efficiently solves the `1-regularized `1-norm square approximation problem. The resulting Accelerated Stochastic Greedy Coordinate Descent (ASGCD) algorithm achieves the optimal convergence rate of \(O(\sqrt{1/\epsilon})\) and reduces the iteration complexity of greedy selection by a factor of the sample size. The paper demonstrates both theoretical and empirical improvements, particularly for high-dimensional and dense problems with sparse solutions.
Strengths:
1. Novelty and Innovation: The paper introduces a new greedy selection rule and the SOTOPO algorithm, which extend the applicability of GCD by addressing its limitations in terms of convexity and iteration complexity. The integration of Nesterov's acceleration and stochastic optimization into GCD is a significant contribution.
2. Theoretical Rigor: The authors provide a thorough theoretical analysis of the ASGCD algorithm, including convergence guarantees and complexity bounds. The derivation of the SOTOPO algorithm is detailed and mathematically sound.
3. Practical Relevance: The proposed ASGCD algorithm is particularly suited for large-scale, high-dimensional problems with sparse solutions, which are common in machine learning applications.
4. Empirical Validation: The experimental results on real-world datasets demonstrate the effectiveness of ASGCD compared to state-of-the-art methods like Katyusha and AFG. The use of data access as a performance metric aligns with recent trends in optimization research.
Weaknesses:
1. Clarity and Accessibility: The paper is dense with technical details, which may hinder accessibility for readers unfamiliar with advanced optimization techniques. The derivation of the SOTOPO algorithm, while rigorous, could benefit from additional intuitive explanations or visual aids.
2. Comparative Analysis: While the paper compares ASGCD with several baseline algorithms, it does not include comparisons with other recent GCD variants or approximate greedy selection methods, such as those in [9]. This could provide a more comprehensive evaluation.
3. Logarithmic Factor: The theoretical bound for ASGCD includes a \(\log(d)\) factor, which the authors acknowledge as a potential limitation. Further discussion or experiments to explore whether this factor is necessary would strengthen the paper.
Pro and Con Arguments for Acceptance:
Pros:
- The paper addresses a critical limitation of GCD algorithms and proposes a well-motivated solution.
- Theoretical and empirical results are robust and demonstrate significant improvements over existing methods.
- The work is relevant to the NIPS community, given its focus on scalable optimization for machine learning.
Cons:
- The paper's clarity could be improved, particularly for non-expert readers.
- The omission of comparisons with other GCD variants leaves some gaps in the evaluation.
Recommendation:
I recommend accepting the paper, as it makes a substantial contribution to the field of optimization for machine learning. However, the authors should consider improving the clarity of the presentation and expanding the comparative analysis in the final version.