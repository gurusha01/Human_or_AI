The paper introduces FSUCRL, a novel algorithm for reinforcement learning (RL) with options that eliminates the need for prior knowledge about the parameters of options, such as their cumulative reward and duration distributions. The authors achieve this by combining the semi-Markov decision process (SMDP) view of options with their intrinsic Markov structure, transforming each option into an irreducible Markov chain. This approach allows the computation of optimistic policies using stationary distributions and SMDP dynamics, bypassing the limitations of prior algorithms like SUCRL. The paper provides theoretical regret bounds for FSUCRL, showing that its performance matches SUCRL up to an additive regret term, and supports these findings with empirical results on grid-world and maze environments.
Strengths:
1. Novelty and Originality: The paper addresses a significant limitation of prior work by removing the need for strong prior knowledge about options, which is often impractical in real-world scenarios. The transformation of options into irreducible Markov chains is a creative and theoretically grounded approach.
2. Theoretical Contributions: The regret analysis is rigorous, and the authors provide clear comparisons between FSUCRL, SUCRL, and UCRL. The introduction of a pseudo-diameter term to quantify the complexity of options is insightful.
3. Practical Relevance: By eliminating the reliance on prior knowledge, FSUCRL is more applicable to scenarios where options are automatically generated, such as in deep RL. This aligns with the growing interest in hierarchical RL.
4. Empirical Validation: The experiments demonstrate that FSUCRL retains the advantages of temporal abstraction while being competitive with SUCRL and UCRL. The results highlight the robustness of FSUCRL, even when prior knowledge is unavailable.
Weaknesses:
1. Complexity of Analysis: While the theoretical contributions are strong, the regret bound for FSUCRL includes terms that are difficult to interpret intuitively, such as the pseudo-diameter. This could limit the accessibility of the results to a broader audience.
2. Empirical Scope: The experiments are limited to toy domains and small-scale environments. While these are sufficient for proof-of-concept, additional experiments on more complex, real-world tasks would strengthen the paper's claims.
3. Comparison to SUCRL: The paper highlights scenarios where FSUCRL outperforms SUCRL, but it would benefit from a deeper analysis of cases where the additional regret term might dominate, leading to worse performance.
4. Algorithmic Complexity: While the authors claim that FSUCRL has similar computational complexity to UCRL, the nested extended value iteration (EVI) in FSUCRLV2 could be computationally expensive in larger state spaces. This aspect is not thoroughly explored.
Recommendation:
The paper makes a solid contribution to the field of hierarchical RL by addressing a critical limitation of prior work and proposing a theoretically sound and empirically validated solution. While the complexity of the analysis and limited empirical scope leave room for improvement, the novelty and significance of the work outweigh these concerns. I recommend acceptance with minor revisions to improve clarity and expand the empirical evaluation. 
Pro and Con Arguments:
Pros:
- Novel and practical approach to learning with options without prior knowledge.
- Rigorous theoretical analysis with clear comparisons to existing methods.
- Empirical results demonstrate the algorithm's effectiveness and robustness.
Cons:
- Limited empirical evaluation on complex tasks.
- Theoretical bounds include terms that are challenging to interpret.
- Potential computational overhead of nested EVI in large-scale problems.