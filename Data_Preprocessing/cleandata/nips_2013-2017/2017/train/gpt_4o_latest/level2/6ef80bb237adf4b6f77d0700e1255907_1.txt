This paper introduces an accelerated first-order method for geodesically convex optimization, extending Nesterov's accelerated gradient descent from Euclidean to nonlinear Riemannian spaces. The authors propose two nonlinear operators to replace the linear extrapolation step in Euclidean space, achieving improved convergence rates for geodesically strongly convex problems (from \(O((1-\mu/L)^k)\) to \(O((1-\sqrt{\mu/L})^k)\)) and geodesically convex problems (from \(O(1/k)\) to \(O(1/k^2)\)). A specific application to matrix Karcher mean problems is presented, with experimental results validating the theoretical improvements.
Strengths:
1. Novelty and Originality: The paper addresses a long-standing open problem in Riemannian optimization, providing a significant extension of Nesterov's method to nonlinear spaces. This work is novel and advances the state of the art in geodesically convex optimization.
2. Theoretical Contributions: The authors rigorously derive two equations for geodesically convex and strongly convex cases, providing detailed convergence analysis. The results align with the optimal rates achieved in Euclidean space, which is a notable achievement.
3. Practical Relevance: The application to matrix Karcher mean problems demonstrates the practical utility of the proposed method, particularly for problems that are non-convex in Euclidean space but geodesically convex.
4. Experimental Validation: The experiments on synthetic data convincingly show that the proposed method converges faster than existing methods (e.g., RGD and LRBFGS), both in terms of iterations and running time.
Weaknesses:
1. Clarity: While the paper is mathematically rigorous, it is dense and may be challenging for readers unfamiliar with Riemannian geometry. Key concepts such as exponential maps and parallel transport could benefit from more intuitive explanations or visual aids.
2. Limited Scope of Experiments: The experiments focus primarily on synthetic data for matrix Karcher mean problems. It would strengthen the paper to include real-world applications or additional problem domains to demonstrate broader applicability.
3. Comparison with Other Accelerated Methods: The paper does not compare its method against other recent advancements in Riemannian optimization, such as stochastic or retraction-based methods, which could provide a more comprehensive evaluation.
4. Reproducibility: While the theoretical framework is detailed, the paper lacks sufficient implementation details (e.g., parameter tuning, computational complexity) to ensure reproducibility.
Suggestions for Improvement:
1. Enhance the clarity of the paper by including intuitive explanations and visualizations for key geometric concepts.
2. Broaden the experimental evaluation to include diverse problem domains and real-world datasets.
3. Provide a more detailed discussion of computational complexity and practical implementation challenges.
4. Compare the proposed method with other recent advancements in Riemannian optimization to contextualize its performance.
Recommendation:
The paper makes a substantial theoretical contribution to Riemannian optimization and demonstrates promising empirical results. However, the clarity and scope of the experiments could be improved. I recommend acceptance with minor revisions, as the work is a valuable contribution to the field and aligns well with the conference's focus on advancing optimization methods.