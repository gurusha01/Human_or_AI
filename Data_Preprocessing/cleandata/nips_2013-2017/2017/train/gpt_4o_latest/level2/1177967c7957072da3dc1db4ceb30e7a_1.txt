The paper presents a novel adaptive importance sampling scheme for coordinate descent (CD) and stochastic gradient descent (SGD) algorithms, addressing the computational inefficiency of gradient-based sampling in large-scale optimization problems. The authors propose a safe approximation of gradient-based sampling using upper and lower bounds on the gradient, which is provably better than uniform or fixed importance sampling. The paper makes four key contributions: (1) theoretical justification for gradient-based sampling in CD, (2) a generic and efficient adaptive sampling strategy, (3) integration of the scheme into CD and SGD for structured optimization problems, and (4) extensive numerical evidence demonstrating the scheme's efficiency.
Strengths:
1. Theoretical Rigor: The paper provides strong theoretical guarantees for the proposed sampling scheme, including proofs that it is always better than fixed importance sampling and achieves near-optimal performance under limited gradient information.
2. Practical Efficiency: The proposed method introduces minimal computational overhead (O(n log n) per iteration), making it feasible for large-scale applications. This is a significant improvement over traditional gradient-based sampling, which requires full gradient computation.
3. Generality: The scheme is generic and can be easily integrated into existing CD and SGD algorithms, broadening its applicability across various machine learning tasks.
4. Empirical Validation: Extensive experiments on real-world datasets demonstrate the practical benefits of the proposed method, including faster convergence and reduced computational cost compared to fixed sampling.
5. Constructive Use of Gradient Bounds: The novel approach of leveraging safe gradient bounds to compute an adaptive sampling distribution is innovative and addresses a key limitation of prior work.
Weaknesses:
1. Limited Impact on SGD: While the method shows significant improvements for CD, its impact on SGD is less pronounced. This may limit its appeal for practitioners focused on SGD-based optimization.
2. Dependence on Gradient Bounds: The method relies on the availability of safe upper and lower bounds for the gradient, which may not always be straightforward to compute for more complex models.
3. Clarity of Presentation: The paper is dense and highly technical, which may hinder accessibility for a broader audience. Simplifying some of the mathematical derivations or providing more intuitive explanations could improve clarity.
4. Comparison with State-of-the-Art: While the paper demonstrates improvements over fixed sampling, it does not thoroughly compare the proposed method with other recent adaptive sampling strategies, leaving its relative performance unclear.
Recommendation:
This paper makes a significant contribution to the field of optimization by addressing a critical limitation of adaptive importance sampling. Its theoretical rigor, practical efficiency, and empirical validation make it a strong candidate for acceptance. However, the authors should consider clarifying the presentation and providing a more comprehensive comparison with related work. Additionally, discussing potential extensions to more complex models or addressing the limited impact on SGD would strengthen the paper further.
Arguments for Acceptance:
- Strong theoretical guarantees and practical efficiency.
- Demonstrated improvements in convergence and computational cost for CD.
- Generic applicability to existing optimization algorithms.
Arguments Against Acceptance:
- Limited empirical impact on SGD.
- Reliance on safe gradient bounds, which may not generalize to all models.
- Dense presentation that could be more accessible.
Overall, I recommend acceptance with minor revisions to improve clarity and address the aforementioned weaknesses.