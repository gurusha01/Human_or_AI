The paper introduces a cognitive model designed to predict when specific questions are asked in particular contexts (belief states). In this framework, the unnormalized log probability of a question is expressed as a weighted sum of informativeness (expected information gain), complexity (approximately, question length), answer type, and "relevance" (whether the question pertains to the belief state or merely to world-independent logical facts). The model is tested in the Battleship domain, where the objective is to locate ships (clusters of grid elements) within a grid. The complete model, incorporating all the aforementioned features, outperformed simplified versions with fewer features in predicting held-out human data (measured via log-likelihood). However, in terms of correlation with human judgments, a model excluding EIG performed comparably to the full model.
The paper appears to be technically sound overall.
I believe it is more successful as an AI project than as a cognitive science study. While the paper claims that "We find that our model predicts what question people will ask" and "from a CogSci standpoint, our results show how people balance informativeness and complexity," I am not persuaded that it provides significant insights into how people weigh these factors. In the Battleship domain, the results suggest that people may be influenced by EIG to a limited extent (if we trust the log-likelihood results, which have a 19% probability of being incorrect based on bootstrap estimates) or not at all (if we rely on the correlation results, which are noisy due to sparse human data). These findings do not seem sufficient to draw strong conclusions about how people generally balance these factors.
The paper is well-written and clear, and I believe I could replicate the results.
However, the paper overlooks some relevant prior work. One of its apparent contributions is the idea of "modeling questions as programs that, when executed on the state of a possible world, output an answer." This concept aligns with the notion of a "goal" discussed in Hawkins 2015 ("Why do you ask? Good questions provoke informative answers.") and is rooted in the question-under-discussion (QUD) framework introduced by Roberts 1996 ("Information structure in discourse: Towards an integrated formal theory of pragmatics."). Hawkins' questioner model also incorporates expected information gain and trades it off against question complexity, though it is applied to a simpler domain.
I find this type of model—featuring a compositional prior over questions and decision-making based on EIG and other factors—to be a promising component for both cognitive and AI models. I anticipate its broader adoption in the future. The primary contribution of this paper, in my view, is demonstrating that this approach can indeed generate meaningful compositional questions in a complex domain.