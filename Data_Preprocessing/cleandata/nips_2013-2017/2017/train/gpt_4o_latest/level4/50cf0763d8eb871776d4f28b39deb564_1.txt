This paper investigates the convergence rates of M-estimators for differential parameter estimation. Two categories of models are analyzed: generative and discriminative. Generative models estimate the (canonical) parameters of two separate sample sets and compute their difference, while discriminative models directly estimate this difference using Bayes' rule and logistic regression. The authors establish convergence rates by introducing the novel concept of local separability, which quantifies how well, broadly, and effectively a loss function behaves as separable. The results encompass both low- and high-dimensional scenarios, with the latter addressed through l1-sparsity regularization. Based on my understanding, the generative approach generally outperforms the discriminative one in both theoretical analysis and simulations. The paper is well-written, logically structured, and the theoretical results appear robust. However, as I am not an expert in this domain, I am unable to fully assess the originality and broader implications of the findings. I do, however, have the following comments:
- Line 75: Correct "xi^{(0}" to "xi^{(0)}".
- Line 94, Equation (5): Why not use "mu" instead of "theta"? Specifically, it represents the negative average log-likelihood (up to constants). The same applies to Equation (6).
- Line 95, Equation (6): Replace "trace(Theta, hatSigma)" with "trace(Theta hatSigma)" (remove the comma).
- Lines 98â€“104: Should "theta2^" be "theta0^"?
- Line 102, Equation (8): Why does Equation (8) not depend on "c^*"?
- Line 103: Replace "phi(t)" with "Phi(t)" or use bold formatting.
- Line 158: Add a period after "Irrepresentability".
- Line 208, Lemma 3: The definitions of "d^Theta" and "kappaSigma^" are missing.
- Line 250: Add a dash between "soft" and "Thresholding".
- Line 259: Add a period before "For any classifier".
- Line 289: Add a period before "In this setting".