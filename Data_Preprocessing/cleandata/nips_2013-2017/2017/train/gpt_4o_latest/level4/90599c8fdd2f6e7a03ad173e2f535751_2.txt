Paraphrased Review
Overview:
The authors propose an enhancement to existing regret estimation techniques for Hierarchical Reinforcement Learning (HRL) methods that utilize options. Specifically, they aim to achieve this improvement without relying on the distribution of cumulative rewards or option durations, which are prerequisites for prior approaches such as UCRL and SUCRL.
Assuming the options are well-defined, the authors reformulate the inner MDP of options, represented by the transition matrix \( Po \), into an equivalent irreducible Markov chain with a new matrix \( P'o \). This transformation is achieved by merging the terminal states with the initial state.
By adopting this approach and assuming that any state with a termination probability less than one is reachable, the stationary distribution \( \mu_o \) of the chain can be derived. This stationary distribution is then employed to estimate the optimistic reward gain. Compared to previous methods, this formulation is more robust to poorly defined parameter estimates and better captures the correlation between cumulative reward and duration. The proposed method is named FSUCRL.
The algorithm is further enhanced by constructing confidence intervals for the reward \( r \), the Semi-MDP transition probabilities, and the inner Markov chain \( P'o \). Two variants of the algorithm, FSUCRL Lvl1 and FSUCRL Lvl2, are introduced. The first variant directly computes an approximate distribution \( \muo \) from the estimate of \( P'_o \), while the second variant employs two nested empirical value iteration procedures to derive the optimal bias.
The paper concludes with a theoretical analysis and a numerical evaluation of FSUCRL. On the theoretical side, FSUCRL Lvl2 is compared to SUCRL. The authors argue that the relative quality of the regret bound provided by FSUCRL Lvl2 versus SUCRL depends on several factors, including the length of the options and the accessibility of states within the options. They also outline conditions under which FSUCRL Lvl2 is expected to outperform SUCRL.
For empirical validation, four algorithms—UCRL, SUCRL, FSUCRL Lvl1, and FSUCRL Lvl2—are tested on a gridworld environment adapted from reference 9, where the maximum option duration is assumed to be known. The results align with the theoretical predictions: FSUCRL Lvl2 outperforms both SUCRL and FSUCRL Lvl1, partly due to the overlap in the actions of the options.
Evaluation:
- Quality: The paper appears to be theoretically robust, with a comprehensive discussion of the problem. The authors thoroughly analyze the strengths and weaknesses of their approach compared to SUCRL. While the numerical simulation is not definitive, it supports the theoretical arguments presented.
- Clarity: Although the paper could benefit from improved clarity, it is adequately understandable. The inclusion of an example and a detailed theoretical discussion aids in grasping the mathematical framework.
- Originality: The work appears to be sufficiently novel compared to its predecessor (SUCRL) and other related works published in NIPS.
- Significance: The paper's motivation is clear and addresses a notable limitation of prior methods, making it a relevant contribution.
Other Comments:
- Line 140: At this point, the first column of \( Qo \) is replaced by \( vo \) to form \( P'_o \), rendering the first state unreachable except from a terminating state. It is assumed that either Assumption 1 (finite option length) or Assumption 2 (the starting state is a terminal state) justifies this choice. If so, the authors should explicitly clarify the connection between these assumptions and this decision.
- Line 283: Replace "four" with "for."
- Line 284: Replace "where" with "were."