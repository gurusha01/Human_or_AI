This paper begins by introducing a concept of local "separability" for a loss function, which is subsequently utilized to derive \( \ell\infty \) convergence rates in terms of the separability parameters for both low- and high-dimensional settings. These rates are then applied to a probabilistic classification problem using both generative and discriminative approaches. By computing the separability parameters for each approach, the authors apply their theorems to obtain \( \ell\infty \) convergence rates for the discriminative method (logistic regression) and two generative methods (specifically for cases where \( x|y \) follows an isotropic Gaussian or a Gaussian graphical model). The paper then transitions to analyzing \( \ell2 \) convergence rates. For the discriminative approach, the \( \ell2 \) rate is straightforward, relying on support consistency and the \( \ell_\infty \) rates. While the naive generative algorithm exhibits poor scaling with increasing dimensionality, the authors demonstrate that incorporating a soft-thresholding operator ensures support consistency and favorable rates. Finally, the paper explores the impact of sparsity on sample complexity for both the discriminative and thresholded generative approaches. As predicted by the theoretical results, the logistic regression approach performs worse compared to the generative method, particularly in scenarios with lower sparsity.
This is an impressive contribution. However, I have a few questions, requests for clarification, and notes:
Notes:
- There is an inconsistency in terminology: the text temporarily switches from "empirical loss function" to "empirical risk function" on line 127 of page 4.
- I find the statement on lines 181–182 of page 5 somewhat confusing: "Theorems 1 and 2 give us an estimate on the number of samples required by a loss to establish convergence. In general, if \( \gamma \) is very small or \( \beta \) is very small, then we would need a lot of samples." Aren't \( \beta \) and \( \gamma \) dependent on \( L_n \), and therefore indirectly on the number of samples (\( n \))? Is this statement actually referring to the behavior of \( \beta \) and \( \gamma \) as functions of \( n \)?
- A similar point of confusion arises in several places (e.g., lines 194–195 on page 6, Corollary 3). There are inequalities where \( n \) appears on both sides of the equations. Since the goal is to derive convergence rates as a function of \( n \), shouldn't the separability parameters also exhibit some dependence on \( n \)? Where is this dependence captured in the analysis?
- Theorems 1 and 2 are claimed to provide \( \ell_\infty \) convergence rates. However, I am unclear on how these rates are derived. Does the analysis require explicit knowledge of how the separability parameters evolve with \( n \)?
- The statement "From Corollaries 3 and 4, we see that for IG, both the discriminative and generative approaches achieve the same convergence rate, but the generative approach does so with only a logarithmic dependence on the dimension" is intriguing. However, how does logistic regression differ in this context? Additionally, Corollary 4 includes the condition \( n \gg \log p \)—is this condition correct? Furthermore, Corollary 3 uses the notation \( \succsim \), which is unfamiliar to me. It might be helpful to define this notation in a footnote for clarity.