This paper introduces an RNN architecture that leverages the strengths of stacked multiscale RNNs for capturing long-term dependencies and deep transition RNNs for modeling complex dynamics, enabling rapid adaptation to input changes. The proposed architecture typically comprises four fast RNN cells (implemented as LSTMs in the paper) forming the lower deep transition layer, along with a single slow RNN cell in the upper layer, which receives input from the first fast cell and updates the state of the second fast cell.
The model is evaluated on the PTB and enwiki8 datasets, where it achieves the lowest character-level perplexity compared to other architectures of similar size (in terms of the number of parameters or cells). The analysis of vanishing gradients and cell state transitions is particularly insightful.
A minor observation: should Fig. 1 label h{t-1}^{Fk}?