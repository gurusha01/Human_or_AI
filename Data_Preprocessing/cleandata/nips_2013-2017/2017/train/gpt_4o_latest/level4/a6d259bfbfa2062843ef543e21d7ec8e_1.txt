This paper applies best arm identification (BAI) techniques to the Monte Carlo tree search problem in a two-player, turn-based setting. The objective is to determine the optimal action for player A by systematically considering all subsequent actions that both player B and player A might take in future rounds. The authors assume access to a stochastic oracle for evaluating leaf values and aim to identify the approximately optimal action at the root with a precision of \(\epsilon\) and a confidence level of at least \(1-\delta\). The proposed algorithms leverage confidence intervals and propagate upper and lower confidence bounds upwards from the leaves to the root, where upper bounds correspond to MAX nodes (player A's actions) and lower bounds correspond to MIN nodes (player B's actions). These algorithms are intuitive, well-articulated, and firmly grounded in the fixed-confidence BAI literature. The paper incorporates recent advancements in this domain while clearly delineating its novel contributions (Section 3.1), which enhances the clarity of the work. The authors provide a theorem on the number of leaf evaluations, demonstrating a complexity of \(H(3)\) that improves upon prior results. The comparisons with existing algorithms and strategies are precise, and the experiments reveal significant improvements over prior methods. Additionally, the paper offers insights into lower bounds and future research directions, which are again well-anchored in recent developments in the fixed-confidence BAI framework.
The paper is exceptionally well-written, with the contributions presented in a manner that is both accessible and well-contextualized. Thank you for this thoughtful and rigorous work.
A few suggestions to further enhance the clarity of the paper: Figure 1 should be explicitly referenced in the text, as it provides an immediate visual understanding that complements the explanation (e.g., near line 111). It may also be helpful to include a discussion clarifying why two candidates, \(at\) and \(bt\), are required at time \(t+1\), as this could initially seem counterintuitive. Regarding the experiments, why is \(\delta\) set to \(0.1 |\mathcal{L}|\)? In Figure 4, the value \(\delta = 2.7\) appears unusual, even though it does not seem to affect the experimental results. Is it standard practice to use such a low confidence parameter for these algorithms? To avoid potential confusion, I suggest running experiments with \(\delta = 0.01 \times 27\) or simply \(\delta = 0.1\) (without scaling by \(|\mathcal{L}|\)). Lemma 7 is somewhat difficult to interpret in its current form, though the accompanying example helps clarify it. Could you also consider adding a line in Figure 3 showing the optimal proportions, ensuring the same \(\delta\) is used? (A minor note: 259.9 Ã— 1.76 = 457.424, so you might want to remove the numerical value of \(\text{kl}(\delta, 1-\delta)\).) Additionally, Figure 4 is challenging to read without zooming in significantly (300% on my computer). Perhaps it could be removed or reformatted. I recommend modifying the figures by adding two columns to display the algorithm names and the total number of samples used, then presenting proportions instead of raw numbers (similar to \(w^*\) in line 281).
In conclusion, this work represents a clear and meaningful contribution to the existing literature.