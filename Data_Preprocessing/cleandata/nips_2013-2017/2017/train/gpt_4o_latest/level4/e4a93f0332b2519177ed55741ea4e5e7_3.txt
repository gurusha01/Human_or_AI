The paper introduces a new architecture called the Fast Slow Recurrent Neural Network (FS-RNN), which aims to combine the advantages of both multiscale RNNs and deep transition RNNs. The authors conducted comprehensive empirical evaluations against various state-of-the-art RNN architectures. Additionally, they provided an open-source implementation along with publicly available datasets, ensuring the reproducibility of their experiments. However, the work could be further strengthened by including comparisons with other recent state-of-the-art models, such as Tree-LSTM, which effectively capture hierarchical long-term dependencies.