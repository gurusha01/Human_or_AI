This paper investigates a variant of contextual linear bandits under a conservative constraint, which requires the player's cumulative reward at any time \( t \) to be at least \( (1-\alpha) \)-times that of a specified baseline policy for a given \( \alpha \). 
The authors address two scenarios: one where the baseline rewards are known and another where they are unknown. For each case, they develop a UCB-type algorithm derived from an existing contextual linear bandit algorithm and establish regret upper bounds. These bounds consist of two components: the regret from the base algorithm and the regret incurred due to adherence to the conservative constraint. Additionally, they perform simulations for the algorithm in the known baseline rewards setting and verify that it satisfies the conservative constraint in practice.
The conservative constraint, proposed algorithms, and derived regret bounds appear reasonable. However, the graph in Figure 1(a) should be improved to better illustrate the initial conservative phases of CLUCB. Furthermore, similar experiments should be conducted for CLUCB2, as it operates in a more practical setting.