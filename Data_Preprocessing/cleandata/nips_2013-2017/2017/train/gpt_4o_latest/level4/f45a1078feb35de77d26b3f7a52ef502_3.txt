This paper introduces a method for learning predictive (one-dimensional) point process models by modeling the count density using a W-GAN.
Detailed comments:
* Abstract: There exist multiple techniques for performing full (or approximate) Bayesian inference on (Cox or renewal) point processes. For example, refer to arxiv.org/pdf/1411.0254.pdf or www.gatsby.ucl.ac.uk%2F~vrao%2Fnips2011_raoteh.pdf.
* It seems that the proposed approach is limited to 1D point processes, as representing higher-dimensional point processes through the count density appears challenging. If this is indeed the case, it would be helpful to state this limitation more explicitly.
* This limitation applies to most inhomogeneous point processes. However, given only a single realization of the point process, it seems quite difficult to derive a low-variance estimate of the distance between the generated count measure and the data. Perhaps this is the primary reason why the W-GAN performs so effectivelyâ€”similar to GP-based intensity-explicit models, the generator/intensity-proxy appears to be heavily regularized.
* The origin of the "real" in the real-world evaluation metrics (e.g., Figure 3) is unclear. How is this ground truth determined?
* A discussion on the ease or difficulty of training these models would have been a valuable addition.
Finally, I am curious about how simpler models compare to the proposed approach, such as KDE with truncation or simple parametric Hawkes processes. My main concern with this work is that the models presented may be excessively complex relative to the signal-to-noise ratio in the data. Consequently, while the W-GAN outperforms other NN/RNN-based methods, a more heavily regularized (i.e., simpler) intensity-based approach might empirically perform better in most scenarios.