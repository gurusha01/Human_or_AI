This paper addresses the network embedding problem, focusing on preserving both proximity and global ranking. The authors introduce an unsupervised Siamese neural network architecture to learn node embeddings in a generative manner. They assert that their model possesses four key characteristics: scalability, asymmetry, unity, and simplicity.
The proposed Siamese neural network is structured as a multi-task learning framework, with shared hidden layers generating embedding vectors for each pair of connected nodes. The node ranking hidden layer is designed to capture global ranking information, while the proximity hidden layer focuses on preserving local proximity information. Consequently, this generative framework updates the two input embedding vectors by propagating gradients from the output layers, which are optimized based on an objective function comprising two reward componentsâ€”one for each task. The network design is straightforward and robust, utilizing only a single hidden layer, which minimizes the number of hyperparameters requiring tuning. Additionally, the authors provide rigorous proofs for second-order proximity preservation and global ranking preservation, including an upper bound based on PageRank.
The paper is well-written and easy to understand. Figure 1 is particularly clear and effectively aids readers in grasping the proposed model. The experimental section is thoughtfully designed, with results demonstrating that the proposed embedding model outperforms competing methods on tasks such as rank prediction, classification/regression, and link prediction. Furthermore, the authors conduct additional experiments to show that their method is more robust to noisy data compared to state-of-the-art approaches.
I did not identify any significant flaws in this paper during my review. It is well-presented and of high technical quality.