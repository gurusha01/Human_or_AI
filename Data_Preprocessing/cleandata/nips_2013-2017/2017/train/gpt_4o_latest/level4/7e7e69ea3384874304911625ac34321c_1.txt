Updated Review After Rebuttal:  
I now believe the paper is suitable for acceptance as a poster. I encourage the authors to refine the writing to better emphasize their contributions, motivation, and design decisions. This would help the work stand out and avoid being perceived as "just another hybrid generative model."
---
The submission introduces PixelGAN Autoencoder, a hybrid generative model that combines an adversarial autoencoder with a PixelCNN autoencoder. The authors present a theoretical foundation for their approach, leveraging a decomposition of the variational evidence lower bound (ELBO). The paper includes qualitative results with various priors on the latent distribution and quantitative evaluations on semi-supervised learning tasks using MNIST, SVHN, and NORB.
The work is closely related to Adversarial Autoencoders (Makhzani et al., ICLR 2016 Workshop), which, to the best of my knowledge, has only been published on arXiv and as a workshop paper at ICLR. Nevertheless, it is a well-known and widely cited contribution. The novelty of this submission hinges on whether Adversarial Autoencoders are considered an established baseline or not. This is a nuanced issue that I believe the ACs and PCs are better positioned to evaluate.
Below, I provide more detailed comments:
Strengths:  
1. The paper demonstrates strong results on semi-supervised learning tasks with MNIST, SVHN, and NORB, as well as unsupervised clustering on MNIST. While it is unclear if the results are state-of-the-art due to missing baseline comparisons, they are at least competitive with the state of the art.  
2. The discussion of the ELBO decomposition and its connection to the architectural choices is clear and well-articulated.
Weaknesses:  
1. If Adversarial Autoencoders are treated as prior work, the novelty of this submission is somewhat limited, as it primarily combines two existing generative models. What makes this particular combination especially compelling?  
2. The paper lacks results on image generation. While I understand that evaluating image generation can be challenging, some metrics such as likelihood bounds (if computable), Inception scores, or qualitative examples of generated images would strengthen the submission. These could even be included in the Appendix.  
3. The two variants of the proposed approach—one with location-dependent biases and another with location-independent biases—are used interchangeably in the experiments but are not directly compared. While the authors acknowledge this in lines 153-157, a more thorough analysis would be valuable.  
4. The discussion of related work, particularly VLAE and PixelVAE (both ICLR 2017), could be expanded. Lines 158-163 provide some context but do not sufficiently clarify the differences, strengths, and weaknesses of these approaches relative to the proposed method.  
5. Certain formulations in the paper are unclear. For example, the distinction between "limited stochasticity" and "powerful decoder" in lines 88 and 96 is not well-explained. Similarly, the statement in line 111 about "approximately optimizing the KL divergence," along with the corresponding footnote, feels overly abstract. Are the authors optimizing the KL divergence or not?  
6. The bibliography often omits the ICLR conference and lists many officially published papers as arXiv preprints. This should be corrected.  
7. Placing an entire section on cross-domain relations in the Appendix is poor practice. While I understand the constraints of the 8-page limit, it is the authors' responsibility to structure the paper so that all key contributions are included in the main text.
Overall Assessment:  
I remain on the borderline. While the results are promising, the novelty of the work appears limited.