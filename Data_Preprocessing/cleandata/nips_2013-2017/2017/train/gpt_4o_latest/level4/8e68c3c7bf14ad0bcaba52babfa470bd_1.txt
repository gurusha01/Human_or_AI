The paper introduces a learning framework that integrates human feedback into reinforcement learning (RL) and applies this approach to the task of image captioning. Specifically, the authors gather human feedback on the machine-generated captions, including both scoring (ranging from perfect to major error) and corrections (when captions are not perfect or acceptable). Subsequently, a feedback network is trained to emulate human feedback by assessing the correctness of phrases. This simulated feedback is then incorporated into the RL training process, where rewards are derived from a combination of automatic metrics (e.g., weighted BLEU) and the feedback network's simulated human evaluations. The authors validate their approach through experiments conducted on the COCO dataset.
While the paper addresses an interesting problem, there are several areas for improvement, such as:
1. The paper's novelty is not well-articulated. Although incorporating human feedback into RL training is an intriguing concept, the implementation does not follow an online human-in-the-loop paradigm. Instead, it adopts a two-stage batch process for human labeling. Consequently, the authors rely on a simulator (i.e., the feedback network) to approximate human feedback during RL training, a technique that is already prevalent in many RL applications. Additionally, as noted in the related work section, using RL to optimize non-differentiable metrics is not a novel contribution.
2. The primary results are presented in Table 6, based on experiments conducted on the COCO dataset. However, the baseline performance is significantly below state-of-the-art levels (e.g., BLEU-4 scores for state-of-the-art models are typically around 30%, whereas the baseline in Table 6 achieves only 20%). Furthermore, the improvement achieved by incorporating feedback is less than 0.5% in BLEU-4, which is unlikely to be statistically significant.
3. The paper lacks detailed analysis and illustrative examples to demonstrate how feedback contributes to the RL framework. For instance, given that the RL baseline (RLB) and the feedback-enhanced model (RLF) achieve similar scores, are they making similar types of errors? Alternatively, does the feedback mechanism lead to qualitatively different predictions? Such insights are missing and would strengthen the paper's contributions.