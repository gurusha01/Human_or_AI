After response: Thank you for the detailed response. Two of my primary concerns—namely, the weakness of the baseline and the lack of comparison with automatic post-editing—have been addressed through the response. I have raised my evaluation with the expectation that these additions will be incorporated into the final camera-ready version.
Regarding the examples, my use of "cherry-picked?" (with a question mark) stemmed from the absence of an explanation about how the examples were selected. If they were chosen randomly or through some other unbiased method, this could be explicitly noted in the paper. It is, of course, acceptable to cherry-pick representative examples, but this should be clarified to improve transparency. Additionally, as I mentioned earlier, a quantitative analysis of which parts of the sentence are being improved would be highly beneficial.
Lastly, the explanation that beam search struggles to recover from search errors made at the beginning of a sentence is certainly valid. However, it remains unclear why this issue is not explicitly addressed during training and testing. Neural machine translation (NMT) maximizes the full-sentence likelihood, which inherently treats errors at any position in the sentence equally. It is important to ensure accuracy here, as this is a key premise of the proposed work.
---
This paper proposes a method to train a network that revises outputs generated by a first-pass decoder in neural sequence-to-sequence models. The second-level model is trained using a sampling-based approach, where outputs are sampled from the first-pass model. While the idea presented in the paper is potentially interesting, I have several concerns regarding the validity and analysis of the results. Additionally, the paper would benefit from better contextualization within the landscape of prior work.
First, the reasoning behind the proposed method is not entirely convincing. The introduction provides a compelling example of a poet using the final word of a sentence to determine the first word. However, in terms of probabilities, standard neural sequence-to-sequence models, despite decoding from left to right, calculate the joint probability over all words in the sentence. This means they indirectly account for later words when making earlier decisions, as poor early decisions would lower the conditional probability of subsequent words. While this does not hold for greedy decoding, beam search allows for recovery. A more careful discussion of this point would strengthen the paper.
Second, regarding the validity of the results, the baselines used in the paper do not appear to be representative of the state of the art for the respective tasks. While I do not expect every paper to achieve state-of-the-art (SOTA) results, the use of "RNNSearch"—if it refers to the original Bahdanau et al. attentional model—omits many advancements in neural MT over recent years. This omission makes it difficult to gauge how the proposed method would perform with stronger models. For instance, the following paper reports single-model BLEU scores on the NIST04-06 datasets that are 4-7 BLEU points higher than the proposed model:
> Deep Neural Machine Translation with Linear Associative Unit. Mingxuan Wang, Zhengdong Lu, Jie Zhou, Qun Liu (ACL 2017)
Third, while the paper includes a nominal amount of qualitative evaluation of (potentially cherry-picked) examples, it lacks analysis explaining why the proposed method performs better. A quantitative analysis demonstrating whether the improvements stem from the network's access to future target words would be valuable. For example, does the proposed method perform better at generating the beginnings of sentences, where standard models struggle due to the lack of contextual information?
Finally, there is a significant body of prior work that is either methodologically similar or addresses similar problems. The closest is automatic post-editing or pre-translation, as seen in the following papers:
> Junczys-Dowmunt, Marcin, and Roman Grundkiewicz. "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing." WMT2016.  
> Niehues, Jan, et al. "Pre-Translation for Neural Machine Translation." COLING2016.  
While these methods are not trained jointly with the first-pass decoder, they have proven effective even without joint training. Additionally, the following paper introduces bilingual or bidirectional models to achieve globally consistent decoding, addressing a similar problem:
> Hoang, Cong Duy Vu, Gholamreza Haffari, and Trevor Cohn. "Decoding as Continuous Optimization in Neural Machine Translation." arXiv 2017.