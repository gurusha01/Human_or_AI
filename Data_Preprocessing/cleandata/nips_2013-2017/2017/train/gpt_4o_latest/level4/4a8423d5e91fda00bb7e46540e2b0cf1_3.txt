The paper tackles the challenge of designing a classifier with a reject option that simultaneously achieves a specified classification risk while minimizing the probability of rejection. The authors focus on the scenario where both the classifier and an associated confidence function are pre-defined, and the objective is to determine a confidence threshold that dictates whether the classifier's prediction is accepted or rejected. They introduce an algorithm to compute this threshold and provide statistical guarantees for the proposed method.
Comments:
- The authors should explicitly define the task their algorithm aims to solve. The description on lines 86-88 appears to outline the ultimate objective, whereas the algorithm in the paper addresses a more specific problem: given $(f,\kappa)$, identify a threshold $\theta$ that defines $g$ in equation (3) such that condition (2) is satisfied while maximizing coverage.
- It appears that for certain values of the input parameters (\delta, r^, S_m, ...), Algorithm 1 may consistently yield a trivial solution. By trivial solution, I mean that the condition on line 10 of Algorithm 1 is never met, resulting in all examples being assigned to the "reject region." Specifically, for $\hat{r}=0$ (zero training error), the bound $B^$ solving equation (4) can be derived analytically as  
  $B^* = 1-(\delta/\log_2(m))^{1/m}$.  
  Consequently, if the desired risk $r^$ is set below this bound, $B^ = 1-(\delta/\log_2(m))^{1/m}$, Algorithm 1 will always return a trivial solution. For instance, with $\delta=0.001$ (as used in the experiments) and $m=500$ training examples, the minimal bound becomes $B^=0.0180$ (1.8%). Therefore, setting $r^ < 0.018$ will invariably lead to a trivial solution, regardless of the dataset. The authors should clarify this limitation.
- The experimental section should include a comparison to a straightforward baseline approach that would likely be the first method attempted. Specifically, one could determine the threshold directly using the empirical risk $\hat{r}_i$ instead of relying on the more complex bound $B^*$. Given the simplicity of the hypothesis space (i.e., "threshold rules") and the relatively large dataset size (e.g., 5000 examples in the experiments), the risk of overfitting seems minimal. Without such a baseline comparison, it is difficult to assess the practical advantages of the proposed method.
- A discussion on the challenges associated with solving the numerical problem in equation (4) is missing. For example, the authors should address which numerical methods are appropriate and whether evaluating the combinatorial coefficient for large $m$ and $j$ introduces numerical difficulties.
Typos:
- Line 80: (f,g)
- Line 116: $B^*(\hat{r},\delta,S_m)$  
- Line 221: "mageNet"