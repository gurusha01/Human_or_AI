Review - Paper Summary:  
The paper proposes an enhanced Greedy Coordinate Descent (GCD) algorithm that combines the benefits of Nesterov's acceleration method and Stochastic Gradient Descent (SGD) to address high-dimensional optimization problems, both sparse and dense. Initially, the authors reformulate an $l_1$-square-regularized approximate optimization problem (finding a solution close to $x^*$ within a neighborhood $\epsilon$) into a convex but challenging problem using a greedy rule. This problem is then solved exactly using the SOTOPO algorithm. Subsequently, the solution is refined by leveraging the faster convergence rate of Nesterov's method and the "reduced-by-one-sample" complexity of SGD. The resulting algorithm, termed Accelerated Stochastic Greedy Coordinate Descent (ASGCD), achieves a convergence rate of $O(\sqrt{1/\epsilon})$ while reducing complexity by one sample compared to the standard GCD.
Originality of the Paper:  
The SOTOPO algorithm exploits the $l_1$ regularization term to identify potential sub-gradient directions, sorting them to determine the optimal direction without requiring a full gradient computation. While the integration of Nesterov's acceleration, SGD's reduced complexity, and GCD's greedy approach is less groundbreaking, the authors deserve credit for constructing an efficient and rigorous algorithm despite the complexity of combining these components.
Contribution:  
- Reduces complexity and improves convergence rates for large-scale, dense, convex optimization problems with sparse solutions (+).  
- Combines existing performance-enhancing techniques to produce a more efficient algorithm (+).  
- Introduces a criterion to reduce complexity by identifying non-zero descent directions and sorting them for faster optimization (+).  
- Eliminates the need for full gradient computation in the proposed algorithm (+).  
- Does not provide a theoretical framework for selecting the regularization parameter $\lambda$ as a function of batch size, which significantly impacts the performance of ASGCD in both batch scenarios (-).
Technical Soundness:  
- All proofs for Lemmas, Corollaries, Theorems, and Propositions are included in the supplementary material (+).  
- The derivations are rigorous and well-founded, though additional references to foundational optimization theorems or Lemmas could improve accessibility for non-specialists (-).
Implementation of Idea:  
The algorithm, particularly the SOTOPO component, is intricate and challenging to implement.
Clarity of Presentation:  
- The paper is detailed, but readers may lose sight of the broader objective amidst the technical details. Periodic reminders of the purpose of each step could improve comprehension (-).  
- The sequence of applying different algorithms or their components to the problem is explained but could benefit from a diagram or pseudo-code for clarity (-).  
- Notation issues: In equation 3, $g$ is not clearly defined, and Algorithm 1 contains two typographical errors in equation references (-).  
- Despite the mathematical complexity, the paper achieves a reasonable level of clarity (+).
Theoretical Basis:  
- All Lemmas and transformations are rigorously proven in the supplementary material (+).  
- Some key literature results on convergence rates and complexity of existing algorithms are not cited (e.g., lines 24, 25, 60, 143), and equation 16 is introduced without sufficient explanation, causing initial confusion (-).  
- Remark 1 appears arbitrary without proper referencing or justification (-).  
- A theoretical comparison of solution accuracy with existing methods would enhance the paper's value (-).  
- In the supplementary material, line 344 omits a $d \theta_t$ term in one of the integrals (-).
Empirical/Experimental Basis:  
- The experimental results validate the proposed algorithm's performance against established methods, with consistent datasets supporting the analysis (+).  
- A better smoothing constant $T_1$ is suggested in line 208, but the rationale for its superiority in the $b=n$ batch size case should be clarified (-).  
- The algorithm underperforms for small regularization ($10^{-6}$) and batch size 1 compared to Katyusha on the Mnist dataset, though it is competitive on Gisette. This suggests potential for improvement or a need to explore the threshold regularization value and its connection to sparsity and batch size (-).  
- The relationship between batch size (stochastic vs. deterministic) and the choice of regularization value that optimizes ASGCD's performance is not discussed (-).
Interest to NIPS Audience [YES]:  
The paper's comparison of the proposed algorithm with established methods and performance improvement strategies makes it relevant to the NIPS audience. It could spark discussions on whether the algorithm's complexity can be reduced without sacrificing its performance.