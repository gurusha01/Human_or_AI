This paper proposes the use of hashed embeddings to reduce the memory requirements of embedding parameters. The method is straightforward, involving the learning of a vector of importance parameters, with one parameter corresponding to each component vector. Both trainable matrices—the matrix of importance parameters and the embedding matrix of component vectors—are significantly smaller than a standard embedding matrix.
To generate a word embedding, the token ID is first hashed into a row of the importance parameters, and then each component of the importance parameters is hashed into the corresponding component vector embedding.
The experimental results demonstrate that, even with simple bag-of-words (BoW) models, their ensemble of hashed embeddings outperforms prior BoW models on classification tasks. This approach is both simple and effective, offering a means to reduce the number of model parameters while also potentially acting as a regularizer.
However, the authors should discuss the computational cost of inference and clarify whether the hashing operation is more computationally expensive than a standard embedding lookup on a GPU.