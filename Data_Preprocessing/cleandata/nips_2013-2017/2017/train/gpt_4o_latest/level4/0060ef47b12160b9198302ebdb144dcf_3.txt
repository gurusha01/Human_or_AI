This paper introduces an image saliency mask generation method capable of processing 100 224x224 images per second on a standard GPU. The proposed approach trains a masking model to identify the tightest rectangular crop that fully encompasses the salient region of a specified class as determined by a black-box classifier, such as AlexNet, GoogleNet, or ResNet. The model architecture relies on image feature maps, such as those generated by ResNet-50, across multiple scales. The final-scale feature map is passed through a feature filter for initial localization, followed by upsampling blocks that refine the generated masks. Experimental results demonstrate that the proposed method outperforms other weakly supervised techniques on the ImageNet localization task.
The paper appears to include adequate references and related works, though they were not fully verified.  
The technical correctness of the paper seems sound, though it was not exhaustively checked.  
The paper provides several intuitions and discussions regarding the design of the proposed approach.  
The presentation quality of the paper is good.
Overall, the paper presents compelling technical results, though I have some reservations about the real-time speed claims and the applicability of the method to real-world images.
Comments:
- Does the reported processing speed of 100 images per second include the image resizing operation? If so, what is the runtime for larger images, such as 640x480 images captured by an iPhone 6s?  
- The salient objects in the presented examples appear relatively large. How does the method perform when the requested class object is small within the image? Is a 224x224 resolution sufficient in such cases?  
- In Table 3, are there any corresponding metrics for other methods listed in Table 2, such as Feed [2]?
MISC:  
- LN 273: "More over, because our model" → "Moreover, because our model"  
- LN 151: "difference that [3] only optimise the mask" → "difference that [3] only optimises the mask"