The authors introduce a general framework for testing conditional independence among three continuous variables. Their approach avoids imposing strong parametric assumptions and instead leverages samples from the joint distribution to generate samples approximating the conditional product distribution in terms of total variation distance. These generated samples are then used to train a binary classifier, such as a deep neural network. By evaluating the predictive error of the classifier on samples from the two distributions, the method determines whether to accept or reject the null hypothesis of conditional independence. The authors benchmark their method against existing approaches in the literature, providing both descriptive comparisons and experimental results on synthetic and real-world datasets.
Conditional independence testing is a critical tool in modern machine learning, particularly in causal inference, where assessing claims of unconfoundedness is a common objective. The proposed method is both conceptually simple and versatile, making it potentially valuable for a variety of causal applications.
The paper is well-written overall. While some of the theoretical results and proofs are dense and challenging to follow, the authors succeed in summarizing their findings and presenting the methodology in a way that is accessible to practitioners.
The theoretical contributions of the paper are particularly noteworthy. The proposed method builds on the framework for independence testing described in "Revisiting Classifier Two-Sample Tests" (https://arxiv.org/pdf/1610.06545.pdf), as acknowledged by the authors. By extending the approach to include a third variable \( Z \) and incorporating neighbor-matching for \( Z \), the method becomes applicable to conditional independence testing. Despite the relative simplicity of this extension, the theoretical guarantees provided by the authors are impressive.
Additional comments:
- Is there a discrete counterpart to the proposed method? It seems that the bootstrap technique could be even more effective when \( Z \) is discrete.
- How robust is the test to the choice of classification model? Could small changes in hyperparameters significantly alter the acceptance or rejection of the null hypothesis? Including an analysis of this sensitivity alongside the simulations would be valuable.
- Section 1.1 on Main Contributions feels repetitive â€” the authors summarize their approach multiple times, which seems unnecessary.
- Algorithm 2 could be omitted or described more concisely, as its steps are already explained elsewhere in the paper.
- Minor typographical errors: "classifier" should replace "classifer" on line 157, and "graphs" should replace "graps" on line 280.