The authors propose a novel algorithm for learning ensembles of deep decision (or regression) trees that jointly optimize prediction accuracy and feature acquisition cost. The resulting prediction framework is adaptive, as it dynamically selects different subsets of features for evaluation depending on the input, thereby reducing overall cost while maintaining high prediction accuracy.
The proposed approach is an iterative algorithm that extends gradient tree boosting by incorporating a cost-sensitive objective function. The algorithm incrementally adds decision trees to the ensemble, with each new tree trained to approximately minimize the overall cost of the ensemble. To enable the learning of individual trees under cost constraints relative to the existing ensemble, the authors introduce a cost-aware impurity function.
The experimental results demonstrate that the proposed algorithm outperforms existing methods for cost-aware ensemble learning across multiple real-world datasets.  
In summary, the paper is well-written, the algorithmic contributions are both novel and compelling, and the addressed problem is of significant importance across various application domains.