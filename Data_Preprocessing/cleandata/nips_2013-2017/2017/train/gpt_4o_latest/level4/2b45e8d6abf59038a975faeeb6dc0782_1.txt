The paper introduces Population Matching Discrepancy (PMD) as the Wasserstein distance computed between two minibatches sampled from the distributions. The Wasserstein distance is calculated using either an exact O(N^3) algorithm or an approximate O(N^2) algorithm.
Pros:
- The experiments using this computation of the Wasserstein distance are intriguing. However, the quality of the generated images is not on par with those produced by Wasserstein GAN.
Cons:
- Estimating the Wasserstein distance between two diverse multimodal distributions using the proposed method would require a large minibatch size (N). This could pose challenges, as issues might already arise when attempting to match a mixture of Gaussians, particularly in learning the variances.
- If N is insufficiently large, the optimization process may converge to a global minimum that does not correspond to the true distribution. For instance, the learned distribution might exhibit reduced entropy. This is evident in the SVHN samples shown in Figure 4, where the diversity appears low, and the digit 8 is overrepresented.
Minor typos:
- Line 141: Replace "usally" with "usually."
Update:
I have reviewed the rebuttal and appreciate the additional experiments provided.
The authors should clarify the limitations of PMD in comparison to MMD. Notably, MMD performs well even with a batch size of 2 and can be trained using stochastic gradient descent (SGD) by employing the unbiased estimator of MMD introduced in the original "A Kernel Two-Sample Test" paper. This allows MMD to converge to the correct distribution even with small minibatches. In contrast, PMD lacks a known unbiased gradient estimator, which limits its applicability in such scenarios.