This paper introduces hash embeddings as a method to generate vector representations of words by leveraging hashing techniques. The proposed model employs a set of hash functions to map each word to a small group of shared embedding vectors. This approach helps reduce the overall number of parameters and addresses challenges faced by traditional word embedding models. The model is straightforward, easy to implement, and demonstrates state-of-the-art performance in experimental evaluations.
Firstly, I have a question regarding the potential challenges in training hash embeddings. Specifically, the model requires training both the importance weights associated with the hash functions and the shared embeddings. Intuitively, this interdependence between the two sets of parameters could complicate the optimization process, making it harder to converge to a suitable local minimum. It would be beneficial if the authors could provide evidence of the model's robustness to different initialization strategies and hyper-parameter settings, as well as offer insights into how to effectively guide the training process toward a good local minimum.
Secondly, I am curious about the connection between "discourse atoms" [1] and the shared embeddings learned through hash embeddings. The concept of "discourse atoms" involves reconstructing word embeddings as a linear combination of a small set of atom vectors, with some atoms capturing higher-level semantic properties of words (e.g., ontological relationships). I suspect that similar characteristics might emerge in the shared embeddings produced by hash embeddings. Do the authors have any observations or insights regarding the relationship between these "atoms" and the representations learned through hashing?
[1] Linear Algebraic Structure of Word Senses, with Applications to Polysemy https://arxiv.org/abs/1601.03764