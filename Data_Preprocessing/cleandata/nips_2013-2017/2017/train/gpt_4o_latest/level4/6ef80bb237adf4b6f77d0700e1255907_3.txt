This paper presents an accelerated first-order method for geodesically convex optimization on Riemannian manifolds. It is demonstrated that the proposed method achieves linear convergence, i.e., O((1-\sqrt{mu/L})^k), for mu-strongly G-convex and L-smooth functions, and O(1/k^2) convergence for G-L-smooth functions. These results extend the Euclidean counterpart, specifically Nesterov's accelerated method. Numerical experiments are provided to compare the performance of the proposed method with RGD and RSGD methods.
The primary concern with this paper lies in the applications of the proposed method. The experimental results are not persuasive. For the Karcher mean on SPD matrices, the cost function is smooth, and the Hessian consistently exhibits a favorable condition number. Methods that leverage higher-order information tend to perform better in such cases. The authors assert that Bini's method, the Riemannian GD method, and the limited-memory Riemannian BFGS method exhibit comparable performance. However, this claim is inaccurate, particularly when the BB step size is used as the initial step size in the line search algorithm instead of a constant step size. Typically, 30 passes can reduce the objective gap by a factor greater than 10^10, as opposed to only 10^3 as reported in this paper.
This application does not appear to be an ideal example for showcasing the proposed method. Since the proposed method does not require the cost function to be C^2, an application involving a C^1 cost function might better highlight the advantages and performance of the proposed approach.