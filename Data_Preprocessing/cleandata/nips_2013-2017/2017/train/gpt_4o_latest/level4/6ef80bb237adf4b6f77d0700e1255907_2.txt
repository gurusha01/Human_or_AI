Summary of the Paper
The paper explores a geodesic extension of Nesterov's accelerated gradient descent (AGD) algorithm for optimization in Riemannian spaces. It introduces two variants: one for the geodesic convex case and another for the geodesic strongly convex smooth case. The proposed methods are applied to Karcher mean computation problems and demonstrate superior performance compared to two prior algorithms (RGD, RSGD) designed for the same problem, particularly in scenarios involving randomized data.
Evaluation
From a theoretical perspective, the paper's approach to generalizing the momentum term to maintain AGD's convergence rate across arbitrary Riemannian spaces is both novel and compelling. From a practical standpoint, however, it is not entirely clear under what conditions the overall runtime is reduced. While the algorithm achieves a significant reduction in the number of iterations, the computational cost of implementing the momentum term in Riemannian spaces can be substantial compared to Euclidean spaces. Nonetheless, the algorithm's broad applicability across diverse settings makes it an attractive general framework and may inspire further advancements in this area. The paper is well-written and relatively easy to follow.
General Comments
- The paper relies heavily on differential geometric concepts and related definitions, which may not be familiar to the typical NIPS audience. I recommend refining and streamlining the definitions section. Specifically, the following terms appear to lack formal definitions in the text: star-concave, star-convex, grad f(x), intrinsic inner-product, diameter of X, conic geometric optimization, and retractions.
- Equations 4 and 5 are presented without sufficient intuition or explanation regarding their derivation. They seem to appear abruptly, and the accompanying figure, which could potentially clarify these equations, is not adequately referenced or discussed in the text.
Minor Comments
- L52: Remove the redundant "The."
- L52 + L101: There is a substantial body of work that provides alternative interpretations of AGD. Presenting the proximal interpretation as the primary one may be somewhat misleading.
- L59: Could you elaborate further on the computational complexity involved in implementing the exponential map and the nonlinear momentum operator \( S \)?
- L70: Why are the linearizations of gradient-like updates considered contributions in their own right?
- L72: Clarify the term "classes."
- L90 + L93: The phrase "we denote" is unclear.
- L113: The sentence "In addition, different..." is somewhat ambiguous.
- L126: Beyond the constraint on \( \alpha \), what other factors should be considered when determining its optimal value?
- L139 + L144: The font used for "Exp" appears to be incorrect.
- L151: The phrasing in Lemma 3 is somewhat unclear.
- L155: In Theorem 1, consider explicitly restating the value of \( \beta \).
- L165: How does the upper bound depend on \( D \)? Additionally, how should \( \alpha \) be set in this context?
- L177: Consider replacing "geometrically" with "geodesically."
- L180: In Section 5, the instantiation of \( S \) for this specific case is unclear.
- L181: In the phrase "For the accelerated scheme in (4)," are you referring to Algorithm 1?
- L183: Should it be \( Yk \) or \( yk \)?
- L193: The phrase "with the geometry" is vague.
- L201: Remove the redundant "The." Also, should "(2)" be replaced with "(3)"?
- L219: Could you provide a more rigorous explanation (or relevant references) for why RGD serves as a good proxy for the algorithms discussed earlier?
- Page 8: Consider including actual runtime measurements, as the current benchmark does not account for per-iteration costs.
- L225: Why is \( C \) explicitly defined?