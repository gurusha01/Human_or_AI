The authors introduce a novel approach to modeling complex long-term dependencies in count vectors by leveraging recurrent neural networks to represent implicit distributions, specifically realized through the gamma distribution. The shape parameters of these gamma distributions, which encode temporal dependencies, are approximated using neural networks. These networks integrate information from both the prior (higher layer) and the likelihood (lower layer). The neural network parameters are optimized based on the expectation of latent variables, which helps mitigate variance introduced by sampling and enhances computational efficiency. This model offers an innovative and effective solution to address the long-dependency challenge in dynamic models. I find the approach compelling, but the manuscript appears to have been prepared somewhat hastily, and I have the following comments:
1. There are critical typographical errors, such as in Equation 10.
2. The authors treat the scale parameter as fixed and only update the shape parameter in the gamma distributions. This design choice requires more detailed discussion and analysis.
3. In the experiments, the authors evaluate the proposed model using only two layers and do not compare its performance with single-layer or multi-layer configurations. This limits the understanding of how the number of layers impacts performance. Additionally, other parameters, such as the window size and the number of factors, also require further discussion.
4. To improve clarity, I recommend the authors provide detailed graphical illustrations of the entire model. For reference, the authors might consider Figure 1 in [1].
5. Recent advancements in deep Poisson factor analysis models, such as [2] and [3], should be discussed in the Introduction to provide a more comprehensive context.
[1] Chung J, Kastner K, Dinh L, et al. A Recurrent Latent Variable Model for Sequential Data. NIPS, 2015.  
[2] Mingyuan Zhou, Yulai Cong, and Bo Chen, Augmentable gamma belief networks, Journal of Machine Learning Research, 17(163), 1-44, 2016.  
[3] Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou, Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC, ICML 2017.