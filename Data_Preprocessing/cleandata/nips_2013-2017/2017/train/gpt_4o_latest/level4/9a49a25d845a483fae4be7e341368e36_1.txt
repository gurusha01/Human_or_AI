This paper proposes a novel framework for transforming a dataset to ensure that the transformed dataset satisfies both group fairness and individual fairness. The proposed framework is formulated as a convex optimization problem under specific conditions. The authors derive generalization error bounds in terms of utility and group fairness and validate the performance of their framework through experimental evaluations.
Firstly, the contributions of this paper are not articulated with sufficient clarity. The introduction should provide a more precise and explicit description of the contributions.
$ Details
From a theoretical perspective, the convexity discussed in Proposition 1 is relatively straightforward. The authors should elaborate on how the convexity of the proposed framework significantly influences the tradeoff between utility, group fairness, and individual fairness.
Proposition 2 is somewhat unclear. The proof omits the critical term $m$ by employing the big-O notation. The bounds should instead be expressed as $O(\sqrt{m \log(1 + n/m)/n + \log(1/\beta)/n})$. Here, the first term within the square root represents the complexity term, as $m$ corresponds to the parameter size of the learning transformation. For instance, if the non-discrimination variable is represented as a $d$-dimensional binary vector, then $m \geq 2^d$, which implies that an exponential number of samples with respect to $d$ is required to achieve a constant-order bound. From the perspective of sample complexity, the proposed framework does not satisfy learnability.
The empirical comparison between the proposed method and LFR is not conducted on a fair basis. LFR includes tunable parameters that allow adjustment of the tradeoff between utility, group fairness, and individual fairness, and these parameters should be appropriately tuned. Additionally, the authors should perform experiments with more granular variations in the values of $\epsilon$. The experiments presented in the manuscript do not sufficiently support the claim that the proposed framework can effectively control the tradeoff between utility and fairness. Furthermore, the authors should compare the proposed method with LFR in terms of individual fairness, as there is no evidence provided to demonstrate that the proposed framework ensures individual fairness.
In its current form, the significance of the contributions appears to be limited in both theoretical and experimental aspects.
----
After the rebuttal discussion, I have become more favorable toward this paper and have increased my score.