This paper investigates the generalized Hamming distance within the framework of fuzzy neural networks. It demonstrates that the computation performed at each neuron can be interpreted as the calculation of the generalized Hamming distance between the associated weight vector and input vector. This perspective is advantageous as it inherently provides a candidate bias parameter for the neuron, eliminating the need to learn additional parameters (e.g., through batch normalization) during training. Moreover, this interpretation enables the authors to analyze the role of rectified linear units and propose a double-thresholding mechanism to enhance the speed and stability of the training process. Building on these insights, the paper introduces generalized Hamming networks (GHN), where each neuron explicitly computes the generalized Hamming distance. These networks also incorporate non-linear activation functions.
The paper proceeds to present simulation results that evaluate the proposed GHN in the contexts of image classification and auto-encoder design. The results highlight the fast learning capabilities of GHN and their strong performance on the given tasks. Notably, the simulations reveal that GHN do not necessarily rely on max-pooling.
The manuscript is well-written and effectively conveys the core concepts. The simulation results are compelling. However, some figures appear incomplete; for instance, Fig. 4.2 lacks a curve corresponding to a learning rate of 0.0001. While I would have appreciated a more comprehensive evaluation, both theoretical and practical, I find the concept of GHN to be intriguing overall.
Minor comments:
1) In the abstract, '(GNN)' should be corrected to '(GHN)'.
2) On line 55 of page 2, 'felxaible' should be corrected to 'flexible'.