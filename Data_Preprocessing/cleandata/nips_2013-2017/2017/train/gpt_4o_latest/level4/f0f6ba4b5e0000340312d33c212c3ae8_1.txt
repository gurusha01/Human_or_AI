This paper presents an enhancement to the standard word hashing trick for embedding representations by employing a weighted combination of multiple vectors, each indexed by distinct hash functions, to represent individual words. This method can be implemented using either a predefined dictionary or during online training. The proposed approach is straightforward to understand and implement while significantly reducing the number of embedding parameters.
The results are generally strong, and the method is notably elegant. Furthermore, the hash embeddings appear to serve as an effective regularization mechanism. However, Table 2 would be improved by including details on the final selected vocabulary sizes and the parameter reduction achieved through hash embeddings. Additionally, Table 3 omits a joint state-of-the-art model for the DBPedia dataset [1].
In Line 255, the claim is made that the ensemble would require the same training time as a single large model. This assertion holds only if each model in the ensemble has an architecture with fewer non-embedding weights. Based on the description, it seems that the number of non-embedding weights in each network of the ensemble matches that of the large model, which would result in a significantly longer training time for the ensemble.
Table 3 highlights the top three models, but a clearer comparison could be achieved by dividing the table into embedding-only approaches and RNN/CNN-based approaches. Additionally, it would be interesting to evaluate these embeddings in more context-sensitive RNN/CNN models.
Minor comments:  
- L9: Typo: million(s)  
- L39: Typo: to(o) much  
- L148: This sentence is awkwardly phrased and should be revised.  
- L207: Clarify the meaning of "patience."  
- L235: Typo: table table  
- Table 4: Were these results obtained by summing the importance weights? What were the rankings of the highest and lowest weights?  
[1] Miyato, Takeru, Andrew M. Dai, and Ian Goodfellow. "Virtual adversarial training for semi-supervised text classification." ICLR 2017.