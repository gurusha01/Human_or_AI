This paper introduces a learning reduction for extreme classification, a multiclass classification problem where the output space can include up to 100,000 distinct classes.
Many existing methods for extreme classification rely on constructing a tree structure over the output labels, such as in hierarchical softmax, to reduce the number of binary predictions required to logarithmic levels, or on label embedding techniques that can be efficiently learned using least-squares or sampling. The authors highlight the limitations of these approaches: the inferred latent tree may be suboptimal and prone to cascading errors, while the label embedding approach may suffer from prediction inaccuracies when the true label matrix does not exhibit low-rank properties.
An alternative strategy is to reformulate extreme classification as a pairwise binary classification problem. The authors adopt this framework and propose a scalable sampling-based method with theoretical guarantees of consistency with the original multiclass ERM formulation.
The authors evaluate their method on five text datasets, comparing it against several competing approaches (including one-vs-all classification where feasible). Results demonstrate that the proposed method generally outperforms others in terms of accuracy and F1 score when the number of classes exceeds 30,000. Additionally, it trains faster and uses significantly less memory, though it exhibits slightly slower prediction speeds.
The Aggressive Double Sampling reduction is governed by two key parameters. The first parameter adjusts the sampling frequency for each class, inversely proportional to the empirical class frequency, to ensure that rare classes in the long tail are not overlooked. The second parameter specifies the number of adversarial examples to sample uniformly. This process is repeated to train the final dyadic classifier.
At inference time, generating pairwise features for all possible classes is computationally infeasible. To address this, the authors propose a heuristic that identifies candidate classes by constructing an input space centroid representation for each class, computed as the average of vectors in that class. Predictions are then made using pairwise classifications restricted to these candidate classes.
Theoretical analysis of the reduction is challenging due to two factors. First, the transformation from multiclass to dyadic binary classification introduces a dependence structure among the dyadic examples, replacing a sum of independent random variables with a sum of partially dependent ones. Second, oversampling rare classes alters the empirical distribution. The authors address these challenges by partitioning the sum of dependent examples into several sums of independent variables based on the graph structure induced by the example construction and applying concentration inequalities for partially dependent variables, as developed by Janson. While the finite-sample risk bound is biased due to the mismatch between true class probabilities and oversampled ones, these biases diminish linearly with the size of the sampled and re-sampled training sets. The analysis appears rigorous, though I have not independently verified the proof in detail.
In summary, this paper presents a robust method for extreme classification that achieves strong performance on large datasets while maintaining low memory and computational requirements. The binary reduction algorithm is theoretically well-founded, innovative, and practical for real-world applications.