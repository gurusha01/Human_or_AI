The paper conducts an analysis of gradient descent as a learning strategy for kernel methods. Its primary contribution lies in demonstrating a limitation on the class of functions that gradient descent can effectively approximate. Specifically, the paper introduces the concept of 'computational reach' for gradient descent and establishes that, for smooth kernels, this 'computational reach' encompasses only a small subset of the function space. To address this limitation, the authors propose a novel method called "EigenPro," which incorporates a preconditioning strategy. The method is implemented efficiently and evaluated on standard datasets, with results showing consistent improvements over baseline approaches. However, the statistical significance of these improvements is not assessed.
Overall, the paper is well-written, with ideas presented clearly and the experimental setup and results appearing robust and convincing. While I did not verify the mathematical claims in detail, they seem credible. On the whole, this work provides valuable insights into the mechanics of large-scale kernel learning and offers meaningful advancements for its improvement.