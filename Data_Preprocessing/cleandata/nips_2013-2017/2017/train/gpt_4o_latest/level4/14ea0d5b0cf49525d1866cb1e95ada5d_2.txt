Review - Summary (Paraphrased):
This paper introduces a novel reduction method for converting multi-class classification problems into binary classification tasks, specifically designed for scenarios with a very large number of classes. The authors propose a hypothesis that assigns scores to (input, class) pairs, with the associated loss function measuring the proportion of incorrect classes that receive higher scores than the true class. More concretely, they assume the existence of a feature transformation, denoted as phi, which maps (input, class) pairs into a p-dimensional feature space, followed by learning a mapping from R^p to scores.
The proposed reduction builds upon the work of Joshi et al. (2015), which, for each data point (x, y), generates K-1 transformed points, where each point conceptually represents a comparison between the true label y and an incorrect label y'. Since the transformed dataset inherently contains correlated training examples, many conventional generalization bounds are not directly applicable. The first key contribution of this paper is an enhanced generalization analysis over Joshi et al. (2015), leveraging novel bounds on the fractional Rademacher complexity in this context. The second major contribution involves the use of sampling techniques during the reduction process to balance class distributions and reduce the size of the training dataset. Specifically, the authors sample classes (with replacement) to ensure roughly equal representation of each class in the transformed training data and, instead of including transformed points for all K-1 class comparisons, they randomly sample a subset of classes. They demonstrate that these sampling strategies do not significantly impact the generalization bounds. Lastly, the authors show that this new approach achieves competitive performance when compared to several strong baseline methods.
Comments (Paraphrased):
This paper introduces an intriguing reduction technique for addressing multi-class classification with a large number of classes by converting it into binary classification, along with corresponding generalization bounds to handle the correlated nature of the reduced dataset. The primary innovation lies in the sampling approach used to rebalance class distributions and reduce the size of the transformed dataset, while the foundational reduction itself is based on prior work by Joshi et al. The empirical comparisons with existing algorithms are particularly compelling.
However, the paper lacks a theoretical analysis of the prediction strategy described in Section 2.3, where comparisons are restricted to the k classes whose centroids are closest to a given input. 
Additionally, a more detailed discussion of the sample complexity bounds would have been valuable, particularly in relation to the generalization bounds derived for the original, non-transformed multi-class problems.