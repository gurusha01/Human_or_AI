This paper is generally well-written and was an enjoyable read. It introduces an expressive density model known as masked autoregressive flow (MAF), which constructs a normalizing flow by stacking multiple MADE layers. While the contribution feels somewhat incremental given that the techniques involved have been explored in IAF and MADE, the paper does a commendable job of elaborating on various types of generative modeling and offering practical guidelines for their use cases. Additionally, it establishes a connection between MAF and IAF. Below are a few comments and questions:
* It would be beneficial to further motivate the advantages of density models. What specific applications or downstream tasks make density models preferable to alternatives like VAEs or GANs? For instance, does the proposed density model enable efficient algorithms for marginalization or other inference tasks?
* When applying multiple transformations in the normalizing flow, does this imply that the conditionals are not required to be Gaussian, as suggested in Eq. 2? Since the density is computed based on the density of u, and u is derived from lower-level random vectors, it seems plausible that u could be non-Gaussian. Is this understanding correct?
* Does the base density of an MAF (line 225) correspond to the density of u in Eq. 3, or does it refer to the density of p(xi|x{1:i-1}) in Eq. 1? My assumption is that it refers to the former.
* In the comparison of results from MAF (5), MAF (10), and MAF MoG (5) on the POWER and MNIST datasets, MAF MoG achieves superior performance relative to the other two. My impression is that if multiple normalizing flow layers are used, with each layer incorporating multiple non-linear MADE layers, MAF should theoretically be universal without requiring MoG. What are the authors' thoughts on this?