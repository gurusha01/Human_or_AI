Review - Summary:  
The authors introduce a method to integrate human feedback in natural language for improving image captioning. Specifically, they take a snapshot of a phrase-based captioning model, generate captions for a subset of images, gather annotations to identify potential errors, train a feedback network, and subsequently use reinforcement learning to refine the model based on this feedback. The experiments demonstrate that incorporating such feedback enhances performance as measured by automatic evaluation metrics for captioning.
Strengths:  
(a) The paper tackles an important and well-motivated problem—leveraging human feedback for learning. The results indicate that human feedback on model performance is more effective than additional caption annotations, which is both intuitive and a promising outcome.  
(b) The authors have carefully designed the process of collecting human feedback in a minimally ambiguous manner, enabling effective fine-tuning of the model. The choice of a phrase-based captioning model facilitates this feedback collection.  
(c) The experiments are thorough, with various ablations provided to validate the utility of human feedback.
Weaknesses:  
(a) The absence of human evaluation on caption quality limits the understanding of the model's performance. Even a small-scale human evaluation would provide more insight, as automatic metrics often correlate poorly with human judgment.  
Comments:  
(a) L69 - The claim about [3] using reinforcement learning is incorrect. Instead, [34] appears to employ reinforcement learning in the context of visual dialog.  
(b) Table 6 - Rouge-L (second row) seems to have an anomalous value. It does not align with the range of other models.  
(c) Figure 2 is incomplete and unclear. Without proper labeling of terms like \(ct\), \(ht\), and \(h_{t-1}\), it is difficult to understand how the image is processed to generate caption phrases in the diagram.  
(d) Section 3.3: The overall reinforcement learning setup for fine-tuning with feedback is not introduced until this section, which causes confusion. For instance, L162 might lead readers to mistakenly assume the feedback network itself is trained via reinforcement learning. Similarly, L164's mention of a "newly sampled caption" lacks context. It is recommended to restructure the text to first explain the overall training procedure before delving into the feedback network.  
(e) L245 - The meaning of this sentence is unclear. Please clarify.  
(f) How is decoding performed during evaluation—beam search, sampling, or greedy decoding?  
(g) L167 - There is inconsistency in the superscript notation.  
(h) L222 - Why is the testing dataset so small, constituting only about 1/20th of the training dataset?  
Typos:  
(a) L28-29 - "reinforcement learning (RL)" should be corrected.  
(b) L64 - Replace "it exploit's" with "it exploits."  
(c) L74 - "Policy Gradients" should be lowercase: "policy gradients."  
(d) L200 - There seems to be a typo in the equation specifying the range for \(\lambda\).  
References:  
[34] Das, Abhishek, et al. "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning." arXiv preprint arXiv:1703.06585 (2017).