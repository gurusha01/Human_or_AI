The paper extends prior work on ℓ∞ recovery in high-dimensional models by generalizing results that were previously limited to linear regression to a broader class of general loss functions. While I did not have sufficient time to rigorously verify all the proofs, they appear to be correct upon initial inspection. However, the paper suffers from a lack of clarity in its presentation, and significant effort should be devoted to improving the exposition.
Comments:  
The discussion on the proposed connections to incoherence could be expanded. Incoherence is a considerably stronger assumption in the linear regression setting for establishing ℓ∞ bounds. Can analogous notions of incoherence be formulated in the context of this more general framework?  
The authors should also consider referencing the work of Ren et al. (ASYMPTOTIC NORMALITY AND OPTIMALITIES IN ESTIMATION OF LARGE GAUSSIAN GRAPHICAL MODELS) to provide a comparison with ℓ∞ bounds, optimal rates, and the assumptions required for precision matrix recovery.  
The text following Corollary 3 and onward is often unclear. For example, under isotropic Gaussian design, ℓ∞ control for classification under mild assumptions on the correlation between the labels and the design enables support recovery with only n > log p samples. How does this relate to the distinction between generative and discriminative models?  
Additionally, certain statements, such as those suggesting one method requires more samples than another (e.g., line 218), should be softened, as no lower bounds are provided to substantiate these claims.