The paper introduces a novel approach termed Conditional Batch Normalization (CBN), designed to integrate language information from the input question into the visual processing pipeline of existing Visual Question Answering (VQA) models at an early stage. Specifically, the method updates only the batch normalization parameters of a pre-trained CNN using the VQA loss, conditioning these parameters on the LSTM embedding of the input question.
The proposed method is evaluated on two VQA datasets: the VQA dataset from Antol et al. (ICCV15) and the GuessWhat?! dataset from Vries et al. (CVPR17). Experimental results demonstrate that CBN significantly improves VQA performance. The paper also explores the impact of adding CBN to different CNN layers, concluding that applying CBN to the top two layers yields the best results. Furthermore, the authors quantitatively show that the performance gains are not merely due to fine-tuning the CNN but are specifically attributable to the language-based modulation, as evidenced by the superior performance of the proposed model compared to a variant where batch normalization parameters are fine-tuned without language conditioning.
Strengths:
1. The paper is well-motivated, and the idea of using language to modulate early visual processing is both novel and relevant for the VQA task.
2. The proposed CBN method is versatile and can be applied to any existing VQA model, enhancing its applicability.
3. The ablation studies are insightful, shedding light on the contribution of early-stage language modulation.
4. The paper provides detailed hyperparameter settings, ensuring reproducibility of the results.
Weaknesses:
1. The primary contribution of the paper is CBN, but the experimental results do not advance the state-of-the-art in VQA on the widely studied VQA dataset. This may be due to the baseline VQA model used in the experiments not being among the strongest available. To strengthen the claim that CBN can benefit more advanced VQA models, the authors should evaluate CBN on multiple models, preferably closer to state-of-the-art (e.g., MCB by Fukui et al., EMNLP16, or HieCoAtt by Lu et al., NIPS16). It remains unclear whether CBN can improve state-of-the-art models, which may already be highly optimized.
2. Line 170: It would be helpful to quantify the performance differences arising from variations in image sizes and ResNet architectures.
3. In Table 1, results on the VQA dataset are reported for the test-dev split. However, per the dataset guidelines (http://www.visualqa.org/vqav1challenge.html), results should be reported on the test-standard split to avoid potential overfitting to test-dev through repeated submissions.
4. In Table 2, applying CBN to layer 2 in addition to layers 3 and 4 results in performance degradation on GuessWhat?! compared to applying CBN to layers 3 and 4 only. The authors should provide an explanation for this behavior.
5. Figure 4 visualization: In Figure 4(a), the ResNet model is not fine-tuned, so the lack of clear answer-type clusters is expected. A more meaningful comparison for Figure 4(b) would be against the fine-tuned ResNet model in Figure 4(a) (Ft BN ResNet), rather than the non-fine-tuned version.
6. The first two contribution points in the introduction could be combined for conciseness.
7. Minor errors and typos:
   a. Lines 14â€“15: Repetition of the word "imagine."
   b. Line 42: Missing reference.
   c. Line 56: "impact" should be "impacts."
Post-rebuttal comments:
The new results demonstrating the application of CBN to the MRN model are compelling, showing that CBN can enhance performance even on fairly advanced VQA models. While the results do not include evaluations on state-of-the-art VQA models, they are sufficient to recommend acceptance of the paper.
However, a few issues remain:
1. There appears to be confusion regarding the test-standard and test-dev splits of the VQA dataset. In the rebuttal, the authors report the MCB model's performance as 62.5% on the test-standard split. However, this figure corresponds to the test-dev split, as per Table 1 of the MCB paper (https://arxiv.org/pdf/1606.01847.pdf).
2. The reproduced performance for the MRN model aligns closely with the original MRN paper when trained on VQA train + val data. The authors should clarify in the final version whether they used train + val or just train for training the MRN and MRN + CBN models. If train + val was used, the performance cannot be directly compared to the 62.5% of MCB, as that figure corresponds to training on train only. When MCB is trained on train + val, its performance is approximately 64% (Table 4 in the MCB paper).
3. The citation for the MRN model in the rebuttal is incorrect. The correct citation is:
   ```
   @inproceedings{kim2016multimodal,
     title={Multimodal residual learning for visual qa},
     author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
     booktitle={Advances in Neural Information Processing Systems},
     pages={361--369},
     year={2016}
   }
   ```
4. As noted by AR2 and AR3, it would be valuable to investigate whether the findings from ResNet generalize to other CNN architectures, such as VGGNet.