This paper introduces a method aimed at reducing the variance of stochastic gradients in variational inference. By leveraging the reparameterization trick, the score function is incorporated into the stochastic gradients. While this score function can act as a control variate, it more frequently increases the variance of the gradients. Importantly, omitting the score function does not bias the stochastic gradients and often yields improved empirical performance.
The topic addressed by the paper is significant, as the reparameterization trick has significantly broadened the scope of variational inference. However, there has been limited systematic exploration of alternative approaches to computing stochastic gradients using the reparameterization trick.
Beyond tackling an important problem, the paper presents some unexpected findings: 1) using a Monte Carlo approximation for the entropy term can result in lower overall variance compared to employing the exact entropy term, and 2) the score function unnecessarily inflates the variance of the stochastic gradients.
The paper does not rely on novel mathematical derivations or extensive theoretical development to support its claims. While the empirical results are valuable, the observed improvements are relatively modest. Nonetheless, the paper is well-written and clear in its presentation.
Regarding equations 5â€“7, I assume their correctness, but further clarification would be beneficial. Specifically, why is differentiation applied solely to the integrand in equation (1), given that the measure of the integrand itself depends on the variational parameters? Additionally, how does equation 7 follow? If it is a straightforward application of the chain rule, could notational adjustments or additional explanations make this more transparent? These notational ambiguities may partly explain why the unnecessary variance introduced by the score function has gone unnoticed until now.