This paper presents a two-step decoding model applied to NMT and summarization. The concept is well-articulated, and the experimental results demonstrate notable improvements over strong and realistic baselines.
The authors approximate the marginalization of the intermediate hypothesis using Monte Carlo sampling. While this is a valid approach, it is unclear how many samples are utilized. Does this hyperparameter influence the performance? Additionally, do the authors employ beam search instead of simple sampling? These technical details should be elaborated upon for greater clarity.
Overall, the work described in this paper is compelling, and the experimental results are meaningful. However, I found the introduction overly ambitious. Two paragraphs are devoted to a "cognitive" justification, as though the authors have reinvented the wheel. Multi-step decoding has been explored for many years, and while it is commendable to propose a solution for end-to-end neural models, there is no need to dismiss prior work or justify the approach with superficial cognitive arguments (note that "cognitive science" includes the term "science").
Page 2: The sentence on line 73 does not add value and could be omitted.
Line 78: While the statement is accurate for end-to-end neural models, there is a long-standing tradition of multi-pass decoding in speech recognition. For instance, in the first step, a word lattice is generated using "weak" acoustic and language models, which is then refined iteratively with more sophisticated models. Therefore, it is incorrect to claim that this approach has not been explored for sequence prediction. It is unnecessary to disregard prior work simply because a novel name has been introduced.
Page 8:  
Line 277: The phrase "In this work, inspired by human cognitive process," could be avoided, as it comes across as overly pretentious.