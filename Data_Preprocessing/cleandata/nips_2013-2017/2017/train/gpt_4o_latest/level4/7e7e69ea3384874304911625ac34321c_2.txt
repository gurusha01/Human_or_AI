The paper proposes an autoencoder with a PixelCNN decoder and an adversarial loss applied to the latent space, comparing a uniform prior with the inference distribution. With an appropriate network design, the model effectively separates global input information, which is stored in the latent space, from local information, which is captured by the PixelCNN, as demonstrated on the MNIST dataset. When using a categorical distribution for the latents, the network is able to capture class-like information in an unsupervised manner. The approach also performs well in semi-supervised tasks. While the paper presents a thoughtful combination of VAE, adversarial networks, and PixelCNN, it represents yet another variation of these existing methods. The experiments and discussions are well-executed, but the model is most closely related to the VAE-PixelCNN combination, with the key difference being the replacement of the KL divergence loss on the latents with an adversarial loss. Although the authors discuss the mathematical differences, it would be valuable to conduct the same experiments (e.g., scaling the latent loss) with the VAE-PixelCNN baseline for a more direct comparison.
Additional comments:  
- In Figure 2c, it would be more informative to include results for the VAE-PixelCNN combination using the same network architecture and varying the scaling of the KL term. While the paper provides a mathematical explanation of the differences, both approaches ultimately penalize the latent space, aiming to make the latents resemble unit Gaussians.  
- Line 98: The claim is not necessarily accurate, as it depends on the function approximations used. The network has the flexibility to allocate information either to the latents or to the input. However, it is indeed more challenging for information to reside in the latents, as they are noisy and the posterior is only an approximation.  
- Do the authors observe any clear unsupervised separation, similar to what is shown in Figure 6 for MNIST, when applying the method to datasets like SVHN or NORB?