The paper introduces a method for automatic image captioning that integrates natural language feedback from humans alongside ground truth captions during training. The proposed framework employs reinforcement learning to optimize a phrase-based captioning model. Initially, the model undergoes supervised training using maximum likelihood estimation, followed by fine-tuning with reinforcement learning. The reward function is a weighted combination of BLEU scores relative to ground truth captions and human-provided feedback sentences. Additionally, the reward incorporates phrase-level evaluations derived from the human feedback.
The model is trained and tested on the MSCOCO image captioning dataset. Comparisons are made against two baselines: a purely supervised learning (SL) model and a reinforcement learning (RL) model without feedback. The proposed approach demonstrates significant improvements over the SL model and marginal gains over the RL model.
Strengths:
1. The paper is well-motivated, emphasizing the use of human-in-the-loop training for image captioning models.
2. The baselines (SL and RL) are reasonable, and the experiment comparing 1 ground truth caption versus 1 feedback caption is both insightful and engaging.
3. The work has the potential for significant impact, particularly if the gains over the RL baseline without feedback are substantial.
Weaknesses:
1. While the paper draws inspiration from how humans provide feedback to teach children, the proposed feedback mechanism also incorporates additional information—such as identifying incorrect phrases, providing corrected phrases, and specifying the type of error. These go beyond natural language feedback, and the authors should clarify this distinction in the introduction.
2. The performance improvement of the proposed model over the RL baseline without feedback is relatively modest (row 3 vs. row 4 in Table 6), and BLEU-1 even shows a slight decline. The authors should verify whether these improvements are statistically significant.
3. The paper does not analyze the contribution of the additional feedback components (incorrect phrase, corrected phrase, and error type) to the feedback network. What is the performance when each of these components is removed, and how does the model perform with only natural language feedback?
4. In Figure 1's caption, the authors state that the feedback network uses natural language feedback along with incorrect and corrected phrases marked by the annotator. However, equations (1)-(4) do not clearly show how this information is utilized. Lines 175-176 are also ambiguous—what is meant by "as an example"?
5. Lines 216-217: The rationale for using cross-entropy loss for the first (P – floor(t/m)) phrases is unclear. How does the model perform when reinforcement learning is applied to all phrases?
6. Line 222: Why was the official MSCOCO test set not used for reporting results?
7. Table 5 (FBN results): The authors should explain why performance degrades when additional information about missing, incorrect, or redundant phrases is included.
8. Table 6: The MLEC accuracy using ROUGE-L is unusually low. Is this a typo, or is there an explanation for this result?
9. The authors should discuss the failure cases of the proposed RLF network to provide insights for future research directions.
10. Minor errors/typos:
   - Line 190: "complete" → "completed"
   - Line 201: "We use either … feedback collection" → unclear phrasing
   - Line 218: "multiply" → "multiple"
   - Line 235: Remove "by"
Post-rebuttal comments:
I acknowledge the authors' emphasis on proper evaluation. It is crucial to ensure that the baseline results from [33] are comparable and that the proposed model builds upon them effectively. 
Based on this, I am revising my rating to marginally below the acceptance threshold.