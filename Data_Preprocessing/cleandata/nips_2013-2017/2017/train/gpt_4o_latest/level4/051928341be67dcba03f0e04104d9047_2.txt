The paper introduces a novel approach to model attention across both images and questions for the Visual Question Answering (VQA) task. The key contribution lies in proposing a generic attention formulation for multiple modalities, combining unary, pairwise, and ternary potentials across three modalities (image, question, and answer). The proposed method is evaluated on the VQA dataset and demonstrates superior performance compared to existing models when ternary potentials are incorporated.
Strengths:
1. The generic attention formulation for multimodal problems is a notable contribution, especially if it can generalize beyond a single task.
2. The ablation study, which evaluates performance without ternary potentials, provides valuable insights into the contribution of ternary potentials.
3. The paper is well-written and easy to follow.
Weaknesses:
1. While the paper discusses attention over three modalities—image, question, and answer—it is unclear what "attention over answers" entails. Most answers are single words, and even multiword answers are treated as single tokens. The lack of visualizations for attention over answers further limits understanding. Clarification on this aspect is needed.
   
2. From the results in Table 1, it appears that the primary performance gains of the proposed model stem from the ternary potential. Without it, the model does not outperform existing models in the two-modality setup (except for HieCoAtt). The authors should provide more insight into this observation.
3. Since the ternary potential is the main driver of performance improvement, the authors should compare their model with existing approaches that also incorporate answers as inputs, such as "Revisiting Visual Question Answering Baselines" (Jabri et al., ECCV16).
4. The paper does not discuss the failure cases of the proposed model. Analyzing failure modes would provide valuable insights for future research directions.
5. Additional errors/typos:
   - Line 38: "mechanism" should be "mechanisms."
   - Line 237 states that evaluation is performed on the validation set, but Table 1 reports results for the test-dev and test-std sets. This discrepancy needs clarification.
Post-Rebuttal Comments:
While the authors' response to the concern regarding "the proposed model not outperforming existing models for two modalities" lacks sufficient quantitative evidence, I recommend acceptance of the paper due to its contribution of a generic attention framework for multiple modalities and the strong quantitative results for three-modality attention, which outperform the state-of-the-art (SOTA). Additionally, the authors' quantitative evaluation of the proposed model's attention maps against human attention maps, as presented in the rebuttal, is compelling. These results suggest that the proposed attention maps exhibit higher correlation with human attention maps compared to existing models. Although the correlation values for SOTA models like MCB are not provided, the results appear significantly better than those for HieCoAtt.
I do, however, have a question regarding one of the authors' responses:
> Authors' response: "MCB vs. MCT": MCT is a generic extension of MCB for n-modalities. Specifically for VQA, the 3-MCT setup yields 68.4% on test-dev, whereas the 2-layer MCB yields 69.4%. We tested other combinations of more than two-modalities MCB and found them to yield inferior results.
Are the reported numbers swapped? Shouldn't 69.4% correspond to the 3-MCT setup? Additionally, the overall performance of MCB in Table 1 is reported as 68.6%. It is unclear which number the authors are referring to when they mention 68.4%. Clarification on this discrepancy would be helpful.