The authors introduce PMD, a population-based divergence between probability distributions, and demonstrate that it serves as a consistent estimator of the Wasserstein distance.
The proposed estimator is both conceptually straightforward and differentiable, making it well-suited for training neural network-based models.
The paper provides a comprehensive comparison of PMD with MMD, which is currently the most widely used population-based divergence in machine learning.
The authors acknowledge the limitations of their method, noting that the exact computation of PMD has cubic complexity. However, they propose an approximation with quadratic complexity and provide empirical evidence showing that this approximation does not significantly compromise statistical performance.
The manuscript is well-organized, clearly written, and appropriately cites relevant prior work.
The theoretical contributions appear to be sound.
The experimental evaluation is satisfactory. The authors benchmark PMD against MMD and other methods in the context of domain adaptation and also compare it to MMD for generative modeling tasks.
That said, I would have appreciated an exploration of the method's performance in generative modeling scenarios involving datasets with numerous modes. Additionally, I am curious about how PMD performs when the sample size \( N \) is smaller than the number of modes.
Overall, this is a strong paper that introduces a promising method for comparing probability distributions, with potential for significant impact.