This paper introduces a model-based approach for conducting conditional independence tests on iid data. The core concept involves utilizing a nearest neighbor bootstrap method to create samples that approximate the distribution \( f^{CI} \). A classifier is then trained and evaluated to determine whether it can distinguish between the observed data distribution and the nearest neighbor bootstrapped distribution. If the classifier's performance is close to random guessing, the null hypothesis—that the data exhibits conditional independence—is not rejected; otherwise, the null hypothesis is accepted.
The authors derive bounds on the closeness of the nearest neighbor bootstrapped distribution to \( f^{CI} \) in terms of total variation distance. Additionally, they provide bounds on the empirical risks under ideal classification scenarios and for nearly independent samples.
Overall, the paper addresses a significant problem and is presented in a clear and comprehensible manner.
Detailed Comments:
Major:
1. The proposed method appears to consist of two primary components: generating samples that approximate \( f^{CI} \) and employing a classifier for decision-making. My question is whether either of these steps could be replaced by existing techniques from the literature. For instance, could the permutation-based method from [7] be combined with the classification-based approach to solve the problem? Alternatively, could the nearest neighbor bootstrap be paired with a kernel two-sample test? I am curious about how these alternative combinations would perform.
2. The nearest neighbor bootstrap distribution is shown to approximate \( f^{CI} \) in finite sample settings. However, if the ground truth indicates that \( x \) and \( y \) are weakly dependent given \( z \), how would the proposed method handle this scenario?
3. Is the method entirely symmetric with respect to \( x \) and \( y \)? From a causal perspective, \( x \) and \( y \) may be dependent given \( z \) due to two distinct causal models: \( x \) causes \( y \), or \( y \) causes \( x \). In these two cases, would it be necessary to decide which variable to sample? The authors have not addressed this issue.
4. I have serious concerns about how the parameter \( \tau \) is selected in Algorithms 2 and 3.
Minor:
In Algorithms 2 and 3, it seems that the empirical risk should be divided by the sample size.