The primary contribution of this paper is the introduction of a novel framework for parallel machine learning algorithms. The core idea involves combining base learners in a more effective manner than simple averaging. Specifically, subsets of hypotheses are replaced by their Radon point. The authors establish complexity bounds for their approach and provide empirical comparisons with parallel algorithms implemented in Spark, as well as with base linear models in Weka.
The paper is intriguing as it essentially proposes a black-box method for combining weak learners. The approach appears to serve as an alternative to bagging weak learners. On the theoretical side, the paper analyzes the complexity of the Radon machine, demonstrating that, since the original samples can be partitioned into multiple subsets, the resulting parallel machine achieves significantly higher efficiency compared to a base learner operating on the full dataset. Additionally, the authors present a PAC bound for the Radon machine. The theoretical contributions in the paper are solid. However, a key concern lies in the practical applicability of the method, particularly for high-dimensional datasets. The performance of the proposed approach in such scenarios remains unclear, as the experiments are conducted on very low-dimensional datasets (e.g., 18 features), which may not reflect the challenges of modern machine learning tasks across most domains. If the authors could address how their method could be extended to handle high-dimensional data effectively, the paper would be considerably stronger.