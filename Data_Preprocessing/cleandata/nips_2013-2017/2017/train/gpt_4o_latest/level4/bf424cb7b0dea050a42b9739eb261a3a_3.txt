This paper examines the challenge of prolonged convergence times for kernel methods when optimized using gradient descent. The authors introduce the concept of the computational reach of gradient descent after \( t \) iterations and analyze scenarios where gradient descent fails to reach the \( \epsilon \)-neighborhood of an optimum within \( t \) iterations. They provide illustrative examples of simple function settings with binary labels, demonstrating cases where gradient descent converges slowly to the optimum. Additionally, they highlight that while adding regularization can improve the condition number of the eigenspectrum (potentially enhancing convergence), it may also lead to overregularization in certain situations.
The authors propose the EigenPro method, which involves pre-multiplying the data with a preconditioner matrix that can be pre-computed. This approach accelerates convergence by reducing the disparity between the smallest and largest eigenvalues and ensures computational efficiency per iteration. They employ randomized SVD on the data matrix or kernel operator to obtain the eigenvalues and construct the preconditioning matrix using the ratio of these eigenvalues. Their experiments demonstrate that the per-iteration computational cost remains comparable to standard kernel methods, while the method achieves lower errors using less GPU time overall. The acceleration achieved is particularly notable for certain kernels.
The paper, while dense in its presentation, is well-written and reasonably accessible. However, additional details could be provided to explore the broader impact of the preconditioning matrix on gradient descent and whether it should always be applied to data matrices when training linear models. The authors argue that reducing the ratio of smaller to larger eigenvalues facilitates convergence, but it would be valuable to include a more intuitive or geometric explanation of how this method enhances gradient descent performance. Furthermore, it would be interesting to investigate whether other faster optimization algorithms, such as proximal or momentum-based methods, could also benefit from this preconditioning approach and achieve improved convergence rates for kernel methods.