Review - Paraphrased Version:
Summary:  
Recent efforts to enhance interpretability in complex models include methods like LIME, DeepLift, and Layer-wise Relevance Propagation. In such models, changes in input features affect the output in a nonlinear manner. Many prior studies have attempted to fairly allocate feature contributions. This paper highlights that the Shapley value — a concept from game theory representing the marginal contribution of each feature — is the only solution that satisfies specific desirable properties. The author introduces a unified framework for interpretable methods by defining additive feature attribution methods and using SHAP as a metric for feature importance. A method for estimating SHAP values is also proposed. The paper supports its claims with user studies and comparisons against related methods on benchmark datasets.
Quality:  
The paper is technically robust, offering detailed proofs and well-supported experimental results. However, I believe the discussion on the trade-off between Shapley value estimation accuracy and computational efficiency could be expanded.
Clarity:  
The paper is well-structured, providing sufficient background and motivation. That said, in Section 4.2 (page 6), which discusses model-specific approximations, the inclusion of an algorithmic flowchart or step-by-step explanation (e.g., how DeepLift can be modified to align with SHAP values) would be beneficial.  
There are also minor clarity issues:  
- On page 4, in Equation (10), the term \(\bar{S}\) is undefined. Should it be \(\bar{S} \subseteq S\)? Additionally, the last symbol in the row should likely be \(x_{\bar{S}}\) instead of \(x\).  
- On page 6, line 207, the citation style appears inconsistent.
Originality:  
This paper effectively identifies the Shapley value as the appropriate method for distributing feature contributions, grounding this claim in game theory. The author connects the Shapley value to several existing methods and rigorously demonstrates how to construct these methods through proofs. Notably, the paper explains the improved performance of the current DeepLift over the original version as a result of better approximations of the Shapley value.  
It would be interesting if the author could also explore connections between the Shapley value and gradient-based approaches [1,2], as these methods remain widely used for interpretability.  
[1]: Simonyan et al. (2013): "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps," http://arxiv.org/abs/1312.6034  
[2]: Springenberg et al. (2015): "Striving for Simplicity - The All Convolutional Net," http://arxiv.org/abs/1412.6806  
Significance:  
This work provides a solid theoretical foundation for feature importance methods. The well-defined SHAP metric has the potential to serve as a guiding standard for evaluating feature importance in the field of interpretability research.