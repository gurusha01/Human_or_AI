Summary: This paper introduces a gradient boosting method, CEGB, designed for prediction tasks under budget constraints. During training, CEGB minimizes a combination of empirical loss and a cost penalty term. It incorporates three types of costs: feature cost per example, feature cost across all examples, and evaluation cost. The algorithm identifies the optimal split by greedily minimizing the second-order approximation of the loss objective. Experimental results demonstrate improved performance compared to state-of-the-art methods on two datasets.
Significance: The proposed algorithm shares similarities with GreedyMiser (Xu et al., 2012), with the primary distinction being in the construction of weak regressors. CEGB's key innovation is its ability to introduce new splits at any current leaf, as opposed to a predefined structure. To reduce computational overhead in identifying the best leaf to split during each iteration, the method leverages a second-order approximation of the loss. While concepts like best-first tree learning and second-order loss approximation are well-established in the gradient boosting literature, their application in cost-efficient settings is novel. CEGB can be seen as a refinement of GreedyMiser.
Results: CEGB is evaluated against state-of-the-art methods, BudgetPrune and GreedyMiser, on two datasets: Yahoo and MiniBooNE. However, the authors did not include the Forest dataset, which was used in BudgetPrune (Nan et al., 2016). Based on my experience, gradient boosting methods often underperform compared to random forest-based methods (e.g., BudgetPrune) on the Forest dataset. Including additional datasets would strengthen the paper. Furthermore, the results in Figure 2(b) for CEGB without tree cost are unexpectedly strong. It is surprising (and somewhat implausible) that precision exceeds 0.14 with an average feature cost of ~20. Could the authors clarify how many trees were used and the average number of nodes per tree to achieve such performance?
Others: The authors repeatedly assert that BudgetPrune assumes feature computation costs are significantly higher than evaluation costs, rendering evaluation costs irrelevant in certain experiments. This claim is inaccurate. The BudgetPrune formulation by Nan et al. (2016) explicitly accounts for evaluation costs, which can be balanced against feature costs.
Overall: This paper offers a refinement of GreedyMiser, and the experimental results are promising. Including additional datasets and comparisons would further enhance the work.
After discussion with the reviewers: While the novelty of this work is somewhat limited given the prior contributions of XGBoost and GreedyMiser, the experimental results demonstrate improvements over state-of-the-art methods. The authors have committed to including additional datasets in the final version, which strengthens the case for acceptance. I am still inclined to recommend accepting this paper.