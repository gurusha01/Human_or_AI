This is an excellent paper that interprets batch normalization and ReLU through the lens of a generalized Hamming distance (GHD) network, while also proposing variations that lead to improvements. The connection drawn is both surprising (especially as illustrated in Figure 2) and highly intriguing.
The relationship between GHD and batch normalization is compelling, though it is somewhat non-intuitive why this correspondence holds. From my understanding, the paper suggests that, in practice, the estimated bias aligns with the sums of \(w\) and \(x\) in Equation (3), and that the entire expression \(wx + b\) corresponds to the GHD. However, is this correspondence averaged across all nodes in a single layer? Additionally, how does this relationship evolve across different layers? Does the GHD primarily function to ensure no information is lost during the transformation from \(x\) to \(w\)? It remains unclear why stacking layers of GHD would be desirable. Further clarification in this regard would be valuable.
A minor note: there is a typo in the critical box on line 80.
Overall, this is a fantastic paper.
----------------------
After author rebuttal:
I appreciate the authors' detailed rebuttal. My scores remain unchanged. Thank you for this outstanding contribution!