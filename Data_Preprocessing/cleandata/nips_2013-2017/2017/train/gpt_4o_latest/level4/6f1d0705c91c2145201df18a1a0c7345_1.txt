The paper introduces an implicit variational inference framework tailored for likelihood-free inference. This method builds upon prior work, particularly Hierarchical Variational Inference and Implicit Variational Bayes.
The central innovation of the paper lies in subtracting the log empirical distribution, log q(xn), and reformulating the ELBO as presented in Equation 4. This reformulation highlights the potential of using log density ratio estimation as a tool for likelihood-free variational inference. The remaining methodological components leverage standard techniques, including log density ratio estimation, reparameterization, and hierarchical variational distributions.
Although I found the proposed approach to address likelihood intractability intriguing, it hinges on performing log density ratio estimation in high-dimensional spaces (specifically, the joint space of data xn and latent variables zn). This poses significant challenges, as log density ratio estimation in high dimensions is notoriously difficult, and the authors do not provide compelling evidence of a robust algorithm to handle this issue. For example, the fact that the method is not applied to a standard GAN (which generates high-dimensional data like images) but instead to an unconventional Bayesian GAN for classification (see page 7) suggests that the current algorithm lacks stability. Indeed, it is difficult to envision how the proposed algorithm could be stabilized, as the "variational joint" distribution is likely to differ substantially from the "real joint" distribution in the early stages of optimization. This discrepancy makes log density ratio estimation highly unreliable, leading to biased gradients during the critical initial iterations of the optimization process.