The authors present a dataset model derived as the solution to a convex optimization problem, incorporating constraints designed to mitigate discrimination with respect to a specified sensitive attribute. These constraints enforce statistical parity not only in the joint distribution, as is standard in the literature, but also introduce quadratically many additional constraints (in terms of value outcomes) to limit discriminatory effects for each value of the protected attribute. Further constraints are imposed to address individual (point-wise) discrimination, while striving to maintain a distribution that closely resembles the original data. The proposed model can be utilized both to reduce discrimination in training data and to appropriately transform unseen test data.
The theoretical claims are well-founded, and the framework is flexible enough to incorporate various distance or similarity measures into the constraints. The authors provide experimental results on representative datasets for selected configurations, demonstrating that their method effectively mitigates discrimination, outperforming or matching alternative approaches from the literature.
The extensive Supplementary Material includes proofs that substantiate the theoretical claims, along with a detailed discussion of the datasets used (including the recently released COMPASS dataset) and comparisons with the datasets generated by their algorithm. A notable contribution is the introduction of a bound on the utility of their method in scenarios where dataset probabilities are estimated via maximum likelihood, as is commonly the case. The tightness of this bound, as expected, depends on the distribution being sufficiently well-behaved.
The authors position their work thoughtfully within the broader context of discrimination-aware machine learning and data mining, providing a comprehensive review of relevant literature.
Overall, I am positive about this work, though there are areas that could be improved, including the following:
i) The choice of the probability ratio as the measure for distribution similarity (Eq. 3) is not well-justified. This measure may disproportionately penalize differences when the target distribution has very low probabilities.
ii) The experimental section lacks a thorough explanation of the distance (distortion) measures used, such as the penalization factor of 10^4 applied in the COMPASS dataset for jumps across more than one category. While the high value appears reasonable, further clarification is needed.
iii) The experimental results on the Adult dataset underperform compared to the LFR method proposed by Zemer.
iv) The experimental evaluation would benefit from additional case studies to demonstrate broader applicability (e.g., datasets like German Credit, Heritage Health Prize, etc.).
v) While the generality of the framework is a strength, it relies on a general convex solver, and the variety of configurations makes it challenging to provide broadly applicable timing benchmarks. Nonetheless, reporting timing measurements for the experiments would enhance the completeness of the submission.
Although I have not rigorously verified the proofs in the supplementary material, the claims appear reasonable, with some being novel and all well-motivated, offering a unifying perspective on advancements in the field. I am confident that this work makes a significant contribution to the area of discrimination-aware machine learning and believe its publication would be highly beneficial to the community.