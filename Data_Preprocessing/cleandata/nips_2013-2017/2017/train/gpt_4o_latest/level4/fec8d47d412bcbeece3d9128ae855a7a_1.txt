The paper introduces a novel approach for controlling the false discovery rate (FDR) for p-values that incorporate additional information. Specifically, for each hypothesis, there is an associated p-value \( pi \) as well as a feature vector \( Xi \). The proposed method learns an optimal threshold for each hypothesis as a function of the feature vector \( X_i \). The concept appears innovative and interesting, and the paper is generally well-explained. Through various simulated and real-world data examples, the authors demonstrate that their method can leverage the additional information to increase the number of rejections while maintaining a given FDR control threshold.
It is crucial, in my view, that the \( Xi \)'s are not used in the computation of the \( pi \)'s, as this would introduce a circularity issue. While the authors acknowledge this in an example, there is no formal discussion of the probabilistic relationship between the \( Xi \)'s and the \( pi \)'s. On one hand, both the \( pi \)'s and the \( Xi \)'s are likely influenced by whether the null or alternative hypothesis is true. On the other hand, if the \( Xi \)'s were already used to compute the \( pi \)'s, they should not contribute to improving the decision boundary.
The authors argue that the multidimensionality of \( X \) rules out non-parametric methods, but this claim may be too strong. For instance, nearest-neighbors regression can adapt to the intrinsic dimensionality of the regression function, even when the ambient dimension of \( X \) is high.
I am uncertain about the robustness of the cross-validation procedure proposed by the authors against overfitting. This is not a standard supervised learning setting, as the "label" (FDR) is unknown and replaced by an estimator. This distinction should be emphasized and elaborated upon more clearly. Additionally, since the FDR estimator is noisy, it may, to my understanding, lead to higher FDR on the test set. The authors provide a bound in Theorem 1, which increases with the number of folds in the cross-validation procedure, but further clarification would be beneficial.
The mirror estimator proposed by the authors may exhibit low bias but high variance when \( t(x) \) is small, as very few p-values will fall within the range \([1-t(x), 1]\). This issue is reminiscent of Storey's approach, where the choice of the \( \lambda \) parameter balances bias and variance in estimating the null proportion using the interval \([\lambda, 1]\).
Minor Comments:
- Line 136: Replace "this" with "these".