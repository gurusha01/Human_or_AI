This paper integrates multiple techniques in large-scale optimization, including smoothing, acceleration, homotopy, and non-uniform sampling. It addresses the optimization problem involving three convex functions and a linear operator, expressed as f(x) + g(x) + h(Ax), where f is smooth. To handle the last term h(Ax), the authors employ the smoothing technique introduced by Nesterov [14]. Subsequently, a block-wise forward-backward splitting method is utilized, where one block is randomly selected and updated at each iteration. Acceleration and homotopy are then incorporated to develop a more rapidly converging algorithm. Notably, this approach represents a primal algorithm that applies the proximal gradient method to the smoothed function.  
The numerical experiments demonstrate the effectiveness of the proposed algorithm in comparison with several existing methods.  
L165: The reference is missing.