This paper introduces a double sampling strategy aimed at enhancing the multiclass classification framework described in [16]. The authors provide both theoretical insights and empirical evaluations of their proposed method. Experimental results on a text dataset demonstrate that the proposed approach effectively addresses the multi-class classification problem, even when the number of classes is large.
Pros:
- The authors perform extensive experimental comparisons against multiple baseline methods. The results indicate that the proposed approach achieves competitive performance while reducing training time across several datasets.
- The extreme classification problem is a significant area of research with numerous practical applications.
Cons:
- The paper heavily builds upon [16]. While it introduces two main contributions—1) the double sampling strategy and 2) new generalization bounds based on local Rademacher complexities—the level of novelty is somewhat limited.
- Although the proposed approach reduces training time, it still incurs a longer prediction time compared to other methods. In practical scenarios, prediction time is arguably more critical.
Comments:
- The interpretation of the generalization bound presented in the paper is unclear. How does this bound compare to those derived for other approaches?
- The experimental methods include both batch learning and online learning algorithms, which have significantly different memory requirements.