The paper introduces a novel approach for training predictive models, where the performance is not guided by traditional likelihood objectives but is instead evaluated based on performance in an external task unrelated to the model itself. To accomplish this, the model parameters are optimized to minimize the loss on the external task, which may involve solving a sub-optimization problem that depends on the model parameters. Both synthetic and real-data experiments are provided, effectively demonstrating the utility of the proposed approach.
The introduction is well-motivated, and the overall exposition is clear and easy to follow. The paper is technically robust, building on solid theoretical foundations, and I did not identify any apparent flaws.
The paper provides strong motivation for the necessity of the proposed approach. I view this work as a valuable and well-justified application of existing technical contributions. The critical technical component enabling this approach—differentiation through an argmax operation—has already been introduced in [Amos 2016]. While leveraging these prior results, this work applies them to a highly relevant and well-motivated formulation of task-based learning, which I believe will be of significant interest to the machine learning community.
One notable advantage, but also a potential limitation, of the proposed approach is that the model's behavior is inherently tied to the specific task. Could the authors provide insights into how much the model might overfit to the given task and whether it generalizes effectively to a different (even slightly modified) task?
Additionally, it would be beneficial to reference related work on meta-learning, such as "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks" by Finn et al., as it represents another example of differentiation through an optimization process.