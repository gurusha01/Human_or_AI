The paper introduces a class of probability models, termed hierarchical implicit models, which involve observations linked to 'local' latent variables that are conditionally independent given a set of 'global' latent variables. Notably, the observation likelihood in these models is not assumed to be tractable. The authors propose a KL-based variational inference approach for such 'likelihood-free' models, leveraging a GAN-style discriminator to estimate the log ratio between a 'variational joint' distribution, \( q(x, z) \), constructed using the empirical distribution of observations, and the true model joint density. Additionally, this method supports implicit variational models, also referred to as 'variational programs.' The paper demonstrates proof-of-concept applications in ecological simulation, a Bayesian GAN, and sequence modeling with a stochastic RNN.
The paper is well-written, with clear exposition, thorough citations, and a careful explanation of the technical framework. While the application of density ratio estimation to variational inference appears to be a concept that has been explored in related work (e.g., the Adversarial VB paper), the synthesis presented here is notably cleaner, more accessible, and broader in scope, particularly due to its support for implicit models, compared to similar papers I have encountered.
The definition of hierarchical implicit models is a valuable theoretical contribution and serves as a foundation for the inference framework introduced in Section 3. However, the factorization in Equation (1), which assumes iid observations, is quite limiting. In fact, it does not technically encompass the Lotka-Volterra or stochastic RNN models discussed in the paper, as both exhibit temporal dependencies. It would be beneficial for the authors to acknowledge that their inference approach is more general than the stated factorization and to discuss potential extensions to accommodate structured observations (e.g., time series, text, or graph data) and latent variables.
The experimental results are the weakest aspect of the paper. The 'Bayesian GAN' example is overly simplistic, and its classification setup feels contrived, as GANs are typically valued for unsupervised learning tasks. The symbol generation RNN experiment lacks comparative evaluation against other methods, making it unclear how well it performs. The Lotka-Volterra simulation is the most compelling experiment, as it effectively illustrates the utility of implicit models and demonstrates improvement over the ABC state of the art, despite the simplicity of the model (few parameters and no latent variables).
While the paper does not present groundbreaking results, and the proposed approach may be challenging to implement in practice (as is often the case with GANs), the method is general and powerful, with the potential to enable effective Bayesian inference for entirely new classes of models. The formulation presented here is likely to serve as a valuable resource for researchers exploring this area further. For these reasons, I consider this paper to be a meaningful contribution.
Miscellaneous Comments and Questions:
1. Lotka-Volterra Model: The equations provided (line 103) may contain errors. Shouldn't \( \beta3 \) be added, rather than subtracted, to model the predator birth rate? As written, \( dx2/dt \) is always negative in expectation, which seems incorrect. Additionally, \( \beta_2 \) appears to serve as both the predator and prey death rate. Is this intentional? Most references, including the cited Papamakarios and Murray paper, use four independent coefficients.
2. Line 118: The text states, "We described two classes of implicit models," but only one (HIMs) is explicitly mentioned.
3. Line 146: The phrase "log empirical log \( q(x_n) \)" is redundant.
4. Explicit Variational Approximations: If one wishes to use an explicit variational approximation (e.g., a mean-field Gaussian, as in the Lotka-Volterra experiment) for an implicit model, is there a natural way to leverage the explicit variational density to achieve faster inference?
5. Objective Interpretation: Subtracting the constant \( \log q(x) \) from the ELBO implies that the ratio objective (Equation 4) no longer provides a lower bound on the true model evidence. This should likely be noted in the paper. Is there a principled interpretation of the quantity in Equation (4)? It appears to be a lower bound on \( \log p(x)/q(x) \), which, intuitively, resembles an estimate of the negative KL divergence between the model and empirical distributions. Could this be useful for model criticism?