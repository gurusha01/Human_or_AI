The authors demonstrate that numerous methods in the literature for explaining individual model predictions can be categorized as "additive feature attribution" methods. They introduce a novel additive feature attribution method grounded in the concept of Shapley values, referring to the resulting explanations as SHAP values. Additionally, the authors propose a new kernel, termed the Shapley kernel, which enables the computation of SHAP values via linear regression (a method they call kernel SHAP). They also discuss how methods like DeepLIFT can be improved by more accurately approximating Shapley values.
Summary of Review:
Positives:
1. The paper presents a novel and theoretically sound framework for addressing the problem of model explanations, an area that has been lacking rigor (as many existing methods were developed in an ad-hoc manner).
2. Early versions of this work have already been cited to support improvements in other methods (e.g., New DeepLIFT).
3. Kernel SHAP offers a significantly improved approach to approximating Shapley values compared to classical Shapley sampling, achieving much lower variance for a given number of model evaluations (Figure 3).
Negatives:
1. The algorithm for Max SHAP is flawed.
2. The proof for kernel SHAP in the supplementary material is incomplete and poorly written.
3. The paper contains numerous typos and inconsistencies: for example, the runtime of Max SHAP is stated as O(M³) in the main text but O(M²) in the supplement, and the equation for L under Theorem 2 is missing a closing parenthesis for f().
4. The claim that SHAP values are a superior form of model explanation is debatable. The case studies in Figure 4 appear to be selected to favor SHAP, and the paper lacks a discussion of kernel SHAP's runtime or the recommended number of function evaluations for reliable performance. Additionally, LIME is absent from the comparison in Figure 5.
---
Detailed Comments:
The adaptation of Shapley values to the problem of model explanations is a significant and insightful contribution. Early versions of this work have already been used by other researchers to justify improvements to their methods (e.g., New DeepLIFT). Kernel SHAP, in particular, represents a substantial advancement, as it achieves much lower variance than classical Shapley sampling when estimating true Shapley values (Figure 3). Furthermore, kernel SHAP is appealing because, unlike LIME, its kernel choice is grounded in a solid theoretical foundation.
However, the paper has several notable issues. The algorithm for Max SHAP assumes that when \( i1 > i2 \) and \( i2 \) is the largest input seen so far, including \( i1 \) contributes \( (i1 - \max(\text{reference}{i1}, i2)) \) to the output. This assumption is only valid if none of the remaining inputs have reference values exceeding \( \max(\text{reference}{i1}, i2) \). For instance, consider two inputs \( a = 10 \) and \( b = 6 \), with reference values \( \text{ref}a = 9 \) and \( \text{ref}b = 0 \). The reference value of \( \max(a, b) \) is 9, and the difference-from-reference is 10. The correct SHAP values are 1 for \( a \) and 0 for \( b \), as \( b \) is too far below \( a \)'s reference to influence the output. However, the line \( \phi[\text{ind}] += \max(\text{ref}, 0)/M \) assigns a SHAP value of 3 to \( b \) (\( \text{ref} = x{\text{sorted}}[i] - r[\text{ind}] \), which is \( 6-0 \) for \( b \), and \( M = 2 \)). The authors appear to have tested only cases where all inputs share the same reference, which is common for max-pooling neurons but not necessarily for maxout neurons. Additionally, the algorithm seems challenging to implement efficiently on a GPU without significantly slowing down backpropagation, especially for max-pooling layers. While this algorithm is not central to the paper's thesis, it appears to have been written hastily, as evidenced by the conflicting runtime claims (O(M³) in the main text vs. O(M²) in the supplement).
The supplementary material's "Shapley kernel proof" is also problematic. The computational proof provided is incomplete, and the explanation is terse and riddled with grammatical errors. After multiple readings, my understanding of the proof is as follows:
1. \( f_x(S) \) represents the model output when all inputs except those in subset \( S \) are masked.
2. Shapley values are a linear function of the vector of \( f_x(S) \) values generated by enumerating all subsets \( S \).
3. Kernel SHAP, which performs weighted linear regression, also produces an output that is linear in the vector of \( f_x(S) \) values.
4. Therefore, if the linear functions in both cases are identical, kernel SHAP computes the Shapley values.
Based on this understanding, I expected the computational proof (performed for functions with up to 10 inputs) to verify that the final coefficients applied to the \( fx(S) \) vector by kernel SHAP match those used in the classic Shapley value computation. Specifically, I expected the proof to compare the coefficients derived from the weighted linear regression formula \( (X^T W X)^{-1} X^T W \), where \( X \) is a binary matrix enumerating all subsets \( S \) and \( W \) contains the Shapley kernel weights. However, the provided code does not compare coefficients. Instead, for a specific model generated by the method `singlepointmodel`, it compares the SHAP values computed by kernel SHAP to those computed by the classic Shapley value method. The code checks for negligible differences between `kernelvals` and `classicvals`, but `classicvals` is returned by the method `classicshapely`, which takes a specific model \( f \) as input. Moreover, the source code for several methods (e.g., `singlepointmodel`, `rawShapely`, and `kernelshapely`) is missing.
The claim that SHAP values are a superior form of model explanation is contentious. While the examples in Figure 4 align well with human intuition for feature importance, counterexamples can be constructed. For instance, consider a convolutional neural network filter with ReLU activation and a negative bias. This filter outputs a positive value when it detects a strong match to a pattern and zero otherwise. For an input producing zero output, a human might assign zero importance to all input pixels, as the input failed to match the pattern. However, SHAP values might assign positive importance to pixels resembling the pattern and negative importance to others, even for white noise inputs.
Similarly, the argument for kernel SHAP over LIME would be more compelling if LIME were included in Figure 5. The authors state that LIME's standard implementation uses superpixel segmentation, which is unsuitable here. However, kernel SHAP and LIME differ only in their weighting kernels, so it is unclear why the authors could not adapt kernel SHAP to replicate LIME. Additionally, the paper lacks a discussion of runtime or the recommended number of function evaluations. LIME's use of superpixel segmentation is primarily to reduce computational costs, and DeepLIFT's main advantage is its computational efficiency.
The section on Deep SHAP also raises concerns. The authors claim that "SHAP values for simple components of the network can be analytically solved efficiently," enabling fast approximations for the entire model. However, they discuss only two analytical solutions: Linear SHAP and Max SHAP. Linear SHAP yields backpropagation rules identical to original DeepLIFT, while the Max SHAP algorithm appears flawed. Although the authors mention Low-order SHAP, it is unsuitable for many neural networks, as individual neurons often have thousands of inputs. Even when neurons have few inputs, it is unclear whether the weighted linear regression can be efficiently implemented on a GPU without slowing down backpropagation.
Despite these issues, the adaptation of Shapley values for model interpretation and the development of kernel SHAP are significant contributions, warranting acceptance of this paper.