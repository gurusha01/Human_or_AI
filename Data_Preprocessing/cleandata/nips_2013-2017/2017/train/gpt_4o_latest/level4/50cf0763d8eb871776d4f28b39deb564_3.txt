The paper conducts a comparative analysis of generative and discriminative models within the framework of general exponential families, extending the discussion to high-dimensional settings. It introduces a concept of separability for general loss functions and establishes a framework for deriving \( l{\infty} \) convergence rates for M-estimators. This theoretical apparatus is subsequently applied to study the convergence rates (in both \( l{\infty} \) and \( l_2 \)) for generative and discriminative models.
While the paper presents interesting results, I found it challenging to follow, which may partly stem from my limited expertise in high-dimensional estimation. The introduction, along with the background and setup in Section 2, is relatively clear and accessible. However, the subsequent sections become dense with equations and technical derivations, often lacking sufficient guidance for the reader. For example, the authors' comments in lines 215-218, where Corollaries 3 and 4 are compared (pertaining to discriminative and generative models), left me confused. The authors claim that "both the discriminative and generative approach achieve the same convergence rate, but the generative approach does so with only a logarithmic dependence on the dimension." However, from my reading, the dimensional dependence in both Corollaries 3 and 4 appears to be the same (\( \sqrt{\log p} \)).
The central contribution of the paper lies in its introduction of the separability concept for loss functions. The motivating example in the introduction highlights that, under the generative model with a conditional independence assumption, the log-likelihood loss is fully separable into components that can be estimated independently. In contrast, the corresponding discriminative model (e.g., logistic regression) exhibits a less separable loss. This observation motivates the need for a more general notion of separability, which is formalized in Section 3. The authors demonstrate that losses with higher separability require fewer samples for convergence.
However, I struggled to connect this motivation to the formal definitions of separability (Definitions 1 and 2). These definitions assert that the error in the first-order approximation of the gradient is bounded by the error in the parameters. It would significantly enhance the paper's clarity if the authors could better articulate why this specific definition is appropriate and how it aligns with the intuitive idea of loss separability into independent components.
Additionally, starting from Corollary 3, the authors introduce the notation "\(\succsim\)" without any explanation. Clarifying its meaning would improve the paper's readability.
I am unable to fully assess the paper's significance due to my limited familiarity with prior work in this area (aside from the work of Ng & Jordan). Nevertheless, I found the results intriguing, albeit hindered by the presentation's lack of clarity. I do believe the theoretical contributions are strong. However, the paper would benefit from a clearer takeaway regarding when to prefer generative versus discriminative methods in high-dimensional settings, akin to the insights provided by Ng & Jordan.
Minor comments:
- Line 36: "could be shown to separable" → "could be shown to be separable"
- Line 65: "n" → "in"
- Line 87: ":" and "j" should be in the subscript
- Line 259: "bayes" → "Bayes"
- Lines 254-265: In the "Notation and Assumptions" section, the Bayes classifier is introduced only after its error has already been mentioned.