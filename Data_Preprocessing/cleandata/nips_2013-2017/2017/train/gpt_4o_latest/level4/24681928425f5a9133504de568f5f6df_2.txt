The authors investigate human question-asking behavior in scenarios where answers are K-ary. They manually define a probabilistic context-free grammar (PCFG) for a "battleship" domain, where hidden colored shapes are distributed across a partially observable grid (i.e., some tiles are revealed as containing part of a ship of a specific color or being empty). The agent's objective is to formulate a question with a single-word answer that maximizes the information gained about the board's state. The PCFG acts as a prior over questions, which are represented as statements in lambda calculus. The quality of a question is quantified as a linear combination of its informativeness (expected information gain), complexity (measured by its length or the negative log probability of being generated by the PCFG with a uniform distribution over rewrite rules), and "answer type" (e.g., whether the answer is true/false or a color). Human data, sourced from a previously published study, were manually encoded into the grammar. The authors evaluated lesioned versions of their model using leave-one-out cross-validation, demonstrating that all factors contributed meaningfully to model performance, with complexity emerging as the most significant. Additionally, they introduced a set of novel questions.
Question-asking is a critical topic in linguistics and hypothesis testing. The authors offer a fresh perspective that integrates contemporary linguistic and cognitive theories to explore how humans formulate questions. This approach has the potential to inspire new research in active learning, particularly in developing models capable of generating more sophisticated questions. The paper is exceptionally clear and well-written, with the model rigorously evaluated through both quantitative and qualitative analyses.
From a computational cognitive science standpoint, this work advances our understanding of self-directed learning. Prior research suggested that question informativeness alone is a poor predictor of human question-asking behavior. This study addresses part of this discrepancy, showing that informativeness is secondary to complexity and may also be influenced by the size of the model's question space. Consequently, the paper makes a valuable contribution to the cognitive science literature, which could be sufficient grounds for its acceptance at NIPS.
Nevertheless, the approach has notable limitations that may restrict its appeal to the broader NIPS audience. While automating the mapping of human question text to semantic logic representations might be feasible, the scalability and applicability of the method to more complex domains of interest remain unclear. The battleship domain is relatively small, with limited features, and the current method already faces computational challenges. Despite these concerns, the underlying ideas—particularly the focus on active learning with richer questions—are compelling. To enhance its relevance, the authors should consider presenting a concrete scenario of broader interest where their method could be applied effectively and yield novel insights.
Minor comments:
1. While it may not be conclusive, performing a nested model comparison using the log-likelihoods of the lesioned models could provide statistical significance tests.
2. For future work on sequential question-asking, there are alternative models in the linguistics literature, particularly within the formal semantics and pragmatics framework. The "Questions under Discussion" literature may be especially relevant. Below are a few references for further exploration:
   - Ginzburg, J. (1995). Resolving questions, part I. Linguistics and Philosophy, 18(5), 459–527.
   - Rojas-Esponda, T. (2013). The roadsigns of communication. Proceedings of Semdial (DialDam), 17, 131–139.