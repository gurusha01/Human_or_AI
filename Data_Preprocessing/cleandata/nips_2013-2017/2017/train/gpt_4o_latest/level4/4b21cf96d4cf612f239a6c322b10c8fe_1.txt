This paper explores the task of image-conditioned caption generation using deep generative models. In contrast to existing methods that rely solely on an LSTM-based pipeline, the proposed approach enhances the representation by incorporating an additional data-dependent latent variable. The authors frame the problem within the variational auto-encoder (VAE) framework, optimizing the variational lower bound as the training objective. To address the limited representation capacity of VAEs in caption generation, a data-dependent additive Gaussian prior is introduced. Empirical evaluations demonstrate that the proposed method generates sentences that are both diverse and accurate when compared to the pure LSTM baseline.
== Qualitative Assessment ==  
I appreciate the motivation behind incorporating a stochastic latent variable into the caption generation framework. While augmenting the prior in a VAE is not a novel concept, its application to the caption generation task is a noteworthy contribution. In terms of performance, the proposed AG-CVAE achieves more accurate results compared to both the LSTM baseline and other CVAE baselines (refer to Table 2). Additionally, the paper examines the diversity of generated captions relative to the pure LSTM-based approach (see Figure 5). Overall, the paper is well-written and provides sufficient experimental details.
However, considering the additive Gaussian prior as the primary contribution, the current version of the paper does not fully convince me. I am open to raising my score if the authors address the following concerns in the rebuttal.
* Evidence supporting AG-CVAE's advantages over CVAE/GMM-CVAE:  
The performance improvements reported in Table 2 are not particularly significant. Furthermore, qualitative comparisons (e.g., Figure 5 and supplementary materials) lack side-by-side evaluations of AG-CVAE against CVAE/GMM-CVAE. It remains unclear whether AG-CVAE truly offers greater representation power compared to these baselines. I encourage the authors to address this in the rebuttal and include such comparisons in the final version of the paper or supplementary materials.
* Diversity evaluation:  
It is unclear why AG-CVAE underperforms CVAE in terms of diversity. Additionally, the performance gap across different variations of AG-CVAE is not explained. Since CVAE is a stochastic generative model, I question whether evaluating the top 10 sentences is sufficient for diversity analysis. The results would be more compelling if the authors provide a curve where the y-axis represents the diversity measure and the x-axis represents the number of sentences.
Additional comments:  
* Pre-defined means of clusters for GMM-CVAE and AG-CVAE (Line 183):  
It is unsurprising that the authors did not achieve better results when the cluster means \( uk \) were treated as free variables. Without constraints (e.g., sparsity or orthogonality) on \( uk \) or \( c_k \), it is possible to learn redundant representations. I encourage the authors to explore this direction further in future work. A learnable data-dependent prior could potentially improve performance to some extent.