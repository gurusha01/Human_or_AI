The paper introduces a deep hashing method designed for a scenario where the training set includes class annotations. The model parameters are optimized using a loss function that incorporates pairwise similarities and class separability under a linear model. The key contribution lies in directly handling the binary embedding during training without relying on relaxation, achieved through alternating minimization. The results outperform numerous baselines across two datasets, and the paper includes a thorough analysis of various design choices. The writing is clear and easy to follow.
As someone not deeply familiar with this field, my primary concern is the level of novelty. It appears that the proposed optimization technique has been explored in prior work ([9, 17, 21]), so I assume the distinction here is the inclusion of the linear classification component in the loss function. Additionally, the overall approach seems quite intricate and potentially computationally expensive to train (how slow is it?).