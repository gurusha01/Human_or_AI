The paper introduces a supervised hashing algorithm leveraging a neural network. The network is designed to generate binary codes while minimizing classification error through an optimized loss function. To simultaneously optimize two distinct loss functions, the proposed approach employs an alternating optimization strategy. Experimental results on two datasets demonstrate competitive performance compared to other hashing methods, including those based on deep learning frameworks.
Pros:
The paper is well-written and easy to understand. The proposed algorithm is generally well-motivated, with a sound and effective optimization strategy.
Cons:
Some key references are missing, particularly [1], which shares similar ideas. It is crucial to discuss [1] in detail and conduct a more comprehensive performance evaluation to highlight the advantages of this work.
[1] Zhang et al., Efficient Training of Very Deep Neural Networks for Supervised Hashing, CVPR16  
[2] Liu et al., Deep Supervised Hashing for Fast Image Retrieval, CVPR16  
Additionally, the paper lacks sufficient details about the network training process (e.g., hyper-parameters), which may hinder reproducibility. Providing the code upon acceptance would address this issue.
Since the method relies on an alternating optimization strategy, it would be beneficial to include the loss curves to illustrate convergence and validate the approach's effectiveness.
The analysis in Figure 1 is intriguing, but the results and explanations (Lines 229-233) are inconsistent. For instance, DSDH-B does not consistently outperform DSDH-A. Furthermore, the explanation for why DSDH-C only slightly outperforms DSDH-A (Lines 231-232) seems unconvincing. This discrepancy might stem from differences in binarization strategies and warrants further experimental validation.
Lastly, why does DSDH-B outperform DSDH in Figure 1(a) when using more bits?