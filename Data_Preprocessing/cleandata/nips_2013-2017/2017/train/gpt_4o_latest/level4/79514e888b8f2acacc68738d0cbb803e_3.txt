After reviewing the authors' rebuttal, I have updated my score to 7. Overall, this is an engaging paper with a compelling idea. While the theoretical contributions are highlighted, I find the empirical results to be more impactful. However, the theoretical framework presented in the paper is not entirely convincing, particularly regarding aspects such as input versus feature space and convexity.
I believe the connection to classical semi-supervised learning and the cluster assumption should be more explicitly emphasized, along with the low-density assumption on the boundary as discussed in the following paper:
Semi-Supervised Classification by Low Density Separation  
Olivier Chapelle, Alexander Zien  
[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5826&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.5826&rep=rep1&type=pdf)
I am revising my review to a score of 7, with the expectation that the authors will situate their contributions more clearly within the context of established work in semi-supervised learning, particularly emphasizing that the decision boundary should lie in low-density regions. This contextualization will strengthen the paper.
---
This paper analyzes how GANs can be applied to semi-supervised learning (in the "K+1" setting of [15]). The authors propose a set of assumptions under which a complement generator is shown to improve the supervised task's accuracy. Specifically, the generator is required to target low-density regions in the input space.
Building on this idea, the paper introduces an algorithm that combines the feature matching criterion with density estimation (using a PixelCNN) to guide the generator toward low-density regions of the estimated model (subject to a threshold on log probability). Additionally, entropy regularizers are incorporated to encourage diversity in the generator. The paper reports positive empirical results.
Understanding and improving GANs in the semi-supervised setting is an important research problem. However, the paper has several limitations:
1. Mismatch between analysis and method: The theoretical analysis is conducted in the feature space under numerous assumptions, but the proposed method operates in the input space. This creates a significant disconnect, as the convexity assumptions used throughout the proofs no longer hold.
2. Incorrect KL expression: The KL term on line 238 is incorrect. The term assumed to be constant is not, in fact, constant. It equals `-log(C) P(x ~ p_g, p(x) <= epsilon)`, which should also be optimized. The authors need to develop an alternative approach to derive the presented objective. Perhaps simply motivating the minimization of the cross-entropy term and adding a cross-entropy regularization could be more appropriate.
3. Generator behavior and manifold sampling: Based on the analysis and the 2D example with uniform sampling off the manifold, the paper suggests that the generator should provide samples outside the manifold to reinforce the classifier's boundaries by supplying only negative samples. However, the optimal behavior lies somewhere in between: the generator should produce samples that are neither too strong (highly realistic and easily classified as a specific class) nor too weak (easily labeled as fake). Instead, it should provide samples both on the manifold to reinforce positive predictions and off the manifold to reinforce negatives. A more realistic probabilistic setup might assume the assumptions hold with probability `1-delta` off the manifold and `delta` on the manifold. While corollary 1 is not proven and its validity is unclear, balancing this `delta` appears crucial and aligns with the analysis in [https://arxiv.org/pdf/1705.08850.pdf](https://arxiv.org/pdf/1705.08850.pdf).
4. Ablation study: The performance of the method in Section 6 should be reported for a range of epsilon values to better illustrate the discussion above. Additionally, the choice of the 10th quantile should be justified.
5. Ad hoc nature of estimators: The use of density estimation (PixelCNN) and entropy regularizers alongside another estimator feels somewhat ad hoc. It would be interesting to hear whether the authors have considered alternative approaches to encourage low-density sampling, such as purely implicit modeling or hybrid implicit/prescribed modeling, while maintaining end-to-end training.
Minor Comments:
- The loss function combines `Lsupervised` and `Lunsupervised`. Have the authors experimented with balancing these terms using a penalty parameter `lambda`? How does this balancing interact with the threshold `epsilon`?
- Lines 162–163: The statement "This builds a connection with ... [2,25] which leverage 'the low-density boundary assumption'" is unclear. Could the authors elaborate on what this assumption entails, for example, in the context of Laplacian regularization?
- Line 178: Typo – "the a bounded."
- Corollary 1: The proof is missing. It is unclear how the claims hold uniformly over all `X`, and the meaning of `|G| → ∞` is ambiguous. Stronger assumptions may be required here.