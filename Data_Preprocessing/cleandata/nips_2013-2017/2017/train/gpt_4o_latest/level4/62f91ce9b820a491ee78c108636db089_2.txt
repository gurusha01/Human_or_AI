Post-rebuttal comments: I appreciate the authors' feedback and have accordingly increased my overall rating.
Summary: This paper explores an intriguing and promising approach to leveraging deterministic quadrature rules for kernel approximation. However, the primary weakness lies in the experimental evaluation: the comparisons in computation time are insufficient, and there is no direct comparison with QMC-based methods proposed by Yang et al. (ICML2014). Without these, the advantages of the proposed method remain unclear.
- Limitations of the results:
The authors assume that the kernel spectrum is sub-Gaussian, which is valid for widely used Gaussian kernels. However, this assumption excludes other popular kernels, such as Matern kernels, whose spectra decay polynomially. This restricts the applicability of the results presented in the paper.
- Eq. (3):
The notation $e_l$ is unclear. Please clarify.
- Corollaries 1, 2, 3, and Theorem 4:
All these results exhibit an exponential dependence on the domain diameter $M$: the required feature size grows exponentially with $M$. While this dependence does not worsen as the error tolerance $\varepsilon$ decreases, it significantly impacts the constant factor in the required feature size. Figure 1 illustrates that the performance degrades more rapidly than standard random features, which may highlight a limitation of the proposed methods (or the theoretical results).
- Equation in Line 170:
The notation $e_i$ is unclear. Please provide clarification.
- Subsampled dense grid:
This method is employed in the experimental section (Section 5), but it appears to lack theoretical guarantees. The methods with theoretical guarantees seem impractical for real-world applications.
- Reweighted grid quadrature:
(i) This method also appears to lack theoretical guarantees.  
(ii) The approach resembles Bayesian quadrature, which minimizes the worst-case error in the RKHS unit ball to determine weights. A comparison with Bayesian quadrature would be insightful.  
(iii) Could the authors provide an analysis of the time complexity for this method?  
(iv) How is the regularization parameter $\lambda$ chosen in the $\ell_1$ approach? Please elaborate.
- Experiments in Section 5:
(i) The computation time results are reported briefly (e.g., 320 seconds vs. 384 seconds for 28,800 features on MNIST, and "The quadrature-based features ... are about twice as fast to generate, compared to random Fourier features ..." on TIMIT). These results are insufficient. The authors should present a more detailed analysis, such as a table showing computation time for varying numbers of features.  
(ii) A comparison with QMC-based methods from Yang et al. (ICML2014, JMLR2016) is missing. Without this, the advantages of the proposed method remain unclear.  
(iii) The experimental setup for the MNIST and TIMIT classification tasks is inadequately described. What classifiers were used, and how were the hyperparameters determined? At a minimum, these details should be included in the appendix.