Thank you for an engaging and thought-provoking submission.
This paper presents an information geometric (IG) perspective on the f-GAN algorithm. It begins by demonstrating that f-GAN converges in parameter space through the 1-1 correspondence between f-divergence and chi-divergence, supported by a result involving Bregman divergence. The paper then discusses an appropriate implementation strategy for f-GAN. Finally, it provides a factorization result for deep neural network representations and examines the choice of activation functions, which exhibit a 1-1 mapping to the chi (or f) function. While I did not verify every detail in the appendix, the proofs appear correct to me, with the exception of Theorem 8, which I did not have time to review in detail.
This paper is dense and introduces numerous novel results that are likely to be of interest to the machine learning and information theory communities. In particular, I found the exposition of Theorem 4, which elucidates the IG perspective of f-GAN, to be especially impressive. For these reasons, I strongly support the acceptance of this paper.
However, I believe the paper may be challenging to follow for readers unfamiliar with the connections between information theory (IT) and IG—particularly the foundational concepts, such as the mapping of KL divergence for exponential families to Bregman divergence and Fenchel duality. Many deep learning practitioners, who are a key audience for GAN-related research, may fall into this category. To improve clarity and accessibility, I offer the following suggestions:
1. I found Figure 1 in the appendix, which illustrates the connections between IT and IG in the context of f-GAN, to be very helpful. Consider moving this figure to the main text.
2. It might be beneficial to provide a clear explanation early in the paper about why adopting the IG perspective is useful. My understanding is that it allows for leveraging the geometry of parameter space to analyze optimization behavior. While this is briefly mentioned on page 7, I believe it would be more impactful if introduced earlier.
3. The connection between Section 5 and f-GAN is unclear to me. It appears that the section primarily uses the deformed exponential family to describe the distributions representable by a deep neural network, rather than deriving results from f-GAN optimization. While I understand the 1-1 correspondence between v, f, and chi, the paper does not elaborate on how this insight could inform the design of the f-GAN game—for example, by guiding the choice of f-divergence or determining the optimal f-GAN objective for a given activation function in terms of convergence.
4. Regarding Theorem 6, why can the φ_l functions be interpreted as "deep sufficient statistics"? Equation (13) does not seem to align with the form of a deformed exponential family.
5. Consider relocating lines 257–269 to a different section for better flow. Additionally, the utility theory discussion does not appear directly relevant to the IG perspective and could be moved to the appendix to free up space for elaborating on the main results.
6. The experiments seem tangential to the core claims of the paper. For instance, the results in Section 5 could stand alone without requiring the IG perspective of f-GAN, as they primarily pertain to new activation functions. Similarly, the results in Section 4 are not tightly coupled to the IG view. The inclusion of WGAN results, in particular, feels distracting and somewhat out of place, as this paper is primarily focused on f-GANs. I spent some time locating the brief mention of WGAN (lines 227–228), which could be clarified or omitted.
In conclusion, while this paper offers valuable insights and rigorous derivations, its presentation could benefit from improved organization and clarity. As it stands, the material feels somewhat compressed, as though multiple papers were condensed into an 8-page NeurIPS submission. Despite these concerns, I remain supportive of acceptance and encourage revisions to enhance the coherence and accessibility of the paper.