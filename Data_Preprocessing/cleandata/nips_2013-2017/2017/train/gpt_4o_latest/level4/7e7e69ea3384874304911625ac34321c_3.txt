The paper introduces an adversarial autoencoder model, which integrates an autoencoder with a GAN-like training scheme to align the latent variables with a prior distribution. In this model, the decoder is autoregressive, enabling it to leverage both the latent representation and autoregressive connections for data modeling. The division of responsibilities between these components can be influenced by adjustments to the architecture and the choice of prior distribution for the latents.
The experiment illustrated in Figure 2 is excellent, as it effectively highlights the advantages of combining autoregressive and latent variable-based modeling. Additionally, it is intriguing to observe that the model encodes the digit label despite the prior's continuity (Figure 3).
However, I am uncertain about the extent to which the observed decompositions (e.g., global vs. local structure or digit identity vs. details) are driven solely by the choice of prior. This is because other factors, such as the biasing mechanism and the increased depth of the model with a categorical prior (as discussed at the end of Section 2.1), also differ between the two setups. Consequently, the paper seems to attribute the differences in decomposition too readily to the prior change. A more compelling analysis would involve taking the model with the categorical prior (and its associated modifications) and replacing the prior with a Gaussian while keeping everything else unchanged. My intuition suggests that the decomposition would remain largely similar (as in Figure 3(a)), as I believe the architectural changes play a more significant role than the prior choice. Addressing this point in the paper would strengthen the argument.
The experiment in Figure 5 is another strong point, as it clearly demonstrates the "discretizing" effect of the GAN loss on the latent code. The inclusion of the semi-supervised classification experiment is also a valuable addition.
In summary, the model represents a natural progression of adversarial autoencoders and autoregressive models, serving as a thoughtful continuation of prior work like VLAE and PixelVAE, which explored VAEs with autoregressive decoders. The experimental design is well-executed, and the results are compelling. However, as noted, I remain unconvinced that the choice of prior alone is as pivotal as the paper suggests in driving the observed decomposition differences.
Remarks:
- Line 45: The statement "the global latent code no longer has to model all the irrelevant and fine details" may be overly generalized, as its utility is highly task-dependent. For instance, in a texture classification task, such details might be crucial.
- Lines 187â€“188: The adversarial network receives continuous inputs (probabilities) from the softmax layer. What about the decoder? Presumably, it receives discrete inputs, but this should be explicitly clarified for better understanding.
- While scaling up is not the primary focus of the paper, it would be interesting to see an experiment on a larger dataset. My intuition is that this model would scale better than standard PixelCNNs, as it can leverage the latents to encode global structure effectively.