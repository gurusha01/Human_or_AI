This paper introduces a constrained optimization framework aimed at mitigating (ethically/socially sensitive) discrimination in classification tasks. Discrimination in machine learning can arise due to (1) the training data reflecting biases inherent in an unfair world, (2) the algorithm introducing biases even when the training data is unbiased, or (3) the discriminatory use of the model's outputs. The proposed approach falls under the preprocessing category, focusing on addressing unfairness originating from (1) by transforming the data to reduce or eliminate its discriminatory characteristics.
The paper uses the following notation: D denotes one or more sensitive variables (e.g., race or sex) for which discrimination is to be minimized, X represents other predictors, and Y is the binary outcome variable. The fairness literature provides various formal definitions of (un)fairness, two of which are employed here: (I) the predicted outcome \(\hat{Y}\) should exhibit minimal dependence on D, and (II) similar individuals should receive similar predicted outcomes. These two criteria serve as constraints in the optimization problem, while the objective is to minimize a probability distance measure between the joint distributions of the transformed and original data. The paper establishes conditions under which the optimization problem is (quasi)convex and provides a theoretical bound on errors arising from misspecification of the prior data distribution. The proposed method is empirically validated on two datasets and shown to perform competitively with the method of Zemel et al. (2013).
One notable limitation of this work is its assumption that the joint distribution of the data is known. Proposition 2 represents a commendable effort to address this limitation, but subtle fairness-related issues embedded in the joint distribution are only briefly mentioned in the Supplementary Material (Figures 2-4). For instance, what does it mean for individuals with different values of D to be "similar" in other respects? Could there be important dimensions of similarity missing from the available data, such as confounding variables? These questions highlight fundamental challenges with individual fairness (IF). While I may have reservations about (IF), it is an established concept in the literature and not a novel contribution of this paper. However, even accepting (IF), Proposition 2 appears to address only the other two aspects of the optimization problem, leaving the impact of prior distribution misspecification on (IF) unaddressed. Addressing this gap would strengthen the paper. Additionally, a more explicit discussion of the complexities in defining fairness—particularly the relationships between D, X, and Y—would enhance the exposition. Relevant works such as Johnson et al. ("Impartial Predictive Modeling: Ensuring Fairness in Arbitrary Models") and Kusner et al. ("Counterfactual Fairness") could provide useful context.
Another limitation is the extensive calibration required for the tuning parameters. Each constraint has associated tolerances, and the potentially conflicting nature of the two constraints (discrimination vs. distortion) is acknowledged only in the Supplementary Materials, where Figure 1 demonstrates that some constraint tolerances may be infeasible. Including this discussion in the main text and providing conditions (perhaps in the Supplementary Materials) under which the optimization problem is feasible would improve the paper.
Finally, the choice of distortion metric raises subtle issues. The fundamental goal of fairness is implicit in this choice—a limitation of (IF)—and it introduces a potentially high-dimensional calibration challenge. While the metric can be customized for specific applications using domain expertise, this flexibility is both a strength and a limitation. How should this customization be approached to ensure a meaningful definition of (IF)? Are there guidelines, default options, or references to other works that address this? Figure 2 is not particularly informative, as neither the proposed method nor its competitor has been fully tuned. While the comparison is likely fair, readers are left wondering about the best achievable performance of each method and whether one consistently outperforms the other.
Quality
The paper is well-supported by theoretical analysis and empirical experiments, particularly when the Supplementary Materials are included. While the work is comprehensive, the need for extensive tuning and application-specific implementation decisions is both a strength and a limitation. The paper could be improved by discussing guidelines, principles, or references for tuning and by acknowledging that the definition of fairness may vary significantly depending on these choices.
Clarity
The paper is well-organized and clearly written. However, it would benefit from explicitly acknowledging the existence of multiple (often contradictory) fairness definitions and discussing how the two constraints used here can conflict, potentially rendering the optimization problem infeasible for certain parameter settings.
Originality
The paper appropriately references the closely related work of Zemel et al. (2013), explains its distinctions, and provides empirical comparisons. These comparisons could be improved by systematically exploring the performance of both methods under optimal tuning conditions.
Significance
The paper addresses a critical problem of significant importance. Given the popularity of (IF) and the modularity of preprocessing approaches to fairness, this work is likely to be utilized or extended in future research.