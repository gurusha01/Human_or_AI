Paraphrased Review:
Summary:  
The authors introduce a probabilistic transformation of training data as a preprocessing step to mitigate algorithmic discrimination, which refers to biased treatment of individuals based on legally protected attributes such as race or gender.  
The proposed transformation is designed to generate a new data distribution that closely resembles the original while adhering to two key constraints:  
- Distortion control: Ensuring that the representation of each individual is minimally altered.  
- Discrimination control: Ensuring that the outcome distributions, conditioned on protected attributes, are as similar as possible.  
Overall:  
The method appears to be technically sound and conceptually reasonable. However, the experimental results fail to convincingly demonstrate an advantage over existing preprocessing methods (e.g., LFR) and do not include comparisons with post-processing or in-processing approaches. This makes it challenging to assess the practical utility of the proposed method or determine scenarios in which it would be preferable.  
Detailed Comments:  
Related Work / Experimental Comparisons:  
- While the benefits of preprocessing over in-processing and post-processing methods are understandable, it would be valuable to include experimental comparisons with these alternative approaches. Such comparisons would provide practical insights into whether the added complexity of the proposed method is justified.  
Methods:  
- The need to define a distinct distortion metric for each new dataset could pose challenges. How sensitive are the results to the specific choices made when defining these metrics?  
- The distortion constraint is expressed in expectation, which implies that a small subset of individuals might experience significant shifts in their features. This could lead to perceived "unfairness" for individuals who were initially highly qualified for a task (e.g., loan approval) but had their features drastically altered, even if such cases occur infrequently.  
- The discrimination constraint is based on the distribution of labels. While the authors acknowledge the potential need for conditioning to avoid Simpson's paradox, it would be beneficial to discuss alternative fairness definitions, such as calibration (see Definition 1 in "Fair prediction with disparate impact" by Chouldechova). Calibration requires conditioning on the predictive score, which depends on all features. Although the current approach may not align with the calibration definition, explicitly addressing this would strengthen the discussion.  
- The two optimization constraints (distortion and discrimination) appear to be in conflict. In certain scenarios, it seems possible that no feasible solution exists. In such cases, practitioners would need to relax the thresholds and attempt the optimization again, which is not ideal. Could this indicate that some datasets inherently require excessive distortion to achieve fairness?  
- The authors assume that maintaining a small KL-divergence between the original and transformed distributions will preserve classifier utility. Is this assumption provable? While no immediate counterexamples come to mind, it seems plausible that a small KL-divergence could still result in a significant drop in utility, depending on the hypothesis class of the classifiers being used.  
Results:  
- On the COMPAS dataset, LFR appears to outperform the proposed method, achieving a higher AUC with less discrimination. For the Adult dataset, the two methods operate at different points on the trade-off curve, making direct comparisons difficult. It would be helpful to explore a range of parameter settings for LFR to better understand its performance on the Adult dataset. Overall, the experimental results do not provide clear evidence that the proposed method is superior or even comparable to LFR. However, the explicit formulation of the discrimination constraint in the proposed method is a notable advantage.  
- The AUC losses observed with the proposed approach are substantial for both datasets. These losses are significantly larger than, for instance, the performance gap between a log-linear model and a random forest. This raises concerns about the practicality of applying the proposed method in real-world applications.