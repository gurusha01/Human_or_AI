This paper introduces a two-stage decoding approach for Seq2Seq models: initially, an output sequence is generated, followed by a second, refined target sequence that leverages attention over both the source and the initially generated target sequence. The rationale behind this method is that having access to the entire target sequence during the second stage could aid in making more informed decisions regarding word selection and other aspects of generation.
While the authors demonstrate some improvements in BLEU scores using this approach, I find the overall methodology somewhat unconventional. Seq2Seq models have largely replaced traditional SMT approaches, such as phrase-based models (PBSMT), due to their elegant end-to-end framework with a unified global training objective, as opposed to the modular and independently trained components of PBSMT, which were later combined in a log-linear framework.
Essentially, the proposed method involves training a second NMT model to translate from an initial target hypothesis to a refined version, which bears similarities to "statistical post-editing (SPE)." However, this approach is computationally intensive, requiring approximately 50% more training time and doubling the decoding time.
Do both decoder networks employ beam search during inference?
Additionally, I would note that the baseline used in this work is somewhat outdated: RNNSearch was one of the earliest NMT models. Since then, more advanced architectures have been developed, and it would strengthen the paper to demonstrate that the proposed two-stage approach yields improvements when applied to these more modern systems (as you yourself mention deeper encoder/decoder architectures).
As an alternative and simpler baseline, you could consider performing a forward and backward decoding pass (generating the sequence from the last word to the first) and combining the results.