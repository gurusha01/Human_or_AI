This paper introduces a method to learn a parametric function that maps a set of features characterizing a hypothesis to a corresponding significance threshold. Theoretically, if this function is learned effectively, the approach could prioritize certain hypotheses based on their feature representations, thereby enhancing statistical power. The function that defines the per-hypothesis rejection regions is trained to control the False Discovery Proportion (FDP) with high probability, contingent on the satisfaction of certain restrictive assumptions.
Although multiple hypothesis testing is not currently a central topic in machine learning, it remains critical in application areas like computational biology and healthcare, where multiple comparisons are routine. A prominent trend in this field is the development of methods that integrate prior knowledge to adjust the weighting of hypotheses. The method proposed in this paper aligns with this trend.
Despite the relevance of the topic, I have significant reservations about the practical applicability of the proposed approach and other aspects of the paper.
---
Major Points
1. Over-reliance on Unrealistic Assumptions  
   The paper depends heavily on assumptions that are invalid for the motivating applications described and tested in the experiments.  
   - The method assumes that triplets \((P{i}, \mathbf{X}{i}, H{i})\) are i.i.d., which is critical because cross-validation would otherwise be unsuitable for validating decision rules learned on a training set. However, this independence assumption is untenable in bioinformatics. For instance, in genome-wide association studies, variants exhibit statistical dependence due to linkage disequilibrium (LD). Similarly, in RNA-Seq analysis, gene expression levels are correlated through complex co-expression networks, often tissue-specific. Additionally, unobserved confounding variables like population structure, batch effects, or cryptic relatedness can introduce further dependencies. While extensions of the Benjamini-Hochberg (BH) procedure exist to ensure FDR control under general dependence, the proposed method does not account for such dependencies. Without addressing these issues, the method is invalid for the applications tested. Furthermore, the assumption of identical distribution for all triplets \((P{i}, \mathbf{X}{i}, H{i})\) is also difficult to justify in practice.  
   - Another assumption, though less critical, is that the alternative distribution \(f_{1}(p \mid \mathbf{x})\) must be non-increasing. The authors claim this is standard, but this is not universally true (e.g., see [1], where the alternative distribution is modeled as a beta distribution with learnable mean and precision).
2. Limited Role of Neural Networks  
   The paper emphasizes the use of neural networks, as reflected in the title and method name. However, neural networks play a minimal role in the approach, as any parametric function capable of mapping the input feature space to a significance threshold would suffice.  
   - The motivating examples involve low-dimensional data (1D to 5D), suggesting that simpler parametric functions with sufficient capacity could perform equally well. The authors should conduct extensive experiments to compare the performance of different function classes. This would clarify the necessity of neural networks in their approach. If simpler, more interpretable functions achieve comparable results, they may be preferable.
3. Clarity and Exposition  
   While the paper is grammatically sound and easy to read, the quality of the exposition could be improved significantly.  
   - Section 3 lacks detail in describing Algorithm 1. The notion of cross-validation is confusing, as the folds correspond to subsets of features (i.e., hypotheses) rather than samples used to compute P-values. This distinction should be clarified early. Additionally, the variables \(\gamma_{i}\) are introduced without sufficient explanation. The authors should consider adopting the notation from Equation (8) in Supplementary Section 2 for Equation (3) to clarify that the optimization involves a smooth approximation of the true objective function.  
   - The implementation and training procedure described in Supplementary Section 2 lack justification for hyperparameter choices (e.g., network architecture, \(\lambda\) values, optimization scheme) and experimental results on robustness to these hyperparameters. Practitioners in computational biology and statistical genetics, who may not be familiar with deep learning techniques, would face challenges in setting these hyperparameters. Greater detail is needed to make the method accessible to these communities.
---
Minor Points  
- The use of cross-validation introduces randomness into the results, which is undesirable in statistical association testing. The authors should evaluate the stability of their results across different random partitions and investigate whether stratification of features affects the cross-validation process.  
- Decision rules in multiple hypothesis testing depend strongly on the number \(n\) of hypotheses tested. However, the proposed approach does not explicitly account for \(n\), instead relying on cross-validation and test sets with the same number of hypotheses and i.i.d. assumptions. Relaxing this requirement by explicitly incorporating \(n\) would be an interesting extension.  
The paper also contains several incorrect or imprecise claims that, while not critical to the overall contribution, should be addressed:  
- "The P-value is the probability of a hypothesis being observed under the null" -> Incorrect. A P-value is the probability of observing a test statistic as extreme or more extreme under the null hypothesis.  
- "The two most popular quantities that conceptualize the false positives are the false discovery proportion (FDP) and the false discovery rate (FDR)" -> Family-Wise Error Rate (FWER) remains widely used and predates FDP and FDR. While FDR is indeed popular, FDP is less commonly used compared to FWER.  
- Definition 1 is mathematically imprecise, as it does not handle the case \(D(t) = 0\) properly.  
- "Moreover, \(\mathbf{x}\) may be multidimensional, ruling out the possibility of non-parametric modelings, such as spline-based methods or the kernel-based methods, whose number of parameters grows exponentially with the dimensionality" -> Kernel-based methods can address this issue via inducing variables (e.g., [2]). Additionally, many approximators other than neural networks do not require exponentially growing parameters with dimensionality.  
- Lemma 1 does not demonstrate that the "mirroring estimator" bounds the true FD from above, as claimed in Line 155. It only shows that its expectation bounds the expected FD, which is a different statement.  
- The claim in Lines 193-194 that five dimensions constitute a high-dimensional setting is incorrect. High-dimensional settings typically involve thousands of features.  
The paper also contains frequent typos, such as:  
- Line 150: \((t(\mathbf{x}), 1)\) -> \((1-t(\mathbf{x}), 1)\)  
- Line 157: "approaches \(0\) very fast as \(p \rightarrow 0\)" -> "approaches \(0\) very fast as \(p \rightarrow 1\)"  
- Line 202: "Fig. 3 (b,c)" -> "Fig. 4 (b,c)"  
- Lines 253-254: The alternative distribution \(f_{1}\) is missing parentheses.  
- Supplementary Material, proof of Theorem 1 and Lemma 2: Terms involving Chernoff's bound lack parentheses, making the proof harder to read.  
---
References  
[1] Li, Y., & Kellis, M. (2016). Joint Bayesian inference of risk variants and tissue-specific epigenomic enrichments across multiple complex human diseases. Nucleic Acids Research, 44(18), e144-e144.  
[2] Tran, D., Ranganath, R., & Blei, D. M. (2016). The variational Gaussian process. International Conference on Learning Representations (ICLR).