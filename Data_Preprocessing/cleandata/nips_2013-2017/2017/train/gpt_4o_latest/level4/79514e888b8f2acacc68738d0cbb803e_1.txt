This study builds upon and enhances the performance of GAN-based approaches for semi-supervised learning, as previously explored in "Improved Techniques for Training GANs" (Salimans 2016) and "Unsupervised and Semi-Supervised Learning with Categorical Generative Adversarial Networks" (Springenberg 2015).
The paper introduces the concept of a complement generator, which aims to sample from low-density regions of the data distribution (in feature space). It also investigates a range of objective terms that are motivated by or connected to this analysis. While the exact motivated objective is challenging to implement due to the difficulty of estimating quantities such as density and entropy, the paper employs various approximations as substitutes.
Beyond an illustrative case study using synthetic data, the paper includes a comprehensive set of experiments on standardized semi-supervised benchmarks, along with ablation studies for the proposed terms. The empirical results demonstrate a significant improvement over the Feature Matching criterion introduced in "Improved Techniques for Training GANs."
The inclusion of ablation studies is both important and commendable, given the variety of objective terms proposed. However, the studies are not entirely exhaustive. For example, why are there five experiments for SVHN, but only two for MNIST and three for CIFAR-10? Why are the Approximate Entropy Maximization terms evaluated exclusively on SVHN? How do these terms perform on CIFAR-10? It would be helpful if the author(s) could clarify why a more complete set of comparisons was not conducted.
The benefits of the proposed terms are not consistently observed across all datasets. While the text briefly acknowledges and discusses this, a more detailed investigation—particularly through completing the ablation studies—would be valuable for researchers seeking to build upon or refine the ideas presented in this work.