The authors explore several intriguing questions related to GANs, such as the extent to which solving the GAN optimization problem ensures convergence in parameter space, what the generator is actually approximating upon convergence, and—perhaps most pertinent from an architectural perspective—how to select the discriminator's output activation function to guarantee the proper compositeness of the loss function.
To address these questions, the authors adopt an information-theoretic framework based on deformed exponential distributions, deriving several theoretical insights, including the following:
They propose a variational generalization (compatible with the f-GAN framework) of a known theorem that links an f-divergence between distributions to a corresponding Bregman divergence between the parameters of those distributions. This result establishes an interesting connection between the information-theoretic approach of quantifying dissimilarity between distributions and the information-geometric perspective of measuring dissimilarity between their parameters. They further demonstrate that, under a reversibility assumption, deep generative networks decompose into so-called escorts of deformed exponential distributions. Their theoretical findings also suggest that carefully selecting the generator's hidden activation functions and the discriminator's output activation function could improve GAN performance. Additionally, they briefly propose an alternative interpretation of the GAN framework within the context of expected utility theory.
Overall, while the authors present some compelling theoretical results, I have several concerns regarding the practical relevance and applicability of their findings in their current form.
Questions & Concerns:
1. Assumption of Deformed Exponential Distributions:  
   Many of the theorems and insights rely on the assumption that the data and model distributions \( P \) and \( Q \) belong to the deformed exponential family. Can the authors justify this assumption by discussing its validity in practical scenarios? Furthermore, could similar results (e.g., Theorems 4 and 6) be derived without this restrictive assumption?
2. Information-Geometric f-GAN Identity (Eq. 7):  
   One of the key contributions, the information-geometric f-GAN identity, connects the variational f-divergence formulation over distributions to an optimization problem over parameters. What practical advantages does this parameter-based perspective offer? Is it feasible to implement GANs using this parameter-based optimization, and what would the benefits be? Experimental comparisons between the two optimization approaches would have been valuable. Surprisingly, the authors do not seem to utilize the right-hand side of this identity beyond asserting that solving the GAN game implies convergence in parameter space (assuming a small residual \( J(Q) \)). Why is this implication not straightforward? Can the authors provide a realistic scenario where convergence in the variational f-divergence formulation does not imply convergence in parameter space?
3. Output Activation Function and Proper Compositeness:  
   The authors highlight the importance of the discriminator's output activation function \( gf \) (as discussed in Section 2.4 of Ref. [34]) for ensuring the proper compositeness of the GAN loss. This result is particularly intriguing, but I would have appreciated experimental comparisons of their theoretically derived \( gf \) (as the composition of \( f' \) and the link function) against the heuristic choice described in Ref. [34].
4. Reversibility Assumption and Activation Functions:  
   Assuming the generator network is reversible, the authors show that there exists an activation function \( v \) such that the generator's hidden layers decompose as escorts of the deformed exponential family. However, in Table 2 (left), the comparison of generator architectures with different activation functions reveals that the theoretically superior \( \mu \)-ReLU performs worse than the baseline ReLU (a limiting case as \( \mu \to 1 \)). This result is counterintuitive, as the invertible \( \mu \)-ReLU (satisfying the reversibility assumption) should theoretically outperform the non-invertible baseline. Can the authors explain this discrepancy? Additionally, can they comment on whether the DCGAN architecture satisfies the reversibility assumption and how these results compare to the Wasserstein GAN, which is not based on f-divergences and thus falls outside the scope of their theoretical framework? Finally, the results in Table 2 (center and right) comparing different activation and link functions appear to fall within error bars, which raises questions about the practical significance of their theoretical findings.
Minor Concerns:
1. Intuition for Deformed Exponential Densities:  
   Since the concept of deformed exponential densities may be unfamiliar to the broader machine learning community, it would be helpful if the authors provided more intuition about deformations (or signatures) and escorts.
2. Upper Bounds on \( J(Q) \) (Theorem 8):  
   The authors derive several upper bounds for \( J(Q) \). How do these bounds compare, and what are the implications for selecting activation functions? Additionally, they note that the upper bound on \( J(Q) \) decreases with the normalization parameter of the escort \( Z \). Can they elaborate on why this is beneficial and how it can be leveraged in practice?
Conclusion:  
In light of these comments, I find the theoretical analysis interesting but insufficiently impactful from a practical perspective for GAN practitioners. Experimental validation and further exploration of the practical implications of their findings would significantly strengthen the paper.