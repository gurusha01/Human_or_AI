I have reviewed the other evaluations and the authors' rebuttal.
This paper addresses the deterministic construction of a feature map \( z \) for subgaussian kernels, which serves as an alternative to random Fourier transforms. By Bochner's theorem, representing a kernel through its Fourier transform requires the computation of an intractable integral. Traditionally, this integral is approximated using Monte Carlo (MC) integration, but the authors propose leveraging Gaussian quadrature for this purpose. For the specific case of the ANOVA kernel, the Gaussian quadrature-based Fourier transform is demonstrated to yield accurate estimates with a computational complexity of \( O(D) \) instead of \( O(D^3) \). The effectiveness of this approach is supported by experimental results on classification tasks using the MNIST and TIMIT datasets.
The paper is well-written and easy to follow. The introduction to kernels and quadrature is particularly well-crafted and provides a solid foundation for readers. I appreciated that the detailed proofs are included in the appendix; however, I recommend explicitly mentioning this in the main text, as I initially assumed they were omitted.
This work is highly impactful. Replacing high-dimensional Monte Carlo integration with deterministic methods, as demonstrated in this paper, can lead to substantial computational speed-ups when executed appropriately. Naturally, the success of this approach depends on tailoring the integration rule to the specific integrand. The use of the ANOVA kernel is a compelling example, as it captures sparse dependencies and mitigates the "curse of dimensionality," i.e., the uncontrolled variability of functions in unmodeled dimensions. I believe this contribution will be of significant interest to the research community.
Additionally, I would like to draw the authors' attention to an intriguing line of research on probabilistic Gaussian quadrature rules, which they may not be aware of. Sarkka et al. [1] have demonstrated that Gaussian quadrature rules (which are polynomially exact) can be derived as posterior means in Bayesian quadrature with appropriate kernels. This could potentially be integrated with the proposed methods if quantifying numerical uncertainty in the Fourier map computation is relevant.
Finally, I would like to address a minor concern regarding one of the key references in the paperâ€”[18], titled "How to scale up kernel methods to be as good as neural nets." This reference appears to be an arXiv preprint from 2014 and does not seem to have undergone peer review. If this is indeed the case, I would suggest either replacing it with a peer-reviewed source or clarifying its status, especially given its foundational role in motivating this work.
In conclusion, this is an excellent paper with both theoretical and practical significance. I recommend its acceptance.
[1] https://arxiv.org/pdf/1504.05994.pdf