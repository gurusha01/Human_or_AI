Selective classification addresses the dual challenge of determining which data examples to classify and subsequently classifying them. In essence, it equips a classifier with the ability to abstain from making predictions on certain data points when its confidence is low. Prior work has largely concentrated on methods that assign a small cost to abstention. This paper introduces a post-hoc approach that leverages confidence estimates: by thresholding the classifier's confidence, the method ensures a guaranteed error rate with high probability.
The key contribution of this paper is the SGR algorithm and its theoretical framework. The approach assumes access to an ideal confidence function, which is not directly available in practice. To address this, the authors evaluate two practical methods: SR and MC-dropout. The experimental results are encouraging, demonstrating low test error rates alongside reasonably high coverage.
On specific points: it is unclear how Equation (4) is solved. I assume it involves a simple 1D line search, but explicitly clarifying this would be beneficial. Additionally, what is the computational complexity of the overall procedure? My estimation is that it scales as mlog(m), but this should be confirmed.
It is noteworthy that MC-dropout underperformed on Imagenet. Do you have any insights into why this might be the case? Visualizing the differences in confidence functions for a given model could provide useful intuition. In practice, it seems feasible to test both methods and select the one that performs better.
Regarding the datasets, I noticed some ambiguities in the experimental setup. For CIFAR-10 and CIFAR-100, there is no standard validation set; both datasets consist of 50,000 training examples and a separate 10,000-example test set. Did you partition the test set into two halves, or use a subset of the training data for Sm? A more rigorous approach might involve reserving a portion of the training set for validation and evaluating on the full test set. For Imagenet, it appears that you split the validation set into two halves and tested on one half. Why not evaluate on the full test set, which contains approximately 100,000 images? Clarifying these details would strengthen the experimental methodology.
There is a minor typo in Section 5.3 ("mageNet").
One limitation of the empirical results is the lack of comparison with alternative approaches, such as those that assign a small cost to abstention. These methods could be tuned to achieve a desired coverage or error rate. It is not immediately evident that the proposed post-hoc approach is superior to such methods, though it may indeed be computationally more efficient.
Overall, I find this paper to be a compelling contribution. The proposed method is practical, well-motivated, and opens up several promising avenues for future research.