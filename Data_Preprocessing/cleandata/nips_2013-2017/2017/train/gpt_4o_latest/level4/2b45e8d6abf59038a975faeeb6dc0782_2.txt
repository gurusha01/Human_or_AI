This paper introduces the population matching discrepancy (PMD) as a superior alternative to the maximum mean discrepancy (MMD) for distribution matching tasks. The authors demonstrate that PMD can be interpreted as a sampled version of the Wasserstein metric, or earth mover's distance, and highlight several advantages over MMD, including stronger gradients, the ability to use smaller mini-batch sizes, and fewer hyperparameters.
For the specific context of training generative models, MMD is known to suffer from weak gradients and the need for large mini-batches. The approach proposed in this paper offers an effective solution to both of these challenges. The empirical results convincingly validate the claim regarding small mini-batch applicability. However, the evidence supporting the stronger gradients claim is less robust. Since the effectiveness of MMD is influenced by the choice of the scale parameter sigma, it is important to evaluate either the optimal sigma or a range of sigma values when making such a comparison.
Regarding the claim of having fewer hyperparameters, I find this assertion less substantiated. PMD relies on a distance metric, which may introduce additional hyperparameters, similar to MMD. Furthermore, obtaining a reliable distance metric in high-dimensional spaces is inherently challenging, meaning PMD could face the same limitations as MMD in this regard. Additionally, there are well-established heuristics for selecting the bandwidth parameter in MMD, and it would be beneficial to compare PMD against these heuristics while treating MMD as a potentially hyperparameter-free metric.
In summary, the proposed method offers the appealing advantage of enabling small mini-batch sizes, which facilitates faster training. It represents a meaningful improvement over MMD methods that require large batches. However, the reliance on a distance metric may constrain its effectiveness when applied to high-dimensional data, which remains an important limitation to consider.