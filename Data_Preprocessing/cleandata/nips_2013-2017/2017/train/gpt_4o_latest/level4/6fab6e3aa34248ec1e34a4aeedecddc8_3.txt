Review - Summary:
This paper presents a novel approach to fusing modalities for tasks at the intersection of language and vision. Unlike most existing works that fuse modalities after extracting independent representations, the authors propose modulating visual features using text. Specifically, they condition the batch normalization parameters of ResNet, a standard architecture for visual feature extraction, on the text input. This technique, termed 'Conditional Batch Normalization' (CBN), results in a network referred to as ModRNet. The effectiveness of the proposed approach is demonstrated through experiments on two tasks.
Strengths:
(a) The idea of fusing modalities as early as during visual feature extraction is, to the best of my knowledge, novel. The paper provides strong motivation for this approach, drawing parallels with neuroscience findings that highlight how language can influence visual responses in the human brain.
(b) Using batch normalization parameters as a mechanism to fuse modalities is an excellent design choice. It scales only with the number of channels, significantly reducing the number of additional parameters to be learned. Furthermore, freezing the other network parameters underscores the simplicity of the approach.
(c) The paper includes comprehensive ablation studies and a thoughtful comparison between fine-tuning ResNet and using ModRNet. I found the experiments and the accompanying discussions to be highly insightful.
Weaknesses:
(a) The experiments in the paper are restricted to the ResNet architecture for feature extraction. While this is not a strict limitation, it raises questions about the generalizability of CBN to other architectures, such as batch-normalized versions of VGG. Evidence supporting whether this behavior is unique to ResNet, potentially due to its residual connections, would also address this concern.
(b) The manuscript contains several typos and grammatical errors that detract from the readability and clarity of the proposed approach. I strongly recommend the authors carefully proofread the paper to address these issues. A few specific errors are listed below.
Comments:
(a) Lines 13â€“16: The sentences in this section do not flow well. The initial statements suggest that we are capable of modeling such tasks, but the subsequent sentences frame these as long-standing challenges.
(b) Line 185: The oracle task has not been clearly described (i.e., its inputs and outputs). This omission makes it difficult to follow the specifics discussed later.
(c) Line 212: Did the authors retrain their model using the same visual features and image sizes as the current work to ensure a fair comparison?
Typos:
(a) Line 4: "inputs"  
(b) Line 17: "towards"  
(c) Line 42: Missing reference or typo?  
(d) Line 150: LaTeX formatting errors  
(e) Line 190: "ReLU"  
(f) Lines 218, 267: Typos  
Post-Discussion:
The authors have addressed most of the concerns raised during the rebuttal phase. After considering the rebuttal and the comments from other reviewers, I maintain that the paper offers novel contributions and provides valuable insights for future research. However, I remain curious about the generalizability of the proposed approach to other architectures, as highlighted by other reviewers. Demonstrating results on additional networks, such as batch-normalized versions of VGG, would strengthen the paper, even if the findings suggest that the approach does not generalize well to other architectures. While the authors mention the unavailability of pretrained batch-normalized VGG models in TensorFlow, it would be beneficial to explore PyTorch pretrained models in time for the camera-ready submission.