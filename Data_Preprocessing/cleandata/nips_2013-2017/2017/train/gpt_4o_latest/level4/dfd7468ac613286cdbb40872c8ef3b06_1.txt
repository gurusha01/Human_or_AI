This paper introduces MMD-GAN, a novel generative architecture that integrates the strengths of generative adversarial networks (GANs) and generative moment matching networks (GMMNs). The approach leverages the maximum mean discrepancy (MMD) as its foundation, with a kernel function that is adaptively learned by an adversarial mechanism aimed at maximizing MMD. Theoretical analysis is provided to establish non-degeneracy, and empirical results demonstrate that the proposed method produces samples that are competitive with those generated by Wasserstein GANs (W-GANs).
In summary, this is a well-executed paper that presents a straightforward yet impactful idea. The proposed approach is conceptually appealing, with modeling choices that are clearly justified. The writing is concise and accessible, the theoretical contributions are compelling, and the connections to existing methods are well-articulated. The experimental evaluation benchmarks the method against strong baselines on challenging datasets, and the results are impressive. Achieving performance parity between GMMNs and WGANs is particularly noteworthy, as prior work has highlighted a significant performance gap between these approaches.
That said, the paper would benefit from deeper analysis to clarify the source of the observed performance improvements. Specifically, how does MMD-GAN address the need for large mini-batches? (Wouldn't the samples within each mini-batch still need to adequately cover the space to achieve low MMD?) Additionally, what accounts for its superior performance relative to WGANs?
Minor comments:
- Section 3 situates the method in the context of GANs, but wouldn't WGANs provide a more direct comparison? If my understanding is correct, MMD-GAN can be viewed as a kernelized version of WGAN.
- The statement "Also, the gradient has to be bounded, which can be done by clipping phi" seems analogous to the Lipschitz constraint in WGANs. If this is the case, why not adopt the gradient penalty regularization from the "improved WGAN" paper, which has been shown to be more effective than the original gradient clipping approach?
- If the claim is that the proposed method mitigates mode collapse, estimating log-likelihoods might provide a more robust quantitative evaluation. For instance, see Wu et al., 2017.
- The experiment in Section 5.3 does not appear to directly assess stability. (Does this refer to avoiding the degenerate solutions that standard GANs are prone to?)