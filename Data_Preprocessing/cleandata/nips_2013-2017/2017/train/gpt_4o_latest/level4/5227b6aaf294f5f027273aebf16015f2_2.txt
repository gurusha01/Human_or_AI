This paper introduces SphereNet, a novel architecture that substitutes the conventional dot product with geodesic distance in convolutional operations and fully-connected layers. Additionally, SphereNet enforces weight regularization for angular softmax by constraining the weights to have a norm of 1. The experimental results demonstrate that SphereNet outperforms traditional methods in terms of accuracy, convergence rate, and addressing the vanishing/exploding gradient problem in deep networks.
Novelty: While the use of angular similarity instead of dot product similarity has been explored extensively in the deep learning literature, most prior work focuses on applying angular similarity to softmax or loss functions. This paper, however, introduces spherical operations for convolution, which, to the best of my knowledge, is a novel contribution to the field.
Significance: The spherical operations proposed in this paper yield faster convergence rates and improved accuracy. Furthermore, they address the long-standing issue of vanishing/exploding gradients caused by dot products. These contributions are likely to attract significant interest from the machine learning community.
Suggestions for Improvement: I have several comments and questions that the authors may wish to address:
- For angular softmax, the bias term is omitted. However, bias plays a critical role in calibrating outputs, particularly for imbalanced datasets. The authors are encouraged to either reintroduce the bias term or evaluate the current model on an imbalanced dataset to assess its impact.
- In the first two subfigures of Figure 4, baseline methods like standard CNNs initially converge faster but experience a significant accuracy drop midway. The paper does not provide an explanation for this phenomenon. Clarifying the root cause would strengthen the discussion.
- Although the experimental results demonstrate faster convergence in terms of accuracy versus iterations, it remains unclear whether this trend holds for accuracy versus time. Traditional dot product operations benefit from optimized matrix multiplication libraries, whereas angular operators may lack similar computational support.
Minor Grammar Issues:
- Line 216: "this problem will automatically solved" → "this problem will automatically be solved"
- Line 319: "The task can also viewed as a ..." → "The task can also be viewed as a ..."