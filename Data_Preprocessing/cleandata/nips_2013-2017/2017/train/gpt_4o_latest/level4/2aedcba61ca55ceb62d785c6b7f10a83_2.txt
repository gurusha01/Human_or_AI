Review:
Note: Since the supplement appears to include the main paper, I have reviewed that, and all line numbers below refer to the supplement.
Summary:  
This paper investigates group-additive nonparametric regression models, where the regression function is additive across groups of predictor variables based on a certain partitioning. This framework bridges the gap between fully nonparametric models, which are challenging to fit, and additive models, which can sometimes be overly restrictive. The focus of the study is on scenarios where the group structure is not predefined and must be inferred from the data. To address this, the authors introduce a novel penalty function derived from the covering numbers of RKHS balls, which is incorporated into the kernel ridge regression objective. This leads to an optimization problem that is solved over both the group structure (which, along with the kernel, defines a function space via the direct sum of RKHSs for each group of variables) and the regression function within each group. The paper presents two algorithms for approximately solving this joint optimization problem. Theoretical results are provided, demonstrating (a) the convergence rate of the empirical risk to the true risk of the optimal solution, and (b) the consistency of the group structure estimation, where the probability of correctly identifying the true group structure approaches 1 as the sample size grows to infinity. Finally, the paper includes experimental results on both synthetic and real-world datasets.
Main Comments:  
The primary contribution of the paper seems to lie in the insight that the complexity of a given group structure can be effectively quantified using the covering numbers of the direct sum space. The paper is thorough, presenting a well-motivated and novel approach, along with reasonably robust theoretical and empirical results. While I am not deeply familiar with alternative methods for modeling between additive and nonparametric approaches, assuming the discussion in Lines 31â€“50 accurately represents the state of the field, this work appears to be a meaningful advancement. As acknowledged in the Discussion section, the main limitation of the proposed method is the computational challenge of solving the optimization problem over group structures when the number of variables is large. Overall, the paper is relatively well-written, though there are numerous typos that could be addressed.
Minor Comments/Questions:  
- Out of curiosity: Is there a simple way to characterize "interaction" between two variables that does not require expressing the entire model as in Equation (1)?  
- Line 169: I am unclear about the use of "almost surely" in this context. Is this intended to apply as \( n \to \infty \)?  
- Equation (3): A summation (over \( i \)) appears to be missing.  
- Line 205: Would "translation-invariant kernel" be a more commonly used term than "convolutional kernel"?  
Typos:  
- Line 145: "on RHS" should be "on the RHS."  
- Line 152: "not only can the bias of \(\hat{f}{\lambda,G}\) reduces" should be "not only can the bias of \(\hat{f}{\lambda,G}\) reduce."  
- Line 160: "G\(^\) exists and unique" should be "G\(^\) exists and is unique."  
- Line 247: "turning parameters" should be "tuning parameters."  
- Algorithm 2, Line 1: "State with" should be "Start with."  
- Line 251: "by compare" should be "by comparing."