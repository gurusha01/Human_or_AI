1. Summary of Paper
This paper presents a method for training a gradient-boosted regression tree ensemble that accounts for both feature costs and the costs associated with evaluating splits within the tree. As such, the work is related to that of Xu et al., 2012. The key distinctions lie in the input-specific nature of feature and evaluation costs, the dependency of evaluation cost on the number of tree splits, the use of a different optimization approach (leveraging the Taylor expansion around \( T_{k-1} \), as outlined in the XGBoost paper), and the adoption of best-first tree growth constrained by a maximum number of splits (instead of a maximum depth). The authors demonstrate that their approach is effective in scenarios where either feature costs or evaluation costs dominate and provide experimental results to support these claims.
---
2. High-Level Subjective
The paper is well-written and, despite introducing a substantial amount of notation, is generally easy to follow.
However, the contribution of the paper feels somewhat incremental in light of prior work, particularly XGBoost and GreedyMiser.
The experimental evaluation appears adequate, with comparisons to prior methods and analyses of how model parameters influence performance. Nonetheless, I have one point of confusion that may necessitate an additional experiment, as detailed below.
This work could be of interest to practitioners working on classification problems in time-sensitive settings.
---
3. High-Level Technical
A key point of ambiguity lies in Figures 2a and 2b: how was the cost computed for each method to generate the Precision vs. Cost curves? It seems that for CEGB, cost is calculated per-input and per-split, whereas for the other methods, cost is calculated per-tree. If this is indeed the case, the comparison appears unfair. To ensure a fair evaluation, the cost for all methods should be measured in the same way, even if the original papers used different definitions. Without this clarification, it remains unclear how much better CEGB truly is compared to the other methods. If the costs are indeed measured differently, I strongly recommend revising Figure 2 to reflect a consistent cost metric across all methods.
I also believe the paper would benefit from removing Figure 1. The concepts of breadth-first, depth-first, and best-first tree growth are fairly intuitive and do not require a dedicated figure. Instead, I suggest replacing it with a figure that illustrates the trees produced by CEGB, GreedyMiser, and BudgetPrune on the Yahoo! Learning to Rank dataset, highlighting the features and associated costs at each split. This would provide readers with deeper insights into why CEGB outperforms the other methods.
Additionally, it would be helpful to more explicitly outline the improvements over GreedyMiser that lead to CEGB. For instance, rewriting GreedyMiser in the same notation as CEGB and systematically showing the innovations (e.g., best-first growth, removal of depth constraints, alternative optimization procedure) would clarify the contributions. A figure illustrating how each innovation impacts the Precision vs. Cost curve would further strengthen the paper.
Finally, in Section 5.2, it is not entirely clear why GreedyMiser or BudgetPrune cannot be applied in the described setting. Why is the cost of feature responses treated as an evaluation cost rather than a feature cost?
---
4. Low-Level Technical
- In Equation 7, why does \( \lambda \) only appear in the first cost term and not the second?
- In Equation 12, should the denominators of the first three terms include a term with \( \lambda \), similar to the XGBoost formulation?
- What do "levels" refer to in Figure 4b?
- Line 205: Consider redefining \( \beta_m \) here, as readers may have forgotten its earlier definition. Similarly, redefine \( \alpha \) on Line 213 for clarity.
---
5. Summary of Review
This paper proposes a novel approach for cost-efficient learning that (a) outperforms state-of-the-art methods and (b) operates effectively in scenarios where prior methods are unsuitable. However, it is unclear whether these results are due to (a) differences in how costs are measured and (b) the classification of certain costs as evaluation costs rather than feature costs. Until these ambiguities are addressed, I believe the paper falls slightly below the acceptance threshold for NIPS.