The paper is well-written and rigorous, effectively demonstrating how (a certain type of) functional spaces realized by DCNNs can be characterized using multilayer kernels. It also highlights the appealing properties of the architecture: (near) invariance to group transformations and stability under diffeomorphic transformations. I recommend acceptance, subject to addressing the following comments:
* To the best of the reviewer's understanding, the primary contribution of the paper lies in recasting the architecture of DCNNs within the framework of RKHS. Please include in the text some discussion on why this reformulation is or could be practically beneficial (e.g., could the kernel formulation provide insights into generalization bounds? Could it offer computational advantages through standard kernel-based techniques or approximations?). More broadly, does the novelty of the approach extend beyond the reformulation of DCNNs in the RKHS context? Additionally, please clarify how this work differs from the contributions of the paper "Convolutional Kernel Networks" by Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid.
* The authors emphasize the importance of "not losing the signal information." Could you elaborate on why this is critical? DCNNs are typically employed for specific tasks (e.g., image classification), where preserving only task-relevant information is necessary unless the objective is signal reconstruction. Please provide commentary on this point.
* Regarding group invariance: is there any relationship between the approach presented here and the invariant kernel methods proposed by Hans Burkhardt (invariant kernels)?
* As a point of curiosity: you define patch, pooling, and kernel maps that commute with group actions. Would any other operator (from the set of operators that commute with all group elements) also ensure the equivariance property of DCNNs with respect to group transformations?