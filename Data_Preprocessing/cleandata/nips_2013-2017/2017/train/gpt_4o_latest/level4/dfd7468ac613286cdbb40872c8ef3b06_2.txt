This paper introduces a method to enhance the performance of the generative moment matching network (GMMN) by learning the kernel of the maximum mean discrepancy (MMD). The optimization process results in a min-max game akin to that of generative adversarial networks (GANs): the adversarial kernel is trained to maximize the MMD distance between the model distribution and the data distribution, while the generator is simultaneously trained to minimize this distance by "fooling" the MMD. The kernel of the MMD must be characteristic and is constructed using Gaussian kernels combined with an injective function, which is learned through an autoencoder.
I have a few questions for the authors:
1. In Algorithm 1, the decoder of the autoencoder \( f_{\text{dec}} \) does not appear to be explicitly trained. Is it simply the transpose of the encoder? Could you clarify the architecture of the decoder and specify the type of up-sampling it employs?  
2. The encoder of the autoencoder \( f\phi \) seems to be the same as the discriminator in DCGAN. Since the DCGAN discriminator includes max-pooling layers, how does \( f\phi \) maintain injectivity? Additionally, how is the reconstruction quality of the autoencoder affected, given that the latent space is limited to a vector of approximately 100 dimensions and that max-pooling is used?  
3. Is training the autoencoder truly essential for this approach to function? This algorithm bears a strong resemblance to W-GAN, which does not require an autoencoder. While I understand that the autoencoder training is necessary to enforce injectivity, is it indispensable for the overall performance?
The final algorithm can be interpreted as generative moment matching in the latent space of an autoencoder, where the features of the autoencoder are also optimized to maximize the MMD distance. The approach is conceptually similar to WGAN, but instead of matching only the means, it matches higher-order moments via the kernel trick. While I do not anticipate this algorithm to significantly outperform other GAN variants, I find the intuition behind the method and its connections to both GMMN and W-GAN to be intriguing.