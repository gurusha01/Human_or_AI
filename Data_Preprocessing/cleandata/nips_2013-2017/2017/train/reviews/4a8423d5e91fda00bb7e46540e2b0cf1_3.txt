The paper addresses the problem of constructing a classifier with the reject
option that has a desired classification risk and, at the same time, minimizes the
probability the "reject option". The authors consider the case when the
classifiers and an associate confidence function are both known and the task is
to determine a threshold on the confidence that determines whether the
classifier prediction is used or rejected. The authors propose an algorithm
finding the threshold and they provide a statistical guarantees for the method.
Comments:
- The authors should provide an exact definition of the task that they attempt
to solve by their algorithm. The definition on line 86-88 describes rather the
ultimate goal while the algorithm proposed in the paper solves a simpler
problem: given $(f,\kappa)$ find a threshold $\theta$ defining $g$ in equation
(3) such that (2) holds and the coverage is maximal.
- It seems that for a certain values of the input arguments (\delta,r^*,S_m,...)
the Algorithm 1 will always return a trivial solution. By trivial solution I
mean that the condition on line 10 of the Algorithm 1 is never satisfied and
thus all examples will be at the end in the "reject region". It seems to me that
for $\hat{r}=0$ (zero trn error) the bound B^* solving equation (4) can be
determined analytically as 
 $B^* = 1-(\delta/log_2(m))^{1/m}$. 
Hence, if we set the desired risk $r^$ less than the number $B^ =
1-(\delta/log_2(m))^{1/m}$ then the Algorithm 1 will always return a trivial
solution. For example, if we set the confidence $\delta=0.001$ (as in the
experiments) and the number of training examples is $m=500$ then the minimal
bound is $B^=0.0180$ (1.8%). In turn, setting the desired risk $r^ < 0.018$
will always produce a trivial solution whatever data are used. I think this
issue needs to be clarified by the authors.
- The experiments should contain a comparison to a simple baseline that anyone
would try as the first place. Namely, one can find the threshold directly using
the empirical risk $\hat{r}_i$ instead of the sophisticated bound B^*. One would
assume that the danger of over-fitting is low (especially for 5000 examples used
in experiments) taking into account the simple hypothesis space (i.e. "threshold
rules"). Without the comparing to baseline it is hard to judge the practical
benefits of the proposed method.
 
- I'm missing a discussion of the difficulties connected to solving the
numerical problem (4). E.g. which numerical method is suitable and whether there
are numerical issues when evaluating the combinatorial coefficient for large m
and j. 
Typos:
- line 80: (f,g)
- line 116: B^*(\hat{r},\delta,S_m) 
- line 221: "mageNet"