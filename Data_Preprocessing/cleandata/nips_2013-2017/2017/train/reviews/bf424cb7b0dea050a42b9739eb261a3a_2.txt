The paper presents an analysis of gradient descent as learning strategy for kernel methods. The main result of the paper shows that there is a restriction on the functions that can be effectively approximated by gradient descent. In particular, the paper defines the concept of 'computational reach' of gradient descent and shows that for smooth kernels the 'computational reach' includes only a small fraction of the function space. This limitation is overcame by a new method called "EigenPro" which uses a precondition strategy. The method is efficiently implemented and evaluated over standard datasets. The results show systematic improvements by the proposed algorithm, however the statistical significance of these differences is not evaluated. 
In general, the papers is well written, the ideas are clearly presented, and the experimental setup, as well as the results are convincing. Even though I didn't check the details of the mathematical claims, they seem to be sound. Overall, I think the work contributes important insights on how large-scale kernel learning works and how it could be improved.