The authors propose a novel method to capture the complex long-time dependence over the count vectors by exploiting recurrent neural networks to represent implicit distributions, which is realized by the gamma distribution. The shape parameters of those gamma distributions, capturing the time dependence, are approximated by neural networks, which try to cover the information from prior (from higher layer) and likelihood (from low layer). The parameters in neural network are updated based on the expectation of latent variables, which is helpful to reduce the variance caused by sampling and increase the computational efficiency. The model provides an interesting and smart way to handle the long dependency problem in the dynamic model. I like it, but this manuscript may be written a little rush and I I have some comments as below:
1. Some critical typos, i.e. the equation 10.
2. The authors consider the scale parameter as fixed and only update the shape parameter in those gamma distributions, which need some detailed discussion and analysis.
3. In the experiments, the authors only show the performance of the proposed model with two layers and do not compare it with the model with the single layer and more layers, which is not enough to understand the influence of the layers on the performance. Some other parameters also need discussing, i.e. the size of the window, the number of factors.
4. For better understanding, I suggest the authors display the detailed graphical illustrations of the whole model. According to the problem, the author can refer to the figure 1 in [1].
5. Recently some new deep Poisson factor analysis models have been proposed, such as [2] and [3], which need to discuss in Introduction.
 
[1] Chung J, Kastner K, Dinh L, et al. A Recurrent Latent Variable Model for Sequential Data. NIPS, 2015.
[2] Mingyuan Zhou, Yulai Cong, and Bo Chen, Augmentable gamma belief networks, Journal of Machine Learning Research,17(163), 1-44, 2016.
[3] Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou, Deep latent Dirichlet allocation with topic-layer-adaptive stochastic gradient Riemannian MCMC, ICML 2017.