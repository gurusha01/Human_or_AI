The paper tackles the problem of variance reduction in the setting in which each example may have been randomly perturbed (e.g. for images, by small rotations, scalings, addition of noise, etc). The proposed algorithm is is based on the MISO/Finito algorithms, with the surrogate function associated with each (perturbed) example being a moving average of quadratic lower bounds, although, due to the permutations, these lower bounds are only approximate (the lower bound the particular current instance of the permutation on the current example, instead of the expectation over all permutations). They prove that the convergence rate of their algorithm, in contrast with SGD, depends only on the portion of the variance due to the permutations (\sigma_{\rho}) at the optimum, rather than the overall variance (which also includes a portion due to sampling of the examples). Finally, they close with a convincing set of experiments.
While the algorithm itself seems a bit incremental, the setting is an important one, the algorithm is intuitive, and the results are complete, in the sense that both a good theoretical and experimental validation are included.
One complaint I have involves the plots: the font sizes on the legends are very small, the yellow curve is too bright (not enough contrast with the white background), and the curves are differentiated by color alone, instead of by symbols or dashing, which makes them hard (or impossible) to read in black and white.