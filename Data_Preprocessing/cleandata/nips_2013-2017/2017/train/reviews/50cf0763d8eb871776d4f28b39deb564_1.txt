This paper compares the convergence rates of M estimators for differential parameter estimators. Two classes of models are considered: generative and discriminative ones. Generative models estimate (canonical) parameters of two distinct sets of samples, and compute the difference. Discriminative models directly estimate the difference based on Bayes rule and logistic regression. Convergence rates are established by introducing a new concept of local separability: a measure of how much, well, and wide a loss function behaves as separable. Results cover low and high dimensional settings, the latter being treated with l1-sparsity regularization. From my understanding, the generative method is in general better in both theory and simulations. The paper is well written, well organized and theory look sound. Nevertheless, as I am not expert in this field, I cannot judge about the originality and implications of these results. I just have the following remarks:
- line 75: xi^{(0} -> xi^{(0)}
- line 94, eq (5): why not writing mu instead of theta? To be precise it is the negative average log-likelihood (up to constants). Same for eq (6).
- line 95, eq (6): trace(Theta, hatSigma) -> trace(Theta hatSigma) (no comma)
- line 98 to 104: should theta2^ be theta0^?
- line 102, eq (8): why (8) does not depend on c^*?
- line 103: phi(t) should be Phi(t) or in bold.
- line 158: Missing a period after "Irrepresentability".
- line 208, Lemma 3: I could not find the definition of d^Theta and kappaSigma^.
- line 250: Missing a dash between "soft" and "Thresholding".
- line 259: Missing a period before "For any classifier".
- line 289: Missing a period before "In this setting".