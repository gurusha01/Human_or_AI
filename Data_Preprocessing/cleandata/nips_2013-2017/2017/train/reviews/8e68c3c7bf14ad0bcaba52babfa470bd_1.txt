The paper presents a learning framework of incorporating human feedback in RL and applies the method to image captioning. specifically, the authors collect human feedback on machine's caption output, including scoring (from perfect to major error) and correction (if not perfect or acceptable). Then authors train a feedback network to simulate human's feedback assignment on the correctness of a phrase. Later, the authors use reinforcement learning to train the captioning model, with rewards computed from both auto-metrics (like weighted BLEU) and simulated human feedback based on the feedback network. The authors conducted experiments on COCO to evaluate the effectiveness of the model. 
Though a interesting task, the paper has room to improve, e.g.,
1.	The novelty of the paper is not very clear. The setting of including human in the loop for RL training is interesting. However, the actual implementation is not an online human-in-the-loop setting, instead it is a two-stage batch mode of human labeling. Therefore, the authors need to design a simulator (e.g., the feedback network) to estimate human feedback in RL training, which is quite common in many RL settings. BTW, as presented in the related work, using RL to optimize non-differentiable metric is not new. 
2.	The authors report major results on COCO in table 6. However, the performance of the baselines are far from state-of-the-art (e.g., in BLEU-4 score, usually the state of the art results are around 30%, while in table 6 the baseline result is at 20%), and the improvement from using feedback over baseline is less than 0.5% in BLEU-4, which usually is not statistically significant. 
3.	There is lack of detailed analysis and examples showing how feedback helps in the RL framework. e.g., given RLB(RL baseline) and RLF (w/ feedback) give similar scores, are they making similar error? Or w/feedback, the model predict very differently?