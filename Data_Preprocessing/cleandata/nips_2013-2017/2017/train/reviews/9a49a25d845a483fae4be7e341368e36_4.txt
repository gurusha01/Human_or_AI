Summary: 
The authors propose to use a probabilistic transformation of training data as preprocessing to avoid algorithmic discrimination (prejuducial treatment of an individual based on legally protected attributes such as race, gender, etc.) by machine-learned algorithms.
The transformation is optimized to produce a new distribution as close as possible to the old distribution while respecting two constraints:
-Distortion control: Each individual's representation should not change too much
-Discrimination control: The distributions over outcomes, conditioned on protected attributes should be as similar to each other as possible.
Overall: The method looks reasonable and technically sound. However, the experiments do not clearly demonstrate an advantage over existing preprocessing work (LFR), nor do they compare to other post-processing or in-processing procedures, so it is difficult to draw a practical conclusion of whether/when it would be useful to use this new method.
Detailed comments:
Related work / experimental comparisons:
- While I understand the appeal of preprocessing over in-processing and post-processing methods, I would still be interested in seeing experimental results that explicitly compare to these other methods as it would be helpful to practically decide whether it is worth investing in more complex procedures.
Methods:
- Defining a different metric for distortion for every new dataset sounds like it could be difficult. What is the sensitivity of the results to the choices made in determining these measures?
- The distortion constraint is currently formulated in expectation, which means that a small number of individuals may experience extreme shifts in features. That sounds like it could be "unfair" to particular individuals who were initially extremely suitable for e.g., a loan application, and then had their features shifted drastically (although it happens with low probability).
- Discrimination constraint is formulated in terms of distribution of the labels. The authors recognize that conditioning may be necessary to avoid Simpson's paradox. It would also be worth discussing other notions of fairness - e.g., calibration (see Definition 1 in "Fair prediction with disparate impact" by Chouldechova) which requires conditioning on the predictive score which is a function of all of the features. While the current work may not be compatible with the calibration definition of fairness, I think this would be worth discussing explicitly.
- The two constraints in the optimization problem (distortion & discrimination) are at odds with each other. It seems like there may be settings where there is no feasible solution. At that point, the practitioner would then need to either relax their thresholds and try again. This sounds non-ideal. Does this say something about the dataset if it requires too much distortion to become fair?
- The authors assume that a small KL-divergence between two the original distribution and the target distribution will maintain the utility of the classifier. Is this provable? Offhand I don't have any obvious counter-examples, but it seems that it may be possible to have a small KL-divergence but a big change in utility between the original and transformed datasets. It would seem that this would also depend on the hypothesis class from which classifiers are being chosen?
Results
- LFR looks like it dominates the proposed approach on the COMPAS dataset (higher AUC with less discrimination). On the Adult dataset it operates a different operating characteristic so it is hard to compare. It would be useful to see a range of parameters being used in LFR to see the range of its performance on the Adult dataset. Overall, the experimental results do not give a clear picture that the proposed result is better or even comparable to LFR. The proposed method does give an explicit way of setting the discrimination constraint - which is a plus.
- The losses in AUC due to the proposed approach seem significant on both data sets (much bigger than e.g., the difference between a log-linear model and random forest) which makes me question the practicality of applying this method in real applications.