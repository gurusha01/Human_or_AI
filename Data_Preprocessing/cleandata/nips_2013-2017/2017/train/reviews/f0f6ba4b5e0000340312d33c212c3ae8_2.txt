This paper uses hashed embeddings to reduce the memory footprint of the embedding parameters. Their approach is simple, where they learn a vector of importance parameters, one for each component vectors. Both the trainable matrices: the matrix of importance parameters and tThe embedding matrix of component vectors is much smaller than a regular embedding matrix. 
To obtain an embedding for a word, they first hash the token id into row of the importance parameters and then hash each component of the importance parameters into the component vector embedding. 
Their results show that just using bow models, their ensemble of hash embeddings do better than previous bow models on classification tasks. This is a simple yet effective approach to decrease the number of parameters in the model and can also serve as a regularizer. 
The authors should point out the compute time required for inference and if hashing is more expensive than an embedding gather on the GPU.