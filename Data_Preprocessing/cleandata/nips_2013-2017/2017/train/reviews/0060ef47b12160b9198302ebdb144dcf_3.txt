This paper image saliency mask generation approach that can process a hundred 224x224 images per second on a standard GPU. Their approach trains a masking model that finds the tightest rectangular crop that contains the entire salient region of a particular requested class by a black box classifier, such as Alexnet, GoogleNet, and ResNet. Their model architecture requires image feature map, such as those by ResNet-50, over different scales. The final scale feature will be passed through a feature filter that performs the initial localisation, while the following upsampling blocks fine-tune the produced masks. Experiment shows that their method outperforms other weakly supervised techniques at the ImageNet localisation task. 
This paper appears to have sufficient references and related works. Do not completely check. 
This paper appears to be technically correct. Do not completely check.
This paper present a number of intuition and discussion on how they design their approach. 
This paper's presentation is good.
Overall, this paper presents interesting technical results that I am a little concerned about the real time speed claim and applications to real world images.
Comments:
- Does the processing time for 100 images per second include the image resizing operation? If so, what is the running time for other larger images, such as 640 X 480 images taken from iPhone 6s?
- Salient objects in this paper is quite large, what if the requested class object is small in the images? Will 224x224 image be enough?
- In Table 3, is there any corresponding metrics for other works in Table 2, such as Feed [2]?
MISC
- LN 273: "More over, because our model" -> "Moreover, because our model"
- LN 151: "difference that [3] only optimise the mask" -> "difference that [3] only optimises the mask"