As is well known, the leave-one-out CV (LOOCV) error can be efficiently computed for ridge regression, thanks to efficient formulas for adjusting the loss function value when one sample point is removed. The authors propose an approximate formula for LOOCV for other models by generalizing this idea.
I haven't seen this proposed before. Looks like the idea has plenty of potential.
The paper is really well written and polished, and appears technically solid.
I could't find any typos or other minor details to fix, except:
- references: please use capital initials in "Bayesian" and for all words in book and journal titles