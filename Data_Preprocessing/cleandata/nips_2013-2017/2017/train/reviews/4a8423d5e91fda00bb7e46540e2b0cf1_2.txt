Selective classification is the problem of simultaneously choosing which data examples to classify, and subsequently classifying them. Put another way, it's about giving a classifier the ability to ignore certain data if it's not confident in its prediction. Previous approaches have focused on assigning a small cost for abstaining. This paper proposes a post-hoc strategy where, if a classifier's confidence can be accurately gauged, then this confidence is thresholded such that the classifier obtains a guaranteed error rate with high probability.
The main novelty with this paper is the proposed SGR algorithm and associated theory. This relies on an ideal confidence function, which is not available in practice, so two methods, SR and MC-dropout are tested. The results are promising, obtaining a low test error with a reasonably high coverage.
Getting into specifics: it's not obvious how you solve Equation (4). I'm assuming it's a simple line search in 1D, but it would be helpful to be explicit about this. Also, what is the complexity of this whole procedure? It looks like it's mlog(m)?
It's interesting that mc-dropout performed worse on Imagenet, do you have any intuition as to why this might be the case? It may be helpful to visualize how the confidence functions differ for a given model. I suppose one can easily test both and take the one that works better in practice.
As far as I know, there is no explicit validation set for CIFAR-10 and CIFAR-100. They each have 50,000 training points with a separate 10,000-point test-set. Did you split up the test-set into 5,000 points? Or did you use the last batch of the training set for Sm? I think a more proper way to evaluate this would be to use some portion of the last batch of the training sets as validation, and evaluate on the full test set. It would be helpful for you to mention what you did for Imagenet as well; it looks like you split the validation set up into two halves and tested on one half? Why not use the full test set, which I think has 100,000 images?
There's a typo in section 5.3 (mageNet).
One point of weakness in the empirical results is that you do not compare with any other approaches, such as those based on assigning a small cost for abstaining. This cost could be tuned to get a desired coverage, or error rate. It's not clear that a post-hoc approach is obviously better than this approach, although perhaps it is less expensive overall.
Overall I like this paper, I think it's a nice idea that is quite practical, and opens a number of interesting directions for future research.