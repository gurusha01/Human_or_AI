This paper presents and tests two variants of a prior on the VAE latent space for image captioning. Specifically they both employ a Gaussian mixture model prior on the latent apace, and then a better performing variant whereby the clusters are encourage to represent objects in a given image --- i.e. any given image will be generated via a linear combination of samples from clusters corresponding to the objects it contains.
 Detailed comments:
 * The paper is well written and appears to be well tested against previous baselines. While the idea is straightforward, and perhaps not a standalone contribution by itself the testing appears to be reasonably conclusive, and would appear to yield state of the art results (when checked against the online MSCOCO baseline dataset). I am curious as to why the CGAN was not also benchmarked, however (there are now techniques for sentence generating GAN models via e.g. the Gumbel-Softmax trick)?
 * It would be interesting to combine the GMM latent space model with a DPP prior over the cluster centers, to further encourage diversity in representation, a hierarchical latent space might also be an interesting direction to explore, although I am strongly of the opinion that the performance of these models (particularly on small datasets) is entirely dependent on how close the implicit prior encoded by the choice of mapping and latent structure is to the true data generating distribution, in which case it may be that the AG-CVAE is close to optimal.