This paper presents an RNN architecture that combines the advantage of stacked multiscale RNN for storing long-term dependencies with deep transition RNN for complex dynamics that allow quick adaptation to changes in the inputs. The architecture consists of typically four fast RNN cells (the paper uses LSTMs) for the lower deep transition layer and of one slow RNN upper cell that receives from faster cell 1 and updates the state of faster cell 2.
The model is evaluated on PTB and enwiki8, where it achieves the lowest character-based perplexity when compared to similar-sized (parameters or number of cells) architectures.
The analysis of vanishing gradients and of cell change is insightful.
One small note: should fig 1 say h{t-1}^{Fk} ?