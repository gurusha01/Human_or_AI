The paper considers kernel regression in the high dimensional setting. The starting point of the paper is the "additive model", which consider regressors on the form
 f = sump fp(X_p)
i.e. correlations between different dimensions are ignored. Obviously it is problematic to ignore such correlations so the authors instead consider the "group additive model", where different dimensions are grouped, such that correlations between some dimensions can be modeled. The authors provide a formal treatment of this setting, and provide some (somewhat) ad hoc algorithms for finding optimal groupings. Results are presented for small synthetic and real-world cases.
For me, the key issue with the paper is that it does a rather poor job of describing related work, so it is difficult for me (as a non-expert) to determine the true novelty of the work. The task of finding optimal groups in data is e.g. also considered in "structured sparsity" and when learning graphical models/Bayesian networks. Yet, the authors make no mention of these fields. Likewise, the problem can be seen as an instance of "feature selection", but again this is not mentioned. Effectively, the related work described in the paper boils down to a single KDD-2015 paper [8], which is not enough for me to determine the actual novelty of paper.
That being said, the theoretical treatment does strike me as novel. The authors end up defining a simple complexity measure on a grouping (this measure favors many small groups). This is derived as an upper bound on the covering number. This seems sensible enough. The downside, is that this measure does not lend itself to easy optimization, so the authors either propose an exhaustive search (which scales poorly) or a greedy method (akin to forward feature selection). While I acknowledge that discrete functions (such as one describing optimal groupings) are generally difficult to optimize, I must say I was somewhat disappointed that the authors hadn't arrived at a measure that could be optimized more efficiently than just using cross-validation. The authors mention that they have been "investigating and comparing different measures" (line 174), but they provide no further insights, so it is hard for me to determine the suitability of the chosen measure.
Experimentally, the authors provide simple synthetic validation of their framework along with a simple example on the Boston Housing data. They provide no baseline and do not compare with other methods. Again, it is difficult for me to determine if the presented results are good as I have nothing to compare with.
== Post rebuttal ==
The authors clarified some of my concerns in their rebuttal. I still think the experiments are thin, and that the authors did an unacceptably poor job of relating their work to ideas in machine learning. However, I do acknowledge that the paper has valid contributions and have improved my score somewhat. I strongly encourage the authors to (at least) improve their discussion of related work when the paper is published.