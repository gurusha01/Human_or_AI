The authors use a notion of generalized hamming distance, to shed light on the success of Batch normalization and ReLU units. 
After reading the paper, I am still very confused about its contribution. The authors claim that generalized hamming distance offers a better view of batch normalization and relus, and explain that in two paragraphs in pages 4,5. The explanation for batch normalization is essentially contained in the following phrase:
"It turns out BN is indeed attempting to compensate for deficiencies in neuron outputs with respect to GHD. This surprising observation indeed adheres to our conjecture that an optimized neuron should faithfully measure the GHD between inputs and weights."
I do not understand how this is explaining the effects or performance of batch normalization.
The authors then propose a generalized hamming network, and suggest that "it demystified and confirmed effectiveness of practical
techniques such as batch normalization and ReLU".
Overall, this is a poorly written paper, with no major technical contribution, or novelty, and does not seem to provide any theoretical insights on the effectiveness of BN or ReLUs. Going beyond the unclear novelty and technical contribution, the paper is riddled with typos, grammar and syntax mistakes (below is a list from just the abstract and intro). 
This is a clear rejection.
Typos and grammar/syntax mistakes:
—— abstract —— 
generalized hamming network (GNN)
-> generalized hamming network (GHN)
 GHN not only lends itself to rigiour analysis
-> GHN not only lends itself to rigorous analysis
"but also demonstrates superior performances"
-> but also demonstrates superior performance
—— 
—— intro —— 
"computational neutral networks"
-> computational neural networks
"has given birth"
-> have given birth
"to rectifying misunderstanding of neural computing"
-> not sure what the authors are trying to say
Once the appropriate rectification is applied ,
-> Once the appropriate rectification is applied,
the ill effects of internal covariate shift is automatically eradicated
-> the ill effects of internal covariate shift are automatically eradicated
The resulted learning process
-> The resulting learning process
lends itself to rigiour analysis
-> lends itself to rigorous analysis
the flexaible knowledge
-> the flexible knowledge
are equivalent and convertible with other
-> are equivalent and convertible with others, or other architectures?
successful applications of FNN
-> successful applications of FNNs