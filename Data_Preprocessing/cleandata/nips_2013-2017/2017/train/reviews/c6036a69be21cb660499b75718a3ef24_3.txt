This paper proposes a two stage decoding for Seq2Seq models: first, an output sequence is generated, and then, a second improved target sequence is produced using attention on the source and this initially created target sequence. The motivation behind is that this procedure would help to see the whole target sequence when making decision about word choice, etc.
Although the authors show some improvements in the BLEU score by this method, I find the overall approach somehow awkward. Seq2Seq models have replaced traditional SMT, eg. phrase-based models (PBSMT) because it's a nice end-to-end framework with a global training objective, instead of many individually models which were first trained independently and then combined in a log-linear framework (PBSMT).
You basically train a second NMT model which translates from a 1st target hypothesis to an improved one. This is similar in spirit to "statistical post-editing (SPE)". This is computationally expensive (+50% training time, decode 2 times slower).
Do you use beam search for both decoder networks ?
I would also say that your baseline is quite old: RNNSearch was the first model for NMT. In the meantime, better architectures are known and it would be good to show that the proposed two-stage approach improves on top of those systems (you mention yourself deeper encoder/decoder architectures).
An alternative, easy to implement baseline, would be to perform a forward and backward decode (last to 1st word) and combine both.