Summary:
The authors propose to incorporate human feedback in natural language for the task of image captioning. Specifically, they take a snapshot of a phrase-based caption model, predict captions for a subset of images, collect annotations to identify the mistakes made by the model if any, train a feedback network and 'teach' the model to improve over it's earlier snapshot with this feedback using reinforcement learning. Experiments show that such feedback helps improve the performance using automatic evaluation metrics for captioning.
Strengths:
(a) The paper addresses a well-motivated aspect for learning - human feedback. Their results suggest that human feedback on the model's performance is more valuable than additional caption annotations, which is intuitive and a positive outcome.
(b) The authors have done a good job on collecting human feedback in a least ambiguous way to finetune the model later. Their selection of a phrase-based captioning model aids this collection procedure.
(c) Experiments are comprehensive with different ablations to prove the effectiveness of the human feedback.
Weaknesses:
(a) Human evaluation on the quality of captions would have given a better sense of performance. Even though on a smaller scale, such evaluations throw more light than automatic correlations which have been shown to correlate poorly with respect to human judgement.
Comments:
(a) L69 - [3] does not use reinforcement learning. [34] seems to be doing reinforcement learning on visual dialog.
(b) Table 6 -Rouge - L second row - Is the value incorrect ? Does not seem to be in same ballpark as other models.
(c) Figure 2 appears incomplete. Without properly labeling (ct, ht, h_{t-1}, etc), it is very unclear as to how takes in the image and outputs caption phrases in the diagram.
(d) Sec 3.3: The overall setup of RL finetuning using feedback has not been discussed until that point. However, this section seems to describe feedback network assuming the setup thereby raising confusion. For example, after reading L162, one might assume that feedback network is trained via reinforcement learning. L164 - 'Newly sampled caption' has no context. I recommend adjusting the text accordingly to setup the overall training procedure.
(e) L245 - What does this sentence mean ?
(f) How is decoding done at evaluation -- beam search / sampling / greedy policy ?
(g) L167 - inconsistency in superscript.
 
(h) L222 - Why such a small testing dataset, which is almost 1/20th of the training dataset ?
Typos:
(a) L28-29 - reinforcement learning (RL)
(b) L64 - it exploits
(c) L74 Policy Gradients -> policy gradients 
(d) L200 - typo in the equation specifying range for \lambda?
References:
[34] Das, Abhishek, et al. "Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning." arXiv preprint arXiv:1703.06585 (2017).