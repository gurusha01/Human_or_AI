The paper studies the effect of decay of eigenvalues on function approximation using Gradient Descent(GD). It argues that Gradient Descent machinery may lead to large number of iterations when infinitely diff kernels are used to approximate non-smooth functions. 
To remedy this one could use second order methods or use an explicit regularisation. While explicit Regularisation leads to bias 
the alternative of deploying second order methods are too expensive. To remedy the situation the paper proposes Eigen-Pro.
The result showing that GD is ineffective when one wishes to approximate a not so smooth function using an infinitely differentiable kernel is novel and insightful. It is linked to the decay of eigen structure of the kernel functions is again insightful. The use of eigen-pro which achieves a sort of implicit regularisation is an interesting alternative where the computational challenge is finessed with randomisation which should have practical significance
While the above mentioned paragraph mentions the pluses there are two negative points that needs to be mentioned.
Firstly, the implications of the derived results are not clear. If it is speed-up we are concerned with then the comparisons with PEGASOS Is only marginal and often Eigen-pro is slower. It is commendable that the method can match performance with state of the art algorithms with kernel methods more needs to be done to have an explicit comparison to understand time and accuracy 
trade-offs. Would it make sense to show results by training a Deep Network(since this is what is used as the state of the art ) and comparing it with the kernel approximation in a synthetic setting. This would strengthen the understanding of time and accuracy trade-offs.
Secondly, the longer version of the paper is easy to read but the submitted version is not very easy to read. 
Overall, a nice paper, but lacking in explaining the experimental results and readability.