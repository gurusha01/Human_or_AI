This submission proposes a method that often reduces the variance of stochastic gradients for variational inference. With the reparameterization trick, the score function is effectively included in the stochastic gradients. Though it may serve as a control variate, more often it increases the variance of the stochastic gradients. Excluding the score function does not bias the stochastic gradients, and often leads to better empirical results.
The topic the paper addresses is important: the reparameterization trick has greatly expanded the applicability of variational inference. Yet there has been little systematic evaluation of the different ways of computing stochastic gradients with the reparametrization trick. 
In addition to addressing an important topic, this paper presents somewhat surprising results: 1) a Monte Carlo approximation to the entropy term may lead to lower variance overall than using the exact entropy term, 2) the score function adds variance unnecessarily to the stochastic gradients.
The paper does not require novel mathematical arguments to make its case, or even much math at all. The empirical results are appreciated, but the improvements appear modest. Overall, the paper is well written.
I presume equations 5--7 are all correct, but it would help to have them clarified. Why are we differentiating just the integrand of (1), when the measure of the integrand itself involves the variational parameters? How does equality 7 follow? If it's just the chain rule, are there notational changes that would make that more clear? Or some additional text to explain it? Notational challenges may be partly why no one has noticed that the score function is adding unnecessary variance to the gradients until now.