summary: The paper proposes a gradient boosting approach (CEGB) for prediction under budget constraints. During training CEGB minimizes empirical loss plus a cost penalty term. Three types of costs 
	 are incorporated: feature cost per example and across all examples as well as evaluation cost. It then looks for the best split greedily that minimizes the second order approximation of the loss objective. Experiments on
	 show improved performance over state of the art methods on two datasets.	 
	 
	 Significance: The proposed algorithm is closely related to GreedyMiser (Xu et al 2012). The main difference lies in the construction of the weak regressors. The key contribution of CEGB is to allow new splits at 
	 any current leaves rather than in a pre-defined fashion. To alleviate the computation for searching for the best leaf to split in each iteration, second-order approximation of the loss is used. Although the ideas of 
	 best-first tree learning, second-order loss approximation are well explored in the gradient boosting literature, applying them to the cost efficient setting is novel. CEGB is thus a refinement of GreedyMiser. 
	 
	 Results: CEGB is only compared to the state-of-the-art methods BudgetPrune and GreedyMiser on two datasets: Yahoo and MiniBooNE. In particular, the authors did not include the Forest dataset as used by BudgetPrune (Nan et al 2016).
	 My experience is that gradient boosting based methods perform poorly compared to random forests based methods (BudgetPrune) on the Forest dataset. This paper would be made stronger if more datasets are included to compare different methods.
	 On another note, the results in Figure 2(b) is exceedingly good for CEGB without tree cost. It's quite surprising (and hard to believe) that the precision can rise above 0.14 with average feature cost of ~20. 
	 How many trees are used and what's the average number of nodes in each tree to get such a good performance?
	 
	 Others: The authors repeatedly claimed that BudgetPrune assumes that feature computation is much more expensive than evaluation cost and therefore irrelevant in several experiments. This is not true. The formulation of of BudgetPrune by Nan et al 2016 
	 does take into account evaluation costs and it can be traded-off with the feature cost.
	 
	 Overall: I think this paper provides a refinement of GreedyMiser and the experimental results look promising. Adding more datasets and comparisons would make it even stronger. 	 
	 
 After discussion with the reviewers, I feel that the novelty of this paper is not that much given the work of XGBoost and GreedyMiser. But the experimental results show improvement over the state-of-the-art. The authors promised additional datasets to be included in the final version. So I'm still leaning to accept this paper.