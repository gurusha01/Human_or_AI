The paper presents a cognitive model that is intended to predict when certain questions are asked in certain contexts (belief states). Under this model, the unnormalized log probability of a question is defined as the weighted sum of informativeness (expected information gain), complexity (roughly, question length), answer type, and "relevance" (whether the question refers to the belief state or just world-independent logical facts). This model is applied to the Battleship domain where the task is to figure out where within a grid a number of ships (clusters of grid elements) are located. The full model with all of the features mentioned above did better at predicting held-out human data (in terms of log-likelihood) than lesioned versions with fewer features. In terms of correlation with human judgments, the model that didn't take into account EIG did about as well as the full model.
The paper seems generally technically sound.
I think it succeeds more as an AI project than as a cognitive science project. The paper states that "We find that our model predicts what question people will ask" and "from a CogSci standpoint, our results show how people balance informativeness and complexity." However, I'm not convinced that we learn much about how people balance these factors. In the Battleship domain, it seems that people are either influenced by EIG a little (if we believe the log-likelihood results, which according to bootstrap estimates have a 19% chance of being wrong) or not at all (if we believe the correlation results, which are also pretty noisy due to sparse human data). I don't think these results allow us to conclude much about how people balance these factors in general.
The paper is well-written and clear. I could probably reproduce the results.
The paper is missing some prior work. One of the apparent innovations of the paper is to "model questions as programs that, when executed on the state of a possible world, output an answer." This is the same as the notion of a "goal" used in Hawkins 2015 ("Why do you ask? Good questions provoke informative answers."), and is based on the notion of a question-under-discussion (QUD) introduced by Roberts 1996 ("Information structure in discourse: Towards an integrated formal theory of pragmatics."). The questioner model by Hawkins also uses expected information gain and trades it off with the complexity of questions, but is applied to a much simpler domain.
I think this sort of model (that has a compositional prior over questions and chooses based on EIG and other factors) makes a lot of sense as a component for cognitive and AI models, and I expect it to be used more in the future. I view the main contribution of this paper as showing that you can in fact generate interesting compositional questions for a non-trivial domain using this approach.