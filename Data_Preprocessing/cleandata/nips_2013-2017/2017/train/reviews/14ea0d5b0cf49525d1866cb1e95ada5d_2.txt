Summary:
This paper proposes a new reduction from multi-class classification to binary classification that is especially suitable when the number of classes is very large. They consider a hypothesis that map (input,class) pairs to scores, and the underlying loss function counts the fraction of the wrong classes that are scored higher than the true class. More specifically, they suppose they have a feature transformation phi that maps (input,class) pairs to a p-dimensional feature space, and they learn a mapping from R^p to scores. 
 Their reduction extends the work of Joshi et al. (2015) which, for each data point (x,y), creates K-1 transformed points where each transformed point intuitively corresponds to the comparison of label y with some incorrect label y'. Given that the transformed dataset contains correlated training examples, many standard generalization bounds cannot be applied. The first contribution of this paper is an improved generalization analysis over Joshi et al. (2015) using new bounds on the fractional Rademacher complexity in this setting. The second main contribution is showing that it is possible to employ sampling techniques during the reduction to balance the class distributions and to decrease the training set size. In particular, they sample each class (with replacement) in order to ensure that each class has roughly equal representation in the transformed training data, and rather than including transformed points for all K-1 possible class comparisons, they use only a random sample of classes. They show that these two sampling procedures do not drastically affect their generalization bounds. Finally, they show that this new procedure performs very competitively against several strong baselines.
Comments:
This paper presents an interesting reduction from multi-class classification with many classes to binary classification and corresponding generalization bounds for dealing with the fact that the reduced dataset contains correlated data. The main novelty of the reduction presented in the paper is the sampling technique for rebalancing the class distributions and for reducing the size of the transformed dataset, while the basic reduction appeared already in an earlier paper by Joshi etl al. The empirical comparison to existing algorithms are very strong.
One thing that is missing from the paper is a theoretical analysis of the impact of the prediction algorithm given in section 2.3, where comparisons are only performed for the k classes whose centroids are closest to a given input point. 
I would have also appreciated some additional discussion of the sample complexity bounds, possibly comparing them to the generalization bounds obtained for the non-transformed multi-class problems.