The paper proposes a practical scheme of adding selective classification capabilities to an existing neural network. The method consists of:
1. Choosing a score function that captures how confident the network is in its prediction, analysed are MC-dropout scores for networks trained with dropout and the maximum softmax score for networks with a softmax output with the second performing empirically better.
2. Defining the desired confidence level and error rate.
3. Running a binomial search to establish a score threshold such that with the desired confidence level the classifier will have an error rate smaller than the specified one on the samples it chooses to classify.
The procedure uses an existing bound on the true error rate of a classifier based on a small sample estimate (Lemma 3.1) and uses binomial search with a Bonferroni correction on the confidence level (Algorithm 1) to find the score threshold.
Experimental results validate the approach and show good agreement between the algorithm inputs (desired error rate) and observed empirical error rates on a test set.
The strong points of the paper are the practical nature of it (with the softmax response score function the procedure can be readily applied to any pretrained neural network) and the ease of specifying the algorithm's desired confidence level and error rate (which is modeled after ref [5]). While the paper builds on well known concepts, the careful verification of the concepts adds a lot of value.
The paper lacks simple baselines, that could showcase the importance of using the binomial search and the bound on the classifier's error rate. In particular, I would like to know what happens if one chooses the score threshold as the lowest value for which the error rate on a given tuning set is lower than e specified value- would the results be much more different than using the bound from Lemma 3.1? Knowing this baseline would greatly motivate the advanced techniques used in the paper (and would raise my score of this paper).
Nitpicks: the Algorithm 1 uses an uninitialized variable r*