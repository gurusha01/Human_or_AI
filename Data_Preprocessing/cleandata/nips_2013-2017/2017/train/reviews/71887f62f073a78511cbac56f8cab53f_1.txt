This paper combines several techniques in large-scale optimization: smoothing, acceleration, homotopy, and non-uniform sampling. It solves the problem with three convex functions and a linear operator f(x)+g(x)+h(Ax), where f is smooth. The smoothing technique from Nesterov [14] is applied to smooth the last term h(Ax), then a method based on block-wise forward-backward splitting is used to update one randomly chosen block at every iteration. Then acceleration and homotopy is applied to obtain a faster convergent algorithm. In fact this algorithm is a primal algorithm using the proximal gradient on the smoothed function. 
The numerical results show the performance of the proposed algorithm compared with some existing algorithms. 
L165, the reference is missing.