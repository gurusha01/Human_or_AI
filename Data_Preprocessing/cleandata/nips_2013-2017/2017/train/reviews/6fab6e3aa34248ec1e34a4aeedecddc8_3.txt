Summary:
The current work takes a different approach to fusing modalities for tasks in the intersection of language and vision, than most of the other works. Instead of fusing these modalities after extracting independent representations, the authors aim to modulate the visual features using the text. In particular, they aim to learn the batch normalization parameters of ResNet, which is typically used to extract visual representation, conditioned on the text. They call such it 'Conditional Batch Normalization' (CBN) and the resultant network as ModRNet. Experiments on two tasks shows the effectiveness of the proposed approach.
Strengths:
(a) Fusing both the modalities as early as the visual feature extraction is novel idea, to the best of my knowledge. This is well motivated by the paper, drawing parallels to findings in neuroscience on how language can influence the response to visual stimulus in human brain.
(b) Batch normalization parameters as a means to fuse modalities is a good choice as it scales only with the number of channels, thus greatly limiting the number of additional parameters to be learnt. Further, other parameters of the network being frozen exemplifies the simplicity of the approach.
(c) The paper contains good ablation studies along with analyzing finetuning ResNet on the task vs ModRNet. I thoroughly enjoyed the experiments and the discussions that followed.
Weaknesses:
(a) Experiments in the paper are limited to ResNet architecture for feature extraction. Though not a strict weakness, one does wonder about the generalizability of CBN on other networks, perhaps batch normalized version of VGG? Evidence that this is somehow a property of ResNet, perhaps due to the residual connection, would also suffice.
(b) Highly recommend the authors to carefully read the manuscript and resolve typos/grammatical issues that hinder the reading and understanding of the proposed approach. Few of the errors are listed towards the end.
Comments:
(a) L13-16 sentences don't flow well. While the initial sentences suggest our capability to model such tasks, the later sentences state that these remain a long-standing challenge.
(b) L185 - The task of oracle has not been described (with the input - outputs clearly mentioned) till this point. This makes the understanding of specifics that follow difficult.
(c) L212 - Retrain their model with the same visual features on same sized images as the current work, for perfect comparison ?
Typos:
(a) L4 - inputs
(b) L17 - towards
(c) L42 - Missing reference or a typo?
(d) L150 - latex typos
(e) L190 - ReLU
(f) L218, L267 - typos
Post discussion:
The authors seemed to have mostly done a good job with the rebuttal. After looking at other reviewer's comments and the rebuttal, I continue to feel that the paper has novel contributions bringing new insights which future works can build on. However, I am still curious about the generalization to other networks like my other reviewers. Showing some evidence on other networks (batch normalized version of VGG) would make the paper stronger, even if the findings suggest that it is not useful for other architectures. The authors do mention the lack of pretrained batch norm VGG models on tensorflow, but would be great if they could use pytorch pretrained models in time for the camera ready.