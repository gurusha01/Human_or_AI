The authors describe an approximation algorithm for k-mer (with mismatches) based string kernels.
The contribution is centered around a closed form expression of the intersection size of mismatching neighbourhoods.
The algorithm is evaluated in the context of sequence classification using SVM.
I think this is a great paper: clear motivation, good introduction, clear contribution, theoretical back-up, nice experimental results.
I have a few concerns regarding presentation and structuring, as well as doubts on relevance on the presented theory.
Presentation.
I think Section 3 is too technical. It contains a lot of notation, and a lot of clutter that actually hinder understanding the main ideas. On the other hand, intuition on crucial ideas is missing, so I suggest to move a few details into Appendix and rather elaborate high-level ideas.
-A table with all relevant quantities (and explanations) would make it much easier for a non-string-kernel person to follow establishing Theorem 3.3. An intuitive description of the theorem (and proof idea) would also help.
-Algorithm 1 is not really referenced from the main test. It first appears in Thm 3.10. It is also very sparsely annotated/explained. For example, the parameter meanings are not explained. Subroutines also need annotation. I think further deserves to be in the center of Section 3's text, and the text should describe the high level ideas.
-It should be pointed out more that Thm 3.3 is the open combinatoric problem mentioned in the abstract.
-In the interest of readability, I think the authors should comment more on what it is that their algorithm approximates, what the parameters that control the approximation quality are, and what would be possible failure cases.
-Theorem 3.13 should stay in the main text as this concentration inequality is the main theoretical message. All results establishing the Chebbyshev inequality (Thm 3.11, Lemma 3.12, Lemma 3.14 requirements should go to the appendix. Rather, the implications of the Theorem 3.13 should be elaborated on more. I appreciate that the authors note that these bounds are extremely loose.
Theory.
-Theorem 3.13 is a good first step to understanding the approximation quality (and indeed consistency) of the algorithm. It is, however, not useful in the sense that we do not care about the kernel approximation itself, but we care about the generalization error in the downstream algorithm. Kernel method theory has seen a substantial amount of work in that respect (e.g. for Nystrom, Fourier Feature regression/classification). This should be acknowledged, or even better: established. A simple approach would be perturbation analysis using the established kernel approximation error, better would be directly controlling the error of the estimated decision boundary of the SVM.
Experiments.
-Runtime should be checked for various m
-Where does the approximation fail? An instructive synthetic example would help understanding when the algorithm is appropriate and when it is not.
-The authors mention that their algorithm allows for previously impossible settings of (k,m). In Table 3, however, there is only a single case where they demonstrate an improved performance as opposed to the exact algorithm (and the improvement is marginal). Either the authors need to exemplify the statement that their algorithm allows to solve previously unsolved problems (or allow for better accuracy), or they should remove it.
Minor.
-Figure 1 has no axis units
-Typo line 95 "theset"
-Typo line 204 "Generating these kernels days;"