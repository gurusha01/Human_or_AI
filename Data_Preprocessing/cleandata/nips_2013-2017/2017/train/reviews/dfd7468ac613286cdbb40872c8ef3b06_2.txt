This paper proposes to improve the performance of the generative moment matching network (GMMN) by learning the kernel of the MMD. The optimization will result in a min-max game similar to that of the generative adversarial networks: the adversarial kernel is trained to maximize the MMD distance of the model distribution and the data distribution, and the generator at the same time is trained to fool the MMD. The kernel of MMD needs to be characteristic and is composed of Gaussian kernels with an injective function that is learned using an autoencoder.
I have some questions from the authors:
The decoder of the autoencoder f_dec does not seem to be learned in Algorithm 1. Is it just the transpose of the encoder? What is the architecture of the decoder and what kind of up-sampling does it have?
The encoder of the autoencoder f\phi is the same as the discriminator of DCGAN, right? Given that the DCGAN discriminator has max-pooling layers, how does f\phi become injective? How is the reconstruction quality of the autoencoder given that only h~100 dimensional vector is used for the latent space and given that there is max-pooling? 
Is it really necessary to train the autoencoder to get the model working? (this algorithm is very similar to W-GAN and there is no autoencoder there --- although I understand the autoencoder training is necessary to have an injective function.)
The final algorithm can be considered as the generative moment matching in the code space of an autoencoder, where the features of the autoencoder is also trained to maximize the MMD distance. The algorithm is also very similar to WGAN where instead of only matching the means, it is matching high-order moments due to the kernel trick. So in general I wouldn't expect this algorithm to work much better that other GAN variants, however I found the intuition behind the algorithm and its connection to GMMN and W-GAN interesting.