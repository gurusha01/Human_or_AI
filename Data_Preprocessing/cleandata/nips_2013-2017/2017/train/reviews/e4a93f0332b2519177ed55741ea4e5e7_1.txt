The paper proposed a new RNN structure called Fast-slow RNN and showed improved performance on a few language modeling data set. 
Strength: 
1. The algorithm combines the advantages of a deeper transition matrix (fast RNN) and a shorter gradient path (slow RNN). 
2. The algorithm is straightforward and can be applied to any RNN cells. 
Weakness: 
1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly. 
Here are some examples: 
(1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is "trivial" to convert to the sequential LSTM Fig2(b). Where are the h{t-1}^{1..5} in Fig2(b)? What is h{t-1} in Figure2(b)? 
(2) In line 96, I do not understand the sentence "our lower hierarchical layers zoom in time" and the sentence following that.
2. It seems to me that the multi-scale statement is a bit misleading, because the slow and fast RNN do not operate on different physical time scale, but rather on the logical time scale when the stacks are sequentialized in the graph. Therefore, the only benefit here seems to be the reduce of gradient path by the slow RNN. 
3. To reduce the gradient path on stacked RNN, a simpler approach is to use the Residual Units or simply fully connect the stacked cells. However, there is no comparison or mention in the paper. 
4. The experimental results do not contain standard deviations and therefore it is hard to judge the significance of the results.