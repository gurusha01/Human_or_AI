This paper starts by developing a notion of local "separability" of a loss
function, which they use to get l_infty convertence rates, in terms of the
separability parameters, for low and high dimensional settings. These rates are
then applied to then applied to a probabilistic classification problem with both
a generative and discriminative approach. After computing the teh separability
parameters for each, they can apply the theorems to get l_infty convergence
rates for the discriminative approach (logistic regression), as well as two
generative approaches (for the cases that x|y is isotropic Gaussian and gaussian
graphical model). They next consider l_2 convergence rates. The discriminative
rate is trivial based on the support consistency and the l_infty rates. The
naive generative algorithm would not have good scaling with the number of
dimensions, but with a soft-thresholding operator, they can get support
consistency and good rates. Finally they show the effect of sparsity on sample
complexity for the discriminative and the thresholded generative approaches. As
predicted by their theory, logistic rate suffers compared to the generative
approach, especially with less sparsity.
This seems to be impressive work. I have a few questions / requests for clarifications and notes:
Notes
- Terminology temporarily switches from empirical loss function to empirical
 risk function on line 127 on page 4
- I'm a little confused by the comment on lines 181-182 page 5: "Theorems 1 and 2
 give us an estimate on the number of samples required by a loss to establish
 convergence. In general, if gamma is very small or beta is very small then we
 would need a lot of samples." Aren't beta and gamma dependent on L_n, and thus
 the number of samples? Is this really about the behavior of beta and gamma as
 a function of n? 
- Probably the same confusion again: In several places (e.g. line 194, 195 on
 page 6, cor 3), we have inequalities with dependence on n on both sides of the
 equations. We're trying to get convergence rates in terms of n, but won't the
 separability parameters have their own behavior w.r.t. n. Where is that
 captured?
- It's claimed that Theorems 1 and 2 give us l_infty convergence rates.
 Embarrassingly, I don't see how we get the rates. Do we need to
 know how the separability changes with n?
- "From Cor 3 and 4, we see that for IG, both the discriminativce and generative
 approach achieve the same convergence rate, but the generative approach does
 so with only a logarithmic dependence on the dimension." In what way is
 logistic different? Also corollary 4 has a n >> log p -- is that right?
 Corollary 3 has the notation \succsim -- I'm not familiar with that notation,
 and perhaps should be defined in a footnote?