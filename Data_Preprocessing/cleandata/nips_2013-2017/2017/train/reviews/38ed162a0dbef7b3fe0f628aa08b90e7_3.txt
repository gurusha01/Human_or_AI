The paper is well written and rigorous and it successfully shows how (some type of) functional spaces implemented by DCNNs can be described in terms of multilayer kernels. It also shows how the architecture has nice properties: (almost) invariance to group transformations and stability to diffeomorphic transformations. I recommend the acceptance given the comments below: 
*To the reviewer's understanding the main paper contribution is a rewriting of CNNS architecture in the context of RKHS. 
Please add in the text some comments why this reformulation is/will be useful (e.g. kernel formulation may lead to interesting results on generalization bounds?, is it computationally more efficient to implement using standard techniques/approximation on kernels?)
More in general is the novelty of the approach going beyond a reformulation of DCNNs in the context of RKHS?
Also clarify the novelty w.r.t. the paper "Convolutional Kernel Networks" of
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid.
* The authors stress the importance of "not loosing the signal information". 
Why is so? DCNNs are often used in relation to some tasks (e.g. image classification). Unless the task is signal reconstruction only task-relevant information need to be preserved. Please comment on this.
* Regarding group invariance: is there any connection with the invariant kernel approach adopted by Hans Burkhardt (invariant kernels)?
* More of a curiosity: you define patch, pooling and kernel maps that commute with the group action. Will any other operator (in the set of operators that commutes with all group elements) ensure the equivariance property of DCNN w.r.t. the group transformations?