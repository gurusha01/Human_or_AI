This paper studies a kind of contextual linear bandits with a conservative constraint,
	which enforces the player's cumulative reward at any time t to be at least (1-alpha)-times
	as larger as that of the given baseline policy for a give alpha.
	They consider two cases, the cases with known and unknown baseline rewards.
	For each case, they propose a UCB-type algorithm based on an existing algorithm for contextual linear bandits and prove its regret upper bound, which are composed of
	the regret caused by the base algorithm and the regret caused by satisfying the conservative constraint.
	They also conducted simulations of the algorithm with known baseline rewards, and checked that the algorithm really satisfies the conservative constraint.
	The conservative constraint, their algorithms and regret bounds seem natural.
	The graph of Figure 1(a) should be improved so as to be able to see the initial conservative phases of CLUCB more.
	The same experiments for CLUCB2 should be done because it works in a more realistic setting.