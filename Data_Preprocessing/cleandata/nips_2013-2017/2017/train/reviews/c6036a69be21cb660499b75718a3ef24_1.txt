This paper introduces a two steps decoding model applied to NMT and summarization. The idea is well explained and the experimental results show significant improvements over realistic baseline. 
The authors chose to approximate the marginalization of the intermediate hypothesis with monte-carlo. This is indeed a solution, but it is not clear how many samples are used? Does this hyperparameter have an impact on the performance ? Do you use beam search instead of simple sampling? These technical details should be more precisely described. 
At the end, the work described in this paper is interesting and the experimental part is meaningful. However, I found the introduction very pretentious. Two paragraphs are dedicated to a "cognitive" justification as the authors have reinvented the wheel. Multi-steps decoding exists for many years and this is really a great idea to propose a solution for end to end neural model, but you don't need to discard previous work or motivate it with pop cognitive consideration (note that in "cognitive science" there is "science").
Page 2: The sentence in line 73 is useless.
Line 78: this sentence is correct for end to end neural models, but there is a long history in speech recognition with multiple pass decoding: in the first step, a word lattice is generated with "weak" acoustic and language models, then more sophisticated and powerful models are used to refine iteratively this word lattice. So you cannot say that for sequence prediction it wasn't explored. It is not mandatory to discard previous work because you find a nice name. 
Page 8:
Line 277: "In this work, inspired by human cognitive process," You could avoid this kind of pretentious phrase.