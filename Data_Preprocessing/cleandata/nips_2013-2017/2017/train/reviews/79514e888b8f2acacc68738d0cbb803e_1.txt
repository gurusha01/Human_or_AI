This work extends and improves the performance of GAN based approaches to semi-supervised learning as explored in both "Improved techniques for training gans" (Salimans 2016) and "Unsupervised and semi-supervised learning with categorical generative adversarial networks" (Springenberg 2015). 
The paper introduces the notion of a complement generator which tries to sample from low-density areas of the data distribution (in feature space) and explores a variety of objective terms motivated/connected to this analysis. It is difficult to exactly match the motivated objective, due to various hard to estimate quantities like density and entropy, the paper uses a variety of approximations in their place.
In addition to an illustrative case study on synthetic data, the paper has a suite of experiments on standardized semi-supervised tests including ablation studies on the various terms proposed. The overall empirical results are a significant improvement over the Feature Matching criteria proposed in "Improved techniques for training gans".
Given the variety of objective terms suggested it is important and appreciated that the authors included ablation studies. It is unfortunate that they are not completely thorough, however. For instance, why does SVHN have a 5 different experiments but MNIST 2 and CIFAR-10 3? Why are the Approximate Entropy Maximization terms only tested on SVHN? How do they perform on CIFAR-10? Could the author(s) comment on why a fuller suite of comparisons was not completed?
The benefits of the various terms are not always consistent across datasets. The text mentions and discusses this briefly but a more thorough investigation by completing the ablation studies would be helpful for people wishing to extend/improve upon the ideas presented in this paper.