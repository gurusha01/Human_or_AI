Overall Impression:
I think this paper introduces a novel and interesting idea that is likely to spark future experimentation towards multi-modal early-fusion methods. However, the presentation and the writing could use additional attention. The experiments demonstrate the effectiveness of the approach on multiple tasks though they are a bit narrow to justify the proposed method outside of the application domain of vision + language. I think further iterations on the text and additional experiments with other model architectures or different types of multi-modal data would strengthen this submission.
Strengths:
+ I like the neurological motivations for the CBN approach and appreciate its simplicity.
+ Comparing fine-tuning batch norm parameters (Ft BN) vs the question-conditioned batch norm predictions provided an interesting ablation. It seems like adjusting to the new image statistics (Ft BN) results in significant improvement (~1% VQA, 2% Crop GuessWhich) which is then doubled by conditioned on question (~2% VQA, 4% Crop GuessWhich).
+ I appreciate the promise of public code to reproduce the experimental results.
+ The tSNE plots are quite interesting and show that the language conditional modulation seems to have a significant effect on the visual features. 
Weaknesses:
- I don't understand why Section 2.1 is included. Batch Normalization is a general technique as is the proposed Conditional Batch Normalization (CBN). The description of the proposed methodology seems independent of the choice of model and the time spent describing the ResNet architecture could be better used to provide greater motivation and intuition for the proposed CBN approach. 
- On that note, I understand the neurological motivation for why early vision may benefit from language modulation, but the argument for why this should be done through the normalization parameters is less well argued (especially in Section 3). The intro mentions the proposed approach reduces over-fitting compared to fine-tuning but doesn't discuss CBN in the context of alternative early-fusion strategies. 
- As CBN is a general method, I would have been more convinced by improvements in performance across multiple model architectures for vision + language tasks. For instance, CBN seems directly applicable to the MCB architecture. I acknowledge that needing to backprop through the CNN causes memory concerns which might be limiting.
- Given the argument for early modulation of vision, it is a bit surprising that applying CBN to Stage 4 (the highest level stage) accounts for majority of the improvement in both the VQA and GuessWhat tasks. Some added discussion in this section might be useful. The supplementary figures are also interesting, showing that question conditioned separations in image space only occur after later stages.
- Figures 2 and 3 seem somewhat redundant. 
Minor things:
- I would have liked to see how different questions change the feature representation of a single image. Perhaps by applying some gradient visualization method to the visual features when changing the question?
- Consider adding a space before citation brackets. 
- Bolding of the baseline models is inconsistent. 
- Eq 2 has a gammaj rather than gammac
L34 'to let the question to attend' -> 'to let the question attend'
L42 missing citation
L53 first discussion of batch norm missing citation
L58 "to which we refer as" -> "which we refer to as"
L89 "is achieved a" -> "is achieved through a"