The paper presents a generalization to previous work on ellinfty recovery in high-dimensional models. Past results exist but only for the setting of linear regression. The authors generalize the existing results to that of general loss functions. I did not have enough time to thoroughly go through the proofs, but they seem correct. I would say that parts of the paper are haphazardly written, and so the other should focus a lot of effort on fixing the presentation.
Comments:
More discussion of the proposed links to incoherence. Incoherence is of course a much stronger assumption used in the linear setting to establish ellinfty bounds. Can analogous notions of incoherence be developed in this setting?
The authors should look at the work of Ren et. al. (ASYMPTOTIC NORMALITY AND OPTIMALITIES IN ESTIMATION OF LARGE GAUSSIAN GRAPHICAL MODELS) to contrast against ellinfty bounds and optimal rates as well as assumptions for precision matrix recovery.
A lot of the text following Corollary 3 and beyond isn't clear. For instance, under isotropic Gaussian design, ellinfty control for classification under mild assumptions on the correlation between the labels and the design allows one to identify the support with only n > log p samples. What is the connection to generative versus discriminative models here?
Some remarks that suggest one method requires more samples than another (e.g. line 218) should be softened since there aren't lower bounds.