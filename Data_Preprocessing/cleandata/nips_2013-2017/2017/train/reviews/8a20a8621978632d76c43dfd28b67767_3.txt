Summary:
There are many recent efforts in providing interpretability to complex models, such as LIME, DeepLift and Layer-wise relevance propagation. In complex models, change in input features contributes to the output change nonlinearly. Many previous works tried to distribute the feature contributions fairly. In this work, the author pointed that Shapley value -- marginal contribution of each feature -- from game theory is the unique solution that satisfies desired properties. The author provides a unified framework for many of the interpretable methods by introducing the concept of additive feature attribution methods and SHAP as a measurement for feature importance. A SHAP value estimation method is provided. The author supported the method with user studies and comparisons to related works on benchmark datasets.
Quality:
This paper is technically sound with detailed proofs and supportive experiments. I think there could be more discussion on the shapley value estimation accuracy vs computational efficiency.
Clarity:
The paper organized the materials well. The author provides enough background and motivations. However, in the implementation of model specific approximations on page 6, section 4.2, it could be more helpful if the author can provide an algorithm flow (for example, how do we change the algorithm in DeepLift to satisfy SHAP values).
There are some other minor issues about the clarity:
In page 4, equation (10), \bar{S} is not defined, should it be \bar{S} \subseteq S, and the last symbol in the row should be x_{\bar{S}} instead of x?
In page 6, line 207, the citation style seems to be inconsistent.
Originality:
I think this paper nicely pointed out that the proper distribution of the feature contribution is Shapley value from game theory. And the author linked the Shapley value to many existing method to identify the proper way of constructing the methods by proofs. It is interesting that the author explains the enhanced performance in original vs current DeepLift from better approximation in shapley value.
I am wondering if the author could also link the shapley value to the gradient-based approaches [1,2], since there are still a lot of interpretations based on these approaches. 
[1]: Simonyan et al. (2013): "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", http://arxiv.org/abs/1312.6034
[2]: Springenberg et al. (2015): "Striving for Simplicity - The All Convolutional Net", http://arxiv.org/abs/1412.6806
Significance:
This work provides a theoretical framework for feature importance methods. The better defined metric SHAP could become a guidance of feature importance evaluation in the research field of interpretable methods.