The authors present PMD a population based divergence between probability distributions and show it is a consistent estimator of the Wasserstein distance.
The estimator presented is conceptually simple and differentiable, which is a clear alllows training NN based models.
The authors thoroughly compare PMD to MMD, which is the most prominent population based divergence in machine learning.
The authors comment on the drawbacks of their method: exact calculation has cubic complexity, but propose the use of an approximation which has quadratic complexity, and show in their empirical results that this does not degrade statistical performance too much.
The paper is well structured and written and includes references to previous work where due.
The theoretical results seem correct.
The experimental analysis is adequate. They compare PMD to MMD and other methods for domain adaptation and compare to MMD for generative modelling.
I would have liked to see the method being used for generative modelling in domains with many modes. I wonder if PMD works when N is smaller than the number of modes.
All things considered I think this is a good paper, that presents a possibly very useful method for comparing distributions.