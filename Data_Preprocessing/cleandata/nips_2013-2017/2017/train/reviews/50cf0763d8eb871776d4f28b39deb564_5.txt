In this paper, the convergence of maximum likelihood estimates w.r.t. sample complexities, for generative as well as discriminative models, is studied. I find this paper interesting. It is well written, though some scope for improvements in the terms of exposition.
The authors find that generative models can outperform discriminative methods, under certain sparsity related conditions, if the number of samples is small while the dimension of data is high. The theoretical analysis is claimed to be novel, and should have an impact for real world problems in the long run. Logistic regression is considered as an example discriminative model. For generative models, Gaussian and Gaussian Graphical models are considered. The authors managed to keep the exposition such that it is of interest to a general machine learning audience, which can be somewhat hard for a theoretical paper.
I have a few suggestions.
(1) Is it possible to draw a figure explaining the idea of separability, in reference to the Definition 1 ?
(2) I didn't understand the argument written before the Corollary 5, w.r.t. the comparison between Corollary 3 and 4. 
(3) There are some notation mistakes.
On a separate note, it would be nice to extend the paper, with more simulations, and put the extended version on ArXiv, etc.