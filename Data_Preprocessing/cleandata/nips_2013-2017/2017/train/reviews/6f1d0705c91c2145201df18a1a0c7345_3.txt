Thank you for an interesting read.
This paper proposed a hierarchical probabilistic model using implicit distributions. To perform posterior inference the authors also proposed a variational method based on GAN-related density ratio estimation techniques. The proposed method is evaluated with a number of different tasks including ABC, supervised learning and generative modeling.
I like the idea in general but I think there are a few points that need to be made clearer.
1. How is your method related to AVB [36] and ALI [13]? I can see these connections, but not all the readers you're targeting could see it easily.
2. In the AVB paper they mentioned a crucial trick (adaptive contrast) to improve the density ratio estimations in high dimensions. You only did a toy case (2D linear regression) to demonstrate the stability of your method, and your findings are essentially the same as in the toy example in the AVB paper (naive density ratio estimation works well in low dimensional case). It would be better if you could provide an analysis in high dimensional case, e.g. your BNN example.
3. Hinge loss: why the optimal r is the log ratio?
4. Generating discrete data: yes using r(x, w) instead of r(x) could provide gradients, however this means you need to input w to the discriminator network as well. Usually you need quite a deep (and wide) neural network to generate realistic data so I presume w could be of very high dimensions. How scalable is your approach here?
5. I'm a bit worried about no quantitative results for the sequence generation part. I think it's not a good practice for just including generative samples and letting the readers judge the fidelity.
In summary I think this paper is borderline. I would be happy to see clarifications if the authors think I've missed some important points.