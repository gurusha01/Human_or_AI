This paper proposes a recurrent network for sparse estimation inspired on sparse Bayesian learning (SBL). It first shows that a recurrent architecture can implement a variant of SBL, showing through simulations that different quantities have different time dynamics, which motivates leveraging ideas from recurrent-network design to adapt the architecture. This leads to a recurrent network that seems to outperform approaches based on optimization in simulations and two applications. 
Strengths: The idea of learning a recurrent network for sparse estimation has great potential impact. The paper is very well written. The authors motivate their design decisions in detail and report numerical experiments that are quite thorough and indicate that the technique is successful for challenging sparse-decomposition problems. 
Weaknesses: A lot of the details about the implementation of the method and the experiments are deferred to the supplementary material, so that the main paper is a bit vague. I find this understandable due to length limitations.