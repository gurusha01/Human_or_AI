This paper presents a method for learning predictive (one-dimensional) point process models through modelling the count density using a W-GAN.
 Detailed comments:
 * Abstract: there are several approaches for doing full (or approximate) Bayesian inference on (Cox or renewal) point processes. E.g. see arxiv.org/pdf/1411.0254.pdf or www.gatsby.ucl.ac.uk%2F~vrao%2Fnips2011_raoteh.pdf.
 * To me it appears that the approach will only work for 1D point processes, as otherwise it is hard to represent them via the count density? If this is the case, it would be good to see this more explicitly stated.
 * This is the case for most inhomogenous point processes, but given only a single realisation of the point process, it would seem very hard to characterise a low variance estimate of distance from the generated count measure to the data? Perhaps this is the main reason that the W-GAN performs so well---much like the GP based intensity-explicit models, a heavy regularisation is applied to the generator/intensity-proxy.
 * I don't understand the origin of 'real' in the real world evaluation metrics e.g. Figure 3? How do you arrive at this ground truth?
 * A discussion of how easy/difficult these models are to train would have been interesting.
 
 Finally I am very interested to know how simple models compare to this: e.g. KDE with truncation, simple parametric Hawkes etc? My main concern with this work would be that these models are all horrendously over-complex for the signal-to-noise available, and that therefore while the W-GAN does outperform other NN/RNN based approaches, a more heavily regularised (read simpler) intensity based approach would empirically outperform in most cases.