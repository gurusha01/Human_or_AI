The paper describes an idea to learn phrasal representation and facilitate them
in RNN-based language models and neural machine translation
-Strengths:
The  idea to incorporate phrasal information into the task is interesting.
- Weaknesses:
- The description is hard to follow. Proof-reading by an English native speaker
would benefit the understanding
- The evaluation of the approach has several weaknesses
- General discussion
- In Equation 1 and 2 the authors mention a phrase representation give a
fix-length word embedding vector. But this is not used in the model. The
representation is generated based on an RNN. What the propose of this
description?
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is
the combination of two architectures a reason for your improvements?
- What is the simplified version of the GRU? Why is it performing better? How
is it performing on the large data set?
- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline)
in Table 4?
-  What is the motivation for only using the ending phrases and e.g. not using
the starting phrases?
- Did you use only the pyramid encoder? How is it performing? That would be a
more fair comparison since it normally helps to make the model more complex.
- Why did you run RNNsearch several times, but PBNMT only once?
- Section 5.2: What is the intent of this section