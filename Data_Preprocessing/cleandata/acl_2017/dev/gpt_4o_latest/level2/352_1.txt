Review
Summary and Contributions
This paper introduces an adversarial multi-task learning framework that aims to improve the separation between shared and task-specific feature spaces in multi-task learning (MTL). The primary contributions of the paper are as follows:
1. Precise Shared-Private Feature Separation: The proposed model introduces orthogonality constraints and adversarial training to ensure that shared and private feature spaces are disjoint, addressing contamination issues in traditional shared-private models.
2. Extension of Adversarial Training: The paper extends binary adversarial training to a multi-class setting, enabling joint training across multiple tasks and the incorporation of unlabeled data for semi-supervised learning.
3. Transferable Shared Knowledge: The shared feature extractor is designed to condense task-invariant knowledge into an off-the-shelf neural layer that can be transferred to new tasks, demonstrating its utility in knowledge transfer scenarios.
Strengths
1. Novelty: The combination of adversarial training and orthogonality constraints to enforce disjoint shared-private feature spaces is innovative and addresses a key limitation in existing MTL approaches. The extension of adversarial training to multi-class settings is also a notable contribution.
2. Comprehensive Evaluation: The model is evaluated on 16 diverse text classification tasks, demonstrating consistent improvements over baseline methods, including SP-MTL and FS-MTL. The results show a significant reduction in error rates, with ASP-MTL outperforming other models by a notable margin.
3. Knowledge Transfer: The experiments on knowledge transfer are compelling, showing that the shared layer learned by ASP-MTL can generalize well to unseen tasks. This highlights the practical utility of the proposed approach.
4. Qualitative Insights: The paper includes detailed qualitative analyses, such as neuron behavior visualization and pattern extraction, which provide valuable insights into how the model achieves its improvements.
5. Reproducibility: The paper provides sufficient details about datasets, hyperparameters, and experimental setups, which enhances reproducibility.
Weaknesses
1. Limited Comparison with Non-Neural MTL Models: While the paper compares ASP-MTL with neural baselines (e.g., SP-MTL, FS-MTL), it does not benchmark against non-neural MTL methods, which could provide a broader perspective on the model's performance.
2. Scalability Concerns: The adversarial training and orthogonality constraints introduce additional computational overhead. The paper does not discuss the scalability of the approach to larger datasets or tasks with higher complexity.
3. Ablation Studies: While the paper evaluates the overall framework, it lacks detailed ablation studies to isolate the contributions of adversarial training and orthogonality constraints individually. This would help quantify the impact of each component.
4. Task Diversity: The evaluation focuses primarily on text classification tasks. It is unclear how well the proposed framework generalizes to other domains, such as computer vision or speech processing, where MTL is also widely used.
Questions to Authors
1. How does the computational cost of ASP-MTL compare to SP-MTL and FS-MTL, particularly in terms of training time and resource requirements?
2. Have you considered applying the proposed framework to non-text domains (e.g., vision or speech)? If so, what challenges do you anticipate?
3. Could you provide more details on how the hyperparameters λ and γ were tuned, and how sensitive the model is to their values?
Recommendation
I recommend acceptance of this paper, as it presents a novel and well-supported approach to improving multi-task learning through adversarial training and orthogonality constraints. While there are minor weaknesses, they do not detract significantly from the overall contributions and impact of the work. The paper is likely to stimulate further research in adversarial MTL and its applications.