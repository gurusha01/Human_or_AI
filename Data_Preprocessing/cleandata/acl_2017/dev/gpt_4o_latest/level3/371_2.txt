Review of the Submission
Summary and Contributions
This paper introduces a novel architecture, Phrasal Recurrent Neural Networks (pRNN), designed for sequence-to-sequence tasks such as language modeling and machine translation. The key contribution of the pRNN framework is its ability to represent sentential history as a set of candidate phrases of varying lengths, which are processed using a parallel RNN pyramid. The model employs a soft attention mechanism to selectively combine phrase representations for predicting the next word. The authors claim significant improvements in perplexity for language modeling tasks over state-of-the-art LSTM baselines on the Penn Treebank (PTB) and FBIS datasets. Additionally, the pRNN model demonstrates competitive performance in Chinese-English machine translation tasks, outperforming both phrase-based statistical machine translation (PBSMT) and RNNsearch baselines.
The primary contributions of this work are:
1. A novel parallel RNN pyramid architecture that explicitly models phrases of varying lengths without requiring human-labeled data or external models.
2. The integration of a phrasal-level attention mechanism to enhance sequence generation tasks.
3. Empirical results showing improvements in perplexity and BLEU scores over strong baselines in language modeling and machine translation tasks.
Strengths
1. Novel Architecture: The introduction of the pRNN framework is innovative, particularly the use of parallel RNNs to represent phrases of varying lengths. This approach provides an alternative to deeper RNN stacks by exploring parallelism.
2. Unsupervised Structure Learning: The model does not rely on external resources like human-labeled data or word alignment models, which is a significant advantage in terms of scalability and applicability to diverse datasets.
3. Empirical Improvements: The reported improvements in perplexity (over 10 points) and BLEU scores demonstrate the effectiveness of the proposed approach over strong baselines.
Weaknesses
1. Technical Clarity: 
   - The paper does not clearly specify whether there is a limit on phrase length in the pRNN, which directly impacts model complexity and tractability.
   - The scalability of the phrasal-level attention mechanism during inference on larger datasets is not adequately addressed.
2. Empirical Concerns:
   - The choice of datasets for training and evaluation is suboptimal. While PTB is a standard benchmark, the use of FBIS for training introduces an unfair advantage in perplexity evaluation on PTB. More appropriate datasets, such as Common Crawl or enwiki8, should have been considered.
   - The GRU version of pRNN is not evaluated on the FBIS dataset, leaving a gap in the empirical comparison.
   - The results section lacks sufficient explanation of perplexity and BLEU scores, and Table 4 results are inconsistent with Section 4.3. The method for averaging test scores should be clarified.
   - The significance of BLEU improvements over baselines is not reported, making it difficult to assess the robustness of the results.
3. Baseline Comparisons:
   - The comparison with baselines is unclear, particularly regarding the use of attention mechanisms. The inclusion of multiple phrasal attention layers in pRNN creates an unfair comparison with a simple encoder-decoder baseline without attention.
4. Evaluation Metrics:
   - BLEU evaluation alone is insufficient for assessing the quality of phrase coherence. Additional intrinsic evaluations, such as phrase-level coherence or structural accuracy, are necessary.
Questions to Authors
1. Is there a limit on the phrase length in the pRNN model? If so, how does it affect model complexity and performance?
2. How does the phrasal-level attention mechanism scale during inference on larger datasets? Are there any computational bottlenecks?
3. Why was the GRU version of pRNN not evaluated on the FBIS dataset? Can you provide these results for a fair comparison?
4. How were the test scores in Table 4 averaged, and why do they appear inconsistent with Section 4.3?
5. Can you provide significance testing for the BLEU improvements over baselines?
Additional Comments
- Figures 2 and 3 are redundant and could be combined or removed to allocate space for a more detailed explanation of the results.
- The captions for figures and tables are overly verbose and should be integrated into the main text for better readability.
- Spacing in equations can be reduced to improve formatting and conserve space.
Recommendation
While the paper introduces a novel and promising architecture, the technical and empirical weaknesses, particularly the lack of clarity in evaluation and baseline comparisons, limit its impact. Addressing these issues during the author response period could significantly strengthen the submission. Currently, I recommend weak reject, contingent on the authors' ability to clarify and address the identified concerns.