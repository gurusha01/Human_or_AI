Review of the Paper
Summary and Contributions
This paper introduces an adversarial multi-task learning (MTL) framework to address the problem of feature contamination in shared-private models. The key contributions of the paper are as follows:
1. Adversarial Training for Feature Separation: The paper proposes the use of adversarial loss to ensure that shared representations are task-invariant and devoid of task-specific features. This is a novel application of adversarial training in the MTL context.
2. Orthogonality Constraints: The authors introduce orthogonality constraints to reduce redundancy by ensuring that shared and private feature spaces capture distinct information. This is a significant improvement over traditional shared-private models.
3. Transferability of Shared Knowledge: The paper demonstrates that the shared feature extractor can be treated as an off-the-shelf module for transfer learning, showcasing its utility in unseen tasks.
Strengths
1. Novel Approach to Feature Separation: The combination of adversarial loss and orthogonality constraints is a compelling solution to the problem of feature contamination in shared-private models, which is a well-known challenge in MTL.
2. Extensive Experiments: The paper evaluates the proposed model on 16 diverse text classification tasks, providing robust empirical evidence for its effectiveness. The results consistently show improvements over baseline models, including SP-MTL and FS-MTL.
3. Transfer Learning Potential: The shared layer's ability to generalize to unseen tasks is a valuable contribution, as it highlights the model's practical applicability beyond the training tasks.
4. Visualization and Qualitative Analysis: The paper includes insightful visualizations and qualitative analyses, such as neuron behavior and feature patterns, which help explain the quantitative improvements.
Weaknesses
1. Lack of Clarity in Adversarial Training: The description of the adversarial system is somewhat confusing, particularly regarding how effectively task-specific information is removed from shared representations. A more detailed explanation or ablation study would strengthen this aspect.
2. Insufficient Motivation and Logical Flow: The paper does not clearly articulate why the shared-private model has inherent issues and how the proposed methods directly address them. A stronger theoretical foundation or motivation would improve the paper's impact.
3. Limited Comparison with State-of-the-Art: While the paper compares its model to SP-MTL and FS-MTL, it does not include comparisons with hierarchical MTL approaches or other recent advancements in the field. This omission weakens the evaluation.
4. Missing Best-Known Results: The evaluation section does not include best-known results for the tasks, making it difficult to gauge the absolute performance of the proposed model.
5. Sparse Related Work Discussion: The paper does not adequately cite or discuss related work, particularly hierarchical MTL approaches that explore task similarity and its impact on performance.
Questions to Authors
1. Can you provide more details on how adversarial training ensures that task-specific features are removed from the shared space? An ablation study isolating the effect of adversarial loss would be helpful.
2. How does the proposed model perform compared to hierarchical MTL methods that explicitly model task similarity?
3. Could you elaborate on the choice of hyperparameters for adversarial loss and orthogonality constraints? How sensitive is the model to these values?
Conclusion
Overall, the paper presents a novel and promising approach to addressing feature contamination in MTL. While the experimental results and qualitative analyses are strong, the paper would benefit from clearer explanations, additional comparisons with state-of-the-art methods, and a more thorough discussion of related work. With these improvements, the paper could make a significant contribution to the field.