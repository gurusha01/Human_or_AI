Review of the Paper
Summary and Contributions
This paper introduces Phrasal Recurrent Neural Networks (pRNN), a novel framework that incorporates phrasal information into RNN-based language models and neural machine translation (NMT). The key contributions of this work are:
1. Innovative Use of Phrasal Information: The pRNN framework explicitly models task-specific nested phrases from plain text without relying on human-labeled data or external models. This approach represents a significant departure from traditional RNN-based models, which typically rely on sequential word representations.
2. RNN Pyramid Architecture: The proposed RNN pyramid generates fixed-length vector representations for phrases of arbitrary lengths, enabling the model to capture richer structural information.
3. Performance Improvements: The pRNN model achieves substantial improvements in perplexity for language modeling tasks (over 10 points better than a strong LSTM baseline) and outperforms both phrase-based statistical machine translation (PBSMT) and RNNsearch models in NMT tasks.
Strengths
1. Innovative Idea: The incorporation of phrasal information into RNNs is a novel and promising direction, particularly as it avoids the need for external resources like human-labeled data or word alignment models.
2. Parallelization of RNNs: The exploration of parallel RNN sequences in the pyramid architecture is a creative alternative to the conventional approach of stacking deeper RNN layers, potentially opening new avenues for model design.
3. Empirical Results: The model demonstrates strong empirical performance in both language modeling and NMT tasks, with significant improvements over established baselines.
Weaknesses
1. Clarity of Writing: The paper is difficult to follow due to poor writing quality. It would benefit greatly from thorough proofreading by a native English speaker to improve readability and clarity.
2. Evaluation Methodology: The experimental setup raises concerns about fairness and reproducibility:
   - The number of runs for RNNsearch and PBNMT is inconsistent, which could bias the results.
   - The standalone performance of the pyramid encoder is not evaluated, making it difficult to isolate its contribution.
3. Unclear Design Choices:
   - The rationale for using GRU in the pyramid and LSTM in the sequential part is not explained, leaving the effectiveness of this combination unclear.
   - The simplified GRU variant performs better, but its behavior on larger datasets is not sufficiently analyzed.
4. Ambiguities in Results:
   - In Table 4, the distinction between RNNsearch (groundhog) and RNNsearch (baseline) is unclear, making it difficult to interpret the reported BLEU scores.
   - The motivation for focusing only on ending phrases (and not starting phrases) is not justified.
5. Section 5.2 (Deep Memory Network): The purpose of this section is unclear, as it does not directly contribute to the core findings of the paper.
Questions to Authors
1. Why was GRU chosen for the pyramid encoder while LSTM was used for the sequential part? Could the authors provide empirical evidence or theoretical justification for this design choice?
2. What is the purpose of the fixed-length phrase representation described in Equations 1 and 2, given that it is not directly used in the model?
3. Why were only ending phrases considered in the experiments? Would starting phrases not provide complementary information?
4. Could the authors clarify the distinction between RNNsearch (groundhog) and RNNsearch (baseline) in Table 4?
5. How does the simplified GRU variant behave on larger datasets, and why does it outperform the standard GRU?
Recommendation
While the paper presents a novel and promising idea, the weaknesses in writing, evaluation methodology, and clarity of design choices significantly detract from its overall quality. I recommend major revisions before acceptance. The authors should address the methodological concerns, clarify design decisions, and improve the paper's readability to make it more accessible to the research community.