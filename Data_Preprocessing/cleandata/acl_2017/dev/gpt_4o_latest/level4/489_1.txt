Paraphrased Review:
---
COMMENTS AFTER AUTHOR RESPONSE:
Thank you for your detailed response, especially for clarifying the hypothesis. I concur with your point regarding cross-modal mapping. However, I disagree with the implicit equivalence you seem to draw between "visual" and "referential." While a referent can indeed be visually represented, visual information can also complement a word's representation in an aggregated form to encode perceptual aspects of meaning—similar to how textual information is incorporated. For example, the fact that bananas are yellow is not often explicitly mentioned in text, but incorporating visual information derived from images can capture this semantic aspect. This is a technical nuance tied to how distributional models are constructed, but it is also relevant when considering human cognition—our mental representation of "banana" likely aggregates information from all the bananas we've seen, touched, tasted, etc. It would be beneficial to explicitly discuss this distinction, differentiating between multi-modal distributional semantics and the specific use of cross-modal mapping.
Regarding the "all models perform similarly" comment: I strongly encourage you, if the paper is accepted, to present this conclusion explicitly, even if it does not fully align with your initial hypotheses or objectives (you have enough other results that do). This phrasing provides a more accurate and community-relevant description of the findings than emphasizing marginal differences (whether statistically significant or not). For instance, if one bridge has a 49% chance of collapsing and another a 50% chance, the statistical significance of the difference does not make the first bridge a safer choice.
On a minor note, could you consider revising the title to make it more concise and focused? A title that reflects either the general exploration or the specific findings might be more effective.
---
SUMMARY OF THE PAPER:
This paper addresses a compelling topic that the authors term "referential word meaning," which refers to the relationship between a word's meaning and the referents (external objects) it applies to. If I understand correctly, the authors argue that this differs from traditional word meaning representations (e.g., those derived from distributional methods). Specifically, they distinguish between the abstract "lexical meaning" of a word and the appropriateness of a label for a particular referent with specific properties (context is deliberately excluded in this study). This hypothesis builds on prior work by Schlangen and colleagues (cited in the paper). The authors empirically investigate referential word meaning through a specific variation of the Referential Expression Generation (REG) task: generating the correct noun for a visually represented object.
---
STRENGTHS:
1. The problem addressed is highly engaging. As the authors argue, REG has traditionally been tackled using symbolic methods, which do not easily allow for an exploration of how speakers select object names. The research has implications beyond REG, as it examines the broader connection between semantic representations and reference.
2. The paper effectively leverages modern techniques and datasets (e.g., cross-modal mapping, word classifiers, and the ReferIt dataset with human-generated referring expressions) to address the research question.
3. The study includes a substantial number of experiments and analyses, which provide valuable insights.
---
WEAKNESSES:
1. The primary weakness lies in the articulation of the specific hypothesis being tested. I found the hypothesis confusing, making it difficult to interpret how the results inform the initial research question. The numerous results presented do not all align, further complicating the interpretation.
   The paper states:  
   "This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g., as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels."
   The first part of this hypothesis is unclear. What does it mean to "fully integrate" visual and lexical knowledge? Is the goal simply to demonstrate that generic distributional representations perform worse than dataset-specific, word-adapted classifiers? If so, the authors should explicitly discuss the limitations of this finding. For example, word classifiers must be trained on the dataset itself and are only feasible for words with sufficient dataset coverage, whereas word vectors are widely available and derived from independent sources (even if the cross-modal mapping is dataset-trained). Additionally, the authors use Ridge Regression rather than the optimal method from Lazaridou et al. (2014), so any conclusions about method superiority should be approached cautiously. However, I hope the research goal is broader and more constructive. Please clarify.
2. The originality of the paper is somewhat limited, as it employs three pre-existing methods on a previously available dataset. The problem itself has also been explored in prior work (e.g., by Schlangen et al.).
3. The authors restrict their analysis to a small subset of the ReferIt dataset, resulting in a limited vocabulary (159 words). The rationale for this restriction is unclear (see detailed comments below).
4. Several aspects of the paper could be explained more clearly (see detailed comments).
5. While the paper presents numerous empirical results and analyses, I found it challenging to grasp the overarching narrative. What do the experiments collectively reveal about the broader research question and the specific hypothesis? How do the various findings fit together into a coherent picture?
---
GENERAL DISCUSSION (AFTER AUTHOR RESPONSE):
Despite its weaknesses, the paper addresses a highly relevant and novel topic. It applies modern techniques to an "old" problem—REG and reference more broadly—in a way that enables the exploration of underexamined aspects. The experiments and analyses represent a significant contribution, though I would appreciate a clearer narrative tying the results together and addressing the central research question.
---
DETAILED COMMENTS:
- Section 2: Include the following related work on computational semantic approaches to reference:  
  - Abhijeet Gupta et al. (2015). Distributional vectors encode referential attributes.  
  - Aurélie Herbelot and Eva Maria Vecchi (2015). Building a shared world: Mapping distributional to model-theoretic semantic spaces.
- Line 142: How does Roy's work extend early REG research?  
- Line 155: "focusses links"—rephrase for clarity.  
- Line 184: Clarify "flat 'hit @k metric.'"  
- Section 3: Provide dataset statistics in a table, including image regions, referring expressions, total words, and object names for both the original and restricted ReferIt datasets. Will you release your data? The paper indicates "Yes" for data availability, but I could not find this information.  
- Line 229: Rephrase "cannot be considered to be names" as "image object names."  
- Line 230: Clarify "the semantically annotated portion" of ReferIt.  
- Line 247: Why not retain head nouns like "girl" in non-relational referring expressions? More generally, explain the rationale for the restricted subset of ReferIt.  
- Line 258: Specify the seven features used and their extraction process.  
- Line 383: Explain how the results of Frome et al. (2013) and Norouzi et al. (2013) support the claim about cross-modal projection.  
- Line 394-395: Relocate these numbers to the data section.  
- Table 1: Are the differences statistically significant? The numerical differences are minimal, so the conclusion that "methods perform similarly" seems more appropriate. Also, explain the counterintuitive hit @5 results for sim-wap.  
- Section 5.2: Why use a manually defined ensemble classifier instead of learning it? Does the method amount to majority voting?  
- Table 2: Ensure consistent model ordering across tables and text.  
- Table 3: Report cosine similarities instead of distances for clarity and alignment with standard practice. Discuss the right-column results, particularly the higher gold-top k data similarities for transfer+sim-wap.  
- Line 496: Correct the formatting of "wac."  
- Section 6: Clarify the task described in lines 550-553 with an example.  
- Line 558: Correct "Testsets."  
- Line 574ff: Why not include hypernyms and non-hypernyms in the training set?  
- Line 697: Specify what "more even" refers to.  
- Line 774ff: Clarify the claim about previous cross-modal mapping models.  
- Line 792: Before moving to other datasets, consider fully exploiting ReferIt by using more of its data.
---