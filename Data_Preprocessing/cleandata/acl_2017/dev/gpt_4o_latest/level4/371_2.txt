This paper introduces a novel phrasal RNN (pRNN) architecture for sequence-to-sequence generation. The proposed architecture is evaluated on two tasks: (i) language modeling using the PTB and FBIS datasets and (ii) Chinese-English machine translation on the NIST MT02-08 evaluation sets. The pRNN architecture is realized by generating subnetworks for phrases.
---
Strengths
- Introduction of a new phrasal RNN architecture.
---
Weaknesses
Technical
1. The paper does not clearly specify whether there is a limit on the phrase length in the pRNN. If such a limit exists, it should be explicitly stated, as it significantly impacts the model. For instance, if the maximum phrase length equals the sentence length, the model could be simplified into a convolutional RNN where each RNN state passes through a convolution layer before a final softmax and attention mechanism.
2. If a phrase length limit is imposed, the system becomes more tractable. However, this would imply that phrases are determined by token n-grams, creating a sliding window of "pyramid encoders" for each sentence. In such cases, parameters for certain phrases might be set close to zero to disable them. This could serve as an intrinsic evaluation metric for the pRNN, complementing the extrinsic evaluation based on perplexity and BLEU scores.
3. The attention mechanism at the phrasal level might face scalability issues without proper pruning. While the authors describe a form of greedy pruning in the caption of Figure 4, a fixed set of phrase pairs at training time could allow pre-computation of attention. However, during inference, applying attention to new data might become problematic, especially when scaling to larger datasets.
Empirical
1. The choice of training and evaluation datasets for the language modeling experiment is suboptimal. A dataset like Common Crawl or enwiki8 might be more appropriate for such experiments.
2. The experimental design and results reporting require significant improvement:
   - The evaluation on PTB (Table 2) is unfair, as the model is trained on a larger corpus (FBIS) and then tested on PTB. The reported LSTM perplexity of 106.9 (trained on FBIS) is lower than the baseline perplexity of 126 from previous studies, suggesting an advantage due to the training corpus.
   - In Section 3.3, the authors should cite relevant publications for the "previous work" listed in the tables. Additionally, it is unclear whether the previous work used the same training set.
   - The GRU version of pRNN is not reported for the FBIS evaluation in Table 3. This omission should be addressed.
3. The results section lacks sufficient explanation:
   - Tables presenting BLEU and perplexity scores (e.g., Table 2) require detailed interpretation. For instance, the authors should explain why the LSTM perplexity from prior work is higher than their implementation.
   - Table 4 results do not align with the description in Section 4.3. The claim that pRNN outperforms both PBSMT and Enc-Dec models is inaccurate. The authors should clarify that performance varies across evaluation sets, and pRNN performs better only on averaged test scores.
   - The "Test Avg." in Table 4 should specify whether it is a micro-average (concatenating all test sets into one) or a macro-average (averaging individual test set scores). Additionally, the statistical significance of BLEU improvements should be reported using tools like [multeval](https://github.com/jhclark/multeval).
---
General Discussion
Since the primary contribution of this work is the phrasal aspect of the new RNN architecture, it is crucial to demonstrate that the generated phrases are more coherent than those produced by vanilla LSTM/RNN models. BLEU evaluation alone is insufficient. A more detailed analysis of phrases on a subset of the evaluation set is necessary to substantiate the claims.
1. Does the baseline system (GroundHog) include an attention mechanism?
   - If yes, this should be explicitly stated in Section 4.2 and Table 4.
   - If no, the attention layer after the encoder in Figure 5 should be removed. The absence of attention in the baseline enc-dec system places it at an unfair disadvantage compared to the pRNN, which uses multiple phrasal attention layers. This discrepancy raises questions about whether pRNN outperforms or merely complements an enc-dec system with attention.
2. A simpler approach to implementing a phrasal RNN might involve applying an average pooling layer to the "pyramid" RNNs of a phrase.
---
Minor Issues
1. Figure 2 appears redundant, as Figure 1 suffices for comparison with the pRNN (Figures 3 and 4). Consider removing Figure 2.
2. Figures 3 and 4 could be combined to illustrate the pyramid structure, freeing space for a more detailed explanation of the results section.
3. Avoid overloading figure/table captions with verbose descriptions. Move detailed explanations for Figures 3, 4, 5, and Table 4 into the main text.
4. Reduce spacing between equations where possible (e.g., in LaTeX, use `\vspace{-5mm}`).