This paper proposes novel configurations and training objectives for neural sequence models in a multi-task learning (MTL) framework. As the authors effectively explain, the multi-task setting is significant because certain tasks share information, and in some cases, learning multiple tasks simultaneously can lead to improved overall performance.
The methods section is reasonably clear and logically structured, and I appreciate the direction it ultimately takes, though its organization could be slightly refined. Upon reading, I recognized two key challenges: 1) shared features inadvertently entering the private feature space, and 2) private features leaking into the shared space. The authors propose one novel method to address each of these issues. Presenting this organization upfront would enhance the cohesiveness of the methods section. Specifically, the paper introduces a method to prevent task-specific features from contaminating the shared representation (via adversarial loss) and another to ensure shared features do not infiltrate task-specific representations (via orthogonality constraints). However, I found the adversarial system somewhat confusing. After the LSTM output, there is an additional layer, \( D(s^kT, \thetaD) \), which depends on parameters \( U \) and \( b \). This layer's output is treated as a probability distribution and compared against the actual distribution. My concern is that the system might simply learn \( U \) and \( b \) in a way that masks task-specific information from the LSTM outputs, without guaranteeing that task-specific information is genuinely removed.
Before delving into the evaluation section, I outlined my expectations for the experiments, and the paper largely met them. The proposed idea is intriguing, and while there are numerous additional experiments one could envision, the paper provides the foundational experiments necessary to validate the methods. Including the best-known results for these tasks would further strengthen the evaluation.
My primary critique of the paper lies in its lack of deeper motivation for the proposed approach. It is straightforward to understand that a fully shared model might face issues due to conflicts in the feature space. The transition to a shared-private model seems like a natural response to this problem—one would anticipate that shared latent spaces capture useful shared information, while task-specific spaces learn features unique to each task. While this reasoning is intuitive, the authors appear to assume that the shared-private model inherently suffers from the issues they aim to address. I expected the argument to follow a clearer progression: 1) Fully shared models obviously encounter feature space conflicts; 2) shared-private models appear to mitigate this issue; 3) in practice, shared-private models do not fully resolve this problem due to reasons a, b, and c; 4) the authors propose a method to more effectively constrain the feature spaces. Table 4 partially clarifies the shortcomings of shared-private models and the improvements introduced by the proposed methods. For example, certain terms tend to align with specific connotations, which likely causes them to enter the shared feature space. Including a simple explanation, a concrete example, and a more structured argument flow in the introduction would significantly enhance the paper's readability and impact.
Lastly, I believe this work is related to some uncited MTL research [1], which explores deep hierarchical MTL, where lower-level tasks (e.g., POS tagging) are supervised at lower layers, and higher-level tasks (e.g., chunking, CCG tagging) are supervised at higher layers. That work concludes that MTL is effective only "when tasks are sufficiently similar." This paper reminded me of that prior research because the proposed model could potentially learn what "sufficiently similar" means—i.e., if two tasks are not sufficiently similar, the shared model might learn little and default to training independent systems. This contrasts with a shared-private baseline, which might overfit and perform poorly in such cases.
[1]  
@inproceedings{sogaard2016deep,  
  title={Deep multi-task learning with low level tasks supervised at lower layers},  
  author={S{\o}gaard, Anders and Goldberg, Yoav},  
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},  
  volume={2},  
  pages={231--235},  
  year={2016},  
  organization={Association for Computational Linguistics}  
}