AFTER AUTHOR RESPONSE
I acknowledge the authors' response regarding the emphasis on the novelty of the task and comparison with prior work. I also increase my ratings for the dataset and software, which are promised to be made public before the article's publication.
======================
GENERAL  
This paper provides an intriguing empirical comparison of three referring expression generation (REG) models. The primary novelty lies in the inclusion of a yet-to-be-published model, SIM-WAP (currently in press by Anonymous). While the model is outlined in SECTION 4.3, it remains unclear whether it has been extended or modified in any way for the purposes of this paper.  
The paper's novelty could be interpreted as the comparative evaluation of the unpublished SIM-WAP model against two existing models. However, this complicates the assessment of novelty since similar experiments have already been conducted for the other two models. It is also unclear why this comparative analysis was not included in the original paper introducing the SIM-WAP model. A potentially significant contribution could be the combined model, but this is neither clearly stated nor sufficiently detailed in the paper.  
The contributions of the paper can be summarized as follows: a side-by-side comparison of three REG methods; an analysis of zero-shot experiment results, which largely align with prior findings; and an exploration of the complementarity of the combined model.  
WEAKNESSES  
The paper's novelty and the significance of its contributions are unclear. The work appears to be an experimental extension of the cited Anonymous paper, where the main method was initially introduced.  
Another limitation is the small vocabulary size used in the zero-shot experiments, which seems to be the most significant part of the contribution.  
Additionally, the authors did not provide significance scores for their accuracy results. Including these would have strengthened the empirical contribution, which is the paper's primary value.  
Overall, the paper seems more suitable for a conference focused on empirical methods, such as EMNLP.  
Finally, I could not find any links to usable software. The work relies on existing datasets.  
Observations by Sections:  
ABSTRACT  
"We compare three recent models" – Later in the abstract, you mention experimenting with a combination of approaches. In Section 2, you state, "we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning." This phrasing might better summarize the novelty of the paper and highlight its contributions.  
I recommend revising the abstract (and potentially other sections of the paper) to focus on the novel model and results, rather than merely stating that you compare existing models.  
INTRODUCTION  
- "Determining such a name is is" – typo.  
- "concerning e.g." -> "concerning, e.g.,"  
- "having disjunct extensions." – Please specify or provide an example.  
- "building in Figure 1" -> "building in Figure 1 (c)"  
SECTION 4  
- "Following e.g. Lazaridou et al. (2014)," – Omit "e.g."  
SECTION 4.2  
- "associate the top n words with their corresponding distributional vector" – What values of N were used? If experiments were conducted to determine optimal values, please describe them. The choice of top N = K is not self-evident, and it is unclear why this would be optimal (e.g., why not consider similar vectors for each of the top 5 in the top 20?).  
SECTION 4.3  
- "we annotate its training instances with a fine-grained similarity signal according to their object names." – Please provide an example.  
LANGUAGE  
The draft contains several typos. The language should be polished (e.g., "as well such as").  
Additionally, I recommend using American English spelling (e.g., "summarise" -> "summarize"). Please confirm with your conference track chairs.