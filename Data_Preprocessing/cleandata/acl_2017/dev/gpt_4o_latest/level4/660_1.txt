The paper introduces two distinct methodologies for generating English poetry. The first method integrates a neural phonetic encoder, which predicts the next phoneme, with a phonetic-orthographic HMM decoder that determines the most probable word corresponding to a sequence of phonemes. The second method combines a character-based language model with a weighted FST to enforce rhythm constraints on the language model's output. Additionally, for the second approach, the authors propose a heuristic mechanism that allows the generated poetry to be constrained by themes (e.g., love) or poetic devices (e.g., alliteration). The generated poems are evaluated using both intrinsic and extrinsic metrics: intrinsically, by comparing the rhythm of the generated lines to a gold standard, and extrinsically, through a study involving 70 human evaluators who (i) assessed whether the poem was human- or machine-generated and (ii) rated the poems based on readability, form, and evocation. The results demonstrate that the second model outperforms the first and that human evaluators often struggle to distinguish between human-written and machine-generated poetry.
This is a well-written and engaging paper that presents innovative ideas (two distinct models for poetry generation: one leveraging a phonetic language model and the other a character-based language model) and provides compelling results.
However, the evaluation section could benefit from additional clarity regarding the evaluators and the protocol. For instance, did all evaluators assess all poems? If not, how many judgments were collected per poem for each task? Furthermore, the paper mentions nine non-native English speakers among the evaluators. Given the inherent difficulty of reading poetry, how fluent were these participants?
Regarding the second model (character-based), it is unclear whether a mechanism is in place to prevent the generation of non-words. If such a mechanism is absent, how frequently do non-words appear in the generated poems?
For the first model, why was an HMM chosen for transliteration from phonetic to orthographic representation instead of a CRF?
Since the first model is ultimately deemed unsuitable as a general-purpose poetry generation model, it might have been more effective to allocate less space to its description and instead focus more on evaluating the second model. Specifically, a deeper discussion of the heuristic used to constrain themes or poetic devices would have been valuable. How do these constraints affect the evaluation results? Could they be combined to simultaneously constrain both theme and poetic devices?
Finally, the combination of a neural model with a WFST is reminiscent of prior work that combines a character-based neural model for generating text from dialog acts with a WFST to prevent the generation of non-words. The authors should consider relating their work to this study and citing it:
Natural Language Generation through Character-Based RNNs with Finite-State Prior Knowledge  
Goyal, Raghav; Dymetman, Marc; Gaussier, Eric; LIG, Uni  
COLING 2016