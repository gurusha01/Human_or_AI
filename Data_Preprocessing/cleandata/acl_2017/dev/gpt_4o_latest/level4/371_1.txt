The paper introduces an approach to learn phrasal representations and integrate them into RNN-based language models and neural machine translation systems.
Strengths:
- The concept of incorporating phrasal information into the task is intriguing.
Weaknesses:
- The paper's description is difficult to follow. Proofreading by a native English speaker would enhance clarity.
- The evaluation of the proposed approach has notable shortcomings.
General Discussion:
- In Equations 1 and 2, the authors describe a phrase representation that produces a fixed-length word embedding vector. However, this is not utilized in the model, as the representation is instead generated using an RNN. What is the purpose of this description?
- Why is a GRU used for the Pyramid structure and an LSTM for the sequential component? Is the combination of these two architectures responsible for the observed improvements?
- What is the simplified version of the GRU, and why does it perform better? How does it perform on a larger dataset?
- In Table 4, what distinguishes RNNsearch (groundhog) from RNNsearch (baseline)?
- What is the rationale for focusing solely on ending phrases rather than also considering starting phrases?
- Did you exclusively use the pyramid encoder? How does it perform in isolation? This would provide a fairer comparison, as adding complexity typically benefits the model.
- Why was RNNsearch run multiple times, whereas PBNMT was only run once?
- Section 5.2: What is the purpose of this section?