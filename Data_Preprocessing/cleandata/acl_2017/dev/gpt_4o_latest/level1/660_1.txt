Review of the Paper: Automatic Generation of Rhythmic Poetry
Summary and Contributions
This paper presents two novel methodologies for the automatic generation of rhythmic poetry. The first approach involves a neural language model trained on phonetic encodings to implicitly learn both poetic form and content. The second approach frames poetry generation as a constraint satisfaction problem, where a generative language model handles content, and a discriminative weighted finite state machine (WFST) enforces form constraints. The authors demonstrate that their models can generate poetry that adheres to specific rhythmic and structural constraints, such as Iambic Pentameter, and even produce thematically coherent poems. A large-scale human evaluation indicates that participants were unable to reliably distinguish machine-generated poetry from human-written poetry, with one machine-generated poem rated as the best overall.
The primary contributions of this work are:
1. Constrained Character-Level Model: The introduction of a two-step pipeline that separates content generation (via a character-level LSTM) from form enforcement (via WFSTs). This approach allows for flexible generation of poetry in arbitrary forms and themes, overcoming the limitations of corpus-specific training.
2. Phonetic-Level Model: A neural language model trained on phonetic encodings to learn poetic devices like rhyme and rhythm. While limited in generalization, this model demonstrates the feasibility of phonetic-level representations for generating structured poetry.
3. Human Evaluation of Generated Poetry: A rigorous extrinsic evaluation showing that machine-generated poetry can be indistinguishable from human-written poetry, with some poems rated as highly aesthetic and emotive.
Strengths
1. Novelty of the Constrained Model: The separation of content and form in the constrained character-level model is a significant advancement. By using WFSTs to enforce rhythmic constraints, the model achieves flexibility and control over the generated poetry, enabling the creation of diverse poetic forms without retraining.
2. Rigorous Evaluation: The authors conducted a large-scale human evaluation, which not only tested the indistinguishability of machine-generated poetry but also assessed its quality in terms of readability, form, and emotional evocation. The results are compelling, showing that machine-generated poetry can rival human poetry.
3. Thematic and Poetic Device Control: The ability to introduce themes and poetic devices (e.g., alliteration, assonance) through heuristic boosting is a practical and effective way to enhance the aesthetic quality of generated poetry.
4. Technical Depth: The paper demonstrates a strong technical foundation, with detailed descriptions of phonetic encoding, transliteration, and WFST-based constraint modeling. The use of Expectation Maximization to derive stress patterns from latent structures is particularly innovative.
Weaknesses
1. Limited Generalization of the Phonetic Model: While the phonetic-level model is an interesting contribution, its reliance on corpus-specific training limits its applicability to novel poetic forms. This limitation is acknowledged by the authors but reduces the overall impact of this contribution.
2. Evaluation Bias: The human evaluation focuses on "nonsense verse" for human poetry, which may not represent the full spectrum of poetic quality. A broader comparison with more semantically rich human poetry could provide a more comprehensive assessment of the models' capabilities.
3. Grammatical Errors in Output: Despite the strong rhythmic adherence, some generated poems exhibit grammatical inconsistencies, which detract from their overall quality. This issue is particularly evident in the phonetic-level model outputs.
4. Lack of Thematic Complexity: While the thematic boosting approach is effective, it remains heuristic and does not capture deeper semantic or narrative coherence. This limits the ability to generate poetry with complex, multi-layered themes.
Questions to Authors
1. How does the constrained character-level model perform when generating poetry in non-English languages or with less rigid rhythmic structures?
2. Could the phonetic-level model be improved by incorporating morpheme-level representations, as suggested in the conclusion?
3. How scalable is the WFST-based constraint mechanism for longer poetic forms or free verse, where constraints may be less rigid?
Conclusion
This paper makes substantial contributions to the field of automatic poetry generation, particularly through the constrained character-level model and its ability to enforce rhythmic and thematic constraints. While the phonetic-level model is less impactful due to its limited generalization, the overall work is technically robust and demonstrates significant progress in generating human-like poetry. The paper is well-written, and the evaluation is thorough, though there is room for improvement in addressing grammatical errors and thematic complexity. I recommend acceptance, as the work represents a meaningful step forward in computational creativity.