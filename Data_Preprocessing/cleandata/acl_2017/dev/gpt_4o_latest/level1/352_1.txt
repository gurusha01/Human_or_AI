Review of the Submitted Paper
Summary and Contributions  
This paper proposes an adversarial multi-task learning (MTL) framework aimed at improving the separation of shared and task-specific feature spaces in neural network models for text classification. The key innovation lies in the use of adversarial training and orthogonality constraints to ensure that shared features are task-invariant and private features remain task-specific. The authors evaluate their approach on 16 diverse text classification tasks and demonstrate its effectiveness in reducing error rates compared to existing MTL baselines. Additionally, the paper explores the transferability of shared knowledge to unseen tasks, showcasing the utility of the learned shared representations.
The main contributions of the paper, as I interpret them, are as follows:
1. Adversarial Training for Feature Separation: The introduction of adversarial training to purify the shared feature space by preventing task-specific contamination is a novel application in the MTL context.
2. Orthogonality Constraints: The use of orthogonality constraints to minimize redundancy between shared and private feature spaces is a significant methodological contribution.
3. Transferability of Shared Knowledge: The demonstration of how the learned shared layer can be used as off-the-shelf knowledge for new tasks provides practical value and broadens the applicability of the proposed framework.
Strengths  
1. Novel Methodology: The combination of adversarial training and orthogonality constraints is innovative and addresses a critical limitation of existing shared-private MTL models. This approach is theoretically well-motivated and empirically validated.
2. Comprehensive Evaluation: The experiments are conducted on 16 datasets, which is a commendable effort to demonstrate the generalizability of the proposed method. The results consistently show improvements over strong baselines, including SP-MTL and FS-MTL.
3. Transfer Learning Insights: The exploration of shared knowledge transfer to unseen tasks is a valuable addition, highlighting the practical utility of the proposed framework.
4. Qualitative Analysis: The visualization of neuron behaviors and the analysis of shared versus private patterns provide intuitive insights into the model's functioning, complementing the quantitative results.
Weaknesses  
1. Limited Comparison with Recent MTL Methods: While the paper compares its approach with SP-MTL, FS-MTL, MT-CNN, and MT-DNN, it does not include comparisons with more recent or advanced MTL models that may have emerged in the literature. This omission weakens the claim of state-of-the-art performance.
2. Scalability Concerns: The proposed framework introduces additional computational overhead due to adversarial training and orthogonality constraints. The paper does not provide a detailed analysis of the computational cost or scalability of the method, which could be a concern for larger datasets or tasks.
3. Hyperparameter Sensitivity: The paper mentions a grid search for hyperparameter tuning but does not discuss the sensitivity of the model to these hyperparameters. This could impact the reproducibility and robustness of the results.
Questions to Authors  
1. How does the proposed framework scale with larger datasets or tasks with significantly more classes? Have you evaluated its computational efficiency compared to simpler baselines?  
2. Could you provide more details on the sensitivity of the model to the hyperparameters λ and γ? How robust is the performance across different settings?  
3. Are there specific types of tasks or datasets where the proposed method underperforms? If so, what are the potential reasons?
Conclusion  
Overall, this paper presents a novel and well-executed approach to improving multi-task learning through adversarial training and orthogonality constraints. The results are compelling, and the proposed method has practical implications for both MTL and transfer learning. However, the paper would benefit from broader comparisons with recent methods and a deeper analysis of computational efficiency and hyperparameter sensitivity. I recommend this paper for acceptance, provided the authors address the noted weaknesses during the rebuttal phase.