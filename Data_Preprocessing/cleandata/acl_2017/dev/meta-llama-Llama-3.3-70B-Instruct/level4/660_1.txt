The paper proposes two distinct methods for generating English poetry. The first approach integrates a neural phonetic encoder, which predicts the next phoneme, with a phonetic-orthographic Hidden Markov Model (HMM) decoder that computes the most likely word corresponding to a sequence of phonemes. In contrast, the second approach combines a character language model with a weighted Finite State Transducer (FST) to impose rhythm constraints on the language model's output. Additionally, the authors introduce a heuristic method for the second approach, allowing for the generation of poems constrained by theme (e.g., love) or poetic devices (e.g., alliteration). The generated poems are evaluated both intrinsically, by comparing the rhythm of generated lines to a gold standard, and extrinsically, through a survey of 70 human evaluators who assess whether a poem was written by a human or a machine and rate poems based on readability, form, and evocation. The results indicate that the second model outperforms the first and that human evaluators struggle to distinguish between human-written and machine-generated poems.
This article is interesting and well-written, presenting novel ideas, including two different models for poetry generation, one based on a phonetic language model and the other on a character language model, along with convincing results.
However, for the evaluation, more detailed information about the evaluators and the protocol would be beneficial. Specifically, it would be helpful to know whether all evaluators assessed all poems and, if not, how many judgments were collected for each poem for each task. The mention of 9 non-English native speakers raises questions about their fluency, given that poems can be notoriously difficult to read.
Regarding the second model (character-based), it is unclear whether there is a mechanism in place to prevent the generation of non-words. If such a mechanism is absent, it would be useful to know the frequency of non-words in the generated poems.
In the first model, the choice to use an HMM for transliterating from phonetic to orthographic representation, rather than a Conditional Random Field (CRF), warrants explanation.
Given that the first model is ultimately deemed less suitable as a generic model for generating poetry, it might have been more engaging to allocate less space to this model and more to the evaluation of the second model. A more detailed discussion on the impact of the heuristic used to constrain themes or poetic devices would be particularly interesting. Understanding how these constraints affect evaluation results and whether they can be combined to jointly constrain theme and poetic devices would provide valuable insights.
The combination of a neural model with a WFST is reminiscent of previous work, such as the paper "Natural Language Generation through Character-Based RNNs with Finite-State Prior Knowledge" by Goyal, Dymetman, and Gaussier (COLING 2016), which combines character-based neural models for generating dialogue acts with a WFST to avoid generating non-words. Relating this work to the referenced paper and including a citation would enhance the context and significance of the proposed methods.