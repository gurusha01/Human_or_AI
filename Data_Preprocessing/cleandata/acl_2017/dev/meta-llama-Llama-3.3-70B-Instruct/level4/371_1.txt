This paper proposes a novel approach to learning phrasal representations and integrating them into RNN-based language models and neural machine translation systems.
The strengths of this work include:
The incorporation of phrasal information into the task is a noteworthy concept.
However, several weaknesses are identified:
The text is difficult to comprehend, and having a native English speaker proofread the manuscript would enhance clarity.
The evaluation methodology employed in this approach has multiple shortcomings.
In the general discussion:
The phrase representation described in Equations 1 and 2 as a fixed-length word embedding vector is not utilized in the model, which instead relies on an RNN for representation generation. The purpose of including this description is unclear.
The use of GRU for the Pyramid and LSTM for the sequential part raises questions - is the combination of these two architectures responsible for the observed improvements?
Furthermore, the simplified version of the GRU is reported to perform better, but the reasons behind this and its performance on large datasets are not provided.
Table 4 lists RNNsearch (groundhog) and RNNsearch (baseline), but the distinction between these two is not explained.
The motivation behind exclusively using ending phrases, as opposed to starting phrases, is not justified.
It is also unclear whether the pyramid encoder was used in isolation and how it performed, as this would provide a more equitable comparison by adding complexity to the model.
Additionally, the rationale behind running RNNsearch multiple times while only running PBNMT once is not provided.
Regarding Section 5.2, the intent and purpose of this section are not clearly stated.