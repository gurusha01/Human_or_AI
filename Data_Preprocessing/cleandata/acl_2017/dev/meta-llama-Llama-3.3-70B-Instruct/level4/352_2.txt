Paper summary
This manuscript proposes a novel approach to learning shared and task-specific feature spaces for LSTM text classifiers, utilizing multiclass adversarial training to encourage generic representations that cannot be identified by a discriminative classifier. The authors evaluate several models, including a fully-shared, shared-private, and adversarial shared-private (ASP) model, with the latter being a key contribution. Additionally, they employ orthogonality constraints to promote distinct shared and private spaces. The ASP model demonstrates a lower error rate compared to single-task and other multi-task neural models. The authors also conduct task-level cross-validation experiments, which suggest that the shared representation can be effectively transferred across tasks. Furthermore, an analysis of shared layer activations indicates that the ASP model is not dominated by strong weights learned on a specific task.
Review summary
The paper presents a well-structured and well-tested set of ideas. While there are some minor issues that require attention, the overall quality of the work is high.
Strengths
* The authors have introduced a compelling set of ideas that work well together, with a particular focus on creating useful shared representations, which have been successfully applied in the computer vision community but require more effort to adapt to NLP.
* Sections 2, 3, and 4 are exceptionally clear and well-expressed.
* The task-level cross-validation in Section 5.5 provides a robust evaluation of the transferability of the shared representation.
* The availability of an implementation and data is a significant strength.
Weaknesses
* There are several minor typographical and phrasing errors throughout the manuscript, including:
  + An unusual example on line 84, which may benefit from clarification.
  + Minor errors in wording, such as "are different in" instead of "differ in" (line 233), "working adversarially towards" instead of "working against" or "competing with" (line 341), and "two matrics" instead of "two matrices" (line 434).
  + Number agreement errors in Section 6 (lines 745, 746, 765, 766, 767, and 770) require careful editing.
  + The shading in the final row of Tables 2 and 3 appears unusual when printed.
* The mention of unlabelled data in Table 1 and semi-supervised learning in Section 4.2 is not supported by corresponding results, which may indicate an omission or a misunderstanding.
* While the error rate differences in Tables 2 and 3 are promising, statistical significance testing would strengthen the conclusions, particularly when comparing SP-MLT and ASP-MTL results to highlight the benefits of adversarial training. Applying a non-parametric approximate randomisation test, as described in the referenced paper, could provide a straightforward solution.
* The colours used in the caption of Figure 5 (b) appear inconsistent with those used in Figure 5 (a), which may cause confusion.
General Discussion
* The authors may want to explore the connection between the proposed approach and regularisation techniques, as the adversarial training with orthogonal constraints can be seen as a means to limit the shared feature space, similar to the effect of regularisation. Drawing parallels with existing regularisation literature could provide additional insights and context.