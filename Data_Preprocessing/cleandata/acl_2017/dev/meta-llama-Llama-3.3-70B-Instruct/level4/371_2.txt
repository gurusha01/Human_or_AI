This paper presents a novel phrasal Recurrent Neural Network (RNN) architecture for sequence-to-sequence generation tasks. The proposed architecture, referred to as phrasal RNN (pRNN), is evaluated on two primary tasks: language modeling on the Penn Treebank (PTB) and Facebook's AI Research (FBIS) datasets, and Chinese-English machine translation on the NIST MT02-08 evaluation sets. The pRNN architecture is achieved by generating subnetworks of phrases, which is a unique approach to sequence-to-sequence modeling.
Strengths
====
The introduction of a new phrasal architecture is a significant contribution to the field of sequence-to-sequence modeling.
Weaknesses
====
Technical: 
One technical concern is that the paper does not clearly specify whether there is a limit set on the phrase length of the pRNN. This limitation is crucial, as it affects the model's complexity and performance. If the largest phrase length is the sentence length, the model can be simplified into a convolutional RNN, where each state of the RNN goes through a convolutional layer before a final softmax and attention. However, if there is a limit set on the phrase length, it makes the system more tractable, allowing for the evaluation of phrases using token n-grams, which can produce a sliding window of "pyramid encoders" for each sentence.
The use of an attention mechanism without pruning may be problematic at the phrasal level. Although the authors have implemented a greedy pruning approach, as described in the caption of Figure 4, it is unclear how this will perform when the architecture is scaled to larger datasets. Pre-computing the attention mechanism at the phrasal level during training may not be feasible during inference, especially when dealing with new data.
Empirical: 
One issue with the language modeling experiment is the choice of evaluation and training sets. Using a dataset like Common Crawl or enwiki8 might be more suitable for language modeling experiments. The main concern with the paper is the presentation of experiments and results, which requires significant reworking.
The evaluation on PTB (Table 2) is not entirely fair, as the model was trained on a larger corpus (FBIS) and then tested on PTB. The fact that the previous study reported a 126 perplexity baseline using LSTM, and the LSTM's perplexity of 106.9 provided by the authors, suggests that the FBIS dataset gives an advantage to computing the language model's perplexity when tested on PTB. Additionally, the authors should cite relevant publications for the "previous work" presented in the tables and clarify whether the previous work used the same training set.
The result section lacks explanation and discussion. For instance, in Table 2, it is necessary to explain why the LSTM's perplexity from previous work is higher than the authors' implementation. Similarly, in Table 3, the authors should provide more context and discussion about the results.
The results presented in Table 4 do not match the description in Section 4.3. The authors should clarify that the pRNN outperforms both PBSMT and Enc-Dec models only on certain evaluation sets and that it is the averaged test scores that show the pRNN performing better. Furthermore, the authors should specify whether the "Test Avg." is a micro-average or macro-average score.
General Discussion
====
As the primary contribution of this work is the phrasal effect of the new RNN architecture, it is essential to demonstrate that the phrases are more coherent than those produced by vanilla LSTM/RNN models. The BLEU evaluation alone is insufficient; a more in-depth analysis of the phrases in a subset of the evaluation set is necessary to support the claims.
It is unclear whether the baseline system (Groundhog) includes an attention mechanism. If it does, the authors should provide more details in Section 4.2 and Table 4. If not, the attention layer after the encoder in Figure 5 should be removed, as the lack of attention mechanism may give the baseline Enc-Dec system an unfair disadvantage.
A simpler approach to achieving phrasal RNN might be to use average pooling layers to combine the "pyramid" RNNs of a phrase.
Minor Issues 
====
Figure 2 is somewhat redundant, as Figure 1 is sufficient for comparison with the pRNN (Figures 3 and 4). Figure 3 could be combined with the pyramid part of Figure 4 to free up space for a more detailed explanation of the results. The authors should avoid using verbose captions for figures and tables and instead include the descriptions in the main text. Additionally, the spacing between equations can be reduced using LaTeX commands such as \vspace{-5mm}.