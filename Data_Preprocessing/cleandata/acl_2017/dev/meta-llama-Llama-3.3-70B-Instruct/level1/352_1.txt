This paper proposes an adversarial multi-task learning framework for text classification tasks. The main contributions of this work are:
1. Adversarial multi-task learning framework: The authors introduce an adversarial training strategy to learn task-invariant features that are shared across multiple tasks, while preventing task-specific features from contaminating the shared space.
2. Orthogonality constraints: The authors propose orthogonality constraints to penalize redundant latent representations and encourage the shared and private extractors to encode different aspects of the inputs.
3. Extension of binary adversarial training to multi-class: The authors extend the binary adversarial training to multi-class, enabling multiple tasks to be jointly trained.
The strengths of this paper are:
1. Effective use of adversarial training: The authors demonstrate the effectiveness of adversarial training in learning task-invariant features and improving the performance of multi-task learning models.
2. Improved performance on text classification tasks: The authors show that their proposed model outperforms several baseline models on 16 different text classification tasks.
3. Qualitative analysis: The authors provide qualitative analysis and visualization of the learned features, which provides insights into the effectiveness of their proposed model.
The weaknesses of this paper are:
1. Complexity of the model: The proposed model involves multiple components, including adversarial training, orthogonality constraints, and multi-task learning, which may make it challenging to train and tune.
2. Limited interpretation of the results: While the authors provide some qualitative analysis, it is not clear how the learned features relate to the specific tasks and datasets used in the experiments.
3. Lack of comparison to other multi-task learning models: The authors only compare their proposed model to a few baseline models, and it is not clear how it performs compared to other state-of-the-art multi-task learning models.
Questions to authors:
1. How do the authors plan to extend their proposed model to other NLP tasks, such as sequence labeling or machine translation?
2. Can the authors provide more detailed analysis of the learned features and how they relate to the specific tasks and datasets used in the experiments?
3. How do the authors plan to address the complexity of the proposed model and make it more efficient to train and tune?