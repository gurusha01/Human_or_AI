Summary of the Paper
The paper proposes a novel framework called Phrasal Recurrent Neural Networks (pRNNs) for language modeling and machine translation. pRNNs store the sentential history as a set of candidate phrases with different lengths that precede the word to predict, and employ a soft attention mechanism to selectively combine the suggestions of candidate phrases. The model is evaluated on language modeling and machine translation tasks, and achieves significant improvements over state-of-the-art baselines.
Main Contributions
1. pRNN framework: The paper introduces a new framework for language modeling and machine translation that explicitly models task-specific nested phrases from plain text.
2. RNN pyramid: The paper proposes a novel architecture called RNN pyramid, which consists of parallel RNN sequences that capture the history of all subsequence of words that precede the word to predict.
3. Attention mechanism: The paper employs a soft attention mechanism to selectively combine the suggestions of candidate phrases, which allows the model to focus on the most relevant phrases when predicting the next word.
Strengths
1. Improved performance: The paper demonstrates significant improvements over state-of-the-art baselines on language modeling and machine translation tasks.
2. Novel architecture: The paper proposes a novel architecture that explicitly models task-specific nested phrases from plain text, which is a significant departure from traditional RNN-based language models.
3. No external resources required: The paper shows that the model does not require any external resources such as human-labeled data or word alignment models to construct the phrases.
Weaknesses
1. Computational complexity: The paper notes that the RNN pyramid architecture can be computationally expensive, which may limit its applicability to large-scale datasets.
2. Limited evaluation: The paper only evaluates the model on two datasets (PTB and FBIS), which may not be representative of all language modeling and machine translation tasks.
3. Lack of analysis: The paper does not provide a detailed analysis of the learned phrases and their impact on the model's performance, which may be an interesting direction for future work.
Questions to Authors
1. How do the learned phrases differ from traditional phrase-based models, and what are the implications for language modeling and machine translation?
2. Can the RNN pyramid architecture be applied to other NLP tasks, such as text classification or sentiment analysis?
3. How does the model's performance change when using different attention mechanisms or RNN architectures?