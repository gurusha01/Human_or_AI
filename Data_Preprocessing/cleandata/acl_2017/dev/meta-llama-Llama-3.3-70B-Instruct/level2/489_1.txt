Summary of the Paper
This paper compares three models of referential word meaning that link visual object representations to lexical representations in a distributional vector space. The models are: (1) Direct Cross-Modal Mapping (TRANSFER), which maps visual representations to linguistic representations; (2) Lexical Mapping Through Individual Word Classifiers (WAC), which trains separate visual classifiers for each word; and (3) Word Prediction via Cross-Modal Similarity Mapping (SIM-WAP), which combines ideas from cross-modal mapping and WAC. The paper evaluates these models on a standard object naming task and a zero-shot naming task, where the correct name is not seen during training.
Main Contributions
1. The paper proposes a new approach to modeling referential word meaning by combining visual and lexical information.
2. The paper evaluates the performance of three different models on a standard object naming task and a zero-shot naming task.
3. The paper shows that combining the predictions of different models can improve performance on object naming tasks.
Strengths
1. The paper provides a thorough evaluation of the three models on two different tasks, providing insights into their strengths and weaknesses.
2. The paper shows that the SIM-WAP model is particularly effective in the zero-shot naming task, suggesting that it is able to capture referential meaning more effectively than the other models.
3. The paper provides a detailed analysis of the results, including examples of objects where the combination of models is accurate, and discusses the implications of the findings for referring expression generation (REG) systems.
Weaknesses
1. The paper uses a relatively small vocabulary, which may limit the generality of the conclusions.
2. The paper does not provide a detailed comparison with other state-of-the-art models for object naming and zero-shot learning.
3. The paper could benefit from additional evaluation metrics, such as human evaluation of the generated object names.
Questions to Authors
1. How do the authors plan to scale up the findings to larger test sets, and what are the potential challenges and limitations of doing so?
2. Can the authors provide more details on the implementation of the SIM-WAP model, and how it is able to capture referential meaning more effectively than the other models?
3. How do the authors plan to extend the approach to include contextual information during object naming, and what are the potential benefits and challenges of doing so?