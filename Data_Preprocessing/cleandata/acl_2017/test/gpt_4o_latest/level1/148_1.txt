Review of the Submission
Summary and Contributions  
This paper investigates the quality of reading comprehension (RC) datasets by introducing two classes of evaluation metrics: prerequisite skills and readability. The authors analyze six existing RC datasets (e.g., MCTest, SQuAD) using these metrics, revealing insights into the relationship between text readability and question difficulty. The study demonstrates that readability does not necessarily correlate with question difficulty and highlights the potential to design RC datasets that are easy to read but challenging to answer. The authors also refine the classification of prerequisite skills for RC, incorporating insights from psychology and textual entailment research, and annotate six datasets with these metrics, making the results publicly available. The key contributions of this work are:  
1. Adoption of two evaluation metric classes to analyze RC datasets, revealing a weak correlation between readability and question difficulty.  
2. Refinement of prerequisite skills for RC, integrating psychological and linguistic insights.  
3. Annotation of six RC datasets with the proposed metrics, providing a resource for future research.  
Strengths  
1. Novel Dataset Analysis Framework: The introduction of two complementary evaluation metrics—prerequisite skills and readability—is a significant contribution. This framework provides a nuanced understanding of RC dataset quality, moving beyond traditional metrics like accuracy or question types.  
2. Insightful Findings: The paper's finding that readability and question difficulty are weakly correlated is both novel and impactful. This insight challenges assumptions in RC dataset design and opens avenues for creating datasets that are both accessible and rigorous.  
3. Comprehensive Dataset Annotation: The annotation of six diverse RC datasets with the proposed metrics is a valuable resource for the community. The public release of these annotations enhances the reproducibility and utility of the work.  
4. Theoretical Grounding: The refinement of prerequisite skills is well-grounded in psychological and linguistic theories, lending credibility to the proposed metrics.  
5. Practical Implications: The study offers actionable insights for RC dataset construction and system development, such as using the metrics to design stepwise curricula for RC systems.  
Weaknesses  
1. Limited Scope of Dataset Selection: While six datasets are analyzed, the exclusion of widely used datasets like CNN/Daily Mail and CBTest limits the generalizability of the findings. The reasons for exclusion (e.g., errors, task misalignment) are valid but could have been addressed with additional effort.  
2. Subjectivity in Annotation: The annotation process, while thorough, relies on human annotators, which introduces subjectivity. The paper does not sufficiently discuss how disagreements were resolved or how annotation consistency was ensured.  
3. Lack of System Evaluation: Although the metrics are intended to guide RC system development, the paper does not evaluate any RC systems using these metrics. This omission weakens the practical applicability of the proposed framework.  
4. Overemphasis on Readability Metrics: While readability is a key focus, the paper does not explore how these metrics align with machine comprehension capabilities. For example, readability metrics designed for humans may not directly translate to machine performance.  
5. Limited Discussion of Annotation Challenges: The paper briefly mentions issues like "nonsense" questions and "no answer" questions but does not propose solutions or methodologies to address these challenges.  
Questions to Authors  
1. How were disagreements among annotators resolved during the annotation process? Could you provide more details on inter-annotator agreement?  
2. Have you considered evaluating RC systems using your annotated datasets to validate the utility of the proposed metrics?  
3. Could the readability metrics be adapted to better reflect machine comprehension capabilities?  
Overall Assessment  
This paper makes a valuable contribution to the evaluation of RC datasets through its novel metrics and comprehensive analysis. However, the lack of system evaluation and limited dataset scope slightly diminish its impact. Addressing these weaknesses in future work could significantly enhance the utility and generalizability of the proposed framework.