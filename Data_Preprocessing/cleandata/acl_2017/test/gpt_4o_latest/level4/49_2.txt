- Summary:
This paper presents a chunk-level architecture for existing Neural Machine Translation (NMT) models. The authors propose three models to capture the correlation between word-level and chunk-level representations on the target side within NMT frameworks.
- Strengths:
The paper is well-structured and clearly articulates the proposed models and their contributions.
The proposed models for integrating chunk information into NMT are novel and well-justified. These models have the potential to be broadly applicable across various language pairs.
- Weaknesses:
There are a few minor issues that need to be addressed, as outlined below:
1) Figure 1: It is surprising that function words outnumber content words in a Japanese sentence. Apologies if this stems from my lack of familiarity with Japanese.
2) In all equations, sequences/vectors (such as matrices) should be denoted in bold to distinguish them from scalars, e.g., hᵢ, xᵢ, c, s, etc.
3) Equation 12: Replace sj with sj-1.
4) Line 244: All encoder states should be explicitly referred to as bidirectional RNN states.
5) Line 285: The phrase "non-sequential information such as chunks" is somewhat unclear. Aren't chunks still considered sequential information?
6) Equation 21: This is slightly confusing. For clarity, consider inserting k into s₁(w), such as s₁(w)(k), to indicate the word within a chunk.
7) Questions about the experiments:
   - Table 1: Could you provide source language statistics?
   - For the baselines, why not include a baseline that does not incorporate chunk information, rather than relying solely on the (Li et al., 2016) baseline (where |V_src| differs)? This would make it easier to isolate the impact of chunk-based models. Did (Li et al., 2016) and the other baselines use identical pre-processing and post-processing steps? The baselines seem not entirely comparable. Even after the authors' response, I still believe that while (Li et al., 2016) can serve as a reference, a baseline from the existing model without chunk information should also be included.
   - Figure 5: Including baseline results would enhance the comparison. Were the chunks in the translated examples generated automatically by the model or manually by the authors? Could you compare the number of chunks generated by the model with those produced by the bunsetsu-chunking toolkit? If so, chunk statistics for the Dev and Test sets in Table 1 would be necessary. By the way, the authors' response did not fully address this concern.
8) I am somewhat surprised by the beam size of 20 used during decoding. A large beam size might bias the model toward generating shorter sentences.
9) Past tense should be used when describing the experiments. For instance:
   - Line 558: "We use" → "We used".
   - Lines 579-584: "We perform" → "We performed"; "We use" → "We used".
   - ...
- General Discussion:
Overall, this is a strong and well-executed study—the first to explore chunk-based NMT. It is a significant contribution to the field and deserves to be accepted at ACL.