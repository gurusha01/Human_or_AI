Paraphrased Review
- Strengths:
This paper introduces a compelling extension to attention-based neural machine translation (NMT) models by incorporating source-sentence chunking as an additional source of information. The architecture is adapted such that the chunking information is utilized differently across two recurrent layers: one layer is responsible for generating a chunk at a time, while the other focuses on producing the words within each chunk. This approach is novel and intriguing, and I believe it will capture the interest of readers who are keen to explore innovative methods and their performance.
The manuscript is exceptionally well-written, with clear explanations and thorough comparisons to alternative approaches. The evaluation is well-executed, directly comparing the proposed method to prior work through detailed tables and analyses. While there is room for improvement (as noted in my comments below), the results are convincing and add value to the field.
- Weaknesses:
As is often the case, the experimental section could be expanded to further solidify the argument for chunk-based models. For instance, while Table 3 demonstrates strong results for Model 2 and Model 3 compared to prior work, a discerning reader might question whether these gains are primarily due to the switch from LSTMs to GRUs. To address this, it would be beneficial to include results for a GRU-based tree-to-sequence model to confirm that the chunk-based approach itself is responsible for the improvements.
Another notable limitation is the absence of ensembling results. The authors emphasize that their model is the best single NMT model to date, which is likely accurate. However, the best WAT system for English-to-Japanese translation achieves a BLEU score of 38.20 (if I've interpreted the table correctly) using a 3-model ensemble. If the authors were to demonstrate that a 3-way ensemble of their chunk-based model surpasses this benchmark, the paper's impact would be significantly enhanced.
Lastly, Table 3 would be more informative if it included decoding times. The authors briefly mention that the character-based model is less time-intensive (presumably referencing Eriguchi et al., 2016), but they do not provide a citation or specific figures. Additionally, no decoding speed metrics are reported for the chunk-based model. Is it faster or slower than word-based decoding? Comparable? Including a column in Table 3 with decoding times would add valuable context to the results.
- General Discussion:
Overall, I find this paper to be both interesting and worthy of publication. I have a few minor suggestions for improving the clarity and presentation, which I outline below:
1. The authors should explicitly state early in the paper that the chunks are supplied externally, meaning the model does not learn to chunk on its own. This only became clear to me upon reading about CaboCha on page 6, but it is a critical detail that should be introduced earlier.
2. The frequent comparisons to the character-based baseline in the text (e.g., the +4.68 BLEU improvement mentioned multiple times) seem unnecessary. Readers are likely more interested in the performance gains over the strongest baseline rather than the character-based one.
3. It would be helpful to provide more detail about how unknown tokens (UNKs) are handled by the neural decoder. Alternatively, the authors could include a citation for the dictionary-based replacement strategy they are using.
4. The sentence on line 212 ("We train a GRU that encodes a source sentence into a single vector") is not entirely accurate. A more precise description would be that a bidirectional encoder is used to encode the source sentence into a set of vectors, as depicted in Figure 2.
5. The motivating example in lines 69â€“87 is somewhat unclear. Does "you" depend on "bite," or does it depend on the source-side context? If the dependency is not on "bite," then the argument that this is a long-dependency issue does not seem to hold.