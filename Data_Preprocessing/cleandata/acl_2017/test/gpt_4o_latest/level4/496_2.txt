Paraphrased Review
Strengths:
- This paper presents experiments aimed at tackling a critical challenge in NMT: understanding the extent to which the model captures morphology, syntax, and related linguistic features.  
- The objectives are clearly defined, and the experiments are well-structured. The paper provides a solid review of the state of the art and offers meaningful comparisons. Overall, it is an engaging and well-written manuscript.  
- The experimental framework is robust. POS and morphological classifiers are trained using the outputs of the encoder/decoder recurrent layers. The authors demonstrate how specific modifications to the framework (e.g., using character-based inputs instead of word-based ones) impact classifier accuracy.  
- The experiments are conducted across multiple language pairs.  
- The conclusions drawn from the work are thought-provoking and, in some cases, counterintuitive.  
Weaknesses:
- The comparison between character-based and word-based representations feels somewhat incomplete. Given the strong performance of byte-pair encoding (BPE) in the literature, it would have been more relevant to include BPE in the experiments or to replace word-based representations if including all three was impractical.  
- Section 1 states, "â€¦ while higher layers are more focused on word meaning," which is echoed in Section 7. While this intuition is plausible, the experiments presented in the paper do not directly support this claim. The authors should either omit this statement or explicitly clarify that it is a reasonable hypothesis based on indirect evidence (e.g., translation performance improves, but higher layers do not enhance morphological accuracy).  
Discussion:
This is an excellent paper that provides a comprehensive and systematic analysis of NMT models, offering several intriguing insights based on extensive data across multiple language pairs. I found the following observations particularly compelling: (a) the target language influences the quality of source-side encodings; specifically, when the target language is morphologically simple (e.g., English), the encoder's POS tagging accuracy improves; (b) increasing the encoder's depth does not enhance POS tagging accuracy, suggesting further experiments are needed to determine what benefits deeper encoders bring; and (c) the attention mechanism negatively impacts the quality of decoder representations.  
I am curious whether (a) and (c) might be interconnected. If the attention mechanism degrades decoder representations, this could make it harder to learn representations for morphologically rich target languages. Consequently, the encoder might adapt based on the global objective, with this adaptation backpropagating through the decoder. Would this not strongly suggest the need for distinct objectives to guide the encoder and decoder components of the NMT model?