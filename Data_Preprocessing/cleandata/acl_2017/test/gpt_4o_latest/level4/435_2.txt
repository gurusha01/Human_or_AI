This paper introduces a method for detecting causal relationships between clauses using neural networks ("deep learning"), though the networks employed are not particularly deep, as is common in many studies. While certain discourse connectives unambiguously signal causal relations (e.g., 'because'), the authors leverage a recent dataset (AltLex, by Hidey and McKeown, 2016) to address the challenge of identifying causal versus non-causal relations when no explicit markers are present. The authors argue that convolutional networks are less effective at capturing the relevant features of clauses compared to LSTMs. Consequently, they propose a classification architecture that uses Glove-based clause representations as input to an LSTM layer, followed by three densely connected layers with tanh activations, and a final softmax decision layer.
The proposed system achieves an improvement of 0.5-1.5% F1 over the 2016 SVM-based system by Hidey and McKeown. The paper provides examples of generalizations where the system performs well, such as correctly identifying indicator words as non-causal in the test data despite their consistent causal usage in the training data. This combination of qualitative and quantitative analysis is commendable.
The paper is well-written, with a particularly clear description of the problem. However, it would benefit from a clarification of how this task differs from implicit connective recognition. A discussion of why prior methods for implicit connective recognition are not applicable here would also be valuable.
The authors' decision to upload their code with the submission is highly appreciated, as is the inclusion of the older dataset, which provides useful examples. However, the meaning of the 0-1-2 coding in the TSV files is unclear, given that the task is described as binary classification. Additionally, it is worth questioning whether the authors have the right to repost the dataset from Hidey and McKeown. Another point that requires clarification is the term "bootstrapping," which is mentioned as extending the corpus by about 15%. While the corpus construction is briefly but clearly explained, the details of this additional bootstrapping process are not provided.
Although experimenting with neural networks for this task is interesting, the contributions of the proposed system are not entirely convincing. It appears that the best configuration (out of 4-7 options) is selected based on the test data, and this configuration is reported as achieving a 2.13% F1 improvement over Hidey and McKeown. A fairer comparison would involve selecting the best configuration on the development set.
Furthermore, the significance of the reported improvement is unclear. Given the dataset size, statistical significance metrics should be computed. Additionally, the reliability of the gold-standard annotations should be considered, potentially with input from the dataset creators. Upon inspection, the annotations derived from English/Simple English Wikipedia are imperfect, which suggests that the reported scores should be interpreted cautiously.
Finally, while neural methods are known to outperform human-engineered features for binary classification tasks, the results primarily confirm this established trend. It would be useful to include experiments with simpler baseline models, such as a single-layer LSTM. The results analysis could also delve into why the neural method appears to favor precision over recall.