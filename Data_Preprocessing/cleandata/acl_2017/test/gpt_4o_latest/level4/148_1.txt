Strengths:
- This paper bridges two distinct fields: human text readability and machine comprehension of texts.
Weaknesses:
- The primary objective of the paper is not clearly articulated. Despite reading the paper multiple times, I still struggle to grasp its central premise.  
- The manuscript is ambiguous in its focus, oscillating between machine comprehension and human text readability.  
- Key prior work in the field of readability is missing from the references.  
- Section 2.2 includes a tangential discussion of theoretical topics that are not directly relevant.  
- The paper attempts to tackle too many questions simultaneously, which weakens its overall impact. For instance, the question of whether text readability affects RC datasets should be explored independently of prerequisite skills.  
General Discussion:
- The title is somewhat unclear and should explicitly indicate that the focus is on machine comprehension of text, as "reading comprehension" and "readability" are typically associated with human reading comprehension.  
- The claim that "dataset analysis suggested that the readability of RC datasets does not directly affect the question difficulty" is contingent on the method or features used for answer detection, such as POS or dependency parse features.  
- The paper requires thorough proofreading, as there are significant language issues. For example, on page 1, the sentence "the question is easy to solve simply look..." is incomplete.  
- The process of annotating datasets with "metrics" is not well explained.  
- The paper conflates machine reading comprehension and human reading comprehension, which, while related, are distinct fields with substantial differences.  
- The term "readability of text" is not synonymous with "difficulty of reading contents." For clarification, refer to DuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact Information.  
- The paper would benefit from clearer distinctions between its work and the readability of questions for humans, as the current presentation is highly ambiguous. For instance, on page 1, the statement "These two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions" should specify "for machine comprehension."  
- Section 3.1 raises similar concerns: Are the skills being discussed applicable to humans or machines? If the focus is on machines, why are human-focused studies being cited, and how confident are you that these studies are relevant to machines?  
- How many questions were annotated, and were the annotators explicitly instructed to evaluate the questions with machines in mind rather than humans?