The paper introduces a convolutional neural network (CNN) approach to model text coherence. This method builds upon the established entity grid representation for coherence, incorporating a CNN as an additional layer.
The approach is well-justified and clearly articulated. I particularly value the detailed explanation of the rationale behind specific design choices, such as the use of CNNs and the insights provided in the section titled "Why it works."
The paper includes a comprehensive evaluation across multiple tasks, demonstrating that the proposed method outperforms prior approaches. However, it is concerning that one prior result could not be replicated. Specifically, the results on Li and Hovy (2014) suggest a potential implementation or modeling error that warrants further investigation.
That said, the model essentially represents a straightforward "neuralization" of the entity grid framework. I found it unclear why 100-dimensional vectors are required to encode a four-dimensional grid entry (or slightly more in the case of the extended grid). What specific advantage does this bring? While I agree that directly optimizing for coherence ranking could enhance the model, the difference in transition chains for up to k=3 sentences versus k=6 may not have a significant impact, especially given the brevity of many WSJ articles.
The writing could benefit from conciseness, as certain sections are repetitive. For instance, the introduction to entity grids is reiterated in multiple places. Notably, section 2 already covers related work, making the first two-thirds of section 6 redundant. These portions should either be removed or integrated into section 2 where relevant. The remaining content in section 6 could be appended to section 2 under a new subsection, with section 2 renamed as "Related Work."
In summary, this paper presents a competent application of neural network techniques to the entity-grid-based coherence model. However, given the emphasis on consolidating prior work, I would have expected more substantial contributions, such as innovations in feature representations or the exploration of novel tasks.
Minor Points:
- The paper would benefit from proofreading by a native English speaker, as several articles are missing, e.g., "the WSJ corpus" (2x), "the Brown ... toolkit" (2x), etc.
- Page 1, bottom left column: "Figure 2" should be "Figure 1."
- Page 1: Replace "Firstly/Secondly" with "First, Second."
- Page 1: Rephrase "limits the model to" as "prevents the model from considering ..."
- Consider removing the standard concluding paragraph in section 1, as it is unnecessary for such a concise paper.