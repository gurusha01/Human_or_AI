This paper tackles the problem of lexical entailment detection in context, such as determining whether "chess" is a type of "game" based on sentences containing these wordsâ€”a task relevant to QA. The primary contributions of the paper are:
(1) the creation of a novel dataset derived from WordNet using synset exemplar sentences, and  
(2) the introduction of a "context relevance mask" for word vectors, achieved by elementwise multiplication with feature vectors derived from the context sentence. When these masked word vectors are input into a logistic regression classifier, they slightly outperform the state of the art on entailment prediction using a PPDB-derived dataset from prior work. When combined with other existing features, they achieve a modest improvement over the state of the art. On the newly introduced WordNet-derived dataset, the masked representations also outperform the baseline, though the best-performing method on this dataset does not rely on the masked representations.
The paper also proposes simple word similarity features (e.g., cosine and Euclidean distance) to complement other cross-context similarity features from prior work. While these similarity features collectively lead to significant classification improvements, the specific features introduced in this paper represent a relatively minor contribution.
The task is interesting, and the methodology appears sound, but the work is somewhat incremental. The approach for generating mask vectors is adapted from prior work on encoding variable-length sequences into min/max/mean vectors. However, their use as masks appears to be novel. That said, excluding the PPDB features, the best-performing results do not seem to rely on the representations introduced in this paper.
Some specific points of feedback:
1. Regarding the creation of the new Context-WN dataset, are there many false negatives caused by similar synsets in the "permuted" examples? For instance, if a word w has synsets i and j, is it guaranteed that the exemplar context for a hypernym synset of j is unsuitable as an entailment context for i? What happens if i and j are semantically close?
2. Why does the masked representation degrade classification performance when combined with context-agnostic word vectors (rows 3 and 5 in Table 3), given that row 1 performs so well? Shouldn't the classifier be able to disregard the context-agnostic features if they are unhelpful?
3. The paper should clarify which similarity measures are novel and which are adopted from prior work. The current description, which mentions that prior work used the "most salient" similarity features, is too vague to be helpful.
4. The paper should better delineate the contributions of the masked vectors versus the similarity features, as it appears that the similarity features are driving most of the performance gains.
5. The intuition behind the Macro-F1 measure and its connection to "sensitivity to changes in context" is unclear. What specific changes are being referred to, and how should Macro-F1 differ from standard F1 in this context?
6. The motivation for the cross-language task is underdeveloped.
Additionally, the paper is missing a relevant citation: "Learning to Distinguish Hypernyms and Co-Hyponyms" by Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir, and Bill Keller (COLING 2014).
==
After reviewing the author response, I maintain my earlier observations. A close examination of the tables indicates that the similarity features contribute the most to the improvement in F-score across both datasets (excluding the PPDB features). While the authors argue that the similarity features include contextualized representations, these features are a mix of both contextualized and non-contextualized representations. This distinction needs to be explored further, as acknowledged in the response.
Moreover, neither Table 3 nor Table 4 provides results using only the masked representations without the similarity features. This omission makes it difficult to isolate the specific contribution of the masked representations.