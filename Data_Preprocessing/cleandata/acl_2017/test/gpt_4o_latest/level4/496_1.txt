- Strengths: The authors provide a comprehensive exploration of various language settings to examine how linguistic relatedness and morphological complexity interact (e.g., translating between closely related morphologically rich languages versus distant ones) and influence what the system learns about morphology. They present a compelling analysis of which components of the architecture contribute to learning morphology, particularly focusing on how the attention mechanism results in less detailed target-side representations. Their findings are both highly relevant and practically valuable for the broader NMT community.
- Weaknesses: The paper lacks sufficient detail regarding the implementation of their character-based encoder. There are numerous approaches to learning character-based representations, and the absence of a discussion on their specific method raises questions about the broader applicability of their results. Additionally, the analysis could have been more impactful if the authors had selected languages with more complex and challenging morphological systems, such as Turkish or Finnish, and complemented this with a more granular examination of morphology prediction and analysis.
- General Discussion: This study sheds light on what NMT models learn about morphology by training NMT systems and leveraging encoder or decoder representations as input features for POS- or morphology-tagging classification tasks. The work builds on the methodology of "Does String-Based Neural MT Learn Source Syntax?" but applies it to the domain of morphology. The findings provide valuable insights into the morphological learning capabilities of NMT systems.