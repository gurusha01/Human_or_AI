This paper introduces an LSTM-based model designed to classify connective uses based on whether they signal an intended causal relation. The central premise is that the expression of causal relations is highly diverse and not easily captured by syntactic rules, making the abstract representations produced by neural models more appropriate for such tasks.
The experiments are conducted on the AltLex corpus developed by Hidey and McKeown. The results provide modest yet consistent evidence supporting the main idea and offer preliminary insights into how this concept can be effectively implemented in a model. The paper also includes the TensorFlow-based models used in the experiments.
Some critical comments and questions:
* The introduction is unconventional, resembling a literature review more than a comprehensive overview of the paper's content. This creates redundancy with the subsequent related work section. While a non-standard introduction can work, this one falls short: it reviews numerous ideas but fails to take a clear stance on what causation is or how it is expressed. Instead, it primarily focuses on the negative claim that causation cannot be reduced to syntax. The positive contribution is only hinted at in the final paragraph of the section, leaving the reader uncertain about the paper's main contribution.
* Expanding on the above, the paper does not clearly articulate the theory of causation it assumes. The authors seem to lean toward a counterfactual view akin to David Lewis's, where causation is framed as a modal sufficiency claim with additional counterfactual conditions. This is evident in line 238 and subsequent lines, where the arrow implies a specific type of implication. However, Lewis's theory has well-documented issues (see http://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). While the paper occasionally suggests the authors do not fully endorse the counterfactual view, it remains unclear what alternative theory is being assumed. It cannot simply be the temporal constraint mentioned on page 3.
* The comments regarding the example on line 256 are unclear. The authors appear to consider the sentence false. If it were true, there should be a causal link between the argument and the breakage. However, the paper does not address the broader issues of dividing events into sub-events, which are critical to causal theories. This omission leaves the discussion confusing.
* The caption for Figure 1 is misleading, as the diagram represents only the "Pair_LSTM" variant of the model. A larger issue is the diagram's lack of precision. While it is acceptable to omit certain details of the standard model in the text, the diagrams should have clear and consistent semantics. For instance, what do the empty circles between the input and "LSTM" boxes represent? The text mentions a look-up layer and a Glove layer, but what follows? How many layers of representation are there? Additionally, the "LSTM" boxes seem to correspond to the leftmost/final representation directly connected to the layers above, but this connection is not clearly depicted. I recommend revising the diagram to make these details explicit.
* The sentence beginning on line 480 is confusing. The models discussed do not inherently require padding. If padding is necessary due to TensorFlow or for efficient training, this should be explicitly stated. However, the final clause of the sentence is unclear. How does this issue relate to the question of "the most convenient way to encode the causal meaning"? The connection between convenience and causal meaning is not apparent.
* The authors find that using two independent LSTMs ("Stated_LSTM") performs slightly better than a setup where one LSTM feeds into the other. This finding parallels discussions in the natural language entailment literature, where researchers debate whether to represent premise and hypothesis independently or sequentially. This remains an open question for entailment and likely requires further exploration for causal relations as well. Therefore, I cannot fully endorse the claim on line 587: "This behaviour means that our assumption about the relation between the meanings of the two input events does not hold, so it is better to encode each argument independently and then to measure the relation between the arguments by using dense layers." This conclusion is surprising, given that subparts of a sentence often share significant information.
* The hyperparameters leading to the best performance across tasks are difficult to interpret. For example, compare line 578 with line 636. Should these differences be attributed to the unpredictable interactions between the model and the data, or is there a deeper explanation?
* Section 4.3 concludes by asserting that the system can "correctly disambiguate [the causal meaning of] 'which then'," whereas the model by Hidey and McKeown cannot. While this claim may be valid, a single example is insufficient to substantiate it. To strengthen this point, I recommend creating a diverse set of examples that exhibit the ambiguity and evaluating how often the system produces correct results. This would help determine whether the system's success in Table 8 was due to chance.