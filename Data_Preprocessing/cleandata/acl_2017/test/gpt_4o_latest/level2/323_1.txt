Review
Summary of the Paper
The paper proposes a novel neural model for assessing local text coherence using a convolutional neural network (CNN) applied to the entity grid representation of text. The model captures long-range entity transitions and incorporates entity-specific features, addressing limitations of traditional entity grid models such as discrete feature representation and task-agnostic feature extraction. The authors employ a pairwise ranking approach for end-to-end training, enabling the model to learn task-specific high-level features. The proposed model is evaluated on three coherence assessment tasks—discrimination, insertion, and summary coherence rating—achieving state-of-the-art results. The authors also release their code, enhancing reproducibility.
Main Contributions
1. Neuralizing the Entity Grid Model: The paper introduces a CNN-based architecture that operates on distributed representations of entity transitions, enabling the modeling of longer transitions without overfitting.
2. Task-Specific Feature Learning: The model employs an end-to-end training approach, allowing it to learn features tailored to specific coherence assessment tasks.
3. Empirical Validation: The model demonstrates significant improvements over traditional entity grid models and other baselines on three benchmark tasks, achieving state-of-the-art performance.
Strengths
1. Novelty and Innovation: The paper presents a significant advancement by neuralizing the entity grid model, addressing its key limitations such as the curse of dimensionality and task-agnostic feature extraction.
2. Strong Empirical Results: The model achieves consistent improvements across all three tasks, with statistically significant gains over non-neural counterparts. The results are robust, as evidenced by multiple runs and statistical tests.
3. Reproducibility: The release of the code and detailed hyperparameter settings enhance the reproducibility of the work, which is a critical strength in AI research.
4. Comprehensive Evaluation: The paper evaluates the model on diverse tasks, including sentence ordering (discrimination and insertion) and summary coherence rating, demonstrating its versatility.
5. Incorporation of Entity-Specific Features: The ability to integrate entity-specific features, such as named entity types, further strengthens the model's applicability to real-world tasks.
Weaknesses
1. Limited Scope of Entity-Specific Features: While the model incorporates some entity-specific features, it does not include all features from prior work (e.g., Elsner and Charniak, 2011), which could limit its performance in certain scenarios.
2. Task-Specific Training: The model is not directly trained on the insertion task, which may explain its relatively smaller gains in this task compared to discrimination. Training specifically for insertion could yield further improvements.
3. Limited Dataset Diversity: The evaluation primarily focuses on the Wall Street Journal (WSJ) corpus, which may not generalize to other domains or genres of text. Additional experiments on more diverse datasets would strengthen the claims.
4. Pretraining Dependency: For the summary coherence rating task, the model relies on pretraining from the discrimination task, which may not always be feasible in low-resource scenarios.
Questions to Authors
1. Could the model's performance on the insertion task improve if it were trained specifically for this task rather than relying on discrimination training?
2. Have you considered evaluating the model on more diverse datasets, such as those with conversational or informal text, to test its generalizability?
3. How does the model handle cases where entity-specific features are sparse or unavailable? Would this impact its performance significantly?
Overall Assessment
The paper makes a strong contribution to the field of text coherence modeling by introducing a neuralized entity grid model that addresses key limitations of traditional approaches. The proposed method is innovative, well-validated, and achieves state-of-the-art results across multiple tasks. While there are minor limitations, such as the reliance on pretraining for certain tasks and the limited diversity of datasets, these do not detract significantly from the overall impact of the work. The paper is well-written, clearly structured, and provides sufficient detail for reproducibility. I recommend acceptance.