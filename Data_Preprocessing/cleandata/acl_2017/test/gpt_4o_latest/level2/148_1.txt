Review of the Submitted Paper
Summary and Contributions
This paper addresses the quality evaluation of Reading Comprehension (RC) datasets by introducing two classes of evaluation metrics: prerequisite skills and readability. The authors apply these metrics to six existing datasets, including MCTest and SQuAD, to analyze their characteristics and the correlation between readability and question difficulty. The main contributions of the paper are as follows:
1. The adoption of two evaluation metrics—prerequisite skills and readability—to analyze RC datasets, revealing a weak correlation between readability and question difficulty.
2. A revised classification of prerequisite skills for RC, incorporating insights from psychology and textual entailment research.
3. Annotation of six RC datasets using the proposed metrics, with results made publicly available to guide the development of improved RC datasets and systems.
Strengths
1. Novel Evaluation Framework: The paper introduces a systematic framework for evaluating RC datasets, which is a significant step forward in understanding dataset quality. The use of metrics grounded in psychology and textual entailment research adds theoretical rigor.
2. Comprehensive Dataset Analysis: The authors analyze six diverse RC datasets, providing valuable insights into their characteristics. The finding that readability does not strongly correlate with question difficulty is particularly impactful, as it challenges common assumptions in dataset design.
3. Publicly Available Annotations: By making the annotated results available, the paper contributes to the broader research community, enabling reproducibility and further exploration of RC dataset quality.
4. Practical Implications: The study provides actionable insights for constructing better RC datasets, such as emphasizing the importance of narrative texts and diverse question types. This guidance is useful for researchers and practitioners in the field.
Weaknesses
1. Limited Scope of Datasets: While the paper analyzes six datasets, it excludes other prominent datasets like CNN/Daily Mail and LAMBADA, citing issues such as anonymization and formatting. This limits the generalizability of the findings.
2. Annotation Methodology: The annotation process relies on human annotators, but the paper does not provide sufficient details about inter-annotator disagreements or how they were resolved. This raises concerns about the reliability of the annotations.
3. Evaluation of Sentence Selection: The methodology does not evaluate a system's ability to identify relevant sentences for answering questions, which is a critical aspect of RC tasks. This omission reduces the completeness of the proposed evaluation framework.
4. Correlation Analysis: The reported correlations between readability and question difficulty are weak but not deeply explored. The paper could benefit from a more detailed discussion of why these correlations are weak and what other factors might influence question difficulty.
Questions to Authors
1. How were disagreements among annotators resolved during the annotation process? Could this have impacted the reliability of the results?
2. Could the exclusion of datasets like CNN/Daily Mail and LAMBADA have biased the findings? Are there plans to extend the analysis to these datasets in the future?
3. How do you envision the proposed metrics being integrated into automated dataset evaluation pipelines?
Additional Comments
This paper provides a valuable contribution to the field of RC dataset evaluation by introducing a novel framework and offering actionable insights. However, addressing the limitations in dataset selection, annotation methodology, and system evaluation would strengthen the work further. Encouragingly, the paper has significant potential to guide future research and dataset development.