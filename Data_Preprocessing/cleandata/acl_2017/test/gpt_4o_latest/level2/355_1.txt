Review
Summary and Contributions:  
This paper introduces a novel approach to Japanese Predicate Argument Structure (PAS) analysis using Grid-type Recurrent Neural Networks (Grid-RNNs). The authors aim to address the issue of error propagation caused by reliance on syntactic parsers in existing methods. The proposed model captures multi-predicate interactions directly from word sequence information without requiring syntactic input. The main contributions of the paper are:  
1. A single-sequence model based on bidirectional RNNs (Bi-RNNs) for PAS analysis, which learns context-sensitive representations from word sequences.  
2. A multi-sequence model leveraging Grid-RNNs to capture multi-predicate interactions, improving performance for zero arguments.  
3. Empirical results on the NAIST Text Corpus demonstrating state-of-the-art performance without syntactic information, particularly for challenging zero argument identification.
Strengths:  
1. Novelty of Approach: The use of Grid-RNNs to model multi-predicate interactions is a significant innovation over prior methods that rely on syntactic parsers. This approach is particularly effective for zero argument identification, a challenging aspect of Japanese PAS analysis.  
2. Empirical Validation: The experimental results are robust, showing consistent improvements over baseline models, including the state-of-the-art joint model by Ouchi et al. (2015). The multi-sequence model achieves statistically significant improvements, particularly for zero arguments and dative cases.  
3. Practical Implications: The model's ability to achieve high performance without syntactic information reduces dependency on error-prone parsers, making it more robust and potentially applicable to other languages and tasks, such as Semantic Role Labeling (SRL).  
4. Reproducibility: The authors provide implementation details and make their source code publicly available, facilitating reproducibility and further research.  
Weaknesses:  
1. Limited Scope of Evaluation: The paper focuses exclusively on intra-sentential arguments (Dep and Zero) and does not address inter-sentential arguments (Inter-Zero), which are acknowledged as more challenging but critical for comprehensive PAS analysis.  
2. Comparison with External Resource Models: While the authors compare their model to baselines using the NAIST Text Corpus 1.5, they do not benchmark against models that utilize external resources, which could provide a more comprehensive evaluation of the model's competitiveness.  
3. Interpretability: While the Grid-RNN architecture is effective, the paper lacks a detailed analysis of how the model captures multi-predicate interactions. Visualizations or qualitative examples could enhance interpretability and understanding of the model's behavior.  
4. Generalization to Other Languages: Although the authors suggest applicability to multilingual SRL tasks, no experiments are conducted to validate this claim, leaving its generalizability untested.
Questions to Authors:  
1. How does the model's performance compare when external resources (e.g., pre-trained embeddings) are incorporated?  
2. Could you provide qualitative examples or visualizations to illustrate how the Grid-RNN captures multi-predicate interactions?  
3. Have you considered extending the model to inter-sentential arguments (Inter-Zero), and if so, what challenges do you foresee?  
Conclusion:  
This paper presents a significant advancement in Japanese PAS analysis by introducing a syntactic-independent neural approach that achieves state-of-the-art results. While the work is well-executed and impactful, addressing the limitations in scope, interpretability, and generalization could further strengthen its contribution. I recommend acceptance, with minor revisions to address the weaknesses highlighted above.