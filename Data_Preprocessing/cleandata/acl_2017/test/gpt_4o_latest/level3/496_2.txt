Review
Summary and Contributions:  
This paper investigates what neural machine translation (NMT) models learn about morphology and syntax, focusing on part-of-speech (POS) and morphological tagging tasks. The authors propose a robust methodology that leverages encoder and decoder outputs to train classifiers, providing a quantitative evaluation of the linguistic representations learned by NMT systems. Key contributions include:  
1. Demonstrating that character-based representations significantly outperform word-based ones for learning morphology, especially for low-frequency words.  
2. Revealing that lower encoder layers capture word structure, while higher layers focus on word meaning.  
3. Highlighting the influence of the target language on source-side representations, with morphologically-poor target languages leading to better encoder POS tagging accuracy.  
4. Identifying that the attention mechanism negatively impacts decoder representation quality, particularly for morphologically-rich languages.  
Strengths:  
1. Critical Problem Addressed: The paper tackles an essential and underexplored question in NMTâ€”what linguistic features are learned during training. This is a significant step toward understanding and improving NMT systems.  
2. Clear and Rigorous Methodology: The experimental framework is well-structured and methodologically sound. The use of classifiers to evaluate representations is a thoughtful approach, and the experiments are conducted across multiple language pairs, enhancing the study's generalizability.  
3. Novel and Counterintuitive Insights: The findings, such as the negative impact of attention on decoder representations and the unexpected influence of morphologically-poor target languages, are both novel and thought-provoking. These insights could inspire future research on NMT architectures and objectives.  
4. Thorough Analysis: The paper provides a detailed analysis of various factors, including word frequency, encoder depth, and representation type (character vs. word), offering a comprehensive view of the problem.  
Weaknesses:  
1. Limited Comparison of Representations: The omission of byte-pair encoding (BPE), a widely-used subword representation, weakens the analysis. Including BPE would provide a more complete picture of representation quality in NMT.  
2. Unsubstantiated Claims: The claim that higher layers focus on word meaning is not directly supported by the experiments. This assertion should either be clarified with additional evidence or removed.  
3. Decoder Analysis Could Be Expanded: While the paper highlights the decoder's limitations, it does not explore potential solutions or alternative architectures to address these issues.  
Questions to Authors:  
1. Could you elaborate on why byte-pair encoding (BPE) was excluded from the analysis? Do you plan to include it in future work?  
2. How do you define and measure "word meaning" in the context of higher encoder layers? Can this claim be substantiated with additional experiments?  
3. Have you considered experimenting with deeper encoder architectures to further explore the relationship between depth and representation quality?  
Recommendation:  
This paper provides valuable insights into the linguistic representations learned by NMT models and addresses a critical gap in the literature. Despite some limitations, the strengths of the work outweigh its weaknesses. I recommend acceptance with minor revisions to address the unsubstantiated claims and expand the analysis to include BPE.