Review of the Submitted Paper
Summary and Contributions
This paper explores the intersection of text readability for humans and machine comprehension of texts, proposing two classes of evaluation metrics—prerequisite skills and readability—to analyze reading comprehension (RC) datasets. The authors annotate six existing RC datasets (e.g., MCTest, SQuAD) with these metrics and present insights into the relationship between text readability and question difficulty. The paper makes three main contributions: (1) it demonstrates a weak correlation between readability and question difficulty, suggesting that easy-to-read texts can still pose challenging questions; (2) it refines the classification of prerequisite skills for RC tasks, incorporating insights from psychology and textual entailment research; and (3) it provides annotated datasets and analysis results to guide future RC dataset construction and system development.
Strengths
1. Bridging Two Fields: The paper effectively bridges the domains of human text readability and machine comprehension, offering a novel perspective on RC dataset evaluation. This interdisciplinary approach is valuable for advancing both fields.
2. Comprehensive Dataset Analysis: The annotation and analysis of six RC datasets provide a useful comparative overview, highlighting strengths and weaknesses in existing datasets. For example, the identification of MCTest as an easy-to-read but challenging-to-answer dataset is insightful.
3. Refinement of Prerequisite Skills: The paper's reorganization of prerequisite skills, informed by psychological theories and textual entailment research, is a meaningful contribution that could improve the granularity of RC task evaluation.
Weaknesses
1. Unclear Goal and Focus: The paper's primary objective is ambiguous, and the distinction between human readability and machine comprehension is conflated throughout. For instance, the title and abstract fail to clarify whether the focus is on improving RC datasets for human or machine use.
2. Lack of Prior Work: The paper omits key references in the readability domain, such as foundational works on readability metrics and their application to NLP. This weakens the theoretical grounding of the readability analysis.
3. Unrelated Theoretical Discussions: Section 2.2 includes tangential discussions on human text comprehension theories, which do not directly contribute to the paper's goals and dilute its focus.
4. Annotation Methodology Issues: The process for annotating datasets with metrics is insufficiently detailed, particularly regarding how annotators were trained and whether they were clear about the machine-focused nature of the task.
5. Overextension: The paper attempts to address too many questions simultaneously—readability, question difficulty, dataset quality, and system development—resulting in a lack of depth in any single area.
Questions to Authors
1. Could you clarify the primary goal of the paper? Is the focus on improving RC datasets for machine comprehension, human readability, or both?
2. How were annotators trained to ensure consistency in applying the prerequisite skills and readability metrics? Were any inter-annotator agreement measures reported beyond the brief mention of 90.1%?
3. How does the paper's claim that "readability does not affect question difficulty" account for the specific features used in answer detection? Could this relationship vary across different RC models?
Additional Comments
The paper requires significant proofreading to address numerous grammatical errors and omissions. Additionally, the title should be revised to reflect the machine-centric focus of the study. The distinction between human and machine comprehension needs to be consistently maintained throughout the paper, as the current conflation undermines the clarity of the contributions. Finally, the authors should consider narrowing the scope of the paper to focus on a single, well-defined research question.