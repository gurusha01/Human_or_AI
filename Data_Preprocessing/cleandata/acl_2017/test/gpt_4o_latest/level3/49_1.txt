Review of the Paper
Summary and Contributions  
This paper introduces an innovative chunk-based decoder for neural machine translation (NMT), which incorporates hierarchical modeling of target sentences by leveraging chunk structures. The proposed approach addresses two key challenges in NMT: modeling long-distance dependencies and handling flexible word order in free word-order languages like Japanese. The authors propose three models: (1) a standard chunk-based NMT model with a chunk-level and word-level decoder, (2) an improved model with inter-chunk connections, and (3) a further enhanced model with word-to-chunk feedback. Experimental results on the WAT '16 English-to-Japanese translation task demonstrate that the best model outperforms all single NMT models reported in WAT '16. The paper's primary contributions are:  
1. Introducing chunk structures into NMT to improve translation performance.  
2. Designing a novel hierarchical decoder that explicitly models intra-chunk and inter-chunk dependencies.  
Strengths  
1. Innovative Methodology: The hierarchical chunk-based decoder is a novel and engaging approach that effectively tackles long-distance dependencies and flexible word order, making it highly relevant for free word-order languages.  
2. Strong Empirical Results: The proposed models achieve significant improvements in BLEU and RIBES scores over baseline and state-of-the-art single NMT models, demonstrating the effectiveness of the approach.  
3. Clarity and Comparisons: The paper is well-written, with clear explanations of the models and thorough comparisons to alternative approaches, including tree-to-sequence and character-based models. The qualitative analysis further illustrates the advantages of the proposed models.  
Weaknesses  
1. Impact of GRUs vs. LSTMs: The experimental results do not isolate the impact of switching from LSTMs to GRUs. Since the baseline tree-to-sequence model uses LSTMs, it is unclear whether the performance gains are due to the chunk-based approach or the choice of GRUs.  
2. Lack of Ensembling: While the proposed models outperform single NMT models, the lack of ensembling results weakens the claim of being the best NMT approach. Ensemble comparisons would provide stronger evidence of the model's superiority.  
3. Decoding Times: The paper does not report decoding times in Table 3, making it difficult to assess whether the chunk-based models are computationally efficient compared to word-based models.  
Questions to Authors  
1. Could you provide experimental results isolating the impact of GRUs versus LSTMs to validate the chunk-based model's effectiveness?  
2. Why were ensembling techniques not explored, and how do you anticipate they would affect the performance of the proposed models?  
3. Can you clarify whether the chunk structures are externally supplied during inference, and if so, how this affects the generalizability of the approach?  
Additional Comments  
- The motivating example in lines 69-87 is somewhat unclear and could be revised to better illustrate the long-dependency problem.  
- Overemphasis on gains over the character-based baseline may detract from the more important comparison with the best word-based baseline.  
- A correction is needed in line 212 to accurately describe the bidirectional GRU encoder.  
- Including decoding times and computational costs would strengthen the evaluation.  
Overall, this paper presents a compelling and well-executed contribution to NMT research, with potential for significant impact, particularly for free word-order languages. However, addressing the noted weaknesses would further solidify its claims and practical relevance.