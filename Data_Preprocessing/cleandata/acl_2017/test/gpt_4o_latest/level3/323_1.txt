Review of the Paper
Summary and Contributions:  
This paper proposes an extension to the entity grid model by integrating a convolutional neural network (CNN) to improve local coherence modeling. The key contribution lies in its ability to capture long-range entity transitions and incorporate entity-specific features while maintaining generalization through distributed representations. The authors present a pairwise ranking method for end-to-end training, enabling task-specific feature learning. The model is evaluated on three coherence assessment tasks—discrimination, insertion, and summary coherence rating—achieving state-of-the-art results. The paper builds on existing work rather than introducing a fully neural model, addressing limitations of prior approaches, such as the discrete representation of grammatical roles and task-agnostic feature extraction.
Strengths:  
1. Extension of the Entity Grid Model: The paper effectively extends the widely used entity grid model by leveraging CNNs to capture longer entity transitions, addressing a key limitation of traditional grid models. This is a significant improvement over prior approaches.  
2. Task-Specific Learning: The end-to-end training framework allows the model to learn task-specific features, which is a notable advancement over existing methods that decouple feature extraction from downstream tasks.  
3. Comprehensive Evaluation: The model is evaluated on three distinct tasks, demonstrating consistent improvements over non-neural counterparts and achieving state-of-the-art results. The inclusion of multiple datasets and tasks strengthens the validity of the findings.  
4. Incorporation of Entity-Specific Features: The ability to include entity-specific features without exponential growth in parameters is an important contribution, showcasing the flexibility of the proposed approach.  
5. Clarity and Structure: The paper is well-organized, with clear motivation, methodology, and experimental results. The authors provide sufficient technical details, making the work reproducible.
Weaknesses:  
1. Limited Comparison with Li and Hovy (2014): The paper does not provide a thorough comparison with the neural coherence model by Li and Hovy. The reported poor performance of their model on the WSJ dataset raises questions about the experimental setup and corpus dependency, which are not adequately addressed.  
2. Lack of Investigation into Li and Hovy's Model: The low performance of Li and Hovy's model is mentioned but not deeply analyzed. A more detailed examination of why their approach underperforms and its implications for coherence modeling would strengthen the paper.  
3. Proofreading Issues: While the paper is generally well-written, it contains minor grammatical errors, such as missing definite articles, and some expressions could be phrased more clearly. This detracts slightly from the overall readability.  
4. Corpus Dependency: The choice of the WSJ dataset over smaller, domain-specific corpora (e.g., AIRPLANES and EARTHQUAKES) is justified, but the generalizability of the results to other domains remains unclear.  
Questions to Authors:  
1. Could you elaborate on the experimental setup for Li and Hovy's model? Were there any specific challenges in reproducing their results?  
2. How does the model perform on smaller, domain-specific datasets like AIRPLANES and EARTHQUAKES? Would the results generalize to such constrained styles?  
3. Have you considered incorporating rhetorical relations or discourse structures into the current model? If so, how would this affect the architecture?  
Final Recommendation:  
I recommend acceptance of this paper with a score of 4. To further strengthen the work, I suggest the authors document the challenges faced in testing Li and Hovy's system and attempt to reproduce its results on the original datasets. Additionally, proofreading for grammatical errors and clearer phrasing would enhance the paper's presentation.