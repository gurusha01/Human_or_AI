This paper proposes a novel approach to neural machine translation (NMT) by incorporating chunk-based decoders. The main contributions of this work are:
1. Introduction of chunk information into NMT: The authors propose to use chunks (or phrases) as the basic translation unit, rather than individual words, to capture long-distance dependencies and handle free word-order languages like Japanese.
2. Hierarchical decoder architecture: The authors design a hierarchical RNN-like architecture, consisting of a word-level decoder and a chunk-level decoder, to model the chunk structure in the target language.
3. Improved translation performance: The authors demonstrate that their chunk-based decoders can significantly improve translation performance on the WAT '16 English-to-Japanese translation task, outperforming all single models reported in WAT '16.
The strengths of this paper are:
1. Novel approach to NMT: The authors propose a new approach to NMT that incorporates chunk-based decoders, which can capture long-distance dependencies and handle free word-order languages.
2. Improved translation performance: The authors demonstrate significant improvements in translation performance on the WAT '16 English-to-Japanese translation task.
3. Well-motivated architecture: The authors provide a clear motivation for their hierarchical decoder architecture and demonstrate its effectiveness in capturing the chunk structure in the target language.
The weaknesses of this paper are:
1. Limited evaluation: The authors only evaluate their approach on a single translation task (English-to-Japanese) and do not provide a comprehensive evaluation on other languages or tasks.
2. Complexity of the model: The authors' hierarchical decoder architecture may be more complex and difficult to train than traditional NMT models.
3. Lack of comparison to other chunk-based approaches: The authors do not provide a comparison to other chunk-based approaches to NMT, which would be useful to understand the strengths and weaknesses of their approach.
Questions to the authors:
1. How do the authors plan to extend their approach to other languages and tasks?
2. Can the authors provide more details on the training process and the hyperparameters used in their experiments?
3. How do the authors plan to address the complexity of their hierarchical decoder architecture and make it more efficient to train?