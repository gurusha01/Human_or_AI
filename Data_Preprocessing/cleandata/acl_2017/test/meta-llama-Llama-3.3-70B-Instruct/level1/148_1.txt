This paper presents a comprehensive analysis of reading comprehension (RC) datasets using two classes of evaluation metrics: prerequisite skills and readability. The authors refine the prerequisite skills proposed by Sugawara et al. (2017) and Sugawara and Aizawa (2016) and define thirteen skills, including object tracking, mathematical reasoning, and coreference resolution. They also adopt readability metrics based on linguistic features proposed by Vajjala and Meurers (2012). The authors annotate six existing RC datasets with these metrics and present the results, highlighting the characteristics of each dataset.
The main contributions of this paper are:
1. The adoption of two classes of evaluation metrics to analyze RC datasets, providing a comprehensive understanding of the datasets' quality and characteristics.
2. The refinement of prerequisite skills for RC, organizing them into thirteen skills that cover various aspects of text comprehension.
3. The annotation of six existing RC datasets with the defined metrics, making the results publicly available for future research.
The strengths of this paper are:
1. The thorough analysis of RC datasets using two classes of evaluation metrics, providing a detailed understanding of the datasets' characteristics.
2. The refinement of prerequisite skills for RC, which can be used to develop more effective RC systems.
3. The public availability of the annotated datasets, which can facilitate future research in RC.
The weaknesses of this paper are:
1. The limited number of datasets annotated, which may not be representative of all RC datasets.
2. The potential bias in the annotation process, as the authors relied on human annotators to select sentences and annotate prerequisite skills.
3. The lack of a clear methodology for evaluating the competence of selecting necessary sentences, which is an important aspect of RC.
Questions to authors:
1. How do the authors plan to address the potential bias in the annotation process, and what measures can be taken to ensure the reliability of the annotated datasets?
2. Can the authors provide more details on the calculation of sentence distance, and how it relates to the competence of selecting necessary sentences?
3. How do the authors plan to use the analysis in this study to construct a system that can be applied to multiple datasets, and what are the potential challenges and limitations of such an approach?