This paper presents a thorough analysis of the representations learned by neural machine translation (MT) models, with a focus on their ability to capture word structure and morphology. The authors investigate various aspects of the MT system, including the effect of word representation, encoder depth, target language, and attention mechanism on the quality of the learned representations.
The main contributions of this work are:
1. Character-based representations are superior for learning morphology: The authors show that character-based representations outperform word-based representations in learning morphology, especially for low-frequency words.
2. Lower layers of the encoder are better for capturing word structure: The authors find that lower layers of the encoder are more focused on word structure, while higher layers are better for learning word meaning.
3. Translating into morphologically-poorer languages leads to better source-side representations: The authors observe that translating into morphologically-poorer languages results in better source-side representations, which is partly correlated with BLEU scores.
The strengths of this paper include:
1. Comprehensive analysis: The authors conduct a thorough investigation of various aspects of the MT system, providing a detailed understanding of how the representations are learned.
2. Quantitative evaluation: The authors use a quantitative evaluation metric (POS and morphological tagging accuracy) to assess the quality of the learned representations.
3. Insights for future work: The authors provide valuable insights for future work, such as jointly learning translation and morphology, and extending the analysis to other representations and tasks.
The weaknesses of this paper include:
1. Limited scope: The authors focus primarily on the encoder and do not thoroughly investigate the decoder's role in learning morphology.
2. Lack of comparison to other methods: The authors do not compare their results to other methods for learning morphology, such as traditional machine learning approaches.
3. No clear conclusions for practical applications: The authors do not provide clear conclusions for how their findings can be applied in practice to improve MT systems.
Questions to authors:
1. How do the authors plan to extend their analysis to other representations, such as byte-pair encoding, and deeper networks?
2. Can the authors provide more insights into the decoder's role in learning morphology and how it can be improved?
3. How do the authors think their findings can be applied in practice to improve MT systems, particularly for morphologically-rich languages?