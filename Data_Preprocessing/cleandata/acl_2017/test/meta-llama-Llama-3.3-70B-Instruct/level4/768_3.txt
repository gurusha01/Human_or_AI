This manuscript explores the task of detecting lexical entailment in context, which is crucial for question answering. The key contributions of this work are:
(1) the development of a novel dataset derived from WordNet, utilizing synset exemplar sentences, and 
(2) the introduction of a "context relevance mask" for word vectors, achieved through element-wise multiplication with feature vectors extracted from the context sentence. When fed into a logistic regression classifier, the masked word vectors achieve state-of-the-art performance on entailment prediction in a PPDB-derived dataset from previous literature, albeit by a small margin. Furthermore, when combined with existing features, they surpass the state-of-the-art by a few points. The masked representations also outperform the baseline on the new WN-derived dataset, although the top-performing method on this dataset does not utilize the masked representations.
The paper also presents simple word similarity features, including cosine and Euclidean distance, which complement other cross-context similarity features from prior literature. While the collective similarity features significantly enhance classification results, the contribution of the features introduced in this paper is relatively modest.
The research task is intriguing, and the methodology appears to be sound, albeit incremental. The approach to generating mask vectors is adapted from existing work on encoding variable-length sequences into min/max/mean vectors, but its application as a mask is novel. However, excluding PPDB features, it seems that the best result is achieved without using the representation introduced in the paper.
Several specific points warrant further clarification:
In constructing the new Context-WN dataset, it is unclear whether the "permuted" examples lead to a significant number of false negatives due to similar synsets. Specifically, if a word has synsets i and j, is it guaranteed that the exemplar context for a hypernym synset of j is a poor entailment context for i, especially when i and j are semantically close?
The performance degradation caused by the masked representation when used with context-agnostic word vectors (as seen in rows 3 and 5 of Table 3) is puzzling, given the strong performance in row 1. It is expected that the classifier would learn to disregard the context-agnostic features.
The paper should clearly distinguish between the new similarity measures and those drawn from previous literature. The current statement that prior work used the "most salient" similarity features lacks clarity.
The contribution of the masked vectors versus the similarity features should be more explicitly stated. It appears that the similarity features are driving the majority of the improvement.
The intuition behind the Macro-F1 measure and its relation to the sensitivity of models to context changes is unclear. What specific changes are being referred to, and how does Macro-F1 compare to F1?
The cross-language task lacks clear motivation.
A relevant citation is missing: "Learning to Distinguish Hypernyms and Co-Hyponyms" by Julie Weeds et al. (COLING 2014).
Upon reviewing the author's response, it is noted that the similarity features indeed contribute the most to the improvement in F-score on the two datasets, aside from PPDB features. The response acknowledges that the similarity features include both contextualized and non-contextualized representations, which requires further distinction.
Neither Table 3 nor 4 provides results using only the masked representations without the similarity features, making it challenging to isolate the contribution of the masked representations.