Strengths:
- This paper successfully integrates two distinct fields: human text readability and machine text comprehension.
Weaknesses:
- The primary objective of the paper remains unclear, requiring multiple readings to grasp the intended message, which still lacks clarity.
- The article suffers from ambiguity, failing to clearly distinguish between machine comprehension and human text readability.
- Key contributions in the readability field are overlooked.
- Section 2.2 contains an unrelated discussion of theoretical topics, disrupting the paper's focus.
- The paper attempts to address multiple questions simultaneously, weakening its overall impact. For instance, the influence of text readability on Reading Comprehension (RC) datasets should be analyzed separately from prerequisite skills.
General Discussion:
- The title's ambiguity could be resolved by specifying that it refers to machine text comprehension, rather than human reading comprehension, as the terms "reading comprehension" and "readability" typically imply the latter.
- The claim that dataset analysis suggests readability of RC datasets does not directly impact question difficulty is method-dependent, such as the use of POS/dependency parse features for answer detection.
- The paper requires thorough proofreading to address English omissions, such as the phrase "the question is easy to solve simply look.." on page 1.
- The method of annotating datasets with "metrics" is unclear.
- The paper conflates machine reading comprehension and human reading comprehension, which, despite similarities, are distinct areas of study.
- The terms "readability of text" and "difficulty of reading contents" are not interchangeable; reference to DuBay (2004) clarifies this distinction.
- To reduce ambiguity, the paper should provide more context distinguishing its focus from human question readability, such as adding "for machine comprehension" to statements like "These two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions" on page 1.
- Section 3.1 lacks clarity on whether the skills referred to are for humans or machines. If the focus is on machines, the relevance of cited human-focused papers and the annotators' understanding of the task (machine vs. human comprehension) need clarification.
- Details about the annotation process, including the number of questions annotators had to annotate and their understanding of the task's focus (machine or human comprehension), are necessary for completeness.