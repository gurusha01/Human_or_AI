Review- Strengths:
- The paper presents a valuable investigation into the learning dynamics of NMT models, specifically focusing on their understanding of morphology and syntax.
- The objectives and experimental design are well-defined and clearly outlined, making for a compelling read. The review of existing literature and comparison to state-of-the-art methods are also noteworthy.
- A robust experimentation framework is established, utilizing encoder/decoder recurrent layer outputs to train POS/morphological classifiers and exploring the impact of framework modifications on classifier accuracy, such as the use of characters versus words.
- The experiments are conducted across multiple language pairs, enhancing the study's breadth and applicability.
- The conclusions drawn from this work are intriguing and occasionally counterintuitive, contributing to the paper's overall value.
Weaknesses:
- The comparison between character-based and word-based representations could be more comprehensive, particularly given the strong performance of NMT with byte-pair encoding (BPE) in recent literature. Incorporating BPE into the analysis or replacing word-based representations could strengthen the paper.
- The assertion in Sections 1 and 7 that higher layers focus more on word meaning, while intuitive, is not directly supported by the experiments. This statement should either be omitted or clearly marked as a hypothesis based on indirect evidence, such as improved translation performance without corresponding improvements in morphology on higher layers.
Discussion:
This paper offers a meticulous and systematic examination of the NMT model, yielding several interesting insights from a multitude of data points across various language pairs. Notably, the findings suggest that the target language influences the quality of source-side encoding, with morphologically poor languages like English leading to improved POS tagger accuracy for the encoder. Additionally, increasing the encoder's depth does not necessarily enhance POS accuracy, and the attention layer appears to degrade the quality of decoder representations. It is intriguing to consider whether these observations are interrelated, potentially indicating the need for separate objectives for the encoder and decoder modules within the NMT model to optimize their performance independently.