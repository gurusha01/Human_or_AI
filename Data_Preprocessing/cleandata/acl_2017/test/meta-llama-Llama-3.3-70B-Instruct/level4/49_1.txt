Review
Strengths:
The manuscript presents a novel extension of attention-based neural machine translation approaches, incorporating source-sentence chunking as an additional information source. This chunking information is utilized differently by two recurrent layers, with one layer generating a chunk at a time and the other focusing on generating words within the chunk. The paper is well-written, and alternative approaches are clearly contrasted. The evaluation is convincingly conducted, with direct comparisons to other papers and evaluation tables, making it enjoyable for readers to learn about this approach and its performance.
Weaknesses:
To further strengthen the case for chunk-based models, additional experiments could be conducted. For instance, Table 3 shows promising results for Model 2 and Model 3 compared to previous papers, but it is unclear whether these improvements are due to the switch from LSTMs to GRUs. Including the GRU tree-to-sequence result would help verify the effectiveness of the chunk-based approach. Moreover, the lack of ensembling results is notable, as the authors emphasize that this is the best single NMT model published. Reporting the results of a 3-way chunk-based ensemble could significantly enhance the paper's impact. Additionally, including decoding times in Table 3 would provide more value to the paper, as it would allow for a comparison of the chunk-based model's efficiency with other models.
General Discussion:
Overall, the paper is interesting and worthy of publication. Some minor suggestions for improvement include clearly stating that the chunks are supplied externally, as this information is only apparent when reading about CaboCha on page 6. The authors may also want to reduce the frequency of comparisons to the character-based baseline and instead focus on gains over the best baseline. Providing more detail about the handling of unknown words (UNKs) by the neural decoder or citing the dictionary-based replacement strategy used would be beneficial. Furthermore, a sentence in line 212 contains an inaccuracy regarding the encoding process, which could be corrected to reflect the bidirectional encoder used. Finally, the motivating example in lines 69-87 could be revised to better illustrate the long-dependency problem.