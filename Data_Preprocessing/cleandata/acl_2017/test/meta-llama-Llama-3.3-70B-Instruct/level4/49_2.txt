This paper presents a novel chunk-level architecture for existing Neural Machine Translation (NMT) models, proposing three models to capture the correlation between word and chunk modeling on the target side.
The paper's strengths include its clarity and well-written proposal, as well as the novelty and motivation of the proposed models, which have the potential to be applied to various language pairs.
However, there are some minor weaknesses and areas for improvement:
1. Figure 1 is somewhat surprising, as it appears that function words dominate content words in Japanese sentences, which may be due to a lack of understanding of the Japanese language.
2. To enhance readability, sequences and vectors, such as matrices, should be represented in bold text to differentiate them from scalars, as in hi, xi, c, s, etc.
3. Equation 12 contains a minor error, where sj-1 should be used instead of sj.
4. On Line 244, it is essential to refer to all encoder states as bidirectional RNN states.
5. The phrase "non-sequential information such as chunks" on Line 285 is somewhat confusing, as chunks can be considered sequential information.
6. Equation 21 requires clarification, potentially by inserting k into s1(w) as s1(w)(k) to indicate the word within a chunk.
7. Several questions arise regarding the experiments:
   - Table 1 lacks source language statistics.
   - The baselines used for comparison are not entirely comparable, and it would be beneficial to include a baseline without chunk information for a more straightforward comparison.
   - The use of the same pre-processing and post-processing steps for all baselines should be confirmed.
   - Figure 5 would benefit from including baseline results for comparison.
   - It is unclear whether the chunks in the translated examples were generated automatically by the model or manually by the authors.
   - A comparison of the number of chunks generated by the model and the bunsetsu-chunking toolkit would be valuable, requiring the chunk information for Dev and Test in Table 1.
8. The beam size of 20 used in the decoding process is somewhat surprising, as a larger beam size may lead to a preference for shorter generated sentences.
9. The paper would benefit from using past tenses consistently in the experiments section, such as on Lines 558, 579-584, etc.
Overall, this is a solid piece of work that tackles chunk-based NMT for the first time and deserves consideration for presentation at ACL.