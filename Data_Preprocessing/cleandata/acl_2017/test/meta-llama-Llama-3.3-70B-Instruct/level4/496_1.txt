The authors demonstrate a comprehensive exploration of various language settings, effectively isolating the interaction between relatedness and morphological complexity in neural machine translation (NMT) systems. This is evident in their comparison of translation tasks involving closely related, morphologically rich languages and those that are more distant. A notable aspect of their work is the in-depth analysis of the system's architectural components, particularly how the attention mechanism influences the learning of morphology, resulting in less robust target-side representations. The findings of this study are not only intriguing but also hold significant practical value for the broader NMT community.
However, the paper lacks a detailed explanation of the character-based encoder's implementation, which is a crucial aspect given the diversity of methods for learning character-based representations. This omission raises questions about the generalizability of the results. Furthermore, the analysis could be enhanced by incorporating languages with more complex and challenging morphological systems, such as Turkish or Finnish, and conducting a more nuanced prediction and analysis of morphology.
In general, this research contributes valuable insights into the morphological learning capabilities of NMT models. By leveraging the representations from the encoder or decoder as inputs for part-of-speech or morphology tagging tasks, the study sheds light on what NMT systems learn about morphology. This work can be seen as a logical extension of previous research, such as "Does String-Based Neural MT Learn Source Syntax?," applying a similar methodology but with a focus on morphology. The conclusions drawn offer useful perspectives on the learning capabilities of NMT systems, underscoring the significance of this study.