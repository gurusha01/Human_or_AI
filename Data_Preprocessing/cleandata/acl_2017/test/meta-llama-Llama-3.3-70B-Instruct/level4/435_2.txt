This manuscript presents a novel approach to identifying causal relationships between clauses using deep learning techniques, specifically leveraging neural networks. Although the networks employed are not exceptionally complex, the authors capitalize on the recently introduced AltLex dataset (Hidey and McKeown, 2016) to tackle the challenge of distinguishing between causal and non-causal relations in the absence of explicit markers. The proposed architecture utilizes a Glove-based representation of clauses as input to an LSTM layer, followed by three densely connected tanh layers and a final softmax decision layer, motivated by the argument that convolutional networks are less effective in capturing relevant clause features compared to LSTMs.
The optimal configuration of the system yields an improvement of 0.5-1.5% in F1 score over the 2016 model by Hidey and McKeown, which used an SVM classifier. The authors provide several examples demonstrating the system's ability to generalize, including correctly identifying indicator words as non-causal in the test data despite being causal in the training data. This qualitative and quantitative analysis of the system is commendable.
The paper is well-structured, with a particularly clear description of the problem. However, further clarification on the distinction between this task and implicit connective recognition would be beneficial, potentially including a discussion on why existing methods for implicit connective recognition are not applicable in this context.
The authors' decision to upload their code and data to the submission site is appreciated, as it facilitates transparency and reproducibility. Nevertheless, the meaning of the 0-1-2 coding in the TSV files is unclear, given that the paper focuses on binary classification. Additionally, it is uncertain whether the authors have the necessary permissions to redistribute the data from Hidey and McKeown. The paper could also benefit from a clearer explanation of the "bootstrapping" process, which appears to expand the corpus by approximately 15%.
While the experiment with neural networks on this task is intriguing, the merits of the proposed system are not entirely convincing. The best configuration, selected from 4-7 options, is determined based on the test data, and this configuration is reported to achieve a 2.13% F1 improvement over Hidey's model. A more rigorous comparison would involve selecting the best configuration using the development set.
The significance of the improvement is also not entirely clear. Given the dataset's size, it should be possible to calculate statistical significance indicators. Furthermore, the reliability of the gold-standard annotation, potentially influenced by the dataset creators, should be considered. Upon examination, the annotation derived from the English/SimpleEnglish Wikipedia is not flawless, which may impact the scores.
Lastly, the outperformance of human-engineered features by neural methods in binary classification tasks is a known property, making the results somewhat expected. Experiments with simpler networks, such as a 1-layer LSTM, as baselines would be interesting to see. The analysis could also explore why the neural method appears to favor precision over recall.