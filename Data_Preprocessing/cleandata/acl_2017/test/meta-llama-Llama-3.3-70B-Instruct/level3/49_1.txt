This paper presents an interesting extension to attention-based neural machine translation (MT) approaches using source-sentence chunking. The model's performance is convincing, but could be strengthened with more experiments, such as comparing the chunk-based approach to other models using the same recurrent layers.
The main contributions of this work are:
1. The proposal of a neural model that automatically induces features sensitive to multi-predicate interactions from word sequence information of a sentence.
2. The introduction of a grid-type recurrent neural network (Grid-RNN) that effectively captures multi-predicate interactions.
3. The achievement of state-of-the-art results on the NAIST Text Corpus without using syntactic information.
The strengths of this paper are:
1. The model's ability to capture long-distance dependencies and multi-predicate interactions, which is beneficial for Japanese predicate argument structure (PAS) analysis.
2. The use of Grid-RNNs, which allows for the effective modeling of complex interactions between predicates.
3. The achievement of state-of-the-art results without using syntactic information, which demonstrates the potential of neural models for PAS analysis.
The weaknesses of this paper are:
1. The lack of ensembling results, which makes it difficult to compare the model's performance to other state-of-the-art models.
2. The absence of decoding times in the evaluation tables, which makes it difficult to assess the model's efficiency.
3. The need for more detail on how unknown words are handled by the neural decoder, which could impact the model's performance on out-of-vocabulary words.
Questions to the authors:
1. How do the authors plan to address the issue of unknown words in the neural decoder?
2. Can the authors provide more detail on the implementation of the Grid-RNNs, including the number of layers and the size of the hidden states?
3. How do the authors plan to extend this work to other languages and tasks, such as semantic role labeling (SRL)?