This paper proposes a novel approach to detecting lexical entailment in context, which involves transforming context-agnostic word representations into contextualized representations that highlight salient properties of the exemplar context. The authors introduce a new dataset, CONTEXT-WN, and evaluate their models on two other datasets, CONTEXT-PPDB and a cross-lingual dataset. The main contributions of this paper are:
1. Introduction of a new dataset: The authors introduce a new dataset, CONTEXT-WN, which is designed to evaluate the sensitivity of models to changes in context and to quantify the extent to which models detect asymmetric entailment relations.
2. Contextualized word representations: The authors propose a novel approach to contextualizing word representations, which involves applying a filter to word type representations to highlight the salient dimensions of the exemplar context.
3. Similarity features: The authors propose simple word similarity features, including cosine and Euclidean distance, which improve classification results when combined with other features.
The strengths of this paper are:
1. Improved performance: The authors demonstrate that their approach improves performance over context-agnostic baselines on all three datasets.
2. Novel dataset: The introduction of a new dataset, CONTEXT-WN, provides a valuable resource for evaluating models of lexical entailment in context.
3. Simple yet effective features: The authors' similarity features are simple to implement and effective in improving classification results.
The weaknesses of this paper are:
1. Lack of clarity: The paper lacks clarity on which similarity measures are new and which are from previous literature, making it difficult to understand the contribution of the proposed features.
2. Unclear contribution of masked vectors: The contribution of the masked vectors versus the similarity features is unclear, with the similarity features seeming to do most of the work in improving classification results.
3. Missing citation: The paper is missing a relevant citation, specifically the paper "Learning to Distinguish Hypernyms and Co-Hyponyms" from COLING 2014.
4. Limited analysis: The analysis of the results is limited, and the authors do not provide a clear isolation of the contribution of the masked representations without the similarity features.
Overall, this paper presents a novel approach to detecting lexical entailment in context and introduces a new dataset, but lacks clarity and has some methodological limitations. 
Questions to authors:
1. Can you provide more details on how the CONTEXT-WN dataset was created and how the examples were permuted to generate negative examples?
2. How do you plan to address the issue of false negatives in the CONTEXT-WN dataset due to similar synsets in permuted examples?
3. Can you provide more analysis on the contribution of the masked vectors versus the similarity features?
4. How do you plan to improve the performance of your model on the cross-lingual dataset?