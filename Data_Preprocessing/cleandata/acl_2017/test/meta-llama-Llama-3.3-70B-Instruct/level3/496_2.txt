This paper explores a crucial problem in Neural Machine Translation (NMT), investigating what the model learns about morphology and syntax through experiments with clear objectives and a sound experimentation framework. The paper effectively reviews and compares the state of the art, making it a pleasure to read, and derives interesting conclusions that sometimes contradict intuition.
The main contributions of this work are:
1. The proposal of a neural model that automatically induces features sensitive to multi-predicate interactions from word sequence information of a sentence, without relying on syntactic information.
2. The introduction of a grid-type recurrent neural network (Grid-RNN) architecture that effectively captures multi-predicate interactions.
3. The demonstration that the proposed model achieves state-of-the-art results on the NAIST Text Corpus, outperforming previous models that rely on syntactic information.
The strengths of this paper are:
1. The clear and well-motivated experimental design, which allows for a thorough evaluation of the proposed model.
2. The effective use of Grid-RNNs to capture multi-predicate interactions, which is a key challenge in Japanese PAS analysis.
3. The impressive results achieved by the proposed model, which demonstrate its potential for improving the accuracy of Japanese PAS analysis.
The weaknesses of this paper are:
1. The lack of comparison between character-based and word-based representations, particularly with byte-pair encoding showing strong performance in the literature.
2. The claim that higher layers are more focused on word meaning is not supported by the experiments, and this sentence should be revised or removed.
3. The discussion highlights several interesting findings, including the impact of the target language on source-side encoding quality and the negative effect of the attention layer on decoder representations, but these findings could be further explored and analyzed.
Questions to the authors:
1. How do the authors plan to address the lack of comparison between character-based and word-based representations in future work?
2. Can the authors provide more insight into the negative effect of the attention layer on decoder representations, and how this could be mitigated in future models?
3. How do the authors plan to explore the use of large-scale unlabeled data to learn the neural models, as mentioned in the conclusion?