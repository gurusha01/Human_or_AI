This paper introduces a novel chunk-level architecture for existing Neural Machine Translation (NMT) models, proposing three models to capture the correlation between word and chunk modeling on the target side. The paper is well-written and clear, with novel and well-motivated proposed models that can be applicable to many other language pairs.
The main contributions of this work are:
1. The proposal of a chunk-level architecture for NMT models, which can effectively capture the correlation between word and chunk modeling.
2. The introduction of three models to model the correlation between word and chunk modeling, including a single-sequence model and a multi-sequence model using Grid-RNNs.
3. The demonstration of the effectiveness of the proposed models on the NAIST Text Corpus, achieving state-of-the-art results without syntactic information.
The strengths of this paper include:
1. The novelty and motivation of the proposed models, which address a significant problem in NMT.
2. The clarity and readability of the paper, making it easy to understand the proposed models and their contributions.
3. The thorough evaluation of the proposed models, including comparisons with baseline models and analysis of the results.
However, there are some weaknesses and questions that need to be addressed:
1. The experimental design could be improved, including the choice of baseline models and the use of chunk information.
2. The decoding process could be clarified, including how the proposed models are used to generate translations.
3. The use of past tense in the experiments section could be changed to present tense to improve clarity.
4. More comparative results could be added to demonstrate the effectiveness of the proposed models.
Some questions to the authors include:
1. How do the proposed models handle out-of-vocabulary words and rare chunks?
2. Can the proposed models be applied to other NLP tasks, such as semantic role labeling and question answering?
3. How do the proposed models compare to other state-of-the-art NMT models, such as those using attention mechanisms and transformer architectures?