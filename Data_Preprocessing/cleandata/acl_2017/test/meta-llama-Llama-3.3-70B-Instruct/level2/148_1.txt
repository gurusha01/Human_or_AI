Summary of the Paper
The paper presents a study on evaluating the quality of reading comprehension (RC) datasets using two classes of metrics: prerequisite skills and readability. The authors annotate six existing RC datasets with these metrics and demonstrate the characteristics of the datasets according to each metric and the correlation between the two classes. The study reveals that the readability of RC datasets does not directly affect the difficulty of the questions and that it is possible to create an RC dataset that is easy-to-read but difficult-to-answer.
Main Contributions
1. The authors adopt two classes of evaluation metrics to show the qualitative features of RC datasets.
2. They revise the previous classification of prerequisite skills for RC, organizing skills of knowledge reasoning in terms of entailment phenomena and human text comprehension in psychology.
3. They annotate six existing RC datasets with the organized metrics and make the results publicly available.
Strengths
1. The paper provides a comprehensive analysis of RC datasets using two classes of metrics, which can help researchers develop better RC datasets and improve RC systems.
2. The study reveals interesting insights into the characteristics of RC datasets, such as the weak correlation between readability and question difficulty.
3. The authors provide a detailed description of their methodology and annotation procedure, making it easy to replicate their study.
Weaknesses
1. The paper focuses on a limited number of RC datasets, and it would be beneficial to extend the study to more datasets to increase the generalizability of the results.
2. The authors acknowledge that their methodology cannot evaluate the competence of selecting necessary sentences to answer questions, which is an important aspect of RC.
3. The study could benefit from a more detailed analysis of the results, including a discussion of the implications of the findings for RC system development and dataset construction.
Questions to Authors
1. How do you plan to extend this study to more RC datasets, and what datasets do you think would be most beneficial to include in future studies?
2. Can you provide more details on how you plan to address the limitation of your methodology in evaluating the competence of selecting necessary sentences to answer questions?
3. How do you think the findings of this study can be used to improve RC system development, and what are the potential applications of this research in natural language processing?