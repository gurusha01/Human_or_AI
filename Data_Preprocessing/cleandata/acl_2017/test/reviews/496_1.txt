- Strengths: The authors have nice coverage of a different range of language
settings to isolate the way that relatedness and amount of morphology interact
(i.e., translating between closely related morphologically rich languages vs
distant ones) in affecting what the system learns about morphology. They
include an illuminating analysis of what parts of the architecture end up being
responsible for learning morphology, particularly in examining how the
attention mechanism leads to more impoverished target side representations.
Their findings are of high interest and practical usefulness for other users of
NMT. 
- Weaknesses: They gloss over the details of their character-based encoder.
There are many different ways to learn character-based representations, and
omitting a discussion of how they do this leaves open questions about the
generality of their findings. Also, their analysis could've been made more
interesting had they chosen languages with richer and more challenging
morphology such as Turkish or Finnish, accompanied by finer-grained morphology
prediction and analysis.
- General Discussion: This paper brings insight into what NMT models learn
about morphology by training NMT systems and using the encoder or decoder
representations, respectively, as input feature representations to a POS- or
morphology-tagging classification task. This paper is a straightforward
extension of "Does String-Based Neural MT Learn Source Syntax?," using the same
methodology but this time applied to morphology. Their findings offer useful
insights into what NMT systems learn.