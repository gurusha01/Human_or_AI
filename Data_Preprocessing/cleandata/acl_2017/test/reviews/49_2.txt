- Summary
This paper introduces chunk-level architecture for existing NMT models. Three
models are proposed to model the correlation between word and chunk modelling
on the target side in the existing NMT models. 
- Strengths:
The paper is well-written and clear about the proposed models and its
contributions. 
The proposed models to incorporating chunk information into NMT models are
novel and well-motivated. I think such models can be generally applicable for
many other language pairs. 
- Weaknesses:
There are some minor points, listed as follows:
1) Figure 1: I am a bit surprised that the function words dominate the content
ones in a Japanese sentence. Sorry I may not understand Japanese. 
2) In all equations, sequences/vectors (like matrices) should be represented
as bold texts to distinguish from scalars, e.g., hi, xi, c, s, ...
3) Equation 12: sj-1 instead of sj.
4) Line 244: all encoder states should be referred to bidirectional RNN states.
5) Line 285: a bit confused about the phrase "non-sequential information such
as chunks". Is chunk still sequential information???
6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k)
to indicate the word in a chunk.  
7) Some questions for the experiments:
Table 1: source language statistics? 
For the baselines, why not running a baseline (without using any chunk
information) instead of using (Li et al., 2016) baseline (|V_src| is
different)? It would be easy to see the effect of chunk-based models. Did (Li
et al., 2016) and other baselines use the same pre-processing and
post-processing steps? Other baselines are not very comparable. After authors's
response, I still think that (Li et al., 2016) baseline can be a reference but
the baseline from the existing model should be shown. 
Figure 5: baseline result will be useful for comparison? chunks in the
translated examples are generated automatically by the model or manually by
the authors? Is it possible to compare the no. of chunks generated by the model
and by the bunsetsu-chunking toolkit? In that case, the chunk information for
Dev and Test in Table 1 will be required. BTW, the authors's response did not
address my point here. 
8) I am bit surprised about the beam size 20 used in the decoding process. I
suppose large beam size is likely to make the model prefer shorter generated
sentences. 
9) Past tenses should be used in the experiments, e.g.,
Line 558: We use (used) ...
Line 579-584: we perform (performed) ... use (used) ...
...
- General Discussion:
Overall, this is a solid work - the first one tackling the chunk-based NMT;
and it well deserves a slot at ACL.