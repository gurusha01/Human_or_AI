- Strengths: A well written paper, examining the use of context in lexical
entailment task is a great idea, a well defined approach and experimental
set-up and good analysis of the results 
- Weaknesses: Some information is missing or insufficient, e.g., the table
captions should be more descriptive, a clear description for each of the word
type features should be given.
General Discussion: 
The paper presents a proposal of consideration of context
in lexical entailment task. The results from the experiments demonstrate that
context-informed models do better than context-agnostic models on the
entailment task. 
I liked the idea of creating negative examples to get negative annotations
automatically in the two ways described in the paper based on WordNet positive
examples. (new dataset; an interesting method to develop dataset)
I also liked the idea of transforming already-used context-agnostic
representations into contextualized representations, experimenting with
different ways to get contextualized representations (i.e., mask vs
contetx2vec), and testing the model on 3 different datasets (generalizability
not just across different datasets but also cross-linguistically).
Motivations for various decisions in the experimental design were good to
see, e.g., why authors used the split they used for CONTEXT-PPDB (it showed
that they thought out clearly what exactly they were doing and why).
Lines 431-434: authors might want to state briefly how the class weights were
determined and added to account for the unbalanced data in the CONTEXT-WN
experiments. Would it affect direct comparisons with previous work, in what
ways? 
Change in Line 589: directionality 4 --> directionality, as in Table 4
Suggested change in Line 696-697: is-a hierarchy of WordNet --> "is-a"
hierarchy of WordNet 
For the sake of completeness, represent "mask" also in Figure 1.
I have read the author response.