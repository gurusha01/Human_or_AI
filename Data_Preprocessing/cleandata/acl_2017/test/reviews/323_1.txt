The paper introduces an extension of the entity grid model. A convolutional
neural network is used to learn sequences of entity transitions indicating
coherence, permitting better generalisation over longer sequences of entities
than the direct estimates of transition probabilities in the original model.
This is a nice and well-written paper. Instead of proposing a fully neural
approach, the authors build on existing work and just use a neural network to
overcome specific issues in one step. This is a valid approach, but it would be
useful to expand the comparison to the existing neural coherence model of Li
and Hovy. The authors admit being surprised by the very low score the Li and
Hovy model achieves on their task. This makes the reader wonder if there was an
error in the experimental setup, if the other model's low performance is
corpus-dependent and, if so, what results the model proposed in this paper
would achieve on a corpus or task where the other model is more successful. A
deeper investigation of these factors would strengthen the argument
considerably.
In general the paper is very fluent and readable, but in many places definite
articles are missing (e.g. on lines 92, 132, 174, 488, 490, 547, 674, 764 and
probably more). I would suggest proofreading the paper specifically with
article usage in mind. The expression "...limits the model to do X...", which
is used repeatedly, sounds a bit unusual. Maybe "limits the model's capacity to
do X" or "stops the model from doing X" would be clearer.
--------------
Final recommendation adjusted to 4 after considering the author response. I
agree that objective difficulties running other people's software shouldn't be
held against the present authors. The efforts made to test the Li and Hovy
system, and the problems encountered in doing so, should be documented in the
paper. I would also suggest that the authors try to reproduce the results of Li
and Hovy on their original data sets as a sanity check (unless they have
already done so), just to see if that works for them.