This paper develops an LSTM-based model for classifying connective uses for
whether they indicate that a causal relation was intended. The guiding idea is
that the expression of causal relations is extremely diverse and thus not
amenable to syntactic treatment, and that the more abstract representations
delivered by neural models are therefore more suitable as the basis for making
these decisions.
The experiments are on the AltLex corpus developed by Hidley and McKeown. The
results offer modest but consistent support for the general idea, and they
provide some initial insights into how best to translate this idea into a
model. The paper distribution includes the TensorFlow-based models used for the
experiments.
Some critical comments and questions:
* The introduction is unusual in that it is more like a literature review than
a full overview of what the paper contains. This leads to some redundancy with
the related work section that follows it. I guess I am open to a non-standard
sort of intro, but this one really doesn't work: despite reviewing a lot of
ideas, it doesn't take a stand on what causation is or how it is expressed, but
rather only makes a negative point (it's not reducible to syntax). We aren't
really told what the positive contribution will be except for the very general
final paragraph of the section.
* Extending the above, I found it disappointing that the paper isn't really
clear about the theory of causation being assumed. The authors seem to default
to a counterfactual view that is broadly like that of David Lewis, where
causation is a modal sufficiency claim with some other counterfactual
conditions added to it. See line 238 and following; that arrow needs to be a
very special kind of implication for this to work at all, and there are
well-known problems with Lewis's theory (see
http://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments
elsewhere in the paper that the authors don't endorse the counterfactual view,
but then what is the theory being assumed? It can't just be the temporal
constraint mentioned on page 3!
* I don't understand the comments regarding the example on line 256. The
authors seem to be saying that they regard the sentence as false. If it's true,
then there should be some causal link between the argument and the breakage.
There are remaining issues about how to divide events into sub-events, and
these impact causal theories, but those are not being discussed here, leaving
me confused.
* The caption for Figure 1 is misleading, since the diagram is supposed to
depict only the "Pair_LSTM" variant of the model. My bigger complaint is that
this diagram is needlessly imprecise. I suppose it's okay to leave parts of the
standard model definition out of the prose, but then these diagrams should have
a clear and consistent semantics. What are all the empty circles between input
and the "LSTM" boxes? The prose seems to say that the model has a look-up
layer, a Glove layer, and then ... what? How many layers of representation are
there? The diagram is precise about the pooling tanh layers pre-softmax, but
not about this. I'm also not clear on what the "LSTM" boxes represent. It seems
like it's just the leftmost/final representation that is directly connected to
the layers above. I suggest depicting that connection clearly.
* I don't understand the sentence beginning on line 480. The models under
discussion do not intrinsically require any padding. I'm guessing this is a
requirement of TensorFlow and/or efficient training. That's fine. If that's
correct, please say that. I don't understand the final clause, though. How is
this issue even related to the question of what is "the most convenient way to
encode the causal meaning"? I don't see how convenience is an issue or how this
relates directly to causal meaning.
* The authors find that having two independent LSTMs ("Stated_LSTM") is
somewhat better than one where the first feeds into the second. This issue is
reminiscent of discussions in the literature on natural language entailment,
where the question is whether to represent premise and hypothesis independently
or have the first feed into the second. I regard this as an open question for
entailment, and I bet it needs further investigation for causal relations too.
So I can't really endorse the sentence beginning on line 587: "This behaviour
means that our assumption about the relation between the meanings of the two
input events does not hold, so it is better to encode each argument
independently and then to measure the relation between the arguments by using
dense layers." This is very surprising since we are talking about subparts of a
sentence that might share a lot of information.
* It's hard to make sense of the hyperparameters that led to the best
performance across tasks. Compare line 578 with line 636, for example. Should
we interpret this or just attribute it to the unpredictability of how these
models interact with data?
* Section 4.3 concludes by saying, of the connective 'which then', that the
system can "correctly disambiguate its causal meaning", whereas that of Hidey
and McKeown does not. That might be correct, but one example doesn't suffice to
show it. To substantiate this point, I suggest making up a wide range of
examples that manifest the ambiguity and seeing how often the system delivers
the right verdict. This will help address the question of whether it got lucky
with the example from table 8.