This paper proposes a method for detecting causal relations between clauses,
using neural networks ("deep learning", although, as in many studies, the
networks are not particularly deep).  Indeed, while certain discourse
connectives are unambiguous regarding the relation they signal (e.g. 'because'
is causal) the paper takes advantage of a recent dataset (called AltLex, by
Hidey and McKeown, 2016) to solve the task of identifying causal vs. non-causal
relations when the relation is not explicitly marked.  Arguing that
convolutional networks are not as adept as representing the relevant features
of clauses as LSTMs, the authors propose a classification architecture which
uses a Glove-based representation of clauses, input in an LSTM layer, followed
by three densely connected layers (tanh) and a final decision layer with a
softmax.
The best configuration of the system improves by 0.5-1.5% F1 over Hidey and
MCkeown's 2016 one (SVM classifier).  Several examples of generalizations where
the system performs well are shown (indicator words that are always causal in
the training data, but are found correctly to be non causal in the test data).
Therefore, I appreciate that the system is analyzed qualitatively and 
quantitatively.
The paper is well written, and the description of the problem is particularly
clear. However a clarification of the differences between this task and the 
task of implicit connective recognition would be welcome.  This could possibly 
include a discussion of why previous methods for implicit connective 
recognition cannot be used in this case.
It is very appreciable that the authors uploaded their code to the submission
site (I inspected it briefly but did not execute it).  Uploading the (older)
data (with the code) is also useful as it provides many examples.  It was not
clear to me what is the meaning of the 0-1-2 coding in the TSV files, given
that the paper mentions binary classification. I wonder also, given that this
is the data from Hidey and McKeown, if the authors have the right to repost it
as they do.  -- One point to clarify in the paper would be the meaning of
"bootstrapping", which apparently extends the corpus by about 15%: while the
construction of the corpus is briefly but clearly explained in the paper, the
additional bootstrapping is not. 
While it is certainly interesting to experiment with neural networks on this
task, the merits of the proposed system are not entirely convincing.  It seems
indeed that the best configuration (among 4-7 options) is found on the test
data, and it is this best configuration that is announced as improving over
Hidey by "2.13% F1".  However, a fair comparison would involve selecting the
best configuration on the devset.
Moreover, it is not entirely clear how significant the improvement is. On the
one hand, it should be possible, given the size of the dataset, to compute some
statistical significance indicators.  On the other hand, one should consider
also the reliability of the gold-standard annotation itself (possibly from the
creators of the dataset).  Upon inspection, the annotation obtained from the
English/SimpleEnglish Wikipedia is not perfect, and therefore the scores might
need to be considered with a grain of salt.
Finally, neural methods have been previously shown to outperform human
engineered features for binary classification tasks, so in a sense the results 
are rather a confirmation of a known property. It would be interesting to see
experiments with simpler networks used as baselines, e.g. a 1-layer LSTM.  The
analysis of results could try to explain why the neural method seems to favor 
precision over recall.