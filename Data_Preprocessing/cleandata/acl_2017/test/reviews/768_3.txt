This paper addresses the task of lexical entailment detection in context, e.g.
is "chess" a kind of "game" given a sentence containing each of the words --
relevant for QA. The major contributions are:
(1) a new dataset derived from WordNet using synset exemplar sentences, and 
(2) a "context relevance mask" for a word vector, accomplished by elementwise
multiplication with feature vectors derived from the context sentence. Fed to a
logistic regression classifier, the masked word vectors just beat state of the
art on entailment prediction on a PPDB-derived dataset from previous
literature. Combined with other existing features, they beat state of the art
by a few points. They also beats the baseline on the new WN-derived dataset,
although the best-scoring method on that dataset doesn't use the masked
representations.
The paper also introduces some simple word similarity features (cosine,
euclidean distance) which accompany other cross-context similarity features
from previous literature. All of the similarity features, together, improve the
classification results by a large amount, but the features in the present paper
are a relatively small contribution.
The task is interesting, and the work seems to be correct as far as it goes,
but incremental. The method of producing the mask vectors is taken from
existing literature on encoding variable-length sequences into min/max/mean
vectors, but I don't think they've been used as masks before, so this is novel.
However, excluding the PPDB features it looks like the best result does not use
the representation introduced in the paper.
A few more specific points:
In the creation of the new Context-WN dataset, are there a lot of false
negatives resulting from similar synsets in the "permuted" examples? If you
take word w, with synsets i and j, is it guaranteed that the exemplar context
for a hypernym synset of j is a bad entailment context for i? What if i and j
are semantically close?
Why does the masked representation hurt classification with the
context-agnostic word vectors (rows 3, 5 in Table 3) when row 1 does so well?
Wouldn't the classifier learn to ignore the context-agnostic features?
The paper should make clearer which similarity measures are new and which are
from previous literature. It currently says that previous lit used the "most
salient" similarity features, but that's not informative to the reader.
The paper should be clearer about the contribution of the masked vectors vs the
similarity features. It seems like similarity is doing most of the work.
I don't understand the intuition behind the Macro-F1 measure, or how it relates
to "how sensitive are our models to changes in context" -- what changes? How do
we expect Macro-F1 to compare with F1?
The cross-language task is not well motivated.
Missing a relevant citation: Learning to Distinguish Hypernyms and Co-Hyponyms.
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller. COLING
2014.
==
I have read the author response. As noted in the original reviews, a quick
examination of the tables shows that the similarity features make the largest
contribution to the improvement in F-score on the two datasets (aside from PPDB
features). The author response makes the point that similarities include
contextualized representations. However, the similarity features are a mixed
bag, including both contextualized and non-contextualized representations. This
would need to be teased out more (as acknowledged in the response).
Neither Table 3 nor 4 gives results using only the masked representations
without the similarity features. This makes the contribution of the masked
representations difficult to isolate.