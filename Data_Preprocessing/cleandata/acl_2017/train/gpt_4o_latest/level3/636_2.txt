Review of the Paper
Summary and Contributions
This paper introduces Iterated Dilated Convolutional Neural Networks (ID-CNNs) as a novel alternative to Bi-directional LSTMs (Bi-LSTMs) for sequence labeling tasks such as Named Entity Recognition (NER). The key contributions of the paper are:
1. Speed Advantage: The ID-CNN architecture achieves significant test-time speed improvements over Bi-LSTM-based models. Specifically, the paper claims an 8x speed-up for structured prediction tasks and a 14x speed-up for independent classification tasks, without substantial loss in accuracy.
2. Broad Context Aggregation: By leveraging dilated convolutions, ID-CNNs efficiently aggregate long-range context while maintaining token-level resolution, outperforming traditional CNNs and Bi-LSTMs in certain settings.
3. Document-Level Context: The paper demonstrates that ID-CNNs can incorporate document-level context effectively, achieving competitive or superior performance compared to Bi-LSTM-CRFs on long sequences, particularly in scenarios where broad context is critical.
Strengths
1. Efficiency Gains: The primary strength of the paper lies in its focus on computational efficiency. The ID-CNN architecture is well-suited for large-scale NLP tasks where speed is critical, such as processing web-scale data.
2. Broad Context Representation: The use of dilated convolutions to aggregate exponentially growing context with minimal parameter overhead is innovative and addresses the limitations of traditional CNNs.
3. Practical Relevance: The paper addresses a real-world need for faster and more resource-efficient NLP models, making it highly relevant for industrial applications.
Weaknesses
1. Clarity of Sections 3 and 4: The sections describing the architecture and parameterization of ID-CNNs (especially Section 4.1) lack sufficient clarity and precision. For example, the iterative nature of the model and its parameter-sharing mechanism could be explained more rigorously.
2. Unsubstantiated Speed Claims: While the paper claims significant speed advantages, the experimental results (4-6x speed-up for Viterbi decoding and 14x for independent classification) are underwhelming compared to the stated goals. Additional experiments with larger datasets or more diverse tasks would strengthen these claims.
3. Terminology Ambiguity: The term "Viterbi prediction" is not well-defined and could confuse readers unfamiliar with the context. A clearer explanation or alternative phrasing is needed.
4. Typographical Error: There is a typo in the reference to Weiss et al., 2015, which should be corrected.
Questions to Authors
1. Could you provide a more detailed explanation of the parameter-sharing mechanism in ID-CNNs and how it prevents overfitting?
2. How does the performance of ID-CNNs compare to Bi-LSTMs on datasets with significantly longer sequences or more complex tagging tasks?
3. Can you clarify the meaning of "Viterbi prediction" and its role in your experiments?
Additional Comments
The paper makes a strong case for the use of ID-CNNs as a faster alternative to Bi-LSTMs for sequence labeling tasks. However, the clarity of the technical sections and the experimental validation of speed claims need improvement. Addressing these weaknesses during the author response period would significantly enhance the paper's impact.