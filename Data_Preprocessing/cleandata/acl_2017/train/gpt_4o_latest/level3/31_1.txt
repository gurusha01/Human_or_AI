Review of the Paper
Summary and Contributions
This paper introduces a supervised framework for event factuality identification, leveraging a combination of attention-based BiLSTM and CNN models. The authors propose a two-step process: first, extracting essential factors such as events, Source Introducing Predicates (SIPs), relevant sources, and cues; and second, using a deep neural network to classify event factuality. The model is designed with two outputs to address imbalances in factuality categories and improve the identification of speculative and negative cues. The authors claim that their approach outperforms state-of-the-art methods on the FactBank dataset.
The primary contributions of the paper, as I see them, are:
1. The integration of BiLSTM and CNN with an attention mechanism to learn representations from syntactic paths and words, respectively. This combination is claimed to improve the identification of speculative and negative factuality values.
2. The introduction of a two-output design to address class imbalance and enhance the precision of cue-related factuality identification.
3. The use of pruned sentence structures and auxiliary words to improve SIP detection and event factuality classification.
Strengths
1. Novelty in Model Design: The combination of BiLSTM and CNN with attention mechanisms is a thoughtful approach to capturing both syntactic and lexical features. The use of attention to focus on key factors like SIPs and cues is well-motivated and aligns with recent trends in NLP.
2. Comprehensive Evaluation: The authors conduct detailed experiments, including ablation studies to evaluate the impact of different inputs (e.g., RS Path, SIP Path, auxiliary words). This provides valuable insights into the model's performance and the importance of various features.
3. Improved Performance on Challenging Categories: The model demonstrates notable improvements in identifying speculative and negative factuality values (e.g., CT-, PR+, PS+), which are often underrepresented in datasets. This is a meaningful contribution to the field.
Weaknesses
1. Hyperparameter Optimization: The paper does not adequately address how hyperparameters were optimized, particularly in the context of 5-fold cross-validation. This omission raises concerns about the fairness and reproducibility of the reported results.
2. Experimental Results on a Small Dataset: The reported 2% improvement on FactBank, a dataset with only 200 examples per fold, is not compelling given the complexity of the proposed model. The small dataset size also raises concerns about overfitting.
3. Reliance on Hand-Crafted Features: Despite the use of deep learning, the model heavily relies on manually designed features (e.g., pruned sentence structures, lexical features). This contradicts the motivation of using neural networks for latent representation learning and limits the generalizability of the approach.
4. Baseline Misrepresentation: The baseline described in the paper is part of the proposed model itself, which undermines the clarity and fairness of the evaluation. A more rigorous comparison with external baselines is necessary.
5. Unclear Justifications: The claim of "properly" combining BiLSTM and CNN is vague and lacks theoretical or empirical justification. Similarly, the rationale for the two-output design is weak and not sufficiently supported by experimental evidence.
Questions to Authors
1. How were hyperparameters (e.g., learning rate, regularization coefficient, embedding dimensions) optimized during 5-fold cross-validation? Were the test folds used in any way during this process?
2. Can you provide more details on how the two-output design improves performance compared to a single-output design? Are there specific examples where this design leads to better predictions?
3. Why was FactBank chosen as the sole dataset for evaluation? Have you considered testing the model on other factuality-related datasets to demonstrate its generalizability?
Additional Comments
While the paper introduces an interesting model, the reliance on hand-crafted features and the lack of clarity in experimental design diminish its impact. Addressing these concerns and providing stronger justifications for design choices would significantly strengthen the submission.