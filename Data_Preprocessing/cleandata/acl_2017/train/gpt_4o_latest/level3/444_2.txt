Review of the Paper
Summary and Contributions
This paper addresses the challenging task of evaluating ghostwriting systems for rap lyrics, proposing a comprehensive evaluation methodology that combines manual and automatic metrics. The manual evaluation focuses on fluency, coherence, and style matching, while the automatic evaluation assesses uniqueness and stylistic similarity using rhyme density and cosine similarity. The authors also introduce a publicly available dataset of rap lyrics for 13 artists, annotated for stylistic similarity, which serves as a valuable resource for future research. The primary contributions of the paper are:  
1. A well-justified manual evaluation method for style matching, which could be generalized to other creative text generation tasks.  
2. An improvement to the semi-automatic evaluation method by automating the handling of repetitive text, enabling large-scale analysis.  
3. A publicly available annotated dataset that provides a foundation for future work in rap lyric generation.  
Strengths
1. Comprehensive Evaluation Framework: The paper provides a robust evaluation methodology that captures complementary aspects of the ghostwriting task. The manual evaluation for style matching is particularly well-justified and demonstrates high relevance to the task.  
2. Dataset Contribution: The creation of a publicly available dataset annotated for stylistic similarity is a significant contribution, as it provides a gold standard for evaluating future models.  
3. Insightful Analysis: The results highlight the multifaceted nature of the task, emphasizing the need for complementary metrics to evaluate fluency, coherence, and style. The discussion of correlations between metrics and dataset characteristics (e.g., number of verses) is insightful.  
Weaknesses
1. Fluency/Coherence Evaluation Granularity: The evaluation of fluency and coherence at the line level is a limitation, as it overlooks the topical and structural coherence required at the verse level. This is particularly important for rap lyrics, where thematic consistency is crucial.  
2. Validation of Automatic Metrics: The automatic metrics for uniqueness and stylistic similarity are not validated against manual ratings. This raises concerns about their reliability and generalizability, especially since low similarity scores can still result in coherent verses without true originality.  
3. Rhyme Density as a Stylistic Metric: While rhyme density is a useful metric, it is insufficient for capturing comprehensive stylistic similarity. Positional and thematic aspects of rhymes, as well as semantic evaluation of common themes, should be considered for a more holistic assessment.  
4. Numeric Scores and Comparisons: The claim that numeric scores enable better comparisons is less compelling than ensuring the reliability and interpretability of the judgments.  
Questions to Authors
1. Have you considered evaluating fluency and coherence at the verse level to account for thematic and structural consistency?  
2. How do you plan to validate the automatic metrics against manual ratings to ensure their reliability?  
3. Could additional stylistic metrics, such as rhyme positioning or semantic evaluation of themes, improve the assessment of stylistic similarity?  
Additional Comments
The paper is well-written and provides a strong foundation for evaluating creative text generation tasks. However, addressing the identified weaknesses, particularly the validation of automatic metrics and the granularity of fluency/coherence evaluation, would significantly enhance the robustness of the proposed methodology. Additionally, there is a minor typographical issue with incorrect quotation marks in Line 389.