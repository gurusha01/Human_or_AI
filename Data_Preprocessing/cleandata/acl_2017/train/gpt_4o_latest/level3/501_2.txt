Review of the Paper
Summary and Contributions
This paper introduces the Dual Machine Comprehension (DMC) task, a novel multimodal challenge that requires systems to select the most suitable caption for an image from a set of semantically similar options. The authors propose an innovative dataset generation mechanism that uses a combination of linguistic surface and semantic similarity to create challenging decoys. The resulting dataset, derived from the COCO corpus, is large-scale and will be made publicly available, which is a significant contribution to the vision-language research community. The paper also presents several baseline models and competitive neural architectures, demonstrating that systems trained on the DMC task outperform traditional caption generation models in comprehension tasks. Additionally, the authors show that performance on the DMC task correlates positively with image captioning performance in a multi-task learning setup.
Strengths
1. Dataset Creation and Availability: The creation of a "rephrased" caption dataset with carefully selected decoys is a major strength. The authors' plan to make this dataset publicly available will likely have a significant impact on future research in multimodal AI.
2. Task Simplicity and Evaluation: The DMC task introduces a straightforward evaluation metric—accuracy—which avoids the complexities and limitations of metrics like BLEU or METEOR commonly used in caption generation tasks.
3. Innovative Decoy Selection Mechanism: The proposed algorithm for generating decoys balances surface and semantic similarity using a lambda factor, ensuring that the task remains challenging for both humans and machines.
4. Empirical Findings: The results convincingly demonstrate that systems trained on the DMC task achieve better semantic alignment between vision and language modalities compared to traditional caption generation models.
5. Multi-task Learning Insights: The positive correlation between DMC task performance and image captioning performance highlights the potential of the proposed task to improve other vision-language tasks.
Weaknesses
1. Insufficient Justification for Task Superiority: The paper lacks a clear explanation of why image captioning is unsuitable for comprehension tasks and how the DMC task provides a better alternative. A more thorough discussion of the limitations of existing approaches would strengthen the paper.
2. Limited Novelty in the Approach: While the decoy selection mechanism is innovative, the overall approach heavily relies on existing caption generation techniques, which limits its novelty.
3. Ambiguity in Formula (4): The description of Formula (4) and the role of decoys in the second loss term is unclear, which could confuse readers and hinder reproducibility.
4. Evaluation Gaps: While the authors provide human evaluation results, they do not compare the DMC task to other existing vision-language benchmarks, which would help contextualize its significance.
Questions to Authors
1. Can you elaborate on the specific limitations of image captioning tasks that the DMC task addresses? How does this task compare to other vision-language tasks like Visual Question Answering (VQA)?
2. Formula (4) is ambiguous. Could you clarify the role of decoys in the second loss term and provide more details on how this term is optimized?
3. Have you considered evaluating the DMC task on datasets beyond COCO to assess its generalizability?
Recommendation
This paper makes a valuable contribution by introducing a novel dataset and task that emphasize semantic alignment between vision and language. However, the lack of clarity in some technical aspects and insufficient justification for the task's superiority over existing approaches limit its impact. I recommend acceptance with minor revisions, contingent on addressing the identified weaknesses.