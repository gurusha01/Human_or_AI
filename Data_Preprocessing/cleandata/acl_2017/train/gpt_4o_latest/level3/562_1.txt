Review
Summary and Contributions
This paper addresses the intriguing problem of zero-shot relation extraction by reframing it as a reading comprehension task. The authors introduce a large-scale dataset of 30 million examples, generated through a combination of distant supervision and crowd-sourced annotations, where relations are expressed as natural-language questions. They adapt the BiDAF reading comprehension model (Seo et al., 2016) to handle unanswerable questions, enabling zero-shot generalization to unseen relations. Experiments demonstrate the model's ability to generalize to unseen entities, question templates, and relations, albeit with varying levels of success. The paper's primary contributions are:
1. Dataset Creation: The introduction of a large-scale dataset for relation extraction as question answering, which could serve as a valuable resource for the research community.
2. Zero-Shot Relation Extraction: A novel framing of relation extraction as a reading comprehension task, enabling zero-shot learning for unseen relations.
3. Model Adaptation: Extending the BiDAF model to handle unanswerable questions, a necessary adaptation for the relation extraction task.
Strengths
1. Dataset Contribution: The dataset is a significant contribution, offering a large-scale resource for exploring relation extraction as question answering. The use of schema querification to efficiently generate examples is particularly innovative and cost-effective.
2. Reframing the Task: The reduction of relation extraction to reading comprehension is an interesting and promising approach, leveraging advancements in machine reading to tackle a challenging problem.
3. Zero-Shot Capability: The paper demonstrates the feasibility of zero-shot relation extraction, setting a baseline for future work in this area.
4. Scalability: The methodology for dataset creation is scalable and cost-efficient, potentially enabling further expansion or adaptation to other domains.
Weaknesses
1. Lack of Novelty: While the dataset is a valuable contribution, the methodological novelty is limited. The approach heavily relies on existing reading comprehension models, and the adaptation to handle unanswerable questions is incremental.
2. Insufficient Comparison to Prior Work: The paper does not adequately differentiate itself from prior works, such as those by Neelakantan et al. (2015), Verga et al. (2017), and Bordes et al. (2015). Missing citations, including a NAACL 2016 paper on memory networks for relation extraction, further weaken the contextualization of the contributions.
3. Empirical Weaknesses: The experimental evaluation is limited to the proposed dataset. The authors do not test their model on existing relation extraction datasets or compare the performance of existing models on their dataset, making it difficult to assess the broader applicability and impact of their approach.
4. Dataset Validation: The paper does not convincingly demonstrate that the dataset enables better generalization or that the proposed approach outperforms existing methods. The lack of rigorous validation undermines the dataset's claimed utility.
Questions to Authors
1. How does the proposed approach compare to existing relation extraction methods on standard benchmarks? Could you evaluate your model on datasets like TAC KBP or SemEval?
2. How does the dataset's quality compare to other relation extraction datasets in terms of annotation accuracy and coverage?
3. Could you provide more details on the missing citations and how your work builds upon or differs from them?
Recommendation
While the dataset is a valuable contribution, the lack of methodological novelty, insufficient comparison to prior work, and limited empirical validation weaken the paper's overall impact. I recommend rejecting the paper in its current form but encourage the authors to address these issues and resubmit. The dataset and framing of the task have potential, but the work requires stronger contextualization and validation to justify its contributions.