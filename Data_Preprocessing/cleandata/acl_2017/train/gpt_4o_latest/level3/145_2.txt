Review of the Paper
Summary and Contributions
This paper introduces a novel approach to word embeddings by representing words as Gaussian mixtures, enabling the capture of multiple word meanings (polysemy) and uncertainty. The authors propose a max-margin learning objective and utilize the expected likelihood kernel as a similarity metric, which allows for efficient training and analytic tractability. The model, named Word to Gaussian Mixture (w2gm), is evaluated on tasks such as word similarity and entailment, demonstrating superior performance compared to baseline methods like word2vec and unimodal Gaussian embeddings. Key contributions include:
1. Multimodal Word Representations: The use of Gaussian mixtures to model words, allowing for expressive representations of polysemous words with distinct meanings.
2. Efficient Training Objective: A max-margin energy-based objective paired with the expected likelihood kernel, enabling scalable learning of multimodal distributions.
3. Empirical Validation: Strong performance on benchmark datasets for word similarity and entailment, with qualitative evidence of capturing multiple word senses.
Strengths
1. Novelty and Expressiveness: The use of Gaussian mixtures is a significant advancement over unimodal Gaussian embeddings, as it effectively captures polysemy and reduces the variance issue for polysemous words. This is demonstrated both qualitatively (nearest neighbor analysis) and quantitatively (benchmark results).
2. Scalability: The proposed model is computationally efficient, capable of training on large corpora with billions of tokens. The use of diagonal or spherical covariance matrices further enhances scalability without sacrificing performance.
3. Empirical Results: The model consistently outperforms baselines on several datasets, including WordSim-353 and SCWS, and demonstrates strong entailment capabilities. The reduction in variance for polysemous words is a particularly compelling result.
4. Clarity and Accessibility: The paper is well-written, with clear explanations of the methodology, derivations, and practical considerations. The inclusion of an interactive visualization tool for embeddings is a valuable addition.
Weaknesses
1. Choice of Gaussian Components (K): The paper does not provide a thorough analysis of how the number of Gaussian components (K) impacts performance across tasks. While K=2 is used for most experiments, the rationale for this choice is not fully justified, and the performance of higher values of K is only briefly mentioned.
2. Covariance Matrix Assumptions: The use of spherical covariance matrices may limit the model's ability to capture complex uncertainty patterns. While this choice is computationally efficient, the paper does not explore the trade-offs between spherical and diagonal covariance matrices in detail.
3. Minor Issues: There is an unreferenced table and a missing publication year in one citation, which slightly detracts from the paper's polish.
Questions to Authors
1. How does the choice of the number of Gaussian components (K) affect performance across different tasks? Would higher values of K improve results for words with more than two distinct meanings?
2. Have you considered using diagonal covariance matrices instead of spherical ones? If so, how does this impact performance and computational efficiency?
3. Can the proposed model be extended to incorporate external lexical resources (e.g., WordNet) to further enhance performance on tasks like entailment?
Additional Comments
Overall, this paper represents a significant contribution to the field of word embeddings, particularly in its ability to model polysemy and uncertainty. Addressing the questions and weaknesses raised above would further strengthen the work. I recommend acceptance with minor revisions.