Review of the Submission
Summary and Contributions
The paper presents an automated scoring and support system for Japanese short-answer tests, specifically targeting the new National Center Test for university entrance examinations. The system combines machine learning (Random Forests) with human oversight to evaluate semantic similarity between student responses and model answers. The authors emphasize the system's ability to handle both surface-level lexical matches and deeper semantic relationships. The primary contributions of the paper are as follows:
1. Development of a Hybrid Scoring System: The system integrates automated scoring with human raters, allowing for corrections and adjustments. This hybrid approach ensures that the final scores are both efficient and reliable.
2. Incorporation of Semantic and Surface-Level Features: The system evaluates responses using a combination of semantic similarity (e.g., cosine similarity) and surface-level lexical matches, which improves the robustness of scoring.
3. Application to Real-World Educational Contexts: The system is tailored for short-answer tests in social studies, with potential scalability to other subjects like Japanese literature, addressing a significant need in the Japanese education system.
Strengths
1. Practical Relevance: The proposed system addresses a pressing need in the Japanese education system, where short-answer tests are being introduced in national examinations. The hybrid approach balances automation with human oversight, making it a practical solution for large-scale deployment.
2. Innovative Use of Random Forests: The use of Random Forests for scoring prediction is well-justified, given its ability to handle multiple predictor variables and provide insights into feature importance. This adds methodological rigor to the work.
3. Evaluation on Real-World Data: The system is evaluated on eight real-world test items, and the results demonstrate reasonable alignment with human scores (71â€“95% agreement within one point). This empirical validation strengthens the paper's claims.
4. Flexibility in Handling Variability: The system accounts for variations in phrasing and mandatory elements, making it adaptable to the nuances of natural language.
Weaknesses
1. Limited Scope of Evaluation: While the system is evaluated on eight test items, the diversity of these items is unclear. Expanding the evaluation to include more subjects and test types would strengthen the generalizability of the results.
2. Lack of Comparative Baselines: The paper does not compare its system's performance with existing automated scoring methods, such as those mentioned in the introduction (e.g., c-rater, Pulman and Sukkarieh's system). This omission makes it difficult to assess the novelty and effectiveness of the proposed approach.
3. Dependence on Human Oversight: While the hybrid approach is practical, the system's reliance on human raters for final judgment limits its scalability. Further discussion on reducing this dependency would be beneficial.
4. Insufficient Discussion of Limitations: The paper briefly mentions challenges like imbalanced training data and low performance on certain items (e.g., Japanese History B1 2) but does not provide detailed analyses or solutions.
Questions to Authors
1. How does the proposed system compare quantitatively to existing automated scoring systems mentioned in the introduction?
2. Could the system's reliance on human raters be reduced in future iterations, and if so, how?
3. What measures were taken to ensure the diversity of the test items used for evaluation? Are there plans to expand the dataset?
Additional Comments
The paper makes a valuable contribution to the field of automated scoring, particularly in the context of Japanese education. However, addressing the weaknesses outlined above would significantly enhance the paper's impact and clarity.