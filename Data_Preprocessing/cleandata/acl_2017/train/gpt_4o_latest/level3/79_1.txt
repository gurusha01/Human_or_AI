Review of the Paper
Summary and Contributions  
The paper introduces ITransF, a novel knowledge base completion (KBC) model that focuses on parameter sharing across relations to address data sparsity, unlike STransE, which uses distinct projection matrices for each relation. ITransF employs a tensor-based approach with a selection vector (\(\alpha\)) to compose relation-specific projection matrices from a shared set of concept matrices. The model incorporates a sparse attention mechanism to enhance interpretability and reduce computational overhead. The authors propose a block iterative optimization algorithm to enforce sparsity and evaluate the model on two benchmark datasets (WN18 and FB15k), demonstrating improvements in mean rank and Hits@10 over previous intrinsic models. The paper claims three main contributions: (1) a novel approach to knowledge transfer via shared statistical regularities, (2) an interpretable sparse attention mechanism, and (3) empirical evidence of effectiveness on benchmark datasets.
Strengths  
1. Addressing Data Sparsity through Parameter Sharing: The paper tackles a critical limitation of previous models like STransE by introducing a shared tensor of concept matrices, which allows rare relations to benefit from statistical strength transfer. This is particularly evident in the performance gains on rare relations in WN18 and FB15k.  
2. Interpretability via Sparse Attention: The sparse attention mechanism provides an interpretable mapping between relations and shared concepts, which is a valuable contribution for understanding how knowledge is transferred. The visualization of attention vectors for reverse and symmetric relations strengthens this claim.  
3. Model Compression: The parameter-sharing mechanism significantly reduces the number of projection matrices required without compromising performance, achieving up to 90x compression on FB15k. This makes the model computationally efficient and scalable.  
Weaknesses  
1. Lack of Justification for \(\ell0\) Regularization: The paper dismisses \(\ell1\) regularization without providing experimental evidence, opting for \(\ell0\) constraints that lead to a complex and crude optimization process. A comparison of \(\ell1\) and \(\ell_0\) regularization would strengthen the justification for the chosen approach.  
2. Contradiction Between Sparse Attention and Information Sharing: The use of sparse attention vectors seems to contradict the goal of maximizing information sharing across relations. This raises concerns about optimization trade-offs and whether sparsity might hinder generalization in certain cases.  
3. Misleading Use of "Attention": The term "attention" for \(\alpha\) is inconsistent with its common usage in NLP, where attention mechanisms typically involve weighted focus on input elements. This could confuse readers and detracts from the clarity of the proposed method.  
4. Fairness Concerns in Initialization: The use of pre-trained TransE embeddings for initialization introduces potential biases, making it difficult to fairly compare ITransF with TransE and other baselines.  
5. Limited Positioning in Related Work: The paper could better position its contributions by referencing prior research on inter-relational correlation and similarity in NLP, which would provide a stronger theoretical foundation for the proposed approach.
Questions to Authors  
1. Why was \(\ell1\) regularization dismissed without experimental evaluation? Could you provide results comparing \(\ell1\) and \(\ell_0\) regularization?  
2. How does the sparsity constraint impact the generalization ability of the model, especially for common relations?  
3. Could you clarify why tensor decomposition methods (e.g., CP or Tucker decomposition) were not considered for better information sharing across relational matrices?  
4. How does the use of pre-trained TransE embeddings affect the fairness of comparisons with other baselines?  
Conclusion  
Overall, the paper presents a promising approach to addressing data sparsity in KBC through parameter sharing and sparse attention. However, the lack of clarity on certain design choices, contradictions in optimization goals, and limited contextualization in related work weaken its overall impact. With additional experiments and clearer positioning, the contributions could be more compelling.