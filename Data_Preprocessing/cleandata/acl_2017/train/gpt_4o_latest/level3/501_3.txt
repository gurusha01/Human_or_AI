Review of the Submitted Paper
Summary and Contributions
This paper introduces the Dual Machine Comprehension (DMC) task, a novel multimodal benchmark designed to evaluate the alignment of visual and linguistic semantic understanding. The authors propose a dataset, MCIC, derived from COCO captions, where the task is to select the most appropriate caption for an image from a set of highly similar decoys. The paper makes the following key contributions:
1. Task Definition and Dataset Creation: The DMC task is framed as a classification problem with a clear evaluation metric (accuracy). The authors present an extensible algorithm for generating challenging decoys and release a large-scale dataset (MCIC) for public use.
2. Human Performance Baseline: Human evaluation on the dataset provides an upper-bound for task performance and highlights the gap between human and machine comprehension.
3. Model Development and Analysis: The paper benchmarks several models, including a novel Vec2seq+FFNN architecture, which combines caption generation and classification in a multitask learning setup. The results demonstrate that comprehension performance correlates positively with caption generation quality.
Strengths
1. Novel Task and Dataset: The DMC task is a well-motivated addition to the multimodal AI landscape, addressing limitations in existing tasks like image captioning and VQA by requiring deeper semantic understanding. The dataset creation process is systematic and ensures challenging decoys, which enhances the task's utility as a benchmark.
2. Clear Evaluation Metric: The use of accuracy as the primary metric makes the task straightforward to evaluate and compare across models, avoiding the pitfalls of subjective or imperfect metrics like BLEU or CIDEr.
3. Insightful Findings: The paper highlights the limitations of caption generation models, which perform worse than random chance on the DMC task, underscoring the need for better semantic alignment between modalities.
4. Multitask Learning Contribution: The Vec2seq+FFNN model demonstrates that multitask learning can improve both comprehension and generation, offering a promising direction for future research.
Weaknesses
1. Missing Baseline: The absence of a state-of-the-art VQA model with a yes/no label vocabulary as a baseline is a significant omission. Such a comparison would provide a clearer understanding of the DMC task's difficulty relative to existing benchmarks.
2. Dataset Analysis: The paper lacks a detailed analysis of the dataset's properties, such as token overlap, linguistic complexity, or the types of visual understanding required. This would help clarify how the DMC task compares to similar tasks in terms of difficulty and novelty.
3. Human Performance Details: The description of human evaluation is insufficient, particularly regarding ambiguous captions and potential data cleaning. More transparency is needed to assess the reliability of the human upper-bound.
4. Architectural Limitations: The neural architectures used in the experiments do not fully leverage state-of-the-art attention mechanisms, which are known to improve performance in multimodal tasks. This limits the generalizability of the findings.
5. Data Generation Novelty: The decoy generation algorithm, while effective, lacks novelty and relies on existing techniques like paragraph vectors and BLEU scores. Its contribution is incremental unless proven to be unexpectedly effective.
Questions to Authors
1. Why was a state-of-the-art VQA model not included as a baseline? How does the DMC task compare to VQA in terms of difficulty and model performance?
2. Could you provide more details on the human evaluation process, particularly how ambiguous captions were handled and whether any data cleaning was applied to improve accuracy?
3. Have you analyzed the MCIC dataset for linguistic or visual biases that might make the task easier or gameable for certain models?
Conclusion
The paper presents a well-motivated task and dataset that address key gaps in multimodal AI research. However, the lack of stronger baselines, detailed dataset analysis, and state-of-the-art architectures limits the impact of the findings. With these improvements, the DMC task could become a valuable benchmark for advancing vision-language understanding.