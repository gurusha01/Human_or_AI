Review
Summary and Contributions
This paper addresses the task of event factuality identification, proposing a two-step supervised framework that combines Bidirectional Long Short-Term Memory (BiLSTM) and Convolutional Neural Networks (CNN) with attention mechanisms. The primary contributions of the work are:  
1. The introduction of an attention-based deep neural network model that combines BiLSTM and CNN for factuality classification, showing improved performance over baselines.  
2. The use of attentional features to extract key syntactic and lexical information, particularly Source Introducing Predicates (SIPs) and cues, which are critical for factuality determination.  
3. The suitability of the proposed model for small training datasets, leveraging attention mechanisms to focus on relevant features.  
While the paper demonstrates incremental improvements over prior work, its novelty is limited, as it builds heavily on existing frameworks and lacks significant methodological innovation.
Strengths
1. Clear Presentation: The paper is well-structured and provides a detailed explanation of the proposed framework, including the design of the neural network and its components (e.g., SIP Path, RS Path, Cue Path). The inclusion of examples and diagrams aids comprehension.  
2. Effective Use of Attention Mechanisms: The attention-based BiLSTM-CNN model effectively captures important syntactic and lexical features, outperforming baselines in identifying speculative and negative factuality values. The results demonstrate the utility of the attention mechanism in improving performance.  
3. Performance on Small Datasets: The model is shown to work well with limited training data, which is a valuable contribution given the scarcity of annotated corpora for event factuality tasks.  
Weaknesses
1. Limited Novelty: The proposed framework lacks significant innovation. The combination of BiLSTM, CNN, and attention mechanisms is well-established in NLP, and the paper does not introduce new techniques or architectures. The claim of "proper combination" of BiLSTM and CNN is vague and unsubstantiated.  
2. Weak Baselines: The baselines used for comparison (e.g., rule-based and MaxEnt models) are outdated and do not reflect the current state-of-the-art in neural NLP. This limits the significance of the reported performance improvements.  
3. Evaluation Gaps: The evaluation deviates from prior work, making it difficult to compare results directly. The omission of results on held-out sets or other corpora like PragBank further weakens the generalizability of the findings.  
4. Unclear Implementation Details: Key terms such as "combined properly" and references to FactML event classes are ambiguous. Additionally, the implementation of de Marneffe et al.'s features is not clearly explained, raising concerns about reproducibility.  
5. Limited Analysis of Inputs: While the paper explores the effects of different inputs (e.g., SIP Path, RS Path), the analysis lacks depth. For example, the reduced effectiveness of separate LSTMs for paths is mentioned but not thoroughly investigated.  
Questions to Authors
1. Can you clarify what is meant by "combined properly" in the context of BiLSTM and CNN? How does this differ from prior work?  
2. Why were stronger baselines (e.g., transformer-based models) not considered for comparison?  
3. Could you provide more details on how the features from de Marneffe et al. were implemented?  
4. Why were results on other corpora like PragBank not included? Would the model generalize well to such datasets?  
5. How were the gold standard events and SIPs annotated or verified?  
Additional Comments
The paper addresses an important NLP task and demonstrates incremental improvements using neural networks. However, the lack of novelty, weak baselines, and limited evaluation reduce its overall impact. Addressing these issues and providing clearer implementation details would strengthen the submission. Minor English usage issues should also be revised for clarity.