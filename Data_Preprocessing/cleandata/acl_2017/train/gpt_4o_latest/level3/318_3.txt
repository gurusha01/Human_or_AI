Review of the Submission
Summary and Contributions
This paper introduces a novel approach to Word Representation Learning (WRL) by incorporating sememe information from HowNet, a linguistic knowledge base. The authors propose three models—Simple Sememe Aggregation (SSA), Sememe Attention over Context (SAC), and Sememe Attention over Target (SAT)—to encode sememe information and address polysemy through attention-based word sense disambiguation. The models are evaluated on word similarity and word analogy tasks, demonstrating significant improvements over baseline methods. The primary contributions of this work are:
1. The first attempt to leverage sememe annotations from HowNet to enhance WRL.
2. The introduction of attention-based mechanisms to handle word sense disambiguation and sememe integration.
3. Extensive empirical validation, showing the effectiveness of sememe-encoded models, particularly SAT, in improving word representations.
Strengths
1. Performance Improvements: The proposed models, particularly SAT, achieve significant gains over baselines on word similarity and analogy tasks. This demonstrates the utility of sememe information in WRL and validates the proposed approach.
2. Polysemy Handling: The paper addresses the critical challenge of polysemy in WRL by introducing attention-based mechanisms for soft word sense disambiguation. This is a meaningful contribution that builds on prior work while introducing a novel perspective through sememe integration.
3. Case Studies: The inclusion of qualitative case studies provides additional insights into the effectiveness of the models, particularly in capturing sense-specific word meanings and context-dependent sememe contributions.
Weaknesses
1. Clarity and Presentation: The paper lacks sufficient clarity in describing the proposed models. For instance, the mathematical formulations and the role of sememe embeddings in SSA, SAC, and SAT are not explained in an intuitive manner, making it challenging for readers to follow the methodology.
2. Evaluation Limitations: The evaluation tasks are context-independent, which creates ambiguity in the claim that the models handle polysemy effectively. While the models account for polysemy during training, the evaluation does not validate sense-specific representations in real-world, context-dependent scenarios.
3. Bias in Word Analogy Evaluation: The explicit encoding of semantic relations through sememes may bias the word analogy task, as sememes inherently encode relational information (e.g., "capital" and "Cuba" for "Havana").
4. Insufficient Baselines: The paper does not include comparisons with models that incorporate only word senses (e.g., WordNet-based models). This omission limits the ability to isolate the benefits of sememe information versus word sense disambiguation.
5. Empirical Gaps: The claim that the models benefit low-frequency words is not empirically supported. The datasets used do not provide sufficient evidence to substantiate this claim.
Questions to Authors
1. How are sememe embeddings initialized and trained? Is pre-training required, and if so, how does it affect the results?
2. Can you provide more details on the parameter tuning process, particularly for determining K and K'? What motivated the chosen values?
3. How does the hierarchical structure of sememes in HowNet influence the results? Have you considered leveraging this structure in future work?
4. Could you clarify the role of sememe attention in SAC and SAT? Specifically, how does it compare to traditional word sense disambiguation methods?
Recommendation
While the paper presents a novel and promising approach to WRL by incorporating sememe information, the lack of clarity in methodology, limited baselines, and context-independent evaluation reduce its overall impact. I recommend conditional acceptance, provided the authors address the clarity issues, include additional baselines, and provide empirical evidence for claims such as benefits for low-frequency words.