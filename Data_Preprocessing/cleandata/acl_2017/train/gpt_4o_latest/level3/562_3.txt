Review
Summary and Contributions
This paper proposes a novel framing of relation extraction (RE) as a reading comprehension (RC) task, enabling zero-shot generalization to unseen relations. The method involves two main components: (1) converting relations into natural-language questions via crowdsourcing, and (2) adapting an RC model to handle cases where no answer exists in the text. The authors claim that this approach allows for scalable data generation, improved generalization to unseen relations, and the potential to leverage advances in RC models for RE tasks. The paper also introduces a large dataset generated through schema querification and evaluates the approach on a Wikipedia slot-filling task.
The primary contributions of the paper are:
1. Reduction of Relation Extraction to Reading Comprehension: The paper demonstrates that RE can be reframed as an RC problem, enabling the use of RC models for zero-shot RE.
2. Efficient Data Generation via Schema Querification: The authors introduce a cost-effective method for generating large-scale training data by annotating relations rather than individual instances.
3. Adaptation of RC Models for Answerability: The paper extends an existing RC model to handle unanswerable questions, a necessary feature for RE tasks.
Strengths
1. Clarity and Writing: The paper is well-written, with a clear explanation of the methodology, experimental setup, and results. The use of illustrative examples (e.g., question templates and answer spans) aids comprehension.
2. Interesting Framing: The reduction of RE to RC is an innovative perspective that aligns with recent trends in leveraging RC models for diverse NLP tasks. This framing also opens up opportunities to apply advances in RC to RE.
3. Scalable Data Generation: The schema querification process is a significant contribution, as it allows for the creation of a large dataset (30M examples) at a low cost, addressing a common bottleneck in supervised RE.
Weaknesses
1. Lack of Novelty: While the framing of RE as RC is interesting, the approach heavily relies on prior work, such as crowdsourcing for question generation and adapting existing RC models. The novelty of the method is limited.
2. Weak Empirical Results: The performance on the core task, especially in the zero-shot setting, is unconvincing. For unseen relations, the F1 score of 41% is modest, and the lack of robust generalization raises concerns about the practical utility of the approach.
3. No Comparison with State-of-the-Art: The paper does not compare its method against existing state-of-the-art RE systems, making it difficult to assess its relative effectiveness. Such comparisons are critical for evaluating the contribution.
4. Unclear Data Preparation: The slot-filling data preparation method, particularly how answer sentences are located, is insufficiently detailed. The authors should reference prior work (e.g., Wu and Weld, 2010) to clarify this process.
5. Limited Analysis of Failure Cases: While the paper provides some analysis of errors, it lacks a thorough investigation into the limitations of the approach, particularly in handling distractors or relation-specific variability.
Questions to Authors
1. How does the performance of your method compare to existing state-of-the-art RE systems on the same dataset or task? Can you provide quantitative comparisons?
2. Could you clarify the process for locating answer sentences in the slot-filling data preparation? How does it differ from prior work such as Wu and Weld (2010)?
3. What specific challenges did the model face in the zero-shot setting, and how might these be addressed in future work?
Recommendation
While the paper presents an interesting approach to RE and introduces a scalable data generation method, the lack of novelty, unconvincing empirical results, and absence of comparisons with state-of-the-art systems make it unsuitable for publication in its current form. Addressing these issues, particularly through stronger experimental validation and comparative analysis, would significantly strengthen the paper.