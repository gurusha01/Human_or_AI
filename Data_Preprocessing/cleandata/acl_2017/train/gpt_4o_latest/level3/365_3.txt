Review of the Paper: Historical Text Normalization Using Encoder-Decoder Architectures
Summary and Contributions:  
This paper explores the application of encoder-decoder architectures with attention mechanisms and multi-task learning (MTL) for historical text normalization. The authors propose several neural architectures, including a novel MTL approach that leverages grapheme-to-phoneme mappings as an auxiliary task. Their method achieves a 2% improvement over state-of-the-art baselines on 44 Early New High German datasets. The paper also presents a detailed analysis of the relationship between MTL and attention mechanisms, hypothesizing that MTL can implicitly learn attention-like behavior. The authors make their implementation publicly available, contributing to reproducibility and further research.
Key contributions include:  
1. Demonstrating the efficacy of encoder-decoder architectures for historical text normalization, outperforming traditional baselines.  
2. Introducing a multi-task learning setup that improves performance without requiring explicit attention mechanisms.  
3. Providing an insightful analysis of the interplay between MTL and attention, advancing the understanding of these techniques.
Strengths:  
1. Robust Experimental Setup: The paper evaluates multiple architectures across 44 datasets, ensuring comprehensive and reproducible results. The inclusion of baselines and ablation studies strengthens the validity of the findings.  
2. Qualitative and Quantitative Analysis: The authors provide compelling evidence for their hypothesis about MTL and attention through parameter analysis, saliency maps, and error resolution comparisons. This analysis is a valuable contribution to understanding MTL's role in neural architectures.  
3. State-of-the-Art Results: The proposed models achieve significant improvements over existing baselines, demonstrating the practical utility of the approach.  
4. Clarity and Accessibility: The paper is well-written and provides sufficient methodological details, making it accessible to both researchers and practitioners.  
Weaknesses:  
1. Limited Novelty in Models: While the application of MTL to historical text normalization is novel, the underlying models (e.g., sequence-to-sequence with attention) are well-established in the literature. The primary innovation lies in the application rather than the methodology.  
2. Assumption of Alignment in Hidden Spaces: The paper assumes that the hidden space alignment between MTL and attention models is comparable, but this remains insufficiently clarified. The authors are encouraged to address this in more depth.  
3. Lack of Comparison with Azawi's Model: The omission of a direct comparison with Azawi et al.'s bi-LSTM model for historical text normalization is a notable gap, as it would provide a more comprehensive evaluation of the proposed approach.  
Questions to Authors:  
1. Could you provide additional evidence or theoretical justification for the assumption of alignment in hidden layer spaces between the MTL and attention models?  
2. How do the two models (MTL and attention) independently resolve errors, and what are the implications for their combined use?  
3. Why was Azawi's model excluded from the comparison, and how do you anticipate your approach would perform relative to it?  
Conclusion:  
This paper makes a strong case for the use of encoder-decoder architectures and MTL in historical text normalization, achieving state-of-the-art results and offering valuable insights into the relationship between MTL and attention mechanisms. However, the limited novelty in the core models and the lack of comparison with Azawi's work slightly diminish its impact. Overall, the paper is a solid contribution to the field, particularly for its application-driven focus and detailed analysis.