Review of the Submission
Summary and Contributions:  
This paper provides a systematic investigation into the variation in performance of word embedding models based on different context types (linear vs. dependency-based) and context representations (bound vs. unbound). The authors evaluate these configurations across six tasks, including intrinsic evaluations (word similarity and analogy) and extrinsic tasks (POS tagging, chunking, NER, and text classification). The study's primary contribution lies in its comprehensive comparison of context types and representations, offering insights into their suitability for different tasks. Additionally, the authors generalize popular embedding models (Skip-Gram, CBOW, and GloVe) to support these configurations and provide an open-source toolkit for reproducibility.
Strengths:  
1. Systematic Investigation: The paper systematically explores the interplay of context types, representations, and tasks, addressing a gap in the literature. This level of granularity is valuable for the ACL community, given the increasing reliance on embeddings in NLP.  
2. Relevance and Novelty: The study addresses a pertinent question—what constitutes the "best" context for learning embeddings—building on well-cited prior work while extending the scope to include context representations.  
3. Comprehensive Evaluation: The authors evaluate embeddings across a diverse set of tasks, ensuring that the findings are broadly applicable. The inclusion of both intrinsic and extrinsic tasks strengthens the study's practical relevance.  
4. Reproducibility: The release of the word2vecPM toolkit and detailed experimental setup enhances transparency and encourages further research.
Weaknesses:  
1. Lack of Hyperparameter Tuning: The absence of hyperparameter tuning undermines the validity of the comparisons. For instance, differences in performance may stem from suboptimal configurations rather than inherent properties of the contexts or representations.  
2. Inconsistent Classifiers: The use of neural classifiers for text classification and linear classifiers for sequence labeling is inconsistent and lacks justification. This raises concerns about the reliability of the results, as the choice of classifier can significantly impact performance.  
3. Unclear Explanations: Some explanations for the results are contradictory or insufficiently detailed. For example, the claim that bound representations "already contain syntactic information" is not adequately substantiated, leaving the reader uncertain about the underlying mechanisms.  
4. Insufficient Citations: The paper fails to adequately cite and build upon related prior work, such as Nayak et al. (2016), which provides recommendations relevant to the study's scope. This omission weakens the contextualization of the contributions.  
5. Analysis Depth: While the authors present 120 accuracy values, the analysis could benefit from advanced techniques like factor analysis or pattern mining to uncover deeper patterns and insights.
Questions to Authors:  
1. How do you justify the use of different classifiers for text classification and sequence labeling tasks? Would consistent classifiers affect the observed trends?  
2. Why was hyperparameter tuning omitted, and how do you address the potential bias this introduces in comparing models?  
3. Can you provide more detailed explanations or evidence for the claim that bound representations hinder syntactic learning?  
Recommendation:  
While the paper addresses an important problem and offers valuable insights, the methodological weaknesses—particularly the lack of hyperparameter tuning and inconsistent classifiers—limit the reliability of the findings. Addressing these issues during the author response period could significantly strengthen the submission.