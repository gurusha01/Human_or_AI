Review of the Submission
Summary and Contributions
This paper addresses the critical issue of incomplete coverage in Head-driven Phrase Structure Grammar (HPSG) systems, proposing and evaluating several techniques for robust parsing. The primary contributions of the work are:
1. Development of Robust Parsing Techniques: The paper introduces five distinct methods for enhancing the robustness of the English Resource Grammar (ERG), including bridging, Pacman, PCFG-based approaches, and hybrid methods. These techniques aim to mitigate coverage gaps while maintaining semantic precision.
   
2. Creative Evaluation Dataset Construction: The authors propose novel methodologies for creating evaluation datasets, such as the "alchemy" process, which fabricates gold-standard annotations for previously unparseable sentences. This contribution is particularly valuable for evaluating robust parsing methods in the absence of readily available gold-standard data.
3. Empirical Comparison of Techniques: The paper provides a detailed empirical evaluation of the proposed methods across four datasets, offering insights into their trade-offs in terms of coverage, accuracy (EDM F1), and computational efficiency.
Strengths
1. Novel Dataset Creation Methodology: The "alchemy" approach for generating gold-standard annotations is innovative and could be applied to other grammar frameworks, making it a significant contribution to the field.
   
2. Clear and Engaging Writing: The paper is exceptionally well-written, with a clarity and engagement level that surpasses the typical standard of ACL publications. This makes the technical content accessible and enjoyable to read.
3. Relevance of Hand-Written Grammars: The work highlights the continued importance of hand-written precision grammars, which are often overshadowed by statistical methods, and demonstrates their potential for integration with robust parsing techniques.
Weaknesses
1. Small Evaluation Datasets: The datasets used for evaluation, particularly alchemy45, are small, which limits the statistical reliability of the results. This is especially concerning given the variability in performance across datasets.
2. Limited Metrics and Analysis: The evaluation is restricted to F1 and coverage scores, without deeper analysis such as error breakdowns or the impact of specific grammatical constructions. This limits the interpretability and broader applicability of the results.
3. Unclear Proportion of Out-of-Coverage Items: The paper does not clearly quantify the proportion of out-of-coverage items due to factors like resource limitations, long-tail grammar phenomena, or extra-grammatical elements. This makes it difficult to assess the true scope of the problem being addressed.
Questions to Authors
1. Could you clarify the proportion of out-of-coverage items attributable to resource limitations versus linguistic gaps in the grammar? This would help contextualize the robustness improvements.
   
2. Have you considered conducting an error analysis to identify specific grammatical constructions or phenomena where each method performs well or poorly? Such insights could guide future improvements.
3. How do you envision the proposed techniques being integrated into downstream applications? Are there specific use cases where the trade-offs in speed and accuracy are particularly favorable?
Recommendation
This paper makes meaningful contributions to the field of computational linguistics, particularly in the area of robust parsing for precision grammars. However, the limitations in dataset size and evaluation depth reduce the overall impact of the findings. I recommend acceptance with minor revisions, focusing on expanding the evaluation and providing more detailed analysis of the results.