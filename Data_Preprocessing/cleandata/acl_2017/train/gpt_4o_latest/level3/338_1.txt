Review of the Submission
Summary and Contributions
This paper addresses the cold-start problem in review spam detection, a significant and underexplored area in combating online review fraud. The authors propose a novel neural network model that jointly embeds textual and behavioral information to detect spam reviews from new reviewers with minimal behavioral data. The key contributions of the paper are:
1. Novel Problem Definition: The paper is the first to explicitly focus on the cold-start problem in review spam detection, highlighting the limitations of traditional linguistic and behavioral features in such scenarios.
2. Proposed Model: The authors introduce a neural network model that encodes textual and behavioral information into review embeddings, leveraging global behavioral footprints from existing reviewers in an unsupervised manner.
3. Experimental Validation: The model demonstrates strong performance on the Yelp dataset across two domains (hotel and restaurant), outperforming traditional methods and intuitive baselines. The results also indicate the model's domain adaptability and scalability.
Strengths
1. Thorough Related Work: The paper provides a comprehensive review of prior work, effectively situating the proposed method within the broader context of review spam detection.
2. Strong Hypothesis and Evaluation: The hypothesis that textual and behavioral information are correlated is well-supported by experimental results. The evaluation is extensive, covering multiple baselines, metrics, and domains, which strengthens the validity of the claims.
3. Innovative Approach: The joint embedding of textual and behavioral information, combined with the use of a review graph and unsupervised learning, represents a novel and compelling solution to the cold-start problem.
4. Clear Writing: Despite minor grammatical issues, the paper is well-structured, with clear explanations of the methodology and results.
Weaknesses
1. Lack of Dataset Details: The paper does not provide sufficient details about the training dataset, such as its size and time frame. This omission limits the interpretability of the results in Section 3, as readers cannot fully assess the scope and generalizability of the experiments.
2. Reviewer Statistics: The paper lacks a detailed discussion of key reviewer statistics, such as the number of labeled reviewers and the average number of reviews per person. These statistics are crucial for understanding the dataset's characteristics and the model's performance.
3. Minor Grammatical Issues: There are minor grammatical inconsistencies, such as inconsistent tense usage and a preference for long forms over contractions, which slightly detract from the readability.
Questions to Authors
1. Can you provide more details about the training dataset, including its size, time frame, and any preprocessing steps?
2. How many labeled reviewers were included in the experiments, and what was the average number of reviews per reviewer?
3. Could you elaborate on the computational efficiency of the proposed model, particularly in terms of training time and scalability to larger datasets?
Recommendation
The paper presents a novel and well-validated approach to an important problem in review spam detection. While the lack of dataset details and reviewer statistics are notable weaknesses, they do not undermine the overall contribution of the work. I recommend acceptance with minor revisions to address the dataset and reviewer statistics issues.