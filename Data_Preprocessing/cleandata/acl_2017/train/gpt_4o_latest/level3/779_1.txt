Review
Summary and Contributions  
This paper introduces a novel teacher-student framework for zero-resource neural machine translation (NMT), leveraging source-pivot and pivot-target corpora to train a source-to-target model without requiring direct parallel data. The proposed method outperforms standard pivot-based approaches by +3.0 BLEU points across various language pairs. The primary contributions of this work are:  
1. Novel Framework for Zero-Resource Translation: The teacher-student approach directly models source-to-target translation, avoiding the error propagation and inefficiency of traditional pivot-based methods.  
2. Minimal Assumptions for Broad Applicability: The method is agnostic to specific NMT architectures, making it widely applicable across different systems.  
3. Empirical Validation of Sentence- and Word-Level Teaching: The paper demonstrates the benefits of sentence-level beam search and word-level sampling for improving translation quality and diversity.  
Strengths  
1. Innovative Approach: The teacher-student framework is a significant departure from traditional pivot-based methods, addressing their key limitations (e.g., error propagation and inefficiency). This innovation is supported by strong empirical results, with BLEU score improvements of up to 3.29 and 3.15 for Spanish-French and German-French translations, respectively.  
2. Robust Evaluation: The method is evaluated on both Europarl and WMT datasets, demonstrating its scalability and effectiveness across different corpora and language pairs. The inclusion of comparisons with state-of-the-art methods highlights its superiority.  
3. Practical Applicability: By requiring only source-pivot and pivot-target corpora, the method is highly practical for real-world scenarios, especially for low-resource languages. The experiments on low-resource source-pivot corpora further underscore its utility.  
4. Diversity in Training: The exploration of word-level sampling introduces diversity in training data, which is shown to enhance performance, particularly in low-resource settings.  
Weaknesses  
1. Limited Testing on Dissimilar Languages: While the method is robust for related languages (e.g., Spanish-French, German-French), its performance on more dissimilar language pairs remains unexplored. This is critical for assessing its generalizability.  
2. Unexplained Observations: The paper does not adequately explain why word beam search underperforms word greedy or why sentence-level models exhibit more peaked distributions. These gaps in analysis weaken the interpretability of the results.  
3. High Variance in Sentence-Level Beam Search: The claimed advantage of sentence-level beam search may be influenced by noise due to high variance in results, which is not thoroughly addressed.  
4. Uniform Distribution Comparison: The comparison with a uniform distribution is less informative than comparisons with models trained on varying amounts of data, which would provide a more nuanced understanding of the method's strengths.  
Questions to Authors  
1. How does the proposed method perform on more dissimilar source-pivot language pairs (e.g., Chinese-English-French)?  
2. Could you clarify how the current context is determined for K=1 and K=5 comparisons in beam search?  
3. Why does word beam search perform worse than word greedy? Is this related to the diversity introduced by sampling?  
4. Can you provide more insights into the high variance observed in sentence-level beam search results?  
Additional Comments  
- Consider rephrasing "Despite its simplicity" to "Due to its simplicity" for clarity.  
- The transition from word-based to sentence-based diversity as the student model converges is an interesting idea but is not fully explored. Future work could investigate this transition in greater detail.  
Recommendation: Accept with minor revisions. The paper presents a significant contribution to zero-resource NMT, but addressing the weaknesses and clarifying the questions raised would strengthen its impact.