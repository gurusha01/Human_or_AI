Review
Summary and Contributions  
This paper addresses the practical problem of automated short-answer scoring (SAS) for Japanese, a language that has received less attention in SAS research compared to English. The authors present a system that combines machine learning (Random Forests) with human oversight to score short answers in social studies tests. The system evaluates semantic similarity and surface-level agreement between model and student answers, providing tentative scores that human raters can adjust. The primary contribution of this work is the development of a prototype system tailored to Japanese SAS, which incorporates both semantic and lexical features. Additionally, the paper provides a detailed description of the system's architecture and its application to a trial dataset.
Strengths  
1. Focus on Japanese SAS: The paper tackles an underexplored problem in automated scoring for Japanese, a language with unique linguistic challenges. This focus is valuable given the dominance of English-based research in the field.  
2. Human-AI Collaboration: The system's design, which allows human raters to refine automated scores, is pragmatic and aligns with the current limitations of AI in understanding nuanced semantics.  
3. Prototype Implementation: The authors demonstrate a working prototype and provide some experimental results, showing the feasibility of their approach.  
Weaknesses  
1. Lack of Depth in Experiments: The paper primarily describes the system without conducting sufficient experiments to validate its effectiveness. For instance, feature ablation studies, algorithm comparisons, or qualitative evaluations (e.g., user studies) are absent. This limits the paper's scientific contribution.  
2. Limited Engagement with Related Work: The authors show limited familiarity with recent advancements in SAS, particularly in English. They do not discuss or compare their approach with state-of-the-art methodologies, such as those from recent Kaggle competitions on SAS.  
3. Inappropriate Metric: The use of accuracy as the primary evaluation metric is suboptimal for ordinal human scores. Metrics like Pearson/Spearman correlation or kappa scores would be more appropriate for assessing agreement with human raters.  
4. Overemphasis on System Description: The paper focuses heavily on system implementation details but provides minimal insights into the challenges or innovations in Japanese SAS. This makes the work appear more like a technical report than a research paper.  
Questions to Authors  
1. Why was Random Forest chosen over other machine learning models, such as neural networks, which have shown strong performance in SAS tasks?  
2. Can you provide more details on the semantic similarity features used? How were they computed, and how do they compare to state-of-the-art methods?  
3. How does your system handle cases where multiple correct answers are possible, especially in Japanese, where expressions can vary significantly?  
Additional Comments  
The paper has potential but requires significant improvements in experimental rigor and engagement with related work. Expanding the evaluation to include diverse metrics and conducting more comprehensive experiments would strengthen the contribution. Additionally, situating the work within the broader SAS literature would enhance its relevance to the research community.