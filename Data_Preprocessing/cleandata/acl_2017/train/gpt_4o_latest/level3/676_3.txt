Review of the Paper
Summary and Contributions
This paper introduces a novel method for reducing the computational complexity of the output layer in neural machine translation (NMT) systems by predicting binary codes for words instead of using the traditional softmax approach. The primary contributions of the paper are:  
1. Binary Code Prediction for Words: The authors propose a method to represent words as binary bit arrays, reducing the computational complexity of the output layer to \(O(\log V)\), where \(V\) is the vocabulary size.  
2. Hybrid Model: A hybrid approach is introduced, combining softmax for frequent words and binary code prediction for rare words, leveraging the strengths of both methods.  
3. Error-Correcting Codes: The paper incorporates convolutional error-correcting codes to improve the robustness of binary code predictions, addressing issues of prediction errors.  
4. Efficiency Gains: The proposed methods significantly reduce memory requirements and improve decoding speed, especially on CPUs, while maintaining competitive BLEU scores compared to softmax-based models.  
Strengths
1. Novelty and Efficiency: The binary code prediction method is a novel approach to reducing the computational and memory overhead of the output layer in NMT systems. The hybrid model and error-correcting codes further enhance the robustness and practicality of the approach, making it a potentially impactful contribution to the field.  
2. Clarity and Presentation: The paper is well-written, with clear explanations of the proposed methods. The inclusion of effective diagrams, such as the comparison of output layers and training curves, aids understanding. The loss curves are particularly appreciated for providing insights into model convergence.  
3. Practical Impact: The proposed methods achieve significant reductions in memory usage (up to 1/1000 of the original size) and decoding speed improvements (up to 20x on CPUs), which are critical for deploying NMT systems in resource-constrained environments.  
Weaknesses
1. Lack of Comparative Evaluation: The paper does not compare its method against related approaches like hierarchical softmax or differentiated softmax using BLEU scores, which would provide a more comprehensive evaluation of its effectiveness.  
2. Linguistic Perspective: The theoretical basis and linguistic implications of using binary code prediction versus softmax are not discussed. For example, how natural or interpretable are the binary predictions in the context of language modeling?  
3. Modest Training Speed Gains: While the method achieves impressive decoding speed improvements, the training speed gains are less than 2x, which is relatively modest. A detailed breakdown of output layer computation time would help clarify this limitation.  
4. Survey of Prior Work: The discussion of prior work does not explicitly relate to the desiderata outlined in the introduction, making it harder to assess how the proposed method addresses gaps in the literature.  
5. Qualitative Analysis: The paper lacks an analysis of the qualitative strengths and weaknesses of binary code prediction compared to softmax variants, which would provide deeper insights into the trade-offs involved.  
Questions to Authors
1. How does the proposed method compare to hierarchical and differentiated softmax in terms of BLEU scores and computational efficiency?  
2. Can you provide a more detailed breakdown of the output layer computation time to better understand the modest training speed gains?  
3. What are the linguistic implications of using binary code prediction? How does it affect the naturalness of the generated translations?  
4. Could you elaborate on the rationale behind the choice of convolutional codes and whether alternative error-correcting codes were considered?  
Additional Comments
- Clarify the distinction between \(id(w) = id(w')\) and \(w = w'\) in Equation 5.  
- Define "GPGPU" on line 335 for readers unfamiliar with the term.  
- Highlight the best BLEU scores in bold in Table 3 for easier comparison.  
- Remind readers of the definitions of \(q\) and \(b\) in Equation 15 for clarity.  
Recommendation
The paper presents a novel and efficient approach to reducing the computational complexity of NMT output layers, with significant practical implications. However, the lack of comparative evaluations, theoretical discussions, and qualitative analyses limits its impact. I recommend acceptance with minor revisions, contingent on addressing the weaknesses and clarifying the questions raised.