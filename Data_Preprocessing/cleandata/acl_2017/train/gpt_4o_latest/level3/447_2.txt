Review
Summary and Contributions
This paper explores the integration of discourse structure, as defined by Rhetorical Structure Theory (RST), into deep neural network (DNN)-based text categorization models. The authors propose a recursive neural network architecture that leverages discourse dependency trees and introduces a novel, unnormalized attention mechanism to compute document representations. The paper evaluates the model on five datasets across different tasks and compares it with state-of-the-art baselines. The main contributions of the paper are:
1. Integration of Discourse Structure into Neural Models: The paper demonstrates a novel approach to incorporating discourse structure into neural models for text categorization, using a recursive neural network and attention mechanism tailored to RST trees.
2. Comprehensive Evaluation: The authors conduct experiments on diverse datasets, providing insights into the utility of discourse structure across different genres and tasks.
3. Analysis of Parsing Quality and Model Variants: The paper investigates the impact of discourse parsing quality on classification performance and compares multiple model variants, offering a nuanced understanding of when discourse structure is beneficial.
Strengths
1. Novel Use of Discourse Structure in Attention Mechanisms: The integration of RST into the attention mechanism is a creative and theoretically motivated approach. The unnormalized attention mechanism aligns well with the RST framework, which does not impose competition among sibling nodes.
2. Thorough Evaluation: The paper evaluates its models on five datasets, including sentiment analysis, framing, and legislative prediction tasks, and compares them with strong baselines. This breadth of evaluation strengthens the generalizability of the findings.
3. Clear Explanations of RST and Model Design: The authors provide a detailed explanation of RST and its transformation into dependency trees, making the methodology accessible to readers unfamiliar with discourse parsing.
4. Insightful Analysis: The analysis of parsing quality and its impact on downstream tasks is particularly valuable, as it highlights the importance of improving discourse parsers for better task performance.
Weaknesses
1. Limited Gains in Performance: The results do not strongly support the claim that discourse structure significantly improves text classification. The FULL model outperforms baselines on only one dataset (Yelp), while the simpler UNLABELED model achieves better results on others. In some cases (e.g., legislative bills), discourse structure negatively impacts performance.
2. Data Intensity of the FULL Model: The FULL model is highly data-intensive, requiring large datasets to avoid overfitting. This limits its applicability to smaller datasets, and the paper does not propose a concrete solution to mitigate this issue.
3. Missed Opportunities for Future Directions: The paper does not explore potential extensions, such as task-independent discourse embeddings or end-to-end optimization of discourse parsing with the main task. These could have strengthened the paper's contributions and provided a roadmap for future work.
Questions to Authors
1. Could you provide more details on why the FULL model underperforms on smaller datasets, despite its theoretically stronger design?
2. Have you considered exploring task-independent discourse embeddings that could generalize across datasets and tasks?
3. How sensitive is the model to errors in discourse parsing, particularly for genres where the parser is less accurate (e.g., legislative bills)?
Recommendation
While the paper presents a promising approach to integrating discourse structure into neural models, the limited empirical gains and lack of significant performance improvements weaken its impact. The work is a valuable initial step, but further refinement and exploration of the proposed ideas are needed to establish their broader utility. I recommend acceptance as a workshop paper or a poster presentation, contingent on addressing the concerns raised above.