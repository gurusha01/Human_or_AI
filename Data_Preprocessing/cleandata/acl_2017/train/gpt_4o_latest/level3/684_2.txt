Review of the Paper
Summary and Contributions
This paper introduces the Gated-Attention (GA) Reader, a novel model for reading comprehension tasks, particularly cloze-style question answering. The GA Reader combines a multi-hop architecture with a gated-attention mechanism that leverages multiplicative interactions between query embeddings and document token representations. This approach enables the model to iteratively refine token representations and focus on query-relevant information across multiple layers. The paper demonstrates the model's effectiveness by achieving near state-of-the-art or state-of-the-art results on four benchmark datasets: CNN, Daily Mail, CBT (NE and CN), and Who Did What (WDW). Key contributions include:
1. Gated-Attention Mechanism: The introduction of a fine-grained attention mechanism that enables query-specific filtering of token representations at the semantic level.
2. Empirical Validation: Extensive experiments show that the GA Reader outperforms competitive baselines and ensemble models on multiple datasets, with improvements of up to 4%.
3. Ablation Studies and Analysis: The authors provide detailed ablation studies and attention visualizations, which highlight the importance of gated-attention and multi-hop reasoning in improving model performance.
Strengths
1. Novelty of the Gated-Attention Mechanism: The proposed mechanism effectively integrates query-specific information into token representations, addressing limitations of prior attention mechanisms. The use of multiplicative interactions is empirically validated as superior to addition or concatenation.
2. Strong Empirical Results: The GA Reader achieves competitive or state-of-the-art performance on multiple datasets, including CNN, Daily Mail, and WDW. The results are robust across different settings, such as with or without feature engineering.
3. Comprehensive Evaluation: The authors conduct thorough ablation studies, demonstrating the impact of key components like token-specific attention, character embeddings, and pre-trained GloVe vectors. This strengthens the paper's claims about the effectiveness of the proposed model.
4. Interpretability through Attention Visualization: The visualization of attention distributions across layers provides valuable insights into the model's reasoning process and supports the utility of the multi-hop architecture.
Weaknesses
1. Limited Comparison to Related Work: While the paper references several prior works, it lacks a direct experimental comparison with models like those proposed by Caiming Xiong et al. and Shuohang Wang et al., which also focus on enhancing attention mechanisms. Including these comparisons would provide a clearer picture of the GA Reader's relative strengths.
2. Insufficient Theoretical Justification: Although the empirical results validate the effectiveness of multiplicative gating, the paper does not provide a theoretical explanation for why this operation outperforms alternatives like addition or concatenation.
3. Dataset-Specific Observations: The paper notes that feature engineering (e.g., qe-comm) and fixed word embeddings improve performance on smaller datasets like WDW but not on larger ones like CNN and Daily Mail. However, these observations are not explored in depth, leaving open questions about the generalizability of the model's design choices.
Questions to Authors
1. How does the GA Reader compare to models by Caiming Xiong et al. and Shuohang Wang et al., which also focus on advanced attention mechanisms? Could you include these comparisons in the final version?
2. Can you provide a theoretical explanation or intuition for why multiplicative gating outperforms addition and concatenation in this context?
3. The paper notes that feature engineering and fixed embeddings are less effective on larger datasets. Could you elaborate on why this might be the case and whether this trend holds for other reading comprehension tasks?
Recommendation
This paper presents a strong contribution to the field of reading comprehension with its novel gated-attention mechanism and robust empirical results. However, addressing the weaknesses outlined above, particularly the lack of comparisons with closely related work, would further strengthen the submission. I recommend acceptance with minor revisions.