Review of the Paper
Summary and Contributions
This paper addresses the challenging task of knowledge base-based question answering (KB-QA) by proposing a novel cross-attention neural network model. The key contribution lies in the dynamic representation of questions based on candidate answer aspects, which is achieved through a cross-attention mechanism. Unlike prior neural network (NN)-based KB-QA approaches, which often rely on fixed question representations, this method allows for mutual influence between questions and answers, enhancing expressiveness and flexibility. Additionally, the authors integrate global knowledge from the knowledge base (KB) using TransE embeddings, which alleviates the out-of-vocabulary (OOV) problem and incorporates global KB structure into the model. Experimental results on the WebQuestions dataset demonstrate that the proposed approach outperforms state-of-the-art end-to-end methods, with ablation studies highlighting the contributions of the cross-attention mechanism and global KB information.
Strengths
1. Novelty and Relevance: The paper makes a significant contribution to the KB-QA field by introducing a cross-attention mechanism that dynamically captures the interaction between questions and candidate answers. This is a novel and promising direction for improving NN-based KB-QA systems.
2. Integration of Global Knowledge: The use of TransE embeddings to incorporate global KB information is a strong addition. This not only enhances answer representations but also mitigates the OOV problem, which is a common challenge in KB-QA tasks.
3. Comprehensive Evaluation: The experimental results are robust, showing that the proposed approach outperforms existing end-to-end methods. The ablation studies provide clear evidence of the impact of individual components, such as the cross-attention mechanism and global KB information.
4. Clarity and Structure: The paper is well-structured, with a clear explanation of the methodology, including detailed descriptions of the cross-attention mechanism and training process. The use of visualizations, such as the attention heat map, aids in understanding the model's behavior.
5. Practical Impact: The framework is entirely end-to-end and does not rely on manually designed rules or external resources, making it more adaptable and scalable compared to integrated systems.
Weaknesses
1. Minor Typos: The paper contains minor typographical errors, such as "re-read" instead of "reread" and "pairs be" instead of "pairs to be." While these do not detract from the technical quality, they should be corrected for clarity.
2. Lack of Naming for the Approach: The authors do not provide a specific name for their proposed model, which could hinder its identification and citation in future work. A concise name, such as "CA-LSTM" (Cross-Attention LSTM), would improve its visibility.
3. Performance Comparison: While the proposed approach outperforms other end-to-end methods, its performance is still weaker than semantic parsing-based (SP-based) or integrated systems that leverage external resources. This limitation should be acknowledged more explicitly in the discussion.
4. Equation Design: In Equation 2, the reviewer suggests using separate weights (W) and biases (b) for different candidate answer aspects to enhance the flexibility of the model.
Questions to Authors
1. Have you considered using separate weights and biases for different candidate answer aspects in Equation 2? If not, could you elaborate on why a shared parameterization was chosen?
2. Could you provide additional details on how the margin Î³ was determined during training? Was it tuned on the validation set or fixed based on prior work?
3. Are there any plans to extend this approach to handle more complex questions, such as those requiring aggregation or temporal reasoning?
Recommendation
This paper presents a novel and effective approach to KB-QA, with strong experimental results and a well-motivated methodology. While there are minor weaknesses, they do not detract significantly from the overall quality of the work. I recommend acceptance, with minor revisions to address the typographical errors and suggestions for improvement.