Review of the Paper
Summary and Contributions
This paper explores the application of sequence-to-sequence (seq2seq) models for Abstract Meaning Representation (AMR) parsing and realization. The authors propose a novel paired training procedure that leverages large-scale unlabeled data through self-training and back-translation, addressing data sparsityâ€”a key challenge in AMR tasks. The paper demonstrates competitive performance in AMR parsing (61.9 SMATCH) and achieves state-of-the-art results in AMR realization (32.3 BLEU), outperforming prior methods by over 5 BLEU points. Key contributions include:
1. A paired training procedure that combines self-training and back-translation to improve both AMR parsing and realization.
2. A preprocessing pipeline that anonymizes named entities, reduces sparsity, and introduces scope markers to improve seq2seq model performance.
3. Extensive ablation and qualitative analyses, showing the robustness of seq2seq models to graph linearization artifacts and highlighting the importance of preprocessing.
Strengths
1. Innovative Use of Back-Translation and Self-Training: The paper effectively demonstrates how back-translation with additional monolingual data can significantly enhance AMR realization performance. This is a noteworthy contribution, as it addresses the data sparsity issue in a principled manner.
2. Comprehensive Ablation Studies: The authors provide detailed analyses of preprocessing components (e.g., anonymization, scope markers) and linearization strategies, offering valuable insights into the factors driving model performance.
3. State-of-the-Art Results in AMR Realization: The paper achieves a substantial improvement over previous methods, demonstrating the practical utility of the proposed approach.
4. Robustness to Linearization Artifacts: The finding that seq2seq models can learn to ignore artifacts introduced by graph-to-sequence conversion is significant, as it suggests flexibility in graph representation for seq2seq tasks.
Weaknesses
1. Empirical Comparisons Are Not Fully Robust: The claims of improvement over state-of-the-art methods are weakened by differences in training data. For example, the comparison with PBMT for realization is invalid due to the use of different datasets. A re-evaluation on the same data is necessary for a fair comparison.
2. Potential Test Set Contamination: The use of Gigaword as an external corpus raises concerns about test set contamination, as overlapping data sources may inflate performance metrics. This issue needs verification.
3. Lack of Clarity in Encoder Modifications: The paper introduces modifications to the encoder but does not adequately explain their significance or motivation, leaving readers uncertain about their impact on performance.
4. Missing Implementation Details: Key details such as the seq2seq framework, sequence length, and decoding method are omitted, hindering reproducibility.
5. Inconsistencies in Presentation: Table 1 lacks proper labeling, and there are mismatches between table content and text descriptions. Additionally, proofreading issues and unclear references (e.g., "stochastic example") detract from the paper's clarity.
Questions to Authors
1. Can you confirm whether there is any overlap between the Gigaword corpus and the AMR test set? If so, how was this handled to avoid contamination?
2. What is the rationale behind the encoder modifications (concatenating hidden states and introducing dropout)? Have these changes been empirically validated?
3. Why was a concluding section omitted? A discussion on the broader implications of using AMR versus other semantic formalisms would strengthen the paper.
Recommendation
While the paper presents significant contributions, particularly in AMR realization, the weaknesses in empirical comparisons, potential test set contamination, and missing implementation details limit its overall impact. Addressing these issues during the author response period could improve the paper's standing.