Review of the Submission
Summary and Contributions:  
This paper addresses a critical limitation in the training of recurrent neural networks (RNNs) by proposing a scalable Bayesian learning framework based on Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC). The key contribution is the integration of SG-MCMC with dropout to model weight uncertainty in RNNs, which is shown to improve generalization and alleviate overfitting. The proposed method is evaluated across several tasks, including language modeling, image captioning, and sentence classification, demonstrating consistent improvements over traditional stochastic optimization methods like RMSProp. The authors provide detailed experimental setups, ensuring reproducibility, and highlight the benefits of model averaging during testing.
Strengths:  
1. Novel Contribution to Bayesian Learning in RNNs: The paper effectively bridges the gap between stochastic gradient MCMC methods and stochastic optimization, offering a systematic approach to Bayesian learning in deep learning. This is a significant contribution, as Bayesian methods are underexplored in the context of RNNs.  
2. Empirical Superiority: The combination of SG-MCMC and dropout outperforms RMSProp with dropout in language modeling tasks, demonstrating the value of uncertainty modeling. The results are robust across different datasets and tasks, including character/word-level language modeling, image captioning, and sentence classification.  
3. Reproducibility: The authors provide detailed descriptions of the model architectures, hyperparameter settings, and experimental setups, ensuring that the results can be replicated by other researchers.  
4. Practical Insights: The paper highlights the benefits of model averaging and explores different strategies for sample collection, providing practical insights for practitioners.  
Weaknesses:  
1. Lack of Theoretical Analysis: While the empirical results are compelling, the paper lacks theoretical proofs or convergence analysis for the proposed algorithm. This omission weakens the rigor of the contribution, particularly for a Bayesian framework.  
2. Limited Comparisons: The experimental comparisons focus primarily on SG-MCMC versus RMSProp, with limited exploration of preconditioned SGLD (pSGLD) versus RMSProp. A deeper analysis of pSGLD's advantages over other SG-MCMC variants would strengthen the claims.  
3. Training Speed Analysis: The paper does not discuss the computational overhead of the proposed method in detail, particularly the impact of SG-MCMC on training speed. While the authors claim that it is computationally efficient, this needs to be substantiated with quantitative evidence.  
Questions to Authors:  
1. Can you provide theoretical insights or convergence guarantees for the proposed SG-MCMC framework?  
2. How does the training speed of SG-MCMC compare to RMSProp in practice, especially for large-scale datasets?  
3. Have you explored the impact of different dropout rates or noise levels on the performance of SG-MCMC?  
Recommendation:  
The paper presents a novel and practical contribution to Bayesian learning in RNNs, supported by strong empirical results. However, the lack of theoretical analysis and limited comparisons slightly detract from its overall impact. I recommend acceptance, provided the authors address the theoretical and computational concerns during the author response period.