Review of the Submission
Summary and Contributions
This paper introduces a general framework for Cross-Context Lexical Analysis (CCLA) to analyze variations in term meaning across different contexts. The authors propose a flexible methodology that accommodates diverse contexts, similarity functions, and word annotations, enabling applications such as semantic change detection, comparative lexical analysis, and word embedding stability evaluation. The key contributions of this work are:
1. Generalizable Framework: The proposed CCLA framework is highly adaptable, allowing for the analysis of lexical variations across various contexts and tasks without relying on parametric assumptions.
2. Novel Applications: The paper demonstrates the utility of CCLA in three distinct areas: semantic change detection, comparative lexical analysis (e.g., identifying context-sensitive terms), and evaluating word embedding stability.
3. Flexibility and Reusability: The framework supports a wide range of similarity functions and word annotations, making it a potentially valuable toolkit for multiple NLP tasks.
Strengths
1. Generality and Adaptability: The framework is well-designed to handle diverse contexts (e.g., temporal, sentiment, or domain-based) and can be applied to a variety of tasks, showcasing its versatility.
2. Clarity of Presentation: The paper is well-written and easy to follow, with clear explanations of the methodology and its applications.
3. Public Availability: The authors provide source code and datasets, which enhances reproducibility and encourages further exploration by the community.
Weaknesses
1. Reliance on Qualitative Metrics: The evaluation primarily relies on qualitative assessments (e.g., inspecting word lists and examples), which limits the ability to rigorously validate the effectiveness of the proposed framework.
2. Lack of Quantitative Rigor: While the experiments demonstrate the framework's potential, they lack robust quantitative metrics to compare CCLA against baseline methods. For example, downstream task performance or statistical significance testing could strengthen the claims.
3. Broader Applicability: The experiments focus on specific datasets (e.g., IMDB, Yelp, COHA) and tasks, raising concerns about the generalizability of the results to other domains or downstream applications.
Questions to Authors
1. Could you provide quantitative metrics (e.g., precision, recall, or F1-score) to evaluate the semantic change detection task and compare it with existing methods?
2. How does the framework perform in downstream tasks, such as sentiment analysis or domain adaptation, when using the scored terms as features?
3. Can the framework handle multilingual corpora or cross-lingual contexts? If so, what adaptations are required?
Recommendation
While the paper presents a promising and flexible framework, the lack of quantitative evaluation and rigorous experimental validation weakens its impact. Addressing these concerns in the author response or a revised version would significantly enhance the submission's contribution to the field.