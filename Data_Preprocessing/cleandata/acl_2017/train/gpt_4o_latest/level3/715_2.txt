Review of the Paper
Summary and Contributions
This paper addresses the challenging task of open-domain question answering (QA) using Wikipedia as the sole knowledge source. The proposed system, DrWiki, consists of two key components: a Document Retriever for identifying relevant articles and a Document Reader for extracting precise answer spans. The authors employ distant supervision and multitask learning to enhance the system's performance across multiple QA datasets. The paper's primary contributions are:  
1. A novel pipeline integrating document retrieval and machine comprehension tailored for open-domain QA using Wikipedia.  
2. Demonstration of competitive performance of the Document Retriever compared to the Wikipedia Search API, with improvements from bigram hashing.  
3. Application of multitask learning and distant supervision to improve the generalization of the Document Reader across diverse QA datasets.  
Strengths
1. Challenging Problem Addressed: The paper tackles an ambitious task of answering open-domain questions using Wikipedia, which combines the complexities of large-scale document retrieval and machine comprehension. This focus on a single knowledge source (Wikipedia) is a notable contribution, as it eliminates reliance on redundant external resources.  
2. Strong Document Reader Performance: The Document Reader achieves state-of-the-art results on the SQuAD benchmark, surpassing several recent QA models. The feature ablation analysis further highlights the importance of specific features, such as aligned question embeddings and exact match, in improving performance.  
3. Effective Use of Distant Supervision: The authors demonstrate the utility of distant supervision for generating training data in resource-scarce settings, particularly for datasets like WebQuestions and CuratedTREC. This approach is a practical contribution for scaling QA systems.  
4. Multitask Learning: The multitask learning framework improves the system's ability to generalize across datasets, achieving reasonable performance on multiple benchmarks, including SQuAD, CuratedTREC, and WikiMovies.  
Weaknesses
1. Inferior Final Results: While the Document Reader performs well in isolation, the full DrWiki system underperforms compared to other QA models like YodaQA, particularly on WebQuestions. This raises concerns about the sufficiency of Wikipedia as the sole knowledge source for open-domain QA.  
2. Lack of Error Analysis: The absence of error analysis limits the understanding of system weaknesses, such as the types of questions or retrieval scenarios where the system fails. This omission hinders actionable insights for future improvements.  
3. Retrieval Bottleneck: The Document Retriever, while competitive with the Wikipedia Search API, remains a bottleneck in the pipeline. The authors do not compare it with standard information retrieval (IR) baselines, leaving its relative effectiveness unclear.  
4. Unclear Distant Supervision Quality: The paper does not provide statistics or analysis on the quality of the training data generated via distant supervision, making it difficult to assess its reliability and impact on performance.  
5. Discrepancies in Results: The discrepancy in F1 values between Tables 4 and 5, as well as the unexplained absence of "No f_emb" in Table 5, introduces ambiguity in the evaluation.  
Questions to Authors
1. Can you provide an error analysis to identify specific question types or scenarios where DrWiki underperforms?  
2. How does the Document Retriever compare to standard IR baselines, such as BM25 or dense retrieval methods?  
3. What are the statistics on the quality of the training data generated via distant supervision? How does noise in this data affect the Document Reader's performance?  
4. Can you clarify the discrepancy in F1 values between Tables 4 and 5? Why is "No f_emb" missing in Table 5?  
5. Have you considered methods to identify questions that are more suitable for Wikipedia-based answers?  
Recommendation
While the paper addresses an important and challenging problem, the limitations in retrieval performance, lack of error analysis, and inferior results compared to other models weaken its overall impact. The work is a valuable contribution to open-domain QA research, but significant improvements are needed to make the system competitive. I recommend acceptance with major revisions, focusing on retrieval improvements, error analysis, and clarifications in the evaluation.