Review of the Paper
Summary and Contributions
This paper presents a novel end-to-end neural model for semantic graph parsing, specifically targeting linguistically deep semantic representations such as Minimal Recursion Semantics (MRS). The authors propose a transition-based parser that incrementally predicts semantic graphs using a well-designed arc-eager transition system. The model is evaluated on multiple benchmarks, including Dependency MRS (DMRS), Elementary Dependency Structures (EDS), and Abstract Meaning Representation (AMR), demonstrating competitive performance and significant speed improvements over grammar-based parsers like ACE. The key contributions of this work, as I see them, are:
1. Novel Transition System for Semantic Graph Parsing: The paper introduces a robust arc-eager transition system tailored for semantic graphs, which supports non-planar dependencies and enables efficient GPU batch processing. This is a significant improvement over existing neural baselines and grammar-based parsers.
2. Resolution of Scope Underspecification: The model effectively handles scope underspecification in MRS, a challenging aspect of computational semantics, while maintaining low computational cost.
3. Generalized Framework for Multiple Semantic Representations: By applying the same model architecture to both MRS-based graphs (DMRS, EDS) and AMR, the authors demonstrate the flexibility and generalizability of their approach.
Strengths
1. Innovative Design of the Transition System: The use of an arc-eager transition system, combined with stack-based features and hard attention, is well-motivated and leads to significant performance gains. The stack-based model achieves state-of-the-art results for DMRS parsing and outperforms existing neural AMR parsers.
2. Efficiency and Scalability: The parser's ability to process graphs incrementally and leverage GPU batch processing makes it an order of magnitude faster than grammar-based parsers like ACE, while maintaining competitive accuracy.
3. Empirical Rigor: The paper provides extensive experimental results across multiple datasets and metrics, including EDM and Smatch, and compares different model architectures (e.g., top-down vs. arc-eager, soft vs. hard attention). This thorough evaluation strengthens the validity of the proposed approach.
4. Potential for Broader Impact: By demonstrating the advantages of MRS over AMR (e.g., higher Smatch scores and better alignment with syntactic structures), the paper could encourage further adoption of MRS in semantic parsing research.
Weaknesses
1. Insufficient Explanation of Key Concepts: The paper assumes familiarity with DMRS, EPs, and the arc-eager transition system, which may hinder accessibility for readers without prior background in computational semantics. A more detailed introduction to these concepts would improve readability.
2. Limited Discussion of Error Analysis: While the paper reports performance metrics, it lacks a detailed error analysis to identify specific challenges (e.g., concept prediction in AMR parsing) and areas for improvement.
3. Lack of Open-Source Resources: The paper does not mention whether the proposed model or datasets will be made publicly available. Open-sourcing the code and resources would enhance reproducibility and encourage further research.
Questions to Authors
1. Could you provide more details or examples to clarify the role of Elementary Predications (EPs) in the transition system and how they are predicted?
2. How does the model handle cases where the input sentence contains ambiguous or incomplete alignments, especially for AMR parsing?
3. Are there plans to release the code and pre-trained models to facilitate reproducibility and adoption by the research community?
Recommendation
This paper makes significant contributions to semantic graph parsing and demonstrates strong empirical results. However, the lack of detailed explanations for key concepts and the absence of open-source resources slightly detract from its impact. I recommend acceptance, provided the authors address these issues during the revision phase.