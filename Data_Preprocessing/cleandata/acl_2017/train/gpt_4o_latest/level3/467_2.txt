Review of the Submission
Summary and Contributions
This paper proposes a self-learning bootstrapping framework for bilingual word embeddings, which iteratively refines a mapping between monolingual embedding spaces using minimal bilingual supervision. The method achieves competitive results with as little as a 25-word seed dictionary or even a list of numerals, significantly reducing the reliance on rich bilingual resources. The authors claim that their approach exploits the structural similarity of embedding spaces and implicitly optimizes a global objective function that does not depend on bilingual data. The paper also highlights the potential for future extensions to operate without any bilingual evidence.
The primary contributions of this work, as I see them, are:
1. Practical Reduction in Bilingual Resource Requirements: The method demonstrates strong performance with minimal bilingual supervision, which is particularly valuable for low-resource language pairs.
2. Empirical Validation of Structural Similarity: The experiments provide evidence that structural similarity between embedding spaces can be effectively leveraged for bilingual mapping, even with sparse bilingual evidence.
3. Analysis of Convergence and Optimization: The authors present a theoretical and empirical analysis of their self-learning framework, shedding light on its convergence behavior and implicit optimization objective.
Strengths
1. Practical Applicability: The proposed method is simple, efficient, and adaptable to various embedding mapping techniques, making it a practical solution for low-resource scenarios. Its ability to work with trivial bilingual evidence (e.g., numerals) is a notable strength.
2. Empirical Robustness: The method performs consistently across different language pairs, including typologically distant ones like English-Finnish, demonstrating its robustness.
3. Insightful Analysis: The paper provides a clear analysis of the optimization objective and convergence behavior, which enhances the interpretability of the method and its results.
4. Reproducibility: The authors provide sufficient experimental details and promise to release code, which supports reproducibility and further research.
Weaknesses
1. Limited Novelty: The bootstrapping approach is not entirely new and closely resembles prior work, particularly that of Artetxe et al. (2016). The paper lacks a clear articulation of its unique contributions beyond slight reparametrization and empirical validation.
2. Insufficient Comparison with Related Work: The paper omits discussion and experimental comparison with several relevant works, such as Duong et al. (2016), Vulic and Korhonen (2016), and Smith et al. (2017). This weakens its positioning within the broader literature.
3. Performance Plateau: While the method performs well with minimal bilingual evidence, it does not surpass state-of-the-art methods using richer resources. This suggests a performance ceiling inherent to linear mapping approaches, which the authors acknowledge but do not address.
4. Unclear Convergence Criterion: The convergence criterion is vaguely defined, particularly for language pairs without cross-lingual word similarity datasets, raising concerns about the method's generalizability.
Questions to Authors
1. How does the proposed method fundamentally differ from Artetxe et al. (2016) beyond reparametrization? Could you clarify its unique contributions?
2. Why were works such as Duong et al. (2016) and Vulic and Korhonen (2016) excluded from the discussion and experiments? How does your method compare to theirs?
3. Could you elaborate on the convergence criterion for language pairs without cross-lingual similarity datasets? How would the method behave in such cases?
Recommendation
While the paper offers a practical and well-executed method for bilingual word embeddings with minimal supervision, its limited novelty and insufficient engagement with related work are significant drawbacks. I recommend acceptance only if the authors address the novelty concerns and provide a more comprehensive comparison with prior work in the final version.