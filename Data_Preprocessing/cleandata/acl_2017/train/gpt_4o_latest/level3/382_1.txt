Review of the Submission
Summary and Contributions
This paper proposes a novel framework for creating linguistically challenging data-to-text corpora from existing Knowledge Bases (KBs), with a focus on supporting the training of wide-coverage microplanners for Natural Language Generation (NLG). The authors apply their framework to DBpedia data, generating a dataset (DBPNLG) that they compare against the RNNLG dataset (Wen et al., 2016). The primary contributions of the paper are:
1. Dataset Creation Framework: A semi-automatic method for generating datasets from KBs, which incorporates a sophisticated content selection module and a crowdsourcing process for text generation.
2. Dataset Analysis: A thorough comparison of the DBPNLG dataset with the RNNLG dataset, highlighting its greater semantic and linguistic diversity, despite being smaller in size.
3. Empirical Evaluation: A proof-of-concept study demonstrating that DBPNLG is more challenging for neural sequence-to-sequence models, thus motivating further research in this area.
Strengths
1. Timely Contribution: The paper addresses an important gap in the field of data-to-text NLG by focusing on creating datasets that support wide-coverage microplanning, a critical yet underexplored area.
2. Dataset Diversity: The DBPNLG dataset is shown to be more diverse in terms of attributes, input patterns, and input shapes compared to RNNLG. This diversity is essential for training robust NLG models capable of generating linguistically complex texts.
3. Rigorous Analysis: The paper provides a detailed comparison of datasets using various metrics (e.g., input patterns, lexical diversity, syntactic complexity) and demonstrates the limitations of existing neural models on the proposed dataset.
4. Crowdsourcing Quality Control: The paper employs a selective crowdsourcing process with validation steps, ensuring high-quality text-data pairs. The rejection of 8.7% of crowd-sourced texts during verification underscores the authors' commitment to data quality.
Weaknesses
1. Unclear Novelty Over Prior Work: The novelty of the proposed framework compared to Perez-Beltrachini et al. (2016) is not well-articulated. While the authors claim advancements, the lack of a direct comparison and in-depth analysis of differences weakens the contribution.
2. Baseline Comparison: The comparison to Wen et al. (2016)'s dataset is useful but limited. The much higher BLEU scores reported in Wen et al.'s work raise concerns about the informativeness of the baseline used for evaluation.
3. Corpus Complexity Claims: The claims regarding the complexity of the DBPNLG corpus would benefit from additional syntactic construction analysis and clearer details on how path shapes influence linguistic diversity.
4. Discourse Relations: The framework does not incorporate richer discourse relations (e.g., Contrast, Consequence), which are central to NLG. Comparisons to prior work on discourse-level generation (e.g., Walker et al., 2007; Isard, 2016) are missing.
5. Domain Dependence: The paper lacks clarity on the domain dependence of the proposed dataset and its "wide coverage" claims. This is critical for understanding the generalizability of the framework.
Questions to Authors
1. How does your framework improve upon Perez-Beltrachini et al. (2016) beyond the inclusion of crowdsourcing? Could you provide a direct comparison or highlight specific advancements?
2. The BLEU scores for the neural baseline are significantly lower on DBPNLG compared to RNNLG. How do you interpret this result in light of the dataset's complexity? Could you explore alternative evaluation metrics?
3. Could you elaborate on the role of path shapes in determining syntactic diversity? Are there specific linguistic constructs that are better represented in DBPNLG due to this feature?
Recommendation
While the paper has notable strengths, particularly in its focus on dataset diversity and its rigorous analysis, the unclear novelty over prior work and the limited baseline comparison are significant weaknesses. The paper is a valuable contribution to the field, but addressing these issues in the final version would strengthen its impact.