Review of the Submission
Summary and Contributions
This paper introduces a Sememe-Encoded Word Representation Learning (SE-WRL) model that incorporates sememe information from HowNet to improve word representation learning (WRL). The authors propose three sememe-encoded strategies—Simple Sememe Aggregation (SSA), Sememe Attention over Context (SAC), and Sememe Attention over Target (SAT)—to model sememes, senses, and words simultaneously. The SAT model, in particular, applies attention mechanisms to select word senses based on context, aiming to address polysemy in WRL. The paper evaluates these models on word similarity and word analogy tasks, claiming significant performance improvements over baselines. The authors also provide case studies to demonstrate the effectiveness of sememe-based attention in word sense disambiguation (WSD).
The primary contributions of this work, as I see them, are:
1. Integration of Sememes for WRL: The use of sememe annotations from HowNet to improve WRL is novel and demonstrates potential for better capturing word semantics, especially for polysemous words.
2. Attention-Based Sense Selection: The SAT model introduces a soft attention mechanism for word sense disambiguation, which is more flexible and robust compared to hard sense selection approaches like MST.
3. Empirical Results: The experiments show that sememe-based models, particularly SAT, outperform baselines on word similarity and analogy tasks, with notable improvements for rare and low-frequency words.
Strengths
1. Novel Use of Sememes: This is the first work to explicitly incorporate sememe annotations into WRL, leveraging their compositional nature to address polysemy. This contribution is both innovative and impactful, as sememes provide a fine-grained semantic structure.
2. Attention Mechanism for WSD: The SAT model's use of context-based attention for sense selection is a strength, as it allows for soft disambiguation and avoids errors associated with hard sense selection methods like MST.
3. Performance on Rare Words: The paper highlights the advantages of sememe-based models for rare words, which is a significant challenge in WRL. This is supported by experimental results and qualitative case studies.
4. Well-Written and Structured: The paper is clear, well-organized, and provides sufficient theoretical and empirical grounding for its claims.
Weaknesses
1. Unclear Gains for WSI: While the paper claims improvements in WRL, it does not provide a dedicated evaluation for word sense induction (WSI). The effectiveness of sememe-based models for WSI remains unclear.
2. Ambiguity in Dataset Selection: The choice of datasets, particularly the use of the Sogou-T corpus and word similarity datasets, is poorly justified. The paper does not explain why these datasets are appropriate or how they generalize to other languages or domains.
3. State-of-the-Art Claims: The claim that the SAT model achieves state-of-the-art results for Chinese word similarity tasks is not substantiated with sufficient comparisons to recent methods. The baselines used (e.g., Skip-gram, CBOW) are relatively dated.
4. Lack of Implementation Details: The implementation of the MST baseline is not clearly described, making it difficult to assess its fairness as a comparison. Additionally, details about model parameters, such as vocabulary size and parameter sharing between senses and sememes, are missing.
5. Limited Experimental Insights: While the SAT model performs best, the paper does not provide detailed insights into why it outperforms SAC and SSA. Furthermore, claims about rare word performance are not evaluated on a dedicated rare word dataset.
Questions to Authors
1. Can you provide more details about the implementation of the MST baseline and how it differs from SAT? 
2. Why were the Sogou-T corpus and the specific word similarity datasets chosen for evaluation? How do you ensure the generalizability of your results?
3. Have you considered evaluating your models on a dedicated word sense induction (WSI) task to better demonstrate the benefits of sememe-based modeling?
4. Can you clarify the parameter settings, such as vocabulary size, embedding dimensions, and whether parameters are shared between senses and sememes?
Recommendation
While the paper presents a novel and promising approach to WRL using sememe information, the unclear gains for WSI, insufficient justification of dataset choices, and lack of detailed experimental insights weaken its overall impact. I recommend acceptance with minor revisions, provided the authors address the concerns about dataset selection, implementation details, and state-of-the-art claims.