Review
This paper presents a novel approach to zero pronoun (ZP) resolution, addressing the challenge of limited annotated data by proposing a method to automatically generate large-scale pseudo training data. The authors adapt a cloze-style reading comprehension neural network to the ZP resolution task and introduce a two-step training mechanism for domain adaptation. The paper is well-written, with clear experiments and significant improvements over state-of-the-art systems.
Summary and Contributions
The primary contribution of this paper is the introduction of a simple yet effective method to generate pseudo training data for ZP resolution. By treating ZP resolution as a reading comprehension problem, the authors leverage a cloze-style data generation approach to create large-scale training samples. This is a significant advancement, as it reduces reliance on expensive annotated datasets.
The second major contribution is the two-step training mechanism, which bridges the gap between pseudo training data and task-specific data. This method effectively combines the benefits of large-scale pre-training with domain-specific fine-tuning, leading to robust performance improvements.
Finally, the paper introduces an attention-based neural network model tailored for ZP resolution. The model incorporates a novel unknown word processing technique, which assigns distinct tags to unknown words, improving the model's ability to handle out-of-vocabulary terms during inference.
Strengths
1. Novel Data Generation: The proposed method for generating pseudo training data is innovative and scalable, addressing a critical bottleneck in ZP resolution research.
2. Effective Domain Adaptation: The two-step training process demonstrates clear benefits, with empirical results showing significant performance gains across multiple domains.
3. Strong Experimental Results: The proposed approach achieves a 3.1% improvement in F-score over the state-of-the-art, with consistent gains across most domains.
4. Unknown Word Handling: The distinct tagging of unknown words is a simple yet effective solution that improves the model's interpretability and performance.
5. Clarity and Rigor: The paper is well-structured, with detailed explanations of the methodology and thorough experimental analysis.
Weaknesses
1. Antecedent Identification: The process for identifying antecedents when the predicted word is a pronoun or when the head word is not a pronoun is unclear. This ambiguity could hinder reproducibility and practical application.
2. Handling of Novel Nouns: The paper does not adequately address how the model handles predictions that are nouns not found in the preceding context, which could limit its robustness.
3. Evaluation Design: While the overall performance is strong, the authors do not evaluate the system in two distinct steps (recovering ZPs and finding antecedents), which could provide deeper insights into the model's strengths and weaknesses.
4. Choice of Neural Architecture: The rationale for using an attention-based neural network is not well-justified. Alternative architectures could have been considered or compared.
5. Minor Error in Figure 2: The notation "d1, d2…" should be corrected to "s1, s2…" for consistency with the text.
Questions to Authors
1. Could you clarify how antecedents are identified when the predicted word is a pronoun or when the head word is not a pronoun?
2. How does the model handle cases where the predicted antecedent is a noun that does not appear in the preceding context?
3. Why was an attention-based neural network chosen over other architectures, such as transformers or graph-based models?
Conclusion
This paper makes significant contributions to the field of ZP resolution, particularly in addressing the data scarcity issue through pseudo training data generation and domain adaptation. While there are some ambiguities in antecedent identification and architectural choices, the overall methodology and results are compelling. I recommend acceptance, with minor revisions to address the noted weaknesses.