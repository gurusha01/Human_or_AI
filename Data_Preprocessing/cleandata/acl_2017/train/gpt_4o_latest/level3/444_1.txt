Review of the Research Paper
Summary and Contributions  
This paper introduces a novel evaluation methodology for the task of rap lyrics generation, focusing on four key aspects: originality, stylistic similarity, fluency, and coherence. The authors propose both manual and automated metrics, including a unique "Similarity" metric, and provide a dataset of annotated rap lyrics for 13 artists. The primary contributions of the paper are:  
1. A comprehensive manual evaluation framework that assesses fluency, coherence, and stylistic similarity, which could be extended to other creative text generation tasks.  
2. An automated evaluation methodology that combines rhyme density and uniqueness, with improvements to handle repetitive text, enabling large-scale analysis.  
3. A publicly available dataset of rap lyrics annotated for stylistic similarity, which can serve as a benchmark for future research.  
Strengths  
1. Clear Motivation and Comprehensive Methodology: The paper is well-written, with a strong motivation for the need for evaluation metrics in creative text generation tasks. The manual and automated evaluation methods are complementary and address multiple facets of the task.  
2. Public Dataset and Annotation Framework: The creation of a dataset annotated for stylistic similarity is a valuable contribution to the field, providing a foundation for future work. The annotation framework is well-documented and could be applied to other domains.  
3. Insightful Analysis: The evaluation results highlight the limitations of current generative models, such as the LSTM's difficulty in incorporating large vocabularies, and provide actionable insights for future research directions.  
4. Automation of Evaluation: The fully automated methodology for rhyme density and uniqueness is a significant improvement over the semi-automatic approach in prior work, making the evaluation process scalable.  
Weaknesses  
1. Lack of Validation for the "Similarity" Metric: While the "Similarity" metric is a novel contribution, its low correlation with manual annotations raises concerns about its effectiveness. The authors claim it provides meaningful insights into system performance, but no concrete validation or evidence is provided.  
2. Baseline Comparison: The poor performance of the baseline system does not necessarily validate the quality of the proposed metrics. A more robust baseline or additional comparison with state-of-the-art models would strengthen the claims.  
3. Missing References: The paper overlooks relevant prior work on automating coherence (e.g., Li et al., 2015) and style matching (e.g., Dethlefs et al., 2014; Pennebaker). Incorporating these references would situate the work more firmly within the existing literature.  
4. Limited Generalization: While the methodology is tailored to rap lyrics, the paper could benefit from a discussion on its applicability to other creative text generation tasks, such as poetry or prose.  
Questions to Authors  
1. Can you provide additional evidence or experiments to validate the effectiveness of the "Similarity" metric? For example, how does it perform in distinguishing between stylistically similar and dissimilar artists?  
2. How does the proposed evaluation framework compare to existing metrics for creative text generation tasks, such as BLEU or ROUGE, in terms of capturing creativity and style?  
3. Could the dataset and evaluation methodology be extended to other genres or creative domains? If so, what adaptations would be necessary?  
Conclusion  
Overall, this paper makes a meaningful contribution to the evaluation of creative text generation tasks, particularly in the domain of rap lyrics. The proposed methodologies are well-motivated and provide a foundation for future research. However, the lack of validation for the "Similarity" metric and missing references to related work are notable weaknesses. Addressing these issues would significantly strengthen the paper. I recommend acceptance with minor revisions.