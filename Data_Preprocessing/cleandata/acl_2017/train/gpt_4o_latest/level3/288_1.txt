Review of the Paper
Summary and Contributions
This paper investigates the impact of writing tasks on writing style, using the story cloze task corpus as a case study. It proposes a simple linear classifier based on stylistic features such as word and character n-grams, achieving state-of-the-art results in distinguishing between "right" and "wrong" story endings. The study highlights that different writing constraints lead to measurable stylistic differences, even without considering the story context. The authors also demonstrate that their style-based classifier significantly improves performance on the story cloze challenge, outperforming prior deep learning models. The paper makes three primary contributions: (1) it sheds light on how cognitive load and task framing influence writing style, (2) it emphasizes the importance of task design in NLP dataset creation, and (3) it establishes a new benchmark for the story cloze task.
Strengths
1. State-of-the-Art Performance: The proposed model achieves a 15.3% improvement over the previous best result on the story cloze task, demonstrating the effectiveness of stylistic features in this context. This is a significant contribution to the field.
2. Novel Insights into Writing Style: The paper provides compelling evidence that writing tasks influence style, offering a new perspective on how cognitive processes manifest in written text. This insight has implications for both cognitive science and NLP.
3. Simplicity and Interpretability: The use of a linear classifier with interpretable features (e.g., word and character n-grams) makes the approach accessible and easy to understand, contrasting with the black-box nature of many deep learning models.
4. Combination with Neural Models: The integration of stylistic features with a neural language model further improves performance, showcasing the complementary strengths of traditional and modern approaches.
Weaknesses
1. Unclear Definition of "Style": The paper does not provide a clear or consistent definition of "style," which undermines the interpretability of its claims. The connection between stylistic features (e.g., POS tags, n-grams) and the broader concept of "style" needs to be better articulated.
2. Insufficient Dataset Details: The paper lacks critical details about the dataset, such as the number of stories, author demographics, and coherence judgment criteria. This omission makes it difficult to assess the generalizability of the findings.
3. Weak Justification for Feature Selection: The rationale for using specific features, such as frequent POS tags and character 4-grams, is not adequately explained or linked to the concept of "style." This weakens the theoretical grounding of the model.
4. Ambiguity in Terminology: Terms like "right" and "wrong" endings are used inconsistently, and their definitions are not clarified. This creates confusion, particularly when discussing "original" vs. "new" pairs.
5. Unclear Connection Between Sections: The "Design of NLP Tasks" section feels disconnected from the rest of the paper. The authors need to better integrate this discussion with their findings and contributions.
Questions to Authors
1. Could you provide a more precise definition of "style" and explain how the selected features (e.g., n-grams, POS tags) capture it?
2. How many stories and authors were included in the dataset, and what were the criteria for coherence judgments? Were there significant variations in judgments across annotators?
3. Can you clarify the sampling process for "original" and "new" pairs? How was variability in coherence judgments addressed?
4. What is the publication source for lines 873â€“875, and why is it missing from the references?
Additional Comments
The paper presents an interesting and impactful study, but it requires significant restructuring and clarification to improve readability and coherence. Addressing the ambiguities in terminology, providing more detailed justifications for feature selection, and offering additional dataset details would strengthen the paper's contributions.