Review of the Submission
Summary and Contributions:  
This paper addresses a critical and long-standing challenge in the automatic evaluation of Natural Language Generation (NLG) and translation systems. The authors conduct a comprehensive analysis of 21 existing metrics, including word-based and grammar-based metrics, and highlight their limitations in correlating with human judgments. A novel metric, RAINBOW, is introduced, which combines the strengths of these metrics and achieves significantly higher sentence-level correlation with human ratings (up to œÅ = 0.81). The paper also provides an extensive error analysis, demonstrating the variability of metric performance across datasets and systems. The authors make their code and data publicly available, offering a valuable resource for the community. The bibliography is particularly useful for newcomers seeking foundational and state-of-the-art references in NLG evaluation.
Strengths:  
1. Relevance and Impact: The paper tackles a well-recognized problem in NLG evaluation, emphasizing the inadequacy of current metrics in reflecting human judgments. This is a significant contribution to the field.  
2. Novel Metric: The introduction of RAINBOW, which combines word-based and grammar-based metrics, demonstrates a clear improvement in correlation with human ratings. The use of ensemble learning to integrate diverse metrics is innovative and effective.  
3. Comprehensive Analysis: The paper provides a thorough evaluation of existing metrics, including detailed error analysis. The insights into why current metrics fail, particularly for outputs of medium quality, are valuable for future research.  
4. Resource Contribution: The availability of code, data, and detailed results enhances reproducibility and provides a foundation for further exploration in this area.  
5. Clarity and Accessibility: The bibliography and background sections are well-structured, making the paper accessible to both experts and newcomers.
Weaknesses:  
1. Lack of Linguistic Insights: While the paper excels in numerical analysis, it does not delve deeply into linguistic aspects, such as the specific challenges posed by ungrammatical outputs or semantic nuances in human-system differences. Including linguistic examples would improve reader intuition.  
2. Crowdsourced References: The reliance on crowdsourced references raises questions about the quality and reliability of the training and evaluation data. The paper does not adequately address how these limitations might affect the proposed metric's generalizability.  
3. Practical Applicability: The computational cost of RAINBOW, particularly when using all 21 metrics, may limit its practical use in real-time applications. While the authors propose a reduced version (RAINBOW Top5), its performance is notably weaker.  
4. Limited Contextual Evaluation: The paper focuses on sentence-level evaluation but does not explore how RAINBOW performs in a broader dialogue or contextual setting, which is increasingly relevant for NLG systems.
Questions to Authors:  
1. Could you provide concrete linguistic examples where RAINBOW succeeds or fails compared to existing metrics?  
2. How does RAINBOW perform when applied to other domains (e.g., dialogue systems or image captioning) beyond the datasets used in this study?  
3. Have you considered incorporating extrinsic evaluation metrics (e.g., task success) into RAINBOW to enhance its practical applicability?  
4. How robust is RAINBOW to noisy or low-quality human references, given the issues identified in the crowdsourced datasets?
Recommendation:  
This paper makes a strong contribution to the field of NLG evaluation by addressing a critical gap and introducing a novel metric that significantly improves correlation with human judgments. While there are some limitations, particularly regarding linguistic insights and practical applicability, the strengths outweigh the weaknesses. I recommend acceptance, with the suggestion to address the identified weaknesses in a future revision.