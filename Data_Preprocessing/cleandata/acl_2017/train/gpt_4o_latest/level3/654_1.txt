Review of the Paper
Summary and Contributions:  
This paper presents a novel deep learning model for semantic role labeling (SRL) that builds upon Zhou and Xu's ACL 2015 approach by employing an 8-layer BiLSTM architecture with highway connections, constrained decoding, and ensembling. The authors incorporate recent best practices such as orthonormal initialization and recurrent dropout, achieving state-of-the-art results on the CoNLL 2005 and 2012 datasets. The ensemble model achieves an F1 score of 83.2 on CoNLL 2005 and 83.4 on CoNLL 2012, representing a 10% relative error reduction over prior work. The paper also provides extensive error analysis, highlighting the model's strengths in capturing long-distance dependencies and its limitations in structural consistency and adjunct-argument distinctions. Additionally, the authors explore the role of syntax in SRL, showing that accurate syntactic input can further improve performance.
The primary contributions of the paper are:  
1. A state-of-the-art deep SRL model with publicly available code and models.  
2. Detailed error analysis that identifies key strengths (e.g., handling long-distance dependencies) and weaknesses (e.g., structural inconsistencies).  
3. Empirical evidence that high-quality syntactic input can still benefit neural SRL models, despite the trend toward syntax-free approaches.
Strengths:  
1. Strong Model Design and Performance: The proposed model achieves significant improvements over prior work, demonstrating the effectiveness of deep BiLSTMs with highway connections and constrained decoding. The incorporation of best practices (e.g., dropout, orthonormal initialization) is well-motivated and empirically validated.  
2. Insightful Error Analysis: The paper provides a thorough breakdown of error types, including label confusion, attachment mistakes, and structural inconsistencies. This analysis offers valuable insights into the model's behavior and areas for improvement.  
3. Exploration of Syntax's Role: The authors revisit the role of syntax in SRL, showing that gold-standard syntactic input can yield substantial gains. This nuanced discussion challenges the prevailing assumption that syntax is unnecessary for modern SRL systems.  
4. Reproducibility: The commitment to releasing code and models enhances the paper's impact and facilitates future research.  
Weaknesses:  
1. Limited Task-Specific Insights: While the model and analysis are strong, the paper provides relatively few insights into the semantic role labeling task itself. For example, the discussion of linguistic phenomena (e.g., adjunct-argument distinctions) could be more detailed.  
2. Reliance on Ensemble Models: The reported state-of-the-art results rely on an ensemble of models, which may limit the practical applicability of the approach in resource-constrained settings.  
3. Constrained Decoding Dependency: The model's reliance on constrained decoding to enforce structural consistency suggests that it does not fully learn these constraints during training. This could indicate a gap in the model's ability to generalize structural rules.  
Questions to Authors:  
1. Could you elaborate on how the model's performance varies across different genres in the CoNLL 2012 dataset? Are there specific genres where the model struggles more?  
2. Have you considered alternative approaches to enforce structural consistency during training, rather than relying on constrained decoding at inference time?  
3. Can you provide more examples of cases where syntax improves SRL performance, particularly in out-of-domain settings?  
Recommendation:  
This paper makes a strong contribution to the field of semantic role labeling by advancing the state of the art and providing valuable insights into the strengths and limitations of deep neural models. While the lack of task-specific insights is a minor limitation, the overall quality of the work, its reproducibility, and its thorough analysis make it a strong candidate for acceptance.