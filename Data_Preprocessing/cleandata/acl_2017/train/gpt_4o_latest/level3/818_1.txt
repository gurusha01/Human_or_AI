Review
Summary and Contributions
This paper addresses the challenging and creative task of extracting commonsense physical knowledge from unstructured text, focusing on overcoming reporting bias. The authors propose a joint inference model that simultaneously learns (1) relative physical knowledge about object pairs and (2) physical implications of actions applied to those objects. The work introduces a novel dataset, VERBPHYSICS, which compiles crowd-sourced knowledge about actions and objects along five physical dimensions: size, weight, strength, rigidness, and speed. The paper claims that joint inference improves performance over baselines, and the proposed approach has potential applications in natural language understanding, computer vision, and robotics.
The primary contributions of the paper are:
1. Introducing a new task of extracting physical commonsense knowledge from text, focusing on both object-object relations and verb-object implications.
2. Proposing a joint inference model using factor graphs to integrate object and action knowledge.
3. Developing and releasing the VERBPHYSICS dataset, which provides a valuable resource for future research in this domain.
Strengths
1. Novel Problem and Dataset: The paper tackles a worthwhile and underexplored problem of extracting physical commonsense knowledge from text. The introduction of the VERBPHYSICS dataset is a significant contribution to the field, as it provides a structured resource for studying physical implications of actions and object relations.
2. Joint Inference Approach: The use of factor graphs to model the interplay between object-object relations and verb-object implications is a creative and promising approach. The integration of multiple knowledge dimensions (e.g., size, weight) is particularly noteworthy.
3. Potential Impact: The work has broad applicability, with potential to benefit fields like natural language understanding, computer vision, and robotics, where physical reasoning is often required.
Weaknesses
1. Lack of Clarity in Methodology: The explanation of how knowledge about objects and verbs interact to overcome reporting bias is unclear. The key insight of joint inference is not sufficiently explained, and terms like "grounded," "pre-condition," and "post-condition" are used ambiguously.
2. Incomplete Experiments: The experimental section is incomplete. Results for the second task (object pair prediction) are missing for Model B, and an obvious experiment—comparing the joint inference model to a simpler non-joint baseline—is not conducted. This omission weakens the empirical validation of the proposed approach.
3. Visual and Presentation Issues: Figures (e.g., Figure 1 and Figure 3) are unintelligible and poorly formatted, making it difficult to follow the model's structure and contributions. Additionally, table references are inconsistent, and the formatting of references needs improvement.
4. Insufficient Baseline Justification: The choice of the EMB-MAXENT classifier as a baseline is not well justified. It is unclear why this baseline is appropriate or how it compares to other potential baselines.
Questions to Authors
1. Can you clarify how joint inference specifically helps overcome reporting bias? What is the key mechanism that links object-object and verb-object knowledge in this context?
2. Why were results for Model B omitted for the second task (object pair prediction)? Are there any plans to include these results in a future revision?
3. Could you provide more details on the EMB-MAXENT classifier and explain why it was chosen as a baseline? How does it compare to other potential baselines?
Recommendation
The paper addresses an important and creative problem and proposes a promising approach. However, the lack of clarity in the methodology, incomplete experiments, and poor presentation significantly detract from its quality. Substantial revisions are required to improve the clarity of the approach, complete the experimental evaluation, and address presentation issues. I recommend a major revision before the paper can be considered for publication.