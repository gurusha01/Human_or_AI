Review of the Submission
Summary and Contributions
This paper presents a novel approach to Abstract Meaning Representation (AMR) parsing and realization using sequence-to-sequence (seq2seq) models. The authors tackle the challenges of data sparsity and effective graph linearization by employing a self-training procedure that leverages a small annotated corpus and a large unlabeled dataset. Key contributions include:
1. First Successful Application of Seq2Seq Models to AMR Parsing and Realization: The paper demonstrates that seq2seq models, with careful preprocessing and self-training, can achieve competitive results in AMR parsing and state-of-the-art performance in AMR realization.
2. Delexicalization for Named Entities: The authors employ a meticulous delexicalization strategy to reduce data sparsity, which significantly contributes to the model's performance.
3. Paired Training Procedure: The proposed self-training algorithm effectively bootstraps the parser and realizer, enabling the use of millions of unlabeled sentences to improve performance.
Strengths
1. Well-Executed Methodology: The proposed approach is methodologically sound, with detailed preprocessing steps, a novel paired training procedure, and extensive experimentation. The use of self-training to address data sparsity is particularly innovative and effective.
2. Strong Empirical Results: The paper achieves competitive results in AMR parsing (61.9 SMATCH) and outperforms the state-of-the-art in AMR realization (32.3 BLEU), demonstrating the practical utility of the approach.
3. Reproducibility: The methodology is described in sufficient detail, including preprocessing steps, training procedures, and hyperparameter settings, making it reproducible.
4. Robustness to Linearization Orders: The experiments show that seq2seq models are largely agnostic to graph linearization orders, which is a significant insight for future work in AMR tasks.
5. Comprehensive Analysis: The paper includes thorough ablative and qualitative analyses, which provide valuable insights into the contributions of different components of the system.
Weaknesses
1. Statistical Significance: The paper does not include statistical significance tests for performance differences, which would strengthen the claims about the results.
2. Random Seed Bias: Experiments on linearization order should be repeated with different random seeds to rule out potential biases.
3. Paper Structure and Writing: The paper is dense and lacks a formal conclusion. Some sections are difficult to follow due to unclear explanations, and there are minor issues with punctuation, typos, and figure/table clarity.
4. Missing Experiment Details: Important details, such as vocabulary size and a more detailed comparison with Luong et al. (2015), are missing.
5. Handcrafted Components: While the approach is promising, some manual effort (e.g., named entity clustering) is still required, which limits scalability.
Questions to Authors
1. Have you conducted statistical significance tests to validate the performance improvements reported? If not, could you provide these results during the author response period?
2. Can you clarify the specific contributions of your approach over Luong et al. (2015) in more detail?
3. How sensitive is the performance of your model to the choice of hyperparameters, particularly in the self-training procedure?
Additional Comments
The authors have addressed concerns about data overlap and comparison validity during the response period, which resolves major issues. However, the paper would benefit from proofreading and a clearer structure. Overall, this submission presents a significant contribution to the field of AMR parsing and realization, and its strengths outweigh its weaknesses. With revisions, this work has the potential to make a lasting impact.