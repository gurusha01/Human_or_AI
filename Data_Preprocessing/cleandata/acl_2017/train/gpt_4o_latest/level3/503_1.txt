Review of the Paper
Summary and Contributions
This paper explores the concept of Regular Graph Languages (RGLs) as a potential formalism for representing probabilistic graph models in natural language processing (NLP). The authors focus on two key contributions: (1) proving that RGLs are closed under intersection, and (2) presenting a parsing algorithm for RGLs with linear runtime in the size of the input graph. These contributions are positioned as addressing the limitations of existing graph language families, such as Hyperedge Replacement Languages (HRLs) and Monadic Second Order Languages (MSOL), in terms of probabilistic modeling and closure properties. The paper also discusses the potential applications of RGLs in NLP tasks involving semantic graph representations, such as Abstract Meaning Representation (AMR).
Strengths
1. Technical Contributions: The formal proof of intersection closure for RGLs is a valuable theoretical result, as it bridges the gap between HRLs and MSOL in terms of desirable properties for graph languages. Similarly, the proposed parsing algorithm, with its linear runtime, is a significant step toward practical usability of RGLs in NLP applications.
2. Relevance to NLP: The paper addresses an important problem in NLP—modeling semantic graphs—and situates its contributions within the broader context of probabilistic graph languages. This focus aligns well with current trends in NLP research.
3. Exploration of Related Work: The discussion of related formalisms, such as Tree-like Grammars (TLG) and Restricted DAG Grammars (RDG), provides a broader perspective on the landscape of graph languages and their applicability to NLP.
Weaknesses
1. Lack of Cohesion: The paper suffers from poor organization and writing quality, which makes it difficult to follow the logical flow of arguments. For instance, the connection between the parsing algorithm and the broader goals of the paper is not clearly articulated.
2. Misleading Title: The title's reference to "probabilistic languages" is misleading, as the paper does not rigorously define or demonstrate probabilistic reasoning for RGLs. This undermines the paper's overarching claim of addressing probabilistic graph modeling.
3. Incomplete Soundness and Completeness: The soundness and completeness of the parsing algorithm are relegated to supplementary materials, which diminishes the impact of this contribution. These proofs should have been integrated into the main text for clarity and completeness.
4. Overstated Significance: The intersection-closure result, while technically correct, is overstated in its significance. It appears to be a straightforward application of existing results in MSOL and HRL theory, rather than a groundbreaking discovery.
5. Undefined Core Concept: The concept of a "probabilistic" formal language is not rigorously defined, leaving the reader unconvinced of the paper's central premise. This lack of clarity weakens the theoretical foundation of the work.
Questions to Authors
1. How do you define "probabilistic" in the context of RGLs, and how does this definition align with existing probabilistic formalisms in NLP?
2. Could you clarify the practical implications of the intersection-closure result for real-world NLP tasks? Are there specific use cases where this property is critical?
3. Why were the soundness and completeness proofs for the parsing algorithm not included in the main text? Could you summarize the key ideas of these proofs?
Recommendation
While the paper presents some valuable technical results, particularly the parsing algorithm and the intersection-closure proof, its lack of cohesion, misleading title, and unclear probabilistic framework significantly detract from its overall quality. I recommend rejecting this submission in its current form. However, with substantial revisions to improve organization, clarify the probabilistic framework, and integrate key proofs into the main text, the paper could become a strong contribution to the field.