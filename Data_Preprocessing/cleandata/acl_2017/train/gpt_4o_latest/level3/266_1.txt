Review of the Paper
Summary and Contributions
This paper investigates the impact of task-specific corpora on word embeddings for sentiment analysis, comparing their performance against embeddings trained on large, generic datasets. The authors propose methods to quantify the subjectivity of corpora and explore techniques to combine task-specific and generic embeddings. The paper also extends its experiments to under-resourced languages, specifically Catalan, demonstrating the potential of these methods in low-resource settings. The key contributions of this paper, as I see them, are:
1. Empirical Validation of Task-Specific Embeddings: The paper provides evidence that embeddings trained on smaller, task-specific corpora outperform those trained on larger, generic corpora for sentiment analysis tasks.
2. Combination of Embeddings: It demonstrates that combining embeddings from task-specific and generic corpora, particularly through vector concatenation, yields better performance than using either source alone.
3. Application to Under-Resourced Languages: The paper extends its findings to Catalan, showing that these techniques can benefit languages with limited resources.
Strengths
1. Relevance to Sentiment Analysis: The paper addresses an important problem in sentiment analysisâ€”how to effectively leverage task-specific data for embedding generation. This is a practical and timely issue given the increasing reliance on embeddings in NLP tasks.
2. Well-Structured and Comprehensive Experiments: The experiments are methodically designed, covering multiple datasets, languages, and embedding combination techniques. The inclusion of under-resourced languages adds value to the study.
3. Practical Insights: The findings provide actionable insights, such as the recommendation to concatenate embeddings for low-resource languages, which could guide future research and applications.
Weaknesses
1. Lack of Novelty: The core idea of using task-specific data for embeddings is not new and has been explored in prior work. The paper's contribution is incremental, focusing on specific applications and combinations rather than introducing fundamentally new methods.
2. Predictable Results: The experimental outcomes, such as the superiority of task-specific embeddings and the benefits of combining embeddings, are intuitive and expected. This limits the paper's impact as it does not challenge existing assumptions or provide surprising insights.
3. Lack of Statistical Significance Testing: The absence of statistical significance testing undermines the reliability of the reported improvements, especially when the differences between configurations are marginal.
4. Practical Limitations: The reliance on tools like OpinionFinder, which are unavailable for most languages, limits the practicality of the proposed methods for truly low-resource settings.
5. Clarity and Detail: Certain sections, such as the motivation for specific methods and the process of isolating subjective information, are unclear. Additionally, there are mismatches in variable definitions and formulas, which detract from the paper's readability and rigor.
Questions to Authors
1. How do you address the reliance on tools like OpinionFinder for under-resourced languages where such tools are unavailable? Could you propose alternative methods for subjectivity detection in these cases?
2. Why was statistical significance testing omitted, and how confident are you in the reported improvements given the lack of such testing?
3. Could you clarify the motivation behind the splicing method and its practical advantages over appending or concatenation?
Recommendation
While the paper provides useful insights and practical recommendations for sentiment analysis, its lack of novelty, predictable results, and insufficient statistical rigor limit its contribution to the field. I recommend rejection unless the authors can address the statistical and clarity issues and better position their work within the context of prior research.