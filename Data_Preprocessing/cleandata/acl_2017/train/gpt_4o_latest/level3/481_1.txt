Review of the Submission
Summary and Contributions
This paper introduces FOIL-COCO, a diagnostic dataset designed to evaluate the integrative capabilities of vision and language (LaVi) models. The dataset extends MS-COCO by introducing "foil captions," which are nearly identical to correct captions but contain a single incorrect word. The authors propose three tasks: (1) distinguishing correct captions from foil captions, (2) identifying the incorrect word, and (3) suggesting the correct replacement. The dataset and tasks expose significant limitations in state-of-the-art LaVi models, which perform poorly compared to human benchmarks. The work highlights fundamental flaws in the models' ability to deeply integrate visual and textual modalities, raising important questions about their underlying representations.
The primary contributions of the paper are:
1. FOIL-COCO Dataset: A novel dataset that systematically challenges LaVi models by requiring fine-grained understanding of image-text relationships.
2. Insightful Evaluation Framework: The proposed tasks provide a diagnostic lens to analyze model failures, revealing gaps in both visual and linguistic representations.
3. Empirical Findings: The results demonstrate catastrophic failures of state-of-the-art models on the proposed tasks, emphasizing the need for better integration of vision and language.
Strengths
1. Dataset Contribution: The FOIL-COCO dataset is a significant addition to the field, as it minimizes language bias and provides a challenging benchmark for LaVi models. The one-word foil approach is a key innovation that sets this work apart from prior efforts like Ding et al.
2. Research Significance: The paper raises critical questions about the true capabilities of LaVi models, challenging the community to move beyond superficial performance metrics. The diagnostic nature of the tasks provides valuable insights into model limitations.
3. Human Baseline Comparison: The inclusion of human performance benchmarks underscores the stark contrast between human understanding and model capabilities, making the findings more compelling.
4. Novelty in Task Design: The three proposed tasks are well-motivated and go beyond traditional captioning or VQA setups, requiring models to demonstrate fine-grained reasoning and error correction.
Weaknesses
1. Results Section: The results section is underdeveloped and lacks detailed analysis. While the authors identify key failure modes, more in-depth exploration of model errors (e.g., specific failure cases or visual/textual correlations) would strengthen the paper.
2. Baseline Models: The paper does not include a simple vision+language baseline, such as a Bag-of-Words (BoW) combined with CNN features. This omission makes it harder to contextualize the performance of state-of-the-art models.
3. Bias in Dataset Creation: The use of NeuralTalk to select the "hardest" foils introduces a potential self-reinforcement bias. While the authors address this concern in their response, further validation of the dataset's robustness would be beneficial.
4. Clarity Issues: Certain terms, such as "supercategory," and baseline models, like the "chance model," are not clearly explained, which could hinder reproducibility and understanding.
5. Similarity to Prior Work: Although the one-word foil approach is a meaningful distinction, the paper is conceptually similar to Ding et al. A stronger discussion of how this work advances beyond prior efforts would be helpful.
Questions to Authors
1. Could you provide more detailed examples of model failures, particularly in Tasks 2 and 3? For instance, are there specific types of foil words (e.g., semantically similar ones) that models consistently struggle with?
2. How does the dataset generalize to other LaVi tasks or domains? Could FOIL-COCO be extended to evaluate models trained on datasets beyond MS-COCO?
3. Have you considered alternative methods to select "hardest" foils that do not rely on NeuralTalk, to mitigate potential biases?
Recommendation
This paper makes a valuable contribution to the field by introducing a diagnostic dataset and tasks that expose critical weaknesses in LaVi models. Despite some weaknesses in the results section and baseline comparisons, the novelty and significance of the dataset and evaluation framework warrant acceptance. I recommend acceptance with minor revisions to address the clarity and analysis issues.