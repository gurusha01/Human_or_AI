Review of the Paper
Summary and Contributions
This paper systematically explores the suitability of various subword units (characters, character trigrams, morphs) and composition functions (bi-LSTMs, CNNs, addition) for word representation in language modeling. The authors conduct experiments across ten typologically diverse languages, aiming to evaluate the effectiveness of these combinations. The key contributions of the paper are:
1. A novel finding that character-trigram representations composed with bi-LSTMs outperform most other configurations across multiple languages.
2. A detailed comparison of subword representation models, revealing that character-level representations are effective but do not fully substitute for explicit morphological knowledge.
3. Insights into the interaction between morphological typology and the effectiveness of subword models, with notable results for agglutinative and root-and-pattern languages.
Strengths
1. Well-Described Motivation: The paper provides a clear and compelling motivation for studying subword representations, especially in the context of morphologically diverse languages. The discussion of the limitations of existing word-level representations and the potential of subword models is thorough and well-grounded.
2. Comprehensive Evaluation: The experiments are conducted on ten typologically diverse languages, offering a broad perspective on the effectiveness of subword models. This diversity strengthens the generalizability of the findings.
3. Novel Insights: The discovery that character trigrams with bi-LSTMs outperform other configurations in most languages is a significant contribution. The paper also highlights the limitations of character-level models in capturing morphological regularities, which is an important nuance.
4. Qualitative Analysis: The inclusion of targeted perplexity results and nearest-neighbor analyses provides valuable qualitative insights into the strengths and weaknesses of the models.
Weaknesses
1. Selection Bias in Experimental Languages: The choice of languages, while diverse, raises concerns about representativeness. For instance, the inclusion of only one language with reduplication morphology (Indonesian) limits the generalizability of findings for this typology.
2. Insufficient Experimental Support for Claims: While the paper claims that character-level models are effective across morphological typologies, the results for certain languages (e.g., Japanese and Malay) show only modest improvements. This weakens the claim of universal applicability.
3. Limited Scope of Morphological Analysis: The evaluation of explicit morphological knowledge is restricted to Arabic and Czech, despite the availability of analyzers for other morphologically rich languages like Turkish and Japanese. This limits the robustness of the conclusions.
4. Exclusive Focus on Character Trigrams: The paper does not justify its focus on character trigrams over other n-grams (e.g., bigrams, fourgrams). Exploring these alternatives could have provided a more comprehensive understanding of subword representations.
Questions to Authors
1. Why were other n-gram configurations (e.g., bigrams, fourgrams) not explored alongside character trigrams? Could these yield different results?
2. What criteria were used to select the ten experimental languages? How representative are they of their respective typological categories?
3. Could the authors elaborate on why automatic morphological analysis for languages like Turkish and Japanese was not included in the experiments?
Additional Comments
The paper is well-written and addresses an important problem in NLP. However, addressing the concerns about selection bias and the limited scope of morphological analysis would significantly strengthen the work. Minor typos, such as the missing reference in the introduction (line 88) and the typo "root-and-patter" (line 524), should also be corrected.