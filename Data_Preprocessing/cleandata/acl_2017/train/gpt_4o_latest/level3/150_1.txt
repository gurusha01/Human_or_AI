Review of the Paper
Summary and Contributions:  
This paper introduces a novel deep character-level neural machine translation (DCNMT) model that addresses key challenges in neural machine translation (NMT), such as large vocabulary bottlenecks and handling out-of-vocabulary (OOV) words. The proposed architecture combines a hierarchical decoder and a word encoder that learns morphological representations, enabling efficient training and effective translation. The model is evaluated on English-French (En-Fr), English-Czech (En-Cs), and Czech-English (Cs-En) translation tasks, demonstrating competitive BLEU scores compared to state-of-the-art character-based models. Notably, the model achieves reasonable translation quality after just one training epoch and showcases its ability to handle unseen or misspelled words by leveraging learned morphemes. The release of the code and trained models further enhances the reproducibility and practical utility of the work.
Strengths:  
1. Novel Architecture: The hierarchical design, which integrates character, subword, and word-level representations, is an innovative adaptation of the encoder-decoder framework. The ability to learn morphological tokenizations and back off to morphemes for unseen words is a significant advancement.  
2. Efficiency: The model demonstrates impressive training efficiency, achieving competitive BLEU scores after just one epoch, outperforming some baselines. This is particularly notable given the computational challenges of character-level NMT.  
3. Practical Contributions: The release of the code and trained models for En-Fr, Cs-En, and En-Cs tasks provides a valuable resource for the research community, enabling replication and further exploration.  
4. Robustness: The model's capacity to handle misspelled and nonce words is a practical strength, addressing a common limitation in word-level NMT systems.  
5. Morphology Learning: The qualitative analysis of learned morphemes demonstrates the model's ability to capture meaningful linguistic structures, which is a compelling contribution to character-based NMT research.
Weaknesses:  
1. Incomplete Dataset Details: The paper does not explicitly mention the WMT test and development sets used, requiring readers to refer to the code README. This omission could hinder accessibility for readers unfamiliar with the dataset.  
2. Visualization Clarity: Figure 2, which illustrates the six-level architecture, lacks clarity and sufficient explanation. Improved captions or additional diagrams would enhance comprehension.  
3. Comparison with State-of-the-Art: While the model achieves competitive BLEU scores, it does not surpass state-of-the-art systems. This limitation is not adequately discussed, leaving room for improvement in contextualizing the results.  
4. Documentation Issues: The software README, while functional, could benefit from explicit examples and a `--help` flag to improve usability for practitioners.  
5. Language and Formatting: Minor English disfluencies, misspellings, and LaTeX formatting issues detract from the paper's overall polish and readability.
Questions to Authors:  
1. Could you clarify why the results do not surpass state-of-the-art systems and discuss potential avenues for improvement?  
2. Are there specific linguistic phenomena (e.g., compound words, agglutinative languages) where the model particularly excels or struggles?  
3. How does the model's performance scale with longer training epochs or larger datasets?  
Conclusion:  
This paper is a valuable contribution to character-based NMT research, offering a novel and efficient architecture that addresses key challenges in the field. While there are areas for improvement, particularly in documentation, visualization, and contextualizing results, the strengths of the work outweigh its weaknesses. I recommend acceptance with minor revisions to address the identified issues.