Review of the Submission
Summary and Contributions:  
This paper introduces a novel text similarity measure, TextFlow (XF), inspired by DNA sequence alignment algorithms. The proposed measure leverages the sequential nature of language by representing text pairs as continuous curves and computing similarity based on sub-sequence matching, word positions, and mismatches. The key contributions of the paper are:  
1. The introduction of TextFlow, a standalone similarity measure that is asymmetric, adaptable, and provides consistent high performance across diverse tasks such as paraphrase detection, textual entailment recognition, and ranking.  
2. A neural network architecture to train TextFlow parameters for task-specific optimization.  
3. A new evaluation metric, CORE, designed to assess the consistency of high-performance systems across datasets.  
4. Comprehensive evaluation on eight datasets spanning three tasks, demonstrating the robustness and versatility of TextFlow.  
Strengths:  
1. Innovative Evaluation Measure: The introduction of the CORE metric is a significant contribution, as it provides a nuanced way to evaluate systems based on both performance and consistency across datasets. This is particularly valuable in multi-task settings.  
2. High Accuracy: The proposed TextFlow measure achieves competitive or superior performance compared to state-of-the-art similarity measures across multiple datasets. The results highlight its adaptability and effectiveness in capturing semantic relationships.  
3. Diverse Evaluation: The experiments are conducted on a large and diverse set of datasets, ensuring the generalizability of the findings. The inclusion of multiple tasks (entailment, paraphrase detection, ranking) further strengthens the evaluation.  
Weaknesses:  
1. Typos and Presentation Issues: The paper contains several typographical errors and inconsistencies:  
   - Line 116-117: "to design of a new" → "to design a new."  
   - Line 176-177: Incorrect reference to Figure 2 → should be Figure 1.  
   - Line 265: "among the the top" → "among the top."  
   - Line 320: Figure 4 is not introduced in the body of the article.  
   - Line 434: "the dataset was contains" → "the dataset contains."  
   - Line 486-487: Incorrect reference to Table 3 → should be Table 1.  
   - "Tensorflow" should be replaced by "TextFlow."  
2. Lack of Reproducibility: While the neural networks are implemented in Python, the paper does not mention code availability. This omission hinders reproducibility and limits the impact of the work.  
3. Imprecisions in Feature Computation: The accuracy of lemma, POS, and WordNet synset feature computation is not detailed, nor is their impact on similarity accuracy discussed. This lack of clarity weakens the interpretability of the results.  
4. Dataset Sharing Details Missing: The paper does not specify how the training and evaluation datasets will be shared (e.g., on-demand or under license). This omission raises concerns about the reproducibility of the experiments.  
Questions to Authors:  
1. Can you provide more details on the computation of lemma, POS, and WordNet synset features, and their specific contributions to the similarity measure's accuracy?  
2. Will the code for TextFlow and the neural network training be made publicly available? If not, how do you plan to address reproducibility concerns?  
3. How do you plan to share the datasets used for training and evaluation? Are there any licensing restrictions?  
Conclusion:  
Overall, the paper presents a compelling and innovative approach to text similarity measurement. The introduction of TextFlow and the CORE metric are significant contributions, and the experimental results are promising. However, the paper's impact is diminished by typographical errors, lack of reproducibility, and insufficient details on feature computation. Addressing these weaknesses in the revision process would significantly strengthen the submission.  
Recommendation: Accept with minor revisions.