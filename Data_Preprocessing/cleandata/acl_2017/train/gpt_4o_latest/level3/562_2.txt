Review of the Paper
Summary and Contributions:  
This paper presents a novel approach to relation extraction by reframing it as a reading comprehension task. The authors propose mapping relations to natural-language questions, enabling the use of question-answering (QA) techniques for extracting relations from text. The key contributions of the paper are:  
1. Reduction of Relation Extraction to QA: The paper introduces a method to transform relation extraction into a QA problem by associating natural-language questions with relations. This allows the use of advanced QA models and facilitates zero-shot learning for unseen relations.  
2. Large-Scale Dataset Creation: The authors construct a massive dataset of over 30 million examples using a cost-efficient schema querification process, combining distant supervision and crowdsourced question templates.  
3. Zero-Shot Relation Extraction: The proposed method demonstrates the ability to generalize to unseen relations and question templates, achieving an F1 score of 41% in zero-shot settings, which sets a benchmark for future work.  
Strengths:  
1. Strong Performance: The proposed method achieves competitive results, with an F-measure of ~90% on seen relations and ~41% in zero-shot settings. The ability to generalize to unseen questions and relations is a significant strength, as it demonstrates the flexibility of the approach.  
2. Scalability: The schema querification process is highly efficient, enabling the creation of a large-scale dataset at a modest cost. This scalability is a valuable contribution to the field.  
3. Well-Written and Clear Presentation: The paper is well-structured and clearly articulates the methodology, experiments, and results. The analysis of errors and the discussion of generalization mechanisms are insightful.  
4. Zero-Shot Learning Insights: The paper highlights the importance of global type detection and relation paraphrase detection, providing valuable insights into the challenges and opportunities in zero-shot relation extraction.  
Weaknesses:  
1. Lack of Evaluation on Standard Datasets: The paper does not evaluate the proposed method on widely used relation extraction benchmarks, such as TACRED or SemEval datasets. This omission makes it difficult to compare the approach with existing methods and assess potential biases.  
2. Missing Baseline Comparison: The authors do not compare their QA-based approach with a traditional relation extraction model trained on the same data. This comparison would help quantify the benefits of the proposed reframing.  
3. Limited Error Analysis: While the paper provides some analysis of errors, it could benefit from a more detailed breakdown of failure cases, particularly in the zero-shot setting, to identify specific areas for improvement.  
Questions to Authors:  
1. How does the proposed method perform on standard relation extraction datasets, such as TACRED or SemEval?  
2. Have you considered comparing the QA approach to a traditional relation extraction model trained on the same dataset? If so, what were the results?  
3. Could the schema querification process be extended to other tasks beyond relation extraction?  
General Impression:  
The paper presents an innovative and promising approach to relation extraction, with strong results and valuable insights into zero-shot learning. However, the lack of evaluation on standard datasets and missing baseline comparisons limit the ability to fully assess the method's impact. Addressing these weaknesses would significantly strengthen the paper. Overall, this work makes a meaningful contribution to the field and opens up new avenues for research in zero-shot relation extraction.