Review of the Paper
Summary and Contributions
This paper introduces Iterated Dilated Convolutional Neural Networks (ID-CNNs) as a novel approach to sequence tagging, specifically for Named Entity Recognition (NER). The authors propose the use of dilated convolutions to efficiently capture long-range dependencies while maintaining computational efficiency. The key contributions of the paper are:
1. Novel Architectural Design: The ID-CNN architecture combines dilated convolutions with parameter sharing across convolutional blocks. This design enables the model to capture broad context without overfitting, and the intermediate supervision at each convolutional block refines predictions iteratively.
   
2. Efficiency Gains: The ID-CNN achieves significant test-time speedups compared to Bi-LSTM-CRF models, with up to 14x faster decoding for greedy predictions and 8x faster decoding for Viterbi predictions on long sequences.
3. Comprehensive Evaluation: The paper provides extensive ablation studies and comparisons against strong baselines, including Bi-LSTM and CNN architectures, demonstrating the effectiveness of ID-CNNs on two benchmark NER datasets (CoNLL-2003 and OntoNotes 5.0).
Strengths
1. Thorough Experiments: The authors conduct detailed experiments, including comparisons with various architectures (e.g., Bi-LSTM, Bi-LSTM-CRF, and non-dilated CNNs). The results convincingly show the speed and accuracy trade-offs of ID-CNNs.
   
2. Novel Training Approach: The use of intermediate supervision at each convolutional block is a unique contribution that improves gradient flow and allows later blocks to refine earlier predictions. This is a promising idea for training deep architectures in NLP.
3. Practical Relevance: The focus on computational efficiency is highly relevant for large-scale NLP applications, where speed and scalability are critical. The ability to process entire documents efficiently is a significant advantage.
4. Clarity of Writing: The paper is well-written, with clear explanations of the model architecture, training procedures, and experimental setup.
Weaknesses
1. Limited Scope: Despite the title suggesting broader applicability to sequence tagging tasks, the experiments are restricted to English NER. The generalizability of the proposed method to other sequence tagging tasks (e.g., part-of-speech tagging or chunking) or languages remains unexplored.
2. Lack of Clarity in Section 4.1: The explanation of padding and output resolution in the model architecture is unclear. A more detailed discussion or visual illustration would help readers better understand how the model handles input-output alignment.
3. Missing Ablation on Model Depth: While the paper includes several ablation studies, it does not explore the impact of the number of convolutional layers on performance. This is a critical factor for understanding the trade-off between receptive field size and overfitting.
Questions to Authors
1. Can the proposed ID-CNN architecture be easily extended to other sequence tagging tasks, such as part-of-speech tagging or chunking? If so, what modifications (if any) would be required?
2. How does the model handle padding for sequences longer than the receptive field size? Does this introduce any edge effects in predictions?
3. Would incorporating character-level embeddings or external lexicons further improve the performance of ID-CNNs, particularly on datasets like OntoNotes?
Additional Comments
The authors' decision to focus on NER is reasonable given the rebuttal period, but future work should explore the broader applicability of ID-CNNs to other NLP tasks. Additionally, the inclusion of implementation details (e.g., hyperparameters, dropout rates) in the appendix is appreciated and aids reproducibility. Overall, this paper makes a strong contribution to the field of efficient sequence tagging.