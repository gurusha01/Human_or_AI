The paper presents a novel exploration of historical text normalization using encoder-decoder architectures, with a particular focus on multi-task learning (MTL) and its relationship to attention mechanisms. The primary contribution is the hypothesis that MTL with a pronunciation task as an auxiliary objective can implicitly learn attention mechanisms, thereby improving performance without explicitly incorporating attention. This is a compelling idea, as it bridges the gap between two prominent areas in machine learning and offers insights into the inner workings of MTL.
Strengths
The connection drawn between MTL and attention mechanisms is both intriguing and thought-provoking. The authors provide a robust experimental framework, evaluating their models across 44 datasets of Early New High German, and demonstrate a modest but consistent improvement of 2% over state-of-the-art baselines. The use of grapheme-to-phoneme mappings as an auxiliary task is well-motivated, given the phonological roots of spelling variation. Additionally, the paper includes a detailed analysis of learned vector representations, saliency, and parameter changes, which adds depth to the findings and enhances interpretability. The writing is clear, and the authors' response to reviewer feedback demonstrates a willingness to address concerns, raising the overall quality of the submission.
Weaknesses
Despite its strengths, the paper suffers from several critical weaknesses. Most notably, the relationship between the normalization and pronunciation tasks is not sufficiently justified, leaving the rationale for why MTL induces an implicit attention mechanism unclear. Furthermore, while the methods are appropriate, the models themselves lack novelty, as they are extensions of standard encoder-decoder architectures. Key details about the implementation of the attention mechanism are omitted, which weakens the reproducibility and interpretability of the results. The experimental results also reveal a puzzling contradiction: while the models exhibit significant structural changes (e.g., in saliency and parameter weights), the accuracy improvements are relatively small (2%). This discrepancy is not adequately addressed. Additionally, Figure 4, which compares output vectors and hidden layer dimensions, is poorly justified and lacks sufficient explanation.
Suggestions for Improvement
1. Provide a stronger theoretical or empirical justification for the connection between normalization and pronunciation tasks in the MTL setup.
2. Include more details about the attention mechanism and its implementation to enhance reproducibility.
3. Address the contradiction between large model changes and small accuracy improvements, possibly by analyzing failure cases or exploring complementary evaluation metrics.
4. Consider ensembling the new and old models, as suggested, to leverage their distinct failure cases and potentially improve performance.
Conclusion
Overall, the paper is solid and makes a meaningful contribution to the field of historical text normalization and MTL. However, the lack of novelty in the models and the omission of critical details limit its impact. With revisions addressing the aforementioned weaknesses, the paper could become a stronger candidate for acceptance. Current score: 4/5.