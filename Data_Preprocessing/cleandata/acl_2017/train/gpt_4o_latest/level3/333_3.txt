Review of "Selective Encoding for Abstractive Sentence Summarization"
Summary and Contributions
This paper introduces a novel selective encoding model (SEASS) for abstractive sentence summarization. The proposed method extends the sequence-to-sequence (seq2seq) framework by incorporating a selective gating mechanism that filters and tailors encoded hidden states for summarization. The model consists of three key components: a bidirectional GRU encoder, a selective gate network, and an attention-equipped GRU decoder. The selective gating mechanism is designed to improve the encoding process by focusing on salient information, thereby reducing the burden on the decoder.
The paper demonstrates the effectiveness of SEASS through extensive evaluations on three datasets: English Gigaword, DUC 2004, and MSR-ATC. The results show consistent improvements over state-of-the-art baselines, with gains of 1-2 ROUGE points across datasets. The selective mechanism is highlighted as a novel contribution with potential applications beyond summarization.
Strengths
1. Novelty of the Selective Mechanism: The introduction of a selective gating network is innovative and addresses a key limitation in standard seq2seq models by explicitly modeling the selection of salient information. This approach has potential applications in other tasks requiring selective encoding, such as machine translation or text simplification.
2. Consistent Performance Gains: The proposed method achieves significant improvements over competitive baselines, including seq2seq models with attention. The gains are consistent across multiple datasets and evaluation metrics, demonstrating the robustness of the approach.
3. Thorough Evaluation: The paper provides a comprehensive evaluation, including comparisons with strong baselines, ablation studies, and visualizations of the selective gating mechanism. The heatmap analysis effectively illustrates the model's ability to focus on important input words.
4. Clear Technical Exposition: Most of the technical details are well-explained, making it easier to understand the architecture and training process. The use of visualizations and detailed descriptions of the gating mechanism adds clarity.
Weaknesses
1. Equation 16 Notation: The notation in Equation 16 is unclear and requires further explanation. This is a critical part of the decoder's operation, and the lack of clarity may hinder reproducibility.
2. Baseline Comparisons: While the paper compares SEASS to several baselines, it does not include stronger baselines, such as seq2seq models with additional encoder layers or transformer-based models. This omission raises concerns about whether the observed gains are due to the selective mechanism or architectural differences.
3. GRU/LSTM Mismatch: The authors use GRUs in their model but compare it to baselines that use LSTMs. The performance gains may partially stem from differences in recurrent unit architectures rather than the proposed selective mechanism. A fair comparison should use consistent architectures across all models.
4. Writing Quality: The writing in the introduction, abstract, and related work sections is subpar, with grammatical errors and poor organization. The related work section should be moved closer to the introduction and better contextualized to highlight similarities and differences with prior approaches.
Questions to Authors
1. Can you clarify the notation and derivation of Equation 16? Specifically, how does the maxout layer interact with the decoder's hidden states and context vectors?
2. Have you tested SEASS with stronger baselines, such as seq2seq models with additional encoder layers or transformer-based architectures? If not, why were these omitted?
3. How do you ensure that the observed performance gains are due to the selective gating mechanism and not differences in GRU vs. LSTM architectures?
Recommendation
While the paper presents a novel and promising approach to abstractive summarization, the concerns about baseline comparisons and unclear notation in critical sections must be addressed. Additionally, the writing quality should be improved to enhance readability. With these revisions, the paper would make a strong contribution to the field. I recommend acceptance with minor revisions.