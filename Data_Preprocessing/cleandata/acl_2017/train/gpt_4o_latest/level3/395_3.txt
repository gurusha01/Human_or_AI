Review of the Paper
Summary and Contributions
This paper introduces DRL-Sense, a novel framework for learning multi-sense word representations using reinforcement learning (RL). The model jointly optimizes a sense selection module and a sense representation module, leveraging a reward passing mechanism to facilitate training. The authors claim the following contributions: (1) the use of RL for multi-sense embeddings, (2) linear time complexity for sense selection, (3) non-parametric learning for automatic sense induction, and (4) state-of-the-art performance on contextual word similarity and competitive results on synonym selection tasks with significantly less training data compared to prior work.
Strengths
1. Performance Improvements: The proposed model demonstrates state-of-the-art results on contextual word similarity (SCWS) and significant improvements on synonym selection tasks (ESL-50 and RD-300). The ability to achieve comparable performance to word2vec with only 1/100th of the training data is a notable strength.
2. Non-Parametric Learning: The non-parametric mechanism for automatic sense induction is effective, as evidenced by the model's ability to identify polysemous words without prior knowledge, aligning with results from related work.
3. Qualitative Analysis: The paper provides clear qualitative evidence of the model's ability to differentiate between senses (e.g., "mercury" as a planet vs. an element), which supports the claim of robust sense representation learning.
4. Modular Design: The modular architecture, which separates sense selection and representation, is well-motivated and allows for efficient training and scalability.
Weaknesses
1. Unclear Claims about Equation (2): The paper claims that optimizing equation (2) underestimates the probability of senses, but this assertion is not well-supported. The optimization problem appears straightforward, and the reasoning for this claim is not adequately justified.
2. Ambiguity in Reward Definition: The reward computation in the Q-learning framework is not clearly explained. Specifically, the relationship between the reward signal and cross-entropy in equation (4) is vague, and the dimensions of variables in the cross-entropy loss are inconsistent.
3. Lack of RL Properties: The connection between DRL-Sense and Q-learning is tenuous. The model lacks key RL components such as state-action transitions and the Markov property, which raises concerns about whether the RL formulation is appropriate.
4. Dropout for Exploration: The use of dropout for exploration is unconventional and not well-justified. Dropout is typically used for regularization, and its role in exploration-exploitation trade-offs is unclear.
5. Negative Sampling Omission: The absence of negative sampling for distribution modeling is a notable limitation. Incorporating noise contrastive estimation could improve the robustness of the sense embeddings.
6. Timing of Sense Creation: Creating new senses during the early, unstable training phase may introduce noise, potentially degrading the quality of the learned embeddings.
Questions to Authors
1. Can you provide a more detailed explanation of why optimizing equation (2) underestimates the probability of senses? Are there empirical results to support this claim?
2. How does the reward signal in the Q-learning framework relate to the cross-entropy loss in equation (4)? Can you clarify the dimensionality issues in the loss computation?
3. Why was dropout chosen as the exploration mechanism instead of more conventional methods like epsilon-greedy? How does dropout impact the stability of training?
4. Have you considered incorporating negative sampling techniques such as noise contrastive estimation? If so, why were they omitted?
5. How does the timing of sense creation during training affect the overall performance? Have you explored delaying sense creation until the model stabilizes?
Recommendation
While the paper presents a novel approach with promising results, the lack of clarity in key theoretical claims and ambiguities in the RL formulation weaken its overall impact. I recommend acceptance with major revisions, contingent on addressing the issues related to reward definition, RL properties, and the justification of unconventional design choices.