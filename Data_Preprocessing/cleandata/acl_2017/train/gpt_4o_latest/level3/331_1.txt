Review of the Submission
Summary and Contributions
This paper introduces a novel task, concept-map-based multi-document summarization (MDS), and provides a comprehensive benchmark corpus to support research in this area. The authors propose a new crowdsourcing scheme, low-context importance annotation, to efficiently determine the importance of propositions extracted from large document clusters. The contributions of the paper are as follows:
1. Benchmark Corpus: The creation of a publicly available dataset of 30 topics, each with around 40 web documents and corresponding concept maps, is a significant contribution. The dataset is large-scale, heterogeneous, and well-annotated, making it a valuable resource for the community.
2. Novel Crowdsourcing Scheme: The low-context importance annotation method is innovative and addresses challenges in crowdsourcing reliability for summarization tasks. The authors demonstrate its effectiveness through pilot studies and reliability tests.
3. Baseline and Evaluation Protocol: The paper provides a baseline system for concept-map-based MDS and proposes evaluation metrics, including strict matching, METEOR, and ROUGE-2, to compare generated and reference concept maps.
Strengths
1. Detailed Guidelines and Illustrations: The paper is well-organized and provides clear explanations of the task, dataset creation process, and evaluation protocol. The inclusion of explicit illustrations, such as example concept maps, enhances understanding.
2. Benchmark Contribution: The dataset fills a critical gap in the field by providing a benchmark for concept-map-based MDS, which has been underexplored compared to traditional summarization tasks.
3. Clarity and Supplementary Materials: The writing is clear and concise, with sufficient supplementary materials, including scripts and documentation, to enable reproducibility and further research.
Weaknesses
1. Reliability of Document-Independent Annotation: The low-context importance annotation method, while innovative, raises concerns about its reliability. Determining the importance of propositions without document context may lead to subjective or inconsistent annotations, as concept relevance is often context-dependent.
2. Overlap with Generic Summarization: The task of concept map extraction overlaps significantly with traditional summarization systems. The paper does not adequately justify why concept map generation should be treated as a separate task, especially given the challenges of maintaining readability as the number of nodes grows.
3. Nature of the Dataset: The dataset resembles a common-sense knowledge graph more than a true summary. This distinction may limit its applicability to summarization tasks that require more abstract or interpretive outputs.
Questions to Authors
1. Given the overlap with traditional summarization systems, how do you envision concept-map-based MDS being used in real-world applications? What advantages does it offer over textual summaries?
2. How do you address the challenge of readability in concept maps as the number of nodes increases? Have you considered methods to optimize the visual structure of the maps?
3. Can you provide more evidence or analysis to support the claim that low-context importance annotation reliably captures the relevance of propositions without document context?
Conclusion
This paper makes a valuable contribution by introducing a new benchmark corpus and crowdsourcing scheme for concept-map-based MDS. However, concerns about the reliability of document-independent annotation and the overlap with traditional summarization tasks need to be addressed. While the dataset and evaluation protocol are strong contributions, the justification for treating concept map generation as a distinct task requires further elaboration. Overall, the submission is a promising step toward advancing research in this area but would benefit from addressing the identified weaknesses.