Review of Submission
Summary and Contributions
This paper investigates techniques to enhance coverage in parsing with Head-driven Phrase Structure Grammar (HPSG) by evaluating several robust parsing methods. The primary contributions of the work are:
1. Comprehensive Evaluation Framework: The authors present a well-defined evaluation methodology for robust HPSG parsing, including the creation of multiple datasets and the use of Elementary Dependency Matching (EDM) metrics for intrinsic evaluation.
2. Empirical Comparison of Techniques: The paper systematically compares baseline HPSG parsing with five robust parsing methods, ranging from grammar-augmented approaches (e.g., bridging, Pacman) to PCFG-based methods (e.g., csaw, hybrid approaches).
3. Insights into Robust Parsing: While not introducing a novel parsing technique, the study provides valuable insights into the trade-offs between coverage, speed, and accuracy for robust parsing methods.
Strengths
1. Comprehensive Evaluation: The paper excels in its empirical rigor, comparing a wide range of techniques across multiple datasets. The inclusion of both intrinsic metrics (e.g., EDM F1) and speed measurements provides a holistic view of performance.
2. Diverse Techniques: By evaluating both grammar-augmented and PCFG-based methods, the paper offers a broad perspective on robust parsing strategies, highlighting their strengths and limitations.
3. Practical Relevance: The work addresses a critical challenge in HPSG parsing—coverage gaps—making it relevant for real-world applications where unparseable inputs can disrupt downstream tasks.
Weaknesses
1. Lack of In-depth Analysis: While the paper provides overall results, it lacks detailed analysis of method-specific behaviors. For example, the authors could include qualitative examples of sentences where specific techniques succeed or fail, shedding light on their strengths and weaknesses.
2. Limited Novelty: The paper focuses on empirical evaluation rather than introducing a novel parsing technique. While the evaluation is valuable, the lack of methodological innovation may limit its impact.
3. Clarity of Presentation: Some aspects of the paper are unclear. For instance, it is not explicitly stated whether "pacnv+ut" in Tables 1 and 2 corresponds to "pacnv" in Section 3.4.3, which could confuse readers.
Suggestions for Improvement
1. Detailed Analysis: Include qualitative examples of sentences parsed differently by the techniques to illustrate their strengths and limitations. This would provide deeper insights into the results.
2. Evaluation Metrics: Add EDM precision and recall figures to Table 2 to complement the F1 scores and provide a more granular understanding of performance.
3. Clarity: Clarify whether "pacnv+ut" in the tables is the same as "pacnv" in Section 3.4.3 to avoid ambiguity.
Recommendation
While the paper does not introduce a novel technique, its comprehensive evaluation and practical relevance make it a valuable contribution. However, the lack of detailed analysis and clarity in some areas slightly diminishes its impact. I recommend acceptance with minor revisions to address the identified weaknesses.
Questions to Authors
1. Can you provide examples of sentences where specific techniques (e.g., bridging vs. hybrid-ww) succeed or fail? This would help clarify the trade-offs between methods.
2. Is "pacnv+ut" in Tables 1 and 2 the same as "pacnv" in Section 3.4.3? If not, what does "ut" represent?
Additional Comments
The paper could benefit from a more explicit discussion of how the results might generalize to extrinsic evaluations or downstream applications. Additionally, exploring ways to adapt the parse ranking model to robust parsing scenarios could be an interesting direction for future work.