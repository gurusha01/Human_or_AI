Review of the Paper
Summary and Contributions:
This paper introduces the first neural network-based approach to argumentation mining using a Pointer Network (PN) model with multi-task learning. The authors propose a joint PN model to simultaneously predict argument component (AC) types (e.g., claim, premise) and extract links between ACs, achieving state-of-the-art results on two datasets: the Persuasive Essay corpus and the Microtext corpus. The main contributions of the paper are as follows:
1. Novel Application of Pointer Networks: The paper adapts PNs, originally designed for algorithmic tasks, to the domain of argumentation mining, leveraging their ability to model sequential data and enforce partial tree structures.
2. Joint Modeling for Multi-task Learning: The proposed model jointly optimizes for both link prediction and AC classification, demonstrating that this dual-task approach significantly improves performance.
3. Empirical Validation: The model outperforms existing methods, including feature-rich SVMs and Integer Linear Programming (ILP) joint models, on multiple metrics across two datasets.
Strengths:
1. State-of-the-Art Performance: The proposed model achieves superior results compared to baseline and prior methods on key tasks, particularly in link prediction, where it outperforms even models with explicit structural constraints.
2. Effective Multi-task Learning: The joint optimization approach is well-motivated and empirically validated, with results showing that combining the two tasks improves performance for both.
3. Comprehensive Evaluation: The paper includes a detailed ablation study and analysis of feature importance, providing insights into the contributions of different components of the model.
Weaknesses:
1. Limited Novelty: While the application of PNs to argumentation mining is novel, the approach primarily adapts existing methods (PNs and multi-task learning) without introducing significant methodological innovations. This may limit its suitability for a high-impact venue like ACL.
2. Lack of Qualitative and Error Analysis: The paper does not provide qualitative examples or a detailed error analysis, which would help clarify the model's strengths and weaknesses and guide future improvements.
3. Insufficient Justification for PN Choice: The motivation for using PNs over alternatives like bi-directional LSTMs with attention is not well-articulated. While the results favor PNs, stronger theoretical or empirical justification is needed.
4. Clarity Issues: Figures 2 and 3 are unclear, with inconsistencies in decoder inputs and undefined abbreviations. Equation (8) is also confusing in its explanation of component type probabilities, which could hinder reproducibility.
5. Experimental Setup Ambiguities: The description of the experimental setup lacks clarity, particularly regarding the use of PN for the Microtext corpus and the training of the BLSTM model. Additionally, differences from discourse parsing studies using attention mechanisms are not adequately discussed.
Questions to Authors:
1. Could you provide a more detailed comparison of PNs versus bi-directional LSTMs with attention? What specific advantages do PNs offer for argumentation mining tasks?
2. Can you include qualitative examples of successful and failed predictions to better illustrate the model's behavior?
3. How does the model handle cases where the argument structure deviates from a strict tree (e.g., graphs or cycles)?
4. Could you clarify the inconsistencies in Figures 2 and 3, as well as the calculation in Equation (8)?
5. What is the rationale for the choice of hyperparameters, particularly the dropout rate of 0.9?
Recommendation:
While the paper presents a strong empirical contribution and introduces a novel application of PNs to argumentation mining, it lacks sufficient methodological novelty and qualitative analysis to justify acceptance at a top-tier conference. Addressing the clarity issues, providing stronger motivation for the use of PNs, and including qualitative insights would significantly strengthen the submission.