Review of the Paper
Summary and Contributions:
This paper presents a novel A* CCG parsing model that integrates supertagging and dependency scoring, achieving state-of-the-art results in both English and Japanese CCG parsing. The key contributions of the paper are as follows:
1. Efficient A* Decoding with Dependency Modeling: The proposed model decomposes the probability of a CCG tree into supertagging and dependency factors, enabling efficient precomputation of probabilities while explicitly modeling sentence structures through dependencies. This approach avoids the need for deterministic heuristics, such as the "attach low" heuristic used in prior work.
2. Cross-Linguistic Effectiveness: The model demonstrates significant improvements in Japanese CCG parsing, outperforming prior work by a large margin (10 points in clause dependency accuracy), highlighting its adaptability to languages with freer word order.
3. Empirical Validation of Head Modeling: The paper shows that modeling the first word of a constituent as the head outperforms traditional linguistically motivated head rules, such as LEWISRULE, particularly in both English and Japanese experiments.
Strengths:
1. State-of-the-Art Performance: The model achieves the best reported F1 scores for English CCG parsing (88.8% labeled and 94.0% unlabeled) and significantly improves Japanese parsing, demonstrating its robustness across languages.
2. Efficiency of A Decoding: By maintaining a locally factored model, the proposed approach retains the efficiency of A search while incorporating dependency modeling, which is a notable improvement over recursive neural network-based methods.
3. Innovative Head Modeling: The paper provides compelling evidence that simpler head-first or head-final rules can outperform linguistically motivated rules (e.g., LEWISRULE) in terms of both accuracy and adherence to normal form constraints.
4. Comprehensive Evaluation: The authors conduct thorough experiments, comparing their method against strong baselines and exploring the impact of different dependency conversion strategies and tri-training.
Weaknesses:
1. Lack of Baseline Comparisons for Simpler Variations: The paper could benefit from including baseline results for simpler variations of the model, such as not conditioning the tag on the head dependency, to better isolate the contributions of each component.
2. Limited Cross-Linguistic Evaluation: While the model is tested on English and Japanese, additional experiments on other typologically diverse languages would strengthen claims of cross-linguistic effectiveness.
3. Comparison with Lee et al. (2016) on Japanese Data: The paper does not retrain the Lee et al. parser on the Japanese dataset, which would provide a more direct comparison of the two approaches.
4. Citation Omission: The paper does not cite Lewis, He, and Zettlemoyer (2015), whose work on combined dependency and supertagging models is relevant to the proposed approach.
Questions to Authors:
1. How does the performance of the model change if the dependency component is removed entirely (i.e., a pure supertagging model)?
2. Could you provide insights into why HEADFIRST dependencies outperform LEWISRULE in English parsing? Is it primarily due to reduced conflicts between supertagging and dependency predictions?
3. Have you considered testing the model on other languages with different syntactic properties, such as morphologically rich or head-initial languages?
Additional Comments:
The paper is well-written and provides a significant contribution to the field of CCG parsing. Addressing the suggested weaknesses and questions could further enhance the impact and clarity of the work.