Review of the Submission
Summary and Contributions
This paper proposes a cross-lingual name tagging and linking framework for 282 languages, leveraging Wikipedia and external knowledge bases (KBs) like DBpedia and YAGO. The framework identifies name mentions, assigns entity types, and links them to an English KB. The key contributions of the paper are:
1. Scalable Multilingual Framework: The framework is applied to 282 languages, making it one of the most comprehensive multilingual name tagging and linking systems to date.
2. Silver-Standard Annotations: The authors propose a novel method to generate "silver-standard" annotations by transferring labels from English Wikipedia to other languages using cross-lingual links and KB properties.
3. Morphology Analysis for Rich Languages: The paper introduces a method to derive language-specific morphology features from Wikipedia markups, improving performance for morphologically rich languages.
4. Self-Training and Topic Customization: The use of self-training and topic-based data selection enhances the quality of annotations and adapts the system to specific domains.
5. Public Resources: The authors commit to releasing the system, datasets, and annotations, which can serve as benchmarks for future research.
Strengths
1. Clear Contributions: The paper clearly outlines its contributions, which are significant for multilingual NLP and cross-lingual information extraction.
2. Reproducibility: The authors provide both the system and dataset, ensuring reproducibility and encouraging further research.
3. Comprehensive Evaluation: The framework is evaluated on both Wikipedia and non-Wikipedia datasets, demonstrating its robustness across diverse languages and domains.
4. Effective Visualizations: Figures and diagrams effectively illustrate the proposed approach, aiding comprehension.
5. Detailed Methodology: The approach is described in depth, offering insights into the techniques used for annotation generation, self-training, and linking.
Weaknesses
1. Overly Detailed Explanations: Some sections, such as the description of self-training, are overly detailed and repetitive. This could be streamlined in the final version to improve readability.
2. Lack of Related Work References: While the paper mentions related work, certain comparisons (e.g., to Sil and Florian's supervised approach) could benefit from more explicit references and discussion.
3. Missing Dataset Documentation: The absence of a README file for the dataset is a notable gap. Although the authors have committed to adding it, this is essential for usability.
4. Unclear Use of DBpedia Properties: The paper does not clarify whether all DBpedia properties or a subset were used as features. This should be explicitly stated for replicability.
5. Domain-Specific Performance: The impact of domain-dependent features on performance is not quantified, which limits the understanding of the framework's adaptability to specific applications.
Questions to Authors
1. Could you clarify whether all DBpedia properties or only a subset were used as features? If a subset, which properties were selected and why?
2. In Section 2.3, could you explicitly state that word embeddings are used as input, similar to Lample et al. (2016)?
3. For Figure 3, are the knowledge networks (KNs) in the source language or English? Please clarify.
4. Could you provide quantitative results on the impact of domain-dependent features in the final version?
Recommendation
This paper makes significant contributions to multilingual NLP and cross-lingual entity linking, particularly in its scalability and resource release. However, the authors should address the weaknesses mentioned, including clarifying methodology, reducing redundancy, and providing missing documentation. With these improvements, the paper would be a strong candidate for acceptance.