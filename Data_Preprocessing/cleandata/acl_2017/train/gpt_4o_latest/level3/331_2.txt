Review of the Submission
Summary and Contributions
This paper introduces a crowdsourcing-based approach to create concept maps for multi-document summarization (MDS) and presents a new dataset for this task. The key contributions of the paper are:  
1. A novel task formulation, concept-map-based MDS, which extends traditional summarization by producing structured outputs in the form of concept maps.  
2. A crowdsourcing scheme for annotating the importance of propositions, which combines Likert-scale tasks and pairwise comparisons.  
3. A publicly available dataset of 30 topics with large, heterogeneous document clusters and corresponding gold-standard concept maps.  
4. An evaluation protocol and baseline system for concept-map-based MDS, facilitating future research in this area.  
While the dataset and task formulation are significant contributions, the novelty of the crowdsourcing scheme and the claim of creating a new corpus are less convincing upon closer examination.
Strengths
1. Dataset Contribution: The dataset is a valuable resource for the NLP community, addressing the lack of benchmarks for concept-map-based summarization. Its large, heterogeneous document clusters make it a challenging and realistic testbed for summarization systems.  
2. Effort and Execution: The authors demonstrate significant effort in combining automatic preprocessing, crowdsourcing, and expert annotations to create high-quality concept maps. The detailed methodology ensures reproducibility and reliability.  
3. Baseline and Evaluation Protocol: The inclusion of a baseline system and evaluation metrics (e.g., strict matching, METEOR, ROUGE-2) adds practical value to the dataset and provides a starting point for future research.
Weaknesses
1. Overstated Novelty of Crowdsourcing Scheme: The proposed "low-context importance annotation" largely applies existing best practices in crowdsourcing rather than introducing fundamentally new methods. The authors claim novelty but fail to provide a clear comparison with prior work to substantiate this.  
2. Reliance on Existing Dataset: The claim of creating a "new corpus" is weakened by its dependence on the DIP dataset (Section 4.1). While the authors add value through preprocessing and annotation, the reliance on pre-existing data diminishes the originality of this contribution.  
3. Presentation Issues: The paper's presentation could be improved for clarity. For example, the relationship between the proposed clusters and the TAC2008b dataset (mentioned in Section 3.2) is unclear. Additionally, the methodology for annotator training and expertise is insufficiently detailed, leaving questions about the quality of the gold-standard concept maps.  
4. Subjectivity in Crowdsourcing Tasks: The subjectivity of the Likert-scale and pairwise comparison tasks raises concerns about the reliability of the importance annotations. While the authors report agreement metrics, the inherent subjectivity of the task may limit the generalizability of the dataset.  
Questions to Authors
1. Can you provide more details on how the clusters in your dataset relate to the TAC2008b dataset? Are there overlaps or dependencies?  
2. How were the annotators trained, and what expertise did they have in creating the gold-standard concept maps?  
3. Could you clarify how the "low-context importance annotation" scheme differs from prior crowdsourcing approaches, such as those by Lloret et al. (2013)?  
Recommendation
While the dataset and task formulation are valuable contributions, the overstated claims of novelty and reliance on existing datasets weaken the overall impact of the work. The paper is a strong candidate for acceptance if the authors address the presentation issues and provide clearer justification for their claims during the author response period.