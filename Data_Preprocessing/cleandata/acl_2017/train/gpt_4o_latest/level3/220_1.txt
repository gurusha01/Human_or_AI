Review
Summary and Contributions
This paper addresses the challenging task of Metonymy Resolution (MR) in NLP by introducing a minimalist neural approach called Predicate Window (PreWin), which achieves state-of-the-art (SOTA) results on the SemEval 2007 MR task. The authors also contribute a new dataset, ReLocaR, which improves upon existing datasets by addressing annotation inconsistencies and class imbalances. The paper's main contributions are as follows:
1. Predicate Window (PreWin): A novel feature extraction method that focuses on a small, linguistically informed context window around the target entity, leveraging dependency parsing to improve classification accuracy while reducing irrelevant input.
2. ReLocaR Dataset: A carefully annotated Wikipedia-based dataset for MR, designed to address limitations in the SemEval 2007 dataset, including annotation disagreements and class bias.
3. Minimalist Neural Approach: The paper demonstrates that a small neural network with minimal external resources can outperform prior methods that rely on extensive feature engineering and external tools.
Strengths
1. Innovative Methodology: The introduction of PreWin is a creative and effective approach to MR, achieving SOTA results while significantly reducing reliance on external resources and handcrafted features. The method's focus on dependency-based context selection is well-motivated and supported by experimental results.
2. Valuable Dataset Contribution: The ReLocaR dataset is a significant resource for the MR community, addressing key limitations in existing datasets. The detailed annotation guidelines and inter-annotator agreement metrics further enhance its credibility and utility.
3. Insightful Qualitative Analysis: The qualitative discussion in Section 5 provides meaningful insights into the strengths and limitations of PreWin, highlighting its ability to balance precision and recall while identifying areas for improvement.
4. Replicability: The authors provide annotated subsets of the CoNLL 2003 dataset, along with models and tools, ensuring that their work can be reproduced and extended by other researchers.
Weaknesses
1. Unclear Explanation in Section 4.1: The description of PreWin's input selection process contains an unclear sentence that requires additional clarification. Specifically, the process of skipping conjunct relationships and punctuation could benefit from more detailed examples.
2. Ambiguity in Figure 2: The description of the input layer in Figure 2 is either incorrect or ambiguous, which could confuse readers attempting to understand the model architecture.
3. Justification for LSTMs: The use of LSTMs in Section 4.2 is not well-justified, as the problem does not inherently involve sequential data. A clearer rationale or comparison with non-sequential models is needed to validate this choice.
Questions to Authors
1. Could you clarify the sentence in Section 4.1 regarding the skipping of conjunct relationships and punctuation? A concrete example would help.
2. The description of the input layer in Figure 2 appears ambiguous. Could you provide a more detailed explanation or correct any inconsistencies?
3. Why were LSTMs chosen for this task, given that MR does not appear to be inherently sequential? Did you experiment with non-sequential architectures, and how did they compare?
Recommendation
This paper presents a strong contribution to the field of MR, with innovative methods, valuable dataset contributions, and insightful analysis. While there are minor issues with clarity and justification, these can be addressed during the author response period. I recommend acceptance with minor revisions.