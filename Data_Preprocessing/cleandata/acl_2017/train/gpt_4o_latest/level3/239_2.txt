Review
Summary and Contributions
This paper introduces "data efficiency" as a novel metric for evaluating word embeddings, focusing on their performance in supervised tasks under varying dataset sizes. The authors separate and address three critical questions: (1) the utility of supervised versions of Word Similarity (WS) and Word Analogy (WA) benchmarks, (2) the stability of embedding rankings under changing dataset sizes, and (3) the benefit of nonlinear models for certain embeddings. The paper presents a comprehensive experimental evaluation across multiple datasets and models, offering insights into embedding quality and their transfer learning capabilities. The authors also critique traditional intrinsic evaluation methods and propose a shift towards supervised, data-efficient evaluation frameworks. 
The primary contributions of this work are:
1. Introduction of Data Efficiency as a Metric: The paper highlights the importance of evaluating embeddings based on their ability to support fast learning under limited data, aligning with transfer learning goals.
2. Comprehensive Experimental Analysis: The authors provide a detailed evaluation of various embeddings, tasks, and models, revealing nuanced insights into embedding performance, particularly under supervised settings.
3. Critique of Traditional Evaluation Methods: The paper challenges the utility of cosine-based intrinsic evaluations, advocating for supervised tasks that better reflect real-world applications.
Strengths
1. Novel Metric: The introduction of "data efficiency" is a significant contribution, as it aligns embedding evaluation with practical transfer learning scenarios. This metric provides a more granular understanding of embedding utility, particularly in low-data regimes.
2. Thorough Experimental Design: The authors conduct experiments across a diverse set of tasks, datasets, and models, offering valuable insights into embedding performance. The inclusion of nonlinear models and supervised benchmarks adds depth to the analysis.
3. Community Value: The findings, such as the task-dependent nature of embedding rankings and the utility of supervised WS and WA tasks, are valuable for both researchers and practitioners. The availability of results and scripts online enhances reproducibility and community engagement.
Weaknesses
1. Limited Practical Utility: While the analysis enhances understanding, the results primarily confirm known or suspected insights (e.g., embeddings are task-dependent, nonlinear models can extract more information). This limits the actionable value for machine learning practitioners.
2. Lack of Real-World Applications: The proposed evaluation framework, while theoretically sound, lacks direct applicability to real-world NLP tasks. The paper does not provide concrete examples of how practitioners can leverage these insights in practical settings.
3. Presentation Issues: The paper contains several presentation flaws, including typos (e.g., L250, L579), missing citations (e.g., L425), and an absent appendix (L164). Figures and tables are not well-integrated with the text (e.g., Figure 3 is too small), and Equation 1 could be replaced with a textual explanation for clarity.
Questions to Authors
1. Can you provide specific examples of how practitioners can use the proposed data efficiency metric to improve real-world NLP systems?
2. How does the proposed framework compare to existing evaluation methods in terms of computational cost and scalability?
3. Could you clarify the strong claim made in L591-593 regarding the limitations of unsupervised pretraining? Are there specific scenarios where this claim does not hold?
Additional Comments
- The observation in L642-645 about embedding specialization is important but requires additional evidence or visualizations to support the claim.
- Examples of "specialized word embeddings" (L672) should be provided, along with distinctions from general-purpose embeddings.
- The appendix must be included, as it is referenced multiple times in the text.
- The paper would benefit from a clearer explanation of how the proposed supervised tasks align with real-world applications.
Recommendation
While the paper introduces a novel and valuable metric and provides thorough experimental analysis, its limited practical utility and presentation issues detract from its impact. I recommend acceptance with major revisions, contingent on addressing the weaknesses and improving the clarity and applicability of the findings.