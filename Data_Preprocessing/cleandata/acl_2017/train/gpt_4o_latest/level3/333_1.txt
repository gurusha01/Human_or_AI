Review
Summary and Contributions
This paper introduces a Selective Encoding Model (SEASS) as an extension to the sequence-to-sequence framework for abstractive sentence summarization. The model incorporates a selective gate network that explicitly filters and tailors sentence representations before decoding, addressing the challenge of selecting highlights while filtering out secondary information. The authors evaluate their approach on three standard datasets: English Gigaword, DUC 2004, and MSR-ATC, demonstrating superior performance over state-of-the-art baselines. The primary contributions of the paper are:
1. A novel selective encoding mechanism that explicitly models the selection process in abstractive summarization, improving the effectiveness of sentence representation.
2. Extensive evaluation on multiple datasets, showing consistent improvements in ROUGE scores compared to existing methods.
3. A detailed analysis of the selective gate mechanism through visualizations, providing insights into its contribution to summarization quality.
Strengths
1. Novelty of the Selective Encoding Mechanism: The proposed selective gate network is a meaningful extension to the sequence-to-sequence framework, explicitly addressing the unique challenges of abstractive summarization. This innovation is well-motivated and clearly explained.
2. Comprehensive Evaluation: The paper evaluates the model on three diverse datasets and compares it against strong baselines, including ABS, CAs2s, and s2s+att. The results demonstrate significant improvements, particularly in ROUGE-2 scores, which are critical for summarization tasks.
3. Clarity and Organization: The paper is well-written, with clear descriptions of the methodology, experimental setup, and results. The inclusion of visualizations (e.g., heat maps) adds depth to the analysis and helps illustrate the effectiveness of the selective gate mechanism.
Weaknesses
1. Limited Examples of Abstractive Outputs: While the paper claims to produce abstractive summaries, it lacks sufficient examples and statistics on novel words in the output to substantiate this claim. Providing more examples and a quantitative analysis of novel word usage would strengthen the argument.
2. Overlapping Representations: The necessity of both the sentence word vector (hi) and the sentence representation vector (s) is unclear, as they appear to overlap in functionality. A more detailed justification or ablation study would help clarify their roles.
3. Incomplete Implementation Details: The paper does not specify the neural network library used for implementation or the training data used for comparison systems. It is also unclear whether the authors trained the baseline models themselves.
4. Undefined Elements in Formulas: Several notations in the formulas (e.g., b, W, U, V) are not defined, and the depiction of the readout state rt in Figure 2 is missing. This lack of clarity may hinder reproducibility.
5. Minor Organizational Issues: The distinction between abstractive and extractive summarization should be moved to the introduction for better context. Additionally, the related work section should precede the methods section to provide a stronger foundation for the proposed approach.
Questions to Authors
1. Can you provide examples and statistics on the proportion of novel words in the generated summaries to better support the claim of abstractive summarization?
2. Why are both hi and s necessary in the selective gate network? Have you conducted ablation studies to evaluate their individual contributions?
3. Did you train the comparison systems (e.g., ABS, CAs2s) yourself, or did you use pre-trained models? If trained, what training data was used?
4. What neural network library or framework was used for implementation?
Additional Comments
- The paper contains several minor typographical errors (e.g., "SGD as our the optimizing algorithms" and "supper script" instead of "superscript"). These should be corrected for clarity.
- "MLP" in Figure 2 is not defined in the text, and the sigmoid function and element-wise multiplication in Section 3.1 formulas are not explained.
- References for beam search and sequence-to-sequence model descriptions are missing and should be included.
- Consistency in naming (e.g., Figure 1 vs. Table 1) should be ensured, and redundant examples for abstractive summarization should be removed.
Recommendation
While the paper presents a novel and effective approach to abstractive sentence summarization, the aforementioned weaknesses, particularly the lack of clarity in implementation details and insufficient evidence for abstractive outputs, need to be addressed. I recommend acceptance with minor revisions, provided the authors address these concerns during the rebuttal phase.