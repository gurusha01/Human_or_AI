Review of the Paper
Summary and Contributions
This paper introduces the Gated-Attention (GA) Reader, a novel model for answering cloze-style questions over documents. The key contribution of the work is the gated-attention mechanism, which applies multiplicative interactions between query embeddings and intermediate document representations, enabling fine-grained attention at the semantic level. The model is integrated into a multi-hop architecture, allowing iterative refinement of token representations. The GA Reader achieves state-of-the-art results on several benchmark datasets, including CNN, Daily Mail, and Who Did What (WDW). The authors also provide an ablation study to demonstrate the effectiveness of the gated-attention mechanism and compare it to alternative compositional operators.
The primary contributions of this paper are:
1. Gated-Attention Mechanism: A novel attention mechanism that enables query-specific filtering of token representations, which is shown to outperform addition and concatenation operations.
2. Empirical Results: The GA Reader achieves state-of-the-art performance on multiple datasets, demonstrating its effectiveness across different data sizes and tasks.
3. Ablation Study and Analysis: The authors provide detailed ablation studies and attention visualizations, offering insights into the model's behavior and the importance of its components.
Strengths
1. Innovative Attention Mechanism: The gated-attention mechanism is a promising contribution to the field, offering a fine-grained approach to query-document interaction that is both novel and empirically effective.
2. Strong Empirical Results: The model achieves state-of-the-art performance on multiple datasets, including challenging benchmarks like WDW, with significant improvements over prior work.
3. Comprehensive Analysis: The ablation study and attention visualizations provide valuable insights into the model's functionality and the impact of its components, enhancing the paper's scientific rigor.
4. Scalability: The model demonstrates robustness across datasets of varying sizes, suggesting its applicability to diverse real-world scenarios.
Weaknesses
1. Lack of Rigorous Comparison with State-of-the-Art: While the GA Reader achieves strong results, the reliance on engineering tricks (e.g., qe-comm feature) raises concerns about whether the improvements are due to the model itself or external enhancements. A clearer comparison with vanilla GA or other baselines is needed.
2. Incomplete Bibliography: The heavy reliance on arXiv preprints and the omission of key prior works weaken the paper's positioning within the broader literature.
3. Inconsistencies in Tables: Tables 1 and 2 contain naming and implementation conflicts regarding GA-- and its relation to AS Reader, causing confusion about the baseline comparisons.
4. Limited Qualitative Analysis: While Figure 2 demonstrates the benefits of gated attention, the paper would benefit from more qualitative examples comparing GA to other models to substantiate its claims.
5. Insufficient Related Work Discussion: The related work section does not explicitly highlight the differences between the proposed method and prior approaches, making it harder to assess the novelty and significance of the contributions.
Questions to Authors
1. Can you clarify the differences between GA-- and the proposed GA Reader? Why was GA-- included instead of a vanilla GA baseline?
2. How does the model perform without the qe-comm feature? Can you provide results for a purely model-driven approach without feature engineering?
3. Can you address the inconsistencies in Tables 1 and 2 regarding the naming and implementation of GA--?
Recommendation
While the gated-attention mechanism is a promising contribution, the paper requires revisions to address the lack of rigorous comparisons, incomplete bibliography, and inconsistencies in the results presentation. I recommend acceptance with major revisions, contingent on the authors addressing these concerns.