Review
Summary and Contributions:  
This paper introduces ADEM, a novel evaluation metric for non-task-oriented dialogue systems. ADEM leverages RNN-based continuous vector space representations to predict human-like scores for dialogue responses. The model compares context-response and reference-response pairs using dot products after projecting responses into learned context and reference spaces. The primary contributions of this work are:  
1. A novel evaluation framework that moves beyond traditional word-overlap metrics like BLEU by incorporating semantic similarity and contextual relevance.  
2. The use of learned projection matrices to transform response representations, enabling a more nuanced evaluation of dialogue quality.  
3. Demonstration of ADEM's ability to generalize to unseen dialogue models, a critical step for practical deployment.  
Strengths:  
1. Novel Metric Design: The introduction of learned projection matrices to compare responses in transformed spaces is a significant advancement over existing metrics. This approach captures semantic and contextual nuances that word-overlap metrics fail to address.  
2. Correlation with Human Judgments: ADEM demonstrates strong correlations with human evaluations at both the utterance and system levels, outperforming traditional metrics like BLEU and ROUGE. This highlights its practical utility for dialogue evaluation.  
3. Generalization Capability: The leave-one-out experiments show that ADEM can effectively evaluate responses from unseen models, making it a robust tool for diverse dialogue systems.  
4. Pragmatic Focus: The paper acknowledges the limitations of current evaluation methods and positions ADEM as a step toward more meaningful chatbot evaluation, aligning with real-world needs.  
Weaknesses:  
1. Implementation Details: The paper lacks clarity on critical aspects, such as the source and aggregation of human scores and the dataset splitting methodology. This omission makes it challenging to reproduce the results.  
2. Presentation of Results: The correlation results in Table 2 are inconsistently presented across metrics, which could confuse readers. A uniform presentation format is necessary for better interpretability.  
3. Pre-training Section: The explanation of VHRED pre-training is overly technical and lacks a clear high-level summary of its purpose and benefits. This could alienate readers unfamiliar with the method.  
4. Title Misalignment: The title, "Towards an Automatic Turing Test," is misleading. The Turing Test evaluates human-like intelligence, whereas ADEM focuses on response quality. A more accurate title would better reflect the paper's scope.  
Questions to Authors:  
1. Could you provide more details on how human scores were collected and aggregated? Were there any quality control measures for annotators?  
2. How do the learned projection matrices evolve during training? A discussion on this could provide deeper insights into the model's interpretability.  
3. Could you elaborate on the rationale behind using PCA to reduce the embedding size to 50? Was this empirically determined?  
Additional Comments:  
- The paper would benefit from referencing related work from the WOCHAT workshop series to provide broader context.  
- There is a minor grammatical issue: "and has has been used" should be corrected to "and it has been used."  
Conclusion:  
This paper presents a significant contribution to the field of dialogue evaluation by introducing a novel metric that aligns well with human judgments and addresses the shortcomings of traditional word-overlap metrics. However, the lack of clarity in implementation details and some inconsistencies in presentation detract from its overall impact. With revisions to address these weaknesses, this work has the potential to be a valuable resource for the dialogue system community.