Review of the Paper
Summary and Contributions
This paper introduces a novel deep character-level neural machine translation (DCNMT) architecture that leverages sub-word representations to address challenges like large vocabulary size and out-of-vocabulary (OOV) issues in neural machine translation (NMT). The proposed model incorporates a hierarchical structure with deep word-encoding and decoding layers, enabling character-level translation while maintaining efficiency. The authors claim that their model achieves competitive BLEU scores compared to state-of-the-art methods and demonstrates the ability to learn morphology, which is further analyzed qualitatively. The main contributions of this work are:
1. A novel word encoder that learns morphemes and their compositional rules, enabling efficient sub-word representation learning.
2. A hierarchical decoder that decodes at the character level, addressing the OOV issue and improving translation quality.
3. Experimental results that show competitive BLEU scores and qualitative analyses suggesting the model's ability to learn morphology.
Strengths
1. Novel Architecture: The hierarchical design of the proposed DCNMT model, combining character-level and sub-word-level representations, is innovative and addresses key challenges in NMT, such as vocabulary size and OOV issues.
2. Efficiency: Despite its complexity (six RNNs), the model is computationally efficient, achieving competitive results with fewer parameters and faster training compared to other character-level models.
3. Qualitative Insights: The paper provides interesting qualitative analyses, such as PCA visualizations and examples of morphology learning, which add depth to the discussion.
4. Clarity and Structure: The paper is well-written and logically structured, making it easy to follow the methodology and results.
Weaknesses
1. Insufficient Evidence for Morphology Learning: The claims about the model's ability to learn morphology are not rigorously supported. Section 5.2 relies on qualitative examples and PCA visualizations, but lacks formal evaluations or quantitative metrics to substantiate these claims.
2. Minimal BLEU Score Improvement: The BLEU score improvements over the bpe2char baseline are marginal and lack statistical significance evaluation, raising concerns about the practical impact of the proposed approach.
3. Incomplete Comparisons: The paper does not provide a comprehensive comparison of training times across datasets, and the advantages of DCNMT over bpe2char are not convincingly demonstrated.
4. Ambiguities and Missing Details: Acronyms like LSTM, HGRU, CNN, and PCA are not defined early, and some concepts (e.g., "energy" and "peaks" in Section 5.2) are poorly explained. Additionally, the absence of links for WMT'14 training corpora and incomplete references detracts from the paper's rigor.
5. Typographical and Formatting Issues: The paper contains several grammatical and typographical errors, inconsistent capitalization of "BPE," and unclear figures (e.g., missing energy value colors in Figure 4a).
Questions to Authors
1. Can you provide quantitative metrics or formal evaluations to support the claims about morphology learning in Section 5.2?
2. How does the proposed model handle longer training times compared to bpe2char, and can you provide a more detailed comparison of training efficiency across datasets?
3. Why is the BLEU score improvement over bpe2char minimal, and is it statistically significant?
4. Could you clarify the necessity of mentioning the absence of a monolingual corpus in Section 4.1?
Recommendation
While the paper presents a novel and well-structured approach to character-level NMT, the lack of rigorous evidence for key claims (e.g., morphology learning) and the marginal BLEU score improvements weaken its overall impact. I recommend acceptance with major revisions, contingent on addressing the above weaknesses and providing stronger experimental support.