Review of the Submission
Summary and Contributions
This paper addresses the challenge of building Named Entity Recognition (NER) systems for new languages without human-annotated data. It proposes two weakly supervised cross-lingual NER approaches: (1) annotation projection, which uses alignment information between English and target languages to generate weakly labeled data, and (2) representation projection, which maps word embeddings from target languages into English space for direct model transfer. Additionally, the paper introduces two co-decoding schemes to combine outputs from the two systems. The authors evaluate their methods on in-house and CoNLL datasets, demonstrating improvements over prior cross-lingual NER approaches.
The main contributions of the paper, as I see them, are:
1. Data Selection Scheme: A heuristic method for selecting high-quality projection-labeled data, which significantly improves the annotation projection approach, especially for languages with low alignment accuracy.
2. Co-Decoding Schemes: Two strategies for combining outputs from the annotation and representation projection systems, which yield higher accuracy than individual systems.
3. Evaluation on Multiple Datasets: Comprehensive experiments on in-house and CoNLL datasets, showing competitive performance compared to state-of-the-art cross-lingual NER methods.
Strengths
1. Data Selection's Impact: The proposed data selection scheme is a notable strength, as it addresses the noise in projection-labeled data and demonstrates significant performance gains (e.g., +13.9 F1 for Japanese, +20.0 F1 for Korean). This contribution is well-motivated and empirically validated.
2. Co-Decoding Effectiveness: The co-decoding schemes are an interesting addition, leveraging the complementary strengths of the two systems. The rank-based scheme, in particular, shows consistent improvements across languages.
3. Comprehensive Evaluation: The paper evaluates its methods on diverse datasets (in-house and CoNLL) and multiple languages, providing a robust assessment of the proposed approaches. The results demonstrate competitive performance compared to prior work.
Weaknesses
1. Lack of Novelty: While the methods are effective, they lack significant novelty for an ACL-level paper. Annotation projection and representation projection are well-established techniques, and the co-decoding schemes are relatively straightforward extensions.
2. Unclear Threshold Calculation: The data selection scheme is a key contribution, but the process for determining the quality and entity number thresholds (q, n) is not clearly explained. This lack of detail makes it difficult to reproduce or generalize the approach.
3. Parameter Tuning Ambiguity: The paper does not provide sufficient guidance on how to tune parameters (e.g., thresholds, embedding mappings) for new datasets or languages, which limits the practical applicability of the methods.
4. Limited Exploration of System Combinations: The paper proposes combining outputs from two systems but does not explore combining more than two weakly supervised systems, which could further enhance performance.
5. Dataset Details Missing: The "in-house" datasets are not described in sufficient detail, particularly regarding the types of texts and domains. This omission reduces the clarity and generalizability of the results.
Questions to Authors
1. How are the quality and entity number thresholds (q, n) determined for the data selection scheme? Are they tuned on a validation set, and if so, how does this align with the weakly supervised setting?
2. Why were only two systems combined in the co-decoding schemes? Could combining additional weakly supervised systems further improve performance?
3. Can you provide more details about the "in-house" datasets, such as the domains and text types? How do these datasets compare to the CoNLL datasets in terms of language and content diversity?
Recommendation
While the paper demonstrates strong empirical results and makes incremental contributions, its lack of methodological novelty and unclear details in key areas (e.g., threshold calculation, dataset description) limit its impact. I recommend acceptance with major revisions if the authors can address the clarity issues and provide more insights into the novelty and generalizability of their approach.