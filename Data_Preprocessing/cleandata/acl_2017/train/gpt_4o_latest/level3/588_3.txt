Review
Summary and Contributions
This paper introduces the novel task of Rare Entity Prediction, where models predict missing entities in web documents using both document context and external knowledge from lexical resources. The authors propose a new dataset, the Wikilinks Rare Entity Prediction dataset, derived from the Wikilinks dataset, and demonstrate the utility of external knowledge in improving performance on this challenging task. Two novel model architectures, the Double Encoder (DOUBENC) and the Hierarchical Double Encoder (HIERENC), are proposed, leveraging lexical definitions from Freebase to enhance predictions. The authors show that their HIERENC model achieves significant performance gains over baselines, demonstrating the importance of external knowledge in solving tasks involving rare entities.
The primary contributions of this paper are:
1. The introduction of the Rare Entity Prediction task, which emphasizes the integration of external knowledge for reasoning about rare entities.
2. The creation of the Wikilinks Rare Entity Prediction dataset, which is specifically designed to evaluate models on this task.
3. The development of two novel model architectures (DOUBENC and HIERENC) that effectively utilize external knowledge, achieving substantial improvements over baseline methods.
Strengths
1. Empirical Demonstration of External Knowledge Utility: The paper convincingly demonstrates the benefits of incorporating external knowledge in NLP tasks, particularly for rare entity prediction. The significant performance gains of the proposed models over baselines highlight the importance of external knowledge.
2. Novel Dataset: The Wikilinks Rare Entity Prediction dataset is a valuable contribution to the NLP community, as it provides a challenging benchmark for evaluating models on tasks involving rare entities.
3. Model Design: The hierarchical double encoder (HIERENC) is a well-motivated and effective architecture that captures both local and global context, as well as external knowledge, leading to state-of-the-art performance on the proposed task.
4. Analysis and Insights: The paper provides useful insights into the challenges of rare entity prediction, such as the limitations of co-occurrence-based models and the importance of lexical definitions.
Weaknesses
1. Task Motivation: The motivation for the Rare Entity Prediction task is not well-justified. While the authors argue that it is distinct from existing tasks, its practical relevance to real-world NLP applications is unclear. Stronger connections to real-world use cases, such as question answering or dialogue systems, would enhance the paper's impact.
2. Task Realism: The task setup, where candidate entities are limited to those present in the document, is somewhat artificial. This simplification reduces the realism of the task compared to more practical scenarios, such as open-domain entity prediction.
3. Dataset Characterization: The dataset lacks detailed characterization, particularly regarding the cognitive processes involved in predictions. For example, it is unclear which contextual cues are most informative for the task or how humans might approach it.
4. Baseline Appropriateness: The baselines used for comparison are suboptimal. For instance, embeddings for rare entities are unreliable, and pre-trained embeddings (e.g., GloVe) might yield fairer comparisons. Additionally, alternative architectures like Pointer Networks could provide stronger baselines.
5. Training Imbalance: The imbalance in positive and negative labels during training is not adequately addressed. This could bias the models and affect their generalization, raising concerns about the robustness of the results.
Questions to Authors
1. Can you provide more justification for the practical relevance of the Rare Entity Prediction task? Are there specific real-world applications where this task would be directly beneficial?
2. How does the task setup (limiting candidate entities to those present in the document) affect the generalizability of the models to more realistic scenarios?
3. Did you explore alternative baselines, such as Pointer Networks or other architectures designed for sequence-to-sequence tasks? If not, why were these omitted?
Additional Comments
The paper makes a valuable contribution by highlighting the importance of external knowledge in NLP tasks. However, addressing the concerns about task motivation, realism, and baseline selection would significantly strengthen the work.