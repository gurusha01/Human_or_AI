Review of the Paper
Summary and Contributions
This paper presents a novel framework for creating data-to-text corpora aimed at supporting the training of wide-coverage microplanners. The framework leverages DBpedia, a large-scale knowledge base, to semi-automatically generate data units, which are then paired with human-authored texts through a crowdsourcing process. The authors compare their dataset (DBPNLG) with the widely used RNNLG dataset (Wen et al., 2016), demonstrating that DBPNLG is more diverse in terms of attributes, input patterns, and syntactic complexity. The paper also highlights the challenges posed by DBPNLG for neural sequence-to-sequence models, suggesting its utility as a benchmark for advancing NLG research. The main contributions of the paper are:
1. A novel framework for creating linguistically diverse and semantically rich data-to-text corpora from knowledge bases.
2. A detailed comparison of the DBPNLG dataset with the RNNLG dataset, showcasing the former's advantages in diversity and complexity.
3. Empirical evidence of the challenges posed by DBPNLG for existing neural generation models, emphasizing its potential as a benchmark for future research.
Strengths
1. Novel Dataset Creation Framework: The proposed framework addresses limitations of existing datasets by generating semantically diverse and linguistically complex data-to-text corpora. This is a valuable contribution to the NLG field, particularly for training microplanners.
2. Comprehensive Dataset Analysis: The authors provide a thorough comparison of DBPNLG and RNNLG, using metrics such as input patterns, input shapes, lexical diversity, and syntactic complexity. This analysis highlights the strengths of DBPNLG and its potential to drive advancements in NLG.
3. Challenging Benchmark for Neural Models: The paper demonstrates that DBPNLG is more challenging for sequence-to-sequence models, motivating the need for more sophisticated neural architectures capable of handling complex microplanning tasks.
Weaknesses
1. Limited Awareness of Related Work: The paper overemphasizes Wen et al. (2016) while neglecting broader comparisons to other well-known corpora and NLG/DBpedia-related projects. This weakens the contextualization of the proposed framework within the broader NLG landscape.
2. Feasibility of Domain-Independent Microplanning: The authors claim to support wide-coverage microplanning but fail to address the theoretical challenges of training domain-independent microplanners, given the inherent domain and genre dependencies of microplanning tasks.
3. Quality of Crowdsourced Texts: While the authors describe their crowdsourcing process in detail, they do not adequately justify the use of crowdsourced texts for training microplanners. Concerns about the fluency, grammaticality, and semantic adequacy of such texts remain unaddressed.
4. Presentation Issues: Figures 1 and 2 contain text that is too small to read, violating ACL font size guidelines. This detracts from the clarity and accessibility of the paper.
Questions to Authors
1. How does your framework compare to other datasets beyond RNNLG, such as those used in the KBGen shared task or AMR-based corpora?
2. Can you provide theoretical justifications or empirical evidence supporting the feasibility of training domain-independent microplanners using DBPNLG?
3. What measures were taken to ensure the linguistic quality of crowdsourced texts, and how do you address potential biases introduced by crowdworkers?
Overall Assessment
The paper presents an interesting and potentially valuable contribution to the NLG field by proposing a novel dataset creation framework and introducing the DBPNLG dataset. However, significant high-level concerns remain, particularly regarding the limited awareness of related work, the feasibility of domain-independent microplanning, and the quality of crowdsourced texts. The paper is rated as borderline, pending author response to address these issues.