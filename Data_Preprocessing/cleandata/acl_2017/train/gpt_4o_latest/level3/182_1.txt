Review of the Paper
Summary and Contributions
This paper addresses multimodal sentiment analysis by proposing an LSTM-based framework that incorporates contextual information from surrounding utterances in videos. The authors argue that most existing methods treat utterances as independent entities, ignoring their temporal dependencies. The proposed approach models these dependencies using LSTMs, achieving a 5â€“10% improvement over state-of-the-art methods on benchmark datasets. The hierarchical fusion framework for multimodal data is another key contribution, which combines unimodal contextual features into a unified model. The paper also emphasizes speaker-independent evaluation, a notable step toward real-world applicability. The experimental results demonstrate the effectiveness of the proposed method across multiple datasets, including MOSI, MOUD, and IEMOCAP.
Strengths
1. Comprehensive Experiments: The paper is well-written and supported by extensive experiments on multiple datasets, demonstrating consistent improvements over baselines and state-of-the-art methods. The hierarchical fusion framework is particularly effective in leveraging multimodal data.
2. Speaker-Independent Evaluation: The focus on speaker-independent splits enhances the robustness and generalizability of the proposed model, addressing a critical gap in multimodal sentiment analysis research.
3. Clear Methodology: The paper provides a detailed explanation of the proposed architecture, including unimodal feature extraction, LSTM-based contextual modeling, and fusion strategies. This clarity aids reproducibility.
Weaknesses
1. Lack of Novelty: The paper primarily applies existing technologies (e.g., LSTMs, CNNs, and 3D-CNNs) without introducing groundbreaking methodological innovations. The hierarchical fusion framework, while effective, is not conceptually novel.
2. Predictable Results: The improvements over baselines are expected, given the inclusion of contextual information and hierarchical fusion. The paper does not explore alternative approaches, such as attention mechanisms, which could provide deeper insights.
3. Code Availability: The absence of publicly available code limits the reproducibility of the results and the potential for further research based on this work.
Questions to Authors
1. Why were established audio features like MFCCs not considered, given their widespread use in audio-based sentiment analysis?
2. What motivated the choice of 3D-CNN for visual feature extraction? Were alternative methods (e.g., pre-trained models like ResNet or ViT) considered?
3. How does the model handle cases with a large number of unknown persons in the test set? Does the performance degrade significantly in such scenarios?
Minor Comments
- Correct "a LSTM" to "an LSTM" (L155) and "a SVM" to "an SVM" (L420).
- Address unnecessary hyphens after text (L160, L216).
- Clarify the explanation of convolution (L205) and adjust the positioning of "^" above "y" (L375, 378).
- Move Table 1 earlier to page 2 for better readability.
- Replace "doesnt" with "does not" (L651) and remove the comma after "Since" (L626).
- Improve subject-verb agreement in "Output ... are" (L448) and avoid splitting "concatenation" into a new line (L516, 519).
- Clean up references: remove whitespace (L823) and fix formatting (L831, L860, L888, L894, L951, L956).
Recommendation
While the paper demonstrates strong experimental results and is well-written, the lack of novelty and the absence of publicly available code are significant drawbacks. I recommend acceptance with minor revisions, provided the authors address the reproducibility concerns and clarify the rationale behind some design choices.