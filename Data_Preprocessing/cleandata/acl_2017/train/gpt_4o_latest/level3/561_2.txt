Review of the Paper
Summary and Contributions  
This paper proposes a semi-supervised method that combines pre-trained word embeddings with context-sensitive embeddings derived from bidirectional neural language models (LMs) to enhance sequence tagging tasks. The method, referred to as TagLM, achieves state-of-the-art results on two standard benchmarks: CoNLL 2003 Named Entity Recognition (NER) and CoNLL 2000 Chunking. The primary contributions of the paper are as follows:  
1. The integration of bidirectional LM embeddings into a supervised sequence tagging model, which leads to significant performance improvements on NER and Chunking tasks.  
2. A demonstration that bidirectional LM embeddings are robust across domains, as evidenced by their application to an out-of-domain dataset (ScienceIE).  
3. An analysis showing that the use of both forward and backward LM embeddings consistently outperforms forward-only embeddings.  
Strengths  
1. Clarity and Documentation: The paper is well-written and easy to follow, with clear explanations of the methodology and thorough experimental analysis. The inclusion of ablation studies and detailed comparisons with prior work strengthens the paper's credibility.  
2. Strong Empirical Results: The proposed method achieves state-of-the-art results on both NER and Chunking tasks, with statistically significant improvements over previous methods. The analysis also highlights the scalability of the approach to small datasets, making it relevant for low-resource scenarios.  
3. Robustness Across Domains: The method's ability to generalize to out-of-domain tasks (e.g., ScienceIE) demonstrates its practical applicability and robustness.  
Weaknesses  
1. Limited Scope: The paper focuses exclusively on English NER and Chunking tasks, without exploring other sequence tagging tasks such as Part-of-Speech (POS) tagging, grammatical error detection, or supersense tagging. This limits the generalizability of the method to a broader range of tasks.  
2. Multilingual Evaluation: The paper does not evaluate the method on multilingual datasets, which is a significant limitation given the growing emphasis on multilingual NLP.  
3. Low-Resource Scenarios: While the paper mentions low-resource scenarios, the experiments do not explicitly address the scalability of the method to languages or tasks with very limited labeled data.  
4. Method Simplicity and Novelty: The method, while effective, is relatively straightforward and lacks significant novelty. The idea of leveraging pre-trained embeddings is well-established, and the contribution lies more in its application than in methodological innovation.  
5. Figures: Figures 1 and 2 overlap in content, and only one is necessary. This redundancy detracts slightly from the paper's presentation quality.  
Questions to Authors  
1. Have you considered evaluating the method on additional sequence tagging tasks such as POS tagging or supersense tagging? If so, what were the results?  
2. Can the proposed method be adapted to multilingual settings? If yes, what challenges do you foresee in extending it to languages other than English?  
3. How does the method perform when trained on datasets with significantly fewer labeled examples, especially for low-resource languages?  
Recommendation  
While the paper presents strong empirical results and is well-documented, its limited scope, lack of multilingual evaluation, and relatively straightforward methodology make its contribution borderline for a long paper. I lean slightly negative on acceptance for this format. However, the work could be a strong candidate for a short paper submission with additional experiments addressing broader tasks or multilingual settings.