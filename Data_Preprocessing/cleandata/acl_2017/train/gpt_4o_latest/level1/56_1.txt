Review of the Paper
Summary:  
The paper introduces ngrams into four widely-used word representation methodsâ€”SGNS, GloVe, PPMI, and its SVD factorization. The authors propose a novel approach to incorporate ngram co-occurrence statistics into these models and evaluate their effectiveness on word analogy and similarity tasks. The results indicate that ngram-based representations improve word embeddings, particularly in capturing semantic and syntactic patterns. Additionally, the authors propose an efficient method for constructing ngram co-occurrence matrices, mitigating the computational burden associated with ngram-based models. The implementation is made publicly available as the "ngram2vec" toolkit.
Main Contributions:  
1. Integration of Ngrams into Word Representation Models:  
   The paper's primary contribution is the systematic integration of ngram co-occurrence statistics into SGNS, GloVe, PPMI, and SVD. This extension demonstrates significant improvements in word analogy tasks, particularly for semantic questions, and provides a novel perspective on enhancing word embeddings with richer contextual information.
2. Efficient Co-occurrence Matrix Construction:  
   The authors propose a hybrid "mixture" and "stripes" strategy for constructing ngram co-occurrence matrices. This approach reduces memory and computational costs, enabling the use of ngram-based models on modest hardware, which is a practical and impactful contribution.
3. Qualitative Analysis of Ngram Embeddings:  
   The qualitative evaluation shows that the trained ngram embeddings capture both semantic meanings and syntactic patterns, such as negative forms and passive voice. This analysis highlights the utility of ngram embeddings in tasks like finding antonyms or understanding syntactic structures.
Strengths:  
1. Novelty and Practical Impact:  
   The integration of ngrams into established word representation models is a novel contribution, and the proposed matrix construction method addresses a significant computational bottleneck, making the approach accessible to researchers with limited resources.
2. Comprehensive Evaluation:  
   The paper evaluates the proposed methods on a variety of datasets for both word analogy and similarity tasks. The results consistently demonstrate the effectiveness of ngram-based representations, particularly in analogy tasks.
3. Open-Source Implementation:  
   The release of the "ngram2vec" toolkit ensures reproducibility and facilitates further research in this area, enhancing the paper's impact on the community.
4. Qualitative Insights:  
   The qualitative analysis provides valuable insights into the properties of ngram embeddings, showcasing their potential applications in NLP tasks beyond the scope of this paper.
Weaknesses:  
1. Limited Exploration of Hyperparameters:  
   The paper strictly adheres to default hyperparameter settings from baseline models, which may not be optimal for ngram-based methods. A more thorough exploration of hyperparameters could potentially yield better results, particularly for GloVe and SVD.
2. Sparse Improvements in Similarity Tasks:  
   While the ngram-based models show significant gains in analogy tasks, the improvements in similarity tasks are modest or inconsistent. This raises questions about the generalizability of the proposed approach across different evaluation metrics.
3. Scalability to Higher-Order Ngrams:  
   The paper focuses primarily on bi-grams, with limited discussion of higher-order ngrams. While the authors cite sparsity as a concern, further exploration of this limitation would strengthen the paper.
Questions to Authors:  
1. Have you considered optimizing hyperparameters specifically for ngram-based models? If so, what were the results?  
2. Can you provide more insights into the challenges of incorporating higher-order ngrams, and do you plan to address this in future work?  
3. How do you envision the practical applications of ngram embeddings in downstream NLP tasks, such as text classification or machine translation?
Overall Assessment:  
The paper presents a novel and practical extension to existing word representation methods by incorporating ngram statistics, supported by strong experimental results and qualitative analysis. While there are areas for improvement, particularly in hyperparameter tuning and generalizability, the contributions are significant and relevant to the NLP community. I recommend acceptance, contingent on addressing the outlined weaknesses.