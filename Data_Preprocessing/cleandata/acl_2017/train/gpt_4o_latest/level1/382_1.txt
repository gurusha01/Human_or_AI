Review of the Research Paper
Summary
This paper introduces a novel framework for creating data-to-text corpora aimed at training wide-coverage microplanners for Natural Language Generation (NLG). The authors critique existing NLG benchmarks and propose a semi-automated method for generating linguistically diverse datasets from knowledge bases, specifically DBpedia. The resulting dataset (DBPNLG) is compared to an existing dataset (RNNLG), demonstrating its superior diversity in terms of input patterns, linguistic complexity, and syntactic variety. The authors also evaluate the performance of a sequence-to-sequence model on both datasets, showing that DBPNLG presents a more challenging learning task.
Contributions
1. Framework for Dataset Creation: The primary contribution is the introduction of a semi-automated framework for generating data-to-text corpora from knowledge bases. This framework supports the creation of datasets that are semantically and linguistically diverse, addressing limitations of existing benchmarks that rely on artificial data or domain-specific corpora.
   
2. DBPNLG Dataset: The authors present a dataset derived from DBpedia, which is more diverse in terms of attributes, input patterns, and syntactic structures compared to the widely used RNNLG dataset. This dataset is particularly valuable for training KB verbalisers and microplanners capable of handling complex NLG subtasks.
3. Empirical Evaluation: The paper provides a thorough comparison of DBPNLG and RNNLG using various metrics, including input diversity, text complexity, and neural model performance. The results highlight the challenges posed by DBPNLG, making it a valuable resource for advancing NLG research.
Strengths
1. Novelty and Practicality: The proposed framework is innovative and addresses a critical gap in NLG research by enabling the creation of datasets from real-world knowledge bases. This is a significant step forward for training KB verbalisers and microplanners.
   
2. Comprehensive Evaluation: The authors provide a detailed comparison of DBPNLG and RNNLG, using both linguistic metrics and machine learning performance. This thorough evaluation strengthens the paper's claims about the dataset's diversity and complexity.
3. Relevance to the Community: By making the DBPNLG dataset publicly available and proposing its use in a shared task, the authors contribute a valuable resource that can drive future research in NLG, particularly in neural generation models.
4. Focus on Microplanning: The dataset is designed to support the learning of complex microplanning subtasks, such as lexicalisation, aggregation, and sentence segmentation, which are often overlooked in existing benchmarks.
Weaknesses
1. Limited Scope of Evaluation: While the paper evaluates the dataset using a sequence-to-sequence model, it does not explore other state-of-the-art NLG models or techniques. This limits the generalisability of the findings regarding the dataset's complexity.
2. Dataset Size: Although the DBPNLG dataset is shown to be more diverse, its smaller size compared to RNNLG may limit its immediate applicability for training large-scale neural models. The authors mention plans to expand the dataset, but this is not yet realized.
3. Crowdsourcing Quality Control: While the paper describes measures to ensure the quality of crowdsourced text, it does not provide detailed statistics on inter-annotator agreement or the effectiveness of the validation process. This could raise concerns about the dataset's reliability.
Questions to Authors
1. How do you plan to address the smaller size of DBPNLG compared to RNNLG in future iterations of the dataset?
2. Have you considered evaluating the dataset using other state-of-the-art NLG models, such as transformers or pretrained language models, to provide a broader assessment of its complexity?
3. Can you provide more details on the inter-annotator agreement and validation process for the crowdsourced text? How consistent were the crowdworkers in producing high-quality outputs?
Additional Comments
Overall, this paper presents a significant contribution to the field of NLG by addressing the limitations of existing benchmarks and introducing a framework for creating linguistically diverse datasets. While there are some areas for improvement, the proposed DBPNLG dataset has the potential to drive advancements in microplanning and neural generation research. I recommend acceptance with minor revisions.