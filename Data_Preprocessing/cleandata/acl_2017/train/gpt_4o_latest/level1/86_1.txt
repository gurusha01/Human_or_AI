Review of the Paper
Summary of the Paper:  
The paper addresses the problem of generating source code in general-purpose programming languages (e.g., Python) from natural language (NL) descriptions. Unlike prior work that treats this as a sequence-to-sequence task, the authors propose a novel syntax-driven neural model that explicitly incorporates the grammar of the target programming language. The model generates Abstract Syntax Trees (ASTs) using a probabilistic grammar model, which is then deterministically converted into code. The approach is evaluated on two Python datasets (HEARTHSTONE and DJANGO) and a domain-specific dataset (IFTTT), achieving state-of-the-art results.
Main Contributions:  
1. Grammar-Driven Code Generation Framework: The primary contribution is the introduction of a syntax-driven neural model that generates ASTs by applying grammar rules. This approach ensures the generation of syntactically valid code and reduces the hypothesis space, enabling the model to focus on learning compositionality.  
2. Parent Feeding Mechanism: The authors extend the decoder with a parent feeding mechanism, which incorporates structural information from parent nodes in the AST. This improves the model's ability to capture recursive and hierarchical relationships in programming languages.  
3. Empirical Results and Robustness: The model achieves significant improvements in accuracy (11.7% and 9.3% on HEARTHSTONE and DJANGO, respectively) over state-of-the-art baselines. It also demonstrates robustness for generating larger and more complex ASTs.
Strengths:  
1. Novel Syntax-Driven Approach: The explicit use of grammar rules to guide the generation process is a significant advancement over prior sequence-to-sequence models. This guarantees syntactically valid outputs, addressing a key limitation of existing methods.  
2. Empirical Performance: The model achieves state-of-the-art results across multiple datasets, including general-purpose and domain-specific code generation tasks. The improvements in accuracy and BLEU scores are substantial and well-documented.  
3. Comprehensive Evaluation: The authors conduct thorough experiments, including ablation studies and performance analysis based on AST size. This provides valuable insights into the model's strengths and limitations.  
4. Generality: The approach is programming language-agnostic, making it applicable to other general-purpose languages beyond Python.  
Weaknesses:  
1. Limited Analysis of Failure Cases: While the paper includes some qualitative examples, a more detailed analysis of failure cases (e.g., incorrect lambda function generation) would provide deeper insights into the model's limitations and areas for improvement.  
2. Scalability to Larger Datasets: The datasets used are relatively small for neural models. It is unclear how well the approach would scale to larger datasets or more diverse programming languages.  
3. Complexity of Training and Inference: The proposed model introduces additional complexity (e.g., parent feeding, pointer networks, unary closures). The paper could benefit from a more detailed discussion of computational overhead and scalability.  
Questions to Authors:  
1. How does the model handle ambiguous or underspecified NL descriptions where multiple valid code snippets could be generated?  
2. Can the proposed approach be extended to handle multi-line code generation tasks or entire program synthesis?  
3. What are the computational trade-offs of incorporating grammar rules compared to sequence-to-sequence models?  
Conclusion:  
This paper presents a significant contribution to the field of code generation by introducing a syntax-driven neural model that explicitly incorporates grammar rules. The approach is well-motivated, achieves state-of-the-art results, and is evaluated comprehensively. However, further exploration of scalability and failure cases would strengthen the work. Overall, this is a strong submission that advances the state of the art in code generation and semantic parsing.