Review of the Paper
Summary and Contributions  
This paper addresses the challenge of zero-resource neural machine translation (NMT) by proposing a novel teacher-student framework. The method assumes that parallel sentences in a source-pivot language pair and pivot-target language pair should have close probabilities of generating a target sentence. The proposed approach directly trains a source-to-target NMT model ("student") using a pivot-to-target NMT model ("teacher") and a source-pivot parallel corpus. The authors introduce both sentence-level and word-level teaching strategies to guide the student model, avoiding the error propagation and inefficiency issues inherent in pivot-based methods. Experimental results on the Europarl and WMT datasets demonstrate significant improvements in BLEU scores over state-of-the-art pivot-based and multilingual approaches.
The main contributions of the paper are as follows:
1. Teacher-Student Framework for Zero-Resource NMT: The paper introduces a novel framework that directly trains a source-to-target NMT model without parallel corpora, leveraging a teacher model and source-pivot data. This approach eliminates the need for two-step decoding, addressing error propagation and computational inefficiency.
2. Word-Level and Sentence-Level Teaching Strategies: The authors propose two distinct strategies for guiding the student model, with word-level teaching (via sampling) showing superior performance by introducing greater data diversity.
3. Empirical Validation and State-of-the-Art Results: The proposed method achieves significant BLEU score improvements (e.g., +3.29 BLEU on Spanish-French and +3.15 BLEU on German-French) over existing zero-resource methods, demonstrating its effectiveness across multiple datasets and language pairs.
---
Strengths  
1. Innovative Framework: The teacher-student framework is a novel and elegant solution to the zero-resource NMT problem, addressing key limitations of pivot-based methods (e.g., error propagation and inefficiency).
2. Comprehensive Evaluation: The paper provides thorough experimental results on multiple datasets (Europarl and WMT) and language pairs, demonstrating consistent and significant improvements over strong baselines.
3. Strong Empirical Results: The proposed word-level sampling method not only outperforms pivot-based and multilingual methods but also rivals standard NMT models trained on parallel corpora, highlighting its practical utility.
4. Theoretical and Practical Insights: The paper offers a clear theoretical foundation (e.g., translation equivalence assumptions) and practical implementation details (e.g., beam search, sampling), making the approach reproducible and interpretable.
5. Low-Resource Adaptability: The method is shown to be effective even with limited source-pivot data, making it applicable to real-world low-resource scenarios.
---
Weaknesses  
1. Limited Analysis of Word-Level Sampling: While word-level sampling achieves the best results, the paper does not fully explore why it outperforms other methods or the trade-offs between data diversity and KL divergence.
2. Scalability Concerns: The computational cost of word-level sampling (e.g., Monte Carlo estimation) is not thoroughly discussed, particularly for large-scale datasets or high-resource languages.
3. Assumption Validation: While the translation equivalence assumptions are empirically validated, the analysis is limited to specific datasets and may not generalize to more diverse or morphologically complex languages.
4. Comparison with Multilingual Methods: The paper briefly compares its approach to multilingual NMT methods but does not explore hybrid approaches that combine multilingual training with the proposed framework.
---
Questions to Authors  
1. How does the performance of the proposed method vary with different pivot languages? For example, would a morphologically rich pivot language affect the results?
2. Could the teacher-student framework be extended to multilingual scenarios, where multiple pivot languages are available?
3. How does the computational cost of word-level sampling compare to sentence-level methods and pivot-based approaches in practice?
---
Overall Recommendation  
This paper makes a significant contribution to zero-resource NMT by introducing a novel and effective teacher-student framework. The strong empirical results, combined with the practical applicability of the method, make it a valuable addition to the field. However, further analysis of the word-level sampling method and scalability considerations would strengthen the work. I recommend acceptance with minor revisions.