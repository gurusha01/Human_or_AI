Review of the Paper
Summary
This paper presents ITransF, a novel knowledge embedding model aimed at addressing the data sparsity problem in knowledge base completion (KBC). The model introduces a sparse attention mechanism to enable knowledge transfer by discovering shared statistical regularities across relations. The sparse attention vectors allow for interpretable associations between relations and latent concepts. ITransF achieves state-of-the-art performance on two benchmark datasets, WN18 and FB15k, without relying on external information. Additionally, the paper proposes a block iterative optimization algorithm to enforce sparsity in the attention mechanism and introduces a domain-aware sampling method for generating negative samples.
Contributions
1. Sparse Attention Mechanism for Knowledge Transfer: The primary contribution is the introduction of a sparse attention mechanism that enables the sharing of projection matrices across relations. This approach alleviates the data sparsity problem, particularly for rare relations, and provides interpretable associations between relations and shared concepts.
2. Block Iterative Optimization Algorithm: The paper proposes a novel optimization algorithm to enforce sparsity in the attention vectors, which improves interpretability and computational efficiency while maintaining competitive performance.
3. Empirical Validation: The model achieves state-of-the-art results on WN18 and FB15k, outperforming all intrinsic models that do not use external information. The paper also demonstrates the model's robustness on rare relations and its ability to compress parameters significantly without performance degradation.
Strengths
1. Addressing Data Sparsity: The sparse attention mechanism is a significant advancement over prior models like STransE and TransR, as it effectively transfers statistical strength from common to rare relations. This is particularly evident in the performance gains on rare relations.
2. Interpretability: The sparse attention vectors provide clear insights into how relations share underlying concepts, which is a notable improvement over black-box embedding models. The visualizations of attention vectors further enhance the interpretability of the method.
3. Empirical Performance: The model achieves competitive results on both mean rank and Hits@10 metrics, surpassing strong baselines like STransE and TransR. The proposed domain-aware sampling method further boosts performance.
4. Model Compression: The ability to reduce the number of projection matrices by up to 90x without significant performance loss is a practical advantage, making the model more efficient and scalable.
5. Thorough Analysis: The paper provides detailed ablation studies and visualizations to validate the effectiveness of its components, including sparse attention, domain sampling, and model compression.
Weaknesses
1. Limited Novelty in Optimization: While the block iterative optimization algorithm is effective, it is inspired by existing methods (e.g., LightRNN). The novelty of this contribution is incremental.
2. Scalability Concerns: Although the model demonstrates efficiency gains through parameter compression, the computational cost of the sparse attention mechanism and iterative optimization may still be prohibitive for very large knowledge bases.
3. Comparison with External Information Models: While the paper focuses on intrinsic models, it would have been valuable to discuss how ITransF might integrate external information (e.g., textual data) to compete with models like IRN or NLFeat.
4. Path-Based Inference: The paper acknowledges that path-based models outperform intrinsic models on FB15k but does not extend ITransF to multi-step inference. This limitation reduces the model's applicability to tasks requiring complex reasoning.
Questions to Authors
1. How does the computational cost of ITransF compare to STransE and TransR for large-scale knowledge bases with millions of entities and relations?
2. Can the sparse attention mechanism be extended to incorporate external information, such as textual relations or node features? If so, how would this affect interpretability?
3. Have you explored the impact of varying the number of concept matrices (m) on datasets larger than FB15k and WN18? Would the model generalize well to such datasets?
Overall Assessment
The paper presents a strong contribution to the field of knowledge base completion, particularly in addressing the data sparsity problem and improving interpretability. The sparse attention mechanism and parameter-sharing approach are well-motivated and empirically validated. While there are minor concerns regarding scalability and novelty in optimization, the strengths of the paper outweigh its weaknesses. I recommend acceptance, with the suggestion to explore extensions for multi-step inference and external information integration in future work. 
Rating: 8/10