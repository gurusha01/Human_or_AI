Review of the Paper
Summary of the Paper:  
This paper introduces a novel framework that integrates cognitive features derived from human eye-movement data with textual features for sentiment polarity and sarcasm detection tasks. The authors propose a Convolutional Neural Network (CNN)-based architecture to automatically extract features from both text and gaze data, addressing limitations of traditional handcrafted feature-based approaches. The proposed method is evaluated on two publicly available datasets enriched with eye-movement information, demonstrating improved performance over existing systems that rely solely on text or manually engineered gaze features.
Main Contributions:  
1. Automatic Feature Extraction from Gaze Data: The primary contribution of this work is the development of a CNN-based framework to automatically learn features from gaze data, eliminating the need for manual feature engineering. This is a significant advancement, as it addresses the challenges of capturing linguistic subtleties that are difficult to model manually.  
2. Multimodal Feature Integration: The paper demonstrates the effectiveness of combining gaze-based cognitive features with textual features in a unified CNN architecture, achieving superior performance on sentiment and sarcasm classification tasks compared to text-only models.  
3. Empirical Validation: The authors provide a thorough evaluation of their approach on two datasets, showing statistically significant improvements over state-of-the-art methods. The analysis of learned features offers insights into the role of gaze data in capturing semantic and pragmatic nuances.
Strengths:  
1. Novelty and Relevance: The paper introduces an innovative approach to cognitive NLP by leveraging gaze data in a deep learning framework, which is a relatively unexplored area. This work is timely and relevant to advancing multimodal NLP research.  
2. Performance Gains: The proposed method achieves notable improvements in F-scores for sentiment and sarcasm detection tasks, particularly on Dataset 1, where the multimodal CNN outperforms both traditional and text-only CNN approaches.  
3. Thorough Analysis: The authors provide detailed analyses of their results, including the impact of embedding dimensions, gaze channels (fixation vs. saccade), and learned feature representations. This enhances the interpretability and credibility of their findings.  
4. Scalability and Automation: By automating feature extraction from gaze data, the framework reduces human bias and the effort required for manual feature engineering, making it more scalable for real-world applications.
Weaknesses:  
1. Limited Generalization: The proposed method is evaluated on only two datasets, both of which are relatively small and domain-specific. This raises concerns about the generalizability of the approach to larger, more diverse datasets or other NLP tasks.  
2. Overfitting on Dataset 2: The authors acknowledge overfitting issues on Dataset 2, which suggests that the model's regularization techniques and hyperparameter tuning require further optimization.  
3. Dependence on Gaze Data: While the use of gaze data is innovative, its reliance on specialized hardware (e.g., eye-trackers) limits the practical applicability of the approach in scenarios where such data is unavailable.  
4. Limited Improvement for Sarcasm Detection: The addition of gaze data provides only marginal improvements for sarcasm detection, which raises questions about the robustness of the multimodal approach for this task.
Questions to Authors:  
1. How does the proposed method perform on larger datasets or datasets without gaze information? Could the framework be adapted to work with synthetic or inferred gaze data?  
2. Have you considered using alternative deep learning architectures, such as transformers, which have shown strong performance in NLP tasks?  
3. Could you elaborate on the potential challenges of extending this framework to document-level sentiment analysis or other NLP tasks like emotion detection?  
Conclusion:  
This paper presents a compelling approach to integrating cognitive and textual features for sentiment and sarcasm detection, with promising results and insightful analyses. However, its reliance on gaze data and limited evaluation scope may hinder its broader applicability. Further exploration of scalability, generalization, and alternative architectures would strengthen the impact of this work.