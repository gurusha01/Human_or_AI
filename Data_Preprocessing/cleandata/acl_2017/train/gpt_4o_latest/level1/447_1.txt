Review of the Paper
Summary and Contributions
This paper investigates the utility of discourse structure, as defined by Rhetorical Structure Theory (RST), for text categorization tasks. The authors propose a recursive neural network model that incorporates discourse dependency trees derived from an automatic parser. A novel, unnormalized attention mechanism is introduced to weight the importance of different parts of the text based on their discourse relations. The paper evaluates the proposed model on five datasets spanning diverse text categorization tasks, demonstrating that discourse structure can improve performance in most cases.
The primary contributions of this work are:
1. Integration of Discourse Structure in Neural Models: The paper demonstrates how discourse dependency trees can be leveraged for text categorization, providing an inductive bias that highlights salient parts of the text.
2. Novel Attention Mechanism: The proposed unnormalized attention mechanism, inspired by RST, avoids competition among sibling nodes in the discourse tree, which is a departure from traditional normalized attention mechanisms.
3. Empirical Validation Across Tasks: The model is evaluated on five datasets, showing state-of-the-art performance on four of them, and providing insights into the limitations of discourse parsing for certain genres (e.g., legislative bills).
Strengths
1. Novel Use of Discourse Structure: The paper makes a compelling case for the utility of discourse structure in text categorization, which is a novel and underexplored direction in the field.
2. Comprehensive Evaluation: The authors evaluate their model on diverse datasets, including sentiment analysis, framing detection, and legislative prediction tasks. This breadth of evaluation strengthens the generalizability of the findings.
3. Insightful Analysis: The paper provides a detailed analysis of the impact of discourse parsing quality on model performance, as well as qualitative examples that illustrate the strengths and weaknesses of the approach.
4. State-of-the-Art Results: The proposed model achieves state-of-the-art performance on four out of five datasets, demonstrating the practical utility of the approach.
5. Open Implementation: The authors make their implementation publicly available, which promotes reproducibility and further research.
Weaknesses
1. Limited Generalizability Across Genres: The model underperforms on legislative bills, a genre with discourse conventions that differ significantly from the parser's training data. This raises concerns about the robustness of the approach for out-of-domain tasks.
2. Overparameterization of the FULL Model: The FULL model performs worse than simpler variants (e.g., UNLABELED) on smaller datasets, suggesting that it may be overparameterized for tasks with limited training data.
3. Dependence on Discourse Parser Quality: The model's performance is tightly coupled to the quality of the discourse parser, which may limit its applicability in domains where high-quality discourse parsers are unavailable.
4. Lack of Comparison to Non-Neural Baselines: While the paper compares its model to prior neural approaches, it does not provide a detailed comparison to simpler, non-neural baselines, which could help contextualize the gains achieved by the proposed method.
Questions to Authors
1. How does the proposed model handle texts with noisy or incorrect discourse parses? Would incorporating uncertainty in the parser's output improve robustness?
2. Could the attention mechanism be extended to incorporate additional linguistic features, such as syntactic or semantic information, to further enhance performance?
3. Have you considered fine-tuning the discourse parser on task-specific data to improve its domain adaptation?
Additional Comments
Overall, this paper makes a significant contribution to the field by demonstrating the utility of discourse structure for text categorization. While there are some limitations, particularly in genre generalizability and dependence on parser quality, the proposed approach opens up exciting avenues for future research in discourse-aware NLP models.