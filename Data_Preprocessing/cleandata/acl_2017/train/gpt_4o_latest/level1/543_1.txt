Review
Summary of the Paper  
This paper introduces a novel approach to character-level compositionality by leveraging the visual characteristics of characters to create embeddings using convolutional neural networks (CNNs). Unlike traditional character embeddings that rely on lookup tables, the proposed method generates embeddings by processing character images, enabling better generalization for rare characters. The authors evaluate their model on a text classification task using a dataset of Wikipedia article titles in Chinese, Japanese, and Korean. The results demonstrate that the proposed visual embedding model outperforms traditional lookup-based embeddings for instances with rare characters. Additionally, the paper explores fusion methods that combine visual and lookup embeddings, achieving further performance improvements. Qualitative analyses reveal that the model learns visually coherent embeddings, with similar visual features corresponding to similar semantic meanings.
Main Contributions  
1. Visual Character Embeddings: The primary contribution is the development of a framework that generates character embeddings based on their visual appearance using CNNs. This approach is particularly effective for languages with logographic writing systems, such as Chinese and Japanese, where sub-character components carry semantic or phonetic information.
2. Handling Rare Characters: The paper demonstrates that the proposed visual embedding model is more robust than traditional lookup embeddings for rare characters, addressing a key limitation in existing character-level models.
3. Fusion of Visual and Lookup Embeddings: The authors propose and evaluate three fusion methods (early, late, and fallback fusion) to combine the strengths of visual and lookup embeddings, showing that these methods yield consistent improvements over individual models.
Strengths  
1. Novelty and Applicability: The idea of leveraging visual features for character embeddings is innovative and well-suited for languages with complex writing systems. The method is generalizable and could be extended to other tasks or languages.
2. Empirical Validation: The paper provides thorough experimental results, including comparisons with baseline models, performance on low-resource scenarios, and qualitative analyses of learned embeddings. The visualization of character embeddings and their semantic coherence is particularly compelling.
3. Complementary Fusion Methods: The exploration of fusion methods adds practical value, demonstrating how the proposed visual embeddings can be integrated with existing approaches to achieve better results.
4. Dataset Contribution: The authors construct a new dataset of Wikipedia titles in Chinese, Japanese, and Korean, which satisfies the compositionality requirements of the task. This dataset is a valuable resource for future research.
Weaknesses  
1. Limited Baseline Comparisons: The paper primarily compares the proposed model to a single baseline (lookup embeddings). Additional comparisons with other character-level models, such as those incorporating morphological or subword information, would strengthen the evaluation.
2. Generalization Beyond Logographic Languages: While the method is effective for logographic languages, its applicability to languages with phonemic orthographies (e.g., English) is not thoroughly explored. This limits the perceived generalizability of the approach.
3. Fusion Method Evaluation: While the fusion methods improve performance, the fallback fusion method relies on a manually defined threshold for character frequency, which may not generalize well across tasks or datasets. A more principled approach to determining this threshold would be beneficial.
4. Ablation Studies: The paper lacks ablation studies to isolate the contributions of different components of the visual embedding model (e.g., CNN architecture, image resolution). Such analyses would provide deeper insights into the model's design choices.
Questions to Authors  
1. How does the proposed model perform on languages with phonemic orthographies, such as English or Korean Hangul, where visual features may not directly correspond to semantics?  
2. Have you considered alternative CNN architectures or pre-trained visual models to improve the quality of visual embeddings?  
3. Could the fallback fusion method be automated by learning the threshold for character frequency during training?  
4. How does the model handle characters with similar visual appearances but different meanings (e.g., homographs)?  
Overall Recommendation  
The paper presents a novel and well-executed approach to character-level compositionality, with strong empirical results and practical contributions. However, the limited baseline comparisons and lack of generalization to non-logographic languages slightly weaken its impact. With additional experiments and broader applicability, this work has the potential to make a significant contribution to the field.