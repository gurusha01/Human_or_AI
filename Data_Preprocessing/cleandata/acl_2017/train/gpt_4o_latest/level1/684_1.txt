Review of the Paper
Summary and Contributions  
This paper presents the Gated-Attention (GA) Reader, a novel model for answering cloze-style questions over documents. The GA Reader integrates a multi-hop architecture with a unique attention mechanism based on multiplicative interactions between query embeddings and intermediate document representations. This approach enables the model to build query-specific token representations for accurate answer selection. The model achieves state-of-the-art results on three benchmark datasets: CNN, Daily Mail, and Who Did What (WDW). The authors demonstrate the effectiveness of their approach through extensive experiments, ablation studies, and attention visualizations.
The primary contributions of the paper are:  
1. Gated-Attention Mechanism: The proposed attention mechanism enables fine-grained, query-specific filtering of token representations across multiple hops, significantly improving performance over existing methods.  
2. Empirical Validation: The GA Reader achieves state-of-the-art performance on multiple datasets, surpassing prior models by a significant margin, including improvements of 4% on CNN and Daily Mail datasets.  
3. Ablation Studies and Analysis: The paper provides a detailed analysis of the GA mechanism, demonstrating its superiority over alternative compositional operators (e.g., addition, concatenation) and highlighting the importance of multi-hop reasoning and pre-trained embeddings.
Strengths  
1. Novelty and Effectiveness of Gated-Attention: The multiplicative gating mechanism is a novel contribution that addresses limitations in prior attention mechanisms by enabling token-level, query-specific filtering. This is supported by strong empirical results and ablation studies.  
2. State-of-the-Art Results: The GA Reader consistently outperforms competitive baselines across multiple datasets, including CNN, Daily Mail, and WDW. The improvements are statistically significant and demonstrate the robustness of the approach.  
3. Comprehensive Evaluation: The paper evaluates the model on five datasets, performs ablation studies, and provides attention visualizations, offering valuable insights into the model's behavior and effectiveness.  
4. Clarity and Reproducibility: The paper is well-written, with detailed descriptions of the model architecture, training procedure, and hyperparameters, making it easier for researchers to reproduce the results.
Weaknesses  
1. Limited Theoretical Justification: While the empirical results strongly support the effectiveness of the gated-attention mechanism, the paper lacks a theoretical explanation for why multiplicative gating outperforms other compositional operators.  
2. Feature Engineering Dependency: The model's reliance on the qe-comm feature raises concerns about generalizability to datasets where such features are unavailable or less effective. The performance drop in its absence highlights this limitation.  
3. Scalability to Larger Contexts: The GA Reader is evaluated on datasets with relatively constrained document lengths. It is unclear how well the model scales to longer documents or more complex queries, which could limit its applicability to real-world scenarios.
Questions to Authors  
1. Can you provide a theoretical explanation or intuition for why multiplicative gating outperforms addition and concatenation?  
2. How does the GA Reader perform on datasets with significantly longer documents or more complex queries?  
3. Would the model's reliance on the qe-comm feature limit its applicability to datasets without such engineered features?  
Conclusion  
This paper makes a significant contribution to the field of machine reading comprehension by introducing a novel attention mechanism and achieving state-of-the-art results on multiple benchmarks. While there are minor concerns regarding theoretical justification and scalability, the strengths of the paper outweigh these limitations. I recommend acceptance of this submission.