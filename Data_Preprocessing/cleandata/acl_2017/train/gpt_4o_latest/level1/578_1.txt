Review
Summary of the Paper
The paper presents a novel neural encoder-decoder transition-based parser for semantic graph parsing, specifically targeting Minimal Recursion Semantics (MRS). The proposed model is the first full-coverage semantic graph parser for MRS, leveraging deep learning to predict linguistically expressive graphs incrementally. The parser is evaluated on MRS-based graph representations (DMRS and EDS) and Abstract Meaning Representation (AMR), achieving state-of-the-art performance on MRS parsing and competitive results on AMR parsing. The model is also significantly faster than traditional grammar-based parsers due to GPU batch processing.
Main Contributions
1. Development of a Full-Coverage Semantic Graph Parser for MRS: The paper introduces a robust and efficient parser for MRS, which outperforms existing attention-based baselines and achieves a high Smatch score of 86.69%, surpassing the upper bound for AMR parsing. This contribution is significant as it addresses the gap in robust, wide-coverage parsing of linguistically deep semantic representations.
   
2. Integration of Transition-Based Parsing with Neural Architectures: The use of a stack-based model with hard attention and pointer networks for alignment prediction is a novel approach, enabling accurate and efficient parsing of non-planar dependencies. This method demonstrates clear improvements over simpler encoder-decoder baselines.
3. Cross-Domain Applicability: The model is applied to AMR parsing, achieving competitive results compared to state-of-the-art parsers that rely on external resources. This demonstrates the generalizability of the proposed approach to other semantic graph representations.
Strengths
1. State-of-the-Art Performance on MRS Parsing: The parser achieves significant improvements over existing baselines, with a notable increase in EDM and Smatch scores. The results are robust across different MRS graph representations (DMRS and EDS).
   
2. Efficiency and Scalability: The parser is an order of magnitude faster than grammar-based parsers like ACE, thanks to GPU batch processing and incremental prediction. This makes it suitable for large-scale applications.
3. Innovative Use of Neural Architectures: The integration of stack-based features and hard attention mechanisms enhances the model's ability to handle complex graph structures, contributing to its superior performance.
4. Comprehensive Evaluation: The paper provides extensive experiments, comparing different graph linearizations, attention mechanisms, and model architectures. The evaluation is thorough and includes both accuracy and speed metrics.
Weaknesses
1. Limited Comparison with Non-Neural Methods: While the parser is compared to ACE, the paper does not provide a detailed analysis of how its linguistic expressiveness compares to grammar-based approaches. This limits the interpretability of the results for linguistically motivated tasks.
   
2. AMR Parsing Performance: Although the model performs well on AMR parsing, it still lags behind state-of-the-art parsers that utilize external resources like syntax trees. The paper could have explored incorporating such resources to further improve performance.
3. Data Dependency: The model relies heavily on supervised alignments for MRS parsing, which may limit its applicability to low-resource languages or domains where such annotations are unavailable.
Questions to Authors
1. How does the parser handle sentences with ambiguous or incomplete semantic structures, particularly in the absence of gold-standard alignments?
2. Could the proposed model be extended to support multilingual semantic parsing, and if so, what modifications would be required?
3. Have you considered incorporating external linguistic resources (e.g., syntactic trees or semantic role labels) to improve AMR parsing performance?
Additional Comments
The paper makes a strong case for the use of neural transition-based parsing for linguistically deep semantic representations. However, future work could explore semi-supervised or unsupervised approaches to reduce reliance on annotated data. Additionally, a more detailed qualitative analysis of parsing errors would provide valuable insights into the model's limitations.