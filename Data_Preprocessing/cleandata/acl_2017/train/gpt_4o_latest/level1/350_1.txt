Review of the Paper
Summary and Contributions  
This paper addresses the challenge of event extraction (EE) in natural language processing (NLP), particularly the reliance on small, manually annotated datasets that limit scalability and coverage. The authors propose a novel method to automatically label training data for EE using a combination of world knowledge (Freebase) and linguistic knowledge (FrameNet). The key contributions of the paper, as I see them, are as follows:
1. Automatic Data Labeling for EE: The paper introduces a method to automatically generate large-scale labeled data for EE by leveraging Freebase for key argument detection and FrameNet for trigger word filtering and expansion. This is a significant contribution, as it addresses the scalability issue of supervised EE methods.
   
2. Soft Distant Supervision (SDS): The authors propose a Soft Distant Supervision approach for labeling data, which assumes that sentences containing key arguments and corresponding trigger words are likely to express events. This method effectively mitigates the challenges of noisy data in distant supervision.
3. Evaluation and Dataset Release: The authors demonstrate that their automatically labeled data is competitive with human-annotated datasets through both manual and automatic evaluations. Additionally, they release the labeled dataset for further research, which is a valuable resource for the community.
Strengths  
1. Scalability: The proposed method significantly reduces the dependency on expensive human annotation, enabling large-scale event extraction across diverse domains.
   
2. Comprehensive Evaluation: The paper provides both manual and automatic evaluations to validate the quality of the automatically labeled data. The results show that the data achieves high precision and can augment human-annotated datasets effectively.
3. Integration of World and Linguistic Knowledge: The combination of Freebase and FrameNet is innovative and demonstrates the potential of integrating structured knowledge bases with linguistic resources for EE.
4. Baseline Model: The use of a CNN-based model with multi-instance learning (DMCNN-MIL) as a baseline for the automatically labeled data is well-motivated and provides a solid foundation for further research.
5. Public Dataset: The release of the labeled dataset is a significant contribution to the research community, fostering reproducibility and further exploration.
Weaknesses  
1. Trigger Word Detection: While the use of FrameNet to filter and expand triggers is effective, the reliance on predefined lexical units may limit the method's adaptability to new or emerging event types. A discussion on this limitation would strengthen the paper.
2. Limited Event Types: The experiments focus on a subset of 21 event types from Freebase. While the results are promising, the scalability of the method to a broader range of event types remains unclear.
3. Noise in Automatically Labeled Data: Despite the use of SDS and multi-instance learning, the paper does not provide a detailed analysis of the types of noise present in the labeled data and their impact on downstream tasks.
4. Comparison with Other Weakly Supervised Methods: The paper does not compare its approach with other weakly supervised or unsupervised EE methods, which would provide a clearer picture of its relative strengths and weaknesses.
Questions to Authors  
1. How does the method handle event types or arguments that are not well-represented in Freebase or FrameNet?  
2. Can the proposed approach be extended to multilingual event extraction tasks?  
3. What are the computational costs associated with the automatic data labeling process, and how do they compare to manual annotation efforts?
Conclusion  
Overall, this paper presents a novel and impactful approach to addressing the data bottleneck in event extraction. The integration of world and linguistic knowledge, along with the release of a large-scale dataset, makes it a valuable contribution to the field. However, addressing the limitations related to trigger detection, scalability, and noise analysis would further strengthen the work. I recommend acceptance with minor revisions.