Review
Summary
This paper introduces a novel A* CCG parsing model that incorporates bilexical dependency modeling into the parsing process. By extending the bi-directional LSTM (bi-LSTM) architecture, the authors propose a factored model that predicts both supertags and head dependencies, enabling efficient precomputation of probabilities while explicitly modeling sentence structures. The proposed method achieves state-of-the-art results on English and Japanese CCG parsing, demonstrating its effectiveness across languages with different syntactic characteristics.
Main Contributions
1. Integration of Dependency Modeling in A* CCG Parsing: The primary contribution is the introduction of a dependency-factored model that resolves attachment ambiguities without relying on deterministic heuristics. This is a significant improvement over prior methods, such as the attach-low heuristic, which are language-specific and prone to errors.
   
2. Efficient A Search with Precomputed Probabilities: The authors demonstrate that their model retains the efficiency of A parsing by precomputing probabilities for both supertags and dependencies. This ensures tractability while enhancing the expressiveness of the model.
3. State-of-the-Art Results on English and Japanese Parsing: The model achieves the best-reported F1 scores for English CCG parsing and significantly outperforms existing methods on Japanese CCG parsing. The results highlight the model's robustness across languages with different syntactic properties.
Strengths
1. Novelty and Generality: The integration of dependency modeling into A* CCG parsing is a novel approach that addresses a key limitation of prior methods. The model's ability to generalize across English and Japanese demonstrates its versatility.
   
2. Empirical Performance: The proposed method achieves state-of-the-art results on both labeled and unlabeled F1 metrics for English CCG parsing and significantly improves Japanese parsing accuracy. The results are well-supported by experiments and ablation studies.
3. Efficiency: Despite the added complexity of dependency modeling, the model maintains competitive parsing speeds due to the precomputation of probabilities. The detailed comparison with existing parsers highlights its efficiency in A* search.
4. Thorough Evaluation: The paper provides extensive experiments, including comparisons with baseline models, ablation studies, and evaluations of different dependency conversion strategies. The analysis of normal form constraints and tri-training further strengthens the empirical findings.
Weaknesses
1. Implementation Details: While the paper mentions implementation differences (e.g., Python vs. C++), it does not provide sufficient details about the optimization of the supertagging component, which is slower than competing methods. This could hinder reproducibility and practical adoption.
2. Limited Discussion on Linguistic Insights: Although the paper demonstrates the effectiveness of dependency modeling, it does not delve deeply into the linguistic implications of the proposed method, particularly for languages with freer word order like Japanese.
3. Evaluation on Broader Datasets: The experiments are limited to English and Japanese CCGbanks. Evaluating the model on additional languages or datasets could further validate its generalizability.
Questions to Authors
1. Can you provide more details on the implementation of the supertagging component and potential optimizations to improve its speed?
2. How does the model handle cases where dependency predictions conflict with supertag predictions during parsing?
3. Have you considered evaluating the model on other languages or datasets to further demonstrate its generalizability?
Additional Comments
The paper is well-written and provides a significant contribution to the field of CCG parsing. Addressing the weaknesses mentioned above could further enhance the impact and applicability of the proposed method.