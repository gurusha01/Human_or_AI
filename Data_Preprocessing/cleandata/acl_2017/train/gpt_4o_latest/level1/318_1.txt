Review of the Paper
Summary and Contributions
This paper presents a novel approach to Word Representation Learning (WRL) by incorporating sememe information from HowNet, a linguistic common-sense knowledge base. The authors propose three models—Simple Sememe Aggregation (SSA), Sememe Attention over Context (SAC), and Sememe Attention over Target (SAT)—to encode sememe information into word embeddings. The key idea is to use sememes as the minimum semantic units to better capture word meanings and disambiguate word senses in context. The proposed models are evaluated on word similarity and word analogy tasks, where they outperform baseline methods such as Skip-gram, CBOW, and GloVe. 
The main contributions of the paper, as I see them, are:
1. Integration of Sememe Information into WRL: The paper is the first to incorporate sememe annotations from HowNet into WRL, providing a novel way to address word sense disambiguation (WSD) and improve word embeddings.
2. Attention-Based Framework for WSD: The proposed SAC and SAT models leverage attention mechanisms to dynamically select word senses based on context, offering a more nuanced approach to polysemy compared to prior methods.
3. Empirical Validation: The extensive experiments on word similarity and word analogy tasks demonstrate the effectiveness of the proposed models, particularly the SAT model, which achieves state-of-the-art results.
Strengths
1. Novelty of Approach: The integration of sememe information into WRL is a unique contribution that bridges linguistic knowledge bases with modern embedding techniques. This is a significant step forward in addressing the limitations of traditional WRL methods, which often ignore polysemy or rely on less granular sense representations.
2. Strong Empirical Results: The SAT model consistently outperforms baselines on both word similarity and word analogy tasks. The results also highlight the advantages of soft word sense disambiguation over hard sense selection, as demonstrated by the comparison with the MST baseline.
3. Interpretability: By explicitly modeling sememes, the proposed approach provides interpretable embeddings that can be analyzed at the level of semantic units, senses, and words. The case studies further illustrate the ability of the models to capture nuanced word meanings in context.
4. Comprehensive Evaluation: The paper evaluates the models on multiple tasks and provides detailed analyses, including case studies and attention visualizations, which strengthen the claims of the paper.
Weaknesses
1. Limited Exploration of Sememe Hierarchies: While the paper uses sememe annotations from HowNet, it does not leverage the hierarchical structure or relational information between sememes. This is a missed opportunity to further enhance the embeddings and improve WSD.
2. Language-Specific Focus: The approach is evaluated only on Chinese data, which limits its generalizability. While the authors suggest that the method could be extended to other languages, no experiments or discussions are provided to support this claim.
3. Computational Overhead: The attention-based models, particularly SAT, introduce additional computational complexity compared to simpler WRL methods like Skip-gram. The paper does not provide a detailed analysis of the trade-offs between performance gains and computational costs.
4. Evaluation Scope: The evaluation is limited to word similarity and word analogy tasks. While these are standard benchmarks for WRL, additional tasks such as downstream NLP applications (e.g., sentiment analysis, machine translation) would provide a more comprehensive assessment of the practical utility of the proposed models.
Questions to Authors
1. How does the computational cost of the SAT model compare to baseline methods like Skip-gram? Could you provide runtime or complexity analyses?
2. Have you considered leveraging the hierarchical structure of sememes in HowNet? If so, what challenges did you encounter, and how might they be addressed in future work?
3. How generalizable is the proposed approach to languages other than Chinese? Have you considered applying it to languages with different linguistic characteristics or without sememe-annotated resources like HowNet?
Additional Comments
Overall, this paper makes a strong contribution to WRL by introducing sememe-encoded models and demonstrating their effectiveness. Addressing the weaknesses, particularly the use of sememe hierarchies and extending the approach to other languages, could further enhance the impact of this work.