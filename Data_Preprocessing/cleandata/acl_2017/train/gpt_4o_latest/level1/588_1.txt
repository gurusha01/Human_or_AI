Review of the Paper
Summary and Contributions  
This paper introduces the novel task of rare entity prediction, where models predict missing entities in web documents by leveraging external knowledge in the form of lexical resources. The authors present a significantly enhanced version of the Wikilinks dataset, termed the Wikilinks Rare Entity Prediction dataset, which includes Freebase-derived entity descriptions. They propose two model architectures—Double Encoder (DOUBENC) and Hierarchical Double Encoder (HIERENC)—that integrate external knowledge with contextual understanding. The experimental results demonstrate that models utilizing external knowledge outperform those relying solely on contextual information, with HIERENC achieving the best performance. The paper highlights the importance of external knowledge in addressing challenges posed by rare entities in NLP tasks.
Main Contributions  
1. Novel Task and Dataset: The introduction of the rare entity prediction task and the accompanying Wikilinks Rare Entity Prediction dataset is the most significant contribution. The dataset is well-motivated, addressing a gap in existing reading comprehension tasks by focusing on rare entities and requiring models to integrate external knowledge.  
2. Model Architectures: The proposed DOUBENC and HIERENC models represent a meaningful step forward in combining contextual and external knowledge. HIERENC, in particular, demonstrates the utility of incorporating document-level context through a hierarchical structure.  
3. Empirical Insights: The experimental results provide valuable insights into the importance of external knowledge in NLP tasks involving rare entities. The comparison with strong baselines highlights the limitations of context-only models and the benefits of leveraging lexical resources.
Strengths  
1. Novelty and Relevance: The task of rare entity prediction is novel and addresses an important challenge in NLP, particularly in scenarios where rare entities are underrepresented in training data.  
2. Dataset Quality: The enhanced Wikilinks dataset is a valuable resource for the community, with careful preprocessing and integration of Freebase descriptions. Its focus on rare entities fills a gap in existing datasets.  
3. Strong Empirical Results: The proposed models achieve significant performance improvements over baselines, demonstrating the effectiveness of integrating external knowledge. The HIERENC model's 17% accuracy improvement over the language model baseline is particularly noteworthy.  
4. Clarity of Presentation: The paper is well-written, with clear explanations of the task, dataset, and model architectures. The inclusion of detailed examples and ablation studies enhances the reader's understanding.
Weaknesses  
1. Limited Exploration of Knowledge Sources: The paper focuses exclusively on lexical definitions from Freebase, ignoring other potentially valuable knowledge sources, such as relational information or graph-based representations. This limits the generalizability of the findings.  
2. Model Complexity vs. Performance: While HIERENC achieves the best results, the performance gain over DOUBENC is modest (2.6% on the test set). The added complexity of the hierarchical model may not justify the relatively small improvement.  
3. Scalability Concerns: The task formulation assumes that candidate entities are restricted to those present in the document, which simplifies the problem. Scaling the task to larger candidate sets (e.g., all entities in the dataset) may pose significant computational challenges.  
4. Lack of Robust Error Analysis: While the paper includes a few qualitative examples, a more systematic error analysis could provide deeper insights into the model's limitations and failure modes.
Questions to Authors  
1. Have you considered incorporating relational information from Freebase alongside lexical definitions? If so, what challenges did you encounter?  
2. How does the performance of the proposed models scale when the candidate set includes entities not present in the document?  
3. Could you elaborate on why increasing the context window size had minimal impact on performance?  
Additional Comments  
This paper makes a strong case for the importance of external knowledge in NLP tasks involving rare entities. While there are some limitations in the scope of the exploration, the contributions are significant and provide a solid foundation for future research.