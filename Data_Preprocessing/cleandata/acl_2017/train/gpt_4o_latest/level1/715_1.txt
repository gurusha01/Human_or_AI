Review of the Paper
Summary:  
This paper addresses the challenging task of open-domain question answering (QA) using Wikipedia as the sole knowledge source. The proposed system, DrWiki, integrates a two-stage pipeline: (1) a Document Retriever module that efficiently identifies relevant Wikipedia articles using bigram hashing and TF-IDF matching, and (2) a Document Reader module, a multi-layer recurrent neural network trained to extract answer spans from the retrieved articles. The authors evaluate their system on multiple QA datasets (SQuAD, CuratedTREC, WebQuestions, and WikiMovies) and demonstrate that multitask learning with distant supervision improves performance across datasets. The work highlights the unique challenges of combining document retrieval and machine comprehension in a single system.
Main Contributions:  
1. Development of DrWiki as a unified QA system: The paper presents a novel approach to open-domain QA by combining document retrieval and machine comprehension into a cohesive pipeline. The system is designed to operate solely on Wikipedia, making it a focused and resource-efficient solution compared to systems that rely on multiple redundant knowledge sources.
   
2. Evaluation across multiple benchmarks: The authors evaluate DrWiki on diverse QA datasets, demonstrating its generalizability and robustness. The multitask learning approach, which combines SQuAD training with distant supervision from other datasets, is shown to improve performance across most datasets.
3. Analysis of individual components: The paper provides a detailed analysis of the Document Retriever and Document Reader modules, highlighting their strengths and limitations. For example, the Document Retriever outperforms Wikipedia's built-in search engine, and the Document Reader achieves state-of-the-art results on the SQuAD benchmark for machine comprehension.
Strengths:  
1. Clear focus on a challenging problem: The paper tackles the realistic and underexplored problem of QA using a single knowledge source, which avoids the redundancy and complexity of multi-source systems like IBM's DeepQA.
   
2. Strong empirical results: The system achieves competitive performance on multiple datasets, and the multitask learning approach demonstrates the benefits of leveraging diverse training data.
3. Thorough evaluation and ablation studies: The authors provide detailed evaluations of both the Document Retriever and Document Reader, including feature ablation studies and comparisons to baseline systems like YodaQA.
4. Scalability and generalizability: The system is designed to be generic and adaptable to other document collections, making it a versatile contribution to the QA field.
Weaknesses:  
1. Limited novelty in individual components: While the integration of retrieval and comprehension is valuable, the individual components (e.g., TF-IDF-based retrieval and LSTM-based comprehension) rely on well-established techniques, limiting the methodological novelty of the work.
2. Performance gap in open-domain QA: The system's performance drops significantly when transitioning from machine comprehension (given a paragraph) to open-domain QA (given all of Wikipedia). For example, the exact match score on SQuAD drops from 69.5% to 26.7%, indicating limitations in the end-to-end pipeline.
3. Lack of end-to-end training: The Document Retriever and Document Reader are trained independently, which may limit the system's ability to optimize for the overall QA task. End-to-end training could potentially improve performance.
Questions to Authors:  
1. Have you considered incorporating end-to-end training for the Document Retriever and Document Reader? If not, what are the technical challenges in doing so?  
2. How does the system handle ambiguous or multi-answer questions, especially when multiple relevant spans are retrieved?  
3. Could the Document Reader be enhanced to better aggregate information across multiple paragraphs or documents?
Conclusion:  
This paper makes a meaningful contribution to the field of open-domain QA by presenting a scalable and generalizable system for answering questions using Wikipedia as the sole knowledge source. While the system demonstrates strong performance and thoughtful design, there is room for improvement in terms of methodological novelty and end-to-end optimization. The work provides a solid foundation for future research in this area.