Review of the Submission
Summary of the Paper  
This paper introduces Knowledge-Guided Structural Attention Networks (K-SAN), a novel extension of Recurrent Neural Networks (RNNs) for natural language understanding (NLU) in dialogue systems. Unlike traditional RNN-based slot-filling models that treat input sequences as flat structures, K-SAN incorporates non-flat network topologies guided by external knowledge (e.g., dependency trees, AMR graphs). The model employs an attention mechanism to focus on salient substructures, enabling improved generalization and robustness, particularly in low-resource settings. Experiments on the ATIS dataset demonstrate that K-SAN outperforms state-of-the-art baselines, achieving significant gains in slot-filling performance.
Main Contributions  
1. Knowledge-Guided Attention Mechanism: The primary contribution is the introduction of a novel attention mechanism that leverages external knowledge to guide the model's focus on linguistically or semantically salient substructures. This approach allows the model to learn robust sentence representations in an end-to-end manner, distinguishing it from prior feature-enrichment-based methods.  
2. Generalization Across Knowledge Sources: The model demonstrates flexibility by incorporating various types of external knowledge (e.g., dependency trees, AMR graphs) without requiring a fixed schema. This generalization capability is validated through experiments with multiple knowledge resources.  
3. Robustness in Low-Resource Scenarios: The proposed model achieves significant performance improvements on small training datasets, highlighting its ability to generalize effectively with limited data. This is a critical advancement for NLU tasks in resource-constrained domains.
Strengths  
1. Innovative Use of Knowledge in Attention Mechanisms: The integration of external knowledge into the attention mechanism is novel and addresses limitations of prior approaches, such as error propagation and poor generalization. The end-to-end learning framework is elegant and well-motivated.  
2. Comprehensive Evaluation: The experimental results are thorough, covering multiple dataset sizes (small, medium, large) and knowledge sources (dependency trees, AMR graphs). The visualization of attention weights provides valuable insights into the model's interpretability.  
3. State-of-the-Art Performance: K-SAN achieves superior F1 scores compared to baseline and structural models across all dataset sizes, particularly excelling in low-resource settings. This demonstrates the model's practical utility and robustness.  
4. Generalization Across Knowledge Types: The ability to seamlessly incorporate different knowledge formats (syntactic and semantic) showcases the model's flexibility and adaptability, which is a significant strength for real-world applications.
Weaknesses  
1. Limited Analysis of Failure Cases: While the paper demonstrates strong quantitative results, it lacks a detailed analysis of failure cases or scenarios where the model underperforms. Understanding these limitations could provide insights for further improvement.  
2. Dependence on Knowledge Quality: The model's reliance on external knowledge raises concerns about its performance when the quality of the knowledge (e.g., dependency parses or AMR graphs) is suboptimal. Although the paper claims robustness, this aspect is not extensively analyzed.  
3. Scalability Concerns: While the paper claims efficiency and parallelizability, the computational overhead of encoding and attending to multiple substructures is not explicitly quantified. This could be a concern for large-scale deployment.  
Questions to Authors  
1. How does the model handle noisy or incomplete external knowledge, and how does this impact performance?  
2. Could you provide more details on the computational efficiency of K-SAN compared to baseline models, particularly in terms of training and inference time?  
3. Have you considered extending K-SAN to other NLU tasks beyond slot filling, such as intent detection or dialogue state tracking?  
Conclusion  
This paper presents a well-executed and impactful contribution to the field of NLU. The proposed K-SAN model is innovative, flexible, and effective, addressing key challenges in sequence tagging tasks. While there are minor concerns regarding scalability and robustness to noisy knowledge, the strengths of the work outweigh these limitations. I recommend acceptance of this submission.