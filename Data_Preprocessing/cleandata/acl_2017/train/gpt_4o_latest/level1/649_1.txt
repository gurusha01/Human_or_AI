Review of the Paper
Summary of the Paper:  
The paper addresses the critical challenge of evaluating dialogue responses in non-task-oriented systems, where existing metrics like BLEU and ROUGE fail to correlate well with human judgments. The authors propose ADEM (Automatic Dialogue Evaluation Model), a learning-based evaluation framework that predicts human-like scores for dialogue responses. ADEM is trained on a dataset of human-annotated appropriateness scores and leverages a hierarchical recurrent neural network (RNN) encoder pre-trained using the VHRED model. The model demonstrates significant improvements in correlation with human judgments at both the utterance and system levels, outperforming traditional word-overlap metrics. Additionally, ADEM generalizes well to unseen dialogue models, making it a promising step toward scalable and accurate automatic evaluation of dialogue systems.
Main Contributions:  
1. ADEM Framework for Automatic Evaluation: The paper introduces ADEM, a novel evaluation model that captures semantic similarity and incorporates both dialogue context and reference responses. This is a significant step beyond word-overlap metrics, addressing their inability to handle response diversity and context dependency.  
2. Generalization to Unseen Models: The authors demonstrate that ADEM can generalize to evaluate dialogue models not seen during training, an essential capability for practical deployment. This is validated through leave-one-out experiments.  
3. Empirical Evidence of Improved Correlation: The paper provides comprehensive experimental results showing that ADEM achieves much higher correlations with human judgments at both the utterance and system levels compared to existing metrics.  
Strengths:  
1. Novel Approach to Evaluation: ADEM's use of hierarchical RNNs and pre-training with VHRED to capture context and semantic similarity is innovative and well-motivated. It addresses key limitations of existing metrics like BLEU and ROUGE.  
2. Thorough Experimental Validation: The authors provide extensive empirical results, including comparisons with multiple baselines, system-level evaluations, and generalization tests. The high Pearson correlation (0.954) at the system level is particularly compelling evidence of ADEM's effectiveness.  
3. Practical Utility: The ability of ADEM to generalize to unseen models and its fast evaluation process make it highly practical for real-world applications, where frequent human evaluations are infeasible.  
4. Open-Source Commitment: The authors' intention to release the ADEM implementation upon publication enhances reproducibility and encourages further research in this area.  
Weaknesses:  
1. Conservatism in Score Predictions: ADEM tends to predict scores closer to the mean, as noted in the qualitative analysis. This limitation may reduce its ability to distinguish between highly appropriate and inappropriate responses effectively.  
2. Bias Toward Generic Responses: The model may favor generic responses, as humans often rate them highly for appropriateness. This bias could hinder the evaluation of more creative or diverse dialogue systems.  
3. Limited Domain Exploration: The experiments are conducted primarily on the Twitter Corpus. While the authors acknowledge this limitation, the lack of evaluation on other datasets (e.g., task-oriented or domain-specific dialogues) restricts the generalizability of their findings.  
Questions to Authors:  
1. How does ADEM perform when evaluating responses in task-oriented dialogue systems or domain-specific datasets?  
2. Could the model's conservatism in score predictions be mitigated by using a different loss function, such as one that penalizes deviations from extreme values less heavily?  
3. Have you considered incorporating adversarial training to address the bias toward generic responses?  
Conclusion:  
This paper presents a significant advancement in the automatic evaluation of dialogue systems, offering a scalable and accurate alternative to traditional metrics. While there are some limitations, such as conservatism in predictions and potential biases, the strengths of ADEM outweigh its weaknesses. The work is well-executed, with strong empirical evidence and practical implications, making it a valuable contribution to the field. I recommend acceptance with minor revisions to address the identified weaknesses.