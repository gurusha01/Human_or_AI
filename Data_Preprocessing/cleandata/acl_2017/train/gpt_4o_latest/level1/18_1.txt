Review
Summary
The paper introduces a novel neural network architecture, the Attention-over-Attention (AoA) Reader, for cloze-style reading comprehension tasks. The AoA Reader introduces an innovative mechanism that places an additional attention layer over document-level attention, creating "attended attention" for final answer predictions. The model is designed to be simpler than existing approaches while achieving superior performance. Additionally, the paper proposes an N-best re-ranking strategy to refine predictions by leveraging linguistic features such as global and local N-gram language models. Experimental results on public datasets, including CNN/Daily Mail and Children's Book Test (CBTest), demonstrate that the proposed method significantly outperforms state-of-the-art models.
Contributions
1. Attention-over-Attention Mechanism: The primary contribution is the introduction of the AoA mechanism, which computes attention weights not only for the document but also for the query. This mutual attention approach improves the model's ability to capture relationships between the document and query, leading to significant performance gains.
2. Simplicity and Efficiency: Unlike prior models that rely on complex architectures or heuristic-based merging functions, the AoA Reader is simpler and computationally efficient while achieving state-of-the-art results.
3. N-best Re-ranking Strategy: The paper proposes a novel re-ranking approach that mimics human double-checking by incorporating linguistic features (e.g., global/local language models and word-class models) to refine predictions, further boosting performance.
Strengths
1. State-of-the-Art Performance: The AoA Reader achieves substantial improvements over existing models, with up to 3.7% absolute gains on CBTest datasets. Its single-model performance rivals or surpasses ensemble baselines, demonstrating its robustness.
2. Novelty of the AoA Mechanism: The attention-over-attention mechanism is a well-motivated and innovative approach that addresses limitations in prior models by explicitly learning the importance of individual attentions.
3. Thorough Evaluation: The paper provides extensive experimental results, including ablation studies, comparisons with baselines, and detailed analyses of document length and candidate frequency. These analyses strengthen the validity of the proposed approach.
4. Generality: The AoA Reader is a general framework that can potentially be applied to other tasks requiring mutual attention between two sequences, as suggested in the conclusion.
5. Re-ranking Strategy: The N-best re-ranking approach is a practical addition that leverages linguistic features to improve predictions, showing a clear and measurable impact on performance.
Weaknesses
1. Limited Novelty in Re-ranking: While the re-ranking strategy is effective, it relies on established techniques (e.g., language models and feature weighting) and does not introduce fundamentally new ideas. Its contribution is incremental compared to the AoA mechanism.
2. Lack of Interpretability: Although the AoA mechanism improves performance, the paper does not provide sufficient qualitative analysis or visualizations to illustrate how the model attends to document-query relationships. This would help validate the claim that the model "comprehends" language rather than functioning as a document-level language model.
3. Dataset Dependence: The evaluation is limited to cloze-style datasets. It remains unclear how well the AoA Reader generalizes to other reading comprehension tasks or datasets with more complex reasoning requirements.
4. Computational Complexity of Re-ranking: While the AoA Reader itself is efficient, the re-ranking step introduces additional computational overhead, particularly with the training of local language models for each test document.
Questions to Authors
1. Can you provide qualitative examples or visualizations of the attention-over-attention mechanism to demonstrate its interpretability and effectiveness in capturing document-query relationships?
2. How does the AoA Reader perform on datasets requiring multi-sentence reasoning or tasks beyond cloze-style reading comprehension?
3. What is the computational cost of the N-best re-ranking strategy, and how does it scale with larger datasets or longer documents?
Recommendation
The paper presents a significant contribution to the field of reading comprehension with its novel AoA mechanism and achieves state-of-the-art results. Despite minor concerns about interpretability and generalizability, the strengths of the proposed approach outweigh its limitations. I recommend acceptance.