- Strengths:
The proposed approach is innovative, and the results are highly encouraging, outperforming the current state-of-the-art.
- Weaknesses:
The linguistic rationale underlying the paper raises concerns (see below). A more nuanced interpretation of the results would significantly enhance the paper.
- General Discussion:
This paper introduces a novel approach to Zero Pronoun (ZP) Resolution in Chinese. The authors propose a unique method for generating a substantial amount of relevant data from unlabeled documents. These data are then effectively incorporated into a neural network-based architecture during the pre-training phase. The results demonstrate improvements over the state-of-the-art.
I have mixed opinions about this work. On the positive side, the approach appears methodologically sound and yields promising results, surpassing recent systems (e.g., Chen & Ng 2016). However, the framing of the primary contribution is problematic from a linguistic perspective. Specifically, (zero) pronoun resolution is inherently a context modeling task, requiring a careful interpretation of discourse/salience, as well as semantic and syntactic cues. It assumes that (zero) pronouns are employed in specific contexts where full noun phrases (NPs) would typically be inappropriate. From this standpoint, generating ZP data by replacing nominal elements with zeroes ("blanks") seems unconvincing. Indeed, as the authors themselves acknowledge, the pre-training module alone does not achieve satisfactory performance. In summary, I am skeptical that these generated pseudo-data can be accurately described as AZP data. It seems more plausible that they encode some form of selectional preferences. It would be beneficial if the authors could dedicate more effort to understanding what the pre-training module is actually learning and revise the relevant sections accordingly.
Additionally, the paper would benefit from proofreading by a native English speaker. For instance, the sentence on lines 064-068 is ungrammatical.
- Other Points:
- Lines 78-79: Are there any constraints on the nouns and, in particular, the pronouns? For example, do you apply this strategy to very common pronouns (such as English "it")? If so, how do you ensure that two occurrences of the same token are indeed coreferential?
- Line 91: The term "antecedent" is conventionally used to refer to a preceding mention that is coreferential with the anaphor, which does not align with your usage here.
- Line 144: Typographical error in "OntoNotes."
- Lines 487-489: It has been established that evaluation on gold-annotated data does not provide a reliable estimate of performance. Indeed, recent studies on coreference resolution evaluate using system mentions. For instance, the Chen & Ng studies you cite include various evaluation setups, such as those based on system mentions. Please consider rerunning your experiments to adopt a more realistic evaluation framework.
- Line 506: The meaning of the dagger symbol over the system's name is unclear. Does it indicate that your improvement is statistically significant across all domains, including BN and TC?
- Line 565: Typographical error in "learn."
- Section 3.3: In this section, you use the abbreviation AZP instead of ZP without introducing it. Please ensure consistency in terminology.
- References: Please verify capitalization in the reference list.