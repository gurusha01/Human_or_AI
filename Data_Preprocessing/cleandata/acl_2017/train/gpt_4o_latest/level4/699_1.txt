This paper introduces an encoder-decoder framework for keyphrase generation. The experimental results demonstrate that the proposed model surpasses other baselines when supervised data is available.
- Strengths:  
The paper is well-structured and easy to understand, with the intuition behind the proposed method clearly explained. It provides sufficient details to facilitate the replication of experiments. While the application of an encoder-decoder framework with a copy mechanism is relatively straightforward, the experimental results are convincing and substantiate the paper's claim regarding the generation of absent keyphrases.
- Weaknesses:  
As noted earlier, the proposed approach lacks novelty. Additionally, as discussed in Section 5.3, the trained model does not generalize effectively to new domains, performing worse than unsupervised models. Although one of the contributions of this paper is ensuring the training corpora are of sufficient quantity and quality, this is not explicitly emphasized.
- General Discussion:  
I found the paper enjoyable to read and would be happy to see it accepted. However, I am curious about how the size and diversity of the training corpus influence the performance of the proposed method. Furthermore, it would be helpful to include the actual values of pg and pc (along with examples from Figure 1) in the CopyRNN model. Based on my experience with the CopyNet, the copying mechanism occasionally behaves unexpectedly, and it is unclear why this occurs.