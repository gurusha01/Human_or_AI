The paper introduces a neural model capable of predicting SQL queries directly from natural language inputs, bypassing the need for an intermediate formal representation. Additionally, it proposes and evaluates an interactive online feedback loop on a limited scale.
- Strengths:
1. The paper is well-written, clearly structured, and appropriately contextualized, making it an enjoyable read.
2. The proposed model demonstrates strong performance across three distinct domains: academic, geographic queries, and flight booking.
3. The online feedback loop is an intriguing and promising concept, despite the limited scale of the experiments conducted.
4. The authors contribute a new semantic corpus and convert two existing corpora into SQL format, which will likely support further research in this area.
- Weaknesses / clarifications:
1. Section 4.2 (Entity anonymization): The rationale behind progressively reducing the span length for querying the search engine is unclear. Could the authors elaborate on the reasoning and methodology behind this choice (line 333)?
2. Section 5 (Benchmark experiments): It appears that the feedback loop (Algorithm 1) is not utilized in these experiments. If this interpretation is correct, it is unclear when data augmentation occurs. Is the annotated training data augmented with paraphrases in its entirety? When is the "initial data" derived from templates incorporated? Is this data also added to the gold training set? If so, it is unsurprising that it provides limited benefit, as the gold queries may already exhibit greater diversity. Clarifying this process would be helpful. Additionally, it would be valuable to report the performance of the "vanilla" model without any augmentation, as this seems to be missing from the paper.
3. Tables 2 and 3: The evaluation metric used is somewhat ambiguous. Does the accuracy metric reflect the correctness of the query's execution (i.e., the retrieved answer), as suggested by line 471? Or does it compare the queries themselves, as appears to be the case for Dong and Lapata in Table 2? If the evaluation methods differ across systems (e.g., Dong and Lapata), how are the results comparable? Furthermore, the text mentions that the SQL model has "slightly lower accuracy than the best non-SQL results" (line 515), but Table 2 shows a nearly 9-point accuracy gap. Was a significance test conducted to support this observation? If not, while the results remain impressive for direct SQL parsing, the wording should be revised, as the performance difference appears significant.
4. Line 519 (Data recombination technique): The data recombination technique from Jia and Liang (2016) seems applicable in this context. Why was it not explored? Is this left for future work, or are there specific constraints preventing its use?
5. Section 6.2 (Three-stage online experiment): Several details are missing or unclear:
   - What was the technical background of the recruited users?
   - Who were the crowd workers, and how were they recruited and trained?
   - The text states, "we recruited 10 new users and asked them to issue at least 10 utterances." Does this mean 10 queries per user (i.e., 100 total) or 10 queries overall?
   - What was the size of the initial synthesized training set?
   - Could the authors provide statistics on the queries, such as lexical variability, length, or SQL complexity? This is particularly important for the first phase, which performs surprisingly well. Additionally, since SCHOLAR uses both SQL and NL, including it in the submission would have facilitated a more thorough review.
6. Section 6.3 (SCHOLAR dataset):
   - The dataset appears small by modern standards (816 utterances in total), especially considering the scalability advantage of the proposed approach. What prevented the creation of a larger dataset?
   - Is it possible to evaluate another baseline on this newly created dataset to compare against the reported 67% accuracy (line 730)?
7. Evaluation of interactive learning experiments (Section 6): The experiments are somewhat difficult to replicate due to their reliance on manual queries from specific annotators. For instance, how can one ensure that annotators in the final phase did not simply ask simpler questions? While this is a common challenge in online learning scenarios, steps could be taken to enable more objective comparisons. For example:
   - Providing query statistics (as mentioned earlier) could help assess whether simpler queries were issued.
   - Introducing a held-out test set might offer a more objective evaluation, though this could be challenging given the model's reliance on seen queries. Scaling up the experiment, as suggested earlier, might mitigate this issue.
   - Exploring whether a different baseline could be assessed using the online learning technique might also be worthwhile, though it is unclear if this is feasible given that prior methods were not designed for online learning.
- Minor comments:
1. Line 48: Replace "requires" with "require."
2. Footnote 1 is overly lengthy. Consider moving some of its content into the main text.
3. Algorithm 1: The term "new utterances" is unclear (presumably referring to new user queries). Adding a caption to the algorithm could improve clarity.
4. Line 218: Replace "Is is" with "It is."
5. Line 278: The term "anonymized utterance" caused initial confusion. If it refers to the anonymization process described in Section 4.2, consider adding a forward reference.
- General Discussion:
Overall, I found the paper compelling and would like to see it accepted at the conference, provided the authors address the questions and concerns raised above.
- Author Response:
I appreciate the authors' detailed responses and encourage them to incorporate these clarifications into the final version of the paper.