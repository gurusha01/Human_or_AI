- General Discussion:
This paper addresses the challenge of predicting missing entities within a given context by leveraging their Freebase definitions. The authors emphasize the significance of this problem, particularly due to the long-tailed distribution of the entities involved. They employ widely-used sequence encoders to represent both the context and the definitions of candidate entities, scoring them based on their similarity to the context. While the importance of the task is evident, and the dataset has potential as a benchmark, the proposed approach exhibits notable shortcomings, and the evaluation leaves several key issues unresolved.
- Strengths:
The task presented in the paper necessitates the integration of external knowledge, and the accompanying dataset could serve as a valuable benchmark for assessing hybrid natural language understanding (NLU) systems.
- Weaknesses:
1) Most of the evaluated models, apart from the best-performing one (HIERENC), lack access to contextual information beyond a single sentence. This limitation seems insufficient for accurately predicting a missing entity. It is unclear whether any efforts were made to address coreference or anaphora resolution. Additionally, it would be helpful to assess how well humans perform on this task for comparison.
2) The choice of predictors across all models is unconventional. It is not clear why the similarity between the context embedding and the entity definition is a reliable measure of the suitability of an entity as a filler.
3) The explanation of HIERENC is ambiguous. Based on my understanding, each input (h_i) to the temporal network is computed as the average of the representations of all possible context instantiations filled by every entity in the vocabulary. This approach seems problematic, as only one of these instantiations is likely correct, which could introduce significant noise into the model.
4) The results provided are not particularly insightful. Since this is a rare entity prediction task, it would be beneficial to report type-level accuracies and analyze how the performance of the proposed models varies with the frequency of entities.
- Questions to the Authors:
1) A key assumption in the paper is that d_e can effectively replace entity embeddings. Was this assumption empirically validated?
2) Have you experimented with training a classifier that uses h_i^e as input features?
I have reviewed the authors' responses. Despite their clarifications, I still believe that the task and dataset would benefit from a human evaluation. This task has the potential to serve as a valuable benchmark for NLU systems, but understanding its difficulty level is essential. Unfortunately, the results presented in the paper do not adequately address this due to the aforementioned concerns. As such, I am not revising my scores.