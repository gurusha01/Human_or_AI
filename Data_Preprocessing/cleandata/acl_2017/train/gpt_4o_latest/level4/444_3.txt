This paper introduces a novel and more comprehensive evaluation methodology aimed at assessing the similarity of automatically generated rap lyrics to those of a target artist. While the evaluation of creative work generation is undeniably a challenging and significant topic for the research community, the proposed methodology falls short of delivering on its claims of providing a comprehensive solution to this problem.
Ultimately, any evaluation of this nature hinges on a subjective measure: can the generated sample convince an expert that it was created by the true artist rather than an automated process? This essentially amounts to a specialized version of the Turing Test. The authors' attempt to automate parts of the evaluation process to facilitate optimization and gain insights into how humans assess artistic similarity is commendable. However, the findings presented in this work do not inspire confidence that these aspects have been reliably identified.
Specifically, consider the core question: Was the sample generated by the target artist? The human annotators tasked with answering this question were unable to provide consistent responses. This inconsistency suggests either (1) the annotators lacked sufficient expertise to perform the task, or (2) the task itself was inherently too difficult, or perhaps a combination of both factors.
Furthermore, the proposed automatic evaluation measures failed to demonstrate reliable agreement with human raters performing the same task. This significantly undermines their utility as a proxy for human assessment. While the low inter-annotator agreement (IAA) may be "expected" given the subjective nature of the task, the decomposition of evaluation into fluency and coherence components was intended to make the task more manageable and improve rater consistency. A low IAA for an evaluation metric raises concerns about its robustness and limits its applicability as a general-purpose tool.
Specific questions and comments:
- Line-by-line vs. verse-level evaluation: Why is a line-by-line evaluation preferred over a verse-level analysis? Particularly for "coherence," a line-by-line approach restricts the evaluation of coherence to consecutive lines, potentially overlooking broader contextual relationships within a verse.
- Style matching terminology: The term "style matching" assumes that each of the 13 artists analyzed has a distinct and consistent style. However, many of these artists (e.g., Kanye West, Eminem, Jay-Z, Drake, Tupac, and The Notorious B.I.G.) have produced work across multiple styles. A more accurate term might be "artist matching."
- Automated evaluation metrics: In Section 4.2, the central automated evaluation components include low tf*idf similarity with existing verses and comparable rhyme density. Given the known limitations of rhyme density as a metric, how effective is this approach, even with the manual interventions described?
- Details on human judges: In Section 6.2, the description of the human evaluation process should include details such as the number of judges involved in the study. Additionally, how often did the judges already recognize the verse they were evaluating? If judges were familiar with the verses, the evaluation would assess their recall and rap knowledge rather than the ease of style matching.