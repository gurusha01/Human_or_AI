This paper introduces evaluation metrics for lyrics generation, emphasizing the need for the lyrics to be original while maintaining a style similar to a specific artist, as well as being fluent and coherent. The paper is well-written, and the motivation behind the proposed metrics is clearly articulated.
The authors propose both manually annotated metrics (fluency, coherence, and style match) and an automated metric for 'Similarity.' While the Similarity metric is novel and intriguing, the paper does not provide sufficient evidence to demonstrate its effectiveness as an automated metric, as the correlations between this metric and the manual ones are low (the authors themselves suggest these metrics should be used independently). Although the authors assert that the Similarity metric can be used to meaningfully evaluate system performance, this claim lacks substantiation, as there is no demonstrated correlation with any manually annotated performance metric. Achieving worse scores than a baseline system does not necessarily validate the metric's ability to capture quality (e.g., the baseline system could be exceptionally strong).
The paper is missing some relevant references, such as recent work on automating coherence using mutual information density (e.g., Li et al., 2015). Additionally, references from the NLG community on style matching are absent, such as Dethlefs et al. (2014) and Pennebaker's work on style matching.