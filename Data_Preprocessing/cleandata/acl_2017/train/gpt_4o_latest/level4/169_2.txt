The paper introduces an innovative method for evaluating grammatical error correction (GEC) systems. This method enables performance assessment by error type, considering both recall and precision, which was previously unfeasible due to the lack of error category annotations in system outputs.
Strengths:
- The proposed evaluation framework is a significant advancement for analyzing GEC system behavior.
- The paper evaluates a diverse range of systems.
- The approach offers several advantages over prior methods:
  - It calculates precision by error type.
  - It operates independently of manual error annotations.
  - It evaluates performance on multi-token errors.
- Human experts largely approve the automatically assigned error tags for pre-computed error spans.
Weaknesses:
- A critical component—the rules for deriving error types—is not described in detail.
- The classifier evaluation lacks a comprehensive error analysis and does not provide directions for improving the classifier based on its shortcomings.
- The evaluation is limited to English, leaving uncertainties about the effort required to adapt the approach to other languages.
---
Classifier and Classifier Evaluation
The basis for the error categories is unclear. Are these categories derived from prior research?
While the approach is generally independent of the alignment algorithm, the rules for error classification might not be. However, the paper does not provide sufficient details on this aspect. Given the centrality of error categories to the paper, readers should at least be provided with examples of rules used to assign error types.
The evaluation is restricted to English, and the paper does not discuss the modifications needed to apply the approach to other languages. It is likely that the rules for determining edit boundaries and error tags, as well as the error categories themselves, would need to be adapted for different languages. However, the authors do not provide any insight into the complexity or number of rules, making it difficult to estimate the effort required for such adaptations.
The pre-processing step produces inherently continuous error spans, which poses challenges for tagging errors that require discontinuous spans. For instance, in German, verbs with separable prefixes are split across the sentence (e.g., [1st constituent] [verb] [other constituents] [verb prefix]). It is unclear whether the classifier can handle such discontinuous spans effectively.
The authors report that human judges rated at least 95% of the automatically assigned error tags as appropriate, despite the noise introduced by automatic edit extraction. However, this claim should be interpreted cautiously. The raters might have been biased by noisy boundaries and system outputs, as they were not asked to assign tags independently of the system's output. Furthermore, the absence of a middle-ground rating option between "Bad (Not Appropriate)" and "Appropriate" might have skewed the results toward "Appropriate." To strengthen the evaluation, the authors should assess how human judges rate classifier outputs when boundaries are manually created, eliminating noise from faulty boundaries.
The classifier evaluation lacks a detailed error analysis. For instance, while the paper mentions that "Bad" ratings are often due to incorrect POS tags, it does not explore when raters selected "Bad" versus "Appropriate" or whether expert ratings suggest ways to improve the classifier.
---
Gold Reference vs. Auto Reference
The paper does not clarify the data used for significance testing. If the test was conducted on F0.5 scores, this is problematic because F0.5 is a derived measure with limited discriminative power—systems with vastly different recall and precision can achieve the same F0.5 score. Additionally, while Section 4.1 mentions mismatches between automatic and reference outputs in terms of alignment and classification, the comparison between gold and reference appears to focus solely on boundaries, not classification.
---
Error Type Evaluation
It is unsurprising that five teams (~line 473) failed to correct unnecessary token errors. For at least two systems, this is easily explained. For example, UFC's rule-based approach relies on POS tags (Ng et al., 2014), which are insufficient for identifying superfluous words. Similarly, Rozovskaya & Roth (2016) explain AMU's poor performance on this error type.
The authors do not analyze or comment on the results in Table 6 regarding whether the systems were designed to handle specific error types. For some error types, there is a straightforward mapping between gold-standard and auto-reference error types (e.g., word order errors). However, it remains unclear whether systems failed entirely on certain error types or were simply not designed to address them. For example, CUUI is reported with precision+recall=0.0 for word order errors, but this system does not target such errors. This suggests a classification error that is neither analyzed nor discussed.
Raw values for TP, FP, TN, and FN should be included in the appendix for Table 6. This would facilitate comparisons using alternative metrics. Additionally, for some error types and systems, the results in Table 6 appear to be based on very few instances. Reporting raw values would make this clearer.
The authors state, "All but 2 teams (IITB and IPN) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement different error types" (line 606). It would be helpful to note that this finding aligns with prior research.
The multi-token error analysis is valuable for future work, but the results require further interpretation. Some systems may be inherently incapable of correcting such errors. Moreover, none of the systems were trained on a parallel corpus of learner data and fluent corrections (as defined by Sakaguchi et al., 2016).
---
Other Comments
- The authors should acknowledge that for some GEC approaches, error annotations were not entirely infeasible before. For instance, systems with submodules dedicated to specific error types could provide such annotations, albeit with modifications. The proposed approach, however, enables a unified comparison of GEC systems that do not naturally produce error-tagged outputs.
- References: Some titles lack proper capitalization. The URL for Sakaguchi et al. (2016) needs to be wrapped. Page numbers are missing for Efron and Tibshirani (1993).
---
Author Response
While the approach is promising, the paper is not yet ready for publication. The lack of detail on the rules for classifying errors is a major limitation. This is not merely a matter of providing additional examples; the rules must be described in sufficient detail to ensure replicability and adaptability.
Generalization to other languages should not be treated as an afterthought. If the approach is inherently limited to English, this would be a serious drawback. Even without performing adaptations for other languages, the paper should provide enough transparency for readers to estimate the effort required for such adaptations and their potential effectiveness. Simply stating that most research focuses on ESL does not address this issue.
The authors claim that the error types targeted by certain systems are "usually obvious from the tables." This is not always the case, as illustrated by the CUUI example and unnecessary token errors. For instance, five systems fail to correct unnecessary token errors (Table 5), yet the paper states, "There is also no obvious explanation as to why these teams had difficulty with this error type." Such inconsistencies should be addressed.