Paraphrased Review - Comments after Author Response
- Thank you for clarifying that the ambiguous reference to the "two-step framework" does not pertain to the two facets. However, I still find the use of a pipeline in this context to lack significant novelty or interest as a contribution.  
- You mention that "5. de Marneffe (2012) used additional annotated features in their system. For fair comparison, we re-implement their system with annotated information in FactBank." However, the feature from de Marneffe et al. cited in the paper, "Predicate Classes," only requires a dependency parser and vocabulary lists from Roser Saurí's PhD thesis. The reference to "general classes of event" might relate to FactML event classes. While their work is admittedly not entirely clear on this point, I am confident they could provide clarification.  
- I still find the phrase "combined properly" to be vague. While I agree that employing LSTM and CNN in contexts where they are most suitable is valuable, your phrasing seems to suggest that prior work was improperly combined and that your method achieves a "proper" combination.  
- Thank you for providing details about using separate LSTMs for each path. I am curious why this approach might be less effective. Regardless, experiments exploring such alternative structures should be reported.  
---
This paper applies deep neural network techniques to the task of factuality classification as defined by FactBank, achieving performance that surpasses other neural network models and baselines re-implemented from prior literature.  
Strengths:  
This paper is well-written and clearly presents a sophisticated model for factuality classification, along with its evaluation. It demonstrates that attentional features and BiLSTM offer clear advantages over alternative pooling strategies, and the model outperforms a traditional feature-based log-linear model. Given the limited training data available in FactBank, the use of a highly-engineered model appears appropriate. It is notable that the BiLSTM/CNN model achieves improvements despite the small dataset.  
Weaknesses:  
My primary concerns with this work pertain to (a) deviations from the evaluation procedures used in prior research, (b) insufficiently strong baselines from prior literature, and (c) limited novelty.  
While I acknowledge the originality of applying deep neural networks to the factuality classification task and recognize the value of this effort, the approach itself does not strike me as particularly novel. Furthermore, the claim of proposing a "two-step supervised framework" (line 087) is not especially compelling, given that FactBank has always been described in terms of two facets (assuming my interpretation of "two-step" as referring to these facets is correct, though I may be mistaken).  
The paper cites Saurí and Pustejovsky (2012) but uses their earlier (2008) and less robust system as a baseline. Additionally, it does not consider Qian et al.'s (IALP 2015) work, which directly compares to the former. Both of these prior studies were developed on the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT TimeBank section, whereas the current work does not report results on a held-out set.  
The de Marneffe et al. (2012) system is also used as a baseline, but not all of its features are implemented. Furthermore, the current system is not evaluated on the PragBank corpus or other alternative representations of factuality proposed in works such as Prabhakaran et al. (*SEM 2015) or Lee et al. (EMNLP 2015). As a result, the evaluation lacks comparability with prior research.  
Several key evaluation questions remain unanswered, such as the impact of using gold-standard events or SIPs.  
Given the well-documented success of BiLSTMs with minimal feature engineering, it is somewhat disappointing that this work does not explore a simpler system using deep neural networks for this task. For instance, one could consider using only the dependency path from a candidate event to its SIP, along with a bag of modifiers for that path. The inclusion of heterogeneous information in a single BiLSTM is an intriguing aspect of the model, which warrants additional experimentation. For example, what happens if the order of inputs is permuted? What if delimiters are used to concatenate the dependency paths in RS instead of the unusual second "nsubj" in the RS chain (line 456)? What if SIPpath, RSpath, and Cue_path are each input into separate LSTMs and then combined? Furthermore, the attentional features are evaluated collectively for the CNN and BiLSTM components, but it would be valuable to report whether these features benefit each component individually. Could the model benefit from providing path information for auxiliary words? Could character-level embeddings help capture the impact of morphology on factuality through tense/aspect?  
The proposed future work section lacks specificity, especially given the many open questions raised by this model and the range of related tasks to which it could potentially be applied.  
General Discussion:  
194: What are the event classes into which events are being classified?  
280: Please specify which parameters are part of the model.  
321: What is meant by "properly"? This term is also used in line 092, but it is unclear which prior work is considered improper and why.  
353: Is "the chain form" defined anywhere? If so, please provide a citation. The repeated "nsubj" in the example on line 456 appears to be an unusual feature for the LSTM to learn.  
356: It might be helpful to note that each cue is classified separately.  
359: Replace "distance" with "surface distance."  
514: How many SIPs and cues are there? Consider adding this information to Table 3.  
Table 2: It would be helpful to include counts for embedded and author events. Percentages can be omitted if necessary.  
532: Why use 5-fold cross-validation? Given the limited training data, 10-fold cross-validation would likely be more informative and would not significantly increase computational costs.  
594: It is unclear whether the reported benefit is attributable to PSen or whether the improvement is statistically significant or substantial. Does it significantly impact overall results?  
674: Does this significance apply across all metrics?  
683: Is the drop in F1 score due to precision, recall, or both?  
686: This sentence is unclear.  
Table 4: Based on the corpus sizes, most columns should be reported with only two significant figures (except for CT+, Uu, and Micro-A).  
711: It is unsurprising that RSpath alone is insufficient, given that the task is defined with respect to a SIP and other inputs do not encode this information. It would be more interesting to see the performance of SIPpath alone.  
761: This claim appears imprecise. To my understanding, de Marneffe et al. (2012) evaluated their system on PragBank, not FactBank.  
Minor Issues in English Usage:  
112: Replace "non-application" with "not applicable."  
145: Replace "relevant" with "relative."  
154: The phrase "can be displayed by a simple source" is unclear.  
166: Do you mean "pipeline" instead of "baseline"?