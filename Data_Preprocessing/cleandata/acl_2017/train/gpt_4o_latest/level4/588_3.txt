- Strengths:  
The paper provides empirical evidence demonstrating the advantages of incorporating external knowledge into NLP tasks.
- Weaknesses:  
While the authors introduce the Rare Entity Prediction task to illustrate the utility of external knowledge, the motivation for proposing this task is insufficiently justified. Why is this task significant, and how would it concretely benefit real-world NLP applications? The paper does not present a compelling argument for the necessity of introducing a new task. In existing reading comprehension tasks, the evidence for correct answers is typically found within the provided text, which aligns with the goal of learning a model of the world (e.g., causality) or basic reasoning. In contrast, Rare Entity Prediction seems less realistic, as humans are generally poor at recalling names. Although the authors argue that the task is challenging due to the large number of rare entities, there are other equally or even more difficult tasks, such as predicting the correct morphological form of a word in morphologically rich languages, which have clear applications in domains like machine translation.
- General Discussion:  
The paper would benefit from a more detailed characterization of the dataset. Based on Figure 1 and Table 4, overlapping entities appear to be a critical feature. For instance, it seems impossible to predict the blank in Figure 1 without encountering the word "London" in Peter Ackroyd's description. This highlights the importance of understanding the dataset's characteristics and the cognitive processes involved in finding the correct answer before relying heavily on neural networks.
Given the lack of dataset characterization, the choice of baselines appears suboptimal. For example, CONTENC seems like a natural baseline at first glance. However, as the authors note, candidate entities are rare, making their embeddings unreliable. Consequently, it is unsurprising that CONTENC performs poorly. Would it be fairer to initialize embeddings using pre-trained vectors from a large dataset? For instance, one might expect some degree of similarity between "Larnaca" and "Cyprus" in the embedding space, which could enable CONTENC to make a correct prediction in Table 4. Additionally, how would TF-IDF+COS and AVGEMB+COS perform if only entities were used to compute the vectors?
From a modeling perspective, the use of a sigmoid predictor to output a numerical score between (0,1) is a thoughtful choice, as it avoids normalization over the list of candidates, which are rare and difficult to assign reliable weights to. However, alternative techniques, such as Pointer Networks, could be considered. For example, an LSTM or BiLSTM could compute a representation \( hi \) for each candidate \( Ci \) (including the blank), and the Pointer Network could provide a probabilistic interpretation \( p(ek|Ci) \propto \exp(\text{dot}(d{ek}, h_i)) \). This approach might serve as a more appropriate baseline. Furthermore, does the imbalance between negative and positive labels affect training? During training, the models make one positive prediction while the number of negative predictions is at least four times higher.
While I find the Rare Entity Prediction task to be somewhat unrealistic, the dataset itself could be leveraged to explore the reasoning process behind correct predictions. For example, it would be interesting to analyze which specific words the model attends to when making its predictions.