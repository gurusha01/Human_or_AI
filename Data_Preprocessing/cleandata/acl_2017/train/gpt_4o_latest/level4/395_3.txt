Strength: The paper introduces the DRL-Sense model, which demonstrates a marginal improvement on the SCWS dataset and a notable improvement on the ESL-50 and RD-300 datasets.
Weakness:  
The technical aspects of the paper raise several issues:  
Could the authors clarify the two drawbacks mentioned in Section 3.2? The first drawback claims that optimizing Equation (2) results in an underestimation of the probability of a sense. Based on my understanding, Equation (2) represents the expected reward for sense selection, where \( z{ik} \) and \( z{jl} \) are independent actions, and there are only two actions to optimize. This optimization should be relatively straightforward. In NLP, optimizing expected rewards over a sequence of actions for episodic tasks has been shown to be feasible (e.g., Sequence Level Training with Recurrent Neural Networks, Ranzato 2015), even in more complex settings like machine translation, where the number of actions is ~30,000 and the average sequence length is ~30 words. The DRL-Sense model involves a maximum of three actions and lacks the sequential nature of reinforcement learning. This makes the claim about the first drawback difficult to accept.  
The second drawback, supported by detailed mathematics in Appendix A, states that the update formula minimizes the likelihood because the log-likelihood is negative. It is worth noting that most standard optimizers (e.g., Adam, SGD, Adadelta) minimize a function \( f \). When maximizing \( f \), a common practice is to minimize \(-f\). Since the reward defined in the paper is negative, any standard optimizer can be applied to the expected negative reward, which is always greater than 0. This approach is common in many modeling tasks, such as language modeling, where negative log-likelihood is minimized instead of maximizing likelihood. Additionally, the authors claim that when "the log-likelihood reaches 0, it also indicates that the likelihood reaches infinity and computational flow on \( U \) and \( V \)" (lines 1046–1049). Why would likelihood → infinity? Should it not be likelihood → 1?  
Could the authors also clarify how DRL-Sense is based on Q-learning? The model's horizon is of length 1, with no transitions between state-actions, and it does not exhibit the Markov property (as \( k \) and \( l \) are drawn independently). I am struggling to see the connection between Q-learning and DRL-Sense. In (Mnih et al., 2013), the reward is provided by the environment, whereas in this paper, the reward is computed by the model. What exactly is the reward in DRL-Sense? Is it 0 for all (state, action) pairs, or is it the cross-entropy in Equation (4)?  
Regarding cross-entropy, it is defined as \( H(p, q) = -\sum{x} q(x)\log q(x) \). Over which variable do the authors sum in Equation (4)? From my understanding, \( q(Ct, z{ik}) \) is a scalar (computed in Equation (3)), while \( Co(z{ik}, z_{jl}) \) is a distribution over the total number of senses (Equation (1)). These two categorical variables do not share the same dimensions. How, then, is the cross-entropy \( H \) in Equation (4) computed?  
Could the authors justify the use of dropout for exploration? Why not employ epsilon-greedy exploration? Dropout is typically used for model regularization to prevent overfitting. How do the authors determine that the observed gain from using dropout is due to exploration rather than regularization?  
The authors state that the Q-value is a probabilistic estimation (line 419). Could you elaborate on the set of variables over which the distribution is defined? When summing over that set of variables, does the result equal 1? I interpret \( Q \) as a distribution over senses per word. However, the definition of \( q \) in Equation (3) does not include a normalizing constant, so I do not see \( q \) as a valid distribution. This also relates to the value 0.5 in Section 3.4, which is used as a threshold for exploration. Why was 0.5 chosen, given that \( q \) is an arbitrary number between (0, 1) and the constraint \( \sum_z q(z) = 1 \) does not hold? Additionally, do the authors allow the creation of a new sense at the very beginning of training or only after a few epochs? I would expect that at the start of training, the model is unstable, and introducing new senses might add noise. Could the authors comment on this?  
General Discussion:  
What is the justification for omitting negative samples (line 517)? Negative sampling has been successfully used in word2vec due to the nature of its task: learning representations. However, negative sampling does not perform well when the primary goal is to model a distribution \( p() \) over senses/words. Noise contrastive estimation is often preferred for distribution modeling. Since DRL-Sense uses collocation likelihood to compute the reward, I am curious about how the approximation presented in the paper impacts the learning of embeddings.  
Would the authors consider task-specific evaluations for sense embeddings, as suggested in recent research [1, 2]?  
[1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims.  
[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks. Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, and Chris Dyer.  
---  
I have reviewed the authors' response.