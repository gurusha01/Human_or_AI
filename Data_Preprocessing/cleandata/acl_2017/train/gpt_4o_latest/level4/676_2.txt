- Strengths:  
  The paper is well-written and includes clear, thoughtfully designed figures. These figures alone provide sufficient clarity for readers to understand the methodology.  
  Predicting binary codes directly is an innovative approach to reducing the parameter space, and the use of error-correction codes is remarkably effective. It is impressive that 44 bits can achieve a BLEU score of 26 out of 31.  
  The parameter reduction technique proposed in this work is complementary to existing methods such as weight pruning and sequence-level knowledge distillation.  
  Additionally, the proposed method is not limited to Neural Machine Translation and can be applied to other tasks involving large output vocabularies.  
- Weaknesses:  
  A notable limitation is that on the relatively large ASPEC dataset, the best proposed model still falls 1 BLEU point short of the softmax model. How would the method perform on even larger datasets, such as the French-English dataset, which contains up to 12 million sentences? Would the performance gap widen further?  
  Similarly, the paper does not provide results for other language pairs, which raises questions about the generalizability of the approach.  
  It might also be worth referencing this paper: [https://arxiv.org/abs/1610.00072](https://arxiv.org/abs/1610.00072), which achieves a 10x speedup in decoding with a BLEU loss of less than 0.5.  
- General Discussion:  
  The paper introduces a parameter reduction method for large-vocabulary softmax. By combining error-corrected codes with a hybrid softmax approach, the BLEU score approaches that of the original full-vocabulary softmax model.  
  One clarification: what is the hidden dimension size of the models? This detail seems to be missing from the experimental setup.  
  Achieving 26 out of 31 BLEU on E2J with just 44 bits is surprisingly effective. However, is there a way to increase the number of bits to enhance classification power? A bit size of 44 seems small, and there appears to be significant potential to use more bits without impacting GPU computation time.  
  Another intriguing aspect is that by predicting binary codes, the model essentially predicts word ranks. How should these bit-embeddings be interpreted? There does not seem to be any semantic relationship among words with odd ranks. Could this be because the model is powerful enough to memorize the data?