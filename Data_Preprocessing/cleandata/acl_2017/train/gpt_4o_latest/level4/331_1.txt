- Strengths:  
The paper provides comprehensive guidelines and clear, detailed illustrations.
- Weaknesses:  
The reliability of document-independent crowdsourcing annotations is questionable.
- General Discussion:  
This study introduces a novel benchmark corpus for concept-map-based multi-document summarization (MDS). The paper is well-structured, and the writing is clear. Additionally, the supplementary materials are thorough and sufficient. However, I have two main questions:  
1) Is it necessary to treat concept map extraction as a standalone task?  
On one hand, many general summarization systems construct similar knowledge graphs as an intermediate step before generating summaries. On the other hand, as the number of nodes increases, the concept map becomes increasingly difficult to interpret. Consequently, general summaries might offer better readability.  
2) How can the importance of a concept be determined independently of the documents?  
Summarization inherently involves preserving the key concepts of the documents, which means the importance of a concept is highly context-dependent. For instance, in the topic of coal mining accidents, consider two concepts: (A) an instance of coal mining accidents and (B) a cause of coal mining accidents. If the document primarily describes a series of coal mining accidents, concept A would be more important than B. Conversely, if the document focuses on the causes of such accidents, concept B would hold greater significance than A. Without the context provided by the documents, it is impossible to accurately assess the relative importance of concepts A and B.  
I commend the authors for their significant effort in constructing this dataset. However, the resulting dataset appears to resemble a common-sense-based knowledge graph more than a true summarization dataset.