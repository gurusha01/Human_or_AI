This paper introduces the gated self-matching network for addressing reading comprehension-style question answering. The proposed solution is structured around three key components:
(a) The authors present a gated attention-based recurrent network designed to generate question-aware representations for the passage. This involves integrating an additional gating mechanism into attention-based recurrent networks to assess the importance of different passage segments and focus on those most relevant to the question. The approach incorporates both word and character embeddings to effectively handle out-of-vocabulary (OOV) words. This component draws inspiration from Wang and Jiang (2016).
(b) The paper then proposes a self-matching attention mechanism aimed at enhancing the representations of both the question and the passage by leveraging a broader passage context, which is critical for inferring the correct answer. This mechanism is a novel contribution of the paper.
(c) Finally, the output layer employs pointer networks to identify the answer boundaries. This aspect is also inspired by Wang and Jiang (2016).
Overall, I find the paper to be a valuable contribution to the field.
- Strengths:
The paper effectively decomposes the network into three components for clarity, relates each component to prior work, and highlights its unique contributions. The authors conduct a thorough empirical analysis, including an ablation study to evaluate the impact of each component. This level of rigor is commendable.
The reported results are impressive!
- Weaknesses:
The paper provides results for both a single model and an ensemble model, but it lacks details about the ensemble's construction. It appears that the ensemble might combine the character-based and word-based models, but this is not explicitly stated. I recommend that the authors clarify this in both the rebuttal and the final paper.
- General Discussion:
In addition to the ablation study, it would be beneficial to include a qualitative analysis showcasing example cases where specific components—such as gating, character embeddings, or self-matching attention—play a critical role. For instance, examples where a simpler model fails to answer correctly but the inclusion of one or more of these components leads to success would be insightful. This analysis could be included as an appendix or supplementary material.