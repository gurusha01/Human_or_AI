- Strengths: The concept of training word2vec-style models using ngrams (specifically bigrams in this case) instead of individual words is highly innovative. The breadth of experimental configurations (four word2vec-style algorithms and multiple word/bigram conditions) is extensive and well-considered. The qualitative analysis of the bigram embeddings is compelling and highlights the potential of this approach for capturing multi-word expressions.
- Weaknesses: The paper would greatly benefit from a review by a native English speaker, particularly to address issues with article usage. Additionally, the placement of the description for the similarity and analogy tasks within the "4.1 Datasets" section feels misplaced and could be better situated elsewhere in the paper.
- General Discussion: As is eventually clarified later in the paper, it would be helpful to state upfront that this work is essentially a generalization of the original word2vec framework, where the concept of a "word" is extended to include ngrams (unigrams and bigrams). Furthermore, it would be valuable to provide a justification for not exploring larger ngrams in the study.