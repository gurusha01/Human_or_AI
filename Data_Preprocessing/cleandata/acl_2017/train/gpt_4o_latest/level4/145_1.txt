Review: Multimodal Word Distributions
- Strengths: Overall, this is a very solid and well-executed paper.
- Weaknesses: The comparisons with related approaches could be expanded.
- General Discussion:
This paper primarily focuses on presenting a novel model for learning multimodal word distributions, where multiple word meanings are represented using Gaussian mixturesâ€”essentially modeling a word as a collection of Gaussian distributions. The proposed method builds upon the work of Vilnis and McCallum (2014), which represented words as unimodal Gaussian distributions. By adopting a multimodal approach, the authors effectively address the challenge of polysemy.
Overall, this is a strong and well-structured paper with clear exposition. The experiments are conducted appropriately, and the qualitative analysis in Table 1 demonstrates results consistent with the proposed approach. There are very few issues to critique, and the following comments are intended to further enhance the paper's clarity and comprehensiveness.
Some comments:
_ It might be helpful to include a brief explanation of how the current approach differs from that of Tian et al. (2014). Both methods decompose single-word representations into multiple prototypes using a mixture model.
_ There are a few missing citations that could be added to the related work section, such as:
Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space - Neelakantan, A., Shankar, J., Passos, A., McCallum. EMNLP 2014  
Do Multi-Sense Embeddings Improve Natural Language Understanding? - Li and Jurafsky, EMNLP 2015  
Topical Word Embeddings - Liu Y., Liu Z., Chua T., Sun M. AAAI 2015  
_ Including the results from these approaches in Tables 3 and 4 could also be valuable.
_ A question for the authors: What do you believe causes the performance drop of w2gm compared to w2g in the SWCS analysis?
I have reviewed the authors' response.