- Strengths:  
  1) The paper addresses an intriguing and relevant task.  
  2) It is written in a clear and accessible manner, making it easy to follow.  
  3) The dataset created as part of this work has the potential to be valuable for other researchers.  
  4) The paper provides a thorough analysis of the model's performance.  
- Weaknesses:  
  1) The paper does not adapt any existing methods from related work for direct result comparison.  
  2) Additional discussion on the uniqueness of the task and the limitations of prior research in addressing this problem could help better highlight the contributions of this work.  
- General Discussion:  
The paper introduces supervised and weakly supervised models for frame classification in tweets. It generates predicate rules by leveraging both linguistic and Twitter-specific behavioral signals, which are then used within a probabilistic soft logic framework to build classification models. The task involves multi-label classification of 17 political frames in tweets. The experimental results underscore the utility of predicates derived from behavior-based signals. Below are my detailed comments:  
The paper would benefit from a discussion on how frame classification is distinct from stance classification. Are these tasks conceptually related but differ in their level of granularity?  
A brief discussion on the challenges introduced by transitioning from long congressional speeches to short tweets would strengthen the paper. For instance, does prior research rely on cross-sentential features that are less applicable to tweets? Additionally, the authors could consider adapting a frame classification method designed for congressional speech (or a stance classification method for any text) to the extent feasible, given the constraints of Twitter data, and compare its results with those of the proposed approach.  
The terms "weakly supervised" and "unsupervised" appear to have been used interchangeably in the paper (if this is not the case, please clarify in the author response). I believe "weakly supervised" is the more accurate term for the setup described in this work and should be used consistently. While the initial data may not have been labeled by human annotators, the classification process does rely on weak or noisy labels, and the keywords are derived from expert input. The method does not align with traditional unsupervised approaches like clustering, topic modeling, or word embeddings, which use completely unlabeled data.  
The calculated Cohen's Kappa may not directly reflect the difficulty of frame classification for tweets (lines: 252-253). Presenting it as proof of this difficulty seems like a strong claim. The Kappa value primarily indicates annotation difficulty or disagreement, which can be influenced by various factors such as poorly designed annotation guidelines, biased annotator selection, or insufficient annotator training, in addition to the inherent difficulty of the task. That said, a Kappa value of 73.4% is sufficiently strong for this task and supports the reliability of the annotated labels.  
Equation (1) (lines: 375-377) does not account for contextual information (e.g., negation or hypothetical/conditional statements) when calculating the similarity between a frame and a tweet. Could this omission affect the performance of the frame prediction model? Did the authors consider alternative approaches that incorporate context, such as skip-thought vectors or vector compositionality methods, to compute similarity over larger text units?  
An ideal experimental setup would exclude annotated data when calculating statistics used to select the top N bi/tri-grams (line: 397 mentions that the entire tweet dataset was used). Otherwise, statistics from test folds (or labeled data in the weakly supervised setup) might inadvertently influence the selection process. While this likely had minimal impact on the current results due to the large size of the unlabeled dataset, adhering to this cleaner experimental setup would improve rigor.  
Please include precision and recall metrics in Table 4 for a more comprehensive evaluation.  
- Minor Comments:  
Please verify the placement of footnotes to ensure consistency with formatting rules, particularly regarding their position relative to punctuation.