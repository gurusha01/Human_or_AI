Review - Summary:
This paper explores the use of a sequence-to-sequence (seq2seq) model for the normalization of German historical texts and demonstrates that incorporating grapheme-to-phoneme generation as an auxiliary task within a multi-task learning (MTL) seq2seq framework enhances performance. The authors claim that the MTL approach eliminates the need for an attention mechanism, providing experimental evidence that attention negatively impacts MTL performance. Additionally, the authors attempt to establish a statistical correlation between the weights of an MTL-based normalizer and an attention-based one.
Strengths:
1) The paper presents a novel application of seq2seq models to historical text normalization, though seq2seq has recently been applied to sentence-level grammatical error correction [1].
2) The study successfully shows that integrating grapheme-to-phoneme generation as an auxiliary task in an MTL framework improves the accuracy of text normalization.
Weaknesses:
1) Rather than asserting that the MTL approach replaces the attention mechanism, the authors should investigate why attention underperforms in the MTL setting and consider modifying the attention mechanism to prevent it from degrading performance.
2) The authors should cite prior work on seq2seq MTL, such as [2] and [3]. Notably, the MTL approach in [2] also utilized non-attention-based seq2seq models.
3) The evaluation is limited to a single German historical text dataset comprising 44 documents. It would be valuable to test the proposed approach on additional datasets or languages to assess its generalizability.
References:
[1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016. Sentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.
[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR'16.
[3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. Multi-task learning for multiple language translation. ACL'15.
---
Response to Authors' Rebuttal:
I am maintaining my review score of 3, indicating that I do not object to the paper's acceptance. However, I am not increasing my score for the following reasons:
1) The authors did not address my concerns regarding prior work on seq2seq MTL models that also avoided using attention mechanisms. As a result, the primary novelty of the paper appears to be its application to text normalization.
2) While it is relatively straightforward to demonstrate that attention mechanisms underperform in an MTL setting, the real contribution would lie in investigating the reasons for this failure and proposing modifications to the attention mechanism to make it effective.