Summary: The paper introduces a neural model designed to predict Python syntax trees from textual descriptions. The model operates by generating tree nodes sequentially in a depth-first manner, guided by Python's grammar. Key contributions include incorporating parent node information into the LSTM input, employing a pointer network for copying terminal symbols, and applying unary closure to collapse chains of unary productions, thereby simplifying the tree structure. The model is evaluated across three datasets from different domains and demonstrates superior performance compared to nearly all prior approaches.
Strengths:
The paper is well-written, with clear explanations of the system and thorough analysis.
The proposed system builds naturally on existing ideas. Closely related works include tree-based generation with parent feeding (Dong and Lapata, 2016) and RNN-based semantic parsing with copy mechanisms (Jia and Liang, 2016; Ling et al., 2016). [Grammar-guided parsing has also been explored in Chen Liang et al., 2016 (https://arxiv.org/abs/1611.00020), where a code-assist system ensures code validity.] However, this paper's model distinguishes itself by generating significantly longer and more complex programs than most of the aforementioned works.
Weaknesses:
The evaluation relies on code accuracy (exact match) and BLEU score, which may not be ideal for assessing program correctness. For example, as shown in the first example in Table 5, while the first two lines in boxes A and B differ, their semantics are identical. Similarly, variable names may differ without affecting functionality. Evaluation based on program behavior (e.g., using test cases or static code analysis) would provide stronger evidence of correctness.
Another evaluation concern is that baseline systems (e.g., NMT) may produce syntactically invalid code. Could the authors include results for the highest-scoring syntactically valid code (e.g., using beam search) generated by these baselines? This would enable a fairer comparison, as these systems could choose to discard malformed outputs.
General Discussion:
* Lines 120-121: Some prior approaches using domain-specific languages also incorporated grammar guidance. For instance, Berant and Liang, 2014, employed a limited grammar for logical forms (Table 1). Comparing to such works while emphasizing that this paper's grammar is significantly larger would strengthen the contribution.
* Lines 389-397: Regarding the parent feeding mechanism, does the model account for the child index? Specifically, is p_t different when generating a first child versus a second child? In Seq2Tree (Dong and Lapata, 2016), different hidden states are used for the two non-terminals.
* Line 373: Are the possible tokens embedded? Does the model assume that the set of possible tokens is predefined?
* The examples in the appendix are well-presented and helpful.
---
I have reviewed the authors' response.