This paper investigates the use of discourse structure, as defined by Rhetorical Structure Theory (RST), to enhance text categorization. A Recurrent Neural Network (RNN) with an attention mechanism is utilized to generate text representations. The experiments conducted on various datasets demonstrate the effectiveness of the proposed approach. Below are my comments:
(1) From Table 2, it can be observed that the "UNLABELED" model outperforms the "FULL" model on four out of five datasets. The authors should provide a more detailed explanation for this result, as it is counterintuitive; incorporating additional relation labels would typically be expected to yield improvements. Could it be that the performance of relation labeling is suboptimal, thereby negatively impacting the overall performance?
(2) The paper preprocesses the RST tree by converting it into a dependency structure. Instead of this transformation, would it be feasible to retain the original tree structure and train a hierarchical model directly on it?
(3) Regarding the experimental datasets, rather than comparing each prior work with only one dataset, the authors might consider conducting experiments on more widely-used datasets from previous studies to provide a more comprehensive comparison.