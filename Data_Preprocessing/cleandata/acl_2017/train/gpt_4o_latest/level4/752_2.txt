The authors employ self-training to develop a seq2seq-based AMR parser, leveraging a small annotated corpus alongside a large volume of unlabeled data. Subsequently, they train a similar seq2seq-based AMR-to-text generator using the annotated corpus and automatically generated AMRs from the parser applied to the unlabeled data. To mitigate data sparsity, they carefully delexicalize named entities in both tasks. This work represents the first successful application of seq2seq models to both AMR parsing and generation, with the generation task likely surpassing the current state-of-the-art.
Overall, I found the approach, experiments, and performance analysis to be well-conceived and compelling. While the methods are not groundbreaking, they are skillfully integrated to achieve practical results. The approach is described in sufficient detail, making it feasible to replicate the experiments without major difficulties. Although some manual effort is still required, I believe this limitation can be addressed in future work, and the authors are moving in a promising direction.
(RESOLVED BY AUTHORS' RESPONSE) A concern raised by another reviewer, which I echo, pertains to potential data overlap between the Gigaword corpus and the Semeval 2016 dataset. This issue is critical—if there is substantial overlap in the test set, it could invalidate the generation results, which are the paper's primary contribution. Unless the authors ensured that no test set sentences were inadvertently included in the training data via Gigaword, the results cannot be deemed reliable.
(RESOLVED BY AUTHORS' RESPONSE) Another issue, also highlighted by another reviewer, involves the 5.4-point improvement claim over a prior system tested on an earlier version of the AMR dataset. While the paper likely still demonstrates state-of-the-art performance, the direct comparison to Pourdamghani et al. is questionable. Why did the authors not test their system on the older dataset version or obtain Pourdamghani et al.'s results on the newer version?
Beyond these points, I have two minor experimental suggestions:
- Statistical significance tests should be conducted, even if the performance gains in generation appear substantial.
- The linearization order experiment should be repeated multiple times with different random seeds to account for potential bias from the specific random order used.
The paper's presentation could benefit from refinement. Certain sections are overly dense, and proofreading by an independent reviewer, preferably a native English speaker, is recommended. The model's details, particularly the improvements over Luong et al. (2015), could be elaborated further—consider adding a figure for clarity. Additionally, the experimental setup omits the vocabulary size used. Most importantly, the paper lacks a formal conclusion, ending abruptly after presenting qualitative results without summarizing the work or discussing future directions.
Minor factual notes:
- Clarify that the JAMR aligner, not the full parser, is used (lines 361–364). Also, confirm whether the recorded mappings are used during parser testing (lines 366–367).
- The non-Gigaword model improves over other seq2seq models by 3.5 F1 points, not 5.4 (line 578).
- In Figure 1, "voters" should be represented as "person :ARG0-of vote-01" in AMR.
Minor writing notes:
- Reword and simplify text in sections near lines 131–133, 188–190, 280–289, 382–385, 650–659, 683, and 694–695.
- Inter-sentential punctuation is occasionally inconsistent with standard English syntax, with both excessive and missing commas.
- Address typos (e.g., lines 375, 615), and ensure all footnotes end with full stops.
- The linearization description (lines 429–433) is redundant and could reference Section 3.3 instead.
- Use brackets instead of commas when referring to algorithms or figures (e.g., lines 529, 538, 621–623).
- Provide references for AMR and the multi-BLEU script.
- Mention the removal of AMR variables in Footnote 3.
- Rename Section 7 to "Linearization Evaluation" for clarity.
- Reorganize Tables 1 and 2 to make it clear which systems belong to the authors, ideally placing them at the bottom. Also, Table 1 appears to list development set scores despite its description suggesting otherwise.
- Clarify the labels in Table 3, as they may confuse readers unfamiliar with the text.
- In Figure 2, the distinction between month names and month numbers is not visually apparent, despite the claim at line 376.
- Ensure proper capitalization in the bibliography for paper titles, abbreviations, and proper nouns (use curly braces in BibTeX to prevent automatic lowercasing).
- Correct the citation for "Peng and Xue, 2017," as it lists only two authors instead of four.
*
Summary:
This paper presents the first competitive results for neural AMR parsing and likely establishes a new state-of-the-art for AMR generation using seq2seq models with innovative preprocessing and the exploitation of a large unlabeled corpus. While the text requires revisions, I found the work to be of high quality and would like to see it accepted at the conference.
(RESOLVED BY AUTHORS' RESPONSE) My primary concerns were the validity of the comparison to prior state-of-the-art in generation and the potential data overlap between Gigaword and the test set, which could undermine the results.
*
Comments after the authors' response:
I appreciate the authors' efforts in addressing the major issues I raised. Their explanations are satisfactory, and I have adjusted my scores accordingly, assuming the final version of the paper will incorporate the discussed revisions.