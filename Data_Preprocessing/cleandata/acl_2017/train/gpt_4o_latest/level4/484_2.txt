The paper explores a synergistic integration of two non-HMM-based speech recognition approaches: CTC and attention-based seq2seq networks. The integration is achieved through two key mechanisms:  
1. First, akin to Kim et al. (2016), multitask learning is employed to train a model using a joint CTC and seq2seq loss.  
2. Second (and the novel contribution), the scores from the CTC and seq2seq models are combined during decoding. Specifically, the results of beam search over the seq2seq model are rescored using the CTC model.  
The primary novelty lies in leveraging the CTC model not only as an auxiliary training objective (as originally proposed by Kim et al., 2016) but also as part of the decoding process.  
- Strengths:  
The paper effectively highlights several challenges arising from the flexibility of the attention mechanism and demonstrates that integrating the seq2seq network with CTC helps address these issues.  
- Weaknesses:  
The work represents an incremental improvement over Kim et al. (2016), as the two models' outputs could already be combined via ensembling. Nonetheless, it is commendable that such a straightforward modification yields significant performance gains for ASR systems.  
- General Discussion:  
A substantial portion of the paper is devoted to explaining well-established, classical ASR systems. The description of the paper's core contribution (an improved decoding algorithm) only begins to appear on page 5.  
The explanation of CTC is unconventional and might benefit from either being presented in a more standard format or expanded for clarity. Typically, the relationship p(C|Z) (Equation 5) is deterministic, with a single unique character sequence corresponding to the blank-expanded form Z. Additionally, the final transformation in Equation 5 is somewhat unclear.