This paper introduces a custom-designed neural network architecture for textual entailment/NLI, structured around a three-step process: encoding, attention-based matching, and aggregation. The proposed model comes in two variants—one leveraging TreeRNNs and the other using sequential BiLSTMs. The sequential variant achieves state-of-the-art performance, and an ensemble combining the sequential and tree-based models performs even better.
The paper is well-written, the model is thoughtfully designed, and the results are impressive. While the contributions are incremental, I still recommend acceptance.
Major points for discussion:
– The paper frequently suggests that the proposed system could serve as a new baseline for future NLI research. However, this claim is not particularly meaningful, as it could apply to almost any model in any domain. While the model might be considered relatively simple or elegant, this does not seem to be its primary strength. 
– The architecture exhibits symmetry in certain aspects that may be excessive. For instance, attention is computed bidirectionally across sentences, and a separate inference composition (aggregation) network is applied for each direction. This likely doubles the runtime of the model. Is this level of symmetry necessary for the inherently asymmetric task of NLI? Have you conducted ablation studies to explore this?  
– Results are provided for the full sequential model (ESIM) and the ensemble of the sequential and tree-based models (HIM). However, the performance of the tree-based model on its own is not reported. Why is this omitted?
Minor points:
– The quote from Barker and Jacobson may not align with the intended meaning. In its original context, it addresses a specific and unresolved issue regarding direct compositionality in formal grammar. A broader statement about the widely accepted principle of compositionality might be more appropriate.  
– The vector difference feature used in the model (also seen in prior work) is somewhat unconventional, as it introduces redundant parameters. Any model that uses vectors `a`, `b`, and `(a - b)` as input to a matrix multiplication is mathematically equivalent to another model that uses only `a` and `b` with a different matrix parameter. While there might be learning-related benefits to including this feature, it would be helpful to discuss this.  
– How are the tree-structured components of the model implemented? Are there significant challenges related to speed or scalability?  
– Typo: (Klein and D. Manning, 2003).  
– Figure 3: Consider using standard tree-drawing tools like (tikz-)qtree, which produce more readable parse trees without crossing lines.
---
Thank you for the response! I continue to support publication. While this work is not groundbreaking, it introduces some novel elements, and the results are sufficiently surprising to add value to the conference.