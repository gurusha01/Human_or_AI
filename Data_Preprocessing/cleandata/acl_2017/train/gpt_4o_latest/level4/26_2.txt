Review
Strengths:  
This paper addresses the domain of knowledge base-based question answering (KB-QA), which involves retrieving results from a structured knowledge base (KB) in response to a natural language query. KB-QA is a significant and challenging area of research.  
The authors effectively outline the contributions and novelty of their work, provide a comprehensive overview of prior research, and present a performance comparison of their method against related approaches.  
Previous neural network-based KB-QA methods represent questions and answers as fixed-length vectors, often treating them as mere bags of words, which limits the expressiveness of these models. Additionally, earlier works do not utilize unsupervised training over the knowledge graph (KG), which could potentially improve the generalization of trained models. This paper introduces two key innovations in addressing the Question Answering problem:  
1) The proposed approach is built on a cross-attention-based neural network architecture, where attention mechanisms are employed to capture different aspects of both questions and answers. The cross-attention model consists of two complementary components. The A-Q attention component dynamically captures various aspects of the question, resulting in diverse embedding representations of the question. Meanwhile, the Q-A attention component assigns distinct attention weights to different parts of the question when computing the Q-A similarity score with the answer aspects.  
2) The answer embeddings are not only learned through the QA task but are also modeled using TransE, which incorporates additional prior knowledge from the KB.  
Experimental results on the WebQuestions dataset demonstrate that the proposed approach outperforms state-of-the-art end-to-end methods. The two contributions are further substantiated through ablation experiments, which show that both the cross-attention mechanism and the integration of global information significantly enhance QA performance.  
The paper is rich in content, and the proposed framework is both innovative and impressive when compared to prior work.  
Weaknesses:  
The paper is well-organized, and the language is clear and accurate. However, there are a few minor typographical errors:  
1. Page 5, column 1, line 421: "re-read" → "reread"  
2. Page 5, column 2, line 454: "pairs be" → "pairs to be"  
General Discussion:  
In Equation 2, the four aspects of candidate answers share the same weight matrix (W) and bias (b). Would it be beneficial to use separate W and b for each aspect?  
Additionally, I recommend assigning a specific name to the proposed approach instead of referring to it as "our approach." For example, a name like ANN or CA-LSTM could work, though it should differ from the terminology used in Table 2.  
Overall, I find the idea of capturing different aspects of question-answer similarity compelling, and the cross-attention-based neural network model offers a novel solution to this task. The experimental results validate the effectiveness of the proposed method. While the overall performance is still weaker than semantic parsing-based methods or some integrated systems, this paper represents a valuable contribution to the end-to-end KB-QA domain and deserves recognition.