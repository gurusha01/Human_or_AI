The paper introduces a novel neural approach for summarization. It extends the standard encoder-decoder with attention framework by incorporating a gating network that selectively processes each encoded hidden state based on summary vectors derived from earlier encoding stages. The proposed method achieves a performance improvement of 1-2 points over standard seq2seq models across three evaluation datasets.
The technical sections of the paper are generally well-explained. However, Equation 16 requires further clarification, as the notation is unclear. The selective mechanism, which is the core contribution of the paper, appears to be innovative and could have broader applicability in other domains.
The evaluation is thorough and demonstrates consistent gains. However, a more appropriate baseline might involve adding an additional encoder layer in place of the selective layer, as this would increase model expressivity (given that the GRU baseline uses only one bi-GRU). This approach seems to align with the implementation in Luong-NMT. A concern remains regarding the LSTM/GRU mismatchâ€”could the observed improvements be attributed solely to the switch to GRU?
The quality of the writing, particularly in the introduction, abstract, and related work sections, is subpar. Since the paper does not represent a significant departure from prior work, placing the related work section closer to the introduction would be more effective. A good practice in related work is to explicitly compare and contrast the proposed approach with prior methods, using descriptive text before delving into equations. Merely listing prior works without connecting them to the current study is not particularly helpful. Moving the related work section nearer to the introduction would also allow the introduction to focus more on high-level concepts rather than being weighed down by excessive background details.