In this study, the authors enhance the MS-COCO dataset by introducing an incorrect caption for each original caption, differing by only a single word. They demonstrate that two state-of-the-art models (one for visual question answering and one for captioning) perform remarkably poorly on three tasks: a) identifying whether a caption is fake, b) pinpointing the incorrect word in a fake caption, and c) selecting an appropriate replacement for the incorrect word.
This work builds on a substantial body of research highlighting the underperformance of vision/language models relative to their perceived capabilities. It addresses fundamental questions in this domain, such as whether vision/language models are performing genuinely "intelligent" tasks. The authors employ a thoughtful combination of tasks and models to explore the limitations of these systems and provide insightful analyses of factors contributing to model failures (e.g., Figure 3).
My primary concern with the paper is its similarity to Ding et al. However, I believe the authors present a compelling case for the novelty of their approach. While Ding et al. generate similar captions, the captions in this work differ by only one word and still cause models to fail catastrophically. This distinction underscores that Ding et al.'s engineering is not a prerequisite, as the simpler approach here achieves similar results, revealing fundamental weaknesses in the models.
Another concern is the reliance on NeuralTalk to select the most challenging foils. While this is a clever approach, it raises the possibility of self-reinforcement bias, as NeuralTalk's inherent biases may become embedded in FOIL-COCO.
I also feel the results section could be expanded relative to the rest of the paper. For instance, I would have appreciated more than a single paragraph in this section, as I found it particularly engaging.
Overall, I find this paper valuable, as it builds on prior work to expose deficiencies in vision/language integration. The similarity to Ding et al. is not a critical issue, in my view. If anything, this work reinforces the idea that vision/language models are so fragile that Ding et al.'s more complex methodology is unnecessary.
Minor suggestions:
- I would have liked to see an additional baseline that combines bag-of-words (BoW) features with extracted CNN features and trains a softmax classifier. While the "blind" model is a nice addition, a "simple" vision+language baseline might perform comparably to the LSTM/Co-attention model. Including this could strengthen the paper's argument.
- Line 330: What is meant by "supercategory"? Is this derived from WordNet or COCO? While the concept is clear, the specifics are not.
- Line 397: Replace "has been" with "were."
- Line 494: Replace "that" with "than."
- Line 693: Consider revising "artefact" to "undesirable artifacts."
- Line 701: I recommend including a chance baseline in Table T1. Is the 19.53% figure [Line 592] a constant-prediction baseline? Is it 50%? If so, could flipping all "blind" predictions yield a better baseline? Clarifying this with a "chance" line would address much of the confusion.
- Line 719: Typo in "ariplane" (should be "airplane").
~~  
After reviewing the author response...
The author response effectively addresses my concerns regarding NeuralTalk biases and additional baselines. I am confident these issues can be resolved in the final version, so I will maintain my current score.