Strengths:
This paper introduces an approach for generating concept maps through crowdsourcing. The overarching ideas are compelling, and the primary contribution lies in the dataset's creation. I anticipate that this dataset will serve as a valuable asset for future research in this domain. It is evident that significant effort has been devoted to this work.
Weaknesses:
Overall, I found parts of the paper somewhat overstated. For instance, the authors identify a novel crowdsourcing scheme as one of their contributions. However, this claim feels rather exaggerated, as the work appears to apply established best practices in crowdsourcing rather than introducing a genuinely novel method. It is more of a thoughtful and robust application of existing techniques than a groundbreaking innovation.
Similarly, the authors assert that they have developed and presented a new corpus. While this claim holds merit, and it is clear that substantial effort went into its creation, Section 4.1 clarifies that the corpus is actually derived from an existing dataset.
These points are more about the way the work is presented than about the work itself.
General Discussion:
Where do the summary sentences for the crowdsourcing task originate? Aren't they still quite subjective?
What is the source of the clusters? Are they derived from the TAC2008b dataset?
In Section 4.6, expert annotators are employed to construct the gold standard concept maps. This section seems pivotal, yet it lacks sufficient detail. How were the annotators trained, and what qualifications made them experts?