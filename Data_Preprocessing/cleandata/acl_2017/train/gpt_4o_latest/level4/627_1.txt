This paper introduces a dialogue agent where the belief tracker and dialogue manager are jointly optimized using the REINFORCE algorithm. The system learns through interactions with a user simulator. Training is conducted in two phases: an initial imitation learning phase, where the system is initialized via supervised learning from a rule-based model, followed by a reinforcement learning phase, during which the system is jointly optimized using the RL objective.
- Strengths: The paper integrates differentiable access to the knowledge base (KB) within the joint optimization framework, which stands out as its most significant contribution.
- Weaknesses: First, the system cannot be considered fully end-to-end, as the response generation component is handcrafted rather than learned. Additionally, the end-to-end model overfits to the simulator and performs poorly in human evaluations. This raises concerns about whether the paper is primarily advocating for end-to-end learning or the soft-KB access mechanism. While the soft-KB access consistently improves performance, the benefits of end-to-end learning are less convincing. The authors attempt to demonstrate the value of end-to-end learning in Figure 5, but the distinction remains unclear. Furthermore, the paper does not adequately justify the use of the REINFORCE algorithm, which is known to suffer from high variance. The authors did not address this issue by incorporating a baseline or exploring alternative approaches, such as the natural actor-critic algorithm, which is generally more effective.
- General Discussion: Despite the aforementioned weaknesses, the experiments are robust, and the paper is generally acceptable. However, the paper would be stronger if it focused on the aspect that demonstrably improves performance—the soft-KB access—rather than emphasizing end-to-end learning, which is less impactful.