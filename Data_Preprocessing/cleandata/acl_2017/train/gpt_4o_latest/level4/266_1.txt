This paper evaluates various approaches to inducing embeddings for polarity classification. The authors investigate the impact of different corpora types and demonstrate that the largest corpus is not always the most effective for this task. Instead, embeddings derived from corpora (or subcorpora) with a higher concentration of subjective content—referred to as "task-specific data"—yield better results. Additionally, the paper explores embeddings that combine information from both "task-specific" and generic corpora, showing that such combinations outperform embeddings derived from a single corpus. These findings are validated not only for English but also for a low-resource language, Catalan.
- Strengths:  
The paper addresses a critical aspect of sentiment analysis: how to effectively induce embeddings for training supervised classifiers in polarity classification. It is well-structured and clearly written. The authors provide sufficient experimental evidence to support their primary claims.
- Weaknesses:  
The experimental outcomes are largely predictable. The methods employed are straightforward and ad hoc, offering little in terms of novel contributions. The central idea—focusing on task-specific data to train more accurate embeddings—has already been explored in prior work, such as Joshi et al. (2015) in the context of named-entity recognition. The contributions of this paper are incremental and lack significant innovation.  
Some experiments appear inconclusive due to the absence of statistical significance testing between classifiers. For example, in Tables 2, 3, and 6, several classifier configurations yield very similar scores. Statistical significance testing is essential to determine whether these differences are meaningful. For instance, in Table 3 (left side, RT results), it is unclear whether the "Wikipedia Baseline" differs significantly from any of the combinations or whether there is a meaningful difference between the combinations themselves (e.g., "subj-Wiki," "subj-Multiun," and "subj-Europarl").  
While the improvement achieved by focusing on subjective subsets is plausible, its practical utility in resource-scarce scenarios is questionable. The pre-selection step using OpinionFinder, as described, is not feasible for most languages other than English due to the lack of equivalent tools or fine-grained datasets. This limitation is evident in the Catalan experiments, where such information is not utilized.  
- Minor Details:  
  - Lines 329–334: The discussion of this dataset is unclear. The task appears to be plain polarity classification, yet the authors mention "opinion holder" and "opinion targets." If these aspects are irrelevant to the experiments, they should be omitted.  
  - Lines 431–437: The motivation for the "splicing" variation is unclear. Why is this approach necessary, and how does it improve upon simple "appending"?  
  - Lines 521–522: How is subjective information isolated in these configurations? It seems OpinionFinder is used again, but this is not explicitly stated.  
  - Lines 580–588: The variable definitions do not align with Equation 3. For instance, the variable \( n_k \) is not present in the equation.  
  - Lines 689–695: Similar to lines 329–334, it is unclear whether opinion holders and targets are considered in the task.  
After Authors' Response:  
Thank you for your clarifications. However, I still find the explanation regarding the incorporation of opinion holders and targets unclear.  
Overall, I maintain my initial assessment. This work lacks sufficient novelty, and the points raised in the authors' response do not address this concern adequately. The submission remains too incremental in its contributions.