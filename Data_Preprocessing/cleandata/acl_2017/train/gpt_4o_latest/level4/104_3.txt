- Strengths:  
The paper introduces compelling ideas, employs a straightforward neural learning approach, demonstrates interesting performance (albeit not groundbreaking), and supports a wide range of applications.
- Weaknesses:  
Limited amount of novel contributions. Lack of clarity in certain sections.
The paper proposes a neural learning framework for entity disambiguation and linking. It presents an innovative approach by integrating entity, mention, and sense modeling within a unified neural language modeling framework. The simplicity of the training process, combined with the modeling approach, enables support for a broad range of applications.
While the paper is formally clear, the quality of discussion does not consistently match the strength of the technical ideas.
The empirical evaluation is solid, though it does not report significant performance improvements. Although the work appears to build upon (Yamada et al., CoNLL 2016), it incorporates novel ideas and is of notable interest.
The main weaknesses of the paper are as follows:  
- The writing is not consistently clear. For instance, Section 3 lacks clarity. Some aspects of Figure 2 are insufficiently explained, and the terminology is occasionally redundant. For example, why are terms like "dictionary of mentions" and "dictionary of entity-mention pairs" used? Are these distinct from text anchors or annotated text anchor types?  
- The paper is closely related to (Yamada et al., 2016), and the authors should explicitly highlight the differences.
A broader observation regarding the current version of the paper is:  
The proposed Multiple Embedding model is evaluated on entity linking and disambiguation tasks. However, word embeddings are not exclusively used for such tasks; they are also applied in other areas that do not directly involve knowledge base entities, such as parsing, coreference resolution, or semantic role labeling.  
The authors should demonstrate that the word embeddings generated by the proposed MPME method are not inferior to simpler word embedding spaces when applied to these other semantic tasks, particularly those that involve entity mentions indirectly.
I have reviewed the authors' response.