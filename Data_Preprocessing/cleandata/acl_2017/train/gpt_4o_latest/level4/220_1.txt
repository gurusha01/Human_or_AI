- Strengths: This is an excellent paper: it is exceptionally well-written, presents intriguing results, employs a creative methodology, and offers meaningful and illuminating comparisons with prior approaches. Additionally, the meticulously annotated corpus will undoubtedly serve as a valuable resource for other researchers in the field. I particularly appreciated the qualitative discussion in Section 5. Many machine learning papers tend to merely present a results table without much elaboration, but the discussion in this paper provides genuine insights for the reader.
- Weaknesses: In Section 4.1, the statement "The rest of the model's input is set to zeroes..." is somewhat unclear until one examines Figure 2. Adding a brief explanatory sentence here would enhance clarity. Moreover, in Figure 2, the input layers to the LSTMs are labeled as "5*Embeddings(50D)," even for the networks that process dependency labels. This seems incorrectâ€”or, if it is accurate, further clarification is needed to explain what is meant.
- General Discussion: Regarding the remark in Section 4.2, "LSTMs are excellent at modelling language sequences ... which is why we use this type of model," this statement strikes me as odd. The problem at hand is not sequential in the conventional sense. For each data point, all five words in an example are fed to the network simultaneously, and there is no dependency between consecutive examples. While the LSTM architecture may indeed be the best choice, it would not be for the reason stated. Alternatively, have I misunderstood something? I would be interested in hearing the authors' clarification on this point.