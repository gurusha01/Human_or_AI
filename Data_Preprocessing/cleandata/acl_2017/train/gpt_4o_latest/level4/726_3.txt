This paper introduces a method for training a semantic parser using an encoder-decoder neural architecture, with the notable feature that the semantic output consists of complete SQL queries. The approach is evaluated on two standard datasets (Geo880 and ATIS) and a novel dataset focused on document search.
The paper is well-executed and presents a solid contribution, leveraging a relatively established technique—an encoder-decoder model with enhancements such as data augmentation through paraphrasing—to generate SQL queries. The authors argue that SQL is more expressive than other semantic formalisms commonly used in the field and that SQL queries can be edited by untrained crowd workers familiar with SQL but not semantic parsing. While I agree that SQL is indeed more expressive, I am left wondering whether the datasets used in the experiments truly highlight this expressivity. Specifically, are there any queries in the three datasets where standard semantic formalisms fail to capture the full semantics of the query? If not, are these datasets the most appropriate to showcase SQL's advantages? Additionally, regarding the model's learning, what proportion of SQL's expressivity does it actually utilize? How much of SQL's potential does the model capture, and does it exhibit biases in the types of queries it generates? I would have appreciated a deeper exploration of not just the potential expressivity of SQL, but the actual expressivity captured by the model, particularly in the context of the datasets used. Relatedly, in Section 5, the authors claim that directly producing SQL is harder but do not provide evidence to support this. This assertion seems more about SQL's expressivity than its practical difficulty, which again raises the question of how much of SQL the model is actually generating.
Furthermore, I would have liked to see a more detailed analysis of the types of SQL queries generated by the model, especially for the SCHOLAR dataset in the second part of the evaluation. For example, when a query is ill-formed, in what specific ways is it ill-formed? When crowd workers are required to post-edit queries, how much effort does this entail? Similarly, how accurate are the crowd workers at constructing SQL queries? It seems unlikely that they consistently produce perfect queries, given the complexity of the task. Along similar lines, I would have appreciated error analysis and agreement metrics between annotators, particularly for Incomplete Result queries, which appear to rely heavily on pre-existing knowledge and could therefore be quite subjective.
Overall, the paper achieves impressive results and is well-executed. However, I would have liked to see more insights into the model's ability to generate SQL, including a clearer understanding of the subset of the language it captures.
A few minor points:
- Line 107: The phrase "non-linguists can write SQL" seems unnecessary. Most linguists would also struggle to write SQL queries. A better phrasing might be "annotators without specific training in the semantic translation of queries" can perform the task.  
- Line 218: Replace "Is is" with "It is."  
- Line 278: The term "anonymized utterance" is unclear at this point in the paper and should be clarified.  
- Line 403: Am I correct in understanding that you paraphrase only single words at a time? Do you exclude "entities" from paraphrasing?  
- Line 700: Consider introducing a visual variable, such as line type, to differentiate the three lines for readers viewing the figure in grayscale.  
Finally, there are inconsistencies in the references, including casing issues (e.g., "freebase," "ccg"). Wang et al. (2016) is missing critical publication details, and there is an "In In" error in the citation for Wong and Mooney (2007).