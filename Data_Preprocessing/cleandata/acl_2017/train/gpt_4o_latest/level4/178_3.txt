The authors propose a method for jointly embedding words, phrases, and concepts using plain text corpora and a manually constructed ontology, where concepts are represented by one or more phrases. Their approach is applied in the medical domain using the UMLS ontology and in the general domain using the YAGO ontology. To evaluate their method, the authors compare it against simpler baselines and prior work, primarily using intrinsic similarity and relatedness benchmarks. They leverage existing benchmarks for the medical domain and create a new general-domain concept similarity and relatedness dataset using mechanical turkers, which they plan to release. Their results are reported to be comparable to prior work.
Strengths:
- The proposed joint embedding model is intuitive and conceptually reasonable. Its primary value lies in achieving a configurable balance between treating phrases as atomic units and considering their compositionality. Similarly, the approach extends to concepts, which are treated as being composed of multiple representative phrases.
- The paper encompasses a substantial amount of work, including model development, the creation of a new evaluation dataset, and multiple evaluations and analyses.
Weaknesses:
- The evaluation focuses solely on intrinsic tasks, primarily using similarity and relatedness datasets. As the authors acknowledge, such evaluations are limited in their ability to predict the utility of embeddings in extrinsic tasks. It is now standard practice to include at least one or two extrinsic tasks in embedding model evaluations.
- The similarity and relatedness datasets are described as capturing human judgments of concept similarity. However, it appears that the judgments were based on phrases presented to annotators, making these phrase similarity datasets rather than concept similarity datasets. This distinction should be explicitly addressed.
- The medical concept evaluation dataset, "mini MayoSRS," is very small (29 pairs), while its larger superset, "MayoSRS," is only slightly larger (101 pairs) and has been reported to exhibit low inter-annotator agreement. The other medical dataset, "UMNSRS," is more reasonable in size but is limited to single-word concepts, which were presented as such to annotators. This limitation should be noted in the paper, as it raises questions about the dataset's relevance for evaluating representations of phrases and general concepts.
- The authors extensively fine-tune their hyperparameters on the same datasets used for reporting results and comparisons with prior work. This practice undermines the credibility of the reported results and analyses.
- The claim that their method outperforms prior work by achieving comparable results with less manual annotation is unconvincing. The authors rely on large manually constructed ontologies, and the manually annotated dataset used in prior work is derived from existing clinical records, which did not require dedicated annotations.
- The paper lacks deeper insights into the underlying mechanisms behind the reported results. The authors attempt to treat the relationship between a phrase and its component words and the relationship between a concept and its alternative phrases as analogous compositional relations. However, these relationships differ fundamentally and warrant separate analyses. For instance, around line 588, a more detailed NLP analysis of the relationship between phrases and their component words would be valuable. The observed behavior may be influenced by dominant headwords in phrases, among other factors. Additionally, an investigation into the impact of hyperparameters controlling the balance between atomic and compositional views of phrases and concepts would strengthen the work.
General Discussion:
Given the aforementioned weaknesses, I recommend rejecting this submission. I encourage the authors to enhance their evaluation datasets and methodology before resubmitting the paper.
Minor Comments:
- Line 069: Replace "contexts" with "concepts."
- Line 202: Clarify how phrase overlaps are handled.
- Line 220: The dimensions should be |W| x d. Additionally, the term "negative sampling matrix" is misleading, as these embeddings are also used to represent contexts in positive instances.
- Line 250: The phrase "the observed phrase just completed" is unclear. It is not evident how words are trained in the joint model. The text seems to suggest that only the last words of a phrase are considered as target words, which is illogical.
- Equation 1: The notation is confusing; consider using "o" instead of "c."
- Line 361: The reference to Pedersen et al. 2007 is missing.
- Line 388: Using a fine-grained similarity scale (1-100) for human annotations seems unusual.
- Line 430: The term "strings" is introduced here but is confusing. It would be better to continue using "phrases."
- Line 496: Specify which task was used for hyperparameter tuning. This information is critical and is not provided, even in the appendix.
- Table 3: The trends are difficult to discern. For example, PM+CL behaves differently from PM or CL alone. Including development set trends for these hyperparameters would be insightful.
- Line 535: Add a reference to Table 5.