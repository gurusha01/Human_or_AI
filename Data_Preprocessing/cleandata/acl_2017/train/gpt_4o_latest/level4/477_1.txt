Paraphrased Review
Strengths:
i. The paper provides a clear and well-articulated motivation.  
ii. It includes comprehensive comparisons across multiple models and languages, showcasing diversity.
Weaknesses:  
i. The conclusions are influenced by the specific selection of languages.  
ii. The experiments do not fully substantiate the paper's overarching claims.
General Discussion:  
This paper addresses a straightforward yet essential question in word representation: which subunits of a word are most appropriate for capturing morphological characteristics, and how should these units be composed? To investigate this, the authors experimented with word representations based on different subunits (characters, character-trigrams, and morphs) and composition methods (LSTM, CNN, and simple addition) within the context of a language modeling task. The study spans over ten languages, chosen to reflect typological diversity, as the effectiveness of word representations and composition methods may vary across languages. Based on the experimental results, the paper concludes that character-level representations are generally more effective, though they remain suboptimal compared to models incorporating explicit morphological knowledge. Additionally, the authors find that character-trigrams yield consistent perplexity performance across most languages.
Nonetheless, the paper leaves several issues unresolved:  
- Firstly, there appears to be a potential selection bias in the choice of experimental languages. The authors selected ten languages across four categories, with up to three languages per category. However, a key question arises: how can it be asserted that these languages are representative of their respective categories? Do all languages within the same typological category exhibit similar tendencies in terms of word representation and composition methods? The results presented in the paper suggest otherwise, as two agglutinative languages from the same typology demonstrate differing outcomes. Consequently, it might be more prudent to focus on the specific languages tested in this study rather than making broad generalizations about all languages.  
- Secondly, there is a disconnect between the paper's claims and its experimental scope. Is language modeling truly the most appropriate task to validate the claims made in this study? Could the conclusions drawn here fail to hold in the context of other tasks? Further elaboration on this point would strengthen the paper.  
- Thirdly, in Section 5.2, the proposed method is evaluated exclusively on Arabic. Why was Arabic chosen as the sole language for this experiment? There are other languages, such as Japanese and Turkish, that also have automatic morphological analyzers and could have been included for a more comprehensive evaluation.  
- Finally, the paper only considers character-trigrams among various n-gram representations. What motivated the exclusive focus on character-trigrams? Are they consistently superior to character-bigrams or character-fourgrams? It is worth noting that the performance of n-gram-based language modeling can be influenced by factors such as corpus size and other contextual elements.
Minor Typos:  
- A reference is missing in the Introduction (line 88, Page 1).  
- "root-and-patter" should be corrected to "root-and-pattern" (line 524, Page 6).