The paper presents a general approach for enhancing NLP tasks by leveraging embeddings derived from language models. While context-independent word representations have proven highly effective, this work offers a compelling extension by utilizing context-dependent word representations extracted from the hidden states of neural language models. The authors demonstrate substantial performance gains in tagging and chunking tasks through the incorporation of embeddings from large language models. Additionally, the paper includes intriguing analyses that address several natural questions.
Overall, this is a strong paper, though I have a few recommendations for improvement:
- There is an over-reliance on test set experiments. I suggest revising Tables 5 and 6 to report results using development data instead.
- It would be valuable to evaluate the method on a broader range of tasks. NER tagging and chunking do not exhibit many interesting long-range dependencies, where the language model's capabilities might shine. Results on tasks like SRL or CCG supertagging would be particularly insightful.
- The paper asserts that a task-specific RNN is essential because a CRF applied to language model embeddings performs poorly. However, it was unclear whether the language model embeddings in this experiment were fine-tuned through backpropagation. If they were not, it seems plausible that fine-tuning could eliminate the need for a task-specific RNN.