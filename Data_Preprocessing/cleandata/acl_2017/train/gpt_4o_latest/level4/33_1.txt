Strengths:
- The paper introduces an innovative approach by incorporating sentiment information through regularization.  
- The experimental setup appears to be technically sound and well-executed.  
- The analysis of the model is detailed and provides valuable insights.  
Weaknesses:
- The approach is conceptually very close to distant supervision.  
- The baselines used for comparison are largely underwhelming and not well-informed.  
General Discussion:
This paper proposes an extension to the vanilla LSTM model by integrating sentiment information via regularization. The introduction outlines the main claims of the paper: CNN-based methods perform poorly without phrase-level supervision, and phrase-level annotation is costly. The authors instead propose a "simple model" that leverages other linguistic resources.  
The related work section provides a solid overview of sentiment analysis literature. However, it lacks references to prior work on linguistic regularization, such as [YOG14].  
The explanation of the regularizers in Section 4 is overly verbose and repetitive. The listing on page 3 could be consolidated with subsections 4.1â€“4.4 for better readability. The notation in this section is inconsistent and difficult to follow. For instance, the parameter \( p \) alternates between subscript and superscript usage, and \(\beta\) is not explicitly defined in the text. Furthermore, the term "position \( t \)" is ambiguous. While \( t \) is a parameter of the LSTM output and appears to represent the index of a sentence, the description of the regularizers refers to preceding words rather than sentences. This suggests that \( p_t \) may be overloaded to represent either the sentiment of a sentence or a word, but this distinction needs to be clarified in the text.  
A significant concern is that the paper blurs the line between regularization and distant supervision. There are alternative methods to incorporate lexical information, such as polarity or negation, into a model (e.g., by embedding this information into features). The comparison against Teng et al.'s NSCL model is problematic without clarity on whether the same lexicons were used in both cases. If different lexicons were employed, the comparison would be unfair. Additionally, it is unclear why the authors did not evaluate NSCL on the MR dataset, as this should simply involve swapping datasets. The remaining baselines lack the use of lexical information, making them relatively weak. A stronger baseline, such as a vanilla LSTM with lexical information appended to word vectors, would have been more appropriate.  
The paper concludes with a useful analysis of the model's behavior, demonstrating that it learns aspects like intensification and negation to some degree. However, it would be insightful to evaluate how the model handles out-of-vocabulary (OOV) words relative to the lexicons. This would help determine whether the model generalizes beyond memorization. Additionally, the figures and tables are too small to be legible in print, which detracts from the presentation.  
The paper is generally well-written, though it would benefit from proofreading to address grammatical errors and typos. The abstract, in particular, is difficult to follow at the beginning.  
Overall, the paper explores a reasonable research direction. The primary concern lies in the somewhat weak comparison to related work, which could be addressed by including stronger baselines. Establishing the comparability of the experiments is crucial, and I hope the authors can provide more clarity on this in their response.  
[YOG14] http://www.aclweb.org/anthology/P14-1074  
---
Update After Author Response:  
Thank you for addressing the concerns regarding the experimental setup.  
- NSCL: I now believe that the comparison with Teng et al. is fair.  
- LSTM: It is reassuring to know that this baseline was included. However, this aspect is critical to the paper. The baselines remain weak, and the reported marginal improvement is insufficiently detailed. A more robust comparison, including significance testing, would strengthen the results.  
- OOV Words: While I understand the model's definition, it would be valuable to explore its behavior with OOV words. This would provide a more compelling experiment than the current focus on regularization.