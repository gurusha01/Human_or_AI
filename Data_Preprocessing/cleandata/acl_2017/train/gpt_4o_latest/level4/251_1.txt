This paper explores the mathematical underpinnings of the skip-gram model, shedding light on its effectiveness in analogy tasks and the broader success of additive composition models. Additionally, it draws a connection between skip-gram and Sufficient Dimensionality Reduction (SDR).
I appreciated the paper's focus on elucidating the properties of skip-gram and found it generally thought-provoking. The effort to analyze the model's assumptions and their interaction with the composition operations it facilitates is commendable. In this regard, I believe the paper offers valuable insights to the community.
However, my primary critique concerns the paper's linguistic treatment, which I found somewhat simplistic. The authors' interpretation of 'compositionality'—as an operation that combines a set of words to yield another with equivalent meaning—is unusual. Two words can certainly combine to form a vector that: a) is distant from both; b) does not correspond to any other concept in the space; and c) still retains meaning (a prerequisite for linguistic productivity). In linguistic terms, compositionality refers to the process of combining linguistic units to create higher-level constructs, without imposing additional constraints beyond a general (and debatable) notion of semantic transparency. Furthermore, the claim on line 254 that composition operates over sets is incorrect; word order is crucial (e.g., 'sugar cane' is not equivalent to 'cane sugar'). This limitation of additive composition is well-documented.
Another significant issue is the neglect of pragmatic factors that influence human preference for phrases over single words (and vice versa) in specific contexts, thereby altering word distributions in large corpora. For instance, using 'male royalty' instead of 'king' or 'prince' often reflects the speaker's intent (e.g., emphasizing gender distinctions). Consequently, the equation on line 258 (and the KL-divergence modification) does not hold—not due to noise in the data, but because of intrinsic linguistic processes. While this might be addressed in the SDR section, I am uncertain (see comments below).
In summary, while I find the authors' presentation of composition flawed, the paper convincingly demonstrates that this is indeed how skip-gram operates, which I consider a noteworthy contribution.
The discussion on Sufficient Dimensionality Reduction feels somewhat disconnected from the earlier arguments. I struggled to fully grasp this section and would appreciate clarification in the authors' response. If I understand correctly, the claim is that skip-gram generates representations where a word's neighbors follow an exponential parameterization of a categorical distribution. However, it is unclear whether this reflects the corpus distribution (as opposed to, say, a count-based model). The paper suggests that skip-gram's success lies in implementing SDR, which does not rely on assumptions about the data's underlying form. If so, would it be accurate to say that the resulting representations are optimized for tasks emphasizing geometric regularities, irrespective of the corpus's actual distribution? In other words, is there an implicit denoising effect at play?
Minor Comments:
- The abstract is unusually lengthy and could benefit from being more concise.
- Paragraph starting at line 71: It seems misleading to interpret circularity here. Firth's observation that co-occurrence correlates with similarity judgments aligns with the cognitive processes statistical methods aim to model. Co-occurrence effects and vector space representations are essentially modeling the same underlying linguistic phenomena, for which we lack direct observations. Pairwise similarity is not meant to resolve circularity but rather to better model human judgments.
- Line 296: The term 'paraphrase' might be more appropriate than 'synonym,' as the comparison involves a set of words and a unique lexical item.
- Paragraph starting at line 322: The discussion is intriguing, and indeed, much of the Zipfian distribution's long tail exhibits relative uniformity.
- Line 336: It may be worth noting that analogy relations often fail to hold perfectly in practice, requiring the exclusion of the first returned neighbor (typically one of the observed terms) in analogy computations.
- Paragraph starting at line 343: I disagree with the intuitive claim that 'man' would be a synonym or paraphrase of anything involving 'woman.' The subtraction in analogy computations involves an implicit negation, making it distinct from straightforward composition.
- A minor general suggestion: The paper uses 'p(w|c)' to denote the probability of a word given a context, but here 'w' represents the context and 'c' the target word. This unconventional notation makes the paper harder to follow. Consider revising for clarity.
Literature:
The assertion that Arora (2016) is the sole work attempting to understand vector composition is overstated. For example, see Paperno & Baroni's work on the success of addition as a composition method for PMI-weighted vectors:
D. Paperno and M. Baroni. 2016. When the whole is less than the sum of its parts: How composition affects PMI values in distributional semantic vectors. Computational Linguistics 42(2): 345-350.
---
I thank the authors for their response and look forward to seeing this paper accepted.