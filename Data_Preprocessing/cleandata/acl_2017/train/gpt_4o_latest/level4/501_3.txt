- Strengths:
The DMC task serves as a promising benchmark for evaluating the interplay between language and vision. I appreciate that the task includes a well-defined evaluation metric.
The observed failure of the caption generation model on the DMC task is particularly intriguing. This finding reinforces the notion that while these models excel as language models, they fall short in effectively capturing the semantic content of images.
- Weaknesses:
The experimental setup lacks a critical baseline: a state-of-the-art VQA model trained with a restricted yes/no label vocabulary.
More details on the human performance experiments would have been beneficial. For instance, how many of the ~20% of misclassified images are due to genuinely ambiguous captions? Could further refinement of the dataset improve human accuracy even more?
- General Discussion:
A primary concern with this paper is the potential for the dataset to be either too simple or susceptible to gaming strategies. The authors could address this by benchmarking their dataset against a range of strong baselines and reporting their performance. The current experiments are not entirely convincing, as the neural network architectures used differ significantly from state-of-the-art models for similar tasks, which often incorporate attention mechanisms over the image.
An additional valuable contribution to the paper would be a thorough analysis of the dataset. For example, how many tokens does the correct caption typically share with distractors? What specific types of understanding are required to differentiate between correct and incorrect captions? Such an analysis would help contextualize the task's significance compared to other related tasks.
The data generation method employed is relatively straightforward and does not constitute a substantial contribution unless it demonstrates unexpectedly strong performance.
- Notes:
The FFNN architecture is not described in sufficient detail in either the main paper or the supplementary material. It appears to function as a convolutional network over tokens, but its specifics remain unclear. Additionally, the application of the Veq2Seq+FFNN model to both classification and caption generation is confusing. Is the log-likelihood of the caption combined with the FFNN prediction during classification? How is the FFNN score integrated during caption generation?
The statistically significant underperformance of the caption generation model compared to random chance warrants further explanation. How is such a result possible?
Line 528: The description of the neural network architecture is difficult to follow. The final paragraph of the section provides clarity, so consider beginning the section with that explanation.