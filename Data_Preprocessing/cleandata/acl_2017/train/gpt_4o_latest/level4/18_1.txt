- Strengths:  
-- The paper presents a well-justified approach, with a clear explanation and robust results.
- Weaknesses:  
-- No significant weaknesses, apart from the points noted below.
- General Discussion:  
The paper introduces a novel method termed attention-over-attention for reading comprehension. The initial layers of the network generate a vector for each query and document word, resulting in a |Q|xK matrix for the query and a |D|xK matrix for the document. Since the answer corresponds to a word in the document, an attention mechanism is employed to assign weights to each word based on its interaction with query words. The authors extend the conventional attention mechanism by first computing a weight for each query word through a separate attention mechanism and then using these weights to modulate the primary attention over document words. The evaluation is carried out rigorously on benchmark datasets, and the results are analyzed in detail, including comparisons with prior work. Overall, this is a strong contribution to an important problem. The method is well-motivated and clearly articulated, enabling researchers to replicate the results and adapt the techniques to related tasks.
- Other Remarks:  
-- p4, Equation 12: Does "i" iterate over the training set, and does "p(w)" refer to P(w|D,Q) from the previous equation? Please clarify to avoid ambiguity.  
-- Have you considered initializing word embeddings with pre-trained vectors such as Google News or GloVe? Is there a specific reason to believe that general-purpose word semantics might not be beneficial for this task?  
-- p6, L589-592: The phrase "letting the model explicitly learn weights between individual attentions" is unclear. Are you referring to your architecture, specifically how the GRU output indirectly influences the attention applied to query and document words? Clarifying this would be helpful. Additionally, the improvement on validation appears to be 4.0 (72.2 - 68.2), not 4.1.  
-- p7, Table 5: Why do you think the weight for the local LM is relatively higher for the CN task, even though its benefit is smaller? Since the table is included, providing insights into this observation would enhance the reader's understanding.  
-- It would have been valuable to release the software as part of this submission.  
-- Typographical Errors:  
1. p2, L162, right column: "is not that effective than expected" → "is not as effective as expected."  
2. p7, L689, right column: "appear much frequent" → "appears more frequently."  
3. p8, L719-721, left column: "the model is hard to" → "it is hard for the model to," and "hard to made" → "hard to make."