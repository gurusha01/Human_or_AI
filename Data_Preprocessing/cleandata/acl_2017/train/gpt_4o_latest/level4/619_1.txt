This paper introduces a corpus of annotated essay revisions and demonstrates two potential applications:
1) Analysis of Student Revision Behavior, and 2) Automatic Revision Identification.
The latter is framed as a text classification task employing an SVM classifier alongside a diverse set of features. The authors indicate that the corpus will be made freely available for research purposes.
The paper is well-structured and clearly written. The use of a detailed annotation scheme, applied by two annotators, enhances the value of the corpus. I believe this resource could be of interest to researchers studying the writing process and related areas. Additionally, I appreciated the inclusion of two well-defined use cases for the corpus.
However, I have two significant concerns. While the first can be addressed relatively easily if the paper is accepted, the second would require more substantial effort.
1) The paper lacks statistical details about the corpus, which are essential when describing such a resource. Key information, such as the number of documents (presumably 180, based on 60 essays with 3 drafts each), the number of tokens (approximately 400 words per essay?), and the number of sentences, is missing. If my assumptions are correct, the corpus comprises 60 unique essays totaling around 24,000 words, which, when considering all three drafts, amounts to roughly 72,000 words, albeit with significant overlap between drafts. A table summarizing these statistics should be included in the paper.
2) Assuming the above figures are accurate, the corpus is relatively small. While I recognize the challenges of creating hand-annotated datasets—something I view as a strength of this work—I question the broader utility of this resource for the NLP community. Perhaps this work would be better suited for a specialized venue, such as the BEA workshop or a conference focused on language resources like LREC, rather than a general NLP conference like ACL. In the final paragraph, the authors mention plans to expand the corpus with additional annotations. Would they also consider including more essays to address the size limitation?
Additional Comments/Minor Points:
- Given that the corpus includes essays by both native and non-native speakers, another potential application could be native language identification (NLI).
- Page 7: Replace "where the unigram feature was used as the baseline" with "where the word unigram feature was used as the baseline" for clarity.
- Page 7: The phrase "and the SVM classifier was used as the classifier" is redundant and should be revised.