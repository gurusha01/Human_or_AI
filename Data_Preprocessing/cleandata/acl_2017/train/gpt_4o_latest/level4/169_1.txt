- Strengths: The application is highly beneficial for both teachers and learners, and it enables fine-grained comparisons of GEC systems.
- Weaknesses: The system's description is overly superficial, and the evaluation methodology is unsatisfactory.
- General Discussion:
This paper introduces an approach for automatically enriching the output of GEC systems with error types. This is a valuable contribution, as it provides useful insights for both teachers and learners, who can benefit from explicit error type information (a feature often absent in many GEC systems, which typically only provide corrected outputs). Additionally, the approach facilitates more detailed comparisons of GEC systems, offering insights into overall precision as well as recall and precision for specific error types.
However, the description of the system is overly superficial. The system appears to rely on a set of (manually?) created rules, but the paper provides no substantial details about these rules. For instance, the authors could include examples of the rules, specify their quantity, discuss their complexity, and explain their ordering (e.g., whether earlier rules could block the application of later ones). Instead of offering these critical details, a significant portion of the paper is dedicated to evaluating the systems that participated in CoNLL-2014. Table 6, which occupies an entire page, lists results for all systems, and the accompanying text redundantly reiterates many of the facts and figures already visible in the table.
The evaluation of the proposed system has several shortcomings.  
First, instead of merely having annotators rate the system's output, the authors should have independently annotated a gold standard for the 200 test sentences. Given the fixed set of tags, creating a gold standard for this relatively small dataset should have been feasible. The current approach likely inflates the ratings for the system's annotations compared to what would be observed with a true gold standard (see, for example, Marcus et al. (1993) for a discussion on agreement when reviewing pre-annotated data versus annotating from scratch).  
Second, the claim that "all 5 raters individually considered at least 95% of our rule-based error types to be either 'Good' or 'Acceptable'" is problematic. It is not standard practice to average individual ratings in this way. If the "bad" ratings were distributed across different edits (a detail not provided in the paper), this could mean that up to 18.5% of the edits were deemed "bad" by at least one annotator, which is far more concerning than the reported average of 3.7%.  
Third, the test data is insufficiently described. The paper does not specify the number of error categories present in the test data or indicate which error categories are covered (based on those rated as "good" by the annotators).  
Fourth, the phrase "edit boundaries might be unusual" is vague. A clearer explanation, along with examples, is necessary to understand this issue and assess whether it poses challenges for the system's application.
The authors claim that their system is less domain-dependent than systems requiring training data, but this assertion is questionable. For instance, Hunspell's vocabulary likely varies in coverage across domains, and manually created rules can also exhibit domain dependence. Furthermore, the system is entirely language-dependent, which is a significant limitation compared to machine learning approaches. Additionally, the test data used (FCE-test, CoNLL-2014) is drawn from a single domain: student essays.
The rationale for designing a new set of error categories is unclear. The authors provide one justification: to facilitate searches for underspecified categories (e.g., "NOUN" in general). However, it seems that the tagset proposed by Nicholls (2003) supports such searches as well. Alternatively, the authors could have adopted the CoNLL-2014 tagset, which would have allowed them to use the CoNLL gold standard for evaluation.
In summary, the paper's primary motivation remains ambiguous. Is the focus on a new system? If so, critical details about the system are missing. Is it about a new set of error categories? If so, the motivation and discussion surrounding the categories are insufficient. Is it about evaluating the CoNLL-2014 systems? If so, the presentation of the results lacks depth.
- Typos:  
  - l129 (and others): "c.f." → "cf."  
  - l366 (and others): "M2" → "M²" (superscript 2)  
  - l319: "50-70 F1" → Clarify whether this means 50-70%.  
- Check references for incorrect case:  
  - l908: "esl" → "ESL"  
  - l878/79: "fleiss, kappa" → "Fleiss, Kappa"