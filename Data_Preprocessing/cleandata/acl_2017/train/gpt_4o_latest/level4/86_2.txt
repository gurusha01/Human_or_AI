This paper introduces a method for converting natural language descriptions into source code by employing a model constrained by the programming language's grammar. I found the paper to be well-written, tackling a challenging and intriguing problem by leveraging inherent constraints, and demonstrating notable performance improvements.
Strengths:
- Tackles a significant and compelling problem area.
- Effectively integrates the constraints inherent to the output space into the model.
- Provides strong evaluation and comparisons, including an analysis of how various components of the model influence performance.
- The paper is clearly articulated and easy to follow.
Weaknesses:
- My main concern with the paper lies in the evaluation metrics. While accuracy and BLEU4 are straightforward to compute, they do not provide a sufficiently comprehensive assessment. Accuracy can fail to account for correctly generated code due to minor, functionally irrelevant differences, potentially resulting in 0% accuracy despite 100% functional correctness. Similarly, BLEU may not be well-suited for evaluating code, as significant structural changes (e.g., tree transformations of the AST) can preserve functionality but escape BLEU's token-level n-gram evaluation. While I understand the rationale for using BLEU, it seems particularly problematic in this context. A possible improvement might involve applying BLEU to normalized ASTs of both reference and generated code. However, what I would truly like to see is an evaluation metric that assesses functional equivalence between the reference and generated code. I recognize that this is challenging, as it would require writing test cases for each reference. Nonetheless, even conducting such an evaluation on a random, reasonably small subset of the dataset could provide a far more meaningful perspective.
Minor issues:
- Page 2, paragraph 2: The phrase "structural information helps to model information flow within the network" is unclear. By "network," are you referring to the AST?
  
- Section 4.2.1, Action Embedding: Are the action embedding vectors in \( WR \) and \( WG \) simple one-hot vectors, or do they involve a more complex embedding? If the latter, how are these embeddings computed? If the former, what distinguishes the vectors in \( W_R \) from \( e(r) \) in Equation 4?
- Section 5.2, Preprocessing: When quoted strings in the DJANGO dataset descriptions are replaced, how are cases handled where those strings need to be copied into the generated code? Additionally, the supplementary material mentions filtering out infrequent words. How are situations managed where such words are necessary to represent variable names or literals in the generated code?
I have reviewed the author response.