The paper examines the final sentences of 5-sentence stories from the corpus developed for the story cloze task (Mostafazadeh et al., 2016) and introduces a model leveraging character and word n-grams to classify story endings. Additionally, the paper demonstrates improved performance on the story cloze task (distinguishing between "right" and "wrong" endings) compared to prior work.
While style analysis is an engaging topic and the results surpass previous work on the story cloze task, the paper has several shortcomings. First, the definition of "style" is unclear. Furthermore, the structure of the paper requires revision (e.g., the "Results" section conflates results with new experiments) and greater clarity (see detailed questions/comments below). Currently, it is challenging for the reader to discern which data is used in each experiment and which data the discussion pertains to.
(1) More information about the data is essential to evaluate the claim that a "subtle writing task [...] imposes different styles on the author" (lines 729-732). How many stories are analyzed, and how many authors contributed? How many stories per author? Based on your description of the post-analysis of coherence, only pairs of stories written by the same author, where one is labeled "coherent" and the other "neutral," are selected. Can you confirm this? While this might support your claim for "Experiment 1," my understanding is that in "Experiment 2," where comparisons are made between "original" vs. "right" or "original" vs. "wrong," the authors differ. Thus, I am skeptical of the validity of lines 370-373.
(2) Many claims in the paper lack justification. For example, how are the "five frequent" POS and words selected? Are they the most frequent ones? (Also, the tables are confusing: why are there two bars in the legend for each category?) Why are character 4-grams used? Were they tuned on the development set? If these features are not the most frequent but were chosen from among frequent POS and words, the rationale for their selection must be provided, particularly in relation to "style." How do these features reflect "style"?
(3) The connection between the "Design of NLP tasks" section and the rest of the paper, including the results, is unclear. This confusion may stem from ambiguity regarding the "training" and "test" sets referenced in this section.
(4) It is difficult to discern how your model differs from prior work. How do you reconcile lines 217-219 ("These results suggest that real understanding of text is required in order to solve the task") with your proposed approach?
(5) The terms "right" and "wrong" endings, borrowed from Mostafazadeh et al., are problematic. What precisely do "right" and "wrong" signify ("right" as in "coherent" or "right" as in "morally good")? I briefly reviewed the original prompts given to the Turkers but could not locate the exact instructions. This needs clarification. As it stands, the first paragraph of the "Story cloze task" section (lines 159-177) is difficult to comprehend.
Additional questions/comments:
- Table 1: Why does the "original" story differ from the coherent and incoherent versions? According to your corpus description, a Turker was shown the first four sentences of the original story and asked to write a "right" ending (or was it a "coherent" ending?) and a "wrong" ending (or was it an "incoherent" ending?). The "incoherent" ending you provide does not seem particularly incoherent—for example, if Kathy only likes $300 shoes, it makes sense that she dislikes buying shoes. This raises questions about how coherence was judged. How many Turkers evaluated coherence, and how consistent were their judgments? What criteria were used to determine coherence? Did the same Turker evaluate both the "right" and "wrong" endings, making it a relative judgment, or were these absolute judgments? This distinction has significant implications for the ratings.
- Lines 380-383: What does "We randomly sample 5 original sets" mean?
- Line 398: The phrase "Virtually all sentences" needs quantification.
- Table 5: Could you provide the weights of the features?
- Line 614: The phrase "compared to ending an existing task" is unclear; Turkers are not ending a "task."
- Lines 684-686: The statement "made sure each pair of endings was written by the same author" applies to "right"/"wrong" pairs but not to "original"-"new" pairs, based on your description.
- Line 694: The reference to "shorter text spans" is vague—shorter spans of what?
- Lines 873-875: Where is this published?