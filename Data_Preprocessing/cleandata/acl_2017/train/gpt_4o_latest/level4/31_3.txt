This paper introduces a supervised deep learning model for event factuality identification. The experimental results demonstrate that the proposed model achieves superior performance compared to state-of-the-art systems on the FactBank corpus, particularly excelling in three specific classes (CT-, PR+, and PS+). The primary contribution of the work lies in the development of an attention-based two-step deep neural model that leverages bidirectional long short-term memory (BiLSTM) and convolutional neural network (CNN) architectures for event factuality identification.
[Strengths:]
- The paper's structure is reasonably well-organized, though not flawless.
- The empirical results provide statistically significant and convincing evidence of the proposed model's performance gains over strong baselines.
[Weaknesses:]
The following weaknesses are detailed below:
- The paper does not clearly articulate its novelties.
- A detailed error analysis is absent.
- The comparison of features with prior work is superficial and omits two relevant papers.
- The paper contains several unclear descriptions and typographical errors.
[General Discussion:]
The paper would benefit from explicitly stating its novelties. For instance, is this the first neural network-based approach for event factuality identification? If so, this should be clearly emphasized.
Providing a detailed error analysis would help clarify the challenges that remain in event factuality identification and better guide future research. Specifically, the results in Tables 3 and 4 could be analyzed in greater depth. What are the primary sources of errors made by the best-performing system, BiLSTM+CNN(Att)? How do errors in basic factor extraction (Table 3) affect the overall performance of factuality identification (Table 4)? The analysis in Section 5.4 resembles a feature ablation study rather than a comprehensive error analysis.
A stronger comparison with prior work in terms of features would enhance the paper. Does the proposed model introduce any novel features that have not been explored in previous studies? It is unclear whether the model's advantages stem solely from deep learning or from a combination of neural networks and novel features. Additionally, the paper overlooks two relevant studies:
- Kenton Lee, Yoav Artzi, Yejin Choi, and Luke Zettlemoyer. 2015. Event Detection and Factuality Assessment with Non-Expert Supervision. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643-1648.
- Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob Eisenstein. 2014. Modeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 415-420.
The paper would also be more comprehensible if additional examples were provided to illustrate the underspecified modality (U) and underspecified polarity (u). This is important for two reasons. First, the concept of 'underspecified' is less intuitive compared to other classes like 'probable' or 'positive.' Second, more examples would help elucidate the challenges of Uu detection, as noted in lines 690-697. Among the seven examples (S1-S7), only S7 pertains to Uu, and its explanation is insufficient to illustrate the associated difficulties.
Minor comments regarding unclear descriptions and typos are as follows:
- The explanations of features in Section 3.2 are somewhat entangled and could be more clearly organized. The section would benefit from separate paragraphs dedicated to lexical features and sentence-level features, specifically by:
  - (1) Introducing the SIP feature as comprising two components (lexical-level and sentence-level) and defining their corresponding variables (l and c) at the outset;
  - (2) Moving the description of lexical feature embeddings (lines 280-283) to the first paragraph; and
  - (3) Presenting the last paragraph on relevant source identification as a separate subsection, as it is not directly related to SIP detection.
- The title of Section 3 ('Baseline') is misleading. A more accurate title would be 'Basic Factor Extraction' or 'Basic Feature Extraction,' as the section focuses on extracting basic factors (features) rather than describing a baseline end-to-end system for event factuality identification.
- The neural network architectures would be more compelling if the paper elaborated on the benefits of the attention mechanism for the task.
- Table 2 appears to present factuality statistics for all sources. Including statistics for 'Author' and 'Embed' alongside Table 4 would make the table more informative.
- Table 4 would be more effective if the highest system performance for each source-factuality value combination were highlighted in bold.
- Section 4.1 states, "Aux_Words can describe the syntactic structures of sentences," while Section 5.4 claims, "they (auxiliary words) can reflect the pragmatic structures of sentences." These two statements are inconsistent and neither adequately summarizes the utility of the dependency relations 'aux' and 'mark' for the task.
- S7 could serve as another example to demonstrate the effectiveness of auxiliary words, but its explanation is less detailed compared to S6. What is the auxiliary word for 'ensure' in S7?
- Line 162: 'event go in S1' should be corrected to 'event go in S2.'
- Line 315: 'in details' should be revised to 'in detail.'
- Line 719: 'in Section 4' should be specified as 'in Section 4.1.'
- Line 771: 'recent researches' should be changed to 'recent research' or 'recent studies,' as 'research' is uncountable.
- Line 903: 'Factbank' should be corrected to 'FactBank.'