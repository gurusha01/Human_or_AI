Strengths:
- The proposed technique for constructing a dataset to evaluate out-of-coverage items is noteworthy and could potentially be applied to assess other grammars as well.  
- The paper is well-written, engaging, and clear, which is a refreshing change compared to the typical ACL publication.
Weaknesses:
- The evaluation datasets used in the study are relatively small, which makes the results less compelling, particularly with respect to the alchemy45 dataset, where the best results were reported.  
- The evaluation is limited to F1 scores and coverage metrics, with little to no deeper analysis of the results. For example, a breakdown of errors by type or grammatical construction would have added significant value.  
- The proportion of out-of-coverage items attributable to various factors remains unclear. Specifically, it is not evident how much is due to resource limitations, lack of coverage for "genuine" long-tail grammatical constructions, extra-grammatical factors like interjections or disfluencies, or insufficient lexical coverage.
General Discussion:
This paper tackles the issue of "robustness" or the lack of coverage in a hand-written HPSG grammar (English Resource Grammar). It compares several strategies for improving coverage and introduces two innovative methods for generating evaluation datasetsâ€”a challenging task given that gold-standard evaluation data is inherently available only for in-coverage inputs.
While hand-written precision grammars have largely fallen out of favor and have been replaced by statistical treebank-based grammars, I believe it is crucial to continue research in this area. The high precision and deep semantic analysis offered by these grammars remain unmatched by non-handwritten approaches. For this reason, despite the shortcomings highlighted above, I am assigning this paper a score of 4.