This paper tackles the network embedding challenge by proposing a neural network model that integrates both the network structure and node-associated text, employing an attention mechanism to adjust the textual representation based on the neighboring nodes' text.
- Strengths:
The proposed model effectively combines network structure and textual information to generate latent representations, and the mutual attention mechanism appears to be a reasonable approach.
The paper provides a fairly comprehensive evaluation, utilizing multiple datasets, baseline methods, and evaluation tasks.
- Weaknesses:
Similar to many papers in the "network embedding" domain that adopt neural network techniques inspired by word embeddings to derive latent representations of network nodes, this work overlooks prior research on statistical and probabilistic modeling of networks. Specifically, all "network embedding" studies should begin referencing and benchmarking against foundational work, such as the latent space model introduced by Peter Hoff et al., along with subsequent contributions in statistical and probabilistic machine learning literature:
P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social network analysis. J. Amer. Statist. Assoc., 97(460):1090â€“1098, 2002.
This latent space model, which embeds nodes into a low-dimensional latent space, was developed as early as 2002, predating neural network-based network embeddings.
Given this paper's goal of modeling the varying roles of social network actors, it should also reference and compare its approach to the mixed membership stochastic blockmodel (MMSB):
Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed membership stochastic blockmodels. Journal of Machine Learning Research.
The MMSB framework enables nodes to probabilistically adopt different "roles" when forming edges.
- General Discussion:
While the statistical models mentioned above do not incorporate textual data or employ scalable neural network implementations with techniques like negative sampling, they are grounded in principled generative modeling rather than heuristic neural network objectives. Moreover, there are more recent extensions of these models that improve scalability and incorporate textual information.
Is the performance difference between CENE and CANE in Figure 3 statistically insignificant? (Additionally, were the experiments conducted multiple times with different random train/test splits?)
Regarding the hyperparameter grid searches described in Section 5.3, were they performed using the test set (which would be problematic), a validation set, or the training set?