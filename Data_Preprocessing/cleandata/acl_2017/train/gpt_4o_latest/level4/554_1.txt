Review - Strengths:
a) The paper introduces a Bayesian learning framework for recurrent neural network language models, demonstrating superior performance compared to standard SGD with dropout across three tasks.  
b) The application of Bayesian learning to RNNs appears to be an original contribution.  
c) The proposed computationally efficient Bayesian algorithm for RNNs has the potential to attract significant interest from the NLP community due to its applicability across various tasks.
Weaknesses:
The primary concern lies in the evaluation methodology:
- Section 5.1: The paper evaluates different architectures (LSTM/GRU/vanilla RNN) on the character-level language modeling task while comparing learning algorithms on the Penn Treebank dataset. Additionally, RMSprop and pSGLD are compared for the character-level task, whereas SGD +/- dropout is compared with SGLD +/- dropout for the word-level task. This inconsistency in evaluation is problematic. It would be more informative to present results for both dimensions (architectures and learning algorithms) across both character- and word-level language modeling tasks. This would help assess whether the proposed Bayesian learning methods generalize across tasks and datasets.  
- Line 529: The paper claims that "the performance gain mainly comes from adding gradient noise and model averaging." However, this conclusion lacks empirical support. An A/B experiment isolating the effects of gradient noise and model averaging is necessary to substantiate this statement.  
- Line 724: Gal's dropout is evaluated only on the sentence classification task, but not on the language modeling or captioning tasks. Since Gal's dropout is not inherently specific to sentence classification, its performance should also be reported for the other two tasks. This would provide a more comprehensive comparison of the proposed algorithms against existing dropout methods.  
- Line 544: The paper does not clarify whether the samples (\(\theta1, ..., \thetaK\)) are ordered in any specific way, such as by posterior probability. Additionally, the authors could report results for randomly selecting \(K\) samples out of \(S\) as an alternative, which would provide further insights into the robustness of the sampling strategy.  
- Regular RNN language models are known to be computationally expensive to train and evaluate. A comparison of training and evaluation times between the proposed Bayesian learning algorithms and SGD with dropout would be highly valuable. This would allow readers to weigh the performance improvements against the computational overhead.
Clarifications:
- Line 346: What does \(\theta_s\) represent? Is it the MAP estimate of the parameters based solely on sample \(s\)?  
- Lines 453-454: Please clarify the meaning of \(\theta\) in the context of dropout and dropconnect.  
Typos:
- Line 211: "output"  
- Line 738: "RMSProp"