Review
Strengths:
This paper systematically explores the performance of embedding models by presenting a 2 x 2 x 3 x 10 array of accuracy results, varying the following parameters:
- Context type ∈ {Linear, Syntactic}  
- Position sensitivity ∈ {True, False}  
- Embedding model ∈ {Skip Gram, BOW, GLOVE}  
- Task ∈ {Word Similarity, Analogies, POS, NER, Chunking, 5 text classification tasks}  
The study investigates how performance changes with these parameter variations, and the results are presented in terms of accuracy. The goal of this work is relevant to the ACL community, as similar studies have been well-received and cited, such as Nayak et al.'s paper mentioned later in this review. The topic is timely and aligns with the interests of the field.
---
Weaknesses:
As this paper primarily focuses on analyzing the effects of systematically varying context types and position sensitivity, my critique centers on the execution of the experiments and the interpretation of the results, which I find lacking in several aspects:
A) Lack of hyperparameter tuning:  
The absence of proper hyperparameter tuning raises concerns about the validity of the findings. Examples include:  
- Line 395: "Unless otherwise noted, the number of word embedding dimensions is set to 500."  
- Line 232: "It still enlarges the context vocabulary about 5 times in practice."  
- Line 385: "Most hyperparameters are the same as Levy et al.'s best configuration."  
This omission makes it difficult to draw meaningful comparisons, such as determining whether method A outperforms method B. For instance, bound methods might benefit from lower dimensionality due to their larger effective context vocabulary, whereas unbound methods may not. Without tuning, such insights remain speculative.
B) Ambiguous explanations:  
Some of the explanations provided for the results are unclear or contradictory. For example:  
- Line 115: "Experimental results suggest that although it's hard to find any universal insight, the characteristics of different contexts on different models are concluded according to specific tasks."  
  This statement is vague and does not convey a clear conclusion.  
- Line 580: "Sequence labeling tasks tend to classify words with the same syntax to the same category. The ignorance of syntax for word embeddings which are learned by bound representation becomes beneficial."  
  These explanations conflict with each other. If sequence labeling tasks rely on syntax to classify words, then syntax should be a critical feature. The claim that ignoring syntax benefits bound representations contradicts this logic and is not adequately justified.
C) Insufficient engagement with related work:  
The paper briefly mentions Lai et al. (2016) but does not adequately engage with related studies. For instance, the paper "Evaluating Word Embeddings Using a Representative Suite of Practical Tasks" by Nayak, Angeli, and Manning (ACL 2016, Repeval Workshop) should have been cited. While the focus of Nayak et al.'s work differs slightly, their recommendations on hyperparameter tuning and experimental design are highly relevant. They also provide a web interface for running tagging experiments using neural networks, which contrasts with the "simple linear classifiers" used in this paper.
D) Inconsistent choice of classifiers:  
The paper employs a neural BOW classifier for text classification tasks but uses a simple linear classifier for sequence labeling tasks. This inconsistency is not justified. Why not use a neural classifier for tagging tasks as well? This choice is particularly concerning because the tagging task is the only one where bound representations consistently outperform unbound representations, making it an outlier. The lack of justification for the classifier choice undermines the reliability of the results.
---
General Discussion:
Finally, I offer a speculative suggestion for improving the analysis. The primary contribution of this paper is the analysis of a table with 120 accuracy values, corresponding to the combinations of (context type, position sensitivity, embedding model, task). To better understand the patterns in this data, the authors could consider applying factor analysis or other pattern-mining techniques. Such methods might reveal underlying trends or relationships that are not immediately apparent from the raw results.