- Strengths:  
This is a well-executed and robust piece of research that effectively builds on prior studies. The paper is well-written and presented with clarity.
- Weaknesses:  
There are very few weaknesses, though the authors might consider revising some statements that feel somewhat redundant or uninformative:  
191: For instance, if the task involves identifying words used similarly across contexts, the scoring function can be designed to assign high scores to terms exhibiting comparable usage across these contexts.  
537: It is insightful to examine how annotations derived from the same dataset align or differ.  
- General Discussion:  
In the initial sections, it was not immediately clear that the work introduced significant novelty or interest, as the methods appeared to closely resemble approaches from the past 25 years for measuring similarity, albeit with some new statistical refinements. Conceptually, these methods seemed to follow a familiar trajectory. However, Section 5 presents an engaging and valuable contribution that holds promise for advancing future research in this domain. Upon reflection, the foundational material in Sections 2-4, while not groundbreaking, provides essential context to support the experiments detailed in Section 5.  
Overall, the findings and contributions of this paper will be beneficial to researchers in the field, and the work merits presentation at ACL.  
- Minor Comments:  
1. Missing word or punctuation:  
264: For word annotations, we utilized PPMI, SVD, and SGNS (skip-gram with negative sampling from Mikolov et al. (2013b)) word vectors as provided by Hamilton et al. (2016).  
2. Clarify "multiple methods":  
278: It is unclear what "multiple methods" refers to in the statement: "some words were detected by multiple methods with CCLA."