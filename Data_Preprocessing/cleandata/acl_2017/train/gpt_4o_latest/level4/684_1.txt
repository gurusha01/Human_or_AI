This paper introduces a gated attention (GA) mechanism tailored for machine reading comprehension. The central concept involves extending the Attention Sum Reader (Kadlec et al., 2016) to support multi-hop reasoning through a fine-grained gated filter. The approach is both intriguing and intuitive for the task of machine reading. While I appreciate the novelty and the significant performance gains on benchmark datasets, I have substantial reservations about its readiness for publication at ACL.
- The proposed gated attention mechanism appears promising; however, it does not sufficiently establish its superiority over other state-of-the-art systems. This is partly because the engineering tricks described in Section 3.1.4 contribute significantly to the reported accuracy improvements, making it difficult to isolate the true impact of the GA mechanism itself.
- The bibliography is incomplete, with nearly all references pointing to arXiv preprints rather than published versions. This raises concerns about whether the work has been thoroughly compared against prior research. If published versions of the cited works are available, they should be included to ensure credibility and facilitate proper comparison for future readers.
- The results from unpublished work (GA) in Tables 1 and 3 are problematic. The GA baseline is cited as prior work but is an unpublished preprint. Including such results is unnecessary and potentially misleading. I recommend replacing it with a vanilla GA model or a variant of the proposed approach as a baseline. Presenting results from a preprint that overlaps with this ACL submission undermines the manuscript's integrity. For fairness in blind review, I refrained from searching for the preprint on arXiv.
- There is a conflict between Tables 1 and 2. Specifically, GA-- in Table 1 appears to correspond to K=1 (AS) in Table 2, while GA (fix L(w)) aligns with K=3 in Table 2. Does this imply that GA-- is essentially the AS Reader? It is unclear whether GA-- represents a re-implementation of the AS Reader. I assumed that K=1 (AS) in Table 2 also uses GloVe initialization and token-attention, but this does not seem to be the case for GA--. Clarification is needed here.
- The related work section would benefit from a more explicit comparison with prior research. It is important to clearly articulate how the proposed method differs from and improves upon related approaches.
- Figure 2 effectively demonstrates the advantages of the gated attention mechanism, particularly its ability to facilitate a multi-hop architecture. The visualization is impressive, but it would be even more compelling to include qualitative examples with comparisons to further illustrate the benefits of the proposed approach.