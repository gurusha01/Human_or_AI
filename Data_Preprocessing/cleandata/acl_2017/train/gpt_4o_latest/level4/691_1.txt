- Overview:  
This paper introduces a novel model for training sense embeddings that are anchored in a lexical-semantic resource, specifically WordNet. However, the paper does not directly evaluate the meaningfulness of the learned sense embeddings. Instead, these sense embeddings are aggregated back into word embeddings, which are then assessed through a downstream task: prepositional phrase (PP) attachment prediction.
- Strengths:  
The results for PP attachment prediction appear robust and convincing.
- Weaknesses:  
The meaningfulness of the sense embeddings themselves is not directly evaluated.  
Certain aspects of the probabilistic model are unclear. For example, are the \(\lambdawi\) parameters hyperparameters or are they learned during training? Additionally, the source of the "rank" is ambiguous—does it correspond to the sense ranks provided in WordNet?  
In terms of related work, the concept of representing word embeddings as a convex combination of sense embeddings has been explored in prior research. For example, Johansson and Nieto Piña's work, "Embedding a semantic network in a word space" (NAACL, 2015), decomposed word embeddings into sense embeddings grounded in an ontology using a similar approach. Similarly, this idea has been applied in unsupervised sense vector training, such as in Arora et al.'s "Linear Algebraic Structure of Word Senses, with Applications to Polysemy."
- Minor Comments:  
There is no need to define types and tokens, as these are standard terms in the field.  
In equation 4, why is the first \(\lambdawi\) necessary if the probability is unnormalized?
- General Discussion: