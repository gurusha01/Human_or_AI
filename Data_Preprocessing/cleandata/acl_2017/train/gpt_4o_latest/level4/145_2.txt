This study employs Gaussian mixtures to represent words and highlights their ability to capture multiple meanings for polysemous words. The training process is guided by a max-margin objective, and the expected likelihood kernel is utilized to measure the similarity between word distributions. Experimental results on word similarity and entailment tasks validate the effectiveness of the proposed approach.
- Strengths:
The paper provides a clear motivation and well-defined problem statement. Gaussian mixtures offer a more expressive alternative to deterministic vector representations, with the ability to capture multiple word meanings through their modes, as well as the associated probability mass and uncertainty. This represents a significant contribution to the field of word embeddings.
The proposed method introduces a max-margin learning objective along with a closed-form similarity measure, enabling efficient training.
The paper is generally well written.
- Weaknesses:
Please refer to the questions below.
- General Discussion:
In Gaussian mixture models, the number of components (k) is typically a critical parameter. In this study, k is fixed at 2 for all experiments. What criteria were used to select this value? Does increasing k negatively impact the model's performance? Additionally, how does the learned distribution behave for words with only a single dominant meaning?
The experiments exclusively use the spherical case (where the covariance matrix is reduced to a single scalar). Was this choice made purely for computational efficiency? How would the model perform with a general diagonal covariance matrix? The latter could potentially capture varying degrees of uncertainty along different directions in the semantic space, which seems conceptually more intriguing.
Minor comments:
Table 4 is not cited in the text.
In the references, the citation for Luong et al. is missing the publication year.
I have reviewed the authors' response.