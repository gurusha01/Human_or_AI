Paraphrased Review:
---
Update after author response:
1. My primary concern regarding the optimization of the model's numerous hyperparameters remains unresolved. This issue is particularly critical given that the reported results are based on folded cross-validation.
2. The claim that the advantages of the proposed method are experimentally validated with a 2% improvement—achieved through 5-fold cross-validation on 200 examples—is not particularly persuasive.
---
Summary:
This paper introduces a sophisticated neural model for determining the factuality of event mentions in text. The proposed model integrates the following components: (1) a collection of traditional classifiers for identifying event mentions, factuality sources, and source-introducing predicates (SIPs); (2) a bidirectional attention-based LSTM model that learns latent representations for elements along various dependency paths, which are then used as input; and (3) a CNN that leverages the LSTM's representations to perform two output predictions—one to distinguish specific cases from underspecified ones and another to classify the actual factuality.
From a methodological perspective, the authors combine well-established techniques (attention-based BiLSTM and CNN) into a relatively complex architecture. However, the model does not use raw text (i.e., sequences of word embeddings) as input but instead relies on manually crafted features (e.g., dependency paths that incorporate factuality concepts such as sources, SIPs, and clues). This reliance on hand-crafted features is somewhat unexpected, especially when paired with a deep learning model of such complexity. Additionally, the evaluation raises concerns, as the authors report results based on folded cross-validation but do not clarify how the model's hyperparameters were optimized. Lastly, the results are underwhelming—given the model's complexity and the extensive preprocessing required (e.g., extracting event mentions, SIPs, and clues), a 2% macro-average improvement over a rule-based baseline and an overall performance of 44% seem modest at best. Notably, the proposed model does not outperform a simple MaxEnt classifier when considering micro-average performance.
The paper is generally well-written and relatively easy to follow. However, in its current state, it falls short of being a strong candidate for a top-tier conference.
---
Remarks:
1. You frequently assert that the LSTM and CNN in your model are combined "properly." Could you elaborate on what this "properness" entails? How does it manifest in your approach, and what would constitute an improper combination?
2. The justification for the two-output design appears weak:
   - The first argument, which suggests that this design facilitates the later addition of cues (i.e., manually designed features), undermines the purported advantage of deep learning models in learning representations.
   - The second argument, which claims that this design addresses class imbalance in the training set, lacks experimental evidence to support it and feels somewhat speculative.
3. You initially justify the use of your complex deep learning architecture by emphasizing its ability to learn latent representations and reduce reliance on manual feature design. However, you then introduce a set of manually designed features (e.g., dependency paths and lexical features) as input to the model. Do you recognize the inconsistency in this approach?
4. Attention-based bidirectional LSTMs have become a standard model for various NLP tasks. As such, the detailed description of this component in your paper feels unnecessary.
5. What you describe as a baseline in Section 3 is actually a component of your model, as it generates input for the proposed architecture. Referring to it as a baseline may confuse readers and detract from the paper's clarity.
6. The reported results are based on 5-fold cross-validation, yet the model includes numerous hyperparameters (e.g., the number and size of CNN filters) that require optimization. How were these hyperparameters optimized? Reporting results from folded cross-validation does not allow for fair hyperparameter tuning: either the hyperparameters were not optimized at all, or they were tuned on the test set, which would be inappropriate.
7. Regarding the statement, "Notice that some values are non-application (NA) grammatically, e.g., PRu, PSu, U+/-"—why is underspecification allowed in only one dimension (e.g., polarity or certainty)? It is easy to imagine a scenario where an event's polarity is clear (e.g., negative), but its certainty (e.g., certain, probable, or possible) remains unspecified.
---
Language & Style:
1. Replace "to a great degree" with either "to a great extent" or "to a large degree," as the original phrase is uncommon.
2. Replace "events that can not" with "events that cannot" or "events that do not."
3. Rephrase "describes out networks...in details shown in Figure 3" to "...shown in Figure 3 in detail."