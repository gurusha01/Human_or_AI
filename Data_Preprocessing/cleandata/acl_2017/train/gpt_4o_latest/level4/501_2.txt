- Strengths:
The authors have created a dataset of "rephrased" captions and plan to make it publicly accessible, which is a valuable contribution to the research community.
The authors' approach to the Dual Machine Comprehension (DMC) task offers a distinct advantage over tasks like Visual Question Answering (VQA) or caption generation in terms of evaluation metrics. Specifically, the problem of selecting the best caption is simpler and more direct to evaluate using accuracy, whereas caption generation relies on metrics such as BLEU or METEOR, which have limitations in capturing semantic similarity.
The authors introduce an intriguing method for "rephrasing," particularly in the selection of decoys. Decoys are drawn from an image-caption dataset, where captions for other images are used as decoys for a given image. These decoys are designed to be similar both in surface form (e.g., BLEU score) and semantics (e.g., paragraph vector similarity). A lambda factor is employed to balance these two components of the similarity score. This approach could have interesting implications for paraphrasing tasks.
The authors substantiate their motivation for the task with evaluation results. They demonstrate that a system trained specifically to differentiate between similar captions outperforms a system trained solely for caption generation. However, these results primarily highlight that a system optimized for a particular task performs better on that task.
- Weaknesses:
The paper does not clearly explain why the image captioning task is unsuitable for comprehension tasks or why the proposed system is better suited for this purpose. To convincingly argue that the system comprehends image and sentence semantics more effectively, the authors should compare learned representations (e.g., embeddings) from different systems on the same task.
A significant concern is that the authors' approach appears to converge toward existing caption generation techniques, such as those proposed by Bahdanau et al. and Chen et al., without offering substantial novelty in this regard.
The presentation of formula (4) is somewhat unclear. The formula suggests that both decoy and true captions are used for both loss terms, but the authors clarify in the text that decoys are not used for the second term. Including decoys in the second term would degrade model performance, as the model might learn to generate decoys. This ambiguity should be resolved either by revising the formula or clarifying the explanation in the text. The current formulation could mislead readers, but the intended approach—training the model to generate only true captions while distinguishing them from decoys—is logical.
- General Discussion:
The authors define the task of Dual Machine Comprehension, which involves challenging a computer system to choose between two highly similar captions for a given image. They argue that solving this task requires the system to "understand" both the image and the captions, capturing not just keywords but also the semantic alignment between the image and the captions.
The paper would benefit from a stronger emphasis on why the proposed approach is superior to caption generation and why the authors believe caption generation is less effective for learning image-text representations and their alignment.
Regarding formula (4), it would be interesting to explore whether the model could be trained to "not generate" decoys by modifying the second loss term to include decoys with a negative sign. Did the authors experiment with such an approach?