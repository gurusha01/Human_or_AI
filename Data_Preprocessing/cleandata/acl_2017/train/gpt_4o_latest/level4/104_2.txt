This paper tackles the challenge of disambiguating and linking textual entity mentions to a predefined background knowledge base, specifically English Wikipedia. (While the title and introduction are somewhat exaggerated and potentially misleading—since bridging text and knowledge encompasses much more than the EDL task—EDL remains a fundamental component of the broader objective.) The proposed approach introduces an intermediate representation layer, referred to as mention senses, and follows a two-step process: (1) mapping mentions to mention senses, and (2) mapping mention senses to entities. The method involves learning various embedding representations for words, mention senses, and entities, which are then jointly optimized using a single objective function that balances all three types of embeddings.
From a technical standpoint, the methodology is relatively clear and aligns with contemporary trends in deep learning and established best practices for embeddings. While alternative approaches could be suggested, it is unclear whether they would yield significant improvements. My comments, therefore, focus on the core approach. Specifically, the paper does not adequately explain why the two-step process involving mention senses is superior to a direct, one-step mapping from word mentions to entities (as employed by Yamada et al. in the ALIGN algorithm). Table 2 demonstrates that the two-step MPME (and even the simplified SPME) outperforms the direct approach, but the rationale for this improvement is not explicitly clarified. What additional information or distinctions do mention senses provide compared to entities? To better understand this, I suggest verifying and potentially clarifying the following points in the paper:
For entities: Their profiles are derived from neighboring entities in a relatedness graph, which (I assume) is constructed based on word-level relatedness within the entity definitions (i.e., Wikipedia pages). These profiles are represented as (extended skip-gram-based) embeddings.
For words: Their profiles are generated using the standard distributional semantics approach without sense disambiguation.
For mention senses: Their profiles also use the standard distributional semantics approach but incorporate sense disambiguation. This disambiguation is achieved using a sense-based profile (or "language model") derived from local context words and neighboring mentions, as briefly mentioned before Section 4. However, the details of this process are lacking, which is a critical issue. How are the mention senses defined and differentiated? Who determines the number of senses a mention string can have? If this is based on the knowledge base, does this result in a one-to-one mapping between mention senses and entities (even if there are multiple entities)? If so, are the definitional profiles for mention senses constructed using entity text as "seed words"? If this is the case, what additional information is captured at the mention sense level that is not already present at the entity level? Are these additional words those that reliably associate with the mention sense but do not appear in the corresponding entity's Wikipedia page? On average, how many such words exist for a mention sense? In essence, how critical is it to maintain this separate mention sense space, as opposed to simply incorporating these additional words into the entity space by augmenting the Wikipedia entity pages?
If the above interpretation is correct, I recommend updating Section 5 to explicitly clarify these points, as they represent the primary novel contribution of the paper.
Contrary to the claim in Section 6 that "…this is the first work to deal with mention ambiguity in the integration of text and knowledge representations, so there is no exact baselines for comparison," this is not accurate. The TAC KBP evaluations over the past two years have included EDL tasks, with eight or nine systems addressing this exact problem, albeit using Freebase, which is larger and noisier than Wikipedia. I suggest the authors refer to http://nlp.cs.rpi.edu/kbp/2016/ for further context.
On a positive note, I found the smoothing parameter introduced in Section 6.4.2 to be an innovative and valuable addition.
Post-response: After reviewing the authors' responses, I remain unconvinced by their argument that the KBP evaluation is irrelevant because their focus is on the quality of the embeddings. Ultimately, the only meaningful way to assess the "quality" of embeddings is through their performance in a specific application. While conceptual elegance is appreciated, the primary question is whether the proposed approach delivers superior results.