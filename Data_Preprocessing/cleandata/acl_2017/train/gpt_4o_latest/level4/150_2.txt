- Strengths: Overall, the paper is well-organized and clearly written. Most of the explanations are easy to follow, the ideas presented are novel, and the results obtained are quite compelling.
- Weaknesses: There are some uncertainties regarding the interpretation of the results. Additionally, certain claims about the proposed method's ability to learn morphology are not adequately supported by scientific evidence.
- General Discussion:
This paper investigates a sophisticated architecture for character-level neural machine translation (NMT). The proposed model builds upon the traditional encoder-decoder framework by introducing a deep word-encoding layer that encodes character-level input into sub-word representations of the source-language sentence. Similarly, a deep word-decoding layer is added to the output, converting the target-language sub-word representations into a character sequence as the final output of the NMT system. The goal of this architecture is to leverage the advantages of character-level NMT (such as reduced vocabulary size and flexibility in handling unseen words) while enhancing overall system performance by using intermediate sub-word representations to shorten the input character sequence. Furthermore, the authors assert that their deep word-encoding model can better learn morphology compared to other state-of-the-art methods.
I have some reservations about the evaluation. The authors compare their approach to other state-of-the-art systems based on two metrics: training time and BLEU score. However, the benefits of the proposed model (DCNMT) over competing approaches like bpe2char are not entirely clear. The BLEU score differences between the two methods are minimal (0.04 for Cs-En and 0.1 for En-Cs), making it difficult to conclude superiority without statistical significance testing: has statistical significance been assessed? Regarding training time, it is noteworthy that bpe2char requires 8 fewer days than DCNMT for Cs-En. However, training time for En-Cs is not reported (why is this omitted?), and bpe2char is not evaluated for En-Fr. A more comprehensive comparison with this system is necessary to convincingly demonstrate the advantages of the proposed model.
My second concern pertains to Section 5.2, where the authors claim to investigate their system's ability to learn morphology. However, this section primarily consists of examples and commentary. While the examples are well-chosen and effectively explained, there is a lack of experiments or formal evaluations to substantiate the authors' claims. I strongly encourage the authors to expand this intriguing aspect of their work, which could even serve as the foundation for a separate paper. For the current submission, I suggest moving this section to an appendix and tempering the claims regarding the system's morphological learning capabilities.
- Other comments, doubts, and suggestions:
  - Several acronyms (e.g., LSTM, HGRU, CNN, PCA) are used without definition, while others (e.g., RNN, BPE) are defined only after their initial usage. Even though some of these acronyms are widely known in the deep learning community, defining them would enhance clarity.
  
  - The concept of energy is first introduced in Section 3.1. While the explanation provided there is sufficient, it would be helpful to revisit and clarify this concept in Section 5.2, where it is used extensively. For instance, does high energy on a character indicate that the current morpheme should split at that point? Additionally, the term "peak" in Figure 5 is not explicitly described.
  
  - When the acronym BPE is first defined, it is written in uppercase, but subsequent mentions use lowercase. Is there a specific reason for this inconsistency?
  
  - In Section 4.1, it may not be necessary to state that no monolingual corpus is used.
  
  - There appears to be an issue with Figure 4a, as energy values are not displayed for every character.
  
  - In Table 1, the results for model (3) (Chung et al., 2016) for Cs-En are not reported in the original paper. If the authors computed these results themselves, this should be explicitly mentioned.
  
  - Rather than describing French as morphologically poor, it would be more accurate to state that it is less morphologically rich compared to Slavic languages like Czech.
  
  - Why is a link provided for the WMT'15 training corpora but not for WMT'14?
  
  - Several references are incomplete.
- Typos:
  - "..is the bilingual, parallel corpora provided..." → "..are the bilingual, parallel corpora provided..."
  - "Luong and Manning (2016) uses" → "Luong and Manning (2016) use"
  - "HGRU (It is" → "HGRU (it is"
  - "coveres" → "covers"
  - "both consists of two-layer RNN, each has 1024" → "both consist of two-layer RNN, each have 1024"
  - "the only difference between CNMT and DCNMT is CNMT" → "the only difference between CNMT and DCNMT is that CNMT"