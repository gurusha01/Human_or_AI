The paper proposes a method leveraging multigraphs (where multiple edges can connect two nodes) to identify potentially overlapping entities.
Strengths:  
The problem addressed in the paper is potentially intriguing, particularly in cases involving crossing entities, where determining which entity is actually referenced in a text can be challenging. The proposed technique appears to function effectively, though the empirical results do not demonstrate a particularly substantial improvement. I appreciate that the paper dedicates some discussion to efficiency in comparison to a prior system. Overall, the paper is well-written, though it could benefit from further refinement in certain areas (see minor remarks below).
Weaknesses:  
The motivation for the problem is not clearly articulated. For instance, why is it significant to identify "China" as an entity within "Bank of China," as mentioned in the introduction? While I understand the relevance of crossing entities, the practical utility of nested entities remains unclear. Stronger motivation and use-case scenarios would help engage the reader.  
Regarding the proposed approach, some critical details are missing. For example, what criteria are used to decide whether to include an edge? Lines 229–233 mention several options for the \( I^k_t \) nodes, but it is not clarified which edges should be included.  
In terms of empirical evaluation, while the results are better than some prior methods, the improvements are not particularly significant. Referring to these modest gains as "outperformed" seems overstated. What is the effect size? Would a user notice a meaningful difference with a two-percentage-point increase in \( F_1 \)? How many "important" entities are identified that previous methods missed? Additionally, how would a simple dictionary-based method or a commercial system like Google's NLP Cloud perform on the same datasets? Including such comparisons would provide valuable context for the results.  
The discussion of results could also be improved by focusing more on crossing entities, which I find to be the more compelling subset of overlapping entities compared to nested ones. How does the system perform specifically on crossing entities? How many additional crossing entities are detected compared to previous methods, and which ones are still missed? Is the observed performance improvement primarily due to better detection of nested entities, or does it also extend to crossing entities? A more detailed error analysis, contrasting the proposed system with prior approaches, would strengthen this section.  
General Discussion:  
I find the topic of named entity recognition interesting and see the value in addressing crossing entities. However, the motivation for nested entities remains unclear. The paper neither justifies this scenario convincingly nor addresses it adequately in the evaluation. Including a discussion of errors, advantages, and example cases—particularly with a focus on crossing entities—would have been more persuasive. As it stands, the paper feels like another incremental attempt without adequately addressing the critical issue of crossing entities. My overall impression is lukewarm, leaning slightly toward rejection.  
Minor Remarks:  
- The first mention of multigraphs could benefit from a brief explanation for readers unfamiliar with the concept.  
- "Previously noted by ... many previous" sounds awkward.  
- "Solving this task": Which task? Please clarify.  
- Why is "e.g." italicized?  
- "Time linear in \( n \)": If \( n \) refers to sentence length, does it matter whether the complexity is linear or cubic?  
- "Spurious structures": The meaning is unclear in the introduction.  
- "Regarded as a chunk" (missing article).  
- "NP chunking": Does this refer to noun phrase chunking?  
- "Since they set": Who is "they"?  
- "Pervious" should be "previous."  
- "Of Lu and Roth~(2015)" needs rephrasing.  
- "The following five types": Spell out small numbers in sentences without larger numbers.  
- "Types of states": What is a state in a (hyper-)graph? Later, "state" seems to be used synonymously with "node."  
- Add commas after enumeration items at the end of page 2 and a period after the last one.  
- What are child nodes in a hypergraph?  
- In Figure 2, it is not immediately clear why this is a hypergraph. Grayscale printing obscures the colors. Why are some nodes/edges gray? How were the highlighted edges selected?  
- Why should both entities in Figure 2 be detected? How does this differ from simply recognizing the longer entity?  
- "Denoting ...": Sometimes in brackets, sometimes not—please standardize.  
- Place footnotes after punctuation marks, not before.  
- Footnote 2: How was it determined that the missing edge should be absent?  
- "On whether the separator defines ...": How was this determined?  
- "The mention hypergraph" (missing article).  
- In the last paragraph before 4.1: How is the CS-edge algorithmically chosen to represent the entity separator?  
- Add a comma after Equation 1.  
- "To find out" sounds awkward; consider rephrasing.  
- "We extract entities.\footnote" (punctuation issue).  
- "We make two": Consider "we conduct" or similar phrasing.  
- Footnote 3: Why is favoring nested entities good? Why not prioritize crossing entities? Provide examples for clarity.  
- "The combination of states alone does not" (subject-verb agreement).  
- "The simple first-order assumption": What does this refer to?  
- "The previous section" (missing article).  
- "We see that our model": Consider "demonstrated" or "shown."  
- "Used in this experiments": Should be "these experiments."  
- "Each of these distinct interpretations" (pluralization).  
- "Published on their website" (missing preposition).  
- "The statistics of each dataset are shown" (subject-verb agreement).  
- "Allows us to use to make use": Remove redundancy.  
- "Tried to follow as close ...": Rephrase as "tried to use the features suggested in previous works as closely as possible."  
- "Following (Lu and Roth, 2015)": Avoid using references as nouns; rephrase as "Following Lu and Roth (2015)."  
- "Using the BILOU scheme" (missing article).  
- "Highlighted in bold": What about the effect size?  
- "Significantly better": In what sense? Provide effect size.  
- "In GENIA dataset": Should be "On the GENIA dataset."  
- "Outperforms by about 0.4 points": Avoid calling this "outperforming."  
- "That the GENIA dataset" (missing article).  
- "This low recall": Specify which recall.  
- "Due to an insufficient" (missing article).  
- In Table 5, all \( F_1 \) scores appear similar. Again, "outperform" seems exaggerated.  
- "Is more confident": Why does this increase recall?  
- "Converge than the mention hypergraph" (word choice).  
- References: Ensure consistency in capitalization of paper titles.