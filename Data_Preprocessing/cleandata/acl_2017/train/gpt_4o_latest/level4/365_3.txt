- Strengths: The paper is well-written, features a robust experimental setup, and includes an engaging qualitative analysis.
- Weaknesses: Aside from the qualitative analysis, the paper might be more appropriate for the applications track, as the novelty lies primarily in the application rather than in the models themselves, which are not particularly innovative.
- General Discussion: This work introduces a "sequence-to-sequence" model incorporating attention mechanisms and an auxiliary phonetic prediction task to address historical text normalization. While none of the models or techniques employed are novel, their application to this specific problem appears to be unprecedented, demonstrating improvements over the state-of-the-art.
The majority of the paper seems more aligned with the applications track, except for the final analysis, where the authors explore the connection between attention mechanisms and multi-task learning, positing that the two yield similar effects. This hypothesis is thought-provoking and is supported by substantial evidence, at least within the scope of the presented task. However, I have some concerns regarding this analysis:
1) In Section 5.1, are you not assuming that the hidden layer spaces of the two models are aligned? Is this assumption justified?
2) In Section 5.2, I am unclear on what you mean by the errors that each model resolves independently. Are you referring to a symmetric difference? In other words, if the two models are combined, would these errors no longer be resolved?
On a separate note, 3) Why is there no comparison with Azawi's model?
---
After reviewing the author's response:
I am now more concerned than before about the claims regarding alignment in the hidden spaces of the two models. If this paper is accepted, I would strongly recommend that the authors explicitly include in the manuscript the detailed discussion they have shared with us, explaining why they believe this alignment assumption holds in practice.