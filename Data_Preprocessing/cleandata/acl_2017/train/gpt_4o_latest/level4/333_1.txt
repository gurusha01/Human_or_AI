- Strengths:  
The authors introduce a selective encoding model as an enhancement to the sequence-to-sequence framework for abstractive sentence summarization. The paper is well-structured, and the methods are described with clarity. The proposed approach is evaluated on standard benchmarks, with comparisons to other state-of-the-art methods, including statistical significance scores.
- Weaknesses:  
Certain implementation details and information about the systems used for comparison are insufficiently explained and require further elaboration.
- General Discussion:  
  * Major Review:  
  - It is unclear whether the summaries generated by the proposed method are truly abstractive. While I understand that the target vocabulary is derived from the words in the training data summaries, the example in Figure 4 gives the impression that the summaries are more extractive in nature. The authors should select a more representative example for Figure 4 and provide statistics on the proportion of words in the output sentences that do not appear in the input sentences across all test sets.  
  - Page 2, lines 266–272: Although the mathematical distinction between the vectors \( h_i \) and \( s \) is clear, they both seem to represent the meaning of the input. Is it necessary to use both? Did the authors experiment with using only one of them?  
  - The paper does not specify which neural network library was used for implementation. This information should be included.  
  - Page 5, Section 4.4: The training data used for each of the systems compared in the experiments is not specified. Did the authors train any of these systems themselves?  
  * Minor Review:  
  - Page 1, line 44: Although the distinction between abstractive and extractive summarization is discussed in Section 2, it would be more appropriate to introduce this concept in the introduction, as some readers may not be familiar with it at this stage.  
  - Page 1, lines 93–96: Please provide a reference for the statement: "This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required."  
  - Page 2, Section 1, last paragraph: While the contribution of the work is clear, the authors should emphasize that such a selective encoding model has not been proposed before (if this is accurate). Additionally, the related work section should precede the methods section.  
  - Figure 1 vs. Table 1: The authors present two examples of abstractive summarization, but one example should suffice. Furthermore, one is labeled as a figure and the other as a table, which is inconsistent.  
  - Section 3.2, lines 230–234 and 234–235: Please provide references for the following statements: "In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence"; "Some previous works apply this framework to summarization generation tasks."  
  - Figure 2: The term "MLP" is used but not explained in the paper.  
  - Page 3, lines 289–290: The sigmoid function and element-wise multiplication are not defined in the formulas presented in Section 3.1.  
  - Page 4, first column: Several components of the formulas are undefined, including \( b \) (Equation 11), \( W \) (Equations 12, 15, 17), \( U \) (Equations 12, 15), and \( V \) (Equation 15).  
  - Page 4, line 326: The readout state \( r_t \) is not depicted in Figure 2 (workflow).  
  - Table 2: The meaning of "(ref)" is unclear and should be clarified.  
  - Section 4.3, Model Parameters and Training: The authors should explain how they determined the values for the various parameters, such as word embedding size, GRU hidden states, \( \alpha \), \( \beta1 \), \( \beta2 \), \( \epsilon \), and beam size.  
  - Page 5, line 450: Consider removing the word "the" in this line: "SGD as our optimizing algorithms" instead of "SGD as our the optimizing algorithms."  
  - Page 5, Beam Search: Please include a reference for beam search.  
  - Figure 4: There appears to be a typo in the true sentence: "council of europe again slams french prison conditions" (should "again" be "against"?).  
  - Typo: Replace "supper script" with "superscript" (occurs four times).