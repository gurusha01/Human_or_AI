This paper explores various methods for encoding arbitrarily long sequences of digits using the major system. In this system, each digit is associated with one or more characters representing consonantal phonemes, with the mappings between digits and phonemes being predefined. The encoded output typically consists of a sequence of words, where the digits in the original sequence correspond to specific characters or digraphs in the resulting word sequence. Vowels are added around the consonantal phonemes to form words, but their placement is unconstrained. The paper proposes several techniques to encode digit sequences in a way that enhances the memorability of the output, primarily by employing syntactic constraints and heuristics.
I found the application of natural language processing (NLP) concepts in this work somewhat intriguing, as I have not encountered an ACL paper addressing this specific topic before. However, the paper and its underlying ideas come across as somewhat dated. The heavy reliance on n-gram models for generation, frequent POS-tag sequences, and heuristic-based approaches gives the work an old-fashioned feel, reminiscent of research from 15-20 years ago. I am uncertain whether the level of novelty in the ideas presented here is sufficient to justify publication in ACL in 2017. The paper does not offer a significant contribution to NLP itself, such as advancements in modeling or search techniques, nor does it make a compelling case for its contribution to the application domain, which is essentially a form of constrained generation.
Given that the task involves transforming one sequence into another through a straightforward monotonic mapping, a character-level sequence-to-sequence encoder-decoder model (e.g., Sequence to Sequence Learning with Neural Networks by Sutskever et al., 2014) seems well-suited for this problem. Such a model would likely produce fluent outputs with fewer components (e.g., no need for trigram models, POS tagging, scoring heuristics, or postprocessing with a bigram model). Additionally, it could leverage large-scale training data from any genre, eliminating the need for a pre-tagged corpus or a dependency on a parser. This approach would align better with the expectations of a 2017 paper.