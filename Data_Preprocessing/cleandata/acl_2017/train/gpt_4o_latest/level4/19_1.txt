- Strengths:  
This paper presents a novel approach to enhancing zero pronoun resolution performance. The primary contributions of the paper are as follows: 1) it introduces a straightforward method to automatically generate a large training dataset for the zero pronoun resolution task; 2) it employs a two-step learning process to transfer knowledge from a large dataset to domain-specific data; and 3) it distinguishes unknown words by assigning them different tags. Overall, the paper is well-written, and the experiments are meticulously designed.
- Weaknesses:  
However, I have a few concerns regarding the identification of a zero pronoun's antecedent:  
1. How is the antecedent identified when the predicted word is a pronoun? The authors propose matching the head of noun phrases, but it is unclear how the system handles cases where the head word is not a pronoun.  
2. What happens if the predicted word is a noun that does not appear in the preceding text?  
3. While the system demonstrates excellent performance on standard datasets, would it be possible to evaluate the system in two stages? Specifically, the first stage could assess the model's ability to recover the dropped zero pronoun into a word, and the second stage could evaluate how effectively the system identifies the antecedent.  
Additionally, I am curious about the authors' decision to use an attention-based neural network. Including a brief explanation of the rationale behind this choice would be valuable for other researchers.  
- Minor Comment:  
In Figure 2, should the labels be "s1, s2, ..." instead of "d1, d2, ..."?  
- General Discussion:  
Overall, this is an excellent paper with innovative ideas and a robust experimental setup.