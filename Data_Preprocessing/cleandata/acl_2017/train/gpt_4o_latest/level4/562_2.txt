The paper introduces a method for relation extraction by reframing the task as a question answering problem. The central premise is that questions serve as a more versatile medium for encoding content compared to specific examples of relations, and they are also easier to generate. The results indicate promising performance, although a direct evaluation on a standard relation extraction benchmark is absent.
- Strengths:  
The proposed technique demonstrates strong capability in identifying relations, achieving an F-measure slightly below 90. It performs effectively on unseen questions for known relations and reasonably well on entirely unseen relations. Additionally, the authors outline a method for constructing a large-scale training dataset.
- Weaknesses:  
The lack of evaluation on standard relation extraction datasets is a limitation, as it becomes difficult to assess potential biases in the data used (relations are derived from Wikidata via WikiReading, which is based on Wikipedia, rather than sources like newswire or newsgroups). The NIST TAC-KBP slot filling dataset would be a suitable benchmark for comparison.  
Another missing comparison is training a traditional relation detection model on the generated dataset to evaluate its performance against the proposed QA-based approach.
- General Discussion:  
The paper is well-written, with a compelling argument and an intriguing idea that appears to work effectively. Notably, the zero-shot natural language method performed comparably to the single-question baseline and was not far behind the multi-question system, which I found particularly interesting.