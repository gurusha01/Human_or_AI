- Update after rebuttal:
I appreciate the authors' efforts in clarifying the baseline implementation and providing evidence regarding the significance of their reported improvements. These clarifications should certainly be included in the camera-ready version. I find the idea of leveraging visual features for these languages intriguing, and I look forward to seeing how this approach might be extended to more challenging tasks in future work.
- Strengths:
  - Treating Chinese/Japanese/Korean characters as visual entities is a novel and compelling idea!
- Weaknesses:
  - The experimental results demonstrate only marginal improvements over the baseline, and the evaluation setup makes it difficult to validate a key claim: that visual features enhance performance for rare or unseen words.
  - Some baseline implementation details remain unclear, which complicates the interpretation of results and poses challenges for reproducibility.
- General Discussion:
The paper explores the use of computer vision techniques (CNNs applied to text images) to enhance language processing for Chinese, Japanese, and Korean, where characters exhibit compositional properties. The authors evaluate their model on a straightforward text-classification task (assigning Wikipedia page titles to categories). Their findings indicate that while a simple one-hot representation of characters outperforms CNN-based representations, combining visual representations with one-hot encodings yields better performance than either approach alone. They also provide some evidence that visual features outperform one-hot encodings for rare words and include qualitative results suggesting that the CNN learns meaningful semantic embeddings of characters.
The idea of processing languages like Chinese and Japanese through a visual lens is compelling, and the paper's motivation is well-founded. However, I remain unconvinced by the experimental results. The evaluations are relatively weak, making it difficult to determine whether the results are robust or coincidental. I would prefer to see more rigorous evaluation to strengthen the paper's case for publication. If the authors can demonstrate statistical significance in their rebuttal, I would be inclined to support acceptance, though ideally, a more targeted evaluation would be preferable.
- Specific Comments:
  - In Section 3, under the "lookup model" paragraph, the embeddings used are not explicitly stated, nor is it clear whether they are fine-tuned via backpropagation like the visual embeddings. This lack of clarity hinders the comparability of the baseline and visual embeddings. If the baseline embeddings were not task-specific while the visual embeddings were, this raises concerns about the fairness of the comparison.
  - The choice to evaluate on Wikipedia page title classification is puzzling. The primary justification for the visual model is its ability to generalize to rare or unseen characters. Why not focus on a task that directly tests this capability? For instance, evaluating on machine translation of out-of-vocabulary (OOV) words would better highlight the advantages of visual features. While the authors' argument for visual conceptualization of certain languages is valid, the current evaluation does not effectively expose the limitations of standard approaches, thereby weakening the case for visual features.
  - In Table 5, are the reported improvements statistically significant? Clarifying this would strengthen the paper's claims.
  - Figure 4 is difficult to interpret. Since it represents a key result, it should be presented more clearly to emphasize the model's contribution. If I understand correctly, the x-axis represents word rarity (likely log frequency), with rarer words on the left. The visual model's intersection with the x-axis further left than the lookup model suggests better performance on rare words. However, why don't both models intersect at the same x-axis point, given they are evaluated on the same dataset and trained with identical data? A concise summary of this figure's intended message in the rebuttal would be helpful.
  - Regarding fallback fusion, why not present performance across various thresholds? Using a threshold of 0 seems like an edge case and may not reflect the broader applicability of the technique.
  - The simple/traditional experiment for unseen characters is a promising direction but is treated as an afterthought. I would have liked to see more extensive evaluation in this area, such as classification of unseen words.
  - Consider adding translations to Figure 6 for readers unfamiliar with Chinese.