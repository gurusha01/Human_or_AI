This paper introduces a framework for evaluating word embeddings based on data efficiency and straightforward supervised tasks. The primary motivation stems from the common use of word embeddings in transfer learning scenarios, where evaluation often focuses on how quickly a target model can be trained. The proposed approach employs a collection of simple supervised tasks, including standard benchmarks such as word similarity and word analogy. Experiments conducted on a diverse set of embeddings reveal that rankings tend to vary across tasks and depend on the amount of training data available.
Strengths
- The focus on transfer learning and data efficiency is compelling, as it aligns with the practical use of embeddings as a straightforward "semi-supervised" technique.
Weaknesses
- A robust evaluation method should demonstrate consistency in downstream tasks. Specifically, if the approach assigns a rank R to a set of embeddings, this ranking should ideally hold for end tasks such as text classification, parsing, or machine translation. However, the paper does not assess this, making it difficult to determine whether the proposed technique is genuinely more effective than traditional methods.
- The discussion on injective embeddings appears unrelated to the main topic and does not seem to contribute to the paper's overall understanding.
- The experimental section is unclear. For instance, Section 3.7 claims to answer questions like "is it worth fitting syntax-specific embeddings even when the supervised dataset is large?" but it is unclear how these conclusions were derived from the evaluation.
- Also in Section 3.7, the manuscript states, "This hints that purely unsupervised large-scale pretraining might not be suitable for NLP applications." This is a strong claim, but it is unclear how this conclusion follows from the proposed evaluation method.
- All embeddings used in the experiments are off-the-shelf pretrained models, meaning there is no control over the training corpora. This lack of control undermines the validity of the evaluation results.
- The manuscript requires proofreading, particularly regarding the placement of figure citations (e.g., Figure 1, located on page 3, is only referenced on page 6).
General Discussion
The paper begins with an intriguing motivation but fails to convincingly demonstrate the effectiveness of the proposed approach. As noted, intrinsic evaluation methods should ideally be validated by showing that their conclusions generalize to downstream tasks, which is not addressed in this work. Additionally, the lack of clarity and the need for proofreading detract from the overall readability. Moving forward, the paper could benefit significantly from extrinsic evaluations and a more controlled experimental setup (e.g., training all embeddings on the same corpora). However, in its current form, I do not believe it is a suitable addition to the conference.