- Strengths:  
The paper addresses the challenging task of answering open-domain questions using Wikipedia. The authors propose two key components: (1) a document retriever to identify relevant Wikipedia articles for a given question, and (2) a document reader to extract precise answers from the retrieved paragraphs. The model is fine-tuned using distant supervision. Experimental results demonstrate that the document retriever outperforms the WikiSearch API, while the document reader achieves better performance compared to some recent QA models.
- Weaknesses:  
The final results fall short of the performance achieved by other models, as acknowledged by the authors. Furthermore, the paper lacks an error analysis.
- General Discussion:  
The proposed system is an end-to-end pipeline and presents an interesting approach. However, I have the following concerns:
Document Retriever: While the authors show improved retrieval performance over WikiSearch, the exact methodology for using the WikiSearch API is not clearly described. WikiSearch may not be an ideal baseline for handling "questions," as it is more suited for structured retrieval tasks. Why didn't the authors compare their approach against standard information retrieval (IR) baselines?
Distant Supervision: The effectiveness and reliability of distant supervision remain unclear. The authors appear to have limited the number of training examples due to this approach, but among the examples used, what proportion was "close to correct"? Providing statistics on this would help clarify whether further fine-tuning of the distant supervision process could improve performance.
Full Wikipedia Results: The authors aim to answer open-domain questions using the entirety of Wikipedia. However, as they report, the full system achieves a performance of 26.7 (49.6 when the correct document is provided, and 69.5 when the correct paragraph is given). This highlights the need for further improvements in the retrieval component. For WebQuestions, the system performs significantly worse than YodaQA, raising the question of whether Wikipedia alone is sufficient for answering all open-domain questions. Should the authors consider developing an integrated model that incorporates additional resources to address this limitation?
Overall, the results presented in Tables 4 and 5 are not competitive with other models. While the authors restrict their system to using Wikipedia, the findings do not suggest that this is the most effective strategy.
- Other Points:  
1. The F1 score reported in Table 5 (78.4) differs from the values in Table 4 (both Dev and Test).  
2. In Table 5, why is there no result for "No f_emb"?  
3. The paper lacks an error analysis for the system's various components. Are there specific types of questions where the system underperforms? Could the authors identify which questions are better suited for Wikipedia-based answers and apply this method selectively?  
4. For WebQuestions, the use of distant supervision appears to degrade performance further.