- Strengths:  
This paper introduces a novel and significant metric for assessing the quality of word embeddings, specifically focusing on "data efficiency" when applied to various supervised tasks.  
Another notable aspect of the paper is the authors' clear delineation of three key questions: (1) whether supervised tasks provide deeper insights into evaluating embedding quality, (2) the stability of rankings with respect to labeled dataset size, and (3) the relative advantages of linear versus non-linear models.  
The authors conducted an extensive set of experiments to address these questions, and the findings are engaging and potentially valuable for the research community.  
- Weaknesses:  
The overall findings may have limited practical utility for machine learning practitioners in this domain, as they largely reaffirm existing knowledge or assumptions—namely, that performance depends on factors such as the task, labeled dataset size, and model type. Consequently, the results are not particularly actionable. However, the reviewer acknowledges that this thorough analysis contributes to a deeper understanding of the topic.  
- General Discussion:  
The presentation of the paper could be improved in several ways:  
1. The order of figures and tables should align with their appearance in the text. Currently, their sequence appears somewhat arbitrary.  
2. There are several typos (e.g., L250, L579). A spell checker should be used to address these.  
3. Equation 1 is not particularly useful, and its presentation feels awkward. It could be removed, with the textual explanation sufficing.  
4. L164 references an "Appendix," but no appendix is included in the paper.  
5. The public skip-gram dataset mentioned in L425 lacks a citation.  
6. The claim in L591–593 is overly strong and requires clarification, particularly regarding the conditions under which it holds true.  
7. The observation in L642–645 is both intriguing and significant. Expanding on this with concrete evidence or examples from specific embeddings would strengthen the paper. Visualization could also enhance this discussion.  
8. L672 should include examples of "specialized word embeddings" and explain how they differ from general-purpose embeddings.  
9. Figure 3 is too small and difficult to read.