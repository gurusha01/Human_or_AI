Strengths:
The proposed model is novel, with a particularly appealing feature: its ability to generate keyphrases that are not explicitly present in the source text.
Weaknesses:
The paper needs to clarify whether all evaluated models are trained and tested on identical datasets. Additionally, the explanation of the copy mechanism is somewhat unclear and not entirely convincing.
General Discussion:
This work introduces a supervised neural network approach for keyphrase generation. The model employs an encoder-decoder architecture, where the encoder uses an RNN to process the input text, and the decoder leverages an attention mechanism to generate keyphrases from the hidden states. A more advanced version of the decoder incorporates an attention mechanism that conditions on the keyphrase generated in the previous time step.
The model is both interesting and innovative, with its ability to generate keyphrases not present in the source text being a particularly strong aspect. However, my primary concern lies with the evaluation: it is unclear whether all evaluated models are trained on the same amount of data and tested on the same datasets. For instance, in the case of the NUS dataset, Section 4.2 (line 464) mentions that the supervised baselines are evaluated using cross-validation, but this point requires further clarification.
Other Comments:
The paper is generally well-written and easy to follow, but certain sections could benefit from greater clarity:
- Absent Keyphrases vs. OOV: The distinction between absent keyphrases and out-of-vocabulary (OOV) words needs to be made explicit, and the term "OOV" should be used consistently. According to Section 3.4 (line 372) and Section 5.1 (line 568), the RNN models use the 50,000 most frequent words as their vocabulary, implying that OOV refers to words outside this vocabulary. However, in line 568, are you referring to OOV words or absent keyphrases? Additionally, how many keyphrases fall outside this 50,000-word vocabulary? The term "unknown words" in line 380 is also ambiguous. It might be clearer to state that the RNN models can generate words not found in the source text, provided they appear elsewhere in the corpus and are part of the 50,000-word vocabulary.
- Copy Mechanism (Section 3.4): The explanation of the copy mechanism could be improved. This mechanism appears to have a more specific locality than the attention model in the basic RNN. However, the intuition behind it is somewhat misleading. If I understand correctly, the copy mechanism is conditioned on the source text locations that match the keyphrase generated in the previous time step (y_{t-1}). This might result in a higher likelihood of generating n-grams observed in the source text (as illustrated in Figure 1). While I agree that the more advanced attention model likely contributes to CopyRNN's superior performance compared to the basic RNN, it is unclear why this model performs particularly well for absent keyphrases. It seems that both models perform similarly on present keyphrases, so further clarification is needed.
- Word Embedding Initialization: How are the word embeddings initialized? This detail is missing and would be helpful to include.