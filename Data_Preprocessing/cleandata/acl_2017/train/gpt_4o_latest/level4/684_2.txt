This paper introduces an intriguing model for reading comprehension by illustrating the multiplicative interactions between the query and the local context surrounding a word in a document. The authors also propose a novel gated-attention mechanism to capture this relationship. The work is robust, achieving near state-of-the-art performance across all four cloze-style datasets. However, some additional improvements could further benefit similar tasks.
That said, I have the following concerns:
1. While the authors cite several papers from arXiv, I believe some highly relevant works have been overlooked. For instance, the work by Caiming Xiong et al. (https://openreview.net/pdf?id=rJeKjwvclx) and the work by Shuohang Wang et al. (https://openreview.net/pdf?id=B1-q5Pqxl) focus on improving attention mechanisms to model interactions between documents and queries. Although these studies are evaluated on SQuAD rather than cloze-style datasets, a fundamental or experimental comparison with these approaches may be necessary.
2. There are prior studies that employ attention mechanisms or their variants specifically tailored for reading comprehension tasks, and this paper shares conceptual similarities with them. I recommend conducting comparisons with such works to further strengthen the experimental results presented in this paper.