- Strengths:  
The concept of hard monotonic attention is innovative and significantly distinct from existing approaches.
- Weaknesses:  
The experimental outcomes on morphological inflection generation are somewhat mixed. The proposed model demonstrates effectiveness when the training data is limited (e.g., CELEX) or when the alignment is predominantly monotonic and less reliant on context (e.g., Russian, German, and Spanish).
- General Discussion:  
The authors introduced a novel neural model for morphological inflection generation that employs "hard attention" and utilizes character alignments obtained separately through a Bayesian method for transliteration. This approach is markedly different from the prior state-of-the-art neural model for the task, which relies on "soft attention" and jointly addresses character alignment and conversion within a probabilistic framework.
The proposed idea is both original and well-founded. The paper is well-written, and the experimental evaluation is thorough. However, a notable limitation is that the proposed method does not consistently achieve state-of-the-art performance under all conditions. It is particularly well-suited for tasks involving predominantly monotonic alignment and less context-sensitive phenomena. The paper would be more compelling if it elaborated on the practical advantages of the proposed method, such as ease of implementation and computational efficiency.