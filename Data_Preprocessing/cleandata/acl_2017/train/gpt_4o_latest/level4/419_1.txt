The paper presents a straightforward and effective approach for morphological paradigm completion in low-resource scenarios. The method employs a character-based seq2seq model trained on a combination of examples from two languages: a resource-poor language and a closely-related resource-rich language. Each training instance is annotated with paradigm properties and a language identifier, enabling transfer learning across languages that share common characters and paradigms. While the multi-lingual approach itself is not novel (similar architectures have been explored in syntax, language modeling, and machine translation), the novelty of this work lies in its application to morphology. The experimental results demonstrate significant improvements over monolingual baselines and provide a comprehensive analysis of how language similarities influence performance. The paper is engaging, well-written, and would make a valuable addition to the conference program.
Detailed comments:
— My primary concern is why the proposed multilingual approach was restricted to language pairs rather than extended to groups of related languages. For instance, incorporating all Romance languages into the training could potentially enhance Spanish paradigm completion, while combining all Slavic languages using Cyrillic script might improve Ukrainian. Exploring the extension from bilingual to multilingual settings would be an interesting direction.
— Using Arabic as a baseline seems inappropriate and largely uninformative, given the significant differences in script and morphology compared to the target languages. A more meaningful baseline might involve a language with a partially overlapping alphabet but differing typology. For example, a Slavic language with Latin script could serve as a baseline for Romance languages. Even if Arabic is excluded and a more distant language within the same family is used as a baseline, the experimental results remain strong.
— The half-page discussion on the role of Arabic as a regularizer adds little value to the paper. I would recommend removing Arabic from the experiments entirely and instead incorporating a regularizer, which, as noted in footnote 5, outperforms the inclusion of Arabic as a transfer language.
— The related work section overlooks a body of research on "language-universal" RNN models that adopt a similar approach: learning shared parameters across multiple languages and using a language tag to mediate between them. Relevant studies include a multilingual parser (Ammar et al., 2016), multilingual language models (Tsvetkov et al., 2016), and multilingual machine translation (Johnson et al., 2016).
Minor:
— The claim in line 144 that POS tags are easy to transfer across languages appears to be inaccurate. Transferring POS annotations is itself a challenging task.
References:
Waleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah A. Smith. "Many languages, one parser." TACL 2016.
Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. "Polyglot neural language models: A case study in cross-lingual phonetic representation learning." NAACL 2016.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat et al. "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation." arXiv preprint arXiv:1611.04558 2016.
— Response to author response:
Thank you for your response! I look forward to reviewing the final version of the paper.