- Strengths: The paper introduces a novel encoder-decoder model that explicitly incorporates monotonicity.
- Weaknesses: The proposed model might essentially be a standard BiRNN with decoupled alignments. Furthermore, the evaluation is limited to morphology without exploring other monotonic Seq2Seq tasks.
- General Discussion:
The authors present a new encoder-decoder neural network architecture with "hard monotonic attention" and evaluate it on three morphology datasets.
This paper is challenging to assess. On the one hand, it is well-written, mostly clear, and proposes a novel idea—integrating monotonicity into morphology tasks. 
The motivation for incorporating monotonicity is evident: unlike machine translation, many Seq2Seq tasks exhibit monotonicity, making general encoder-decoder models less suitable. The fact that such models still perform reasonably well highlights the robustness of neural techniques overall. The paper's core idea is to explicitly enforce monotonicity during output character generation. This is achieved by decoupling alignment and transduction, first aligning input-output sequences monotonically and then training the model to generate outputs consistent with these monotonic alignments. However, some aspects of this approach remain unclear. I have the following questions:
1) Could you clarify the nature of your alignments? The alignments appear to be 1-to-many, as shown in the example in Fig. 1, where one input character aligns with zero, one, or multiple output characters. However, this seems inconsistent with the description in lines 311-312, which suggests several input characters aligning to a single output character. Are your alignments 1-to-many, many-to-1, or many-to-many?
2) A straightforward approach to monotonic Seq2Seq tasks involves a two-step process: first, align input and output characters monotonically with a 1-to-many constraint (using any monotone aligner, such as Jiampojamarn and Kondrak's toolkit). Then, train a standard sequence tagger (e.g., an LSTM) to predict these alignments. For instance, in the example "flog → fliege" (line 613), one could align as "f-l-o-g / f-l-ie-ge" and train the tagger to predict "f-l-ie-ge" (output sequence of length 4) from "f-l-o-g" (input sequence of length 4). This approach has been discussed in prior work, such as [*, Section 4.2]. 
2a) How does your method differ from this simpler approach?
2b) Why was this approach not included as a baseline?
Additional concerns:
3) It is unfortunate that the evaluation is restricted to morphology. There are numerous other monotonic Seq2Seq tasks where your method could demonstrate its advantages, given its explicit modeling of monotonicity (see also [*]).
4) The claim of performing "on par or better" (line 791) seems overly optimistic. There is a tendency in NLP to classify cases of underperformance as "on par" and all others as "better." I suggest revising this phrasing, though the experimental results themselves are acceptable.
5) The linguistic features used in your model are not sufficiently detailed. From Fig. 1, it appears that features such as POS are included.
5a) Where were these features sourced from?
5b) Could these features, rather than the monotonicity constraints, be responsible for the improved performance in some cases?
Minor points:
6) Equation (3): Please rewrite \( NN \) as \( \text{NN} \) or a similar format.
7) Line 231: "Where" should begin with a lowercase "w."
8) Line 237 and elsewhere: \( x1 \ldots xn \). The mathematical convention is to write \( x1, \ldots, xn \) but \( x1 \cdots xn \), with dots aligned to surrounding symbols.
9) Figure 1: Is the use of Cyrillic font necessary? It may be inaccessible to readers without the required fonts.
10) Line 437: Replace "these" with "those."
[*] 
@InProceedings{schnober-EtAl:2016:COLING, 
  author    = {Schnober, Carsten  and  Eger, Steffen  and  Do Dinh,
Erik-L\^{a}n  and  Gurevych, Iryna},
  title     = {Still not there? Comparing Traditional Sequence-to-Sequence
Models to Encoder-Decoder Neural Networks on Monotone String Translation
Tasks},
  booktitle = {Proceedings of COLING 2016, the 26th International Conference on
Computational Linguistics: Technical Papers},
  month     = {December},
  year                                                      = {2016},
  address   = {Osaka, Japan},
  publisher = {The COLING 2016 Organizing Committee},
  pages     = {1703--1714},
  url                                               =
{http://aclweb.org/anthology/C16-1160}
}
AFTER AUTHOR RESPONSE
Thank you for the clarifications. It seems there may have been some confusion in your response regarding alignments (possibly due to a coding issue). However, I understand that you are aligning 1-0, 0-1, 1-1, and later constructing many-to-many alignments from these. 
While you compare your approach to Nicolai, Cherry, and Kondrak (2015), my question was more specific: why not use 1-x (x ∈ {0,1,2}) alignments as in Schnober et al. and train a neural tagger (e.g., BiLSTM) on these? I am curious how much your results would differ from such a simpler baseline. A tagger is inherently monotonic, and given monotonic alignments, the entire process remains monotonic. In contrast, your method starts with a general model and then imposes hard monotonicity constraints.
NOTES FROM AC
Additionally, Cohn et al. (2016) (http://www.aclweb.org/anthology/N16-1102) is highly relevant.
Is your architecture related to methods like the Stack LSTM, which also predicts a sequence of actions to modify or annotate an input?
Finally, do you believe using a greedy alignment approach results in any loss compared to Rastogi et al. (2016), which employs hard monotonic attention but sums over all alignments?