Paraphrased Review:
Strengths:  
Zero-shot relation extraction is a compelling and challenging problem. The authors have constructed a substantial dataset for relation extraction framed as a question-answering task, which has the potential to be a valuable resource for the research community.
Weaknesses:  
The paper lacks sufficient comparison to and acknowledgment of prior work. Additionally, the contributions of the paper do not appear to be particularly novel.
General Discussion:  
The authors approach relation extraction as a reading comprehension task. To train reading comprehension models for relation extraction, they generate a large dataset of 30 million "querified" (converted into natural language) relations by employing mechanical turk annotators to write natural language queries for relations derived from a schema. They adapt the reading comprehension model proposed by Seo et al. (2016) by adding the capability to return a "no relation" response, as the original model was designed to always provide an answer. The primary motivation and outcome of the paper seem to be the ability to perform zero-shot relation extraction, where relations unseen during training are extracted at test time.
The paper is well-written, and the idea is intriguing. However, the experimental evaluation and comparison to prior work are insufficient to convincingly establish the novelty and impact of the contributions.
First, the paper overlooks a significant amount of related work. For example, Neelakantan et al. (2015) (https://arxiv.org/abs/1504.06662) address zero-shot relation extraction using RNNs over knowledge base paths. Verga et al. (2017) (https://arxiv.org/abs/1606.05804) focus on relation extraction for unseen entities. The authors do cite Bordes et al. (https://arxiv.org/pdf/1506.02075.pdf), who collected a similar dataset and performed relation extraction using memory networks (commonly applied in reading comprehension tasks). However, the authors only mention that Bordes et al.'s dataset was annotated at the "relation" level rather than at the triple (relation, entity pair) level. Could Bordes et al. not have adopted a similar annotation strategy? If there is a meaningful distinction here, it is not clearly articulated in the paper. Additionally, the NAACL 2016 paper (https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) introduces a novel memory network-based model for relation extraction. There are likely additional relevant works as well. Given the strong resemblance of this work to prior studies, the authors should cite and clarify their novelty relative to these works, ideally as early as the introduction. The lack of clarity on this point was noticeable from the outset.
Second, the authors fail to either 1) evaluate their model on an existing dataset or 2) benchmark previously published models on their dataset. This omission significantly weakens the empirical results. Considering the abundance of prior work addressing similar tasks and the limited novelty of this approach, the authors must include experiments demonstrating that their method outperforms existing techniques on the task or provide evidence that their dataset offers distinct advantages over others (e.g., its larger size enabling better generalization).