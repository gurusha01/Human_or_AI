This paper extends existing word embedding algorithms (GloVe, Skip-Gram, PPMI, SVD) by incorporating ngram-ngram cooccurrence statistics. To address the computational challenges of storing large matrices, the authors propose an algorithm that employs two distinct strategies for collecting counts.
- Strengths:
* The proposed approach is a logical extension of prior work on word embedding learning. By incorporating bigram information, the method has the potential to capture richer syntactic and semantic relationships.
- Weaknesses:
* Although the authors aim to learn embeddings for bigrams (bi_bi case), they do not quantitatively evaluate the learned bigram embeddings, apart from the qualitative analysis presented in Table 7. A more rigorous quantitative evaluation on tasks such as paraphrasing or other related applications involving bigram representations would have strengthened the contribution.
* The evaluation and corresponding results are not particularly compelling. The trends observed in the results are inconsistent, and some of the reported improvements lack statistical significance.
* The paper suffers from numerous grammar and spelling issues, making it difficult to read smoothly. A thorough editing pass is necessary to improve the overall readability.
- General Discussion:
This work builds upon standard embedding learning techniques by incorporating bigram-bigram cooccurrence information. While the idea is interesting and represents a natural progression of existing research, the methods and evaluation leave several questions unanswered. In addition to the concerns outlined in the weaknesses, I have a few minor questions for the authors:
* What accounts for the significant differences between the overlap and non-overlap cases? Beyond the quantitative differences observed in the tasks, a deeper explanation would be valuable.
I have reviewed the author response and look forward to seeing a revised version of the paper.