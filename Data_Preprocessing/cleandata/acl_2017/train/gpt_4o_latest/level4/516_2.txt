Paraphrased Review
Strengths:
- Provides a clear and detailed explanation of methods and evaluation.
- Effectively utilizes and interprets a diverse set of evaluations.
- Demonstrates the practical applicability of the technique in real-world interactive topic modeling scenarios.
Weaknesses:
- Lacks discussion of related work on anchor words.
- The evaluation on the 20 Newsgroups dataset is suboptimal.
- The theoretical contribution is relatively minor.
General Discussion:
The paper introduces a novel method for interactive user-driven topic specification, termed Tandem Anchors. This method builds on the anchor words algorithm, a matrix factorization-based approach for learning topic models, by substituting individual anchors derived from the Gram-Schmidt process with constructed anchor pseudowords. These pseudowords are formed by combining sparse vector representations of multiple words that represent a topic facet. The authors demonstrate that using a harmonic mean function to construct pseudowords is optimal, as it yields the greatest improvement in classification accuracy of document-topic distribution vectors compared to Gram-Schmidt. Additionally, the method is shown to be faster than existing interactive approaches, enabling iterative interaction, and a user study confirms that multiword anchors are both more intuitive and more effective for users.
Overall, I found this contribution to be highly valuable. It represents a straightforward modification of an existing algorithm that delivers significant benefits in an interactive context. I commend the authors for their comprehensive evaluation across multiple dimensions. Although the technical contribution itself is modest—primarily a strategy for constructing pseudowords based on topic facets—the depth and rigor of the evaluation justify its acceptance as a full paper rather than a short one. However, I would have appreciated more exploration of how to construct these facets in the absence of convenient resources like category titles in the 20 Newsgroups dataset or when initializing a topic model for interactive learning.
One issue I encountered with the paper is the reliance on the 20 Newsgroups dataset for evaluation, which I believe is not ideal for topic modeling. The dataset contains documents of widely varying lengths, preprocessing plays a significant role, and many of the messages are difficult for users to interpret. Moreover, naive bag-of-words models often outperform topic models on this dataset. While classification tasks are a useful proxy for assessing how well a topic model captures meaningful distinctions in text, alternative tasks—such as classifying news articles by section or reviews by subject category—might have been more appropriate. Additionally, it would have been beneficial to include a use case that aligns more closely with a common application of topic models, such as corpus exploration.
The paper also omits several relevant comparisons. It provides limited discussion of work published since the original anchor word model was proposed. Beyond comparing to standard Gram-Schmidt, it would have been valuable to include comparisons to methods like Lee et al. (2014), "Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference." References to Nguyen et al. (2013), "Evaluating Regularized Anchor Words," and Nguyen et al. (2015), "Is Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models," would also have enriched the discussion, as these works offer important insights into the anchor selection process.
Additional Notes:
- Line 164: Consider revising "…entire dataset" for clarity.
- Lines 164–166: The meaning here is unclear. Are you suggesting that a single pass takes too long? I assumed you would retrain the model on a subset of the data rather than the full dataset. Please clarify.
- Lines 261 & 272: Why didn't you explore the use of the "and" operator or element-wise max? These seem analogous to union and intersection concepts from the "or" operator and element-wise min, and it's unclear why the chosen options were preferred.
- Line 337: Capitalize "Usenet."
- Lines 338–340: Why limit the number of words to fewer than 100? This seems like a strict threshold. Did you remove headers, footers, or quotes from the messages?
- Lines 436–440: A more detailed explanation of what this reveals about confusion would be helpful.
- Line 692: Consider revising the phrasing around "using tandem anchors."
In conclusion, this paper represents a significant contribution to interactive topic modeling. I believe it has the potential to be a valuable resource for researchers and practitioners outside the machine learning community, enabling them to explore, classify, and test hypotheses about their corpora.
Post-Response:
I appreciate the authors' thoughtful responses to my questions. However, I still believe that comparing to complementary non-interactive work is valuable, even if it addresses a different aspect of the problem.