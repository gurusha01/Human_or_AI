This paper presents a cutting-edge CCG parsing model that decomposes into tagging and dependency scores while employing an efficient A* decoding algorithm. Notably, the model slightly surpasses the performance of Lee et al. (2016)'s more expressive global parsing approach, likely due to the factorization simplifying the learning process. The authors also provide results for another language, demonstrating significant improvements over prior work on Japanese CCG parsing. A particularly intriguing and novel finding is that treating the first word of a constituent as the head yields substantially better performance compared to linguistically motivated head rules.
Overall, this is a strong paper that makes a valuable contribution. I have only a few suggestions for improvement:
- The interaction between the dependency and supertagging models is well-designed, but it would be helpful to include baseline results for simpler variations (e.g., not conditioning the tag on the head dependency).
- The paper achieves new state-of-the-art results on Japanese with a notable margin. However, given the relatively limited prior work on this dataset, would it be feasible to train the Lee et al. parser on the same data for a direct comparison?
- The work by Lewis, He, and Zettlemoyer (2015), which explores combined dependency and supertagging models for CCG and SRL, might be relevant and worth citing.