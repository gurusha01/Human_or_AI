This paper addresses the problem of knowledge base (KB) completion and introduces ITransF as a solution. Unlike STransE, which assigns an independent matrix to each relation, this work proposes parameter sharing across relations. Specifically, the authors construct a tensor \( D \), where each slice represents a relational matrix, and use a selection vector \( \alpha \) to identify a subset of relevant relational matrices for composing a specific semantic relation. Additionally, the paper describes a method to enforce sparsity in \( \alpha \). Experimental results on two standard benchmark datasets demonstrate the superiority of ITransF over previous approaches.
The paper is generally well-written, and the experimental results are promising. However, I have several concerns that I hope the authors will address in their response.
1. Simply arranging relational matrices into a tensor and applying a selection mechanism (or, more accurately, computing a linearly weighted sum of the relational matrices) does not inherently enable information sharing between the relational matrices. Information sharing would require techniques such as tensor decomposition, where the slices (relational matrices) are projected into a shared lower-dimensional core tensor. It is unclear why this approach was not considered, given the stated motivation to share information across relational matrices.
2. The two objectives—(a) sharing information across relational matrices and (b) enforcing sparsity in the attention vectors—appear somewhat contradictory. If the attention vector is truly sparse and contains many zeros, the corresponding slices will not receive updates during optimization, thereby limiting information flow.
3. The authors devote significant space to discussing methods for computing sparse attention vectors. On page 3, they mention that \( \ell1 \)-regularization did not work in preliminary experiments. However, no experimental results are provided to support this claim, nor is there an explanation for why \( \ell1 \)-regularization is unsuitable for this task. As \( \ell1 \)-regularization is a standard baseline and is computationally efficient, it seems like an obvious choice to try. Instead, the authors use \( \ell0 \)-regularization, which leads to NP-hard optimization problems. They then propose a technique and a rather crude approximation to address this issue. Much of this complexity could have been avoided if \( \ell_1 \)-regularization had been used.
4. The vector \( \alpha \) performs a selection or weighting over the slices of \( D \). Referring to this as "attention" is somewhat misleading, as the term "attention" in NLP typically refers to a different class of models (e.g., attention mechanisms in machine translation).
5. The authors initialize optimization using pre-trained embeddings from TransE. It is unclear why random initialization, as used in TransE, cannot be employed here. Using TransE embeddings as the starting point raises concerns about the fairness of comparisons with TransE, as the proposed method benefits from pre-trained embeddings.
The idea of learning associations between semantic relations has been explored in related NLP problems, such as relational similarity measurement [Turney, JAIR 2012] and relation adaptation [Bollegala et al., IJCAI 2011]. It would be beneficial to contextualize the current work in relation to these prior studies, which also model inter-relational correlation and similarity.
Thank you for the opportunity to review this work.