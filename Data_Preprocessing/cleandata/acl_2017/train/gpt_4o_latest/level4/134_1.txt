- Strengths:
The manuscript is well-structured and written in a clear, comprehensible manner. The methods and results presented are engaging and thought-provoking.
- Weaknesses:
The evaluation process and the resulting outcomes raise potential concerns (refer to my detailed comments below).
- General Discussion:
This study introduces a system for end-to-end argumentation mining leveraging neural networks. The authors approach the problem using two methodologies: (1) sequence labeling and (2) dependency parsing. Additionally, the paper explores the application of a multitask learning framework within the sequence labeling approach. The motivation behind the proposed model is articulated effectively. Unlike existing methods that rely on ILP, manual feature engineering, and the manual design of ILP constraints, the proposed model eliminates such manual interventions. Furthermore, it jointly learns the subtasks involved in argumentation mining, thereby mitigating the error propagation issues typically encountered in pipeline methods. While there are a few missing details (highlighted below), the methods are generally well-explained.
The experimental setup is comprehensive, the comparisons are appropriately conducted, and the results are intriguing. However, my primary concern lies with the small dataset size and the high capacity of the employed (Bi)LSTM-based recurrent neural networks (BLC and BLCC). The training dataset comprises only around 320 essays, with 80 essays reserved for testing. The size of the development set, however, is not specified in either the main paper or the supplementary materials. This raises concerns, as the limited number of training essays poses a significant challenge. The total number of tags in the training data likely amounts to only a few thousand, which is considerably smaller compared to standard sequence labeling tasks that often involve hundreds of thousands or even millions of tags. Consequently, I am uncertain whether the model parameters have been adequately trained. Furthermore, the paper does not address the issue of overfitting. It would be beneficial to include an analysis of training and development "loss" values over the course of training (e.g., after each parameter update or epoch). The authors do provide a statement that could be interpreted as evidence of overfitting: Line 622 states, "Our explanation is that taggers are simpler local models, and thus need less training data and are less prone to overfitting."
For similar reasons, I am uncertain about the stability of the models. Reporting the mean and standard deviation across multiple runs (with different parameter initializations) would help address this concern. Additionally, conducting statistical significance tests would provide further insights into the models' stability and the reliability of the results. Without such tests, it is difficult to ascertain whether the improved results are due to the superiority of the proposed method or merely due to chance.
Although the neural networks employed in this study incorporate regularization techniques, the small dataset size necessitates greater attention to regularization. The paper does not discuss regularization in detail, and the supplementary materials only briefly mention regularization in the context of LSTM-ER. This issue needs to be thoroughly addressed in the manuscript.
The authors might also consider adopting Bayesian optimization methods for hyperparameter tuning instead of the current approach described in the supplementary materials.
Additionally, I recommend moving the information about pre-trained word embeddings and the error analysis from the supplementary materials into the main paper. The inclusion of this information should fit within an additional page.
It would also be helpful to include inter-annotator agreement scores. The paper describing the dataset contains relevant information that could provide valuable insights into system performance and the potential for improvement.
Finally, consider enhancing Figure 1 by using distinct colors to improve its clarity, especially for black-and-white prints.
- Edit:
Thank you for addressing my questions. I have updated my recommendation score to 4. Please ensure that the F1-score ranges are included in the paper, along with the mean and variance for different settings. I remain concerned about the stability of the models. For instance, the large variance observed in the Kiperwasser setting warrants further analysis. Even an F1 range of [0.56, 0.61] is relatively wide. Including these score ranges in the paper will facilitate reproducibility of your work.