- Summary:  
This paper presents a novel dataset for sarcasm interpretation and introduces a system named Sarcasm SIGN, which is built upon the Moses machine translation framework. The dataset consists of 3,000 sarcastic tweets (tagged with `sarcasm`) and five human-provided interpretations for each tweet. Sarcasm SIGN modifies Moses by clustering sentiment-related words on the source side (sarcastic tweets) and de-clustering their translations on the target side (non-sarcastic interpretations). While Sarcasm SIGN achieves comparable performance to Moses on standard machine translation (MT) metrics, it surpasses Moses in terms of fluency and adequacy.  
- Strengths:  
The paper is well-written.  
The dataset has been collected in a systematic and appropriate manner.  
The experiments are conducted with care, and the analysis is thorough and reasonable.  
- Weaknesses:  
The paper does not provide key statistics about the dataset (e.g., average sentence length, vocabulary size).  
Moses, as a baseline, is not ideal due to the small size of the dataset.  
The assumption that "sarcastic tweets often differ from their non-sarcastic interpretations in as little as one sentiment word" is not substantiated by the data.  
- General Discussion:  
This section elaborates on the weaknesses of the paper.  
A significant portion of the paper is dedicated to the new sarcasm interpretation dataset. However, critical details about the dataset, such as average sentence length and vocabulary size, are missing. Moreover, the paper does not provide statistical evidence to justify the focus on sentiment-related words in their approach.  
Given the small size of the dataset (only 3,000 tweets), it is likely that many words appear infrequently. As a result, Moses is not an appropriate baseline for comparison. A more suitable baseline would be an MT system specifically designed to handle rare words effectively. Interestingly, the clustering and de-clustering approach used in Sarcasm SIGN is itself a strategy for addressing rare words.  
Sarcasm SIGN is built on the premise that "sarcastic tweets often differ from their non-sarcastic interpretations in as little as one sentiment word." However, Table 1 contradicts this assumption, showing that human-provided interpretations often differ from the original tweets in more than just sentiment-related words. I strongly recommend that the authors provide statistical evidence from the dataset to validate their assumption. Without such evidence, the foundation of Sarcasm SIGN appears to be ad hoc.  
---
I have reviewed the authors' response but maintain my original decision for the following reasons:  
- The authors stated that "the Fiverr workers might not take this strategy." To me, this contradicts the principles of corpus-based NLP. A model should be designed to fit the data, not the other way around, where the data must conform to the model's assumptions.  
- The authors argued that "the BLEU scores of Moses and SIGN are above 60, which is generally considered decent in the MT literature." However, I find this unconvincing because the dataset consists of very short sentences. Furthermore, as shown in Table 6, Moses only changes 42% of the sentences, meaning that in more than half of the cases, the translation is simply a copy of the input. Despite this, the BLEU score exceeds 60, which diminishes its significance.  
- The authors claimed that "while higher scores might be achieved with MT systems that explicitly address rare words, these systems don't focus on sentiment words." While this may be true, it raises the question of whether sentiment words are rare in the dataset. If they are, then MT systems designed to handle rare words should naturally address them as well (alongside other rare words).