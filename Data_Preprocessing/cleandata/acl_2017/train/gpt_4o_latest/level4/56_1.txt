- Strengths:  
This paper introduces an extension to several widely-used methods for learning vector representations of text. Traditional approaches, such as skip-gram with negative sampling, GloVe, and other PMI-based techniques, rely on word co-occurrence statistics. However, these methods can be generalized to incorporate n-gram co-occurrence statistics. While such an extension would significantly increase the complexity of the algorithms—due to the exponential growth in both the embedding vocabulary and the context space—this paper proposes an efficient method for learning embeddings for n-grams using n-gram contexts. The proposed approach is computationally efficient and demonstrates strong performance on similarity and analogy tasks.
- Weaknesses:  
The paper would have been significantly stronger if it included experiments on downstream tasks where these embeddings are utilized as input, beyond the similarity and analogy evaluations already presented.
- General Discussion:  
Despite the above-mentioned limitation, I believe this is a valuable contribution to have at ACL.  
I have reviewed the author response.