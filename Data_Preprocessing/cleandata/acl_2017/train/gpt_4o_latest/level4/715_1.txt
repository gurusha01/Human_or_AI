Paraphrased Review
Strengths:
- Clearly defined task.
- Simple model achieving state-of-the-art results on SQuAD (single model).
- Comprehensive evaluation and comparison.
Weaknesses:
- Limited analysis of errors and results (see detailed comments below).
General Discussion:
This paper introduces a method for directly querying Wikipedia to answer open-domain questions. The proposed system comprises two main components: a module for querying and retrieving Wikipedia articles, and another module for answering questions based on the retrieved articles.
The document retrieval component is a traditional information retrieval (IR) system that leverages term frequency models and n-gram counts. The answering module employs a feature representation for paragraphs that includes word embeddings, indicator features to identify whether a paragraph word appears in the question, token-level features such as POS and NER, and a soft feature designed to capture similarity between question and paragraph tokens in embedding space. These features are combined and input into a bi-directional LSTM RNN for encoding. For questions, a separate RNN processes the word embeddings. The system independently trains classifiers to predict the start and end spans of sentences within paragraphs to generate answers.
The training process incorporates multiple open-domain QA datasets, such as SQuAD and WebQuestions, by modifying the training data to include articles retrieved by the IR engine instead of solely relying on the correct document or passage.
Overall, this is an engaging and easy-to-follow paper, but I have a few concerns and questions:
1) The IR system achieves an impressive Accuracy@5 of over 75%, and the document reader performs well individually, outperforming the best single models on SQuAD. However, there is a notable performance drop in Table 6. The authors mention that when testing with the best paragraph instead of the retrieved results, accuracy improves to 0.49 (from 0.26), but this is still far below the 0.78â€“0.79 accuracy achieved on the SQuAD task. This suggests that the neural network for matching struggles to learn effectively when using the modified training set (which includes retrieved articles) compared to the scenario where training and testing are conducted on the document understanding task. The paper would benefit from an analysis of this issue. What were the training accuracies in both scenarios? Are there potential strategies to mitigate this gap? While the authors briefly touch on this in the conclusion, a more detailed discussion in the main paper would provide valuable insights.
2) The authors chose to treat this as a pure machine comprehension task and avoided relying on external resources like Freebase, which could have assisted with entity typing. While this approach aligns with their goals, it would have been interesting to explore the impact of incorporating such resources. Building on the first question, if the errors stem from highly relevant topical sentences, as the authors suggest, could entity typing have helped reduce these errors?
Additionally, the authors should reference QuASE (Sun et al., 2015, WWW 2015) and similar systems in their related work. QuASE is another open-domain QA system that answers questions using retrieved passages, though it relies on the web rather than solely on Wikipedia.