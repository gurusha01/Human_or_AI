This paper introduces a self-learning bootstrapping method for learning bilingual word embeddings, achieving competitive performance in bilingual lexicon induction and cross-lingual word similarity tasks while requiring minimal bilingual supervision. Notably, the approach performs well even with a very small seed dictionary (as few as 25 entries) or when the dictionary is constructed without language-specific knowledge (e.g., using shared numerals between languages).
The manuscript is exceptionally well-written, and I commend the authors for their clarity. While the contribution is not groundbreaking (it feels more suited for a "short paper" format), the work is "eclectic" in its ability to synthesize ideas and methodologies from prior research. However, the paper does not fully acknowledge earlier work on self-learning/bootstrapping, particularly in pre-embedding contexts. I appreciated the paper overall, but there are several research directions that remain unexplored. Additionally, incomplete recognition of related work and the absence of comparisons with key baselines are significant concerns that should be addressed in future revisions.
Self-learning/bootstrapping of bilingual vector spaces: While this work is among the first to address the highly constrained setup for learning cross-lingual embeddings, it is not the first attempt (e.g., see Miceli Barone and others). The proposed method is the first to apply a truly bootstrapping/self-learning approach to embeddings, but the concept of bootstrapping bilingual vector spaces is not novel. Earlier work using similar ideas with traditional "count-based" bilingual vector spaces should be acknowledged, such as Peirsman and Pado (NAACL 2010) and Vulic and Moens (EMNLP 2013). Additionally, Ellen Riloff's work on bootstrapping semantic lexicons in monolingual settings is relevant and should be cited.
Relation to Artetxe et al.: The proposed bootstrapping algorithm appears to be an iterative extension of the model introduced by Artetxe et al., with the primary difference being the reparametrization (lines 296–305). It is unclear whether the performance gains stem from this reparametrization alone or from a combination of the algorithm and the new parametrization. A more explicit discussion in the text would help clarify this point.
Comparison with prior work: Several closely related papers are not mentioned or discussed. For example, Duong et al. (EMNLP 2016) on "learning cross-lingual word embeddings without bilingual corpora" is highly relevant and should be cited. Similarly, Vulic and Korhonen (ACL 2016) explore the role of seed lexicons in learning bilingual embeddings, including scenarios with very small seed dictionaries. These papers could also inform the discussion on future work, such as improving the selection of reliable translation pairs during the iterative process. Another recent and relevant work is Smith et al. (ICLR 2017), which investigates "offline bilingual word vectors, orthogonal transformations, and the inverted softmax." This paper examines similar limited-resource setups (e.g., using shared words and cognates) and could serve as a useful point of comparison. Reporting results for setups relying solely on shared words would enable direct comparisons with Smith et al. (ICLR 2017).
Seed dictionary size and bilingual lexicon induction: The proposed algorithm appears to be largely invariant to the size of the seed dictionary, achieving similar bilingual lexicon induction (BLI) scores regardless of the starting point. While this is an intriguing finding, it also highlights a limitation of current "offline" approaches: they seem to have reached a performance ceiling. Vulic and Korhonen (ACL 2016) demonstrated that increasing the size of the seed lexicon does not significantly improve results, and this work suggests that even a minimal seed lexicon (25 to 5,000 pairs) suffices to achieve near-optimal performance. Additional discussion on how to surpass this ceiling and further improve BLI results with offline methods would be valuable. For instance, Smith et al. (ICLR 2017) report higher scores on the same dataset, and linking this work to theirs could provide insights into breaking the plateau.
The authors also mention plans to fine-tune the method to work without any bilingual evidence in future work. While this is an admirable goal, it may be more practical to focus on strategies for overcoming the apparent performance ceiling of linear mapping approaches, as shown in Figure 2.
Convergence criterion/training efficiency: The convergence criterion, which is critical for both the efficiency and effectiveness of the algorithm, is only briefly mentioned. It is unclear how the procedure terminates. Based on the description, it seems that the method relies on vanishing variation in cross-lingual word similarity performance as the stopping criterion. However, this would make the approach applicable only to language pairs with available cross-lingual word similarity datasets. For example, it is unclear how the method terminates for English-Finnish, given the lack of such a dataset. A more detailed explanation of the convergence criterion is necessary.
Minor comments:
- The Finnish "Web as a Corpus" (WaC) corpus is available (lines 414–416): https://www.clarin.si/repository/xmlui/handle/11356/1074.
- Since the authors claim the method can work with a seed dictionary containing only shared numerals, it would be interesting to test it on language pairs that do not share an alphabet (e.g., English-Russian, English-Bulgarian, or more distant pairs like English-Arabic or English-Hindi).
After the response: I appreciate the authors' detailed response, which clarified several of my initial concerns. I hope these clarifications will be incorporated into the final version of the paper, should it be accepted.