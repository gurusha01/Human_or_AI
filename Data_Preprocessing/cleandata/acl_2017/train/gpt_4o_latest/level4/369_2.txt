The paper proposes a method to enhance two-step translation using deep learning. Results are demonstrated for Chinese->Spanish translation, though the approach appears to be largely language-agnostic.
The framework follows a standard two-step MT setup. In the first step, translation is performed into a morphologically underspecified version of the target language. The second step employs machine learning to predict the missing morphological categories and generates the final output by inflecting the underspecified forms using a morphological generator. The primary contribution of this work lies in the use of deep neural networks (DNNs) as classifiers in the second step. Additionally, the authors introduce a rescoring step that leverages a language model (LM) to select the best variant.
Overall, this is a well-executed study with strong empirical results: the classifier models achieve high accuracy, significantly outperforming baselines such as SVMs, and the improvement is evident in the final translation quality.
My main concern with the paper is the absence of a comparison with straightforward deep-learning baselines. Specifically, the authors address a structured prediction problem using independent local decisions followed by a rescoring step. (Unless I misunderstood the methodology.) However, this is inherently a sequence labeling task, which recurrent neural networks (RNNs) are well-suited for. For instance, how would a bidirectional LSTM perform when trained and applied in the standard sequence labeling paradigm? After reviewing the author response, I still believe that the baselines (including the standard LSTM) were evaluated in the same framework, i.e., independently for each local label. If this is not the case, the paper and response should have clarified this more explicitly. This is problematic because the authors deviate from the standard usage of RNNs but do not justify why their approach is superior or compare it against the conventional method.
The rescoring step is also not entirely clear. Are you rescoring n-best sentences? What features are used? Or are you searching a weighted graph for the single optimal path? This aspect requires more clarity in the paper. My current understanding is that you construct a graph, identify the K best paths, generate inflected sentences from these paths, and then use a language model (and nothing else) to select the best variant. However, this is not entirely clear from the paper. Unfortunately, this issue was not adequately addressed in the response.
You mention that larger word embeddings result in longer training times. Do they also affect the final performance?
Can you provide an explanation for why incorporating information from the source sentence degrades performance? This seems counterintuitive—does the lack of source information sometimes lead to errors, such as losing number agreement? A more detailed discussion on this, supported by a few illustrative examples, would be valuable in the final version.
The paper contains several typos, and the overall quality of English may not meet the standards required for presentation at ACL.
Minor corrections:
- "context of the application of MT" → "context of application for MT"
- "In this cases, MT is faced in two-steps" → "In this case, MT is divided into two steps"
- "markov" → "Markov"
- "CFR" → "CRF"
- "task was based on a direct translation" → "task was based on direct translation"
- "task provided corpus" → "task provided corpora"
- "the phrase-based system has dramatically" → "the phrase-based approach..."
- "investigated different set of features" → "...sets of features"
- "words as source of information" → "words as the source..."
- "correspondant" → "corresponding"
- "Classes for gender classifier" → "Classes for the..."
- "for number classifier" → "for the..."
- "This layer's input consists in" → "...consists of"
- "to extract most relevant" → "...the most..."
- "Sigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer would produce (-1, 1)."
- "information of a word consists in itself" → "...of itself"
- "this $A$ set" → "the set $A$"
- "empty sentences and longer than 50 words" → "empty sentences and sentences longer than..."
- "classifier is trained on" → "classifier is trained in"
- "aproximately" → "approximately"
- "coverage raises the 99%" → "coverage exceeds 99%" (unless this interpretation is incorrect)
- "in descendant order" → "in descending order"
- "cuadratic" → "quadratic" (in multiple places)
- "but best results" → "but the best results"
- "Rescoring step improves" → "The rescoring step..."
- "are not be comparable" → "are not comparable"