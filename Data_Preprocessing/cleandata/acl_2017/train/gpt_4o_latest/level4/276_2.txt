- Strengths: The paper is clearly written, and the methodology is presented in a straightforward and comprehensible manner. Despite the simplicity of the proposed contribution, the results demonstrate notable improvements, particularly in the error correction task.
- Weaknesses: The level of novelty is relatively modest, as the work primarily involves a different permutation of tasks within the multitask learning paradigm. Additionally, only a single approach to task combination is investigated. For instance, it would have been valuable to explore whether pre-training performs significantly worse compared to joint training or to examine alternative strategies, such as initializing weights using an existing RNN language model trained on unlabeled data.
- General Discussion: I debated between assigning a score of 3 or 4. While the experiments are well-executed and the task combinations are occasionally novel, the field of multitask learning in RNNs is already well-explored (as acknowledged through the citations), making it challenging to find this work particularly groundbreaking. Nonetheless, I recommend acceptance, as the experimental findings could prove beneficial for other researchers.
- Post-rebuttal: After reviewing the rebuttal, my opinion of the paper remains unchanged.