This paper introduces a straightforward attention-based RNN model designed to generate SQL queries directly from natural language, bypassing the need for intermediate representations. To achieve this, the authors adopt a data augmentation strategy, iteratively collecting additional data through crowd annotations based on user feedback regarding the accuracy of the generated SQL queries. Experimental results on both benchmark and interactive datasets demonstrate that data augmentation holds promise as an effective approach.
Strengths:
- The model avoids the use of intermediate representations.  
- The release of a potentially valuable dataset on Google Scholar.  
Weaknesses:
- The paper claims to achieve results comparable to the state of the art, but the reported performance on GeoQuery and ATIS datasets does not substantiate this claim.  
General Discussion:
This is a solid piece of research with potential implications for advancing semantic parsing in downstream applications. However, I found the claims of achieving "near-state-of-the-art accuracies" on ATIS and GeoQuery datasets to be somewhat overstated, as the results show an 8-point gap compared to Liang et al. (2011). That said, achieving state-of-the-art performance need not be the primary focus of this work, as it makes a meaningful contribution in its own right. I would support the acceptance of this paper at ACL, provided the authors revise their claims accordingly. Additionally, I have a few questions and suggestions for the authors:
- What exactly do the authors mean by "minimal intervention"? If this refers to minimal human intervention, the claim seems inconsistent with the methodology. If it refers to the absence of intermediate representations, this term should be used instead for greater clarity.  
- In Table 6, could the authors provide a breakdown of the scores in terms of correctness and incompleteness? What percentage of the queries exhibit incompleteness?  
- What level of expertise is required from the crowd-workers tasked with producing correct SQL queries?  
- An analysis of the 48% of user questions for which the model could not generate SQL queries would be valuable.  
- Figure 3 is somewhat unclear; the sharp dips in performance around the 8th/9th stages are difficult to interpret without additional explanation.  
- Table 4 would benefit from clarification regarding the splits used to compute the ATIS results.  
I appreciate the authors' efforts and thank them for their responses.