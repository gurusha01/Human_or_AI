- Strengths:
1) The paper aims to address the gap between Stochastic Gradient MCMC and Stochastic Optimization within the context of deep learning. Considering that techniques like dropout and DropConnect, as well as variational inference, are widely employed to mitigate overfitting, a more systematic approach to introducing and analyzing Bayesian learning-based algorithms could be highly beneficial for the deep learning community.  
2) For language modeling tasks, the proposed SG-MCMC optimizer combined with dropout demonstrates superior performance compared to RMSProp with dropout. This result clearly highlights that incorporating uncertainty modeling can effectively reduce overfitting, thereby enhancing accuracy.  
3) The paper provides comprehensive details regarding the model and experimental setups, ensuring that the results can be easily reproduced.  
- Weaknesses:
1) The paper does not delve into theoretical proofs or provide an analysis of the convergence properties of the proposed algorithm.  
2) The comparison is limited to SG-MCMC versus RMSProp, with no additional benchmarks. Furthermore, the relationship between pSGLD and RMSProp is only briefly mentioned as counterparts in two distinct families, without sufficient elaboration.  
3) The paper lacks a detailed discussion on the impact of the proposed method on training speed.  
- General Discussion: