- Strengths: This paper investigates a relatively under-explored domain involving the practical application of Bayesian neural network principles to NLP tasks. By employing a Bayesian framework for RNN parameters, the authors demonstrate how model averaging can be effectively incorporated during inference. Additionally, their gradient-based sampling method for posterior estimation offers a straightforward implementation and is potentially more computationally efficient compared to other established model averaging techniques, such as ensembling.  
The proposed approach is validated across three distinct tasks—language modeling, image captioning, and sentence classification—where it achieves performance improvements over the baseline of single model optimization.
- Weaknesses: The experimental setup lacks clarity. Key details, such as burn-in duration, the number of epochs, and the number of samples collected, are relegated to the supplementary material but should be included in the main paper for better transparency. Furthermore, additional clarification on the inference process would be beneficial. Specifically, it is unclear whether the samples obtained via HMC after burn-in on the training data were fixed for inference (i.e., whether the same samples were used for every \(\tilde{Y}\) during test time, as suggested by Equation 5).  
An explicit explanation of the independence assumption \(p(D|\theta) = p(Y, X | \theta) = p(Y | \theta, X)p(X)\), which appears to justify the use of the conditional RNN model for the potential \(U(\theta)\), would also enhance the paper's completeness and clarity.  
In terms of comparisons, the paper would benefit from a discussion or experimental evaluation against ensembling and distillation methods, such as "Sequence Level Knowledge Distillation" (Kim and Rush) and "Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser" (Kuncoro et al.), which share a similar objective of leveraging model averaging effects.  
Further elaboration on the preference for HMC-based sampling methods over alternative sampling techniques or variational approximations would strengthen the discussion.  
Lastly, Equation 8 suggests a potential equivalence between dropout and the proposed approach. A more concrete theoretical justification for combining SGLD and dropout, along with an explanation of this equivalence, would provide deeper insights into the effectiveness of the proposed method.
- General Discussion: The points mentioned above summarize the key areas of strength and improvement.