The paper introduces a recurrent neural network architecture capable of skipping irrelevant input units during processing. This is implemented by defining three parameters: R (the number of words read at each "skim"), K (the maximum jump size), and N (the maximum number of jumps allowed). An LSTM processes R words, predicts a jump size k from the set {0, 1, ..., K} (where 0 indicates stopping), skips the next k-1 words, and continues until either the jump limit N is reached or the end of the input is encountered. Although the model is not differentiable, it can be trained using standard policy gradient methods. The approach appears to draw significant inspiration from Shen et al. (2016), who employed a similar reinforcement learning framework (including variance stabilization) for multi-pass machine reading.
- Strengths:
The proposed model effectively simulates an intuitive "skimming" behavior akin to how readers process text, paralleling the self-terminated repeated reading approach of Shen et al. A key strength of this work lies in its simplicity, which does not compromise its performance. Despite its straightforward design, the model achieves promising results. Notably, the authors demonstrate through a well-crafted synthetic experiment that the model can successfully learn to skip when provided with oracle jump signals. In real-world text classification tasks, the model performs competitively with a non-skimming baseline while offering significant speed advantages.
The model's practical implications are noteworthy: for tasks where skimming is sufficient (e.g., sentiment classification), it suggests that equivalent results can be obtained without processing all input data exhaustively. To the best of my knowledge, this is a novel and valuable finding.
- Weaknesses:
The mechanism by which the model determines its skipping behavior remains somewhat unclear, especially outside the controlled setting of the synthetic dataset. For example, consider a scenario where the latter part of a sentence contains critical information, such as:
"The movie was so so and boring to the last minute but then its ending blew me away."
In this case, the model might decide to skip the remainder of the sentence after reading "so so and boring," thereby missing the pivotal phrase "ending blew me away" and incorrectly classifying the sentiment as negative. While the authors propose running the skimming model in both directions as a potential future solution, such cases may necessitate a more sophisticated architecture to better manage skimming decisions.
Additionally, combining skimming with multi-pass reading (potentially in reverse directions) could enhance performance. This approach mirrors how humans read complex text that cannot be fully understood in a single skimâ€”indeed, it reflects how I approached reading this draft.
Overall, the paper addresses an intriguing problem and offers an effective yet intuitive solution.