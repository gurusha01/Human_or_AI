- Strengths:  
The paper presents a natural and practical extension to recent advancements in interactive topic modeling by enabling human annotators to provide multiple "anchor words" for machine-generated topics. It is well-structured, and the combination of synthetic experiments and user studies contributes to making it a robust and compelling submission.
- Weaknesses:  
The scope of the paper is somewhat narrow in terms of the interactive topic modeling approaches it benchmarks against. While the authors justify this by referencing most of the alternative methods and explaining that these approaches are either too slow for interactive use or not well-suited to the "anchoring" interface being proposed, some empirical evidence to support these claims would have strengthened the argument.  
Additionally, the experiments are limited to a single dataset (20 Newsgroups), which has been extensively studied in the past. Including results from additional datasets would have enhanced the generalizability of the findings.
- General Discussion:  
Overall, this is a strong paper that provides an incremental yet innovative and practical contribution to the field of interactive topic modeling. The authors have thoroughly evaluated several variations of their approach through simulated experiments and conducted detailed quantitative analyses of both simulated and user studies, employing diverse metrics to assess different aspects of topic quality.