- Strengths:
1. The paper is well-written and structured, particularly in the initial sections, where the presentation is excellent, and the argumentation is mostly strong.  
2. It tackles an important problem by exploring the incorporation of word order information into word (and sense) embeddings, and the proposed approach is intriguing.
- Weaknesses:
1. The results lack consistency, leaving doubts about whether the proposed models outperform existing alternatives, especially given their added complexity. While negative results are acceptable, the analysis provided is insufficient to extract meaningful insights. Furthermore, the absence of results on the word analogy task—apart from noting that the proposed models were not competitive—represents a missed opportunity for further exploration and analysis.  
2. Certain aspects of the experimental setup are unclear or insufficiently justified, particularly regarding the choice of corpora and datasets (details provided below).  
3. The quality of the paper declines in the later sections, leaving the reader somewhat disappointed—not only with the results but also with the clarity of the presentation and the strength of the argumentation.
- General Discussion:
1. The authors aim to "learn representations for both words and senses in a shared emerging space," but this is only implemented in the LSTMEmbed_SW model, which consistently underperforms compared to the alternatives. Moreover, the motivation for learning shared representations for words and senses is unclear and not adequately discussed in the paper.  
2. The rationale behind predicting pre-trained embeddings is not explicitly explained. Additionally, it is unclear whether the pre-trained embeddings in the LSTMEmbed_SW model represent words, senses, or a combination of both. If multiple configurations are possible, the specific setup used in the experiments should be clarified.  
3. While the importance of learning sense embeddings is well-recognized and emphasized by the authors, these embeddings are not explicitly evaluated—or, if they are, the evaluation is unclear. Most of the word similarity datasets used in the experiments treat words without considering their context.  
4. The size of the training corpora is not specified. For example, Figure 4 compares different proportions of BabelWiki and SEW, but the comparison may be problematic if the corpus sizes differ significantly. Additionally, SemCor is a very small corpus, and it is uncommon to use such a limited dataset for training embeddings with methods like word2vec. If the proposed models are particularly suited to small corpora, this should be explicitly stated and evaluated.  
5. Some test sets, such as WS353, WSSim, and WSRel, are not independent, which complicates comparisons and inflates the number of "wins" (e.g., three instead of one).  
6. The paper claims that the proposed models are faster to train due to the use of pre-trained embeddings in the output layer, but no evidence is provided to support this claim. Including such evidence would strengthen the paper.  
7. In Table 4, the dimensionality of the embeddings is inconsistent across models. Using the same dimensionality would allow for a fairer comparison.  
8. The paper lacks a section on synonym identification under similarity measurement, which should describe the approach used for the multiple-choice task.  
9. A reference to Table 2 is missing in the text.  
10. There is no description of any training process for the word analogy task, even though the corresponding dataset is mentioned.