This paper categorizes keyphrases into two types: (1) Absent keyphrases (those that do not correspond to any contiguous subsequences in the source document) and (2) Present keyphrases (those that fully align with a portion of the text). The authors employ RNN-based generative models (referred to as RNN and Copy RNN) for keyphrase prediction, leveraging the copy mechanism in RNN to predict phrases that have already appeared.
Strengths:
1. The concept of forming and extracting keyphrases that are absent from the current document is an intriguing idea with substantial research significance.
2. The paper is well-written and easy to follow.
3. The application of RNN and Copy RNN in this context is novel. While deep recurrent neural networks have already demonstrated strong performance in keyphrase extraction, it would be compelling to see a clear motivation for preferring RNN and Copy RNN over these more established deep recurrent neural network approaches.
Weaknesses:
1. The paper requires additional discussion on the convergence of the proposed joint learning process (for RNN and Copy RNN). This would help readers understand how stable points in the probabilistic metric space are achieved. Without this, reproducing the results may prove challenging.
2. The evaluation process compares the current system (which extracts both Present and Absent keyphrases) against baselines that only handle "Present" keyphrases. However, there is no direct comparison of the performance of the proposed system with state-of-the-art or benchmark systems specifically for "Present" keyphrases. Since local (Present) keyphrases are also critical for a document, the lack of explicit discussion on this aspect is a limitation. It would be valuable to analyze the impact of the RNN and Copy RNN models on the automatic extraction of local or Present keyphrases.
3. The effect of document size on keyphrase extraction is another important consideration. Published results from [1] show significantly better performance than the current system on the Inspec (Hulth, 2003) abstracts dataset, which raises concerns about the scalability and robustness of the proposed approach.
4. The paper mentions that 527,830 documents were used for training, while 40,000 publications were withheld for training baselines. It is unclear why all available publications were not utilized for training the baselines. Additionally, the topical details of the dataset (527,830 scientific documents) used for training RNN and Copy RNN are not provided, which could hinder reproducibility.
5. Since the proposed system captures semantics through RNN-based models, it would be more informative to compare its performance against systems that also focus on semantic representation. For instance, Ref-[2] could serve as a strong baseline for comparison.
Suggestions for Improvement:
1. Based on the example provided in Figure 1, it appears that all "Absent" keyphrases are essentially "Topical phrases" (e.g., "video search," "video retrieval," "video indexing," and "relevance ranking"). These phrases define the domain, sub-domain, or topics of the document. To better evaluate the "Absent" keyphrases, it would be interesting to identify all topical phrases across the corpus using methods like tf-idf and then relate the document to the high-ranked extracted topical phrases using metrics such as Normalized Google Distance or PMI. Similar approaches have been applied in query expansion techniques, where the goal is to relate a document to a query when matching terms are absent from the document.
References:
1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257â€“266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845).