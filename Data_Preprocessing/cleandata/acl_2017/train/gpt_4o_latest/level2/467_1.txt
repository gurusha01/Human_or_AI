Review
Summary and Contributions
This paper presents a novel self-learning framework for learning bilingual word embeddings with minimal bilingual resources. The primary contribution is the ability to achieve competitive results using only a 25-word seed dictionary or even an automatically generated list of numerals. The proposed method iteratively refines the embedding mapping and dictionary through a self-learning process, leveraging the structural similarity of monolingual embedding spaces. The authors demonstrate that their approach achieves results comparable to state-of-the-art methods that rely on richer bilingual resources, such as large dictionaries or parallel corpora. Additionally, the paper provides a theoretical analysis of the implicit optimization objective of the self-learning framework, offering insights into its robustness and convergence properties.
Strengths
1. Practical Impact: The method significantly reduces the reliance on bilingual resources, which is crucial for low-resource language pairs. The ability to use as little as 25 word pairs or numerals makes the approach highly practical and accessible.
2. Strong Empirical Results: The experiments on bilingual lexicon induction and cross-lingual word similarity demonstrate that the proposed method achieves competitive performance with minimal resources, outperforming baseline methods under low-resource settings.
3. Theoretical Insight: The analysis of the implicit optimization objective is a valuable addition, providing a deeper understanding of why the self-learning framework works effectively and how it avoids degenerate solutions.
4. Reproducibility: The authors provide detailed experimental settings and promise to release code and resources, ensuring that the work can be reproduced and extended by the community.
Weaknesses
1. Limited Evaluation on Diverse Language Pairs: While the method is tested on English-Italian, English-German, and English-Finnish, the evaluation could be extended to more diverse and typologically distant language pairs (e.g., languages with different scripts or morphologies) to better assess its generalizability.
2. Dependence on Initialization: The paper acknowledges that random initialization performs poorly, and the method relies on a small seed dictionary to avoid poor local optima. This limits the claim of being "unsupervised" and suggests that the method is not yet fully robust to initialization.
3. Scalability Concerns: Although the authors address efficiency by vectorizing computations, the iterative nature of the framework may still pose scalability challenges for very large vocabularies or embedding spaces.
4. Comparison with Parallel Corpus Methods: While the paper compares favorably with dictionary-based methods, the comparison with parallel corpus-based methods (e.g., Luong et al., 2015) is less comprehensive. The authors attribute the poor performance of these methods to smaller training corpora but do not explore hybrid approaches that combine monolingual and parallel data.
Questions to Authors
1. Have you tested the method on language pairs with non-Latin scripts (e.g., Chinese-English) or highly divergent morphologies (e.g., Turkish-English)? If not, how do you anticipate the method would perform in such cases?
2. Could you elaborate on why random initialization fails and whether alternative initialization strategies (e.g., adversarial training) might improve performance?
3. Is the proposed method sensitive to the quality of the monolingual embeddings? How would it perform with embeddings trained on smaller or noisier corpora?
Additional Comments
The paper is well-written and addresses an important problem in cross-lingual NLP. The proposed method is simple yet effective, and the theoretical analysis strengthens its contributions. However, further exploration of its limitations, such as initialization dependence and scalability, would make the work more robust. Overall, this is a strong submission with significant potential for impact in low-resource language processing.