Review
Summary and Contributions
This paper addresses the challenge of extracting commonsense physical knowledge from natural language text, focusing on relative physical attributes (size, weight, strength, rigidness, and speed) of objects and their implications in action contexts. The authors propose a joint inference model using a factor graph to simultaneously learn (1) relative physical knowledge of object pairs and (2) physical implications of actions involving those objects. They introduce a new dataset, VERBPHYSICS, compiled through crowdsourcing, and demonstrate the efficacy of their approach through empirical evaluations. The paper claims three main contributions: (1) introducing a novel task of commonsense physical knowledge extraction, (2) proposing a joint inference model for learning physical relations, and (3) creating the VERBPHYSICS dataset.
Strengths
1. Novelty and Scope: The paper tackles a unique and underexplored problem of extracting physical commonsense knowledge from text, particularly focusing on dimensions like strength and rigidness, which are not easily addressed by existing computer vision techniques. This complements ongoing work in multimodal AI.
2. Joint Inference Model: The proposed factor graph model effectively integrates knowledge about objects and actions, leveraging semantic similarities, action-object compatibility, and cross-knowledge correlations. This is a well-motivated and innovative approach that outperforms baselines in both frame prediction and object pair prediction tasks.
3. Dataset Contribution: The creation of the VERBPHYSICS dataset is a significant contribution. It provides a valuable resource for future research in commonsense reasoning and physical knowledge extraction, particularly given the scarcity of datasets in this domain.
4. Empirical Results: The experiments are thorough, with results demonstrating the superiority of the proposed model over baselines. The ablation study provides insights into the contributions of different components of the model.
Weaknesses
1. Limited Real-World Validation: While the model performs well on the VERBPHYSICS dataset, its applicability to real-world, unstructured text remains unclear. The reliance on curated datasets and controlled crowdsourcing may limit generalizability.
2. Sparse Discussion of Limitations: The paper does not sufficiently discuss limitations, such as the potential biases introduced by crowdsourcing or the challenges posed by metaphorical language, which the authors briefly mention but do not address in depth.
3. Comparison with Related Work: Although the paper situates itself within the broader context of commonsense knowledge extraction, the comparison with related work is somewhat limited. For instance, it would be helpful to benchmark against existing commonsense knowledge bases like ConceptNet or COMET to provide a clearer sense of novelty and improvement.
4. Scalability Concerns: The reliance on crowdsourced annotations for seed knowledge and evaluation raises concerns about the scalability of the approach to larger datasets or more diverse domains.
Questions to Authors
1. How does the model handle ambiguous or conflicting physical relations in real-world text, especially when metaphorical or abstract language is involved?
2. Could the proposed approach be extended to incorporate multimodal data (e.g., images or videos) to complement textual information?
3. How does the model compare to existing commonsense knowledge bases like ConceptNet in terms of coverage and accuracy for physical knowledge?
Overall Assessment
This paper presents a novel and well-executed approach to extracting physical commonsense knowledge from text, with clear contributions in terms of methodology, dataset creation, and empirical results. However, the lack of real-world validation, limited discussion of limitations, and scalability concerns temper its impact. With additional work to address these issues, the approach has significant potential to advance the field of commonsense reasoning. I recommend acceptance with minor revisions.