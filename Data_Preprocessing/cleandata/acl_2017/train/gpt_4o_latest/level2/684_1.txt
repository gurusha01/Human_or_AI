Review of the Paper
Summary and Contributions:  
This paper introduces the Gated-Attention (GA) Reader, a novel model for answering cloze-style questions over documents. The GA Reader integrates a multi-hop architecture with a unique attention mechanism based on multiplicative interactions between query embeddings and intermediate states of a recurrent neural network document reader. The key contributions of the paper are:  
1. The design of the Gated-Attention module, which enables fine-grained, query-specific token representations through multiplicative gating.  
2. Empirical evidence demonstrating state-of-the-art performance on three benchmark datasets (CNN, Daily Mail, and Who Did What), with significant improvements over competitive baselines.  
3. A comprehensive ablation study showing the effectiveness of the GA module and its components, including the superiority of multiplicative gating over alternative compositional operators like addition and concatenation.  
Strengths:  
1. Novelty of the Gated-Attention Mechanism: The proposed GA module introduces a fine-grained attention mechanism that allows query-specific filtering of token representations. This is a meaningful advancement over existing token-wise or sentence-wise attention mechanisms.  
2. Strong Empirical Results: The GA Reader achieves state-of-the-art performance on multiple datasets, with improvements of up to 4% over prior models. The results are robust across both large-scale datasets (CNN, Daily Mail) and smaller, more challenging datasets (Who Did What).  
3. Thorough Ablation Study: The paper provides a detailed analysis of the GA Reader's components, demonstrating the importance of gated attention, character embeddings, and pre-trained GloVe vectors. This enhances the reproducibility and interpretability of the results.  
4. Visualization of Attention Mechanisms: The qualitative analysis of attention distributions across layers provides valuable insights into how the model incrementally refines its understanding of the query and document.  
Weaknesses:  
1. Limited Theoretical Justification: While the empirical performance of the multiplicative gating mechanism is well-documented, the paper lacks a theoretical explanation for why this operation outperforms alternatives like addition or concatenation. A deeper theoretical analysis could strengthen the contribution.  
2. Dataset-Specific Feature Engineering: The reliance on the qe-comm feature for certain datasets raises concerns about the generalizability of the model. The paper does not explore whether the GA Reader can perform equally well without such dataset-specific enhancements.  
3. Scalability to Longer Documents: Although the GA Reader performs well on the provided datasets, its scalability to significantly longer documents or more complex queries is not addressed. This could limit its applicability to real-world scenarios.  
4. Comparison with Ensembles: While the GA Reader outperforms single models, its comparison with ensemble models is less consistent. The paper could have explored ensemble versions of the GA Reader to provide a fairer comparison.  
Questions to Authors:  
1. Can you provide a theoretical explanation for the superior performance of multiplicative gating over addition and concatenation?  
2. How does the GA Reader perform on datasets with significantly longer documents or more complex queries?  
3. Could the model's reliance on the qe-comm feature be mitigated to improve generalizability across datasets?  
Conclusion:  
The paper presents a novel and effective approach to cloze-style question answering, with strong empirical results and a well-structured analysis. While there are some limitations in theoretical grounding and generalizability, the contributions are significant and impactful. I recommend acceptance, with minor revisions to address the outlined weaknesses and questions.