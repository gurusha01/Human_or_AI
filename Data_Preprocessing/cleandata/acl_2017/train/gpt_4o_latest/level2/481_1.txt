Review
Summary and Contributions
This paper introduces FOIL-COCO, a novel diagnostic dataset designed to evaluate the fine-grained integration of language and vision in Language and Vision (LaVi) models. The dataset extends MS-COCO by associating images with both correct captions and "foil" captions, which differ from the original by a single incorrect word. The authors propose three tasks: (1) caption classification (correct vs. foil), (2) foil word detection, and (3) foil word correction. The paper demonstrates that current state-of-the-art LaVi models perform poorly on these tasks, highlighting their inability to deeply integrate language and vision. Humans, in contrast, achieve near-perfect performance. The authors also provide a detailed analysis of model failures and propose FOIL-COCO as a benchmark to challenge and diagnose LaVi models.
Strengths
1. Novel Dataset: FOIL-COCO is a well-motivated and carefully constructed dataset that addresses key weaknesses in existing LaVi benchmarks, such as language priors and dataset biases. By introducing minimal but meaningful perturbations, the dataset provides a challenging testbed for LaVi models.
2. Diagnostic Tasks: The proposed tasks are thoughtfully designed to evaluate different aspects of LaVi models, from coarse classification to fine-grained error detection and correction. These tasks go beyond traditional metrics and provide deeper insights into model capabilities.
3. Empirical Evidence: The paper provides strong empirical evidence of the limitations of current LaVi models, showing that they struggle with tasks requiring fine-grained understanding of the interaction between text and images. The comparison with human performance further underscores these limitations.
4. Analysis of Model Failures: The authors conduct a thorough analysis of model performance, identifying key factors such as semantic similarity and word frequency that influence errors. This analysis is valuable for guiding future research.
Weaknesses
1. Limited Model Coverage: While the paper evaluates two state-of-the-art models (LSTM+norm I and HieCoAtt), it does not include more recent or advanced models, such as multimodal transformers, which could provide a more comprehensive assessment of the field.
2. Foil Word Correction Task: The foil word correction task (T3) is particularly challenging, with models performing only slightly above chance. While this highlights the limitations of current systems, the task may be too ambitious without additional guidance, such as bounding box annotations or object detection modules.
3. Focus on Language Bias: The paper emphasizes minimizing language bias but does not fully explore how visual biases (e.g., object detection limitations) might also contribute to model failures. A deeper investigation into the interplay between language and visual biases would strengthen the analysis.
4. Reproducibility: While the dataset is described in detail, the paper does not provide sufficient implementation details for the evaluated models, such as hyperparameters or training protocols, which could hinder reproducibility.
Questions to Authors
1. Have you considered evaluating more recent LaVi models, such as multimodal transformers (e.g., CLIP, BLIP)? How do you anticipate these models would perform on FOIL-COCO?
2. Could you provide additional details on the training and evaluation protocols for the tested models, including hyperparameters and training datasets?
3. For the foil word correction task, have you considered incorporating object detection outputs or bounding box annotations to guide the correction process?
Recommendation
The paper makes a significant contribution by introducing a novel dataset and diagnostic tasks that highlight critical weaknesses in current LaVi models. While there are some limitations in model coverage and task feasibility, the work is highly relevant and thought-provoking. I recommend acceptance with minor revisions to address the questions raised and improve reproducibility.