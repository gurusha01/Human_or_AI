Review
Summary and Contributions  
This paper presents a semi-supervised approach for augmenting sequence tagging models with pre-trained bidirectional language model (LM) embeddings. The authors propose "TagLM," which combines LM embeddings with hierarchical bidirectional RNNs to enhance token representations. The method is evaluated on two standard NLP tasks—Named Entity Recognition (NER) and Chunking—achieving state-of-the-art results on both datasets without requiring additional labeled data or task-specific resources. Key contributions include:  
1. Demonstrating the utility of pre-trained bidirectional LM embeddings in supervised sequence tagging tasks, leading to significant performance improvements (e.g., +1.06 F1 on CoNLL 2003 NER and +1.37 F1 on CoNLL 2000 Chunking).  
2. Showing that bidirectional LM embeddings outperform forward-only LMs and that domain-specific pre-training is not necessary.  
3. Providing a detailed analysis of where and how LM embeddings should be integrated into sequence tagging models for optimal performance.  
Strengths  
1. Significant Performance Gains: The proposed method achieves substantial improvements over prior state-of-the-art models, even outperforming systems that use additional labeled data or gazetteers. The results are statistically significant and robust across multiple configurations.  
2. Generalizability: The approach is simple, general, and effective across different domains, as evidenced by its success on the ScienceIE dataset despite domain mismatch. This highlights the broad applicability of the method.  
3. Comprehensive Analysis: The authors provide thorough experiments and ablation studies, such as testing the impact of LM size, bidirectionality, and integration points. These analyses offer valuable insights for the community.  
4. Low Resource Utility: The method is particularly beneficial for low-resource settings, as demonstrated by the significant performance boost (+3.35 F1) when training on only 1% of the CoNLL 2003 dataset.  
Weaknesses  
1. Reproducibility Concerns: While the paper provides extensive experimental details, the reliance on pre-trained LMs (e.g., CNN-BIG-LSTM) trained with substantial computational resources (32 GPUs for weeks) may limit reproducibility for researchers with fewer resources.  
2. Limited Novelty in LM Usage: The use of pre-trained LMs for contextual embeddings is not entirely novel, as similar ideas have been explored in prior work (e.g., Li and McCallum, 2005). The novelty lies more in the integration and empirical validation rather than the conceptual framework.  
3. Task-Specific RNN Dependence: The results suggest that the task-specific RNN is crucial for performance, which may limit the applicability of the method to simpler models or tasks where such RNNs are infeasible.  
Questions to Authors  
1. How does the performance of TagLM compare when using smaller, publicly available LMs (e.g., BERT-base) instead of CNN-BIG-LSTM?  
2. Could the proposed method be extended to other sequence tagging tasks, such as POS tagging or dependency parsing?  
3. What are the computational trade-offs of using larger LMs, and how do they impact real-world deployment?  
Conclusion  
This paper provides a strong contribution to the field of NLP by demonstrating the effectiveness of pre-trained bidirectional LM embeddings in sequence tagging tasks. Despite some concerns about computational requirements and novelty, the method's simplicity, generalizability, and significant performance improvements make it a valuable addition to the literature. I recommend acceptance.