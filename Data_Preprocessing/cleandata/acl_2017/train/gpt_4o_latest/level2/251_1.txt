Review of the Paper
Summary and Contributions
This paper provides a theoretical framework to explain the phenomenon of additive compositionality in word embeddings learned using the Skip-Gram (SG) model. The authors present two main contributions: (1) a mathematical formalism for compositionality and a proof that additive compositionality arises under specific assumptions about the corpus, and (2) a connection between the SG model and the Sufficient Dimensionality Reduction (SDR) framework, showing that SG embeddings are information-theoretically optimal in preserving mutual information. The authors also derive the correct composition operator when the assumptions for additivity do not hold and demonstrate how this operator generalizes the use of vector addition for solving word analogies.
Strengths
1. Theoretical Rigor: The paper provides a strong mathematical foundation for understanding compositionality in word embeddings. The proofs are detailed and address key assumptions, such as word frequency distributions and co-occurrence statistics.
2. Novel Insights: The connection between the Skip-Gram model and the SDR framework is novel and significant. It not only explains the optimality of SG embeddings but also opens up avenues for leveraging SG heuristics to efficiently approximate SDR embeddings in other domains.
3. Practical Implications: The work bridges theory and practice by explaining why SG embeddings perform well empirically, even when the underlying assumptions about the corpus are violated. This insight could guide future improvements in embedding models.
4. Clarity of Results: The distinction between linear and non-linear compositionality is well-articulated, and the implications for solving word analogies are clearly demonstrated.
Weaknesses
1. Limited Empirical Validation: While the theoretical contributions are strong, the paper lacks empirical experiments to validate the practical applicability of the proposed non-linear composition operator. Demonstrating its effectiveness on real-world datasets would strengthen the work.
2. Assumptions on Corpus: The assumption of uniform word frequency for additive compositionality is unrealistic, as natural language corpora typically follow a Zipfian distribution. While the authors acknowledge this limitation, they do not explore alternative assumptions or methods to address it.
3. Accessibility of Results: The mathematical formalism, while rigorous, may be challenging for readers unfamiliar with information theory or advanced linear algebra. A more intuitive explanation or visualization of the results could improve accessibility.
4. Connection to Prior Work: Although the paper builds on prior work (e.g., Arora et al., 2016), it does not provide a comprehensive comparison of its theoretical framework with other models of compositionality, such as matrix factorization approaches.
Questions to Authors
1. Have you conducted any experiments to validate the proposed non-linear composition operator on tasks like word analogy or sentence composition? If not, could you provide insights into its practical feasibility?
2. Could the connection between Skip-Gram and SDR be extended to other embedding models, such as GloVe or FastText? If so, how would the theoretical guarantees change?
3. How sensitive are the results to deviations from the assumptions (e.g., non-uniform word frequencies or non-random corpus generation processes)?
Conclusion
This paper makes significant theoretical contributions to understanding compositionality in word embeddings and the optimality of the Skip-Gram model. However, the lack of empirical validation and reliance on strong assumptions limit its immediate practical impact. With additional experiments and exploration of more realistic assumptions, this work has the potential to influence both theoretical and applied research in word embeddings. I recommend acceptance with minor revisions to address the weaknesses outlined above.