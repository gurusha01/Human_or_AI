Review
Summary of the Paper
This paper proposes a novel joint modeling approach to simultaneously identify salient discussion points and predict discourse relations in spoken meetings. The model leverages the interaction between content and discourse to improve prediction performance for both tasks. A variation of the model treats discourse relations as latent variables to address the challenge of acquiring labeled training data. The approach is evaluated on the AMI and ICSI meeting corpora, demonstrating significant improvements over SVM-based baselines for both phrase-based content selection and discourse relation prediction. Additionally, the paper explores the utility of the model in predicting team members' consistency of understanding (COU) in group decisions, achieving state-of-the-art results.
Contributions
1. Joint Modeling of Content and Discourse: The paper introduces a joint model that integrates content selection and discourse relation prediction, achieving better performance than separate learning approaches.
2. Latent Discourse Relation Modeling: A variation of the model treats discourse relations as latent variables, enabling its application in scenarios with limited labeled data.
3. Practical Applications: The model demonstrates utility in downstream tasks like extractive summarization and predicting consistency of understanding, outperforming state-of-the-art methods.
Strengths
1. Significant Performance Gains: The model consistently outperforms SVM-based baselines on both phrase selection and discourse relation prediction across multiple datasets. For example, the model achieves a notable improvement in F1 scores (e.g., 62.6 vs. 54.6 for phrase selection on AMI-SUB).
2. Comprehensive Evaluation: The paper evaluates the model on multiple datasets (AMI and ICSI) and tasks (content selection, discourse relation prediction, summarization, and COU prediction), demonstrating its robustness and versatility.
3. Innovative Use of Latent Variables: The latent discourse relation variation addresses the practical challenge of limited labeled data, making the approach more scalable.
4. Practical Utility: The model's ability to generate phrase-based summaries and predict COU highlights its potential for real-world applications, such as meeting summarization and intelligent assistant systems.
Weaknesses
1. Limited Error Analysis: While the paper briefly discusses errors (e.g., inconsistent predictions for semantically similar phrases), it does not provide an in-depth analysis or propose specific solutions to address these issues.
2. Scalability of Joint Inference: The alternating optimization approach for joint inference may not scale well to larger datasets or more complex discourse structures, as the search space grows significantly.
3. Evaluation Metrics: The use of ROUGE for summarization evaluation is standard but limited in capturing the semantic quality of summaries. Incorporating human evaluation or more advanced metrics could strengthen the analysis.
4. Discourse Relation Ambiguity: The model struggles with ambiguous discourse relations (e.g., distinguishing between OPTION EXCLUSION and NEGATIVE), but the paper does not explore advanced semantic modeling techniques to address this limitation.
Questions to Authors
1. How does the model perform on other types of goal-oriented dialogues, such as customer service transcripts or negotiations? Could the approach generalize to these domains?
2. Have you considered incorporating pre-trained language models (e.g., BERT) to enhance semantic understanding for both content and discourse relation prediction?
3. What are the computational costs of the joint learning and inference processes, and how do they compare to simpler baselines like SVMs?
Additional Comments
The paper presents a well-motivated and technically sound approach to a challenging problem in dialogue analysis. While the results are promising, addressing the identified weaknesses could further enhance the model's impact and applicability.