Review of the Paper
Summary and Contributions
This paper addresses the challenging task of open-domain question answering (QA) using Wikipedia as the sole knowledge source. The proposed system, DrWiki, integrates a document retrieval module (Document Retriever) with a machine comprehension model (Document Reader) to identify and extract answers from Wikipedia articles. The authors claim three main contributions: (1) a competitive Document Retriever that outperforms the built-in Wikipedia search engine, (2) a Document Reader achieving state-of-the-art results on the SQuAD benchmark, and (3) a multitask learning framework that improves performance across multiple QA datasets by leveraging distant supervision. The work is positioned as a step toward building a unified QA system capable of answering factoid questions without relying on structured knowledge bases or redundant external resources.
Strengths
1. Novelty and Scope: The paper tackles an important and underexplored problem of using Wikipedia as the sole knowledge source for QA, which is both ambitious and practical. The focus on combining document retrieval and machine comprehension in a unified pipeline is a significant contribution.
   
2. Strong Experimental Results: The Document Reader achieves state-of-the-art results on the SQuAD benchmark (70.0% EM and 79.0% F1), demonstrating the efficacy of the proposed architecture. The multitask learning approach also improves performance across diverse datasets, showcasing the system's generalizability.
3. Comprehensive Evaluation: The authors evaluate their system on multiple QA datasets (SQuAD, CuratedTREC, WebQuestions, WikiMovies), providing a thorough analysis of its strengths and limitations. The ablation study on feature importance is particularly insightful.
4. Practical Contributions: The use of distant supervision to generate training data for datasets without associated paragraphs is a practical and scalable approach that enhances the system's applicability.
Weaknesses
1. Limited End-to-End Integration: While the Document Retriever and Document Reader are individually strong, the lack of end-to-end training across the pipeline is a missed opportunity. This could potentially improve the system's ability to jointly optimize retrieval and comprehension.
2. Performance Gap on Open-Domain QA: The system's performance on open-domain QA (e.g., 26.7% EM on SQuAD with Wikipedia as the source) is significantly lower than its performance on machine comprehension tasks. This highlights a bottleneck in the retrieval process, where many false positives arise from topical but irrelevant sentences.
3. Comparison to Baselines: The comparison to existing open-domain QA systems like YodaQA is limited. While the authors acknowledge that YodaQA uses additional resources, a more detailed analysis of the performance gap would strengthen the evaluation.
4. Scalability Concerns: The reliance on retrieving and processing multiple paragraphs independently may limit scalability for larger datasets or real-time applications. Incorporating paragraph aggregation during training could address this issue.
Questions to Authors
1. Have you considered incorporating end-to-end training across the Document Retriever and Document Reader? If so, what challenges do you foresee?
2. How does the system handle ambiguous or multi-answer questions where multiple spans in different documents could be valid?
3. Could you provide more details on the computational efficiency of DrWiki, particularly for large-scale deployments?
Recommendation
The paper presents a well-motivated and technically sound approach to open-domain QA using Wikipedia. While there are some limitations in terms of end-to-end integration and performance on open-domain tasks, the contributions are significant, and the work opens up promising avenues for future research. I recommend acceptance with minor revisions to address the weaknesses outlined above.