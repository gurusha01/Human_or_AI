Review of the Paper
Summary and Contributions:  
This paper presents a novel approach to Abstract Meaning Representation (AMR) parsing and realization using sequence-to-sequence (seq2seq) models. The authors address the challenges of data sparsity and graph-to-sequence linearization by introducing a preprocessing pipeline and a paired training procedure that leverages millions of unlabeled sentences. The main contributions of the paper are:  
1. A paired training procedure that combines self-training and pretraining using external unlabeled data, achieving competitive results in AMR parsing (61.9 SMATCH) and state-of-the-art results in AMR realization (32.3 BLEU).  
2. A robust preprocessing pipeline that anonymizes named entities, reduces sparsity, and introduces scope markers to encode graph structure effectively.  
3. Extensive ablative and qualitative analyses demonstrating the contributions of preprocessing, paired training, and the robustness of seq2seq models to artifacts introduced by graph linearization.  
Strengths:  
1. Significant Performance Gains: The paper demonstrates substantial improvements in both AMR parsing and realization tasks. The use of external unlabeled data for self-training is particularly impactful, as evidenced by the 9-point improvement over comparable seq2seq models in parsing and a 5-point BLEU gain in realization.  
2. Innovative Training Procedure: The paired training approach is a notable contribution, effectively addressing data sparsity by leveraging weakly labeled data. This method is well-documented and reproducible, with clear algorithmic steps provided.  
3. Comprehensive Evaluation: The authors conduct thorough experiments, including ablation studies and error analyses, to isolate the contributions of preprocessing components and training strategies. The robustness of seq2seq models to different linearization orders is particularly noteworthy.  
4. Practical Preprocessing Pipeline: The anonymization and clustering of named entities, as well as the use of scope markers, are practical solutions to sparsity and long-range dependency challenges. These preprocessing steps are well-motivated and validated through experiments.  
Weaknesses:  
1. Limited Novelty in Model Architecture: While the paired training procedure and preprocessing are innovative, the seq2seq model itself relies on standard architectures (stacked LSTMs with attention). The paper could have explored more advanced architectures, such as transformers, to further improve performance.  
2. Scalability to Larger Datasets: The paper does not fully explore the scalability of the proposed method to larger datasets or corpora. For example, the paired training procedure is only applied to 2 million Gigaword sentences, leaving the potential of scaling to the full corpus unexplored.  
3. Limited Discussion of Limitations: While the authors acknowledge some limitations, such as the reliance on human-authored AMR linearization orders for evaluation, a more explicit discussion of the method's shortcomings (e.g., computational cost of paired training) would strengthen the paper.  
4. Error Analysis Depth: The error analysis identifies general categories (e.g., disfluency, coverage, attachment) but does not provide actionable insights into how these errors could be mitigated in future work.  
Questions to Authors:  
1. How does the computational cost of the paired training procedure compare to other AMR parsing and realization methods?  
2. Could the proposed method benefit from incorporating more recent architectures, such as transformers, instead of LSTMs?  
3. Have you considered scaling the paired training procedure to the full Gigaword corpus, and if so, what challenges do you anticipate?  
Overall Assessment:  
This paper makes meaningful contributions to AMR parsing and realization, particularly in addressing data sparsity through paired training and preprocessing. While the seq2seq model architecture lacks novelty, the proposed methods are practical, effective, and well-supported by experimental results. The paper is a strong candidate for acceptance, with minor revisions to address scalability and limitations.