Review of the Paper
Summary and Contributions  
This paper introduces a novel framework for semi-automatically creating linguistically challenging data-to-text corpora from existing knowledge bases (KBs), specifically DBpedia. The framework aims to support the training of wide-coverage microplanners capable of handling key NLG subtasks such as lexicalisation, aggregation, sentence segmentation, and referring expression generation. The authors compare their dataset (DBPNLG) with an existing dataset (RNNLG) and demonstrate that DBPNLG is more diverse in terms of input patterns, attributes, and syntactic complexity, despite being smaller in size. The paper also evaluates the performance of a sequence-to-sequence model on both datasets, highlighting the increased complexity of DBPNLG. The authors propose their framework as a method for creating datasets that can train NLG models to generate linguistically rich texts from KB data.
Strengths  
1. Novelty of the Framework: The proposed framework addresses key limitations of existing datasets by creating semantically and syntactically diverse corpora. The focus on KB-derived data and the inclusion of deep input structures (beyond depth-1 trees) are significant contributions to the field of NLG.
2. Comprehensive Comparison: The paper provides a thorough comparison between DBPNLG and RNNLG, using metrics such as input diversity, input patterns, text complexity, and vocabulary richness. This analysis is well-structured and highlights the advantages of DBPNLG in supporting the development of wide-coverage microplanners.
3. Practical Contribution: The framework is practically useful for generating datasets from RDF-based KBs, which are increasingly prevalent in the semantic web. The authors also emphasize the potential of DBPNLG to train KB verbalisers, making the work relevant to real-world applications.
4. Evaluation of Neural Models: The use of a sequence-to-sequence model to assess dataset complexity is a valuable addition, demonstrating the challenges posed by DBPNLG and motivating further research in neural NLG.
Weaknesses  
1. Limited Scope of Evaluation: While the paper evaluates a sequence-to-sequence model, the analysis could benefit from comparisons with other state-of-the-art NLG models. This would provide a more comprehensive understanding of the dataset's challenges.
2. Crowdsourcing Quality Control: Although the authors describe their crowdsourcing process in detail, the paper does not provide quantitative evidence of inter-annotator agreement or the reliability of the majority vote validation process. This raises questions about the consistency of the collected texts.
3. Scalability: The framework relies on manual clarification of KB properties and crowdsourcing for text generation, which may limit its scalability to larger KBs or more categories. The authors do not discuss how these challenges might be addressed in future work.
4. Reproducibility: While the paper provides a detailed description of the framework, it lacks sufficient implementation details (e.g., ILP formulation, parameter settings) to ensure reproducibility. Sharing the code or additional technical documentation would enhance the paper's impact.
Questions to Authors  
1. How does the framework handle ambiguous or noisy data in KBs, and how does this impact the quality of the resulting dataset?  
2. Could you elaborate on how the ILP-based content selection module scales with larger KBs or more complex categories?  
3. Have you considered evaluating the dataset with other NLG tasks (e.g., content selection, aggregation) beyond microplanning?  
Conclusion  
This paper presents a significant contribution to the field of NLG by proposing a framework for creating diverse and challenging data-to-text corpora from KBs. While the framework and dataset are promising, the paper could benefit from additional evaluations, scalability discussions, and reproducibility enhancements. I encourage the authors to address these issues and consider releasing their code and extended dataset to maximize the impact of their work.