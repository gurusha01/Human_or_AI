Review of the Submission
Summary and Contributions
This paper proposes a novel method for generating context-sensitive token embeddings by grounding word representations in WordNet synsets and leveraging hypernymy relations. Unlike traditional type-level word embeddings, the proposed method estimates a probability distribution over relevant semantic concepts for each word token in context. The embeddings are integrated into a bidirectional LSTM model for the task of prepositional phrase (PP) attachment disambiguation, achieving a 5.4% absolute improvement in accuracy over the baseline. The primary contributions of the paper are:
1. A method for generating context-sensitive token embeddings based on WordNet, which incorporates lexical ambiguity and semantic generalization.
2. A demonstration of the effectiveness of these embeddings in improving PP attachment predictions, with significant error reduction compared to baselines.
3. A detailed analysis of the model's performance, including its regularization benefits, robustness to rare words, and integration with dependency parsing.
Strengths
1. Novelty and Innovation: The paper introduces a unique approach to token-level embeddings by combining distributional semantics with ontological knowledge from WordNet. This hybrid approach is innovative and addresses the limitations of type-level embeddings in handling lexical ambiguity.
2. Empirical Results: The proposed model achieves a 34.4% relative error reduction in PP attachment accuracy compared to the baseline, demonstrating its effectiveness. The integration of the model with a dependency parser further highlights its practical utility.
3. Thorough Analysis: The paper provides a detailed breakdown of the model's components (e.g., sense priors, attention mechanism) and their contributions to performance. The qualitative analysis of rare word handling and context sensitivity is insightful.
4. Reproducibility: The authors describe the dataset, hyperparameters, and implementation details comprehensively, and the availability of the method as a Keras layer facilitates reproducibility and adoption by other researchers.
Weaknesses
1. Limited Task Scope: While the results on PP attachment are impressive, the paper evaluates the proposed embeddings on a single task. Broader evaluation across diverse NLP tasks (e.g., semantic role labeling, question answering) would strengthen the claim of general applicability.
2. Dependency on WordNet: The reliance on WordNet limits the method's applicability to languages or domains where such lexical resources are unavailable or incomplete. This limitation is acknowledged but not sufficiently addressed in the experiments.
3. Comparison with Baselines: The baselines used (e.g., LSTM-PP with GloVe and GloVe-retro) are relatively simple. A comparison with more advanced contextual embedding methods (e.g., BERT, ELMo) would provide a stronger benchmark and situate the work within the current state of the field.
4. Computational Overhead: The proposed method introduces additional complexity by requiring synset embeddings and attention mechanisms. While the authors claim comparable parameter space to the baseline, a more detailed analysis of computational efficiency (e.g., runtime, scalability) is missing.
Questions to Authors
1. How does the proposed method compare to modern contextual embeddings like BERT or ELMo in terms of performance on PP attachment or other tasks?
2. Can the model handle cases where WordNet synsets are unavailable or incomplete? Have you considered augmenting WordNet with other resources (e.g., Freebase, ConceptNet)?
3. How does the model's performance scale with larger datasets or more complex tasks? Are there diminishing returns for the WordNet grounding as training data increases?
Recommendation
Overall, this paper presents a novel and well-executed approach to context-sensitive embeddings, with strong empirical results and detailed analysis. However, the limited task scope and lack of comparison with state-of-the-art contextual embeddings slightly weaken its impact. I recommend acceptance with minor revisions, encouraging the authors to address the broader applicability and computational considerations of their method.