Review of the Paper
Summary and Contributions  
This paper investigates the limitations of existing automatic metrics for evaluating Natural Language Generation (NLG) systems and proposes a novel metric, RAINBOW, which combines the strengths of word-based metrics (WBMs) and grammar-based metrics (GBMs). The authors present a comprehensive evaluation of 21 metrics across three datasets and demonstrate that current metrics correlate weakly with human judgments, particularly at the sentence level. A detailed error analysis highlights the challenges in distinguishing outputs of medium and high quality. The proposed RAINBOW metric achieves a significantly higher correlation with human ratings (ρ = 0.81) compared to existing metrics (maximum ρ = 0.33). The authors also make their code and data publicly available, contributing to reproducibility and further research.
Strengths  
1. Comprehensive Evaluation: The paper evaluates a wide range of metrics, including novel grammar-based ones, across multiple datasets and systems. This breadth of analysis is a significant strength, as it provides a holistic view of the limitations of current metrics.
2. Novel Metric: The introduction of RAINBOW is a key contribution. By combining WBMs and GBMs using ensemble learning, the authors address the weaknesses of individual metrics and achieve robust correlations with human judgments across datasets.
3. Error Analysis: The detailed error analysis is insightful, revealing that existing metrics perform poorly for outputs rated as medium or good by humans. This analysis underscores the need for improved evaluation methods.
4. Reproducibility: The authors make their code and data publicly available, which is commendable and aligns with best practices in research transparency.
5. Practical Implications: The findings are highly relevant to the NLG community, as they challenge the reliance on existing metrics and provide a promising alternative.
Weaknesses  
1. Limited Generalization: While RAINBOW shows strong performance, its reliance on a combination of metrics may limit its applicability in scenarios where computational resources are constrained or where reference texts are unavailable.
2. Over-reliance on Crowd-Sourced Data: The datasets used contain grammatical errors and inconsistencies, which may affect the reliability of the findings. A discussion on the impact of higher-quality datasets would strengthen the paper.
3. Scalability of RAINBOW: The computational cost of combining 21 metrics is not addressed in detail. While the authors propose a "Top 5" feature model, its performance is significantly lower than the full RAINBOW model.
4. Limited Exploration of Referenceless Metrics: Although the authors acknowledge the potential of referenceless approaches, this direction is not explored in the current work, which could have added depth to the study.
5. Human Evaluation Design: The paper uses Likert-scale ratings for human judgments, which may introduce subjectivity. Alternative evaluation designs, such as pairwise comparisons, could provide more robust insights.
Questions to Authors  
1. How does the computational cost of RAINBOW compare to existing metrics, and how feasible is its deployment in real-time NLG evaluation scenarios?  
2. Could the performance of RAINBOW be further improved by incorporating referenceless metrics or weighting references based on quality?  
3. How would the proposed metric perform on datasets with higher-quality references or in domains beyond dialogue systems?
Conclusion  
This paper makes a significant contribution to the field of NLG evaluation by highlighting the limitations of existing metrics and proposing a novel, high-performing alternative. While there are some concerns regarding scalability and dataset quality, the work is well-executed, and its findings are both impactful and actionable. I recommend acceptance, provided the authors address the scalability and generalization concerns in the final version.