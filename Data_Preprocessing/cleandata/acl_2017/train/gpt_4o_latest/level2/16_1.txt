Review of the Paper
Summary and Contributions
This paper addresses the task of Event Detection (ED) by proposing a novel supervised attention mechanism that explicitly leverages argument information to improve ED performance. The authors argue that existing methods either ignore or indirectly exploit arguments, which limits their effectiveness for ED. The proposed method constructs gold attention vectors based on annotated arguments and employs these as supervision during training. Experimental results on the ACE 2005 dataset demonstrate that the approach achieves state-of-the-art performance, with significant improvements in F1 score. The key contributions of the paper are:  
1. A systematic analysis of the limitations of joint models for ED, particularly their inability to effectively utilize argument information for ED.  
2. A supervised attention-based ED model that explicitly incorporates argument information and explores different attention strategies.  
3. Experimental validation of the proposed method, achieving the best performance on the ACE 2005 dataset and demonstrating robustness with additional training data from FrameNet.
Strengths
1. Novelty and Problem Analysis: The paper provides a clear and compelling analysis of the limitations of joint models, identifying the imbalance in annotated triggers and arguments as a key issue. The proposed solution is novel and well-motivated, addressing a gap in the literature.  
2. Supervised Attention Mechanism: The use of supervised attention to explicitly incorporate argument information is innovative. The two strategies (S1 and S2) for constructing gold attention vectors are well-designed and provide valuable insights into the role of arguments in ED.  
3. Experimental Rigor: The experiments are comprehensive, with comparisons to a wide range of state-of-the-art methods. The results convincingly demonstrate the effectiveness of the proposed approach, with significant gains in F1 score. The inclusion of additional training data from FrameNet further strengthens the evaluation.  
4. Clarity and Reproducibility: The paper is well-written, with detailed descriptions of the methodology, architecture, and experimental setup. This ensures that the work is reproducible.
Weaknesses
1. Limited Generalization Beyond ACE 2005: While the results on ACE 2005 are impressive, the paper does not evaluate the method on other datasets. This raises concerns about the generalizability of the approach to different domains or event types.  
2. Dependence on Annotated Arguments: The proposed method relies heavily on annotated arguments, which may not always be available in real-world scenarios. The authors could have explored the performance of the model in low-resource settings or with noisy argument annotations.  
3. Comparison with Unsupervised Methods: The paper does not compare its supervised attention mechanism with recent unsupervised or semi-supervised attention approaches, which could provide additional context for the significance of the proposed method.  
4. Scalability and Efficiency: The computational cost of supervised attention mechanisms, particularly the construction of gold attention vectors, is not discussed. This could be a limitation for large-scale applications.
Questions to Authors
1. How does the proposed method perform on datasets other than ACE 2005? Have you considered testing it on more diverse or domain-specific datasets?  
2. Can the model be adapted to scenarios where annotated arguments are unavailable or noisy? If so, how would this impact performance?  
3. What is the computational overhead of constructing and using gold attention vectors? How does this scale with larger datasets?  
Overall Assessment
This paper makes a significant contribution to the field of Event Detection by proposing a novel supervised attention mechanism that explicitly incorporates argument information. The approach is well-motivated, innovative, and supported by strong experimental results. However, the reliance on annotated arguments and the lack of evaluation on additional datasets limit the broader applicability of the method. Addressing these limitations in future work could further enhance the impact of this research.  
Recommendation: Accept with minor revisions.