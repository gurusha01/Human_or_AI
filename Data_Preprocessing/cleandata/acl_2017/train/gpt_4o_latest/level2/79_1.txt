Review of the Paper
Summary and Contributions
This paper introduces ITransF, a novel embedding model for knowledge base completion (KBC) that addresses the data sparsity issue inherent in existing methods like TransR and STransE. The key innovation lies in the use of a sparse attention mechanism to discover shared concepts among relations, enabling knowledge transfer and improving generalization. The sparse attention mechanism also enhances interpretability by associating relations with shared concept matrices. The authors propose a block iterative optimization algorithm to enforce sparsity and evaluate the model on two benchmark datasets, WN18 and FB15k, achieving state-of-the-art results among models that do not use external information. The contributions of the paper can be summarized as:
1. Proposing ITransF, a knowledge embedding model that enables interpretable knowledge transfer through shared concept matrices.
2. Introducing a learning algorithm to optimize sparse attention vectors for interpretable knowledge transfer.
3. Demonstrating empirical improvements over baselines on WN18 and FB15k datasets.
Strengths
1. Novelty and Innovation: The sparse attention mechanism is a significant contribution, enabling interpretable knowledge transfer. This addresses a critical limitation in prior models, such as STransE, which lack parameter sharing across relations.
2. Empirical Performance: ITransF achieves state-of-the-art results on WN18 and FB15k without relying on external information, demonstrating its effectiveness. The model particularly excels on rare relations, showcasing its ability to transfer statistical strength.
3. Interpretability: The sparse attention vectors provide a clear and interpretable mapping of relations to shared concepts, which is a valuable addition to the field of KBC.
4. Model Compression: The proposed method significantly reduces the number of parameters (e.g., 90Ã— compression on FB15k) while maintaining competitive performance, making it computationally efficient.
5. Thorough Evaluation: The paper provides detailed analyses, including performance on rare relations, interpretability of attention vectors, and comparison with dense attention and sparse encoding methods.
Weaknesses
1. Limited Scope of Evaluation: While the model performs well on WN18 and FB15k, these datasets are known to have limitations, such as redundancy in WN18. Evaluation on more challenging and diverse datasets (e.g., FB15k-237 or YAGO) would strengthen the claims.
2. Sparse Attention Optimization: The block iterative optimization algorithm, while effective, is described as a crude approximation. The authors acknowledge this but do not provide a clear roadmap for improving the optimization method.
3. Lack of Multi-Step Inference: The paper highlights the importance of multi-step inference for KBC but does not incorporate it into the proposed model. This limits its applicability to more complex reasoning tasks.
4. Comparison with External Information Models: While the authors focus on intrinsic models, a more detailed discussion of how ITransF could integrate external information (e.g., textual data) would be valuable for future extensions.
Questions to Authors
1. How does ITransF perform on datasets like FB15k-237, which are designed to address the redundancy issues in FB15k?
2. Can the block iterative optimization algorithm be replaced with more sophisticated methods, such as combinatorial optimization or reinforcement learning, to improve sparsity and performance?
3. How scalable is ITransF to larger knowledge bases with millions of entities and relations?
Recommendation
This paper makes a significant contribution to the field of knowledge base completion by addressing data sparsity and interpretability through a novel sparse attention mechanism. Despite some limitations, the strengths of the proposed method and its empirical results justify its acceptance. I recommend acceptance with minor revisions to address the evaluation scope and optimization concerns.