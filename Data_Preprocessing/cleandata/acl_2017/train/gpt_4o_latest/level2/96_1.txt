Review
This paper introduces the novel task of sarcasm interpretation, defined as generating non-sarcastic utterances that convey the same meaning as sarcastic ones. The authors present a new dataset of 3,000 sarcastic tweets, each annotated with five human-provided non-sarcastic interpretations. They approach the task as monolingual machine translation (MT), experimenting with both phrase-based and neural MT systems. The authors further propose SIGN (Sarcasm Sentimental Interpretation GeNerator), a sentiment-focused MT algorithm that replaces sentiment words in sarcastic tweets with their opposites, leveraging clustering and contextual selection strategies. SIGN demonstrates superior performance in human evaluation metrics such as adequacy and sentiment polarity, though it does not significantly outperform baseline MT systems in automatic measures like BLEU.
Strengths
1. Novelty of Task: The paper introduces sarcasm interpretation as a new and challenging NLP task. This is a significant contribution, as sarcasm presents unique challenges for sentiment analysis, opinion mining, and other downstream tasks.
   
2. Dataset Contribution: The authors provide a high-quality parallel dataset of sarcastic tweets and their non-sarcastic interpretations, annotated by human judges. This dataset fills a gap in the field and is a valuable resource for future research.
3. Human-Centric Evaluation: The paper emphasizes human evaluation metrics (fluency, adequacy, sentiment polarity), which are more aligned with the task's objectives than traditional MT metrics like BLEU. This focus highlights the limitations of existing automatic measures for sarcasm interpretation.
4. Algorithmic Innovation: SIGN is a creative approach that leverages sentiment word clustering and contextual selection to improve sarcasm interpretation. The use of sentiment-targeted transformations is well-motivated and demonstrates promising results.
Weaknesses
1. Limited Generalization: SIGN performs well on tweets with explicit sentiment words but struggles with sarcasm requiring world knowledge or implicit sentiment (e.g., "Can you imagine if Lebron had help?"). This limits its applicability to more complex sarcastic utterances.
2. Reliance on Human Evaluation: While the focus on human evaluation is commendable, the paper does not propose or adapt automatic metrics that better correlate with human judgments. This is a missed opportunity to advance evaluation methodologies for the task.
3. Dataset Size and Domain: The dataset is relatively small (3,000 tweets) and domain-specific (Twitter). This raises concerns about the scalability and generalizability of the proposed methods to other domains or larger datasets.
4. Neural MT Underperformance: The neural MT baseline underperforms significantly, likely due to overfitting on the small dataset. This limits the comparison between traditional and neural approaches, which could have been more informative with better-tuned neural models or data augmentation.
Questions to Authors
1. How does SIGN handle sarcastic tweets that lack explicit sentiment words or require significant world knowledge for interpretation? Could external knowledge sources or pre-trained language models improve performance in such cases?
   
2. Have you considered expanding the dataset to include sarcastic utterances from other domains (e.g., Reddit, Facebook) to test the generalizability of SIGN?
3. Could you propose or adapt automatic evaluation metrics that better correlate with human judgments for sarcasm interpretation?
Conclusion
This paper makes a strong case for sarcasm interpretation as a novel and impactful NLP task. The dataset and SIGN algorithm are valuable contributions, though the work could be strengthened by addressing limitations in generalization, dataset size, and evaluation metrics. I encourage the authors to explore these directions in future iterations. Overall, the paper is a meaningful step forward in understanding and interpreting sarcasm in textual communication.