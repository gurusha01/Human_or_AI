Review of the Paper
Summary and Contributions
This paper introduces a novel A* Combinatory Categorial Grammar (CCG) parsing model that incorporates bilexical dependency modeling into the parsing process. The authors propose a factored model that combines supertag probabilities and dependency probabilities, leveraging bi-directional LSTMs for both components. The key contributions of the paper are:  
1. Dependency-Augmented A Parsing: The integration of bilexical dependencies into the A parsing framework, enabling explicit modeling of sentence structures while maintaining efficiency.  
2. State-of-the-Art Results: The proposed method achieves state-of-the-art performance on both English and Japanese CCG parsing, with significant improvements in labeled and unlabeled F1 scores.  
3. Adaptation for Japanese: The paper demonstrates the effectiveness of the model in parsing Japanese, a language with freer word order, outperforming existing methods by a large margin.  
Strengths
1. Novelty and Innovation: The paper presents a significant improvement over prior work by explicitly modeling dependencies within the A* parsing framework. This is a meaningful contribution to the field, as it addresses limitations of heuristic-based methods like "attach low."  
2. Empirical Performance: The model achieves state-of-the-art results on English CCGbank and Japanese CCGbank datasets, demonstrating its robustness across languages with different syntactic structures. The 10-point improvement in Japanese clause dependency accuracy is particularly impressive.  
3. Efficiency: The authors maintain the efficiency of A* parsing by precomputing probabilities and leveraging a factored model. The detailed comparison of parsing speeds highlights the practical applicability of the approach.  
4. Comprehensive Evaluation: The paper evaluates the model on multiple datasets, with ablation studies (e.g., dependency vs. non-dependency models) and comparisons of different dependency conversion rules (e.g., HEADFIRST vs. LEWISRULE). These experiments provide valuable insights into the model's design choices.  
Weaknesses
1. Limited Discussion of Limitations: While the paper acknowledges the slower supertagging speed compared to other parsers, it does not thoroughly discuss potential trade-offs or limitations of the dependency-augmented approach, such as scalability to larger datasets or languages with more complex grammars.  
2. Dependency Conversion Rules: The HEADFIRST and HEADFINAL rules, while effective, are relatively simplistic and lack linguistic motivation. A deeper analysis of why these rules outperform LEWISRULE would strengthen the paper.  
3. Reproducibility: Although the authors provide software as supplementary material, the paper lacks sufficient implementation details (e.g., hyperparameter tuning, training time) to ensure full reproducibility.  
Questions to Authors
1. How does the model handle cases where the dependency and supertag predictions conflict? Are there specific mechanisms to resolve such conflicts during parsing?  
2. Could the proposed approach generalize to other syntactic formalisms beyond CCG, such as dependency or constituency parsing?  
3. What are the computational trade-offs of using deeper bi-LSTMs (4 layers) compared to the 2-layer models in baseline parsers?  
Conclusion
This paper presents a well-executed and innovative contribution to the field of syntactic parsing, particularly in its integration of bilexical dependencies into A* CCG parsing. The empirical results are compelling, and the method's adaptability to Japanese parsing highlights its versatility. However, the paper could benefit from a more detailed discussion of its limitations and further analysis of the dependency conversion rules. Overall, this work is a strong candidate for acceptance at the conference.  
Recommendation: Accept with minor revisions.