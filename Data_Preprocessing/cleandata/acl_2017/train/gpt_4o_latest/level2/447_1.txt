Review of the Paper
Summary and Contributions
This paper investigates the utility of discourse structure, as defined by Rhetorical Structure Theory (RST), for text categorization tasks. The authors propose a recursive neural network model that incorporates a novel attention mechanism to compute representations of text based on discourse dependency trees. The primary contributions of this work are:  
1. Novel Attention Mechanism: The paper introduces an unnormalized attention mechanism tailored to the hierarchical nature of RST, which differs from traditional normalized attention mechanisms used in NLP tasks.  
2. Empirical Validation: The proposed models outperform state-of-the-art baselines on four out of five datasets, demonstrating the utility of discourse structure for tasks like sentiment analysis and framing detection.  
3. Analysis of Discourse Parsing Quality: The authors explore the relationship between discourse parsing accuracy and text categorization performance, showing that improvements in parsing quality can lead to better results.  
Strengths
1. Innovative Use of Discourse Structure: The paper effectively leverages RST discourse trees, demonstrating that even imperfect discourse parsers can provide useful inductive biases for text categorization. This is a significant advancement over prior works that either ignored discourse or relied on manually crafted weighting schemes.  
2. Comprehensive Experiments: The authors evaluate their models on five diverse datasets, including sentiment analysis, framing detection, and legislative bill survival prediction. The results are robust, with the proposed UNLABELED model outperforming baselines in most cases.  
3. Theoretical and Empirical Rigor: The unnormalized attention mechanism is well-motivated by the properties of RST, and its effectiveness is empirically validated. The authors also provide qualitative examples and ablation studies to support their claims.  
4. Open Implementation: The availability of the implementation fosters reproducibility and encourages further research in this area.  
Weaknesses
1. Limited Generalization to Specialized Genres: The proposed models underperform on legislative bill classification, a task where discourse conventions differ significantly from the training data of the discourse parser. This limitation is acknowledged but not sufficiently addressed, leaving room for improvement in adapting the model to diverse genres.  
2. Overparameterization in Smaller Datasets: The FULL model struggles with smaller datasets like MFC and Movies due to its high parameter count. While the UNLABELED model mitigates this issue, the paper could have explored additional strategies to address overfitting.  
3. Dependence on Discourse Parser Quality: The reliance on the DPLP parser, which is trained on a limited dataset, constrains the model's performance. While the authors analyze this dependency, they do not propose concrete solutions to improve discourse parsing for underrepresented genres.  
4. Limited Comparison to Non-Discourse Models: While the paper compares against state-of-the-art baselines, it does not include recent transformer-based models (e.g., BERT), which could provide stronger baselines for text categorization tasks.  
Questions to Authors
1. How does the proposed attention mechanism compare to transformer-based architectures, which also use attention but without explicit discourse structure?  
2. Could pretraining the discourse parser on a more diverse corpus improve performance on specialized genres like legislative bills?  
3. Have you considered combining your model with pre-trained language models to enhance performance on smaller datasets?  
Overall Assessment
This paper makes a strong case for the integration of discourse structure in text categorization tasks, presenting a novel attention mechanism and achieving state-of-the-art results on multiple datasets. However, its reliance on a single discourse parser and its limited adaptability to specialized genres are notable weaknesses. Despite these limitations, the paper offers valuable insights and provides a solid foundation for future research in discourse-aware NLP.  
Recommendation: Accept with minor revisions.