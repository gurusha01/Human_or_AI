Review
Summary and Contributions
This paper systematically investigates the representation of words using subword units (e.g., characters, character n-grams, and morphemes) and their composition methods in the context of language modeling. The authors evaluate these representations across ten languages with diverse morphological typologies, addressing questions about the adequacy of character-based models as substitutes for explicit morphological analysis. The primary contributions of the paper are:
1. The discovery that character trigram representations composed with bi-LSTMs outperform most other settings across many languages, particularly for morphologically rich languages.
2. A demonstration that character-level models, while effective, do not match the predictive accuracy of models with explicit morphological analysis, especially for fusional and root-pattern languages.
3. A comprehensive analysis of how different subword representations interact with morphological typologies, revealing that factors like orthography influence the effectiveness of these models.
Strengths
1. Systematic and Rigorous Evaluation: The paper systematically varies subword units, composition methods, and languages, providing a robust comparison. The use of ten typologically diverse languages strengthens the generalizability of the findings.
2. Novel Insights: The identification of character trigram + bi-LSTM as a particularly effective combination is a novel contribution. The analysis also highlights the limitations of character-based models in capturing root morphemes and functional relationships.
3. Comprehensive Analysis: The paper goes beyond quantitative results by providing qualitative analyses, such as targeted perplexity evaluations and nearest-neighbor comparisons, which offer deeper insights into the behavior of the models.
4. Practical Relevance: The findings have practical implications for NLP applications in morphologically rich languages, suggesting when character-based models suffice and when explicit morphological analysis is necessary.
Weaknesses
1. Limited Novelty in Methodology: While the experiments are thorough, the methods themselves (e.g., bi-LSTMs, CNNs, BPE) are well-established. The novelty lies more in the experimental design than in methodological innovation.
2. Insufficient Exploration of Semi-Supervised Morphological Analysis: The paper acknowledges the potential of semi-supervised learning but does not explore it, leaving a gap in addressing the practical challenge of limited annotated data.
3. Orthography Bias: The influence of orthography on results is noted but not deeply analyzed. For instance, languages with complex scripts (e.g., Japanese) might require additional considerations that are not fully addressed.
4. Reduplication Analysis: While the paper analyzes reduplication in Indonesian, the results are inconclusive, and the analysis does not fully explain why BPE underperforms in this context.
Questions to Authors
1. Could you elaborate on why character trigrams composed with bi-LSTMs outperform other models for fusional and root-pattern languages? Is it primarily due to their ability to capture local orthographic patterns?
2. How do you envision incorporating semi-supervised morphological analysis into your framework? Would it involve pretraining on annotated data or joint learning with character-based models?
3. For languages with complex scripts (e.g., Japanese), do you think additional preprocessing (e.g., segmentation) could improve the performance of subword models?
Recommendation
This paper makes a significant contribution to understanding subword representations across languages with diverse morphological typologies. While the methodological novelty is limited, the systematic evaluation and insights are valuable for the NLP community. I recommend acceptance, with minor revisions to address the weaknesses mentioned above.