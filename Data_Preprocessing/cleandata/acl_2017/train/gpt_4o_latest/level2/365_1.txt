Review
Summary and Contributions
This paper presents a novel application of encoder-decoder architectures for historical text normalization, particularly focusing on Early New High German texts. The authors propose several architectures, including a multi-task learning (MTL) model that leverages grapheme-to-phoneme mappings as an auxiliary task. Their contributions include: (1) demonstrating that encoder-decoder architectures outperform existing baselines for historical text normalization, (2) showing that MTL improves performance by regularizing the learning process, and (3) providing an analysis of how MTL implicitly learns attention-like mechanisms, making explicit attention mechanisms redundant. The authors also make their implementation publicly available, which enhances reproducibility and practical utility.
Strengths
1. State-of-the-Art Performance: The proposed models achieve a 2-3% absolute improvement in word accuracy over competitive baselines, demonstrating clear advancements in the field. The inclusion of beam search and lexical filtering further enhances performance.
2. Novelty of MTL Application: The use of grapheme-to-phoneme mappings as an auxiliary task is a creative and effective solution to the problem of limited training data. This approach is well-motivated and demonstrates significant performance gains.
3. Comprehensive Analysis: The paper provides a thorough analysis of the models, including saliency analysis, parameter comparisons, and t-SNE visualizations of learned embeddings. This analysis offers valuable insights into the mechanisms underlying MTL and its relationship with attention.
4. Reproducibility: The authors make their implementation publicly available, which is commendable and aligns with best practices in the field. This facilitates further research and practical adoption of their methods.
Weaknesses
1. Limited Contextual Modeling: The models operate on isolated word forms, which limits their ability to handle ambiguous cases or leverage contextual information. While the authors acknowledge this limitation, they do not propose concrete solutions, such as incorporating token context or reranking with a language model.
2. Dataset Size and Domain: The evaluation is limited to a single dataset (Anselm corpus) with relatively small training sets. While the authors argue that this reflects real-world constraints, broader evaluations across diverse historical corpora would strengthen the generalizability of their findings.
3. Redundancy in Attention Analysis: The claim that MTL learns attention-like mechanisms is intriguing but not fully substantiated. While the saliency analysis and parameter comparisons are insightful, the evidence could be more rigorous, such as through ablation studies or deeper exploration of the interplay between MTL and explicit attention.
4. Minor Clarity Issues: The paper is dense, and some technical details (e.g., hyperparameter tuning and decoding strategies) could be more concisely presented. This would improve accessibility for readers less familiar with the domain.
Questions to Authors
1. Could you elaborate on how your models might handle ambiguous cases (e.g., "jn" normalizing to "in" vs. "ihn") in the absence of contextual information?
2. Have you considered evaluating your models on other historical corpora or languages to assess generalizability?
3. Could you provide more evidence or ablation studies to support the claim that MTL inherently learns attention-like mechanisms?
Recommendation
Overall, this paper makes a significant contribution to the field of historical text normalization by introducing novel encoder-decoder architectures and demonstrating the utility of MTL. While there are some limitations in contextual modeling and generalizability, the strengths of the work outweigh its weaknesses. I recommend acceptance, with minor revisions to address the clarity of the analysis and expand on the limitations.