Review
Summary and Contributions
This paper addresses the challenge of zero pronoun (ZP) resolution, particularly the lack of annotated data, by proposing a novel approach to automatically generate large-scale pseudo training data. The authors adapt cloze-style reading comprehension neural networks for ZP resolution and introduce a two-step training mechanism to bridge the gap between pseudo and real-world data. The main contributions of the paper, as I see them, are:
1. Pseudo Training Data Generation: The paper presents a simple yet effective method to generate pseudo training data for ZP resolution, leveraging frequency-based blanking of nouns/pronouns in documents.
2. Two-Step Training Mechanism: The proposed pre-training and adaptation method effectively combines pseudo data and task-specific data, significantly improving performance.
3. Attention-Based Neural Network Model: The authors develop an attention-based neural network architecture tailored for ZP resolution, incorporating mechanisms to handle unknown words.
Strengths
1. Significant Performance Improvement: The proposed approach achieves a 3.1% absolute improvement in F-score over the state-of-the-art on the OntoNotes 5.0 dataset. The results are statistically significant and consistent across most domains.
2. Practical Contribution: The pseudo training data generation method is simple, domain-agnostic, and scalable, making it a valuable contribution for tasks with limited annotated data.
3. Innovative Two-Step Training: The pre-training and adaptation approach effectively leverages both pseudo and task-specific data, demonstrating a clear advantage over using either data type alone.
4. Comprehensive Evaluation: The paper provides detailed experimental results, including domain-specific performance, ablation studies (e.g., UNK processing, domain adaptation), and error analysis, which strengthen the validity of the claims.
5. Novelty in Neural Architecture: The attention-based model, combined with the proposed unknown word processing mechanism, is well-suited for ZP resolution and addresses key challenges like long-distance dependencies.
Weaknesses
1. Limited Error Mitigation: While the paper identifies key error sources (e.g., unknown words and long-distance antecedents), the proposed solutions (e.g., UNK processing) are incremental and do not fully address these challenges. For instance, the performance drop in BN and TC domains suggests that the model struggles with noisy or oral-style text.
2. Lack of Theoretical Justification: While the two-step training method is empirically effective, the paper does not provide a theoretical explanation for why this approach works better than alternatives like joint training.
3. Domain-Specific Limitations: The reliance on task-specific data in the adaptation step limits the generalizability of the approach to domains without annotated data. This dependency is not sufficiently discussed.
4. Evaluation Scope: The evaluation focuses exclusively on Chinese ZP resolution. It would strengthen the paper to include experiments on other languages or tasks to demonstrate the broader applicability of the approach.
Questions to Authors
1. How does the model handle cases where the pseudo training data introduces noise or incorrect labels? Is there a mechanism to filter low-quality pseudo samples?
2. Could the proposed two-step training method be extended to a multi-task learning framework to jointly train on pseudo and task-specific data?
3. Have you considered alternative neural architectures (e.g., transformers) for ZP resolution, and how might they compare to the proposed attention-based model?
Recommendation
Overall, this paper makes a strong contribution to the field of ZP resolution by addressing a critical bottleneck (data scarcity) and achieving state-of-the-art performance. While there are some limitations, the proposed methods are innovative, practical, and well-supported by experimental results. I recommend acceptance with minor revisions to address the identified weaknesses and provide additional theoretical insights.