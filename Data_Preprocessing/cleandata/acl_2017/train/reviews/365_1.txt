[update after reading author response: the alignment of the hidden units does
not match with my intuition and experience, but I'm willing to believe I'm
wrong in this case.  Discussing the alignment in the paper is important (and
maybe just sanity-checking that the alignment goes away if you initialize with
a different seed).  If what you're saying about how the new model is very
different but only a little better performing -- a 10% error reduction -- then
I wonder about an ensemble of the new model and the old one.  Seems like
ensembling would provide a nice boost if the failures across models are
distinct, right?  Anyhow this is a solid paper and I appreciate the author
response, I raise my review score to a 4.]
- Strengths:
  1)  Evidence of the attention-MTL connection is interesting
  2)  Methods are appropriate, models perform well relative to state-of-the-art
- Weaknesses:
  1)  Critical detail is not provided in the paper
  2)  Models are not particularly novel
- General Discussion:
This paper presents a new method for historical text normalization.  The model
performs well, but the primary contribution of the paper ends up being a
hypothesis that attention mechanisms in the task can be learned via multi-task
learning, where the auxiliary task is a pronunciation task.  This connection
between attention and MTL is interesting.
There are two major areas for improvement in this paper.  The first is that we
are given almost no explanation as to why the pronunciation task would somehow
require an attention mechanism similar to that used for the normalization task.
 Why the two tasks (normalization and pronunciation) are related is mentioned
in the paper: spelling variation often stems from variation in pronunciation. 
But why would doing MTL on both tasks result in an implicit attention mechanism
(and in fact, one that is then only hampered by the inclusion of an explicit
attention mechanism?).                    This remains a mystery.  The paper can
leave some
questions unanswered, but at least a suggestion of an answer to this one would
strengthen the paper.
The other concern is clarity.  While the writing in this paper is clear, a
number of details are omitted.                    The most important one is the
description
of
the attention mechanism itself.  Given the central role that method plays, it
should be described in detail in the paper rather than referring to previous
work.  I did not understand the paragraph about this in Sec 3.4.
Other questions included why you can compare the output vectors of two models
(Figure 4), while the output dimensions are the same I don't understand why the
hidden layer dimensions of two models would ever be comparable.  Usually how
the hidden states are "organized" is completely different for every model, at
the very least it is permuted.                    So I really did not understand
Figure 4.
The Kappa statistic for attention vs. MTL needs to be compared to the same
statistic for each of those models vs. the base model.
At the end of Sec 5, is that row < 0.21 an upper bound across all data sets?
Lastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL
approaches make large changes to the model (comparing e.g. Fig 5) but the
experimental improvements in accuracy for either model are quite small (2%),
which seems like a bit of a contradiction.