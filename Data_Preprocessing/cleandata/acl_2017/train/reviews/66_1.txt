This paper describes several ways to encode arbitrarily long sequences of
digits using something called the major system. In the major system, each digit
is mapped to one or more characters representing consonantal phonemes; the
possible mappings between digit and phoneme are predefined. The output of an
encoding is typically a sequence of words constrained such that digits in the
original sequence correspond to characters or digraphs in the output sequence
of words; vowels added surrounding the consonant phonemes to form words are
unconstrained. This paper describes several ways to encode your sequence of
digits such that the output sequence of words is more memorable, generally by
applying syntactic constraints and heuristics.
I found this application of natural language processing concepts somewhat
interesting, as I have not read an ACL paper on this topic before. However, I
found the paper and ideas presented here to have a rather old-school feel. With
much of the focus on n-gram models for generation, frequent POS-tag sequences,
and other heuristics, this paper really could have been written 15-20 years
ago. I am not sure that there is enough novelty in the ideas here to warrant
publication in ACL in 2017. There is no contribution to NLP itself, e.g. in
terms of modeling or search, and not a convincing contribution to the
application area which is just an instance of constrained generation. 
Since you start with one sequence and output another sequence with a very
straightforward monotonic mapping, it seems like a character-based
sequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with
Neural Networks; Sutskever et al. 2014) would work rather well here, very
likely with very fluent output and fewer moving parts (e.g. trigram models and
POS tag and scoring heuristics and postprocessing with a bigram model). You can
use large amounts of training from an arbitrary genre and do not need to rely
on an already-tagged corpus like in this paper, or worry about a parser. This
would be a 2017 paper.