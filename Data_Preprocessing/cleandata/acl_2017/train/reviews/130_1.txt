- Strengths:
This paper proposes to apply NLP to speech transcripts (narratives and
descriptions) in order to identify patients with MCI (mild cognitive
impairment, ICD-10 code F06.7). The authors claim that they were able to
distinguish between healthy control participants and patients with MCI (lines
141-144). However in the conclusion, lines 781-785, they say that "…
accuracy ranging from 60% to 85% …. means that it is not easy to distinguish
between healthy subjects and those with cognitive impairments". So the paper
beginning is more optimistic than the conclusion but anyway the message is
encouraging and the reader becomes curious to see more details about what has
been actually done.
The corpus submitted in the dataset is constructed for 20 healthy patients and
20 control participants only (20+20), and it is non-understandable for people
who do not speak Portuguese. It would be good to incorporate more technological
details in the article and probably to include at least one example of a short
transcript that is translated to English, and eventually a (part of a) sample
network with embeddings for this transcript.
- Weaknesses:
The paper starts with a detailed introduction and review of relevant work. Some
of the cited references are more or less NLP background so they can be omitted
e.g. (Salton 1989) in section 4.2.3. Other references are not directly related
to the topic e.g. "sentiment classification" and "pedestrian detection in
images", lines 652-654, and they can be omitted too. In general lines
608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to
compress the first 5 pages, focusing the review strictly on the paper topic,
and consider the technological innovation in more detail, incl. samples of
English translations of the ABCD and/or Cindarela narratives.
The relatively short narratives in Portuguese esp. in ABCD dataset open the
question how the similarities between words have been found, in order to
construct word embeddings. In lines 272-289 the authors explain that they
generate word-level networks from continuous word representations. What is the
source for learning the continuous word representations; are these the datasets
ABCD+Cinderella only, or external corpora were used? In lines 513-525 it is
written that sub-word level (n-grams) networks were used to generate word
embeddings. Again, what is the source for the training? Are we sure that the
two kinds of networks together provide better accuracy? And what are the
"out-of-vocabulary words" (line 516), from where they come?
- General Discussion:
It is important to study how NLP can help to discover cognitive impairments;
from this perspective the paper is interesting. Another interesting aspect is
that it deals with NLP for Portuguese, and it is important to explain how one
computes embeddings for a language with relatively fewer resources (compared to
English). 
The text needs revision: shortening sections 1-3, compressing 4.1 and adding
more explanations about the experiments. Some clarification about the NURC/SP
N. 338 EF and 331 D2 transcription norms can be given.
Technical comments:
Line 029: '… as it a lightweight …' -> shouldn't this be '… as in
a lightweight …'
Line 188: PLN -> NLP
Line 264: 'out of cookie out of the cookie' – some words are repeated
twice 
Table 3, row 2, column 3: 72,0 -> 72.0
Lines 995-996: the DOI number is the same as the one at lines 1001-1002; the
link behind the title at lines 992-993 points to the next paper in the list