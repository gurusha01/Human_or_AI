- Strengths: Useful application for teachers and learners; supports
fine-grained comparison of GEC systems.
- Weaknesses: Highly superficial description of the system; evaluation not
satisfying.
- General Discussion:
The paper presents an approach of automatically enriching the output of GEC
systems with error types. This is a very useful application because both
teachers and learners can benefit from this information (and many GEC systems
only output a corrected version, without making the type of error explicit). It
also allows for finer-grained comparison of GEC systems, in terms of precision
in general, and error type-specific figures for recall and precision.
Unfortunately, the description of the system remains highly superficial. The
core of the system consists of a set of (manually?) created rules but the paper
does not provide any details about these rules. The authors should, e.g., show
some examples of such rules, specify the number of rules, tell us how complex
they are, how they are ordered (could some early rule block the application of
a later rule?), etc. -- Instead of presenting relevant details of the system,
several pages of the paper are devoted to an evaluation of the systems that
participated in CoNLL-2014. Table 6 (which takes one entire page) list results
for all systems, and the text repeats many facts and figures that can be read
off the table. 
The evaluation of the proposed system is not satisfying in several aspects. 
First, the annotators should have independently annotated a gold standard for
the 200 test sentences instead of simply rating the output of the system. Given
a fixed set of tags, it should be possible to produce a gold standard for the
rather small set of test sentences. It is highly probable that the approach
taken in the paper yields considerably better ratings for the annotations than
comparison with a real gold standard (see, e.g., Marcus et al. (1993) for a
comparison of agreement when reviewing pre-annotated data vs. annotating from
scratch). 
Second, it is said that "all 5 raters individually considered at least 95% of
our rule-based error types to be either "Good" or "Acceptable"".
Multiple rates should not be considered individually and their ratings averaged
this way, this is not common practice. If each of the "bad" scores were
assigned to different edits (we don't learn about their distribution from the
paper), 18.5% of the edits were considered "bad" by some annotator -- this
sounds much worse than the average 3.7%, as calculated in the paper.
Third, no information about the test data is provided, e.g. how many error
categories they contain, or which error categories are covered (according to
the cateogories rated as "good" by the annotators).
Forth, what does it mean that "edit boundaries might be unusual"? A more
precise description plus examples are at need here. Could this be problematic
for the application of the system?
The authors state that their system is less domain dependent as compared to
systems that need training data. I'm not sure that this is true. E.g., I
suppose that Hunspell's vocabulary probably doesn't cover all domains in the
same detail, and manually-created rules can be domain-dependent as well -- and
are completely language dependent, a clear drawback as compared to machine
learning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)
are from one domain only: student essays.
It remains unclear why a new set of error categories was designed. One reason
for the tags is given: to be able to search easily for underspecified
categories (like "NOUN" in general). It seems to me that the tagset presented
in Nicholls (2003) supports such searches as well. Or why not using the
CoNLL-2014 tagset? Then the CoNLL gold standard could have been used for
evaluation.
To sum up, the main motivation of the paper remains somewhat unclear. Is it
about a new system? But the most important details of it are left out. Is it
about a new set of error categories? But hardly any motivation or discussion of
it is provided. Is it about evaluating the CoNLL-2014 systems? But the
presentation of the results remains superficial.
Typos:
- l129 (and others): c.f. -> cf.
- l366 (and others): M2 -> M^2 (= superscribed 2)
- l319: 50-70 F1: what does this mean? 50-70%?
Check references for incorrect case
- e.g. l908: esl -> ESL
- e.g. l878/79: fleiss, kappa