Update after author response: 
1. My major concern about the optimization of model's hyperparameter (which are
numerous) has not been addressed. This is very important, considering that you
report results from folded cross-validation. 
2. The explanation that benefits of their method are experimentally confirmed
with 2% difference -- while evaluating via 5-fold CV on 200 examples -- is
quite unconvincing.
========================================================================
Summary:
In this paper authors present a complex neural model for detecting factuality
of event mentions in text. The authors combine the following in their complex
model:                          (1) a set of traditional classifiers for detecting
event
mentions,
factuality sources, and source introducing predicates (SIPs), (2) A
bidirectional attention-based LSTM model that learns latent representations for
elements on different dependency paths used as input, (2) A CNN that uses
representations from the LSTM and performs two output predictions (one to
detect specific from underspecified cases and another to predict the actual
factuality class). 
From the methodological point of view, the authors are combining a reasonably
familiar methods (att-BiLSTM and CNN) into a fairly complex model. However,
this model does not take raw text (sequence of word embeddings) as input, but
rather hand-crafted features (e.g., different dependency paths combining
factuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted
features is somewhat surprising if coupled with complex deep model. The
evaluation seems a bit tainted as the authors report the results from folded
cross-validation but do not report how they optimized the hyperparameters of
the model. Finally, the results are not too convincing -- considering the
complexity of the model and the amount of preprocessing required (extraction of
event mentions, SIPs, and clues), a 2% macro-average gain over the rule-based
baseline and overall 44% performance seems modest, at best (looking at
Micro-average, the proposed model doesn't outperform simple MaxEnt classifier).
The paper is generally well-written and fairly easy to understand. Altogether,
I find this paper to be informative to an extent, but in it's current form not
a great read for a top-tier conference.   
Remarks:
1. You keep mentioning that the LSTM and CNN in your model are combined
"properly" -- what does that actually mean? How does this "properness"
manifest? What would be the improper way to combine the models?
2. I find the motivation/justification for the two output design rather weak: 
    - the first argument that it allows for later addition of cues (i.e
manually-designed features) kind of beats the "learning representations"
advantage of using deep models. 
        - the second argument about this design tackling the imbalance in the
training set is kind of hand-wavy as there is no experimental support for this
claim. 
3. You first motivate the usage of your complex DL architecture with learning
latent representations and avoiding manual design and feature computation.  And
then you define a set of manually designed features (several dependency paths
and lexical features) as input for the model. Do you notice the discrepancy? 
4. The LSTMs (bidirectional, and also with attention) have by now already
become a standard model for various NLP tasks. Thus I find the detailed
description of the attention-based bidirectional LSTM unnecessary. 
5. What you present as a baseline in Section 3 is also part of your model (as
it generates input to your model). Thus, I think that calling it a baseline
undermines the understandability of the paper. 
6. The results reported originate from a 5-fold CV. However, the model contains
numerous hyperparameters that need to be optimized (e.g., number of filters and
filter sizes for CNNs). How do you optimize these values? Reporting results
from a folded cross-validation doesn't allow for a fair optimization of the
hypeparameters: either you're not optimizing the model's hyperparameters at
all, or you're optimizing their values on the test set (which is unfair). 
7. "Notice that some values are non-application (NA) grammatically, e.g., PRu,
PSu, U+/-" -- why is underspecification in ony one dimension (polarity or
certainty) not an option? I can easily think of a case where it is clear the
event is negative, but it is not specified whether the absence of an event is
certain, probable, or possible. 
Language & style:
1. "to a great degree" -> "great degree" is an unusual construct, use either
"great extent" or "large degree"
2. "events that can not" -> "cannot" or "do not"
3. "describes out networks...in details shown in Figure 3." -> "...shown in
Figure 3 in details."