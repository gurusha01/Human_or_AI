- Strengths:
This paper presents an extension of many popular methods for learning vector
representations of text.  The original methods, such as skip-gram with negative
sampling, Glove, or other PMI based approaches currently use word cooccurrence
statistics, but all of those approaches could be extended to n-gram based
statistics.  N-gram based statistics would increase the complexity of every
algorithm because both the vocabulary of the embeddings and the context space
would be many times larger.  This paper presents a method to learn embeddings
for ngrams with ngram context, and efficiently computes these embeddings.  On
similarity and analogy tasks, they present strong results.
- Weaknesses:
I would have loved to see some experiments on real tasks where these embeddings
are used as input beyond the experiments presented in the paper.  That would
have made the paper far stronger.
- General Discussion:
Even with the aforementioned weakness, I think this is a nice paper to have at
ACL.
I have read the author response.