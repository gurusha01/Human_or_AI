- Strengths: In general, the paper is well structured and clear. It is possible
to follow most of the explanation, the ideas presented are original and the
results obtained are quite interesting.
- Weaknesses: I have some doubts about the interpretation of the results. In
addition, I think that some of the claims regarding the capability of the
method proposed to learn morphology are not propperly backed by scientific
evidence.
- General Discussion:
This paper explores a complex architecture for character-level neural machine
translation (NMT). The proposed architecture extends a classical
encoder-decoder architecture by adding a new deep word-encoding layer capable
of encoding the character-level input into sub-word representations of the
source-language sentence. In the same way, a deep word-decoding layer is added
to the output to transform the target-language sub-word representations into a
character sequence as the final output of the NMT system. The objective of such
architecture is to take advantage of the benefits of character-level NMT
(reduction of the size of the vocabulary and flexibility to deal with unseen
words) and, at the same time, improving the performance of the whole system by
using an intermediate representation of sub-words to reduce the size of the
input sequence of characters. In addition, the authors claim that their deep
word-encoding model is able to learn morphology better than other
state-of-the-art approaches.
I have some concerns regarding the evaluation. The authors compare their
approach to other state-of-the-art systems taking into account two parameters:
training time and BLEU score. However, I do not clearly see the advantage of
the model proposed (DCNMT) in front of other approaches such as bpe2char. The
difference between both approaches as regards BLEU score is very small (0.04 in
Cs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming
the other one without statistical significance information: has statistical
significance been evaluated? As regards the training time, it is worth
mentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs
training time is not provided (why not?) and for En-Fr bpe2char is not
evaluated. I think that a more complete comparison with this system should be
carried out to prove the advantages of the model proposed.
My second concern is on the 5.2 Section, where authors start claiming that they
investigated about the ability of their system to learn morphology. However,
the section only contains a examples and some comments on them. Even though
these examples are very well chosen and explained in a very didactic way, it is
worth noting that no experiments or formal evaluation seem to have been
carried out to support the claims of the authors. I would definitely encourage
authors to extend this very interesting part of the paper that could even
become a different paper itself. On the other hand, this Section does not seem
to be a critical point of the paper, so for the current work I may suggest just
to move this section to an appendix and soften some of the claims done
regarding the capabilities of the system to learn morphology.
Other comments, doubts and suggestions:
 - There are many acronyms that are used but are not defined (such as LSTM,
HGRU, CNN or PCA) or which are defined after starting to use them (such as RNN
or BPE). Even though some of these acronyms are well known in the field of deep
learning, I would encourage the authors to defined them to improve clearness.
 - The concept of energy is mentioned for the first time in Section 3.1. Even
though the explanation provided is enough at that point, it would be nice to
refresh the idea of energy in Section 5.2 (where it is used several times) and
providing some hints about how to interpret it: a high energy on a character
would be indicating that the current morpheme should be split at that point? In
addition, the concept of peak (in Figure 5) is not described.
 - When the acronym BPE is defined, capital letters are used, but then, for the
rest of mentions it is lower cased; is there a reason for this?
 - I am not sure if it is necessary to say that no monolingual corpus is used
in Section 4.1.
 - It seems that there is something wrong with Figure 4a since the colours for
the energy values are not shown for every character.
 - In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not
taken from the papers, since they are not reported. If the authors computed
these results by themselves (as it seems) they should mention it.
 - I would not say that French is morphologically poor, but rather that it is
not that rich as Slavic languages such as Czech.
 - Why a link is provided for WMT'15 training corpora but not for WMT'14?
 - Several references are incomplete
Typos:
  - "..is the bilingual, parallel corpora provided..." -> "..are the bilingual,
parallel corpora provided..."
  - "Luong and Manning (2016) uses" -> "Luong and Manning (2016) use"
  - "HGRU (It is" -> "HGRU (it is"
  - "coveres" -> "covers"
  - "both consists of two-layer RNN, each has 1024" -> "both consist of
two-layer RNN, each have 1024"
  - "the only difference between CNMT and DCNMT is CNMT" -> "the only
difference between CNMT and DCNMT is that CNMT"