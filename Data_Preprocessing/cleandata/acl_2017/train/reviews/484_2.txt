The paper considers a synergistic combination of two non-HMM based speech
recognition techniques: CTC and attention-based seq2seq networks. The
combination is two-fold:
1. first, similarly to Kim et al. 2016 multitask learning is used to train a
model with a joint CTC and seq2seq cost.
2. second (novel contribution), the scores of the CTC model and seq2seq model
are ensembled during decoding (results of beam search over the seq2seq model
are rescored with the CTC model).
The main novelty of the paper is in using the CTC model not only as an
auxiliary training objective (originally proposed by Kim et al. 2016), but also
during decoding.
- Strengths:
The paper identifies several problems stemming from the flexibility offered by
the attention mechanism and shows that by combining the seq2seq network with
CTC the problems are mitigated.
- Weaknesses:
The paper is an incremental improvement over Kim et al. 2016 (since two models
are trained, their outputs can just as well be ensembled). However, it is nice
to see that such a simple change offers important performance improvements of
ASR systems.
- General Discussion:
A lot of the paper is spent on explaining the well-known, classical ASR
systems. A description of the core improvement of the paper (better decoding
algorithm) starts to appear only on p. 5. 
The description of CTC is nonstandard and maybe should either be presented in a
more standard way, or the explanation should be expanded. Typically, the
relation p(C|Z) (eq. 5) is deterministic - there is one and only one character
sequence that corresponds to the blank-expanded form Z. I am also unsure about
the last transformation of the eq. 5.