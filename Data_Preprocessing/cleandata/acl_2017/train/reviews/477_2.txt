tldr: The authors compare a wide variety of approaches towards sub-word
modelling in language modelling, and show that modelling morphology gives the
best results over modelling pure characters. Further, the authors do some
precision experiments to show that the biggest benefit towards sub-word
modelling is gained after words typically exhibiting rich morphology (nouns and
verbs). The paper is comprehensive and the experiments justify the core claims
of the paper. 
- Strengths:
1) A comprehensive overview of different approaches and architectures towards
sub-word level modelling, with numerous experiments designed to support the
core claim that the best results come from modelling morphemes.
2) The authors introduce a novel form of sub-word modelling based on character
tri-grams and show it outperforms traditional approaches on a wide variety of
languages.
3) Splitting the languages examined by typology and examining the effects of
the models on various typologies is a welcome introduction of linguistics into
the world of language modelling.
4) The analysis of perplexity reduction after various classes of words in
Russian and Czech is particularly illuminating, showing how character-level and
morpheme-level models handle rare words much more gracefully. In light of these
results, could the authors say something about how much language modelling
requires understanding of semantics, and how much it requires just knowing
various morphosyntactic effects?
- Weaknesses:
1) The character tri-gram LSTM seems a little unmotivated. Did the authors try
other character n-grams as well? As a reviewer, I can guess that character
tri-grams roughly correspond to morphemes, especially in Semitic languages, but
what made the authors report results for 3-grams as opposed to 2- or 4-? In
addition, there are roughly 26^3=17576 possible distinct trigrams in the Latin
lower-case alphabet, which is enough to almost constitute a word embedding
table. Did the authors only consider observed trigrams? How many distinct
observed trigrams were there?
2) I don't think you can meaningfully claim to be examining the effectiveness
of character-level models on root-and-pattern morphology if your dataset is
unvocalised and thus doesn't have the 'pattern' bit of 'root-and-pattern'. I
appreciate that finding transcribed Arabic and Hebrew with vowels may be
challenging, but it's half of the typology.
3) Reduplication seems to be a different kind of phenomenon to the other three,
which are more strictly morphological typologies. Indonesian and Malay also
exhibit various word affixes, which can be used on top of reduplication, which
is a more lexical process. I'm not sure splitting it out from the other
linguistic typologies is justified.
- General Discussion:
1) The paper was structured very clearly and was very easy to read.
2) I'm a bit puzzled about why the authors chose to use 200 dimensional
character embeddings. Once the dimensionality of the embedding is greater than
the size of the vocabulary (here the number of characters in the alphabet),
surely you're not getting anything extra?
-------------------------------
Having read the author response, my opinions have altered little. I still think
the same strengths and weakness that I have already discussed hold.