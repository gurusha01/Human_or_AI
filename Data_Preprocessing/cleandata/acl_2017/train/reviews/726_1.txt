The paper presents a neural model for predicting SQL queries directly from
natural language utterances, without going through an intermediate formalism.
In addition, an interactive online feedback loop is proposed and tested on a
small scale.
- Strengths:
1\ The paper is very clearly written, properly positioned, and I enjoyed
reading it.
2\ The proposed model is tested and shown to perform well on 3 different
domains (academic, geographic queries, and flight booking)
3\ The online feedback loop is interesting and seems promising, despite of the
small scale of the experiment.
4\ A new semantic corpus is published as part of this work, and additionally
two
existing corpora are converted to SQL format, which I believe would be
beneficial for future work in this area.
- Weaknesses / clarifications:
1\ Section 4.2 (Entity anonymization) - I am not sure I understand the choice
of the length of span for querying the search engine. Why and how is it
progressively reduced? (line 333).
2\ Section 5 (Benchmark experiments) - If I understand correctly, the feedback
loop (algorithm 1) is not used for these experiments. If this is indeed the
case, I'm not sure when does data augmentation occur. Is all the annotated
training data augmented with paraphrases? When is the "initial data" from
templates added? Is it also added to the gold training set? If so, I think it's
not surprising that it doesn't help much, as the gold queries may be more
diverse.  In any case, I think this should be stated more clearly. In addition,
I think it's interesting to see what's the performance of the "vanilla" model,
without any augmentation, I think that this is not reported in the paper.
3\ Tables 2 and 3: I find the evaluation metric used here somewhat unclear. 
Does the accuracy measure the correctness of the execution of the query (i.e.,
the retrieved answer) as the text seem to indicate? (Line 471 mentions
executing the query). Alternatively, are the queries themselves compared? (as
seems to be the case for Dong and Lapata in Table 2). If this is done
differently for different systems (I.e., Dong and Lapata), how are these
numbers comparable? In addition, the text mentions the SQL model has "slightly
lower accuracy than the best non-SQL results" (Line 515), yet in table 2 the
difference is almost 9 points in accuracy.  What is the observation based upon?
Was some significance test performed? If not, I think the results are still
impressive for direct to SQL parsing, but that the wording should be changed,
as the difference in performance does seem significant.
4\ Line 519 - Regarding the data recombination technique used in Jia and Liang
(2016): Since this technique is applicable in this scenario, why not try it as
well?  Currently it's an open question whether this will actually improve
performance. Is this left as future work, or is there something prohibiting the
use of this technique?
5\ Section 6.2 (Three-stage online experiment) - several details are missing /
unclear:
* What was the technical background of the recruited users?
* Who were the crowd workers, how were they recruited and trained?
* The text says "we recruited 10 new users and asked them to issue at least 10
utterances". Does this mean 10 queries each (e.g., 100 overall), or 10 in
total (1 for each).
* What was the size of the initial (synthesized) training  set? 
* Report statistics of the queries - some measure of their lexical variability
/ length / complexity of the generated SQL? This seems especially important for
the first phase, which is doing surprisingly well. Furthermore, since SCHOLAR
uses SQL and NL, it would have been nice if it were attached to this
submission, to allow its review during this period.
6\ Section 6.3 (SCHOLAR dataset)
* The dataset seems pretty small in modern standards (816 utterances in total),
while one of the main advantages of this process is its scalability. What
hindered the creation of a much larger dataset?
* Comparing performance - is it possible to run another baseline on this newly
created dataset to compare against the reported 67% accuracy obtained in this
paper (line 730).
7\ Evaluation of interactive learning experiments (Section 6): I find the
experiments to be somewhat hard to replicate as they involve manual queries of
specific annotators. For example, who's to say if the annotators in the last
phase just asked simpler questions? I realise that this is always problematic
for online learning scenarios, but I think that an effort should be made
towards an objective comparison. For starters, the statistics of the queries
(as I mentioned earlier) is a readily available means to assess whether this
happens. Second, maybe there can be some objective held out test set? This is
problematic as the model relies on the seen queries, but scaling up the
experiment (as I suggested above) might mitigate this risk. Third, is it
possible to assess a different baseline using this online technique? I'm not
sure whether this is applicable given that previous methods were not devised as
online learning methods.
- Minor comments:
1\ Line 48 - "requires" -> "require"
2\ Footnote 1 seems too long to me. Consider moving some of its content to the
body of the text.
3\ Algorithm 1: I'm not sure what "new utterances" refers to (I guess it's new
queries from users?). I think that an accompanying caption to the algorithm
would make the reading easier.
4\ Line 218 - "Is is" -> "It is"
5\ Line 278 mentions an "anonymized" utterance. This confused me at the first
reading, and if I understand correctly it refers to the anonymization described
later in 4.2. I think it would be better to forward reference this. 
- General Discussion:
Overall, I like the paper, and given answers to the questions I raised above,
would like to see it appear in the conference.
- Author Response:
I appreciate the detailed response made by the authors, please include these
details in a final version of the paper.