- Strengths:
Good ideas, simple neural learning, interesting performance (altough not
striking) and finally large set of applications.
- Weaknesses: amount of novel content. Clarity in some sections. 
The paper presents a neural learning method for entity disambiguation and
linking. It introduces a good idea to integrate entity, mention and sense
modeling within the smame neural language modeling technique. The simple
training procedure connected with the modeling allows to support a large set of
application.
The paper is clear formally, but the discussion is not always at the same level
of the technical ideas.
The empirical evaluation is good although not striking improvements of the
performance are reported. Although it seems an extension of (Yamada et al.,
CoNLL 2016), it adds novel ideas and it is of a releant interest.
The weaker points of the paper are:
- The prose is not always clear. I found Section 3 not as clear. Some details
of Figure 2 are not explained and the terminology is somehow redundant: for
example, why do you refer to the dictionary of mentions? or the dictionary of
entity-mention pairs? are these different from text anchors and types for
annotated text anchors?
- Tha paper is quite close in nature to Yamada et al., 2016) and the authors
should at least outline the differences.
One general observation on the current version is:
The paper tests the Multiple Embedding model against entity
linking/disambiguation tasks. However, word embeddings are not only used to
model such tasks, but also some processes not directly depending on entities of
the KB, e.g. parsing, coreference or semantic role labeling. 
The authors should show that the word embeddings provided by the proposed MPME
method are not weaker wrt to simpler wordspaces in such other semantic tasks,
i.e. those involving directly entity mentions.
I did read the author's response.