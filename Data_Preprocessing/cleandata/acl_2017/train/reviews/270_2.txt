The paper proposes a model for the Stanford Natural Language Inference (SNLI)
dataset, that builds on top of sentence encoding models and the decomposable
word level alignment model by Parikh et al. (2016). The proposed improvements
include performing decomposable attention on the output of a BiLSTM and feeding
the attention output to another BiLSTM, and augmenting this network with a
parallel tree variant.
- Strengths:
This approach outperforms several strong models previously proposed for the
task. The authors have tried a large number of experiments, and clearly report
the ones that did not work, and the hyperparameter settings of the ones that
did. This paper serves as a useful empirical study for a popular problem.
- Weaknesses:
Unfortunately, there are not many new ideas in this work that seem useful
beyond the scope the particular dataset used. While the authors claim that the
proposed network architecture is simpler than many previous models, it is worth
noting that the model complexity (in terms of the number of parameters) is
fairly high. Due to this reason, it would help to see if the empirical gains
extend to other datasets as well. In terms of ablation studies, it would help
to see 1) how well the tree-variant of the model does on its own and 2) the
effect of removing inference composition from the model.
Other minor issues:
1) The method used to enhance local inference (equations 14 and 15) seem very
similar to the heuristic matching function used by Mou et al., 2015 (Natural
Language Inference by Tree-Based Convolution and Heuristic Matching). You may
want to cite them.
2) The first sentence in section 3.2 is an unsupported claim. This either needs
a citation, or needs to be stated as a hypothesis.
While the work is not very novel, the the empirical study is rigorous for the
most part, and could be useful for researchers working on similar problems.
Given these strengths, I am changing my recommendation score to 3. I have read
the authors' responses.