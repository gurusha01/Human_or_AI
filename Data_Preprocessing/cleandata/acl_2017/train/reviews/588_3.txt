- Strengths:
The paper empirically verifies that using external knowledge is a benefit.
- Weaknesses:
Real world NLP applications should utilize external knowledge for making better
predictions. The authors propose Rare Entity prediction task to demonstrate
this is the case. However, the motivation of the task is not fully justified.
Why is this task important? How would real world NLP applications benefit from
this task? The paper lacks a convincing argument for proposing a new task. For
current reading comprehension task, the evidence for a correct answer can be
found in a given text, thus we are interested in learning a model of the world
(i.e causality for example), or a basic reasoning model. Comparing to reading
comprehension, rare entity prediction is rather unrealistic as humans are
terrible with remembering name. The authors mentioned that the task is
difficult due to the large number of rare entities, however challenging tasks
with the same or even more difficult level exist, such as predicting correct
morphological form of a word in morphologically rich languages. Such tasks have
obvious applications in machine translation for example.
- General Discussion:
It would be helpful if the authors characterize the dataset in more details.
From figure 1 and table 4, it seems to me that overlapping entities is an
important feature. There is noway i can predict the blank in figure 1 if I
don't see the word London in Peter Ackoyd description. That's being said,
before brutalizing neural networks, it is essential to understand the
characteristic of the data and the cognitive process that searches for the
right answer.
Given the lack of characteristic of the dataset, I find that the baselines are
inappropriate. First of all, the CONTENC is a natural choice at the first sigh.
However as the authors mentioned that candidate entities are rare, the
embeddings of those entities are unrealizable. As a consequence, it is expected
that CONTENC doesn't work well. Would it is fairer if the embeddings are
initialized from pre-trained vectors on massive dataset? One would expect some
sort of similarity between Larnaca and Cyprus in the embedding space and
CONTENC would make a correct prediction in Table 4. What would be the
performance of TF-IDF+COS and AVGEMB+COS if only entities are used to compute
those vectors?
From modeling perspective, I appreciate that the authors chose a sigmoid
predictor that output a numerical score between (0,1). This would help avoiding
normalization over the list of candidates, which are rare and is difficult to
learn reliable weights for those. However, a sidestep technique does exist,
such as Pointer Network. A representation hi for Ci (blank included) can be
computed by an LSTM or BiLSTM, then Pointer Network would give a probabilistic
interpretation p(ek|Ci) \propto exp(dot(d{ek}, h_i)). In my opinion,
Pointer Network would be an appropriate baseline. Another related note: Does
the unbalanced set of negative/positive labels affect the training? During
training, the models make 1 positive prediction while number of negative
predictions is at least 4 times higher?
While I find the task of Rare Entity prediction is unrealistic, having the
dataset, it would be more interesting to learn about the reasoning process that
leads to the right answer such as which set of words the model attends to when
making prediction.