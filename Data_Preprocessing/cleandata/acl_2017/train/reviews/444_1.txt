This paper presents evaluation metrics for lyrics generation exploring the need
for the lyrics to be original,but in a similar style to an artist whilst being
fluent and co-herent. The paper is well written and the motivation for the
metrics are well explained.  
The authors describe both hand annotated metrics (fluency, co-herence and
match) and an automatic metric for 'Similarity'. Whilst the metric for
Similarity is unique and interesting the paper does not give any evidence of
this as an effective automatic metric as correlations between this metric and
the others are low, (which they say that they should be used separately). The
authors claim it can be used to meaningfully analyse system performance but we
have to take their word for it as again there is no correlation with any
hand-annotated performance metric.  Getting worse scores than a baseline system
isn't evidence that the metric captures quality (e.g. you could have a very
strong baseline).
Some missing references, e.g. recent work looking at automating co-herence,
e.g. using mutual information density (e.g. Li et al. 2015). In addition, some
reference to style matching from the NLG community are missing (e.g. Dethlefs
et al. 2014 and the style matching work by Pennebaker).