This paper presents a method for generating morphology, focusing on gender and
number, using deep learning techniques. From a morphologically simplified
Spanish text, the proposed approach uses a classifier to reassign the gender
and number for each token, when necessary. The authors compared their approach
with other learning algorithms, and evaluated it in machine translation on the
Chinese-to-Spanish (Zh->Es) translation direction.
Recently, the task of generating gender and number has been rarely tackled,
morphology generation methods usually target, and are evaluated on,
morphologically-rich languages like German or Finnish.
However, calling the work presented in this paper "morphology
generation" is a bit overselling as the proposed method clearly deals only
with
gender and number. And given the fact that some rules are handcrafted for this
specific task, I do not think this method can be straightforwardly applied to
do more complex morphology generation for morphologically-rich languages.
This paper is relatively clear in the sections presenting the proposed method.
A
lot of work has been done to design the method and I think it can have some
interesting impact on various NLP tasks. However the evaluation part of
this work is barely understandable as many details of what is done, or why it
is done, are missing. From this evaluation, we cannot know if the proposed
method brings improvements over state-of-the-art methods while the experiments
cannot be replicated. Furthermore, no analysis of the results obtained is
provided. Since half a page is still available, there was the possibility
to provide more information to make more clear the evaluation. This work lacks
of motivation. Why do you think deep learning can especially improve gender and
number generation over state-of-the-art methods?
In your paper, the word "contribution" should be used more wisely, as it is
now in the paper, it is not obvious what are the real contributions (more
details below). 
abstract:
what do you mean by unbalanced languages?
section 1:
You claim that your main contribution is the use of deep learning. Just the use
of deep learning in some NLP task is not a contribution.
section 2:
You claim that neural machine translation (NMT), mentioned as "neural
approximations",  does not achieve state-of-the-art results for Zh->Es. I
recommend to remove this claim from the paper, or to discuss it more, since
Junczys-Dowmunt et al. (2016), during the last IWSLT, presented some results
for Zh->Es with the UN corpus, showing that NMT outperforms SMT by around 10
BLEU points.
section 5.1:
You wrote that using the Zh->Es language pair is one of your main
contributions. Just using a language pair is not a contribution. Nonetheless, I
think it is nice to see a paper on machine translation that does not focus of
improving machine translation for English.
The numbers provided in Table 2 were computed before or after preprocessing?
Why did you remove the sentences longer than 50 tokens?
Precise how did you obtain development and test sets, or provide them. Your
experiments are currently no replicable especially because of that.
section 5.2:
You wrote that you used Moses and its default parameters, but the default
parameters of Moses are not the same depending on the version, so you should
provide the number of the version used.
section 5.3:
What do you mean by "hardware cost"?
Table 3: more details should be provided regarding how did you obtain these
values. You chose these values given the classifier accuracy, but how precisely
and on what data did you train and test the classifiers? On the same data used
in section 6?
If I understood the experiments properly, you used simplified Spanish. But I
cannot find in the text how do you simplify Spanish. And how do you use it to
train the classifier and the SMT system? 
section 6:
Your method is better than other classification
algorithms, but it says nothing about how it performs compared to the
state-of-the-art methods. You should at least precise why you chose these
classifications algorithms for comparison. Furthermore, how your rules impact
these results? And more generally, how do you explain such a high accuracy for
you method?
Did you implement all these classification algorithms by yourselves? If not,
you must provide the URL or cite the framework you used.
For the SMT experiments, I guess you trained your phrase table on simplified
Spanish. You must precise it.
You chose METEOR over other metrics like BLEU to evaluate your results. You
must provide some explanation for this choice. I particularly appreciate when I
see a MT paper that does not use BLEU for evaluation, but if you use METEOR,
you must mention which version you used. METEOR has largely changed since 2005.
You cited the paper of 2005, did you use the 2005 version? Or did you use the
last one with paraphrases? 
Are your METEOR scores statistically significant?
section 7:
As future work you mentioned "further simplify morphology". In this paper,
you do not present any simplification of morphology, so I think that choosing
the word
"further" is misleading.
some typos:
femenine
ensambling
cuadratic
style:
plain text citations should be rewritten like this: "(Toutanova et al, 2008)
built " should be "Toutanova et al. (2008) built "
place the caption of your tables below the table and not above, and with more
space between the table and its caption.
You used the ACL 2016 template. You must use the new one prepared for ACL 2017.
More generally, I suggest that you read again the FAQ and the submission
instructions provided on the ACL 2017 website. It will greatly help you to
improve the paper. There are also important information regarding references:
you must provide DOI or URL of all ACL papers in your references.
-----------------------
After authors response:
Thank you for your response.
You wrote that rules are added just as post-processing, but does it mean that
you do not apply them to compute your classification results? Or if you do
apply them before computing these results, I'm still wondering about their
impact on these results.
You wrote that Spanish is simplified as shown in Table 1, but it does not
answer my question: how did you obtain these simplifications exactly? (rules?
software? etc.) The reader need to now that to reproduce your approach.
The classification algorithms presented in Table 5 are not state-of-the-art, or
if they are you need to cite some paper. Furthermore, this table only tells
that deep learning gives the best results for classification, but it does not
tell at all if your approach is better than state-of-the-art approach for
machine translation. You need to compare your approach with other
state-of-the-art morphology generation approaches (described in related work)
designed for machine translation. If you do that your paper will be much more
convincing in my opinion.