Review: Multimodal Word Distributions
- Strengths:  Overall a very strong paper.
- Weaknesses: The comparison against similar approaches could be extended.
- General Discussion:
The main focus of this paper is the introduction of a new model for learning
multimodal word distributions formed from Gaussian mixtures for multiple word
meanings. i. e. representing a word by a set of many Gaussian distributions. 
The approach, extend the model introduced by Vilnis and McCallum (2014) which
represented word as unimodal Gaussian distribution. By using a multimodal, the
current approach attain the problem of polysemy.
Overall, a very strong paper, well structured and clear. The experimentation is
correct and the qualitative analysis made in table 1 shows results as expected
from the approach.  There's not much that can be faulted and all my comments
below are meant to help the paper gain additional clarity. 
Some comments: 
_ It may be interesting to include a brief explanation of the differences
between the approach from Tian et al. 2014 and the current one. Both split
single word representation into multiple prototypes by using a mixture model. 
_ There are some missing citations that could me mentioned in related work as :
Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector
Space Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014
Do Multi-Sense Embeddings Improve Natural Language Understanding? Li and
Jurafsky, EMNLP 2015
Topical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015
_ Also, the inclusion of the result from those approaches in tables 3 and 4
could be interesting. 
_ A question to the authors: What do you attribute the loss of performance of
w2gm against w2g in the analysis of SWCS?
I have read the response.