- Strengths:
The authors focus on a very challenging task of answering open-domain question
from Wikipedia. Authors have developed 1) a document retriever to retrieve
relevant Wikipedia articles for a question, and 2) Document retriever to
retrieve the exact answer from the retrieved paragraphs. 
Authors used Distant Supervision to fine-tune their model. Experiments show
that the document reader performs better than WikiSearch API, and Document
Reader model does better than some recent models for QA.
- Weaknesses:
The final results are inferior to some other models, as presented by the
authors. Also, no error analysis is provided.
- General Discussion:
The proposed systems by the authors is end-to-end and interesting. However, I
have some concerns below.
Document Retriever: Authors have shown a better retrieval performance than Wiki
Search. However, it is not described as to how exactly the API is used.
WikiSearch may not be a good baseline for querying "questions" (API suits
structured retrieval more). Why don't the authors use some standard IR
baselines for this?
Distant Supervision: How effective and reliable was distant supervision?
Clearly, the authors had to avoid using many training examples because of this,
but whatever examples the authors could use, what fraction was actually "close
to correct"? Some statistics would be helpful to understand if some more
fine-tuning of distant supervision could have helped.
Full Wikipedia results: This was the main aim of the authors and as authors
themselves said, the full system gives a performance of 26.7 (49.6 when correct
doc given, 69.5 when correct paragraph is given). Clearly, that should be a
motivation to work more on the retrieval aspect? For WebQuestions, the results
are much inferior to YodaQA, and that raises the question -- whether Wikipedia
itself is sufficient to answer all the open-domain questions? Should authors
think of an integrated model to address this? 
Overall, the final results shown in Tables 4 and 5 are inferior to some other
models. While authors only use Wikipedia, the results are not indicative of
this being the best strategy.
Other points:
The F1 value in Table 5 (78.4) is different from that in Table 4 (Both Dev and
Test).
Table 5: Why not "No f_emb"?
Error analysis: Some error analysis is required in various components of the
system. 
Are there some specific type of questions, where the system does not perform
well? Is there any way one can choose which question is a good candidate to be
answered by Wikipedia, and use this method only for those questions?
For WebQuestions, DS degrades the performance further.