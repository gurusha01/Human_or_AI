- Strengths:
Detailed guidelines and explicit illustrations.
- Weaknesses:
The document-independent crowdsourcing annotation is unreliable. 
- General Discussion:
This work creates a new benchmark corpus for concept-map-based MDS. It is well
organized and written clearly. The supplement materials are sufficient. I have
two questions here.
1)              Is it necessary to treat concept map extraction as a separate
task?
On
the one hand, many generic summarization systems build a similar knowledge
graph and then generate summaries accordingly. On the other hand, with the
increase of the node number, the concept map becomes growing hard to
distinguish. Thus, the general summaries should be more readable.
2)              How can you determine the importance of a concept independent of
the
documents? The definition of summarization is to reserve the main concepts of
documents. Therefore, the importance of a concept highly depends on the
documents. For example, in the given topic of coal mining accidents, assume
there are two concepts: A) an instance of coal mining accidents and B) a cause
of coal mining accidents. Then, if the document describes a series of coal
mining accidents, A is more important than B. In comparison, if the document
explores why coal mining accidents happen, B is more significant than A.
Therefore, just given the topic and two concepts A&B, it is impossible to judge
their relative importance.
I appreciate the great effort spent by authors to build this dataset. However,
this dataset is more like a knowledge graph based on common sense rather than
summary.