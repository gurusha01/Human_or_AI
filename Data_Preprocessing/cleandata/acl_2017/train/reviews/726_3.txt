This paper proposes an approach to learning a semantic parser using an
encoder-decoder neural architecture, with the distinguishing feature that the
semantic output is full SQL queries. The method is evaluated over two standard
datasets (Geo880 and ATIS), as well as a novel dataset relating to document
search.
This is a solid, well executed paper, which takes a relatively well
established technique in the form of an encoder-decoder with some trimmings
(e.g. data augmentation through paraphrasing), and uses it to generate SQL
queries, with the purported advantage that SQL queries are more expressive
than other semantic formalisms commonly used in the literature, and can be
edited by untrained crowd workers (familiar with SQL but not semantic
parsing). I buy that SQL is more expressive than the standard semantic
formalisms, but then again, were there really any queries in any of your three
datasets where the standard formalisms are unable to capture the full
semantics of the query? I.e. are they really the best datasets to showcase the
expressivity of SQL? Also, in terms of what your model learns, what fraction
of SQL does it actually use? I.e. how much of the extra expressivity in SQL is
your model able to capture? Also, does it have biases in terms of the style of
queries that it tends to generate? That is, I wanted to get a better sense of
not just the potential of SQL, but the actuality of what your model is able
to capture, and the need for extra expressivity relative to the datasets you
experiment over. Somewhat related to this, at the start of Section 5, you
assert that it's harder to directly produce SQL. You never actually show this,
and this seems to be more a statement of the expressivity of SQL than anything
else (which returns me to the question of how much of SQL is the model
actually generating).
Next, I would really have liked to have seen more discussion of the types of
SQL queries your model generates, esp. for the second part of the evaluation,
over the SCHOLAR dataset. Specifically, when the query is ill-formed, in what
ways is it ill-formed? When a crowd worker is required to post-edit the query,
how much effort does that take them? Equally, how correct are the crowd
workers at constructing SQL queries? Are they always able to construct perfect
queries (experience would suggest that this is a big ask)? In a similar vein
to having more error analysis in the paper, I would have liked to have seen
agreement numbers between annotators, esp. for Incomplete Result queries,
which seems to rely heavily on pre-existing knowledge on the part of the
annotator and therefore be highly subjective.
Overall, what the paper achieves is impressive, and the paper is well
executed; I just wanted to get more insights into the true ability of the
model to generate SQL, and a better sense of what subset of the language it
generates.
A couple of other minor things:
l107: "non-linguists can write SQL" -- why refer to "non-linguists" here? Most
linguists wouldn't be able to write SQL queries either way; I think the point
you are trying to make is simply that "annotators without specific training in
the semantic translation of queries" are able to perform the task
l218: "Is is" -> "It is"
l278: it's not clear what an "anonymized utterance" is at this point of the
paper
l403: am I right in saying that you paraphrase only single words at a time?
Presumably you exclude "entities" from paraphrasing?
l700: introduce a visual variable in terms of line type to differentiate the
three lines, for those viewing in grayscale
There are various inconsistencies in the references, casing issues
(e.g. "freebase", "ccg"), Wang et al. (2016) is missing critical publication
details, and there is an "In In" for Wong and Mooney (2007)