- Strengths:
The DMC task seems like a good test of understanding language and vision. I
like that the task has a clear evaluation metric.
The failure of the caption generation model on the DMC task is quite
interesting. This result further demonstrates that these models are good
language models, but not as good at capturing the semantics of the image.
- Weaknesses:
The experiments are missing a key baseline: a state-of-the-art VQA model
trained with only a yes/no label vocabulary. 
I would have liked more details on the human performance experiments. How many
of the ~20% of incorrectly-predicted images are because the captions are
genuinely ambiguous? Could the data be further cleaned up to yield an even
higher human accuracy?
- General Discussion:
My concern with this paper is that the data set may prove to be easy or
gameable in some way. The authors can address this concern by running a suite
of strong baselines on their data set and demonstrating their accuracies. I'm
not convinced by the current set of experiments because the chosen neural
network architectures appear quite different from the state-of-the-art
architectures in similar tasks, which typically rely on attention mechanisms
over the image.
Another nice addition to this paper would be an analysis of the data set. How
many tokens does the correct caption share with distractors on average? What
kind of understanding is necessary to distinguish between the correct and
incorrect captions? I think this kind of analysis really helps the reader
understand why this task is worthwhile relative to the many other similar
tasks. 
The data generation technique is quite simple and wouldn't really qualify as a
significant contribution, unless it worked surprisingly well.
- Notes
I couldn't find a description of the FFNN architecture in either the paper or
the supplementary material. It looks like some kind of convolutional network
over the tokens, but the details are very unclear. I'm also confused about how
the Veq2Seq+FFNN model is applied to both classification and caption
generation. Is the loglikelihood of the caption combined with the FFNN
prediction during classification? Is the FFNN score incorporated during caption
generation?
The fact that the caption generation model performs (statistically
significantly) worse than random chance needs some explanation. How is this
possible?
528 - this description of the neural network is hard to understand. The final
paragraph of the section makes it clear, however. Consider starting the section
with it.