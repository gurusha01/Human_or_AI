The paper describes an extension of word embedding methods to also provide
representations for phrases and concepts that correspond to words.  The method
works by fixing an identifier for groups of phrases, words and the concept that
all denote this concept, replace the occurrences of the phrases and words by
this identifier in the training corpus, creating a "tagged" corpus, and then
appending the tagged corpus to the original corpus for training.  The
concept/phrase/word sets are taken from an ontology.  Since the domain of
application is biomedical, the related corpora and ontologies are used.  The
researchers also report on the generation of a new test dataset for word
similarity and relatedness for real-world entities, which is novel.
In general, the paper is nicely written.  The technique is pretty natural,
though not a very substantial contribution. The scope of the contribution is
limited, because of focused evaluation within the biomedical domain.
More discussion of the generated test resource could be useful.  The resource
could be the true interesting contribution of the paper.
There is one
small technical problem, but that is probably just a matter of mathematical
expression than implementation.
Technical problem:
Eq. 8: The authors want to define the MAP calculation.                          This
is a
good
idea,
thought I think that a natural cut-off could be defined, rather than ranking
the entire vocabulary.                          Equation 8 does not define a
probability;
it is
quite
easy to show this, even if the size of the vocabulary is infinite.  So you need
to change the explanation (take out talk of a probability).
Small corrections:
line:
556: most concepts has --> most concepts have