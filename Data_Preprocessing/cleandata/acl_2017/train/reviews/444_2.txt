This paper studies how to properly evaluate systems that produce ghostwriting
of rap lyrics.
The authors present manual evaluation along three key aspects: fluency,
coherence, and style matching.
They also introduce automatic metrics that consider uniqueness via maximum
training similarity, and stylistic similarity via rhyme density.
I can find some interesting analysis and discussion in the paper.
The way for manually evaluating style matching especially makes sense to me.
There also exist a few important concerns for me.
I am not convinced about the appropriateness of only doing fluency/coherence
ratings at line level.
The authors mention that they are following Wu (2014), but I find that work
actually studying a different setting of hip hop lyrical challenges and
responses, which should be treated at line level in nature.
While in this work, a full verse consists of multiple lines that normally
should be topically and structurally coherent.
Currently I cannot see any reason why not to evaluate fluency/coherence for a
verse as a whole.
Also, I do not reckon that one should count so much on automatic metrics, if
the main goal is to ``generate similar yet unique lyrics''.
For uniqueness evaluation, the calculations are performed on verse level.
However, many rappers may only produce lyrics within only a few specific topics
or themes.
If a system can only extract lines from different verses, presumably we might
also get a fluent, coherent verse with low verse level similarity score, but we
can hardly claim that the system ``generalizes'' well.
For stylistic similarity with the specified artist, I do not think rhyme
density can say it all, as it is position independent and therefore may not be
enough to reflect the full information of style of an artist.
It does not seem that the automatic metrics have been verified to be well
correlated with corresponding real manual ratings on uniqueness or stylistic
matching.
I also wonder if one needs to evaluate semantic information commonly expressed
by a specified rapper as well, other than only caring about rhythm.
Meanwhile, I understand the motivation for this study is the lack of sound
evaluation methodology.
However, I still find one statement particularly weird:
``our methodology produces a continuous numeric score for the whole verse,
enabling better comparison.''
Is enabling comparisons really more important than making slightly vague but
more reliable, more convincing judgements?
Minor issue:
Incorrect quotation marks in Line 389