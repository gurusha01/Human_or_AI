- Strengths:
This paper contributes to the field of knowledge base-based question answering
(KB-QA), which is to tackle the problem of retrieving results from a structured
KB based on a natural language question. KB-QA is an important and challenging
task.
The authors clearly identify the contributions and the novelty of their work,
provide a good overview of the previous work and performance comparison of
their approach to the related methods.
Previous approaches to NN-based KB-QA represent questions and answers as fixed
length vectors, merely as a bag of words, which limits the expressiveness of
the models. And previous work also don't leverage unsupervised training over
KG, which potentially can help a trained model to generalize.
This paper makes two major innovative points on the Question Answering problem.
1) The backbone of the architecture of the proposed approach is a
cross-attention based neural network, where attention is used for capture
different parts of questions and answer aspects. The cross-attention model
contains two parts, benefiting each other. The A-Q attention part tries to
dynamically capture different aspects of the question, thus leading to
different embedding representations of the question. And the Q-A attention part
also offer different attention weight of the question towards the answer
aspects when computing their Q-A similarity score. 
2) Answer embeddings are not only learnt on the QA task but also modeled using
TransE which allows to integrate more prior knowledge on the KB side. 
Experimental results are obtained on Web questions and the proposed approach
exhibits better behavior than state-of-the-art end-to-end methods. The two
contributions were made particularly clear by ablation experiment. Both the
cross-attention mechanism and global information improve QA performance by
large margins.
The paper contains a lot of contents. The proposed framework is quite
impressive and novel compared with the previous works.
- Weaknesses:
The paper is well-structured, the language is clear and correct. Some minor
typos are provided below.
1. Page 5, column 1, line 421:                                       re-read               
   
 
reread
2. Page 5, column 2, line 454: pairs be    pairs to be
- General Discussion:
In Equation 2: the four aspects of candidate answer aspects share the same W
and b. How about using separate W and b for each aspect? 
I would suggest considering giving a name to your approach instead of "our
approach", something like ANN or CA-LSTM…(yet something different from Table
2).  
In general, I think it is a good idea to capture the different aspects for
question answer similarity, and cross-attention based NN model is a novel
solution for the above task. The experimental results also demonstrate the
effectiveness of the authors' approach. Although the overall performance is
weaker than SP-based methods or some other integrated systems, I think this
paper is a good attempt in end-to-end KB-QA area and should be encouraged.