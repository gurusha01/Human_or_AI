This paper proposes an approach for classifying literal and metaphoric
adjective-noun pairs. The authors create a word-context matrix for adjectives
and nouns where each element of the matrix is the PMI score. They then use
different methods for selecting dimensions of this matrix to represent each
noun/adjective as a vector. The geometric properties of average, nouns, and
adjective vectors and their normalized versions are used as features in
training a regression model for classifying the pairs to literal or metaphor
expressions. Their approach performs similarly to previous work that learns a
vector representation for each adjective.
Supervision and zero-shot learning. The authors argue that their approach
requires less supervision (compared to previous work)  and can do zero-shot
learning. I don't think this is quite right and given that it seems to be one
of the main points of the paper, I think it is worth clarifying. The approach
proposed in the paper is a supervised classification task: The authors form
vector representations from co-occurrence statistics, and then use the
properties of these representations and the gold-standard labels of each pair
to train a classifier. The model (similarly to any other supervised classifier)
can be tested on words that did not occur in the training data; but, the model
does not learn from such examples. Moreover, those words are not really
"unseen" because the model needs to have a vector representation of those
words.
Interpretation of the results. The authors provide a good overview of the
previous related work on metaphors. However, I am not sure what the intuition
about their approach is (that is, using the geometric properties such as vector
length in identifying metaphors). For example, why are the normalized vectors
considered? It seems that they don't contribute to a better performance.
Moreover, the most predictive feature is the noun vector; the authors explain
that this is a side effect of the data which is collected such that each
adjective occurs in both metaphoric and literal expressions. (As a result, the
adjective vector is less predictive.) It seems that the proposed approach might
be only suitable for the given data. This shortcoming is two-fold: (a) From the
theoretical perspective (and especially since the paper is submitted to the
cognitive track), it is not clear what we learn about theories of metaphor
processing. (b) From the NLP applications standpoint, I am not sure how
generalizable this approach is compared to the compositional models.
Novelty. The proposed approach for representing noun/adjective vectors is very
similar to that of Agres et al. It seems that the main contribution of the
paper is that they use the geometric properties to classify the vectors.