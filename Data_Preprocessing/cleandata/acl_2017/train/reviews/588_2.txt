- General Discussion:
The paper deals with the task of predicting missing entities in a given context
using the Freebase definitions of those entities. The authors highlight the
importance of the problem, given that the entities come from a long-tailed
distribution. They use popular sequence encoders to encode the context and the
definitions of candidate entities, and score them based on their similarity
with the context. While it is clear that the task is indeed important, and the
dataset may be useful as a benchmark, the approach has some serious weaknesses
and the evaluation leaves some questions unanswered. 
- Strengths:
The proposed task requires encoding external knowledge, and the associated
dataset may serve as a good benchmark for evaluating hybrid NLU systems.
- Weaknesses:
1) All the models evaluated, except the best performing model (HIERENC), do not
have access to contextual information beyond a sentence. This does not seem
sufficient to predict a missing entity. It is unclear whether any attempts at
coreference and anaphora resolution have been made. It would generally help to
see how well humans perform at the same task.
2) The choice of predictors used in all models is unusual. It is unclear why
similarity between context embedding and the definition of the entity is a good
indicator of the goodness of the entity as a filler.
3) The description of HIERENC is unclear. From what I understand, each input
(h_i) to the temporal network is the average of the representations of all
instantiations of context filled by every possible entity in the vocabulary.
This does not seem to be a good idea since presumably only one of those
instantiations is correct. This would most likely introduce a lot of noise.
4) The results are not very informative. Given that this is a rare entity
prediction problem, it would help to look at type-level accuracies, and 
analyze how the accuracies of the proposed models vary with frequencies of
entities.
- Questions to the authors:
1) An important assumption being made is that d_e are good replacements for
entity embeddings. Was this assumption tested?
2) Have you tried building a classifier that just takes h_i^e as inputs?
I have read the authors' responses. I still think the task+dataset could
benefit from human evaluation. This task can potentially be a good benchmark
for NLU systems, if we know how difficult the task is. The results presented in
the paper are not indicative of this due to the reasons stated above. Hence, I
am not changing my scores.