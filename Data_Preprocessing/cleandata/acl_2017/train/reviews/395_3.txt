TMP
Strength: The paper propose DRL-Sense model that shows a marginal improvement
on SCWS dataset and a significant improvement on ESL-50 and RD-300 datasets.
Weakness:
The technical aspects of the paper raise several concerns:
Could the authors clarify two drawbacks in 3.2? The first drawback states that
optimizing equation (2) leads to the underestimation of the probability of
sense. As I understand, eq(2) is the expected reward of sense selection, z_{ik}
and z_{jl} are independent actions and there are only two actions to optimize.
This should be relatively easy. In NLP setting, optimizing the expected rewards
over a sequence of actions for episodic-task has been proven doable (Sequence
Level Training with Recurrent Neural Networks, Ranzato 2015) even in a more
challenging setting of machine translation where the number of actions ~30,000
and the average sequence length ~30 words. The DRL-Sense model has maximum 3
actions and it does not have sequential nature of RL. This makes it hard to
accept the claim about the first drawback.
The second drawback, accompanied with the detail math in Appendix A, states
that the update formula is to minimize the likelihood due to the log-likelihood
is negative. Note that most out-of-box optimizers (Adam, SGD, Adadelta, …)
minimize a function f, however, a common practice when we want to maximize f we
just minimize -f. Since the reward defined in the paper is negative, any
standard optimizer can be use on the expected of the negative reward, which is
always greater than 0. This is often done in many modeling tasks such as
language model, we minimize negative log-likelihood instead of maximizing the
likelihood. The authors also claim that when "the log-likelihood reaches 0,
it also indicates that the likelihood reaches infinity and computational flow
on U and V" (line 1046-1049). Why likelihood→infinity? Should it be
likelihood→1?
Could the authors also explain how DRL -Sense is based on Q-learning? The
horizon in the model is length of 1. There is no transition between
state-actions and there is not Markov-property as I see it (k, and l are draw
independently). I am having trouble to see the relation between Q-learning and
DRL-Sense.  In (Mnih et al., 2013), the reward is given from the environment
whereas in the paper, the rewards is computed by the model. What's the reward
in DRL-Sense? Is it 0, for all the (state, action) pairs or the cross-entropy
in eq(4)?  
Cross entropy is defined as H(p, q) = -\sum_{x} q(x)\log q(x), which variable
do the authors sum over in (4)? I see that q(Ct, z{i, k}) is a scalar
(computed in eq(3)), while Co(z{ik}, z{jl}) is a distribution over total
number of senses eq(1). These two categorial variables do not have the same
dimension, how is cross-entropy H in eq(4) is computed then?
Could the authors justify the dropout exploration? Why not epsilon-greedy
exploration? Dropout is often used for model regularization, preventing
overfitting. How do the authors know the gain in using dropout is because of
exploration but regularization?
The authors states that Q-value is a probabilistic estimation (line 419), can
you elaborate what is the set of variables the distribution is defined? When
you sum over that set of variable, do you get 1? I interpret that Q is a
distribution over senses per word, however  definition of q in eq(3) does not
contain a normalizing constant, so I do not see q is a valid distribution. This
also related to the value 0.5 in section 3.4 as a threshold for exploration.
Why 0.5 is chosen here where q is just an arbitrary number between (0, 1) and
the constrain \sum_z q(z) = 1 does not held? Does the authors allow the
creation of a new sense in the very beginning or after a few training epochs? I
would image that at the beginning of training, the model is unstable and
creating new senses might introduce noises to the model.  Could the authors
comment on that?
General discussion
What's the justification for omitting negative samples in line 517? Negative
sampling has been use successfully in word2vec due to the nature of the task:
learning representation. Negative sampling, however does not work well when the
main interest is modeling a distribution p() over senses/words. Noise
contrastive estimation is often preferred when it comes to modeling a
distribution. The DRL-Sense, uses collocation likelihood to compute the reward,
I wonder how the approximation presented in the paper affects the learning of
the embeddings.
Would the authors consider task-specific evaluation for sense embeddings as
suggested in recent research [1,2]
[1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor
Labutov, David Mimno and Thorsten Joachims.
[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks .
Manaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer
---
I have read the response.