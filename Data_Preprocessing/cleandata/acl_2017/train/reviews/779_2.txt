In this paper the authors present a method for training a zero-resource NMT
system by using training data from a pivot language. Unlike other approaches
(mostly inspired in SMT), the author's approach doesn't do two-step
decoding. Instead, they use a teacher/student framework, where the teacher
network is trained using the pivot-target language pairs, and the student
network is trained using the source-pivot data and the teacher network
predictions of the target language.
- Strengths:
The results the authors present, show that their idea is promising. Also, the
authors present several sets of results that validate their assumptions.
- Weaknesses:
However, there are many points that need to be address before this paper is
ready for publication.
1)            Crucial information is missing
Can you flesh out more clearly how training and decoding happen in your
training framework? I found out that the equations do not completely describe
the approach. It might be useful to use a couple of examples to make your
approach clearer.
Also, how is the montecarlo sampling done? 
2)            Organization
The paper is not very well organized. For example, results are broken into
several subsections, while they'd better be presented together.  The
organization of the tables is very confusing. Table 7 is referred before table
6. This made it difficult to read the results.
3)            Inconclusive results:
After reading the results section, it's difficult to draw conclusions when,
as the authors point out in their comparisons, this can be explained by the
total size of the corpus involved in their methods (621  ). 
4)            Not so useful information:
While I appreciate the fleshing out of the assumptions, I find that dedicating
a whole section of the paper plus experimental results is a lot of space. 
- General Discussion:
Other:
578:  We observe that word-level models tend to have lower valid loss compared
with sentence- level methods….
Is it valid to compare the loss from two different loss functions?
Sec 3.2, the notations are not clear. What does script(Y) means?
How do we get p(y|x)? this is never explained
Eq 7 deserves some explanation, or better removed.
320: What approach did you use? You should talk about that here
392 : Do you mean 2016?
Nitty-gritty:
742  : import => important
772  : inline citation style
778: can significantly outperform 
275: Assumption 2 needs to be rewritten … a target sentence y from x should
be close to that from its counterpart z.