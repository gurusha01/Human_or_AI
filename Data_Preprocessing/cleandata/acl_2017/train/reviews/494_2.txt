The authors propose 'morph-fitting', a method that retrofits any given set
of trained word embeddings based on a morphologically-driven objective that (1)
pulls inflectional forms of the same word together (as in 'slow' and
'slowing') and (2) pushes derivational antonyms apart (as in
'expensive' and 'inexpensive'). With this, the authors aim to improve
the representation of low-frequency inflections of words as well as mitigate
the tendency of corpus-based word embeddings to assign similar representations
to antonyms. The method is based on relatively simple manually-constructed
morphological rules and is demonstrated on both English, German, Italian and
Russian. The experiments include intrinsic word similarity benchmarks, showing
notable performance improvements achieved by applying morph-fitting to several
different corpus-based embeddings. Performance improvement yielding new
state-of-the-art results is also demonstrated for German and Italian on an
extrinsic task - dialog state tracking. 
Strengths:
- The proposed method is simple and shows nice performance improvements across
a number of evaluations and in several languages. Compared to previous
knowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a
few manually-constructed rules, instead of a large-scale knowledge base, such
as an ontology.
- Like previous retrofitting approaches, this method is easy to apply to
existing sets of embeddings and therefore it seems like the software that the
authors intend to release could be useful to the community.
- The method and experiments are clearly described.

Weaknesses:
- I was hoping to see some analysis of why the morph-fitted embeddings worked
better in the evaluation, and how well that corresponds with the intuitive
motivation of the authors. 
- The authors introduce a synthetic word similarity evaluation dataset,
Morph-SimLex. They create it by applying their presumably
semantic-meaning-preserving morphological rules to SimLex999 to generate many
more pairs with morphological variability. They do not manually annotate these
new pairs, but rather use the original similarity judgements from SimLex999.
The obvious caveat with this dataset is that the similarity scores are presumed
and therefore less reliable. Furthermore, the fact that this dataset was
generated by the very same rules that are used in this work to morph-fit word
embeddings, means that the results reported on this dataset in this work should
be taken with a grain of salt. The authors should clearly state this in their
paper.
- (Soricut and Och, 2015) is mentioned as a future source for morphological
knowledge, but in fact it is also an alternative approach to the one proposed
in this paper for generating morphologically-aware word representations. The
authors should present it as such and differentiate their work.
- The evaluation does not include strong morphologically-informed embedding
baselines. 
General Discussion:
With the few exceptions noted, I like this work and I think it represents a
nice contribution to the community. The authors presented a simple approach and
showed that it can yield nice improvements using various common embeddings on
several evaluations and four different languages. I'd be happy to see it in
the conference.
Minor comments:
- Line 200: I found this phrasing unclear: "We then query â€¦ of linguistic
constraints".
- Section 2.1: I suggest to elaborate a little more on what the delta is
between the model used in this paper and the one it is based on in Wieting
2015. It seemed to me that this was mostly the addition of the REPEL part.
- Line 217: "The method's cost function consists of three terms" - I
suggest to spell this out in an equation.
- Line 223:  x and t in this equation (and following ones) are the vector
representations of the words. I suggest to denote that somehow. Also, are the
vectors L2-normalized before this process? Also, when computing 'nearest
neighbor' examples do you use cosine or dot-product? Please share these
details.
- Line 297-299: I suggest to move this text to Section 3, and make the note
that you did not fine-tune the params in the main text and not in a footnote.
- Line 327: (create, creates) seems like a wrong example for that rule.

* I have read the author response