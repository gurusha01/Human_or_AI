- Strengths:
1. The proposed models are shown to lead to rather substantial and consistent
improvements over reasonable baselines on two different tasks (word similarity
and word analogy), which not only serves to demonstrate the effectiveness of
the models but also highlights the potential utility of incorporating sememe
information from available knowledge resources for improving word
representation learning.
2. The paper contributes to ongoing efforts in the community to account for
polysemy in word representation learning. It builds nicely on previous work and
proposes some new ideas and improvements that could be of interest to the
community, such as applying an attention scheme to incorporate a form of soft
word sense disambiguation into the learning procedure.
- Weaknesses:
1. Presentation and clarity: important details with respect to the proposed
models are left out or poorly described (more details below). Otherwise, the
paper generally reads fairly well; however, the manuscript would need to be
improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the
semantic relations are explicitly encoded by the sememes, as the authors
themselves point out (more details below).
- General Discussion:
1. The authors stress the importance of accounting for polysemy and learning
sense-specific representations. While polysemy is taken into account by
calculating sense distributions for words in particular contexts in the
learning procedure, the evaluation tasks are entirely context-independent,
which means that, ultimately, there is only one vector per word -- or at least
this is what is evaluated. Instead, word sense disambiguation and sememe
information are used for improving the learning of word representations. This
needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of
the SSA model seems to assume the pre-existence of sememe embeddings. This is
important for understanding the subsequent models. Do the SAC and SAT models
require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider
different senses but not sememes. Perhaps the MST baseline is an example of
such a model? If so, this is not sufficiently described (emphasis is instead
put on soft vs. hard word sense disambiguation). The paper would be stronger
with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly
useful for learning representations for low-frequency words (by mapping words
to a smaller set of sememes that are shared by sets of words). Unfortunately,
no empirical evidence is provided to test the hypothesis. It would have been
interesting for the authors to look deeper into this. This aspect also does not
seem to explain the improvements much since, e.g., the word similarity data
sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to
the incorporation of sememe information than word sense disambiguation in the
learning procedure. As mentioned earlier, the evaluation involves only the use
of context-independent word representations. Even if the method allows for
learning sememe- and sense-specific representations, they would have to be
aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially
the modifiers of "computer".
7. It says that the models are trained using their best parameters. How exactly
are these determined? It is also unclear how K is set -- is it optimized for
each model or is it randomly chosen for each target word observation? Finally,
what is the motivation for setting K' to 2?