- Contents:
This paper proposes a new task, and provides a dataset. The task is to predict
blanked-out named entities from a text with the help of an external
definitional resource, in particular FreeBase. These named entities are
typically rare, that is, they do not appear often in the corpus, such that it
is not possible to train models specifically for each entity. The paper argues
convincingly that this is an important setting to explore. Along with multiple
baselines, two neural network models for the problem are presented that make
use of the external resource, one of which also accumulates evidence across
contexts in the same text. 
- Strengths:
The collection of desiderata for the task is well-chosen to advance the field:
predicting blanked-out named entities, a task that has already shown to be
interesting in the CNN/Daily Mail dataset, but in a way that makes the task
hard for language models; and the focus on rare entities should drive the field
towards more interesting models. 
The collection of baselines is well chosen to show that neither a NN model
without external knowledge nor a simple cosine similarity based model with
external knowledge can do the task well.
The two main models are chosen well.
The text is clear and well argued. 
- Weaknesses:
I was a bit puzzled by the fact that using larger contexts, beyond the
sentences with blanks in them, did not help the models. After all, you were in
a way using additional context in the HierEnc model, which accumulates
knowledge from other contexts. There are two possible explanations: Either the
sentences with blanks in them are across the board more informative for the
task than the sentences without. This is the explanation suggested in the
paper, but it seems a bit unintuitive that this should be the case. Another
possible explanation is that the way that you were using additional context in
HierEnc, using the temporal network, is much more useful than by enlarging
individual contexts C and feeding that larger C into the recurrent network.  Do
you think that that could be what is going on?
- General Discussion:
I particularly like the task and the data that this paper proposes. This setup
can really drive the field forward, I think. This in my mind is the main
contribution.