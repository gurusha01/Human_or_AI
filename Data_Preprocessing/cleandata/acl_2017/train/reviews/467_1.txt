- Strengths:
The paper presents an iterative method to induce bilingual word embeddings
using large monolingual corpora starting with very few (or automatically
obtainable numeral) mappings between two languages. Compared to
state-of-the-art using larger bilingual dictionaries or parallel/comparable
corpora, the results obtained with the presented method that relies on very
little or no manually prepared input are exciting and impressive.
- Weaknesses:
I would have liked to see a discussion on the errors of the method, and
possibly a discussion on how the method could be adjusted to deal with them.
- General Discussion:
Does the frequency of the seeds in the monolingual corpora matter?
It would be interesting to see the partial (in the sense of after n number of
iterations) evolution of the mapping between words in the two languages for a
few words. 
What happens with different translations of the same word (like different
senses)?
One big difference between German and English is the prevalence of compounds in
German. What happens to these compounds? What are they mapped onto? Would a
preprocessing step of splitting the compounds help? (using maybe only
corpus-internal unigram information)
What would be the upper bound for such an approach? An analysis of errors --
e.g. words very far from their counterpart in the other language -- would be
very interesting. It would also be interesting to see a discussion of where
these errors come from, and if they could be addressed with the presented
approach.