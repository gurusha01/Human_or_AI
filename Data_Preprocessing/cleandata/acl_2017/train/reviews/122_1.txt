- Strengths:
This paper proposes a novel approach for dialogue state tracking that benefits
from representing slot values with pre-trained embeddings and learns to compose
them into distributed representations of user utterances and dialogue context.
Experiments performed on two datasets show consistent and significant
improvements over the baseline of previous delexicalization based approach.
Alternative approaches (i.e., XAVIER, GloVe, Program-SL999) for pre-training
word embeddings have been investigated.
- Weaknesses:
Although one of the main motivations for using embeddings is to generalize to
more complex dialogue domains where delexicalization may not scale for, the
datasets used seem limited.    I wonder how the approach would compare with and
without a separate slot tagging component on more complex dialogues. For
example, when computing similarity between the utterance and slot value pairs,
one can actually limit the estimation to the span of the slot values. This
should be applicable even when the values do not match.
I think the examples in the intro is misleading, shouldn't the dialogue state
also include "restaurant_name=The House"? This brings another question, how
does resolution of coreferences impact this task?
- General Discussion:
On the overall, use of pre-trained word embeddings is a great idea, and the
specific approach for using them is exciting.