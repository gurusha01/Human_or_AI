The paper presents a method for relation extraction based on converting the
task into a question answering task. The main hypothesis of the paper is that
questions are a more generic vehicle for carrying content than particular
examples of relations, and are easier to create. The results seem to show good
performance, though a direct comparison on a standard relation extraction task
is not performed.
- Strengths:
The technique seems to be adept at identifying relations (a bit under 90
F-measure). It works well both on unseen questions (for seen relations) and
relatively well on unseen relations. The authors describe a method for
obtaining a large training dataset
- Weaknesses:
I wish performance was also shown on standard relation extraction datasets - it
is impossible to determine what types of biases the data itself has here
(relations are generated from Wikidata via WikiReading - extracted from
Wikipedia, not regular newswire/newsgroups/etc). It seems to me that the NIST
TAC-KBP slot filling dataset is good and appropriate to run a comparison.
One comparison that the authors did not do here (but should) is to train a
relation detection model on the generated data, and see how well it compares
with the QA approach.
- General Discussion:
I found the paper to be well written and argued, and the idea is interesting,
and it seems to work decently. I also found it interesting that the zero-shot
NL method behaved indistinguishably from the single question baseline, and not
very far from the multiple questions system.