This paper presents the gated self-matching network for reading comprehension
style question answering. There are three key components in the solution: 
(a) The paper introduces the gated attention-based recurrent network to obtain
the question-aware representation for the passage. Here, the paper adds an
additional gate to attention-based recurrent networks to determine the
importance of passage parts and attend to the ones relevant to the question.
Here they use word as well as character embeddings to handle OOV words.
Overall, this component is inspired from Wang and Jiang 2016.
(b) Then the paper proposes a self-matching attention mechanism to improve the
representation for the question and passage by looking at wider passage context
necessary to infer the answer. This component is completely novel in the paper.
(c) At the output layer, the paper uses pointer networks to locate answer
boundaries. This is also inspired from Wang and Jiang 2016
Overall, I like the paper and think that it makes a nice contribution.
- Strengths:
The paper clearly breaks the network into three component for descriptive
purposes, relates each of them to prior work and mentions its novelties with
respect to them. It does a sound empirical analysis by describing the impact of
each component by doing an ablation study. This is appreciated.
The results are impressive!
- Weaknesses:
The paper describes the results on a single model and an ensemble model. I
could not find any details of the ensemble and how was it created. I believe it
might be the ensemble of the character based and word based model. Can the
authors please describe this in the rebuttal and the paper.
- General Discussion:
Along with the ablation study, it would be nice if we can have a
qualitative analysis describing some example cases where the components of
gating, character embedding, self embedding, etc. become crucial ... where a
simple model doesn't get the question right but adding one or more of these
components helps. This can go in some form of appendix or supplementary.