- Strengths:
The paper offers a natural and useful extension to recent efforts in
interactive topic modeling, namely by allowing human annotators to provide
multiple "anchor words" to machine-induced topics. The paper is well-organized
and the combination of synthetic and user experiments make for a strong paper.
- Weaknesses:
The paper is fairly limited in scope in terms of the interactive topic model
approaches it compares against. I am willing to accept this, since they do make
reference to most of them and explain that these other approaches are not
necessarily fast enough for interactive experimentation or not conducive to the
types of interaction being considered with an "anchoring" interface. Some level
of empirical support for these claims would have been nice, though.
It would also have been nice to see experiments on more than one data set (20
newsgroups, which is now sort of beaten-to-death).
- General Discussion:
In general, this is a strong paper that appears to offer an incremental but
novel and practical contribution to interactive topic modeling. The authors
made the effort to vet several variants of the approach in simulated
experiments, and to conduct fairly exhaustive quantitative analyses of both
simulated and user experiments using a variety of metrics that measure
different facets of topic quality.