- Strengths:
   - The paper states clearly the contributions from the beginning 
   - Authors provide system and dataset
   - Figures help in illustrating the approach
   - Detailed description of the approach
   - The authors test their approach performance on other datasets and compare
to other published work
- Weaknesses:
   -The explanation of methods in some paragraphs is too detailed and there is
no mention of other work and it is repeated in the corresponding method
sections, the authors committed to address this issue in the final version.
   -README file for the dataset [Authors committed to add README file]
- General Discussion:
   - Section 2.2 mentions examples of DBpedia properties that were used as
features. Do the authors mean that all the properties have been used or there
is a subset? If the latter please list them. In the authors' response, the
authors explain in more details this point and I strongly believe that it is
crucial to list all the features in details in the final version for clarity
and replicability of the paper. 
   - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might
be beneficial to add that the input is word embeddings (similarly to Lample et
al.)
   - Figure 3, KNs in source language or in English? (since the mentions have
been translated to English). In the authors' response, the authors stated that
they will correct the figure.
   - Based on section 2.4 it seems that topical relatedness implies that some
features are domain dependent. It would be helpful to see how much domain
dependent features affect the performance. In the final version, the authors
will add the performance results for the above mentioned features, as mentioned
in their response. 
   - In related work, the authors make a strong connection to Sil and Florian
work where they emphasize the supervised vs. unsupervised difference. The
proposed approach is still supervised in the sense of training, however the
generation of training data doesn't involve human interference