- Strengths:
Novel model.  I particularly like the ability to generate keyphrases not
present in the source text.
- Weaknesses:
 Needs to be explicit whether all evaluated models are trained and tested on
the same data sets.  Exposition of the copy mechanism not quite
clear/convincing.
- General Discussion:
This paper presents a supervised neural network approach for keyphrase
generation.  The model uses an encoder-decoder architecture that
first encodes input text with a RNN, then uses an attention mechanism to
generate keyphrases from
the hidden states.  There is also a more advanced variant of the
decoder which has an attention mechanism that conditions on the
keyphrase generated in the previous time step.
The model is interesting and novel. And I think the ability to
generate keyphrases not in the source text is particularly
appealing.  My main concern is with the evaluation:  Are all
evaluated models trained with the same amount of data and evaluated
on the same test sets?              It's not very clear.  For example, on the
NUS data set, Section 4.2 line 464 says that the supervised baselines
are evaluated with cross validation.
Other comments:
The paper is mostly clearly written and easy to follow.  However,
some parts are unclear:
- Absent keyphrases vs OOV.  I think there is a need to distinguish
  between the two, and the usage meaning of OOV should be consistent.  The RNN
models
  use the most frequent 50000 words as the vocabulary (Section 3.4
  line 372, Section 5.1 line 568), so I suppose OOV are words not in
  this 50K vocabulary.              In line 568, do you mean OOV or absent
  words/keyphrases?  Speaking of this, I'm wondering how many
  keyphrases fall outside of this 50K?              The use of "unknown words"
  in line 380 is also ambiguous.  I think it's probably clearer to say that
 the RNN models can generate words not present in the source text as long as
they appear
somewhere else in the corpus (and the 50K vocabulary)
- Exposition of the copy mechanism (section 3.4).  This mechanism has a
  more specific locality than the attention model in basic RNN model.
  However, I find the explanation of the intuition misleading.              If I
  understand correctly, the "copy mechanism" is conditioned on the
  source text locations that matches the keyphrase in the previous
  time step y_{t-1}.  So maybe it has a higher tendency to generate n-grams
seen source text (Figure 1).  I buy the argument that the more sophisticated
  attention model probably makes CopyRNN better than the RNN
  overall, but why is the former model particularly better for absent
  keyphrases?  It is as if both models perform equally well on present
keyphrases.
- How are the word embeddings initialized?