- Strengths: well written, solid experimental setup and intriguing qualitative
analysis
- Weaknesses: except for the qualitative analysis, the paper may belong better
to the applications area, since the models are not particularly new but the
application itself is most of its novelty
- General Discussion: This paper presents a "sequence-to-sequence" model with
attention mechanisms and an auxiliary phonetic prediction task to tackle
historical text normalization. None of the used models or techniques are new by
themselves, but they seem to have never been used in this problem before,
showing and improvement over the state-of-the-art. 
Most of the paper seem like a better fit for the applications track, except for
the final analysis where the authors link attention with multi-task learning,
claiming that the two produce similar effects. The hypothesis is intriguing,
and it's supported with a wealth of evidence, at least for the presented task. 
I do have some questions on this analysis though:
1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two
models are aligned? Is it safe to do so?
2) Section 5.2, I don't get what you mean by the errors that each of the models
resolve independently of each other. This is like symmetric-difference? That
is, if we combine the two models these errors are not resolved anymore?
On a different vein, 3) Why is there no comparison with Azawi's model?
========
After reading the author's response.
I'm feeling more concerned than I was before about your claims of alignment in
the hidden space of the two models. If accepted, I would strongly encourage the
authors to make clear
in the paper the discussion you have shared with us for why you think that
alignment holds in practice.