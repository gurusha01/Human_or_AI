This paper proposes to present a more comprehensive evaluation methodology for
the assessment of automatically generated rap lyrics (as being similar to a
target artist).  While the assessment of the generation of creative work is
very challenging and of great interest to the community, this effort falls
short of its claims of a comprehensive solution to this problem.
All assessment of this nature ultimately falls to a subjective measure -- can
the generated sample convince an expert that the generated sample was produced
by the true artist rather than an automated preocess?  This is essentially a
more specific version of a Turing Test.   The effort to automate some parts of
the evaluation to aid in optimization and to understand how humans assess
artistic similarity is valuable.  However, the specific findings reported in
this work do not encourage a belief that these have been reliably identified.
Specifically -- Consider the central question: Was a sample generated by a
target artist?        The human annotators who were asked this were not able to
consistently respond to this question.        This means either 1) the annotators did
not have sufficient expertise to perform the task, or 2) the task was too
challenging, or some combination of the two.  
The proposed automatic measures also failed to show a reliable agreement to
human raters performing the same task.        This dramatically limits their efficacy
in providing a proxy for human assessment.   The low interannotator agreement
may be "expected" because the task is subjective, but the idea of decomposing
the evaluation into fluency and coherence components is meant to make it more
tractable, and thereby improve the consistency of rater scores.  A low IAA for
an evaluation metric is a cause for concern and limits its viability as a
general purpose tool.  
Specific questions/comments:
* Why is a line-by-line level evaluation prefered to a verse level analysis. 
Specifically for "coherence", a line by line analysis limits the scope of
coherence to consequtive lines.
* Style matching -- This term assumes that these 13 artists each have a
distinct style, and always operate in that style. I would argue that some of
these artists (kanye west, eminem, jay z, drake, tupac and notorious big) have
produced work in multiple styles.  A more accurate term for this might be
"artist matching".
* In Section 4.2 The central automated component of the evaluation is low
tf*idf with existing verses, and similar rhyme density.  Given the limitations
of rhyme density -- how well does this work.  Even with the manual intervention
described?
* In Section 6.2 -- This description should include how many judges were used
in this study? In how many cases did the judges already know the verse they
were judging?  In this case the test will not assess how easy it is to match
style, but rather, the judges recall and rap knowledge.