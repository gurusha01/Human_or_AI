- Strengths:
This paper systematically investigated how context types (linear vs
dependency-based) and representations (bound word vs unbound word) affect word
embedding learning. They experimented with three models (Generalized
Bag-Of-Words, Generalized Skip-Gram and Glove) in multiple different tasks
(word similarity, word analogy, sequence labeling and text classification).
Overall, 
1)            It is well-written and structured.
2)            The experiments are very thoroughly evaluated. The analysis could
help
researchers to choose different word embeddings or might even motivate new
models. 
3)            The attached software can also benefit the community. 
- Weaknesses:
 The novelty is limited. 
- General Discussion:
For the dependency-based context types, how does the dependency parsing affect
the overall performance? Is it fair to compare those two different context
types since the dependency-based one has to rely on the predicted dependency
parsing results (in this case CoreNLP) while the linear one does not?