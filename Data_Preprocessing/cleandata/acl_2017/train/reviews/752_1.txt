- Strengths:
The paper demonstrates that seq2seq models can be comparatively effectively
applied to the tasks of AMR parsing and AMR realization by linearization of an
engineered pre-processed version of the AMR graph and associated sentence,
combined with 'Paired Training' (iterative back-translation of monolingual data
combined with fine-tuning). While parsing performance is worse than other
reported papers (e.g., Pust et al., 2015), those papers used additional
semantic information. 
On the task of AMR realization, the paper demonstrates that utilizing
additional monolingual data (via back-translation) is effective relative to a
seq2seq model that does not use such information. (See note below about
comparing realization results to previous non-seq2seq work for the realization
task.)
- Weaknesses:
 At a high-level, the main weakness is that the paper aims for empirical
comparisons, but in comparing to other work, multiple aspects/dimensions are
changing at the same time (in some cases, not comparable due to access to
different information), complicating comparisons. 
For example, with the realization results (Table 2), PBMT (Pourdamghani et al.,
2016) is apparently trained on LDC2014T12, which consists of 13,051 sentences,
compared to the model of the paper, which is trained on LDC2015E86, which
consists of 19,572 sentences, according to http://amr.isi.edu/download.html.
This is used in making the claim of over 5 points improvement over the
state-of-the-art (PBMT) in line 28/29, 120/121, and line 595, and is only
qualified in the caption of Table 2. To make a valid comparison, the approach
of the paper or PBMT needs to be re-evaluated after using the same training
data.
- General Discussion:
Is there any overlap between the sentences in your Gigaword sample and the test
sentences of LDC2015E86? Apparently LDC2015E86 contains data from the ''proxy
report data in LDC's DEFT Narrative Text Source Data R1 corpus (LDC2013E19)''
(Accessible with LDC account: https://catalog.ldc.upenn.edu/LDC2015E86). It
seems LDC2013E19 contains data from Gigaword
(https://catalog.ldc.upenn.edu/LDC2013E19). Apparently AMR corpus LDC2014T12
also contained ''data from newswire articles selected from the English Gigaword
Corpus, Fifth Edition'' (publicly accessible link:
https://catalog.ldc.upenn.edu/docs/LDC2014T12/README.txt). Please check that
there is no test set contamination.
Line 244-249: Did these two modifications to the encoder make a significant
difference in effectiveness? What was the motivation behind these changes?
Please make it clear (in an appendix is fine) for replication purposes whether
the implementation is based on an existing seq2seq framework.
Line 321: What was the final sequence length used? (Consider adding such
details in an appendix.)
Please label the columns of Table 1 (presumably dev and test). Also, there is a
mismatch between Table 1 and the text: ''Table 1 summarizes our development
results for different rounds of self-training.'' It appears that only the
results of the second round of self-training are shown.
Again, the columns for Table 1 are not labeled, but should the results for
column 1 for CAMR instead be 71.2, 63.9, 67.3--the last line of Table 2 in
http://www.aclweb.org/anthology/S16-1181 which is the configuration for
+VERB+RNE+SRL+WIKI? It looks like the second from last row of Table 2 in CAMR
(Wang et al., 2016) is currently being used. On this note, how does your
approach handle the wikification information introduced in LDC2015E86? 
7.1.Stochastic is missing a reference to the example.
Line 713-715: This seems like a hypothesis to be tested empirically rather than
a forgone conclusion, as implied here.
Given an extra page, please add a concluding section.
How are you performing decoding? Are you using beam search?
As a follow-up to line 161-163, it doesn't appear that the actual vocabulary
size used in the experiments is mentioned. After preprocessing, are there any
remaining unseen tokens in dev/test? In other words, is the unknown word
replacement mechanism (using the attention weights), as described in Section
3.2, ever used? 
For the realization case study, it would be of interest to see performance on
phenomena that are known limitations of AMR, such as quantification and tense
(https://github.com/amrisi/amr-guidelines/blob/master/amr.md).
The paper would benefit from a brief discussion (perhaps a couple sentences)
motivating the use of AMR as opposed to other semantic formalisms, as well as
why the human-annotated AMR information/signal might be useful as opposed to
learning a model (e.g., seq2seq itself) directly for a task (e.g., machine
translation).
For future work (not taken directly into account in the scores given here for
the review, since the applicable paper is not yet formally published in the
EACL proceedings): For parsing, what accounts for the difference from previous
seq2seq approaches? Namely, between Peng and Xue, 2017 and AMR-only (as in
Table 1) is the difference in effectiveness being driven by the architecture,
the preprocessing, linearization, data, or some combination thereof? Consider
isolating this difference. (Incidentally, the citation for Peng and Xue, 2017
[''Addressing the Data Sparsity Issue in Neural AMR Parsing''] should
apparently be Peng et al. 2017
(http://eacl2017.org/index.php/program/accepted-papers;
https://arxiv.org/pdf/1702.05053.pdf). The authors are flipped in the
References section.
Proofreading (not necessarily in the order of occurrence; note that these are
provided for reference and did not influence my scoring of the paper):
outperform state of the art->outperform the state of the art
Zhou et al. (2016), extend->Zhou et al. (2016) extend
(2016),Puzikov et al.->(2016), Puzikov et al.
POS-based features, that->POS-based features that
language pairs, by creating->language pairs by creating
using a back-translation MT system and mix it with the human
translations.->using a back-translation MT system, and mix it with the human
translations.
ProbBank-style (Palmer et al., 2005)->PropBank-style (Palmer et al., 2005)
independent parameters ,->independent parameters,
for the 9.6% of tokens->for 9.6% of tokens
maintaining same embedding sizes->maintaining the same embedding sizes
Table 4.Similar->Table 4. Similar
realizer.The->realizer. The
Notation: Line 215, 216: The sets C and W are defined, but never subsequently
referenced. (However, W could/should be used in place of ''NL'' in line 346 if
they are referring to the same vocabulary.)