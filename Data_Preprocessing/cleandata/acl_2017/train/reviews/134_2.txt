The work describes a joint neural approach to argumentation mining. There are
several approaches explored including:
 1) casting the problem as a dependency parsing problem (trying several
different parsers)
 2) casting the problem as a sequence labeling problem
3) multi task learning (based on sequence labeling model underneath)
4) an out of the box neural model for labeling entities and relations (LSTM-ER)
5) ILP based state-of-the art models
All the approaches are evaluated using F1 defined on concepts and relations. 
Dependency based solutions do not work well, seq. labeling solutions are
effective.
The out-of-the-box LSTM-ER model performs very well. Especially on paragraph
level.
The Seq. labeling and LSTM-ER models both outperform the ILP approach.
A very comprehensive supplement was given, with all the technicalities of
training
the models, optimizing hyper-parameters etc.
It was also shown that sequence labeling models can be greatly improved by the
multitask
approach (with the claim task helping more than the relation task).
The aper  is a very thorough investigation of neural based approaches to
end-to-end argumentation mining.
- Major remarks  
  - my one concern is with the data set, i'm wondering if it's a problem that
essays in the train set and in the test set might
   be on the same topics, consequently writers might use the same or similar
arguments in both essays, leading to information
   leakage from the train to the test set. In turn, this might give overly
optimistic performance estimates. Though, i think the same
   issues are present for the ILP models, so your model does not have an unfair
advantage. Still, this may be something to discuss.
  - my other concern is that one of your best models LSTM-ER is acutally just a
an out-of-the box application of a model from related
    work. However, given the relative success of sequence based models and all
the experiments and useful lessons learned, I think this 
    work deserves to be published.
- Minor remarks and questions:
222 - 226 - i guess you are arguing that it's possible to reconstruct the full
graph once you get a tree as output? Still, this part is not quite clear.
443-444 The ordering in this section is seq. tagging -> dependency based -> MTL
using seq. tagging, it would be much easier to follow if the order of the first
two were
                  reversed (by the time I got here i'd forgotten what STag_T
stood for)
455 - What does it mean that it de-couples them but jointly models them (isn't
coupling them required to jointly model them?)
         - i checked Miwa and Bansal and I couldn't find it
477 - 479 -  It's confusing when you say your system de-couples relation info
from entity info, my best guess is that you mean it
                        learns some tasks as "the edges of the tree" and some
other tasks as "the labels on those edges", thus decoupling them. 
                        In any case,  I recommend you make this part clearer
Are the F1 scores in the paragraph and essay settings comparable? In particular
for the relation tasks. I'm wondering if paragraph based 
models might miss some cross paragraph relations by default, because they will
never consider them.