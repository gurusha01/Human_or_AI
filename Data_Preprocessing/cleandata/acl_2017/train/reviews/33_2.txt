- Strengths:
This paper proposes a nice way to combine the neural model (LSTM) with
linguistic knowledge (sentiment lexicon, negation and intensity). The method is
simple yet effective. It achieves the state-of-the-art performance on Movie
Review dataset and is competitive against the best models on SST dataset.    
- Weaknesses:
Similar idea has also been used in (Teng et al., 2016). Though this work is 
more elegant in the framework design and mathematical representation, the
experimental comparison with (Teng et al., 2016) is not as convincing as the
comparisons with the rest methods. The authors only reported the
re-implementation results on the sentence level experiment of SST and did not
report their own phrase-level results.
Some details are not well explained, see discussions below.
- General Discussion:
The reviewer has the following questions/suggestions about this work,
1. Since the SST dataset has phrase-level annotations, it is better to show the
statistics of the times that negation or intensity words actually take effect.
For example, how many times the word "nothing" appears and how many times it
changes the polarity of the context.
2. In section 4.5, the bi-LSTM is used for the regularizers. Is bi-LSTM used to
predict the sentiment label?
3. The authors claimed that "we only use the sentence-level annotation since
one of
our goals is to avoid expensive phrase-level annotation". However, the reviewer
still suggest to add the results. Please report them in the rebuttal phase if
possible.
4. "s_c is a parameter to be optimized but could also be set fixed with prior
knowledge."  The reviewer didn't find the specific definition of s_c in the
experiment section, is it learned or set fixed?  What is the learned or fixed
value?
5. In section 5.4 and 5.5, it is suggested to conduct an additional experiment
with part of the SST dataset where only phrases with negation/intensity words
are included. Report the results on this sub-dataset with and without the
corresponding regularizer can be more convincing.