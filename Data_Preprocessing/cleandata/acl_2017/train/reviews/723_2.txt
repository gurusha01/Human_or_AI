This paper continues the line of work for applying word embeddings for the
problem of unsupervised morphological segmentation (e.g. Soricut & Och, 2015;
Üstün & Can, 2016). The proposed method, MORSE, applies a local optimization
for segmentation of each word, based on a set of orthographic and semantic
rules and a few heuristic threshold values associated with them.
- Strengths:
The paper presents multiple ways to evaluate segmentation hypothesis on word
embeddings, and these may be useful also in other type of methods. The results
on English and Turkish data sets are convincing.
The paper is clearly written and organized, and the biliography is extensive.
The submission includes software for testing the English MORSE model and three
small data sets used in the expriments.
- Weaknesses:
The ideas in the paper are quite incremental, based mostly on the work by
Soricut & Och (2015). However, the main problems of the paper concern
meaningful comparison to prior work and analysis of the method's limitations.
First, the proposed method does not provide any sensible way for segmenting
compounds. Based on Section 5.3, the method does segment some of the compounds,
but using the terminology of the method, it considers either of the
constituents as an affix. Unsuprisingly, the limitation shows up especially in
the results of a highly-compounding language, Finnish. While the limitation is
indicated in the end of the discussion section, the introduction and
experiments seem to assume otherwise.
In particular, the limitation on modeling compounds makes the evaluation of
Section 4.4/5.3 quite unfair: Morfessor is especially good at segmenting
compounds (Ruokolainen et al., 2014), while MORSE seems to segment them only
"by accident". Thus it is no wonder that Morfessor segments much larger
proportion of the semantically non-compositional compounds. A fair experiment
would include an equal number of compounds that should be segmented to their
constituents.
Another problem in the evaluations (in 4.2 and 4.3) concerns hyperparameter
tuning. The hyperparameters of MORSE are optimized on a tuning data, but
apparently the hyperparameters of Morfessor are not. The recent versions of
Morfessor (Kohonen et al. 2010, Grönroos et al. 2014) have a single
hyperparameter that can be used to balance precision and recall of the
segmentation. Given that the MORSE outperforms Morfessor both in precision and
recall in many cases, this does not affect the conclusions, but should at least
be mentioned.
Some important details of the evaluations and results are missing: The
"morpheme-level evaluation" method in 5.2 should be described or referred to.
Moreover, Table 7 seems to compare results from different evaluation sets: the
Morfessor and Base Inference methods seem to be from official Morpho Challenge
evaluations, LLSM is from Narasimhan et al. (2015), who uses aggregated data
from Morpho Challenges (probably including both development and training sets),
and MORSE is evaluated Morpho Challenges 2010 development set. This might not
affect the conclusions, as the differences in the scores are rather large, but
it should definitely be mentioned.
The software package does not seem to support training, only testing an
included model for English.
- General Discussion:
The paper puts a quite lot of focus on the issue of segmenting semantically
non-compositional compounds. This is problematic in two ways: First, as
mentioned above, the proposed method does not seem to provide sensible way of
segmenting any compound. Second, finding the level of lexicalized base forms
(e.g. freshman) and the morphemes as smallest meaning-bearing units (fresh,
man) are two different tasks with different use cases (for example, the former
would be more sensible for phrase-based SMT and the latter for ASR). The
unsupervised segmentation methods, such as Morfessor, typically target at the
latter, and critizing the method for a different goal is confusing.
Finally, there is certainly a continuum on the (semantic) compositionality of
the compound, and the decision is always somewhat arbitrary. (Unfortunately
many gold standards, including the Morpho Challenge data sets, tend to be also
inconsistent with their decisions.)
Sections 4.1 and 5.1 mention the computational efficiency and limitation to one
million input word forms, but does not provide any details: What is the
bottleneck here? Collecting the transformations, support sets, and clusters? Or
the actual optimization problem? What were the computation times and how do
these scale up?
The discussion mentions a few benefits of the MORSE approach: Adaptability as a
stemmer, ability to control precision and recall, and need for only a small
number of gold standard segmentations for tuning. As far as I can see, all or
some of these are true also for many of the Morfessor variants (Creutz and
Lagus, 2005; Kohonen et al., 2010; Grönroos et al., 2014), so this is a bit
misleading. It is true that Morfessor works usually fine as a completely
unsupervised method, but the extensions provide at least as much flexibility as
MORSE has.
(Ref: Mathias Creutz and Krista Lagus. 2005. Inducing the Morphological Lexicon
of a Natural Language from Unannotated Text. In Proceedings of the
International and Interdisciplinary Conference on Adaptive Knowledge
Representation and Reasoning (AKRR'05), Espoo, Finland, June 15-17.)
- Miscellaneous:
Abstract should maybe mention that this is a minimally supervised method
(unsupervised to the typical extent, i.e. excluding hyperparameter tuning).
In section 3, it should be mentioned somewhere that phi is an empty string.
In section 5, it should be mentioned what specific variant (and implementation)
of Morfessor is applied in the experiments.
In the end of section 5.2, I doubt that increasing the size of the input
vocabulary would alone improve the performance of the method for Finnish. For a
language that is morphologically as complex, you never encounter even all the
possible inflections of the word forms in the data, not to mention derivations
and compounds.
I would encourage improving the format of the data sets (e.g.  using something
similar to the MC data sets): For example using "aa" as a separator for
multiple analyses is confusing and makes it impossible to use the format for
other languages.
In the references, many proper nouns and abbreviations in titles are written in
lowercase letters. Narasimhan et al. (2015) is missing all the publication
details.