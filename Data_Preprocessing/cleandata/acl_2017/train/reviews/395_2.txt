This paper describes a novel approach for learning multi-sense word
representations using reinforcement learning. A CBOW-like architecture is used
for sense selection, computing a score for each sense based on the dot product
between the sum of word embeddings in the current context and the corresponding
sense vector. A second module based on the skip-gram model is used to train
sense representations, given results from the sense selection module. In order
to train these two modules, the authors apply Q-Learning, where the Q-value is
provided by the CBOW-based sense selection module. The reward is given by the
skip-gram negative sampling likelihood. Additionally, the authors propose an
approach for determining the number of senses for each word non-parametrically,
by creating new senses when the Q-values for existing scores have a score under
0.5.
The resulting approach achieves good results under the "MaxSimC" metric, and
results comparable to previous approaches under "AvgSimC". The authors suggest
that their approach could be used to improve the performance for downstream
tasks by replacing word embeddings with their most probable sense embedding. It
would have been nice to see this claim explored, perhaps in a sequential
labeling task such as POS-tagging or NER, especially in light of previous work
questioning the usefulness of multi-sense representations in downstream tasks.
I found it somewhat misleading to suggest that relying on MaxSimC could reduce
overhead in a real world application, as the sense disambiguation step (with
associated parameters) would still be required, in addition to the sense
embeddings. A clustering-based approach using a weighted average of sense
representations would have similar overhead. The claims about improving over
word2vec using 1/100 of the data are also not particularly surprising on SCWS.
These are misleading contributions, as they do not advance/differ much from
previous work.
The modular quality of their approach results in a flexibility that I think
could have been explored further. The sense disambiguation module uses a vector
averaging (CBOW) approach. A positive aspect of their model is that they should
be able to substitute other context composition approaches (using alternative
neural architecture composition techniques) relatively easily.
The paper applies an interesting approach to a problem that has been explored
now in many ways. The results on standard benchmarks are comparable to previous
work, but not particularly surprising/interesting. However, the approach goes
beyond a simple extension of the skip-gram model for multi-sense representation
learning by providing a modular framework based on reinforcement learning.
Ideally, this aspect would be explored further. But overall, the approach
itself may be interesting enough on its own to be considered for acceptance, as
it could help move research in this area forward.
* There are a number of typos that should be addressed (line
190--representations, 331--selects, 492--3/4th*).
NOTE: Thank you to the authors for their response.