Summary: This paper presents a model for embedding words, phrases and concepts
into vector spaces. To do so, it uses an ontology of concepts, each of which is
mapped to phrases. These phrases are found in text corpora and treated as
atomic symbols. Using this, the paper uses what is essentially the skip-gram
method to train embeddings for words, the now atomic phrases and also the
concepts associated with them. The proposed work is evaluated on the task of
concept similarity and relatedness using UMLS and Yago to act as the backing
ontologies.
Strengths:
The key question addressed by the paper is that phrases that are not lexically
similar can be semantically close and, furthermore, not all phrases are
compositional in nature. To this end, the paper proposes a plausible model to
train phrase embeddings. The trained embeddings are shown to be competitive or
better at identifying similarity between concepts.
The software released with the paper could be useful for biomedical NLP
researchers.
- Weaknesses:
The primary weakness of the paper is that the model is not too novel. It is
essentially a tweak to skip-gram. 
Furthermore, the full model presented by the paper doesn't seem to be the best
one in the results (in Table 4). On the two Mayo datasets, the Choi baseline is
substantially better. A similar trend seems to dominate Table 6 too. On the
larger UMNSRS data, the proposed model is at best competitive with previous
simpler models (Chiu).
- General Discussion:
The paper says that it is uses known phrases as distant supervision to train
embeddings. However, it is not clear what the "supervision" here is. If I
understand the paper correctly, every occurrence of a phrase associated with a
concept provides the context to train word embeddings. But this is not
supervision in the traditional sense (say for identifying the concept in the
text or other such predictive tasks). So the terminology is a bit confusing.
 The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of
the paper.
The use of \beta to control for compositionality of phrases by words is quite
surprising. Essentially, this is equivalent to saying that there is a single
global constant that decides "how compositional" any phrase should be. The
surprising part here is that the actual values of \beta chosen by cross
validation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which
basically argues against compositionality. 
The experimental setup for table 4 needs some explanation. The paper says that
the data labels similarity/relatedness of concepts (or entities). However, if
the concepts-phrases mapping is really many-to-many, then how are the
phrase/word vectors used to compute the similarities? It seems that we can only
use the concept vectors.
In table 5, the approximate phr method (which approximate concepts with the
average of the phrases in them) is best performing. So it is not clear why we
need the concept ontology. Instead, we could have just started with a seed set
of phrases to get the same results.