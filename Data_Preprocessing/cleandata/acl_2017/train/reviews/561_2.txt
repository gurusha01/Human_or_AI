The paper proposes an approach where pre-trained word embeddings and
pre-trained neural language model embeddings are leveraged (i.e., concatenated)
to improve the performance in English chunking and NER on the respective CoNLL
benchmarks, and on an out-of-domain English NER test set. The method records
state-of-the-art scores for the two tasks.
- Strengths:
For the most part, the paper is well-written and easy to follow. The method is
extensively documented. The discussion is broad and thorough.
- Weaknesses:
Sequence tagging does not equal chunking and NER. I am surprised not to see POS
tagging included in the experiment, while more sequence tagging tasks would be
welcome: grammatical error detection, supersense tagging, CCG supertagging,
etc. This way, the paper is on chunking and NER for English, not for sequence
tagging in general, as it lacks both the multilingual component and the breadth
of tasks.
While I welcomed the extensive description of the method, I do think that
figures 1 and 2 overlap and that only one would have sufficed.
Related to that, the method itself is rather straightforward and simple. While
this is by all means not a bad thing, it seems that this contribution could
have been better suited for a short paper. Since I do enjoy the more extensive
discussion section, I do not necessarily see it as a flaw, but the core of the
method itself does not strike me as particularly exciting. It's more of a
"focused contribution" (short paper description from the call) than
"substantial" work (long paper).
- General Discussion:
Bottomline, the paper concatenates two embeddings, and sees improvements in
English chunking and NER.
As such, does it warrant publication as an ACL long paper? I am ambivalent, so
I will let my score reflect that, even if I slightly lean towards a negative
answer. Why? Mainly because I would have preferred to see more breadth: a) more
sequence tagging tasks and b) more languages.
Also, we do not know how well this method scales to low(er)-resource scenarios.
What if the pre-trained embeddings are not available? What if they were not as
sizeable as they are? The experiments do include a notion of that, but still
far above the low-resource range. Could they not have been learned in a
multi-task learning setup in your model? That would have been more substantial
in my view.
For these reasons, I vote borderline, but with a low originality score. The
idea of introducing context via the embeddings is nice in itself, but this
particular instantiation of it leaves a lot to ask for.