This paper presents a purpose-built neural network architecture for textual
entailment/NLI based on a three step process of encoding, attention-based
matching, and aggregation. The model has two variants, one based on TreeRNNs
and the other based on sequential BiLSTMs. The sequential model outperforms all
published results, and an ensemble with the tree model does better still.
The paper is clear, the model is well motivated, and the results are
impressive. Everything in the paper is solidly incremental, but I nonetheless
recommend acceptance. 
Major issues that I'd like discussed in the response:
– You suggest several times that your system can serve as a new baseline for
future work on NLI. This isn't an especially helpful or meaningful claim—it
could be said of just about any model for any task. You could argue that your
model is unusually simple or elegant, but I don't think that's really a major
selling point of the model.
– Your model architecture is symmetric in some ways that seem like
overkill—you compute attention across sentences in both directions, and run a
separate inference composition (aggregation) network for each direction. This
presumably nearly doubles the run time of your model. Is this really necessary
for the very asymmetric task of NLI? Have you done ablation studies on this?
– You present results for the full sequential model (ESIM) and the ensemble
of that model and the tree-based model (HIM). Why don't you present results for
the tree-based model on its own?
Minor issues:
– I don't think the Barker and Jacobson quote means quite what you want it to
mean. In context, it's making a specific and not-settled point about direct
compositionality in formal grammar. You'd probably be better off with a more
general claim about the widely accepted principle of compositionality.
– The vector difference feature that you use (which has also appeared in
prior work) is a bit odd, since it gives the model redundant parameters. Any
model that takes vectors a, b, and (a - b) as input to some matrix
multiplication is exactly equivalent to some other model that takes in just a
and b and has a different matrix parameter. There may be learning-related
reasons why using this feature still makes sense, but it's worth commenting on.
– How do you implement the tree-structured components of your model? Are
there major issues with speed or scalability there?
– Typo: (Klein and D. Manning, 2003) 
– Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much
more readable parse trees without crossing lines. I'd suggest using them.
---
Thanks for the response! I still solidly support publication. This work is not
groundbreaking, but it's novel in places, and the results are surprising enough
to bring some value to the conference.