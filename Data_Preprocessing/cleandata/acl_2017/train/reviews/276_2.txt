- Strengths: The article is well written; what was done is clear and
straightforward. Given how simple the contribution is, the gains are
substantial, at least in the error correction task.
- Weaknesses: The novelty is fairly limited (essentially, another permutation
of tasks in multitask learning), and only one way of combining the tasks is
explored. E.g., it would have been interesting to see if pre-training is
significantly worse than joint training; one could initialize the weights from
an existing RNN LM trained on unlabeled data; etc.
- General Discussion: I was hesitating between a 3 and a 4. While the
experiments are quite reasonable and the combinations of tasks sometimes new,
there's quite a bit of work on multitask learning in RNNs (much of it already
cited), so it's hard to get excited about this work. I nevertheless recommend
acceptance because the experimental results may be useful to others.
- Post-rebuttal: I've read the rebuttal and it didn't change my opinion of the
paper.