- Strengths:
The authors propose a selective encoding model as extension to the
sequence-to-sequence framework for abstractive sentence summarization. The
paper is very well written and the methods are clearly described. The proposed
methods are evaluated on standard benchmarks and comparison to other
state-of-the-art tools are presented, including significance scores. 
- Weaknesses:
There are some few details on the implementation and on the systems to which
the authors compared their work that need to be better explained. 
- General Discussion:
* Major review:
- I wonder if the summaries obtained using the proposed methods are indeed
abstractive. I understand that the target vocabulary is build out of the words
which appear in the summaries in the training data. But given the example shown
in Figure 4, I have the impression that the summaries are rather extractive.
The authors should choose a better example for Figure 4 and give some
statistics on the number of words in the output sentences which were not
present in the input sentences for all test sets.
- page 2, lines 266-272: I understand the mathematical difference between the
vector hi and s, but I still have the feeling that there is a great overlap
between them. Both "represent the meaning". Are both indeed necessary? Did you
trying using only one of them.
- Which neural network library did the authors use for implementing the system?
There is no details on the implementation.
- page 5, section 44: Which training data was used for each of the systems that
the authors compare to? Diy you train any of them yourselves?
* Minor review:
- page 1, line 44: Although the difference between abstractive and extractive
summarization is described in section 2, this could be moved to the
introduction section. At this point, some users might no be familiar with this
concept.
- page 1, lines 93-96: please provide a reference for this passage: "This
approach achieves huge success in tasks like neural machine translation, where
alignment between all parts of the input and output are required."
- page 2, section 1, last paragraph: The contribution of the work is clear but
I think the authors should emphasize that such a selective encoding model has
never been proposed before (is this true?). Further, the related work section
should be moved to before the methods section.
- Figure 1 vs. Table 1: the authors show two examples for abstractive
summarization but I think that just one of them is enough. Further, one is
called a figure while the other a table.
- Section 3.2, lines 230-234 and 234-235: please provide references for the
following two passages: "In the sequence-to-sequence machine translation (MT)
model, the encoder and decoder are responsible for encoding input sentence
information and decoding the sentence representation to generate an output
sentence"; "Some previous works apply this framework to summarization
generation tasks."
- Figure 2: What is "MLP"? It seems not to be described in the paper.
- page 3, lines 289-290: the sigmoid function and the element-wise
multiplication are not defined for the formulas in section 3.1.
- page 4, first column: many elements of the formulas are not defined: b
(equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation
15).
- page 4, line 326: the readout state rt is not depicted in Figure 2
(workflow).
- Table 2: what does "(ref)" mean?
- Section 4.3, model parameters and training. Explain how you achieved the
values to the many parameters: word embedding size, GRU hidden states, alpha,
beta 1 and 2, epsilon, beam size.
- Page 5, line 450: remove "the" word in this line? "SGD as our optimizing
algorithms" instead of "SGD as our the optimizing algorithms."
- Page 5, beam search: please include a reference for beam search.
- Figure 4: Is there a typo in the true sentence? "council of europe again
slams french prison conditions" (again or against?)
- typo "supper script" -> "superscript" (4 times)