- Strengths:
- nice, clear application of linguistics ideas to distributional semantics
- demonstrate very clear improvements on both intrinsic and extrinsic eval
- Weaknesses:
- fairly straightforward extension of existing retrofitting work
- would be nice to see some additional baselines (e.g. character embeddings)
- General Discussion:
The paper describes "morph-fitting", a type of retrofitting for vector spaces
that focuses specifically on incorporating morphological constraints into the
vector space. The framework is based on the idea of "attract" and "repel"
constraints, where attract constraints are used to pull morphological
variations close together (e.g. look/looking) and repel constraints are used to
push derivational antonyms apart (e.g. responsible/irresponsible). They test
their algorithm on multiple different vector spaces and several language, and
show consistent improvements on intrinsic evaluation (SimLex-999, and
SimVerb-3500). They also test on the extrinsic task of dialogue state tracking,
and again demonstrate measurable improvements over using
morphologically-unaware word embeddings.
I think this is a very nice paper. It is a simple and clean way to incorporate
linguistic knowledge into distributional models of semantics, and the empirical
results are very convincing. I have some questions/comments below, but nothing
that I feel should prevent it from being published.
- Comments for Authors
1) I don't really understand the need for the morph-simlex evaluation set. It
seems a bit suspect to create a dataset using the same algorithm that you
ultimately aim to evaluate. It seems to me a no-brainer that your model will do
well on a dataset that was constructed by making the same assumptions the model
makes. I don't think you need to include this dataset at all, since it is a
potentially erroneous evaluation that can cause confusion, and your results are
convincing enough on the standard datasets.
2) I really liked the morph-fix baseline, thank you for including that. I would
have liked to see a baseline based on character embeddings, since this seems to
be the most fashionable way, currently, to side-step dealing with morphological
variation. You mentioned it in the related work, but it would be better to
actually compare against it empirically.
3) Ideally, we would have a vector space where morphological variants are just
close together, but where we can assign specific semantics to the different
inflections. Do you have any evidence that the geometry of the space you end
with is meaningful. E.g. does "looking" - "look" + "walk" = "walking"? It would
be nice to have some analysis that suggests the morphfitting results in a more
meaningful space, not just better embeddings.