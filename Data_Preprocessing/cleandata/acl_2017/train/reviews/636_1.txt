This work proposes to apply dilated convolutions for sequence tagging
(specifically, named entity recognition). It also introduces some novel ideas
(sharing the dilated convolution block, predicting the tags at each convolution
level), which I think will prove useful to the community. The paper performs
extensive ablation experiments to show the effectiveness of their approach.
I found the writing to be very clear, and the experiments were exceptionally
thorough.
Strengths:  
- Extensive experiments against various architectures (LSTM, LSTM + CRF)       
- Novel architectural/training ideas (sharing blocks)  
Weaknesses:  
- Only applied to English NER--this is a big concern since the title of the
paper seems to reference sequence-tagging directly.  
- Section 4.1 could be clearer. For example, I presume there is padding to make
sure the output resolution after each block is the same as the input
resolution.  Might be good to mention this.  
- I think an ablation study of number of layers vs perf might be interesting.
RESPONSE TO AUTHOR REBUTTAL:
Thank you very much for a thoughtful response. Given that the authors have
agreed to make the content be more specific to NER as opposed to
sequence-tagging, I have revised my score upward.