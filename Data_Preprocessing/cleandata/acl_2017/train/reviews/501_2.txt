- Strengths:
Authors generate a dataset of "rephrased" captions and are planning to make
this dataset publicly available.
The way authors approached DMC task has an advantage over VQA or caption
generation in terms of metrics. It is easier and more straightforward to
evaluate problem of choosing the best caption. Authors use accuracy metric.
While for instance caption generation requires metrics like BLUE or Meteor
which are limited in handling semantic similarity.
Authors propose an interesting approach to "rephrasing", e.g. selecting
decoys. They draw decoys form image-caption dataset. E.g. decoys for a single
image come from captions for other images. These decoys however are similar to
each other both in terms of surface (bleu score) and semantics (PV similarity).
Authors use lambda factor to decide on the balance between these two components
of the similarity score. I think it would be interesting to employ these for
paraphrasing.
Authors support their motivation for the task with evaluation results. They
show that a system trained with the focus on differentiating between similar
captions performs better than a system that is trained to generate captions
only. These are, however, showing that system that is tuned for a particular
task performs better on this task.
- Weaknesses:
 It is not clear why image caption task is not suitable for comprehension task
and why author's system is better for this. In order to argue that system can
comprehend image and sentence semantics better one should apply learned
representation, e.g. embeddings. E.g. apply representations learned by
different systems on the same task for comparison.
My main worry about the paper is that essentially authors converge to using
existing caption generation techniques, e.g. Bahdanau et al., Chen et al.
They way formula (4) is presented is a bit confusing. From formula it seems
that both decoy and true captions are employed for both loss terms. However, as
it makes sense, authors mention that they do not use decoy for the second term.
That would hurt mode performance as model would learn to generate decoys as
well. The way it is written in the text is ambiguous, so I would make it more
clear either in the formula itself or in the text. Otherwise it makes sense for
the model to learn to generate only true captions while learning to distinguish
between true caption and a decoy.
- General Discussion:
Authors formulate a task of Dual Machine Comprehension. They aim to accomplish
the task by challenging computer system to solve a problem of choosing between
two very similar captions for a given image. Authors argue that a system that
is able to solve this problem has to "understand" the image and captions
beyond just keywords but also capture semantics of captions and their alignment
with image semantics.
I think paper need to make more focus on why chosen approach is better than
just caption generation and why in their opinion caption generation is less
challenging for learning image and text representation and their alignment.
For formula (4). I wonder if in the future it is possible to make model to
learn "not to generate" decoys by adjusting second loss term to include
decoys but with a negative sign. Did authors try something similar?