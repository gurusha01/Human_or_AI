- Strengths:
a) The paper presents a Bayesian learning approach for recurrent neural network
language model. The method outperforms standard SGD with dropout on three
tasks. 
b) The idea of using Bayesian learning with RNNs appears to be novel. 
c) The computationally efficient Bayesian algorithm for RNN would be of
interest to the NLP community for various applications.
- Weaknesses:
Primary concern is about evaluation:
Sec 5.1: The paper reports the performance of difference types of architectures
(LSTM/GRU/vanilla RNN) on character LM task while comparing the learning
algorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are
compared for the character LM while SGD +/- dropout is compared with SGLD +/-
dropout on word language model task. This is inconsistent!  I would suggest
reporting both these dimensions (i.e. architectures and the exact same learning
algorithms) on both character and word LM tasks. It would be useful to know if
the results from the proposed Bayesian learning approaches are portable across
both these tasks and data sets.
L529: The paper states that 'the performance gain mainly comes from adding
gradient noise and model averaging'. This statement is not justified
empirically. To arrive at this conclusion, an A/B experiment with/without
adding gradient noise and/or model averaging needs to be done. 
L724: Gal's dropout is run on the sentence classification task but not on
language model/captions task. Since Gal's dropout is not specific to sentence
classification,  I would suggest reporting the performance of this method on
all three tasks. This would allow the readers to fully assess the utility of
the proposed algorithms relative to all existing dropout approaches.
L544: Is there any sort order for the samples? (\theta1, ..., \thetaK)? e.g.
are samples with higher posterior probabilities likely to be at higher indices?
Why not report the result of randomly selecting K out of S samples, as an
additional alternative?
Regular RNN LMs are known to be expensive to train and evaluate. It would be
very useful to compare the training/evaluation times for the proposed Bayesian
learning algorithms with SGD+ dropout. That would allow the readers to
trade-off improvements versus increase in training/run times.
Clarifications:
L346: What does \theta_s refer to? Is this a MAP estimate of parameters based
on only the sample s?
L453-454: Clarify what \theta means in the context of dropout/dropconnect. 
Typos:
L211: output
L738: RMSProp