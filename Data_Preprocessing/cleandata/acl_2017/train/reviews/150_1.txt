- Strengths:
The authors present a novel adaptation of encoder-decoder neural MT using an
approach that starts and ends with characters, but in between works with
representations of morphemes and characters. 
The authors release both their code as well as their final learned models for
fr-en, cs-en, and en-cs. This is helpful in validating their work, as well as
for others looking to replicate and extends this work.
The system reported appears to produce translation results of reasonable
quality even after the first training epoch, with continued progress in future
epochs.
The system appears to learn reasonable morphological tokenizations, and appears
able to handle previously unseen words (even nonce words) by implicitly backing
off to morphemes.
- Weaknesses:
In the paper, the authors do not explicitly state which WMT test and dev sets
their results are reported on. This is problematic for readers wishing to
compare the reported results to existing work (for example, the results at
matrix.statmt.org). The only way this reviewer found to get this information
was to look in the README of the code supplement, which indicates that the test
set was newstest2015 and the dev test was newstest2013. This should have been
explicitly described in the paper.
The instructions given in the software README are OK, but not great. The
training and testing sections each could be enhanced with explicit examples of
how to run the respective commands. The software itself should respond to a
--help flag, which it currently does not.
The paper describes a 6-level architecture, but the diagram in Figure 2 appears
to show fewer than 6 layers. What's going on? The caption should be more
explicit, and if this figure is not showing all of the layers, then there
should be a figure somewhere (even if it's in an appendix) showing all of the
layers.
The results show comparison to other character-based neural systems, but do not
show state-of-the-art results for other types of MT system. WMT (and
matrix.statmt.org) has reported results for other systems on these datasets,
and it appears that the state-of-the-art is much higher than any of the results
reported in this paper. That should be acknowledged, and ideally should be
discussed.
There are a handful of minor English disfluencies, misspellings, and minor
LaTeX issues, such as reverse quotation marks. These should be corrected.
- General Discussion:
Paper is a nice contribution to the existing literature on character-based
neural MT.