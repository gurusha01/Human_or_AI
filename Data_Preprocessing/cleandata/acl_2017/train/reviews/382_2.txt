- Strengths:
* Potentially valuable resource
* Paper makes some good points
- Weaknesses:
* Awareness of related work (see below)
* Is what the authors are trying to do (domain-independent microplanning) even
possible (see below)
* Are the crowdsourced texts appropriate (see below)
- General Discussion:
This is an interesting paper which presents a potentially valuable resource,
and I in many ways I am sympathetic to it.  However, I have some high-level
concerns, which are not addressed in the paper.  Perhaps the authors can
address these in their response.
(1) I was a bit surprised by the constant reference and comparison to Wen 2016,
which is a fairly obscure paper I have not previously heard of.  It would be
better if the authors justified their work by comparison to well-known corpora,
such as the ones they list in Section 2. Also, there are many other NLG
projects that looked at microplanning issue when verbalising DBPedia, indeed
there was a workshop in 2016 with many papers on NLG and DBPedia
(https://webnlg2016.sciencesconf.org/  and
http://aclweb.org/anthology/W/W16/3500); see also previous work by Duboue and
Kutlak.  I would like to see less of a fixation on Wen (2016), and more
awareness of other work on NLG and DBPedia.
(2) Microplanning tends to be very domain/genre dependent.  For example,
pronouns are used much more often in novels than in aircraft maintenance
manuals.   This is why so much work has focused on domain-dependent resources. 
  So there are some real questions about whether it is possible even in theory
to train a "wide-coverage microplanner".  The authors do not discuss this at
all; they need to show they are aware of this concern.
(3) I would be concerned about the quality of the texts obtained from
crowdsourcing.              A lot of people dont write very well, so it is not at all
clear
to me that gathering example texts from random crowdsourcers is going to
produce a good corpus for training microplanners.  Remember that the ultimate
goal of microplanning is to produce texts that are easy to read.  Imitating
human writers (which is what this paper does, along with most learning
approaches to microplanning) makes sense if we are confident that the human
writers have produced well-written easy-to-read texts.              Which is a
reasonable
assumption if the writers are professional journalists (for example), but a
very dubious one if the writers are random crowdsourcers.
From a presentational perspective, the authors should ensure that all text in
their paper meets the ACL font size criteria.  Some of the text in Fig 1 and
(especially) Fig 2 is tiny and very difficult to read; this text should be the
same font size as the text in the body of the paper.
I will initially rate this paper as borderline.  I look forward to seeing the
author's response, and will adjust my rating accordingly.