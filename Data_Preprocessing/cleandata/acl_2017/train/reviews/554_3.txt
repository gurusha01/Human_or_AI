- Strengths: This paper explores a relatively under-explored area of practical
application of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian
treatment of the parameters of RNNs, it is possible to incorporate benefits of
model averaging during inference. Further, their gradient
based sampling approximation to the posterior estimation leads to a procedure
which is easy to implement and is potentially much cheaper than other
well-known techniques for model averaging like ensembling.  
The effectiveness of this approach is shown on three different tasks --
language modeling, image captioning and sentence classification; and
performance gains are observed over the baseline of single model optimization.
- Weaknesses: Exact experimental setup is unclear. The supplementary material
contains important details about burn-in, number of epochs and samples
collected that should be in the main paper itself. Moreover, details on how the
inference is performed would be helpful. Were the samples that were taken
following HMC for a certain number of epochs after burn in on the training data
fixed for inference (for every \tilda{Y} during test time, same samples were
used according to eqn 5) ? Also, an explicit clarification regarding an
independence assumption that p(D|\theta) = p(Y,X| \theta) = p(Y| \theta,X)p(X),
which lets one use the conditional RNN model (if I understand correctly) for
the potential U(\theta) would be nice for completeness.
In terms of comparison, this paper would also greatly benefit from a
discussion/ experimental comparison with ensembling and distillation methods
("Sequence level knowledge distillation"; Kim and Rush, "Distilling an Ensemble
of Greedy Dependency Parsers into One MST Parser"; Kuncoro et al.) which  are
intimately related by a similar goal of incorporating effects of model
averaging.
Further discussion related to preference of HMC related sampling
methods over other sampling methods or variational approximation would be
helpful.
Finally, equation 8 hints at the potential equivalence between dropout and the
proposed approach and the theoretical justification behind combining SGLD and
dropout (by making the equivalence more concrete) would lead to a better
insight into the effectiveness of the proposed approach.  
- General Discussion: Points addressed above.