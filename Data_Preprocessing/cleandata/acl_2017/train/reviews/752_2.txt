The authors use self-training to train a seq2seq-based AMR parser using a small
annotated corpus and large amounts of unlabeled data. They then train a
similar,
seq2seq-based AMR-to-text generator using the annotated corpus and automatic
AMRs produced by their parser from the unlabeled data. They use careful
delexicalization for named entities in both tasks to avoid data sparsity. This
is the first sucessful application of seq2seq models to AMR parsing and
generation, and for generation, it most probably improves upon state-of-the
art.
In general, I really liked the approach as well as the experiments and the
final performance analysis.
The methods used are not revolutionary, but they are cleverly combined to
achieve practial results.
The description of the approach is quite detailed, and I believe that it is
possible to reproduce the experiments without significant problems.
The approach still requires some handcrafting, but I believe that this can be
overcome in the future and that the authors are taking a good direction.
(RESOLVED BY AUTHORS' RESPONSE) However, I have been made aware by another
reviewer of a data overlap in the
Gigaword and the Semeval 2016 dataset. This is potentially a very serious
problem -- if there is a significant overlap in the test set, this would
invalidate the results for generation (which are the main achievemnt of the
paper). Unless the authors made sure that no test set sentences made their way
to training through Gigaword, I cannot accept their results.
(RESOLVED BY AUTHORS' RESPONSE)  Another question raised by another reviewer,
which I fully agree with, is the 
5.4 point claim when comparing to a system tested on an earlier version of the
AMR dataset. The paper could probably still claim improvement over state-of-the
art, but I am not sure I can accept the 5.4 points claim in a direct comparison
to Pourdamghani et al. -- why haven't the authors also tested their system on
the older dataset version (or obtained Pourdamghani et al.'s scores for the
newer version)?
Otherwise I just have two minor comments to experiments: 
- Statistical significance tests would be advisable (even if the performance
difference is very big for generation).
- The linearization order experiment should be repeated with several times with
different random seeds to overcome the bias of the particular random order
chosen.
The form of the paper definitely could be improved.
The paper is very dense at some points and proofreading by an independent
person (preferably an English native speaker) would be advisable. 
The model (especially the improvements over Luong et al., 2015) could be
explained in more detail; consider adding a figure. The experiment description
is missing the vocabulary size used.
Most importantly, I missed a formal conclusion very much -- the paper ends
abruptly after qualitative results are described, and it doesn't give a final
overview of the work or future work notes.
Minor factual notes:
- Make it clear that you use the JAMR aligner, not the whole parser (at
361-364). Also, do you not use the recorded mappings also when testing the
parser (366-367)?
- Your non-Gigaword model only improves on other seq2seq models by 3.5 F1
points, not 5.4 (at 578).
- "voters" in Figure 1 should be "person :ARG0-of vote-01" in AMR.
Minor writing notes:
- Try rewording and simplifying text near 131-133, 188-190, 280-289, 382-385,
650-659, 683, 694-695.
- Inter-sentitial punctuation is sometimes confusing and does not correspond to
my experience with English syntax. There are lots of excessive as well as
missing commas.
- There are a few typos (e.g., 375, 615), some footnotes are missing full
stops.
- The linearization description is redundant at 429-433 and could just refer to
Sect. 3.3.
- When refering to the algorithm or figures (e.g., near 529, 538, 621-623),
enclose the references in brackets rather than commas.
- I think it would be nice to provide a reference for AMR itself and for the
multi-BLEU script.
- Also mention that you remove AMR variables in Footnote 3.
- Consider renaming Sect. 7 to "Linearization Evaluation".
- The order in Tables 1 and 2 seems a bit confusing to me, especially when your
systems are not explicitly marked (I would expect your systems at the bottom).
Also, Table 1 apparently lists development set scores even though its
description says otherwise.
- The labels in Table 3 are a bit confusing (when you read the table before
reading the text).
- In Figure 2, it's not entirely visible that you distinguish month names from
month numbers, as you state at 376.
- Bibliography lacks proper capitalization in paper titles, abbreviations and
proper names should be capitalized (use curly braces to prevent BibTeX from
lowercasing everything).
- The "Peng and Xue, 2017" citation is listed improperly, there are actually
four authors.
*
Summary:
The paper presents first competitive results for neural AMR parsing and
probably new state-of-the-art for AMR generation, using seq2seq models with
clever
preprocessing and exploiting large a unlabelled corpus. Even though revisions
to the text are advisable, I liked the paper and would like to see it at the
conference. 
(RESOLVED BY AUTHORS' RESPONSE) However, I am not sure if the comparison with
previous
state-of-the-art on generation is entirely sound, and most importantly, whether
the good results are not actually caused by data overlap of Gigaword
(additional training set) with the test set.
*
Comments after the authors' response:
I thank the authors for addressing both of the major problems I had with the
paper. I am happy with their explanation, and I raised my scores assuming that
the authors will reflect our discussion in the final paper.