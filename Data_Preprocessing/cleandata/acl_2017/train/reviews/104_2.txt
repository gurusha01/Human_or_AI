This paper addresses the problem of disambiguating/linking textual entity
mentions into a given background knowledge base (in this case, English
Wikipedia).  (Its title and introduction are a little overblown/misleading,
since there is a lot more to bridging text and knowledge than the EDL task, but
EDL is a core part of the overall task nonetheless.)  The method is to perform
this bridging via an intermediate layer of representation, namely mention
senses, thus following two steps: (1) mention to mention sense, and (2) mention
sense to entity.  Various embedding representations are learned for the words,
the mention senses, and the entities, which are then jointly trained to
maximize a single overall objective function that maximizes all three types of
embedding equally.  
Technically the approach is fairly clear and conforms to the current deep
processing fashion and known best practices regarding embeddings; while one can
suggest all kinds of alternatives, it's not clear they would make a material
difference.  Rather, my comments focus on the basic approach.  It is not
explained, however, exactly why a two-step process, involving the mention
senses, is better than a simple direct one-step mapping from word mentions to
their entities.  (This is the approach of Yamada et al., in what is called here
the ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its
simplification SPME) do better.  By why, exactly?  What is the exact
difference, and additional information, that the mention senses have compare4ed
to the entities?  To understand, please check if the following is correct (and
perhaps update the paper to make it exactly clear what is going on).  
For entities: their profiles consist of neighboring entities in a relatedness
graph.                    This graph is built (I assume) by looking at word-level
relatedness of
the entity definitions (pages in Wikipedia).  The profiles are (extended
skip-gram-based) embeddings.  
For words: their profiles are the standard distributional semantics approach,
without sense disambiguation.  
For mention senses: their profiles are the standard distributional semantics
approach, but WITH sense disambiguation.  Sense disambiguation is performed
using a sense-based profile ('language model') from local context words and
neighboring mentions, as mentioned briefly just before Section 4, but without
details.  This is a problem point in the approach.  How exactly are the senses
created and differentiated?  Who defines how many senses a mention string can
have?  If this is done by looking at the knowledge base, then we get a
bijective mapping between mention senses and entities -– that is, there is
exactly one entity for each mention sense (even if there may be more entities).
 In that case, are the sense collection's definitional profiles built
starting with entity text as 'seed words'?                    If so, what
information
is used
at the mention sense level that is NOT used at the entity level?  Just and
exactly the words in the texts that reliably associate with the mention sense,
but that do NOT occur in the equivalent entity webpage in Wikipedia?  How many
such words are there, on average, for a mention sense?                    That is,
how
powerful/necessary is it to keep this extra differentiation information in a
separate space (the mention sense space) as opposed to just loading these
additional words into the Entity space (by adding these words into the
Wikipedia entity pages)?  
If the above understanding is essentially correct, please update Section 5 of
the paper to say so, for (to me) it is the main new information in the paper.  
It is not true, as the paper says in Section 6, that "…this is the first
work to deal with mention ambiguity in the integration of text and knowledge
representations, so there is no exact baselines for comparison".  The TAC KBP
evaluations for the past two years have hosted EDL tasks, involving eight or
nine systems, all performing exactly this task, albeit against Freebase, which
is considerably larger and more noisy than Wikipedia.  Please see
http://nlp.cs.rpi.edu/kbp/2016/ .  
On a positive note: I really liked the idea of the smoothing parameter in
Section 6.4.2.
Post-response: I have read the authors' responses.  I am not really satisfied
with their reply about the KBP evaluation not being relevant, but that they are
interested in the goodness of the embeddings instead.  In fact, the only way to
evaluate such 'goodness' is through an application.  No-one really cares how
conceptually elegant an embedding is, the question is: does it perform better?