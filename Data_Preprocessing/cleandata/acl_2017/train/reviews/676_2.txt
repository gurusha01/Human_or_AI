- Strengths:
  This paper is well written, and with clear, well-designed
  figures. The reader can easily understand the methodology even only
  with those figures.
  Predicting the binary code directly is a clever way to reduce the
  parameter space, and the error-correction code just works
  surprisingly well. I am really surprised by how 44 bits can achieve
  26 out of 31 BLEU.  
  The parameter reducing technique described in this work is
  orthogonal to current existing methods: weight pruning and
  sequence-level knowledge distilling.
  The method here is not restricted by Neural Machine Translation, and
  can be used in other tasks as long as there is a big output
  vocabulary.  
- Weaknesses:
  The most annoying point to me is that in the relatively large
  dataset (ASPEC), the best proposed model is still 1 BLEU point lower
  than the softmax model. What about some even larger dataset, like
  the French-English? There are at most 12 million sentences
  there. Will the gap be even larger?
  Similarly, what's the performance on some other language pairs ?
  Maybe you should mention this paper,
  https://arxiv.org/abs/1610.00072. It speeds up the decoding speed by
  10x and the BLEU loss is less than 0.5.  
- General Discussion:
The paper describes a parameter reducing method for large vocabulary
softmax. By applying the error-corrected code and hybrid with softmax,
its BLEU approaches that of the orignal full vocab softmax model.
One quick question: what is the hidden dimension size of the models?
I couldn't find this in the experiment setup.
The 44 bits can achieve 26 out of 31 BLEU on E2J, that was
surprisingly good. However, how could you increase the number of bits
to increase the classification power ? 44 is too small, there's plenty
of room to use more bits and the computation time on GPU won't even
change.
Another thing that is counter-intuitive is that by predicting the
binary code, the model is actually predicting the rank of the
words. So how should we interpret these bit-embeddings ? There seems
no semantic relations of all the words that have odd rank. Is it
because the model is so powerful that it just remembers the data ?