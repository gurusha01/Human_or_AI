This paper divides the keyphrases into two types: (1) Absent key phrases (such
phrases do not match any contiguous subsequences of the source document) and
(2) Present key phrases (such key phrases fully match a part of the text). The
authors used RNN based generative models (discussed as RNN and Copy RNN) for
keyphrase prediction and copy mechanism in RNN to predict the already occurred
phrases. 
Strengths:
1. The formation and extraction of key phrases, which are absent in the current
document is an interesting idea of significant research interests. 
2. The paper is easily understandable.
3. The use of RNN and Copy RNN in the current context is a new idea. As, deep
recurrent neural networks are already used in keyphrase extraction (shows very
good performance also), so, it will be interesting to have a proper motivation
to justify the use of  RNN and Copy RNN over deep recurrent neural networks. 
Weaknesses:
1. Some discussions are required on the convergence of the proposed joint
learning process (for RNN and CopyRNN), so that readers can understand, how the
stable points in probabilistic metric space are obtained? Otherwise, it may be
tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1.
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines
(which contains only "present" type of keyphrases). Here there is no direct
comparison of the performance of the current system w.r.t. other
state-of-the-arts/benchmark systems on only "present" type of key phrases. It
is important to note that local phrases (keyphrases) are also important for the
document. The experiment does not discuss it explicitly. It will be interesting
to see the impact of the RNN and Copy RNN based model on automatic extraction
of local or "present" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important
point. It is found that the published results of [1], (see reference below)
performs better than (with a sufficiently high difference) the current system
on Inspec (Hulth, 2003) abstracts dataset. 
4. It is reported that current system uses 527,830 documents for training,
while 40,000 publications are held out for training baselines. Why are all
publications not used in training the baselines? Additionally,        The topical
details of the dataset (527,830 scientific documents) used in training RNN and
Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So,
it would be better to compare this system, which also captures semantics. Even,
Ref-[2] can be a strong baseline to compare the performance of the current
system.
Suggestions to improve:
1. As, per the example, given in the Figure-1, it seems that all the "absent"
type of key phrases are actually "Topical phrases". For example: "video
search", "video retrieval", "video indexing" and "relevance ranking", etc.
These all define the domain/sub-domain/topics of the document. So, In this
case, it will be interesting to see the results (or will be helpful in
evaluating "absent type" keyphrases): if we identify all the topical phrases of
the entire corpus by using tf-idf and relate the document to the high-ranked
extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As
similar efforts are already applied in several query expansion techniques (with
the aim to relate the document with the query, if matching terms are absent in
document).
Reference:
1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to
find exemplar terms for keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Language Processing, pages
257â€“266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction
using deep recurrent neural networks on Twitter. In Proceedings of the 2016
Conference on Empirical Methods in Natural Language Processing (pp. 836-845).