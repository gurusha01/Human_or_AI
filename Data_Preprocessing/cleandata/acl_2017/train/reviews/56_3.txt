This paper modifies existing word embedding algorithms (GloVe, Skip Gram, PPMI,
SVD) to include ngram-ngram cooccurance statistics. To deal with the large
computational costs of storing such expensive matrices, the authors propose an
algorithm that uses two different strategies to collect counts.  
- Strengths:
* The proposed work seems like a natural extension of existing work on learning
word embeddings. By integrating bigram information, one can expect to capture
richer syntactic and semantic information.
- Weaknesses:
* While the authors propose learning embeddings for bigrams (bi_bi case), they
actually do not evaluate the embeddings for the learned bigrams except for the
qualitative evaluation in Table 7. A more quantitative evaluation on
paraphrasing or other related tasks that can include bigram representations
could have been a good contribution.
* The evaluation and the results are not very convincing - the results do not
show consistent trends, and some of the improvements are not necessarily
statistically significant.
* The paper reads clunkily due to significant grammar and spelling errors,
and needs a major editing pass.
- General Discussion:
This paper is an extension of standard embedding learning techniques to include
information from bigram-bigram coocurance. While the work is interesting and a
natural extension of existing work, the evaluation and methods leaves some open
questions. Apart from the ones mentioned in the weaknesses, some minor
questions for the authors :
* Why is there significant difference between the overlap and non-overlap
cases? I would be more interested in finding out more than the quantitative
difference shown on the tasks.
I have read the author response. I look forward to seeing the revised version
of the paper.