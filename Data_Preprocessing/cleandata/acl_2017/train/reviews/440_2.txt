This paper describes a state-of-the-art CCG parsing model that decomposes into
tagging and dependency scores, and has an efficient A* decoding algorithm.
Interestingly, the paper slightly outperforms Lee et al. (2016)'s more
expressive global parsing model, presumably because this factorization makes
learning easier. It's great that they also report results on another language,
showing large improvements over existing work on Japanese CCG parsing. One
surprising original result is that modeling the first word of a constituent as
the head substantially outperforms linguistically motivated head rules. 
Overall this is a good paper that makes a nice contribution. I only have a few
suggestions:
- I liked the way that the dependency and supertagging models interact, but it
would be good to include baseline results for simpler variations (e.g. not
conditioning the tag on the head dependency).
- The paper achieves new state-of-the-art results on Japanese by a large
margin. However, there has been a lot less work on this data - would it also be
possible to train the Lee et al. parser on this data for comparison?
- Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging
models for CCG and SRL, and may be worth citing.