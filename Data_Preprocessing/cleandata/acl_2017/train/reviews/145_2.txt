This work uses Gaussian mixtures to represent words and demonstrates its
potential in capturing multiple word meanings for polysemy. The training
process is done based on a max-margin objective. The expected likelihood kernel
is used as the similarity between two words' distributions. Experiment results
on word similarity and entailment tasks show the effectiveness of the proposed
work.
- Strengths:
The problem is clearly motivated and defined. Gaussian mixtures are much more
expressive than deterministic vector representations. It can potentially
capture different word meanings by its modes, along with probability mass and
uncertainty around those modes. This work represents an important contribution
to word embedding. 
This work propose a max-margin learning objective with closed-form similarity
measurement for efficient training.
This paper is mostly well written. 
- Weaknesses:
See below for some questions. 
- General Discussion:
In the Gaussian mixture models, the number of gaussian components (k) is
usually an important parameter. In the experiments of this paper, k is set to
2. What is your criteria to select k? Does the increase of k hurt the
performance of this model? What does the learned distribution look like for a
word that only has one popular meaning?
I notice that you use the spherical case in all the experiments (the covariance
matrix reduces to a single number). Is this purely for computation efficiency?
I wonder what's the performance of using a general diagonal covariance matrix.
Since in this more general case, the gaussian mixture defines different degrees
of uncertainty along different directions in the semantic space, which seems
more interesting.
Minor comments:
Table 4 is not referred to in the text.
In reference, Luong et al. lacks the publication year.
I have read the response.