This paper presents a gated attention mechanism for machine reading. 
A key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop
reasoning by fine-grained gated filter. 
It's interesting and intuitive for machine reading. 
I like the idea along with significant improvement on benchmark datasets, but
also have major concerns to get it published in ACL.
- The proposed GA mechanism looks promising, but not enough to convince the
importance of this technique over other state-of-the-art systems, because
engineering tricks presented 3.1.4 boost a lot on accuracy and are blended in
the result.
- Incomplete bibliography: Nearly all published work in reference section
refers arxiv preprint version. 
This makes me (and future readers) suspicious if this work thoroughly compares
with prior work. Please make them complete if the published version is
available. 
- Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned
as previous work that is unpublished preprint. 
I don't think this is necessary at all. Alternately, I would like the author to
replace it with vanilla GA (or variant of the proposed model for baseline). 
It doesn't make sense that result from the preprint which will end up being the
same as this ACL submission is presented in the same manuscript. 
For fair blind-review, I didn't search on arvix archive though.
- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2,
and GA (fix L(w)) is for K=3 in table 2. 
Does this mean that GA-- is actually AS Reader? 
It's not clear that GA-- is re-implementation of AS. 
I assumed K=1 (AS) in table 2 uses also GloVe initialization and
token-attention, but it doesn't seem in GA--. 
- I wish the proposed method compared with prior work in related work section
(i.e. what's differ from related work).
- Fig 2 shows benefit of gated attention (which translates multi-hop
architecture), and it's very impressive. It would be great to see any
qualitative example with comparison.