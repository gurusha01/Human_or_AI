The paper presents an application of Pointer Networks, a recurrent neural
network model original used for solving algorithmic tasks, to two subtasks of
Argumentation Mining: determining the types of Argument Components, and finding
the links between them. The model achieves state-of-the-art results.
Strengths:
- Thorough review of prior art in the specific formulation of argument mining
handled in this paper.
- Simple and effective modification of an existing model to make it suitable
for
the task. The model is mostly explained clearly.
- Strong results as compared to prior art in this task.
Weaknesses:
- 071: This formulation of argumentation mining is just one of several proposed
subtask divisions, and this should be mentioned. For example, in [1], claims
are detected and classified before any supporting evidence is detected.
Furthermore, [2] applied neural networks to this task, so it is inaccurate to
say (as is claimed in the abstract of this paper) that this work is the first
NN-based approach to argumentation mining.
- Two things must be improved in the presentation of the model: (1) What is the
pooling method used for embedding features (line 397)? and (2) Equation (7) in
line 472 is not clear enough: is E_i the random variable representing the
type of AC i, or its identity? Both are supposedly modeled (the latter by
feature representation), and need to be defined. Furthermore, it seems like the
LHS of equation (7) should be a conditional probability.
- There are several unclear things about Table 2: first, why are the three
first
baselines evaluated only by macro f1 and the individual f1 scores are missing?
This is not explained in the text. Second, why is only the "PN" model
presented? Is this the same PN as in Table 1, or actually the Joint Model? What
about the other three?
- It is not mentioned which dataset the experiment described in Table 4 was
performed on.
General Discussion:
- 132: There has to be a lengthier introduction to pointer networks, mentioning
recurrent neural networks in general, for the benefit of readers unfamiliar
with "sequence-to-sequence models". Also, the citation of Sutskever et al.
(2014) in line 145 should be at the first mention of the term, and the
difference with respect to recursive neural networks should be explained before
the paragraph starting in line 233 (tree structure etc.).
- 348: The elu activation requires an explanation and citation (still not
enough
well-known).
- 501: "MC", "Cl" and "Pr" should be explained in the label.
- 577: A sentence about how these hyperparameters were obtained would be
appropriate.
- 590: The decision to do early stopping only by link prediction accuracy
should
be explained (i.e. why not average with type accuracy, for example?).
- 594: Inference at test time is briefly explained, but would benefit from more
details.
- 617: Specify what the length of an AC is measured in (words?).
- 644: The referent of "these" in "Neither of these" is unclear.
- 684: "Minimum" should be "Maximum".
- 694: The performance w.r.t. the amount of training data is indeed surprising,
but other models have also achieved almost the same results - this is
especially surprising because NNs usually need more data. It would be good to
say this.
- 745: This could alternatively show that structural cues are less important
for
this task.
- Some minor typos should be corrected (e.g. "which is show", line 161).
[1] Rinott, Ruty, et al. "Show Me Your Evidence-an Automatic Method for Context
Dependent Evidence Detection." EMNLP. 2015.
[2] Laha, Anirban, and Vikas Raykar. "An Empirical Evaluation of various Deep
Learning Architectures for Bi-Sequence Classification Tasks." COLING. 2016.