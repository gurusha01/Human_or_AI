- Strengths:
- technique for creating dataset for evaluation of out-of-coverage items, that
could possibly be used to evaluation other grammars as well. 
- the writing in this paper is engaging, and clear (a pleasant surprise, as
compared to the typical ACL publication.)
- Weaknesses:
- The evaluation datasets used are small and hence results are not very
convincing (particularly wrt to the alchemy45 dataset on which the best results
have been obtained)
- It is disappointing to see only F1 scores and coverage scores, but virtually
no deeper analysis of the results. For instance, a breakdown by type of
error/type of grammatical construction would be interesting. 
- it is still not clear to this reviewer what is the proportion of out of
coverage items due to various factors (running out of resources,  lack of
coverage for "genuine" grammatical constructions in the long tail, lack of
coverage due to extra-grammatical factors like interjections, disfluencies,
lack of lexical coverage, etc. 
- General Discussion:
This paper address the problem of "robustness" or lack of coverage for a
hand-written HPSG grammar (English Resource Grammar). The paper compares
several approaches for increasing coverage, and also presents two creative ways
of obtaining evaluation datasets (a non-trivial issue due to the fact that gold
standard evaluation data is by definition available only for in-coverage
inputs). 
Although hand-written precision grammars have been very much out of fashion
for a long time now and have been superseded by statistical treebank-based
grammars, it is important to continue research on these in my opinion. The
advantages of high precision and deep semantic analysis provided by these
grammars has not been
reproduced by non-handwritten grammars as yet. For this reason, I am giving
this paper a score of 4, despite the shortcomings mentioned above.