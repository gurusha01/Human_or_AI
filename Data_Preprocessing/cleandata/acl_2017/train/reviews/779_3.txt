- Strengths:
This is  a well written paper.
The paper is very clear for the most part.
The experimental comparisons are very well done.
The experiments are well designed and executed.
The idea of using KD for zero-resource NMT is impressive.
- Weaknesses:
There were many sentences in the abstract and in other places in the paper
where the authors stuff too much information into a single sentence. This could
be avoided. One can always use an extra sentence to be more clear.
There could have been a section where the actual method used could be explained
in a more detailed. This explanation is glossed over in the paper. It's
non-trivial to guess the idea from reading the sections alone.
During test time, you need the source-pivot corpus as well. This is a major
disadvantage of this approach. This is played down - in fact it's not mentioned
at all. I could strongly encourage the authors to mention this and comment on
it. 
- General Discussion:
This paper uses knowledge distillation to improve zero-resource translation.
The techniques used in this paper are very similar to the one proposed in Yoon
Kim et. al. The innovative part is that they use it for doing zero-resource
translation. They compare against other prominent works in the field. Their
approach also eliminates the need to do double decoding.
Detailed comments:
- Line 21-27 - the authors could have avoided this complicated structure for
two simple sentences.
Line 41 - Johnson et. al has SOTA on English-French and German-English.
Line 77-79 there is no evidence provided as to why combination of multiple
languages increases complexity. Please retract this statement or provide more
evidence. Evidence in literature seems to suggest the opposite.
Line 416-420 - The two lines here are repeated again. They were first mentioned
in the previous paragraph.
Line 577 - Figure 2 not 3!