The paper presents a new neural approach for summarization. They build on a
standard encoder-decoder with attention framework but add a network that gates
every encoded hidden state based on summary vectors from initial encoding
stages. Overall, the method seems to outperform standard seq2seq methods by 1-2
points on three different evaluation sets.
Overall, the technical sections of the paper are reasonably clear. Equation 16
needs more explanation, I could not understand the notation. The specific
contribution,  the selective mechanism, seems novel and could potentially be
used in other contexts. 
The evaluation is extensive and does demonstrate consistent improvement. One
would imagine that adding an additional encoder layer instead of the selective
layer is the most reasonable baseline (given the GRU baseline uses only one
bi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My
one concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch? 
The quality of the writing, especially in the intro/abstract/related work is
quite bad. This paper does not make a large departure from previous work, and
therefore a related work nearby the introduction seems more appropriate. In
related work, one common good approach is highlighting similarities and
differences between your work and previous work, in words before they are
presented in equations. Simply listing works without relating them to your work
is not that useful. Placement of the related work near the intro will allow you
to relieve the intro of significant background detail and instead focus on more
high level.