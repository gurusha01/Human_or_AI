This paper proposes to use an encoder-decoder framework for keyphrase
generation. Experimental results show that the proposed model outperforms other
baselines if supervised data is available.
- Strengths:
The paper is well-organized and easy to follow (the intuition of the proposed
method is clear). It includes enough details to replicate experiments. Although
the application of an encoder-decoder (+ copy mechanism) is straightforward,
experimental results are reasonable and support the claim (generation of absent
keyphrases) presented in this paper.
- Weaknesses:
As said above, there is little surprise in the proposed approach. Also, as
described in Section 5.3, the trained model does not transfer well to new
domain (it goes below unsupervised models). One of the contribution of this
paper is to maintain training corpora in good quantity and quality, but it is
not (explicitly) stated.
- General Discussion:
I like to read the paper and would be pleased to see it accepted. I would like
to know how the training corpus (size and variation) affects the performance of
the proposed method. Also, it would be beneficial to see the actual values of
pg and pc (along with examples in Figure 1) in the CopyRNN model. From my
experience in running the CopyNet, the copying mechanism sometimes works
unexpectedly (not sure why this happens).