- Strengths:
1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and
Stochastic Optimization in deep learning context. Given dropout/dropConnect and
variational inference are commonly used to reduce the overfit, the more
systematic way to introduce/analyse such bayesian learning based algorithms
would benefit deep learning community.
2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout
outperforms RMSProp + dropout, which clearly shows that uncertainty modeling
would help reducing the over-fitting, hence improving accuracy.
3) The paper has provided the details about the model/experiment setups so the
results should be easily reproduced.
- Weaknesses:
1) The paper does not dig into the theory profs and show the convergence
properties of the proposed algorithm.
2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not
conduct other comparison. It should explain more about the relation between
pSGLD vs RMSProp other than just mentioning they are conterparts in two
families.
2) The paper does not talk about the training speed impact with more details.
- General Discussion: