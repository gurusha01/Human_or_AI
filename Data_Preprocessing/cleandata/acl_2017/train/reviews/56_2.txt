- Strengths: The idea to train word2vec-type models with ngrams (here
specifically: bigrams) instead of words is excellent. The range of experimental
settings (four word2vec-type algorithms, several word/bigram conditions) covers
quite a bit of ground. The qualitative inspection of the bigram embeddings is
interesting and shows the potential of this type of model for multi-word
expressions. 
- Weaknesses: This paper would benefit from a check by a native speaker of
English, especially regarding the use of articles. The description of the
similarity and analogy tasks comes at a strange place in the paper (4.1
Datasets). 
- General Discussion: As is done at some point well into the paper, it could be
clarified from the start that this is simply a generalization of the original
word2vec idea, redefining the word as an ngram (unigram) and then also using
bigrams. It would be good to give a rationale why larger ngrams have not been
used.
(I have read the author response.)