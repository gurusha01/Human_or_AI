This paper presents a comprehensive evaluation methodology for the task of ghostwriting rap lyrics, which captures complementary aspects of this task and its goals. The authors develop a manual evaluation method that assesses several key properties of generated verse, including fluency, coherence, and style matching, and create a dataset of authentic verse, manually annotated for style matching.
The main contributions of this paper are: 
1. A comprehensive manual evaluation methodology for ghostwriting rap lyrics, which assesses fluency, coherence, and style matching.
2. A fully automated evaluation methodology that replicates the results of a previously available semi-automatic method.
3. A dataset of authentic verse, manually annotated for style matching, which can be used as a gold standard for future experiments.
The strengths of this paper are: 
1. The authors provide a clear and well-structured evaluation methodology that captures complementary aspects of the ghostwriting task.
2. The paper presents a comprehensive analysis of the results, including correlations between different metrics and insights into future directions for generative models.
3. The authors make their dataset and annotation interface publicly available, which can facilitate future research in this area.
The weaknesses of this paper are: 
1. The authors' analysis of the results could be more in-depth, particularly with regards to the correlations between different metrics.
2. The paper could benefit from a more detailed discussion of the limitations of the evaluation methodology and potential avenues for future research.
3. Some of the figures and tables are illegible, and the font usage is inconsistent throughout the paper.
Questions to authors: 
1. Can you provide more details on how you plan to address the limitations of the evaluation methodology in future research?
2. How do you think the results of this paper can be generalized to other genres of music or types of text generation tasks?
3. Can you provide more information on how you plan to make the dataset and annotation interface publicly available, and how you envision they will be used by other researchers?