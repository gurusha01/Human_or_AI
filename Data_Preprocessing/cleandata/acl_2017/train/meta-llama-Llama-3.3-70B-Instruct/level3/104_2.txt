This paper proposes a novel approach to semantic parsing, called the Neural Symbolic Machine (NSM), which integrates a sequence-to-sequence neural "programmer" with a symbolic non-differentiable computer. The NSM uses a key-variable memory to support compositionality and a Lisp interpreter with code assistance to provide a friendly neural computer interface. The model is trained using a combination of iterative maximum likelihood and reinforcement learning, which allows it to directly optimize the task reward.
The main contributions of this work are:
1. The introduction of the Manager-Programmer-Computer framework for neural program induction, which integrates neural networks with a symbolic non-differentiable computer.
2. The development of the Neural Symbolic Machine, which integrates a sequence-to-sequence neural "programmer" with a Lisp interpreter and a key-variable memory.
3. The use of iterative maximum likelihood and reinforcement learning to train the model, which allows it to directly optimize the task reward.
The strengths of this paper are:
1. The NSM achieves state-of-the-art results on the WEBQUESTIONSSP dataset with weak supervision, significantly closing the gap between weak and full supervision.
2. The model is trained end-to-end and does not require any feature engineering or domain-specific knowledge.
3. The use of a symbolic non-differentiable computer provides a friendly neural computer interface and allows for abstract, scalable, and precise operations.
The weaknesses of this paper are:
1. The model relies on a large knowledge base and may not generalize well to smaller or different knowledge bases.
2. The training process is complex and requires careful tuning of hyperparameters.
3. The model may not be able to handle complex queries or queries that require multiple steps to answer.
Questions to the authors:
1. How does the model handle out-of-vocabulary entities or properties?
2. Can the model be extended to handle more complex queries or queries that require multiple steps to answer?
3. How does the model compare to other state-of-the-art models on other semantic parsing datasets?