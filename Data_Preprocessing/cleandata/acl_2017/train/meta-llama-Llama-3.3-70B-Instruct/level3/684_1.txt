This paper proposes a novel gated attention mechanism for machine reading, extending the Attention Sum Reader to multi-hop reasoning with a fine-grained gated filter. The Gated-Attention (GA) Reader achieves state-of-the-art results on three benchmark datasets, demonstrating the effectiveness of the proposed mechanism.
The main contributions of this work are:
1. The introduction of a gated attention mechanism that allows the query to directly interact with each dimension of the token embeddings at the semantic level.
2. The design of a multi-hop architecture that enables the model to iteratively refine token representations and attend to distinct salient aspects of the query.
3. The demonstration of the effectiveness of the proposed mechanism through extensive experiments and ablation studies.
The strengths of this paper include:
1. The proposed GA mechanism shows promising results, outperforming competitive baselines on several large-scale benchmark datasets.
2. The ablation study provides statistically significant improvements of using Gated Attention as information filters.
3. The analysis of document and query attentions in intermediate layers of the reader reveals that the model iteratively attends to different aspects of the query to arrive at the final answer.
However, there are some weaknesses and concerns:
1. The proposed GA mechanism may not be convincingly better than other state-of-the-art systems due to the impact of engineering tricks on accuracy.
2. The bibliography is incomplete, with most references citing arxiv preprint versions, which raises suspicions about the thoroughness of comparisons with prior work.
3. The inclusion of results from unpublished work, specifically the GA baseline, is unnecessary and potentially misleading.
4. There is a conflict between tables 1 and 2, with unclear relationships between GA-- and AS Reader, and inconsistent use of GloVe initialization and token-attention.
To improve the paper, the authors should:
1. Provide a more thorough comparison with prior work, including a discussion of the differences and contributions of the proposed method.
2. Replace the GA baseline with a vanilla GA or variant of the proposed model.
3. Clarify the relationships between GA-- and AS Reader, and ensure consistent use of GloVe initialization and token-attention.
4. Provide more qualitative examples and comparisons to further illustrate the effectiveness of the proposed mechanism.
Questions to the authors:
1. Can you provide more details on the implementation of the gated attention mechanism and how it differs from other attention mechanisms?
2. How do you plan to address the concerns about the completeness of the bibliography and the inclusion of unpublished work?
3. Can you provide more insights into the ablation study and the effects of removing different components of the model?