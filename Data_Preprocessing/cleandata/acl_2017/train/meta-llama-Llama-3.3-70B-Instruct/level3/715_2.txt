This paper proposes a novel approach to zero pronoun resolution by automatically generating large-scale pseudo training data and utilizing an attention-based neural network model. The authors claim that their approach significantly outperforms state-of-the-art systems, with an absolute improvement of 3.1% F-score on the OntoNotes 5.0 dataset.
The main contributions of this work are:
1. The proposal of a simple but effective method for generating large-scale pseudo training data for zero pronoun resolution.
2. The introduction of a two-step training approach, which combines the benefits of large-scale pseudo training data and task-specific data.
3. The development of an attention-based neural network model for zero pronoun resolution, which achieves significant improvements over previous systems.
The strengths of this paper include:
1. The authors' approach to generating pseudo training data is novel and effective, allowing for the creation of large-scale training datasets without requiring manual annotation.
2. The two-step training approach is well-motivated and effective, enabling the model to learn from both large-scale pseudo training data and task-specific data.
3. The attention-based neural network model is well-designed and achieves significant improvements over previous systems.
However, there are also some weaknesses:
1. The authors' approach relies heavily on the quality of the pseudo training data, which may not always be accurate or representative of the real-world data.
2. The two-step training approach may require careful tuning of hyperparameters to achieve optimal results.
3. The authors do not provide a detailed analysis of the errors made by their model, which could provide valuable insights into areas for improvement.
Some questions to the authors include:
1. How do the authors plan to address the issue of unknown words, which can significantly impact the performance of the model?
2. Can the authors provide more details on the two-step training approach, including the specific hyperparameters used and the criteria for selecting the best model?
3. How do the authors plan to extend their approach to other languages or domains, where the availability of training data may be limited?