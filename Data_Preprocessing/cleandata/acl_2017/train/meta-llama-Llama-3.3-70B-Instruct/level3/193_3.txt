This paper presents a novel approach to word embeddings, representing each word as a Gaussian mixture model. The authors propose an energy-based max-margin objective to learn the parameters of the mixture model, which captures multiple distinct meanings of words and uncertainty information. The paper's strength lies in its solid work, state-of-the-art transition-based techniques, and machine learning for parsing techniques, as well as its clear and precise writing.
The main contributions of this work are:
1. A Gaussian mixture model for word embeddings, which can capture multiple distinct meanings of words and uncertainty information.
2. An energy-based max-margin objective to learn the parameters of the mixture model.
3. A comparison with existing word embedding methods, showing the superiority of the proposed approach on several benchmark datasets.
The strengths of this paper are:
1. The proposed approach can capture multiple distinct meanings of words, which is a significant improvement over existing word embedding methods.
2. The energy-based max-margin objective is a novel and effective way to learn the parameters of the mixture model.
3. The paper provides a thorough comparison with existing word embedding methods, demonstrating the superiority of the proposed approach.
However, there are some weaknesses and questions that need to be addressed:
1. The originality of the paper lies mainly in the targeted representations, not in the proposed parser itself.
2. The description of UCCA schemes, particularly regarding non-terminal nodes and discontinuous nodes, needs to be clarified.
3. The motivation for certain claims and conversion procedures needs to be provided.
4. The fairness of comparing the proposed parser to other parsers with default settings needs to be questioned.
5. The conversion procedures and remote edges in bilexical approximations need to be clarified.
Some questions to the authors are:
1. Can you provide more details on how the Gaussian mixture model is initialized and trained?
2. How do you handle the case where a word has multiple meanings, but the meanings are not distinct enough to be captured by separate Gaussian components?
3. Can you provide more comparison with other word embedding methods, such as word2vec and GloVe?
4. How do you plan to extend this work to other NLP tasks, such as text classification and machine translation?