This paper proposes a novel evaluation metric for non-task-oriented dialogue systems, which operates on continuous vector space representations and comprises two components to compare context and reference responses. The metric is a remarkable step forward, going beyond pure semantic similarity by learning projection matrices to transform response vectors into context and reference space representations.
The main contributions of this work are:
1. A novel evaluation metric for non-task-oriented dialogue systems that learns projection matrices to transform response vectors into context and reference space representations.
2. The proposed method can work with as little as 25 word pairs, which are straightforward to obtain assuming some basic knowledge of the languages involved.
3. The method can also work with trivially generated seed dictionaries of numerals, making it possible to learn bilingual word embeddings without any real bilingual data.
The strengths of this paper are:
1. The proposed metric is a significant improvement over existing methods, which rely on pure semantic similarity.
2. The method is able to learn high-quality bilingual embeddings from small seed dictionaries, which is a challenging task.
3. The experiments demonstrate the effectiveness of the proposed method on bilingual lexicon induction and cross-lingual word similarity tasks.
The weaknesses of this paper are:
1. The paper lacks discussion on the learned projection matrices and their differences from the original identity initialization after training.
2. The implementation details are unclear, including the use of human scores, dataset splitting, and cross-validation methods.
3. The paper's title and introduction are misleading, as automatically evaluating dialogue response quality is not equivalent to an automatic Turing test.
4. The paper should focus on the correct angle of introducing the problem of non-task-oriented dialogue systems, which is evaluating chatbot responses based on human evaluators' scores of appropriateness.
Questions to authors:
1. Can you provide more details on the learned projection matrices and their differences from the original identity initialization after training?
2. How did you split the dataset and perform cross-validation in the experiments?
3. Can you clarify the relationship between the proposed metric and the automatic Turing test?
4. How do you plan to address the issue of evaluating chatbot responses based on human evaluators' scores of appropriateness in future work?