Summary of the Paper
This paper presents a comprehensive evaluation methodology for the task of ghostwriting rap lyrics, which involves generating lyrics that are similar in style to a target artist but distinct in content. The authors propose a manual evaluation method that assesses fluency, coherence, and style matching, as well as an automated evaluation method that uses rhyme density and uniqueness metrics. The authors also create a dataset of authentic rap lyrics from 13 artists, annotated for style matching, and use this dataset to evaluate the performance of an LSTM-based ghostwriter model.
Main Contributions
1. Comprehensive evaluation methodology: The authors propose a comprehensive evaluation methodology that captures complementary aspects of the ghostwriting task, including fluency, coherence, style matching, and uniqueness.
2. Manual evaluation method: The authors develop a manual evaluation method that assesses fluency, coherence, and style matching, which provides a more nuanced understanding of the generated lyrics.
3. Automated evaluation method: The authors propose an automated evaluation method that uses rhyme density and uniqueness metrics, which provides a more efficient and scalable way to evaluate generated lyrics.
Strengths
1. Comprehensive evaluation methodology: The authors' evaluation methodology provides a thorough understanding of the ghostwriting task and its goals.
2. Manual evaluation method: The manual evaluation method provides a more nuanced understanding of the generated lyrics and can be used to identify areas for improvement.
3. Automated evaluation method: The automated evaluation method provides a more efficient and scalable way to evaluate generated lyrics, which can be useful for large-scale evaluations.
Weaknesses
1. Limited scope: The authors' evaluation methodology is limited to the ghostwriting task and may not be applicable to other language generation tasks.
2. Subjective nature of evaluation: The manual evaluation method is subjective and may be influenced by personal biases and preferences.
3. Lack of correlation between metrics: The authors find that there is a lack of correlation between the different metrics used in the evaluation methodology, which may indicate that the metrics are not capturing the same aspects of the generated lyrics.
Questions to Authors
1. How do the authors plan to address the subjective nature of the manual evaluation method and ensure that the results are reliable and consistent?
2. Can the authors provide more details on how the automated evaluation method is implemented and how it is used to evaluate the generated lyrics?
3. How do the authors plan to extend the evaluation methodology to other language generation tasks and make it more widely applicable?