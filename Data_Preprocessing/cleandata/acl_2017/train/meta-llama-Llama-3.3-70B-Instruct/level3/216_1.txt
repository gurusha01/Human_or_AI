This paper proposes an alternative to Bi-LSTMs for sequence labeling tasks, utilizing iterated dilated convolutional neural networks (ID-CNNs). The key idea is to leverage dilated convolutions to efficiently aggregate broad context without losing resolution, allowing for fast and accurate sequence labeling. 
The main contributions of this work are: 
1. The introduction of ID-CNNs as a novel architecture for sequence labeling, which achieves state-of-the-art results on benchmark datasets while being significantly faster than traditional Bi-LSTM-based approaches.
2. The demonstration of the effectiveness of dilated convolutions in aggregating broad context for sequence labeling tasks, outperforming traditional convolutional neural networks.
3. The proposal of a training objective that compiles some of the reasoning in output space into ID-CNN feature extraction, allowing for more accurate predictions.
The strengths of this paper include:
1. The novelty and effectiveness of the proposed ID-CNN architecture, which achieves impressive speed improvements for sequence labeling tasks.
2. The thorough evaluation of the proposed approach on benchmark datasets, demonstrating its competitiveness with state-of-the-art models.
3. The well-written and well-structured presentation of the paper, making it easy to follow and understand.
The weaknesses of this paper include:
1. The lack of stronger visualization, particularly in Figure 5, which raises doubts about the actual segmenting and assigning results of the document.
2. The potential issue of underfitting due to the model's flexibility, which requires further explanation to better understand the model's limitations.
3. The need for a more detailed analysis of the results on the OntoNotes dataset, where the proposed approach does not perform as well as expected.
Questions to the authors:
1. Can you provide more insight into the choice of hyperparameters for the ID-CNN architecture, and how they were tuned for optimal performance?
2. How do you plan to address the potential issue of underfitting in the ID-CNN model, and what strategies can be employed to mitigate this problem?
3. Can you provide a more detailed analysis of the results on the OntoNotes dataset, and discuss possible reasons for the decreased performance of the Bi-LSTM-CRF model when incorporating document context?