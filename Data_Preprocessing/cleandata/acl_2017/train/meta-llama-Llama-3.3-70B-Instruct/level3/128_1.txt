This paper proposes a neural network architecture that represents structural linguistic knowledge in a memory network for sequence tagging tasks. The main contribution of the paper is a simple way to "flatten" structured information into an array of vectors, connected to the tagger as additional knowledge. However, the proposed substructure encoder is similar to existing work, such as DCNN, and the architecture does not look entirely novel.
The empirical results are not convincing due to the lack of details on baselines, including how they are used to compute sentence and substructure embeddings. The model uses two RNNs, a chain-based one and a knowledge-guided one, which seems unnecessary and increases model capacity. The hyper-parameters and size of the baseline neural networks are not comparable.
The paper's claim that the model generalizes to different knowledge is questionable, as the substructure has to be represented as a sequence of words. The use of the term "knowledge" is misleading, referring to syntax or semantics rather than world or external knowledge. The experiments are not convincing, and the main ideas are not very novel, requiring more takeaways and clarification to better judge the results.
The authors did not address the reviewer's main concerns, including the use of baselines and the separate RNNs, resulting in no change to the scores. The reviewer suggests including a baseline that inputs additional knowledge as features to the RNN and comments on the model's sensitivity to parser errors.
Summary of the paper: The paper proposes a neural network architecture for sequence tagging tasks that incorporates structural linguistic knowledge into a memory network. The main contribution is a simple way to represent structured information as vectors and connect them to the tagger as additional knowledge.
Main contributions:
1. A simple way to "flatten" structured information into an array of vectors.
2. A neural network architecture that incorporates structural linguistic knowledge into a memory network for sequence tagging tasks.
Strengths:
1. The paper proposes a novel way to represent structured information as vectors.
2. The architecture has the potential to improve sequence tagging tasks by incorporating structural linguistic knowledge.
Weaknesses:
1. The proposed substructure encoder is similar to existing work, such as DCNN.
2. The architecture does not look entirely novel.
3. The empirical results are not convincing due to the lack of details on baselines.
4. The model uses two RNNs, which seems unnecessary and increases model capacity.
5. The paper's claim that the model generalizes to different knowledge is questionable.
Questions to authors:
1. Can you provide more details on the baselines used in the experiments?
2. How do you plan to address the concerns about the novelty of the proposed substructure encoder and the architecture?
3. Can you provide more clarification on the use of the term "knowledge" in the paper?