This paper presents a neural model for factoid question answering over a knowledge graph, learning semantic correlations between candidate answer aspects and question words. The model's key contributions are creating separate components to capture different answer aspects and incorporating global context from the knowledge base. The separation of candidate answer representation into distinct aspects allows for more control over guiding the neural model towards beneficial information.
The main contributions of this work are:
1. The proposed Gated-Attention (GA) reader, which integrates a multi-hop architecture with a novel attention mechanism, allowing the query to directly interact with each dimension of the token embeddings at the semantic-level.
2. The use of a character composition model to generate orthographic embeddings of tokens, which helps in dealing with out-of-vocabulary words.
3. The incorporation of a question evidence common word feature, which significantly boosts reading comprehension performance in some cases.
The strengths of this paper are:
1. The GA reader achieves state-of-the-art performance on several large-scale benchmark datasets, with more than 4% improvements over competitive baselines.
2. The model design is backed up by an ablation study, showing statistically significant improvements of using Gated Attention as information filters.
3. The analysis of document and query attentions in intermediate layers of the reader provides insight into the reading process employed by the model.
The weaknesses of this paper are:
1. The paper lacks clarity in some areas, such as the context aspect of candidate answers and the mention of out-of-vocabulary words.
2. The experiments could be improved by including more systems in the comparison, such as SP-based systems, and providing comparable performance numbers.
3. The random initialization of embeddings may impact the end performance, and using pre-trained embeddings could be explored.
Questions to authors:
1. Can you provide more details on how the character composition model is used to generate orthographic embeddings of tokens?
2. How does the question evidence common word feature affect the performance of the model on different datasets?
3. Can you provide more insight into the attention visualization results, and how they relate to the reading process employed by the model?