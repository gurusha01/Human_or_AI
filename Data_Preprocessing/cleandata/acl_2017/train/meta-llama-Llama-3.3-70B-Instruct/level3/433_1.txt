This paper proposes a novel deep-learning-based model for parsing Singaporean English to Universal Dependencies, leveraging a larger English training set and a small Singlish treebank through neural stacking and hidden representations. The approach yields better results than using an English parser or training a parser on a small Singlish dataset. 
The main contributions of this work are: 
1. A novel multigraph-based model for recognizing overlapping entities, which achieves higher F1-scores compared to previous models on multiple datasets.
2. The model's ability to resolve the spurious structures issue associated with a previous state-of-the-art model, while maintaining the same inference time complexity.
3. The provision of an annotated treebank of 1,200 sentences with an inter-annotator agreement of 85.3 UAS and 75.7 LAS.
The strengths of this paper include its good results, solid experimental setup, careful analyses, and a linguistically informed method that exploits similarity between standard English and Singlish. However, a weakness of the paper is the initially perceived poor annotation quality, which was convincingly addressed by the authors in their rebuttal.
Some questions arise regarding the annotation process, inter-annotator agreement, and specific aspects of the treebank, such as discourse relations and particles. Additionally, low-level comments on the paper suggest comparisons to other work, clarifications of terminology, and additions of glosses to figures.
Overall, the paper presents a significant contribution to the field of natural language processing, particularly in the area of parsing and entity recognition. The proposed model and annotated treebank have the potential to improve the accuracy of parsing and entity recognition tasks, especially for languages with limited resources like Singaporean English. 
The paper's strengths outweigh its weaknesses, and the authors have convincingly addressed the concerns regarding annotation quality. Therefore, this paper is a strong candidate for acceptance at the conference. 
Some potential areas for future work include exploring the application of the proposed model to other languages and domains, investigating the use of other machine learning architectures, and examining the impact of the model on downstream NLP tasks. 
In terms of suggestions for improvement, the authors may consider providing more details on the annotation process, inter-annotator agreement, and the treebank's composition. Additionally, they may want to explore the use of other evaluation metrics and compare their model to other state-of-the-art approaches in the field. 
Overall, this paper presents a well-written, well-structured, and well-researched contribution to the field of natural language processing, and its acceptance would be a valuable addition to the conference proceedings.