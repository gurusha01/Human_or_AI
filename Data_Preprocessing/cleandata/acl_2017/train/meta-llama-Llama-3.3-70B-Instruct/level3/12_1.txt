Summary of the Paper
The paper proposes a novel task of concept-map-based multi-document summarization, which aims to create a concept map that represents the most important content of a set of related documents. The authors introduce a new crowdsourcing scheme, called low-context importance annotation, to determine the importance of propositions in a document cluster. They use this scheme to create a new benchmark corpus for concept-map-based MDS, which consists of 30 topics with around 40 documents each. The corpus is characterized by its large cluster size, heterogeneous documents, and varied language use.
Main Contributions
1. Novel task definition: The paper proposes a new task of concept-map-based MDS, which extends traditional summarization tasks by representing summaries as concept maps.
2. Low-context importance annotation: The authors introduce a new crowdsourcing scheme to determine the importance of propositions in a document cluster, which is more efficient and reliable than traditional annotation methods.
3. Benchmark corpus creation: The paper presents a new benchmark corpus for concept-map-based MDS, which is larger and more diverse than existing corpora.
Strengths
1. Well-written and clearly presented: The paper is well-organized and easy to follow, with clear explanations of the proposed task, crowdsourcing scheme, and corpus creation process.
2. Significant contributions: The paper makes significant contributions to the field of natural language processing, including the proposal of a new task, a novel crowdsourcing scheme, and a benchmark corpus.
3. Thorough evaluation: The authors provide a thorough evaluation of their approach, including a pilot study, quality control measures, and an analysis of the created corpus.
Weaknesses
1. Underspecification issues: The paper lacks detailed information about some aspects of the corpus creation process, such as the specific guidelines used for annotating propositions and the criteria for selecting documents.
2. Limited analysis: The paper could benefit from a more in-depth analysis of the created corpus, including an examination of the concept maps and their properties.
3. Baseline method limitations: The baseline method used for evaluation has some limitations, such as the simple approach to concept extraction and relation identification.
Questions to Authors
1. Can you provide more details about the guidelines used for annotating propositions and the criteria for selecting documents?
2. How do you plan to address the underspecification issues and provide more detailed information about the corpus creation process?
3. Can you provide more analysis of the created corpus, including an examination of the concept maps and their properties?