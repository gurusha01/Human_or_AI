This paper proposes a novel approach to sequence labeling tasks, such as named entity recognition (NER), using iterated dilated convolutional neural networks (ID-CNNs). The key contributions of this work are:
1. ID-CNN architecture: The authors introduce a new architecture that uses dilated convolutions to efficiently aggregate broad context without losing resolution. This allows the model to capture long-range dependencies in the input sequence.
2. Iterated dilated convolutions: The authors propose an iterated approach to dilated convolutions, where the same block of dilated convolutions is applied multiple times to the input sequence. This helps to refine the feature representations and improve the accuracy of the model.
3. Training objective: The authors introduce a new training objective that rewards accurate predictions after each application of the block, allowing later blocks to refine initial predictions.
The strengths of this paper are:
1. Speed improvements: The ID-CNN architecture provides significant speed improvements over traditional recurrent neural network (RNN) architectures, making it more suitable for large-scale NLP tasks.
2. Accuracy gains: The ID-CNN model achieves state-of-the-art results on several benchmark datasets, including CoNLL-2003 and OntoNotes 5.0.
3. Modular architecture: The ID-CNN architecture is modular, allowing for easy substitution of other context composition approaches.
However, there are some weaknesses to this paper:
1. Lack of novelty: While the ID-CNN architecture is new, the use of dilated convolutions is not. The authors could have provided more discussion on how their approach differs from existing work.
2. Limited evaluation: The authors only evaluate their model on two benchmark datasets, and do not provide a thorough comparison with other state-of-the-art models.
3. Overemphasis on speed: While speed is an important consideration in NLP, the authors could have provided more discussion on the trade-offs between speed and accuracy.
Some questions to the authors:
1. How does the ID-CNN architecture handle out-of-vocabulary words, and are there any plans to incorporate subword modeling or other techniques to improve handling of rare words?
2. Can the authors provide more details on the hyperparameter tuning process, and how the optimal hyperparameters were selected?
3. Are there any plans to apply the ID-CNN architecture to other NLP tasks, such as machine translation or question answering?