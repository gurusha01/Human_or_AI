This paper presents a comprehensive evaluation methodology for the task of ghostwriting rap lyrics, which is a challenging task due to the creative and stylistic aspects of language generation. The authors propose a manual evaluation method that assesses several key properties of generated verse, including fluency, coherence, and style matching, as well as a fully automated evaluation method that captures the dual aspects of "unique yet similar" in ghostwriting.
The paper's motivation is well described, and the authors provide detailed comparisons with various models across diverse languages, making it a strong aspect of the research. The experimental results suggest that the proposed evaluation methodology is effective in evaluating the performance of a ghostwriting model, and the results provide valuable insights into the strengths and weaknesses of the model.
However, the conclusion is biased by the selected languages, which may not be representative of each category, and the experiments do not cover the claim of the paper completely, highlighting a significant weakness. Additionally, the paper leaves some issues unaddressed, including selection bias of experimental languages, gaps between claims and experiments, and limited evaluation of the proposed method, which need further explanation and clarification.
The main contributions of this paper are:
1. A comprehensive manual evaluation methodology for ghostwriting rap lyrics, which assesses fluency, coherence, and style matching.
2. A fully automated evaluation method that captures the dual aspects of "unique yet similar" in ghostwriting.
3. A dataset of authentic verse, manually annotated for style matching, which can be used as a gold standard for future work on automatic representation of similarity between artists' styles.
The strengths of this paper are:
1. The proposed evaluation methodology is comprehensive and effective in evaluating the performance of a ghostwriting model.
2. The experimental results provide valuable insights into the strengths and weaknesses of the model.
3. The paper highlights the importance of complementary evaluation methods in capturing different aspects of the ghostwriting task.
The weaknesses of this paper are:
1. The conclusion is biased by the selected languages, which may not be representative of each category.
2. The experiments do not cover the claim of the paper completely.
3. The paper leaves some issues unaddressed, including selection bias of experimental languages, gaps between claims and experiments, and limited evaluation of the proposed method.
Questions to authors:
1. How did you select the languages for the experiments, and why did you choose those specific languages?
2. Can you provide more details on the gaps between claims and experiments, and how you plan to address them in future work?
3. How do you plan to extend the evaluation methodology to other languages and genres, and what are the potential challenges and limitations of doing so?