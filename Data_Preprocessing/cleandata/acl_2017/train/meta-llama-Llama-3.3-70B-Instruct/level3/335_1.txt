This paper proposes a novel approach to automated short-answer scoring and support for written tests, specifically designed for the new Japanese national center test for university entrance examinations. The system utilizes a combination of machine learning and human judgment to evaluate the semantic similarity between model answers and actual written answers.
The main contributions of this work are: 
1. The development of a scoring support system that combines machine learning with human judgment to evaluate short-answer responses.
2. The use of random forests to effectively utilize multiple predictors and improve the accuracy of scoring.
3. The implementation of a mechanism to automatically generate scoring screens from plain scoring criterion files, making it easier to handle different test items and subjects.
The strengths of this submission are: 
1. The system's ability to achieve a high concordance rate with human ratings, with differences within one point for 70-90% of the data.
2. The use of a robust machine learning approach, random forests, which can effectively handle multiple predictors and improve the accuracy of scoring.
3. The system's flexibility and adaptability to different test items and subjects, making it a valuable tool for a wide range of applications.
The weaknesses of this submission are: 
1. The limited scope of the evaluation, which only includes eight test items from social studies, and the need for further validation on additional datasets.
2. The reliance on human scores for supervised learning, which can be time-consuming and expensive to obtain, and may not always be available or reliable.
3. The lack of a clear comparison with other existing automated scoring systems, making it difficult to assess the system's performance relative to the state-of-the-art.
Questions to authors: 
1. How do the authors plan to address the issue of obtaining sufficient and well-balanced human score data for supervised learning, particularly for test items with low scores or illogical responses?
2. Can the authors provide more details on the implementation of the system, including the specific algorithms and techniques used for machine learning and semantic similarity evaluation?
3. How do the authors plan to evaluate the system's performance on other subjects, such as Japanese literature, and what modifications or adaptations may be necessary to accommodate different types of test items and responses?