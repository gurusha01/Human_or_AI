This paper provides a systematic investigation of different context types and representations for learning word embeddings. The authors evaluate various models on intrinsic property analysis, sequence labeling tasks, and text classification, and conclude that context representations play a more important role than context types for learning word embeddings.
The main contributions of this paper are:
1. A thorough evaluation of different context types (linear and dependency-based) and context representations (bound and unbound) for learning word embeddings.
2. The introduction of a new toolkit, word2vecPM, which allows for the use of arbitrary contexts and context representations.
3. The provision of a comprehensive set of experimental results, including numerical results, which can be used by other researchers.
The strengths of this paper are:
1. The paper provides a thorough and systematic evaluation of different context types and representations, which is a significant contribution to the field of word embeddings.
2. The authors use a range of tasks and datasets to evaluate the models, which provides a comprehensive understanding of the strengths and weaknesses of each model.
3. The paper is well-written and easy to follow, with clear explanations of the models and experiments.
The weaknesses of this paper are:
1. The authors change the objective function of GBOW and GSG without comparing the results to the original objective, which is a weakness that deserves more discussion and justification.
2. The hyperparameter settings used in the paper should be discussed further, with consideration of trying different values to ensure optimal results.
3. The paper lacks clarity on the model trained in section 3.4, with the reviewer questioning whether it is the same as the model trained in section 3.5.
Questions to the authors:
1. Can you provide more justification for changing the objective function of GBOW and GSG, and how it affects the results?
2. How did you determine the hyperparameter settings used in the paper, and did you try different values to ensure optimal results?
3. Can you clarify the model trained in section 3.4 and how it relates to the model trained in section 3.5?
Overall, this paper provides a significant contribution to the field of word embeddings, and the authors have done a thorough job of evaluating different context types and representations. However, there are some weaknesses that need to be addressed, and the authors should provide more clarification and justification for some of their choices.