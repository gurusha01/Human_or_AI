This paper proposes a novel approach to zero pronoun resolution by automatically generating large-scale pseudo training data and utilizing an attention-based neural network model. The authors' primary contribution is the development of a two-step training approach, which leverages both pseudo training data and task-specific data to improve model performance. The proposed model achieves state-of-the-art results on the OntoNotes 5.0 dataset, with a 3.1% absolute improvement in F-score over the previous best system.
The strengths of this paper include:
1. Effective use of pseudo training data: The authors demonstrate the effectiveness of generating large-scale pseudo training data for zero pronoun resolution, which can be used to pre-train the model and improve its performance.
2. Attention-based neural network model: The proposed attention-based neural network model is well-suited for zero pronoun resolution, as it can effectively capture the relationships between the document and query.
3. Two-step training approach: The two-step training approach, which combines pre-training and adaptation steps, is a key contribution of this paper, as it allows the model to leverage both pseudo training data and task-specific data.
However, there are some weaknesses and areas for improvement:
1. Unknown word processing: The authors' unknown word processing method, while effective, may not be sufficient for handling out-of-vocabulary words, particularly in domains with limited training data.
2. Long-distance antecedents: The model struggles with resolving zero pronouns with long-distance antecedents, which may require additional techniques, such as using more advanced attention mechanisms or incorporating external knowledge.
3. Domain adaptation: While the two-step training approach is effective, it may not be sufficient for adapting to new domains or datasets, which may require additional techniques, such as domain-invariant feature learning or multi-task learning.
Questions to the authors:
1. How do the authors plan to address the issue of unknown word processing, particularly in domains with limited training data?
2. Can the authors provide more details on the attention mechanism used in the model, and how it is adapted for zero pronoun resolution?
3. How do the authors plan to extend the two-step training approach to other tasks or domains, and what additional techniques may be required for effective domain adaptation?