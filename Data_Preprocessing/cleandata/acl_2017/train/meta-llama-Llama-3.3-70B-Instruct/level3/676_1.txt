This paper proposes a novel approach to zero pronoun resolution by automatically generating large-scale pseudo training data and utilizing an attention-based neural network model. The main contributions of this work are: 
1. The proposal of a simple but effective method to generate large-scale pseudo training data for zero pronoun resolution, which can alleviate the problem of lack of annotated data.
2. The introduction of a two-step training approach, which combines the benefits of large-scale pseudo training data and task-specific data to improve the performance of zero pronoun resolution.
3. The development of an attention-based neural network model for zero pronoun resolution, which can effectively capture the context information and improve the accuracy of zero pronoun resolution.
The strengths of this paper include:
1. The proposed approach can significantly outperform the state-of-the-art systems, with an absolute improvement of 3.1% in F-score on the OntoNotes 5.0 dataset.
2. The two-step training approach is effective in combining the benefits of large-scale pseudo training data and task-specific data, and can be easily adapted to other tasks.
3. The attention-based neural network model is well-designed and can effectively capture the context information for zero pronoun resolution.
The weaknesses of this paper include:
1. The proposed approach relies heavily on the quality of the pseudo training data, and the performance may degrade if the pseudo training data is not well-generated.
2. The model may not perform well when there are lots of unknown words in the context, especially when the unknown words appear near the zero pronoun.
3. The model may make incorrect decisions when the correct antecedents of zero pronouns are in long distance, and more advanced methods for handling long-distance dependencies may be needed.
Questions to authors:
1. How do you plan to improve the handling of unknown words in the context, especially when the unknown words appear near the zero pronoun?
2. Can you provide more details on how to adapt the proposed two-step training approach to other tasks, and what kind of tasks can benefit from this approach?
3. How do you plan to address the issue of long-distance dependencies in zero pronoun resolution, and what kind of methods can be used to improve the performance in this aspect?