This paper presents a novel approach to evaluating grammatical error correction systems, allowing for assessment of performance by error type in terms of both recall and precision. The proposed evaluation is an important stepping stone for analyzing GEC system behavior and has several advantages over previous work, including computing precision by error type and being independent of manual error annotation.
The main contributions of this paper are:
1. A novel evaluation approach for GEC systems, enabling assessment of performance by error type.
2. A thorough comparison of five approaches to increasing the robustness of parsing with the English Resource Grammar (ERG).
3. The introduction of a hybrid approach that combines the strengths of precision grammar and probabilistic parsing.
The strengths of this paper include:
1. The proposed evaluation approach provides a more fine-grained analysis of GEC system performance, enabling better understanding of system strengths and weaknesses.
2. The comparison of five approaches to increasing the robustness of parsing with the ERG provides valuable insights into the trade-offs between different techniques.
3. The hybrid approach shows promise in achieving a balance between precision and robustness.
However, there are also some weaknesses:
1. The rules for deriving error types are not described, making it difficult to replicate or adapt the approach.
2. The classifier evaluation lacks a thorough error analysis and does not provide directions for future work on improving the classifier.
3. The evaluation was only performed for English, and it is unclear how difficult it would be to use the approach on another language.
Some questions to the authors include:
1. How do the authors plan to address the issue of language dependence in their approach?
2. Can the authors provide more details on the rules for deriving error types and how they were developed?
3. How do the authors plan to improve the classifier evaluation and provide more thorough error analysis?