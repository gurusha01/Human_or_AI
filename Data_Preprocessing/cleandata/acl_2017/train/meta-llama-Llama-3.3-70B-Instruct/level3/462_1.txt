This paper proposes a novel task of concept-map-based multi-document summarization (MDS) and presents a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. The authors introduce a crowdsourcing scheme, low-context importance annotation, to determine important elements in large document collections. The corpus creation process combines automatic preprocessing, scalable crowdsourcing, and high-quality expert annotations to efficiently create a gold-standard dataset.
The main contributions of this work are:
1. Proposal of a novel summarization task, concept-map-based MDS, which extends traditional summarization by representing summaries as concept maps.
2. Introduction of a new crowdsourcing scheme, low-context importance annotation, to create reference summaries.
3. Creation of a new dataset for the proposed task, which combines large clusters of heterogeneous documents and provides a necessary benchmark to evaluate the proposed task.
4. Provision of an evaluation protocol and baseline method, which can be used to compare different approaches to concept-map-based MDS.
The strengths of this paper are:
1. The proposed task of concept-map-based MDS is novel and has the potential to improve the way summaries are represented and used.
2. The crowdsourcing scheme, low-context importance annotation, is effective in collecting reliable annotations for the importance of propositions.
3. The created corpus is large and diverse, making it a valuable resource for future research on concept-map-based MDS.
4. The evaluation protocol and baseline method provide a clear direction for future research and comparison of different approaches.
The weaknesses of this paper are:
1. The lack of novelty in the model or proposal of new features, as the approach relies on feature-based methods to improve performance.
2. The need for a more extensive literature search to compare with related works, particularly deep-learning-based methods that can achieve good performances without much feature engineering.
3. The potential limitations of the crowdsourcing scheme, such as the subjectiveness of the annotation task and the quality control measures.
Questions to the authors:
1. How do the authors plan to address the limitations of the crowdsourcing scheme, such as the subjectiveness of the annotation task and the quality control measures?
2. Can the authors provide more details on the evaluation protocol and baseline method, such as the metrics used to compare different approaches and the performance of the baseline method on the created corpus?
3. How do the authors envision the proposed task of concept-map-based MDS being used in real-world applications, and what are the potential benefits and challenges of using this approach?