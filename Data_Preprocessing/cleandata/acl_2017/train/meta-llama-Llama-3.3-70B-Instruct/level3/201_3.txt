This paper presents a comprehensive investigation of the impact of context types and representations on word embedding learning. The authors conduct thorough experiments with three models, namely Continuous Skip-Gram, Continuous Bag-of-Words, and GloVe, using different context types (linear and dependency-based) and representations (unbound and bound). The results show that context representations play a more important role than context types in learning word embeddings.
The paper is well-written, structured, and thoroughly evaluated, making it a valuable resource for researchers to choose word embeddings or develop new models. The attached software has the potential to benefit the community, providing a practical tool for further research and development.
The main contributions of this work are:
1. A systematic investigation of different context types and representations for learning word embeddings.
2. A comprehensive evaluation of the effectiveness of different word embedding models with different contexts on various tasks, including word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification.
3. The provision of a practical tool, the word2vecPM toolkit, for further research and development.
The strengths of this paper are:
1. The thorough evaluation of different context types and representations, which provides valuable insights into their impact on word embedding learning.
2. The use of multiple models and tasks, which demonstrates the robustness of the findings.
3. The provision of a practical tool, which will facilitate further research and development in the field.
The weaknesses of this paper are:
1. The novelty of the paper is limited, as the authors build upon existing work on word embeddings and context types.
2. The comparison between dependency-based and linear context types may be unfair due to the reliance on predicted dependency parsing results for the former.
3. The paper could benefit from a more detailed analysis of the results, particularly in terms of the implications for downstream tasks.
Questions to the authors:
1. How do the authors plan to address the potential unfairness in the comparison between dependency-based and linear context types?
2. Can the authors provide more insights into the implications of the results for downstream tasks, such as text classification and sentiment analysis?
3. How do the authors envision the word2vecPM toolkit being used in future research and development, and what potential applications do they see for the tool?