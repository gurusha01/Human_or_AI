This paper proposes a general framework for cross-context lexical analysis, which enables the comparison of word meanings and representations across different contexts. The framework is flexible and can accommodate various notions of context, similarity functions, and word annotations. The authors demonstrate the framework's applicability to three tasks: semantic change detection, comparative lexical analysis over context, and word embedding stability evaluation.
The main contributions of this work are:
1. General framework for cross-context lexical analysis: The authors propose a framework that can be used to analyze word meanings and representations across different contexts, which is a significant contribution to the field of natural language processing.
2. Flexibility and applicability: The framework is flexible and can be applied to various tasks, including semantic change detection, comparative lexical analysis, and word embedding stability evaluation.
3. Comparison of word embeddings: The authors propose a method to compare word embeddings across different contexts, which can be useful for evaluating the stability of word embeddings and identifying words with similar meanings.
The strengths of this paper are:
1. Interesting and challenging idea: The idea of investigating relations between lexical items across different contexts is interesting and challenging, and the authors provide a well-structured framework to address this challenge.
2. Clear and well-organized presentation: The paper is well-organized, and the authors provide clear explanations of the framework and its applications.
3. Potential for further research: The paper opens up interesting new directions for further study, including investigating framing bias on political viewpoints, fine-grained comparative analysis over specific products, and using CCLA as a tool in a larger system.
However, there are some weaknesses in the paper:
1. Lack of justification for clustering: The authors choose clustering for evaluation, but do not provide a clear justification for this choice. Classification tasks may be more straightforward to evaluate.
2. Lack of comparison to human performance: The paper lacks an explanation of the overall level of the results and a comparison to human performance on the task, which would provide a better understanding of the framework's effectiveness.
3. Limited evaluation: The evaluation is limited to a few datasets and tasks, and the authors do not provide a comprehensive evaluation of the framework's performance across different contexts and tasks.
Questions to the authors:
1. Can you provide more justification for the choice of clustering for evaluation, and consider alternative evaluation methods, such as classification tasks?
2. How do you plan to address the issue of disjoint vocabulary when comparing word embeddings across different contexts?
3. Can you provide more details on the potential applications of CCLA, such as investigating framing bias on political viewpoints, and fine-grained comparative analysis over specific products?