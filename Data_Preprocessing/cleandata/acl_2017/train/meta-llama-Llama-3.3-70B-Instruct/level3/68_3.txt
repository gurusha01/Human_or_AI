This paper proposes a new model, the Gated-Attention (GA) Reader, for answering cloze-style questions over documents. The model features a novel multiplicative gating mechanism combined with a multi-hop architecture. The authors claim that their model achieves state-of-the-art performance on several large-scale benchmark datasets, with improvements of over 4% over competitive baselines.
The main contributions of this work are:
1. The proposal of a new gated-attention mechanism that allows the model to attend to different aspects of the query at different layers.
2. The combination of the gated-attention mechanism with a multi-hop architecture, which enables the model to iteratively refine its representation of the document and query.
3. The achievement of state-of-the-art performance on several benchmark datasets, including CNN, Daily Mail, and Who Did What.
The strengths of this paper are:
1. The novelty of the proposed gated-attention mechanism, which is shown to be effective in capturing the relationships between the document and query.
2. The thorough evaluation of the model on several benchmark datasets, which demonstrates its effectiveness in a variety of settings.
3. The provision of detailed ablation studies, which help to understand the contributions of different components of the model.
The weaknesses of this paper are:
1. The lack of a clear explanation of why the gated-attention mechanism is necessary, and how it improves upon existing attention mechanisms.
2. The limited analysis of the results, which makes it difficult to understand why the model performs well on certain datasets and not others.
3. The absence of a discussion on the potential applications of the proposed model, and how it can be used in real-world settings.
Some questions to the authors are:
1. Can you provide more insight into why the gated-attention mechanism is effective, and how it differs from existing attention mechanisms?
2. How do you plan to extend the proposed model to other natural language processing tasks, such as question answering or text summarization?
3. Can you provide more details on the hyperparameter settings used for each dataset, and how they were chosen? 
Additional comments:
The paper could benefit from more detailed explanations of the model architecture and the gated-attention mechanism. The related work section is also somewhat limited, and could be expanded to include more recent works on attention mechanisms and multi-hop architectures. The experimental results are thorough, but could be improved with more detailed analysis and discussion. Overall, the paper presents a novel and effective model for answering cloze-style questions, but could be improved with more detailed explanations and analysis.