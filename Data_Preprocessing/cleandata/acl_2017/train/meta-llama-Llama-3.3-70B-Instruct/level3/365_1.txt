This paper presents a scoring support system for short-answer tests, which combines machine learning and human evaluation to assess student answers. The system uses random forests to predict scores based on the similarity between student answers and model answers, as well as other factors such as semantic meaning and surface-level features.
The main contributions of this work are: 
1. The development of a scoring support system that can effectively evaluate short-answer tests, 
2. The use of random forests to predict scores based on a combination of surface-level and semantic features, 
3. The implementation of a mechanism to allow human evaluators to review and revise the predicted scores.
The strengths of this paper include: 
1. The system's ability to achieve a high degree of accuracy in predicting scores, with differences between predicted and human scores within one point for 70-90% of the data, 
2. The use of a robust machine learning algorithm (random forests) that can handle multiple predictor variables and provide estimates of variable importance, 
3. The system's flexibility in allowing human evaluators to review and revise predicted scores, which can help to improve the accuracy and reliability of the scoring process.
However, there are also some weaknesses and areas for improvement: 
1. The paper lacks critical details, such as a clear description of the attention mechanism and its implementation, 
2. The comparison of output vectors in Figure 4 is not entirely clear, as the hidden layer dimensions of the two models are likely to be different, 
3. The experimental improvements in accuracy are relatively small, which may not be sufficient to justify the use of the proposed system, 
4. The paper raises questions about the Kappa statistic for attention vs. MTL and the upper bound across all datasets, which need to be clarified.
Some questions to the authors include: 
1. Can you provide more details on the implementation of the attention mechanism and its role in the scoring process? 
2. How do you plan to address the issue of limited training data, particularly in cases where human scores are often zero or illogical? 
3. Can you provide more information on the evaluation metrics used to assess the system's performance, and how they relate to the Kappa statistic and upper bound mentioned in the paper? 
Overall, this paper presents a promising approach to scoring short-answer tests, but requires further clarification and improvement in certain areas to fully demonstrate its effectiveness.