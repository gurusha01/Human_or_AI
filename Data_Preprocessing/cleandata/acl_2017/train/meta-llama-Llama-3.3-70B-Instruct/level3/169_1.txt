This paper presents a useful application for enhancing the coverage of Head-driven Phrase Structure Grammar (HPSG) grammars, specifically the English Resource Grammar (ERG), by exploring several robust parsing techniques. The main contributions of this work are:
1. The evaluation of five approaches to increasing the robustness of parsing with the ERG, including bridging, Pacman, PCFG approximation, and hybrid approaches.
2. The introduction of a new evaluation methodology for robust parsing techniques, which addresses the challenge of lacking gold standard evaluation data for unparseable inputs.
3. The demonstration of significant coverage gains and improved accuracy figures for the proposed techniques, particularly the PCFG-based systems.
The strengths of this paper include:
1. The thorough evaluation of multiple robust parsing techniques, providing a comprehensive comparison of their strengths and weaknesses.
2. The introduction of a new evaluation methodology, which addresses a significant challenge in the field of robust parsing.
3. The demonstration of promising coverage gains and accuracy figures, which suggests that the proposed techniques may be useful in downstream applications.
However, the paper also has some weaknesses:
1. The description of the system is highly superficial, lacking details about the manually created rules that form the core of the system.
2. The evaluation of the proposed system is not satisfying, with issues including the lack of a gold standard, incorrect annotation methodology, and incomplete information about the test data.
3. The paper's claim that the system is less domain-dependent than others is questionable, as the manually-created rules can be domain-dependent and language-dependent.
4. The introduction of a new set of error categories is unclear, with no clear motivation or discussion provided, and existing tagsets could have been used instead.
5. The paper contains several typos and formatting errors, including incorrect citations and case inconsistencies.
To improve the paper, the authors should provide more detailed descriptions of the system and its components, address the issues with the evaluation methodology, and clarify the introduction of new error categories. Additionally, the authors should proofread the paper carefully to eliminate typos and formatting errors.
Questions to authors:
1. Can you provide more details about the manually created rules that form the core of the system?
2. How do you plan to address the issues with the evaluation methodology, such as the lack of a gold standard and incorrect annotation methodology?
3. Can you clarify the motivation and discussion behind the introduction of new error categories, and why existing tagsets were not used instead?
4. How do you plan to improve the accuracy of the parse ranking model, which is not adapted to the novel situations in which it is used?