This paper presents a comprehensive evaluation methodology for the task of ghostwriting rap lyrics, focusing on originality, style similarity, fluency, and coherence. The authors propose a manual evaluation methodology that assesses these key properties of generated verse and create a dataset of authentic verse, manually annotated for style matching. They also automate a previously proposed semi-automatic evaluation method and demonstrate its effectiveness in replicating the results of the original method.
The main contributions of this paper are: 
1. A comprehensive manual evaluation methodology for assessing the quality of generated rap lyrics, which captures complementary aspects of the task, such as fluency, coherence, and style matching.
2. A fully automated evaluation methodology that replicates the results of the previously available semi-automatic method and provides a more efficient and scalable way to evaluate the performance of ghostwriting models.
3. A dataset of authentic verse, manually annotated for style matching, which can be used as a gold standard for future experiments and provides valuable insights into the similarity between artists' styles.
The strengths of this paper include:
1. The proposed evaluation methodology is well-motivated and comprehensive, capturing multiple aspects of the ghostwriting task.
2. The authors provide a thorough analysis of the results, highlighting the strengths and weaknesses of the proposed methodology and the LSTM-based ghostwriter model.
3. The paper demonstrates the effectiveness of the proposed methodology in evaluating the performance of different models and provides valuable insights into future directions for generative models.
However, there are some weaknesses:
1. The paper lacks evidence to support the claim that the automatic metric can meaningfully analyze system performance, and the comparison to a baseline system is insufficient.
2. The paper misses relevant references, including recent work on automating coherence and style matching from the NLG community.
3. The effectiveness of the automatic metric is unclear due to low correlations with other metrics, which may indicate that it is not a reliable measure of system performance.
Questions to the authors:
1. Can you provide more evidence to support the claim that the automatic metric can meaningfully analyze system performance, such as correlations with human evaluations or comparisons to other automated metrics?
2. How do you plan to address the lack of relevant references, and what are the implications of this omission for the validity of the proposed methodology?
3. Can you provide more details on the limitations of the automatic metric and how you plan to improve its effectiveness in future work?