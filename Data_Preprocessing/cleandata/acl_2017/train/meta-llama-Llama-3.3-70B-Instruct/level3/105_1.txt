Summary of the Paper
The paper proposes a novel task of concept-map-based multi-document summarization (MDS), which aims to generate a concept map that represents the most important content of a set of related documents. The authors introduce a new crowdsourcing scheme, called low-context importance annotation, to create a gold-standard corpus for this task. The corpus consists of 30 topics, each with around 40 documents and a summarizing concept map. The authors also provide a baseline system and evaluation protocol to enable further research on this variant of summarization.
Main Contributions
1. Novel Task Definition: The authors propose a new task of concept-map-based MDS, which extends the traditional task of MDS by generating a concept map as the summary.
2. Low-Context Importance Annotation: The authors introduce a novel crowdsourcing scheme to determine the importance of propositions in a document cluster.
3. Gold-Standard Corpus: The authors create a new corpus of 30 topics, each with a summarizing concept map, to serve as a benchmark for the proposed task.
4. Baseline System and Evaluation Protocol: The authors provide a baseline system and evaluation protocol to facilitate further research on this task.
Strengths
1. Novelty of the Task: The proposed task of concept-map-based MDS is new and substantially different from others in the field of MDS.
2. Effectiveness of the Crowdsourcing Scheme: The low-context importance annotation scheme is shown to be effective in collecting reliable annotations for the importance of propositions.
3. Quality of the Corpus: The created corpus is of high quality, with a large number of topics and documents, and a careful annotation process.
4. Baseline System and Evaluation Protocol: The provided baseline system and evaluation protocol will facilitate further research on this task.
Weaknesses
1. Limited Evaluation: The evaluation of the proposed task and the baseline system is limited to a small number of topics and documents.
2. Lack of Comparison to Other Tasks: The paper does not provide a comparison of the proposed task to other related tasks, such as traditional MDS or keyphrase extraction.
3. Need for Further Analysis: The paper could benefit from further analysis of the results, such as an examination of the types of errors made by the baseline system.
Questions to Authors
1. How do the authors plan to extend the proposed task to other domains or genres of text?
2. Can the authors provide more details on the annotation process and the guidelines used to create the gold-standard corpus?
3. How do the authors plan to address the limitations of the evaluation, such as the limited number of topics and documents?