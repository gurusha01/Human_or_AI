This paper proposes an automated Japanese short-answer scoring and support system for the new National Center written test exams. The main strength of the paper is its promise of a speed advantage while maintaining the same accuracy level as human raters. 
The primary contributions of this work are: 
1. The development of a support system for short written tests where a human rater can correct the automated score by referring to the original scores.
2. The use of random forests to utilize many predictors effectively, including the degree of fitness with the scoring guideline and semantic similarity between the answer, model answer, and test item sentences.
3. The evaluation of the performance of the classification using eight test items, which showed a probability of 71-95% that the differences between the scores were within one point.
The strongest arguments supporting the acceptance of this submission are: 
1. The system's ability to reduce the time and effort required for scoring, while maintaining a certain degree of fitness with human ratings.
2. The use of machine learning to improve the accuracy of the scoring system, which can be trained on a large dataset of human scores.
3. The flexibility of the system, which can be applied to other subjects, such as Japanese literature, and can handle different transcriptions of a correct answer.
However, there are also some weaknesses to this submission: 
1. The unclear presentation of the approach, particularly in sections 3 and 4, which need clearer concept definitions and explanations.
2. The lack of sufficient experiments to support the claims of significant speed improvements, with the current results showing only a 4-6X speed-up.
3. The unclear term "Viterbi prediction", which requires a better explanation or phrasing.
4. The typo in the reference to Weiss et al., 2015, which needs to be corrected.
Questions to the authors include: 
1. Can you provide more details on the machine learning algorithm used and how it was trained?
2. How do you plan to address the issue of obtaining a sufficiently large number of human scores for supervised learning?
3. Can you provide more information on the evaluation metrics used to measure the performance of the system?