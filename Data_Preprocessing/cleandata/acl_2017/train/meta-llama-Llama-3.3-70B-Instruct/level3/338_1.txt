This paper proposes a neural network model to identify spam reviews under cold-start conditions, presenting a compelling approach to address the problem. The key contributions of this work include the development of a neural end-to-end solution for computational argumentation mining, outperforming state-of-the-art joint ILP models, and demonstrating the effectiveness of a standard neural sequence tagging model in different environments.
The strengths of this submission lie in its thorough related work and evaluation sections, which make the hypothesis stronger. The authors have conducted extensive experiments, and the results are convincing, showing that their approach outperforms existing methods. Additionally, the paper highlights the importance of considering long-range dependencies in argumentation mining, which is a crucial aspect of the problem.
However, there are some weaknesses in the submission. The dataset used for training and testing is not clearly described, lacking information on size, time frame, and other relevant details. Furthermore, the paper's results may be impacted by the limited number of reviewers who posted multiple reviews, and a more thorough discussion is needed to address this issue. The writing could also be improved, as there are several typos, grammatical errors, and unclear sentences that need to be revised.
The main contributions of this work are:
1. The development of a neural end-to-end solution for computational argumentation mining.
2. The demonstration of the effectiveness of a standard neural sequence tagging model in different environments.
3. The highlighting of the importance of considering long-range dependencies in argumentation mining.
The strongest arguments supporting the acceptance of this submission are:
1. The thorough related work and evaluation sections, which make the hypothesis stronger.
2. The convincing experimental results, showing that the proposed approach outperforms existing methods.
3. The importance of considering long-range dependencies in argumentation mining, which is a crucial aspect of the problem.
The strongest arguments against the acceptance of this submission are:
1. The lack of clear description of the dataset used for training and testing.
2. The potential impact of the limited number of reviewers who posted multiple reviews on the results.
3. The need for improvement in writing quality, with several typos, grammatical errors, and unclear sentences.
Questions to the authors:
1. Can you provide more details about the dataset used for training and testing, including its size, time frame, and other relevant information?
2. How do you plan to address the potential impact of the limited number of reviewers who posted multiple reviews on the results?
3. Can you revise the writing to improve clarity and correctness, addressing the typos, grammatical errors, and unclear sentences?