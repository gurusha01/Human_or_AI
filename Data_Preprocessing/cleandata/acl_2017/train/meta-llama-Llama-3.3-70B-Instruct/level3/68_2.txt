Summary of the Paper
The paper presents a new approach to answering cloze-style questions over documents, called the Gated-Attention (GA) Reader. The GA Reader features a novel multiplicative gating mechanism, combined with a multi-hop architecture, which allows the model to iteratively refine token representations and attend to different aspects of the query to arrive at the final answer. The model is evaluated on several large-scale benchmark datasets, including CNN, Daily Mail, CBT-NE, CBT-CN, and WDW, and achieves state-of-the-art performance with significant improvements over competitive baselines.
Main Contributions
1. Gated-Attention Mechanism: The paper introduces a novel gated-attention mechanism that allows the model to attend to different aspects of the query and filter out irrelevant information.
2. Multi-Hop Architecture: The paper proposes a multi-hop architecture that enables the model to iteratively refine token representations and attend to different aspects of the query.
3. State-of-the-Art Performance: The GA Reader achieves state-of-the-art performance on several large-scale benchmark datasets, with significant improvements over competitive baselines.
Strengths
1. Effective Use of Gated-Attention: The paper demonstrates the effectiveness of the gated-attention mechanism in improving the performance of the model.
2. Multi-Hop Architecture: The multi-hop architecture allows the model to iteratively refine token representations and attend to different aspects of the query, leading to improved performance.
3. Comprehensive Evaluation: The paper evaluates the GA Reader on several large-scale benchmark datasets, providing a comprehensive assessment of its performance.
Weaknesses
1. Lack of Theoretical Justification: The paper lacks a theoretical justification for the use of multiplicative gating, which is an important aspect of the GA Reader.
2. Limited Analysis of Model Components: The paper could benefit from a more detailed analysis of the contributions of each model component, such as the character embeddings and token-specific attentions.
3. Comparison to Other Models: The paper could benefit from a more comprehensive comparison to other models, including those that use different attention mechanisms or architectures.
Questions to Authors
1. Can you provide a theoretical justification for the use of multiplicative gating in the GA Reader?
2. How do the character embeddings and token-specific attentions contribute to the performance of the GA Reader?
3. Can you compare the GA Reader to other models that use different attention mechanisms or architectures?