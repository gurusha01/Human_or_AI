This paper introduces a novel approach to improve zero pronoun resolution performance by automatically generating large-scale pseudo training data and utilizing a two-step training mechanism. The proposed method significantly outperforms state-of-the-art systems, achieving an absolute improvement of 3.1% F-score on the OntoNotes 5.0 dataset.
The main contributions of this work are:
1. Automatic generation of large-scale pseudo training data for zero pronoun resolution, which alleviates the issue of limited annotated data.
2. A two-step training approach, consisting of pre-training and adaptation stages, which effectively leverages both pseudo training data and task-specific data.
3. The application of an attention-based neural network model to zero pronoun resolution, which demonstrates promising performance.
The strengths of this paper include:
1. The proposed approach addresses the limitation of annotated data, which is a significant challenge in zero pronoun resolution.
2. The two-step training mechanism allows for effective adaptation to task-specific data, resulting in improved performance.
3. The attention-based neural network model is well-suited for zero pronoun resolution, as it can capture contextual relationships between words.
However, there are some weaknesses and questions that need to be addressed:
1. The paper raises questions about identifying antecedents of zero pronouns, particularly in situations where the head word is not a pronoun or the predicted noun is not found in previous content.
2. The use of attention-based neural networks may not be the most effective choice for zero pronoun resolution, and alternative architectures should be explored.
3. The unknown words processing mechanism, although effective, may not be sufficient to handle all cases, and more advanced methods should be investigated.
Questions to the authors:
1. How do you plan to handle cases where the antecedent of a zero pronoun is not a noun phrase, but rather a clause or a sentence?
2. Can you provide more details on the attention mechanism used in the neural network model and how it contributes to the overall performance?
3. How do you intend to extend the proposed approach to other languages, considering the differences in linguistic structures and annotation schemes?