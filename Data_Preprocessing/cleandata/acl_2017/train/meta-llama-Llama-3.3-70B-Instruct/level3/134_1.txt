This paper presents a comprehensive study on neural end-to-end approaches for computational argumentation mining (AM). The authors propose several framings for the AM problem, including dependency parsing, sequence tagging, and multi-task learning. The results show that the sequence tagging approach, particularly the BiLSTM-CRF-CNN (BLCC) tagger, performs well on component detection, while the LSTM-ER model, which combines sequential and tree structure information, excels on component identification but struggles with relation identification on longer documents.
The paper's main contributions are:
1. The proposal of neural end-to-end solutions for AM, which eliminate the need for manual feature engineering and costly ILP constraint designing.
2. The demonstration that BiLSTM taggers perform well for component detection.
3. The finding that naively coupling component and relation detection is not optimal, but both tasks should be treated separately and modeled jointly.
The strengths of this paper include:
1. The thorough evaluation of different framings for the AM problem, providing a comprehensive understanding of the strengths and weaknesses of each approach.
2. The use of a robust evaluation metric, which takes into account both component and relation identification.
3. The comparison with a feature-based ILP formulation, which demonstrates the effectiveness of the neural end-to-end approaches.
However, there are also some weaknesses:
1. The small size of the dataset used, which may not be representative of the entire AM problem domain.
2. The lack of analysis on overfitting, which is a concern given the large capacity of the neural networks used.
3. The absence of discussion on regularization methods, which is crucial for preventing overfitting.
To improve the paper, I suggest:
1. Moving some information, such as pre-trained word embeddings and error analysis, from the supplementary material to the main paper to improve clarity.
2. Including inter-annotator agreement scores to assess the quality of the annotations.
3. Illustrating figures with different colors to improve print quality.
4. Providing more detailed analysis on the performance of the models, including mean and variance of different settings, as well as F1-score ranges, to address concerns about model stability.
Overall, this paper presents a significant contribution to the field of AM, and with some revisions, it has the potential to be a strong submission to the AI conference. 
Questions to authors:
1. How do you plan to address the issue of overfitting, given the large capacity of the neural networks used?
2. Can you provide more details on the hyperparameter optimization process, and how it was performed?
3. How do you think the results would change if a larger dataset were used, and what steps would you take to collect and annotate such a dataset?