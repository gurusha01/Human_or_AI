This paper proposes a method for training models on Chinese word segmentation datasets with multiple segmentation criteria. The proposed model achieves significant improvement from baselines, making it a notable contribution. 
The main contributions of this work are: 
1. The proposal of a new state-of-the-art result for natural language inference, achieving an accuracy of 88.6% on the Stanford natural language inference dataset.
2. The demonstration that carefully designing sequential inference models based on chain LSTMs can outperform all previous models, including those using more complicated network architectures.
3. The incorporation of syntactic parsing information into the model, which further improves the performance, even when added to an already strong model.
The strengths of this submission are:
1. The proposed model achieves significant improvement from baselines, making it a notable contribution.
2. The incorporation of syntactic parsing information into the model, which further improves the performance.
3. The use of a hybrid approach, combining sequential and syntactic tree-based models, which allows for a more comprehensive understanding of the input data.
The weaknesses of this submission are:
1. The comparison to other CWS models is lacking, and the baseline model used is not specifically designed for CWS.
2. The purpose of experiments in Section 6.4 is unclear, particularly with regards to the use of fixed shared parameters.
3. The paper could be improved with a more detailed discussion on the datasets where adversarial multi-criteria learning does not enhance performance.
Questions to authors:
1. How do the authors plan to address the lack of comparison to other CWS models, and what baseline models would they consider using in future work?
2. Can the authors provide more clarification on the purpose of the experiments in Section 6.4, and how the use of fixed shared parameters affects the results?
3. How do the authors plan to improve the discussion on the datasets where adversarial multi-criteria learning does not enhance performance, and what insights can be gained from these datasets?