This paper proposes an alternative to Bi-directional LSTMs for sequence labeling tasks, namely iterated dilated convolutional neural networks (ID-CNNs). The main strength of the paper is the incorporation of dilated convolutions, which allows the model to efficiently aggregate broad context without losing resolution. The paper also presents a thorough evaluation of the proposed model, demonstrating significant speed gains over various recurrent models while maintaining similar F1 performance.
The main contributions of this work are:
1. The proposal of ID-CNNs as a fast and efficient feature extractor for sequence labeling tasks.
2. The demonstration of the effectiveness of ID-CNNs in aggregating broad context without losing resolution.
3. The evaluation of ID-CNNs on two benchmark English named entity recognition datasets, showing significant speed gains and competitive F1 performance.
The strengths of the paper are:
1. The clarity of the presentation, making it easy to understand the proposed model and its components.
2. The thorough evaluation of the proposed model, including comparisons with strong baselines and ablation studies.
3. The demonstration of the effectiveness of ID-CNNs in processing entire documents at a time, which is a significant advantage over traditional recurrent models.
The weaknesses of the paper are:
1. The lack of a clear explanation of why ID-CNNs are more effective than traditional recurrent models in certain tasks.
2. The limited evaluation of ID-CNNs on other sequence labeling tasks, which makes it difficult to assess the generalizability of the proposed model.
3. The need for more efficient data utilization, as the proposed model requires a significant amount of data to achieve good performance.
Questions to the authors:
1. Can you provide more insights into why ID-CNNs are more effective than traditional recurrent models in certain tasks?
2. How do you plan to address the issue of data efficiency, and what strategies can be employed to reduce the amount of data required for training?
3. Can you provide more evaluation results on other sequence labeling tasks to demonstrate the generalizability of the proposed model?