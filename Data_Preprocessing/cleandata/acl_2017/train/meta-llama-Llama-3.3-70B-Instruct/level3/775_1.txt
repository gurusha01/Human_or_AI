This paper proposes a novel approach for constructing semantic hierarchies using a fusion learning architecture based on word embeddings. The main contributions of this work are: 
1. A uniform fusion architecture that can learn semantic hierarchies via word embeddings without any background knowledge.
2. The method outperforms state-of-the-art methods on a manually labeled test dataset, especially with a high precision value for application.
3. The fusion learning architecture is language-independent, which can be easily expanded to be suitable for other languages.
The strengths of this submission are:
1. The proposed method achieves a high F1-score of 74.20% on a manually labeled test dataset, outperforming state-of-the-art methods.
2. The method has a high precision value of 91.60%, making it suitable for practical applications.
3. The fusion learning architecture is language-independent, which can be easily expanded to other languages.
The weaknesses of this submission are:
1. The approach requires supervision and is not truly zero-shot learning, as it relies on gold-standard labels and pre-existing vector representations.
2. The interpretation of the results is unclear, with the use of normalized vectors and noun vector dominance potentially being data-specific and limiting the approach's generalizability.
3. The novelty of the approach is questionable, as it bears similarities to previous work, such as Agres et al, with the main contribution being the use of geometric properties for classification.
Questions to authors:
1. How does the proposed method handle out-of-vocabulary words or words with limited context information?
2. Can the authors provide more insights into the choice of hyperparameters and their effects on the performance of the model?
3. How does the proposed method compare to other state-of-the-art methods in terms of computational efficiency and scalability?