This paper introduces a novel approach to learning word embeddings using Gaussian mixtures, which can effectively capture multiple word meanings and uncertainty. The proposed model, Word to Gaussian Mixture (w2gm), extends the unimodal Gaussian distribution approach by Vilnis and McCallum (2014) and addresses the problem of polysemy.
The main contributions of this work are:
1. Multimodal word distributions: The paper proposes a Gaussian mixture model to represent words, allowing for multiple distinct meanings and uncertainty.
2. Energy-based max-margin objective: The authors introduce an energy-based max-margin objective to learn the parameters of the Gaussian mixture model, which maximizes the similarity between word distributions.
3. Expected likelihood kernel: The paper uses the expected likelihood kernel as a measure of similarity between word distributions, which can be evaluated in closed form for Gaussian mixtures.
The strengths of this submission are:
1. Effective handling of polysemy: The proposed model can capture multiple word meanings, which is a significant improvement over unimodal Gaussian distributions.
2. Improved performance on word similarity tasks: The w2gm model outperforms other approaches, including word2vec and Gaussian embeddings, on several word similarity datasets.
3. Interpretability: The Gaussian mixture model provides a more interpretable representation of words, allowing for the identification of distinct meanings and uncertainty.
The weaknesses of this submission are:
1. Lack of comparison to similar approaches: The paper could benefit from a more comprehensive comparison to other multimodal word embedding approaches, such as Tian et al. (2014).
2. Missing citations: The paper lacks citations to related work, including Neelakantan et al. (2014), Li and Jurafsky (2015), and Liu et al. (2015).
3. Loss of performance on SWCS: The authors should investigate the loss of performance of w2gm against w2g on the SWCS dataset.
Questions to the authors:
1. How do the authors plan to address the lack of comparison to similar approaches, such as Tian et al. (2014)?
2. Can the authors provide more insights into the loss of performance on the SWCS dataset?
3. How do the authors plan to incorporate missing citations to related work, such as Neelakantan et al. (2014), Li and Jurafsky (2015), and Liu et al. (2015)?