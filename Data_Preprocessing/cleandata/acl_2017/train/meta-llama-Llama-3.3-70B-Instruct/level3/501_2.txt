This paper proposes a novel approach to word representation learning (WRL) by incorporating sememe information from the HowNet knowledge base. The authors introduce a Sememe-Encoded WRL (SE-WRL) model that utilizes sememe annotations to improve word embeddings. The model is evaluated on two tasks: word similarity and word analogy, and the results show that the SE-WRL model outperforms baseline models.
The main contributions of this paper are:
1. The proposal of a novel SE-WRL model that incorporates sememe information to improve word embeddings.
2. The introduction of a sememe attention mechanism that automatically selects appropriate senses for context words.
3. The evaluation of the SE-WRL model on two tasks, word similarity and word analogy, which demonstrates its effectiveness.
The strengths of this paper are:
1. The use of sememe information to improve word embeddings, which is a novel approach in WRL.
2. The introduction of a sememe attention mechanism, which allows the model to automatically select appropriate senses for context words.
3. The evaluation of the model on two tasks, which demonstrates its effectiveness and versatility.
The weaknesses of this paper are:
1. The lack of clarity on why the image caption task is not suitable for comprehension tasks and why the authors' system is better.
2. The paper converges to using existing caption generation techniques, which is a concern.
3. The formula (4) is presented in a confusing manner, which makes it difficult to understand.
Questions to authors:
1. Can you provide more details on why the image caption task is not suitable for comprehension tasks and why your system is better?
2. How do you plan to address the concern of converging to existing caption generation techniques?
3. Can you provide a clearer explanation of formula (4) and its significance in the SE-WRL model?
Overall, this paper proposes a novel approach to WRL by incorporating sememe information and introduces a sememe attention mechanism. While there are some weaknesses and concerns, the paper demonstrates the effectiveness of the SE-WRL model on two tasks and has the potential to contribute to the field of NLP.