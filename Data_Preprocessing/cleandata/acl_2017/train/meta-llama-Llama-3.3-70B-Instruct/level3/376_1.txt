This paper proposes a self-learning framework to learn bilingual word embedding mappings, which can work with as little as 25 word pairs or even an automatically generated list of numerals. The method is an extension of existing mapping techniques and can be combined with any dictionary-based mapping technique. The authors claim that their method is able to learn high-quality bilingual embeddings from small seed dictionaries, obtaining results comparable to state-of-the-art systems that use much richer bilingual resources.
The main contributions of this work are:
1. A self-learning framework that can learn bilingual word embedding mappings from small seed dictionaries.
2. The method can work with as little as 25 word pairs or an automatically generated list of numerals.
3. The authors provide a detailed analysis of the optimization objective and show that their method is implicitly optimizing a meaningful objective function.
The strengths of this paper are:
1. The proposed method is simple and efficient, making it easy to implement and replicate.
2. The authors provide a thorough analysis of the optimization objective and show that their method is optimizing a meaningful objective function.
3. The method is able to learn high-quality bilingual embeddings from small seed dictionaries, which is a significant improvement over previous methods.
The weaknesses of this paper are:
1. The paper lacks clear explanations for many points, including the baselines, experimentation, and evaluation.
2. The proposed models demonstrate a nice performance increase compared to baselines, but the baselines themselves need more explanation and justification.
3. The training of the recursive NN models is not well-explained, and the supplied code does not include the baselines, making it impossible to replicate the results.
4. The paper has several unclear or unexplained points, including the definition of non-state verbs, the mapping of CAMEO/TABARI categories to positive and negative entries, and the calculation of precision and recall values.
Questions to authors:
1. Can you provide more details on the baselines used in the experimentation and evaluation?
2. How did you select the 25 word pairs used in the small seed dictionary?
3. Can you provide more information on the optimization objective and how it is related to the self-learning framework?
4. How do you plan to extend this work to learn bilingual word embeddings without any bilingual evidence at all?