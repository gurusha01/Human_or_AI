This paper proposes a novel approach to concept-map-based multi-document summarization, which involves creating a concept map that represents the most important content of a set of related documents. The authors introduce a new crowdsourcing scheme, low-context importance annotation, to determine the importance of propositions in a document cluster. They use this scheme to create a new benchmark corpus for concept-map-based MDS, which consists of 30 topics with around 40 documents each.
The main contributions of this paper are:
1. The proposal of a novel summarization task, concept-map-based MDS, which emphasizes the summarization aspect of the task.
2. The development of a new crowdsourcing scheme, low-context importance annotation, to create reference summaries.
3. The creation of a new dataset for the proposed task, which combines large clusters of heterogeneous documents with a necessary benchmark to evaluate the proposed task.
4. The provision of an evaluation protocol and baseline method for the task.
The strengths of this paper are:
1. The thorough related work section, which covers a wide range of relevant topics and provides a clear understanding of the context and background of the proposed task.
2. The excellent comparison with many baseline systems, which demonstrates the effectiveness of the proposed approach.
3. The novelty of the proposed crowdsourcing scheme, which allows for efficient and scalable creation of reference summaries.
4. The creation of a new benchmark corpus, which provides a necessary resource for future research on concept-map-based MDS.
The weaknesses of this paper are:
1. The lack of reporting of ROUGE F-Score for all three datasets, which would provide a more comprehensive evaluation of the proposed approach.
2. The placement of the related work section, which would be more effective if placed earlier in the paper to provide context.
3. The need for a discussion on the context and intended use of the summaries/compressions, which would provide a clearer understanding of the purpose and potential applications of the proposed task.
4. The lack of human evaluations of the compressions, such as readability and coherence metrics, or an extrinsic evaluation, which would be necessary for a more comprehensive assessment of the proposed approach.
Questions to authors:
1. How do the authors plan to address the issue of content selection, which was identified as a major challenge in the baseline experiments?
2. Can the authors provide more details on the evaluation metrics used to compare generated concept maps with reference maps?
3. How do the authors plan to extend the proposed approach to other types of documents or domains?
4. Can the authors provide more information on the potential applications and uses of concept-map-based MDS?