This paper proposes an alternative to Bi-directional LSTMs for sequence labeling tasks, called iterated dilated convolutional neural networks (ID-CNNs). The paper is well-written and well-organized, effectively presenting the proposed model and its experimental setting. The ID-CNN model demonstrates superiority over various recurrent models in terms of speed and accuracy, particularly when processing entire documents at a time.
The main contributions of this work are:
1. The proposal of ID-CNNs as a fast and efficient feature extractor for sequence labeling tasks.
2. The demonstration of the effectiveness of ID-CNNs in aggregating broad context without losing resolution.
3. The achievement of significant speed improvements for sequence labeling, particularly when processing entire documents at a time.
The strengths of this paper are:
1. The ID-CNN model is shown to be faster and more accurate than various recurrent models, including Bi-LSTMs and Bi-LSTM-CRFs.
2. The experimental setting is well-designed, using several datasets and evaluation metrics to demonstrate the superiority of the proposed method.
3. The paper provides a clear and detailed explanation of the ID-CNN model and its training procedure.
The weaknesses of this paper are:
1. The lack of comparison with novel segmentation methods, which would be necessary to validate the proposed method's effectiveness.
2. The limited evaluation of the ID-CNN model on only two datasets, which may not be representative of all sequence labeling tasks.
Questions to authors:
1. How does the ID-CNN model handle out-of-vocabulary words and rare entities?
2. Can the ID-CNN model be applied to other NLP tasks, such as parsing and machine translation?
3. How does the ID-CNN model compare to other convolutional neural network architectures, such as those using attention mechanisms or graph convolutional networks?