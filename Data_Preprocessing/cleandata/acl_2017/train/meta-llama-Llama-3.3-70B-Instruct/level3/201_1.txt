This paper presents a comprehensive study on the variation in performance of embedding models by systematically changing parameters such as context type, position sensitivity, embedding model, and task. The study's goal is interesting and relevant to the ACL community, as it aims to investigate the impact of different context types and representations on the quality of learned word embeddings.
The main contributions of this work are:
1. A systematic investigation of different context types (linear and dependency-based) and representations (bound and unbound) for learning word embeddings.
2. A comprehensive evaluation of the effectiveness of different word embedding models with different contexts on various tasks, including word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification.
3. The provision of a toolkit (word2vecPM) that allows for the reproduction of the experiments and further improvements.
The strengths of this paper are:
1. The paper provides a thorough and systematic evaluation of different context types and representations, which is a significant contribution to the field of word embeddings.
2. The experiments are well-designed and comprehensive, covering a wide range of tasks and datasets.
3. The paper provides a clear and detailed analysis of the results, highlighting the importance of context representations and their impact on different tasks.
However, there are also some weaknesses:
1. The paper lacks hyper-parameter tuning, which makes it difficult to compare the performance of different methods and draw conclusive statements.
2. The paper presents strange and contradictory explanations for its results, which raises concerns about the validity of the findings.
3. The paper fails to properly cite and acknowledge relevant previous work, such as Lai et al. (2016) and Nayak et al.'s paper, which provides recommendations on hyperparameter tuning and experiment design.
4. The paper uses inconsistent classifiers for different tasks, with a neural BOW words classifier for text classification tasks and a simple linear classifier for sequence labeling tasks, without providing justification for this choice.
5. The paper's analysis of the results could be improved by performing factor analysis or other pattern mining techniques on the accuracy values to identify underlying patterns and relationships.
Some questions to the authors are:
1. How did you select the hyper-parameters for the experiments, and why did you not perform hyper-parameter tuning?
2. Can you provide more detailed explanations for the contradictory results and strange explanations presented in the paper?
3. How do you plan to address the lack of citation and acknowledgement of relevant previous work in the field?
4. Can you provide more justification for the choice of classifiers used for different tasks, and why you did not use a consistent classifier throughout the experiments?
5. Are there any plans to perform further analysis of the results using factor analysis or other pattern mining techniques to identify underlying patterns and relationships?