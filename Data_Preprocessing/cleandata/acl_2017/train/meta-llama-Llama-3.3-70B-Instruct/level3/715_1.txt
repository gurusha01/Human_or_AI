This paper presents a novel approach to zero pronoun resolution, a task that has traditionally relied on annotated data. The authors propose a method for automatically generating large-scale pseudo training data, which is then used to train an attention-based neural network model. The model is trained using a two-step approach, where the first step involves pre-training on the pseudo data and the second step involves fine-tuning on task-specific data.
The main contributions of this paper are:
1. The proposal of a novel approach to generating pseudo training data for zero pronoun resolution, which can be used to train a neural network model.
2. The development of a two-step training approach, which combines the benefits of large-scale pseudo data and task-specific data.
3. The achievement of state-of-the-art results on the OntoNotes 5.0 dataset, with an absolute improvement of 3.1% in F-score.
The strengths of this paper include:
1. The novelty of the approach, which addresses the limitation of relying on annotated data for zero pronoun resolution.
2. The effectiveness of the two-step training approach, which demonstrates the benefits of combining pseudo and task-specific data.
3. The thorough evaluation of the model, which includes an analysis of the errors and results.
The weaknesses of this paper include:
1. The lack of analysis of the errors and results, particularly in explaining the significant drop in performance in certain domains.
2. The limited discussion of the related work, which could be expanded to include more references to similar systems.
3. The potential limitations of the unknown words processing method, which may not be effective in all cases.
Questions to the authors:
1. Can you provide more details on the analysis of the errors and results, particularly in explaining the significant drop in performance in certain domains?
2. How do you plan to address the limitations of the unknown words processing method, and what alternative approaches could be explored?
3. Can you discuss the potential applications of this approach to other tasks, such as open domain question answering, and how it could be adapted to these tasks?