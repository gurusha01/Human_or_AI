This paper introduces a novel approach to semantic parsing using a neural sequence-to-sequence model with a 'key variable' memory component and discrete operations. The model, called Neural Symbolic Machine (NSM), consists of a 'programmer' that encodes natural language questions and produces programs, and an 'interpreter/computer' that executes these programs and stores intermediate values. The NSM is trained with weak supervision and policy gradients, using an iterative maximum likelihood approach to find good sequences of actions.
The main contributions of this work are: 
1. The introduction of a novel neural sequence-to-sequence model with a 'key variable' memory component, which allows the model to learn to represent and refer to program variables.
2. The use of a symbolic 'computer' that executes programs and provides code assistance, which helps to prune the search space and improve the efficiency of the model.
3. The development of an iterative maximum likelihood training procedure, which allows the model to learn from weak supervision and improve its performance over time.
The strengths of this paper are:
1. The NSM achieves state-of-the-art results on the WebQuestions dataset, outperforming previous models and demonstrating the effectiveness of the proposed approach.
2. The model is able to learn from weak supervision, which is a significant advantage over previous models that require strong supervision.
3. The use of a symbolic 'computer' and a 'key variable' memory component allows the model to handle complex semantic parsing tasks and to generalize to new domains.
The weaknesses of this paper are:
1. The model requires a large amount of computational resources and memory to train, which may limit its applicability to smaller datasets or less powerful machines.
2. The model may not perform well on datasets with limited or noisy supervision, as it relies heavily on the quality of the supervision signal.
3. The model may not be able to handle very complex or abstract semantic parsing tasks, as it relies on a symbolic 'computer' and a 'key variable' memory component that may not be able to capture the full range of human language and cognition.
Questions to the authors:
1. How does the model handle out-of-vocabulary words or entities that are not present in the training data?
2. Can the model be applied to other domains or tasks, such as natural language generation or dialogue systems?
3. How does the model compare to other state-of-the-art models in terms of efficiency and scalability?
Minor comments:
1. The paper could benefit from more detailed explanations of the model architecture and the training procedure.
2. The paper could include more examples of the programs generated by the model and the intermediate values stored in the 'key variable' memory component.
3. The paper could discuss the potential applications and limitations of the model in more detail.