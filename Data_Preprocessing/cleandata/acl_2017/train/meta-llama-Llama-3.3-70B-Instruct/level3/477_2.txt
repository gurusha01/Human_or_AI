This paper presents a comprehensive evaluation methodology for the task of ghostwriting rap lyrics, which captures complementary aspects of this task and its goals. The authors develop a manual evaluation method that assesses several key properties of generated verse, including fluency, coherence, and style matching, and create a dataset of authentic verse, manually annotated for style matching.
The main contributions of this paper are:
1. A comprehensive manual evaluation methodology for ghostwriting rap lyrics, which assesses fluency, coherence, and style matching.
2. A fully automated evaluation methodology that replicates the results of a previously available semi-automatic method.
3. A dataset of authentic verse, manually annotated for style matching, which can be used as a gold standard for future experiments.
The strengths of this paper are:
1. The authors provide a thorough and well-structured evaluation methodology that captures multiple aspects of the ghostwriting task.
2. The paper presents a comprehensive analysis of the results, including correlations between different metrics and insights into the performance of the LSTM model.
3. The authors make their dataset and annotation interface publicly available, which can facilitate future research in this area.
The weaknesses of this paper are:
1. The choice of 200-dimensional character embeddings is not well-motivated, and it is unclear whether this provides any benefits beyond the size of the vocabulary.
2. The paper's examination of character-level models on root-and-pattern morphology is limited by the use of unvocalised datasets, which lack the 'pattern' aspect of this typology.
3. The inclusion of reduplication as a separate linguistic typology is questionable, as it is a different phenomenon from the other three typologies discussed.
Questions to authors:
1. Can you provide more motivation for the choice of 200-dimensional character embeddings, and explore the impact of different embedding sizes on the results?
2. How do you plan to address the limitations of the current dataset and annotation interface, particularly with regards to root-and-pattern morphology and reduplication?
3. Can you provide more details on how the fully automated evaluation methodology can be used to evaluate other generative models, and what insights this can provide into the strengths and weaknesses of these models?