This paper proposes a novel approach to word representation learning (WRL) by incorporating sememe information, which are minimum semantic units of word meanings. The authors introduce three sememe-encoded models, including Simple Sememe Aggregation Model (SSA), Sememe Attention over Context Model (SAC), and Sememe Attention over Target Model (SAT), to learn representations of sememes, senses, and words. The models are evaluated on two tasks: word similarity and word analogy, and the results show that the proposed models outperform baselines, especially on word analogy.
The main contributions of this work are:
1. The proposal of a novel framework that incorporates sememe information into WRL, which can capture the semantic meanings of words more accurately.
2. The introduction of an attention-based method to automatically select appropriate word senses according to contexts, which can handle polysemy and improve WRL.
3. The evaluation of the proposed models on two tasks, which demonstrates the effectiveness of incorporating sememe information into WRL.
The strengths of this paper are:
1. The proposed models demonstrate substantial improvements over baselines on word similarity and word analogy tasks, highlighting the potential of incorporating sememe information for WRL.
2. The paper contributes to ongoing efforts in accounting for polysemy in WRL, proposing new ideas such as applying an attention scheme for soft word sense disambiguation.
3. The use of sememe information can provide more explicit explanations of both word and sense embeddings, which can be useful for many NLP tasks.
However, there are also some weaknesses:
1. The presentation and clarity of the paper are lacking, with important details poorly described or left out, requiring improvement for acceptance.
2. The evaluation on the word analogy task may be unfair due to the explicit encoding of semantic relations by sememes.
3. The paper stresses the importance of accounting for polysemy, but the evaluation tasks are context-independent, which may not fully capture the benefits of sense-specific representations.
4. The learning of sememe embeddings is unclear, and it is assumed that they pre-exist, which is important for understanding the subsequent models.
To improve this paper, the authors should:
1. Clarify the presentation and provide more details on the models and experiments.
2. Address the potential unfairness in the evaluation on the word analogy task.
3. Consider context-dependent evaluation tasks to fully capture the benefits of sense-specific representations.
4. Provide more information on the learning of sememe embeddings and their impact on the subsequent models.
Questions to authors:
1. How do the authors plan to address the potential unfairness in the evaluation on the word analogy task?
2. Can the authors provide more information on the learning of sememe embeddings and their impact on the subsequent models?
3. How do the authors plan to extend their work to other languages, considering the universality of the concept of sememes?