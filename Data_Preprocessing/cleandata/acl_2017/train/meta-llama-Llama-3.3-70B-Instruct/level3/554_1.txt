This paper presents a novel approach to enhancing coverage in Head-driven Phrase Structure Grammar (HPSG) parsing by exploring several robust parsing techniques. The main contributions of this work are: 
1. The proposal of a Bayesian learning approach for recurrent neural network language models, which offers a new perspective for the NLP community.
2. The evaluation of several robust parsing techniques, including bridging, Pacman, and PCFG approximation, which provides a comprehensive comparison of different methods.
3. The development of a hybrid approach that combines the strengths of precision grammar and robust parsing, which achieves high coverage and accuracy.
The strengths of this paper include:
1. The novelty of the proposed Bayesian learning approach, which has the potential to be of interest to the NLP community for various applications.
2. The thorough evaluation of different robust parsing techniques, which provides valuable insights into their strengths and weaknesses.
3. The development of a hybrid approach that achieves high coverage and accuracy, which is a significant contribution to the field of HPSG parsing.
However, there are also some weaknesses:
1. The evaluation methodology is inconsistent, comparing different architectures and learning algorithms across different tasks.
2. The claim that the performance gain comes from adding gradient noise and model averaging is not empirically justified and requires further experimentation to support.
3. The paper lacks comparison of the proposed Bayesian learning algorithms with existing dropout approaches, such as Gal's dropout, across all three tasks.
4. The sorting order of samples in the Bayesian learning algorithm is unclear, and alternative sampling methods, such as random selection, should be explored.
5. The computational efficiency of the proposed Bayesian learning algorithms compared to SGD with dropout is not evaluated, making it difficult to assess the trade-off between improvements and increased training times.
Questions to authors:
1. Can you provide more details on the Bayesian learning approach and how it is applied to HPSG parsing?
2. How do you plan to address the inconsistency in the evaluation methodology and provide a more comprehensive comparison of different techniques?
3. Can you provide more empirical evidence to support the claim that the performance gain comes from adding gradient noise and model averaging?
4. How do you plan to explore alternative sampling methods and evaluate their impact on the performance of the Bayesian learning algorithm?