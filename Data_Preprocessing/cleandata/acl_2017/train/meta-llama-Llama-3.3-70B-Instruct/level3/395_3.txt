This paper proposes an alternative to Bi-LSTMs for sequence labeling tasks, namely iterated dilated convolutional neural networks (ID-CNNs). The ID-CNN model shows significant speed gains over various recurrent models while maintaining similar F1 performance on CoNLL 2003 and OntoNotes 5.0 English NER datasets.
The main contributions of this work are:
1. The introduction of ID-CNNs as a fast and efficient feature extractor for sequence labeling tasks.
2. The demonstration of significant speed improvements over recurrent models, particularly when processing entire documents at a time.
3. The achievement of state-of-the-art performance on CoNLL 2003 and competitive performance on OntoNotes 5.0.
The strengths of this paper include:
1. The proposed ID-CNN model is able to efficiently aggregate broad context without losing resolution, making it suitable for sequence labeling tasks.
2. The model achieves significant speed improvements over recurrent models, making it a viable option for large-scale NLP tasks.
3. The paper provides a thorough evaluation of the model on two benchmark datasets, demonstrating its effectiveness in practice.
The weaknesses of this paper include:
1. The technical aspects of the paper raise some concerns, such as the claim of underestimation of probability of sense due to optimizing equation (2), which seems relatively easy to optimize.
2. The update formula in the paper is questioned, as most out-of-box optimizers can minimize a function f by maximizing -f, and the reward defined in the paper is negative.
3. The relation between Q-learning and DRL-Sense is unclear, as the horizon in the model is length of 1 and there is no transition between state-actions.
4. The computation of cross-entropy in equation (4) is unclear, as the variables do not have the same dimension.
5. The use of dropout for exploration is questioned, as dropout is often used for model regularization, and epsilon-greedy exploration may be more suitable.
Questions to authors:
1. Can you provide more details on the optimization process of equation (2) and how it affects the performance of the model?
2. How does the update formula in the paper differ from standard Q-learning updates, and what are the implications of this difference?
3. Can you clarify the relation between Q-learning and DRL-Sense, and how the model handles state-action transitions?
4. How do you compute the cross-entropy in equation (4), and what are the implications of this computation on the model's performance?
5. Can you provide more justification for the use of dropout for exploration, and how it compares to other exploration strategies such as epsilon-greedy?