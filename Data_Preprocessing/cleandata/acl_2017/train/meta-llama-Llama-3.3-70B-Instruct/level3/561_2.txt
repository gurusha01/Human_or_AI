This paper proposes a novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution, a task that has been heavily reliant on annotated data. The authors utilize a cloze-style reading comprehension neural network model and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real data. The experimental results show that the proposed approach significantly outperforms the state-of-the-art systems, achieving an absolute improvement of 3.1% F-score on the OntoNotes 5.0 data.
The main contributions of this paper are:
1. The proposal of a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution.
2. The utilization of a two-step training approach, which benefits from both large-scale pseudo training data and task-specific data.
3. The proposal of an attention-based neural network model for zero pronoun resolution, which achieves state-of-the-art performance.
The strengths of this paper are:
1. The proposed approach is able to generate large-scale pseudo training data, which can alleviate the problem of lack of annotated data.
2. The two-step training approach is effective in adapting the model to the real zero pronoun resolution task.
3. The attention-based neural network model is able to capture the context information and achieve state-of-the-art performance.
The weaknesses of this paper are:
1. The proposed approach is limited to zero pronoun resolution and may not be applicable to other tasks.
2. The two-step training approach may require careful tuning of hyperparameters to achieve optimal performance.
3. The attention-based neural network model may not be able to handle long-distance antecedents and unknown words effectively.
Questions to authors:
1. How do the authors plan to extend the proposed approach to other tasks, such as POS tagging and other sequence tagging tasks?
2. Can the authors provide more details on the hyperparameter tuning process for the two-step training approach?
3. How do the authors plan to improve the handling of long-distance antecedents and unknown words in the attention-based neural network model?