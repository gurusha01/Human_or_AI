Summary of the Paper
The paper presents a support system for automated scoring of short-answer tests, specifically designed for the new Japanese national center test for university entrance examinations. The system uses a combination of machine learning and human judgment to evaluate the semantic similarity between the model answers and the actual written answers. The authors propose a scoring approach based on the degree of fitness between the written answer and the model answer, using both superficial and semantic aspects. The system is designed to provide a tentative score, which can be revised by a human rater.
Main Contributions
1. Development of a support system for automated scoring of short-answer tests: The authors propose a novel approach to automated scoring, which combines machine learning and human judgment to evaluate the semantic similarity between the model answers and the actual written answers.
2. Use of random forests for machine learning: The authors use random forests to utilize many predictors effectively, which is suitable for test scoring that requires multiple classifications with values of 0-3 or 0-6.
3. Evaluation of the system's performance: The authors evaluate the system's performance using a trial examination for university entrance examinations and report promising results, with the differences between the scores being within one point for 70-90% of the data.
Strengths
1. Well-written paper with thorough related work: The paper provides a clear overview of the current state of automated scoring systems and establishes its position in the field.
2. Novel approach to automated scoring: The authors propose a unique approach to automated scoring, which combines machine learning and human judgment to evaluate the semantic similarity between the model answers and the actual written answers.
3. Promising results: The authors report promising results, with the differences between the scores being within one point for 70-90% of the data.
4. Useful for practical applications: The system has the potential to be useful for practical applications, such as supporting human raters in scoring short-answer tests.
Weaknesses
1. Unclear aspects of the method: The paper lacks clarity on how phrase representations are derived and the relationship between M^+ and M^- matrices.
2. Typographical errors: The paper contains several typographical errors, including incorrect wording, punctuation, and formatting issues.
3. Limited evaluation: The evaluation of the system's performance is limited to a trial examination for university entrance examinations, and more extensive evaluation is needed to fully assess the system's effectiveness.
4. Need for statistical significance analysis: The authors do not provide an analysis of the statistical significance of the observed improvements, which is necessary to fully understand the system's effectiveness.
Questions to Authors
1. Can you provide more details on how phrase representations are derived and the relationship between M^+ and M^- matrices?
2. How do you plan to address the issue of typographical errors in the paper?
3. Can you provide more extensive evaluation of the system's performance, including analysis of the statistical significance of the observed improvements?
4. How do you plan to extend the system to other subjects, such as Japanese literature, and what challenges do you anticipate in doing so?