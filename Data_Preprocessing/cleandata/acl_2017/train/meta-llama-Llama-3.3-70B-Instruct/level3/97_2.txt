Summary of the Paper
The paper presents an automated Japanese short-answer scoring and support system for the new National Center written test exams. The system uses a combination of natural language processing and machine learning techniques to evaluate the semantic similarity between model answers and actual written answers. The system provides a tentative score based on the scoring criteria and a prediction score using random forests, allowing human raters to review and revise the scores as needed.
Main Contributions
1. Development of an automated short-answer scoring system for Japanese: The paper proposes a system that can evaluate short answers in Japanese, which is a challenging task due to the language's complexity and nuances.
2. Use of random forests for prediction: The system utilizes random forests to predict scores based on the semantic similarity between model answers and actual written answers, which is a novel approach in this context.
3. Mixed-initiative user interface: The system allows human raters to review and revise the scores, providing a collaborative approach to scoring.
Strengths
1. Practical problem-solving: The paper tackles a practical problem of automated short-answer scoring, which is essential for the new National Center written test exams in Japan.
2. Innovative approach: The use of random forests and semantic similarity evaluation is a novel approach in this context, which shows promise for improving scoring accuracy.
3. Collaborative scoring: The mixed-initiative user interface allows human raters to review and revise scores, ensuring that the system is transparent and accountable.
Weaknesses
1. Lack of experiments and insights: The paper lacks comprehensive experiments and insights, reading like a system description, which makes it challenging to evaluate the system's effectiveness.
2. Insufficient evaluation: The paper does not provide sufficient evaluation of the system's performance, such as feature ablation studies and algorithm comparisons, which are essential for motivating the final system design.
3. Limited qualitative evaluation: The paper would benefit from a qualitative evaluation with a user study to assess the impact of the mixed-initiative user interface features on improved scores.
Questions to Authors
1. How do you plan to address the issue of limited human scores for supervised learning, which is essential for the system's accuracy?
2. Can you provide more details on the evaluation metrics used, such as Pearson/Spearman correlation and kappa scores, and how they are more appropriate than accuracy for evaluating ordinal human scores in this context?
3. How do you envision the system being used in practice, and what kind of support and training would be provided to human raters to ensure effective use of the system?