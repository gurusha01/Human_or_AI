This paper proposes a joint embedding model for words, phrases, and concepts, which is a straightforward and reasonable approach to finding a middle ground between treating phrases as atomic units and considering their compositionality. The authors describe a decent volume of work, including model development, a new evaluation dataset, and several evaluations and analyses. However, the evaluation methodology has some weaknesses.
The main contributions of this paper are:
1. The proposal of a joint embedding model for words, phrases, and concepts.
2. The creation of a new evaluation dataset for phrase and concept representations.
3. The achievement of state-of-the-art results on the story cloze task.
The strengths of this paper are:
1. The authors' approach to modeling phrases and concepts is novel and has the potential to improve performance on various NLP tasks.
2. The creation of a new evaluation dataset is a significant contribution to the field, as it provides a new resource for researchers to evaluate their models.
3. The authors' results on the story cloze task are impressive and demonstrate the effectiveness of their approach.
The weaknesses of this paper are:
1. The evaluation is limited to intrinsic tasks, mainly similarity and relatedness datasets, which have limited power in predicting the utility of embeddings in extrinsic tasks.
2. The similarity and relatedness evaluation datasets used are actually phrase similarity datasets, rather than concept similarity datasets, which affects their analysis and interpretation.
3. The medical concept evaluation datasets used are small and have limitations, such as low human annotator agreement or being based on single-word concepts, which makes their relevance to phrase and concept representations questionable.
4. The authors' hyperparameter tuning on the same datasets used for evaluation makes the reported results and analyses questionable, and the comparison to prior work is not entirely fair.
5. The authors' argument that their method is superior to prior work due to requiring less manual annotation is not strong, as they also use large manually-constructed ontologies.
Questions to authors:
1. How do the authors plan to address the limitations of their evaluation methodology, such as the use of phrase similarity datasets instead of concept similarity datasets?
2. Can the authors provide more details on the medical concept evaluation datasets used, such as the size of the datasets and the level of human annotator agreement?
3. How do the authors plan to improve the fairness of their comparison to prior work, such as by using a separate dataset for hyperparameter tuning?