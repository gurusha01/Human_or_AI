This paper presents a novel approach to Zero Pronoun Resolution in Chinese, achieving state-of-the-art results on several benchmark datasets. The proposed Gated-Attention (GA) reader integrates a multi-hop architecture with a novel attention mechanism, allowing the model to build query-specific representations of tokens in the document for accurate answer selection.
The main contributions of this work are: 
1. A novel procedure for generating large amounts of relevant data from unlabeled documents, which is then integrated into a neural network-based architecture.
2. The introduction of a gated-attention mechanism, which enables the model to attend to distinct salient aspects of the query at each layer, leading to accurate answer selections.
3. The achievement of state-of-the-art results on several large-scale benchmark datasets, with significant improvements over competitive baselines.
However, the paper also has some weaknesses. The linguistic motivation behind the paper is troublesome, and the authors' interpretation of the results needs to be more thoughtful. The generated pseudo-data may not be accurate representations of Zero Pronoun data and may instead encode selectional preferences. Additionally, the paper requires proofreading by a native English speaker to correct grammatical errors.
The evaluation setup should be revised to include system mentions and provide a more realistic estimation of performance. The authors should clarify the meaning of the dagger symbol used in the results table and report statistically significant improvements. The terminology used in the paper should be unified, and typos should be corrected. The references should be double-checked for capitalization errors.
Overall, this paper presents a promising approach to Zero Pronoun Resolution in Chinese, but requires further refinement and clarification to fully demonstrate its potential. 
Strengths:
1. The approach presented in the paper is novel and shows promising results, beating state-of-the-art in Zero Pronoun Resolution in Chinese.
2. The paper presents a novel procedure for generating large amounts of relevant data from unlabeled documents, which is then integrated into an NN-based architecture.
3. The gated-attention mechanism is a significant contribution, enabling the model to attend to distinct salient aspects of the query at each layer.
Weaknesses:
1. The linguistic motivation behind the paper is troublesome, and the authors' interpretation of the results needs to be more thoughtful.
2. The generated pseudo-data may not be accurate representations of Zero Pronoun data and may instead encode selectional preferences.
3. The paper requires proofreading by a native English speaker to correct grammatical errors.
Questions to Authors:
1. Can you provide more details on the linguistic motivation behind the paper and how it relates to the proposed approach?
2. How do you ensure that the generated pseudo-data accurately represents Zero Pronoun data and does not encode selectional preferences?
3. Can you provide more information on the evaluation setup and how it can be revised to include system mentions and provide a more realistic estimation of performance?