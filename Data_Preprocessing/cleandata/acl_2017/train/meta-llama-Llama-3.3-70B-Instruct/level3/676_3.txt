This paper proposes a novel approach to zero pronoun resolution by automatically generating large-scale pseudo training data and utilizing a two-step training mechanism with an attention-based neural network model. The main contributions of this work are: 
1. The proposal of a simple but effective method to generate large-scale pseudo training data for zero pronoun resolution, 
2. The introduction of a two-step training approach that benefits from both pseudo training data and task-specific data, and 
3. The application of an attention-based neural network model to zero pronoun resolution.
The strengths of this paper include: 
1. The proposed approach achieves state-of-the-art performance on the OntoNotes 5.0 dataset with an absolute improvement of 3.1% in F-score, 
2. The use of a two-step training mechanism allows the model to adapt to the task-specific data and improve performance, and 
3. The attention-based neural network model is effective in capturing the context information for zero pronoun resolution.
However, there are some weaknesses in this paper: 
1. The lack of comparison with previous related work, such as hierarchical softmax and differentiated softmax, makes it difficult to assess the proposed method's performance, 
2. The paper lacks a linguistic perspective on the proposed binary code prediction method, which raises questions about its naturalness and similarity to human word retrieval, and 
3. The promise of faster training speeds is not fully supported by the results, which show only modest speedups.
Some questions to the authors include: 
1. How does the proposed method handle cases where the correct antecedent is far away from the zero pronoun, and 
2. Can the authors provide more analysis on the effectiveness of the unknown words processing mechanism and its impact on the overall performance? 
Overall, this paper proposes a novel and effective approach to zero pronoun resolution, but there are some areas that need further improvement and analysis.