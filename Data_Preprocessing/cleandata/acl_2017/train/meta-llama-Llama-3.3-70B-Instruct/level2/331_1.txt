Summary of the Paper
The paper proposes a novel task of concept-map-based multi-document summarization (MDS), which aims to create a concept map that represents the most important content of a set of related documents. The authors introduce a new crowdsourcing scheme, called low-context importance annotation, to create a gold-standard corpus for this task. The corpus consists of 30 topics, each with around 40 source documents and a summarizing concept map. The authors also provide a baseline system and evaluation protocol to enable further research on this task.
Main Contributions
1. Proposal of a novel task of concept-map-based MDS
2. Introduction of a new crowdsourcing scheme, low-context importance annotation, to create a gold-standard corpus
3. Creation of a new corpus for concept-map-based MDS, which combines large clusters of heterogeneous documents
4. Provision of a baseline system and evaluation protocol for the task
Strengths
1. The paper proposes a novel and interesting task that has the potential to improve the way we summarize and represent information.
2. The authors introduce a new crowdsourcing scheme that can be used to create high-quality annotations for the task.
3. The corpus created by the authors is large and diverse, making it a valuable resource for researchers.
4. The baseline system and evaluation protocol provided by the authors can serve as a starting point for future research on this task.
Weaknesses
1. The paper assumes that the concept map representation is more useful than traditional text-based summaries, but this assumption is not thoroughly evaluated.
2. The crowdsourcing scheme used to create the corpus may not be scalable to larger datasets.
3. The baseline system provided by the authors is simple and may not be competitive with more advanced systems.
4. The evaluation protocol proposed by the authors may not be comprehensive enough to fully evaluate the quality of the generated concept maps.
Questions to Authors
1. How do the authors plan to evaluate the effectiveness of the concept map representation compared to traditional text-based summaries?
2. Can the crowdsourcing scheme be scaled up to larger datasets, and if so, how?
3. How do the authors plan to improve the baseline system to make it more competitive with other systems?
4. Are there any plans to extend the evaluation protocol to include more comprehensive metrics for evaluating the quality of the generated concept maps?