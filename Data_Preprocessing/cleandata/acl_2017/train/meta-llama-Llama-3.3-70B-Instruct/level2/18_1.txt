Summary of the Paper
This paper presents a novel neural network architecture called Attention-over-Attention Reader (AoA Reader) for cloze-style reading comprehension tasks. The AoA Reader aims to compute attentions not only for the document but also for the query side, which benefits from the mutual information between the document and query. The model uses a weighted sum of attention to get an attended attention over the document for final predictions. The authors also propose an N-best re-ranking strategy to further improve the performance.
Main Contributions
1. The authors propose a novel attention-over-attention mechanism that computes the importance of each individual document-level attention.
2. The AoA Reader outperforms various state-of-the-art systems by a large margin on public datasets, such as CNN and Children's Book Test.
3. The authors propose an N-best re-ranking strategy that significantly improves the performance of the AoA Reader.
Strengths
1. The AoA Reader is a simple yet effective model that achieves significant improvements over state-of-the-art systems.
2. The attention-over-attention mechanism is a novel and innovative approach that benefits from the mutual information between the document and query.
3. The N-best re-ranking strategy is a useful technique that can be applied to other reading comprehension tasks.
Weaknesses
1. The authors do not provide a detailed analysis of the attention-over-attention mechanism and its impact on the model's performance.
2. The model's performance may be sensitive to the choice of hyperparameters, such as the number of layers and the dimension of the embedding layer.
3. The authors do not provide a comparison with other re-ranking strategies, which may be useful for further improving the model's performance.
Questions to Authors
1. Can you provide a more detailed analysis of the attention-over-attention mechanism and its impact on the model's performance?
2. How does the choice of hyperparameters affect the model's performance, and are there any guidelines for selecting the optimal hyperparameters?
3. Can you provide a comparison with other re-ranking strategies, such as beam search or greedy search, to further improve the model's performance?