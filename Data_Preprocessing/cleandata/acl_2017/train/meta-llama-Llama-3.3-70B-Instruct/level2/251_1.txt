This paper presents a theoretical framework for understanding the compositionality of word embeddings learned using the Skip-Gram model. The authors provide a mathematical formalism for compositionality and show that the Skip-Gram model learns information-theoretically optimal word embeddings. The main contributions of this paper are:
1. Theoretical justification for additive compositionality: The authors prove that under certain assumptions, the composition operator for the Skip-Gram model reduces to vector addition, which explains the success of vector calculus for solving word analogies.
2. Connection to Sufficient Dimensionality Reduction (SDR) framework: The authors establish a connection between the Skip-Gram model and the SDR framework, showing that the parameters of Skip-Gram models can be modified to obtain the parameters of SDR models by adding information on symbol frequencies.
3. Information-theoretic optimality of Skip-Gram embeddings: The authors argue that the Skip-Gram model learns optimal word embeddings in the sense of Globerson and Tishby, which preserves the maximal mutual information between any pair of random variables consistent with the observed co-occurrence matrix.
The strengths of this paper are:
1. Rigorous theoretical framework: The authors provide a rigorous mathematical framework for understanding the compositionality of word embeddings, which is a significant contribution to the field.
2. Insightful connection to SDR framework: The connection to the SDR framework provides a new perspective on the Skip-Gram model and its optimality.
3. Implications for practical applications: The paper has implications for practical applications, such as using Skip-Gram approximations in domains where the training information consists of co-occurrence statistics.
The weaknesses of this paper are:
1. Assumptions and limitations: The authors make several assumptions, such as uniform word frequency, which may not hold in practice. The limitations of these assumptions should be further discussed.
2. Lack of empirical evaluation: The paper lacks empirical evaluation of the theoretical results, which would strengthen the claims made by the authors.
3. Complexity of the mathematical framework: The mathematical framework presented in the paper may be challenging to follow for some readers, and additional explanations or simplifications could be helpful.
Overall, this paper presents a significant contribution to the field of natural language processing, providing a rigorous theoretical framework for understanding the compositionality of word embeddings. While there are some weaknesses and limitations, the paper has the potential to inspire further research and improve our understanding of word embeddings. 
Questions to authors:
1. Can you provide more insights into the implications of the connection between Skip-Gram and SDR frameworks for practical applications?
2. How do you plan to address the limitations of the assumptions made in the paper, such as uniform word frequency?
3. Can you provide empirical evaluations of the theoretical results presented in the paper to strengthen the claims made?