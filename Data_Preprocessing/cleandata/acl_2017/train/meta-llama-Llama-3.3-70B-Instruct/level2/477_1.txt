Summary of the Paper
The paper presents a systematic comparison of word representation models with different levels of morphological awareness, across languages with different morphological typologies. The authors investigate the effectiveness of character-level models, morpheme-level models, and models that combine both, using various composition functions such as addition, bi-LSTMs, and CNNs. The experiments are conducted on ten languages, including fusional, agglutinative, root-and-pattern, and reduplication languages.
Main Contributions
1. The paper provides a comprehensive comparison of word representation models across different languages and morphological typologies.
2. The authors show that character-level models are effective for many languages, but do not match the predictive accuracy of models with explicit knowledge of morphology.
3. The paper highlights the importance of considering morphological typology when designing word representation models, as the effectiveness of different models varies across languages.
Strengths
1. The paper presents a thorough and systematic comparison of word representation models, providing valuable insights into their strengths and weaknesses.
2. The authors use a range of languages and morphological typologies, making the results more generalizable.
3. The paper provides a detailed analysis of the results, including qualitative analysis of the learned representations.
Weaknesses
1. The paper relies heavily on perplexity as the evaluation metric, which may not capture all aspects of language modeling performance.
2. The authors do not provide a detailed analysis of the computational resources required for each model, which could be an important consideration in practice.
3. Some of the results, such as the effectiveness of character-level models for reduplication languages, are not fully explained and require further investigation.
Questions to Authors
1. Can you provide more details on the computational resources required for each model, including training time and memory usage?
2. How do you plan to address the limitations of character-level models, such as their inability to capture the meaning of root morphemes?
3. Can you provide more insights into the effectiveness of semi-supervised learning from partially annotated data, and how this approach can be applied in practice?