This paper presents a novel approach to Abstract Meaning Representation (AMR) parsing and realization using sequence-to-sequence (seq2seq) models. The main claims of the paper are that seq2seq models can be effectively used for AMR parsing and realization, and that a paired training procedure can significantly improve the performance of these models.
The contributions of this work are:
1. Effective use of seq2seq models for AMR parsing and realization: The paper demonstrates that seq2seq models can be used to achieve competitive results in AMR parsing and realization, outperforming state-of-the-art systems in some cases.
2. Paired training procedure: The paper introduces a novel paired training procedure that allows the seq2seq models to learn from both labeled and unlabeled data, significantly improving their performance.
3. Robustness to linearization orders: The paper shows that the seq2seq models are robust to different linearization orders, which is an important property for AMR realization.
The strengths of this paper are:
1. Strong experimental results: The paper presents strong experimental results, demonstrating the effectiveness of the proposed approach on both AMR parsing and realization tasks.
2. Novel paired training procedure: The paired training procedure is a novel contribution that allows the seq2seq models to learn from both labeled and unlabeled data.
3. Robustness to linearization orders: The paper's demonstration of the seq2seq models' robustness to different linearization orders is an important contribution to the field.
The weaknesses of this paper are:
1. Limited analysis of error types: While the paper presents some error analysis, it would be beneficial to have a more detailed analysis of the types of errors made by the seq2seq models.
2. Limited comparison to other systems: The paper could benefit from a more comprehensive comparison to other state-of-the-art systems, including those that use different architectures or training procedures.
3. Lack of discussion on potential applications: The paper could benefit from a discussion on the potential applications of the proposed approach, including its potential use in downstream NLP tasks.
Overall, this paper presents a significant contribution to the field of AMR parsing and realization, and its results have important implications for the development of more effective NLP systems. 
Questions to authors:
1. Can you provide more details on the error analysis, including the types of errors made by the seq2seq models and their frequencies?
2. How do you plan to extend this work to other NLP tasks, such as machine translation or question answering?
3. Can you provide more details on the paired training procedure, including the specific hyperparameters used and the effect of different hyperparameters on the performance of the seq2seq models?