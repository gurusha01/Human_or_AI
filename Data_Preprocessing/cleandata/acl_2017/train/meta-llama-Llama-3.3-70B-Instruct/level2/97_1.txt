Summary of the Paper
The paper presents an automated Japanese short-answer scoring and support machine for new National Center written test exams. The system generates automated scores based on evaluation criteria and allows human raters to revise them. The approach uses random forests to utilize many predictors effectively and determines semantic similarity between model answers and actual written answers.
Main Contributions
1. Development of an automated short-answer scoring system: The paper proposes a system that can automatically score short-answer questions with a high degree of accuracy, reducing the need for human raters.
2. Use of random forests for predictor selection: The system uses random forests to select the most effective predictors for scoring, allowing for a high degree of accuracy and flexibility.
3. Integration of human raters for revision and validation: The system allows human raters to revise and validate the automated scores, ensuring that the final scores are accurate and reliable.
Strengths
1. High accuracy: The system demonstrates a high degree of accuracy, with differences between automated and human scores within one point for 70-90% of the data.
2. Flexibility: The system can be applied to various subjects, including social studies and Japanese literature, and can handle different types of questions and scoring criteria.
3. Efficient use of human raters: The system reduces the need for human raters to score every question, allowing them to focus on revising and validating the automated scores.
Weaknesses
1. Limited data: The system requires a large amount of human-scored data to train and validate the model, which can be time-consuming and expensive to obtain.
2. Dependence on scoring criteria: The system relies on well-defined scoring criteria, which can be difficult to develop and may not always be available.
3. Limited handling of semantic nuances: The system may struggle to handle questions that require a deep understanding of semantic nuances, such as idioms or figurative language.
Questions to Authors
1. How do you plan to address the issue of limited data and ensure that the system can be trained and validated on a large and diverse set of questions and scoring criteria?
2. Can you provide more details on how the system handles questions that require a deep understanding of semantic nuances, and how you plan to improve its performance in these areas?
3. How do you envision the system being used in practice, and what kind of support and training would be required for human raters to effectively use the system?