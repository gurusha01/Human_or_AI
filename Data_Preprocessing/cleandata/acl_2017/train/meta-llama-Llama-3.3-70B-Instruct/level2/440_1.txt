Summary of the Paper
The paper proposes a new A* Combinatory Categorial Grammar (CCG) parsing model that decomposes the probability of a CCG tree into local factors of CCG categories and its dependency structure. The model uses bi-directional LSTMs to predict the supertag and head of each word, and a bilinear transformation to model the dependency structure. The paper also introduces a new dependency conversion rule, HEADFIRST, which is simpler and more effective than the existing LEWISRULE. The model is evaluated on English and Japanese CCG parsing tasks, achieving state-of-the-art results.
Main Contributions
1. A new CCG parsing model that decomposes the probability of a CCG tree into local factors of CCG categories and its dependency structure.
2. A bi-LSTM architecture that predicts the supertag and head of each word, and a bilinear transformation to model the dependency structure.
3. A new dependency conversion rule, HEADFIRST, which is simpler and more effective than the existing LEWISRULE.
Strengths
1. The model achieves state-of-the-art results on English and Japanese CCG parsing tasks.
2. The use of bi-LSTMs and bilinear transformation allows for efficient and effective modeling of the dependency structure.
3. The HEADFIRST dependency conversion rule is simpler and more effective than the existing LEWISRULE.
Weaknesses
1. The model requires a large amount of training data to achieve good results.
2. The use of bi-LSTMs and bilinear transformation may require significant computational resources.
3. The model may not perform well on languages with very different grammatical structures.
Questions to Authors
1. How does the model handle cases where the dependency structure is ambiguous or unclear?
2. Can the model be extended to handle other types of grammatical structures, such as semantic role labeling?
3. How does the model compare to other state-of-the-art CCG parsing models in terms of efficiency and accuracy?