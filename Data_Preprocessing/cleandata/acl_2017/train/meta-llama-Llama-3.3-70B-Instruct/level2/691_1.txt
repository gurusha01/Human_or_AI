Summary of the Paper
The paper proposes a novel approach to word embeddings, where each word token is represented as a context-sensitive distribution over WordNet synsets. This approach acknowledges the semantic ambiguity of word types and allows for more accurate representations of words in different contexts. The authors evaluate their approach on the task of prepositional phrase (PP) attachment prediction and show that it outperforms standard type-level word embeddings.
Main Contributions
1. Context-sensitive token embeddings: The authors propose a method to learn context-sensitive token embeddings by estimating a distribution over WordNet synsets for each word token.
2. WordNet-grounded embeddings: The authors use WordNet to ground the word embeddings, allowing for the incorporation of semantic relationships between words.
3. Improved PP attachment prediction: The authors demonstrate that their approach improves the accuracy of PP attachment prediction, outperforming standard type-level word embeddings.
Strengths
1. Effective use of WordNet: The authors demonstrate the effectiveness of using WordNet to improve word embeddings and PP attachment prediction.
2. Context-sensitive representations: The authors' approach allows for context-sensitive representations of words, which is essential for accurate PP attachment prediction.
3. Improved performance: The authors show that their approach outperforms standard type-level word embeddings on the task of PP attachment prediction.
Weaknesses
1. Limited evaluation: The authors only evaluate their approach on the task of PP attachment prediction, and it is unclear how well it would perform on other NLP tasks.
2. Dependence on WordNet: The authors' approach relies heavily on WordNet, which may not always have sufficient coverage or accuracy.
3. Computational complexity: The authors' approach may be computationally expensive, particularly for large datasets.
Questions to Authors
1. How do the authors plan to extend their approach to other NLP tasks, such as semantic role labeling or question answering?
2. How do the authors plan to address the limitations of WordNet, such as its coverage and accuracy?
3. What are the computational requirements of the authors' approach, and how can it be optimized for large-scale datasets?