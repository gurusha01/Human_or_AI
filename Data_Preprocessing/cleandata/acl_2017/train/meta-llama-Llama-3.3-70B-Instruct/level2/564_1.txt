Summary of the Paper
The paper presents Grid Beam Search (GBS), an algorithm that extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model that generates a sequence of text, and is particularly useful in scenarios where additional information is available at inference time. The authors demonstrate the feasibility and flexibility of GBS by conducting experiments on Neural Interactive-Predictive Translation and Domain Adaptation for Neural Machine Translation.
Main Contributions
1. Grid Beam Search Algorithm: The paper introduces a novel decoding algorithm that allows for the specification of subsequences that are required to be present in a model's output.
2. Lexically Constrained Decoding: The authors demonstrate the effectiveness of GBS in incorporating arbitrary subsequences into the output of any model that generates output sequences token-by-token.
3. Domain Adaptation: The paper shows that GBS can be used to adapt a general domain model to a new domain without any retraining, by using a domain-specific terminology to generate target-side constraints.
Strengths
1. Flexibility: GBS can be used with any model that generates output sequences token-by-token, making it a versatile algorithm for a wide range of text generation tasks.
2. Effectiveness: The authors demonstrate significant improvements in translation quality using GBS, particularly in interactive scenarios and domain adaptation tasks.
3. Efficiency: The paper discusses ways to improve the efficiency of GBS, such as parallelizing the search process and keeping the beam size small.
Weaknesses
1. Computational Complexity: The paper notes that the runtime complexity of a naive implementation of GBS is O(ktc), which can be computationally expensive for large models and datasets.
2. Limited Evaluation: The authors only evaluate GBS on Neural Machine Translation tasks, and do not explore its application to other text generation tasks such as automatic summarization or dialog generation.
3. Lack of Comparison to Baselines: The paper does not provide a comprehensive comparison of GBS to other decoding algorithms or baselines, which makes it difficult to assess its relative performance.
Questions to Authors
1. How do the authors plan to address the computational complexity of GBS, and what optimizations can be made to improve its efficiency?
2. Can the authors provide more details on the implementation of GBS, particularly with regards to the generate, start, and continue functions?
3. How do the authors plan to evaluate GBS on other text generation tasks, and what modifications may be necessary to adapt the algorithm to these tasks?