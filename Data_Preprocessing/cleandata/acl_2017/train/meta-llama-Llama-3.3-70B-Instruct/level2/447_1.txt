Summary of the Paper
This paper investigates the use of discourse structure, as defined by Rhetorical Structure Theory (RST), to improve text categorization. The authors propose a recursive neural network model that uses a discourse dependency tree to compute a representation of the text, focusing on salient content from the perspective of both RST and the task. The model is evaluated on five datasets, including sentiment analysis, framing dimensions in news articles, congressional floor debates, movie reviews, and congressional bill corpus.
Main Contributions
1. Discourse structure benefits text categorization: The authors demonstrate that using discourse structure, even with an imperfect parser, can improve text categorization performance on several tasks.
2. Unlabeled model outperforms prior work: The authors' UNLABELED model, which uses a discourse dependency tree without relation labels, outperforms prior work on four out of five datasets.
3. Attention mechanism: The authors propose a new attention mechanism that is inspired by RST's lack of competition for salience among satellites, which is shown to be effective in practice.
Strengths
1. Effective use of discourse structure: The authors demonstrate the benefits of using discourse structure for text categorization, which is a novel and interesting approach.
2. State-of-the-art results: The authors achieve state-of-the-art results on several datasets, which demonstrates the effectiveness of their approach.
3. Well-motivated attention mechanism: The authors propose a well-motivated attention mechanism that is inspired by RST, which is a key contribution of the paper.
Weaknesses
1. Overparameterization: The authors note that their FULL model is overparameterized for smaller datasets, which can lead to poor performance.
2. Limited analysis of discourse parsing performance: The authors only analyze the effect of discourse parsing performance on one dataset, which limits the generality of their findings.
3. No comparison to other linguistic structures: The authors only compare their approach to a flat document structure, which limits the comparison to other linguistic structures, such as syntactic dependency trees.
Questions to Authors
1. How do the authors plan to address the overparameterization issue in their FULL model?
2. Can the authors provide more analysis on the effect of discourse parsing performance on other datasets?
3. How do the authors think their approach would compare to using other linguistic structures, such as syntactic dependency trees?