Summary of the Paper
The paper proposes a neural encoder-decoder transition-based parser for Minimal Recursion Semantics (MRS) and Abstract Meaning Representation (AMR) graphs. The parser uses a stack-based embedding feature and predicts graphs jointly with unlexicalized predicates and their token alignments. The authors evaluate their parser on DMRS, EDS, and AMR graphs and show that it outperforms attention-based baselines and is an order of magnitude faster than a high-precision grammar-based parser.
Main Contributions
1. Development of a fast and robust parser for full MRS-based semantic graphs: The authors propose a neural encoder-decoder transition-based parser that can parse sentences to linguistically expressive semantic representations.
2. Introduction of a stack-based model for parsing semantic graphs: The authors extend the hard attention model to include features based on the transition system stack, which improves the performance of the parser.
3. Application of the parser to AMR parsing: The authors apply their parser to AMR parsing and show that it outperforms existing neural AMR parsers.
Strengths
1. Improved performance over attention-based baselines: The authors show that their parser outperforms attention-based baselines on DMRS, EDS, and AMR graphs.
2. Fast and efficient parsing: The authors demonstrate that their parser is an order of magnitude faster than a high-precision grammar-based parser.
3. Robustness to data sparsity: The authors show that their parser can handle data sparsity by predicting unlexicalized predicates and using a dictionary lookup based on predicted spans.
Weaknesses
1. Limited comparison to state-of-the-art AMR parsers: The authors only compare their parser to a few existing neural AMR parsers and do not evaluate it against state-of-the-art parsers that use external resources.
2. Need for further evaluation on larger datasets: The authors only evaluate their parser on a limited dataset and do not provide results on larger datasets.
3. Lack of analysis on the impact of different hyperparameters: The authors do not provide an analysis of the impact of different hyperparameters on the performance of their parser.
Questions to Authors
1. How do the authors plan to address the limited comparison to state-of-the-art AMR parsers?
2. Can the authors provide more details on the hyperparameter tuning process and the impact of different hyperparameters on the performance of the parser?
3. How do the authors plan to evaluate their parser on larger datasets and more diverse linguistic phenomena?