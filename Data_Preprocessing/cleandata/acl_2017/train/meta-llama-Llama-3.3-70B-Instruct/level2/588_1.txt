Summary of the Paper
This paper proposes a new task called rare entity prediction, which involves predicting missing entities in web articles by leveraging external knowledge in the form of lexical resources. The authors introduce a new dataset, Wikilinks Rare Entity Prediction, and propose two models, Double Encoder (DOUBENC) and Hierarchical Double Encoder (HIERENC), to solve this task. The models use recurrent neural networks (RNNs) with long short-term memory (LSTM) units to incorporate external knowledge from Freebase descriptions. The results show that models that use external knowledge outperform those that do not, with HIERENC achieving the best performance.
Main Contributions
1. Introduction of the rare entity prediction task: The authors propose a new task that requires models to predict missing entities in web articles using external knowledge.
2. Development of the Wikilinks Rare Entity Prediction dataset: The authors create a new dataset that is specifically designed for the rare entity prediction task.
3. Proposal of the Double Encoder and Hierarchical Double Encoder models: The authors introduce two models that use RNNs with LSTM units to incorporate external knowledge from Freebase descriptions.
Strengths
1. Novel task proposal: The authors propose a new task that is relevant to the field of natural language processing (NLP) and requires the use of external knowledge.
2. Effective use of external knowledge: The authors demonstrate the importance of using external knowledge in the form of lexical resources to improve performance on the rare entity prediction task.
3. Well-designed models: The authors propose two models that are well-suited to the task and achieve good performance.
Weaknesses
1. Limited context window size: The authors only consider a limited context window size, which may not capture all the relevant information for predicting missing entities.
2. Simple baseline models: The authors use simple baseline models that do not fully capture the complexity of the task.
3. Room for improvement: The best-performing model, HIERENC, still achieves an accuracy of only 56.6%, indicating that there is room for improvement.
Questions to Authors
1. How do the authors plan to address the issue of limited context window size in future work?
2. Can the authors provide more details on how the Freebase descriptions are used as external knowledge in the models?
3. How do the authors plan to improve the performance of the models, given that the best-performing model still has a relatively low accuracy?