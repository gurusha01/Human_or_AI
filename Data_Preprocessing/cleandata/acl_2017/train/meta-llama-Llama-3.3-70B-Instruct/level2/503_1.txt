This paper presents a significant contribution to the field of natural language processing (NLP) by introducing Regular Graph Languages (RGLs) as a probabilistic and intersectible family of graph languages. The authors prove that RGLs are closed under intersection and provide an efficient parsing algorithm with a linear runtime in the size of the input graph.
The main claims of the paper are:
1. RGLs are a subfamily of both Hyperedge Replacement Languages (HRL) and Monadic Second Order Languages (MSOL), inheriting their properties of probabilistic interpretation and closure under intersection, respectively.
2. The authors prove that RGLs are closed under intersection, which is a crucial property for many NLP applications.
3. A parsing algorithm for RGLs is presented, which is linear in the size of the input graph and exploits the structure of RGG productions.
The strengths of the paper are:
1. The introduction of RGLs as a new family of graph languages that combines the benefits of HRL and MSOL, making it a valuable tool for NLP applications.
2. The proof of closure under intersection, which is a significant contribution to the field.
3. The development of an efficient parsing algorithm, which is essential for practical applications.
The weaknesses of the paper are:
1. The expressivity of RGLs might be limited compared to more general graph languages like HRL, which could restrict their applicability to certain NLP tasks.
2. The parsing algorithm relies on a normal ordering of the edges in the right-hand side of each production, which might not be possible for all RGGs.
3. The paper could benefit from more experimental evaluations and comparisons with other graph language formalisms to demonstrate the practical effectiveness of RGLs.
Questions to the authors:
1. How do the authors plan to address the potential limitations of RGLs in terms of expressivity, and what are the trade-offs between expressivity and efficiency?
2. Can the parsing algorithm be extended to handle more general graph languages, such as HRL or MSOL?
3. What are the plans for future work, and how do the authors envision the application of RGLs in real-world NLP tasks?