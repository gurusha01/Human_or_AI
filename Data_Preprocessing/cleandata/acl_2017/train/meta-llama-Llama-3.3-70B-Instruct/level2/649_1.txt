Summary of the Paper
The paper proposes an automatic dialogue evaluation model (ADEM) that learns to predict human-like scores for input responses. The model is trained on a dataset of human response scores and uses a hierarchical recurrent neural network (RNN) encoder to capture semantic similarity beyond word overlap statistics. The authors evaluate ADEM on a test set and show that it correlates significantly with human judgements at both the utterance and system levels, outperforming existing word-overlap metrics such as BLEU.
Main Contributions
1. ADEM Model: The paper introduces a novel evaluation model that learns to predict human-like scores for input responses, addressing the challenge of automatic dialogue evaluation.
2. Improved Correlation with Human Judgements: ADEM shows significant correlation with human judgements at both the utterance and system levels, demonstrating its effectiveness in evaluating dialogue responses.
3. Generalization to Unseen Models: The model generalizes well to evaluating new models, whose responses were unseen during training, making it a strong first step towards effective automatic dialogue response evaluation.
Strengths
1. Novel Approach: The paper proposes a new approach to automatic dialogue evaluation, which addresses the limitations of existing word-overlap metrics.
2. Strong Experimental Results: The authors provide thorough experimental results, demonstrating the effectiveness of ADEM in evaluating dialogue responses.
3. Generalization Ability: The model's ability to generalize to unseen models makes it a valuable tool for evaluating dialogue systems.
Weaknesses
1. Limited Domain: The paper focuses on non-task-oriented dialogue systems, and it is unclear how ADEM would perform in task-oriented domains.
2. Bias towards Generic Responses: The model may favor generic responses, which are often rated highly by humans due to their appropriateness for many contexts.
3. Lack of Interpretability: The paper could benefit from a more detailed analysis of the model's decisions and how they relate to human judgements.
Questions to Authors
1. How do you plan to address the bias towards generic responses in future work?
2. Can you provide more insight into the model's decisions and how they relate to human judgements?
3. How do you envision ADEM being used in practice, and what are the potential applications of this technology?