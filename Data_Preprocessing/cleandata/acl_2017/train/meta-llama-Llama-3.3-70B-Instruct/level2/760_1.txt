Summary of the Paper
The paper proposes a novel approach to reading text non-sequentially using a recurrent neural network (RNN) that learns to jump over irrelevant information. The model, called LSTM-Jump, uses a policy gradient method to train the network to make discrete jumping decisions. The authors evaluate the model on four different tasks, including number prediction, sentiment analysis, news article classification, and automatic question answering, and demonstrate that it is up to 6 times faster than the standard sequential LSTM model while maintaining the same or even better accuracy.
Main Contributions
1. Non-sequential text reading: The paper introduces a new approach to reading text that allows the model to skip over irrelevant information, reducing the computational cost and improving the efficiency of the model.
2. LSTM-Jump model: The authors propose a novel RNN architecture that uses a policy gradient method to train the network to make discrete jumping decisions.
3. Experimental evaluation: The paper provides a thorough experimental evaluation of the LSTM-Jump model on four different tasks, demonstrating its effectiveness and efficiency.
Strengths
1. Improved efficiency: The LSTM-Jump model is significantly faster than the standard sequential LSTM model, making it more suitable for applications where speed is crucial.
2. Maintained accuracy: The model maintains the same or even better accuracy than the standard sequential LSTM model, demonstrating its effectiveness in various tasks.
3. Flexibility: The LSTM-Jump model can be applied to different levels of text jumping, from character to word and to sentence, making it a versatile approach.
Weaknesses
1. Training complexity: The paper notes that training the LSTM-Jump model using policy gradient methods can be challenging, and the authors had to use a curriculum training scheme to achieve convergence.
2. Limited interpretability: The model's jumping behavior may be difficult to interpret, making it challenging to understand why the model is making certain decisions.
Questions to Authors
1. How do the authors plan to address the training complexity of the LSTM-Jump model, and what strategies can be used to improve its interpretability?
2. Can the authors provide more details on the curriculum training scheme used to train the model, and how it can be applied to other tasks?
3. How do the authors plan to extend the LSTM-Jump model to more complex tasks, such as bidirectional jumping or incorporating advanced neural networks like memory networks or attention mechanisms?