Summary of the Paper
The paper proposes introducing ngrams into four word representation methods: SGNS, GloVe, PPMI, and its SVD factorization. The authors conduct comprehensive experiments on word analogy and similarity tasks, demonstrating that the improved word representations are learned from ngram co-occurrence statistics. They also show that the trained ngram representations are useful in many aspects, such as finding antonyms and collocations. Additionally, a novel approach to building co-occurrence matrices is proposed to alleviate the hardware burden brought by ngrams.
Main Contributions
1. Introduction of ngrams into four word representation methods, resulting in improved word representations.
2. Demonstration of the effectiveness of ngram-based models on word analogy and similarity tasks.
3. Proposal of a novel approach to building co-occurrence matrices, enabling ngram-based models to run on cheap hardware.
Strengths
1. The paper provides a thorough evaluation of the proposed ngram-based models on various tasks, demonstrating their effectiveness.
2. The introduction of ngrams into word representation methods is a novel and interesting idea, with potential applications in many NLP tasks.
3. The proposed approach to building co-occurrence matrices is efficient and scalable, making it possible to run ngram-based models on large datasets.
Weaknesses
1. The paper could benefit from a more detailed analysis of the relationships between ngrams and various hyperparameters, as the default hyperparameter setting may not be optimal for ngram-based models.
2. The results on GloVe and SVD models are not as impressive as those on SGNS, which may indicate that the proposed approach is not equally effective for all word representation methods.
3. The paper could provide more insights into the semantic meanings and syntactic patterns captured by the trained ngram embeddings, which would help to better understand their potential applications.
Questions to Authors
1. How do the authors plan to further explore the relationships between ngrams and hyperparameters to improve the performance of ngram-based models?
2. Can the authors provide more insights into the differences in performance between SGNS and other word representation methods, such as GloVe and SVD?
3. How do the authors envision the potential applications of ngram embeddings in various NLP tasks, and what future work do they plan to conduct in this area?