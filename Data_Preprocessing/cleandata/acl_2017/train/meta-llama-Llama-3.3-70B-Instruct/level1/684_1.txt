This paper presents a novel approach to answering cloze-style questions over documents, introducing the Gated-Attention (GA) Reader. The GA Reader combines a multi-hop architecture with a novel attention mechanism, allowing the model to build query-specific representations of tokens in the document for accurate answer selection.
The main contributions of this work are:
1. Gated-Attention Mechanism: The proposed GA module enables the model to attend to individual components of the token embeddings in the document, allowing for fine-grained information filtering during the multi-step reasoning process.
2. Multi-Hop Architecture: The GA Reader performs multiple hops over the document, mimicking the multi-step comprehension process of human readers, and has shown promising results in several recent models for text comprehension.
3. State-of-the-Art Results: The GA Reader achieves state-of-the-art performance on several large-scale benchmark datasets, with more than 4% improvements over competitive baselines.
The strengths of this submission are:
1. Novel Attention Mechanism: The proposed GA module is a significant contribution, allowing the model to attend to individual components of the token embeddings in the document.
2. State-of-the-Art Results: The GA Reader achieves state-of-the-art performance on several large-scale benchmark datasets, demonstrating the effectiveness of the proposed approach.
3. Comprehensive Evaluation: The paper provides a comprehensive evaluation of the GA Reader, including an ablation study and attention visualization, which helps to understand the strengths and weaknesses of the model.
The weaknesses of this submission are:
1. Lack of Theoretical Justification: The paper lacks a theoretical justification for the proposed GA mechanism, which is based on empirical results.
2. Dependence on Pretrained Word Embeddings: The GA Reader relies heavily on pretrained word embeddings, which may not be available for all languages or domains.
3. Computational Complexity: The multi-hop architecture and GA mechanism may increase the computational complexity of the model, which could be a limitation for large-scale applications.
Questions to Authors:
1. Can you provide a theoretical justification for the proposed GA mechanism, or is it purely based on empirical results?
2. How does the GA Reader perform on languages or domains where pretrained word embeddings are not available?
3. What are the computational requirements of the GA Reader, and how does it compare to other state-of-the-art models in terms of computational complexity?