This paper presents a comprehensive evaluation of automatic metrics for Natural Language Generation (NLG) systems, with a focus on end-to-end data-driven approaches. The authors investigate a wide range of word-based and grammar-based metrics, including state-of-the-art ones, and demonstrate that they only weakly reflect human judgments of system outputs.
The main contributions of this work are:
1. A thorough evaluation of automatic metrics for NLG systems, including word-based and grammar-based metrics, and their correlation with human judgments.
2. The introduction of a new metric, RAINBOW, which combines the strengths of different automatic metrics and achieves a higher correlation with human judgments.
3. A detailed error analysis that highlights the limitations of automatic metrics in distinguishing between system outputs of medium and good quality.
The strengths of this paper are:
1. The comprehensive evaluation of automatic metrics for NLG systems, which provides a thorough understanding of their strengths and limitations.
2. The introduction of a new metric, RAINBOW, which shows promise in improving the correlation with human judgments.
3. The detailed error analysis, which provides insights into the limitations of automatic metrics and highlights the need for further research in this area.
The weaknesses of this paper are:
1. The limited scope of the evaluation, which focuses on end-to-end data-driven NLG systems and may not be generalizable to other types of NLG systems.
2. The reliance on crowd-sourced human judgments, which may be noisy and biased.
3. The lack of a clear explanation for the poor performance of automatic metrics in distinguishing between system outputs of medium and good quality.
Questions to authors:
1. How do the authors plan to address the limitations of the evaluation scope and ensure that the results are generalizable to other types of NLG systems?
2. What measures can be taken to improve the quality and reliability of crowd-sourced human judgments?
3. Can the authors provide more insights into the reasons behind the poor performance of automatic metrics in distinguishing between system outputs of medium and good quality?