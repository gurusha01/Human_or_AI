This paper presents a novel neural model for morphological inflection generation, which employs a hard attention mechanism to directly model the nearly-monotonic alignment between the input and output character sequences. The main contributions of this work are:
1. Hard attention model for morphological inflection generation: The authors propose a hard attention model that directly models the monotonic alignment between the input and output character sequences, which is commonly found in morphological inflection generation tasks.
2. State-of-the-art results on three datasets: The authors evaluate their model on three previously studied morphological inflection generation datasets (CELEX, Wiktionary, and SIGMORPHON2016) and show that it provides state-of-the-art results in various setups compared to previous neural and non-neural approaches.
3. Analysis of learned representations: The authors perform an analysis of the continuous representations learned by both the hard and soft attention models, shedding light on the features that such models extract for the inflection generation task.
The strengths of this paper are:
1. Effective handling of small training sets: The authors demonstrate that their model performs significantly better than previous neural models on small training sets, making it a valuable contribution to the field of morphological inflection generation.
2. Improved performance on large training sets: The authors also show that their model performs well on large training sets, making it a robust and competitive approach to morphological inflection generation.
3. Insightful analysis of learned representations: The authors provide a detailed analysis of the learned representations, which sheds light on the features that the models extract and how they differ between the hard and soft attention models.
The weaknesses of this paper are:
1. Limited comparison to other models: While the authors compare their model to several previous approaches, they do not provide a comprehensive comparison to all existing models, which would be useful to fully understand the strengths and limitations of their approach.
2. Lack of theoretical analysis: The authors do not provide a theoretical analysis of their model, which would be useful to understand the underlying mechanisms and limitations of their approach.
3. Limited evaluation on other tasks: The authors only evaluate their model on morphological inflection generation tasks, and it would be interesting to see how their model performs on other related tasks, such as machine translation or abstractive summarization.
Questions to authors:
1. How do the authors plan to extend their model to handle non-monotonic alignments, which are common in other sequence-to-sequence tasks?
2. Can the authors provide more details on the training procedure and the hyperparameters used to train their model?
3. How do the authors think their model can be applied to other tasks, such as machine translation or abstractive summarization, and what modifications would be necessary to achieve good performance on these tasks?