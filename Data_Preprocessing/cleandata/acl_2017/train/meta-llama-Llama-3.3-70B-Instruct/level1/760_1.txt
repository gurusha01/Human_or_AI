This paper proposes a novel approach to reading text non-sequentially using a recurrent neural network (RNN) that learns to jump over irrelevant information. The main contributions of this work are:
1. Non-sequential text reading: The authors propose a model that can read text non-sequentially, allowing it to skip over irrelevant information and focus on the most important parts of the text.
2. Reinforcement learning-based training: The model is trained using a reinforcement learning algorithm, specifically the REINFORCE algorithm, which enables the model to learn a policy for jumping over text.
3. Improved efficiency and accuracy: The authors demonstrate that their model, called LSTM-Jump, can achieve significant speedups over traditional sequential RNNs while maintaining or even improving accuracy.
The strengths of this paper include:
1. Novel approach: The idea of non-sequential text reading is novel and has the potential to improve the efficiency of many natural language processing tasks.
2. Strong experimental results: The authors provide extensive experimental results on four different tasks, demonstrating the effectiveness of their approach.
3. Well-written and clear presentation: The paper is well-organized and easy to follow, making it accessible to a wide range of readers.
The weaknesses of this paper include:
1. Limited interpretability: The authors do not provide much insight into how the model is making its jumping decisions, which could be an important area for future work.
2. Dependence on hyperparameters: The model's performance is sensitive to the choice of hyperparameters, such as the number of jumps allowed and the maximum size of jumping.
3. Comparison to other models: The authors primarily compare their model to a traditional sequential RNN, but it would be interesting to see comparisons to other models that also aim to improve efficiency, such as those using attention mechanisms.
Questions to authors:
1. Can you provide more insight into how the model is making its jumping decisions, and what features of the text are most important for determining when to jump?
2. How do you plan to extend this work to more complex tasks, such as those requiring multiple passes over the text or more sophisticated reasoning?
3. Have you considered using other reinforcement learning algorithms or techniques, such as actor-critic methods or curriculum learning, to improve the model's performance?