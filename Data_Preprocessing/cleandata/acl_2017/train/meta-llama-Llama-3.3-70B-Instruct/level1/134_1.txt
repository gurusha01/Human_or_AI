Summary of the Paper
The paper presents a study on neural end-to-end computational argumentation mining (AM), which involves identifying argumentative structures in text. The authors investigate several neural network framings for AM, including dependency parsing, sequence tagging, multi-task learning, and a model that combines sequential and tree structure information. They evaluate these models on a dataset of persuasive essays and compare their performance to a feature-based ILP model.
Main Contributions
1. Neural End-to-End AM: The paper presents the first neural end-to-end solutions to computational AM, which eliminate the need for manual feature engineering and costly ILP constraint designing.
2. Effective Sequence Tagging: The authors show that BiLSTM taggers perform very well for component detection, and that a standard neural sequence tagging model can perform robustly in different environments.
3. Importance of Joint Modeling: The paper demonstrates that jointly modeling component and relation detection is crucial, but that naively coupling these tasks is not optimal.
Strengths
1. State-of-the-Art Results: The paper achieves new state-of-the-art results in end-to-end AM on the PE dataset from Stab and Gurevych (2016).
2. Comprehensive Evaluation: The authors conduct a thorough evaluation of different neural network framings and compare their performance to a feature-based ILP model.
3. Insights into Task Difficulty: The paper provides insights into the difficulty of relation detection, particularly in long documents, and highlights the importance of restrained modeling in such cases.
Weaknesses
1. Limited Exploration of Hyperparameters: The paper does not provide an extensive exploration of hyperparameters for the neural network models, which may impact their performance.
2. Lack of Comparison to Other Neural Models: The authors do not compare their models to other neural network architectures, such as encoder-decoder models, which may be suitable for AM.
3. Dependence on Dataset: The paper's results are based on a single dataset, and it is unclear how well the models will generalize to other datasets or domains.
Questions to Authors
1. How do the authors plan to address the issue of hyperparameter tuning for the neural network models?
2. Can the authors provide more insights into why the LSTM-ER model performs poorly on relation identification in long documents?
3. How do the authors plan to extend their work to other datasets or domains, and what challenges do they anticipate in doing so?