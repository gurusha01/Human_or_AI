This paper proposes a novel neural network architecture called Attention-over-Attention Reader (AoA Reader) for cloze-style reading comprehension tasks. The main contributions of this work are: 
1. The introduction of an attention-over-attention mechanism, which allows the model to automatically generate "attended attention" over various document-level attentions.
2. The proposal of an N-best re-ranking strategy to re-score candidates and further improve performance.
3. The achievement of state-of-the-art results on public datasets, including CNN and Children's Book Test.
The strengths of this paper are:
1. The AoA Reader outperforms various state-of-the-art systems by a large margin, demonstrating the effectiveness of the proposed attention-over-attention mechanism.
2. The N-best re-ranking strategy is effective in improving performance, especially when combined with the AoA Reader.
3. The paper provides a thorough analysis of the results, including a quantitative analysis of the AoA Reader's performance on different document lengths and answer frequencies.
4. The proposed model is simple and general, making it applicable to other tasks and datasets.
The weaknesses of this paper are:
1. The paper relies heavily on the assumption that the attention-over-attention mechanism is effective in capturing the relationships between the document and query.
2. The N-best re-ranking strategy may not be effective in all cases, and the choice of features and weights may require careful tuning.
3. The paper does not provide a detailed comparison with other state-of-the-art systems, making it difficult to fully understand the strengths and weaknesses of the proposed model.
4. The paper could benefit from a more detailed analysis of the attention-over-attention mechanism and its implications for the model's performance.
Questions to authors:
1. How does the attention-over-attention mechanism handle cases where the document and query have different lengths or structures?
2. Can the N-best re-ranking strategy be applied to other tasks or datasets, and if so, how would it need to be modified?
3. How does the proposed model handle cases where the answer is not a single word or is not present in the document?
4. Can the attention-over-attention mechanism be used in other neural network architectures, such as recurrent neural networks or convolutional neural networks?