This paper provides a mathematical formalism for compositionality in word-embedding models, specifically the Skip-Gram model. The authors show that the composition operator for the Skip-Gram model is a non-linear function of the vectors of the words being composed, but reduces to vector addition under a uniform word frequency assumption. The paper also establishes a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework, showing that the parameters of Skip-Gram models can be modified to obtain the parameters of SDR models.
The main contributions of this work are:
1. A mathematical formalism for compositionality in word-embedding models, which provides a theoretical understanding of the phenomenon of additive compositionality in word vectors.
2. A proof that the composition operator for the Skip-Gram model is non-linear in general, but reduces to vector addition under a uniform word frequency assumption.
3. A connection between the Skip-Gram model and the SDR framework, which provides a theoretical justification for the use of Skip-Gram models in natural language processing tasks.
The strengths of this paper are:
1. The paper provides a rigorous mathematical framework for understanding compositionality in word-embedding models, which is a key aspect of natural language processing.
2. The connection between the Skip-Gram model and the SDR framework provides a theoretical justification for the use of Skip-Gram models in a wide range of applications.
3. The paper provides a clear and well-written exposition of the mathematical concepts and proofs, making it accessible to a wide range of readers.
The weaknesses of this paper are:
1. The paper assumes a uniform word frequency distribution, which may not be realistic in many natural language processing applications.
2. The paper does not provide empirical evaluations of the proposed framework, which would be necessary to demonstrate its effectiveness in practice.
3. The paper does not discuss the limitations of the proposed framework, such as its ability to handle out-of-vocabulary words or words with multiple meanings.
Questions to authors:
1. How do the authors plan to address the assumption of uniform word frequency distribution, which may not be realistic in many natural language processing applications?
2. Can the authors provide empirical evaluations of the proposed framework, such as comparisons with other word-embedding models or evaluations on specific natural language processing tasks?
3. How do the authors plan to extend the proposed framework to handle more complex linguistic phenomena, such as out-of-vocabulary words or words with multiple meanings?