This paper proposes a novel approach to word embeddings, where each word token is represented as a context-sensitive distribution over semantic concepts (synsets) in WordNet. The authors claim that this approach improves the accuracy of prepositional phrase (PP) attachment predictions by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.
The main contributions of this work are:
1. Context-sensitive token embeddings: The authors propose a method to learn context-sensitive token embeddings by estimating a distribution over relevant semantic concepts in WordNet.
2. WordNet-grounded token embeddings: The authors use WordNet to ground word tokens in a lexical ontology, which provides a more nuanced representation of word meanings.
3. Improved PP attachment predictions: The authors demonstrate that their proposed embeddings improve the accuracy of PP attachment predictions, outperforming standard type-level embeddings and other baseline models.
The strengths of this paper are:
1. Novel approach to word embeddings: The authors propose a new approach to word embeddings that takes into account the context in which a word is used.
2. Effective use of WordNet: The authors demonstrate the effectiveness of using WordNet to ground word tokens in a lexical ontology.
3. Improved performance on PP attachment task: The authors show that their proposed embeddings improve the accuracy of PP attachment predictions, which is a challenging task in NLP.
4. Detailed analysis and evaluation: The authors provide a detailed analysis and evaluation of their proposed model, including qualitative and quantitative results.
The weaknesses of this paper are:
1. Complexity of the model: The authors' proposed model is complex and may be difficult to implement and train.
2. Dependence on WordNet: The authors' approach relies heavily on WordNet, which may not always be available or accurate for all languages and domains.
3. Limited evaluation: The authors only evaluate their proposed model on a single task (PP attachment) and may not have explored its applicability to other NLP tasks.
4. Comparison to other models: The authors may not have compared their proposed model to other state-of-the-art models for PP attachment, which could provide a more comprehensive evaluation of their approach.
Questions to authors:
1. How do the authors plan to extend their approach to other NLP tasks and languages?
2. How do the authors address the complexity of their proposed model and make it more efficient to train and implement?
3. How do the authors evaluate the effectiveness of their proposed model in comparison to other state-of-the-art models for PP attachment?