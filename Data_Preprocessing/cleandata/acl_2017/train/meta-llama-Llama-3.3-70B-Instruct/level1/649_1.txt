This paper proposes a novel approach to automatically evaluating the quality of dialogue responses, which is a crucial problem in the development of conversational AI systems. The authors introduce an evaluation model called ADEM, which learns to predict human-like scores for input responses using a new dataset of human response scores. The main contributions of this work are:
1. ADEM model: The authors propose a hierarchical recurrent neural network (RNN) based model that captures semantic similarity beyond word overlap statistics and exploits both the context and the reference response to calculate its score for the model response.
2. Generalization to unseen models: The authors demonstrate that ADEM can generalize to evaluating new models, whose responses were unseen during training, making it a strong first step towards effective automatic dialogue response evaluation.
3. Correlation with human judgements: The authors show that ADEM scores correlate significantly with human judgements at both the utterance-level and system-level, outperforming existing word-overlap metrics such as BLEU.
The strengths of this paper are:
1. Novel approach: The authors propose a new approach to evaluating dialogue responses, which addresses the limitations of existing word-overlap metrics.
2. Strong experimental results: The authors demonstrate the effectiveness of ADEM through extensive experiments, including utterance-level and system-level correlations with human judgements.
3. Generalization to unseen models: The authors show that ADEM can generalize to evaluating new models, which is a crucial property for a dialogue evaluation model.
The weaknesses of this paper are:
1. Limited domain: The authors focus on non-task-oriented dialogue systems, which may not be directly applicable to task-oriented systems.
2. Dependence on human annotations: The authors rely on human annotations to train and evaluate ADEM, which can be time-consuming and expensive to obtain.
3. Potential bias: The authors note that ADEM may be biased towards generic responses, which can be a limitation of the model.
Questions to authors:
1. How do the authors plan to address the potential bias of ADEM towards generic responses?
2. Can the authors provide more details on the dataset used to train and evaluate ADEM, including the distribution of response scores and the quality of the human annotations?
3. How do the authors envision ADEM being used in practice, and what are the potential applications of this model in the development of conversational AI systems?