This paper introduces a new multi-modal task, Dual Machine Comprehension (DMC), which requires a computer system to identify the most suitable textual description of a scene from several options. The task is designed to measure the alignment between visual and linguistic semantic understanding. The authors propose an effective and extensible algorithm for generating decoys from human-created image captions and create a large-scale machine comprehension dataset, MCIC, based on the COCO images and captions.
The main contributions of this work are:
1. The introduction of the DMC task, which provides a new and challenging benchmark for evaluating machine comprehension.
2. The development of an algorithm for generating decoys, which can be used to create datasets for the DMC task.
3. The creation of the MCIC dataset, which is made publicly available.
The strengths of this paper are:
1. The proposal of a new and interesting task that can help advance the field of multi-modal machine comprehension.
2. The development of a well-designed algorithm for generating decoys, which can be used to create datasets for the DMC task.
3. The creation of a large-scale dataset, MCIC, which can be used to evaluate the performance of machine comprehension models.
4. The empirical results, which demonstrate the effectiveness of the proposed task and dataset in evaluating machine comprehension models.
5. The analysis of the results, which provides insights into the relationship between the DMC task and other vision-language tasks, such as image captioning.
The weaknesses of this paper are:
1. The complexity of the proposed task, which may make it challenging to develop models that can perform well on the task.
2. The limited evaluation of the proposed task, which is only evaluated on a single dataset, MCIC.
3. The lack of comparison with other related tasks, such as visual question answering, which may provide additional insights into the effectiveness of the proposed task.
4. The limited analysis of the results, which may not provide a complete understanding of the strengths and weaknesses of the proposed task and dataset.
5. The potential bias in the dataset, which may affect the performance of models trained on the dataset.
Questions to authors:
1. How do the authors plan to address the complexity of the proposed task, and what strategies can be used to develop models that can perform well on the task?
2. Can the authors provide more details on the evaluation of the proposed task, including the metrics used and the results obtained?
3. How does the proposed task relate to other vision-language tasks, such as visual question answering, and what insights can be gained from comparing the two tasks?
4. Can the authors provide more analysis of the results, including the strengths and weaknesses of the proposed task and dataset?
5. How can the potential bias in the dataset be addressed, and what strategies can be used to ensure that the dataset is representative of the real world?