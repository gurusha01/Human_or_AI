This paper proposes a novel Neural Belief Tracking (NBT) framework for spoken dialogue systems, which overcomes the limitations of current approaches by leveraging pre-trained word vectors to handle lexical and morphological variation. The main contributions of this work are:
1. Introduction of the NBT framework: The authors propose a new framework that couples Spoken Language Understanding (SLU) and Dialogue State Tracking (DST) without relying on hand-crafted semantic lexicons.
2. Use of pre-trained word vectors: The NBT models utilize pre-trained word vectors to learn semantic relations and handle variation in user language.
3. Evaluation on two datasets: The authors evaluate the NBT models on two datasets, DSTC2 and WOZ 2.0, and demonstrate their effectiveness in matching the performance of state-of-the-art models that rely on hand-crafted semantic lexicons.
The strengths of this paper are:
1. Novel approach: The NBT framework offers a new perspective on belief tracking, which has the potential to improve the performance of spoken dialogue systems.
2. Effective use of pre-trained word vectors: The authors demonstrate the importance of using high-quality pre-trained word vectors in improving the performance of the NBT models.
3. Strong evaluation: The paper presents a thorough evaluation of the NBT models on two datasets, which provides a comprehensive understanding of their strengths and weaknesses.
The weaknesses of this paper are:
1. Limited analysis of errors: The authors do not provide a detailed analysis of the errors made by the NBT models, which could help identify areas for improvement.
2. Dependence on pre-trained word vectors: The performance of the NBT models relies heavily on the quality of the pre-trained word vectors, which may not always be available or effective for all languages and domains.
3. Lack of comparison to other neural models: The paper does not compare the NBT models to other neural models for belief tracking, which could provide a more comprehensive understanding of their strengths and weaknesses.
Questions to authors:
1. How do the NBT models handle out-of-vocabulary words or unseen user language?
2. Can the authors provide more details on the hyperparameter tuning process for the NBT models?
3. How do the NBT models perform in multi-domain dialogue systems or languages other than English?