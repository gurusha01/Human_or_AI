This paper proposes a novel approach to historical text normalization using encoder-decoder architectures with multi-task learning. The main contributions of this work are:
1. Introduction of encoder-decoder architectures for historical text normalization: The authors propose using encoder-decoder models for historical text normalization, which is a new approach to this task.
2. Multi-task learning with grapheme-to-phoneme mapping as an auxiliary task: The authors use multi-task learning to improve the performance of the encoder-decoder model, with grapheme-to-phoneme mapping as an auxiliary task.
3. Analysis of the effects of multi-task learning on attention mechanisms: The authors provide an in-depth analysis of how multi-task learning affects the attention mechanisms in the encoder-decoder model.
The strengths of this paper are:
1. State-of-the-art results: The proposed approach achieves state-of-the-art results on the Anselm dataset, outperforming several baseline models.
2. Novel application of multi-task learning: The use of multi-task learning with grapheme-to-phoneme mapping as an auxiliary task is a novel application of this technique to historical text normalization.
3. In-depth analysis of attention mechanisms: The authors provide a detailed analysis of how multi-task learning affects the attention mechanisms in the encoder-decoder model, which is an important contribution to the understanding of these mechanisms.
The weaknesses of this paper are:
1. Limited dataset: The authors only evaluate their approach on a single dataset, the Anselm dataset, which may not be representative of all historical text normalization tasks.
2. Lack of comparison to other neural network architectures: The authors only compare their approach to a few baseline models, and do not evaluate other neural network architectures that may be suitable for historical text normalization.
3. Need for more analysis of the effects of hyperparameter tuning: The authors only tune the hyperparameters on a single manuscript, which may not be sufficient to ensure that the results are generalizable to other datasets.
Questions to authors:
1. How do the authors plan to extend their approach to other historical text normalization datasets?
2. Can the authors provide more details on the hyperparameter tuning process, and how they ensured that the results are generalizable to other datasets?
3. How do the authors think their approach can be applied to other natural language processing tasks, such as machine translation or text classification?