This paper presents a novel approach to automatically annotating parallel error correction data with explicit edit spans and error type information. The main contributions of this work are:
1. Automatic annotation of error correction data: The authors propose a method to automatically extract edits between parallel original and corrected sentences and classify them using a new dataset-independent rule-based classifier.
2. Dataset-independent error type evaluation: The approach enables a detailed error type evaluation of system performance, which is not possible with existing metrics.
3. Release of a tool for automatic annotation: The authors will release a tool that can be used to standardize existing error correction corpora or facilitate a detailed error type evaluation.
The strengths of this paper are:
1. Novel approach to error type evaluation: The authors propose a new framework for error type evaluation that is entirely dataset-independent and relies only on automatically obtained information.
2. Detailed evaluation of system performance: The paper presents a detailed evaluation of system error type performance for the first time for all teams in the CoNLL-2014 shared task on Grammatical Error Correction.
3. Release of a useful tool: The authors will release a tool that can be used by other researchers to standardize existing error correction corpora or facilitate a detailed error type evaluation.
The weaknesses of this paper are:
1. Limited evaluation of the classifier: The evaluation of the classifier is limited to a small-scale manual evaluation, which may not be sufficient to demonstrate its effectiveness.
2. Lack of comparison to existing metrics: The paper does not provide a comparison of the proposed approach to existing metrics for error type evaluation.
3. Limited analysis of results: The paper presents a large amount of data, but the analysis of the results is limited, and some of the findings are not fully explored.
Questions to authors:
1. How do you plan to evaluate the effectiveness of the classifier in a larger-scale setting?
2. Can you provide a comparison of the proposed approach to existing metrics for error type evaluation?
3. How do you plan to address the limitations of the current evaluation, such as the limited analysis of results?