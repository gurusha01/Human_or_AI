Summary of the Paper
The paper proposes a generative model for keyphrase prediction in scientific text using an encoder-decoder framework with a recurrent neural network (RNN) and a copy mechanism. The model aims to capture the deep semantic meaning of the text and generate keyphrases that are not present in the text. The authors evaluate their model on six datasets, including a new dataset called KP20k, and demonstrate its effectiveness in predicting both present and absent keyphrases.
Main Contributions
1. Proposed Model: The authors propose a novel RNN-based generative model for keyphrase prediction that incorporates a copy mechanism to handle rarely-occurred phrases.
2. Absent Keyphrase Prediction: The model is able to predict absent keyphrases, which is a challenging task that has not been addressed by previous methods.
3. Comprehensive Evaluation: The authors conduct a thorough evaluation of their model on six datasets, including a new dataset called KP20k, and demonstrate its effectiveness in predicting both present and absent keyphrases.
Strengths
1. Effective Keyphrase Prediction: The proposed model demonstrates state-of-the-art performance in predicting present keyphrases and is able to predict absent keyphrases, which is a significant improvement over previous methods.
2. Ability to Generalize: The model shows the ability to generalize to other domains and types of text, such as news articles, without requiring additional training.
3. Comprehensive Evaluation: The authors conduct a thorough evaluation of their model on multiple datasets, which provides a comprehensive understanding of its strengths and weaknesses.
Weaknesses
1. Limited Interpretability: The proposed model is a black box, and it is difficult to interpret the results and understand why certain keyphrases are predicted.
2. Dependence on Training Data: The model requires a large amount of training data to learn effective representations, which may not be available for all domains or types of text.
3. Computational Cost: The model requires significant computational resources to train and evaluate, which may be a limitation for large-scale applications.
Questions to Authors
1. How do the authors plan to address the issue of limited interpretability of the proposed model?
2. Can the authors provide more details on the training process and the computational resources required to train the model?
3. How do the authors plan to extend the model to other domains and types of text, such as books or online reviews?