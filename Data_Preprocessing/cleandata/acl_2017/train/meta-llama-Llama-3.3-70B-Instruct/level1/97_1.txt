Summary of the Paper
The paper presents an automated Japanese short-answer scoring and support system for the new National Center written test exams. The system uses a combination of natural language processing and machine learning techniques to evaluate the semantic similarity between model answers and actual written answers. The system generates automated scores based on evaluation criteria and allows human raters to revise them. The authors conducted an experiment using a trial examination for university entrance examinations and compared human scores with automated scores, achieving a high concordance rate.
Main Contributions
1. Development of a short-answer scoring system: The authors developed a system that can evaluate short-answer questions, which is a challenging task due to the need for recognizing textual entailment and semantic similarity.
2. Use of random forests for machine learning: The authors used random forests to utilize many predictors effectively, which improved the accuracy of the scoring system.
3. Introduction of a support system for human raters: The authors designed a system that allows human raters to correct and overwrite automated scores, ensuring the accuracy and reliability of the scoring process.
Strengths
1. High concordance rate: The authors achieved a high concordance rate between human scores and automated scores, demonstrating the effectiveness of the system.
2. Use of semantic similarity measures: The authors used semantic similarity measures, such as cosine similarity and F-measure, to evaluate the similarity between model answers and actual written answers.
3. Flexibility of the system: The system can be applied to various subjects, including Japanese literature, and can handle different transcriptions of correct answers.
4. User-friendly interface: The system has a user-friendly interface that allows human raters to easily review and revise automated scores.
Weaknesses
1. Limited data: The authors noted that obtaining a sufficiently large number of human scores for supervised learning is challenging, which may affect the accuracy of the system.
2. Dependence on human raters: The system relies on human raters to correct and overwrite automated scores, which may introduce variability and bias.
3. Limited evaluation: The authors only evaluated the system using a trial examination for university entrance examinations, which may not be representative of all types of short-answer questions.
Questions to Authors
1. How do the authors plan to address the issue of limited data for supervised learning?
2. Can the authors provide more details on the user-friendly interface and how it facilitates the review and revision process for human raters?
3. How do the authors plan to evaluate the system's performance in different subjects and contexts?