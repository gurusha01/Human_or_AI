This paper proposes a new A* CCG parsing model that decomposes the probability of a tree into factors of CCG categories and its dependency structure, both defined on bi-directional LSTMs. The main contributions of this work are:
1. Joint modeling of CCG categories and dependencies: The authors propose a factored model that combines the probabilities of CCG categories and dependencies, allowing for efficient A* search and achieving state-of-the-art results on English and Japanese CCG parsing.
2. Explicit modeling of bilexical dependencies: The authors introduce a head prediction model on bi-LSTMs to resolve attachment ambiguities, which is a significant improvement over previous methods that relied on deterministic heuristics.
3. Efficient A search: The authors show that their joint model can be used with A search, which is efficient and optimal, and achieves better results than previous methods.
The strengths of this paper are:
1. State-of-the-art results: The authors achieve the best reported F1 scores on English CCG parsing, outperforming previous state-of-the-art models.
2. Efficient parsing: The authors' parser is efficient, processing over 5 times more sentences than a previous state-of-the-art parser in A* search.
3. Improved performance on Japanese parsing: The authors' method shows a significant performance boost on Japanese CCG parsing, outperforming a baseline shift-reduce parser.
The weaknesses of this paper are:
1. Complexity of the model: The authors' model is complex, involving multiple components, including bi-LSTMs, MLPs, and a biaffine transformation, which may make it difficult to train and optimize.
2. Dependence on pre-trained word embeddings: The authors rely on pre-trained word embeddings, which may not always be available or suitable for all languages or domains.
3. Limited evaluation on Japanese parsing: The authors' evaluation on Japanese parsing is limited, and more experiments are needed to fully assess the performance of their method on this language.
Questions to authors:
1. How do the authors plan to address the complexity of their model, and what strategies can be used to simplify or optimize it?
2. Can the authors provide more details on the training process, including the hyperparameter settings and the optimization algorithm used?
3. How do the authors plan to extend their method to other languages or domains, and what challenges do they anticipate in doing so?