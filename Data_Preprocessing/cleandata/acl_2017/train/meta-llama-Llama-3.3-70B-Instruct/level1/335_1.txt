This paper presents a novel approach to reading comprehension style question answering, introducing gated self-matching networks that achieve state-of-the-art results on the SQuAD dataset. The main contributions of this work are:
1. Gated attention-based recurrent networks: The authors propose a variant of attention-based recurrent networks with an additional gate to model the importance of passage parts to the question.
2. Self-matching attention mechanism: The authors introduce a self-matching attention mechanism to refine the passage representation by matching the passage against itself, effectively encoding information from the whole passage.
3. State-of-the-art results: The proposed model achieves state-of-the-art results on the SQuAD dataset, outperforming several strong competing systems.
The strengths of this paper are:
1. Effective use of attention mechanisms: The authors demonstrate the effectiveness of attention mechanisms in reading comprehension, particularly in modeling the importance of passage parts to the question.
2. Innovative self-matching attention mechanism: The self-matching attention mechanism is a novel contribution that allows the model to dynamically refine the passage representation and aggregate evidence from the whole passage.
3. Strong empirical results: The paper presents strong empirical results, achieving state-of-the-art performance on the SQuAD dataset and outperforming several competing systems.
The weaknesses of this paper are:
1. Complexity of the model: The proposed model is complex, with multiple components and mechanisms, which may make it difficult to interpret and analyze.
2. Limited analysis of the self-matching attention mechanism: While the authors demonstrate the effectiveness of the self-matching attention mechanism, they provide limited analysis of how it works and why it is effective.
3. Lack of comparison to other attention mechanisms: The authors do not compare their self-matching attention mechanism to other attention mechanisms, which would provide a more comprehensive understanding of its effectiveness.
Questions to authors:
1. Can you provide more analysis of the self-matching attention mechanism, including visualizations of the attention weights and examples of how it refines the passage representation?
2. How does the gated attention-based recurrent network compare to other attention mechanisms, such as hierarchical attention or multi-perspective attention?
3. Can you provide more details on the training process, including the hyperparameter settings and the optimization algorithm used?