This paper proposes a novel cross-attention based neural network model for knowledge base-based question answering (KB-QA). The main contributions of this work are: 
1. A cross-attention mechanism that dynamically represents questions according to different answer aspects, which is more expressive and flexible than previous methods.
2. The incorporation of global knowledge base information into the training process, which helps to alleviate the out-of-vocabulary (OOV) problem and improves the performance of the model.
3. Experimental results on the WebQuestions dataset demonstrate the effectiveness of the proposed approach, achieving the best performance among end-to-end methods.
The strengths of this paper include:
1. The proposed cross-attention mechanism is a significant improvement over previous methods, allowing for more accurate and flexible question representation.
2. The incorporation of global KB information is a novel approach that addresses the OOV problem and enhances the model's performance.
3. The experimental results are thorough and well-presented, demonstrating the effectiveness of the proposed approach.
The weaknesses of this paper include:
1. The model's performance may be sensitive to the quality of the training data, and the authors could have explored more robust training strategies.
2. The authors could have provided more detailed analysis of the attention weights and their impact on the model's performance.
3. The model's ability to handle complex questions and label errors could be improved, as these are common challenges in KB-QA tasks.
Questions to authors:
1. How do the authors plan to address the issue of complex questions and label errors in future work?
2. Can the authors provide more detailed analysis of the attention weights and their impact on the model's performance?
3. How do the authors plan to extend the proposed approach to other KB-QA datasets and tasks?