This paper proposes a new evaluation methodology for word embeddings, focusing on data efficiency and simple supervised tasks. The authors argue that the current evaluation methods, such as intrinsic and extrinsic evaluations, have limitations and do not accurately reflect the performance of word embeddings in real-world applications. The proposed methodology involves evaluating word embeddings on simple supervised tasks, such as word similarity and analogy, with varying dataset sizes. The authors also suggest including a diverse set of models, including nonlinear and linear models, to evaluate the embeddings.
The main contributions of this paper are:
1. Data efficiency-focused evaluation: The authors propose evaluating word embeddings based on their data efficiency, which is crucial for real-world applications where dataset sizes are often limited.
2. Simple supervised tasks: The authors suggest using simple supervised tasks, such as word similarity and analogy, to evaluate word embeddings, which allows for a more direct assessment of the embeddings' quality.
3. Diverse set of models: The authors recommend including a diverse set of models, including nonlinear and linear models, to evaluate the embeddings, which provides a more comprehensive understanding of the embeddings' performance.
The strengths of this paper are:
1. Novel evaluation methodology: The proposed evaluation methodology is novel and addresses the limitations of current evaluation methods.
2. Comprehensive experiments: The authors conduct comprehensive experiments to validate their proposal, using a variety of word embeddings and tasks.
3. Theoretical analysis: The authors provide a theoretical analysis of the proposed evaluation methodology, which provides a deeper understanding of the underlying principles.
The weaknesses of this paper are:
1. Computational intensity: The proposed evaluation methodology can be computationally intensive, which may limit its applicability in practice.
2. Limited scope: The paper focuses primarily on word embeddings, and it is unclear whether the proposed evaluation methodology can be applied to other types of embeddings.
3. Lack of comparison to existing methods: The authors do not provide a direct comparison of their proposed evaluation methodology to existing methods, which makes it difficult to assess its relative performance.
Questions to authors:
1. How do the authors plan to address the computational intensity of the proposed evaluation methodology?
2. Can the proposed evaluation methodology be applied to other types of embeddings, such as sentence embeddings or graph embeddings?
3. How do the authors plan to compare their proposed evaluation methodology to existing methods, and what metrics will they use to evaluate its performance?