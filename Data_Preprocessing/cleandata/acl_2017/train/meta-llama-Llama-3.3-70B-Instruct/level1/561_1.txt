This paper proposes a semi-supervised approach to sequence tagging tasks, such as named entity recognition (NER) and chunking, by incorporating pre-trained context embeddings from bidirectional language models. The main contributions of this work are:
1. Effective use of pre-trained language models: The authors demonstrate that pre-trained language models can be used to improve the performance of sequence tagging models, even when the language model is trained on a different domain.
2. Bidirectional language models: The authors show that using both forward and backward language models can improve performance over using only a forward language model.
3. State-of-the-art results: The proposed method achieves state-of-the-art results on two popular datasets for NER and chunking.
The strengths of this paper are:
1. Simple and general approach: The proposed method is simple to implement and can be applied to a wide range of sequence tagging tasks.
2. Significant improvements: The authors demonstrate significant improvements over previous state-of-the-art results on two popular datasets.
3. Robustness to domain shift: The proposed method is robust even when the language model is trained on a different domain, making it a useful approach for real-world applications.
The weaknesses of this paper are:
1. Limited analysis: The authors could have provided more detailed analysis of the results, such as ablation studies to understand the contribution of each component of the proposed method.
2. Dependence on pre-trained language models: The proposed method relies on pre-trained language models, which may not always be available or may require significant computational resources to train.
3. Limited comparison to other methods: The authors could have compared their proposed method to other semi-supervised learning methods, such as co-training or expectation maximization, to provide a more comprehensive evaluation.
Questions to authors:
1. How do the authors plan to extend this work to other sequence tagging tasks, such as part-of-speech tagging or dependency parsing?
2. Can the authors provide more detailed analysis of the results, such as ablation studies or visualizations of the learned representations?
3. How do the authors plan to address the dependence on pre-trained language models, such as by developing more efficient training methods or using alternative sources of pre-trained models?