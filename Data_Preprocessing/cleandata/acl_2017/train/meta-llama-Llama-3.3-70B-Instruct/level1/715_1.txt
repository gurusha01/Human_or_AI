This paper proposes a system, DrWiki, for open-domain question answering using Wikipedia as the unique knowledge source. The system combines a document retrieval component with a machine comprehension model to identify answer spans in relevant articles. 
The main contributions of this work are: 
1. The development of a strong document retrieval system that outperforms the built-in Wikipedia search engine, 
2. A state-of-the-art machine comprehension model, Document Reader, that achieves competitive results on the SQuAD benchmark, and 
3. The demonstration of the effectiveness of multitask learning using distant supervision on multiple QA datasets.
The strengths of this paper include: 
1. The authors' thorough evaluation of their system on multiple benchmarks, which demonstrates the efficacy of their approach, 
2. The development of a system that can answer open-domain questions using a single knowledge source, which is a challenging task, 
3. The use of distant supervision to generate additional training data, which improves the performance of the system, and 
4. The authors' identification of the key challenges in open-domain QA, such as integrating search and machine comprehension.
The weaknesses of this paper include: 
1. The system's performance on open-domain QA is still behind that of unconstrained QA systems that use redundant resources, 
2. The document retrieval component may not always retrieve the correct document, which can lead to false positives, 
3. The system's performance on WebQuestions is lower than expected, likely due to the fact that this dataset is based on Freebase, and 
4. The authors do not provide a detailed analysis of the errors made by the system, which could provide insights for future improvements.
Questions to authors: 
1. How do the authors plan to improve the performance of the document retrieval component to reduce false positives? 
2. Can the authors provide more details on the distant supervision process and how it improves the performance of the system? 
3. How do the authors plan to incorporate the fact that Document Reader aggregates over multiple paragraphs and documents directly in the training? 
4. Can the authors provide a detailed analysis of the errors made by the system to identify areas for future improvements?