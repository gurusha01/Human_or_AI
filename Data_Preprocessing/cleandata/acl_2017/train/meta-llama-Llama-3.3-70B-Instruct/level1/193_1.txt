This paper presents a novel parser, TUPA, for Universal Conceptual Cognitive Annotation (UCCA), a cross-linguistically applicable framework for semantic representation. The main contributions of this work are:
1. Introduction of a novel transition-based parser: TUPA is the first parser designed specifically for UCCA, which can handle the unique properties of UCCA, such as reentrancy, discontinuous structures, and non-terminal nodes.
2. Development of a new transition set and features: The authors propose a new set of transitions and features that can effectively capture the complexities of UCCA, including the use of bidirectional LSTMs for feature representation.
3. Evaluation on English UCCA corpora: The authors evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings, and demonstrate its effectiveness in parsing UCCA graphs.
The strengths of this paper are:
1. Novelty of the approach: TUPA is the first parser designed specifically for UCCA, and its transition set and features are tailored to handle the unique properties of UCCA.
2. Effectiveness of the parser: The experimental results show that TUPA outperforms strong baselines, including bilexical graph parsers and tree parsers, in both in-domain and out-of-domain settings.
3. Potential impact on semantic parsing: The development of a parser for UCCA can enable the use of this framework for various applications, such as machine translation evaluation, sentence simplification, and summarization.
The weaknesses of this paper are:
1. Limited comparison to existing parsers: While the authors compare TUPA to bilexical graph parsers and tree parsers, a more comprehensive comparison to other semantic parsers, such as those for Abstract Meaning Representation (AMR), would be beneficial.
2. Need for further evaluation: The authors only evaluate TUPA on English UCCA corpora, and further evaluation on other languages and datasets would be necessary to demonstrate its robustness and applicability.
3. Complexity of the parser: The use of bidirectional LSTMs and a novel transition set may make TUPA more complex and difficult to train than other parsers.
Questions to authors:
1. How do the authors plan to extend TUPA to support other languages and datasets?
2. Can the authors provide more details on the training process and the hyperparameter tuning for TUPA?
3. How do the authors envision the use of TUPA in real-world applications, such as machine translation evaluation and sentence simplification?