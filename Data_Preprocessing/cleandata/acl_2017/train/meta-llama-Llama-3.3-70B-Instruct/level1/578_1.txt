This paper proposes a neural encoder-decoder transition-based parser for Minimal Recursion Semantics (MRS) and Abstract Meaning Representation (AMR) parsing. The main contributions of this work are:
1. Development of a fast and robust parser for full MRS-based semantic graphs: The authors propose a novel parser architecture that uses stack-based embedding features and predicts graphs jointly with unlexicalized predicates and their token alignments.
2. Application of the parser to AMR parsing: The authors apply their model to AMR parsing by introducing structure (alignments and distinguishing between lexical and non-lexical concepts) that is present explicitly in MRS but not in AMR.
3. Achievement of state-of-the-art results on MRS and AMR parsing benchmarks: The authors report significant improvements over existing neural baseline models and achieve competitive results with state-of-the-art models that make extensive use of external resources.
The strengths of this paper are:
1. Novel parser architecture: The proposed parser architecture is innovative and effective, allowing for the parsing of complex semantic graphs.
2. State-of-the-art results: The authors achieve significant improvements over existing neural baseline models and report competitive results with state-of-the-art models.
3. Efficient implementation: The authors implement their model using GPU batch processing, making it an order of magnitude faster than existing parsers.
4. Robustness to data sparsity: The authors propose a method to predict candidate lemmas, which helps to deal with data sparsity and improves the performance of the parser.
The weaknesses of this paper are:
1. Limited comparison to existing work: The authors do not provide a comprehensive comparison to existing work on MRS and AMR parsing, making it difficult to fully evaluate the significance of their contributions.
2. Lack of analysis of error types: The authors do not provide a detailed analysis of the types of errors made by their parser, which could help to identify areas for improvement.
3. Dependence on pre-trained embeddings: The authors use pre-trained embeddings, which may not be optimal for the specific task of MRS and AMR parsing.
4. Limited evaluation on out-of-domain data: The authors do not evaluate their parser on out-of-domain data, which could help to assess its robustness and generalizability.
Questions to authors:
1. How do the authors plan to address the issue of data sparsity in MRS and AMR parsing, and what methods do they propose to improve the performance of the parser on unseen data?
2. Can the authors provide a more detailed analysis of the types of errors made by their parser, and how do they plan to address these errors in future work?
3. How do the authors plan to extend their parser to handle more complex semantic graphs, such as those that involve multiple scopes or quantification?
4. Can the authors provide more information on the computational resources required to train and run their parser, and how do they plan to make their parser more efficient and scalable?