This paper presents a novel approach to building natural language interfaces to databases (NLIDBs) that can learn and improve over time with minimal intervention. The authors propose a neural sequence-to-sequence model that maps utterances directly to SQL queries, bypassing intermediate meaning representations. The model is deployed online to solicit feedback from real users, which is used to improve the model's performance.
The main contributions of this work are:
1. Direct SQL generation: The authors demonstrate that a neural sequence-to-sequence model can generate arbitrary SQL queries from natural language utterances, without requiring manual feature engineering or intermediate meaning representations.
2. Feedback-based learning: The authors propose a feedback-based learning approach that uses user feedback to select utterances for crowd worker annotation, reducing the annotation effort required to train the model.
3. Data augmentation: The authors present two data augmentation strategies, schema templates and paraphrasing, which improve the model's performance and reduce the need for labeled data.
The strengths of this paper are:
1. Novel approach: The authors propose a novel approach to building NLIDBs that combines neural sequence-to-sequence models with feedback-based learning and data augmentation.
2. Effective performance: The authors demonstrate that their approach achieves comparable performance to previous systems on two benchmark datasets, GEO880 and ATIS.
3. Real-world applicability: The authors demonstrate the effectiveness of their approach in a real-world setting by learning a semantic parser for an academic domain from scratch.
The weaknesses of this paper are:
1. Limited evaluation: The authors only evaluate their approach on two benchmark datasets and a small-scale online experiment, which may not be representative of all possible use cases.
2. Dependence on crowd workers: The authors rely on crowd workers to annotate utterances, which may introduce noise and variability in the annotation process.
3. Lack of analysis: The authors do not provide a detailed analysis of the errors made by their model or the types of utterances that are most challenging to parse.
Questions to authors:
1. How do the authors plan to address the issue of noise and variability in the crowd worker annotation process?
2. Can the authors provide more details on the types of utterances that are most challenging to parse and how they plan to improve the model's performance on these utterances?
3. How do the authors plan to extend their approach to other query languages, such as SPARQL or ElasticSearch?