This paper proposes a novel approach to zero-resource neural machine translation (NMT) by leveraging a teacher-student framework. The main contributions of this work are:
1. Teacher-student framework: The authors propose a framework where a pre-trained pivot-to-target model (teacher) guides the learning process of a source-to-target model (student) without parallel corpora available.
2. Sentence-level and word-level teaching: The authors introduce two approaches to teach the student model: sentence-level teaching, which minimizes the KL divergence between the teacher and student models at the sentence level, and word-level teaching, which minimizes the KL divergence at the word level.
3. Experimental results: The authors evaluate their approach on the Europarl and WMT corpora and demonstrate significant improvements over state-of-the-art pivot-based methods and multilingual methods in terms of translation quality and decoding efficiency.
The strengths of this paper are:
1. Novel approach: The teacher-student framework is a novel approach to zero-resource NMT, and the authors demonstrate its effectiveness in experiments.
2. Improved translation quality: The authors show that their approach can significantly improve translation quality over state-of-the-art methods, especially in low-resource scenarios.
3. Efficient decoding: The authors' approach allows for direct modeling of the intended NMT model, avoiding the need to divide training or decoding into two steps, which improves decoding efficiency.
The weaknesses of this paper are:
1. Assumption verification: The authors assume that parallel sentences have close probabilities of generating a sentence in a third language, but they do not provide a thorough verification of this assumption.
2. Limited evaluation: The authors only evaluate their approach on two corpora (Europarl and WMT) and two language pairs (Spanish-French and German-French), which may not be representative of all zero-resource NMT scenarios.
3. Comparison to other methods: The authors compare their approach to pivot-based methods and multilingual methods, but they do not compare it to other zero-resource NMT methods, such as those using multimodal information.
Questions to authors:
1. How do the authors plan to verify the assumption that parallel sentences have close probabilities of generating a sentence in a third language?
2. Can the authors provide more evaluation results on other corpora and language pairs to demonstrate the generalizability of their approach?
3. How does the authors' approach compare to other zero-resource NMT methods, such as those using multimodal information?