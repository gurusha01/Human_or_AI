This paper proposes the integration of n-grams into four word representation methods: SGNS, GloVe, PPMI, and its SVD factorization. The main contributions of this work are:
1. Introduction of n-grams into word representation methods: The authors extend the information source from word co-occurrence statistics to n-gram co-occurrence statistics, which leads to improved word representations.
2. Novel approach to building co-occurrence matrix: The authors propose a novel method to build n-gram co-occurrence matrix, which reduces the disk I/O and alleviates the hardware burden brought by n-grams.
3. Comprehensive experiments and evaluations: The authors conduct comprehensive experiments on word analogy and similarity tasks, and evaluate the trained n-gram representations qualitatively.
The strengths of this paper are:
1. Improved word representations: The introduction of n-grams leads to significant improvements in word analogy and similarity tasks, demonstrating the effectiveness of the proposed approach.
2. Efficient co-occurrence matrix construction: The novel approach to building co-occurrence matrix enables the n-gram-based models to run on cheap hardware, making it more accessible to researchers and practitioners.
3. High-quality n-gram embeddings: The trained n-gram embeddings reflect semantic meanings and syntactic patterns, making them useful for various NLP tasks.
The weaknesses of this paper are:
1. Limited exploration of hyperparameters: The authors follow the default hyperparameter setting of the baseline models, which may not be optimal for the n-gram-based models.
2. Lack of comparison with other n-gram-based methods: The authors do not compare their approach with other n-gram-based methods, which makes it difficult to evaluate the novelty and effectiveness of the proposed approach.
3. Limited evaluation of n-gram embeddings: The authors only evaluate the n-gram embeddings qualitatively, and do not provide a systematic evaluation of their effectiveness in various NLP tasks.
Questions to authors:
1. How do the authors plan to explore the relationships between n-grams and various hyperparameters to improve the performance of the n-gram-based models?
2. Can the authors provide a comparison with other n-gram-based methods to evaluate the novelty and effectiveness of the proposed approach?
3. How do the authors plan to systematically evaluate the effectiveness of the n-gram embeddings in various NLP tasks?