This manuscript presents a modification to established word embedding algorithms, including GloVe, Skip Gram, PPMI, and SVD, by incorporating n-gram co-occurrence statistics. To mitigate the substantial computational costs associated with storing large matrices, the authors introduce an algorithm that employs two distinct strategies for collecting counts.
- Strengths:
* The proposed approach appears to be a logical extension of existing research on word embedding learning, as it integrates bigram information to potentially capture more nuanced syntactic and semantic information.
- Weaknesses:
* Although the authors suggest learning embeddings for bigrams (bi_bi case), they fail to provide a quantitative evaluation of these learned bigram embeddings beyond the qualitative assessment presented in Table 7. A more comprehensive evaluation using tasks such as paraphrasing, which can leverage bigram representations, would have been a valuable contribution.
* The evaluation and results lack conviction, as they do not exhibit consistent trends and some improvements are not statistically significant.
* The manuscript is marred by significant grammatical and spelling errors, necessitating a thorough editing process to enhance clarity and readability.
- General Discussion:
This manuscript extends conventional embedding learning techniques to incorporate bigram-bigram co-occurrence information. While the work is intriguing and a natural progression of existing research, the evaluation and methodology raise several questions. In addition to the weaknesses mentioned earlier, some minor queries for the authors include:
* What accounts for the substantial difference between the overlap and non-overlap cases? A more in-depth explanation beyond the quantitative differences observed in the tasks would be beneficial.
Having reviewed the author's response, I look forward to examining the revised manuscript.