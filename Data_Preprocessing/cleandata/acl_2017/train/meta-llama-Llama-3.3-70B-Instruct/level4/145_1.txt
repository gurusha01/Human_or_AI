Review: Multimodal Word Distributions
- Strengths: This paper presents a robust and well-structured approach to learning multimodal word distributions.
- Weaknesses: A more comprehensive comparison with similar methods would further enhance the paper's validity.
- General Discussion:
This paper introduces a novel model for representing multimodal word distributions using Gaussian mixtures, effectively capturing multiple word meanings as a set of Gaussian distributions. Building upon the unimodal Gaussian distribution model proposed by Vilnis and McCallum (2014), the current approach addresses the issue of polysemy by utilizing a multimodal representation.
The paper is well-organized, clear, and supported by thorough experimentation. The qualitative analysis in Table 1 yields expected results, demonstrating the effectiveness of the approach. The following comments are intended to provide suggestions for further clarification and improvement.
Some key points to consider:
* A brief discussion highlighting the differences between the current approach and that of Tian et al. (2014) would be beneficial, as both methods employ mixture models to split single word representations into multiple prototypes.
* The related work section could be enhanced by incorporating citations from relevant studies, such as:
Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space by Neelakantan et al. (EMNLP 2014),
Do Multi-Sense Embeddings Improve Natural Language Understanding? by Li and Jurafsky (EMNLP 2015), and
Topical Word Embeddings by Liu et al. (AAI 2015).
* Including results from these approaches in Tables 3 and 4 could provide additional insights.
* A question for the authors: What factors contribute to the performance difference between w2gm and w2g in the SWCS analysis?
I have taken into account the authors' response.