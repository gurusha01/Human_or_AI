Following the author's response, I have reevaluated my stance on the alignment of hidden units, and although it still doesn't align with my intuition and experience, I am open to the possibility that my understanding may be incorrect in this context. It is crucial for the paper to discuss this alignment and potentially include a sanity check to verify if the alignment persists with different initialization seeds.
Given the marginal performance improvement of 10% error reduction with the new model, despite its significant differences from the previous one, I suggest exploring the potential of an ensemble model combining both the new and old models. If the failures of these models are distinct, ensembling could yield a substantial boost in performance.
This paper presents a compelling method for historical text normalization, with the model performing well in relation to the state-of-the-art. The connection between attention mechanisms and multi-task learning (MTL) is particularly interesting, as it hypothesizes that attention in the task can be learned via MTL, with the auxiliary task being a pronunciation task.
The paper has several strengths, including:
1. The evidence supporting the attention-MTL connection is intriguing.
2. The methods employed are appropriate, and the models demonstrate good performance relative to the current state-of-the-art.
However, there are also weaknesses:
1. A critical detail is not provided in the paper, which hampers a full understanding of the methodology.
2. The models presented are not particularly novel, which somewhat diminishes the paper's impact.
In the general discussion, the primary contribution of the paper revolves around the hypothesis that attention mechanisms in historical text normalization can be learned through MTL, where the auxiliary task involves pronunciation. This connection between attention and MTL is noteworthy.
There are two primary areas for improvement. Firstly, the paper lacks a clear explanation for why the pronunciation task would necessitate an attention mechanism similar to that used for normalization. While it's mentioned that spelling variations often stem from pronunciation variations, the rationale behind why MTL on both tasks would result in an implicit attention mechanism (which is actually hindered by an explicit attention mechanism) remains unclear. Providing at least a speculative answer to this question would strengthen the paper.
Secondly, clarity is a concern. Although the writing is generally clear, several details are omitted. Most notably, the description of the attention mechanism itself is not detailed in the paper but rather referenced from previous work. The paragraph in Sec 3.4 discussing this was not clear to me.
Additional questions arise, such as the comparability of output vectors of two models (Figure 4), given that while the output dimensions may be the same, the organization of hidden states can be completely different or permuted between models. Therefore, I found Figure 4 perplexing.
It would be beneficial to compare the Kappa statistic for attention vs. MTL to the same statistic for each of those models vs. the base model for a more comprehensive analysis.
Furthermore, at the end of Sec 5, it is unclear if the row < 0.21 represents an upper bound across all datasets.
Lastly, the analysis in Sec 5 implies that the attention and MTL approaches significantly alter the model, as seen in comparisons like Fig 5. However, the actual experimental improvements in accuracy are quite modest (around 2%), which seems contradictory. The paper is solid, and I appreciate the author's response, which has led me to increase my review score to a 4.