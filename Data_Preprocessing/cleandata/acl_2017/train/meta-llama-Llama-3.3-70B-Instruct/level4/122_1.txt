Strengths:
This manuscript presents an innovative method for dialogue state tracking, leveraging pre-trained embeddings to represent slot values and combining them into distributed representations of user utterances and dialogue context. The experimental results on two datasets demonstrate consistent and substantial improvements over the baseline approach that relies on delexicalization. Furthermore, the authors have explored alternative pre-trained word embedding techniques, including XAVIER, GloVe, and Program-SL999.
Weaknesses:
One of the primary motivations for utilizing embeddings is to enhance generalizability to more complex dialogue domains where delexicalization may be limited. However, the datasets employed in this study appear to be restricted. It would be interesting to investigate how the proposed approach performs with and without a separate slot tagging component in more complex dialogues. For instance, when calculating the similarity between utterances and slot value pairs, the estimation could be limited to the span of the slot values, making it applicable even when the values do not match.
Additionally, the examples provided in the introduction seem misleading, as the dialogue state should presumably include "restaurant_name=The House". This raises further questions regarding the impact of coreference resolution on this task.
General Discussion:
Overall, the use of pre-trained word embeddings is a promising concept, and the specific approach presented in this manuscript is noteworthy. The authors' method shows great potential, and its implications are exciting, making it a valuable contribution to the field of dialogue state tracking.