Review
Strengths:
The paper contributes to the development of more challenging corpora for training sentence planners in data-to-text Natural Language Generation (NLG), a crucial and timely research direction. 
Weaknesses:
The paper's novelty and substantial advance over Perez-Beltrachini et al.'s (2016) content selection method are unclear, as a direct comparison between the two is lacking. The additional analysis presented appears superficial, and the main contribution seems to be the comparison of an Neural NLG (NNLG) baseline's performance on this corpus with that of Wen et al. (2016). However, the significantly higher BLEU scores reported in Wen et al.'s paper raise concerns about the sufficiency of the NNLG baseline for a meaningful comparison.
General Discussion:
The authors should clearly articulate the significance of this paper as a substantial advance over existing work, particularly Perez-Beltrachini et al., and justify the use of the NNLG baseline. Unlike LREC, ACL typically requires new results from a system utilizing the corpus for a main session paper on corpus development methodology. 
To strengthen the paper, an analysis of syntactic constructions in both corpora would be beneficial, providing a more direct comparison of complexity. The methodology for determining the number of different path shapes should be detailed, ideally in relation to syntactic constructions. 
Furthermore, the authors should acknowledge the limitation of their method in incorporating richer discourse relations, such as Contrast, Consequence, and Background, which are essential in NLG. A comparison with more comprehensive corpora, like those described by Walker et al. (JAIR-2007) and Isard (LREC-2016), would be informative.
References:
Marilyn Walker, Amanda Stent, François Mairesse, and Rashmi Prasad. 2007. Individual and domain adaptation in sentence planning for dialogue. Journal of Artificial Intelligence Research (JAIR), 30:413–456.
Amy Isard, 2016. "The Methodius Corpus of Rhetorical Discourse Structures and Generated Texts" , Proceedings of the Tenth Conference on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia, May 2016.
Addendum following author response:
The author's response provides valuable clarifications, leading to an increased overall rating. However, the advance over Perez-Beltrachini et al. could be more clearly highlighted. The key difference between this dataset and Wen et al.'s lies in content selection, which is crucial. The methodology for constructing the data-to-text dataset seems to involve straightforward crowd-sourcing steps, and any innovative aspects should be emphasized. 
The rejection of 8.7% of crowd-sourced texts during verification is notable, and examples of rejected texts could provide insight into the quality of the dataset. The main contribution lies in enabling comparisons between the two corpora at both data and text levels. 
Regarding the NNLG baseline, the relative performance difference between the two corpora might disappear with more advanced methods, such as Wen et al.'s. This assumption should be acknowledged, and the comparison remains a useful component of the dataset evaluation. 
The importance of the dataset, rather than the presence of system results, is the key factor in considering publication in ACL. Finally, the role of domain dependence and the meaning of "wide coverage" should be clarified in the final version of the paper, if accepted.