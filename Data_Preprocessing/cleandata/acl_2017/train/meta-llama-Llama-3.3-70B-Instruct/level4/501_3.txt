Strengths:
The DMC task appears to be a suitable assessment of language and vision understanding, with a well-defined evaluation metric. The poor performance of the caption generation model on this task is noteworthy, as it highlights the limitations of these models in capturing image semantics, despite their proficiency in language modeling.
Weaknesses:
A key baseline is missing from the experiments: a state-of-the-art VQA model trained solely on a yes/no label vocabulary. Additional details on the human performance experiments would be beneficial, such as the proportion of incorrectly predicted images due to genuinely ambiguous captions, and potential avenues for data refinement to achieve higher human accuracy.
General Discussion:
A concern with this paper is the potential ease or gameability of the dataset. To address this, the authors could run a range of strong baselines on the dataset and report their accuracies. The current experiments are not entirely convincing, as the chosen neural network architectures differ significantly from state-of-the-art architectures in similar tasks, which often employ attention mechanisms over the image. A valuable addition to the paper would be an analysis of the dataset, including the average number of shared tokens between correct captions and distractors, and the type of understanding required to distinguish between them. This analysis would help readers understand the task's value relative to similar tasks. The data generation technique is relatively simple and may not constitute a significant contribution unless it yields surprisingly good results.
Notes:
The description of the FFNN architecture is unclear and cannot be found in the paper or supplementary material. It appears to be a convolutional network over tokens, but the details are unclear. The application of the Veq2Seq+FFNN model to both classification and caption generation is also confusing. Specifically, it is unclear whether the log likelihood of the caption is combined with the FFNN prediction during classification, or whether the FFNN score is incorporated during caption generation. The fact that the caption generation model performs significantly worse than random chance requires explanation. The description of the neural network on page 528 is difficult to understand and could be improved by starting the section with the final paragraph, which provides clearer context.