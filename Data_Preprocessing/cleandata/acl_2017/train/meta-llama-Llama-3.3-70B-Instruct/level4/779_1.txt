This paper presents a novel approach to zero-resource translation, leveraging (source, pivot) and (pivot, target) parallel corpora. The method involves training a teacher model for p(target|pivot) on the (pivot, target) corpus, followed by training a student model for p(target|source) to minimize relative entropy with respect to the teacher on the (source, pivot) corpus. The use of word-level relative entropy over samples from the teacher yields improved performance compared to previous pivoting variants and other zero-resource strategies.
The contribution is noteworthy, offering a fresh idea, clear explanations, and compelling empirical evidence. Unlike prior work, it relies on minimal assumptions about the underlying NMT systems, making it widely applicable.
To further enhance the research, several experiments are suggested. Firstly, investigating the approach's robustness to more dissimilar source and pivot languages could provide valuable insights, as the true p(target|source) and p(target|pivot) are likely to be more distinct. Secondly, given the success of word-based diversity, exploring sentence n-best or sentence-sampling experiments could be beneficial, albeit more computationally expensive. Additionally, examining the transition from word-based diversity to sentence-based as the student converges could yield interesting results.
Some specific comments are also provided. On line 241, "Despite its simplicity" could be rephrased as "Due to its simplicity". On line 277, "target sentence y" should be changed to "target word y". On line 442, clarification is needed on how the current context is determined, whether greedily or with a beam, when comparing probabilities of the most probable and 5 most probable words.
In Section 4.2, the comparison with a uniform distribution may not be informative, and it would be more interesting to investigate the usefulness of p(y|z) as p(y|x) improves. This could be achieved by comparing p(y|z) to models for p(y|x) trained on varying amounts of data or for different numbers of iterations. The effect of the mode approximation compared to n-best for sentence-level scores could also be explored.
On line 555, it is surprising that word beam performs worse than word greedy, as word beam should be closer to word sampling. An explanation for this discrepancy would be helpful. Finally, on line 582, the claimed advantage of sent-beam may be due to noise, given the high variance of the curves.