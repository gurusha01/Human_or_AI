This paper exhibits several strengths, including its engaging writing style, intriguing results, and innovative methodology. The comparisons drawn with preceding approaches are both informative and enlightening. Furthermore, the meticulously annotated corpus is poised to become a valuable asset for the research community. The qualitative analysis presented in section 5 is particularly noteworthy, as it offers profound insights that complement the results, a rarity in many machine learning papers that often merely present results without further elaboration.
However, there are a few areas that require improvement. In section 4.1, the explanation of the model's input being set to zeroes lacks clarity until referenced against Figure 2. An additional explanatory sentence would enhance understanding. Moreover, Figure 2 contains a potential discrepancy; the input layers to the LSTMs are labeled as "5*Embeddings(50D)" for networks that utilize dependency labels as input, which appears to be incorrect. Clarification or correction of this point would be beneficial.
Regarding the broader discussion, the justification for using LSTMs in section 4.2, citing their excellence in modeling language sequences, seems misplaced. Given that each datapoint involves feeding the network all five words of an example simultaneously, and examples are independent of each other, the problem does not inherently involve sequential modeling. While LSTMs may still offer superior performance, the rationale provided does not align with the problem's nature. The authors' perspective on this observation would be appreciated to address any potential misunderstanding.