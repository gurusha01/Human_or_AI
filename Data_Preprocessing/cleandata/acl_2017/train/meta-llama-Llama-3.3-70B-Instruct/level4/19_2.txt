Strengths:
The proposed approach is innovative, and the results demonstrate significant promise, surpassing the current state-of-the-art.
Weaknesses:
The linguistic justification underlying the paper is problematic, as discussed below. A more nuanced interpretation of the results would substantially enhance the paper's quality.
General Discussion:
This study introduces a novel method for Zero Pronoun Resolution in Chinese, leveraging a unique procedure to generate a large amount of relevant data from unlabeled documents. This data is then cleverly integrated into a neural network-based architecture during the pre-training phase, yielding improved results that exceed the current state-of-the-art.
I have mixed sentiments regarding this research. On one hand, the approach appears sound and exhibits promising results, outperforming recent systems, such as Chen & Ng (2016). On the other hand, the framing of the primary contribution is troubling from a linguistic perspective. Specifically, zero pronoun resolution is, linguistically speaking, a context modeling task that requires accurate interpretation of discourse, salience, semantic, and syntactic cues. It assumes that zero pronouns are used in specific contexts where full noun phrases are not typically possible. From this viewpoint, generating zero pronoun data by replacing nominal phrases with zeroes ("blank") is not convincing. As the authors themselves demonstrate, the pre-training module alone does not achieve reasonable performance. In summary, I do not believe that the generated pseudo-data can be considered authentic zero pronoun data. Instead, it seems to encode some form of selectional preferences. It would be beneficial if the authors could invest effort in better understanding what the pre-training module learns and then reformulate the corresponding sections.
The paper would benefit from proofreading by a native English speaker, as some sentences, such as the one on lines 064-068, are not grammatically correct.
Other points:
Lines 78-79: Are there any restrictions on the nouns and, particularly, pronouns? For instance, do you apply this strategy to very common pronouns, such as English "it"? If so, how do you ensure that the two occurrences of the same token are indeed coreferent?
Line 91: The term "antecedent" typically denotes a preceding mention that is coreferent with the anaphor, which is not the intended meaning here.
Line 144: "OntoNotes" contains a typo.
Lines 487-489: It has been shown that evaluation on gold-annotated data does not provide a reliable estimate of performance. Recent studies on coreference, including those by Chen & Ng, evaluate performance on system mentions. Please consider re-running your experiments to obtain a more realistic evaluation setup.
Line 506: I do not understand the meaning of the dagger symbol over the system's name. Is your improvement statistically significant across all domains, including bn and tc?
Line 565: "Learn" contains a typo.
Section 3.3: In this section, you use the abbreviation "AZP" instead of "ZP" without introducing it. Please unify the terminology throughout the paper.
References: Please double-check the capitalization of references to ensure accuracy.