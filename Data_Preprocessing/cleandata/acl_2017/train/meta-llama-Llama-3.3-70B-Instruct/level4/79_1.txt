This manuscript addresses the issue of knowledge base completion, proposing the ITransF model, which introduces a novel approach to parameter sharing across different relations. Unlike the STransE model, which assigns a unique matrix to each relation, ITransF constructs a tensor D comprising various relational matrices as its slices. A selectional vector, denoted as \alpha, is utilized to select a subset of relevant relational matrices for composing a particular semantic relation. The authors also discuss a method to induce sparsity in \alpha. The experimental results on two benchmark datasets demonstrate the superiority of ITransF over previous proposals.
The paper is well-structured, and the experimental results are compelling. However, several concerns need to be addressed by the authors. 
1. Simply arranging relational matrices in a tensor and selecting (or taking a linearly weighted sum of) the relational matrices does not guarantee information sharing between them. A more effective approach would involve tensor decomposition, projecting the different slices into a common lower-dimensional core tensor, thereby facilitating information sharing. It is unclear why this approach was not adopted, given the motivation to share information across relational matrices.
2. The two objectives of sharing information across different relational matrices and inducing sparsity in the attention vectors seem contradictory. If the attention vector is truly sparse, with many zeros, information will not be propagated to the corresponding slices during optimization.
3. The authors devote considerable space to discussing techniques for computing sparse attention vectors. Although they mention that \ell1 regularization was ineffective in preliminary experiments, no experimental results are provided to support this claim, and the reasons for its unsuitability are not explained. To this reviewer, \ell1 regularization appears to be an obvious baseline, particularly given its ease of optimization. The use of \ell0 regularization leads to NP-hard optimizations, which could have been avoided by using \ell1 regularization.
4. The vector \alpha performs selection or weighting over the slices of D. Referring to this as "attention" may be misleading, as this term is typically used in NLP to describe a different type of model, such as those used in machine translation.
5. It is unclear why the optimization process is initialized with pre-trained embeddings from TransE. Why not randomly initialize the embeddings, as done in TransE, and then update them? Using TransE as the initial point may not provide a fair comparison.
The concept of learning associations between semantic relations has been explored in related NLP problems, such as relational similarity measurement and relation adaptation. It would be beneficial to contextualize the current work within the framework of these prior proposals, which model inter-relational correlation and similarity.
Overall, the paper presents an interesting approach to knowledge base completion, but addressing these concerns will strengthen the manuscript and provide a more comprehensive understanding of the proposed model.