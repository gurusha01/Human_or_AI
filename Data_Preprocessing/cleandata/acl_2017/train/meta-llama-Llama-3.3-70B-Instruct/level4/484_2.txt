This paper explores a hybrid approach to speech recognition by integrating two non-HMM based techniques: Connectionist Temporal Classification (CTC) and attention-based sequence-to-sequence (seq2seq) networks. The proposed combination is twofold: 
1. initially, the model is trained using multitask learning with a combined CTC and seq2seq loss function, similar to the approach presented by Kim et al. in 2016.
2. subsequently, in a novel contribution, the scores from the CTC model and the seq2seq model are combined during the decoding process, where the results of beam search over the seq2seq model are rescored using the CTC model.
The primary innovation of this paper lies in utilizing the CTC model not only as an auxiliary objective during training, as originally proposed by Kim et al. in 2016, but also as a component in the decoding process.
- Strengths:
The paper highlights several issues arising from the flexibility of the attention mechanism and demonstrates that combining the seq2seq network with CTC effectively mitigates these problems.
- Weaknesses:
The paper builds upon the work of Kim et al. 2016, offering an incremental improvement. While it is straightforward to ensemble the outputs of two trained models, the simplicity of this modification yields significant performance enhancements in Automatic Speech Recognition (ASR) systems.
- General Discussion:
A substantial portion of the paper is dedicated to explaining traditional ASR systems, with the core innovation of the improved decoding algorithm only being introduced on page 5.
The explanation of CTC deviates from the standard presentation and could benefit from either a more conventional description or an expanded explanation. Typically, the relationship p(C|Z) (equation 5) is deterministic, implying a one-to-one correspondence between the character sequence and its blank-expanded form Z. Furthermore, the final transformation in equation 5 is unclear.