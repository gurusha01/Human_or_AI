Overview:
This paper presents a novel approach to training sense embeddings, leveraging a lexical-semantic resource, specifically WordNet. Although the learned sense vectors are not directly evaluated for their meaningfulness, they are instead combined to form word embeddings, which are then assessed in a downstream task, namely PP attachment prediction.
Strengths:
The results obtained for PP attachment appear to be robust.
Weaknesses:
The meaningfulness of the sense embeddings remains unexplored. 
The probabilistic model contains certain aspects that are difficult to comprehend. It is unclear whether the \lambdawi are hyperparameters or learned during training. Additionally, the origin of the term "rank" is ambiguous; it is uncertain whether this is derived from the sense rankings in WordNet.
Related work: The concept of representing word embeddings as a convex combination of sense embeddings has been previously proposed. For example, Johansson and Nieto Pi√±a's work, "Embedding a semantic network in a word space" (NAACL, 2015), decomposed word embeddings into ontology-grounded sense embeddings based on this idea. Similarly, this concept has been applied to unsupervised sense vector training, as seen in the work of Arora et al, "Linear Algebraic Structure of Word Senses, with Applications to Polysemy".
Minor comments:
The definitions of types and tokens can be omitted, as they are standard terminology in the field.
The necessity of the first \lambdawi in equation 4 is questionable, given that the probability is unnormalized.
General Discussion: