The authors introduce 'morph-fitting', a novel approach that refines pre-trained word embeddings using a morphologically-driven objective. This objective serves two primary purposes: (1) it brings inflectional forms of the same word closer together, such as 'slow' and 'slowing', and (2) it pushes derivational antonyms further apart, such as 'expensive' and 'inexpensive'. The ultimate goal of this method is to enhance the representation of low-frequency word inflections and mitigate the tendency of corpus-based word embeddings to assign similar representations to antonyms. Morph-fitting relies on relatively simple, manually-constructed morphological rules and has been demonstrated to be effective in multiple languages, including English, German, Italian, and Russian. The experiments conducted by the authors include intrinsic word similarity benchmarks, which show significant performance improvements when morph-fitting is applied to various corpus-based embeddings. Furthermore, the method yields state-of-the-art results in dialog state tracking tasks for German and Italian.
The strengths of this work include:
- The simplicity and effectiveness of the proposed method, which achieves notable performance improvements across multiple evaluations and languages. Unlike previous knowledge-based retrofitting approaches, such as Faruqui et al. (2015), morph-fitting relies on a limited set of manually-constructed rules rather than a large-scale knowledge base.
- The ease of applying this method to existing word embeddings, making the software intended for release potentially useful to the community.
- The clear description of the method and experiments.
However, there are some weaknesses:
- The lack of analysis on why morph-fitted embeddings perform better in evaluations and how well this corresponds to the authors' intuitive motivation.
- The introduction of a synthetic word similarity evaluation dataset, Morph-SimLex, which is generated by applying the same morphological rules used in morph-fitting to SimLex999. The similarity scores in this dataset are presumed and less reliable, and the fact that it was generated using the same rules as the method being evaluated means that the results should be viewed with caution.
- The mention of (Soricut and Och, 2015) as a future source for morphological knowledge, when in fact it is an alternative approach to generating morphologically-aware word representations.
- The absence of strong morphologically-informed embedding baselines in the evaluation.
In general, this work represents a nice contribution to the community, presenting a simple approach that yields significant improvements using various common embeddings on several evaluations and four different languages. However, the authors should address the noted weaknesses to further strengthen their work.
Some minor comments include:
- Clarifying the phrasing in Line 200 regarding the querying of linguistic constraints.
- Elaborating on the differences between the model used in this paper and the one it is based on in Wieting 2015, particularly the addition of the REPEL part.
- Providing a clear equation for the method's cost function, which consists of three terms.
- Specifying the vector representations of words and whether the vectors are L2-normalized before the process, as well as the similarity metric used when computing 'nearest neighbor' examples.
- Moving the text in Lines 297-299 to Section 3 and noting that the parameters were not fine-tuned in the main text.
- Correcting the example in Line 327, as (create, creates) seems incorrect for the given rule.