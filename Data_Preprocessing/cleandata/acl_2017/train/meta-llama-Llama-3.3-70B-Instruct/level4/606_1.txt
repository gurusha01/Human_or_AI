This paper proposes a novel approach to semantic parsing, leveraging a neural sequence-to-sequence (seq2seq) model, referred to as the "programmer", to encode natural language questions and generate programs. The programmer is augmented with a 'key variable' memory component that stores entities from the questions and intermediate variable values formed during program execution, which are utilized to further construct the program. Additionally, the model incorporates discrete operations, such as argmax or 'hop to next edges in a KB', which are executed by a separate "interpreter/computer" component that also stores intermediate values. This component acts as a syntax/type checker, ensuring the decoder generates valid tokens, for instance, requiring the second argument to the "hop" operation to be a KB predicate. The model is trained using weak supervision, directly optimizing the evaluation metric (F score), and due to the discrete operations and non-differentiable reward functions, policy gradients (REINFORCE) are employed. To mitigate the high variance of gradients obtained through REINFORCE, the paper adopts an iterative maximum likelihood approach to find good sequences of actions. The results and discussion are presented clearly, with the model achieving state-of-the-art results on the WebQuestions dataset compared to other weakly supervised models.
The paper is well-written and easy to follow, presenting a new and exciting direction with potential for future research. The presentation of this work at the conference would be highly desirable.
Key questions for the authors:
1. An alternative training approach would involve bootstrapping the parameters (Î¸) from the iterative ML method, eliminating the need for pseudo gold programs in the beam (deleting Line 510). Was this approach explored, and if so, why was it deemed ineffective?
2. The baseline model in REINFORCE and the value function prediction network, if used, should be discussed in detail within the paper.
3. Were there programs that required multiple hop operations, or were they limited to single hops? If multiple hops were used, providing an example would be beneficial (although the word limit is acknowledged).
4. An example illustrating the usage of the filter operation would be helpful.
5. The motivation behind replacing entities in the question with the special ENT symbol is unclear.
Minor comments:
- Line 161: "describe" should be "describing"
- Line 318: "decoder reads" should be "decoder generates"