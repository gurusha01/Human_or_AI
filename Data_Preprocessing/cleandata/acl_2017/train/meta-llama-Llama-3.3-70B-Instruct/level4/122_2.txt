This paper proposes a neural network-based approach to dialogue state tracking, with a primary focus on learning representations of user utterances, system outputs, and ontology entries, leveraging pre-trained word vectors. 
The authors investigate two distinct neural network models, NBT-DNN and NBT-CNN, for utterance representation, and combine the learned representations to inform binary decisions for slot value pairs in the downstream network. 
The experimental results demonstrate significant performance gains over the baseline delexicalized approach.
Overall, the work is of high quality, with a clear objective, reasonable methodology, and improved outcomes compared to prior studies. 
However, the paper's organization could be enhanced to more effectively convey the details, particularly for readers unfamiliar with the field.
To improve clarity, a formal definition of Dialogue State Tracking (DST) should be provided at the outset, as its connection to Spoken Language Understanding (SLU) may cause confusion. 
It is recommended to introduce a general dialogue system architecture in Section 1, followed by a problem definition of DST that highlights its relationships with Automatic Speech Recognition (ASR), SLU, and policy learning.
Additionally, defining all notations used throughout the paper in an earlier section would enhance readability, as some symbols (e.g., tq, ts, t_v) are introduced before their descriptions.
Further comments and questions include:
- Is it possible to perform separate SLU using this model, or could the term 'joint' be misleading, implying the model can handle both tasks?
- Could the authors provide statistics on the number of errors corrected from the original DSTC2 dataset? If the number is not substantial, the experiment could include comparisons with other published work using the same dataset.
- What are the authors' thoughts on utilizing Recurrent Neural Networks (RNNs) or Long Short-Term Memory (LSTM) networks to learn sequential aspects in utterance representations, considering their recent successes in SLU problems?
- More details about the semantic dictionary used with the baseline would be helpful in understanding the cost of manually building such resources.
- It would be beneficial to provide examples of samples that were not correctly predicted by the baseline but were resolved using the proposed models.