This study leverages Gaussian mixtures to model words, showcasing their capability in capturing multiple meanings of polysemous words. The training process employs a max-margin objective, utilizing the expected likelihood kernel to measure the similarity between word distributions. Experimental results on word similarity and entailment tasks demonstrate the efficacy of the proposed approach.
- Strengths:
The problem statement is well-defined and clearly motivated. The use of Gaussian mixtures offers a more expressive representation compared to deterministic vector models, potentially capturing distinct word meanings through their modes, along with the associated probability mass and uncertainty. This work constitutes a significant contribution to the field of word embeddings. 
The proposed max-margin learning objective, accompanied by a closed-form similarity measurement, enables efficient training. The paper is generally well-written, contributing to its overall clarity.
- Weaknesses:
Please refer to the questions posed below for further discussion.
- General Discussion:
A crucial parameter in Gaussian mixture models is the number of components (k). In the experiments presented, k is fixed at 2. It would be beneficial to understand the criteria used to select this value. Does increasing k adversely affect the model's performance? Furthermore, it would be interesting to visualize the learned distribution for a word with a single dominant meaning.
The spherical case is used in all experiments, where the covariance matrix simplifies to a single value. Is this choice primarily driven by computational efficiency? The performance of using a general diagonal covariance matrix, which allows the Gaussian mixture to define varying degrees of uncertainty along different directions in the semantic space, seems worthy of exploration.
Minor comments:
Table 4 is not referenced in the main text. Additionally, the reference to Luong et al. is missing the publication year.
The response to the initial review has been considered.