The paper presents a novel recurrent neural network architecture capable of selectively skipping non-essential input units by defining parameters R (the number of words to process at each step), K (the maximum jump size), and N (the maximum number of jumps permitted). This architecture utilizes an LSTM to process R words, predict a jump size k (where 0 indicates termination), and then skip k-1 words, repeating this process until the maximum number of jumps is reached or the end of the input is attained. Although the model is non-differentiable, it can be trained using standard policy gradient methods, bearing similarities to the reinforcement learning approach employed by Shen et al. (2016) for multi-pass machine reading.
- Strengths:
The proposed model effectively simulates human "skimming" behavior, analogous to Shen et al.'s simulation of repeated, self-terminated reading. A significant advantage of this work is its simplicity, which, despite its straightforward nature, yields favorable outcomes. Notably, the authors demonstrate through a well-designed synthetic experiment that the model can learn to skip irrelevant information when provided with oracle jump signals. Furthermore, in text classification tasks using real-world datasets, the model performs competitively with non-skimming models while exhibiting improved efficiency.
The model has potential practical implications, particularly for tasks where skimming is sufficient (e.g., sentiment analysis), suggesting that equivalent results can be obtained without processing all data, a finding that appears to be novel.
- Weaknesses:
The basis for the model's effective jumping behavior, beyond the synthetic dataset, is somewhat obscure. Consider a scenario where crucial information is located at the end of a sentence, such as: "The movie was so so and boring to the last minute but then its ending blew me away." In this case, the model might decide to skip the remainder of the sentence after reading "so so and boring," potentially missing the crucial phrase "ending blew me away" and misclassifying the instance as negative. While running the skimming model in both directions, as suggested by the authors for future work, could address this issue, the model may require a more sophisticated architecture to control skimming effectively.
Combining skimming with multi-pass reading, potentially in reverse directions, could yield improved performance, mirroring human reading behaviors for complex texts. This approach might offer a more comprehensive understanding of the input.
Overall, the work poses an intriguing problem and provides an effective, albeit intuitive, solution, highlighting the potential for further research and development in this area.