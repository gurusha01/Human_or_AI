This manuscript introduces a novel corpus of annotated essay revisions, accompanied by two illustrative applications: analyzing student revision behavior and identifying revisions automatically, with the latter being a text classification task employing a support vector machine (SVM) classifier and a range of features. The authors intend to make the corpus freely available for research purposes.
The paper is commendable for its clarity and the detailed annotation scheme utilized by two annotators, which significantly enhances the corpus's value. The resource is likely to appeal to researchers focusing on writing process research and related areas. The provision of two clear usage scenarios for the corpus is also a notable strength.
However, there are two significant criticisms that need to be addressed. The first can be rectified with minor revisions if the paper is accepted, but the second requires more substantial work.
1. A critical omission in the paper is the lack of statistical information about the corpus. Essential details such as the number of documents (presumably 180, given 60 essays with 3 drafts each), the number of tokens (approximately 400 words per essay), and the number of sentences should be included. Assuming 60 unique essays with about 400 words each, the total word count would be around 24,000 words, which would triple to about 72,000 words when considering all three drafts, albeit with significant overlap. Incorporating a table with these statistics would enhance the paper.
2. If the aforementioned estimates are accurate, the corpus is relatively small. While the challenge of producing hand-annotated data is acknowledged, and this is indeed a strength of the work, the utility of this resource for the broader NLP community is questionable. It might be more appropriate to present such a specialized resource at a workshop like BEA or a conference focused on language resources, such as LREC, rather than a general NLP conference like ACL. The authors' intention to augment the corpus with additional annotations raises the question of whether they also plan to expand it with more essays.
Minor comments and suggestions include:
- The inclusion of essays from both native and non-native speakers presents an additional potential application for this corpus: native language identification (NLI).
- On page 7, specifying "word unigram" instead of just "unigram feature" would add clarity.
- The statement "and the SVM classifier was used as the classifier" on page 7 is redundant and could be omitted.