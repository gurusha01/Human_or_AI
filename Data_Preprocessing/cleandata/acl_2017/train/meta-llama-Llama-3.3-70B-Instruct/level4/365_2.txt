Review - Summary:
This paper explores the application of a sequence-to-sequence (seq2seq) approach to normalize German historical texts. The authors demonstrate that incorporating grapheme-to-phoneme generation as an auxiliary task within a multi-task learning (MTL) seq2seq framework enhances performance. They claim that the MTL approach renders the attention mechanism unnecessary, providing experimental evidence that attention mechanisms can actually degrade MTL performance. Furthermore, the authors attempt to establish a statistical correlation between the weights of an MTL normalizer and an attention-based one.
Strengths:
1) The paper presents a novel application of seq2seq to historical text correction, building upon recent work in sentence grammatical error identification [1].
2) The authors successfully show that utilizing grapheme-to-phoneme as an auxiliary task in an MTL setting improves the accuracy of text normalization.
Weaknesses:
1) Rather than dismissing the attention mechanism, the authors should investigate the reasons behind its ineffectiveness in the MTL approach and consider modifying it to improve performance.
2) The paper would benefit from referencing prior work on seq2seq MTL, such as [2] and [3], which also explored non-attention seq2seq models.
3) The evaluation is limited to a single German historical text dataset consisting of 44 documents. It would be valuable to assess the approach using additional languages or datasets.
References:
[1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016. Sentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.
[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR'16.
[3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. Multi-task learning for multiple language translation. ACL'15
---------------------------
Reply to Authors' Rebuttal:
I am maintaining my review score of 3, indicating that I do not object to the paper's acceptance. However, I am not increasing my score for two reasons:
* The authors failed to address my questions regarding other papers on seq2seq MTL that also avoided using attention mechanisms. As a result, the primary novelty of this work lies in its application to text normalization.
* While it is relatively straightforward to demonstrate that a particular approach (in this case, attention in seq2seq MTL) is ineffective, the true value lies in understanding why it fails and modifying the approach to make it work.