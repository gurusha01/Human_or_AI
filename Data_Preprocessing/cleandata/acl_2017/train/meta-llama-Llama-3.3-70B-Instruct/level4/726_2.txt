This paper presents a straightforward attention-based RNN model for natural language to SQL query generation, eliminating the need for intermediate representations. The authors implement a data augmentation strategy, where crowd-annotated data is iteratively collected based on user feedback regarding the model's SQL query output. Experimental results on both benchmark and interactive datasets demonstrate the effectiveness of this data augmentation approach.
Strengths:
- The model's design avoids the use of intermediate representations, simplifying the process.
- The release of a potentially valuable dataset on Google Scholar is a notable contribution.
Weaknesses:
- The claim of achieving state-of-the-art performance is not substantiated by the results on GeoQuery and ATIS datasets.
General Discussion:
This research contributes meaningfully to the field and has potential implications for future semantic parsing applications. However, the assertion of "near-state-of-the-art accuracies" on ATIS and GeoQuery seems inaccurate, given the 8-point difference from Liang et al. (2011). The focus on achieving state-of-the-art results may not be the primary emphasis of this paper, as it offers significant contributions in its own right. To enhance the paper's suitability for ACL, it would be beneficial for the authors to moderate their claims. Several questions and suggestions for clarification are also posed:
- Could the authors provide a clearer definition of "minimal intervention," as it is unclear whether this refers to minimal human intervention or the absence of intermediate representations?
- In Table 6, a breakdown of the score by correctness and incompleteness would be helpful, along with the percentage of incompleteness exhibited by these queries.
- What level of expertise is required from crowd workers tasked with generating correct SQL queries?
- Analyzing the 48% of user questions that could not be generated would provide valuable insights.
- Figure 3 is somewhat confusing, particularly the sharp dips in performance around stages 8 and 9, which could be clarified with additional explanation.
- Table 4 requires further clarification regarding the splits used to obtain the ATIS numbers.
The authors' response is appreciated, and addressing these points will strengthen the paper.