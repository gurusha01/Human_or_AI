This manuscript presents various methods for encoding sequences of digits of arbitrary length utilizing the major system, where each digit is associated with one or more consonantal phoneme representations. The encoding process yields a sequence of words, with digits from the original sequence corresponding to characters or digraphs in the output, while vowels added to form words are unconstrained. The paper explores techniques to enhance the memorability of the output sequence by applying syntactic constraints and heuristics.
I found the application of natural language processing concepts in this paper to be somewhat intriguing, given the novelty of the topic in the context of ACL. However, the overall approach and ideas presented seem somewhat outdated, with a strong emphasis on n-gram models, frequent POS-tag sequences, and other heuristics that could have been explored 15-20 years ago. I question whether the ideas presented here demonstrate sufficient novelty to warrant publication in ACL in 2017, as they do not contribute significantly to the field of NLP, either in terms of modeling or search, and the contribution to the application area is limited to a specific instance of constrained generation.
Given the straightforward monotonic mapping between the input and output sequences, it appears that a character-based sequence-to-sequence encoder-decoder model, such as the one proposed by Sutskever et al. in 2014, could be effectively employed in this context. This approach would likely yield fluent output with fewer complexities, such as trigram models, POS tag and scoring heuristics, and postprocessing with a bigram model. Moreover, this method would allow for large-scale training on diverse genres without relying on pre-tagged corpora or parsers, making it a more contemporary and relevant approach for a 2017 paper.