The paper presents a straightforward yet effective approach to morphological paradigm completion in low-resource settings by utilizing a character-based seq2seq model. This model is trained on a combination of examples from two languages: one that is resource-poor and another that is closely related but resource-rich. Each training example is annotated with paradigm properties and a language identifier, enabling the model to facilitate transfer learning across languages that share common characters and paradigms. Although the proposed multi-lingual solution is not entirely novel, as similar architectures have been explored in syntax, language modeling, and machine translation, the innovation of this paper lies in its application to morphology. The experimental results demonstrate significant improvements over monolingual baselines and include a comprehensive analysis of how language similarities impact the quality of the results. The paper is engaging, well-written, and is expected to make a valuable contribution to the conference program.
Detailed comments:
— A primary inquiry is why the proposed general multilingual methodology was restricted to pairs of languages, rather than being extended to sets of similar languages. For instance, incorporating all Romance languages into the training could enhance Spanish paradigm completion, and mixing all Slavic languages that use the Cyrillic script could improve Ukrainian paradigm completion. It would be intriguing to explore the extension of the models from bi-lingual to multilingual settings.
— The use of Arabic as a baseline seems unfair and lacks significance, given the substantial differences in its script and morphology compared to the target languages. A more compelling baseline could be a language that shares a partially shared alphabet but has a different typology, such as a Slavic language using the Latin script as a baseline for Romance languages. Even without Arabic and considering a more distant language within the same family as a baseline, the experimental results remain robust.
— The half-page discussion on the contribution of Arabic as a regularizer adds minimal value to the paper. It might be more effective to remove Arabic from all experiments and introduce a regularizer, which, according to footnote 5, is even more effective than using Arabic as a transfer language.
— The related work section lacks mention of a line of research on "language-universal" RNN models, which employ a similar approach by learning shared parameters for inputs in multiple languages and adding a language tag to the input to differentiate between languages. Relevant studies include a multilingual parser (Ammar et al., 2016), language models (Tsvetkov et al., 2016), and machine translation (Johnson et al., 2016).
Minor comments:
— The claim in line 144 that POS tags are easily transferable across languages may not be accurate. The transfer of POS annotations is also a challenging task.
References:
Waleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah A. Smith. "Many languages, one parser." TACL 2016.
Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. "Polyglot neural language models: A case study in cross-lingual phonetic representation learning." NAACL 2016.
Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat et al. "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation." arXiv preprint arXiv:1611.04558 2016.
Response to author response:
Thank you for your response, and I look forward to reading the final version.