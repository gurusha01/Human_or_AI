Strengths:
The paper proposes innovative concepts, employs straightforward neural learning, and demonstrates intriguing performance, albeit not exceptionally impressive. Additionally, it has a broad range of potential applications.
Weaknesses: The amount of new content and clarity in certain sections are concerns.
This manuscript introduces a neural learning approach for entity disambiguation and linking, presenting a compelling idea to integrate entity, mention, and sense modeling within the same neural language modeling framework. The simplicity of the training procedure, combined with the modeling, enables support for a wide range of applications.
While the paper is formally well-structured, the discussion occasionally lacks consistency with the technical ideas presented.
The empirical evaluation is satisfactory, although the reported performance improvements are not remarkable. Despite appearing to build upon the work of Yamada et al. (CoNLL 2016), the paper contributes novel ideas and remains relevant.
The weaker aspects of the paper include:
- The writing is not always clear, with Section 3 being particularly unclear. Certain details in Figure 2 are unexplained, and the terminology is somewhat redundant. For instance, the distinction between the dictionary of mentions and the dictionary of entity-mention pairs is unclear, as is their relationship to text anchors and types for annotated text anchors.
- The paper bears a strong resemblance to Yamada et al. (2016), and the authors should explicitly outline the differences between the two.
A general observation about the current version is:
The paper evaluates the Multiple Embedding model against entity linking/disambiguation tasks. However, word embeddings are utilized not only for such tasks but also for processes that do not directly depend on entities in the knowledge base, such as parsing, coreference, or semantic role labeling.
The authors should demonstrate that the word embeddings provided by the proposed MPME method are not inferior to those of simpler word spaces in these other semantic tasks, which involve entity mentions directly.
I have reviewed the authors' response.