This submission presents several notable strengths, including the provision of detailed guidelines accompanied by explicit illustrations, which enhances clarity and understanding.
However, a significant weakness lies in the reliability of the document-independent crowdsourcing annotation approach utilized. 
The overall work contributes by establishing a novel benchmark corpus for concept-map-based Multi-Document Summarization (MDS), showcasing a well-organized structure and clear writing. The supplementary materials provided are also deemed sufficient. Two key questions arise from this work:
1. Is the separation of concept map extraction as an independent task truly necessary? While many generic summarization systems construct knowledge graphs to generate summaries, the complexity and distinguishability of concept maps increase with the number of nodes, potentially making general summaries more readable and preferable.
2. What methodology can be employed to determine the importance of a concept independently of the documents? Given that summarization aims to preserve the main concepts of documents, the importance of a concept is heavily document-dependent. For instance, in the context of coal mining accidents, the relative importance of concepts such as specific instances versus causes of accidents would vary based on the document's focus, making it challenging to assess their importance without document-specific context.
The authors' effort in constructing this dataset is appreciated. Nonetheless, the dataset appears to align more closely with a knowledge graph grounded in common sense rather than a summary, highlighting a potential discrepancy in its application.