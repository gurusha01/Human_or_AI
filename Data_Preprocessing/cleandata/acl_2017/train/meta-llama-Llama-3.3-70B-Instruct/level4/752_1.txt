The paper presents a compelling application of seq2seq models to AMR parsing and realization tasks, leveraging a pre-processed and linearized version of the AMR graph and associated sentence, combined with paired training. This approach yields competitive results, particularly in AMR realization, where the incorporation of additional monolingual data via back-translation proves effective. However, the parsing performance falls short of other reported papers, which utilized supplementary semantic information.
A notable strength of the paper lies in its demonstration of the efficacy of seq2seq models in AMR tasks, showcasing the potential of these models in handling complex semantic structures. The use of paired training and back-translation enables the model to capture nuanced relationships between the AMR graph and the corresponding sentence, leading to improved performance in AMR realization.
One of the primary weaknesses of the paper stems from the empirical comparisons drawn with other works, where multiple aspects and dimensions are concurrently changing, rendering direct comparisons challenging. For instance, the realization results in Table 2 are compared to PBMT, which was trained on a different dataset (LDC2014T12) comprising 13,051 sentences, whereas the proposed model was trained on LDC2015E86 with 19,572 sentences. To ensure a valid comparison, it is essential to re-evaluate the approaches using the same training data.
The paper raises several questions and areas for discussion, including the potential overlap between the sentences in the Gigaword sample and the test sentences of LDC2015E86, which may lead to test set contamination. The modifications made to the encoder, as described in lines 244-249, warrant further explanation, particularly regarding their impact on the model's effectiveness. Additionally, the implementation details, such as the final sequence length used and the seq2seq framework employed, should be clearly outlined for replication purposes.
The tables and figures presented in the paper require clarification, including the labeling of columns in Table 1 and the mismatch between the table and the accompanying text. The results for CAMR in Table 1 appear to be inconsistent with the reported values in the original paper. The handling of wikification information introduced in LDC2015E86 is also unclear.
The paper would benefit from a concluding section, summarizing the key findings and implications of the research. Furthermore, the decoding process, including the use of beam search, should be explicitly stated. The vocabulary size used in the experiments and the presence of unseen tokens in the dev/test sets should be addressed, along with the usage of the unknown word replacement mechanism.
The realization case study could be enhanced by evaluating the model's performance on phenomena known to be challenging for AMR, such as quantification and tense. A brief discussion motivating the use of AMR as opposed to other semantic formalisms would provide valuable context, as would an explanation of the benefits of leveraging human-annotated AMR information.
In terms of future work, it would be intriguing to investigate the factors contributing to the difference in effectiveness between the proposed approach and previous seq2seq methods, such as Peng et al. (2017). Isolating the impact of architecture, preprocessing, linearization, and data on the model's performance would provide valuable insights for future research.
Finally, the paper requires proofreading to address minor errors, including punctuation, formatting, and citation inconsistencies, to ensure clarity and readability.