This paper proposes a neural network-based approach to address the network embedding problem, incorporating both network structure and node-associated text through an attention mechanism that adapts textual representations based on neighboring node text.
- Strengths:
The proposed model effectively utilizes both network and text data to construct latent representations, with a sensible mutual attention approach. Additionally, the evaluation is comprehensive, covering multiple datasets, baselines, and tasks.
- Weaknesses:
Similar to other works in the network embedding domain that utilize neural network techniques inspired by word embeddings, this paper overlooks prior research on statistical and probabilistic network modeling. Notably, it fails to acknowledge and compare with the latent space model introduced by Peter Hoff et al. (P.D. Hoff, A.E. Raftery, and M.S. Handcock, Latent space approaches to social network analysis, J. Amer. Statist. Assoc., 97(460):1090â€“1098, 2002), which embeds nodes into a low-dimensional latent space and predates neural network-based approaches. Furthermore, given the paper's focus on modeling social network actors' diverse roles, it should reference and compare with the mixed membership stochastic blockmodel (MMSB) (Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P., Mixed membership stochastic blockmodels, Journal of Machine Learning Research, 2008), which allows nodes to randomly select roles when forming edges.
- General Discussion:
While the aforementioned statistical models do not incorporate text or scalable neural network implementations with negative sampling, they are grounded in well-principled generative models rather than heuristic neural network objectives and algorithms. Recent extensions of these models and inference algorithms offer improved scalability and incorporate text. It is unclear whether the performance difference between CENE and CANE in Figure 3 is statistically significant, prompting questions about the experimental setup, such as whether the experiments were repeated with random train/test splits. Additionally, it is essential to clarify whether the grid searches for hyperparameter values mentioned in Section 5.3 were performed using the test set, a validation set, or the training set, as this impacts the validity of the results.