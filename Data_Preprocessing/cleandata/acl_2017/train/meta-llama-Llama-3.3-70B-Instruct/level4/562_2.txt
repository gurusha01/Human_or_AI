This paper proposes a novel approach to relation extraction by reframing the task as a question answering problem, leveraging the idea that questions can serve as a more versatile and easily creatable medium for conveying content compared to specific relation examples. The reported results indicate promising performance, although a direct comparison with standard relation extraction tasks is lacking.
The strengths of this work include:
The technique demonstrates a high degree of proficiency in identifying relations, achieving an F-measure of nearly 90. It exhibits robust performance on unseen questions for familiar relations and decent performance on unseen relations. Furthermore, the authors outline a method for compiling a substantial training dataset.
However, several weaknesses are noted:
The absence of performance evaluation on conventional relation extraction datasets limits the ability to assess potential biases inherent in the data, which is generated from Wikidata via WikiReading and sourced from Wikipedia, differing from typical newswire or newsgroup datasets. A comparison using the NIST TAC-KBP slot filling dataset would be beneficial and appropriate. Additionally, the authors should consider training a relation detection model on the generated data to compare its efficacy with the question answering approach.
In general, the paper is well-structured, clearly argued, and presents an intriguing idea that yields decent results. Notably, the zero-shot NL method performs similarly to the single question baseline and is not significantly outperformed by the multiple questions system, which is an interesting finding.