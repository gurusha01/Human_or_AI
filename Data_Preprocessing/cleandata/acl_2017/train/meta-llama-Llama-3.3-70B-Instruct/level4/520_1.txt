This paper presents a methodology for creating datasets comprising images constructed from basic elements, along with their corresponding logical forms and linguistic descriptions. The apparent objective is to develop a framework where the complexity of both images and their descriptions can be systematically controlled and parameterized.
However, a significant drawback of this approach is its limited capacity to achieve complexity, falling short of the complexity levels commonly encountered in image-captioning and other multimodal tasks.
Moreover, the proposed method's simplicity diverges notably from the referenced bAbI tasks, which encompass a broad spectrum of reasoning tasks from easy to hard. In contrast, the method primarily allows for the quantitative escalation of difficulty in an easy image recognition task by introducing more objects or noise in somewhat artificial manners.
This limitation is also evident in the experimental section, where unsatisfactory performance results often seem to stem from basic overfitting or underfitting issues. These could potentially be addressed by adjusting the network capacity or utilizing more data, without offering deeper qualitative insights.
The introduction posits that the goal is not to attain optimal performance but to assess whether architectures can demonstrate the desired understanding. This statement contains a fundamental contradiction, as the task is intended to measure an architecture's understanding, yet the performance score is not meant to be taken seriously.
General comments:
The overall approach would benefit from being introduced earlier in the paper, ideally in the introduction rather than in section 3, to provide a clearer understanding of the methodology from the outset.