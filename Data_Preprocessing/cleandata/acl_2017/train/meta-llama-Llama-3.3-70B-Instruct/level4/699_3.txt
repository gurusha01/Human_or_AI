Review
Strengths:
The proposed model is novel and demonstrates a notable capability to generate keyphrases that are not present in the source text, which is a significant advantage.
Weaknesses:
A major concern is the lack of clarity regarding whether all evaluated models are trained and tested on the same datasets. Additionally, the explanation of the copy mechanism is not entirely clear or convincing, which hinders a full understanding of its contribution to the model's performance.
General Discussion:
This paper introduces a supervised neural network approach for keyphrase generation, utilizing an encoder-decoder architecture. The model first encodes the input text using a Recurrent Neural Network (RNN) and then employs an attention mechanism to generate keyphrases from the hidden states. A more advanced variant of the decoder incorporates an attention mechanism conditioned on the keyphrase generated in the previous time step, enhancing the model's capability. The novelty and potential of the model are notable, particularly in its ability to generate keyphrases not found in the source text. However, the evaluation methodology lacks transparency, specifically regarding the training and testing datasets used for the evaluated models. For instance, the use of cross-validation for supervised baselines on the NUS dataset, as mentioned in Section 4.2, line 464, raises questions about the consistency of the evaluation protocol.
Other Comments:
While the paper is generally well-written and easy to follow, there are areas that require clarification:
- The distinction between absent keyphrases and out-of-vocabulary (OOV) words needs to be clearly defined, with consistent usage of OOV throughout the paper. Given that the RNN models use a vocabulary of the 50,000 most frequent words (as stated in Section 3.4, line 372, and Section 5.1, line 568), it would be beneficial to specify how many keyphrases fall outside this vocabulary. The term "unknown words" in line 380 could be misleading and should be clarified to indicate that the RNN models can generate words not present in the source text if they appear elsewhere in the corpus and are within the 50,000-word vocabulary.
- The exposition of the copy mechanism in Section 3.4 could be improved. The mechanism's intuition, as explained, seems misleading. If understood correctly, the copy mechanism is conditioned on source text locations matching the keyphrase in the previous time step (y_{t-1}), potentially leading to a higher tendency to generate n-grams seen in the source text, as illustrated in Figure 1. While the argument that the more sophisticated attention model makes CopyRNN better than the basic RNN overall is plausible, it is less clear why the former model is particularly better for absent keyphrases, as both models seem to perform equally well on present keyphrases.
- The initialization method for word embeddings is not specified and should be included for completeness.