Review
Strengths:
The authors present a novel selective encoding model that extends the sequence-to-sequence framework for abstractive sentence summarization. The paper is well-written, and the methods are clearly described. The proposed approach is evaluated on standard benchmarks, and comparisons to other state-of-the-art tools are provided, including significance scores.
Weaknesses:
Some implementation details and comparisons to other systems require further clarification.
General Discussion:
* Major Review:
  - The summaries generated using the proposed method appear to be extractive rather than abstractive, as suggested by the example in Figure 4. A better example should be chosen, and statistics on the number of words in the output sentences not present in the input sentences should be provided for all test sets.
  - The mathematical difference between vectors hi and s is unclear, and it is uncertain whether both are necessary. The authors should consider using only one of them.
  - The neural network library used for implementation is not specified, and there is a lack of implementation details.
  - The training data used for each compared system is not specified, and it is unclear whether the authors trained any of the systems themselves.
* Minor Review:
  - The difference between abstractive and extractive summarization could be moved to the introduction section for clarity.
  - A reference is needed for the passage describing the success of sequence-to-sequence models in tasks like neural machine translation.
  - The contribution of the work could be emphasized, and the related work section could be moved before the methods section.
  - Figure 1 and Table 1 are redundant, and one of them could be removed.
  - References are needed for passages describing the sequence-to-sequence machine translation model and previous works applying this framework to summarization generation tasks.
  - The term "MLP" is not defined in the paper.
  - The sigmoid function and element-wise multiplication are not defined in the formulas in section 3.1.
  - Many elements of the formulas are not defined, including b, W, U, and V.
  - The readout state rt is not depicted in Figure 2.
  - The meaning of "(ref)" in Table 2 is unclear.
  - The values of model parameters, such as word embedding size and GRU hidden states, should be explained.
  - A reference is needed for beam search.
  - There are typos, including "supper script" instead of "superscript" and a potential typo in the true sentence in Figure 4.