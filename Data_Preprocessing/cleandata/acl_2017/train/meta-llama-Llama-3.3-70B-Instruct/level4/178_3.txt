The authors propose a methodology for jointly embedding words, phrases, and concepts into a shared space, leveraging both plain text corpora and manually constructed ontologies where concepts are represented by one or more phrases. This approach is applied in both the medical domain, utilizing the UMLS ontology, and the general domain, with the YAGO ontology. The evaluation of their method involves comparison with simpler baselines and prior work, primarily focusing on intrinsic similarity and relatedness benchmarks. Existing benchmarks in the medical domain are employed, and a new general-domain concept similarity and relatedness dataset is created through mechanical turkers, with plans for its release. The reported results are comparable to those of prior work.
Strengths of the paper include:
- The joint embedding model proposed is straightforward and reasonable, offering a configurable balance between treating phrases as atomic units and considering their compositional nature. This approach is also applied to concepts composed of several representative phrases.
- The paper encompasses a significant volume of work, including model development, the creation of a new evaluation dataset, and several evaluations and analyses.
However, several weaknesses are identified:
- The evaluation is limited to intrinsic tasks, mainly similarity and relatedness datasets, which are known to have limited predictive power for the utility of embeddings in extrinsic tasks. Recent practices suggest including at least one or two extrinsic tasks in the evaluation of embedding models.
- The similarity and relatedness evaluation datasets are presented as recording human judgments of concept similarity but are based on phrase presentations to human annotators. Thus, they should be considered as phrase similarity datasets and analyzed accordingly.
- The medical concept evaluation datasets, such as 'mini MayoSRS' and its larger version, are either extremely small or have limitations such as low human annotator agreement or being based on single-word concepts. These aspects question the datasets' relevance to phrase and general concept representations.
- The extensive fine-tuning of hyperparameters on the same datasets used for reporting results and comparing with prior work makes the reported analyses and results questionable.
- The argument that the method surpasses prior work due to achieving comparable results with less manual annotation is not strong, given the use of large manually constructed ontologies and the origin of manually annotated datasets in prior work from existing clinical records.
- The paper lacks insightful analysis into the nature of the relations between phrases and their component words, and concepts and their alternative phrases. Each deserves dedicated analysis, potentially including NLP-specific insights and investigations into the effects of hyperparameters controlling the tradeoff between atomic and compositional views.
Given these weaknesses, the recommendation is to reject this submission, with encouragement to the authors to improve their evaluation datasets and methodology before resubmission.
Minor comments include:
- Line 069: The term should be 'concepts' instead of 'contexts'.
- Line 202: Clarification is needed on how phrase overlaps are handled.
- Line 220: The dimensions should be |W| x d, and the term 'negative sampling matrix' is confusing as it represents contexts in positive instances as well.
- Line 250: The training process for words in the joint model is unclear, particularly regarding the consideration of words within phrases.
- The notation in Equation 1 is confusing, using 'c' instead of 'o'.
- Line 361: The reference to Pedersen et al. 2007 is missing.
- Line 388: The use of a fine-grained similarity scale (1-100) for human annotations is noted as odd.
- Line 430: The introduction of the term 'strings' is confusing and should be replaced with 'phrases' for consistency.
- Line 496: The specific task used for hyper-parameter tuning is not clear and should be specified.
- Table 3: Trends are hard to discern, and including development set trends regarding hyper-parameters could be insightful.
- Line 535: A reference to Table 5 is missing.