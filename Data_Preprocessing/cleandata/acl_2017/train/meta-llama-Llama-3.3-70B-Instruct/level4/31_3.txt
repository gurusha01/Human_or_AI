This paper presents a supervised deep learning approach for identifying event factuality, achieving state-of-the-art performance on the FactBank corpus, particularly in the CT-, PR+, and PS+ classes. The primary contribution lies in the proposal of an attention-based two-step deep neural model, leveraging bidirectional long short-term memory (BiLSTM) and convolutional neural network (CNN) for event factuality identification.
The paper's strengths include:
- A well-organized structure, although not perfect.
- Empirical results demonstrating statistically significant performance gains of the proposed model over strong baselines.
However, several weaknesses are noted:
- The novelties of the paper are not clearly defined.
- The absence of detailed error analysis.
- A superficial comparison of features with prior work, omitting relevant studies such as Kenton Lee et al. (2015) and Sandeep Soni et al. (2014).
- The presence of obscure descriptions and typos throughout the paper.
To enhance the paper's impact, it is suggested that the authors explicitly state the novelties of their approach, such as whether it represents the first neural network-based method for event factuality identification. A detailed error analysis regarding the results in Tables 3 and 4 would also be beneficial, including an examination of the dominant error sources and the impact of basic factor extraction errors on overall performance.
Furthermore, a more comprehensive comparison with prior work in terms of features would strengthen the paper. It is unclear whether the proposed system's advantages stem solely from deep learning or from a combination of neural networks and novel features. The inclusion of examples to illustrate underspecified modality (U) and underspecified polarity (u) would also improve the paper's clarity, as the current definition and examples are limited.
Minor suggestions for improvement include:
- Reorganizing Section 3.2 to dedicate separate paragraphs to lexical and sentence-level features.
- Renaming Section 3 to 'Basic Factor Extraction' for clarity.
- Providing a more convincing description of the attention mechanism's benefits.
- Enhancing Tables 2 and 4 with additional factuality statistics and boldfacing the highest system performance.
- Resolving inconsistencies in the description of auxiliary words.
- Correcting typos and minor errors throughout the paper, such as those found in lines 162, 315, 719, 771, and 903.