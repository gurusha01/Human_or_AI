The paper presents a deep learning-based approach to parsing Singaporean English, also known as Singlish, into Universal Dependencies. The authors implement a parser based on the model proposed by Dozat and Manning (2016) and incorporate neural stacking, as introduced by Chen et al. (2016). They train an English model and leverage some of its hidden representations as input to their Singlish parser, enabling the utilization of a larger English training set in conjunction with a smaller annotated Singlish treebank. This approach yields improved results (LAS 76.57) compared to using an English parser alone (LAS 65.6) or training a parser solely on the limited Singlish data (LAS 64.01). The authors also conduct an analysis to determine which common constructions benefit from their approach.
Additionally, the paper describes and evaluates a stacked POS model based on Chen et al. (2016), discusses the analysis of common constructions within the Universal Dependencies framework, and provides an annotated treebank of 1,200 sentences. A subset of 100 sentences was annotated by two individuals, resulting in an inter-annotator agreement of 85.3 UAS and 75.7 LAS.
The strengths of the paper include:
* The authors achieve good results and their experimental setup appears to be well-designed.
* They perform thorough analyses, exploring the impact of various parameters on their model.
* The provision of a small Singlish treebank annotated according to Universal Dependencies v1.4 guidelines is a valuable contribution.
* The proposed guidelines for analyzing common Singlish constructions in UD are sound and well-motivated.
* The method is linguistically informed, effectively exploiting the similarities between standard English and Singlish.
* The paper addresses a low-resource language and presents a method that can be potentially applied to other closely related language pairs.
* The sentence selection method for the treebank is well-motivated.
* The paper is well-written and easy to follow.
However, the weaknesses include:
* The initial annotation quality appears to be relatively poor, with an inter-annotator agreement of 75.72% in terms of LAS. This raises concerns about the reliability of the estimated LAS of the model. Nevertheless, the authors' rebuttal convincingly addresses this issue, revealing that the second annotator's deviation from the annotation guidelines was the primary cause of the low agreement. After rectifying this issue, the inter-annotator agreement becomes reasonable.
In general, despite initial concerns regarding annotation quality, the paper is well-received, and its contributions are deemed valuable to the conference.
Questions for the authors include:
* Who annotated the sentences, and what was the annotation process?
* What were the primary causes of disagreement in the inter-annotator agreement, and how were these issues resolved?
* The high frequency of discourse relations in the treebank is notable; is this a characteristic of colloquial language or a result of the annotation guidelines?
* The classification of discourse particles and imported vocabulary in Table A3 could be clarified, potentially with separate tables and glosses.
Low-level comments suggest:
* A comparison with the approach presented by Martinez et al. (2017) would be interesting and potentially informative.
* The term "grammar" could be replaced with "syntactic constructions" to improve clarity.
* The analysis in Figure 2 is sound, but the mention of "it-extraposition" could be removed.
* The model by Dozat and Manning (2016) is no longer state-of-the-art; a more general description, such as "high-performing model," could be used instead.
* Providing glosses in Figure 2 would enhance the readability and understanding of the analysis.