This paper presents a neural network architecture designed to incorporate structural linguistic knowledge into a memory network for sequence tagging tasks, specifically slot-filling in conversation systems. The approach involves encoding substructures, such as nodes in a parse tree, as vectors (memory slots) and utilizing a weighted sum of these substructure embeddings as additional context in a Recurrent Neural Network (RNN) at each time step for labeling purposes.
-----Strengths-----
The primary contribution of this paper lies in its straightforward method of "flattening" structured information into an array of vectors (the memory), which is then integrated into the tagger as supplementary knowledge. This concept bears resemblance to structured or syntax-based attention mechanisms, such as attention over nodes from treeLSTM, and is related to the work of Zhao et al. on textual entailment, Liu et al. on natural language inference, and Eriguchi et al. on machine translation. The proposed substructure encoder shares similarities with the Deep Convolutional Neural Network (DCNN) approach by Ma et al., where each node is embedded from a sequence of ancestor words. Although the architecture may not be entirely novel, its simplicity and practicality are notable compared to prior works.
-----Weaknesses-----
The empirical results presented are not entirely convincing, primarily due to the lack of detailed information regarding the baselines. The concerns are outlined below in order of decreasing importance:
- The proposed model consists of two main components: sentence embedding and substructure embedding. In Table 1, the baseline models, TreeRNN and DCNN, are originally designed for sentence embedding but can also be used to obtain node/substructure embeddings. However, it is unclear how these models are utilized to compute the two parts.
- The model employs two RNNs: a chain-based one and a knowledge-guided one. The distinction between the two lies in the addition of a "knowledge" vector from the memory to the RNN input (Equations 5 and 8). It appears unnecessary to have separate weights for the two RNNs, as the primary advantage of using two RNNs is an increase in model capacity, i.e., more parameters. Furthermore, the hyperparameters and size of the baseline neural networks should be provided to ensure they have comparable numbers of parameters.
- It would be reasonable to include a baseline that inputs additional knowledge as features to the RNN, such as the head of each word or Named Entity Recognition (NER) results.
- Any discussion or results regarding the model's sensitivity to parser errors would be beneficial.
Comments on the model:
- After computing the substructure embeddings, it seems natural to compute attention over them at each word. The use of static attention for all words warrants explanation, as the "knowledge" appears to function more like a filter to mark important words, making the inclusion of the aforementioned baseline reasonable.
- Given that the weight on a word is computed by the inner product of the sentence embedding and the substructure embedding, and both embeddings are computed by the same RNN/CNN, it implies that nodes or phrases similar to the whole sentence receive higher weights, potentially affecting all leaf nodes.
- The paper claims the model generalizes to different knowledge, but representing substructures as a sequence of words may not be straightforward for all types of knowledge, such as constituent parse.
Lastly, the term "knowledge" might be misleading, as it typically refers to external or world knowledge, such as a knowledge base of entities, whereas in this context, it pertains to syntax or arguably semantics if Abstract Meaning Representation (AMR) parsing is utilized.
-----General Discussion-----
This paper proposes a practical model that appears to perform well on one dataset, but the core ideas are not particularly novel (as discussed in the strengths section). For an ACL paper, more significant takeaways are expected. More importantly, the experiments, as presented, are not convincing and require clarification to better assess the results.
-----Post-rebuttal-----
The authors failed to address the primary concern regarding whether the baselines (e.g., TreeRNN) are used to compute substructure embeddings independently of the sentence embedding and the joint tagger. Another significant concern is the use of two separate RNNs, which gives the proposed model more parameters than the baselines. Therefore, the scores remain unchanged.