This paper introduces a novel neural approach to summarization, building upon the standard encoder-decoder framework with attention by incorporating a gating network that filters encoded hidden states based on summary vectors from initial encoding stages. The results indicate that this method achieves a 1-2 point improvement over conventional seq2seq methods across three evaluation datasets.
The technical aspects of the paper are generally well-explained, although Equation 16 requires additional clarification due to unclear notation. The selective mechanism, which is the primary contribution of this work, appears to be innovative and has potential applications in other areas.
The evaluation is comprehensive and consistently demonstrates improvement. A natural baseline to consider would be the addition of an extra encoder layer instead of the selective layer, which is similar to the approach taken in Luong-NMT, given that the GRU baseline utilizes only one bi-GRU and this adds expressivity. However, one concern is the mismatch between LSTM and GRU - is the observed benefit solely due to the switch to GRU?
The writing quality, particularly in the introduction, abstract, and related work sections, is subpar. Since this paper does not significantly deviate from prior work, it would be more suitable to place the related work section near the introduction. This would allow for a more focused introduction that highlights the high-level aspects, rather than delving into extensive background details. In the related work section, it would be more effective to discuss the similarities and differences between the current work and previous studies in a narrative manner before presenting the equations, rather than simply listing existing works without establishing a connection to the present research.