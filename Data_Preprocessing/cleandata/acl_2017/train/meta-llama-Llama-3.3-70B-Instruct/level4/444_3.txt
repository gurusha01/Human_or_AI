This paper attempts to introduce a more thorough evaluation methodology for assessing automatically generated rap lyrics in terms of their similarity to a target artist. Although evaluating creative work generation is a challenging and intriguing topic for the community, this effort does not fully deliver on its promise of a comprehensive solution.
Ultimately, evaluations of this nature rely on subjective measures, specifically whether the generated sample can convince an expert that it was produced by the actual artist rather than an automated process, essentially a specialized version of the Turing Test. The endeavor to automate parts of the evaluation to facilitate optimization and understand human assessment of artistic similarity is valuable. However, the specific findings presented in this work do not inspire confidence that these aspects have been reliably identified.
A key consideration is the central question of whether a sample was generated by a target artist. Human annotators tasked with this question were unable to respond consistently, suggesting either a lack of sufficient expertise among the annotators, the task being too challenging, or a combination of both.
The proposed automatic measures also failed to demonstrate reliable agreement with human raters performing the same task, significantly limiting their effectiveness as a proxy for human assessment. While low interannotator agreement may be expected due to the subjective nature of the task, decomposing the evaluation into fluency and coherence components aims to make it more manageable and improve the consistency of rater scores. A low interannotator agreement for an evaluation metric is a concern and restricts its viability as a general-purpose tool.
Several specific questions and comments arise:
* Why is a line-by-line level evaluation preferred over a verse-level analysis, particularly for coherence, where a line-by-line analysis restricts the scope of coherence to consecutive lines?
* The term "style matching" assumes each of the 13 artists has a distinct and consistent style, which may not be accurate for artists like Kanye West, Eminem, Jay Z, Drake, Tupac, and Notorious Big, who have produced work in multiple styles. A more precise term might be "artist matching".
 In Section 4.2, the central automated component of the evaluation relies on low tfidf with existing verses and similar rhyme density. Given the limitations of rhyme density, how effective is this approach, even with manual intervention?
* In Section 6.2, it would be beneficial to include details on the number of judges used in the study, how many cases involved judges who were already familiar with the verse they were judging, and how this might impact the test's ability to assess style matching rather than the judges' recall and rap knowledge.