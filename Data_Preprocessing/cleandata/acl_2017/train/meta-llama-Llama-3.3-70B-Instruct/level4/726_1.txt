The paper proposes a neural model that directly predicts SQL queries from natural language utterances, eliminating the need for intermediate formalism. Additionally, it introduces an interactive online feedback loop, which is tested on a small scale.
- Strengths:
1. The paper is well-written, properly positioned, and enjoyable to read.
2. The proposed model demonstrates strong performance across three different domains, including academic, geographic queries, and flight booking.
3. The online feedback loop shows promise, despite the limited scale of the experiment.
4. The publication of a new semantic corpus and the conversion of two existing corpora to SQL format are valuable contributions that will benefit future research in this area.
- Weaknesses / clarifications:
1. In Section 4.2, the choice of span length for querying the search engine is unclear. The progressive reduction of the span length requires further explanation (line 333).
2. Section 5's benchmark experiments do not utilize the feedback loop (Algorithm 1), which raises questions about data augmentation. It is unclear when annotated training data is augmented with paraphrases and when the "initial data" from templates is added. The lack of clarity regarding the "vanilla" model's performance without augmentation is also notable.
3. The evaluation metric used in Tables 2 and 3 is unclear. It is uncertain whether the accuracy measures the correctness of query execution or the queries themselves. The comparability of numbers across different systems is also questionable.
4. The difference in accuracy between the SQL model and the best non-SQL results is significant, contrary to the text's suggestion of "slightly lower accuracy" (Line 515). The basis for this observation and whether a significance test was performed require clarification.
5. The data recombination technique used in Jia and Liang (2016) is applicable to this scenario, but its potential impact on performance is unknown. It is unclear whether this is left as future work or if there are limitations to its use.
6. Section 6.2's three-stage online experiment lacks several details, including the technical background of recruited users, crowd worker recruitment and training, and the size of the initial training set. Statistics on query lexical variability, length, and complexity are also necessary.
7. The SCHOLAR dataset seems small compared to modern standards, which raises questions about the scalability of the process. Running another baseline on this dataset to compare performance would be beneficial.
8. The interactive learning experiments in Section 6 are challenging to replicate due to the involvement of manual queries from specific annotators. Objective comparison methods, such as query statistics or a held-out test set, would improve the experiment's validity.
- Minor comments:
1. Line 48 should be corrected to "require" instead of "requires".
2. Footnote 1 is too long and could be shortened by moving some content to the main text.
3. Algorithm 1 would benefit from an accompanying caption to clarify the meaning of "new utterances".
4. Line 218 contains a typo, "Is is" instead of "It is".
5. Line 278's reference to an "anonymized" utterance is unclear and could be improved with a forward reference to Section 4.2.
- General Discussion:
Overall, the paper is well-liked, and addressing the raised questions and concerns would make it a strong candidate for the conference. The authors' detailed response is appreciated, and incorporating these details into the final paper would be beneficial.