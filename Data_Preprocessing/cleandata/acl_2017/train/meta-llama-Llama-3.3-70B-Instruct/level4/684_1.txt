This manuscript introduces a novel gated attention mechanism designed for machine reading, building upon the Attention Sum Reader (Kadlec et al., 2016) by incorporating a fine-grained gated filter to facilitate multi-hop reasoning. The concept is both interesting and intuitive in the context of machine reading. While the idea is compelling, accompanied by significant improvements on benchmark datasets, several major concerns need to be addressed for consideration for publication in ACL.
The proposed Gated Attention (GA) mechanism shows promise but falls short of convincingly demonstrating its superiority over other state-of-the-art systems. This is partly because the engineering tricks outlined in section 3.1.4 substantially boost accuracy, making it difficult to discern the standalone impact of the GA technique amidst the blended results.
Furthermore, the bibliography is incomplete, with nearly all referenced works citing arXiv preprint versions. This incompleteness raises suspicions about the thoroughness of the comparison with prior work, especially for readers and reviewers. It is essential to update the references with the complete, published versions if available.
The inclusion of results from unpublished work, specifically the GA baseline in tables 1 and 3, which is mentioned as an unpublished preprint, is unnecessary. Instead, the authors could replace this with a vanilla GA or a variant of the proposed model as a baseline for more meaningful comparison. Presenting results from a preprint that could essentially be the same as the current ACL submission within the same manuscript is not only confusing but also potentially problematic for fair blind review.
There appears to be a conflict between the information presented in tables 1 and 2. Specifically, GA-- in table 1 seems to correspond to K=1 (AS) in table 2, while GA (fix L(w)) aligns with K=3 in table 2. This raises questions about whether GA-- is actually a re-implementation of the AS Reader, and if so, why the discrepancies in the use of GloVe initialization and token-attention between GA-- and K=1 (AS) are not clarified.
A more detailed comparison of the proposed method with prior related work in the related work section would be beneficial. This would help in understanding what differentiates the current work from existing research.
Figure 2 effectively demonstrates the benefits of gated attention in translating multi-hop architecture, and it is impressive. To further strengthen the manuscript, it would be valuable to include qualitative examples that provide a comparison, offering deeper insights into the mechanism's effectiveness.