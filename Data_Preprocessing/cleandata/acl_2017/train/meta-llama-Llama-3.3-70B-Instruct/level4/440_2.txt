This manuscript presents a cutting-edge CCG parsing model, which effectively decomposes into tagging and dependency scores and incorporates an efficient A* decoding algorithm. Notably, the model achieves a slight improvement over the more expressive global parsing model proposed by Lee et al. (2016), likely due to the factorization facilitating the learning process. The authors also provide results for an additional language, demonstrating significant advancements in Japanese CCG parsing compared to existing studies. A noteworthy original finding is that designating the first word of a constituent as the head yields substantially better performance than adhering to linguistically motivated head rules.
In general, this is a well-crafted paper that offers a valuable contribution to the field. However, I have a few recommendations for enhancement:
- The interaction between the dependency and supertagging models is a notable strength, but including baseline results for simpler model variations (e.g., omitting the conditioning of the tag on the head dependency) would provide additional insight.
- Although the paper establishes a new state-of-the-art performance on Japanese data with a considerable margin, it is essential to acknowledge that this dataset has received relatively less attention. To facilitate a more comprehensive comparison, it would be beneficial to train the Lee et al. parser on this dataset as well.
- The work by Lewis, He, and Zettlemoyer (2015) on combined dependency and supertagging models for CCG and SRL may be a relevant reference worth incorporating into the discussion.