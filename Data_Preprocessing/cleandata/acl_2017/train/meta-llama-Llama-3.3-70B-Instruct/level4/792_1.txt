Review:
- Strengths:
1. The initial sections of the paper are well-presented, with a clear and logical structure, making for an engaging read. The argumentation is generally sound, contributing to the paper's overall coherence.
2. The research tackles a significant problem by integrating word order information into word and sense embeddings, proposing an intriguing solution that warrants attention.
- Weaknesses:
1. Regrettably, the results lack consistency, failing to convincingly demonstrate the superiority of the proposed models over existing alternatives, particularly considering the added complexity. Although negative results can be valuable, the analysis provided is insufficient to glean meaningful insights. Furthermore, the omission of results from the word analogy task, beyond the acknowledgment of the models' lack of competitiveness, is notable and warrants further examination.
2. Certain aspects of the experimental design are unclear or lack motivation, notably regarding the selection and utilization of corpora and datasets.
3. The paper's quality diminishes in its latter sections, leaving the reader somewhat disappointed, not only due to the results but also the presentation and argumentation, which do not maintain the initial standard.
- General Discussion:
1. The authors' objective of learning shared representations for words and senses in an emerging space is only partially achieved, specifically through the LSTMEmbed_SW version, which consistently underperforms compared to alternative models. The motivation behind learning these representations in a shared semantic space is not clearly articulated and deserves further discussion.
2. The rationale or intuition underlying the prediction of pre-trained embeddings is not explicitly stated. Additionally, it is unclear whether the pre-trained embeddings in the LSTMEmbed_SW model represent words, senses, or a combination thereof, and which specific setup is employed in the experiments.
3. While the importance of learning sense embeddings is acknowledged, their evaluation is not clearly presented. Most word similarity datasets treat words as independent of context, which may not adequately assess the sense embeddings.
4. The size of the training corpora is not specified, which could impact the comparison of results, especially when combining different proportions of datasets like BabelWiki and SEW. The small size of SemCor is also noteworthy, as it is typically considered insufficient for learning embeddings with models like word2vec. If the proposed models are particularly suited for small corpora, this should be highlighted and evaluated.
5. The use of non-independent test sets, such as WS353, WSSim, and WSRel, complicates comparisons and may skew the interpretation of results, potentially inflating the number of "wins" for the proposed models.
6. The claim that the proposed models are faster to train due to the use of pre-trained embeddings in the output layer lacks empirical support. Providing evidence for this claim would strengthen the paper.
7. The comparison in Table 4 would be more robust if the same dimensionality were used across models.
8. A detailed description of how the multiple-choice task for synonym identification is approached is missing from the similarity measurement section.
9. A reference to Table 2 is omitted, which could hinder the understanding of related discussions.
10. The training process for the word analogy task is not described, despite the task being mentioned in the context of the corresponding dataset.