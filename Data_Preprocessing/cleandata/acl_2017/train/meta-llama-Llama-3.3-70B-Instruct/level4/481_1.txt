In this study, the authors expand on the MS-COCO dataset by introducing an erroneous caption for each existing one, differing by only a single word. They demonstrate that two state-of-the-art methods, one for visual question answering and the other for captioning, struggle significantly with identifying fake captions, pinpointing the incorrect word, and selecting a suitable replacement. 
This research contributes to the existing body of work highlighting the underperformance of vision-language models. It raises fundamental questions about the capabilities of these models, exploring whether they are truly performing complex tasks or not. The authors examine a range of tasks and models, providing insightful analyses into the factors contributing to model failure, as seen in Figure 3.
One of my primary concerns is the similarity between this work and that of Ding et al. However, the authors make a compelling argument by showing that even with a simpler approach, where captions differ by only one word, the models still fail catastrophically. This distinction underscores that the engineering efforts of Ding et al. are not necessary to expose the weaknesses of these models.
Another concern is the use of NeuralTalk for selecting the most challenging foils, which, although innovative, may introduce a self-reinforcement bias. This could potentially embed NeuralTalk's biases into FOIL-COCO.
The results section feels somewhat concise compared to the rest of the paper. Expanding on this section, perhaps with additional paragraphs, would provide more depth and insight into the findings.
Overall, I find this paper to be a valuable contribution to the field, as it builds upon previous results highlighting deficiencies in vision-language integration. The similarity to Ding et al.'s work does not detract from its significance, instead reinforcing how easily vision-language models can be fooled.
Some minor suggestions include the addition of a baseline model that concatenates bag-of-words with extracted CNN features and trains a softmax classifier. This "dumb" vision-language baseline could further strengthen the paper's argument.
There are also a few technical clarifications needed, such as the definition of a supercategory and its origin, whether from WordNet or COCO. Additionally, minor corrections in wording and typos, such as "has been" to "were," "that" to "than," "artefact" to "undesirable artifacts," and "ariplane" to "airplane," would improve the manuscript's quality.
Including a chance model in Table 1 could provide a clearer baseline for comparison, addressing potential confusion about the constant-prediction baseline.
After reviewing the author's response, I am pleased to see that my concerns regarding NeuralTalk biases and the need for additional baselines have been addressed. I am confident that these issues can be resolved in the final version, and therefore, I will maintain my initial score.