This paper provides an in-depth examination of the mathematical properties of the skip-gram model, shedding light on its effectiveness in analogy tasks and the superiority of additive composition models. Furthermore, it establishes a connection between skip-gram and Sufficient Dimensionality Reduction.
I found the paper's focus on elucidating the properties of skip-gram to be commendable, and the read was generally inspiring. The authors' effort to comprehend the model's assumptions and their impact on composition operations is laudable. In this regard, I believe the paper is a valuable resource for the community.
However, my primary criticism is that the paper exhibits linguistic naivety. The authors' interpretation of 'compositionality' as an operation that takes a set of words and returns another with the same meaning is unconventional. In reality, two words can be composed to produce a vector that is distant from both, does not correspond to any other concept in the space, and yet still possesses meaning. Linguistic compositionality refers to the process of combining linguistic constituents to form higher-level constructs, without assuming additional constraints beyond semantic transparency. The paper's implication that composition occurs over sets is also incorrect, as ordering significantly affects the outcome (e.g., 'sugar cane' is not equivalent to 'cane sugar'). This is a well-known limitation of additive composition.
Another crucial aspect is that pragmatic factors influence human preferences for certain phrases over single words in specific contexts, altering the underlying distribution of words in a large corpus. For instance, using 'male royalty' instead of 'king' or 'prince' can imply a focus on gender difference. This means that the equation in line 258 (or the KL-divergence modification) does not hold due to fundamental linguistic processes, rather than noise in the data. Although the section on SDR may address this point, I am not entirely certain (see my comments below).
In summary, I think the authors' presentation of composition is flawed, but the paper convincingly demonstrates that this is indeed what occurs in skip-gram, making it an interesting contribution.
The discussion on Sufficient Dimensionality Reduction appears somewhat disconnected from the preceding argument. I had difficulty following the reasoning and would appreciate clarification from the authors. If I understand correctly, the argument is that skip-gram produces a model where a word's neighbors follow an exponential parameterization of a categorical distribution. However, it is unclear whether this reflects the actual distribution of the corpus or is an artifact of the model (e.g., a pure count-based model). The fact that skip-gram performs well despite not reflecting the data suggests that it implements a form of SDR, which does not require assumptions about the underlying data structure. Nevertheless, is it fair to say that the resulting representations are optimized for tasks where geometrical regularities are important, regardless of the actual data pattern? In other words, is there a denoising process at play?
Minor comments:
- The abstract is unusually lengthy and could be condensed.
- The paragraph starting at line 71: I think it would be misleading to perceive circularity here. Firth observed that co-occurrence effects were correlated with similarity judgments, which are the cognitive processes we aim to model using statistical methods. Co-occurrence effects and vector space word representations are, in a sense, 'the same thing,' modeling an underlying linguistic process that we cannot directly observe. Pairwise similarity is not intended to break circularity; rather, it better models the kind of judgments humans are known to make.
- Line 296: I think 'paraphrase' would be a more suitable term than 'synonym' in this context, given that we are comparing a set of words with a unique lexical item.
- The paragraph starting at line 322: This is an interesting point, and a significant portion of the zipfian distribution (the long tail) is fairly uniform.
- Line 336: It is worth noting that the analogy relation does not hold well in practice and often requires 'ignoring' the first returned neighbor of the analogy computation (which is usually one of the observed terms).
- The paragraph starting at line 343: I find it counterintuitive to say that 'man' would be a synonym or paraphrase of anything involving 'woman.' The subtraction involved in the analogy computation is not a straightforward composition operation, as it involves an implicit negation.
- A final, minor comment: It is conventional to write p(w|c) to denote the probability of a word given a context, but in the paper, 'w' represents the context and 'c' represents the target word. This notation makes reading slightly more challenging; perhaps consider changing it.
Literature:
The claim that Arora (2016) is the only work to attempt to understand vector composition is somewhat overstated. For example, see the work by Paperno and Baroni on explaining the success of addition as a composition method over PMI-weighted vectors:
D. Paperno and M. Baroni. 2016. When the whole is less than the sum of its parts: How composition affects PMI values in distributional semantic vectors. Computational Linguistics 42(2): 345-350.
*
I appreciate the authors' response and look forward to seeing this paper accepted.