The authors employ self-training to develop a seq2seq-based AMR parser, leveraging a small annotated corpus and a large amount of unlabeled data. Subsequently, they train a similar seq2seq-based AMR-to-text generator using the annotated corpus and automatically generated AMRs from the unlabeled data, produced by their parser. To mitigate data sparsity, they utilize careful delexicalization for named entities in both tasks. This approach marks the first successful application of seq2seq models to AMR parsing and generation, likely surpassing the state-of-the-art in generation.
Overall, the approach and experiments are commendable, and the final performance analysis is thorough. Although the methods used are not groundbreaking, they are cleverly combined to yield practical results. The description of the approach is detailed, allowing for reproducibility of the experiments without significant issues. While the approach still requires some manual tuning, it is believed that this can be overcome in the future, and the authors are heading in a promising direction.
However, a concern was raised regarding a potential data overlap between the Gigaword and Semeval 2016 datasets, which could invalidate the generation results if test set sentences were used in training. Additionally, a question was posed about the 5.4-point claim in comparison to a system tested on an earlier version of the AMR dataset, suggesting that the authors should have also tested their system on the older dataset or obtained the scores for the newer version.
Minor suggestions for improvement include conducting statistical significance tests and repeating the linearization order experiment with different random seeds to minimize bias. The paper's form could be enhanced through proofreading, adding figures to explain the model, and including a formal conclusion. Furthermore, minor factual and writing notes were provided, such as clarifying the use of the JAMR aligner, rewording certain text, and correcting typos and punctuation.
In summary, the paper presents competitive results for neural AMR parsing and potentially new state-of-the-art results for AMR generation using seq2seq models with clever preprocessing and a large unlabeled corpus. While revisions to the text are advisable, the paper is well-liked, and it is desirable to see it presented at the conference, pending resolution of the aforementioned concerns. Following the authors' response, the major issues were addressed, and the scores were revised accordingly, assuming that the discussion will be reflected in the final paper.