Summary: 
This manuscript presents a novel dataset for sarcasm interpretation and proposes a system, Sarcasm SIGN, which leverages a machine translation framework, Moses. The dataset comprises 3000 sarcastic tweets, each accompanied by five human interpretations. Sarcasm SIGN modifies Moses by replacing sentimental words with their corresponding clusters on the source side and then de-clustering their translations on the target side. The results indicate that Sarcasm SIGN performs comparably to Moses in terms of machine translation evaluation metrics but surpasses it in terms of fluency and adequacy.
Strengths:
The manuscript is well-written, and the dataset has been collected in a systematic manner. The experiments are thoroughly conducted, and the analysis is sound.
Weaknesses:
The manuscript lacks essential statistics about the dataset, such as average length and vocabulary size. The baseline system, Moses, may not be suitable due to the limited size of the dataset. Furthermore, the assumption that sarcastic tweets often differ from their non-sarcastic interpretations in as little as one sentiment word is not substantiated by the data.
General Discussion: 
A significant portion of the manuscript focuses on the new dataset for sarcasm interpretation. However, crucial information about the dataset, such as average length and vocabulary size, is not provided. More importantly, the manuscript fails to present statistical evidence to support the method of focusing on sentimental words. 
Given the small size of the dataset (only 3000 tweets), it is likely that many words are rare. Consequently, Moses alone may not be a suitable baseline, as it may not effectively handle rare words. A more appropriate baseline would be a machine translation system that can adeptly handle rare words. In fact, the use of clustering and de-clustering in Sarcasm SIGN can be seen as a means to address this issue.
Sarcasm SIGN is based on the assumption that sarcastic tweets often differ from their non-sarcastic interpretations in as little as one sentiment word. However, Table 1 contradicts this assumption, as human interpretations often differ from the tweets in more than just sentimental words. Therefore, I strongly suggest that the authors provide statistical evidence from the dataset to support their assumption, as the validity of Sarcasm SIGN relies on this premise.
--------------------------------------------------------------
After reviewing the authors' response, I maintain my initial decision due to the following reasons:
- The authors' statement that "the Fiverr workers might not take this strategy" raises concerns, as it seems to imply that the data should conform to the model's assumptions rather than the model being developed to fit the given data, which is a fundamental principle of corpus-based NLP.
- The authors' argument that "the BLEU scores of Moses and SIGN are above 60, which is generally considered decent in the MT literature" is not convincing, as the sentences in the dataset are extremely short. Furthermore, Table 6 shows that the %changed of Moses is only 42%, indicating that more than half of the time, the translation is simply a copy, which inflates the BLEU score.
- The authors' comment that "while higher scores might be achieved with MT systems that explicitly address rare words, these systems don't focus on sentiment words" is valid, but it prompts the question of whether sentiment words are rare in the corpus. If they are, then MT systems that can handle rare words should also be able to address sentiment words effectively.