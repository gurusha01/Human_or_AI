Strengths:
The authors tackle the challenging task of answering open-domain questions from Wikipedia by developing a two-stage approach: 1) a document retriever that extracts relevant Wikipedia articles for a given question, and 2) a document reader that identifies the exact answer from the retrieved paragraphs. The use of distant supervision for fine-tuning the model yields promising results, with the document reader outperforming the WikiSearch API and surpassing some recent models in question answering.
Weaknesses:
Although the proposed system demonstrates competitive performance, its final results are surpassed by other models, as acknowledged by the authors. Furthermore, the absence of error analysis hinders a deeper understanding of the system's limitations.
General Discussion:
The authors' end-to-end system is intriguing, but several concerns warrant attention. The document retriever's superior performance compared to WikiSearch is notable; however, the specifics of the API's usage are unclear. Given WikiSearch's suitability for structured retrieval, it may not be an ideal baseline for querying questions. The authors could consider employing standard information retrieval baselines for a more comprehensive evaluation. 
Regarding distant supervision, its effectiveness and reliability are uncertain. While the authors had to limit their training examples due to this approach, providing statistics on the fraction of "close to correct" examples would be beneficial in assessing the potential for further fine-tuning. 
The full Wikipedia results, with a performance of 26.7 (and 49.6 and 69.5 when the correct document and paragraph are given, respectively), highlight the need for improvement in the retrieval aspect. The inferior results on WebQuestions compared to YodaQA raise questions about the sufficiency of Wikipedia as a sole knowledge source for open-domain questions. The authors may want to explore integrated models that incorporate additional knowledge sources.
Overall, the results presented in Tables 4 and 5 are outperformed by other models, which may indicate that relying solely on Wikipedia is not the optimal strategy.
Other points:
The discrepancy in F1 values between Tables 4 and 5 (78.4 vs. the values in Table 4 for both Dev and Test) requires clarification. In Table 5, the absence of "No f_emb" is noticeable. Moreover, error analysis is essential for understanding the system's weaknesses, particularly in identifying question types that are challenging for the system. Investigating ways to determine which questions are suitable for Wikipedia-based answering and using this method selectively could enhance the system's performance. The degradation of performance on WebQuestions with distant supervision also warrants further examination.