Strengths: The proposed system has a practical application for both educators and students, as it provides a fine-grained comparison of Grammatical Error Correction (GEC) systems.
Weaknesses: The system's description is overly superficial, and the evaluation methodology is not convincing.
General Discussion:
This paper introduces an approach to automatically enrich the output of GEC systems with error types, which is a valuable application as it benefits both teachers and learners by providing explicit error information. Moreover, it enables a more detailed comparison of GEC systems in terms of precision and error-type-specific recall and precision figures.
However, the system's description lacks essential details, particularly regarding the core component of manually created rules. The authors should provide examples of these rules, specify their number, complexity, and ordering, as well as address potential interactions between rules. Instead, the paper devotes excessive space to evaluating systems from CoNLL-2014, with redundant information in the text and tables.
The evaluation of the proposed system is unsatisfactory in several aspects. Firstly, the annotators should have created an independent gold standard for the 200 test sentences rather than rating the system's output. This would have allowed for a more accurate comparison. Secondly, the paper's method of averaging individual rater scores is unconventional and may mask significant discrepancies. For instance, if each "bad" score was assigned to different edits, the actual percentage of "bad" edits could be substantially higher than the reported average. Thirdly, the paper lacks information about the test data, such as the number and distribution of error categories. Lastly, the statement about "unusual edit boundaries" requires clarification and examples to understand its implications for the system's application.
The authors claim that their system is less domain-dependent than systems requiring training data. However, this assertion is questionable, as manually created rules can be domain-dependent, and the system's language dependence is a notable drawback compared to machine learning approaches. The test data used, FCE-test and CoNLL-2014, are also limited to a single domain: student essays.
The introduction of a new set of error categories seems unnecessary, as existing tagsets, such as the one presented in Nicholls (2003) or the CoNLL-2014 tagset, could have been used. This would have allowed for the utilization of the CoNLL gold standard for evaluation.
In conclusion, the paper's primary motivation remains unclear. Is it presenting a new system? If so, crucial details are omitted. Is it introducing a new set of error categories? If so, the motivation and discussion are lacking. Is it evaluating CoNLL-2014 systems? If so, the presentation of results is superficial.
Typos and formatting issues:
- l129 (and others): "c.f." should be "cf."
- l366 (and others): "M2" should be "M^2" (superscript 2)
- l319: "50-70 F1" is unclear; please specify the intended meaning (e.g., 50-70%).
- References should be checked for incorrect case, such as "esl" instead of "ESL" (l908) and "fleiss, kappa" (l878/79).