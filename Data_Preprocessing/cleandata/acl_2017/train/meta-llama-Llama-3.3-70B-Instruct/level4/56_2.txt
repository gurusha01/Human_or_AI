The paper presents several strengths, notably the innovative approach of utilizing ngrams, specifically bigrams, to train word2vec-type models, which is a commendable idea. The experimental design is comprehensive, covering four distinct word2vec-type algorithms and various word/bigram conditions, thereby exploring a broad spectrum of possibilities. Furthermore, the qualitative analysis of bigram embeddings yields intriguing insights, highlighting the potential of such models in capturing multi-word expressions.
However, there are areas that require improvement. The manuscript would benefit significantly from a thorough review by a native English speaker to rectify issues related to the usage of articles, among other linguistic aspects. Additionally, the placement of the description of similarity and analogy tasks within the section on datasets (4.1) seems out of context and could be rearranged for better coherence.
In terms of broader discussion, it would be advantageous to introduce the concept more clearly from the outset, emphasizing that the work essentially extends the original word2vec concept by redefining the fundamental unit from a word (or unigram) to also include bigrams, thus representing a generalized approach. Moreover, providing a rationale for limiting the ngram size to bigrams and not exploring larger ngrams would add depth to the discussion. This clarification is touched upon later in the paper but would be more effective if presented earlier. Following the author's response, these suggestions are reinforced as key areas for enhancement.