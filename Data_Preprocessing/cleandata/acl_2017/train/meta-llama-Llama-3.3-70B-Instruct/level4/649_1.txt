Review
Strengths:
This paper presents a novel evaluation metric for assessing the quality of dialogue responses in non-task-oriented dialogue systems. The proposed metric leverages continuous vector space representations generated by RNNs and consists of two components: one comparing the context and given response, and the other comparing a reference response and the given response. The comparisons are performed using dot product after projecting the response into corresponding context and reference response spaces, with projection matrices learned by minimizing the squared error between model predictions and human annotations.
The work represents a significant step forward in evaluating non-task-oriented dialogue systems, as it moves beyond pure semantic similarity by learning projection matrices that transform the response vector into context and reference space representations. The authors' approach is elegant, and it would be interesting to explore how the learned projection matrices M and N differ from their initial identity initialization after training. Providing further discussion on this aspect would enhance the paper's value, rather than focusing solely on resulting correlations.
Weaknesses:
The paper raises several questions related to implementation details. For instance, it is unclear whether the human scores used for training and evaluation were based on single AMT annotations or the average of multiple annotations. Additionally, the dataset split into train/dev/test and the use of n-fold cross-validation are not clearly explained. The presentation of correlation results for ADEM-related scores in Table 2 is also confusing, as it is unclear why validation and test sets are used for ADEM scores, while other scores are presented for the full dataset and test set. The section on pre-training with VHRED is unclear and could be improved with a higher-level explanation of the pre-training strategy and its advantages.
General Discussion:
The statement "There are many obvious cases where these metrics fail, as they are often incapable of considering the semantic similarity between responses" is misleading. The issue is not with semantic similarity, but rather that different semantic cues can constitute pragmatically valid responses. Therefore, semantic similarity alone is insufficient for evaluating dialogue system responses, and the proposed M and N matrices help address this limitation.
The comparison to the Turing test is also misleading, as the original intention of the Turing test was to define intelligent behavior, not to evaluate dialogue response quality. Automatically evaluating dialogue response quality is not equivalent to an automatic Turing test. The title "Towards an Automatic Turing Test" is therefore somewhat misleading.
The assumption that a "good" chatbot is one whose responses are scored highly on appropriateness by human evaluators is a correct angle to introduce the problem of non-task-oriented dialogue systems. Related work, such as the WOCHAT workshop series, could be referenced to provide further context. Finally, a minor typo was found in the discussion session: "and has has been used" should be corrected to "and it has been used".