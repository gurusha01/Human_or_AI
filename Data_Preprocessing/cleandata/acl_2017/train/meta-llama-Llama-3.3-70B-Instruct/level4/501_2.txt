The authors' approach to generating a dataset of rephrased captions, which will be made publicly available, is a notable strength of the paper. Their method of tackling the Dual Machine Comprehension (DMC) task has an advantage over Visual Question Answering (VQA) or caption generation in terms of evaluation metrics, as it is more straightforward to assess the problem of selecting the best caption using accuracy metrics, whereas caption generation requires more complex metrics like BLEU or Meteor that struggle to capture semantic similarity.
The authors' proposed approach to rephrasing, including the selection of decoys from an image-caption dataset, is intriguing. They draw decoys from captions of other images, which are similar in both surface and semantic terms, and utilize a lambda factor to balance these components of the similarity score. This approach could be interesting for paraphrasing tasks.
The authors support their motivation for the DMC task with evaluation results, demonstrating that a system trained to differentiate between similar captions performs better than one trained solely for caption generation. However, this finding is not surprising, as a system tuned for a specific task tends to perform better on that task.
One of the weaknesses of the paper is the lack of clarity on why the image caption task is not suitable for comprehension tasks and why the authors' system is better suited for this purpose. To argue that their system can comprehend image and sentence semantics more effectively, the authors should apply learned representations, such as embeddings, and compare them to those learned by different systems on the same task.
A major concern is that the authors' approach essentially converges to using existing caption generation techniques, such as those proposed by Bahdanau et al. and Chen et al. Furthermore, the presentation of formula (4) is confusing, as it appears that both decoy and true captions are employed for both loss terms, whereas the authors mention that decoys are not used for the second term. This ambiguity should be clarified in the formula or the text to avoid confusion.
In general, the authors formulate the task of Dual Machine Comprehension, aiming to challenge computer systems to choose between two very similar captions for a given image. They argue that a system capable of solving this problem must "understand" the image and captions beyond keywords, capturing semantics and alignment with image semantics. However, the paper needs to focus more on why the chosen approach is better than caption generation and why caption generation is less challenging for learning image and text representation and their alignment.
Regarding formula (4), it would be interesting to explore the possibility of adjusting the second loss term to include decoys with a negative sign, allowing the model to learn "not to generate" decoys. It is unclear whether the authors have attempted this approach, and further investigation could be beneficial.