Strengths:
The paper introduces a novel and significant metric for assessing the quality of word embeddings, specifically "data efficiency" in the context of supervised tasks. 
Another notable aspect of the paper is its division into three key questions: 1) whether supervised tasks provide additional insights into evaluating embedding quality; 2) the stability of ranking in relation to labeled dataset size; and 3) the benefits of linear versus non-linear models. 
The authors have conducted extensive experiments to address these questions, yielding results that are noteworthy and of interest to the research community.
Weaknesses:
While the paper offers a comprehensive analysis that deepens understanding of the topic, its overall findings may not be particularly useful for machine learning practitioners. This is because the results largely confirm existing knowledge or suspicions, namely that outcomes depend on factors such as the task at hand, the size of the labeled dataset, and the type of model used. Consequently, the paper's results may not be directly actionable.
General Discussion:
There are several areas where the paper's presentation could be enhanced:
1) The arrangement of figures and tables should align with their order of mention in the text, as the current sequence appears somewhat arbitrary.
2) A spell checker should be utilized to correct numerous typos (e.g., L250, L579).
3) Equation 1 seems unnecessary and is oddly presented; it could be removed, with the text explanations sufficing.
4) The reference to an "Appendix" at L164 is misleading, as no appendix is included in the paper.
5) A citation for the public skip-gram dataset mentioned at L425 is missing.
6) The claim made in L591-593 is overly assertive and requires clearer explanation, including specifics on when it is applicable and when it is not.
7) The observation in L642-645 is both interesting and important. Further investigation, potentially including concrete examples from specific embeddings and visual aids, would be beneficial.
8) At L672, examples of "specialized word embeddings" and how they differ from general-purpose embeddings should be provided.
9) Figure 3 is too small to be legible and should be enlarged for better readability.