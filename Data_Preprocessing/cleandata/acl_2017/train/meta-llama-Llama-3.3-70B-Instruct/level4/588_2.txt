- General Discussion:
This paper addresses the challenge of predicting missing entities in a given context, leveraging Freebase definitions of the entities in question. The authors emphasize the significance of this problem, particularly given the long-tailed distribution of entities. Their approach involves utilizing popular sequence encoders to encode both the context and the definitions of potential entities, then scoring these entities based on their similarity to the context. Although the importance of the task and the potential utility of the dataset as a benchmark are evident, the methodology employed has notable shortcomings, and the evaluation raises several unanswered questions.
- Strengths:
The proposed task necessitates the incorporation of external knowledge, and the accompanying dataset could serve as a valuable benchmark for assessing the performance of hybrid Natural Language Understanding (NLU) systems.
- Weaknesses:
1) With the exception of the top-performing model, HIERENC, all evaluated models lack access to contextual information beyond the sentence level. This limitation seems insufficient for accurately predicting missing entities, and it is unclear whether any efforts were made to address coreference and anaphora resolution. Including human performance benchmarks for the task would provide valuable context.
2) The selection of predictors in all models is unconventional. The rationale behind using similarity between context embeddings and entity definitions as an indicator of an entity's suitability as a filler is not clearly justified.
3) The description of the HIERENC model is ambiguous. As understood, each input to the temporal network represents the average of all possible entity instantiations in the context. This approach may introduce significant noise, as only one instantiation is correct.
4) The results presented are not particularly informative. Given the rare entity prediction nature of the problem, analyzing type-level accuracies and how model accuracies vary with entity frequencies would be beneficial.
- Questions to the authors:
1) A key assumption is that entity definitions (d_e) can effectively replace entity embeddings. Was this assumption validated through testing?
2) Have the authors explored developing a classifier that uses h_i^e as input, and if so, what were the outcomes?
Having reviewed the authors' responses, the potential of the task and dataset to benefit from human evaluation remains. Understanding the difficulty of the task is crucial for it to serve as a meaningful benchmark for NLU systems. However, due to the concerns outlined above, the results as presented do not adequately reflect this, leading to no change in the initial assessment.