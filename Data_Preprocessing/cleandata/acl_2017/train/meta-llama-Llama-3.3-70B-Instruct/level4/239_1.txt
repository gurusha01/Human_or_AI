This paper presents a framework for assessing word embeddings based on their data efficiency and performance on simple supervised tasks. The primary motivation behind this approach is that word embeddings are often utilized in transfer learning settings, where their effectiveness is evaluated based on the speed of training a target model. The proposed method employs a range of simple tasks, including standard benchmarks such as word similarity and word analogy, which are evaluated in a supervised manner. Experimental results across a diverse set of embeddings reveal that rankings tend to be task-specific and vary according to the amount of training data employed.
Strengths
- The motivation rooted in transfer learning and data efficiency is intriguing, as it directly pertains to the concept of leveraging embeddings as a form of semi-supervised learning.
Weaknesses
- A robust evaluation methodology should have implications for downstream tasks. Specifically, if the proposed approach assigns a particular ranking to a set of embeddings, it should ideally yield the same ranking for a downstream task like text classification, parsing, or machine translation. However, since this aspect is not assessed, it is challenging to trust that the technique offers significant improvements over traditional methods.
- The discussion on injective embeddings appears to be unrelated to the paper's core content and does not contribute to its understanding.
- The experimental section is perplexing, particularly Section 3.7, which mentions that the analysis provides insights into questions such as whether it is worthwhile to fit syntax-specific embeddings even with large supervised datasets, but the conclusion drawn is unclear.
- Furthermore, in Section 3.7, the manuscript states that the findings suggest purely unsupervised large-scale pretraining may not be suitable for NLP applications, which is a bold assumption that is not clearly supported by the proposed evaluation approach.
- The fact that all embeddings were obtained as pre-trained models without control over the training corpora limits the validity of the evaluation presented.
- The manuscript requires thorough proofreading, especially regarding the proper citation of figures, such as the reference to Figure 1 on page 6 despite it being located on page 3.
General Discussion
The paper commences with an interesting motivation but fails to adequately evaluate the effectiveness of its approach. As previously mentioned, any intrinsic evaluation method should ideally be accompanied by a study examining whether its conclusions extend to downstream tasks, which is not undertaken in this paper. The lack of clarity and need for proofreading in the manuscript also hinder comprehension. To enhance the paper, future work could benefit from extrinsic studies and a more controlled experimental setup, such as training all embeddings on the same corpora. In its current state, however, the paper does not contribute significantly to the conference.