This paper proposes a set of evaluation metrics for assessing the quality of generated lyrics, with a focus on balancing originality, stylistic similarity to a given artist, and fluency, as well as coherence. The paper is well-structured and effectively conveys the rationale behind the proposed metrics.
The authors introduce a combination of manually annotated metrics, including fluency, coherence, and match, alongside an automated metric for measuring similarity. Although the similarity metric is novel and intriguing, its effectiveness as an automatic evaluation tool is not substantiated by the paper, as it exhibits low correlations with the other metrics, which the authors suggest should be used independently. The claim that this metric can provide meaningful insights into system performance remains unsupported, lacking correlation with any manually annotated performance metrics. The fact that the proposed metric yields worse scores than a baseline system does not necessarily demonstrate its ability to capture quality, as this could be attributed to a strong baseline.
Notably, the paper lacks references to relevant recent studies, such as the automation of coherence evaluation using mutual information density (e.g., Li et al., 2015). Additionally, references to style matching research from the natural language generation community are absent, including works by Dethlefs et al. (2014) and Pennebaker's style matching research.