The paper under review presents several notable strengths, including:
1. The introduction of novel models that yield substantial and consistent improvements over established baselines in two distinct tasks, namely word similarity and word analogy. This not only demonstrates the efficacy of the proposed models but also underscores the potential benefits of integrating sememe information from existing knowledge resources to enhance word representation learning.
2. The contribution to ongoing research efforts aimed at addressing polysemy in word representation learning, building upon prior work and introducing innovative ideas, such as the application of an attention scheme to incorporate soft word sense disambiguation into the learning process.
However, the paper also exhibits several weaknesses, including:
1. Presentation and clarity issues, with crucial details regarding the proposed models being omitted or poorly described, which would need to be addressed to improve the manuscript's overall quality.
2. The evaluation of the word analogy task appears to be somewhat biased, given that the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors themselves.
Upon closer examination, several aspects of the paper warrant further discussion:
1. Although the authors emphasize the importance of accounting for polysemy and learning sense-specific representations, the evaluation tasks are context-independent, resulting in a single vector per word. The use of word sense disambiguation and sememe information is primarily aimed at improving word representation learning, which requires clarification in the paper.
2. The methodology for learning sememe embeddings is unclear, and the description of the SSA model appears to assume the pre-existence of sememe embeddings. It is essential to understand whether the SAC and SAT models require pre-training of sememe embeddings.
3. A comparison between the proposed models and those that consider only different senses, but not sememes, is lacking. The inclusion of additional baselines based on related work would strengthen the paper.
4. While the authors argue that the proposed models are particularly useful for learning representations for low-frequency words, no empirical evidence is provided to support this claim. Investigating this aspect further would be beneficial, as the improvement gains seem to be more attributable to the incorporation of sememe information than word sense disambiguation.
5. The evaluation process involves only context-independent word representations, which may not fully capture the benefits of the proposed models. Even if the method allows for learning sememe- and sense-specific representations, they would need to be aggregated for evaluation purposes.
6. The example illustrating HowNet (Figure 1) is unclear, particularly with regards to the modifiers of "computer".
7. The determination of the best parameters for the models is unclear, as is the setting of K and the motivation for setting K' to 2. Providing more details on these aspects would be beneficial.