This manuscript proposes a dialogue agent that utilizes the REINFORCE algorithm to jointly optimize the belief tracker and dialogue manager through interactions with a user simulator, employing a two-phase training approach. Initially, the system undergoes an imitation learning phase, where it is initialized using supervised learning from a rule-based model, followed by a reinforcement learning phase that fine-tunes the system using the RL objective.
The primary strengths of this paper lie in its introduction of a framework that integrates differentiable access to the knowledge base (KB) into the joint optimization process, representing the most significant contribution of the work.
However, several weaknesses are noted. Firstly, the system cannot be considered truly end-to-end due to the handcrafted nature of the response generation, as opposed to a learned approach. Furthermore, the end-to-end model exhibits overfitting to the simulator, resulting in poor performance during human evaluation. This discrepancy raises questions about the authors' emphasis on end-to-end learning versus the value of soft-KB access. While the soft-KB access consistently yields improvements, the benefits of end-to-end learning are less clear. The authors' attempt to illustrate the merits of end-to-end learning in Figure 5 is not convincing. Additionally, the choice of the REINFORCE algorithm, known for its high variance issues, is not justified, and potential improvements using a baseline or the natural actor-critic algorithm are not explored.
In general, despite the mentioned weaknesses, the experimental design is robust, making this an acceptable paper. Nevertheless, if the authors were to refocus the paper on the aspect that genuinely enhances performance—the soft KB access—rather than the end-to-end learning concept, the manuscript would be significantly strengthened.