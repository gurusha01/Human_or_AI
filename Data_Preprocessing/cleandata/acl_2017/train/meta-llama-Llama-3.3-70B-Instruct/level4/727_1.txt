The paper under review presents a compelling task, with several notable strengths. Firstly, the research question is intriguing, and the manuscript is well-organized and easy to follow. Additionally, the authors have created a valuable dataset that may benefit other researchers in the field. The paper also provides a detailed analysis of the model's performance, which is a significant contribution. 
However, there are some weaknesses that need to be addressed. One major limitation is the lack of comparison with existing methods from related work, which would have strengthened the results. Furthermore, the authors could have provided more context about the uniqueness of the task and the limitations of previous research in this area, thereby emphasizing the contributions of their work.
The paper proposes supervised and weakly supervised models for frame classification in tweets, utilizing language-based and Twitter behavior-based signals to generate predicate rules. These rules are then used in a probabilistic soft logic framework to build classification models. The experimental results demonstrate the effectiveness of the predicates created using behavior-based signals in classifying 17 political frames in tweets.
To further improve the paper, it would be beneficial to discuss the distinction between frame classification and stance classification, and whether they can be considered as related but distinct tasks with different levels of granularity. Moreover, the authors could elaborate on the challenges of transitioning from long congressional speeches to short tweets, such as the potential loss of cross-sentential features that are commonly used in previous research.
The use of terminology in the paper could be clarified, as the terms "weakly supervised" and "unsupervised" appear to be used interchangeably. However, given that the classification model uses weak or noisy labels, the more technically correct term "weakly supervised" should be used consistently throughout the manuscript.
The calculation of Cohen's Kappa may not accurately reflect the difficulty of frame classification for tweets, as it primarily represents annotation difficulty or disagreement. Many factors can contribute to a low Kappa value, such as poorly written annotation guidelines or biased annotator selection. Nevertheless, a Kappa value of 73.4% is sufficient to rely on the annotated labels.
The equation used to calculate similarity between a frame and a tweet (Eq. 1) may ignore contextual information, such as negation or conditional statements, which could impact the frame prediction model. The authors may consider using models that can capture similarity with larger text units, such as skip thought vectors or vector compositionality methods.
In the experimental setup, it would be ideal to exclude annotated data from calculating statistics used to select the top N bi/tri-grams to prevent leakage of statistics from the test fold or labeled data. Although this may not have affected the current results, it would have constituted a cleaner experimental setup.
Additional suggestions include providing precision and recall results in Table 4, and ensuring that footnotes are placed correctly according to the relevant rules. Overall, the paper has the potential to make a significant contribution to the field, and addressing these suggestions will further strengthen the research.