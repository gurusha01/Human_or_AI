This paper presents a novel approach to enhancing NLP tasks by leveraging embeddings derived from language models, building upon the success of context-independent word representations. By utilizing context-dependent word representations extracted from the hidden states of neural language models, the authors demonstrate substantial improvements in tagging and chunking tasks when incorporating embeddings from large language models. The paper also features an intriguing analysis that addresses several key questions.
Overall, the paper is of high quality, but several suggestions can be made to further improve it:
- The extensive use of the test set for experiments is a concern; revising Tables 5 and 6 to utilize development data instead would be beneficial.
- Expanding the range of tasks explored would be valuable, as the current focus on NER tagging and chunking may not fully capture the potential of the language model in handling complex, long-range dependencies; including results on tasks like SRL or CCG supertagging would provide more comprehensive insights.
- The necessity of a task-specific RNN, as claimed by the paper, is attributed to the poor performance of a CRF layered on top of language model embeddings. However, it is unclear whether the language model was fine-tuned during this experiment; if not, exploring this possibility could potentially eliminate the need for a task-specific RNN.