Review - Comments after author response
- I appreciate the clarification regarding the ambiguous "two-step framework" reference, which is not related to the two facets. However, I still find the utilization of a pipeline in this context to be a relatively unremarkable contribution.
- You mention that "5. de Marneffe (2012) used additional annotated features in their system. For fair comparison, we re-implement their system with annotated information in FactBank." Nevertheless, the de Marneffe et al. feature cited in the paper, "Predicate Classes," only requires a dependency parser and vocabulary lists from Roser Saurí's PhD thesis. The term "general classes of event" might be referring to FactML event classes, and although it is not entirely clear in their work, I am confident that they could provide clarification.
- I continue to find the phrase "combined properly" to be obscure. While I agree that utilizing LSTM and CNN in their respective appropriate contexts is valuable, it seems to imply that prior work has been improper and that the combination presented is the correct approach.
- Thank you for providing information on the separate LSTMs for each path. I am intrigued by the potential reasons why this combination may be less effective. In any case, experiments with alternative structures, such as this one, deserve to be reported.
---
This paper applies deep neural network technologies to the task of factuality classification as defined by FactBank, achieving performance that surpasses alternative neural network models and baselines reimplemented from the literature.
- Strengths:
The paper presents a sophisticated model for factuality classification and its evaluation in a clear and concise manner. It demonstrates that the use of attentional features and BiLSTM provides a significant benefit over alternative pooling strategies and that the model outperforms a more traditional feature-based log-linear model. Given the limited amount of training data in FactBank, the development of such a highly engineered model seems justified. It is interesting to note that the BiLSTM/CNN model is able to provide benefits despite the limited training data.
- Weaknesses:
My primary concerns regarding this work are: (a) its apparent deviation from the evaluation procedure in prior literature; (b) the failure to present prior work as a strong baseline; and (c) the novelty of the approach.
While I believe that the work is original in its application of deep neural networks to the factuality classification task, and that such work is valuable, the approach itself is not particularly novel. The proposal of a "two-step supervised framework" (line 087) is not especially interesting, given that FactBank was always described in terms of two facets (assuming I correctly interpret "two-step" as referring to these facets).
The work cites Saurí and Pustejovsky (2012), but presents their earlier (2008) and weaker system as a baseline, without considering Qian et al.'s (IALP 2015) work, which compares to the former. Both of these works were developed on the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT TimeBank section, whereas the present work does not report results on a held-out set.
De Marneffe et al.'s (2012) system is also chosen as a baseline, but not all of their features are implemented, and the present system is not evaluated on their PragBank corpus (or other alternative representations of factuality proposed in Prabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)). Consequently, the evaluation lacks comparability to prior work.
Important questions remain unanswered in the evaluation, such as the effect of using gold standard events or SIPs.
Given the success of BiLSTMs with minimal feature engineering, it is somewhat disappointing that this work does not attempt to consider a more minimal system employing deep neural networks on this task, using, for instance, only the dependency path from a candidate event to its SIP plus a bag of modifiers to that path. The inclusion of heterogeneous information in one BiLSTM was an interesting feature that deserved more experimentation: what if the order of inputs were permuted? What if delimiters were used in concatenating the dependency paths in RS instead of the unusual second "nsubj" in the RS chain of line 456? What if each of SIPpath, RSpath, Cue_path were input to a separate LSTM and combined? The attentional features were evaluated together for the CNN and BiLSTM components, but it might be worth reporting whether it was beneficial for each of these components separately. Could the model benefit from providing path information for aux words? Could character-level embeddings be used to account for morphology's impact on factuality via tense/aspect? The proposed future work lacks specificity, considering the many questions raised by this model and the numerous related tasks to which it could be applied.
- General Discussion:
194: Could you specify the classes into which events are being classified?
280: Please state which parameters are part of the model.
321: What do you mean by "properly"? You use the same term in 092, but it is unclear which work you consider improper or why.
353: Is "the chain form" defined anywhere? Is there a citation for this? The repetition of nsubj in the example of line 456 seems like an unusual feature for the LSTM to learn.
356: It may be worth footnoting that each cue is classified separately.
359: "distance" should be replaced with "surface distance."
514: Could you provide the number of SIPs and cues? Perhaps add this information to Table 3.
Table 2 would benefit from the addition of counts for embedded and author events. Percentages can be removed if necessary.
532: Why use 5-fold cross-validation? Given the limited training data, 10-fold cross-validation would be more useful and would not substantially increase training costs.
594: It is unclear whether the benefit comes from PSen or if the increase is significant or substantial. Does it affect the overall results substantially?
674: Is the significance observed across all metrics?
683: Is the drop in F1 due to precision, recall, or both?
686: The sentence is unclear and difficult to understand.
Table 4: Considering the corpus sizes, it seems that only two significant figures should be reported for most columns (except CT+, Uu, and Micro-A).
711: It is unsurprising that RSpath is insufficient, given that the task is related to a SIP and other inputs do not encode that information. It would be more interesting to see the performance of SIPpath alone.
761: This claim is not precise, to my understanding. De Marneffe et al. (2012) evaluate on PragBank, not FactBank.
Minor issues in English usage:
112: "non-application" should be replaced with "not applicable."
145: I think you mean "relevant" instead of "relative."
154: The phrase "can be displayed by a simple source" is unclear.
166: I am unsure what you mean by "basline." Do you mean "pipeline"?