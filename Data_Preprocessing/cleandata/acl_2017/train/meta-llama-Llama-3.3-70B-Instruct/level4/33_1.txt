Review- Strengths:
- The paper proposes a novel approach to incorporating sentiment information through regularization, which is an innovative idea.
- The experiments conducted appear to be technically sound.
- The in-depth analysis of the model provides useful insights.
Weaknesses:
- The approach bears a strong resemblance to distant supervision, which may raise concerns about its distinctiveness.
- The baselines used for comparison are largely inadequate, with most being poorly informed.
General Discussion:
This paper extends the standard LSTM model by integrating sentiment information via regularization, as presented in the introduction. The key claims made in the paper include the limitations of previous CNN approaches in the absence of phrase-level supervision and the expense of phrase-level annotation. The authors propose a "simple model" that leverages other linguistic resources as a contribution.
The related work section provides a comprehensive review of sentiment literature, although it fails to mention prior attempts at linguistic regularization, such as the work presented in [YOG14].
The explanation of regularizers in section 4 is lengthy and repetitive, with notation inconsistencies that make it difficult to follow. For instance, the variable "p" is sometimes used with a subscript and other times with a superscript, while the parameter "Î²" is not explicitly mentioned. The concept of "position" t is also unclear, as it seems to refer to both sentence and word indices. Clarification is needed to avoid confusion.
A significant concern with this paper is the fine line it treads between regularization and distant supervision. The authors' comparison to Teng et al.'s NSCL model raises questions about the fairness of the comparison, particularly if different lexicons were used. It is also unclear why the authors did not run NSCL on the MR dataset, which could have been achieved by simply swapping the datasets. The remaining baselines lack the use of lexical information, making them weak comparisons. A more informative baseline would be a vanilla LSTM model with lexical information appended to the word vectors.
The authors' analysis of the models is helpful, demonstrating that the model learns intensification and negation to some extent. However, it would be interesting to investigate how the model handles out-of-vocabulary words with respect to the lexicons. Does the model generalize beyond memorization, and does it learn to recognize sentiment in unseen words? A minor remark is that the figures and tables are too small to be read in print.
The paper is generally well-written, although it could benefit from proofreading to address grammatical errors and typos. The beginning of the abstract is particularly difficult to read.
Overall, the paper explores a reasonable research direction, but the comparison to related work is a potential issue. Including stronger baselines in the final model could address this concern. Establishing the comparability of the experiments is crucial, and the authors' response has helped to clarify some of these concerns.
[YOG14] http://www.aclweb.org/anthology/P14-1074
--------------
Update after author response
The authors' clarification of the experimental setup has addressed some concerns. The comparison to Teng et al.'s NSCL model is now considered fair. However, the baselines remain a weakness, and a more comprehensive comparison, including a significance test, would be beneficial. The effect of the model on out-of-vocabulary words is still unclear and would make for a more interesting additional experiment than the current regularization experiments.