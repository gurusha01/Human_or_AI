This manuscript proposes a gated self-matching network architecture for question answering in reading comprehension. The approach consists of three primary components: 
(a) The authors introduce a gated attention-based recurrent network to generate a question-aware representation of the passage. This involves incorporating an additional gate into attention-based recurrent networks to assess the relevance of passage segments to the question, utilizing both word and character embeddings to address out-of-vocabulary words. This component draws inspiration from Wang and Jiang (2016).
(b) The paper presents a novel self-matching attention mechanism designed to enhance question and passage representations by considering a broader passage context essential for answer inference.
(c) For the output layer, the authors employ pointer networks to identify answer boundaries, also inspired by Wang and Jiang (2016).
Overall, the paper makes a valuable contribution and is well-structured.
- Strengths:
The manuscript effectively decomposes the network into three components for clarity, relates each to prior work, and highlights its novelties. The empirical analysis is thorough, including an ablation study that examines the impact of each component. The results are noteworthy and demonstrate the model's effectiveness.
- Weaknesses:
The paper reports results for both a single model and an ensemble model but lacks details on the ensemble's composition and creation process. It is speculated that the ensemble might combine character-based and word-based models. The authors should provide clarification on this in their rebuttal and the manuscript.
- General Discussion:
In addition to the ablation study, a qualitative analysis of example cases where components such as gating, character embedding, and self-embedding prove crucial would be beneficial. This could illustrate scenarios where a simple model fails to answer a question correctly but the inclusion of one or more of these components yields a correct response. Such analysis could be included in an appendix or supplementary material.