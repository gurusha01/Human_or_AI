This study demonstrates the potential benefits of incorporating sememes into word representation learning, particularly when utilized within a suitable attention framework. The authors propose that sememes can serve as a crucial regularizer for both Word Representation Learning (WRL) and Word Sense Induction (WSI) tasks, and introduce the SE-WL model, which simultaneously detects word senses and learns representations. Although the experimental results suggest that WRL benefits from this approach, the exact gains for WSI are unclear, as only a qualitative case study of a limited number of examples has been conducted. Overall, the paper is well-organized and clearly written.
In the final paragraph of the introduction, the authors outline three contributions of their work. However, points (1) and (2) appear to be more related to the novelty of the work rather than its actual contributions. The primary contribution of this study, in my opinion, lies in the results demonstrating that modeling sememe information can lead to better word representations (although the impact on WSI is uncertain) compared to other competitive baselines. Point (3) does not constitute a contribution or a novelty.
The three strategies employed for SE-WRL modeling are logical and can be intuitively ranked based on their potential effectiveness. The authors provide a clear explanation of these strategies, and the experimental results support their intuition. Nevertheless, I consider the Most Suitable Sense (MST) approach to be a fourth strategy rather than a baseline inspired by Chen et al. (2014), as many WSI systems assume one sense per word given a context. MST often outperforms the Sense-Sense Attention (SSA) and Sense-Aware Attention (SAC) approaches. Unless the authors clarify otherwise, MST seems to be equivalent to the Sense-Aware Attention (SAT) approach, with the difference being that the target word is represented by the most probable sense rather than an attention-weighted average of all its senses. MST is still an attention-based scheme where the sense with the maximum attention weight is chosen, although it is unclear whether the target word is represented by the chosen sense embedding or a function of it.
The authors do not provide a clear explanation for the selection of datasets used for training and evaluation tasks. The reference to the Sogou-T text corpus is unhelpful, as I am not familiar with the Chinese language. It is unclear which specific dataset was used, as multiple datasets are mentioned on the referenced page. The use of two word similarity datasets is also unclear, as different models perform differently on these datasets. The choice of datasets does not allow for a comparison with the results of other studies, which raises further questions.
It is unclear whether the proposed SAT model achieves state-of-the-art results for Chinese word similarity. For example, Schnabel et al. (2015) report a score of 0.640 on the WordSim-353 dataset using CBOW word embeddings.
I require clarification on certain model parameters, such as the vocabulary sizes for words (does the Sogou-T corpus contain 2.7 billion unique words?) and word senses (how many word types from HowNet?). Due to the notation used, it is unclear whether the embeddings for senses and sememes for different words are shared. I hope that this is the case, but if so, why were 200-dimensional embeddings used for only 1889 sememes? A discussion of the complexity of model parameters would be beneficial.
Perhaps due to space constraints, the discussion of the experiment results lacks insight into observations beyond the fact that SAT performs the best. Additionally, the authors claim that words with lower frequency are learned better with sememes without evaluating this on a rare words dataset.
I have read the authors' response.