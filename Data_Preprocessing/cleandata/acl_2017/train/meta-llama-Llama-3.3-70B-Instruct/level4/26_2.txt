Strengths:
This manuscript presents a significant contribution to the field of knowledge base-based question answering (KB-QA), addressing the challenge of retrieving results from a structured knowledge base (KB) based on a natural language question. KB-QA is a crucial and complex task that has garnered considerable attention.
The authors have clearly articulated the novelty and contributions of their work, providing a comprehensive overview of previous research and a comparative analysis of their approach with related methods.
Previous neural network-based KB-QA approaches have limitations, as they represent questions and answers as fixed-length vectors, essentially treating them as bags of words, which restricts the expressiveness of the models. Furthermore, prior work has not leveraged unsupervised training over knowledge graphs (KG), which could potentially enhance the generalizability of trained models. This paper introduces two major innovative concepts to the question answering problem.
1) The proposed approach employs a cross-attention-based neural network as its foundation, utilizing attention mechanisms to capture different aspects of questions and answer aspects. The cross-attention model consists of two interdependent components: the A-Q attention part, which dynamically captures various aspects of the question, resulting in distinct embedding representations, and the Q-A attention part, which assigns different attention weights to the question when computing its similarity score with answer aspects.
2) The answer embeddings are learned not only through the QA task but also modeled using TransE, allowing for the integration of prior knowledge from the KB side.
The experimental results, obtained on Web questions, demonstrate that the proposed approach outperforms state-of-the-art end-to-end methods. The ablation experiment clearly highlights the contributions of the cross-attention mechanism and global information, which significantly improve QA performance.
The paper is rich in content, and the proposed framework is impressive and novel compared to previous works.
Weaknesses:
The manuscript is well-structured, and the language is clear and correct. However, some minor typos were identified:
1. Page 5, column 1, line 421: "re-read" should be replaced with "reread".
2. Page 5, column 2, line 454: "pairs be" should be revised to "pairs to be".
General Discussion:
In Equation 2, the four aspects of candidate answer aspects share the same weights (W) and bias (b). It would be interesting to explore the use of separate W and b for each aspect. 
Consider assigning a distinct name to the proposed approach, such as ANN or CA-LSTM, to differentiate it from other methods (e.g., those listed in Table 2).
In general, capturing different aspects of question answer similarity is a good idea, and the cross-attention-based neural network model presents a novel solution for this task. The experimental results demonstrate the effectiveness of the authors' approach. Although the overall performance may be weaker than some SP-based methods or integrated systems, this paper represents a good attempt in the end-to-end KB-QA area and deserves encouragement.