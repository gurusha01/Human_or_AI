This paper presents a direct enhancement to the left-to-right beam search algorithm, enabling it to integrate lexical constraints in the form of mandatory word sequences within machine translation output. The proposed algorithm demonstrates efficacy in both interactive translation and domain adaptation scenarios.
While the suggested extension is relatively straightforward, the paper's formalization of this concept constitutes a valuable contribution. Notably, it is intriguing to observe that neural machine translation (NMT) can effectively handle a set of unordered constraints without associated alignment information. Potential applications of this technique extend beyond the explored domains, such as enhancing NMT's capability to manage non-compositional constructions - an area where NMT may still trail behind traditional statistical machine translation (SMT).
The primary limitation of the paper lies in the restricted scope of the experiments. Although the interactive machine translation simulation verifies the method's functionality, it is challenging to assess its performance comprehensively, including the frequency of successful constraint incorporation (as the substantial BLEU score increases provide only indirect evidence). Furthermore, the adaptation experiments should have included a comparison with the standard "fine-tuning" baseline, which could have been feasibly executed on the 100K Autodesk corpus.
Despite this limitation, the paper presents a worthwhile contribution that merits publication.
Additional comments:
Line 422: The term "coverage vector," commonly used in phrase-based machine translation (PBMT), may be misleading; a "coverage set" appears to be a more suitable data structure.
Table 2 would benefit from including the average number of constraints per source sentence in the test corpora, allowing for a more nuanced interpretation of the BLEU score improvements.