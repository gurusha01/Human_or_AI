Review:
- Strengths:
-- The approach presented is well-founded, with a clear and concise description, accompanied by robust results.
- Weaknesses:
-- There are no significant weaknesses beyond the specific comments outlined below.
- General Discussion:
This paper introduces a novel method known as attention-over-attention, designed to enhance reading comprehension. The initial layers of the network generate vectors for each query word and document word, resulting in a |Q|xK matrix for the query and a |D|xK matrix for the document. Given that the answer is a document word, an attention mechanism is employed to assign weights to each word based on its interaction with query words. The authors extend the traditional attention mechanism by calculating a weight for each query word through a separate attention mechanism, which is then used to weight the primary attention over document words. The evaluation is thoroughly conducted on benchmark datasets, and the results are analyzed and compared to prior work, providing valuable insights. In my opinion, this is a solid contribution to the field, addressing an important problem with a well-motivated and clearly described method, allowing researchers to easily replicate the results and apply the techniques to similar tasks.
- Other remarks:
-- On page 4, Equation 12, it appears that 'i' iterates over the training set, and 'p(w)' refers to P(w|D,Q) from the previous equation. Clarification is necessary to avoid confusion.
-- The exploration of initializing word embeddings with pre-existing vectors, such as Google News or Glove, is not discussed. It would be beneficial to understand whether the general-purpose word semantics would be useful in this task and the reasoning behind the chosen approach.
-- On page 6, lines 589-592, the statement 'letting the model explicitly learn weights between individual attentions' is unclear. Specifically, it is uncertain whether this refers to the authors' architecture, where the GRU output indirectly influences the attention applied to each query and document word. Clarification would be helpful. Additionally, the improvement on the validation set seems to be 4.0 (72.2-68.2), rather than 4.1.
-- On page 7, Table 5, the relatively higher weight for the local LM in the CN task, despite the limited benefit of adding it, warrants further explanation. Providing insights into this observation would enhance the reader's understanding.
-- Releasing the software as part of this submission would be a valuable addition.
-- A typo is present on page 2, line 162, right column: "is not that effective than expected" should be corrected to "is not as effective as expected".
-- Another typo is found on page 7, line 689, right column: "appear much frequent" should be corrected to "appears more frequently".
-- On page 8, lines 719-721, left column, the text "the model is hard to" should be revised to "it is hard for the model to", and "hard to made" should be corrected to "hard to make".