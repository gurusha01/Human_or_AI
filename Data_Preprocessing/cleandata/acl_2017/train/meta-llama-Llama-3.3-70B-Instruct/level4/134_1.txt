Review
Strengths:
The manuscript is well-structured and clearly presented, making it easy to comprehend. The methodologies and results are intriguing and demonstrate a thorough approach to the research.
Weaknesses:
However, the evaluation and results may be compromised due to several concerns, which are elaborated upon below.
General Discussion:
This manuscript proposes an end-to-end argumentation mining system utilizing neural networks, tackling the problem through two distinct approaches: sequence labeling and dependency parsing. The authors also explore a multitask learning setting for the sequence labeling approach, providing a comprehensive explanation of the model's motivation. Unlike existing methods that rely on Integer Linear Programming (ILP), manual feature engineering, and manual ILP constraint design, the proposed model eliminates the need for manual effort. Furthermore, the model jointly learns subtasks in argumentation mining, thereby avoiding the error propagation issue inherent in pipeline methods. Although some details are missing, the methods are generally well-explained.
The experimental design is substantial, comparisons are properly conducted, and the results are noteworthy. Nevertheless, the primary concern lies in the limited size of the dataset and the large capacity of the employed (Bi)LSTM-based recurrent neural networks (BLC and BLCC). With only approximately 320 essays for training and 80 essays for testing, the development set size is not specified in the manuscript or supplementary materials. This raises concerns about the adequacy of training data, which is a critical issue. Compared to standard sequence labeling tasks that typically involve hundreds of thousands or even millions of tags, the total number of tags in the training data is likely only a few thousand. As a result, it is uncertain whether the model parameters are properly trained. The manuscript also fails to analyze the overfitting problem, and it would be beneficial to include training and development "loss" values during training. The authors provide some evidence that may indicate overfitting, as stated in Line 622: "Our explanation is that taggers are simpler local models, and thus need less training data and are less prone to overfitting."
Due to these concerns, the stability of the models is also questionable. To address this, the mean and standard deviation of multiple runs with different parameter initializations should be included. Statistical significance tests would provide additional insights into the stability of the models and the reliability of the results. Without these tests, it is challenging to determine whether the improved results are due to the superiority of the proposed method or chance.
Although the neural networks employed for modeling tasks utilize regularization techniques, the small dataset size necessitates more attention to regularization methods. The manuscript does not mention regularization, and the supplementary material only briefly discusses regularization in LSTM-ER. This issue needs to be properly addressed in the manuscript.
Instead of the current hyper-parameter optimization method described in the supplementary materials, consider using Bayesian optimization methods.
It is recommended to move the information about pre-trained word embeddings and error analysis from the supplementary material to the manuscript, as an additional page should suffice for this purpose.
Including inter-annotator agreement scores would provide valuable insights into the performance of the systems and the available room for improvement. Relevant information can be found in the paper describing the dataset.
To enhance the quality of figure 1 for black and white prints, consider illustrating it with different colors.
Edit:
Following the authors' responses to my questions, I have increased the recommendation score to 4. I suggest including F1-score ranges in the manuscript and reporting the mean and variance of different settings. However, I remain concerned about model stability, particularly the large variance of the Kiperwasser setting, which requires thorough analysis. Even the F1 changes in the range [0.56, 0.61] are relatively large. Including these score ranges in the manuscript will facilitate replication of the work.