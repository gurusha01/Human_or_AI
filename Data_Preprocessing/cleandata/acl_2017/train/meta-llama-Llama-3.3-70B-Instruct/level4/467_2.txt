This study presents a novel self-learning bootstrapping approach for learning bilingual word embeddings, achieving competitive results in bilingual lexicon induction and cross-lingual word similarity tasks with minimal bilingual supervision. The method demonstrates robust performance even with an extremely small seed dictionary or one constructed without language pair specific information.
The paper is well-written, and its original contribution lies in connecting ideas from prior work, drawing inspiration from various papers on the subject, including pre-embedding work on self-learning and bootstrapping. However, the paper's main limitations include a partial recognition of related work and a lack of comparisons with relevant baselines, which should be addressed in future versions.
The concept of self-learning/bootstrapping of bilingual vector spaces is not new, having been applied to traditional count-based bilingual vector spaces in previous work, such as Peirsman and Pado (NAACL 2010) and Vulic and Moens (EMNLP 2013). The authors should acknowledge this and recognize that their proposed bootstrapping approach is not entirely novel in this domain.
The relation to Artetxe et al.'s work is unclear, as the proposed bootstrapping algorithm appears to be an iterative approach utilizing their previously proposed model, with the only difference being reparametrization. A more explicit statement is needed to understand the contribution of the bootstrapping approach and the new parametrization.
Several relevant papers, such as Duong et al. (EMNLP 2016) and Vulic and Korhonen (ACL 2016), have not been mentioned or discussed. These papers should be included in the comparison to provide a more comprehensive understanding of the method's performance.
The proposed algorithm seems almost invariant to the starting seed lexicon size, yielding similar final BLI scores regardless of the starting point. This suggests a limitation of current offline approaches, which seem to have reached a ceiling in terms of performance. The authors should discuss how to break this ceiling and further improve BLI results with such methods.
The convergence criterion is not clearly explained, and it is unclear how the procedure terminates, particularly for languages without a cross-lingual word similarity dataset. The authors should provide a more detailed explanation of the convergence criterion and its implications for the method's efficiency and efficacy.
Minor suggestions include adding a Finnish Web as a Corpus (WaC) corpus and exploring an additional language pair that does not share the alphabet. The authors' response to the initial review helped clarify some points, and it is hoped that they will address these concerns in the final version.