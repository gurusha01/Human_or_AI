Strengths:
The paper provides empirical evidence supporting the benefits of leveraging external knowledge.
Weaknesses:
While the authors propose the Rare Entity prediction task to demonstrate the importance of external knowledge in real-world NLP applications, the motivation behind this task is not thoroughly justified. It is unclear why this task is significant and how it would benefit real-world NLP applications. The paper lacks a compelling argument for introducing a new task. In contrast to reading comprehension tasks, where evidence for the correct answer can be found within the given text, allowing for the learning of a model of the world or basic reasoning, rare entity prediction seems unrealistic due to human limitations in remembering names. Although the authors acknowledge the task's difficulty due to the large number of rare entities, other tasks with similar or greater difficulty, such as predicting the correct morphological form of a word in morphologically rich languages, have more obvious applications, for example, in machine translation.
General Discussion:
A more detailed characterization of the dataset would be beneficial. Based on Figure 1 and Table 4, it appears that overlapping entities are a crucial feature, as predicting the blank in Figure 1 without seeing the word "London" in Peter Ackroyd's description seems impossible. Before evaluating neural networks, it is essential to understand the dataset's characteristics and the cognitive process involved in finding the correct answer.
Given the lack of dataset characterization, the chosen baselines seem inappropriate. Initially, CONTENC appears to be a natural choice, but since the candidate entities are rare, their embeddings are unreliable. As a result, it is expected that CONTENC would not perform well. Would it be fairer to initialize the embeddings from pre-trained vectors on a massive dataset? This might reveal some similarity between entities like Larnaca and Cyprus in the embedding space, allowing CONTENC to make correct predictions, as seen in Table 4. It would be interesting to see the performance of TF-IDF+COS and AVGEMB+COS if only entities are used to compute those vectors.
From a modeling perspective, the authors' choice of a sigmoid predictor that outputs a numerical score between (0,1) is appreciated, as it helps avoid normalization over the list of rare candidate entities, which can be challenging to learn reliable weights for. However, alternative techniques, such as Pointer Networks, could be used. A representation hi for Ci (including the blank) can be computed using an LSTM or BiLSTM, and then Pointer Network can provide a probabilistic interpretation p(ek|Ci) ‚àù exp(dot(dek, h_i)). In my opinion, Pointer Network would be a suitable baseline. Additionally, does the unbalanced set of negative and positive labels affect the training, considering that the models make one positive prediction while the number of negative predictions is at least four times higher?
Although I find the Rare Entity prediction task to be unrealistic, having the dataset, it would be more interesting to explore the reasoning process that leads to the correct answer, such as which set of words the model attends to when making predictions.