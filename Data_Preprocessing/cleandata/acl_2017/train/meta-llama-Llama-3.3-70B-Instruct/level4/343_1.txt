Review:
- Strengths:
i. The paper is well-structured and clear in its presentation
ii. It offers comprehensive comparisons across different experimental settings, demonstrating state-of-the-art performance
- Weaknesses:
i. The comparison in the experiments is primarily with previous supervised approaches, whereas the proposed method employs a semi-supervised approach, even when sufficient training data is available
- General Discussion:
This paper explores the use of a pre-training approach to enhance Chinese word segmentation, building upon transition-based neural word segmentation. It leverages external resources such as punctuation, soft segmentation, POS, and heterogeneous training data through multi-task learning, essentially framing each external source as an auxiliary classification task. The experimental results indicate that the proposed method achieves state-of-the-art performance in six out of seven datasets.
The paper is well-written and easy to follow, with a series of experiments that validate the effectiveness of the proposed method. However, a notable issue arises. The method proposed is semi-supervised, utilizing external resources for pre-training characters, and additionally employs heterogeneous training datasets for pre-training purposes. In contrast, the baseline comparisons are based on supervised learning methods. Generally, semi-supervised learning outperforms supervised learning due to its ability to leverage extensive auxiliary information. Therefore, a more appropriate comparison would be with other semi-supervised approaches to ensure a fair evaluation of the method's superiority.
POST AUTHOR RESPONSE:
The reviewer's concern stems from the use of an additional "gold-labeled" dataset for pre-training character embeddings. Some baseline experiments utilized label information predicted automatically by their base models, as the authors noted. For a comparison to be considered fair, all conditions should be equal. Thus, even though the gold dataset is not directly used for training the segmentation model, the use of another "gold" dataset for training character embeddings seems like an unfair comparison, as it gives the proposed method an advantage.