Review
Strengths: The paper is generally well-organized and easy to follow, with original ideas and interesting results. The explanation is clear, making it possible to understand most of the concepts presented.
Weaknesses: However, there are some concerns regarding the interpretation of the results and the lack of sufficient scientific evidence to support some of the claims, particularly those related to the method's ability to learn morphology.
General Discussion:
This paper proposes a complex architecture for character-level neural machine translation (NMT), extending the classical encoder-decoder architecture with a deep word-encoding layer and a deep word-decoding layer. The goal is to leverage the benefits of character-level NMT, such as reduced vocabulary size and flexibility in handling unseen words, while improving performance by using sub-word representations. The authors claim that their deep word-encoding model can learn morphology more effectively than other state-of-the-art approaches.
There are concerns regarding the evaluation methodology. The comparison with other state-of-the-art systems, such as bpe2char, is based on training time and BLEU score. However, the advantage of the proposed model (DCNMT) is not clearly evident, with minimal differences in BLEU scores (0.04 for Cs-En and 0.1 for En-Cs) and no statistical significance information provided. Furthermore, the training time for bpe2char is 8 days less than DCNMT for Cs-En, and the training time for En-Cs is not provided. A more comprehensive comparison with bpe2char is necessary to demonstrate the advantages of the proposed model.
Another concern is the lack of formal evaluation and experiments to support the claims regarding the system's ability to learn morphology, as presented in Section 5.2. Although the examples provided are well-chosen and explained, the section relies heavily on commentary without empirical evidence. It is suggested that the authors either extend this section with formal evaluations or move it to an appendix and soften their claims.
Other Comments, Doubts, and Suggestions:
* Many acronyms, such as LSTM, HGRU, CNN, and PCA, are used without definition or are defined after their initial use. Defining these acronyms would improve clarity.
* The concept of energy is introduced in Section 3.1 but could be refreshed in Section 5.2, where it is used extensively, along with an explanation of how to interpret it.
* The acronym BPE is initially defined with capital letters but subsequently used in lowercase; the reason for this inconsistency is unclear.
* The mention of not using a monolingual corpus in Section 4.1 may be unnecessary.
* Figure 4a appears to have an issue, as the colors for energy values are not displayed for every character.
* Table 1's results for model (3) (Chung et al. 2016) for Cs-En were not taken from the original paper, as they are not reported. If the authors computed these results themselves, it should be mentioned.
* The description of French as morphologically poor might be misleading; it would be more accurate to say it is less rich than Slavic languages like Czech.
* The provision of a link for WMT'15 training corpora but not for WMT'14 is puzzling.
* Several references are incomplete.
Typos:
* "..is the bilingual, parallel corpora provided..." should be "..are the bilingual, parallel corpora provided..."
* "Luong and Manning (2016) uses" should be "Luong and Manning (2016) use"
* "HGRU (It is" should be "HGRU (it is"
* "coveres" should be "covers"
* "both consists of two-layer RNN, each has 1024" should be "both consist of two-layer RNN, each have 1024"
* "the only difference between CNMT and DCNMT is CNMT" should be "the only difference between CNMT and DCNMT is that CNMT"