This paper proposes a novel approach to fine-grained IsA extraction by learning modifier interpretations, which is an intriguing task with a clear motivation. The overall approach appears to be well-founded, and the experimental results demonstrate an increase in the number of fine-grained classes that can be populated.
However, certain sections of the paper are difficult to comprehend. Specifically, the multiplication of D((e, p, o)) by w in Eq (7) and the explanation of the weight for e in Eq. (8) as the product of its frequency of observation with a particular property and the weight of that property for the class MH are unclear. Furthermore, it is uncertain how effective the introduction of compositional models is in enhancing coverage. The modifier expansion seems to be a significant factor in the increased coverage, and it would be interesting to see how the baseline 'Hearst' performs with this expansion.
In general, while the task is interesting and the approach is solid, the paper's weaknesses, as mentioned above, leave me ambivalent about its overall quality.
On a minor note, some notations are confusing, such as the meaning of 'H', which appears to represent both a class (e.g., (e, H)) and a noun phrase (e.g., (H, p, N, w)). Additionally, in the "Precision-Recall Analysis" section, it is unclear why the authors use the area under the ROC curve instead of the area under the Precision-Recall curve.
After reviewing the response, I remain unsatisfied with the explanation regarding modifier expansion. While I do not believe that modifier expansion can be directly applied to Hearst in the same way as the proposed method, I wonder if there is no way to leverage similar modifiers to improve Hearst's coverage. My recommendation remains unchanged, as the effectiveness of introducing compositional models itself is still unclear, leaving me torn between a score of 3 and 4.