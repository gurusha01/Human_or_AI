Review:
- Strengths:
*- The task addressed is noteworthy
*- The proposed model is simple yet achieves the best results on SQuAD with a single model
*- The evaluation and comparison methods are well-executed
- Weaknesses:
*- The analysis of errors and results is inadequate (see detailed comments below)
- General Discussion:
This paper presents a novel approach to directly querying Wikipedia for answering open-domain questions, comprising two primary components: a module for querying and fetching relevant Wikipedia articles and another for answering questions based on the retrieved articles.
The document retrieval system employs a traditional information retrieval (IR) approach, relying on term frequency models and n-gram counts. In contrast, the question-answering system utilizes a feature representation for paragraphs that incorporates word embeddings, indicator features to identify whether a paragraph word appears in the question, token-level features such as part-of-speech (POS) and named entity recognition (NER), and a soft feature to capture the similarity between question and paragraph tokens in the embedding space. This combined feature representation serves as input to a bi-directional Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) for encoding. For questions, an RNN operating on word embeddings is used, and these are then utilized to train an overall classifier independently for start and end spans of sentences within a paragraph to answer questions.
The system has been trained on various open-domain question answering (QA) datasets, including SQuAD and WebQuestions, by modifying the training data to include articles fetched by the IR engine instead of just the actual correct document or passage.
Overall, the paper is interesting and easy to follow, but several questions arise:
1) The IR system achieves an Accuracy@5 of over 75%, and the document reader performs well individually, outperforming the best single models on SQuAD. However, there is a significant drop in performance, as seen in Table 6. The authors mention that using the best paragraph instead of the fetched results increases the accuracy to 0.49 (from 0.26), but this is still substantially lower than the 0.78-79 achieved on the SQuAD task. This discrepancy suggests that the neural network for matching is not learning the answers as effectively when using the modified training set, which includes fetched articles, compared to the document understanding task. A more in-depth analysis of this issue should be provided, including the training accuracy in both cases and potential strategies for improvement. Although the authors allude to this in the conclusion, it warrants further discussion in the paper to offer meaningful insights.
2) The authors' decision to treat this as a pure machine comprehension task and avoid relying on external sources like Freebase, which could have aided in entity typing, is understandable. However, exploring the use of such sources would have been interesting. Relating to the first question, if the error is indeed due to highly relevant topical sentences, as the authors suggest, could entity typing have potentially improved the results?
The authors should also consider referencing QuASE (Sun et al., 2015, at WWW2015) and similar systems in their related work. QuASE is an open-domain QA system that answers questions using fetched passages, but it relies on the web instead of just Wikipedia.