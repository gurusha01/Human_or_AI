The paper presents a multitask learning approach to sequence labeling, leveraging language modeling as an auxiliary objective. This involves a bidirectional neural network architecture that predicts output labels and either the preceding or succeeding word in a sentence, resulting in improved performance over baselines in tasks such as grammatical error detection, chunking, named entity recognition (NER), and part-of-speech (POS) tagging.
- Strengths:
The paper's contribution is well-articulated and generally easy to comprehend, with the model and experiments being adequately detailed. The benefits of incorporating an auxiliary objective are clearly demonstrated, showcasing the potential of this approach.
- Weaknesses:
A significant limitation of the paper is its lack of engagement with the extensive body of related work across the tasks it addresses. The comparison tables (1-3) only include the systems proposed in the paper, with limited textual references to other studies. For a contribution claiming novelty and advancement, it is crucial to properly document these improvements by reporting relevant scores alongside novel ones and, ideally, through replication. Given the availability of datasets and previous results, as well as the public accessibility of previous systems, the omission of a thorough comparison is notable.
The claim of achieving "new state-of-the-art results for error detection on both FCE and CoNLL-14 datasets" requires more substantial evidence, particularly in light of existing reports and studies such as Rei and Yannakoudakis' (2016) work. The assertion that "The baseline results are comparable to the previous best results on each of these benchmarks" may misleadingly suggest that the baseline system encompasses all previous contributions, which is not accurate.
- General Discussion:
The treatment of POS tagging seems cursory, and the comparison with Plank et al. (2016) is somewhat unfair given their work's broader scope across multiple languages and its relevance to performance across language families. The paper does not adequately address how the proposed system would perform with multiple languages or in low-resource settings, leaving significant questions unanswered.
The concept of integrating language modeling as an auxiliary task is intriguing, and the architecture and initial sections of the paper are commendable. However, there is a noticeable disconnect between these aspects and the experimental sections, suggesting a need for further development.
To enhance the paper, it is recommended that the authors provide a more comprehensive and fair treatment of related work, consider replication, and reflect on the system's multilinguality. Given the maturity of the field, as evidenced by the availability of data and systems, the paper should strive to reflect this maturity. With the potential for these improvements to be implemented, the current assessment is borderline.