The paper proposes the DRL-Sense model, which demonstrates a marginal improvement on the SCWS dataset and significant improvements on the ESL-50 and RD-300 datasets.
However, several technical concerns arise:
1. The authors should clarify two drawbacks mentioned in section 3.2. The first drawback suggests that optimizing equation (2) leads to underestimation of the sense probability. Given that equation (2) represents the expected reward of sense selection, with z{ik} and z{jl} being independent actions and only two actions to optimize, this should be relatively straightforward. In fact, optimizing expected rewards over a sequence of actions for episodic tasks has been successfully done in NLP settings, such as sequence level training with recurrent neural networks, even with more challenging tasks like machine translation. The DRL-Sense model has a maximum of 3 actions and lacks sequential nature, making it difficult to accept the claim about the first drawback.
2. The second drawback, accompanied by detailed math in Appendix A, states that the update formula minimizes the likelihood due to the negative log-likelihood. However, standard optimizers can be used to minimize the negative reward, which is always greater than 0. This is a common practice in many modeling tasks, such as language models, where negative log-likelihood is minimized instead of maximizing the likelihood. The authors claim that when the log-likelihood reaches 0, it indicates that the likelihood reaches infinity, causing computational issues with U and V. It is unclear why the likelihood would reach infinity; should it not reach 1 instead?
Additionally, the authors should explain how DRL-Sense is based on Q-learning, as the horizon in the model is length 1, with no transition between state-actions and no Markov property. The reward in DRL-Sense is also unclear; is it 0 for all state-action pairs or the cross-entropy in equation (4)? The cross-entropy in equation (4) is computed over which variable, as q(Ct, z{i, k}) is a scalar and Co(z{ik}, z{jl}) is a distribution over the total number of senses?
The use of dropout for exploration should be justified, and the authors should consider using epsilon-greedy exploration instead. Dropout is typically used for model regularization to prevent overfitting, and it is unclear whether the gain in using dropout is due to exploration or regularization.
The Q-value is stated to be a probabilistic estimation, but the set of variables that the distribution is defined over is unclear. When summing over this set of variables, does it equal 1? The definition of q in equation (3) lacks a normalizing constant, making it unclear whether q is a valid distribution. This is related to the value 0.5 in section 3.4, which is used as a threshold for exploration. Why was 0.5 chosen, and does the model allow for the creation of new senses at the beginning of training or after a few epochs?
In general, the omission of negative samples in line 517 should be justified, as negative sampling has been successfully used in word2vec for learning representations. However, when modeling a distribution p() over senses/words, noise contrastive estimation is often preferred. The DRL-Sense model uses collocation likelihood to compute the reward, and it is unclear how the approximation presented in the paper affects the learning of the embeddings.
The authors may consider task-specific evaluation for sense embeddings, as suggested in recent research. Evaluation methods for unsupervised word embeddings and problems with evaluation of word embeddings using word similarity tasks should be taken into account.