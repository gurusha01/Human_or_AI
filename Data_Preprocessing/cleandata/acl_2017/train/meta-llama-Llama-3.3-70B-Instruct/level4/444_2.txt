This paper examines the evaluation of systems that generate rap lyrics through ghostwriting, focusing on manual and automatic assessment methods. The authors propose a manual evaluation framework considering three crucial aspects: fluency, coherence, and style matching. Additionally, they introduce automatic metrics to measure uniqueness, using maximum training similarity, and stylistic similarity, via rhyme density.
The paper presents some intriguing analysis and discussions, particularly the approach to manually evaluating style matching, which seems reasonable.
However, several concerns arise. Firstly, I question the decision to only assess fluency and coherence at the line level, citing Wu (2014) as a reference. Nevertheless, Wu's work appears to focus on a different context of hip hop lyrical challenges and responses, which inherently requires line-level evaluation. In contrast, this study involves full verses comprising multiple lines that should exhibit topical and structural coherence. I see no compelling reason not to evaluate fluency and coherence at the verse level.
Furthermore, I am skeptical about the reliance on automatic metrics, given the primary goal of generating "similar yet unique lyrics." The uniqueness evaluation is performed at the verse level, but this may not account for rappers who produce lyrics within a limited range of topics or themes. A system that can only extract lines from different verses might yield a fluent and coherent verse with a low similarity score, but it is unclear whether the system truly generalizes well. Regarding stylistic similarity, I do not believe rhyme density is sufficient, as it is position-independent and may not capture the full stylistic information of an artist.
It appears that the automatic metrics have not been validated to correlate well with manual ratings for uniqueness and stylistic matching. I also wonder if evaluating the semantic information commonly expressed by a specified rapper is necessary, beyond just considering rhythm.
While I understand the motivation behind this study is the lack of a robust evaluation methodology, I find one statement puzzling: "our methodology produces a continuous numeric score for the whole verse, enabling better comparison." Is facilitating comparisons more important than making more reliable, albeit slightly vague, judgments?
A minor issue is the incorrect use of quotation marks in Line 389.