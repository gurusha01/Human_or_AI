Review
Strengths:
This manuscript presents a comprehensive array of accuracy results, organized in a 2 x 2 x 3 x 10 matrix, which systematically explores the effects of varying parameters in embedding models. The parameters under investigation include context type (Linear or Syntactic), position sensitivity (True or False), embedding model (Skip Gram, BOW, or GLOVE), and task (Word Similarity, Analogies, POS, NER, Chunking, and 5 text classification tasks). The study's objective, which aims to examine performance variations as these parameters change, is intriguing and relevant to the ACL community. Similar studies, such as those by Nayak et al., have been well-received in the past.
Weaknesses:
While the paper's investigation into the effects of systematically changing context types and position sensitivity is commendable, the execution and analysis of the results are disappointing.
A) The lack of hyper-parameter tuning is a significant concern. For instance, the dimensionality of word embeddings is set to 500 without justification, and most hyper-parameters are adopted from Levy et al.'s best configuration without further optimization. This oversight makes it challenging to draw conclusive comparisons between methods, as hyper-parameter tuning can substantially impact performance.
B) The paper occasionally provides unclear or contradictory explanations for its results. For example, the statement "Experimental results suggest that although it's hard to find any universal insight, the characteristics of different contexts on different models are concluded according to specific tasks" is unclear. Furthermore, the claim that sequence labeling tasks benefit from ignoring syntax in word embeddings is contradictory, as syntax is a valuable feature in such tasks.
C) The paper fails to adequately acknowledge and engage with relevant prior work, such as Lai et al. (2016) and Nayak et al.'s "Evaluating Word Embeddings Using a Representative Suite of Practical Tasks" (ACL 2016). The latter provides recommendations on hyper-parameter tuning and experiment design, which could have strengthened the current study.
D) The paper's choice of classifiers for different tasks is inconsistent and lacks justification. A neural BOW words classifier is used for text classification tasks, while a simple linear classifier is employed for sequence labeling tasks. It is unclear why a simple neural classifier was not used for tagging tasks, particularly since bound representations consistently outperform unbound representations in this task.
General Discussion:
In conclusion, I propose that the authors consider performing factor analysis or other pattern mining techniques on the 120 accuracy values to uncover underlying patterns and relationships between the model's aspects. This could provide a more nuanced understanding of the results and shed light on the complex interactions between the parameters under investigation.