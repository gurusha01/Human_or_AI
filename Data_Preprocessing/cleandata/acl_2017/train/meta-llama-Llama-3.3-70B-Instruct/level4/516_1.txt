The paper presents a valuable and intuitive expansion of recent interactive topic modeling endeavors by enabling human annotators to provide multiple "anchor words" for machine-generated topics. The manuscript is well-structured, and the integration of synthetic and user experiments enhances its overall quality.
However, the paper's scope is somewhat restricted in terms of the comparative analysis of interactive topic model approaches. Although the authors acknowledge most existing approaches and justify their exclusion due to limitations in speed or interaction compatibility, empirical evidence supporting these claims would be beneficial. 
Additionally, experimenting with a more diverse range of datasets beyond the 20 newsgroups, which has become overly utilized, would be desirable.
Overall, this paper contributes a novel, practical, and incremental advancement to interactive topic modeling. The authors have thoroughly evaluated various approach variants through simulated experiments and conducted comprehensive quantitative analyses of both simulated and user experiments, utilizing a range of metrics to assess topic quality from different perspectives.