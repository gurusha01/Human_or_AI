The paper has several strengths, including:
a) The introduction of a Bayesian learning approach for recurrent neural network language models, which demonstrates superior performance to standard SGD with dropout on three tasks.
b) The novelty of applying Bayesian learning to RNNs, a concept that has not been extensively explored.
c) The development of a computationally efficient Bayesian algorithm for RNNs, which has the potential to be of significant interest to the NLP community for various applications.
However, there are several weaknesses that need to be addressed:
A primary concern lies in the evaluation methodology, specifically:
In Section 5.1, the paper presents the performance of different architectures (LSTM/GRU/vanilla RNN) on the character language model task, while comparing learning algorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are compared for the character language model, whereas SGD with/without dropout is compared with SGLD with/without dropout on the word language model task. This inconsistency should be rectified by reporting both dimensions (architectures and learning algorithms) on both character and word language model tasks to assess the portability of the proposed Bayesian learning approaches across tasks and datasets.
On line 529, the paper claims that the performance gain primarily stems from adding gradient noise and model averaging, but this statement lacks empirical justification. To substantiate this claim, an A/B experiment with/without adding gradient noise and/or model averaging should be conducted.
On line 724, Gal's dropout is only applied to the sentence classification task, whereas its performance on language model and captions tasks is not reported. Since Gal's dropout is not specific to sentence classification, its performance should be evaluated on all three tasks to allow readers to fully assess the utility of the proposed algorithms relative to existing dropout approaches.
On line 544, it is unclear whether there is a specific sort order for the samples (\theta1, ..., \thetaK). For instance, are samples with higher posterior probabilities more likely to be assigned higher indices? Additionally, the result of randomly selecting K out of S samples could be reported as an alternative approach.
Given that regular RNN language models are known to be computationally expensive to train and evaluate, a comparison of the training and evaluation times for the proposed Bayesian learning algorithms with SGD and dropout would be beneficial. This would enable readers to weigh the improvements against the potential increase in training and run times.
Several clarifications are needed:
On line 346, the meaning of \theta_s should be specified, particularly whether it refers to the MAP estimate of parameters based on only sample s.
On lines 453-454, the context of \theta in relation to dropout and dropconnect should be clarified.
Finally, there are a few typos that need to be corrected:
On line 211, "output" should be corrected.
On line 738, "RMSProp" should be corrected.