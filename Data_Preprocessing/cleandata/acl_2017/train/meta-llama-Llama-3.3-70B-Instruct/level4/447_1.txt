This paper investigates the application of discourse structure, as outlined by Rhetorical Structure Theory (RST), to enhance text categorization. The authors utilize a Recurrent Neural Network (RNN) with an attention mechanism to generate a text representation. Experimental results across multiple datasets demonstrate the efficacy of the proposed approach. The following points require consideration:
(1) Table 2 reveals that the "UNLABELED" model outperforms the "FULL" model on four out of five datasets, which is counterintuitive since incorporating additional relation labels would be expected to yield benefits. The authors should provide a more detailed explanation for this phenomenon, particularly whether the performance of relation labeling is subpar and ultimately detrimental to overall performance.
(2) The paper involves transforming the RST tree into a dependency structure as a preprocessing step. An alternative approach could be to retain the original tree structure and train a hierarchical model directly on it, potentially yielding more effective results.
(3) To facilitate more comprehensive comparisons with prior work, the authors may consider conducting experiments on a broader range of commonly used datasets, rather than limiting comparisons to a single dataset per previous study.