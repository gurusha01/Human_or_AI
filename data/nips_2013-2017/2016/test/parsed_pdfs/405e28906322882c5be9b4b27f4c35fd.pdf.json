{
  "name" : "405e28906322882c5be9b4b27f4c35fd.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Tracking the Best Expert in Non-stationary Stochastic Environments",
    "authors" : [ "Chen-Yu Wei", "Yi-Te Hong", "Chi-Jen Lu" ],
    "emails" : [ "cjlu}@iis.sinica.edu.tw" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many situations in our daily life require us to make repeated decisions which result in some losses corresponding to our chosen actions. This can be abstracted as the well-known online decision problem in machine learning [5]. Depending on how the loss vectors are generated, two different worlds are usually considered. In the adversarial world, loss vectors are assumed to be deterministic and controlled by an adversary, while in the stochastic world, loss vectors are assumed to be sampled independently from some distributions. In both worlds, good online algorithms are known which can achieve a regret of about √ T over T time steps, where the regret is the difference between the total loss of the online algorithm and that of the best offline one. Another distinction is about the information the online algorithm can receive after each action. In the full-information setting, it gets to know the whole loss vector of that step, while in the bandit setting, only the loss value of the chosen action is received. Again, in both settings, a regret of about √ T turns out to be achievable.\nWhile the regret bounds remain in the same order in those general scenarios discussed above, things become different when some natural conditions are considered. One well-known example is that in the stochastic multi-armed bandit (MAB) problem, when the best arm (or action) is substantially better than the second best, with a constant gap between their means, then a much lower regret, of the order of log T , becomes possible. This motivates us to consider other possible conditions which can have finer characterization of the problem in terms of the achievable regret.\nIn the stochastic world, most previous works focused on the stationary setting, in which the loss (or reward) vectors are assumed to be sampled from the same distribution for all time steps. With this assumption, although one needs to balance between exploration and exploitation in the beginning, after some trials, one can be confident about which action is the best and rest assured that there are no more surprises. On the other hand, the world around us may not be stationary, in which existing learning algorithms for the stationary case may no longer work. In fact, in a non-stationary world, the\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\ndilemma between exploration and exploitation persists as the underlying distribution may drift as time evolves. How does the non-stationarity affect the achievable regret? How does one measure the degree of non-stationarity?\nIn this paper, we answer the above questions through the notion of dynamic regret, which measures the algorithm’s performance against an offline algorithm allowed to select the best arm at every step.\nRelated Works. One way to measure the non-stationarity of a sequence of distributions is to count the number of times the distribution at a time step differs from its previous one. Let Γ − 1 be this number so that the whole time horizon can be partitioned into Γ intervals, with each interval having a stationary distribution. In the bandit setting, a regret of about √ ΓT is achieved by the EXP3.S algorithm in [2], as well as the discounted UCB and sliding-window UCB algorithms in [8]. The dependency on T can be refined in the full-information setting: AdaNormalHedge [10] and Adapt-ML-Prod [7] can both achieve regret in the form of √ ΓC, where C is the total first-order and second-order excess loss respectively, which is upper-bounded by T . From a slightly different Online Mirror Descent approach, [9] can also achieve a regret of about √ ΓD, where D is the sum of differences between consecutive loss vectors.\nAnother measure of non-stationarity, denoted by V , is to compute the difference between the means of consecutive distributions and sum them up. Note that this allows the possibility for the best arm to change frequently, with a very large Γ, while still having similar distributions with a small V . For such a measure V , [3] provided a bandit algorithm which achieves a regret of about V 1/3T 2/3. This regret upper bound is unimprovable in general even in the full-information setting, as a matching lower bound was shown in [4]. Again, [9] refined the upper bound in the full-information setting through the introduction of D, achieving the regret of about 3 √ Ṽ DT , for a parameter Ṽ different but\nrelated to V : Ṽ calculates the sum of differences between consecutive realized loss vectors, while V measures that between mean loss vectors. This makes the results of [3] and [9] incomparable. The problem stems from the fact that [9] considers the traditional adversarial setting, while [3] studies the non-stationary stochastic setting. In this paper, we will provide a framework that bridges these two seemingly disparate worlds.\nOur Results. We base ourselves in the stochastic world with non-stationary distributions, characterized by the parameters Γ and V . In addition, we introduce a new parameter Λ, which measures the total statistical variance of the distributions. Note that traditional adversarial setting corresponds to the case with Λ = 0 and Γ ≈ V ≈ T , while the traditional stochastic setting has Λ ≈ T and Γ = V = 1. Clearly, with a smaller Λ, the learning problem becomes easier, and we would like to understand the tradeoff between Λ and other parameters, including Γ, V , and T . In particular, we would like to know how the bounds described in the related works would be changed. Would all the dependency on T be replaced by Λ, or would only some partial dependency on T be shifted to Λ?\nFirst, we consider the effect of the variance Λ with respect to the parameter Γ. We show that in the full-information setting, a regret of about √ ΓΛ + Γ can be achieved, which is independent of T . On the other hand, we show a sharp contrast that in the bandit setting, the dependency on T is unavoidable, and a lower bound of the order of √ ΓT exists. That is, even when there is no variance in distributions, with Λ = 0, and the distributions only change once, with Γ = 2, any bandit algorithm cannot avoid a regret of about √ T , while a full-information algorithm can achieve a constant regret independent of T .\nNext, we study the tradeoff between Λ and V . We show that in the bandit setting, a regret of about 3 √\nΛV T + √ V T is achievable. Note that this recovers the V 1/3T 2/3 regret bound of [3] as Λ is at most of the order of T , but our bound becomes better when Λ is much smaller than T . Again, one may notice the dependency on T and wonder if this can also be removed in the full-information setting. We show that in the full-information setting, the regret upper bound and lower bound are both about 3 √ ΛV T + V . Our upper bound is incomparable to the 3 √ Ṽ DT bound of [9], since their adversarial setting corresponds to Λ = 0 and their D can be as large as T in our setting. Moreover, we see that while the full-information regret bound is slightly better than that in the bandit setting, there is still an unavoidable T 1/3 dependency.\nOur results provide a big picture of the regret landscape in terms of the parameters Λ,Γ, V , and T , in both full-information and bandit settings. A table summarizing our bounds as well as previous ones is given in Appendix A in the supplementary material. Finally, let us remark that our effort mostly focuses on characterizing the achievable (minimax) regrets, and most of our upper bounds are achieved by algorithms which need the knowledge of the related parameters and may not be practical. To complement this, we also propose a parameter-free algorithm, which still achieve a good regret bound and may have independent interest of its own."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Let us first introduce some notations. For an integer K > 0, let [K] denote the set {1, . . . ,K}. For a vector ` ∈ RK , let `i denote its i’th component. When we need to refer to a time-indexed vector `t ∈ RK , we will write `t,i to denote its i’th component. We will use the indicator function 1C for a condition C, which gives the value 1 if C holds and 0 otherwise. For a vector `, we let ‖`‖b denote its Lb-norm. While standard notation O(·) is used to hide constant factors, we will use the notation Õ(·) to hide logarithmic factors.\nNext, let us describe the problem we study in this paper. Imagine that a learner is given the choice of a total of K actions, and has to play iteratively for a total of T steps. At step t, the learner needs to choose an action at ∈ [K], and then suffers a corresponding loss `t,i ∈ [0, 1], which is independently drawn from a non-stationary distribution with expected loss E[`t,i] = µt,i, which may drift over time. After that, the learner receives some feedback from the environment. In the full-information setting, the feedback gives the whole loss vector `t = (`t,1, ..., `t,K), while in the bandit setting, only the loss `t,at of the chosen action is revealed. A standard way to evaluate the learner’s performance is to measure her (or his) regret, which is the difference between the total loss she suffers and that of an offline algorithm. While most prior works consider offline algorithms which can only play a fixed action for all the steps, we consider stronger offline algorithms which can take different actions in different steps. Our consideration is natural for non-stationary distributions, although this would make the regret large when compared to such stronger offline algorithms. Formally, we measure the learner’s performance by its expected dynamic pseudo-regret, defined as ∑T t=1 E [ `t,at − `t,u∗t ] = ∑T t=1 ( µt,at − µt,u∗t ) , where u∗t = arg mini µt,i is the best action at step t. For convenience, we will simply refer it as the regret of the learner later in our paper.\nWe will consider the following parameters characterizing different aspects of the environments:\nΓ = 1 + T∑ t=2 1µt 6=µt−1 , V = T∑ t=1 ‖µt − µt−1‖∞, and Λ = T∑ t=1 E [ ‖`t − µt‖22 ] , (1)\nwhere we let µ0 be the all-zero vector. Here, Γ− 1 is the number of times the distributions switch, V measures the distance the distributions deviate, and Λ is the total statistical variance of these T distributions. We will call distributions with a small Γ switching distributions, while we will call distributions with a small V drifting distributions and call V the total drift of the distributions.\nFinally, we will need the following large deviation bound, known as empirical Bernstein inequality. Theorem 2.1. [11] Let X = (X1, ..., Xn) be a vector of independent random variables taking values in [0, 1], and let ΛX = ∑ 1≤i<j≤n(Xi −Xj)2/(n(n− 1)). Then for any δ > 0, we have\nPr [∣∣∣∣∣ n∑ i=1 E [Xi]−Xi n ∣∣∣∣∣ > ρ(n,ΛX , δ) ] ≤ δ, for ρ(n,Λ, δ) = √ 2Λ log 2δ n + 7 log 2δ 3(n− 1) ."
    }, {
      "heading" : "3 Algorithms",
      "text" : "We would like to characterize the achievable regret bounds for both switching and drifting distributions, in both full-information and bandit settings. In particular, we would like to understand the interplay among the parameters Γ, V,Λ, and T , defined in (1). The only known upper bound which is good enough for our purpose is that by [8] for switching distributions in the bandit setting, which is close to the lower bound in our Theorem 4.1. In subsection 3.1, we provide a bandit algorithm for drifting distributions which achieves an almost optimal regret upper bound, when given the parameters\nAlgorithm 1 Rerun-UCB-V Initialization: Set B according to (2) and δ = 1/(KT ). for m = 1, . . . , T/B do\nfor t = (m− 1)B + 1, . . . ,mB do Choose arm at := argmini(µ̂t,i − λt,i), with µ̂t,i and λt,i computed according to (3).\nend for end for\nV,Λ, T . In subsection 3.2, we provide a full-information algorithm which works for both switching and drifting distributions. The regret bounds it achieves are also close to optimal, but it again needs the knowledge of the related parameters. To complement this, we provide a full-information algorithm in subsection 3.3, which does not need to know the parameters but achieves slightly larger regret bounds."
    }, {
      "heading" : "3.1 Parameter-Dependent Bandit Algorithm",
      "text" : "In this subsection, we consider drifting distributions parameterized by V and Λ. Our main result is a bandit algorithm which achieves a regret of about 3 √ ΛV T + √ V T . As we aim to achieve smaller regrets for distributions with smaller statistical variances, we adopt a variant of the UCB algorithm developed by [1], called UCB-V, which takes variances into account when building its confidence interval.\nOur algorithm divides the time steps into T/B intervals I1, . . . , IT/B , each having B steps,1 with B = 3 √ K2ΛT/V 2 if KΛ2 ≥ TV and B = √ KT/V otherwise. (2)\nFor each interval, our algorithm clears all the information from previous intervals, and starts a fresh run of UCB-V. More precisely, before step t in an interval I , it maintains for each arm i its empirical mean µ̂t,i, empirical variance Λ̂t,i, and size of confidence interval λt,i, defined as\nµ̂t,i = ∑ s∈St,i `s,i |St,i| , Λ̂t,i = ∑ r,s∈St,i (`r,i − `s,i)2 |St,i|(|St,i| − 1) , and λt,i = ρ(|St,i|, Λ̂t,i, δ), (3)\nwhere St,i denotes the set of steps before t in I that arm i was played, and ρ is the function given in Theorem 2.1. Here we use the convention that µ̂t,i = 0 if |St,i| = 0, while Λ̂t,i = 0 and λt,i = 1 if |St,i| ≤ 1. Then at step t, our algorithm selects the optimistic arm\nat := argmin i\n(µ̂t,i − λt,i),\nreceives the corresponding loss, and updates the statistics.\nOur algorithm is summarized in Algorithm 1, and its regret is guaranteed by the following, which we prove in Appendix B in the supplementary material. Theorem 3.1. The expected regret of Algorithm 1 is at most Õ( 3 √ K2ΛV T + √ KV T )."
    }, {
      "heading" : "3.2 Parameter-Dependent Full-Information Algorithms",
      "text" : "In this subsection, we provide full-information algorithms for switching and drifting distributions. In fact, they are based on an existing algorithm from [6], which is known to work in a different setting: the loss vectors are deterministic and adversarial, and the offline comparator cannot switch arms. In that setting, one of their algorithms, based on gradient-descent (GD), can achieve a regret of O( √ D)\nwhere D = ∑ t ‖`t − `t−1‖22, which is small when the loss vectors have small deviation. Our first observation is that their algorithm in fact can work against a dynamic offline comparator which switches arms less than N times, given any N , with its regret becoming O( √ ND). Our second observation is that when Λ is small, each observed loss vector `t is likely to be close to its true mean 1For simplicity of presentation, let us assume here and later in the paper that taking divisions and roots to produce blocks of time steps all yield integers. It is easy to modify our analysis to the general case without affecting the order of our regret bound.\nAlgorithm 2 Full-information GD-based algorithm Initialization: Let x1 = x̂1 = (1/K, . . . , 1/K)>. for t = 1, 2, . . . , T do\nPlay x̂t = arg minx̂∈X (〈`t−1, x̂〉+ 1ηt ‖x̂− xt‖ 2 2), and then receive loss vector `t. Update xt+1 = arg minx∈X (〈`t, x〉+ 1ηt ‖x− xt‖ 2 2).\nend for\nµt, and when V is small, `t is likely to be close to `t−1. These two observations make possible for us to adopt their algorithm to our setting.\nWe show the first algorithm in Algorithm 2, with the feasible set X being the probability simplex. The idea is to use `t−1 as an estimate for `t to move x̂t further in a possibly beneficial direction. Its regret is guaranteed by the following, which we prove in Appendix C in the supplementary material. Theorem 3.2. For switching distributions parameterized by Γ and Λ, the regret of Algorithm 2 with ηt = η = √ Γ/(Λ +KΓ), is at most O( √ ΓΛ + √ KΓ).\nNote that for switching distributions, the regret of Algorithm 2 does not depend on T , which means that it can achieve a constant regret for constant Γ and Λ. Let us remark that although using a variant based on multiplicative updates could result in a better dependency on K, an additional factor of log T would then emerge when using existing techniques for dealing with dynamic comparators.\nFor drifting distributions, one can show that Algorithm 2 still works and has a good regret bound. However, a slightly better bound can be achieved as we describe next. The idea is to divide the time steps into T/B intervals of size B, with B = 3 √ ΛT/V 2 if ΛT > V 2 and B = 1 otherwise, and re-run Algorithm 2 in each interval with an adaptive learning rate. One way to have an adaptive learning rate can be found in [9], which works well when there is only one interval. A natural way to adopt it here is to reset the learning rate at the start of each interval, but this does not lead to a good enough regret bound as it results in some constant regret at the start of every interval. To avoid this, some careful changes are needed. Specifically, in an interval [t1, t2], we run Algorithm 2 with the learning rate reset as\nηt = 1/ √√√√4 t−1∑ τ=t1 ‖`τ − `τ−1‖22\nfor t > t1, with ηt1 =∞ initially for every interval. This has the benefit of having small or even no regret at the start of an interval when the loss vectors across the boundary have small or no deviation. The regret of this new algorithm is guaranteed by the following, which we prove in Appendix D in the supplementary material. Theorem 3.3. For drifting distributions parameterized by V and Λ, the regret of this new algorithm is at most O( 3 √ V ΛT + √ KV )."
    }, {
      "heading" : "3.3 Parameter-Free Full-Information Algorithm",
      "text" : "The reason that our algorithm for Theorem 3.3 needs the related parameters is to set its learning rate properly. To have a parameter-free algorithm, we would like to adjust the learning rate dynamically in a data-driven way. One way for doing this can be found in [7], which is based on the multiplicative updates variant of the mirror-descent algorithm. It achieves a static regret of about √∑ t r 2 t,k against any expert k, where rt,k = 〈pt, `t〉 − `t,k is its instantaneous regret for playing pt at step t. However, in order to work in our setting, we would like the regret bound to depend on `t − `t−1 as seen previously. This suggests us to modify the Adapt-ML-Prod algorithm of [7] using the idea of [6], which takes `t−1 as an estimate of `t to move pt further in an optimistic direction.\nRecall that the algorithm of [7] maintains a separate learning rate ηt,k for each arm k at time t, and it updates the weight wt,k as well as ηt,k using the instantaneous regret rt,k. To modify the algorithm using the idea of [6], we would like to have an estimate mt,k for rt,k in order to move pt,k further using mt,k and update the learning rate accordingly. More precisely, at step t, we now play pt, with\npt,k = ηt−1,kw̃t−1,k/〈ηt−1, w̃t−1〉 where w̃t−1,k = wt−1,k exp(ηt−1,kmt,k), (4)\nAlgorithm 3 Optimistic-Adapt-ML-Prod Initialization: Let w0,k = 1/K and `0,k = 0 for every k ∈ [K]. for t = 1, 2, . . . , T do\nPlay pt according to (4), and then receive loss vector `t. Update each weight wt,k according to (5) and each learning rate ηt,k according to (6).\nend for\nwhich uses the estimate mt,k to move further from wt−1,k. Then after receiving the loss vector `t, we update each weight\nwt,k = ( wt−1,k exp ( ηt−1,krt,k − η2t−1,k(rt,k −mt,k)2 ))ηt,k/ηt−1,k (5) as well as each learning rate\nηt,k = min\n{ 1/4, √ (lnK)/ ( 1 + ∑ s∈[t] (rs,k −ms,k)2 )} . (6)\nOur algorithm is summarized in Algorithm 3, and we will show that it achieves a regret of about√∑ t(rt,k −mt,k)2 against arm k. It remains to choose an appropriate estimate mt,k. One attempt is to have mt,k = rt−1,k, but rt,k − rt−1,k = (〈pt, `t〉 − `t,k)− (〈pt−1, `t−1〉 − `t−1,k), which does not lead to a desirable bound. The other possibility is to set mt,k = 〈pt, `t−1〉 − `t−1,k, which can be shown to have (rt,k −mt,k)2 ≤ (2‖`t − `t−1‖∞)2. However, it is not clear how to compute such mt,k because it depends on pt,k which in turns depends on mt,k itself. Fortunately, we can approximate it efficiently in the following way.\nNote that the key quantity is 〈pt, `t−1〉. Given its value α, w̃t−1,k and pt,k can be seen as functions of α, defined according to (5) as w̃t−1,k(α) = wt−1,k exp(ηt,k(α − `t−1,k)) and pt,k(α) = ηt−1,kw̃t−1,k(α)/ ∑ i ηt−1,iw̃t−1,i(α). Then we would like to show the existence of α such that 〈pt(α), `t−1〉 = α and to find it efficiently. For this, consider the function f(α) = 〈pt(α), `t−1〉, with pt(α) defined above. It is easy to check that f is a continuous function bounded in [0, 1], which implies the existence of some fixed point α ∈ [0, 1] with f(α) = α. Using a binary search, such an α can be approximated within error 1/T in log T iterations. As such a small error does not affect the order of the regret, we will ignore it for simplicity of presentation, and assume that we indeed have 〈pt, `t−1〉 and hence mt,k = 〈pt, `t−1〉 − `t−1,k without error. Then we have the following regret bound (c.f. [7, Corollary 4]), which we prove in Appendix E in the supplementary material. Theorem 3.4. The static regret of Algorithm 3 w.r.t. any arm (or expert) k ∈ [K] is at most\nÔ (√∑\nt∈[T ] (rt,k −mt,k)2 lnK + lnK\n) ≤ Ô (√∑ t∈[T ] ‖`t − `t−1‖2∞ lnK + lnK ) ,\nwhere the notation Ô(·) hides a ln lnT factor.\nThe regret in the theorem above is measured against a fixed arm. To achieve a dynamic regret against an offline algorithm which can switch arms, one can use a generic reduction to the so-called sleeping experts problem. In particular, we can use the idea in [7] by creating K̃ = KT sleeping experts, and run our Algorithm 3 on these K̃ experts (instead of on the K arms). More precisely, each sleeping expert is indexed by some pair (s, k), and it is asleep for steps before s and becomes awake for steps t ≥ s. At step t, it calls Algorithm 3 for the distribution p̃t over the K̃ experts, and computes its own distribution pt over K arms, with pt,k proportional to ∑t s=1 p̃t,(s,k). Then it plays pt, receives loss vector `t, and feeds some modified loss vector ˜̀t and estimate vector m̃t to Algorithm 3 for update. Here, we set ˜̀t,(s,k) to its expected loss 〈pt, `t〉 if expert (s, k) is asleep and to `t,k otherwise, while we set m̃t,(s,k) to 0 if expert (s, k) is asleep and to mt,k = 〈pt, `t−1〉− `t−1,k otherwise. This choice allows us to relate the regret of Algorithm 3 to that of the new algorithm, which can be seen in the proof of the following theorem, given in Appendix F in the supplementary material. Theorem 3.5. The dynamic expected regret of the new algorithm is Õ( √\nΓΛ lnK + Γ lnK) for switching distributions and Õ( 3 √ V ΛT lnK + √ V T lnK) for drifting distributions."
    }, {
      "heading" : "4 Lower Bounds",
      "text" : "We study regret lower bounds in this section. In subsection 4.1, we show that for switching distributions with Γ− 1 ≥ 1 switches, there is an Ω( √ ΓT ) lower bound for bandit algorithms, even when there is no variance (Λ = 0) and there are constant loss gaps between the optimal and suboptimal arms. We also show a full-information lower bound, which almost matches our upper bound in Theorem 3.2. In subsection 4.2, we show that for drifting distributions, our upper bounds in Theorem 3.1 and Theorem 3.2 are almost tight. In particular, we show that now even for full-information algorithms, a large 3 √ T dependency in the regret turns out to be unavoidable, even for small V and Λ. This provides a sharp contrast to the upper bound of our Theorem 3.2, which shows that a constant regret is in fact achievable by a full-information algorithm for switching distributions with constant Γ and Λ. For simplicity of presentation, we will only discuss the case with K = 2 actions, as it is not hard to extend our proofs to the general case."
    }, {
      "heading" : "4.1 Switching Distributions",
      "text" : "In contrast to the full-information setting, the existence of switches presents a dilemma with lose-lose situation for a bandit algorithm: in order to detect any possible switch early enough, it must explore aggressively, but this has the consequence of playing suboptimal arms too often. To fool any bandit algorithm, we will switch between two deterministic distributions, with no variance, which have mean vectors `(1) = (1/2, 1)> and `(2) = (1/2, 0)>, respectively. Our result is the following. Theorem 4.1. The worst-case expected regret of any bandit algorithm is Ω( √ ΓT ), for Γ ≥ 2.\nProof. Consider any bandit algorithm A, and let us partition the T steps into Γ/2 intervals, each consisting of B = 2T/Γ steps. Our goal is to make A suffer in each interval an expected regret of Ω( √ B) by switching the loss vectors at most once. As mentioned before, we will only switch between two different deterministic distributions with mean vectors: `(1) and `(2). Note that we can see these two distributions simply as two loss vectors, with `(i) having arm i as the optimal arm.\nIn what follows, we focus on one of the intervals, and assume that we have chosen the distributions in all previous intervals. We would like to start the interval with the loss vector `(1). Let N2 denote the expected number of steps A plays the suboptimal arm 2 in this interval if `(1) is used for the whole interval. If N2 ≥ √ B/2, we can actually use `(1) for the whole interval with no switch, which makes\nA suffer an expected regret of at least (1/2) · √ B/2 = √ B/4 in this interval. Thus, it remains to consider the case with N2 < √ B/2. In this case, A does not explore arm 2 often enough, and we let it pay by choosing an appropriate step to switch to the other loss vector `(2) = (1/2, 0)>, which has arm 2 as the optimal one. For this, let us divide the B steps of the interval into √ B blocks, each consisting of √ B steps. As N2 < √ B/2, there must be a block in which the expected number of\nsteps that A plays arm 2 is at most N2/ √ B < 1/2. By a Markov inequality, the probability that A ever plays arm 2 in this block is less than 1/2. This implies that when given the loss vector `(1) for all the steps till the end of this block, A never plays arm 2 in the block with probability more than 1/2. Therefore, if we make the switch to the loss vector `(2) = (1/2, 0)> at the beginning of the block, then A with probability more than 1/2 still never plays arm 2 and never notices the switch in this block. As arm 2 is the optimal one with respect to `(2), the expected regret of A in this block is more than (1/2) · (1/2) · √ B = √ B/4.\nNow if we choose distributions in each interval as described above, then there are at most Γ/2 · 2 = Γ periods of stationary distribution in the whole horizon, and the total expected regret ofA can be made at least Γ/2 · √ B/4 = Γ/2 · √ 2T/Γ/4 = Ω( √ ΓT ), which proves the theorem.\nFor full-information algorithms, we have the following lower bound, which almost matches our upper bound in Theorem 3.2. We provide the proof in Appendix G in the supplementary material. Theorem 4.2. The worst-case expected regret of any full-information algorithm is Ω( √ ΓΛ + Γ)."
    }, {
      "heading" : "4.2 Drifting Distributions",
      "text" : "In this subsection, we show that the regret upper bounds achieved by our bandit algorithm and full-information algorithm are close to optimal by showing almost matching lower bounds. More precisely, we have the following. Theorem 4.3. The worst-case expected regret of any full-information algorithm is Ω( 3 √ ΛV T + V ), while that of any bandit algorithm is Ω( 3 √ ΛV T + √ V T ).\nProof. Let us first consider the full-information case. When ΛT ≤ 32KV 2, we immediately have from Theorem 4.2 the regret lower bound of Ω(Γ) ≥ Ω(V ) ≥ Ω( 3 √ ΛV T + V ). Thus, let us focus on the case with ΛT ≥ 32KV 2. In this case, V ≤ O( 3 √\nΛV T ), so it suffices to prove a lower bound of Ω( 3 √ ΛV T ). Fix any full-information algorithm A, and we will show the existence of a sequence of loss distributions for A to suffer such an expected regret. Following [3], we divide the time steps into T/B intervals of length B, and we set B = 3 √ ΛT/(32KV 2) ≥ 1. For each interval, we will pick some arm i as the optimal one, and give it some loss distribution P , while other arms are sub-optimal and all have some loss distribution Q. We need P and Q to satisfy the following three conditions: (a) P’s mean is smaller thanQ’s by , (b) their variances are at most σ2, and (c) their KL divergence satisfies (ln 2)KL(Q,P) ≤ 2/σ2, for some , σ ∈ (0, 1) to be specified later. Their existence is guaranteed by the following, which we prove in Appendix H in the supplementary material. Lemma 4.4. For any 0 ≤ σ ≤ 1/2 and 0 ≤ ≤ σ/ √\n2, there exist distributions P and Q satisfying the three conditions above.\nLet Di denote the joint distribution of such K distributions, with arm i being the optimal one, and we will use the same Di for all the steps in an interval. We will show that for any interval, there is some i such that using Di this way can make algorithm A suffer a large expected regret in the interval, conditioned on the distributions chosen for previous intervals. Before showing that, note that when we choose distributions in this way, their total variance is at most TKσ2 while their total drift is at most (T/B) . To have them bounded by Λ and V respectively, we choose σ = √ Λ/(4KT ) and\n= V B/T , which satisfy the condition of Lemma 4.4, with our choice of B.\nTo find the distributions, we deal with the intervals one by one. Consider any interval, and assume that the distributions for previous intervals have been chosen. Let Ni denote the number of steps A plays arm i in this interval, and let Ei[Ni] denote its expectation when Di is used for every step of the interval, conditioned on the distributions of previous intervals. One can bound this conditional expectation in terms of a related one, denoted as Eunif[Ni], when every arm has the distribution Q for every step of the interval, again conditioned on the distributions of previous intervals. Specifically, using an almost identical argument to that in [2, proof of Theorem A.2.], one can show that\nEi [Ni] ≤ Eunif [Ni] + B\n2\n√ B(2 ln 2) ·KL(Q,P).2 (7)\nAccording to Lemma 4.4 and our choice of parameters, we have B(2 ln 2) · KL(Q,P) ≤ 2B · ( 2/σ2) ≤ 1/4. Summing both sides of (7) over arm i, and using the fact that ∑ i Eunif [Ni] = B,\nwe get ∑ i Ei [Ni] ≤ B + BK/4, which implies the existence of some i such that Ei [Ni] ≤ B/K +B/4 ≤ (3/4)B. Therefore, if we choose this distribution Di, the conditional expected regret of algorithm A in this interval is at least (B − Ei[Ni]) ≥ B/4. By choosing distributions inductively in this way, we can make A suffer a total expected regret of at least (T/B) · ( B/4) ≥ Ω( 3 √ ΛV T ). This completes the proof for the full-information case.\nNext, let us consider the bandit case. From Theorem 4.1, we immediately have a lower bound of Ω( √ ΓT ) ≥ Ω( √ V T ), which implies the required bound when √ V T ≥ 3 √ ΛV T . When √ V T ≤ 3 √\nΛV T , we have V ≤ Λ2/T which implies that V ≤ 3 √\nΛV T , and we can then use the fullinformation bound of Ω( 3 √ ΛV T ) just proved before. This completes the proof of the theorem.\n2Note that inside the square root, we use B instead of Eunif[Ni] as in [2]. This is because in their bandit setting, Ni is the number of steps when arm i is sampled and has its information revealed to the learner, while in our full-information case, information about arm i is revealed in every step and there are at most B steps."
    } ],
    "references" : [ {
      "title" : "Exploration-exploitation tradeoff using variance estimates in multi-armed bandits",
      "author" : [ "Jean-Yves Audibert", "Rémi Munos", "Csaba Szepesvári" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1876
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Stochastic multi-armed-bandit problem with non-stationary rewards",
      "author" : [ "Omar Besbes", "Yonatan Gur", "Assaf J. Zeevi" ],
      "venue" : "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Non-stationary stochastic optimization",
      "author" : [ "Omar Besbes", "Yonatan Gur", "Assaf J. Zeevi" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Online optimization with gradual variations",
      "author" : [ "Chao-Kai Chiang", "Tianbao Yang", "Chia-Jung Lee", "Mehrdad Mahdavi", "Chi-Jen Lu", "Rong Jin", "Shenghuo Zhu" ],
      "venue" : "In The 25th Conference on Learning Theory (COLT),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "A second-order bound with excess losses",
      "author" : [ "Pierre Gaillard", "Gilles Stoltz", "Tim van Erven" ],
      "venue" : "In The 27th Conference on Learning Theory (COLT),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "On upper-confidence bound policies for switching bandit problems",
      "author" : [ "Aurélien Garivier", "Eric Moulines" ],
      "venue" : "In The 22nd International Conferenc on Algorithmic Learning Theory (ALT),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Online optimization : Competing with dynamic comparators",
      "author" : [ "Ali Jadbabaie", "Alexander Rakhlin", "Shahin Shahrampour", "Karthik Sridharan" ],
      "venue" : "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTAT),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Achieving all with no parameters: Adanormalhedge",
      "author" : [ "Haipeng Luo", "Robert E. Schapire" ],
      "venue" : "In The 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Empirical bernstein bounds and sample-variance penalization",
      "author" : [ "Andreas Maurer", "Massimiliano Pontil" ],
      "venue" : "In The 22nd Conference on Learning Theory (COLT),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "This can be abstracted as the well-known online decision problem in machine learning [5].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "S algorithm in [2], as well as the discounted UCB and sliding-window UCB algorithms in [8].",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "S algorithm in [2], as well as the discounted UCB and sliding-window UCB algorithms in [8].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 9,
      "context" : "The dependency on T can be refined in the full-information setting: AdaNormalHedge [10] and Adapt-ML-Prod [7] can both achieve regret in the form of √ ΓC, where C is the total first-order and second-order excess loss respectively, which is upper-bounded by T .",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "The dependency on T can be refined in the full-information setting: AdaNormalHedge [10] and Adapt-ML-Prod [7] can both achieve regret in the form of √ ΓC, where C is the total first-order and second-order excess loss respectively, which is upper-bounded by T .",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 8,
      "context" : "From a slightly different Online Mirror Descent approach, [9] can also achieve a regret of about √ ΓD, where D is the sum of differences between consecutive loss vectors.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "For such a measure V , [3] provided a bandit algorithm which achieves a regret of about V T .",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "This regret upper bound is unimprovable in general even in the full-information setting, as a matching lower bound was shown in [4].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "Again, [9] refined the upper bound in the full-information setting through the introduction of D, achieving the regret of about 3 √ Ṽ DT , for a parameter Ṽ different but related to V : Ṽ calculates the sum of differences between consecutive realized loss vectors, while V measures that between mean loss vectors.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : "This makes the results of [3] and [9] incomparable.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 8,
      "context" : "This makes the results of [3] and [9] incomparable.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "The problem stems from the fact that [9] considers the traditional adversarial setting, while [3] studies the non-stationary stochastic setting.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "The problem stems from the fact that [9] considers the traditional adversarial setting, while [3] studies the non-stationary stochastic setting.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 2,
      "context" : "Note that this recovers the V T 2/3 regret bound of [3] as Λ is at most of the order of T , but our bound becomes better when Λ is much smaller than T .",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "Our upper bound is incomparable to the 3 √ Ṽ DT bound of [9], since their adversarial setting corresponds to Λ = 0 and their D can be as large as T in our setting.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "The only known upper bound which is good enough for our purpose is that by [8] for switching distributions in the bandit setting, which is close to the lower bound in our Theorem 4.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "As we aim to achieve smaller regrets for distributions with smaller statistical variances, we adopt a variant of the UCB algorithm developed by [1], called UCB-V, which takes variances into account when building its confidence interval.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "In fact, they are based on an existing algorithm from [6], which is known to work in a different setting: the loss vectors are deterministic and adversarial, and the offline comparator cannot switch arms.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "One way to have an adaptive learning rate can be found in [9], which works well when there is only one interval.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "One way for doing this can be found in [7], which is based on the multiplicative updates variant of the mirror-descent algorithm.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "This suggests us to modify the Adapt-ML-Prod algorithm of [7] using the idea of [6], which takes `t−1 as an estimate of `t to move pt further in an optimistic direction.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "This suggests us to modify the Adapt-ML-Prod algorithm of [7] using the idea of [6], which takes `t−1 as an estimate of `t to move pt further in an optimistic direction.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "Recall that the algorithm of [7] maintains a separate learning rate ηt,k for each arm k at time t, and it updates the weight wt,k as well as ηt,k using the instantaneous regret rt,k.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "To modify the algorithm using the idea of [6], we would like to have an estimate mt,k for rt,k in order to move pt,k further using mt,k and update the learning rate accordingly.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "In particular, we can use the idea in [7] by creating K̃ = KT sleeping experts, and run our Algorithm 3 on these K̃ experts (instead of on the K arms).",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "Following [3], we divide the time steps into T/B intervals of length B, and we set B = 3 √ ΛT/(32KV 2) ≥ 1.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "Note that inside the square root, we use B instead of Eunif[Ni] as in [2].",
      "startOffset" : 70,
      "endOffset" : 73
    } ],
    "year" : 2016,
    "abstractText" : "We study the dynamic regret of multi-armed bandit and experts problem in nonstationary stochastic environments. We introduce a new parameter Λ, which measures the total statistical variance of the loss distributions over T rounds of the process, and study how this amount affects the regret. We investigate the interaction between Λ and Γ, which counts the number of times the distributions change, as well as Λ and V , which measures how far the distributions deviates over time. One striking result we find is that even when Γ, V , and Λ are all restricted to constant, the regret lower bound in the bandit setting still grows with T . The other highlight is that in the full-information setting, a constant regret becomes achievable with constant Γ and Λ, as it can be made independent of T , while with constant V and Λ, the regret still has a T 1/3 dependency. We not only propose algorithms with upper bound guarantee, but prove their matching lower bounds as well.",
    "creator" : null
  }
}