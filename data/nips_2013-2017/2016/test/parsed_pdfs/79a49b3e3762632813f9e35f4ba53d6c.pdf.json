{
  "name" : "79a49b3e3762632813f9e35f4ba53d6c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Estimating the class prior and posterior from noisy positives and unlabeled data",
    "authors" : [ "Shantanu Jain", "Martha White", "Predrag Radivojac" ],
    "emails" : [ "predrag}@indiana.edu" ],
    "sections" : null,
    "references" : [ {
      "title" : "High breakdown mixture discriminant analysis",
      "author" : [ "S. Bashir", "E.M. Carter" ],
      "venue" : "J Multivar Anal,",
      "citeRegEx" : "Bashir and Carter.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bashir and Carter.",
      "year" : 2005
    }, {
      "title" : "Semi-supervised novelty detection",
      "author" : [ "G. Blanchard", "G. Lee", "C. Scott" ],
      "venue" : "J Mach Learn Res,",
      "citeRegEx" : "Blanchard et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Blanchard et al\\.",
      "year" : 2010
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T. Mitchell" ],
      "venue" : "COLT",
      "citeRegEx" : "Blum and Mitchell.,? \\Q1998\\E",
      "shortCiteRegEx" : "Blum and Mitchell.",
      "year" : 1998
    }, {
      "title" : "Robust supervised classification with mixture models: learning from data with uncertain labels",
      "author" : [ "C. Bouveyron", "S. Girard" ],
      "venue" : "Pattern Recognit,",
      "citeRegEx" : "Bouveyron and Girard.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bouveyron and Girard.",
      "year" : 2009
    }, {
      "title" : "Sample selection bias correction theory",
      "author" : [ "C. Cortes", "M. Mohri", "M. Riley", "A. Rostamizadeh" ],
      "venue" : "ALT",
      "citeRegEx" : "Cortes et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning from positive and unlabeled examples",
      "author" : [ "F. Denis", "R. Gilleron", "F. Letouzey" ],
      "venue" : "Theor Comput Sci,",
      "citeRegEx" : "Denis et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Denis et al\\.",
      "year" : 2005
    }, {
      "title" : "Class prior estimation from positive and unlabeled data",
      "author" : [ "M.C. du Plessis", "M. Sugiyama" ],
      "venue" : "IEICE Trans Inf & Syst,",
      "citeRegEx" : "Plessis and Sugiyama.,? \\Q2014\\E",
      "shortCiteRegEx" : "Plessis and Sugiyama.",
      "year" : 2014
    }, {
      "title" : "Learning classifiers from only positive and unlabeled data",
      "author" : [ "C. Elkan", "K. Noto" ],
      "venue" : "KDD",
      "citeRegEx" : "Elkan and Noto.,? \\Q2008\\E",
      "shortCiteRegEx" : "Elkan and Noto.",
      "year" : 2008
    }, {
      "title" : "High-breakdown linear discriminant analysis",
      "author" : [ "D.M. Hawkins", "G.J. McLachlan" ],
      "venue" : "J Am Stat Assoc,",
      "citeRegEx" : "Hawkins and McLachlan.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hawkins and McLachlan.",
      "year" : 1997
    }, {
      "title" : "Nonparametric semi-supervised learning of class proportions",
      "author" : [ "S. Jain", "M. White", "M.W. Trosset", "P. Radivojac" ],
      "venue" : "arXiv preprint arXiv:1601.01944,",
      "citeRegEx" : "Jain et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2016
    }, {
      "title" : "A mutual contamination analysis of mixed membership and partial label models",
      "author" : [ "J. Katz-Samuels", "C. Scott" ],
      "venue" : "arXiv preprint arXiv:1602.06235,",
      "citeRegEx" : "Katz.Samuels and Scott.,? \\Q2016\\E",
      "shortCiteRegEx" : "Katz.Samuels and Scott.",
      "year" : 2016
    }, {
      "title" : "Estimating a kernel Fisher discriminant in the presence of label noise",
      "author" : [ "N.D. Lawrence", "B. Scholkopf" ],
      "venue" : null,
      "citeRegEx" : "Lawrence and Scholkopf.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lawrence and Scholkopf.",
      "year" : 2001
    }, {
      "title" : "Sparse nonparametric density estimation in high dimensions using the rodeo",
      "author" : [ "H. Liu", "J.D. Lafferty", "L.A. Wasserman" ],
      "venue" : "AISTATS",
      "citeRegEx" : "Liu et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2007
    }, {
      "title" : "Random classification noise defeats all convex potential boosters",
      "author" : [ "P.M. Long", "R.A. Servedio" ],
      "venue" : "Mach Learn,",
      "citeRegEx" : "Long and Servedio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Long and Servedio.",
      "year" : 2010
    }, {
      "title" : "Noise tolerance under risk minimization",
      "author" : [ "N. Manwani", "P.S. Sastry" ],
      "venue" : "IEEE T Cybern,",
      "citeRegEx" : "Manwani and Sastry.,? \\Q2013\\E",
      "shortCiteRegEx" : "Manwani and Sastry.",
      "year" : 2013
    }, {
      "title" : "Learning from corrupted binary labels via class-probability estimation",
      "author" : [ "A.K. Menon", "B. van Rooyen", "C.S. Ong", "R.C. Williamson" ],
      "venue" : null,
      "citeRegEx" : "Menon et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Menon et al\\.",
      "year" : 2015
    }, {
      "title" : "Obtaining calibrated probabilities from boosting",
      "author" : [ "A. Niculescu-Mizil", "R. Caruana" ],
      "venue" : "UAI",
      "citeRegEx" : "Niculescu.Mizil and Caruana.,? \\Q2005\\E",
      "shortCiteRegEx" : "Niculescu.Mizil and Caruana.",
      "year" : 2005
    }, {
      "title" : "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods, pages 61–74",
      "author" : [ "J.C. Platt" ],
      "venue" : null,
      "citeRegEx" : "Platt.,? \\Q1999\\E",
      "shortCiteRegEx" : "Platt.",
      "year" : 1999
    }, {
      "title" : "Mixture proportion estimation via kernel embedding of distributions",
      "author" : [ "H.G. Ramaswamy", "C. Scott", "A. Tewari" ],
      "venue" : "arXiv preprint arXiv:1603.02501,",
      "citeRegEx" : "Ramaswamy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ramaswamy et al\\.",
      "year" : 2016
    }, {
      "title" : "Composite binary losses",
      "author" : [ "M.D. Reid", "R.C. Williamson" ],
      "venue" : "J Mach Learn Res,",
      "citeRegEx" : "Reid and Williamson.,? \\Q2010\\E",
      "shortCiteRegEx" : "Reid and Williamson.",
      "year" : 2010
    }, {
      "title" : "Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure",
      "author" : [ "M. Saerens", "P. Latinne", "C. Decaestecker" ],
      "venue" : "Neural Comput,",
      "citeRegEx" : "Saerens et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Saerens et al\\.",
      "year" : 2002
    }, {
      "title" : "Class proportion estimation with application to multiclass anomaly rejection",
      "author" : [ "T. Sanderson", "C. Scott" ],
      "venue" : "AISTATS",
      "citeRegEx" : "Sanderson and Scott.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sanderson and Scott.",
      "year" : 2014
    }, {
      "title" : "Classification with asymmetric label noise: consistency and maximal denoising",
      "author" : [ "C. Scott", "G. Blanchard", "G. Handy" ],
      "venue" : "J Mach Learn Res W&CP,",
      "citeRegEx" : "Scott et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Scott et al\\.",
      "year" : 2013
    }, {
      "title" : "The curse of dimensionality and dimension reduction. Multivariate Density Estimation: Theory, Practice, and Visualization, pages",
      "author" : [ "D.W. Scott" ],
      "venue" : null,
      "citeRegEx" : "Scott.,? \\Q2008\\E",
      "shortCiteRegEx" : "Scott.",
      "year" : 2008
    }, {
      "title" : "The ABC’s (and XYZ’s) of peptide sequencing",
      "author" : [ "H. Steen", "M. Mann" ],
      "venue" : "Nat Rev Mol Cell Biol,",
      "citeRegEx" : "Steen and Mann.,? \\Q2004\\E",
      "shortCiteRegEx" : "Steen and Mann.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "In many domains, however, a sample from one of the classes (say, negatives) may not be available, leading to the setting of learning from positive and unlabeled data (Denis et al., 2005).",
      "startOffset" : 166,
      "endOffset" : 186
    }, {
      "referenceID" : 19,
      "context" : "unlabeled (non-traditional) training is reasonable because the class posterior — and also the optimum scoring function for composite losses (Reid and Williamson, 2010) — in the traditional setting is monotonically related to the posterior in the non-traditional setting.",
      "startOffset" : 140,
      "endOffset" : 167
    }, {
      "referenceID" : 15,
      "context" : "The knowledge of the class prior is also necessary for estimation of the performance criteria such as the error rate, balanced error rate or F-measure, and also for finding the right threshold for the non-traditional scoring function that leads to an optimal classifier with respect to some criteria (Menon et al., 2015).",
      "startOffset" : 300,
      "endOffset" : 320
    }, {
      "referenceID" : 1,
      "context" : "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.",
      "startOffset" : 146,
      "endOffset" : 228
    }, {
      "referenceID" : 22,
      "context" : "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.",
      "startOffset" : 146,
      "endOffset" : 228
    }, {
      "referenceID" : 9,
      "context" : "Class prior estimation in a nonparametric setting has been actively researched in the past decade offering an extensive theory of identifiability (Ward et al., 2009; Blanchard et al., 2010; Scott et al., 2013; Jain et al., 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al.",
      "startOffset" : 146,
      "endOffset" : 228
    }, {
      "referenceID" : 7,
      "context" : ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Sanderson and Scott, 2014; Jain et al., 2016; Ramaswamy et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 180
    }, {
      "referenceID" : 21,
      "context" : ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Sanderson and Scott, 2014; Jain et al., 2016; Ramaswamy et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 180
    }, {
      "referenceID" : 9,
      "context" : ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Sanderson and Scott, 2014; Jain et al., 2016; Ramaswamy et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 180
    }, {
      "referenceID" : 18,
      "context" : ", 2016) and a few practical solutions (Elkan and Noto, 2008; Ward et al., 2009; du Plessis and Sugiyama, 2014; Sanderson and Scott, 2014; Jain et al., 2016; Ramaswamy et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "For example, in the process of peptide identification (Steen and Mann, 2004), bioinformatics methods are usually set to report results with specified false discovery rate thresholds (e.",
      "startOffset" : 54,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "Further, the only approach that does consider similar such noise (Scott et al., 2013) requires density estimation, which is known to be problematic for high-dimensional data.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "Much of the identifiability characterization in this section has already been considered as the case of asymmetric noise (Scott et al., 2013); see Section 7 on related work.",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 22,
      "context" : "To ensure identifiability, it is necessary to choose a canonical form that prefers a class prior that makes the two components as different as possible; this canonical form was introduced as the mutual irreducibility condition (Scott et al., 2013) and is related to the proper novelty distribution (Blanchard et al.",
      "startOffset" : 227,
      "endOffset" : 247
    }, {
      "referenceID" : 1,
      "context" : ", 2013) and is related to the proper novelty distribution (Blanchard et al., 2010) and the max-canonical form (Jain et al.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : ", 2010) and the max-canonical form (Jain et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "Nonparametric (kernel) density estimation is also known to have curse-of-dimensionality issues, both in theory (Liu et al., 2007) and in practice (Scott, 2008).",
      "startOffset" : 111,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "The transform is similar to that in (Jain et al., 2016), except that it is not required to be calibrated like a posterior distribution; as shown below, a good ranking function is sufficient.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "p can be estimated directly by using a probabilistic classifier or by calibrating a classifier’s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of ↵⇤ and β⇤.",
      "startOffset" : 103,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "p can be estimated directly by using a probabilistic classifier or by calibrating a classifier’s score (Platt, 1999; Niculescu-Mizil and Caruana, 2005); |U |/|L| serves as an estimate of p(S=0)/p(S=1); section 5 gives parametric and nonparametric approaches for estimation of ↵⇤ and β⇤.",
      "startOffset" : 103,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "For this purpose, we use the AlphaMax algorithm (Jain et al., 2016), briefly summarized in the Appendix.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al.",
      "startOffset" : 87,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "Algorithms: We compare the AlphaMax-N and MSGMM algorithms to the Elkan-Noto algorithm (Elkan and Noto, 2008) as well as the noiseless version of AlphaMax (Jain et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "The aim under this model is to estimate multiple unknown base distributions, using multiple random samples that are composed of different convex combinations of those base distributions (Katz-Samuels and Scott, 2016).",
      "startOffset" : 186,
      "endOffset" : 216
    }, {
      "referenceID" : 2,
      "context" : "(2013), and previously investigated under a more restrictive setting as co-training (Blum and Mitchell, 1998).",
      "startOffset" : 84,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.",
      "startOffset" : 165,
      "endOffset" : 216
    }, {
      "referenceID" : 14,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.",
      "startOffset" : 165,
      "endOffset" : 216
    }, {
      "referenceID" : 8,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.",
      "startOffset" : 233,
      "endOffset" : 287
    }, {
      "referenceID" : 0,
      "context" : "A natural approach is to use robust estimation to learn in the presence of class noise; this strategy, however, has been shown to be ineffective, both theoretically (Long and Servedio, 2010; Manwani and Sastry, 2013) and empirically (Hawkins and McLachlan, 1997; Bashir and Carter, 2005), indicating the need to explicitly model the noise.",
      "startOffset" : 233,
      "endOffset" : 287
    }, {
      "referenceID" : 11,
      "context" : "Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class.",
      "startOffset" : 95,
      "endOffset" : 153
    }, {
      "referenceID" : 3,
      "context" : "Generative mixture model approaches have also been developed, which explicitly model the noise (Lawrence and Scholkopf, 2001; Bouveyron and Girard, 2009); these algorithms, however, assume labeled data for each class.",
      "startOffset" : 95,
      "endOffset" : 153
    } ],
    "year" : 2016,
    "abstractText" : "We develop a classification algorithm for estimating posterior distributions from positive-unlabeled data, that is robust to noise in the positive labels and effective for high-dimensional data. In recent years, several algorithms have been proposed to learn from positive-unlabeled data; however, many of these contributions remain theoretical, performing poorly on real high-dimensional data that is typically contaminated with noise. We build on this previous work to develop two practical classification algorithms that explicitly model the noise in the positive labels and utilize univariate transforms built on discriminative classifiers. We prove that these univariate transforms preserve the class prior, enabling estimation in the univariate space and avoiding kernel density estimation for high-dimensional data. The theoretical development and parametric and nonparametric algorithms proposed here constitute an important step towards wide-spread use of robust classification algorithms for positive-unlabeled data.",
    "creator" : null
  }
}