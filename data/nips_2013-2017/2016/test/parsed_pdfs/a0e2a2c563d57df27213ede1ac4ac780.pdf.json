{
  "name" : "a0e2a2c563d57df27213ede1ac4ac780.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learned Region Sparsity and Diversity Also Predicts Visual Attention",
    "authors" : [ "Zijun Wei", "Hossein Adeli", "Gregory Zelinsky", "Minh Hoai", "Dimitris Samaras" ],
    "emails" : [ "samaras}@cs.stonybrook.edu", "gregory.zelinsky}@stonybrook.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Learned region sparsity has achieved state-of-the-art performance in classification tasks by exploiting and integrating a sparse set of local information into global decisions. The underlying mechanism resembles how people sample information from an image with their eye movements when making similar decisions. In this paper we incorporate the biologically plausible mechanism of Inhibition of Return into the learned region sparsity model, thereby imposing diversity on the selected regions. We investigate how these mechanisms of sparsity and diversity relate to visual attention by testing our model on three different types of visual search tasks. We report state-of-the-art results in predicting the locations of human gaze fixations, even though our model is trained only on image-level labels without object location annotations. Notably, the classification performance of the extended model remains the same as the original. This work suggests a new computational perspective on visual attention mechanisms, and shows how the inclusion of attention-based mechanisms can improve computer vision techniques."
    }, {
      "heading" : "1 Introduction",
      "text" : "Visual spatial attention refers to the narrowing of processing in the brain to particular objects in particular locations so as to mediate everyday tasks. A widely used paradigm for studying visual spatial attention is visual search, where a desired object must be located and recognized in a typically cluttered environment. Visual search is accompanied by observable estimates—in the form of gaze fixations—of how attention samples information from a scene while searching for a target. Efficient visual search requires prioritizing the locations of features of the target object class over features at locations offering less evidence for the target [31]. Computational models of visual search typically estimate and plot goal directed prioritization of visual space as priority maps for directing attention [32]. This form of target directed prioritization is different from the saliency modeling literature, where bottom-up feature contrast in an image is used to predict fixation behavior during the free-viewing of scenes [16].\nThe field of fixation prediction is highly active and growing [2], although it was not until fairly recently that attention researchers have begun using the sophisticated object detection techniques developed in the computer vision literature [8, 18, 31]. The dominant method used in the visual search literature to generate priority maps for detection has been the exhaustive detection mechanism [8, 18]. Using this method, an object detector is applied to an image to provide bounding boxes that are then combined, weighted by their detection scores, to generate a priority map [8]. While these models have had success in predicting behavior, training these detectors requires human labeled bounding boxes, which are expensive and laborious to collect, and also prone to individual annotator differences.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nAn alternative approach to modeling visual attention is to determine how model and behavioral task performance depends on shared core computational principles [24]. To this end, a new class of attention-inspired models have been developed and applied to tasks ranging from image captioning [30] to hand writing generation [13], where selective spatial attention mechanisms have been shown to emerge [1, 25]. By requiring visual inputs to be gated in a manner similar to the human gating of visual inputs via fixations, these models are able to localize or “attend” selectively to the most informative regions of an input image while ignoring irrelevant visual inputs [25, 1]. This built in attention mechanism enables the model of [30], trained only on generating captions, to bias the visual input so as to gate only relevant information when generating each word to describe an image. Priority maps were then generated to show the mapping of attended image areas to generated words. While these new models show attention-like behavior, to our knowledge none have been used to predict actual human allocations of attention.\nThe current work bridges the behavioral and computer vision literatures by using a classification model that has biologically plausible constraints to create a priority map for the purpose of predicting the allocation of spatial attention as measured by changes in fixation. The specific image-category classification model that we use is called Region Ranking SVM (RRSVM) [29]. This model was developed in our recent work [29], and it achieved state-of-the-art performance on a number of classification tasks by learning categorization with locally-pooled information from input images. This model works by imposing sparsity on selected image areas that contribute to the classification decision, much like how humans prioritize visual space and sample with fixations only a sparse set of image locations while attempting to detect and recognize object categories [4]. We believe that this analogy between sparse sampling and attention makes this model a natural candidate for predicting attention behavior in visual search tasks. It is worth noting that this model was originally created for object classification and not localization, hence no object localization data is used to train it, unlike standard fixation prediction algorithms [16, 17].\nThere are two contributions of our work. First, we show that the RSSVM model approaches stateof-the-art in predicting the fixations made by humans searching for the same targets in the same images. This means that a model trained solely for the purpose of image classification, without any localization data, is also able to predict the locations of fixations that people make while searching for the to-be-classified objects. Second, we incorporate the biologically plausible constraint of Inhibition of Return [10], which we model by requiring a set of diverse (minimally overlapping) sparse regions in RRSVM. Incorporating this constraint, we are able to reduce the error in fixation prediction (up to 21%). Importantly, adding the Inhibition of Return constraint does not affect the classification performance. By building this bridge, we hope to show how automated object detection might be improved by the inclusion of an attention mechanism, and how a recent attention-inspired approach from computer vision might illuminate how the brain prioritizes visual information for the efficient direction of spatial attention."
    }, {
      "heading" : "2 Region Ranking SVM",
      "text" : "Here we review Region Ranking SVM (RRSVM) [29]. The main problem addressed by RRSVM is image classification, which aims to recognize the semantic category of an image, such as whether the image contains a certain object (e.g., car, cat) or portrays a certain action (e.g., jumping, typing). RRSVM evaluates multiple local regions of an image, and subsequently outputs the classification decision based on a sparse set of regions. This mechanism is noteworthy and different from other approaches that aggregate information from multiple regions indistinguishably (e.g., [23, 28, 22, 14]).\nRRSVM assumes training data consisting of images {Bi}ni=1 and associated binary labels {yi}ni=1 indicating the presence or absence of the visual element (object or action) of interest. To account for the uncertainty of each semantic region in an image, RRSVM considers multiple local regions. The number of regions can differ between images, but for brevity, assume each image has the same number of regions. Let m be the number of regions for each image, and d the dimension of each region descriptor. RRSVM represents each image as a matrix Bi ∈ <d×m, but the order of the columns can be arbitrary. RRSVM jointly learns a region evaluation function and a region selection function by minimizing: λ||w||2+ ∑n i=1(w\nT Γ(Bi;w)s+b−yi)2 subject to the constraints: s1 ≥ s2 ≥ · · · ≥ sm ≥ 0 and h(Γ(Bi;w)s) ≤ 1. Here h(·) is the function that measures the spread of the column vectors of a matrix: h([x1, · · · ,xn]) = ∑n i=1 ∣∣∣∣xi − 1n ∑ni=1 xi∣∣∣∣2 . w and b are the weight vector and the bias term of an SVM classifier, which are the parameters of the region\nevaluation function. Γ(B;w) denotes a matrix that can be obtained by rearranging the columns of the matrix B so that wT Γ(B;w) is a sequence of non-increasing values. The vector s is the weight vector for combining the SVM region scores for each image [15]; this vector is common to all images of a class.\nThe objective of the above formulation consists of the regularization term λ||w||2 and the sum of squared losses. This objective is based purely on classification performance. However, note that the classification decision is based on both the region evaluation function (i.e., w, b) and the region selection function (i.e., s), which are simultaneously learned using the above formulation. What is interesting is that the obtained s vector is always sparse. An experiment [29] on the ImageNet dataset [27] with 1000 classes showed that RRSVM generally uses 20 regions or less (from hundreds of local regions considered). This intriguing fact prompted us to consider the connection between sparse region selection and visual attention. Would machine-based discriminative localization reflect the allocation of human attention in visual search? It turns out that there is compelling evidence for a relationship, as will be shown in the experiment section. This relationship can be strengthened if RRSVM is extended to incorporate Inhibition of Return in the region selection process, which will be explained next."
    }, {
      "heading" : "3 Incorporating Inhibition of Return into Region Ranking SVM",
      "text" : "A mechanism critical to the modeling of human visual search behavior is Inhibition of Return: the lower probability of re-fixating on or near already attended areas, possibly mediated by lateral inhibition [16, 20]. This mechanism, however, is not currently enforced in the formulation of RRSVM, and indeed the spatial relationship between selected regions is not considered. RRSVM usually selects a sparse set of regions, but the selected regions are free to overlap and concentrate on a single image area.\nInspired by Inhibition of Return, we consider an extension of RRSVM where non-maxima suppression is incorporated into the process of selecting regions. This mechanism will select the local maximum for nearby activation areas (a potential fixation location) and discard the rest (non-maxima nearby locations). The biological plausibility of non-maxima suppression has been discussed in previous work, where it was shown to be a plausible method for allowing the stronger activations to stand out (see [21, 7] for details).\nTo incorporate non-maxima suppression in the framework of RRSVM, we replaced the region ranking procedure Γ(B;w) of RRSVM by Ψ(Bi;w, α), a procedure that ranks and subsequently returns the list of regions that do not significantly overlap with one another. In particular, we use intersection over union to measure overlap, where α is a threshold for tolerable overlap (we set α = 0.5 in our experiments). This leads to the following optimization problem:\nminimize w,s,b\nλ||w||2 + n∑\ni=1\n(wT Ψ(Bi;w, α)s + b− yi)2 (1)\ns.t. s1 ≥ s2 ≥ · · · ≥ sm ≥ 0, (2) h(Ψ(Bi;w, α)s) ≤ 1. (3)\nThe above formulation can be optimized in the same way as RRSVM in [29]. It will yield a classifier that makes a decision based on a sparse and diverse set of regions. Sparsity is inherited from RRSVM, and location diversity is attained using non-maxima suppression. Hereafter, we refer to this method as Sparse Diverse Regions (SDR) classifier."
    }, {
      "heading" : "4 Experiments and Analysis",
      "text" : "We present here empirical evidence showing that learned region sparsity and diversity can also predict visual attention. We first describe the implementation details of RRSVM and SDR. We then consider attention prediction under three conditions: (1) single-target present, that is to find the one instance of a target category appearing in a stimulus image; (2) target absent, i.e., searching for a target category that does not appear in the image; and (3) multiple-targets present, i.e., searching for multiple object categories where at least one is present in the image. Experiments are performed on three datasets POET [26], PET [11] and MIT900 [8], which are the only available datasets for object search tasks."
    }, {
      "heading" : "4.1 Implementation details of RRSVM and SDR",
      "text" : "Our implementation of RRSVM and SDR is similar to [29], but we consider more local regions. This yields a finer localization map without changing the classification performance. As in [29], the feature extraction pipeline is based on VGG16 [28]. The last fully connected layer of VGG16 is removed and the remaining fully connected layer is converted to a fully convolutional layer. To compute feature vectors for multiple regions of an image, the image is resized and then fed into VGG16 to yield a feature map with 4096 channels. The size of the feature map depends on the size of the resized image, and each feature map corresponds to a subwindow of the original image. By resizing the original image to multiple sizes, one can compute feature vectors for multiple regions of the original image. In this work, we consider 7 different image sizes instead of the three sizes used by [28, 29]. The first three resized images are obtained by scaling the image isotropically so that the smallest dimension is 256, 384, or 512. For brevity, assuming the width is smaller than the height, this yields three images with dimensions 256 × a, 384 × b, and 512 × c. We consider four other resized images with dimensions 256× b, 384× c, 384× a, 512× b. These image sizes correspond to local regions having an aspect ratio of either 2:3 or 3:2, while the isotropically resized images yield square local regions. Additionally, we also consider horizontal flips of the resized images. Overall, this process yields 700 to 1000 feature vectors, each corresponding to a local image region.\nThe RRSVM and SDR classifiers used in the following experiments are trained on the trainval set of PASCAL VOC 2007 dataset [9] unless otherwise stated. This dataset is distinct from the datasets used for evaluation. For SDR, the non-maxima suppression threshold is 0.5, and we only keep the top ranked regions that have non-zero region scores (si ≥ 0.01). To generate a priority map, we first associate each pixel with an integer indicating the total number of selected regions covering that pixel, then apply a Gaussian blur kernel to the integer valued map, with the kernel width tuned on the validation set.\nTo test whether learned region sparsity and diversity predicts human attention, we compare the generated priority maps with the behaviorally-derived fixation density maps. To make this comparison we use the Area Under the ROC Curve (AUC), a commonly used metric for visual search task evaluation [6]. We use the publicly available implementation of the AUC evaluation from the MIT saliency benchmark [5], specifically the AUC-Judd implementation for its better approximation."
    }, {
      "heading" : "4.2 Single-target present condition",
      "text" : "We consider visual attention in the single-target present condition using the POET dataset [26]. This dataset is a subset of PASCAL VOC 2012 dataset [9], and it has 6270 images from 10 object categories (aeroplane, boat, bike, motorbike, cat, dog, horse, cow, sofa and dining table). The task was two-alternative forced choice for object categories, approximating visual search, and eye movement data were collected from 5 subjects as they freely viewed these images. On average, 5.7 fixations were made per image. The SDR classifier is trained on the trainval set of PASCAL VOC 2007 dataset, which does not overlap with the POET dataset. We randomly selected one third of the images for each category to compile a validation set for tuning the width of the Gaussian blur kernel for all categories. The rest were used as test images.\nFor each test image, we compare the priority map generated for the selected regions by RRSVM with the human fixation density map. The overall correlation is high, yielding a mean AUC score of 0.81 (on all images of 10 object classes). This is intriguing because RRSVM is optimized for classification performance only; joint classification is apparently related to discriminative localization by human attention in the context of a visual search task. By incorporating Inhibition of Return into RRSVM, we observe even stronger correlation with human behavior, with the mean AUC score obtained by SDR now being 0.85.\nThe left part of Table 1 shows AUC scores for individual categories of the POET dataset. We compare the performance of other attention prediction baselines. All recent fixation prediction models [8, 19, 31] apply object category detectors on the input image and combine the detection results to create priority maps. Unfortunately, direct comparison to these models is not currently possible due to the unavailability of needed code and datasets. However, our RCNN [12] baseline, which is the state-of-the-art object detector on this dataset, should improve the pipelines of these models. To account for possible localization errors and multiple object instances, we keep all the detections with a detection score greater than a threshold. This threshold is chosen to maximize the\ndetector’s F1 score, which is the harmonic mean between precision and recall. We also consider a variant method where only the top detection is kept, but the result is not as good. We also consider the recently proposed weakly-supervised object localization approach of [34], which is denoted as CAM in Table 1. We use the released model to extract features and train a linear SVM on top of the features. For each test image, we weigh a linear sum of local activations to create an activation map. We normalize the activation map to get the priority map. We even compare SDR with a method that directly uses the annotated object bounding boxes to predict human attention, which is denoted as AnnoBoxes in the table. For this method, the priority map is created by applying a Gaussian filter to a binary map where the center of the bounding box over the target(s) is set to 1 and everywhere else 0. Notably, the methods selected for comparison are strong models for predicting human attention. RCNN has an unfair advantage over SDR because it has access to localized annotations in its training data, and AnnoBoxes even assumes the availability of object bounding boxes for test data. As can be seen from Table 1, SDR significantly outperforms the other methods. This provides strong empirical evidence suggesting that learned region sparsity and diversity is highly predictive human attention. Fig. 1 shows some randomly selected results from SDR on test images.\nNote that the incorporation of Inhibition of Return into RRSVM and the consideration of more local regions does not affect the classification performance. When evaluated on the PASCAL VOC 2007 test set, the RRSVM method that uses local regions corresponding to 3 image scales (as in [29]), the RRSVM method that uses more regions with different aspect ratios (as explained in Sec. 4.1), and the RRSVM method that incorporates the NMS mechanism (i.e., SDR), all achieve a mean AP of 92.9%. SDR, however, is significantly better than RRSVM in predicting fixations during search tasks, increasing the mean AUC score from 0.81 to 0.85. Also note that the predictive power of SDR is not sensitive to the value of α: for aeroplane on the POET dataset, the AUC scores remain the same (0.87) when α is varied from 0.5 to 0.7.\nFigure 2 shows some examples highlighting the difference between the regions selected by RRSVM and SDR. As can be seen, incorporating non-maxima suppression encourages greater dispersion of\nthe sparse areas as opposed to a more clustered distribution in RRSVM. This in turn better predicts attention when there are multiple instances of the target object in the display.\nFigure 3 shows representative cases where the priority maps produced by SDR are significantly different from human fixations. The common failure modes are: (1) failure to locate the correct region for correct classification (see Fig 3a); (2) particularly distracting elements in the scene, such as text (3b) or faces (3c); (3) failure to attend to multiple instances of the target categories. Tuning SDR using human fixation behavioral data [17] and combining SDR with multiple sources of guidance information [8], including saliency and scene context, could mitigate some of the model limitations."
    }, {
      "heading" : "4.3 Target absent condition",
      "text" : "To test whether SDR is able to predict people’s fixations when the search target is absent, we performed experiments on 456 target-absent images from the MIT900 dataset [8]. Human observers were asked to search for people in real world scenes. Eye movement data were collected from 14 searchers who made roughly 6 fixations per image, on average. We picked a random subset of 150 images to tune the Gaussian blur parameter and reported the results for the remaining 306 images. We noticed that the sizes and poses of the people in these images were very different from those of the training samples in VOC2007, which could have led to poor SDR classification performance. In order to address this issue, we augmented the training set of SDR with 456 images from MIT900 that contain people. The added training examples were a disjoint set from the target-absent images for evaluation.\nOn these target absent cases, SDR achieves an AUC score of 0.78. As a reference, the method of Ehinger et al. [8] also achieves AUC of 0.78. But the two methods are not directly comparable because Ehinger et al. [8] used a HOG-based person detector that was trained on a much larger dataset with location annotation.\nFigure 4 shows some randomly selected results from the test set demonstrating SDR’s success in predicting where people attend. Interestingly, SDR looks at regions that either contain person-like objects or are likely to contain persons (e.g., sidewalks), with the latter observation likely the result of sidewalks co-occurring with persons in the positive training samples (a form of scene context effect)."
    }, {
      "heading" : "4.4 Multiple-target attention",
      "text" : "We considered human visual search behavior when there were multiple targets. The experiments were performed on the PET dataset [11]. This dataset is a subset of PASCAL VOC2012 dataset [9], and it contains 4135 images from 6 animal categories (cat, dog, bird, horse cow and sheep). Four subjects were instructed to find all of the animals in each image. Eye movements were recorded, where each subject made roughly 6 fixations per image. We excluded the images that contained people to avoid ambiguity with the animal category. We also removed the images that were shared with the PASCAL VOC 2007 dataset to ensure no overlap between training and testing data. This yielded a total of 3309 images from which a random set of 1300 images were selected for tuning the Gaussian kernel width parameter. The remaining 2309 images were used for testing.\nTo model the search for multiple categories in an image, for all methods except AnnoBoxes we applied six animal classifiers/detectors simultaneously to the test image. For each classifier/detector of each category, a threshold was selected to achieve the highest F1 score on the validation data. The prediction results are shown in the right part of Tab. 1. SDR significantly outperforms other methods. Notably, CAM performs poorly on this dataset, due perhaps to the low classification accuracy of that model (83% mAP on VOC 2007 test set as opposed to 93% of SDR). Some randomly selected results are shown in Fig. 5."
    }, {
      "heading" : "4.5 Center Bias",
      "text" : "For the POET dataset, some of the target objects are quite iconic and in the center of the image. For these cases, a simple center bias map might be a good predictor of the fixations. To test this, we generated priority maps by setting the center of the image to 1 and everywhere else 0, and then applying a Gaussian filter with sigma tuned on the validation set. This simple Center Bias (CB) map achieved an AUC score of 0.84, which is even higher than some of the methods presented in Tab. 1. This prompted us to analyze whether the good performance of SDR is simply due to center bias.\nAn intuitive way to address the CB problem would be to use Shuffled AUC (sAUC) [33]. However, sAUC favors true positives over false negatives and gives more credit to off-center information [3], which may lead to biased results. This is especially true when the datasets are center-biased. The sAUC scores for RCNN, AnnoBox, CAM, SDR, and Inter-Observer [3] are 0.61, 0.61, 0.65, 0.64, and 0.70, respectively. SDR outperforms AnnoBox and RCNN by 3% and is on par with CAM. Also\nnote that sAUC for Inter-Observer is 0.70, which suggests the existence of center bias in POET (the sAUC score of Inter-Observer on MIT300 [17] is 0.81) and raises a concern that sAUC might be misleading for model comparison using this dataset.\nTo further address the concern of center bias, we show in Fig. 6 that the priority maps produced by SDR and Center Bias are quite different. Fig. 6a plots the distribution of the AUC scores for one method when the AUC scores of the other method was low (< 0.6). The spread of these distributions indicate a low correlation between the errors of the two methods. Fig. 6b shows a box plot of the distribution of KL divergence [6] between the priority maps generated by SDR and Center Bias. For each category, the mean KL divergence value is high, indicating a large difference between SDR and Center Bias. For a more qualitative intuition of KL divergence in these distributions, see Figure 2.\nThe center bias effect in PET and MIT900 is not as pronounced as in POET because there are multiple target objects in the PET images and the target objects in the MIT900 dataset are relatively small. For these datasets, Center Bias achieves AUC scores of 0.78 and 0.72, respectively. These numbers are significantly lower than the results obtained by SDR, which are 0.82 and 0.78, respectively."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "We introduced a classification model based on sparse and diverse region ranking and selection, which is trained only on image level annotations. We then provided experimental evidence from visual search tasks under three different conditions to support our hypothesis that these computational mechanisms might be analogous to computations underlying visual attention processes in the brain.\nWhile this work is not the first to use computer vision models to predict where humans look in visual search tasks, it is the first to show that core mechanisms driving high model performance in a search task also predict how humans allocate their attention in the same tasks. By improving upon these core computational principles, and perhaps by incorporating new ones suggested by attention mechanisms, our hope is to shed more light on human visual processing.\nThere are several directions for future work. The first is to create a visual search dataset that mitigates the center bias effect and avoids cases of trivially easy search. The second is to incorporate into the current model known factors affecting search, such as a center bias, bottom-up saliency, scene context, etc., to better predict shifts in human spatial attention.\nAcknowledgment. This project was partially supported by the National Science Foundation Awards IIS-1161876 and IIS-1566248 and the Subsample project from the Digiteo Institute, France."
    } ],
    "references" : [ {
      "title" : "Multiple object recognition with visual attention",
      "author" : [ "J. Ba", "V. Mnih", "K. Kavukcuoglu" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "State-of-the-art in visual attention",
      "author" : [ "A. Borji", "L. Itti" ],
      "venue" : "modeling. PAMI,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Analysis of scores, datasets, and models in visual saliency prediction",
      "author" : [ "A. Borji", "H.R. Tavakoli", "D.N. Sihite", "L. Itti" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Saliency, attention, and visual search: An information theoretic approach",
      "author" : [ "N.D. Bruce", "J.K. Tsotsos" ],
      "venue" : "Journal of Vision,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "What do different evaluation metrics tell us about saliency models",
      "author" : [ "Z. Bylinskii", "T. Judd", "A. Oliva", "A. Torralba", "F. Durand" ],
      "venue" : "arXiv preprint arXiv:1604.03605,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Robots and biological systems: Towards a new bionics",
      "author" : [ "P. Dario", "G. Sandini", "P. Aebischer" ],
      "venue" : "In NATO Advanced Workshop,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Modelling search for people in 900 scenes: A combined source model of eye guidance",
      "author" : [ "K.A. Ehinger", "B. Hidalgo-Sotelo", "A. Torralba", "A. Oliva" ],
      "venue" : "Visual Cognition,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "The pascal visual object classes challenge: A retrospective",
      "author" : [ "M. Everingham", "S.M.A. Eslami", "L.V. Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Salience, relevance, and firing: a priority map for target selection",
      "author" : [ "J.H. Fecteau", "D.P. Munoz" ],
      "venue" : "Trends in cognitive sciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Pet: An eye-tracking dataset for animal-centric pascal object classes",
      "author" : [ "S.O. Gilani", "R. Subramanian", "Y. Yan", "D. Melcher", "N. Sebe", "S. Winkler" ],
      "venue" : "In ICME,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "author" : [ "R. Girshick", "J. Donahue", "T. Darrell", "J. Malik" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "arXiv preprint arXiv:1308.0850,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Regularized max pooling for image categorization",
      "author" : [ "M. Hoai" ],
      "venue" : "In Proc. BMVC.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Improving human action recognition using score distribution and ranking",
      "author" : [ "M. Hoai", "A. Zisserman" ],
      "venue" : "In Proc. ACCV,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "A saliency-based search mechanism for overt and covert shifts of visual attention",
      "author" : [ "L. Itti", "C. Koch" ],
      "venue" : "Vision Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Learning to predict where humans look",
      "author" : [ "T. Judd", "K. Ehinger", "F. Durand", "A. Torralba" ],
      "venue" : "In Proc. ICCV. IEEE,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Sun: Top-down saliency using natural statistics",
      "author" : [ "C. Kanan", "M.H. Tong", "L. Zhang", "G.W. Cottrell" ],
      "venue" : "Visual Cognition,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Clustering appearance and shape by learning jigsaws",
      "author" : [ "A. Kannan", "J. Winn", "C. Rother" ],
      "venue" : "In NIPS",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Shifts in selective visual attention: towards the underlying neural circuitry",
      "author" : [ "C. Koch", "S. Ullman" ],
      "venue" : "In Matters of intelligence,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1987
    }, {
      "title" : "Towards bridging the Gap between Biological and Computational Image Segmentation",
      "author" : [ "I. Kokkinos", "R. Deriche", "T. Papadopoulo", "O. Faugeras", "P. Maragos" ],
      "venue" : "Research Report RR-6317,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories",
      "author" : [ "S. Lazebnik", "C. Schmid", "J. Ponce" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2006
    }, {
      "title" : "An information-theoretic framework for understanding saccadic eye movements",
      "author" : [ "T.S. Lee", "X.Y. Stella" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1999
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "V. Mnih", "N. Heess", "A. Graves" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Training object class detectors from eye tracking data",
      "author" : [ "D.P. Papadopoulos", "A.D. Clarke", "F. Keller", "V. Ferrari" ],
      "venue" : "In ECCV",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Imagenet large scale visual recognition challenge",
      "author" : [ "O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "Very deep convolutional networks for large-scale image recognition",
      "author" : [ "K. Simonyan", "A. Zisserman" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Region ranking svms for image classification",
      "author" : [ "Z. Wei", "M. Hoai" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio" ],
      "venue" : "In ICML,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Modelling eye movements in a categorical search task",
      "author" : [ "G.J. Zelinsky", "H. Adeli", "Y. Peng", "D. Samaras" ],
      "venue" : "Philosophical Transactions of the Royal Society of London B: Biological Sciences,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "The what, where, and why of priority maps and their interactions with visual working memory",
      "author" : [ "G.J. Zelinsky", "J.W. Bisley" ],
      "venue" : "Annals of the New York Academy of Sciences,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "Sun: A bayesian framework for saliency using natural statistics",
      "author" : [ "L. Zhang", "M.H. Tong", "T.K. Marks", "H. Shan", "G.W. Cottrell" ],
      "venue" : "Journal of vision,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2008
    }, {
      "title" : "Learning Deep Features for Discriminative Localization",
      "author" : [ "B. Zhou", "L.A.A. Khosla", "A. Oliva", "A. Torralba" ],
      "venue" : null,
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Efficient visual search requires prioritizing the locations of features of the target object class over features at locations offering less evidence for the target [31].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 30,
      "context" : "Computational models of visual search typically estimate and plot goal directed prioritization of visual space as priority maps for directing attention [32].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 14,
      "context" : "This form of target directed prioritization is different from the saliency modeling literature, where bottom-up feature contrast in an image is used to predict fixation behavior during the free-viewing of scenes [16].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 1,
      "context" : "The field of fixation prediction is highly active and growing [2], although it was not until fairly recently that attention researchers have begun using the sophisticated object detection techniques developed in the computer vision literature [8, 18, 31].",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "The field of fixation prediction is highly active and growing [2], although it was not until fairly recently that attention researchers have begun using the sophisticated object detection techniques developed in the computer vision literature [8, 18, 31].",
      "startOffset" : 243,
      "endOffset" : 254
    }, {
      "referenceID" : 16,
      "context" : "The field of fixation prediction is highly active and growing [2], although it was not until fairly recently that attention researchers have begun using the sophisticated object detection techniques developed in the computer vision literature [8, 18, 31].",
      "startOffset" : 243,
      "endOffset" : 254
    }, {
      "referenceID" : 29,
      "context" : "The field of fixation prediction is highly active and growing [2], although it was not until fairly recently that attention researchers have begun using the sophisticated object detection techniques developed in the computer vision literature [8, 18, 31].",
      "startOffset" : 243,
      "endOffset" : 254
    }, {
      "referenceID" : 6,
      "context" : "The dominant method used in the visual search literature to generate priority maps for detection has been the exhaustive detection mechanism [8, 18].",
      "startOffset" : 141,
      "endOffset" : 148
    }, {
      "referenceID" : 16,
      "context" : "The dominant method used in the visual search literature to generate priority maps for detection has been the exhaustive detection mechanism [8, 18].",
      "startOffset" : 141,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Using this method, an object detector is applied to an image to provide bounding boxes that are then combined, weighted by their detection scores, to generate a priority map [8].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 22,
      "context" : "An alternative approach to modeling visual attention is to determine how model and behavioral task performance depends on shared core computational principles [24].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 28,
      "context" : "To this end, a new class of attention-inspired models have been developed and applied to tasks ranging from image captioning [30] to hand writing generation [13], where selective spatial attention mechanisms have been shown to emerge [1, 25].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "To this end, a new class of attention-inspired models have been developed and applied to tasks ranging from image captioning [30] to hand writing generation [13], where selective spatial attention mechanisms have been shown to emerge [1, 25].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "To this end, a new class of attention-inspired models have been developed and applied to tasks ranging from image captioning [30] to hand writing generation [13], where selective spatial attention mechanisms have been shown to emerge [1, 25].",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 23,
      "context" : "To this end, a new class of attention-inspired models have been developed and applied to tasks ranging from image captioning [30] to hand writing generation [13], where selective spatial attention mechanisms have been shown to emerge [1, 25].",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 23,
      "context" : "By requiring visual inputs to be gated in a manner similar to the human gating of visual inputs via fixations, these models are able to localize or “attend” selectively to the most informative regions of an input image while ignoring irrelevant visual inputs [25, 1].",
      "startOffset" : 259,
      "endOffset" : 266
    }, {
      "referenceID" : 0,
      "context" : "By requiring visual inputs to be gated in a manner similar to the human gating of visual inputs via fixations, these models are able to localize or “attend” selectively to the most informative regions of an input image while ignoring irrelevant visual inputs [25, 1].",
      "startOffset" : 259,
      "endOffset" : 266
    }, {
      "referenceID" : 28,
      "context" : "This built in attention mechanism enables the model of [30], trained only on generating captions, to bias the visual input so as to gate only relevant information when generating each word to describe an image.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "The specific image-category classification model that we use is called Region Ranking SVM (RRSVM) [29].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 27,
      "context" : "This model was developed in our recent work [29], and it achieved state-of-the-art performance on a number of classification tasks by learning categorization with locally-pooled information from input images.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "This model works by imposing sparsity on selected image areas that contribute to the classification decision, much like how humans prioritize visual space and sample with fixations only a sparse set of image locations while attempting to detect and recognize object categories [4].",
      "startOffset" : 277,
      "endOffset" : 280
    }, {
      "referenceID" : 14,
      "context" : "It is worth noting that this model was originally created for object classification and not localization, hence no object localization data is used to train it, unlike standard fixation prediction algorithms [16, 17].",
      "startOffset" : 208,
      "endOffset" : 216
    }, {
      "referenceID" : 15,
      "context" : "It is worth noting that this model was originally created for object classification and not localization, hence no object localization data is used to train it, unlike standard fixation prediction algorithms [16, 17].",
      "startOffset" : 208,
      "endOffset" : 216
    }, {
      "referenceID" : 8,
      "context" : "Second, we incorporate the biologically plausible constraint of Inhibition of Return [10], which we model by requiring a set of diverse (minimally overlapping) sparse regions in RRSVM.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 27,
      "context" : "Here we review Region Ranking SVM (RRSVM) [29].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "The vector s is the weight vector for combining the SVM region scores for each image [15]; this vector is common to all images of a class.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 27,
      "context" : "An experiment [29] on the ImageNet dataset [27] with 1000 classes showed that RRSVM generally uses 20 regions or less (from hundreds of local regions considered).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 25,
      "context" : "An experiment [29] on the ImageNet dataset [27] with 1000 classes showed that RRSVM generally uses 20 regions or less (from hundreds of local regions considered).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "A mechanism critical to the modeling of human visual search behavior is Inhibition of Return: the lower probability of re-fixating on or near already attended areas, possibly mediated by lateral inhibition [16, 20].",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 18,
      "context" : "A mechanism critical to the modeling of human visual search behavior is Inhibition of Return: the lower probability of re-fixating on or near already attended areas, possibly mediated by lateral inhibition [16, 20].",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 19,
      "context" : "The biological plausibility of non-maxima suppression has been discussed in previous work, where it was shown to be a plausible method for allowing the stronger activations to stand out (see [21, 7] for details).",
      "startOffset" : 191,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "The biological plausibility of non-maxima suppression has been discussed in previous work, where it was shown to be a plausible method for allowing the stronger activations to stand out (see [21, 7] for details).",
      "startOffset" : 191,
      "endOffset" : 198
    }, {
      "referenceID" : 27,
      "context" : "The above formulation can be optimized in the same way as RRSVM in [29].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : "Experiments are performed on three datasets POET [26], PET [11] and MIT900 [8], which are the only available datasets for object search tasks.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 9,
      "context" : "Experiments are performed on three datasets POET [26], PET [11] and MIT900 [8], which are the only available datasets for object search tasks.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "Experiments are performed on three datasets POET [26], PET [11] and MIT900 [8], which are the only available datasets for object search tasks.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 27,
      "context" : "Our implementation of RRSVM and SDR is similar to [29], but we consider more local regions.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : "As in [29], the feature extraction pipeline is based on VGG16 [28].",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 26,
      "context" : "As in [29], the feature extraction pipeline is based on VGG16 [28].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 26,
      "context" : "In this work, we consider 7 different image sizes instead of the three sizes used by [28, 29].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 27,
      "context" : "In this work, we consider 7 different image sizes instead of the three sizes used by [28, 29].",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "The RRSVM and SDR classifiers used in the following experiments are trained on the trainval set of PASCAL VOC 2007 dataset [9] unless otherwise stated.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "To make this comparison we use the Area Under the ROC Curve (AUC), a commonly used metric for visual search task evaluation [6].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 24,
      "context" : "We consider visual attention in the single-target present condition using the POET dataset [26].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "This dataset is a subset of PASCAL VOC 2012 dataset [9], and it has 6270 images from 10 object categories (aeroplane, boat, bike, motorbike, cat, dog, horse, cow, sofa and dining table).",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "All recent fixation prediction models [8, 19, 31] apply object category detectors on the input image and combine the detection results to create priority maps.",
      "startOffset" : 38,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "All recent fixation prediction models [8, 19, 31] apply object category detectors on the input image and combine the detection results to create priority maps.",
      "startOffset" : 38,
      "endOffset" : 49
    }, {
      "referenceID" : 29,
      "context" : "All recent fixation prediction models [8, 19, 31] apply object category detectors on the input image and combine the detection results to create priority maps.",
      "startOffset" : 38,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "However, our RCNN [12] baseline, which is the state-of-the-art object detector on this dataset, should improve the pipelines of these models.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 32,
      "context" : "We also consider the recently proposed weakly-supervised object localization approach of [34], which is denoted as CAM in Table 1.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 27,
      "context" : "When evaluated on the PASCAL VOC 2007 test set, the RRSVM method that uses local regions corresponding to 3 image scales (as in [29]), the RRSVM method that uses more regions with different aspect ratios (as explained in Sec.",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "Tuning SDR using human fixation behavioral data [17] and combining SDR with multiple sources of guidance information [8], including saliency and scene context, could mitigate some of the model limitations.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "Tuning SDR using human fixation behavioral data [17] and combining SDR with multiple sources of guidance information [8], including saliency and scene context, could mitigate some of the model limitations.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "To test whether SDR is able to predict people’s fixations when the search target is absent, we performed experiments on 456 target-absent images from the MIT900 dataset [8].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "[8] used a HOG-based person detector that was trained on a much larger dataset with location annotation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "The experiments were performed on the PET dataset [11].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "This dataset is a subset of PASCAL VOC2012 dataset [9], and it contains 4135 images from 6 animal categories (cat, dog, bird, horse cow and sheep).",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 31,
      "context" : "An intuitive way to address the CB problem would be to use Shuffled AUC (sAUC) [33].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "However, sAUC favors true positives over false negatives and gives more credit to off-center information [3], which may lead to biased results.",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "The sAUC scores for RCNN, AnnoBox, CAM, SDR, and Inter-Observer [3] are 0.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "70, which suggests the existence of center bias in POET (the sAUC score of Inter-Observer on MIT300 [17] is 0.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "6b shows a box plot of the distribution of KL divergence [6] between the priority maps generated by SDR and Center Bias.",
      "startOffset" : 57,
      "endOffset" : 60
    } ],
    "year" : 2016,
    "abstractText" : "Learned region sparsity has achieved state-of-the-art performance in classification tasks by exploiting and integrating a sparse set of local information into global decisions. The underlying mechanism resembles how people sample information from an image with their eye movements when making similar decisions. In this paper we incorporate the biologically plausible mechanism of Inhibition of Return into the learned region sparsity model, thereby imposing diversity on the selected regions. We investigate how these mechanisms of sparsity and diversity relate to visual attention by testing our model on three different types of visual search tasks. We report state-of-the-art results in predicting the locations of human gaze fixations, even though our model is trained only on image-level labels without object location annotations. Notably, the classification performance of the extended model remains the same as the original. This work suggests a new computational perspective on visual attention mechanisms, and shows how the inclusion of attention-based mechanisms can improve computer vision techniques.",
    "creator" : null
  }
}