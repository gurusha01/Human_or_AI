{
  "name" : "320722549d1751cf3f247855f937b982.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Coin Betting and Parameter-Free Online Learning",
    "authors" : [ "Francesco Orabona" ],
    "emails" : [ "francesco@orabona.com", "dpal@yahoo-inc.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the Online Linear Optimization (OLO) [4, 25] setting. In each round t, an algorithm chooses a point wt from a convex decision set K and then receives a reward vector gt. The algorithm’s goal is to keep its regret small, defined as the difference between its cumulative reward and the cumulative reward of a fixed strategy u ∈ K, that is\nRegretT (u) = T∑ t=1 〈gt, u〉 − T∑ t=1 〈gt, wt〉 .\nWe focus on two particular decision sets, the N -dimensional probability simplex ∆N = {x ∈ RN : x ≥ 0, ‖x‖1 = 1} and a Hilbert space H. OLO over ∆N is referred to as the problem of Learning with Expert Advice (LEA). We assume bounds on the norms of the reward vectors: For OLO overH, we assume that ‖gt‖ ≤ 1, and for LEA we assume that gt ∈ [0, 1]N . OLO is a basic building block of many machine learning problems. For example, Online Convex Optimization (OCO), the problem analogous to OLO where 〈gt, u〉 is generalized to an arbitrary convex function `t(u), is solved through a reduction to OLO [25]. LEA [17, 27, 5] provides a way of combining classifiers and it is at the heart of boosting [12]. Batch and stochastic convex optimization can also be solved through a reduction to OLO [25]. To achieve optimal regret, most of the existing online algorithms require the user to set the learning rate (step size) η to an unknown/oracle value. For example, to obtain the optimal bound for Online Gradient Descent (OGD), the learning rate has to be set with the knowledge of the norm of the competitor u, ‖u‖; second entry in Table 1. Likewise, the optimal learning rate for Hedge depends on the KL divergence between the prior weighting π and the unknown competitor u, D (u‖π); seventh entry in Table 1. Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24]. These algorithms adapt to the number of experts and to the norm of the optimal predictor, respectively, without the need to tune parameters. However, their design and underlying intuition is still a challenge. Foster et al. [11] proposed a unified framework, but it is not constructive. Furthermore, all existing algorithms for LEA either have sub-optimal regret bound (e.g. extra O(log log T ) factor) or sub-optimal running time (e.g. requiring solving a numerical problem in every round, or with extra factors); see Table 1.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nContributions. We show that a more fundamental notion subsumes both OLO and LEA parameterfree algorithms. We prove that the ability to maximize the wealth in bets on the outcomes of coin flips implies OLO and LEA parameter-free algorithms. We develop a novel potential-based framework for betting algorithms. It gives intuition to previous constructions and, instantiated with the Krichevsky-Trofimov estimator, provides new and elegant algorithms for OLO and LEA. The new algorithms also have optimal worst-case guarantees on regret and time complexity; see Table 1."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We begin by providing some definitions. The Kullback-Leibler (KL) divergence between two discrete distributions p and q is D (p‖q) = ∑ i pi ln (pi/qi). If p, q are real numbers in [0, 1], we denote by D (p‖q) = p ln (p/q)+(1−p) ln ((1− p)/(1− q)) the KL divergence between two Bernoulli distributions with parameters p and q. We denote byH a Hilbert space, by 〈·, ·〉 its inner product, and by ‖·‖ the induced norm. We denote by ‖·‖1 the 1-norm in RN . A function F : I → R+ is called logarithmically convex iff f(x) = ln(F (x)) is convex. Let f : V → R ∪ {±∞}, the Fenchel conjugate of f is f∗ : V ∗ → R∪{±∞} defined on the dual vector space V ∗ by f∗(θ) = supx∈V 〈θ, x〉−f(x). A function f : V → R ∪ {+∞} is said to be proper if there exists x ∈ V such that f(x) is finite. If f is a proper lower semi-continuous convex function then f∗ is also proper lower semi-continuous convex and f∗∗ = f .\nCoin Betting. We consider a gambler making repeated bets on the outcomes of adversarial coin flips. The gambler starts with an initial endowment > 0. In each round t, he bets on the outcome of a coin flip gt ∈ {−1, 1}, where +1 denotes heads and −1 denotes tails. We do not make any assumption on how gt is generated, that is, it can be chosen by an adversary.\nThe gambler can bet any amount on either heads or tails. However, he is not allowed to borrow any additional money. If he loses, he loses the betted amount; if he wins, he gets the betted amount back and, in addition to that, he gets the same amount as a reward. We encode the gambler’s bet in round t by a single number wt. The sign of wt encodes whether he is betting on heads or tails. The absolute value encodes the betted amount. We define Wealtht as the gambler’s wealth at the end of round t and Rewardt as the gambler’s net reward (the difference of wealth and initial endowment), that is\nWealtht = + t∑ i=1 wigi and Rewardt = Wealtht− . (1)\nIn the following, we will also refer to a bet with βt, where βt is such that wt = βt Wealtht−1 . (2)\nThe absolute value of βt is the fraction of the current wealth to bet, and sign of βt encodes whether he is betting on heads or tails. The constraint that the gambler cannot borrow money implies that βt ∈ [−1, 1]. We also generalize the problem slightly by allowing the outcome of the coin flip gt to be any real number in the interval [−1, 1]; wealth and reward in (1) remain exactly the same.\n1These algorithms require to solve a numerical problem at each step. The number K is the number of steps needed to reach the required precision. Neither the precision nor K are calculated in these papers.\n2The proof in [15] can be modified to prove a KL bound, see http://blog.wouterkoolen.info. 3A variant of the algorithm in [11] can be implemented with the stated time complexity [10]."
    }, {
      "heading" : "3 Warm-Up: From Betting to One-Dimensional Online Linear Optimization",
      "text" : "In this section, we sketch how to reduce one-dimensional OLO to betting on a coin. The reasoning for generic Hilbert spaces (Section 5) and for LEA (Section 6) will be similar. We will show that the betting view provides a natural way for the analysis and design of online learning algorithms, where the only design choice is the potential function of the betting algorithm (Section 4). A specific example of coin betting potential and the resulting algorithms are in Section 7.\nAs a warm-up, let us consider an algorithm for OLO over one-dimensional Hilbert space R. Let {wt}∞t=1 be its sequence of predictions on a sequence of rewards {gt}∞t=1, gt ∈ [−1, 1]. The total reward of the algorithm after t rounds is Rewardt = ∑t i=1 giwi. Also, even if in OLO there is no concept of “wealth”, define the wealth of the OLO algorithm as Wealtht = + Rewardt, as in (1).\nWe now restrict our attention to algorithms whose predictions wt are of the form of a bet, that is wt = βt Wealtht−1, where βt ∈ [−1, 1]. We will see that the restriction on βt does not prevent us from obtaining parameter-free algorithms with optimal bounds.\nGiven the above, it is immediate to see that any coin betting algorithm that, on a sequence of coin flips {gt}∞t=1, gt ∈ [−1, 1], bets the amounts wt can be used as an OLO algorithm in a onedimensional Hilbert space R. But, what would be the regret of such OLO algorithms?\nAssume that the betting algorithm at hand guarantees that its wealth is at least F ( ∑T t=1 gt) starting from an endowment , for a given potential function F , then\nRewardT = T∑ t=1 gtwt = WealthT − ≥ F ( T∑ t=1 gt ) − . (3)\nIntuitively, if the reward is big we can expect the regret to be small. Indeed, the following lemma converts the lower bound on the reward to an upper bound on the regret. Lemma 1 (Reward-Regret relationship [22]). Let V, V ∗ be a pair of dual vector spaces. Let F : V → R∪{+∞} be a proper convex lower semi-continuous function and let F ∗ : V ∗ → R∪{+∞} be its Fenchel conjugate. Let w1, w2, . . . , wT ∈ V and g1, g2, . . . , gT ∈ V ∗. Let ∈ R. Then, T∑ t=1\n〈gt, wt〉︸ ︷︷ ︸ RewardT ≥ F\n( T∑ t=1 gt ) − if and only if ∀u ∈ V ∗, T∑ t=1\n〈gt, u− wt〉︸ ︷︷ ︸ RegretT (u)\n≤ F ∗(u) + .\nApplying the lemma, we get a regret upper bound: RegretT (u) ≤ F ∗(u) + for all u ∈ H. To summarize, if we have a betting algorithm that guarantees a minimum wealth of F ( ∑T t=1 gt), it can be used to design and analyze a one-dimensional OLO algorithm. The faster the growth of the wealth, the smaller the regret will be. Moreover, the lemma also shows that trying to design an algorithm that is adaptive to u is equivalent to designing an algorithm that is adaptive to ∑T t=1 gt. Also, most importantly, methods that guarantee optimal wealth for the betting scenario are already known, see, e.g., [4, Chapter 9]. We can just re-use them to get optimal online algorithms!"
    }, {
      "heading" : "4 Designing a Betting Algorithm: Coin Betting Potentials",
      "text" : "For sequential betting on i.i.d. coin flips, an optimal strategy has been proposed by Kelly [14]. The strategy assumes that the coin flips {gt}∞t=1, gt ∈ {+1,−1}, are generated i.i.d. with known probability of heads. If p ∈ [0, 1] is the probability of heads, the Kelly bet is to bet βt = 2p − 1 at each round. He showed that, in the long run, this strategy will provide more wealth than betting any other fixed fraction of the current wealth [14].\nFor adversarial coins, Kelly betting does not make sense. With perfect knowledge of the future, the gambler could always bet everything on the right outcome. Hence, after T rounds from an initial endowment , the maximum wealth he would get is 2T . Instead, assume he bets the same fraction β of its wealth at each round. Let Wealtht(β) the wealth of such strategy after t rounds. As observed in [21], the optimal fixed fraction to bet is β∗ = ( ∑T t=1 gt)/T and it gives the wealth\nWealthT (β ∗) = exp ( T ·D ( 1 2 + ∑T t=1 gt 2T ∥∥∥ 12)) ≥ exp( (∑Tt=1 gt)22T ) , (4)\nwhere the inequality follows from Pinsker’s inequality [9, Lemma 11.6.1].\nHowever, even without knowledge of the future, it is possible to go very close to the wealth in (4). This problem was studied by Krichevsky and Trofimov [16], who proposed that after seeing the coin\nflips g1, g2, . . . , gt−1 the empirical estimate kt = 1/2+\n∑t−1 i=1 1[gi=+1]\nt should be used instead of p. Their estimate is commonly called KT estimator.1 The KT estimator results in the betting\nβt = 2kt − 1 = ∑t−1 i=1 gi t (5)\nwhich we call adaptive Kelly betting based on the KT estimator. It looks like an online and slightly biased version of the oracle choice of β∗. This strategy guarantees2\nWealthT ≥ WealthT (β ∗)\n2 √ T\n= 2 √ T\nexp ( T ·D ( 1 2 + ∑T t=1 gt 2T ∥∥∥ 12)) . This guarantee is optimal up to constant factors [4] and mirrors the guarantee of the Kelly bet.\nHere, we propose a new set of definitions that allows to generalize the strategy of adaptive Kelly betting based on the KT estimator. For these strategies it will be possible to prove that, for any g1, g2, . . . , gt ∈ [−1, 1],\nWealtht ≥ Ft ( t∑ i=1 gi ) , (6)\nwhere Ft(x) is a certain function. We call such functions potentials. The betting strategy will be determined uniquely by the potential (see (c) in the Definition 2), and we restrict our attention to potentials for which (6) holds. These constraints are specified in the definition below.\nDefinition 2 (Coin Betting Potential). Let > 0. Let {Ft}∞t=0 be a sequence of functions Ft : (−at, at) → R+ where at > t. The sequence {Ft}∞t=0 is called a sequence of coin betting potentials for initial endowment , if it satisfies the following three conditions:\n(a) F0(0) = .\n(b) For every t ≥ 0, Ft(x) is even, logarithmically convex, strictly increasing on [0, at), and limx→at Ft(x) = +∞.\n(c) For every t ≥ 1, every x ∈ [−(t − 1), (t − 1)] and every g ∈ [−1, 1], (1 + gβt)Ft−1(x) ≥ Ft(x+ g), where\nβt = Ft(x+1)−Ft(x−1) Ft(x+1)+Ft(x−1) . (7)\nThe sequence {Ft}∞t=0 is called a sequence of excellent coin betting potentials for initial endowment if it satisfies conditions (a)–(c) and the condition (d) below.\n(d) For every t ≥ 0, Ft is twice-differentiable and satisfies x ·F ′′t (x) ≥ F ′t (x) for every x ∈ [0, at).\nLet’s give some intuition on this definition. First, let’s show by induction on t that (b) and (c) of the definition together with (2) give a betting strategy that satisfies (6). The base case t = 0 is trivial. At time t ≥ 1, bet wt = βt Wealtht−1 where βt is defined in (7), then\nWealtht = Wealtht−1 +wtgt = (1 + gtβt) Wealtht−1\n≥ (1 + gtβt)Ft−1 ( t−1∑ i=1 gi ) ≥ Ft ( t−1∑ i=1 gi + gt ) = Ft ( t∑ i=1 gi ) .\nThe formula for the potential-based strategy (7) might seem strange. However, it is derived—see Theorem 8 in Appendix B—by minimizing the worst-case value of the right-hand side of the inequality used w.r.t. to gt in the induction proof above: Ft−1(x) ≥ Ft(x+gt)1+gtβt .\nThe last point, (d), is a technical condition that allows us to seamlessly reduce OLO over a Hilbert space to the one-dimensional problem, characterizing the worst case direction for the reward vectors.\n1Compared to the maximum likelihood estimate ∑t−1 i=1 1[gi=+1]\nt−1 , KT estimator shrinks slightly towards 1/2. 2See Appendix A for a proof. For lack of space, all the appendices are in the supplementary material.\nRegarding the design of coin betting potentials, we expect any potential that approximates the best possible wealth in (4) to be a good candidate. In fact, Ft(x) = exp ( x2/(2t) ) / √ t, essentially the potential used in the parameter-free algorithms in [22, 24] for OLO and in [6, 18, 19] for LEA, approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B. Hence, our framework provides intuition to previous constructions and in Section 7 we show new examples of coin betting potentials.\nIn the next two sections, we presents the reductions to effortlessly solve both the generic OLO case and LEA with a betting potential."
    }, {
      "heading" : "5 From Coin Betting to OLO over Hilbert Space",
      "text" : "In this section, generalizing the one-dimensional construction in Section 3, we show how to use a sequence of excellent coin betting potentials {Ft}∞t=0 to construct an algorithm for OLO over a Hilbert space and how to prove a regret bound for it.\nWe define reward and wealth analogously to the one-dimensional case: Rewardt = ∑t i=1〈gi, wi〉 and Wealtht = + Rewardt. Given a sequence of coin betting potentials {Ft}∞t=0, using (7) we define the fraction\nβt = Ft(‖∑t−1i=1 gi‖+1)−Ft(‖∑t−1i=1 gi‖−1) Ft(‖∑t−1i=1 gi‖+1)+Ft(‖∑t−1i=1 gi‖−1) . (8)\nThe prediction of the OLO algorithm is defined similarly to the one-dimensional case, but now we also need a direction in the Hilbert space:\nwt = βt Wealtht−1 ∑t−1 i=1 gi∥∥∥∑t−1i=1 gi∥∥∥ = βt ∑t−1 i=1 gi∥∥∥∑t−1i=1 gi∥∥∥ ( + t−1∑ i=1 〈gi, wi〉 ) . (9)\nIf ∑t−1 i=1 gi is the zero vector, we define wt to be the zero vector as well. For this prediction strategy\nwe can prove the following regret guarantee, proved in Appendix C. The proof reduces the general Hilbert case to the 1-d case, thanks to (d) in Definition 2, then it follows the reasoning of Section 3. Theorem 3 (Regret Bound for OLO in Hilbert Spaces). Let {Ft}∞t=0 be a sequence of excellent coin betting potentials. Let {gt}∞t=1 be any sequence of reward vectors in a Hilbert space H such that ‖gt‖ ≤ 1 for all t. Then, the algorithm that makes prediction wt defined by (9) and (8) satisfies\n∀T ≥ 0 ∀u ∈ H RegretT (u) ≤ F ∗T (‖u‖) + ."
    }, {
      "heading" : "6 From Coin Betting to Learning with Expert Advice",
      "text" : "In this section, we show how to use the algorithm for OLO over one-dimensional Hilbert space R from Section 3—which is itself based on a coin betting strategy—to construct an algorithm for LEA. Let N ≥ 2 be the number of experts and ∆N be the N -dimensional probability simplex. Let π = (π1, π2, . . . , πN ) ∈ ∆N be any prior distribution. Let A be an algorithm for OLO over the one-dimensional Hilbert space R, based on a sequence of the coin betting potentials {Ft}∞t=0 with initial endowment3 1. We instantiate N copies of A. Consider any round t. Let wt,i ∈ R be the prediction of the i-th copy of A. The LEA algorithm computes p̂t = (p̂t,1, p̂t,2, . . . , p̂t,N ) ∈ RN0,+ as\np̂t,i = πi · [wt,i]+, (10)\nwhere [x]+ = max{0, x} is the positive part of x. Then, the LEA algorithm predicts pt = (pt,1, pt,2, . . . , pt,N ) ∈ ∆N as\npt = p̂t ‖p̂t‖1 . (11)\nIf ‖p̂t‖1 = 0, the algorithm predicts the prior π. Then, the algorithm receives the reward vector gt = (gt,1, gt,2, . . . , gt,N ) ∈ [0, 1]N . Finally, it feeds the reward to each copy of A. The reward for\n3Any initial endowment > 0 can be rescaled to 1. Instead of Ft(x) we would use Ft(x)/ . The wt would become wt/ , but pt is invariant to scaling of wt. Hence, the LEA algorithm is the same regardless of .\nthe i-th copy of A is g̃t,i ∈ [−1, 1] defined as\ng̃t,i = { gt,i − 〈gt, pt〉 if wt,i > 0 , [gt,i − 〈gt, pt〉]+ if wt,i ≤ 0 .\n(12)\nThe construction above defines a LEA algorithm defined by the predictions pt, based on the algorithm A. We can prove the following regret bound for it. Theorem 4 (Regret Bound for Experts). Let A be an algorithm for OLO over the one-dimensional Hilbert space R, based on the coin betting potentials {Ft}∞t=0 for an initial endowment of 1. Let f−1t be the inverse of ft(x) = ln(Ft(x)) restricted to [0,∞). Then, the regret of the LEA algorithm with prior π ∈ ∆N that predicts at each round with pt in (11) satisfies\n∀T ≥ 0 ∀u ∈ ∆N RegretT (u) ≤ f−1T (D (u‖π)) .\nThe proof, in Appendix D, is based on the fact that (10)–(12) guarantee that ∑N i=1 πig̃t,iwt,i ≤ 0 and on a variation of the change of measure lemma used in the PAC-Bayes literature, e.g. [20]."
    }, {
      "heading" : "7 Applications of the Krichevsky-Trofimov Estimator to OLO and LEA",
      "text" : "In the previous sections, we have shown that a coin betting potential with a guaranteed rapid growth of the wealth will give good regret guarantees for OLO and LEA. Here, we show that the KT estimator has associated an excellent coin betting potential, which we call KT potential. Then, the optimal wealth guarantee of the KT potentials will translate to optimal parameter-free regret bounds.\nThe sequence of excellent coin betting potentials for an initial endowment corresponding to the adaptive Kelly betting strategy βt defined by (5) based on the KT estimator are\nFt(x) = 2t·Γ\n( t+1\n2 + x 2\n) ·Γ ( t+1\n2 − x 2 ) π·t! t ≥ 0, x ∈ (−t− 1, t+ 1), (13)\nwhere Γ(x) = ∫∞\n0 tx−1e−tdt is Euler’s gamma function—see Theorem 13 in Appendix E. This\npotential was used to prove regret bounds for online prediction with the logarithmic loss [16][4, Chapter 9.7]. Theorem 13 also shows that the KT betting strategy βt as defined by (5) satisfies (7).\nThis potential has the nice property that is satisfies the inequality in (c) of Definition 2 with equality when gt ∈ {−1, 1}, i.e. Ft(x+ gt) = (1 + gtβt)Ft−1(x). We also generalize the KT potentials to δ-shifted KT potentials, where δ ≥ 0, defined as\nFt(x) = 2t·Γ(δ+1)·Γ\n( t+δ+1\n2 + x 2\n) ·Γ ( t+δ+1\n2 − x 2 ) Γ ( δ+1\n2\n)2 ·Γ(t+δ+1)\n.\nThe reason for its name is that, up to a multiplicative constant, Ft is equal to the KT potential shifted in time by δ. Theorem 13 also proves that the δ-shifted KT potentials are excellent coin\nbetting potentials with initial endowment 1, and the corresponding betting fraction is βt = ∑t−1 j=1 gj\nδ+t ."
    }, {
      "heading" : "7.1 OLO in Hilbert Space",
      "text" : "We apply the KT potential for the construction of an OLO algorithm over a Hilbert space H. We will use (9), and we just need to calculate βt. According to Theorem 13 in Appendix E, the formula\nfor βt simplifies to βt = ‖∑t−1i=1 gi‖ t so that wt = 1 t ( + ∑t−1 i=1〈gi, wi〉 )∑t−1 i=1 gi.\nThe resulting algorithm is stated as Algorithm 1. We derive a regret bound for it as a very simple corollary of Theorem 3 to the KT potential (13). The only technical part of the proof, in Appendix F, is an upper bound on F ∗t since it cannot be expressed as an elementary function. Corollary 5 (Regret Bound for Algorithm 1). Let > 0. Let {gt}∞t=1 be any sequence of reward vectors in a Hilbert spaceH such that ‖gt‖ ≤ 1. Then Algorithm 1 satisfies\n∀T ≥ 0 ∀u ∈ H RegretT (u) ≤ ‖u‖ √ T ln ( 1 + 24T 2‖u‖2 2 ) + ( 1− 1 e √ πT ) .\nAlgorithm 1 Algorithm for OLO over Hilbert spaceH based on KT potential Require: Initial endowment > 0\n1: for t = 1, 2, . . . do 2: Predict with wt ← 1t ( + ∑t−1 i=1〈gi, wi〉 )∑t−1 i=1 gi 3: Receive reward vector gt ∈ H such that ‖gt‖ ≤ 1 4: end for\nAlgorithm 2 Algorithm for Learning with Expert Advice based on δ-shifted KT potential Require: Number of experts N , prior distribution π ∈ ∆N , number of rounds T\n1: for t = 1, 2, . . . , T do 2: For each i ∈ [N ], set wt,i ← ∑t−1 j=1 g̃j,i\nt+T/2\n( 1 + ∑t−1 j=1 g̃j,iwj,i ) 3: For each i ∈ [N ], set p̂t,i ← πi[wt,i]+\n4: Predict with pt ← { p̂t/ ‖p̂t‖1 if ‖p̂t‖1 > 0 π if ‖p̂t‖1 = 0 5: Receive reward vector gt ∈ [0, 1]N\n6: For each i ∈ [N ], set g̃t,i ← { gt,i − 〈gt, pt〉 if wt,i > 0 [gt,i − 〈gt, pt〉]+ if wt,i ≤ 0 7: end for\nIt is worth noting the elegance and extreme simplicity of Algorithm 1 and contrast it with the algorithms in [26, 22–24]. Also, the regret bound is optimal [26, 23]. The parameter can be safely set to any constant, e.g. 1. Its role is equivalent to the initial guess used in doubling tricks [25]."
    }, {
      "heading" : "7.2 Learning with Expert Advice",
      "text" : "We will now construct an algorithm for LEA based on the δ-shifted KT potential. We set δ to T/2, requiring the algorithm to know the number of rounds T in advance; we will fix this later with the standard doubling trick.\nTo use the construction in Section 6, we need an OLO algorithm for the 1-d Hilbert space R. Using the δ-shifted KT potentials, the algorithm predicts for any sequence {g̃t}∞t=1 of reward\nwt = βt Wealtht−1 = βt 1 + t−1∑ j=1 g̃jwj  = ∑t−1i=1 g̃i T/2 + t 1 + t−1∑ j=1 g̃jwj  . Then, following the construction in Section 6, we arrive at the final algorithm, Algorithm 2. We can derive a regret bound for Algorithm 2 by applying Theorem 4 to the δ-shifted KT potential. Corollary 6 (Regret Bound for Algorithm 2). Let N ≥ 2 and T ≥ 0 be integers. Let π ∈ ∆N be a prior. Then Algorithm 2 with inputN, π, T for any rewards vectors g1, g2, . . . , gT ∈ [0, 1]N satisfies\n∀u ∈ ∆N RegretT (u) ≤ √ 3T (3 + D (u‖π)) .\nHence, the Algorithm 2 has both the best known guarantee on worst-case regret and per-round time complexity, see Table 1. Also, it has the advantage of being very simple.\nThe proof of the corollary is in the Appendix F. The only technical part of the proof is an upper bound on f−1t (x), which we conveniently do by lower bounding Ft(x).\nThe reason for using the shifted potential comes from the analysis of f−1t (x). The unshifted algorithm would have a O( √ T (log T + D (u‖π)) regret bound; the shifting improves the bound to\nO( √ T (1 + D (u‖π)). By changing T/2 in Algorithm 2 to another constant fraction of T , it is possible to trade-off between the two constants 3 present in the square root in the regret upper bound.\nThe requirement of knowing the number of rounds T in advance can be lifted by the standard doubling trick [25, Section 2.3.1], obtaining an anytime guarantee with a bigger leading constant,\n∀T ≥ 0 ∀u ∈ ∆N RegretT (u) ≤ √ 2√ 2−1\n√ 3T (3 + D (u‖π)) ."
    }, {
      "heading" : "8 Discussion of the Results",
      "text" : "We have presented a new interpretation of parameter-free algorithms as coin betting algorithms. This interpretation, far from being just a mathematical gimmick, reveals the common hidden structure of previous parameter-free algorithms for both OLO and LEA and also allows the design of new algorithms. For example, we show that the characteristic of parameter-freeness is just a consequence of having an algorithm that guarantees the maximum reward possible. The reductions in Sections 5 and 6 are also novel and they are in a certain sense optimal. In fact, the obtained Algorithms 1 and 2 achieve the optimal worst case upper bounds on the regret, see [26, 23] and [4] respectively.\nWe have also run an empirical evaluation to show that the theoretical difference between classic online learning algorithms and parameter-free ones is real and not just theoretical. In Figure 1, we have used three regression datasets4, and solved the OCO problem through OLO. In all the three cases, we have used the absolute loss and normalized the input vectors to have L2 norm equal to 1. From the empirical results, it is clear that the optimal learning rate is completely data-dependent, yet parameter-free algorithms have performance very close to the unknown optimal tuning of the learning rate. Moreover, the KT-based Algorithm 1 seems to dominate all the other similar algorithms.\nFor LEA, we have used the synthetic setting in [6]. The dataset is composed of Hadamard matrices of size 64, where the row with constant values is removed, the rows are duplicated to 126 inverting their signs, 0.025 is subtracted to k rows, and the matrix is replicated in order to generate T = 32768 samples. For more details, see [6]. Here, the KT-based algorithm is the one in Algorithm 2, where the term T/2 is removed, so that the final regret bound has an additional lnT term. Again, we see that the parameter-free algorithms have a performance close or even better than Hedge with an oracle tuning of the learning rate, with no clear winners among the parameter-free algorithms.\nNotice that since the adaptive Kelly strategy based on KT estimator is very close to optimal, the only possible improvement is to have a data-dependent bound, for example like the ones in [24, 15, 19]. In future work, we will extend our definitions and reductions to the data-dependent case.\n4Datasets available at https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.\nAcknowledgments. The authors thank Jacob Abernethy, Nicolò Cesa-Bianchi, Satyen Kale, Chansoo Lee, Giuseppe Molteni, and Manfred Warmuth for useful discussions on this work."
    } ],
    "references" : [ {
      "title" : "The Gamma Function",
      "author" : [ "E. Artin" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1964
    }, {
      "title" : "Inequalities for the gamma function",
      "author" : [ "N. Batir" ],
      "venue" : "Archiv der Mathematik,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
      "author" : [ "H.H. Bauschke", "P.L. Combettes" ],
      "venue" : "Springer Publishing Company, Incorporated,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "How to use expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1997
    }, {
      "title" : "A parameter-free hedging algorithm",
      "author" : [ "K. Chaudhuri", "Y. Freund", "D. Hsu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Inequalities for the polygamma functions with application",
      "author" : [ "C.-P. Chen" ],
      "venue" : "General Mathematics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Prediction with advice of unknown number of experts",
      "author" : [ "A. Chernov", "V. Vovk" ],
      "venue" : "In Proc. of the 26th Conf. on Uncertainty in Artificial Intelligence. AUAI Press,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Elements of Information Theory",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Adaptive online learning",
      "author" : [ "D.J. Foster", "A. Rakhlin", "K. Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "J. Computer and System Sciences,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1997
    }, {
      "title" : "Inequalities on the Lambert W function and hyperpower function",
      "author" : [ "A. Hoorfar", "M. Hassani" ],
      "venue" : "J. Inequal. Pure and Appl. Math,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "A new interpretation of information rate",
      "author" : [ "J.L. Kelly" ],
      "venue" : "Information Theory, IRE Trans. on,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1956
    }, {
      "title" : "Second-order quantile methods for experts and combinatorial games",
      "author" : [ "W.M. Koolen", "T. van Erven" ],
      "venue" : "In Proc. of the 28th Conf. on Learning Theory,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "The performance of universal encoding",
      "author" : [ "R.E. Krichevsky", "V.K. Trofimov" ],
      "venue" : "IEEE Trans. on Information Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1981
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "N. Littlestone", "M.K. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1994
    }, {
      "title" : "A drifting-games analysis for online learning and applications to boosting",
      "author" : [ "H. Luo", "R.E. Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Achieving all with no parameters: AdaNormalHedge",
      "author" : [ "H. Luo", "R.E. Schapire" ],
      "venue" : "In Proc. of the 28th Conf. on Learning Theory,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "A PAC-Bayesian tutorial with a dropout",
      "author" : [ "D. McAllester" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Minimax optimal algorithms for unconstrained linear optimization",
      "author" : [ "H.B. McMahan", "J. Abernethy" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Unconstrained online linear learning in Hilbert spaces: Minimax algorithms and normal approximations",
      "author" : [ "H.B. McMahan", "F. Orabona" ],
      "venue" : "In Proc. of the 27th Conf. on Learning Theory,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Dimension-free exponentiated gradient",
      "author" : [ "F. Orabona" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Simultaneous model selection and optimization through parameter-free stochastic learning",
      "author" : [ "F. Orabona" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "S. Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "No-regret algorithms for unconstrained online convex optimization",
      "author" : [ "M. Streeter", "B. McMahan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "A game of prediction with expert advice",
      "author" : [ "V. Vovk" ],
      "venue" : "J. Computer and System Sciences,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1998
    }, {
      "title" : "A Course of Modern Analysis",
      "author" : [ "E.T. Whittaker", "G.N. Watson" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1962
    }, {
      "title" : "The context tree weighting method: Basic properties",
      "author" : [ "F.M.J. Willems", "Y.M. Shtarkov", "T.J. Tjalkens" ],
      "venue" : "IEEE Trans. on Information Theory,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "1 Introduction We consider the Online Linear Optimization (OLO) [4, 25] setting.",
      "startOffset" : 64,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : "1 Introduction We consider the Online Linear Optimization (OLO) [4, 25] setting.",
      "startOffset" : 64,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : "For example, Online Convex Optimization (OCO), the problem analogous to OLO where 〈gt, u〉 is generalized to an arbitrary convex function `t(u), is solved through a reduction to OLO [25].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 15,
      "context" : "LEA [17, 27, 5] provides a way of combining classifiers and it is at the heart of boosting [12].",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 25,
      "context" : "LEA [17, 27, 5] provides a way of combining classifiers and it is at the heart of boosting [12].",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : "LEA [17, 27, 5] provides a way of combining classifiers and it is at the heart of boosting [12].",
      "startOffset" : 4,
      "endOffset" : 15
    }, {
      "referenceID" : 10,
      "context" : "LEA [17, 27, 5] provides a way of combining classifiers and it is at the heart of boosting [12].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "Batch and stochastic convex optimization can also be solved through a reduction to OLO [25].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 24,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 21,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 19,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 20,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "Recently, new parameter-free algorithms have been proposed, both for LEA [6, 8, 18, 19, 15, 11] and for OLO/OCO over Hilbert spaces [26, 23, 21, 22, 24].",
      "startOffset" : 132,
      "endOffset" : 152
    }, {
      "referenceID" : 9,
      "context" : "[11] proposed a unified framework, but it is not constructive.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "Algorithm Worst-case regret guarantee Per-round time complexity Adaptive Unified analysis OGD, η = 1 √ T [25] O((1 + ‖u‖(2)) √ T ), ∀u ∈ H O(1) OGD, η = U √ T [25] U √ T for any u ∈ H s.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 23,
      "context" : "Algorithm Worst-case regret guarantee Per-round time complexity Adaptive Unified analysis OGD, η = 1 √ T [25] O((1 + ‖u‖(2)) √ T ), ∀u ∈ H O(1) OGD, η = U √ T [25] U √ T for any u ∈ H s.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : "‖u‖ ≤ U O(1) [23] O(‖u‖ ln(1 + ‖u‖T ) √ T ), ∀u ∈ H O(1) X [22, 24] O(‖u‖ √ T ln(1 + ‖u‖T )), ∀u ∈ H O(1) X This paper, Sec.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : "‖u‖ ≤ U O(1) [23] O(‖u‖ ln(1 + ‖u‖T ) √ T ), ∀u ∈ H O(1) X [22, 24] O(‖u‖ √ T ln(1 + ‖u‖T )), ∀u ∈ H O(1) X This paper, Sec.",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "‖u‖ ≤ U O(1) [23] O(‖u‖ ln(1 + ‖u‖T ) √ T ), ∀u ∈ H O(1) X [22, 24] O(‖u‖ √ T ln(1 + ‖u‖T )), ∀u ∈ H O(1) X This paper, Sec.",
      "startOffset" : 59,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "Hedge, η = √ lnN T , πi = 1 N [12] O( √ T lnN), ∀u ∈ ∆N O(N) Hedge, η = U √ T [12] O(U √ T ) for any u ∈ ∆N s.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "Hedge, η = √ lnN T , πi = 1 N [12] O( √ T lnN), ∀u ∈ ∆N O(N) Hedge, η = U √ T [12] O(U √ T ) for any u ∈ ∆N s.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 5,
      "context" : "√ D (u‖π) ≤ U O(N) [6] O( √ T (1 + D (u‖π)) + ln(2)N), ∀u ∈ ∆N O(N K)1 X [8] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N K)1 X [8, 19, 15]2 O( √ T (ln lnT + D (u‖π))), ∀u ∈ ∆N O(N) X [11] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N ln maxu∈∆N D (u‖π))3 X X This paper, Sec.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "√ D (u‖π) ≤ U O(N) [6] O( √ T (1 + D (u‖π)) + ln(2)N), ∀u ∈ ∆N O(N K)1 X [8] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N K)1 X [8, 19, 15]2 O( √ T (ln lnT + D (u‖π))), ∀u ∈ ∆N O(N) X [11] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N ln maxu∈∆N D (u‖π))3 X X This paper, Sec.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "√ D (u‖π) ≤ U O(N) [6] O( √ T (1 + D (u‖π)) + ln(2)N), ∀u ∈ ∆N O(N K)1 X [8] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N K)1 X [8, 19, 15]2 O( √ T (ln lnT + D (u‖π))), ∀u ∈ ∆N O(N) X [11] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N ln maxu∈∆N D (u‖π))3 X X This paper, Sec.",
      "startOffset" : 118,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "√ D (u‖π) ≤ U O(N) [6] O( √ T (1 + D (u‖π)) + ln(2)N), ∀u ∈ ∆N O(N K)1 X [8] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N K)1 X [8, 19, 15]2 O( √ T (ln lnT + D (u‖π))), ∀u ∈ ∆N O(N) X [11] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N ln maxu∈∆N D (u‖π))3 X X This paper, Sec.",
      "startOffset" : 118,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "√ D (u‖π) ≤ U O(N) [6] O( √ T (1 + D (u‖π)) + ln(2)N), ∀u ∈ ∆N O(N K)1 X [8] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N K)1 X [8, 19, 15]2 O( √ T (ln lnT + D (u‖π))), ∀u ∈ ∆N O(N) X [11] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N ln maxu∈∆N D (u‖π))3 X X This paper, Sec.",
      "startOffset" : 118,
      "endOffset" : 129
    }, {
      "referenceID" : 9,
      "context" : "√ D (u‖π) ≤ U O(N) [6] O( √ T (1 + D (u‖π)) + ln(2)N), ∀u ∈ ∆N O(N K)1 X [8] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N K)1 X [8, 19, 15]2 O( √ T (ln lnT + D (u‖π))), ∀u ∈ ∆N O(N) X [11] O( √ T (1 + D (u‖π))), ∀u ∈ ∆N O(N ln maxu∈∆N D (u‖π))3 X X This paper, Sec.",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 13,
      "context" : "(2)The proof in [15] can be modified to prove a KL bound, see http://blog.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "(3)A variant of the algorithm in [11] can be implemented with the stated time complexity [10].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "Lemma 1 (Reward-Regret relationship [22]).",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "coin flips, an optimal strategy has been proposed by Kelly [14].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 12,
      "context" : "He showed that, in the long run, this strategy will provide more wealth than betting any other fixed fraction of the current wealth [14].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "As observed in [21], the optimal fixed fraction to bet is β∗ = ( ∑T t=1 gt)/T and it gives the wealth WealthT (β ∗) = exp ( T ·D ( 1 2 + ∑T t=1 gt 2T ∥∥∥ 12)) ≥ exp( (∑Tt=1 gt)(2) 2T ) , (4)",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : "This problem was studied by Krichevsky and Trofimov [16], who proposed that after seeing the coin flips g1, g2, .",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 3,
      "context" : "This guarantee is optimal up to constant factors [4] and mirrors the guarantee of the Kelly bet.",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "In fact, Ft(x) = exp ( x(2)/(2t) ) / √ t, essentially the potential used in the parameter-free algorithms in [22, 24] for OLO and in [6, 18, 19] for LEA, approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 22,
      "context" : "In fact, Ft(x) = exp ( x(2)/(2t) ) / √ t, essentially the potential used in the parameter-free algorithms in [22, 24] for OLO and in [6, 18, 19] for LEA, approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B.",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "In fact, Ft(x) = exp ( x(2)/(2t) ) / √ t, essentially the potential used in the parameter-free algorithms in [22, 24] for OLO and in [6, 18, 19] for LEA, approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B.",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "In fact, Ft(x) = exp ( x(2)/(2t) ) / √ t, essentially the potential used in the parameter-free algorithms in [22, 24] for OLO and in [6, 18, 19] for LEA, approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B.",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 17,
      "context" : "In fact, Ft(x) = exp ( x(2)/(2t) ) / √ t, essentially the potential used in the parameter-free algorithms in [22, 24] for OLO and in [6, 18, 19] for LEA, approximates (4) and it is an excellent coin betting potential—see Theorem 9 in Appendix B.",
      "startOffset" : 133,
      "endOffset" : 144
    }, {
      "referenceID" : 14,
      "context" : "This potential was used to prove regret bounds for online prediction with the logarithmic loss [16][4, Chapter 9.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "Also, the regret bound is optimal [26, 23].",
      "startOffset" : 34,
      "endOffset" : 42
    }, {
      "referenceID" : 21,
      "context" : "Also, the regret bound is optimal [26, 23].",
      "startOffset" : 34,
      "endOffset" : 42
    }, {
      "referenceID" : 23,
      "context" : "Its role is equivalent to the initial guess used in doubling tricks [25].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "Figure 1: Total loss versus learning rate parameter of OGD (in log scale), compared with parameter-free algorithms DFEG [23], Adaptive Normal [22], PiSTOL [24] and the KT-based Algorithm 1.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "Figure 1: Total loss versus learning rate parameter of OGD (in log scale), compared with parameter-free algorithms DFEG [23], Adaptive Normal [22], PiSTOL [24] and the KT-based Algorithm 1.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 22,
      "context" : "Figure 1: Total loss versus learning rate parameter of OGD (in log scale), compared with parameter-free algorithms DFEG [23], Adaptive Normal [22], PiSTOL [24] and the KT-based Algorithm 1.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "The competitor algorithms are NormalHedge [6], AdaNormalHedge [19], Squint [15], and the KT-based Algorithm 2.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "The competitor algorithms are NormalHedge [6], AdaNormalHedge [19], Squint [15], and the KT-based Algorithm 2.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 13,
      "context" : "The competitor algorithms are NormalHedge [6], AdaNormalHedge [19], Squint [15], and the KT-based Algorithm 2.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "In fact, the obtained Algorithms 1 and 2 achieve the optimal worst case upper bounds on the regret, see [26, 23] and [4] respectively.",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "In fact, the obtained Algorithms 1 and 2 achieve the optimal worst case upper bounds on the regret, see [26, 23] and [4] respectively.",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "In fact, the obtained Algorithms 1 and 2 achieve the optimal worst case upper bounds on the regret, see [26, 23] and [4] respectively.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "For LEA, we have used the synthetic setting in [6].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : "Notice that since the adaptive Kelly strategy based on KT estimator is very close to optimal, the only possible improvement is to have a data-dependent bound, for example like the ones in [24, 15, 19].",
      "startOffset" : 188,
      "endOffset" : 200
    }, {
      "referenceID" : 13,
      "context" : "Notice that since the adaptive Kelly strategy based on KT estimator is very close to optimal, the only possible improvement is to have a data-dependent bound, for example like the ones in [24, 15, 19].",
      "startOffset" : 188,
      "endOffset" : 200
    }, {
      "referenceID" : 17,
      "context" : "Notice that since the adaptive Kelly strategy based on KT estimator is very close to optimal, the only possible improvement is to have a data-dependent bound, for example like the ones in [24, 15, 19].",
      "startOffset" : 188,
      "endOffset" : 200
    } ],
    "year" : 2016,
    "abstractText" : "In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice. These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices. We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.",
    "creator" : "LaTeX with hyperref package"
  }
}