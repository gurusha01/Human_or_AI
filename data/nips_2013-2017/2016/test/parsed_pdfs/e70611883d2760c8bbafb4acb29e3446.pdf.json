{
  "name" : "e70611883d2760c8bbafb4acb29e3446.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Kernels with Random Features",
    "authors" : [ "Aman Sinha", "John Duchi" ],
    "emails" : [ "amans@stanford.edu", "jduchi@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "An essential element of supervised learning systems is the representation of input data. Kernel methods [27] provide one approach to this problem: they implicitly transform the data to a new feature space, allowing non-linear data representations. This representation comes with a cost, as kernelized learning algorithms require time that grows at least quadratically in the data set size, and predictions with a kernelized procedure require the entire training set. This motivated Rahimi and Recht [24, 25] to develop randomized methods that efficiently approximate kernel evaluations with explicit feature transformations; this approach gives substantial computational benefits for large training sets and allows the use of simple linear models in the randomly constructed feature space.\nWhether we use standard kernel methods or randomized approaches, using the “right” kernel for a problem can make the difference between learning a useful or useless model. Standard kernel methods as well as the aforementioned randomized-feature techniques assume the input of a user-defined kernel—a weakness if we do not a priori know a good data representation. To address this weakness, one often wishes to learn a good kernel, which requires substantial computation. We combine kernel learning with randomization, exploiting the computational advantages offered by randomized features to learn the kernel in a supervised manner. Specifically, we use a simple pre-processing stage for selecting our random features rather than jointly optimizing over the kernel and model parameters. Our workflow is straightforward: we create randomized features, solve a simple optimization problem to select a subset, then train a model with the optimized features. The procedure results in lowerdimensional models than the original random-feature approach for the same performance. We give empirical evidence supporting these claims and provide theoretical guarantees that our procedure is consistent with respect to the limits of infinite training data and infinite-dimensional random features."
    }, {
      "heading" : "1.1 Related work",
      "text" : "To discuss related work, we first describe the supervised learning problem underlying our approach. We have a cost c : R× Y → R, where c(·, y) is convex for y ∈ Y , and a reproducing kernel Hilbert space (RKHS) of functions F with kernel K. Given a sample {(xi, yi)}ni=1, the usual `2-regularized\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nlearning problem is to solve the following (shown in primal and dual forms respectively):\nminimize f∈F n∑ i=1 c(f(xi), yi) + λ 2 ‖f‖22 , or maximizeα∈Rn − n∑ i=1 c∗(αi, y i)− 1 2λ αTGα, (1)\nwhere ‖·‖2 denotes the Hilbert space norm, c∗(α, y) = supz{αz − c(z, y)} is the convex conjugate of c (for fixed y) and G = [K(xi, xj)]ni,j=1 denotes the Gram matrix.\nSeveral researchers have studied kernel learning. As noted by Gönen and Alpaydın [14], most formulations fall into one of a few categories. In the supervised setting, one assumes a base class or classes of kernels and either uses heuristic rules to combine kernels [2, 23], optimizes structured (e.g. linear, nonnegative, convex) compositions of the kernels with respect to an alignment metric [9, 16, 20, 28], or jointly optimizes kernel compositions with empirical risk [17, 20, 29]. The latter approaches require an eigendecomposition of the Gram matrix or costly optimization problems (e.g. quadratic or semidefinite programs) [10, 14], but these models have a variety of generalization guarantees [1, 8, 10, 18, 19]. Bayesian variants of compositional kernel search also exist [12, 13]. In un- and semi-supervised settings, the goal is to learn an embedding of the input distribution followed by a simple classifier in the embedded space (e.g. [15]); the hope is that the input distribution carries the structure relevant to the task. Despite the current popularity of these techniques, especially deep neural architectures, they are costly, and it is difficult to provide guarantees on their performance.\nOur approach optimizes kernel compositions with respect to an alignment metric, but rather than work with Gram matrices in the original data representation, we work with randomized feature maps that approximate RKHS embeddings. We learn a kernel that is structurally different from a user-supplied base kernel, and our method is an efficiently (near linear-time) solvable convex program."
    }, {
      "heading" : "2 Proposed approach",
      "text" : "At a high level, we take a feature mapping, find a distribution that aligns this mapping with the labels y, and draw random features from the learned distribution; we then use these features in a standard supervised learning approach.\nFor simplicity, we focus on binary classification: we have n datapoints (xi, yi) ∈ Rd × {−1, 1}. Letting φ : Rd ×W → [−1, 1] and Q be a probability measure on a spaceW , define the kernel\nKQ(x, x ′) := ∫ φ(x,w)φ(x′, w)dQ(w). (2)\nWe want to find the “best” kernel KQ over all distributions Q in some (large, nonparametric) set P of possible distributions on random features; we consider a kernel alignment problem of the form\nmaximize Q∈P ∑ i,j KQ(x i, xj)yiyj . (3)\nWe focus on sets P defined by divergence measures on the space of probability distributions. For a convex function f with f(1) = 0, the f -divergence between distributions P and Q is Df (P ||Q) = ∫ f( dPdQ )dQ. Then, for a base (user-defined) distribution P0, we consider collections P := {Q : Df (Q||P0) ≤ ρ} where ρ > 0 is a specified constant. In this paper, we focus on divergences f(t) = tk − 1 for k ≥ 2. Intuitively, the distribution Q maximizing the alignment (3) gives a feature space in which pairwise distances are similar to those in the output space Y . Unfortunately, the problem (3) is generally intractable as it is infinite dimensional.\nUsing the randomized feature approach, we approximate the integral (2) as a discrete sum over samples W i iid∼ P0, i ∈ [Nw]. Defining the discrete approximation PNw := {q : Df (q||1/Nw) ≤ ρ} to P , we have the following empirical version of problem (3):\nmaximize q∈PNw ∑ i,j yiyj Nw∑ m=1 qmφ(x i, wm)φ(xj , wm). (4)\nUsing randomized features, matching the input and output distances in problem (4) translates to finding a (weighted) set of points among w1, w2, ..., wNw that best “describe” the underlying dataset, or, more directly, finding weights q so that the kernel matrix matches the correlation matrix yyT .\nGiven a solution q̂ to problem (4), we can solve the primal form of problem (1) in two ways. First, we can apply the Rahimi and Recht [24] approach by drawing D samples W 1, . . . ,WD iid∼ q̂, defining features φi = [φ(xi, w1) · · · φ(xi, wD)]T , and solving the risk minimization problem\nθ̂ = argmin θ { n∑ i=1 c ( 1√ D θTφi, yi ) + r(θ) } (5)\nfor some regularization r. Alternatively, we may set φi = [φ(xi, w1) · · · φ(xi, wNw)]T , where w1, . . . , wNw are the original random samples from P0 used to solve (4), and directly solve\nθ̂ = argmin θ { n∑ i=1 c(θT diag(q̂) 1 2φi, yi) + r(θ) } . (6)\nNotably, if q̂ is sparse, the problem (6) need only store the random features corresponding to non-zero entries of q̂. Contrast our two-phase procedure to that of Rahimi and Recht [25], which samples W 1, . . . ,WD iid∼ P0 and solves the minimization problem\nminimize α∈RNw n∑ i=1 c ( D∑ m=1 αmφ(x i, wm), yi ) subject to ‖α‖∞ ≤ C/Nw, (7)\nwhere C is a numerical constant. At first glance, it appears that we may suffer both in terms of computational efficiency and in classification or learning performance compared to the one-step procedure (7). However, as we show in the sequel, the alignment problem (4) can be solved very efficiently and often yields sparse vectors q̂, thus substantially decreasing the dimensionality of problem (6). Additionally, we give experimental evidence in Section 4 that the two-phase procedure yields generalization performance similar to standard kernel and randomized feature methods.\n2.1 Efficiently solving problem (4)\nThe optimization problem (4) has structure that enables efficient (near linear-time) solutions. Define the matrix Φ = [φ1 · · · φn] ∈ RNw×n, where φi = [φ(xi, w1) · · · φ(xi, wNw)]T ∈ RNw is the randomized feature representation for xi and wm iid∼ P0. We can rewrite the optimization objective as∑\ni,j yiyj Nw∑ m=1 qmφ(x i, wm)φ(xj , wm) = Nw∑ m=1 qm ( n∑ i=1 yiφ(xi, wm) )2 = qT ((Φy) (Φy)) ,\nwhere denotes the Hadamard product. Constructing the linear objective requires the evaluation of Φy. Assuming that the computation of φ isO(d), construction of Φ isO(nNwd) on a single processor. However, this construction is trivially parallelizable. Furthermore, computation can be sped up even further for certain distributions P0. For example, the Fastfood technique can approximate Φ in O(nNw log(d)) time for the Gaussian kernel [21].\nThe problem (4) is also efficiently solvable via bisection over a scalar dual variable. Using λ ≥ 0 for the constraint Df (Q||P0) ≤ ρ, a partial Lagrangian is\nL(q, λ) = qT ((Φy) (Φy))− λ (Df (q||1/Nw)− ρ) .\nThe corresponding dual function is g(λ) = supq∈∆ L(q, λ), where ∆ := {q ∈ R Nw + : q T1 = 1} is the probability simplex. Minimizing g(λ) yields the solution to problem (4); this is a convex optimization problem in one dimension so we can use bisection. The computationally expensive step in each iteration is maximizing L(q, λ) with respect to q for a given λ. For f(t) = tk − 1, we define v := (Φy) (Φy) and solve\nmaximize q∈∆ qT v − λ 1 Nw Nw∑ m=1 (Nwqm) k. (8)\nThis has a solution of the form qm = [ vm/λN k−1 w + τ ] 1 k−1 + , where τ is chosen so that ∑ m qm = 1. We can find such a τ by a variant of median-based search in O(Nw) time [11]. Thus, for any k ≥ 2, an -suboptimal solution to problem (4) can be found in O(Nw log(1/ )) time (see Algorithm 1).\nAlgorithm 1 Kernel optimization with f(t) = tk − 1 as divergence INPUT: distribution P0 onW , sample {(xi, yi)}ni=1, Nw ∈ N, feature function φ, > 0 OUTPUT: q ∈ RNw that is an -suboptimal solution to (4). SETUP: Draw Nw samples wm\niid∼ P0, build feature matrix Φ, compute v := (Φy) (Φy). Set λu ←∞, λl ← 0, λs ← 1 while λu =∞ q ← argmaxq∈∆ L(q, λs) // (solution to problem (8)) if Df (q||1/Nw) < ρ then λu ← λs else λs ← 2λs while λu − λl > λs λ← (λu + λl)/2 q ← argmaxq∈∆ L(q, λ) // (solution to problem (8)) if Df (q||1/Nw) < ρ then λu ← λ else λl ← λ"
    }, {
      "heading" : "3 Consistency and generalization performance guarantees",
      "text" : "Although the procedure (4) is a discrete approximation to a heuristic kernel alignment problem, we can provide guarantees on its performance as well as the generalization performance of our subsequent model trained with the optimized kernel.\nConsistency First, we provide guarantees that the solution to problem (4) approaches a population optimum as the data and random sampling increase (n → ∞ and Nw → ∞, respectively). We consider the following (slightly more general) setting: let S : X × X → [−1, 1] be a bounded function, where we intuitively think of S(x, x′) as a similarity metric between labels for x and x′, and denote Sij := S(xi, xj) (in the binary case with y ∈ {−1, 1}, we have Sij = yiyj). We then define the alignment functions\nT (P ) := E[S(X,X ′)KP (X,X ′)], T̂ (P ) := 1 n(n− 1) ∑ i 6=j SijKP (x i, xj),\nwhere the expectation is taken over S and the independent variables X,X ′. Lemmas 1 and 2 provide consistency guarantees with respect to the data sample (xi and Sij) and the random feature sample (wm); together they give us the overall consistency result of Theorem 1. We provide proofs in the supplement (Sections A.1, A.2, and A.3 respectively). Lemma 1 (Consistency with respect to data). Let f(t) = tk−1 for k ≥ 2. Let P0 be any distribution on the spaceW , and let P = {Q : Df (Q||P0) ≤ ρ}. Then\nP (\nsup Q∈P ∣∣∣∣T̂ (Q)− T (Q)∣∣∣∣ ≥ t) ≤ √2 exp(− nt216(1 + ρ) ) .\nLemma 1 shows that the empirical quantity T̂ is close to the true T . Now we show that, independent of the size of the training data, we can consistently estimate the optimal Q ∈ P via sampling (i.e. Q ∈ PNw ). Lemma 2 (Consistency with respect to sampling features). Let the conditions of Lemma 1 hold. Then, with Cρ = 2(ρ+1)√ 1+ρ−1 and Dρ =\n√ 8(1 + ρ), we have∣∣∣∣ sup\nQ∈PNw T̂ (Q)− sup Q∈P T̂ (Q)\n∣∣∣∣ ≤ 4Cρ √ log(2Nw)\nNw +Dρ √ log 2δ Nw\nwith probability at least 1− δ over the draw of the samples Wm iid∼ P0.\nFinally, we combine the consistency guarantees for data and sampling to reach our main result, which shows that the alignment provided by the estimated distribution Q̂ is nearly optimal.\nTheorem 1. Let Q̂w maximize T̂ (Q) over Q ∈ PNw . Then, with probability at least 1− 3δ over the sampling of both (x, y) and W , we have∣∣∣∣T (Q̂w)− sup\nQ∈P T (Q)\n∣∣∣∣ ≤ 4Cρ √ log(2Nw)\nNw +Dρ √ log 2δ Nw + 2Dρ √ 2 log 2δ n .\nGeneralization performance The consistency results above show that our optimization procedure nearly maximizes alignment T (P ), but they say little about generalization performance for our model trained using the optimized kernel. We now show that the class of estimators employed by our method has strong performance guarantees. By construction, our estimator (6) uses the function class\nFNw := { h(x) = Nw∑ m=1 αm √ qmφ(x,w m) | q ∈ PNw , ‖α‖2 ≤ B } ,\nand we provide bounds on its generalization via empirical Rademacher complexity. To that end, define Rn(FNw) := 1nE[supf∈FNw ∑n i=1 σif(x\ni)], where the expectation is taken over the i.i.d. Rademacher variables σi ∈ {−1, 1}. We have the following lemma, whose proof is in Section A.4.\nLemma 3. Under the conditions of the preceding paragraph,Rn(FNw) ≤ B √ 2(1+ρ) n .\nApplying standard concentration results, we obtain the following generalization guarantee. Theorem 2 ([8, 18]). Let the true misclassification risk and ν-empirical misclassification risk for an estimator h be defined as follows:\nR(h) := P(Y h(X) < 0), R̂ν(h) := 1\nn n∑ i=1 min { 1, [ 1− yh(xi)/ν ] + } .\nThen suph∈FNw {R(h)− R̂ν(h)} ≤ 2 νRn(FNw) + 3 √ log 2δ 2n with probability at least 1− δ.\nThe bound is independent of the number of terms Nw, though in practice we let B grow with Nw."
    }, {
      "heading" : "4 Empirical evaluations",
      "text" : "We now turn to empirical evaluations, comparing our approach’s predictive performance with that of Rahimi and Recht’s randomized features [24] as well as a joint optimization over kernel compositions and empirical risk. In each of our experiments, we investigate the effect of increasing dimensionality of the randomized feature space D. For our approach, we use the χ2-divergence (k = 2 or f(t) = t2 − 1). Letting q̂ denote the solution to problem (4), we use two variants of our approach: when D < nnz(q̂) we use estimator (5), and we use estimator (6) otherwise. For the original randomized feature approach, we relax the constraint in problem (7) with an `2 penalty. Finally, for the joint optimization in which we learn the kernel and classifier together, we consider the kernel-learning objective, i.e. finding the best Gram matrix G in problem (1) for the soft-margin SVM [14]:\nminimizeq∈PNw supα α T1− 12 ∑ i,j αiαjy iyj ∑Nw m=1 qmφ(x i, wm)φ(xj , wm)\nsubject to 0 α C1, αT y = 0. (9)\nWe use a standard primal-dual algorithm [4] to solve the min-max problem (9). While this is an expensive optimization, it is a convex problem and is solvable in polynomial time.\nIn Section 4.1, we visualize a particular problem that illustrates the effectiveness of our approach when the user-defined kernel is poor. Section 4.2 shows how learning the kernel can be used to quickly find a sparse set of features in high dimensional data, and Section 4.3 compares our performance with unoptimized random features and the joint procedure (9) on benchmark datasets. The supplement contains more experimental results in Section C."
    }, {
      "heading" : "4.1 Learning a new kernel with a poor choice of P0",
      "text" : "For our first experiment, we generate synthetic data xi iid∼ N(0, I) with labels yi = sign(‖x‖2− √ d), where x ∈ Rd. The Gaussian kernel is ill-suited for this task, as the Euclidean distance used in this kernel does not capture the underlying structure of the classes. Nevertheless, we use the Gaussian kernel, which corresponds [24] to φ(x, (w, v)) = cos((x, 1)T (w, v)) where (W,V ) ∼ N(0, I) × Uni(0, 2π), to showcase the effects of our method. We consider a training set of size n = 104 and a test set of size 103, and we employ logistic regression with D = nnz(q̂) for both our technique as well as the original random feature approach.1\n1For 2 ≤ d ≤ 15, nnz(q̂) < 250 when the kernel is trained with Nw = 2 · 104, and ρ = 200.\nFigure 1 shows the results of the experiments for d ∈ {2, . . . , 15}. Figure 1(a) illustrates the output of the optimization when d = 2. The selected kernel features wm lie near (1, 1) and (−1,−1); the offsets vm are near 0 and π, giving the feature φ(·, w, v) a parity flip. Thus, the kernel computes similarity between datapoints via neighborhoods of (1, 1) and (−1,−1) close to the classification boundary. In higher dimensions, this generalizes to neighborhoods of pairs of opposing points along the surface of the d-sphere; these features provide a coarse approximation to vector magnitude. Performance degradation with d occurs because the neighborhoods grow exponentially larger and less dense (due to fixed Nw and n). Nevertheless, as shown in Figure 1(b), this degradation occurs much more slowly than that of the Gaussian kernel, which suffers a similar curse of dimensionality due to its dependence on Euclidean distance. Although somewhat contrived, this example shows that even in situations with poor base kernels our approach learns a more suitable representation."
    }, {
      "heading" : "4.2 Feature selection and biological sequences",
      "text" : "In addition to the computational advantages rendered by the sparsity of q after performing the optimization (4), we can use this sparsity to gain insights about important features in high-dimensional datasets; this can act as an efficient filtering mechanism before further investigation. We present one example of this task, studying an aptamer selection problem [6]. In this task, we are given n = 2900 nucleotide sequences (aptamers) xi ∈ A81, where A = {A,C,G,T} and labels yi indicate (thresholded) binding affinity of the aptamer to a molecular target. We create one-hot encoded forms of k-grams of the sequence, where 1 ≤ k ≤ 5, resulting in d = ∑5 k=1 |A|k(82 − k) = 105,476\nfeatures. We consider the linear kernel, i.e. φ(x,w) = xw, where w ∼ Uni({1, . . . , d}). Figure 2(a) compares the misclassification error of our method with that of random k-gram features, while Figure 2(b) indicates the weights qi given to features by our method. In under 0.2 seconds, we whittle down the original feature space to 379 important features. By restricting random selection to just these features, we outperform the approach of selecting features uniformly at random when D d. More importantly, however, we can derive insights from this selection. For example, the circled features in Figure 2(b) correspond to k-gram prefixes for the 5-grams GGTTG and GTTGG at indices 60 through 64; G-complexes are known to be relevant for binding affinities in aptamers [6], so this is reasonable."
    }, {
      "heading" : "4.3 Performance on benchmark datasets",
      "text" : "We now show the benefits of our approach on large-scale datasets, since we exploit the efficiency of random features with the performance of kernel-learning techniques. We perform experiments on three distinct types of datasets, tracking training/test error rates as well as total (training + test) time. For the adult2 dataset we employ the Gaussian kernel with a logistic regression model, and for the reuters3 dataset we employ a linear kernel with a ridge regression model. For the buzz4 dataset we employ ridge regression with an arc-cosine kernel of order 2, i.e. P0 = N (0, I) and φ(x,w) = H(wTx)(wTx)2, where H(·) is the Heavyside step function [7].\n2https://archive.ics.uci.edu/ml/datasets/Adult 3http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm. We con-\nsider predicting whether a document has a CCAT label. 4http://ama.liglab.fr/data/buzz/classification/. We use the Twitter dataset.\nComparison with unoptimized random features Results comparing our method with unoptimized random features are shown in Figure 3 for many values of D, and Table 1 tabulates the best test error and corresponding time for the methods. Our method outperforms the original random feature approach in terms of generalization error for small and moderate values of D; at very large D the random feature approach either matches our surpasses our performance. The trends in speedup are opposite: our method requires extra optimizations that dominate training time at extremely small D; at very large D we use estimator (6), so our method requires less overall time. The nonmonotonic behavior for reuters (Figure 3(e)) occurs due to the following: at D . nnz(q̂), sampling indices from the optimized distribution takes a non-neglible fraction of total time, and solving the linear system requires more time when rows of Φ are not unique (due to sampling).\nPerformance improvements also depend on the kernel choice for a dataset. Namely, our method provides the most improvement, in terms of training time for a given amount of generalization error, over random features generated for the linear kernel on the reuters dataset; we are able to surpass the best results of the random feature approach 2 orders of magnitude faster. This makes sense when considering the ability of our method to sample from a small subset of important features. On the other hand, random features for the arc-cosine kernel are able to achieve excellent results on the buzz dataset even without optimization, so our approach only offers modest improvement at small to moderate D. For the Gaussian kernel employed on the adult dataset, our method is able to achieve the same generalization performance as random features in roughly 1/12 the training time.\nThus, we see that our optimization approach generally achieves competitive results with random features at lower computational costs, and it offers the most improvements when either the base kernel is not well-suited to the data or requires a large number of random features (large D) for good performance. In other words, our method reduces the sensitivity of model performance to the user’s selection of base kernels.\nComparison with joint optimization Despite the fact that we do not choose empirical risk as our objective in optimizing kernel compositions, our optimized kernel enjoys competitive generalization performance compared to the joint optimization procedure (9). Because the joint optimization is very costly, we consider subsampled training datasets of 5000 training examples. Results are shown in Table 2, where it is evident that the efficiency of our method outweighs the marginal gain in classification performance for joint optimization."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have developed a method to learn a kernel in a supervised manner using random features. Although we consider a kernel alignment problem similar to other approaches in the literature, we exploit computational advantages offered by random features to develop a much more efficient and scalable optimization procedure. Our concentration bounds guarantee the results of our optimization procedure closely match the limits of infinite data (n→∞) and sampling (Nw →∞), and our method produces models that enjoy good generalization performance guarantees. Empirical evaluations indicate that our optimized kernels indeed “learn” structure from data, and we attain competitive results on benchmark datasets at a fraction of the training time for other methods. Generalizing the theoretical results for concentration and risk to other f−divergences is the subject of further research. More broadly, our approach opens exciting questions regarding the usefulness of simple optimizations on random features in speeding up other traditionally expensive learning problems.\nAcknowledgements This research was supported by a Fannie & John Hertz Foundation Fellowship and a Stanford Graduate Fellowship."
    } ],
    "references" : [ {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Kernel methods for predicting protein–protein interactions",
      "author" : [ "A. Ben-Hur", "W.S. Noble" ],
      "venue" : "Bioinformatics, 21(suppl 1):i38–i46,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Robust solutions of optimization problems affected by uncertain probabilities",
      "author" : [ "A. Ben-Tal", "D. den Hertog", "A.D. Waegenaere", "B. Melenberg", "G. Rennen" ],
      "venue" : "Management Science,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Nonlinear Programming",
      "author" : [ "D. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1999
    }, {
      "title" : "Concentration Inequalities: a Nonasymptotic Theory of Independence",
      "author" : [ "S. Boucheron", "G. Lugosi", "P. Massart" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Quantitative selection and parallel characterization of aptamers",
      "author" : [ "M. Cho", "S.S. Oh", "J. Nie", "R. Stewart", "M. Eisenstein", "J. Chambers", "J.D. Marth", "F. Walker", "J.A. Thomson", "H.T. Soh" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Kernel methods for deep learning",
      "author" : [ "Y. Cho", "L.K. Saul" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Generalization bounds for learning kernels",
      "author" : [ "C. Cortes", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Algorithms for learning kernels based on centered alignment",
      "author" : [ "C. Cortes", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "On kernel target alignment",
      "author" : [ "N. Cristianini", "J. Kandola", "A. Elisseeff", "J. Shawe-Taylor" ],
      "venue" : "In Innovations in Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Efficient projections onto the `1-ball for learning in high dimensions",
      "author" : [ "J.C. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Structure discovery in nonparametric regression through compositional kernel search",
      "author" : [ "D. Duvenaud", "J.R. Lloyd", "R. Grosse", "J.B. Tenenbaum", "Z. Ghahramani" ],
      "venue" : "arXiv preprint arXiv:1302.4922,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Hierarchic bayesian models for kernel learning",
      "author" : [ "M. Girolami", "S. Rogers" ],
      "venue" : "In Proceedings of the 22nd international conference on Machine learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Multiple kernel learning algorithms",
      "author" : [ "M. Gönen", "E. Alpaydın" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Using deep belief nets to learn covariance kernels for gaussian processes",
      "author" : [ "G.E. Hinton", "R.R. Salakhutdinov" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Optimizing kernel alignment over combinations of kernel",
      "author" : [ "J. Kandola", "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Lp-norm multiple kernel learning",
      "author" : [ "M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Empirical margin distributions and bounding the generalization error of combined classifiers",
      "author" : [ "V. Koltchinskii", "D. Panchenko" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "Complexities of convex combinations and bounding the generalization error in classification",
      "author" : [ "V. Koltchinskii", "D. Panchenko" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Learning the kernel matrix with semidefinite programming",
      "author" : [ "G.R. Lanckriet", "N. Cristianini", "P. Bartlett", "L.E. Ghaoui", "M.I. Jordan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Fastfood-computing hilbert space expansions in loglinear time",
      "author" : [ "Q. Le", "T. Sarlós", "A. Smola" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Optimization by Vector",
      "author" : [ "D. Luenberger" ],
      "venue" : "Space Methods. Wiley,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1969
    }, {
      "title" : "A framework for multiple kernel support vector regression and its applications to sirna efficacy prediction",
      "author" : [ "S. Qiu", "T. Lane" ],
      "venue" : "Computational Biology and Bioinformatics, IEEE/ACM Transactions on,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Weighted sums of random kitchen sinks: replacing minimization with randomization in learning",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Concentration of measure inequalities for Markov chains and φ-mixing processes",
      "author" : [ "P. Samson" ],
      "venue" : "Annals of Probability,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2000
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2004
    }, {
      "title" : "Enhanced protein fold recognition through a novel data integration approach",
      "author" : [ "Y. Ying", "K. Huang", "C. Campbell" ],
      "venue" : "BMC bioinformatics,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2009
    }, {
      "title" : "Multiclass multiple kernel learning",
      "author" : [ "A. Zien", "C.S. Ong" ],
      "venue" : "In Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Kernel methods [27] provide one approach to this problem: they implicitly transform the data to a new feature space, allowing non-linear data representations.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "This motivated Rahimi and Recht [24, 25] to develop randomized methods that efficiently approximate kernel evaluations with explicit feature transformations; this approach gives substantial computational benefits for large training sets and allows the use of simple linear models in the randomly constructed feature space.",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 24,
      "context" : "This motivated Rahimi and Recht [24, 25] to develop randomized methods that efficiently approximate kernel evaluations with explicit feature transformations; this approach gives substantial computational benefits for large training sets and allows the use of simple linear models in the randomly constructed feature space.",
      "startOffset" : 32,
      "endOffset" : 40
    }, {
      "referenceID" : 13,
      "context" : "As noted by Gönen and Alpaydın [14], most formulations fall into one of a few categories.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "In the supervised setting, one assumes a base class or classes of kernels and either uses heuristic rules to combine kernels [2, 23], optimizes structured (e.",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "In the supervised setting, one assumes a base class or classes of kernels and either uses heuristic rules to combine kernels [2, 23], optimizes structured (e.",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "linear, nonnegative, convex) compositions of the kernels with respect to an alignment metric [9, 16, 20, 28], or jointly optimizes kernel compositions with empirical risk [17, 20, 29].",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "linear, nonnegative, convex) compositions of the kernels with respect to an alignment metric [9, 16, 20, 28], or jointly optimizes kernel compositions with empirical risk [17, 20, 29].",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "linear, nonnegative, convex) compositions of the kernels with respect to an alignment metric [9, 16, 20, 28], or jointly optimizes kernel compositions with empirical risk [17, 20, 29].",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 27,
      "context" : "linear, nonnegative, convex) compositions of the kernels with respect to an alignment metric [9, 16, 20, 28], or jointly optimizes kernel compositions with empirical risk [17, 20, 29].",
      "startOffset" : 93,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "linear, nonnegative, convex) compositions of the kernels with respect to an alignment metric [9, 16, 20, 28], or jointly optimizes kernel compositions with empirical risk [17, 20, 29].",
      "startOffset" : 171,
      "endOffset" : 183
    }, {
      "referenceID" : 19,
      "context" : "linear, nonnegative, convex) compositions of the kernels with respect to an alignment metric [9, 16, 20, 28], or jointly optimizes kernel compositions with empirical risk [17, 20, 29].",
      "startOffset" : 171,
      "endOffset" : 183
    }, {
      "referenceID" : 28,
      "context" : "linear, nonnegative, convex) compositions of the kernels with respect to an alignment metric [9, 16, 20, 28], or jointly optimizes kernel compositions with empirical risk [17, 20, 29].",
      "startOffset" : 171,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "quadratic or semidefinite programs) [10, 14], but these models have a variety of generalization guarantees [1, 8, 10, 18, 19].",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "quadratic or semidefinite programs) [10, 14], but these models have a variety of generalization guarantees [1, 8, 10, 18, 19].",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "quadratic or semidefinite programs) [10, 14], but these models have a variety of generalization guarantees [1, 8, 10, 18, 19].",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "quadratic or semidefinite programs) [10, 14], but these models have a variety of generalization guarantees [1, 8, 10, 18, 19].",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "quadratic or semidefinite programs) [10, 14], but these models have a variety of generalization guarantees [1, 8, 10, 18, 19].",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "quadratic or semidefinite programs) [10, 14], but these models have a variety of generalization guarantees [1, 8, 10, 18, 19].",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "quadratic or semidefinite programs) [10, 14], but these models have a variety of generalization guarantees [1, 8, 10, 18, 19].",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "Bayesian variants of compositional kernel search also exist [12, 13].",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "Bayesian variants of compositional kernel search also exist [12, 13].",
      "startOffset" : 60,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "[15]); the hope is that the input distribution carries the structure relevant to the task.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "First, we can apply the Rahimi and Recht [24] approach by drawing D samples W (1), .",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "Contrast our two-phase procedure to that of Rahimi and Recht [25], which samples W (1), .",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "For example, the Fastfood technique can approximate Φ in O(nNw log(d)) time for the Gaussian kernel [21].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "We can find such a τ by a variant of median-based search in O(Nw) time [11].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 23,
      "context" : "We now turn to empirical evaluations, comparing our approach’s predictive performance with that of Rahimi and Recht’s randomized features [24] as well as a joint optimization over kernel compositions and empirical risk.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "finding the best Gram matrix G in problem (1) for the soft-margin SVM [14]: minimizeq∈PNw supα α 1− 12 ∑ i,j αiαjy y ∑Nw m=1 qmφ(x , w)φ(x , w) subject to 0 α C1, α y = 0.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "We use a standard primal-dual algorithm [4] to solve the min-max problem (9).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "Nevertheless, we use the Gaussian kernel, which corresponds [24] to φ(x, (w, v)) = cos((x, 1) (w, v)) where (W,V ) ∼ N(0, I) × Uni(0, 2π), to showcase the effects of our method.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "We present one example of this task, studying an aptamer selection problem [6].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "For example, the circled features in Figure 2(b) correspond to k-gram prefixes for the 5-grams GGTTG and GTTGG at indices 60 through 64; G-complexes are known to be relevant for binding affinities in aptamers [6], so this is reasonable.",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 6,
      "context" : "P0 = N (0, I) and φ(x,w) = H(wx)(wx)(2), where H(·) is the Heavyside step function [7].",
      "startOffset" : 83,
      "endOffset" : 86
    } ],
    "year" : 2016,
    "abstractText" : "Randomized features provide a computationally efficient way to approximate kernel machines in machine learning tasks. However, such methods require a user-defined kernel as input. We extend the randomized-feature approach to the task of learning a kernel (via its associated random features). Specifically, we present an efficient optimization problem that learns a kernel in a supervised manner. We prove the consistency of the estimated kernel as well as generalization bounds for the class of estimators induced by the optimized kernel, and we experimentally evaluate our technique on several datasets. Our approach is efficient and highly scalable, and we attain competitive results with a fraction of the training cost of other techniques.",
    "creator" : null
  }
}