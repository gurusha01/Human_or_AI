{
  "name" : "286674e3082feb7e5afb92777e48821f.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Graph Clustering: Block-models and model free results",
    "authors" : [ "Yali Wan" ],
    "emails" : [ "yaliwan@washington.edu", "mmp@stat.washington.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction: a framework for clustering with guarantees without model assumptions",
      "text" : "In the last few years, model-based clustering in networks has witnessed spectacular progress. At the central of intact are the so-called block-models, the Stochastic Block Model (SBM), DegreeCorrected SBM (DC-SBM) and Preference Frame Model (PFM). The understanding of these models has been advanced, especially in understanding the conditions when recovery of the true clustering is possible with small or no error. The algorithms for recovery with guarantees have also been improved. However, the impact of the above results is limited by the assumption that the observed data comes from the model.\nThis paper proposes a framework to provide theoretical guarantees for the results of model based clustering algorithms, without making any assumption about the data generating process. To describe the idea, we need some notation. Assume that a graph G on n nodes is observed. A modelbased algorithm clusters G, and outputs clustering C and parameters M(G, C). The framework is as follows: if M(G, C) fits the data G well, then we shall prove that any other clustering C� of G that also fits G well will be a small perturbation of C. If this holds, then C with model parameters M(G, C) can be said to capture the data structure in a meaningful way. We exemplify our approach by obtaining model-free guarantees for the SBM and PFM models. Moreover, we show that model-free and model-based results are intimately connected."
    }, {
      "heading" : "2 Background: graphs, clusterings and block models",
      "text" : "Graphs, degrees, Laplacian, and clustering Let G be a graph on n nodes, described by its adjacency matrix Â. Define d̂i = �n j=1 Âij the degree of node i, and D̂ = diag{d̂i} the diagonal matrix of the node degrees. The (normalized) Laplacian of G is defined as1 L̂ = D̂−1/2ÂD̂−1/2. In 1Rigorously speaking, the normalized graph Laplacian is I − L̂ [10].\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nextension, we define the degree matrix D and the Laplacian L associated to any matrix A ∈ Rn×n, with Aij = Aji ≥ 0, in a similar way. Let C be a partitioning (clustering) of the nodes of G into K clusters. We use the shorthand notation i ∈ k for “node i belongs to cluster k”. We will represent C by its n×K indicator matrix Z, defined by Zik = 1 if i ∈ k, 0 otherwise, for i = 1, . . . n, k = 1, . . . K. (1) Note that ZTZ = diag{nk} with nk counting the number of nodes in cluster k, and ZT ÂZ = [nkl] K k,l=1 with nkl counting the edges in G between clusters k and l. Moreover, for two indicator matrices Z,Z � for clusterings C, C�, (ZTZ �)kk� counts the number of points in the intersection of cluster k of C with cluster k� of C�, and (ZT D̂Z �)kk� computes � i∈k∩k� d̂i the volume of the same intersection.\n“Block models” for random graphs (SBM, DC-SBM, PFM) This family of models contains Stochastic Block Models (SBM) [1, 18], Degree-Corrected SBM (DC-SBM) [17] and Preference Frame Models (PFM) [20]. Under each of these model families, a graph G with adjacency matrix Â over n nodes is generated by sampling its edges independently following the law Âij ∼ Bernoulli(Aij), for all i > j. The symmetric matrix A = [Aij ] describing the graph is the edge probability matrix. The three model families differ in the constraints they put on an acceptable A. Let C∗ be a clustering. The entries of A are defined w.r.t C∗ as follows (and we say that A is compatible with C∗).\nSBM : Aij = Bkl whenever i ∈ k, j ∈ l, with B = [Bkl] ∈ RK×K symmetric and nonnegative.\nDC-SBM : Aij = wiwjBkl whenever i ∈ k, j ∈ l, with B as above and w1, . . . wn non-negative weights associated with the graph nodes.\nPFM : A satisfies D = diag(A1), D−1AZ = ZR where 1 denotes the vector of all ones, Z is the indicator matrix of C∗, and R is a stochastic matrix (R1 = 1, Rkl ≥ 0), the details are in [20]\nWhile perhaps not immediately obvious, the SBM is a subclass of the DC-SBM, and the latter a subclass of the PFM. Another common feature of block-models, that will be significant throughout this work is that for all three, Spectral Clustering algorithms [15] have been proved to work well estimating C∗."
    }, {
      "heading" : "3 Main theorem: blueprint and results for PFM, SBM",
      "text" : "Let M be a model class, such as SBM, DC-SBM, PFM, and denote M(G, C) ∈ M to be a model that is compatible with C and is fitted in some way to graph G (we do not assume in general that this fit is optimal).\nTheorem 1 (Generic Theorem) We say that clustering C fits G well w.r.t M iff M(G, C) is “close to” G. If C fits G well w.r.t M, then (subject to other technical conditions) any other clustering C� which also fits G well is close to C, i.e. dist(C, C�) is small.\nIn what follows, we will instantiate this Generic Theorem, and the concepts therein; in particular the following will be formally defined. (1) Model construction, i.e an algorithm to fit a model in M to (G, C). This is necessary since we want our results to be computable in practice. (2) A goodness of fit measure between M(C,G) and the data G. (3) A distance between clusterings. We adopt the widely used Misclassification Error (or Hamming) distance defined below.\nThe Misclassification Error (ME) distance between two clusterings C, C� over the same set of n points is\ndist(C, C�) = 1− 1 n max π∈SK\n�\ni∈k∩π(k) 1, (2)\nwhere π ranges over all permutations of K elements SK , and π(k) indexes a cluster in C�. If the points are weighted by their degrees, a natural measure on the node set, the Weighted ME (wME)\ndistance is distd̂(C, C�) = 1− 1 �n\ni=1 d̂i max π∈SK\n�\ni∈k∩π(k) d̂i . (3)\nIn the above, �\ni∈k∩k� d̂i represents the total weight of the set of points assigned to cluster k by C and to cluster k� ( or π(k)) by C�. Note that in the indicator matrix representation of clusterings, this is the (k, k�) element of the matrix ZT D̂Z � ∈ RK×K . While dist is more popular, we believe distd̂ is more natural, especially when node degrees are dissimilar, as d̂ can be seen as a natural measure on the set of nodes, and distd̂ is equivalent to the earth-mover’s distance."
    }, {
      "heading" : "3.1 Main result for PFM",
      "text" : "Constructing a model Given a graph G and a clustering C of its nodes, we wish to construct a PFM compatible with C, so that its Laplacian L satisfies that ||L̂− L|| is small. Let the spectral decomposition of L̂ be\nL̂ = [Ŷ Ŷlow]\n� Λ̂ 0\n0 Λ̂low\n� � Ŷ T\nŶ Tlow\n� = Ŷ Λ̂Ŷ T + ŶlowΛ̂lowŶ T low (4)\nwhere Ŷ ∈ Rn×K , Ŷlow ∈ Rn×(n−K), Λ̂ = diag(λ̂1, · · · , λ̂K), Λ̂low = diag(λ̂K+1, · · · , λ̂n). To ensure that the matrices Ŷ , Ŷlow are uniquely defined we assume throughout the paper that L̂’s K-th eigengap, i.e, |λK |− |λK+1|, is non-zero.\nAssumption 1 The eigenvalues of L̂ satisfy λ̂1 = 1 ≥ |λ̂2| ≥ . . . ≥ |λ̂K | > |λ̂K+1| ≥ . . . |λ̂n|. Denote the subspace spanned by the columns of M , for any M matrix, by R(M), and || || the Euclidean or spectral norm.\nPFM Estimation Algorithm\nInput Graph G with Â, D̂, L̂, Ŷ , Λ̂, clustering C with indicator matrix Z. Output (A,L) = PFM(G, C)\n1. Construct an orthogonal matrix derived from Z.\nYZ = D̂ 1/2ZC−1/2, with C = ZTD̂Z the column normalization of Z. (5)\nNote Ckk = � i∈k d̂i is the volume of cluster k.\n2. Project YZ on Ŷ and perform Singular Value Decomposition.\nF = Y TZ Ŷ = UΣV T (6)\n3. Change basis in R(YZ) to align with Ŷ .\nY = YZUV T . Complete Y to an orthonormal basis [Y B] of Rn. (7)\n4. Construct Laplacian L and edge probability matrix A.\nL = Y Λ̂Y T + (BBT )L̂(BBT ), A = D̂1/2LD̂1/2. (8)\nProposition 2 Let G, Â, D̂, L̂, Ŷ , Λ̂ and Z be defined as above, and (A,L) = PFM(G, C). Then,\n1. D̂ and L, or A define a PFM with degrees d̂1:n.\n2. The columns of Y are eigenvectors of L with eigenvalues λ̂1:K .\n3. D̂1/21 is an eigenvector of both L and L̂ with eigenvalue λ̂1 = 1.\nThe proof is relegated to the Supplement, as are all the omitted proofs.\nPFM(G, C) is an estimator for the PFM parameters given the clustering. It is evidently not the Maximum Likelihood estimator, but we can show that it is consistent in the following sense.\nProposition 3 (Informal) Assume that G is sampled from a PFM with parameters D∗, L∗ and compatible with C∗, and let L = PFM(G, C∗). Then, under standard recovery conditions for PFM (e.g [20]) ||L∗ − L|| = o(1) w.r.t. n.\nAssumption 2 (Goodness of fit for PFM) ||L̂− L|| ≤ ε.\nPFM(G, C) instantiates M(G, C), and Assumption 2 instantiates the goodness of fit measure. It remains to prove an instance of Generic Theorem 1 for these choices.\nTheorem 4 (Main Result (PFM)) Let G be a graph with d̂1:n, D̂, L̂, λ̂1:n as defined, and L̂ satisfy Assumption 1. Let C, C� be two clusterings with K clusters, and L,L� be their corresponding Laplacians, defined as in (8), and satisfy Assumption 2 respectively. Set δ = (K−1)ε 2\n(|λ̂K |−|λ̂K+1|)2 and\nδ0 = mink Ckk/maxk Ckk with C defined as in (5), where k indexes the clusters of C. Then, whenever δ ≤ δ0,\ndistd̂(C, C�) ≤ maxk Ckk�\nk Ckk δ, (9)\nwith distd̂ being the weighted ME distance (3).\nIn the remainder of this section we outline the proof steps, while the partial results of Proposition 5, 6, 7 are proved in the Supplement. First, we apply the perturbation bound called the Sinus Theorem of Davis and Kahan, in the form presented in Chapter V of [19].\nProposition 5 Let Ŷ , λ̂1:n, Y be defined as usual. If Assumptions 1 and 2 hold, then\n|| diag(sin θ1:K(Ŷ , Y ))|| ≤ ε |λ̂K |− |λ̂K+1| = ε� (10)\nwhere θ1:K are the canonical (or principal) angles between R(Ŷ ) and R(Y ) (see e.g [8]).\nThe next step concerns the closeness of Y, Ŷ in Frobenius norm. Since Proposition 5 bounds the sinuses of the canonical angles, we exploit the fact that the cosines of the same angles are the singular values of F = Y T Ŷ of (6).\nProposition 6 Let M = Y Y T , M̂ = Ŷ Ŷ T and F, ε� as above. Assumptions 1 and 2 imply that\n1. ||F ||2F = traceMM̂T ≥ K − (K − 1)ε�2.\n2. ||M − M̂ ||2F ≤ 2(K − 1)ε�2.\nNow we show that all clusterings which satisfy Proposition 6 must be close to each other in the weighted ME distance. For this, we first need an intermediate result. Assume we have two clusterings C, C�, with K clusters, for which we construct YZ , Y, L,M , respectively Y �Z , Y �, L�,M � as above. Then, the subspaces spanned by Y and Y � will be close.\nProposition 7 Let L̂ satisfy Assumption 1 and let C, C� represent two clusterings for which L,L� satisfy Assumption 2. Then, ||Y TZ Y �Z ||2F ≥ K − 4(K − 1)ε�2 = K − δ\nThe main result now follows from Proposition 7 and Theorem 9 of [13], as shown in the Supplement. This proof approach is different from the existing perturbation bounds for clustering, which all use counting arguments. The result of [13] is a local equivalence, which bounds the error we need in terms of δ defined above (“local” meaning the result only holds for small δ)."
    }, {
      "heading" : "3.2 Main Theorem for SBM",
      "text" : "In this section, we offer an instantiation of Generic Theorem 1 for the case of the SBM. As before, we start with a model estimator, which in this case is the Maximum Likelihood estimator.\nSBM Estimation Algorithm\nInput Graph with Â, clustering C with indicator matrix Z. Output A = SBM(G, C)\n1. Construct an orthogonal matrix derived from Z: YZ = ZC−1/2 with C = ZTZ.\n2. Estimate the edge probabilities: B = C−1ZT ÂZC−1. 3. Construct A from B by A = ZBZT .\nProposition 8 Let B̃ = C1/2BC1/2 and denote the eigenvalues of B̃, ordered by decreasing magnitude, by λ1:K . Let the spectral decomposition of B̃ be B̃ = UΛUT , with U an orthogonal matrix and Λ = diag(λ1:K). Then\n1. A is a SBM.\n2. λ1:K are the K principal eigenvalues of A. The remaining eigenvalues of A are zero.\n3. A = Y ΛY T where Y = YZU .\nAssumption 3 (Eigengap) B is non-singular (or, equivalently, |λK | > 0.\nAssumption 4 (Goodness of fit for SBM) ||Â−A|| ≤ ε.\nWith the model (SBM), estimator, and goodness of fit defined, we are ready for the main result.\nTheorem 9 (Main Result (SBM)) Let G be a graph with incidence matrix Â, and λ̂AK the K-th singular value of Â. Let C, C� be two clusterings with K clusters, satisfying Assumptions 3 and 4. Set δ = 4Kε 2\n|λ̂AK |2 and δ0 = mink nk/maxk nk, where k indexes the clusters of C. Then, whenever\nδ ≤ δ0, dist(C, C�) ≤ δmaxk nk/n, where dist represents the ME distance (2).\nNote that the eigengap of Â, Λ̂AK is not bounded above, and neither is ε. Since the SBM is less flexible than the PFM, we expect that for the same data G, Theorem 9 will be more restrictive than Theorem 4."
    }, {
      "heading" : "4 The results in perspective",
      "text" : ""
    }, {
      "heading" : "4.1 Cluster validation",
      "text" : "Theorems like 4, 9 can provide model free guarantees for clustering. We exemplify this procedure in the experimental Section 6, using standard spectral clustering as described in e.g [18, 17, 15]. What is essential is that all the quantities such as ε and δ are computable from the data.\nMoreover, if Y is available, then the bound in Theorem 4 can be improved.\nProposition 10 Theorem 4 holds when δ is replaced by δY = K− < M̂,M >F +(K − 1)(ε�)2 + 2 � 2(K − 1)ε�||M̂ −M ||F , with ε� = ε/(|λ̂K |− |λ̂K+1|) and M, M̂ defined in Proposition 6."
    }, {
      "heading" : "4.2 Using existing model-based recovery theorems to prove model-free guarantees",
      "text" : "We exemplify this by using (the proof of) Theorem 3 of [20] to prove the following.\nTheorem 11 (Alternative result based on [20] for PFM) Under the same conditions as in Theorem 4, distd̂(C, C�) ≤ δWM , with δWM = 128 Kε 2\n(|λ̂K |−|λ̂K+1|)2 .\nIt follows, too, that with the techniques in this paper, the error bound in [20] can be improved by a factor of 128.\nSimilarly, if we use the results of [18] we obtain alternative model-free guarantee for the SBM.\nAssumption 5 (Alternative goodness of fit for SBM) ||L̂2−L2||F ≤ ε, where L̂, L are the Laplacians of Â and A = SBM(G, C) respectively.\nTheorem 12 (Alternative result based on [18] for SBM) Under the same conditions as in Theorem 9, except for replacing Assumption 4 with 5, dist(C, C�) ≤ δRCY with δRCY = ε 2\n|λ̂K |4 16maxk nk n .\nA problem with this result is that Assumption 5 is much stronger than 4 (being in Frobenius norm). The more recent results of [17] (with unspecified constants) in conjunction with our original Assumptions 3, 4, and the assumption that all clusters have equal sizes, give a bound of O(Kε2/λ̂2K) for the SBM; hence our model-free Theorem 9 matches this more restrictive model-based theorem."
    }, {
      "heading" : "4.3 Sanity checks and Extensions",
      "text" : "It can be easily verified that if indeed G is sampled from a SBM, or PFM, then for large enough n, and large enough model eigengap, Assumptions 1 and 2 (or 3 and 4) will hold.\nSome immediate extensions and variations of Theorems 4, 9 are possible. For example, one could replace the spectral norm by the Frobenius norm in Assumptions 2 and 4, which would simplify some of the proofs. However, using the Frobenius norm would be a much stronger assumption [18] Theorem 4 holds not just for simple graphs, but in the more general case when Â is a weighted graph (i.e. a similarity matrix). The theorems can be extended to cover the case when C� is a clustering that is α-worse than C, i.e when ||L� − L̂|| ≥ ||L− L̂||(1− α)."
    }, {
      "heading" : "4.4 Clusterability and resilience",
      "text" : "Our Theorems also imply the stability of a clustering to perturbations of the graph G. Indeed, let L̂� be the Laplacian of G�, a perturbation of G. If ||L̂� − L̂|| ≤ ε, then ||L̂� − L|| ≤ 2ε, and (1) G� is well fitted by a PFM whenever G is, and (2) C is δ stable w.r.t G�, hence C is what some authors [9] call resilient.\nA graph G is clusterable when G can be fitted well by some clustering C∗. Much work [4, 7] has been devoted to showing that clusterability implies that finding a C close to C∗ is computationally efficient. Such results can be obtained in our framework, by exploiting existing recovery theorems such as [18, 17, 20], which give recovery guarantees for Spectral Clustering, under the assumption of sampling from the model. For this, we can simply replace the model assumption with the assumption that there is a C∗ for which L (or A) satisfies Assumptions 1 and 2 (or 3 and 4)."
    }, {
      "heading" : "5 Related work",
      "text" : "To our knowledge, there is no work of the type of Theorem 1 in the literature on SBM, DC-SBM, PFM. The closest work is by [6] which guarantees approximate recovery assuming G is close to a DC-SBM.\nSpectral clustering is also used for loss-based clustering in (weighted) graphs and some stability results exist in this context. Even though they measure clustering quality by different criteria, so that the ε values are not comparable, we review them here. The recent paper of [16], Theorem 1.2 states that if the K-way Cheeger constant of G is ρ(k) ≤ (1 − λ̂K+1)/(cK3) then the clustering error2 distd̂(C, Copt) ≤ C/c = δPSZ . In the current proof, the constant C = 2 × 105; moreover, ρ(K) cannot be computed tractably. In [14], the bound δMSX depends on εMSX , the Normalized Cut scaled by the eigengap. Since both bounds refer to the result of spectral clustering, we can compare the relationship between δMSX and εMSX ; for [14], this is δMSX = 2εMSX [1− εMSX/(K − 1)],\n2The results is stronger, bounding the perturbation of each cluster individually by δPSZ , but it also includes a factor larger than 1, bounding the error of K-means algorithm.\nwhich is about K − 1 times larger than δ when � = �MSX . In [5], dist(C, C�) is defined in terms of ||Y TZ − Y �Z ||2F , and the loss is (closely related) to ||Â − SBM(G, C)||2F . The bound does not take into account the eigengap, that is, the stability of the subspace Ŷ itself.\nBootstrap for validating a clustering C was studied in [11] (see also references therein for earlier work). In [3] the idea is to introduce a statistics, and large deviation bounds for it, conditioned on sampling from a SBM (with covariates) and on a given C."
    }, {
      "heading" : "6 Experimental evaluation",
      "text" : "Experiment Setup Given G, we obtain a clustering C0 by spectral clustering [15]. Then we calculate clustering C by perturbing C0 with gradually increasing noise. For each C, we construct PFM (C,G)and SBM(C,G) model, and further compute �, δ and δ0. If δ ≤ δ0, C is guaranteed to be stable by the theorems. In the remainder of this section, we describe the data generating process for the simulated datasets and the results we obtained.\nPFM Datasets We generate from PFM model with K = 5, n = 10000, λ1:K = (1, 0.875, 0.75, 0.625, 0.5). eigengap = 0.48, n1:K = (2000, 2000, 2000, 2000, 2000). The stochastic matrix R and its stationary distribution ρ are shown below. We sample an adjacency matrix Â from A (shown below).\nρ = � 25 .12 .17 .18 .28 �\nR =   .79 .02 .06 .03 .10 .03 .71 .23 .00 .02 .09 .16 .69 .00 .06\n.04 .00 .00 .80 .16 .10 .01 .03 .11 .76\n \nA Â\nPerturbed PFM Datasets A is obtained from the previous model by perturbing its principal subspace (details in Supplement). Then we sample Â from A.\nLancichinetti-Fortunato-Radicchi (LFR) simulated matrix [12] The LFR benchmark graphs are widely used for community detection algorithms, due to heterogeneity in the distribution of node degree and community size. A LFR matrix is simulated with n = 10000, K = 4, nk = (2467, 2416, 2427, 2690) and µ = 0.2, where µ is the mixing parameter indicating the fraction of edges shared between a node and the other nodes from outside its community.\nPolitical Blogs Dataset A directed network �A of hyperlinks between weblogs on US politics, compiled from online directories by Adamic and Glance [2], where each blog is assigned a political leaning, liberal or conservative, based on its blog content. The network A contains 1490 blogs. After erasing the disconnected nodes, n = 983. We study Â = ( �AT �A)3, which is a smoothed undirected graph. For �AT �A we find no guarantees.\nThe first two data sets are expected to fit the PFM well, but not the SBM, while the LFR data is expected to be a good fit for a SBM. Since all bounds can be computed on weighted graphs as well, we have run the experiments also on the edge probability matrices A used to generate the PFM and perturbed PFM graphs.\nThe results of these experiments are summarized in Figure 1. For all of the experiments, the clustering C is ensured to be stable by Theorem 4 as the unweighted error grows to a breaking point, then the assumptions of the theorem fail. In particular, the C0 is always stable in the PFM framework.\nComparing δ from Theorem 9 to that from Theorem 4, we find that Theorem 9 (guarantees for SBM) is much harder to satisfy. All δ values from Theorem 9 are above 1, and not shown.3 In particular, for the SBM model class, the C cannot be proved stable even for the LFR data. Note that part of the reason why with the PFM model very little difference from the clustering C0 can be tolerated for a clustering to be stable is that the large eigengap makes PFM(G, C) differ from PFM(G, C0) even for very small perturbations. By comparing the bounds for Â with the bounds for the “weighted graphs” A, we can evaluate that the sampling noise on δ is approximately equal to that of the clustering perturbation. Of course, the sampling noise varies with n, decreasing for larger graphs. Moreover, from Political Blogs data, we see that “smoothing” a graph, by e.g. taking powers of its adjacency matrix, has a stability inducing effect."
    }, {
      "heading" : "7 Discussion",
      "text" : "This paper makes several contributions. At a high level, it poses the problem of model free validation in the area of community detection in networks. The stability paradigm is not entirely new, but using it explicitly with model-based clustering (instead of cost-based) is. So is “turning around” the model-based recovery theorems to be used in a model-free framework.\nAll quantities in our theorems are computable from the data and the clustering C, i.e do not contain undetermined constants, and do not depend on parameters that are not available. As with distribution-free results in general, making fewer assumptions allows for less confidence in the conclusions, and the results are not always informative. Sometimes this should be so, e.g when the data does not fit the model well. But it is also possible that the fit is good, but not good enough to satisfy the conditions of the theorems as they are currently formulated. This happens with the SBM bounds, and we believe tighter bounds are possible for this model. It would be particularly interesting to study the non-spectral, sharp thresholds of [1] from the point of view of model-free recovery. A complementary problem is to obtain negative guarantees (i.e that C is not unique up to perturbations).\nAt the technical level, we obtain several different and model-specific stability results, that bound the perturbation of a clustering by the perturbation of a model. They can be used both in model-free and in existing or future model-based recovery guarantees, as we have shown in Section 3 and in the experiments. The proof techniques that lead to these results are actually simpler, more direct, and more elementary than the ones found in previous papers.\n3We also computed δRCY but the bounds were not informative."
    } ],
    "references" : [ {
      "title" : "Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms",
      "author" : [ "Emmanuel Abbe", "Colin Sandon" ],
      "venue" : "arXiv preprint arXiv:1503.00609,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "The political blogosphere and the 2004 us election: divided they blog",
      "author" : [ "Lada A Adamic", "Natalie Glance" ],
      "venue" : "In Proceedings of the 3rd international workshop on Link discovery,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Confidence sets for network structure",
      "author" : [ "Edoardo M. Airoldi", "David S. Choi", "Patrick J. Wolfe" ],
      "venue" : "Technical Report arXiv:1105.6245,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Clustering under stability assumptions",
      "author" : [ "Pranjal Awasthi" ],
      "venue" : "In Encyclopedia of Algorithms,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Learning spectral clustering with applications to speech separation",
      "author" : [ "Francis Bach", "Michael I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Finding endogenously formed communities",
      "author" : [ "Maria-Florina Balcan", "Christian Borgs", "Mark Braverman", "Jennifer Chayes", "Shang-Hua Teng" ],
      "venue" : "In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Computational feasibility of clustering under clusterability",
      "author" : [ "Shai Ben-David" ],
      "venue" : "assumptions. CoRR,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Matrix analysis, volume 169",
      "author" : [ "Rajendra Bhatia" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Are stable instances easy",
      "author" : [ "Yonatan Bilu", "Nathan Linial" ],
      "venue" : "CoRR, abs/0906.3162,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Spectral graph theory, volume 92",
      "author" : [ "Fan RK Chung" ],
      "venue" : "American Mathematical Soc.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "Robustness of community structure in networks",
      "author" : [ "Brian Karrer", "Elizaveta Levina", "M.E.J. Newman" ],
      "venue" : "Phys. Rev. E,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Benchmark graphs for testing community detection algorithms",
      "author" : [ "Andrea Lancichinetti", "Santo Fortunato", "Filippo Radicchi" ],
      "venue" : "Physical review E,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "Local equivalence of distances between clusterings – a geometric perspective",
      "author" : [ "Marina Meilă" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Regularized spectral learning",
      "author" : [ "Marina Meilă", "Susan Shortreed", "Liang Xu" ],
      "venue" : "Proceedings of the Artificial Intelligence and Statistics Workshop(AISTATS",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "On spectral clustering: Analysis and an algorithm",
      "author" : [ "A.Y. Ng", "M.I. Jordan", "Y. Weiss" ],
      "venue" : "Advances in Neural Information Processing Systems 14,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "Partitioning well-clustered graphs with k-means and heat kernel",
      "author" : [ "Richard Peng", "He Sun", "Luca Zanetti" ],
      "venue" : "In Proceedings of the Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Regularized spectral clustering under the degree-corrected stochastic blockmodel",
      "author" : [ "Tai Qin", "Karl Rohe" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Spectral clustering and the high-dimensional stochastic blockmodel",
      "author" : [ "Karl Rohe", "Sourav Chatterjee", "Bin Yu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Matrix perturbation theory, volume 175",
      "author" : [ "Gilbert W Stewart", "Ji-guang Sun" ],
      "venue" : "Academic press New York,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1990
    }, {
      "title" : "A class of network models recoverable by spectral clustering",
      "author" : [ "Yali Wan", "Marina Meila" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), page (to appear),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "In (1)Rigorously speaking, the normalized graph Laplacian is I − L̂ [10].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "“Block models” for random graphs (SBM, DC-SBM, PFM) This family of models contains Stochastic Block Models (SBM) [1, 18], Degree-Corrected SBM (DC-SBM) [17] and Preference Frame Models (PFM) [20].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "“Block models” for random graphs (SBM, DC-SBM, PFM) This family of models contains Stochastic Block Models (SBM) [1, 18], Degree-Corrected SBM (DC-SBM) [17] and Preference Frame Models (PFM) [20].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "“Block models” for random graphs (SBM, DC-SBM, PFM) This family of models contains Stochastic Block Models (SBM) [1, 18], Degree-Corrected SBM (DC-SBM) [17] and Preference Frame Models (PFM) [20].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "“Block models” for random graphs (SBM, DC-SBM, PFM) This family of models contains Stochastic Block Models (SBM) [1, 18], Degree-Corrected SBM (DC-SBM) [17] and Preference Frame Models (PFM) [20].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "PFM : A satisfies D = diag(A1), D−1AZ = ZR where 1 denotes the vector of all ones, Z is the indicator matrix of C∗, and R is a stochastic matrix (R1 = 1, Rkl ≥ 0), the details are in [20]",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 14,
      "context" : "Another common feature of block-models, that will be significant throughout this work is that for all three, Spectral Clustering algorithms [15] have been proved to work well estimating C∗.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "First, we apply the perturbation bound called the Sinus Theorem of Davis and Kahan, in the form presented in Chapter V of [19].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "The main result now follows from Proposition 7 and Theorem 9 of [13], as shown in the Supplement.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "The result of [13] is a local equivalence, which bounds the error we need in terms of δ defined above (“local” meaning the result only holds for small δ).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 19,
      "context" : "We exemplify this by using (the proof of) Theorem 3 of [20] to prove the following.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "Theorem 11 (Alternative result based on [20] for PFM) Under the same conditions as in Theorem 4, distd̂(C, C�) ≤ δWM , with δWM = 128 Kε 2 (|λ̂K |−|λ̂K+1|)2 .",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "It follows, too, that with the techniques in this paper, the error bound in [20] can be improved by a factor of 128.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : "Similarly, if we use the results of [18] we obtain alternative model-free guarantee for the SBM.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "Theorem 12 (Alternative result based on [18] for SBM) Under the same conditions as in Theorem 9, except for replacing Assumption 4 with 5, dist(C, C�) ≤ δRCY with δRCY = ε 2 |λ̂K |4 16maxk nk n .",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "The more recent results of [17] (with unspecified constants) in conjunction with our original Assumptions 3, 4, and the assumption that all clusters have equal sizes, give a bound of O(Kε(2)/λ̂(2)K) for the SBM; hence our model-free Theorem 9 matches this more restrictive model-based theorem.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "However, using the Frobenius norm would be a much stronger assumption [18] Theorem 4 holds not just for simple graphs, but in the more general case when Â is a weighted graph (i.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "t G�, hence C is what some authors [9] call resilient.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 3,
      "context" : "Much work [4, 7] has been devoted to showing that clusterability implies that finding a C close to C∗ is computationally efficient.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 6,
      "context" : "Much work [4, 7] has been devoted to showing that clusterability implies that finding a C close to C∗ is computationally efficient.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 17,
      "context" : "Such results can be obtained in our framework, by exploiting existing recovery theorems such as [18, 17, 20], which give recovery guarantees for Spectral Clustering, under the assumption of sampling from the model.",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "Such results can be obtained in our framework, by exploiting existing recovery theorems such as [18, 17, 20], which give recovery guarantees for Spectral Clustering, under the assumption of sampling from the model.",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 19,
      "context" : "Such results can be obtained in our framework, by exploiting existing recovery theorems such as [18, 17, 20], which give recovery guarantees for Spectral Clustering, under the assumption of sampling from the model.",
      "startOffset" : 96,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "The closest work is by [6] which guarantees approximate recovery assuming G is close to a DC-SBM.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "In [14], the bound δMSX depends on εMSX , the Normalized Cut scaled by the eigengap.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "Since both bounds refer to the result of spectral clustering, we can compare the relationship between δMSX and εMSX ; for [14], this is δMSX = 2εMSX [1− εMSX/(K − 1)], (2)The results is stronger, bounding the perturbation of each cluster individually by δPSZ , but it also includes a factor larger than 1, bounding the error of K-means algorithm.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "In [5], dist(C, C�) is defined in terms of ||Y T Z − Y � Z ||(2)F , and the loss is (closely related) to ||Â − SBM(G, C)||(2)F .",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 10,
      "context" : "Bootstrap for validating a clustering C was studied in [11] (see also references therein for earlier work).",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "In [3] the idea is to introduce a statistics, and large deviation bounds for it, conditioned on sampling from a SBM (with covariates) and on a given C.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 14,
      "context" : "Experiment Setup Given G, we obtain a clustering C0 by spectral clustering [15].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "Lancichinetti-Fortunato-Radicchi (LFR) simulated matrix [12] The LFR benchmark graphs are widely used for community detection algorithms, due to heterogeneity in the distribution of node degree and community size.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "Political Blogs Dataset A directed network � A of hyperlinks between weblogs on US politics, compiled from online directories by Adamic and Glance [2], where each blog is assigned a political leaning, liberal or conservative, based on its blog content.",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "For the Political Blogs: Truth means C0 is true clustering of [2], spectral means C0 is obtained from spectral clustering.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "It would be particularly interesting to study the non-spectral, sharp thresholds of [1] from the point of view of model-free recovery.",
      "startOffset" : 84,
      "endOffset" : 87
    } ],
    "year" : 2016,
    "abstractText" : "Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain “correctness” guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.",
    "creator" : null
  }
}