{
  "name" : "7bb060764a818184ebb1cc0d43d382aa.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improved Dropout for Shallow and Deep Learning",
    "authors" : [ "Zhe Li", "Boqing Gong", "Tianbao Yang" ],
    "emails" : [ "zhe-li-1@uiowa.edu", "tianbao-yang@uiowa.edu", "bgong@crcv.ucf.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters [9, 16], which usually identically and independently at random samples neurons and sets their outputs to be zeros. Extensive experiments [4] have shown that dropout can help obtain the state-of-the-art performance on a range of benchmark data sets. Recently, dropout has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [21].\nIn this paper, instead of identically and independently at random zeroing out features or neurons, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. Intuitively, it makes more sense to use non-uniform multinomial sampling than identical and independent sampling for different features/neurons. For example, in shallow learning if input features are centered, we can drop out features with small variance more frequently or completely allowing the training to focus on more important features and consequentially enabling faster convergence. To justify the multinomial sampling for dropout and reveal the optimal sampling probabilities, we conduct a rigorous analysis on the risk bound of shallow learning by stochastic optimization with multinomial dropout, and demonstrate that a distribution-dependent dropout leads to a smaller expected risk (i.e., faster convergence and smaller generalization error).\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nInspired by the distribution-dependent dropout, we propose a data-dependent dropout for shallow learning, and an evolutional dropout for deep learning. For shallow learning, the sampling probabilities are computed from the second order statistics of features of the training data. For deep learning, the sampling probabilities of dropout for a layer are computed on-the-fly from the second-order statistics of the layer’s outputs based on a mini-batch of examples. This is particularly suited for deep learning because (i) the distribution of each layer’s outputs is evolving over time, which is known as internal covariate shift [5]; (ii) passing through all the training data in deep neural networks (in particular deep convolutional neural networks) is much more expensive than through a mini-batch of examples. For a mini-batch of examples, we can leverage parallel computing architectures to accelerate the computation of sampling probabilities.\nWe note that the proposed evolutional dropout achieves similar effect to the batch normalization technique (Z-normalization based on a mini-batch of examples) [5] but with different flavors. Both approaches can be considered to tackle the issue of internal covariate shift for accelerating the convergence. Batch normalization tackles the issue by normalizing the output of neurons to zero mean and unit variance and then performing dropout independently 1. In contrast, our proposed evolutional dropout tackles this issue from another perspective by exploiting a distribution-dependent dropout, which adapts the sampling probabilities to the evolving distribution of a layer’s outputs. In other words, it uses normalized sampling probabilities based on the second order statistics of internal distributions. Indeed, we notice that for shallow learning with Z-normalization (normalizing each feature to zero mean and unit variance) the proposed data-dependent dropout reduces to uniform dropout that acts similarly to the standard dropout. Because of this connection, the presented theoretical analysis also sheds some lights on the power of batch normalization from the angle of theory. Compared to batch normalization, the proposed distribution-dependent dropout is still attractive because (i) it is rooted in theoretical analysis of the risk bound; (ii) it introduces no additional parameters and layers without complicating the back-propagation and the inference; (iii) it facilitates further research because its shares the same mathematical foundation as standard dropout (e.g., equivalent to a form of data-dependent regularizer) [18].\nWe summarize the main contributions of the paper below.\n• We propose a multinomial dropout and demonstrate that a distribution-dependent dropout leads to a faster convergence and a smaller generalization error through the risk bound analysis for shallow learning.\n• We propose an efficient evolutional dropout for deep learning based on the distributiondependent dropout.\n• We justify the proposed dropouts for both shallow learning and deep learning by experimental results on several benchmark datasets.\nIn the remainder, we first review some related work and preliminaries. We present the main results in Section 4 and experimental results in Section 5."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we review some related work on dropout and optimization algorithms for deep learning.\nDropout is a simple yet effective technique to prevent overfitting in training deep neural networks [16]. It has received much attention recently from researchers to study its practical and theoretical properties. Notably, Wager et al. [18], Baldi and Sadowski [2] have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer. The most simple form of dropout is to multiply hidden units by i.i.d Bernoulli noise. Several recent works also found that using other types of noise works as well as Bernoulli noise (e.g., Gaussian noise), which could lead to a better approximation of the marginalized loss [20, 7]. Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework [23, 7]. Graham et al. [3] used the same noise across a batch of examples in order to speed up the computation. The adaptive dropout proposed in[1] overlays a binary belief network over a neural netowrk, incurring more computational overhead to dropout because one has to train the additional binary belief network. In constrast,\n1The author also reported that in some cases dropout is even not necessary\nthe present work proposes a new dropout with noise sampled according to distribution-dependent sampling probabilities. To the best of our knowledge, this is the first work that rigorously studies this type of dropout with theoretical analysis of the risk bound. It is demonstrated that the new dropout can improve the speed of convergence.\nStochastic gradient descent with back-propagation has been used a lot in optimizing deep neural networks. However, it is notorious for its slow convergence especially for deep learning. Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning [17, 12, 22, 5, 6], which tackle the problem from different perspectives. Among them, we notice that the developed evolutional dropout for deep learning achieves similar effect as batch normalization [5] addressing the internal covariate shift issue (i.e., evolving distributions of internal hidden units)."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "In this section, we present some preliminaries, including the framework of risk minimization in machine learning and learning with dropout noise. We also introduce the multinomial dropout, which allows us to construct a distribution-dependent dropout as revealed in the next section.\nLet (x, y) denote a feature vector and a label, where x ∈ Rd and y ∈ Y . Denote by P the joint distribution of (x, y) and denote by D the marginal distribution of x. The goal of risk minimization is to learn a prediction function f(x) that minimizes the expected loss, i.e., minf∈H EP [`(f(x), y)], where `(z, y) is a loss function (e.g., the logistic loss) that measures the inconsistency between z and y and H is a class of prediction functions. In deep learning, the prediction function f(x) is determined by a deep neural network. In shallow learning, one might be interested in learning a linear model f(x) = w>x. In the following presentation, the analysis will focus on the risk minimization of a linear model, i.e.,\nmin w∈Rd\nL(w) , EP [`(w>x, y)] (1)\nIn this paper, we are interested in learning with dropout, i.e., the feature vector x is corrupted by a dropout noise. In particular, let ∼ M denote a dropout noise vector of dimension d, and the corrupted feature vector is given by x̂ = x ◦ , where the operator ◦ represents the element-wise multiplication. Let P̂ denote the joint distribution of the new data (x̂, y) and D̂ denote the marginal distribution of x̂. With the corrupted data, the risk minimization becomes\nmin w∈Rd\nL̂(w) , EP̂ [`(w >(x ◦ ), y)] (2)\nIn standard dropout [18, 4], the entries of the noise vector are sampled independently according to Pr( j = 0) = δ and Pr( j = 11−δ ) = 1 − δ, i.e., features are dropped with a probability δ and scaled by 11−δ with a probability 1 − δ. We can also write j = bj 1−δ , where bj ∈ {0, 1}, j ∈ [d] are i.i.d Bernoulli random variables with Pr(bj = 1) = 1 − δ. The scaling factor 11−δ is added to ensure that E [x̂] = x. It is obvious that using the standard dropout different features will have equal probabilities to be dropped out or to be selected independently. However, in practice some features could be more informative than the others for learning purpose. Therefore, it makes more sense to assign different sampling probabilities for different features and make the features compete with each other.\nTo this end, we introduce the following multinomial dropout. Definition 1. (Multinomial Dropout) A multinomial dropout is defined as x̂ = x ◦ , where i =\nmi kpi , i ∈ [d] and {m1, . . . ,md} follow a multinomial distribution Mult(p1, . . . , pd; k) with∑d\ni=1 pi = 1 and pi ≥ 0.\nRemark: The multinomial dropout allows us to use non-uniform sampling probabilities p1, . . . , pd for different features. The value of mi is the number of times that the i-th feature is selected in k independent trials of selection. In each trial, the probability that the i-th feature is selected is given by pi. As in the standard dropout, the normalization by kpi is to ensure that E [x̂] = x. The parameter k plays the same role as the parameter 1− δ in standard dropout, which controls the number of features to be dropped. In particular, the expected total number of the kept features using multinomial dropout is k and that using standard dropout is d(1 − δ). In the sequel, to make fair comparison between\nthe two dropouts, we let k = d(1− δ). In this case, when a uniform distribution pi = 1/d is used in multinomial dropout to which we refer as uniform dropout, then i = mi1−δ , which acts similarly to the standard dropout using i.i.d Bernoulli random variables. Note that another choice to make the sampling probabilities different is still using i.i.d Bernoulli random variables but with different probabilities for different features. However, multinomial dropout is more suitable because (i) it is easy to control the level of dropout by varying the value of k; (ii) it gives rise to natural competition among features because of the constraint ∑ i pi = 1; (iii) it allows us to minimize the sampling dependent risk bound for obtaining a better distribution than uniform sampling.\nDropout is a data-dependent regularizer Dropout as a regularizer has been studied in [18, 2] for logistic regression, which is stated in the following proposition for ease of discussion later.\nProposition 1. If `(z, y) = log(1 + exp(−yz)), then\nEP̂ [`(w >x̂, y)] = EP [`(w >x, y)] +RD,M(w) (3)\nwhereM denotes the distribution of and RD,M(w) = ED,M [ log exp(w> x◦ 2 )+exp(−w > x◦ 2 )\nexp(w>x/2)+exp(−w>x/2)\n] .\nRemark: It is notable that RD,M ≥ 0 due to the Jensen inequality. Using the second order Taylor expansion, [18] showed that the following approximation of RD,M(w) is easy to manipulate and understand:\nR̂D,M(w) = ED[q(w >x)(1− q(w>x))w>CM(x ◦ )w] 2\n(4)\nwhere q(w>x) = 1 1+exp(−w>x/2) , and CM denotes the covariance matrix in terms of . In particular, if is the standard dropout noise, then CM[x ◦ ] = diag(x21δ/(1 − δ), . . . , x2dδ/(1 − δ)), where diag(s1, . . . , sn) denotes a d×d diagonal matrix with the i-th entry equal to si. If is the multinomial dropout noise in Definition 1, we have\nCM[x ◦ ] = 1\nk diag(x2i /pi)−\n1 k xx> (5)"
    }, {
      "heading" : "4 Learning with Multinomial Dropout",
      "text" : "In this section, we analyze a stochastic optimization approach for minimizing the dropout loss in (2). Assume the sampling probabilities are known. We first obtain a risk bound of learning with multinomial dropout for stochastic optimization. Then we try to minimize the factors in the risk bound that depend on the sampling probabilities. We would like to emphasize that our goal here is not to show that using dropout would render a smaller risk than without using dropout, but rather focus on the impact of different sampling probabilities on the risk. Let the initial solution be w1. At the iteration t, we sample (xt, yt) ∼ P and t ∼M as in Definition 1 and then update the model by\nwt+1 = wt − ηt∇`(w>t (xt ◦ t), yt) (6)\nwhere∇` denotes the (sub)gradient in terms of wt and ηt is a step size. Suppose we run the stochastic optimization by n steps (i.e., using n examples) and compute the final solution as ŵn = 1n ∑n t=1 wt.\nWe note that another approach of learning with dropout is to minimize the empirical risk by marginalizing out the dropout noise, i.e., replacing the true expectations EP and ED in (3) with empirical expectations over a set of samples (x1, y1), . . . , (xn, yn) denoted by EPn and EDn . Since the data dependent regularizer RDn,M(w) is difficult to compute, one usually uses an approximation R̂Dn,M(w) (e.g., as in (4)) in place of RDn,M(w). However, the resulting problem is a non-convex optimization, which together with the approximation error would make the risk analysis much more involved. In contrast, the update in (6) can be considered as a stochastic gradient descent update for solving the convex optimization problem in (2), allowing us to establish the risk bound based on previous results of stochastic gradient descent for risk minimization [14, 15]. Nonetheless, this restriction does not lose the generality. Indeed, stochastic optimization is usually employed for solving empirical loss minimization in big data and deep learning.\nThe following theorem establishes a risk bound of ŵn in expectation.\nTheorem 1. Let L(w) be the expected risk of w defined in (1). Assume ED̂[‖x ◦ ‖ 2 2] ≤ B2 and `(z, y) is G-Lipschitz continuous. For any ‖w∗‖2 ≤ r, by appropriately choosing η, we can have\nE[L(ŵn) +RD,M(ŵn)] ≤ L(w∗) +RD,M(w∗) + GBr√ n\nwhere E[·] is taking expectation over the randomness in (xt, yt, t), t = 1, . . . , n.\nRemark: In the above theorem, we can choose w∗ to be the best model that minimizes the expected risk in (1). Since RD,M (w) ≥ 0, the upper bound in the theorem above is also the upper bound of the risk of ŵn, i.e., L(ŵn), in expectation. The proof of the above theorem follows the standard analysis of stochastic gradient descent. The detailed proof of theorem is included in the appendix."
    }, {
      "heading" : "4.1 Distribution Dependent Dropout",
      "text" : "Next, we consider the sampling dependent factors in the risk bounds. From Theorem 1, we can see that there are two terms that depend on the sampling probabilities, i.e., B2 - the upper bound of ED̂[‖x ◦ ‖ 2 2], and RD,M(w∗)−RD,M(ŵn) ≤ RD,M(w∗). We note that the second term also depends on w∗ and ŵn, which is more difficult to optimize. We first try to minimize ED̂[‖x◦ ‖ 2 2] and present the discussion on minimizing RD,M(w∗) later. From Theorem 1, we can see that minimizing ED̂[‖x ◦ ‖ 2 2] would lead to not only a smaller risk (given the same number of total examples, smaller ED̂[‖x ◦ ‖ 2 2] gives a smaller risk bound) but also a faster convergence (with the same number of iterations, smaller ED̂[‖x ◦ ‖ 2 2] gives a smaller optimization error).\nDue to the limited space, the proofs of Proposition 2, 3, 4 are included in supplement. The following proposition simplifies the expectation ED̂[‖x ◦ ‖ 2 2]. Proposition 2. Let follow the distributionM defined in Definition 1. Then\nED̂[‖x ◦ ‖ 2 2] =\n1\nk d∑ i=1 1 pi ED[x 2 i ] + k − 1 k d∑ i=1 ED[x 2 i ] (7)\nGiven the expression of ED̂[‖x ◦ ‖ 2 2] in Proposition 2, we can minimize it over p, leading to the following result. Proposition 3. The solution to p∗ = arg minp≥0,p>1=1 ED̂[‖x ◦ ‖ 2 2] is given by\np∗i = √ ED[x2i ]∑d\nj=1 √ ED[x2j ] , i = 1, . . . , d (8)\nNext, we examine RD,M(w∗). Since direct manipulation on RD,M(w∗) is difficult, we try to minimize the second order Taylor expansion R̂D,M(w∗) for logistic loss. The following theorem establishes an upper bound of R̂D,M(w∗).\nProposition 4. Let follow the distribution M defined in Definition 1. We have R̂D,M(w∗) ≤ 1 8k‖w∗‖ 2 2 (∑d i=1 ED[x 2 i ] pi − ED[‖x‖22] ) Remark: By minimizing the relaxed upper bound in Proposition 4, we obtain the same sampling probabilities as in (8). We note that a tighter upper bound can be established, however, which will yield sampling probabilities dependent on the unknown w∗.\nIn summary, using the probabilities in (8), we can reduce both ED̂[‖x ◦ ‖ 2 2] and RD,M(w∗) in the risk bound, leading to a faster convergence and a smaller generalization error. In practice, we can use empirical second-order statistics to compute the probabilities, i.e.,\npi =\n√ 1 n ∑n j=1[[xj ]\n2 i ]∑d\ni′=1 √ 1 n ∑n j=1[[xj ] 2 i′ ]\n(9)\nwhere [xj ]i denotes the i-th feature of the j-th example, which gives us a data-dependent dropout. We state it formally in the following definition.\nDefinition 2. (Data-dependent Dropout) Given a set of training examples (x1, y1), . . . , (xn, yn). A data-dependent dropout is defined as x̂ = x ◦ , where i = mikpi , i ∈ [d] and {m1, . . . ,md} follow a multinomial distribution Mult(p1, . . . , pd; k) with pi given by (9).\nRemark: Note that if the data is normalized such that each feature has zero mean and unit variance (i.e., according to Z-normliazation), the data-dependent dropout reduces to uniform dropout. It implies that the data-dependent dropout achieves similar effect as Z-normalization plus uniform dropout. In this sense, our theoretical analysis also explains why Z-normalization usually speeds up the training [13]."
    }, {
      "heading" : "4.2 Evolutional Dropout for Deep Learning",
      "text" : "Next, we discuss how to implement the distribution-dependent dropout for deep learning. In training deep neural networks, the dropout is usually added to the intermediate layers (e.g., fully connected layers and convolutional layers). Let xl = (xl1, . . . , x l d) denote the outputs of the l-th layer (with the index of data omitted). Adding dropout to this layer is equivalent to multiplying xl by a dropout noise vector l, i.e., feeding x̂l = xl ◦ l as the input to the next layer. Inspired by the datadependent dropout, we can generate l according to a distribution given in Definition 1 with sampling probabilities pli computed from {xl1, . . . ,xln} similar to that (9). However, deep learning is usually trained with big data and a deep neural network is optimized by mini-batch stochastic gradient descent. Therefore, at each iteration it would be too expensive to afford the computation to pass through all examples. To address this issue, we propose to use a mini-batch of examples to calculate the second-order statistics similar to what was done in batch normalization. Let X l = (xl1, . . . ,x l m) denote the outputs of the l-th layer for a mini-batch of m examples. Then we can calculate the probabilities for dropout by\npli =\n√ 1 m ∑m j=1[[x\nl j ] 2 i ]∑d\ni′=1 √ 1 m ∑m j=1[[x l j ] 2 i′ ] , i = 1, . . . , d (10)\nwhich define the evolutional dropout named as such because the probabilities pli will also evolve as the the distribution of the layer’s outputs evolve. We describe the evolutional dropout as applied to a layer of a deep neural network in Figure 1.\nFinally, we would like to compare the evolutional dropout with batch normalization. Similar to batch normalization, evolutional dropout can also address the internal covariate shift issue by adapting the sampling probabilities to the evolving distribution of layers’ outputs. However, different from batch normalization, evolutional dropout is a randomized technique, which enjoys many benefits as standard dropout including (i) the back-propagation is simple to implement (just multiplying the gradient of X̂ l by the dropout mask to get the gradient of X l); (ii) the inference (i.e., testing) remains the same 2; (iii) it is equivalent to a data-dependent regularizer with a clear mathematical explanation;\n2Different from some implementations for standard dropout which doest no scale by 1/(1− δ) in training but scale by 1− δ in testing, here we do scale in training and thus do not need any scaling in testing.\n(iv) it prevents units from co-adapting of neurons, which facilitate generalization. Moreover, the evolutional dropout has its root in distribution-dependent dropout, which has theoretical guarantee to accelerate the convergence and improve the generalization for shallow learning."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In the section, we present some experimental results to justify the proposed dropouts. In all experiments, we set δ = 0.5 in the standard dropout and k = 0.5d in the proposed dropouts for fair comparison, where d represents the number of features or neurons of the layer that dropout is applied to. For the sake of clarity, we divided the experiments into three parts. In the first part, we compare the performance of the data-dependent dropout (d-dropout) to the standard dropout (s-dropout) for logistic regression. In the second part, we compare the performance of evolutional dropout (e-dropout) to the standard dropout for training deep convolutional neural networks. Finally, we compare e-dropout with batch normalization."
    }, {
      "heading" : "5.1 Shallow Learning",
      "text" : "We implement the presented stochastic optimization algorithm. To evaluate the performance of data-dependent dropout for shallow learning, we use the three data sets: real-sim, news20 and RCV13. In this experiment, we use a fixed step size and tune the step size in [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001] and report the best results in terms of convergence speed on the training data for both standard dropout and data-dependent dropout. The left three panels in Figure 2 show the obtained results on these three data sets. In each figure, we plot both the training error and the testing error. We can see that both the training and testing errors using the proposed data-dependent dropout decrease much faster than using the standard dropout and also a smaller testing error is achieved by using the data-dependent dropout."
    }, {
      "heading" : "5.2 Evolutional Dropout for Deep Learning",
      "text" : "We would like to emphasize that we are not aiming to obtain better prediction performance by trying different network structures and different engineering tricks such as data augmentation, whitening, etc., but rather focus on the comparison of the proposed dropout to the standard dropout using Bernoulli noise on the same network structure. In our experiments, we use the default splitting of training and testing data in all data sets. We directly optimize the neural networks using all training images without further splitting it into a validation data to be added into the training in later stages, which explains some marginal gaps from the literature results that we observed (e.g., on CIFAR-10 compared with [19]).\nWe conduct experiments on four benchmark data sets for comparing e-dropout and s-dropout: MNIST [10], SVHN [11], CIFAR-10 and CIFAR-100 [8]. We use the same or similar network structure as in the literatures for the four data sets. In general, the networks consist of convolution layers, pooling layers, locally connected layers, fully connected layers, softmax layers and a cost layer. For the detailed neural network structures and their parameters, please refer to the supplementary materials. The dropout is added to some fully connected layers or locally connected layers. The rectified linear activation function is used for all neurons. All the experiments are conducted using the cuda-convnet library 4. The training procedure is similar to [9] using mini-batch SGD with momentum (0.9). The\n3https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ 4https://code.google.com/archive/p/cuda-convnet/\nsize of mini-batch is fixed to 128. The weights are initialized based on the Gaussian distribution with mean zero and standard deviation 0.01. The learning rate (i.e., step size) is decreased after a number of epochs similar to what was done in previous works [9]. We tune the initial learning rates for s-dropout and e-dropout separately from 0.001, 0.005, 0.01, 0.1 and report the best result on each data set that yields the fastest convergence.\nFigure 3 shows the training and testing error curves in the optimization process on the four data sets using the standard dropout and the evolutional dropout. For SVHN data, we only report the first 12000 iterations, after which the error curves of the two methods almost overlap. We can see that using the evolutional dropout generally converges faster than using the standard dropout. On CIFAR100 data, we have observed significant speed-up. In particular, the evolutional dropout achieves relative improvements over 10% on the testing performance and over 50% on the convergence speed compared to the standard dropout."
    }, {
      "heading" : "5.3 Comparison with the Batch Normalization (BN)",
      "text" : "Finally, we make a comparison between the evolutional dropout and the batch normalization. For batch normalization, we use the implementation in Caffe 5. We compare the evolutional dropout with the batch normalization on CIFAR-10 data set. The network structure is from the Caffe package and can be found in the supplement, which is different from the one used in the previous experiment. It contains three convolutional layers and one fully connected layer. Each convolutional layer is followed by a pooling layer. We compare four methods: (1) No BN and No dropout - without using batch normalization and dropout; (2) BN; (3) BN with standard dropout; (4) Evolutional Dropout. The rectified linear activation is used in all methods. We also tried BN with the sigmoid activation function, which gives worse results. For the methods with BN, three batch normalization layers are inserted before or after each pooling layer following the architecture given in Caffe package (see supplement). For the evolutional dropout training, only one layer of dropout is added to the the last convolutional layer. The mini-batch size is set to 100, the default value in Caffe. The initial learning rates for the four methods are set to the same value (0.001), and they are decreased once by ten times. The testing accuracy versus the number of iterations is plotted in the right panel of Figure 2, from which we can see that the evolutional dropout training achieves comparable performance with BN + standard dropout, which justifies our claim that evolutional dropout also addresses the internal covariate shift issue."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have proposed a distribution-dependent dropout for both shallow learning and deep learning. Theoretically, we proved that the new dropout achieves a smaller risk and faster convergence. Based on the distribution-dependent dropout, we developed an efficient evolutional dropout for training deep neural networks that adapts the sampling probabilities to the evolving distributions of layers’ outputs. Experimental results on various data sets verified that the proposed dropouts can dramatically improve the convergence and also reduce the testing error."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank anonymous reviewers for their comments. Z. Li and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995). B. Gong is supported in part by NSF (IIS-1566511) and a gift from Adobe.\n5https://github.com/BVLC/caffe/"
    } ],
    "references" : [ {
      "title" : "Adaptive dropout for training deep neural networks",
      "author" : [ "Jimmy Ba", "Brendan Frey" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Understanding dropout",
      "author" : [ "Pierre Baldi", "Peter J Sadowski" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Efficient batchwise dropout training using submatrices",
      "author" : [ "Benjamin Graham", "Jeremy Reizenstein", "Leigh Robinson" ],
      "venue" : "CoRR, abs/1502.02478,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Variational dropout and the local reparameterization trick",
      "author" : [ "Diederik P. Kingma", "Tim Salimans", "Max Welling" ],
      "venue" : "CoRR, abs/1506.02557,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny",
      "author" : [ "Alex Krizhevsky", "Geoffrey Hinton" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "Reading digits in natural images with unsupervised feature learning",
      "author" : [ "Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng" ],
      "venue" : "In NIPS workshop on deep learning and unsupervised feature learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Path-sgd: Path-normalized optimization in deep neural networks",
      "author" : [ "Behnam Neyshabur", "Ruslan R Salakhutdinov", "Nati Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Factored 3-way restricted boltzmann machines for modeling natural images",
      "author" : [ "Marc’Aurelio Ranzato", "Alex Krizhevsky", "Geoffrey E. Hinton" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Stochastic convex optimization",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "In The 22nd Conference on Learning Theory (COLT),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1929
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton" ],
      "venue" : "In Proceedings of the 30th international conference on machine learning",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Dropout training as adaptive regularization",
      "author" : [ "Stefan Wager", "Sida Wang", "Percy S Liang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Regularization of neural networks using dropconnect",
      "author" : [ "Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Fast dropout training",
      "author" : [ "Sida Wang", "Christopher Manning" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "Feature noising for log-linear structured prediction",
      "author" : [ "Sida I Wang", "Mengqiu Wang", "Stefan Wager", "Percy Liang", "Christopher D Manning" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Deep learning with elastic averaging sgd",
      "author" : [ "Sixin Zhang", "Anna Choromanska", "Yann LeCun" ],
      "venue" : "arXiv preprint arXiv:1412.6651,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Adaptive dropout rates for learning with corrupted features",
      "author" : [ "Jingwei Zhuo", "Jun Zhu", "Bo Zhang" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters [9, 16], which usually identically and independently at random samples neurons and sets their outputs to be zeros.",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "Dropout has been widely used to avoid overfitting of deep neural networks with a large number of parameters [9, 16], which usually identically and independently at random samples neurons and sets their outputs to be zeros.",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "Extensive experiments [4] have shown that dropout can help obtain the state-of-the-art performance on a range of benchmark data sets.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 19,
      "context" : "Recently, dropout has also been found to improve the performance of logistic regression and other single-layer models for natural language tasks such as document classification and named entity recognition [21].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 4,
      "context" : "This is particularly suited for deep learning because (i) the distribution of each layer’s outputs is evolving over time, which is known as internal covariate shift [5]; (ii) passing through all the training data in deep neural networks (in particular deep convolutional neural networks) is much more expensive than through a mini-batch of examples.",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "We note that the proposed evolutional dropout achieves similar effect to the batch normalization technique (Z-normalization based on a mini-batch of examples) [5] but with different flavors.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 16,
      "context" : ", equivalent to a form of data-dependent regularizer) [18].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 14,
      "context" : "Dropout is a simple yet effective technique to prevent overfitting in training deep neural networks [16].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "[18], Baldi and Sadowski [2] have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[18], Baldi and Sadowski [2] have analyzed the dropout from a theoretical viewpoint and found that dropout is equivalent to a data-dependent regularizer.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : ", Gaussian noise), which could lead to a better approximation of the marginalized loss [20, 7].",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : ", Gaussian noise), which could lead to a better approximation of the marginalized loss [20, 7].",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework [23, 7].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 6,
      "context" : "Some works tried to optimize the hyper-parameters that define the noise level in a Bayesian framework [23, 7].",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "[3] used the same noise across a batch of examples in order to speed up the computation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "The adaptive dropout proposed in[1] overlays a binary belief network over a neural netowrk, incurring more computational overhead to dropout because one has to train the additional binary belief network.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning [17, 12, 22, 5, 6], which tackle the problem from different perspectives.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning [17, 12, 22, 5, 6], which tackle the problem from different perspectives.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning [17, 12, 22, 5, 6], which tackle the problem from different perspectives.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning [17, 12, 22, 5, 6], which tackle the problem from different perspectives.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "Recently, there emerge a battery of studies trying to accelearte the optimization of deep learning [17, 12, 22, 5, 6], which tackle the problem from different perspectives.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Among them, we notice that the developed evolutional dropout for deep learning achieves similar effect as batch normalization [5] addressing the internal covariate shift issue (i.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "In standard dropout [18, 4], the entries of the noise vector are sampled independently according to Pr( j = 0) = δ and Pr( j = 1 1−δ ) = 1 − δ, i.",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "In standard dropout [18, 4], the entries of the noise vector are sampled independently according to Pr( j = 0) = δ and Pr( j = 1 1−δ ) = 1 − δ, i.",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "Dropout is a data-dependent regularizer Dropout as a regularizer has been studied in [18, 2] for logistic regression, which is stated in the following proposition for ease of discussion later.",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Dropout is a data-dependent regularizer Dropout as a regularizer has been studied in [18, 2] for logistic regression, which is stated in the following proposition for ease of discussion later.",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "Using the second order Taylor expansion, [18] showed that the following approximation of RD,M(w) is easy to manipulate and understand:",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 13,
      "context" : "In contrast, the update in (6) can be considered as a stochastic gradient descent update for solving the convex optimization problem in (2), allowing us to establish the risk bound based on previous results of stochastic gradient descent for risk minimization [14, 15].",
      "startOffset" : 260,
      "endOffset" : 268
    }, {
      "referenceID" : 12,
      "context" : "In this sense, our theoretical analysis also explains why Z-normalization usually speeds up the training [13].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "We conduct experiments on four benchmark data sets for comparing e-dropout and s-dropout: MNIST [10], SVHN [11], CIFAR-10 and CIFAR-100 [8].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "We conduct experiments on four benchmark data sets for comparing e-dropout and s-dropout: MNIST [10], SVHN [11], CIFAR-10 and CIFAR-100 [8].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "We conduct experiments on four benchmark data sets for comparing e-dropout and s-dropout: MNIST [10], SVHN [11], CIFAR-10 and CIFAR-100 [8].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : "The training procedure is similar to [9] using mini-batch SGD with momentum (0.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : ", step size) is decreased after a number of epochs similar to what was done in previous works [9].",
      "startOffset" : 94,
      "endOffset" : 97
    } ],
    "year" : 2016,
    "abstractText" : "Dropout has been witnessed with great success in training deep neural networks by independently zeroing out the outputs of neurons at random. It has also received a surge of interest for shallow learning, e.g., logistic regression. However, the independent sampling for dropout could be suboptimal for the sake of convergence. In this paper, we propose to use multinomial sampling for dropout, i.e., sampling features or neurons according to a multinomial distribution with different probabilities for different features/neurons. To exhibit the optimal dropout probabilities, we analyze the shallow learning with multinomial dropout and establish the risk bound for stochastic optimization. By minimizing a sampling dependent factor in the risk bound, we obtain a distribution-dependent dropout with sampling probabilities dependent on the second order statistics of the data distribution. To tackle the issue of evolving distribution of neurons in deep learning, we propose an efficient adaptive dropout (named evolutional dropout) that computes the sampling probabilities on-the-fly from a mini-batch of examples. Empirical studies on several benchmark datasets demonstrate that the proposed dropouts achieve not only much faster convergence and but also a smaller testing error than the standard dropout. For example, on the CIFAR-100 data, the evolutional dropout achieves relative improvements over 10% on the prediction performance and over 50% on the convergence speed compared to the standard dropout.",
    "creator" : null
  }
}