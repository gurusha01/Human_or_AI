{
  "name" : "57bafb2c2dfeefba931bb03a835b1fa9.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Exploiting Tradeoffs for Exact Recovery in Heterogeneous Stochastic Block Models",
    "authors" : [ "Amin Jalali", "Maryam Fazel" ],
    "emails" : [ "amjalali@uw.edu", "royhan@uw.edu", "dumitriu@uw.edu", "mfazel@uw.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The Stochastic Block Model (SBM) is a widely used random graph model for networks with communities. Despite the recent burst of interest in community detection under the SBM from statistical and computational points of view, there are still gaps in understanding the fundamental limits of recovery. In this paper, we consider the SBM in its full generality, where there is no restriction on the number and sizes of communities or how they grow with the number of nodes, as well as on the connectivity probabilities inside or across communities. For such stochastic block models, we provide guarantees for exact recovery via a semidefinite program as well as upper and lower bounds on SBM parameters for exact recoverability. Our results exploit the tradeoffs among the various parameters of heterogenous SBM and provide recovery guarantees for many new interesting SBM configurations."
    }, {
      "heading" : "1 Introduction",
      "text" : "A fundamental problem in network science and machine learning is to discover structures in large, complex networks (e.g., biological, social, or information networks). Community or cluster detection underlies many decision tasks, as a basic step that uses pairwise relations between data points in order to understand more global structures in the data. Applications include recommendation systems [27], image segmentation [24, 20], learning gene network structures in bioinformatics, e.g., in protein detection [9] and population genetics [17].\nIn spite of a long history of heuristic algorithms (see, e.g., [18] for an empirical overview), as well as strong research interest in recent years on the theoretical side as briefly reviewed in the sequel, there are still gaps in understanding the fundamental information theoretic limits of recoverability (i.e., if there is enough information to reveal the communities) and computational tractability (if there are efficient algorithms to recover them). This is particularly true in the case of sparse graphs (that test the limits of recoverability), graphs with heterogeneous communities (communities varying greatly in size and connectivity), graphs with a number of communities that grows with the number of nodes, and partially observed graphs (with various observation models).\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain."
    }, {
      "heading" : "1.1 Exact Recovery for Heterogenous Stochastic Block Model",
      "text" : "The stochastic block model (SBM), first introduced and studied in mathematical sociology by Holland, Laskey and Leinhardt in 1983 [16], can be described as follows. Consider n vertices partitioned into r communities V1, V2, . . . , Vr , of sizes n1, n2, . . . , nr. We endow the kth community with an Erdős-Rényi random graph model G(nk, pk) and draw an edge between pairs of nodes in different communities independently with probability q; i.e., for any pair of nodes i and j , if i, j ∈ Vk for some k ∈ {1, . . . , r} we draw an edge with probability pk, and draw an edge with probability q if they are in different communities. We assume q < mink pk in order for the idea of communities to make sense. This defines a distribution over random graphs known as the stochastic block model. In this paper, we assume the above model while allowing the number of communities to grow with the number of nodes (similar to [13, 15, 23]). We refer to this model as the heterogeneous stochastic block model to contrast our study of this general setting with previous works on special cases of SBM such as 1) homogenous SBM where the communities are equivalent (they are of the same size and the connectivity probabilities are equal,) e.g., [12], or, 2) SBM with linear-sized communities, where the number of communities is fixed and all community sizes are O(n); e.g., [1]."
    }, {
      "heading" : "1.2 Statistical and Computational Regimes",
      "text" : "What we can infer about the community structure from a single draw of the random graph varies based on the regime of model parameters. Often, the following scenarios are considered.\n1. Recovery, where the proportion of misclassified nodes is negligible; either 0 (corresponding to exact recovery with strong consistency, and considered in [12, 1]) or asymptotically 0 (corresponding to exact recovery with weak consistency as considered in [23, 22, 28]) as the number of nodes grows.\n2. Approximation, where a finite fraction (bounded away from 1) of the vertices is recovered. This regime was first introduced in [13, 14], and has been considered in many other works since then; e.g., see [15] and references therein.\nBoth recovery and approximation can be studied from statistical and computational points of view.\nStatistically, one can ask about the parameter regimes for which the model can be recovered or approximated. Such characterizations are specially important when an information-theoretical lower bound (below which recovery is not possible with high probability) is shown to be achievable with an algorithm (with high probability), hence characterizing a phase transition in model parameters. Recently, there has been significant interest in identifying such sharp thresholds for various parameter regimes.\nComputationally, one might be interested to study algorithms for recovery or approximation. In the older approach, algorithms were studied to provide upper bounds on the parameter regimes for recovery or approximation. See [10] or [1, Section 5] for a summary of such results. More recently, the paradigm has shifted towards understanding the limitations and strengths of tractable methods (e.g. see [21] on semidefinite programming based methods) and assessing whether successful retrieval can be achieved by tractable algorithms at the sharp statistical thresholds or there is a gap. So far, it is understood that there is no such gap in the case of exact recovery (weak and strong) and approximation of binary SBM as well as the exact recovery of linear-sized communities [1]. However, this is still an open question for more general cases; e.g., see [2] and the list of unresolved conjectures therein.\nThe statistical-computational picture for SBM with only two equivalent communities has been fully characterized in a series of recent papers. Apart from the binary SBM, the best understood cases are where there is a finite number r of equivalent or linear-sized communities. Outside of the settings described above, the full picture has not yet emerged and many questions are unresolved."
    }, {
      "heading" : "1.3 This paper",
      "text" : "The community detection problem studied in this paper is stated as: given the adjacency matrix of a graph generated by the heterogenous stochastic block model, for what SBM parameters we can recover the labels of all vertices, with high probability, using an algorithm that has been proved to do so. We consider a convex program in (2.4) and an estimator similar to the maximum likelihood\nestimator in (2.5) and characterize parts of the model space for which exact recovery is possible via these algorithms. Theorems 1 and 2 provide sufficient conditions for the convex recovery program and Theorem 3 provides sufficient conditions for the modified maximum likelihood estimator to exactly recover the underlying model. In Section 2.3, we extend the above bounds to the case of partial observations, i.e., when each entry of the matrix is observed uniformly at random with some probability γ and the results are recorded. We also provide an information-theoretic lower bound, describing an impossibility regime for exact recovery in heterogenous SBM in Theorem 4. All of our results only hold with high probability, as this is the best one can hope for; with tiny probability the model can generate graphs like the complete graph where the partition is unrecoverable.\nThe results of this paper provide a clear improvement in the understanding of stochastic block models by exploiting tradeoffs among SBM parameters. We identify a key parameter (or summary statistic), defined in (2.1) and referred to as relative density, which shows up in our results and provides improvements in the statistical assessment and efficient computational approaches for certain configurations of heterogenous SBM; examples are given in in Section 3 to illustrate a number of such beneficial tradeoffs such as\n• semidefinite programming can successfully recover communities of size O( √ logn) under\nmild conditions on other communities (see Example 3 for details) while logn has long been believed to be the threshold for the smallest community size.\n• The sizes of the communities can be very spread, or the inter- and intra-community probabilities can be very close, and the model still be efficiently recoverable, while existing methods (e.g., peeling strategy [3]) providing false negatives.\nWhile these results are a step towards understanding the information-computational picture about the heterogenous SBM with a growing number of communities, we cannot comment on phase transitions or a possible information-computational gap (see Section 1.2) in this setup based on the results of this paper."
    }, {
      "heading" : "2 Main Results",
      "text" : "Consider the heterogenous stochastic block model described above. In the proofs, we can allow for isolated nodes (communities of size 1) which are omitted from the model here to simplify the presentation. Denote by Y the set of admissible adjacency matrices according to a community assignment as above, i.e.,\nY := {Y ∈ {0, 1}n×n : Y is a valid community matrix w.r.t. V1, . . . , Vr where |Vk| = nk} .\nDefine the relative density of community k as\nρk = (pk − q)nk (2.1) which can be seen as the increase in the average degree of a node in community k in the SBM, relative to its average degree in an Erdős-Rényi model. Define nmin and nmax as the minimum and maximum of n1, . . . , nk respectively. The total variance over the kth community is defined as σ2k = nkpk(1 − pk) , and we let σ20 = nq(1− q) . Moreover, consider\nσ2max = max k=1,...,r σ2k = max k=1,...,r nkpk(1− pk) . (2.2)\nA Bernoulli random variable with parameter p is denoted by Ber(p) , and a Binomial random variable with parameters n and p is denoted by Bin(n, p) . The Neyman Chi-square divergence between the two discrete random variables Ber(p) and Ber(q) is given by\nD̃(p, q) := (p− q)2 q(1 − q) (2.3)\nand we have D̃(p, q) ≥ DKL(p, q) := DKL(Ber(p),Ber(q)) . Chi-square divergence is an instance of a more general family of divergence functions called f -divergences or Ali-Silvey distances. This family also has KL-divergence, total variation distance, Hellinger distance and Chernoff distance as special cases. Moreover, the divergence used in [1] is an f -divergence.\nLastly, log denotes the natural logarithm (base e), and the notation θ & 1 is equivalent to θ ≥ O(1) ."
    }, {
      "heading" : "2.1 Convex Recovery",
      "text" : "Inspired by the success of semidefinite programs in community detection (e.g., see [15, 21]) we consider a natural convex relaxation of the maximum likelihood estimator, similar to the one used in [12], for exact recovery of the heterogeneous SBM with a growing number of communities. Assuming that ζ = ∑r k=1 n 2 k is known, we solve\nŶ = argmax Y\n∑ i,j AijYij\nsubject to ‖Y ‖⋆ ≤ n , ∑ i,j Yij = ζ , 0 ≤ Yij ≤ 1 . (2.4)\nwhere ‖ · ‖⋆ denotes the nuclear norm (the sum of singular values of the matrix). We prove two theorems giving conditions under which the above convex program outputs the true community matrix with high probability. In establishing these performance guarantees, we follow the standard dual certificate argument in convex analysis while utilizing strong matrix concentration results from random matrix theory [8, 25, 26, 5]. These results allow us to bound the spectral radius of the matrix A−E[A] where A is an instance of adjacency matrix generated under heterogenous SBM. The proofs for both theorems along with the matrix concentration bounds are given in Appendix A.\nTheorem 1 Under the heterogenous stochastic block model, the output of the semidefinite program in (2.4) coincides with Y ⋆ with high probability, provided that\nρ2k & σ 2 k log nk , D̃(pmin, q) & lognmin nmin , ρ2min & max{σ2max, nq(1− q), logn}\nand ∑r\nk=1 n −α k = o(1) for some α > 0 .\nProof Sketch. For Y ⋆ to be the unique solution of (2.4), we need to show that for any feasible Y 6= Y ⋆ , the following quantity\n〈A, Y ⋆ − Y 〉 = 〈E[A], Y ⋆ − Y 〉+ 〈A− E[A], Y ⋆ − Y 〉 is strictly positive. In bounding the second term above, we make use of the constraint ‖Y ‖⋆ ≤ n = ‖Y ⋆‖⋆ by constructing a dual certificate from A − E[A] . This is where the bounds on the spectral norm (dual norm for the nuclear norm) of A − E[A] enter and we use matrix concentration bounds (see Lemma 7 in Appendix A).\nThe first condition of Theorem 1 is equivalent to each community being connected, second condition ensures that each community is identifiable (pmin − q is large enough), and the third condition requires minimal density to dominate global variability. The assumption ∑r k=1 n −α k = o(1) is tantamount to saying that the number of tiny communities cannot be too large (e.g., the number of polylogarithmic-size communities cannot be a power of n). In other words, one needs to have mostly large communities (growing like nǫ, for some ǫ > 0) for this assumption to be satisfied. Note, however, that the condition does not restrict the number of communities of size nǫ for any fixed ǫ > 0 . In fact, Theorem 1 allows us to describe a regime in which tiny communities of size O( √ logn) are recoverable provided that they are very dense and that only few tiny or small communities exist; see Example 3. The second theorem imposes more stringent conditions on the relative density, hence only allowing for communities of size down to logn , but relaxes the condition that only a small number of nodes can be in small communities.\nTheorem 2 Under the heterogenous stochastic block model, the output of the semidefinite program in (2.4) coincides with Y ⋆ , with high probability, provided that\nρ2k & σ 2 k logn , D̃(pmin, q) & logn nmin , ρ2min & max{σ2max , nq(1− q)} .\nThe proof of Theorem 2 is similar to the proof of Theorem 1 except that we use a different matrix concentration bound (see Lemma 10 in Appendix A)."
    }, {
      "heading" : "2.2 Recoverability Lower and Upper Bounds",
      "text" : "Next, we consider an estimator, inspired by maximum likelihood estimation, and identify a subset of the model space which is exactly recoverable via this estimator. The proposed estimation approach\nis not computationally tractable and is only used to examine the conditions for which exact recovery is possible. For a fixed Y ∈ Y and an observed matrix A , the likelihood function is given by\nPY (A) = ∏\ni<j\np AijYij τ(i,j) (1− pτ(i,j))(1−Aij)YijqAij(1−Yij)(1 − q)(1−Aij)(1−Yij),\nwhere τ : {1, . . . , n}2 → {1, . . . , r} and τ(i, j) = k if and only if i, j ∈ Vk , and arbitrary in {1, . . . , r} otherwise. The log-likelihood function is given by\nlogPY (A) = ∑\ni<j\nlog (1− q)pτ(i,j) q(1− pτ(i,j)) AijYij + ∑\ni<j\nlog 1− pτ(i,j)\n1− q Yij + terms not involving {Yij}.\nMaximizing the log-likelihood involves maximizing a weighted sum of {Yij}’s where the weights depend on the (usually unknown) values of q, p1, . . . , pr . To be able to work with less information, we will use the following modification of maximum likelihood estimation, which only uses the knowledge of n1, . . . , nr ,\nŶ = argmax Y ∈Y\nn∑\ni,j=1\nAijYij . (2.5)\nTheorem 3 Suppose nmin ≥ 2 and n ≥ 8 . Under the heterogenous stochastic block model, if\nρmin ≥ 4(17 + η) ( 1\n3 + pmin(1 − pmin) + q(1 − q) pmin − q\n) logn ,\nfor some choice of η > 0 , then the optimal solution Ŷ of the non-convex recovery program in (2.5) coincides with Y ⋆, with a probability not less than 1− 7 pmax−qpmin−q n 2−η .\nNotice that ρmin = mink=1,...,r nk(pk − q) and pmin = mink=1,...,r pk do not necessarily correspond to the same community. Similar to the proof of Theorem 1, we establish 〈A, Y ⋆ − Y 〉 > 0 for any Y ∈ Y , while this time, we use a counting argument (see Lemma 11 in Appendix B) similar to the one in [12]. The proofs for this Theorem and the next one are given in Appendix B.\nFinally, to provide a better picture of community detection for heterogenous SBM we provide the\nfollowing necessary conditions for exact recovery. Notice that Theorems 1 and 2 require D̃(q, pk) (in their first condition) and D̃(pk, q) (in their second condition) to be bounded from below for recoverability by the SDP. Similarly, the conditions of Theorem 4 can be seen as average-case and worst-case upper bounds on these divergences.\nTheorem 4 If any of the following conditions holds, (1) 2 ≤ nk ≤ n/e , and 4 ∑r k=1 n 2 kD̃(pk, q) ≤ 12 ∑ k nk log n nk − r − 2 (2) n ≥ 128 , r ≥ 2 and maxk { nkD̃(pk, q) + nkD̃(q, pk) } ≤ 112 log(n− nmin)\nthen inf Ŷ supY ⋆∈Y P[Ŷ 6= Y ⋆] ≥ 12 where the infimum is taken over all measurable estimators Ŷ based on the realization A generated according to the heterogenous stochastic block model."
    }, {
      "heading" : "2.3 Partial Observations",
      "text" : "In the general stochastic block model, we assume that the entries of a symmetric adjacency matrix A ∈ {0, 1}n×n have been generated according to a combination of Erdős-Rényi models with parameters that depend on the true community matrix. In the case of partial observations, we assume that the entries of A has been observed independently with probability γ . In fact, every entry of the input matrix falls into one of these categories: observed as one denoted by Ω1, observed as zero denoted by Ω0, and unobserved which corresponds to Ω\nc where Ω = Ω0 ∪Ω1 . If an estimator only takes the observed part of the matrix as the input, one can revise the underlying probabilistic model to incorporate both the stochastic block model and the observation model; i.e. a revised distribution for entries of A as\nAij = { Ber(γpk) i, j ∈ Vk for some k Ber(γq) i ∈ Vk and j ∈ Vl for k 6= l .\nyields the same output from an estimator that only takes in the observed values. Therefore, the estimators in (2.4) and (2.5), as well as the results of Theorems 1, 2, 3, can be easily adapted to the case of partially observed graphs. It is worth mentioning that the above model for partially observed SBM (which is another SBM) is different from another random model known as Censored Block Model (CBM) [4]. In SBM, absence of an edge provides information, whereas in CBM it does not."
    }, {
      "heading" : "3 Tradeoffs in Heterogenous SBM",
      "text" : "As it can be seen from the results presented in this paper, and the main summary statistics they utilize (the relative densities ρ1, . . . , ρr), the parameters of SBM can vary significantly and still satisfy the same recoverability conditions. In the following, we examine a number of such tradeoffs which leads to recovery guarantees for interesting SBM configurations. Here, a configuration is a list of community sizes nk, their connectivity probabilities pk, and the inter-community connectivity probability q . A triple (m, p, k) represents k communities of size m each, with connectivity parameter p . We do not worry about whether m and k are always integers; if they are not, one can always round up or down as needed so that the total number of vertices is n, without changing the asymptotics. Moreover, when the O(·) notation is used, we mean that appropriate constants can be determined. A detailed list of computations for the examples in this section are given in Appendix D.\nBetter Summary Statistics. It is intuitive that using summary statistics such as (pmin, nmin), for a heterogenous SBM where nk’s and pk’s are allowed to take very different values, can be very limiting. Examples 1 and 2 are intended to give configurations that are guaranteed to be recoverable by our results but fail the existing recoverability conditions in the literature. Example 1 Suppose we have two communities of sizes n1 = n− √ n, n2 = √ n, with p1 = n −2/3 and p2 = 1/ logn while q = n −2/3−0.01 . The bound we obtain here in Theorem 3 makes it clear that this case is theoretically solvable (the modified maximum likelihood estimator successfully recovers it). By contrast, Theorem 3.1 in [7] (specialized for the case of no outliers), requiring\nn2min(pmin − q)2 & ( √ pminnmin + √ nq)2 logn , (3.1)\nwould fail and provide no guarantee for recoverability.\nExample 2 Consider a configuration as\n(n− n2/3, n−1/3+ǫ, 1) , ( √ n, O( 1logn ), n 1/6) , q = n−2/3+3ǫ\nwhere ǫ is a small quantity, e.g., ǫ = 0.1 . Either of Theorems 1 and 2 certify this case as recoverable via the semidefinite program (2.4) with high probability. By contrast, using the pmin = n\n−1/3+ǫ and nmin = √ n heuristic, neither the condition of Theorem 3.1 in [7] (given in (3.1)) nor the condition of Theorem 2.5 in [12] is fulfilled, hence providing no recovery guarantee for this configuration."
    }, {
      "heading" : "3.1 Small communities can be efficiently recovered",
      "text" : "Most algorithms for clustering the SBM run into the problem of small communities [11, 6, 19], often because the models employed do not allow for enough parameter variation to identify the key quantities involved. The next three examples attempt to provide an idea of how small the community\nsizes can be, how many small communities are allowed, and how wide the spread of community sizes can be, as characterized by our results.\nExample 3 (smallest community size for convex recovery) Consider a configuration as\n( √ logn, O(1), m) , (n2, O( log n√ n ), √ n) , q = O( log nn )\nwhere n2 = √ n−m √ logn/n to ensure a total of n vertices. Here, we assume m ≤ n/(2√logn) which implies n2 ≥ √ n/2 . It is straightforward to verify the conditions of Theorem 1.\nTo our knowledge, this is the first example in the literature for which semidefinite programming based recovery works and allows the recovery of (a few) communities of size smaller than logn. Previously, logn was considered to be the standard bound on the community size for exact recovery, as illustrated by Theorem 2.5 of [12] in the case of equivalent communities. We have thus shown that it is possible, in the right circumstances (when sizes are spread and the smaller the community the denser it is), to recover very small communities (up to √ logn size), if there are just a few of them (at most polylogarithmic in n). The significant improvement we made in the bound on the size of the smallest community is due to the fact that we were able to perform a closer analysis of the semidefinite program by utilizing stronger matrix concentration bounds, mainly borrowed from [8, 25, 26, 5]. For more details, see Appendix A.2.\nNotice that the condition of Theorem 3 is not satisfied. This is not an inconsistency (as Theorem 3 gives only an upper bound for the threshold), but indicates the limitation of this theorem in characterizing all recoverable cases.\nSpreading the sizes. As mentioned before, while Theorem 1 allows for going lower than the standard logn bound on the community size for exact recovery, it requires the number of very small communities to be relatively small. On the other hand, Theorem 2 provides us with the option of having many small communities but requires the smallest community to be of size O(log n) . We explore two cases with many small communities in the following.\nExample 4 Consider a configuration where small communities are dense and there is one big community,\n(12n ǫ, O(1), n1−ǫ) , (12n, n −α logn, 1) , q = O(n−β logn)\nwith 0 < ǫ < 1 and 0 < α < β < 1. We are interested to see how large the number of small communities can be. Then the conditions of Theorems 1 and 2 both require that\n1 2 (1− α) < ǫ < 2(1− α) , ǫ > 2α− β (3.2)\nand are depicted in Figure 1. Since we have not specified the constants in our results, we only consider strict inequalities.\nNotice that the small communities are as dense as can be, but the large one is not necessarily very dense. By picking ǫ to be just over 1/4, we can make α just shy of 1/2, and β very close to 1. As\nfar as we can tell, there are no results in the literature surveyed that cover such a case, although the clever “peeling” strategy introduced in [3] would recover the largest community. The strongest result in [3] that seems applicable here is Corollary 4 (which works for non-constant probabilities). The algorithm in [3] works to recover a large community (larger than O( √ n log2 n)), subject to existence of a gap in the community sizes (roughly, there should be no community sizes between O( √ n) and O( √ n log2 n)). Therefore, in this example, after a single iteration, the algorithm will stop, despite the continued existence of a gap, as there is no community with size above the gap. Hence the “peeling” strategy on this example would fail to recover all the communities.\nExample 5 Consider a configuration with many small dense communities of size logn . We are interested to see how large the spread of community sizes can be for the semidefinite program to work. As required by Theorems 1 and 2 and to control σmax (defined in (2.2)), the larger a community the smaller its connectivity probability should be; therefore we choose the largest community at the threshold of connectivity (required for recovery). Consider the community sizes and probabilities:\n(log n, O(1), n/logn−m √ n/logn) , ( √ n logn, O( √ (logn)/n), m) , q = O((log n)/n)\nwhere m is a constant. Again, we round up or down where necessary to make sure the sizes are integers and the total number of vertices is n. All the conditions of Theorem 2 are satisfied and exact convex recovery is possible via the semidefinite program. Note that the last condition of Theorem 1 is not satisfied since there are too many small communities. Also note that alternative methods proposed in the literature surveyed would not be applicable; in particular, the gap condition in [3] is not satisfied for this case from the start."
    }, {
      "heading" : "3.2 Weak communities are efficiently recoverable",
      "text" : "The following examples illustrate how small pmin − q can be in order for the recovery, respectively, the convex recovery algorithms to still be guaranteed to work. When some pk is very close to q , the Erdős-Rényi model G(nk, pk) looks very similar to the ambient edges from G(n, q) . Again, we are going to exploit the possible tradeoffs in the parameters of SBM to guarantee recovery. Note that the difference in pmin − q for the two types of recovery is noticeable, indicating that there is a significant difference between what we know to be recoverable and what we can recover efficiently by our convex method. We consider both dense graphs (where pmin is O(1)) and sparse ones.\nExample 6 Consider a configuration where all of the probabilities are O(1) and\n(n1, pmin, 1) , (nmin, p2, 1) , (n3, p3, n−n1−nmin\nn3 ) , q = O(1)\nwhere p2 − q and p3 − q are O(1) . On the other hand, we assume pmin − q = f(n) is small. For recoverability by Theorem 3, we need f(n) & (logn)/nmin and f 2(n) & (logn)/n1 . Notice that,\nsince n & n1 & nmin , we should have f(n) & √ log n/n . For the convex program to recover this configuration (by Theorem 1 or 2), we need nmin & √ n and f2(n) & max{n/n21 , logn/nmin} , while all the probabilities are O(1) .\nNote that if all the probabilities, as well as pmin − q , are O(1), then by Theorem 3 all communities down to a logarithmic size should be recoverable. However, the success of convex recovery is guaranteed by Theorems 1 and 2 when nmin & √ n .\nFor a similar configuration to Example 6, where the probabilities are not O(1) , recoverability by Theorem 3 requires f(n) & max{ √ pmin(logn)/n , n −c} for some appropriate c > 0 ."
    }, {
      "heading" : "4 Discussion",
      "text" : "We have provided a series of extensions to prior works (especially [12, 1]) by considering the exact recovery for stochastic block model in its full generality with a growing number of communities. By capturing the tradeoffs among the various parameters of SBM, we have identified interesting SBM configurations that are efficiently recoverable via semidefinite programs. However there are still interesting problems that remain open. Sharp thresholds for recovery or approximation of heterogenous SBM, models for partial observation (non-uniform, based on prior information, or adaptive as in [28]), as well as overlapping communities (e.g., [1]) are important future directions. Moreover, other estimators similar to the ones considered in this paper can be analyzed; e.g. when the unknown parameters in the maximum likelihood estimator, or ζ in (2.4), are estimated from the given observations."
    } ],
    "references" : [ {
      "title" : "Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms",
      "author" : [ "E. Abbe", "C. Sandon" ],
      "venue" : "arXiv preprint arXiv:1503.00609",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Detection in the stochastic block model with multiple clusters: proof of the achievability conjectures",
      "author" : [ "E. Abbe", "C. Sandon" ],
      "venue" : "acyclic bp, and the information-computation gap. arXiv preprint arXiv:1512.09080",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Breaking the small cluster barrier of graph clustering",
      "author" : [ "N. Ailon", "Y. Chen", "H. Xu" ],
      "venue" : "ICML, pages 995–1003",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An efficient algorithm for exact recovery of vertex variables from edge measurements",
      "author" : [ "A.S. Bandeira" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Sharp nonasymptotic bounds on the norm of random matrices with independent entries",
      "author" : [ "A.S. Bandeira", "R. van Handel" ],
      "venue" : "arXiv preprint arXiv:1408.6185,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Eigenvalues and graph bisection: An average-case analysis",
      "author" : [ "R.B. Boppana" ],
      "venue" : "Foundations of Computer Science, 1987., 28th Annual Symposium on, pages 280–285. IEEE",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Robust and computationally feasible community detection in the presence of arbitrary outlier nodes",
      "author" : [ "T.T. Cai", "X. Li" ],
      "venue" : "Ann. Statist., 43(3):1027–1059",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Matrix estimation by universal singular value thresholding",
      "author" : [ "S. Chatterjee" ],
      "venue" : "Ann. Statist., 43(1):177–214",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Detecting functional modules in the yeast protein–protein interaction network",
      "author" : [ "J. Chen", "B. Yuan" ],
      "venue" : "Bioinformatics, 22(18):2283–2290",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Clustering partially observed graphs via convex optimization",
      "author" : [ "Y. Chen", "A. Jalali", "S. Sanghavi", "H. Xu" ],
      "venue" : "J. Mach. Learn. Res., 15:2213–2238",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Clustering sparse graphs",
      "author" : [ "Y. Chen", "S. Sanghavi", "H. Xu" ],
      "venue" : "Advances in neural information processing systems, pages 2204–2212",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices",
      "author" : [ "Y. Chen", "J. Xu" ],
      "venue" : "J. Mach. Learn. Res., 17(27):1–57",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Graph partitioning via adaptive spectral techniques",
      "author" : [ "A. Coja-Oghlan" ],
      "venue" : "Combin. Probab. Comput., 19(2):227–284",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications",
      "author" : [ "A. Decelle", "F. Krzakala", "C. Moore", "L. Zdeborová" ],
      "venue" : "Physical Review E, 84(6):066106",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Community detection in sparse networks via grothendieck’s inequality",
      "author" : [ "O. Guédon", "R. Vershynin" ],
      "venue" : "Probability Theory and Related Fields, pages 1–25",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Stochastic blockmodels: First steps",
      "author" : [ "P.W. Holland", "K.B. Laskey", "S. Leinhardt" ],
      "venue" : "Social networks, 5(2):109–137",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Cluster analysis for gene expression data: A survey",
      "author" : [ "D. Jiang", "C. Tang", "A. Zhang" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on, 16(11):1370–1386",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Empirical comparison of algorithms for network community detection",
      "author" : [ "J. Leskovec", "K.J. Lang", "M. Mahoney" ],
      "venue" : "Proceedings of the 19th international conference on World wide web, pages 631–640. ACM",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Spectral partitioning of random graphs",
      "author" : [ "F. McSherry" ],
      "venue" : "Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Symposium on, pages 529–537. IEEE",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A random walks view of spectral segmentation",
      "author" : [ "M. Meila", "J. Shi" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "Semidefinite programs on sparse random graphs",
      "author" : [ "A. Montanari", "S. Sen" ],
      "venue" : "arXiv preprint arXiv:1504.05910",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Consistency thresholds for the planted bisection model",
      "author" : [ "E. Mossel", "J. Neeman", "A. Sly" ],
      "venue" : "Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 69–75. ACM",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Spectral clustering and the high-dimensional stochastic blockmodel",
      "author" : [ "K. Rohe", "S. Chatterjee", "B. Yu" ],
      "venue" : "The Annals of Statistics, 39(4):1878–1915",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Normalized cuts and image segmentation",
      "author" : [ "J. Shi", "J. Malik" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 22(8):888–905",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Distributed user profiling via spectral methods",
      "author" : [ "D.-C. Tomozei", "L. Massoulié" ],
      "venue" : "Stoch. Syst., 4(1):1–43",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A simple svd algorithm for finding hidden partitions",
      "author" : [ "V. Vu" ],
      "venue" : "arXiv:1404.3918",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Jointly clustering rows and columns of binary matrices: Algorithms and trade-offs",
      "author" : [ "J. Xu", "R. Wu", "K. Zhu", "B. Hajek", "R. Srikant", "L. Ying" ],
      "venue" : "The 2014 ACM international conference on Measurement and modeling of computer systems, pages 29–41. ACM",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Community detection via random and adaptive sampling",
      "author" : [ "S.-Y. Yun", "A. Proutiere" ],
      "venue" : "Proceedings of The 27th Conference on Learning Theory, pages 138–175",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Applications include recommendation systems [27], image segmentation [24, 20], learning gene network structures in bioinformatics, e.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "Applications include recommendation systems [27], image segmentation [24, 20], learning gene network structures in bioinformatics, e.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "Applications include recommendation systems [27], image segmentation [24, 20], learning gene network structures in bioinformatics, e.",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : ", in protein detection [9] and population genetics [17].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 16,
      "context" : ", in protein detection [9] and population genetics [17].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : ", [18] for an empirical overview), as well as strong research interest in recent years on the theoretical side as briefly reviewed in the sequel, there are still gaps in understanding the fundamental information theoretic limits of recoverability (i.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "The stochastic block model (SBM), first introduced and studied in mathematical sociology by Holland, Laskey and Leinhardt in 1983 [16], can be described as follows.",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 12,
      "context" : "In this paper, we assume the above model while allowing the number of communities to grow with the number of nodes (similar to [13, 15, 23]).",
      "startOffset" : 127,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "In this paper, we assume the above model while allowing the number of communities to grow with the number of nodes (similar to [13, 15, 23]).",
      "startOffset" : 127,
      "endOffset" : 139
    }, {
      "referenceID" : 22,
      "context" : "In this paper, we assume the above model while allowing the number of communities to grow with the number of nodes (similar to [13, 15, 23]).",
      "startOffset" : 127,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : ", [12], or, 2) SBM with linear-sized communities, where the number of communities is fixed and all community sizes are O(n); e.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 11,
      "context" : "Recovery, where the proportion of misclassified nodes is negligible; either 0 (corresponding to exact recovery with strong consistency, and considered in [12, 1]) or asymptotically 0 (corresponding to exact recovery with weak consistency as considered in [23, 22, 28]) as the number of nodes grows.",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "Recovery, where the proportion of misclassified nodes is negligible; either 0 (corresponding to exact recovery with strong consistency, and considered in [12, 1]) or asymptotically 0 (corresponding to exact recovery with weak consistency as considered in [23, 22, 28]) as the number of nodes grows.",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 22,
      "context" : "Recovery, where the proportion of misclassified nodes is negligible; either 0 (corresponding to exact recovery with strong consistency, and considered in [12, 1]) or asymptotically 0 (corresponding to exact recovery with weak consistency as considered in [23, 22, 28]) as the number of nodes grows.",
      "startOffset" : 255,
      "endOffset" : 267
    }, {
      "referenceID" : 21,
      "context" : "Recovery, where the proportion of misclassified nodes is negligible; either 0 (corresponding to exact recovery with strong consistency, and considered in [12, 1]) or asymptotically 0 (corresponding to exact recovery with weak consistency as considered in [23, 22, 28]) as the number of nodes grows.",
      "startOffset" : 255,
      "endOffset" : 267
    }, {
      "referenceID" : 27,
      "context" : "Recovery, where the proportion of misclassified nodes is negligible; either 0 (corresponding to exact recovery with strong consistency, and considered in [12, 1]) or asymptotically 0 (corresponding to exact recovery with weak consistency as considered in [23, 22, 28]) as the number of nodes grows.",
      "startOffset" : 255,
      "endOffset" : 267
    }, {
      "referenceID" : 12,
      "context" : "This regime was first introduced in [13, 14], and has been considered in many other works since then; e.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "This regime was first introduced in [13, 14], and has been considered in many other works since then; e.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "See [10] or [1, Section 5] for a summary of such results.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 20,
      "context" : "see [21] on semidefinite programming based methods) and assessing whether successful retrieval can be achieved by tractable algorithms at the sharp statistical thresholds or there is a gap.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "So far, it is understood that there is no such gap in the case of exact recovery (weak and strong) and approximation of binary SBM as well as the exact recovery of linear-sized communities [1].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : ", see [2] and the list of unresolved conjectures therein.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : ", peeling strategy [3]) providing false negatives.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Moreover, the divergence used in [1] is an f -divergence.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : ", see [15, 21]) we consider a natural convex relaxation of the maximum likelihood estimator, similar to the one used in [12], for exact recovery of the heterogeneous SBM with a growing number of communities.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 20,
      "context" : ", see [15, 21]) we consider a natural convex relaxation of the maximum likelihood estimator, similar to the one used in [12], for exact recovery of the heterogeneous SBM with a growing number of communities.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : ", see [15, 21]) we consider a natural convex relaxation of the maximum likelihood estimator, similar to the one used in [12], for exact recovery of the heterogeneous SBM with a growing number of communities.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "In establishing these performance guarantees, we follow the standard dual certificate argument in convex analysis while utilizing strong matrix concentration results from random matrix theory [8, 25, 26, 5].",
      "startOffset" : 192,
      "endOffset" : 206
    }, {
      "referenceID" : 24,
      "context" : "In establishing these performance guarantees, we follow the standard dual certificate argument in convex analysis while utilizing strong matrix concentration results from random matrix theory [8, 25, 26, 5].",
      "startOffset" : 192,
      "endOffset" : 206
    }, {
      "referenceID" : 25,
      "context" : "In establishing these performance guarantees, we follow the standard dual certificate argument in convex analysis while utilizing strong matrix concentration results from random matrix theory [8, 25, 26, 5].",
      "startOffset" : 192,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "In establishing these performance guarantees, we follow the standard dual certificate argument in convex analysis while utilizing strong matrix concentration results from random matrix theory [8, 25, 26, 5].",
      "startOffset" : 192,
      "endOffset" : 206
    }, {
      "referenceID" : 11,
      "context" : "Similar to the proof of Theorem 1, we establish 〈A, Y ⋆ − Y 〉 > 0 for any Y ∈ Y , while this time, we use a counting argument (see Lemma 11 in Appendix B) similar to the one in [12].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "It is worth mentioning that the above model for partially observed SBM (which is another SBM) is different from another random model known as Censored Block Model (CBM) [4].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "1 in [7] (specialized for the case of no outliers), requiring",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 11,
      "context" : "5 in [12] is fulfilled, hence providing no recovery guarantee for this configuration.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "Most algorithms for clustering the SBM run into the problem of small communities [11, 6, 19], often because the models employed do not allow for enough parameter variation to identify the key quantities involved.",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Most algorithms for clustering the SBM run into the problem of small communities [11, 6, 19], often because the models employed do not allow for enough parameter variation to identify the key quantities involved.",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 18,
      "context" : "Most algorithms for clustering the SBM run into the problem of small communities [11, 6, 19], often because the models employed do not allow for enough parameter variation to identify the key quantities involved.",
      "startOffset" : 81,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "5 of [12] in the case of equivalent communities.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 7,
      "context" : "The significant improvement we made in the bound on the size of the smallest community is due to the fact that we were able to perform a closer analysis of the semidefinite program by utilizing stronger matrix concentration bounds, mainly borrowed from [8, 25, 26, 5].",
      "startOffset" : 253,
      "endOffset" : 267
    }, {
      "referenceID" : 24,
      "context" : "The significant improvement we made in the bound on the size of the smallest community is due to the fact that we were able to perform a closer analysis of the semidefinite program by utilizing stronger matrix concentration bounds, mainly borrowed from [8, 25, 26, 5].",
      "startOffset" : 253,
      "endOffset" : 267
    }, {
      "referenceID" : 25,
      "context" : "The significant improvement we made in the bound on the size of the smallest community is due to the fact that we were able to perform a closer analysis of the semidefinite program by utilizing stronger matrix concentration bounds, mainly borrowed from [8, 25, 26, 5].",
      "startOffset" : 253,
      "endOffset" : 267
    }, {
      "referenceID" : 4,
      "context" : "The significant improvement we made in the bound on the size of the smallest community is due to the fact that we were able to perform a closer analysis of the semidefinite program by utilizing stronger matrix concentration bounds, mainly borrowed from [8, 25, 26, 5].",
      "startOffset" : 253,
      "endOffset" : 267
    }, {
      "referenceID" : 2,
      "context" : "far as we can tell, there are no results in the literature surveyed that cover such a case, although the clever “peeling” strategy introduced in [3] would recover the largest community.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "The strongest result in [3] that seems applicable here is Corollary 4 (which works for non-constant probabilities).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "The algorithm in [3] works to recover a large community (larger than O( √ n log(2) n)), subject to existence of a gap in the community sizes (roughly, there should be no community sizes between O( √ n) and O( √ n log(2) n)).",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "Also note that alternative methods proposed in the literature surveyed would not be applicable; in particular, the gap condition in [3] is not satisfied for this case from the start.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "We have provided a series of extensions to prior works (especially [12, 1]) by considering the exact recovery for stochastic block model in its full generality with a growing number of communities.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "We have provided a series of extensions to prior works (especially [12, 1]) by considering the exact recovery for stochastic block model in its full generality with a growing number of communities.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 27,
      "context" : "Sharp thresholds for recovery or approximation of heterogenous SBM, models for partial observation (non-uniform, based on prior information, or adaptive as in [28]), as well as overlapping communities (e.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : ", [1]) are important future directions.",
      "startOffset" : 2,
      "endOffset" : 5
    } ],
    "year" : 2016,
    "abstractText" : "The Stochastic Block Model (SBM) is a widely used random graph model for networks with communities. Despite the recent burst of interest in community detection under the SBM from statistical and computational points of view, there are still gaps in understanding the fundamental limits of recovery. In this paper, we consider the SBM in its full generality, where there is no restriction on the number and sizes of communities or how they grow with the number of nodes, as well as on the connectivity probabilities inside or across communities. For such stochastic block models, we provide guarantees for exact recovery via a semidefinite program as well as upper and lower bounds on SBM parameters for exact recoverability. Our results exploit the tradeoffs among the various parameters of heterogenous SBM and provide recovery guarantees for many new interesting SBM configurations.",
    "creator" : null
  }
}