{
  "name" : "792c7b5aae4a79e78aaeda80516ae2ac.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Global Analysis of Expectation Maximization for Mixtures of Two Gaussians",
    "authors" : [ "Ji Xu", "Daniel Hsu", "Arian Maleki" ],
    "emails" : [ "jixu@cs.columbia.edu", "djhsu@cs.columbia.edu", "arian@stat.columbia.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Since Fisher’s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering. The asymptotic consistency and optimality of MLEs have provided users with the confidence that, at least in some sense, there is no better way to estimate parameters for many standard statistical models. Despite its appealing properties, computing the MLE is often intractable. Indeed, this is the case for many latent variable models {f(Y, z;η)}, where the latent variables z are not observed. For each setting of the parameters η, the marginal distribution of the observed data Y is (for discrete z)\nf(Y;η) = ∑ z f(Y, z;η) .\nIt is this marginalization over latent variables that typically causes the computational difficulty. Furthermore, many algorithms based on the MLE principle are only known to find stationary points of the likelihood objective (e.g., local maxima), and these points are not necessarily the MLE."
    }, {
      "heading" : "1.1 Expectation Maximization",
      "text" : "Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984). EM is an iterative algorithm for climbing the likelihood objective starting from an initial setting of the parameters η̂〈0〉. In iteration t, EM performs the following steps:\nE-step: Q̂(η | η̂〈t〉) , ∑ z f(z | Y; η̂〈t〉) log f(Y, z;η) , (1)\nM-step: η̂〈t+1〉 , arg max η Q̂(η | η̂〈t〉) , (2)\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nIn many applications, each step is intuitive and can be performed very efficiently.\nDespite the popularity of EM, as well as the numerous theoretical studies of its behavior, many important questions about its performance—such as its convergence rate and accuracy—have remained unanswered. The goal of this paper is to address these questions for specific models (described in Section 1.2) in which the observation Y is an i.i.d. sample from a mixture of two Gaussians. Towards this goal, we study an idealized execution of EM in the large sample limit, where the E-step is modified to be computed over an infinitely large i.i.d. sample from a Gaussian mixture distribution in the model. In effect, in the formula for Q̂(η | η̂〈t〉), we replace the observed data Y with a random variable Y ∼ f(y;η?) for some Gaussian mixture parameters η? and then take its expectation. The resulting E- and M-steps in iteration t are\nE-step: Q(η | η〈t〉) , EY [∑ z f(z | Y ;η〈t〉) log f(Y , z;η) ] , (3)\nM-step: η〈t+1〉 , arg max η Q(η | η〈t〉) . (4)\nThis sequence of parameters (η〈t〉)t≥0 is fully determined by the initial setting η〈0〉. We refer to this idealization as Population EM, a procedure considered in previous works of Srebro (2007) and Balakrishnan et al. (2014). Not only does Population EM shed light on the dynamics of EM in the large sample limit, but it can also reveal some of the fundamental limitations of EM. Indeed, if Population EM cannot provide an accurate estimate for the parameters η?, then intuitively, one would not expect the EM algorithm with a finite sample size to do so either. (To avoid confusion, we refer the original EM algorithm run with a finite sample as Sample-based EM.)"
    }, {
      "heading" : "1.2 Models and Main Contributions",
      "text" : "In this paper, we study EM in the context of two simple yet popular and well-studied Gaussian mixture models. The two models, along with the corresponding Sample-based EM and Population EM updates, are as follows:\nModel 1. The observation Y is an i.i.d. sample from the mixture distribution 0.5N(−θ?,Σ) + 0.5N(θ?,Σ); Σ is a known covariance matrix in Rd, and θ? is the unknown parameter of interest.\n1. Sample-based EM iteratively updates its estimate of θ? according to the following equation:\nθ̂ 〈t+1〉 = 1\nn n∑ i=1 ( 2wd ( yi, θ̂ 〈t〉) − 1 ) yi, (5)\nwhere y1, . . . ,yn are the independent draws that comprise Y ,\nwd(y,θ) , φd(y − θ)\nφd(y − θ) + φd(y + θ) ,\nand φd is the density of a Gaussian random vector with mean 0 and covariance Σ. 2. Population EM iteratively updates its estimate according to the following equation:\nθ〈t+1〉 = E(2wd(Y ,θ〈t〉)− 1)Y , (6) where Y ∼ 0.5N(−θ?,Σ) + 0.5N(θ?,Σ).\nModel 2. The observation Y is an i.i.d. sample from the mixture distribution 0.5N(µ?1,Σ) + 0.5N(µ?2,Σ). Again, Σ is known, and (µ ? 1,µ ? 2) are the unknown parameters of interest.\n1. Sample-based EM iteratively updates its estimate of µ?1 and µ ? 2 at every iteration according\nto the following equations:\nµ̂ 〈t+1〉 1 =\n∑n i=1 vd(yi, µ̂ 〈t〉 1 , µ̂\n〈t〉 2 )yi∑n\ni=1 vd(yi, µ̂ 〈t〉 1 , µ̂ 〈t〉 2 )\n, (7)\nµ̂ 〈t+1〉 2 =\n∑n i=1(1− vd(yi, µ̂ 〈t〉 1 , µ̂\n〈t〉 2 ))yi∑n\ni=1(1− vd(yi, µ̂ 〈t〉 1 , µ̂ 〈t〉 2 ))\n, (8)\nwhere y1, . . . ,yn are the independent draws that comprise Y , and\nvd(y,µ1,µ2) , φd(y − µ1)\nφd(y − µ1) + φd(y − µ2) .\n2. Population EM iteratively updates its estimates according to the following equations:\nµ 〈t+1〉 1 =\nEvd(Y ,µ〈t〉1 ,µ 〈t〉 2 )Y\nEvd(Y ,µ〈t〉1 ,µ 〈t〉 2 )\n, (9)\nµ 〈t+1〉 2 =\nE(1− vd(Y ,µ〈t〉1 ,µ 〈t〉 2 ))Y\nE(1− vd(Y ,µ〈t〉1 ,µ 〈t〉 2 ))\n, (10)\nwhere Y ∼ 0.5N(µ?1,Σ) + 0.5N(µ?2,Σ).\nOur main contribution in this paper is a new characterization of the stationary points and dynamics of EM in both of the above models.\n1. We prove convergence for the sequence of iterates for Population EM from each model: the sequence (θ〈t〉)t≥0 converges to either θ?, −θ?, or 0; the sequence ((µ〈t〉1 ,µ 〈t〉 2 ))t≥0\nconverges to either (µ?1,µ ? 2), (µ ? 2,µ ? 1), or ((µ ? 1 + µ ? 2)/2, (µ ? 1 + µ ? 2)/2). We also fully\ncharacterize the initial parameter settings that lead to each limit point. 2. Using this convergence result for Population EM, we also prove that the limits of the Sample-\nbased EM iterates converge in probability to the unknown parameters of interest, as long as Sample-based EM is initialized at points where Population EM would converge to these parameters as well.\nFormal statements of our results are given in Section 2."
    }, {
      "heading" : "1.3 Background and Related Work",
      "text" : "The EM algorithm was formally introduced by Dempster et al. (1977) as a general iterative method for computing parameter estimates from incomplete data. Although EM is billed as a procedure for maximum likelihood estimation, it is known that with certain initializations, the final parameters returned by EM may be far from the MLE, both in parameter distance and in log-likelihood value (Wu, 1983). Several works characterize convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Vaida, 2005; Chrétien and Hero, 2008). However, these analyses do not distinguish between global maximizers and other stationary points (except, e.g., when the likelihood function is unimodal). Thus, as an optimization algorithm for maximizing the log-likelihood objective, the “worst-case” performance of EM is somewhat discouraging.\nFor a more optimistic perspective on EM, one may consider a “best-case” analysis, where (i) the data are an iid sample from a distribution in the given model, (ii) the sample size is sufficiently large, and (iii) the starting point for EM is sufficiently close to the parameters of the data generating distribution. Conditions (i) and (ii) are ubiquitous in (asymptotic) statistical analyses, and (iii) is a generous assumption that may be satisfied in certain cases. Redner and Walker (1984) show that in such a favorable scenario, EM converges to the MLE almost surely for a broad class of mixture models. Moreover, recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence guarantees in certain models; importantly, these results permit one to quantify the accuracy of a pilot estimator required to effectively initialize EM. Thus, EM may be used in a tractable two-stage estimation procedures given a first-stage pilot estimator that can be efficiently computed.\nIndeed, for the special case of Gaussian mixtures, researchers in theoretical computer science and machine learning have developed efficient algorithms that deliver the highly accurate parameter estimates under appropriate conditions. Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either dα or kβ for some α, β > 0 for a mixture of k Gaussians in Rd (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a). More recent work employs the method-of-moments, which permit the means of the\nmixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015). In particular, Hardt and Price (2015) characterize the information-theoretic limits of parameter estimation for mixtures of two Gaussians, and that they are achieved by a variant of the original method-of-moments of Pearson (1894).\nMost relevant to this paper are works that specifically analyze EM (or variants thereof) for Gaussian mixture models, especially when the mixture components are well-separated. Xu and Jordan (1996) show favorable convergence properties (akin to super-linear convergence near the MLE) for wellseparated mixtures. In a related but different vein, Dasgupta and Schulman (2007) analyze a variant of EM with a particular initialization scheme, and proves fast convergence to the true parameters, again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians, it is possible to exploit symmetries to get sharper analyses. Indeed, Chaudhuri et al. (2009b) uses these symmetries to prove that a variant of Lloyd’s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption. Lastly, for the specific case of our Model 1, Balakrishnan et al. (2014) proves linear convergence of EM (as well as a gradient-based variant of EM) when started in a sufficiently small neighborhood around the true parameters, assuming a minimum separation between the mixture components. Here, the permitted size of the neighborhood grows with the separation between the components, and a recent result of Klusowski and Brinda (2016) quantitatively improves this aspect of the analysis (but still requires a minimum separation). Remarkably, by focusing attention on the local region around the true parameters, they obtain nonasymptotic bounds on the parameter estimation error. Our work is complementary to their results in that we focus on asymptotic limits rather than finite sample analysis. This allows us to provide a global analysis of EM without separation or initialization conditions, which cannot be deduced from the results of Balakrishnan et al. or Klusowski and Brinda by taking limits.\nFinally, two related works have appeared following the initial posting of this article (Xu et al., 2016). First, Daskalakis et al. (2016) concurrently and independently proved a convergence result comparable to our Theorem 1 for Model 1; for this case, they also provide an explicit rate of linear convergence. Second, Jin et al. (2016) show that similar results do not hold in general for uniform mixtures of three or more spherical Gaussian distributions: common initialization schemes for (Population or Sample-based) EM may lead to local maxima that are arbitrarily far from the global maximizer. Similar results were well-known for Lloyd’s algorithm, but were not previously established for Population EM (Srebro, 2007)."
    }, {
      "heading" : "2 Analysis of EM for Mixtures of Two Gaussians",
      "text" : "In this section, we present our results for Population EM and Sample-based EM under both Model 1 and Model 2, and also discuss further implications about the expected log-likelihood function. Without loss of generality, we may assume that the known covariance matrix Σ is the identity matrix Id. Throughout, we denote the Euclidean norm by ‖ · ‖, and the signum function by sgn(·) (where sgn(0) = 0, sgn(z) = 1 if z > 0, and sgn(z) = −1 if z < 0)."
    }, {
      "heading" : "2.1 Main Results for Population EM",
      "text" : "We present results for Population EM for both models, starting with Model 1.\nTheorem 1. Assume θ? ∈ Rd \\ {0}. Let (θ〈t〉)t≥0 denote the Population EM iterates for Model 1, and suppose 〈θ〈0〉,θ?〉 6= 0. There exists κθ ∈ (0, 1)—depending only on θ? and θ〈0〉—such that∥∥∥θ〈t+1〉 − sgn(〈θ〈0〉,θ?〉)θ?∥∥∥ ≤ κθ · ∥∥∥θ〈t〉 − sgn(〈θ〈0〉,θ?〉)θ?∥∥∥ . The proof of Theorem 1 and all other omitted proofs are given in the full version of this article (Xu et al., 2016). Theorem 1 asserts that if θ〈0〉 is not on the hyperplane {x ∈ Rd : 〈x,θ?〉 = 0}, then the sequence (θ〈t〉)t≥0 converges to either θ? or −θ?.\nOur next result shows that if 〈θ〈0〉,θ?〉 = 0, then (θ〈t〉)t≥0 still converges, albeit to 0.\nTheorem 2. Let (θ〈t〉)t≥0 denote the Population EM iterates for Model 1. If 〈θ〈0〉,θ?〉 = 0, then\nθ〈t〉 → 0 as t→∞ .\nTheorems 1 and 2 together characterize the fixed points of Population EM for Model 1, and fully specify the conditions under which each fixed point is reached. The results are simply summarized in the following corollary.\nCorollary 1. If (θ〈t〉)t≥0 denote the Population EM iterates for Model 1, then\nθ〈t〉 → sgn(〈θ〈0〉,θ?〉)θ? as t→∞ .\nWe now discuss Population EM with Model 2. To state our results more concisely, we use the following re-parameterization of the model parameters and Population EM iterates:\na〈t〉 , µ 〈t〉 1 + µ 〈t〉 2\n2 − µ\n? 1 + µ ? 2\n2 , b〈t〉 ,\nµ 〈t〉 2 − µ 〈t〉 1\n2 , θ? , µ?2 − µ?1 2 . (11)\nIf the sequence of Population EM iterates ((µ〈t〉1 ,µ 〈t〉 2 ))t≥0 converges to (µ ? 1,µ ? 2), then we expect b〈t〉 → θ?. Hence, we also define β〈t〉 as the angle between b〈t〉 and θ?, i.e.,\nβ〈t〉 , arccos ( 〈b〈t〉,θ?〉 ‖b〈t〉‖‖θ?‖ ) ∈ [0, π] .\n(This is well-defined as long as b〈t〉 6= 0 and θ? 6= 0.)\nWe first present results on Population EM with Model 2 under the initial condition 〈b〈0〉,θ?〉 6= 0.\nTheorem 3. Assume θ? ∈ Rd \\ {0}. Let (a〈t〉, b〈t〉)t≥0 denote the (re-parameterized) Population EM iterates for Model 2, and suppose 〈b〈0〉,θ?〉 6= 0. Then b〈t〉 6= 0 for all t ≥ 0. Furthermore, there exist κa ∈ (0, 1)—depending only on ‖θ?‖ and |〈b〈0〉,θ?〉/‖b〈0〉‖|—and κβ ∈ (0, 1)—depending only on ‖θ?‖, 〈b〈0〉,θ?〉/‖b〈0〉‖, ‖a〈0〉‖, and ‖b〈0〉‖—such that\n‖a〈t+1〉‖2 ≤ κ2a · ‖a〈t〉‖2 + ‖θ?‖2 sin2(β〈t〉)\n4 ,\nsin(β〈t+1〉) ≤ κtβ · sin(β〈0〉) .\nBy combining the two inequalities from Theorem 3, we conclude\n‖a〈t+1〉‖2 = κ2ta ‖a〈0〉‖2 + ‖θ?‖2\n4 t∑ τ=0 κ2τa · sin2(β〈t−τ〉)\n≤ κ2ta ‖a〈0〉‖2 + ‖θ?‖2\n4 t∑ τ=0 κ2τa κ 2(t−τ) β · sin 2(β〈0〉)\n≤ κ2ta ‖a〈0〉‖2 + ‖θ?‖2 4 t ( max { κa, κβ })t sin2(β〈0〉) .\nTheorem 3 shows that the re-parameterized Population EM iterates converge, at a linear rate, to the average of the two means (µ?1 + µ ? 2)/2, as well as the line spanned by θ\n?. The theorem, however, does not provide any information on the convergence of the magnitude of b〈t〉 to the magnitude of θ?. This is given in the next theorem.\nTheorem 4. Assume θ? ∈ Rd \\ {0}. Let (a〈t〉, b〈t〉)t≥0 denote the (re-parameterized) Population EM iterates for Model 2, and suppose 〈b〈0〉,θ?〉 6= 0. Then there exist T0 > 0, κb ∈ (0, 1), and cb > 0—all depending only on ‖θ?‖, |〈b〈0〉,θ?〉/‖b〈0〉‖|, ‖a〈0〉‖, and ‖b〈0〉‖—such that∥∥∥b〈t+1〉 − sgn(〈b〈0〉,θ?〉)θ?∥∥∥2 ≤ κ2b · ∥∥∥b〈t〉 − sgn(〈b〈0〉,θ?〉)θ?∥∥∥2 + cb · ‖a〈t〉‖ ∀t > T0 .\nIf 〈b〈0〉,θ?〉 = 0, then we show convergence of the (re-parameterized) Population EM iterates to the degenerate solution (0,0).\nTheorem 5. Let (a〈t〉, b〈t〉)t≥0 denote the (re-parameterized) Population EM iterates for Model 2. If 〈b〈0〉,θ?〉 = 0, then\n(a〈t〉, b〈t〉) → (0,0) as t→∞ .\nTheorems 3, 4, and 5 together characterize the fixed points of Population EM for Model 2, and fully specify the conditions under which each fixed point is reached. The results are simply summarized in the following corollary.\nCorollary 2. If (a〈t〉, b〈t〉)t≥0 denote the (re-parameterized) Population EM iterates for Model 2, then\na〈t〉 → µ ? 1 + µ ? 2\n2 as t→∞ ,\nb〈t〉 → sgn(〈b〈0〉,µ?2 − µ?1〉) µ?2 − µ?1\n2 as t→∞ ."
    }, {
      "heading" : "2.2 Main Results for Sample-based EM",
      "text" : "Using the results on Population EM presented in the above section, we can now establish consistency of (Sample-based) EM. We focus attention on Model 2, as the same results for Model 1 easily follow as a corollary. First, we state a simple connection between the Population EM and Sample-based EM iterates. Theorem 6. Suppose Population EM and Sample-based EM for Model 2 have the same initial parameters: µ̂〈0〉1 = µ 〈0〉 1 and µ̂ 〈0〉 2 = µ 〈0〉 2 . Then for each iteration t ≥ 0,\nµ̂ 〈t〉 1 → µ 〈t〉 1 and µ̂ 〈t〉 2 → µ 〈t〉 2 as n→∞ ,\nwhere convergence is in probability.\nNote that Theorem 6 does not necessarily imply that the fixed point of Sample-based EM (when initialized at (µ̂〈0〉1 , µ̂ 〈0〉 2 ) = (µ 〈0〉 1 ,µ 〈0〉 2 )) is the same as that of Population EM. It is conceivable that as t→∞, the discrepancy between (the iterates of) Sample-based EM and Population EM increases. We show that this is not the case: the fixed points of Sample-based EM indeed converge to the fixed points of Population EM. Theorem 7. Suppose Population EM and Sample-based EM for Model 2 have the same initial parameters: µ̂〈0〉1 = µ 〈0〉 1 and µ̂ 〈0〉 2 = µ 〈0〉 2 . If 〈µ 〈0〉 2 − µ 〈0〉 1 ,θ ?〉 6= 0, then\nlim sup t→∞\n|µ̂〈t〉1 − µ 〈t〉 1 | → 0 and lim sup t→∞ |µ̂〈t〉2 − µ 〈t〉 2 | → 0 as n→∞ ,\nwhere convergence is in probability."
    }, {
      "heading" : "2.3 Population EM and Expected Log-likelihood",
      "text" : "Do the results we derived in the last section regarding the performance of EM provide any information on the performance of other ascent algorithms, such as gradient ascent, that aim to maximize the loglikelihood function? To address this question, we show how our analysis can determine the stationary points of the expected log-likelihood and characterize the shape of the expected log-likelihood in a neighborhood of the stationary points. Let G(η) denote the expected log-likelihood, i.e.,\nG(η) , E(log fη(Y )) = ∫ f(y;η∗) log f(y;η) dy,\nwhere η∗ denotes the true parameter value. Also consider the following standard regularity conditions:\nR1 The family of probability density functions f(y;η) have common support. R2 ∇η ∫ f(y;η∗) log f(y;η) dy = ∫ f(y;η∗)∇η log f(y;η) dy, where ∇η denotes the gradient\nwith respect to η.\nR3 ∇η(E ∑ z f(z | Y ;η〈t〉)) log f(Y , z; η) = E ∑ z f(z | Y ;η〈t〉)∇η log f(Y , z;η).\nThese conditions can be easily confirmed for many models including the Gaussian mixture models. The following theorem connects the fixed points of the Population EM and the stationary points of the expected log-likelihood.\nLemma 1. Let η̄ ∈ Rd denote a stationary point ofG(η). Also assume thatQ(η | η〈t〉) has a unique and finite stationary point in terms of η for every η〈t〉, and this stationary point is its global maxima. Then, if the model satisfies conditions R1–R3, and the Population EM algorithm is initialized at η̄, it will stay at η̄. Conversely, any fixed point of Population EM is a stationary point of G(η).\nProof. Let η̄ denote a stationary point ofG(η). We first prove that η̄ is a stationary point ofQ(η | η̄).\n∇ηQ(η | η̄) ∣∣ η=η̄ = ∫ ∑ z f(z | y; η̄) ∇ηf(y, z;η) ∣∣ η=η̄ f(y, z; η̄) f(y;η∗) dy\n= ∫ ∑ z ∇ηf(y, z;η) ∣∣ η=η̄ f(y; η̄) f(y;η∗) dy\n= ∫ ∇ηf(y,η)∣∣η=η̄ f(y; η̄) f(y;η∗) dy = 0 ,\nwhere the last equality is using the fact that η̄ is a stationary point of G(η). Since Q(η | η̄) has a unique stationary point, and we have assumed that the unique stationary point is its global maxima, then Population EM will stay at that point. The proof of the other direction is similar.\nRemark 1. The fact that η∗ is the global maximizer of G(η) is well-known in the statistics and machine learning literature (e.g., Conniffe, 1987). Furthermore, the fact that η∗ is a global maximizer of Q(η | η∗) is known as the self-consistency property (Balakrishnan et al., 2014).\nIt is straightforward to confirm the conditions of Lemma 1 for mixtures of Gaussians. This lemma confirms that Population EM may be trapped in every local maxima. However, less intuitively it may get stuck at local minima or saddle points as well. Our next result characterizes the stationary points of G(θ) for Model 1. Corollary 3. G(θ) has only three stationary points. If d = 1 (so θ = θ ∈ R), then 0 is a local minima of G(θ), while θ∗ and −θ∗ are global maxima. If d > 1, then 0 is a saddle point, and θ? and −θ? are global maxima.\nThe proof is a straightforward result of Lemma 1 and Corollary 1. The phenomenon that Population EM may stuck in local minima or saddle points also happens in Model 2. We can employ Corollary 2 and Lemma 1 to explain the shape of the expected log-likelihood functionG. To simplify the notation, we consider the re-parametrization a , µ1+µ22 and b , µ2−µ1 2 . Corollary 4. G(a, b) has three stationary points:( µ?1 + µ ? 2\n2 , µ?2 − µ?1 2\n) ,\n( µ?1 + µ ? 2\n2 , µ?1 − µ?2 2\n) , and\n( µ?1 + µ ? 2\n2 , µ?1 + µ ? 2 2\n) .\nThe first two points are global maxima. The third point is a saddle point."
    }, {
      "heading" : "3 Concluding Remarks",
      "text" : "Our analysis of Population EM and Sample-based EM shows that the EM algorithm can, at least for the Gaussian mixture models studied in this work, compute statistically consistent parameter estimates. Previous analyses of EM only established such results for specific methods of initializing EM (e.g., Dasgupta and Schulman, 2007; Balakrishnan et al., 2014); our results show that they are not really necessary in the large sample limit. However, in any real scenario, the large sample limit may not accurately characterize the behavior of EM. Therefore, these specific methods for initialization, as well as non-asymptotic analysis, are clearly still needed to understand and effectively apply EM.\nThere are several interesting directions concerning EM that we hope to pursue in follow-up work. The first considers the behavior of EM when the dimension d = dn may grow with the sample size\nn. Our proof of Theorem 7 reveals that the parameter error of the t-th iterate (in Euclidean norm) is of the order √ d/n as t → ∞. Therefore, we conjecture that the theorem still holds as long as dn = o(n). This would be consistent with results from statistical physics on the MLE for Gaussian mixtures, which characterize the behavior when dn ∝ n as n→∞ (Barkai and Sompolinsky, 1994). Another natural direction is to extend these results to more general Gaussian mixture models (e.g., with unequal mixing weights or unequal covariances) and other latent variable models.\nAcknowledgements. The second named author thanks Yash Deshpande and Sham Kakade for many helpful initial discussions. JX and AM were partially supported by NSF grant CCF-1420328. DH was partially supported by NSF grant DMREF-1534910 and a Sloan Fellowship."
    } ],
    "references" : [ {
      "title" : "On spectral learning of mixtures of distributions",
      "author" : [ "D. Achlioptas", "F. McSherry" ],
      "venue" : "In Eighteenth Annual Conference on Learning Theory,",
      "citeRegEx" : "Achlioptas and McSherry.,? \\Q2005\\E",
      "shortCiteRegEx" : "Achlioptas and McSherry.",
      "year" : 2005
    }, {
      "title" : "Learning mixtures of separated nonspherical Gaussians",
      "author" : [ "S. Arora", "R. Kannan" ],
      "venue" : "The Annals of Applied Probability,",
      "citeRegEx" : "Arora and Kannan.,? \\Q2005\\E",
      "shortCiteRegEx" : "Arora and Kannan.",
      "year" : 2005
    }, {
      "title" : "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
      "author" : [ "S. Balakrishnan", "M.J. Wainwright", "B. Yu" ],
      "venue" : "arXiv preprint arXiv:1408.2156,",
      "citeRegEx" : "Balakrishnan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Balakrishnan et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistical mechanics of the maximum-likelihood density estimation",
      "author" : [ "N. Barkai", "H. Sompolinsky" ],
      "venue" : "Physical Review E,",
      "citeRegEx" : "Barkai and Sompolinsky.,? \\Q1994\\E",
      "shortCiteRegEx" : "Barkai and Sompolinsky.",
      "year" : 1994
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "M. Belkin", "K. Sinha" ],
      "venue" : "In Fifty-First Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Belkin and Sinha.,? \\Q2010\\E",
      "shortCiteRegEx" : "Belkin and Sinha.",
      "year" : 2010
    }, {
      "title" : "Isotropic PCA and affine-invariant clustering",
      "author" : [ "S.C. Brubaker", "S. Vempala" ],
      "venue" : "In Forty-Ninth Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Brubaker and Vempala.,? \\Q2008\\E",
      "shortCiteRegEx" : "Brubaker and Vempala.",
      "year" : 2008
    }, {
      "title" : "Learning mixtures of product distributions using correlations and independence",
      "author" : [ "K. Chaudhuri", "S. Rao" ],
      "venue" : "In Twenty-First Annual Conference on Learning Theory,",
      "citeRegEx" : "Chaudhuri and Rao.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chaudhuri and Rao.",
      "year" : 2008
    }, {
      "title" : "Multi-view clustering via canonical correlation analysis",
      "author" : [ "K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning mixtures of Gaussians using the k-means algorithm",
      "author" : [ "K. Chaudhuri", "S. Dasgupta", "A. Vattani" ],
      "venue" : "CoRR, abs/0912.0086,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2009
    }, {
      "title" : "On EM algorithms and their proximal generalizations",
      "author" : [ "S. Chrétien", "A.O. Hero" ],
      "venue" : "ESAIM: Probability and Statistics,",
      "citeRegEx" : "Chrétien and Hero.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chrétien and Hero.",
      "year" : 2008
    }, {
      "title" : "Expected maximum log likelihood estimation",
      "author" : [ "D. Conniffe" ],
      "venue" : "Journal of the Royal Statistical Society. Series D,",
      "citeRegEx" : "Conniffe.,? \\Q1987\\E",
      "shortCiteRegEx" : "Conniffe.",
      "year" : 1987
    }, {
      "title" : "Learning mixutres of Gaussians",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In Fortieth Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Dasgupta.,? \\Q1999\\E",
      "shortCiteRegEx" : "Dasgupta.",
      "year" : 1999
    }, {
      "title" : "A probabilistic analysis of EM for mixtures of separated, spherical Gaussians",
      "author" : [ "S. Dasgupta", "L. Schulman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dasgupta and Schulman.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dasgupta and Schulman.",
      "year" : 2007
    }, {
      "title" : "Ten steps of EM suffice for mixtures of two Gaussians",
      "author" : [ "C. Daskalakis", "C. Tzamos", "M. Zampetakis" ],
      "venue" : "arXiv preprint arXiv:1609.00368,",
      "citeRegEx" : "Daskalakis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Daskalakis et al\\.",
      "year" : 2016
    }, {
      "title" : "Maximum-likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "J. Royal Statist. Soc. Ser. B,",
      "citeRegEx" : "Dempster et al\\.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dempster et al\\.",
      "year" : 1977
    }, {
      "title" : "On the mathematical foundations of theoretical statistics",
      "author" : [ "R.A. Fisher" ],
      "venue" : "Philosophical Transactions of the Royal Society,",
      "citeRegEx" : "Fisher.,? \\Q1922\\E",
      "shortCiteRegEx" : "Fisher.",
      "year" : 1922
    }, {
      "title" : "Tight bounds for learning a mixture of two Gaussians",
      "author" : [ "M. Hardt", "E. Price" ],
      "venue" : "In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,",
      "citeRegEx" : "Hardt and Price.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hardt and Price.",
      "year" : 2015
    }, {
      "title" : "Learning mixtures of spherical Gaussians: moment methods and spectral decompositions",
      "author" : [ "D. Hsu", "S.M. Kakade" ],
      "venue" : "In Fourth Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Hsu and Kakade.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hsu and Kakade.",
      "year" : 2013
    }, {
      "title" : "Local maxima in the likelihood of Gaussian mixture models: Structural results and algorithmic consequences",
      "author" : [ "C. Jin", "Y. Zhang", "S. Balakrishnan", "M.J. Wainwright", "M. Jordan" ],
      "venue" : "arXiv preprint arXiv:1609.00978,",
      "citeRegEx" : "Jin et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jin et al\\.",
      "year" : 2016
    }, {
      "title" : "Efficiently learning mixtures of two Gaussians",
      "author" : [ "A.T. Kalai", "A. Moitra", "G. Valiant" ],
      "venue" : "In Forty-second ACM Symposium on Theory of Computing,",
      "citeRegEx" : "Kalai et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 2010
    }, {
      "title" : "The spectral method for general mixture models",
      "author" : [ "R. Kannan", "H. Salmasian", "S. Vempala" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Kannan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kannan et al\\.",
      "year" : 2008
    }, {
      "title" : "Statistical guarantees for estimating the centers of a twocomponent Gaussian mixture by EM",
      "author" : [ "J.M. Klusowski", "W.D. Brinda" ],
      "venue" : "arXiv preprint arXiv:1608.02280,",
      "citeRegEx" : "Klusowski and Brinda.,? \\Q2016\\E",
      "shortCiteRegEx" : "Klusowski and Brinda.",
      "year" : 2016
    }, {
      "title" : "Least squares quantization in PCM",
      "author" : [ "S.P. Lloyd" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "Lloyd.,? \\Q1982\\E",
      "shortCiteRegEx" : "Lloyd.",
      "year" : 1982
    }, {
      "title" : "Some methods for classification and analysis of multivariate observations",
      "author" : [ "J.B. MacQueen" ],
      "venue" : "In Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability,",
      "citeRegEx" : "MacQueen.,? \\Q1967\\E",
      "shortCiteRegEx" : "MacQueen.",
      "year" : 1967
    }, {
      "title" : "Settling the polynomial learnability of mixtures of Gaussians",
      "author" : [ "A. Moitra", "G. Valiant" ],
      "venue" : "In Fifty-First Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Moitra and Valiant.,? \\Q2010\\E",
      "shortCiteRegEx" : "Moitra and Valiant.",
      "year" : 2010
    }, {
      "title" : "Mixture densities, maximum likelihood and the EM algorithm",
      "author" : [ "R.A. Redner", "H.F. Walker" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Redner and Walker.,? \\Q1984\\E",
      "shortCiteRegEx" : "Redner and Walker.",
      "year" : 1984
    }, {
      "title" : "Are there local maxima in the infinite-sample likelihood of Gaussian mixture estimation",
      "author" : [ "N. Srebro" ],
      "venue" : "In 20th Annual Conference on Learning Theory,",
      "citeRegEx" : "Srebro.,? \\Q2007\\E",
      "shortCiteRegEx" : "Srebro.",
      "year" : 2007
    }, {
      "title" : "An analysis of the EM algorithm and entropy-like proximal point methods",
      "author" : [ "P. Tseng" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Tseng.,? \\Q2004\\E",
      "shortCiteRegEx" : "Tseng.",
      "year" : 2004
    }, {
      "title" : "Parameter convergence for EM and MM",
      "author" : [ "F. Vaida" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Vaida.,? \\Q2005\\E",
      "shortCiteRegEx" : "Vaida.",
      "year" : 2005
    }, {
      "title" : "A spectral algorithm for learning mixtures models",
      "author" : [ "S. Vempala", "G. Wang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Vempala and Wang.,? \\Q2004\\E",
      "shortCiteRegEx" : "Vempala and Wang.",
      "year" : 2004
    }, {
      "title" : "On the convergence properties of the EM algorithm",
      "author" : [ "C.F.J. Wu" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Wu.,? \\Q1983\\E",
      "shortCiteRegEx" : "Wu.",
      "year" : 1983
    }, {
      "title" : "Global analysis of Expectation Maximization for mixtures of two Gaussians",
      "author" : [ "J. Xu", "D. Hsu", "A. Maleki" ],
      "venue" : "arXiv preprint arXiv:1608.07630,",
      "citeRegEx" : "Xu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "On convergence properties of the EM algorithm for Gaussian mixtures",
      "author" : [ "L. Xu", "M.I. Jordan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Xu and Jordan.,? \\Q1996\\E",
      "shortCiteRegEx" : "Xu and Jordan.",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Since Fisher’s 1922 paper (Fisher, 1922), maximum likelihood estimators (MLE) have become one of the most popular tools in many areas of science and engineering.",
      "startOffset" : 26,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984).",
      "startOffset" : 172,
      "endOffset" : 220
    }, {
      "referenceID" : 25,
      "context" : "Among the algorithms mentioned above, Expectation Maximization (EM) has attracted more attention for the simplicity of its iterations, and its good performance in practice (Dempster et al., 1977; Redner and Walker, 1984).",
      "startOffset" : 172,
      "endOffset" : 220
    }, {
      "referenceID" : 30,
      "context" : "Although EM is billed as a procedure for maximum likelihood estimation, it is known that with certain initializations, the final parameters returned by EM may be far from the MLE, both in parameter distance and in log-likelihood value (Wu, 1983).",
      "startOffset" : 235,
      "endOffset" : 245
    }, {
      "referenceID" : 30,
      "context" : "Several works characterize convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Vaida, 2005; Chrétien and Hero, 2008).",
      "startOffset" : 134,
      "endOffset" : 195
    }, {
      "referenceID" : 27,
      "context" : "Several works characterize convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Vaida, 2005; Chrétien and Hero, 2008).",
      "startOffset" : 134,
      "endOffset" : 195
    }, {
      "referenceID" : 28,
      "context" : "Several works characterize convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Vaida, 2005; Chrétien and Hero, 2008).",
      "startOffset" : 134,
      "endOffset" : 195
    }, {
      "referenceID" : 9,
      "context" : "Several works characterize convergence of EM to stationary points of the log-likelihood objective under certain regularity conditions (Wu, 1983; Tseng, 2004; Vaida, 2005; Chrétien and Hero, 2008).",
      "startOffset" : 134,
      "endOffset" : 195
    }, {
      "referenceID" : 11,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either d or k for some α, β > 0 for a mixture of k Gaussians in R (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 221,
      "endOffset" : 444
    }, {
      "referenceID" : 1,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either d or k for some α, β > 0 for a mixture of k Gaussians in R (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 221,
      "endOffset" : 444
    }, {
      "referenceID" : 12,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either d or k for some α, β > 0 for a mixture of k Gaussians in R (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 221,
      "endOffset" : 444
    }, {
      "referenceID" : 29,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either d or k for some α, β > 0 for a mixture of k Gaussians in R (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 221,
      "endOffset" : 444
    }, {
      "referenceID" : 20,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either d or k for some α, β > 0 for a mixture of k Gaussians in R (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 221,
      "endOffset" : 444
    }, {
      "referenceID" : 0,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either d or k for some α, β > 0 for a mixture of k Gaussians in R (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 221,
      "endOffset" : 444
    }, {
      "referenceID" : 6,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either d or k for some α, β > 0 for a mixture of k Gaussians in R (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 221,
      "endOffset" : 444
    }, {
      "referenceID" : 5,
      "context" : "Several of these algorithms, starting with that of Dasgupta (1999), assume that the means of the mixture components are well-separated—roughly at distance either d or k for some α, β > 0 for a mixture of k Gaussians in R (Dasgupta, 1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004; Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al., 2009a).",
      "startOffset" : 221,
      "endOffset" : 444
    }, {
      "referenceID" : 19,
      "context" : "mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 96,
      "endOffset" : 211
    }, {
      "referenceID" : 4,
      "context" : "mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 96,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 96,
      "endOffset" : 211
    }, {
      "referenceID" : 17,
      "context" : "mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 96,
      "endOffset" : 211
    }, {
      "referenceID" : 16,
      "context" : "mixture components to be arbitrarily close, provided that the sample size is sufficiently large (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade, 2013; Hardt and Price, 2015).",
      "startOffset" : 96,
      "endOffset" : 211
    }, {
      "referenceID" : 23,
      "context" : "(2009b) uses these symmetries to prove that a variant of Lloyd’s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.",
      "startOffset" : 75,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "(2009b) uses these symmetries to prove that a variant of Lloyd’s algorithm (MacQueen, 1967; Lloyd, 1982) (which may be regarded as a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture component means, without any separation assumption.",
      "startOffset" : 75,
      "endOffset" : 104
    }, {
      "referenceID" : 31,
      "context" : "Finally, two related works have appeared following the initial posting of this article (Xu et al., 2016).",
      "startOffset" : 87,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : "Similar results were well-known for Lloyd’s algorithm, but were not previously established for Population EM (Srebro, 2007).",
      "startOffset" : 109,
      "endOffset" : 123
    }, {
      "referenceID" : 31,
      "context" : "The proof of Theorem 1 and all other omitted proofs are given in the full version of this article (Xu et al., 2016).",
      "startOffset" : 98,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, the fact that η∗ is a global maximizer of Q(η | η∗) is known as the self-consistency property (Balakrishnan et al., 2014).",
      "startOffset" : 107,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "Previous analyses of EM only established such results for specific methods of initializing EM (e.g., Dasgupta and Schulman, 2007; Balakrishnan et al., 2014); our results show that they are not really necessary in the large sample limit.",
      "startOffset" : 94,
      "endOffset" : 156
    }, {
      "referenceID" : 3,
      "context" : "This would be consistent with results from statistical physics on the MLE for Gaussian mixtures, which characterize the behavior when dn ∝ n as n→∞ (Barkai and Sompolinsky, 1994).",
      "startOffset" : 148,
      "endOffset" : 178
    } ],
    "year" : 2016,
    "abstractText" : "Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.",
    "creator" : null
  }
}