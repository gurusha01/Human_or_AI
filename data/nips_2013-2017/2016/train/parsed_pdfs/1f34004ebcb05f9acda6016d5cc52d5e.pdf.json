{
  "name" : "1f34004ebcb05f9acda6016d5cc52d5e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods",
    "authors" : [ "Lev Bogolubsky", "Gleb Gusev", "Andrei Raigorodskii", "Aleksey Tikhonov", "Maksim Zhukovskii", "Pavel Dvurechensky", "Alexander Gasnikov" ],
    "emails" : [ "zhukmax}@yandex-team.ru", "pavel.dvurechensky@wias-berlin.de,", "gasnikov@yandex.ru", "yurii.nesterov@uclouvain.be" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The most acknowledged methods of measuring importance of nodes in graphs are based on random walk models. Particularly, PageRank [18], HITS [11], and their variants [8, 9, 19] are originally based on a discrete-time Markov random walk on a link graph. Despite undeniable advantages of PageRank and its mentioned modifications, these algorithms miss important aspects of the graph that are not described by its structure. In contrast, a number of approaches allows to account for different properties of nodes and edges between them by encoding them in restart and transition probabilities (see [3, 4, 6, 10, 12, 20, 21]). These properties may include, e.g., the statistics about users’ interactions with the nodes (in web graphs [12] or graphs of social networks [2]), types of edges (such as URL redirecting in web graphs [20]) or histories of nodes’ and edges’ changes [22].\nIn the general ranking framework called Supervised PageRank [21], weights of nodes and edges in a graph are linear combinations of their features with coefficients as the model parameters. The existing optimization method [21] of learning these parameters and the optimizations methods proposed in the presented paper have two levels. On the lower level, the following problem is solved: to estimate the value of the loss function (in the case of zero-order oracle) and its derivatives (in the case of first-order oracle) for a given parameter vector. On the upper level, the estimations obtained on the lower level of the optimization methods (which we also call inexact oracle information) are used for tuning the parameters by an iterative algorithm. Following [6], the authors of Supervised PageRank consider a non-convex loss-minimization problem for learning the parameters and solve\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nit by a two-level gradient-based method. On the lower level of this algorithm, an estimation of the stationary distribution of the considered Markov random walk is obtained by classical power method and estimations of derivatives w.r.t. the parameters of the random walk are obtained by power method introduced in [23, 24]. On the upper level, the obtained gradient of the stationary distribution is exploited by the gradient descent algorithm. As both power methods give imprecise values of the stationary distribution and its derivatives, there was no proof of the convergence of the state-of-the-art gradient-based method to a stationary point.\nThe considered non-convex loss-minimization problem [21] can not be solved by existing optimization methods such as [16] and [7] due to presence of constraints for parameter vector and the impossibility to calculate the exact value of the loss function. Moreover, standard global optimization methods can not be applied, because they require unbiased estimations of the loss function.\nIn our paper, we propose two two-level methods to solve the problem [21]. On the lower level of these methods, we use the linearly convergent method [17] to calculate an approximation to the stationary distribution of Markov random walk. We show that this method allows to approximate the value of the loss function at any given accuracy and has the lowest proved complexity bound among methods proposed in [5]. We develop a gradient method for general constrained non-convex optimization problems with inexact oracle, estimate its convergence rate to the stationary point of the problem. We exploit this gradient method on the upper level of the two-level algorithm for learning Supervised PageRank. Our contribution to the gradient-free methods framework consists in adapting the approach of [16] to the case of constrained optimization problems when the value of the function is calculated with some known accuracy. We prove a convergence theorem for this method and exploit it on the upper level of the second two-level algorithm.\nAnother contribution consists in investigating both for the gradient and gradient-free methods the trade-off between the accuracy of the lower-level algorithm, which is controlled by the number of iterations of method in [17] and its generalization (for derivatives estimation), and the computational complexity of the two-level algorithm as a whole. Finally, we estimate the complexity of the whole two-level algorithms for solving the loss-minimization problem with a given accuracy.\nIn the experiments, we apply our algorithms to learning Supervised PageRank on a real ranking task. Summing up, both two-level methods, unlike the state-of-the-art [21], have theoretical guarantees on convergence rate, and outperform it in the ranking quality in experiments. The main advantages of the first gradient-based algorithm: the guarantees of a convergence do not require the convexity, this algorithm has less input parameters than gradient-free one. The main advantage of the second gradient-free algorithm is that it avoids calculating the derivative for each element of a large matrix."
    }, {
      "heading" : "2 MODEL DESCRIPTION",
      "text" : "We concider the following random walk on a directed graph Γ = (V,E) introduced in [21]. Assume that each node i ∈ V and each edge i→ j ∈ E is represented by a vector of features Vi ∈ Rm1+ and a vector of features Eij ∈ Rm2+ respectively. A surfer starts from a random page v0 of a seed set U ⊂ V . The restart probability that v0 = i equals\n[π0]i = 〈ϕ1,Vi〉∑ l∈U 〈ϕ1,Vl〉 , i ∈ U (2.1)\nand [π0]i = 0 for i ∈ V \\ U , where ϕ1 ∈ Rm1 is a parameter, which conducts the random walk. We assume that ∑ l∈U 〈ϕ1,Vl〉 should be non-zero.\nAt each step, the surfer makes a restart with probability α ∈ (0, 1) (originally [18], α = 0.15) or traverses an outgoing edge (makes a transition) with probability 1− α. In the former case, the surfer chooses a vertex according to the distribution π0. In the latter one, the transition probability of traversing an edge i→ j ∈ E is\n[P ]i,j = 〈ϕ2,Eij〉∑ l:i→l〈ϕ2,Eil〉 , (2.2)\nwhere ϕ2 ∈ Rm2 is a parameter and the current position i has non-zero outdegree, and [P (ϕ)]i,j = [π0(ϕ)]j for all j ∈ V if the outdegree of i is zero (thus the surfer always makes a restart in this case). We assume that ∑ l:i→l〈ϕ2,Eil〉 is non-zero for all i with non-zero outdegree.\nBy Equations 2.1 and 2.2, the total probability of choosing vertex j ∈ V conditioned by the surfer being at vertex i equals α[π0(ϕ)]j + (1− α)[P (ϕ)]i,j , where ϕ = (ϕ1, ϕ2)T and we use π0(ϕ) and P (ϕ) to express the dependence of π0, P on the parameters.\nThe stationary distribution π(ϕ) ∈ Rp of the described Markov process is a solution of the system\nπ = απ0(ϕ) + (1− α)PT (ϕ)π. (2.3)\nIn this paper, we learn an algorithm, which ranks nodes i according to scores [π(ϕ)]i.\nLet Q be a set of queries and a set of nodes Vq ⊂ V is associated to each query q. For example, vertices in Vq may represent web pages visited by users after submitting query q. For each q ∈ Q, some nodes of Vq are manually judged by relevance labels 1, . . . , `. Our goal is to learn the parameter vector ϕ of a ranking algorithm πq = πq(ϕ) which minimizes the discrepancy of its ranking scores [πq]i, i ∈ Vq , from the the assigned labels. We consider the square loss function [12, 21, 22]\nf(ϕ) = 1\n|Q| |Q|∑ q=1 ‖(Aqπq(ϕ))+‖22. (2.4)\nEach row of matrix Aq ∈ Rrq×pq corresponds to some pair of pages i1, i2 ∈ Vq such that the label of i1 is strictly greater than the label of i2 (we denote by rq the number of all such pairs from Vq and pq := |Vq|). The i1-th element of this row is equal to −1, i2-th element is equal to 1, and all other elements are equal to 0. Vector x+ has components [x+]i = max{xi, 0}. To make ranking scores (2.3) query–dependent, we assume that π is defined on a query–dependent graph Γq = (Vq, Eq) with query-dependent feature vectors V q i , i ∈ Vq, E q ij , i → j ∈ Eq. For example, these features may reflect different aspects of query–page relevance. For a given q ∈ Q, we consider all the objects related to the graph Γq introduced above: Uq := U , π0q := π\n0, Pq := P , πq := π. In this way, the ranking scores πq depend on query via the query–dependent features, but the parameters of the model α and ϕ are not query–dependent. In what follows, we use the following notations throughout the paper: nq := |Uq|, m = m1 + m2, r = maxq∈Q rq, p = maxq∈Q pq, n = maxq∈Q nq, s = maxq∈Q sq, where sq = maxi∈Vq |{j : i → j ∈ Eq}|. In order to guarantee that the probabilities in (2.1) and (2.2) are correctly defined, we need to appropriately choose a set Φ of possible values of parameters ϕ. We choose some ϕ̂ and R > 0 such that Φ = {ϕ ∈ Rm : ‖ϕ− ϕ̂‖2 ≤ R} lies in the set of vectors with positive components Rm++1. In this paper, we solve the following loss-minimization problem:\nmin ϕ∈Φ\nf(ϕ),Φ = {ϕ ∈ Rm : ‖ϕ− ϕ̂‖2 ≤ R}. (2.5)"
    }, {
      "heading" : "3 NUMERICAL CALCULATION OF f(ϕ) AND ∇f(ϕ)",
      "text" : "Our goal is to provide methods for solving Problem 2.5 with guarantees on rate of convergence and complexity bounds. The calculation of the values of f(ϕ) and its gradient ∇f(ϕ) is problematic, since it requires to calculate those for |Q| vectors πq(ϕ) defined by Equation 2.3. While the exact values are impossible to derive in general, existing methods provide estimations of πq(ϕ) and its derivatives dπq(ϕ)\ndϕT in an iterative way with a trade-off between time and accuracy. To be able to\nguarantee convergence of our optimization algorithm in this inexact oracle setting, we consider numerical methods that calculate approximation for πq(ϕ) and its derivatives with any required accuracy. We have analysed state-of-the-art methods summarized in the review [5] and power method used in [18, 2, 21] and have found that the method [17] is the most suitable.\nIt constructs a sequence πk and outputs π̃q(ϕ,N) by the following rule (integerN > 0 is a parameter):\nπ0 = π 0 q (ϕ), πk+1 = P T q (ϕ)πk, π̃q(ϕ,N) =\nα 1− (1− α)N+1 N∑ k=0 (1− α)kπk. (3.1)\n1As probablities [π0q(ϕ)]i, i ∈ Vq , [Pq(ϕ)]̃i,i, ĩ→ i ∈ Eq , are scale-invariant (π 0 q(λϕ) = π 0 q(ϕ), Pq(λϕ) = Pq(ϕ)), in our experiments, we consider the set Φ = {ϕ ∈ Rm : ‖ϕ− em‖2 ≤ 0.99} , where em ∈ Rm is the vector of all ones, that has large intersection with the simplex {ϕ ∈ Rm++ : ‖ϕ‖1 = 1}\nLemma 1. Assume that, for some δ1 > 0, Method 3.1 with N = ⌈ 1 α ln 8r δ1 ⌉ − 1 is used to calculate\nthe vector π̃q(ϕ,N) for every q ∈ Q. Then f̃(ϕ, δ1) = 1|Q| ∑|Q| q=1 ‖(Aqπ̃q(ϕ,N))+‖22 satisfies |f̃(ϕ, δ1)− f(ϕ)| ≤ δ1. Moreover, the calculation of f̃(ϕ, δ1) requires not more than |Q|(3mps+ 3psN + 6r) a.o.\nThe proof of Lemma 1 is in Supplementary Materials.\nLet pi(ϕ) be the i-th column of the matrix PTq (ϕ). Our generalization of the method [17] for calculation of dπq(ϕ) dϕT\nfor any q ∈ Q is the following. Choose some non-negative integer N1 and calculate π̃q(ϕ,N1) using (3.1). Choose some N2 ≥ 0, calculate Πk, k = 0, ..., N2 and Π̃q(ϕ,N2)\nΠ0 = α dπ0q (ϕ)\ndϕT + (1− α) pq∑ i=1 dpi(ϕ) dϕT [π̃q(ϕ,N1)]i, Πk+1 = P T q (ϕ)Πk, (3.2)\nΠ̃q(ϕ,N2) = 1 1− (1− α)N2+1 N2∑ k=0 (1− α)kΠk. (3.3)\nIn what follows, we use the following norm on the space of matrices A ∈ Rn1×n2 : ‖A‖1 = maxj=1,...,n2 ∑n1 i=1 |aij |. Lemma 2. Let β1 be some explicitly computable constant (see Supplementary Materials). Assume that Method 3.1 with N1 = ⌈ 1 α ln 24β1r αδ2 ⌉ − 1 is used for every q ∈ Q to calculate the vector\nπ̃q(ϕ,N1) and Method 3.2, 3.3 with N2 = ⌈ 1 α ln 8β1r αδ2 ⌉ − 1 is used for every q ∈ Q to calculate the\nmatrix Π̃q(ϕ,N2) (3.3). Then the vector g̃(ϕ, δ2) = 2|Q| ∑|Q| q=1 ( Π̃q(ϕ,N2) )T ATq (Aqπ̃q(ϕ,N1))+ satisfies ‖g̃(ϕ, δ2)−∇f(ϕ)‖∞ ≤ δ2. Moreover, the calculation of g̃(ϕ, δ2) requires not more than |Q|(10mps+ 3psN1 + 3mpsN2 + 7r) a.o.\nThe proof of Lemma 2 can be found in Supplementary Materials."
    }, {
      "heading" : "4 RANDOM GRADIENT-FREE OPTIMIZATION METHODS",
      "text" : "In this section, we first describe general framework of random gradient-free methods with inexact oracle and then apply it for Problem 2.5. Lemma 1 allows to control the accuracy of the inexact zero-order oracle and hence apply random gradient-free methods with inexact oracle."
    }, {
      "heading" : "4.1 GENERAL FRAMEWORK",
      "text" : "Below we extend the framework of random gradient-free methods [1, 16, 7] for the situation of presence of uniformly bounded error of unknown nature in the value of an objective function in general optimization problem. Unlike [16], we consider a constrained optimization problem and a randomization on a Euclidean sphere which seems to give better large deviations bounds and doesn’t need the assumption that the objective function can be calculated at any point of Rm. Let E be a m-dimensional vector space and E∗ be its dual. In this subsection, we consider a general function f(·) : E → R and denote its argument by x or y to avoid confusion with other sections. We denote the value of linear function g ∈ E∗ at x ∈ E by 〈g, x〉. We choose some norm ‖ · ‖ in E and say that f ∈ C1,1L (‖ ·‖) iff |f(x)−f(y)−〈∇f(y), x−y〉| ≤ L 2 ‖x−y‖\n2, ∀x, y ∈ E . The problem of our interest is to find minx∈X f(x), where f ∈ C1,1L (‖ · ‖), X is a closed convex set and there exists a number D ∈ (0,+∞) such that diamX := maxx,y∈X ‖x− y‖ ≤ D. Also we assume that the inexact zero-order oracle for f(x) returns a value f̃(x, δ) = f(x) + δ̃(x), where δ̃(x) is the error satisfying for some δ > 0 (which is known) |δ̃(x)| ≤ δ for all x ∈ X . Let x∗ ∈ arg minx∈X f(x). Denote f∗ = minx∈X f(x).\nUnlike [16], we define the biased gradient-free oracle gτ (x, δ) = mτ (f̃(x+ τξ, δ)− f̃(x, δ))ξ, where ξ is a random vector uniformly distributed over the unit sphere S = {t ∈ Rm : ‖t‖2 = 1}, τ is a smoothing parameter.\nAlgorithm 1 Gradient-type method Input: Point x0 ∈ X , stepsize h > 0, number of steps M . Set k = 0. repeat\nGenerate ξk and calculate corresponding gτ (xk, δ). Calculate xk+1 = ΠX(xk − hgτ (xk, δ)) (ΠX(·) – Euclidean projection onto the set X). Set k = k + 1.\nuntil k > M Output: The point yM = arg minx{f(x) : x ∈ {x0, . . . , xM}}.\nTheorem 1. Let f ∈ C1,1L (‖ · ‖2) and convex. Assume that x∗ ∈ intX , and the sequence xk is generated by Algorithm 1 with h = 18mL . Then for any M ≥ 0, we have EΞM−1f(yM ) − f\n∗ ≤ 8mLD2\nM+1 + τ2L(m+8) 8 + δmD 4τ + δ2m Lτ2 . Here Ξk = (ξ0, . . . , ξk) is the history of realizations of the\nvector ξ.\nThe full proof of the theorem is in Supplementary Materials."
    }, {
      "heading" : "4.2 SOLVING THE LEARNING PROBLEM",
      "text" : "Now, we apply the results of Subsection 4.1 to solve Problem 2.5. Note that presence of constraints and oracle inexactness do not allow to directly apply the results of [16]. We assume that there is a local minimum ϕ∗, and Φ is a small vicinity of ϕ∗, in which f(ϕ) (2.4) is convex (generally speaking, it is nonconvex). We choose the desired accuracy ε for f∗ (the optimal value) approximation in the sense that EΞM−1f(yM ) − f∗ ≤ ε. In accordance with Theorem 1, ε gives the number of steps M of Algorithm 1, the value of τ , the value of the required accuracy δ of the inexact zero-order oracle. The value δ, by Lemma 1, gives the number of steps N of Method 3.1 required to calculate a δ-approximation f̃(ϕ, δ) for f(ϕ). Then the inexact zero-order oracle f̃(ϕ, δ) is used to make Algorithm 1 step. Theorem 1 and the choice of the feasible set Φ to be a Euclidean ball make it natural to choose ‖ · ‖2-norm in the space Rm of parameter ϕ. It is easy to see that in this norm diamΦ ≤ 2R. Algorithm 2 in Supplementary Materials is a formal record of these ideas. The most computationally hard on each iteration of the main cycle of this method are calculations of f̃(ϕk + τξk, δ), f̃(ϕk, δ). Using Lemma 1, we obtain the complexity of each iteration and the following result, which gives the complexity of Algorithm 2. Theorem 2. Assume that the set Φ in (2.5) is chosen in a way such that f(ϕ) is convex on Φ and some ϕ∗ ∈ arg minϕ∈Φ f(ϕ) belongs also to intΦ. Then the mean total number of arithmetic operations of the Algorithm 2 for the accuracy ε (i.e. for the inequality EΞM−1f(ϕ̂M )− f(ϕ∗) ≤ ε to hold) is not more than\n768mps|Q|LR 2\nε\n( m+ 1\nα ln\n128mrR √ L(m+ 8)\nε3/2 √ 2 + 6r\n) ."
    }, {
      "heading" : "5 GRADIENT-BASED OPTIMIZATION METHODS",
      "text" : "In this section, we first develop a general framework of gradient methods with inexact oracle for non-convex problems from rather general class and then apply it for the particular Problem 2.5. Lemma 1 and Lemma 2 allow to control the accuracy of the inexact first-order oracle and hence apply proposed framework."
    }, {
      "heading" : "5.1 GENERAL FRAMEWORK",
      "text" : "In this subsection, we generalize the approach in [7] for constrained non-convex optimization problems. Our main contribution consists in developing this framework for an inexact first-order oracle and unknown \"Lipschitz constant\" of this oracle.\nWe consider a composite optimization problem of the form minx∈X{ψ(x) := f(x) + h(x)}, where X ⊂ E is a closed convex set, h(x) is a simple convex function, e.g. ‖x‖1. We assume that f(x) is\na general function endowed with an inexact first-order oracle in the following sense. There exists a number L ∈ (0,+∞) such that for any δ ≥ 0 and any x ∈ X one can calculate f̃(x, δ) ∈ R and g̃(x, δ) ∈ E∗ satisfying\n|f(y)− (f̃(x, δ)− 〈g̃(x, δ), y − x〉)| ≤ L 2 ‖x− y‖2 + δ. (5.1)\nfor all y ∈ X . The constant L can be considered as \"Lipschitz constant\" because for the exact firstorder oracle for a function f ∈ C1,1L (‖ · ‖) Inequality 5.1 holds with δ = 0. This is a generalization of the concept of (δ, L)-oracle considered in [25] for convex problems.\nWe choose a prox-function d(x) which is continuously differentiable and 1-strongly convex on X with respect to ‖ · ‖. This means that for any x, y ∈ X d(y)− d(x)− 〈∇d(x), y − x〉 ≥ 12‖y − x‖\n2. We define also the corresponding Bregman distance V (x, z) = d(x)− d(z)− 〈∇d(z), x− z〉.\nAlgorithm 2 Adaptive projected gradient algorithm Input: Point x0 ∈ X , number L0 > 0. Set k = 0, z = +∞. repeat\nSet Mk = Lk, flag = 0. repeat\nSet δ = ε16Mk . Calculate f̃(xk, δ) and g̃(xk, δ). Find wk = arg minx∈Q {〈g̃(xk, δ), x〉+MkV (x, xk) + h(x)} and calculate f̃(wk, δ). If the inequality f̃(wk, δ) ≤ f̃(xk, δ) + 〈g̃(xk, δ), wk − xk〉+ Mk2 ‖wk − xk‖\n2 + ε8Mk holds, set flag = 1. Otherwise set Mk = 2Mk.\nuntil flag = 1 Set xk+1 = wk, Lk+1 = Mk2 . If ‖Mk(xk − xk+1)‖ < z, set z = ‖Mk(xk − xk+1)‖, K = k. Set k = k + 1.\nuntil z ≤ ε Output: The point xK+1.\nTheorem 3. Assume that f(x) is endowed with the inexact first-order oracle in a sense (5.1) and that there exists a number ψ∗ > −∞ such that ψ(x) ≥ ψ∗ for all x ∈ X . Then after M iterations of Algorithm 2 it holds that ‖MK(xK − xK+1)‖2 ≤ 4L(ψ(x0)−ψ ∗) M+1 + ε 2 . Moreover, the total number of inexact oracle calls is not more than 2M + 2 log2 2L L0 .\nThe full proof of the theorem is in Supplementary Materials."
    }, {
      "heading" : "5.2 SOLVING THE LEARNING PROBLEM",
      "text" : "In this subsection, we return to Problem 2.5 and apply the results of the previous subsection. Note that we can not directly apply the results of [7] due to the inexactness of the oracle. For this problem, h(·) ≡ 0. It is easy to show that in 1-norm diamΦ ≤ 2R √ m. For any δ > 0, Lemma 1 with δ1 = δ2 allows us to obtain f̃(ϕ, δ1) such that inequality |f̃(ϕ, δ1)− f(ϕ)| ≤ δ1 holds and Lemma 2 with δ2 =\nδ 4R √ m allows us to obtain g̃(ϕ, δ2) such that inequality ‖g̃(ϕ, δ2) − ∇f(ϕ)‖∞ ≤ δ2 holds. Similar to [25], since f ∈ C1,1L (‖ · ‖2), these two inequalities lead to Inequality 5.1 for f̃(ϕ, δ1) in the role of f̃(x, δ), g̃(ϕ, δ2) in the role of g̃(x, δ) and ‖ · ‖2 in the role of ‖ · ‖. We choose the desired accuracy ε for approximating the stationary point of Problem 2.5. This accuracy gives the required accuracy δ of the inexact first-order oracle for f(ϕ) on each step of the inner cycle of the Algorithm 2. Knowing the value δ1 = δ2 and using Lemma 1, we choose the number of steps N of Method 3.1 and thus approximate f(ϕ) with the required accuracy δ1 by f̃(ϕ, δ1). Knowing the value δ2 = δ4R√m and using Lemma 2, we choose the number of stepsN1 of Method 3.1 and the number of steps N2 of Method 3.2, 3.3 and obtain the approximation g̃(ϕ, δ2) of∇f(ϕ) with the required accuracy δ2. Then we use the inexact first-order oracle (f̃(ϕ, δ1), g̃(ϕ, δ2)) to perform a step of Algorithm 2. Since Φ is the Euclidean ball, it is natural to set E = Rm and ‖ · ‖ = ‖ · ‖2,\nchoose the prox-function d(ϕ) = 12‖ϕ‖ 2 2. Then the Bregman distance is V (ϕ, ω) = 1 2‖ϕ − ω‖ 2 2. Algorithm 4 in Supplementary Materials is a formal record of the above ideas.\nThe most computationally consuming operations of the inner cycle of Algorithm 4 are calculations of f̃(ϕk, δ1), f̃(ωk, δ1) and g̃(ϕk, δ2). Using Lemma 1 and Lemma 2, we obtain the complexity of each iteration. From Theorem 3 we obtain the following result, which gives the complexity of Algorithm 4. Theorem 4. The total number of arithmetic operations in Algorithm 4 for the accuracy ε (i.e. for the inequality ‖MK(ϕK − ϕK+1)‖22 ≤ ε to hold) is not more than(\n8L(f(ϕ0)− f∗) ε + log2 2L\nL0\n) · (\n7r|Q|+ 6mps|Q| α ln 1024β1rRL\n√ m\nαε\n) ."
    }, {
      "heading" : "6 EXPERIMENTAL RESULTS",
      "text" : "In this section, we compare our gradient-free and gradient-based methods with the state-of-the-art gradient-based method [21] on the web page ranking problem. In the next section, we describe the dataset. In Section 6.2, we report the results of the experiments."
    }, {
      "heading" : "6.1 DATA",
      "text" : "We consider the user web browsing graph Γq = (Vq, Eq), q ∈ Q, introduced in [12]. Unlike a link graph, a user browsing graph is query–dependent. The set of vertices Vq consists of all different pages visited by users during their sessions started from q. The set of directed edges Eq represents all the ordered pairs of neighboring elements (̃i, i) from such sessions. We add a page i in the seed set Uq if and only if there is a session where i is the first page visited after submitting query q.\nAll experiments are performed with data of a popular commercial search engine Yandex2. We chose a random set of 600 queries Q and collected user sessions started with them. There are ≈ 11.7K vertices and ≈ 7.5K edges in graphs Γq , q ∈ Q, in total. For each query, a set of pages was labelled by professional assessors with standard 5 relevance grades (≈ 1.7K labeled query–document pairs in total). We divide our data into two parts. On the first part Q1 (50% of the set of queries Q) we train the parameters and on the second part Q2 we test the algorithms. For each q ∈ Q and i ∈ Vq, vector Vqi of size m1 = 26 encodes features for query–document pair (q, i). Vector E q ĩi of m2 = 52 features for an edge ĩ→ i ∈ Eq is obtained as the concatenation of Vqĩ and V q i .\nTo study a dependency between the efficiency of the algorithms and the sizes of the graphs, we sort the sets Q1, Q2 in ascending order of sizes of the respective graphs. Sets Q1j , Q 2 j , Q 3 j contain first (in terms of these order) 100, 200, 300 elements respectively for j ∈ {1, 2}."
    }, {
      "heading" : "6.2 PERFORMANCES OF THE OPTIMIZATION ALGORITHMS",
      "text" : "We optimized the parameters ϕ by three methods: our gradient-free method GFN (Algorithm 2), the gradient-based method GBN (Algorithm 4), and the state-of-the-art gradient-method GBP. The values of hyperparameters are the following: the Lipschitz constant L = 10−4 in GFN (and L0 = 10−4 in GBN), the accuracy ε = 10−6 (in both GBN and GFN), the radius R = 0.99 (in both GBN and GFN). On all sets of queries, we compare final values of the loss function for GBN when L0 ∈ {10−4, 10−3, 10−2, 10−1, 1}. The differences are less than 10−7. We choose L in GFN to be equal to L0 (we show how the choice of L influences the output of the gradient-free algorithm, see supplementary materials, Figure 2). Moreover, we evaluate both our gradient-based and gradient-free algorithms for different values of the accuracies. The outputs of the algorithms differ insufficiently on all test sets Qi2, i ∈ {1, 2, 3}, when ε ≤ 10−6. On the lower level of the state-of-the-art gradientbased algorithm, the stochastic matrix and its derivative are raised to the power 100. We evaluate GBP for different values of the step size (50, 100, 200, 500). We stop the GBP algorithms when the differences between the values of the loss function on the next step and the current step are less than −10−5 on the test sets.\n2yandex.com\nIn Table 1, we present the performances of the optimization algorithms in terms of the loss function f (2.4). We also compare the algorithms with the untuned Supervised PageRank (ϕ = ϕ0 = em). On Figure 1, we give the outputs of the optimization algorithms on each iteration of the upper levels of the learning processes on the test set Q32, similar results were obtained for the sets Q 1 2, Q 2 2.\nGFN significantly outperforms the state-of-the-art algorithms on all test sets. GBN significantly outperforms the state-of-the-art algorithm on Q12 (we obtain the p-values of the paired t-tests for all the above differences on the test sets of queries, all these values are less than 0.005). However, GBN requires less iterations of the upper level (until it stops) than GBP for step sizes 50 and 100 onQ22, Q 3 2. Finally, we show that Nesterov–Nemirovski method converges to the stationary distribution faster than the power method (in supplementary materials, on Figure 2, we demonstrate the dependencies of the value of the loss function onQ11 for both methods of computing the untuned Supervised PageRank ϕ = ϕ0 = em)."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "We propose a gradient-free optimization method for general convex problems with inexact zero-order oracle and an adaptive gradient method for possibly nonconvex general composite optimization problems with inexact first-order oracle. For both methods, we provide convergence rate analysis. We also apply our new methods for known problem of learning a web-page ranking algorithm. Our new algorithms not only outperform existing algorithms, but also are guaranteed to solve this learning problem. In practice, this means that these algorithms can increase the reliability and speed of a search engine. Also, to the best of our knowledge, this is the first time when the ideas of random gradient-free and gradient optimization methods are combined with some efficient method for huge-scale optimization using the concept of an inexact oracle.\nAcknowledgments The research by P. Dvurechensky and A. Gasnikov presented in Section 4 of this paper was conducted in IITP RAS and supported by the Russian Science Foundation grant (project 14-50-00150), the research presented in Section 5 was supported by RFBR."
    } ],
    "references" : [ {
      "title" : "O",
      "author" : [ "A. Agarwal" ],
      "venue" : "Dekel and L. Xiao, Optimal algorithms for online convex optimization with multi-point bandit feedback",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Supervised random walks: predicting and recommending links in social networks, 2011, WSDM",
      "author" : [ "L. Backstrom", "J. Leskovec" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Freshness Matters: In Flowers, Food, and Web Authority, 2010, SIGIR",
      "author" : [ "Na Dai", "Brian D. Davison" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "K",
      "author" : [ "N. Eiron" ],
      "venue" : "S. McCurley and J. A. Tomlin, Ranking the web frontier",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Efficient randomized algorithms for PageRank problem",
      "author" : [ "A. Gasnikov", "D. Dmitriev" ],
      "venue" : "Comp. Math. & Math. Phys",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "T",
      "author" : [ "B. Gao", "T.-Y. Liu", "W.W. Huazhong" ],
      "venue" : "Wang and H. Li, Semi-supervised ranking on very large graphs with rich metadata",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Stochastic first- and zeroth-order methods for nonconvex stochastic programming",
      "author" : [ "S. Ghadimi", "G. Lan" ],
      "venue" : "SIAM Journal on Optimization",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Efficient computation of PageRank",
      "author" : [ "T.H. Haveliwala" ],
      "venue" : "Stanford University",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Authoritative sources in a hyperlinked environment, 1998, SODA",
      "author" : [ "J.M. Kleinberg" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "H",
      "author" : [ "Y. Liu", "B. Gao", "T.-Y. Liu", "Y. Zhang", "Z. Ma", "S. He" ],
      "venue" : "Li, BrowseRank: Letting Web Users Vote for Page Importance",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Random optimization",
      "author" : [ "J. Matyas" ],
      "venue" : "Automation and Remote Control",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Random Gradient-Free Minimization of Convex Functions",
      "author" : [ "Yu. Nesterov", "V. Spokoiny" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Finding the stationary states of Markov chains by iterative methods",
      "author" : [ "Yu. Nesterov", "A. Nemirovski" ],
      "venue" : "Applied Mathematics and Computation,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "The PageRank citation ranking: Bringing order to the web",
      "author" : [ "L. Page", "S. Brin", "R. Motwani", "T. Winograd" ],
      "venue" : "Stanford InfoLab",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The intelligent surfer: Probabilistic combination of link and content information",
      "author" : [ "M. Richardson", "P. Domingos" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "P",
      "author" : [ "M. Zhukovskii", "G. Gusev" ],
      "venue" : "Serdyukov, URL Redirection Accounting for Improving Link-Based Ranking Methods",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "P",
      "author" : [ "M. Zhukovskii", "G. Gusev" ],
      "venue" : "Serdyukov, Supervised Nested PageRank",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "P",
      "author" : [ "M. Zhukovskii", "A. Khropov", "G. Gusev" ],
      "venue" : "Serdyukov, Fresh BrowseRank",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convergence of an iterative method for derivatives of eigensystems",
      "author" : [ "A.L. Andrew" ],
      "venue" : "Journal of Computational Physics",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Iterative computation of derivatives of eigenvalues and eigenvectors",
      "author" : [ "A. Andrew" ],
      "venue" : "IMA Journal of Applied Mathematics",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Yu",
      "author" : [ "O. Devolder", "F. Glineur" ],
      "venue" : "Nesterov, First-order methods of smooth convex optimization with inexact oracle, Mathematical Programming",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Cubic regularization of Newton method and its global performance",
      "author" : [ "Yu. Nesterov", "B.T. Polyak" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Gradient methods for minimizing composite functions",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Particularly, PageRank [18], HITS [11], and their variants [8, 9, 19] are originally based on a discrete-time Markov random walk on a link graph.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "Particularly, PageRank [18], HITS [11], and their variants [8, 9, 19] are originally based on a discrete-time Markov random walk on a link graph.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "Particularly, PageRank [18], HITS [11], and their variants [8, 9, 19] are originally based on a discrete-time Markov random walk on a link graph.",
      "startOffset" : 59,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "Particularly, PageRank [18], HITS [11], and their variants [8, 9, 19] are originally based on a discrete-time Markov random walk on a link graph.",
      "startOffset" : 59,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "In contrast, a number of approaches allows to account for different properties of nodes and edges between them by encoding them in restart and transition probabilities (see [3, 4, 6, 10, 12, 20, 21]).",
      "startOffset" : 173,
      "endOffset" : 198
    }, {
      "referenceID" : 3,
      "context" : "In contrast, a number of approaches allows to account for different properties of nodes and edges between them by encoding them in restart and transition probabilities (see [3, 4, 6, 10, 12, 20, 21]).",
      "startOffset" : 173,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "In contrast, a number of approaches allows to account for different properties of nodes and edges between them by encoding them in restart and transition probabilities (see [3, 4, 6, 10, 12, 20, 21]).",
      "startOffset" : 173,
      "endOffset" : 198
    }, {
      "referenceID" : 9,
      "context" : "In contrast, a number of approaches allows to account for different properties of nodes and edges between them by encoding them in restart and transition probabilities (see [3, 4, 6, 10, 12, 20, 21]).",
      "startOffset" : 173,
      "endOffset" : 198
    }, {
      "referenceID" : 16,
      "context" : "In contrast, a number of approaches allows to account for different properties of nodes and edges between them by encoding them in restart and transition probabilities (see [3, 4, 6, 10, 12, 20, 21]).",
      "startOffset" : 173,
      "endOffset" : 198
    }, {
      "referenceID" : 17,
      "context" : "In contrast, a number of approaches allows to account for different properties of nodes and edges between them by encoding them in restart and transition probabilities (see [3, 4, 6, 10, 12, 20, 21]).",
      "startOffset" : 173,
      "endOffset" : 198
    }, {
      "referenceID" : 9,
      "context" : ", the statistics about users’ interactions with the nodes (in web graphs [12] or graphs of social networks [2]), types of edges (such as URL redirecting in web graphs [20]) or histories of nodes’ and edges’ changes [22].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : ", the statistics about users’ interactions with the nodes (in web graphs [12] or graphs of social networks [2]), types of edges (such as URL redirecting in web graphs [20]) or histories of nodes’ and edges’ changes [22].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : ", the statistics about users’ interactions with the nodes (in web graphs [12] or graphs of social networks [2]), types of edges (such as URL redirecting in web graphs [20]) or histories of nodes’ and edges’ changes [22].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 18,
      "context" : ", the statistics about users’ interactions with the nodes (in web graphs [12] or graphs of social networks [2]), types of edges (such as URL redirecting in web graphs [20]) or histories of nodes’ and edges’ changes [22].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 17,
      "context" : "In the general ranking framework called Supervised PageRank [21], weights of nodes and edges in a graph are linear combinations of their features with coefficients as the model parameters.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "The existing optimization method [21] of learning these parameters and the optimizations methods proposed in the presented paper have two levels.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 5,
      "context" : "Following [6], the authors of Supervised PageRank consider a non-convex loss-minimization problem for learning the parameters and solve",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 19,
      "context" : "the parameters of the random walk are obtained by power method introduced in [23, 24].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "the parameters of the random walk are obtained by power method introduced in [23, 24].",
      "startOffset" : 77,
      "endOffset" : 85
    }, {
      "referenceID" : 17,
      "context" : "The considered non-convex loss-minimization problem [21] can not be solved by existing optimization methods such as [16] and [7] due to presence of constraints for parameter vector and the impossibility to calculate the exact value of the loss function.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "The considered non-convex loss-minimization problem [21] can not be solved by existing optimization methods such as [16] and [7] due to presence of constraints for parameter vector and the impossibility to calculate the exact value of the loss function.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "The considered non-convex loss-minimization problem [21] can not be solved by existing optimization methods such as [16] and [7] due to presence of constraints for parameter vector and the impossibility to calculate the exact value of the loss function.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "In our paper, we propose two two-level methods to solve the problem [21].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "On the lower level of these methods, we use the linearly convergent method [17] to calculate an approximation to the stationary distribution of Markov random walk.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "We show that this method allows to approximate the value of the loss function at any given accuracy and has the lowest proved complexity bound among methods proposed in [5].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 12,
      "context" : "Our contribution to the gradient-free methods framework consists in adapting the approach of [16] to the case of constrained optimization problems when the value of the function is calculated with some known accuracy.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Another contribution consists in investigating both for the gradient and gradient-free methods the trade-off between the accuracy of the lower-level algorithm, which is controlled by the number of iterations of method in [17] and its generalization (for derivatives estimation), and the computational complexity of the two-level algorithm as a whole.",
      "startOffset" : 221,
      "endOffset" : 225
    }, {
      "referenceID" : 17,
      "context" : "Summing up, both two-level methods, unlike the state-of-the-art [21], have theoretical guarantees on convergence rate, and outperform it in the ranking quality in experiments.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "We concider the following random walk on a directed graph Γ = (V,E) introduced in [21].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "At each step, the surfer makes a restart with probability α ∈ (0, 1) (originally [18], α = 0.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 9,
      "context" : "We consider the square loss function [12, 21, 22]",
      "startOffset" : 37,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "We consider the square loss function [12, 21, 22]",
      "startOffset" : 37,
      "endOffset" : 49
    }, {
      "referenceID" : 18,
      "context" : "We consider the square loss function [12, 21, 22]",
      "startOffset" : 37,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "We have analysed state-of-the-art methods summarized in the review [5] and power method used in [18, 2, 21] and have found that the method [17] is the most suitable.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "We have analysed state-of-the-art methods summarized in the review [5] and power method used in [18, 2, 21] and have found that the method [17] is the most suitable.",
      "startOffset" : 96,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "We have analysed state-of-the-art methods summarized in the review [5] and power method used in [18, 2, 21] and have found that the method [17] is the most suitable.",
      "startOffset" : 96,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "We have analysed state-of-the-art methods summarized in the review [5] and power method used in [18, 2, 21] and have found that the method [17] is the most suitable.",
      "startOffset" : 96,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "We have analysed state-of-the-art methods summarized in the review [5] and power method used in [18, 2, 21] and have found that the method [17] is the most suitable.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "Our generalization of the method [17] for calculation of dπq(φ) dφT for any q ∈ Q is the following.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "Below we extend the framework of random gradient-free methods [1, 16, 7] for the situation of presence of uniformly bounded error of unknown nature in the value of an objective function in general optimization problem.",
      "startOffset" : 62,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "Below we extend the framework of random gradient-free methods [1, 16, 7] for the situation of presence of uniformly bounded error of unknown nature in the value of an objective function in general optimization problem.",
      "startOffset" : 62,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "Below we extend the framework of random gradient-free methods [1, 16, 7] for the situation of presence of uniformly bounded error of unknown nature in the value of an objective function in general optimization problem.",
      "startOffset" : 62,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "Unlike [16], we consider a constrained optimization problem and a randomization on a Euclidean sphere which seems to give better large deviations bounds and doesn’t need the assumption that the objective function can be calculated at any point of R.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 12,
      "context" : "Unlike [16], we define the biased gradient-free oracle gτ (x, δ) = mτ (f̃(x+ τξ, δ)− f̃(x, δ))ξ, where ξ is a random vector uniformly distributed over the unit sphere S = {t ∈ R : ‖t‖2 = 1}, τ is a smoothing parameter.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 12,
      "context" : "Note that presence of constraints and oracle inexactness do not allow to directly apply the results of [16].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "In this subsection, we generalize the approach in [7] for constrained non-convex optimization problems.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "This is a generalization of the concept of (δ, L)-oracle considered in [25] for convex problems.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "Note that we can not directly apply the results of [7] due to the inexactness of the oracle.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 21,
      "context" : "Similar to [25], since f ∈ C(1,1) L (‖ · ‖2), these two inequalities lead to Inequality 5.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 17,
      "context" : "In this section, we compare our gradient-free and gradient-based methods with the state-of-the-art gradient-based method [21] on the web page ranking problem.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : "We consider the user web browsing graph Γq = (Vq, Eq), q ∈ Q, introduced in [12].",
      "startOffset" : 76,
      "endOffset" : 80
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we consider a non-convex loss-minimization problem of learning Supervised PageRank models, which can account for features of nodes and edges. We propose gradient-based and random gradient-free methods to solve this problem. Our algorithms are based on the concept of an inexact oracle and unlike the state-ofthe-art gradient-based method we manage to provide theoretically the convergence rate guarantees for both of them. Finally, we compare the performance of the proposed optimization methods with the state of the art applied to a ranking task.",
    "creator" : null
  }
}