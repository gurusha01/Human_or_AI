{
  "name" : "5487315b1286f907165907aa8fc96619.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Convex Two-Layer Modeling with Latent Structure",
    "authors" : [ "Vignesh Ganapathiraman", "Xinhua Zhang", "Yaoliang Yu", "Junfeng Wen" ],
    "emails" : [ "zhangx}@uic.edu,", "yaoliang.yu@uwaterloo.ca,", "junfengwen@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Over the past decade deep learning has achieved significant advances in many application areas [1]. By automating the acquisition of latent descriptive and predictive representation, they provide highly effective models to capture the relationships between observed variables. Recently more refined deep models have been proposed for structured output prediction, where several random variables for prediction are statistically correlated [2–4]. Improved performance has been achieved in applications such as image recognition and segmentation [5] and natural language parsing [6], amongst others.\nSo far, most deep models for structured output are designed for supervised learning where structured labels are available. Recently an extension has been made to the unsupervised learning. [7] proposed a conditional random field auto-encoder (CRF-AE)—a two-layer conditional model—where given the observed data x, the latent structure y is first generated based on p(y|x), and then applied to reconstruct the observations using p(x|y). The motivation is to find the predictive and discriminative (rather than common but irrelevant) latent structure in the data. Along similar lines, several other discriminative unsupervised learning methods are also available [8–11].\nExtending auto-encoders X→Y →X to general two-layer models X→Y →Z is not hard. [12, 13] addressed transliteration between two languages, where Z is the observed binary label indicating if two words match, and higher accuracy can be achieved if we faithfully recover a letter-wise matching represented by the unobserved structure Y . In essence, their model optimizes p(z|arg maxy p(y|x)), uncovering the latent y via its mode under the first layer model. This is known as bi-level optimization because the arg max of inner optimization is used. A soft variant adopts the mean of y [14]. In general, conditional models yield more accurate predictions than generative models X−Y −Z (e.g. multi-wing harmoniums/RBMs), unless the latter is trained in a discriminative fashion [15].\nIn computation, all methods require certain forms of tractability in inference. CRF-AE leverages marginal inference on p(y|x)p(x|y) (over y) for EM. Contrastive divergence, instead, samples from p(y|x) [11]. For some structures like graph matching, neither of them is tractable [16, 17] (unless assuming first-order Markovian). In single-layer models, this challenge has been resolved by max-margin estimation, which relies only on the MAP of p(y|x) [18]. This oracle is much less demanding than sampling or normalization, as finding the most likely state can be much easier than summarizing over all possible y. For example, MAP for graph matching can be solved by max-flow.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nUnfortunately a direct extension of max-margin estimation to two-layer modeling meets with immediate obstacles, because here one has to solve maxy p(y|x)p(z|y). In general, p(z|y) depends on y in a highly nonlinear form, making this augmented MAP inference intractable. This seems to leave the aforementioned bi-level optimization the only option that retains the sole dependency on MAP. However, solving this optimization poses substantial challenge when y is discrete, because the mode of p(y|x) is almost always invariant to small perturbations of model parameters (i.e. zero gradient). In this paper we demonstrate that this optimization can be relaxed into a convex formulation while still preserving sufficient regularities to recover a non-trivial, nonlinear predictive model that supports structured latent representations. Recently a growing body of research has investigated globally trainable deep models. But they remain limited. [19] formulated convex conditional models using layer-wise kernels, connected through nonlinear losses. However these losses are data dependent, necessitating a transductive setting to retain the context. [20] used boosting but the underlying oracle is generally intractable. Specific global methods were also proposed for polynomial networks [21] and sum-product networks [22]. None of these methods accommodate structures in latent layers.\nOur convex formulation is achieved by enforcing the first-order optimality conditions of inner level optimization via sublinear constraints. Using a semi-definite relaxation, we amount to the first two-layer model that allows latent structures to be inferred concurrently with model optimization while still admitting globally optimal solutions (§3). To the best of our knowledge, this is the first algorithm in machine learning that directly constructs a convex relaxation for a bi-level optimization based on the inner optimality conditions. Unlike [19], it results in a truly inductive model, and its flexibility is demonstrated with two example structures in the framework of total unimodularity (§4). The only inference required is MAP on p(y|x), and the overall scalability is further improved by a refined optimization algorithm (§5). Experimental results demonstrate its useful potentials in practice."
    }, {
      "heading" : "2 Preliminaries and Background",
      "text" : "We consider a two-layer latent conditional model X → Y → Z, where X is the input, Z is the output, and Y is a latent layer composed of h random variables: {Yi}hi=1. Instead of assuming no interdependency between Yi as in [19], our major goal here is to model the structure in the latent layer Y . Specifically, we assume a conditional model for the first layer based on an exponential family p(y|x) = q0(y) exp(−y′Ux− Ω(Ux)), where q0(y) = Jy ∈ YK . (1) Here U is the first layer weight matrix, and Ω is the log-partition function. q0(y) is the base measure, with JxK = 1 if x is true, and 0 otherwise. The correlation among Yi is instilled by the support set Y , which plays a central role here. For example, when Y consists of all h-dimensional canonical vectors, p(y|x) recovers the logistic multiclass model. In general, to achieve a tradeoff between computational efficiency and representational flexibility, we make the following assumptions on Y: Assumption 1 (PO-tractable). We assume Y is bounded, and admits an efficient polar operator. That is, for any vector d ∈ Rh, miny∈Y d′y is efficiently solvable.\nNote the support set Y (hence the base measure q0) is fixed and does not contain any more parameter. PO-tractability is available in a variety of applications, and we give two examples here.\nGraph matching. In a bipartite graph with two sets of vertices {ai}ni=1 and {bj}nj=1, each edge between ai and bj has a weight Tij . The task is to find a one-to-one mapping (can be extended) between {ai} and {bj}, such that the sum of weights on the edges is maximized. Denote the matching by Y ∈ {0, 1}n×n where Yij = 1 iff the edge (ai, bj) is selected. So the optimal matching is the mode of p(Y ) ∝ JY ∈ YK exp(tr(Y ′T )) where the support is Y = {Y ∈ {0, 1}n×n : Y 1 = Y ′1 = 1}. Graphical models. For simplicity, consider a linear chain model V1 − V2 − · · · − Vp. Here each Vi can take one of C possible values, which we encode using the C-dimensional canonical basis vi. Suppose there is a node potential mi ∈ RC for each Vi, and each edge (Vi, Vi+1) has an edge potential Mi ∈ RC×C . Then we could directly define a distribution on {Vi}. Unfortunately, it will involve quadratic terms such as v′iMivi+1, and so a different parameterization is in order. Let Yi ∈ {0, 1}C×C encode the values of (Vi, Vi+1) via row and column indices of Yi respectively. Then the distribution on {Vi} can be equivalently represented by a distribution on {Yi}:\np({Yi}) ∝ J{Yi} ∈ YK exp (∑p\ni=1 m′iYi1 + ∑p−1 i=1 tr(M ′iYi) ) , (2)\nwhere Y = { {Yi} : Yi ∈ {0, 1}C×C } ∩ H, withH := {{Yi} : 1′Yi1 = 1, Y ′i 1 = Yi+11} . (3)\nThe constraints inH encode the obvious consistency constraints between overlapping edges. This model ultimately falls into our framework in (1).\nIn both examples, the constraints in Y are totally unimodular (TUM), and therefore the polar operator can be computed by solving a linear programming (LP), with the {0, 1} constraints relaxed to [0, 1]. In §4.1 and 4.2 we will generalize y′Ux to y′d(Ux), where d is an affine function of Ux that allows for homogeneity in temporal models. For clarity, we first develop a general framework using y′Ux.\nOutput layer As for the output layer, we assume a conditional model from an exponential family p(z|y) = exp(z′R′y −G(R′y))q1(z) = exp(−DG∗(z||∇G(R′y)) +G∗(z))q1(z), (4)\nwhere G is a smooth and strictly convex function, and DG∗ is the Bregman divergence induced by the Fenchel dual G∗. Such a parameterization is justified by the equivalence between regular Bregman divergence and regular exponential family [23]. Thanks to the convexity of G, it is trivial to extend p(z|y) to y ∈ convY (the convex hull of Y), and G(R′y) will still be convex over convY (fixing R). Finally we highlight the assumptions we make and do not make. First we only assume PO-tractability of Y , hence tractability in MAP inference of p(y|x). We do not assume it is tractable to compute the normalizer Ω or its gradient (marginal distributions). We also do not assume that unbiased samples of y can be drawn efficiently from p(y|x). In general, PO-tractability is a weaker assumption. For example, in graph matching MAP inference is tractable while marginalization is NP-hard [16] and sampling requires MCMC [24]. Finally, we do not assume tractability of any sort for p(y|x)p(z|y) (in y), and so it may be hard to solve miny∈Y{d′y +G(R′y)− z′R′y}, as G is generally not affine."
    }, {
      "heading" : "2.1 Training principles",
      "text" : "At training time, we are provided with a set of feature-label pairs (x, z) ∼ p̃, where p̃ is the empirical distribution. In the special case of auto-encoder, z is tied with x. The “bootstrapping\" style estimation [25] optimizes the joint likelihood with the latent y imputed in an optimistic fashion\nmin U,R E (x,z)∼p̃ [ min y∈Y − log p(y|x)p(z|y) ] = min U,R E (x,z)∼p̃ [ min y∈Y y′Ux + Ω(Ux)− z′R′y +G(R′y) ] .\nThis results in a hard EM estimation, and a soft version can be achieved by adding entropic regularizers on y. Regularization can be imposed on U and R which we will make explicit later (e.g. bounding the L2 norm). Since the log-partition function Ω in p(y|x) is hard to compute, the max-margin approach is introduced which replaces Ω(Ux) by an upper bound maxŷ∈Y −ŷ′Ux, leading to a surrogate loss\nmin U,R E (x,z)∼p̃ [ min y∈Y { −z′R′y +G(R′y) + y′Ux−min ŷ∈Y ŷ′Ux }] . (5)\nHowever, the key disadvantage of this method is the augmented inference on y, because we have only assumed the tractability of miny∈Y d′y for all d, not miny∈Y{y′d+G(R′y)− z′R′y}. In addition, this principle intrinsically determines the latent y as a function of both the input and the output, while at test time the output itself is unknown and is the subject of prediction. The common practice therefore requires a joint optimization over y and z at test time, which is costly in computation.\nThe goal of this paper is to design a convex formulation in which the latent y is completely determined by the input x, and both the prediction and estimation rely only on the polar operator: arg miny∈Y y\n′Ux. As a consequence of this goal, it is natural to postulate that the y found this way renders an accurate prediction of z, or a faithful recovery of x in auto-encoders. This idea, which has been employed by [e.g., 9, 26], leads to the following bi-level optimization problem\nmax U,R E (x,z)∼p̃\n[ log p ( z ∣∣∣arg max\ny∈Y p(y|x)\n)] ⇔ max\nU,R E\n(x,z)∼p̃\n[ log p ( z ∣∣∣arg min\ny∈Y y′Ux\n)] (6)\n⇔ min U,R E (x,z)∼p̃ [−z′R′y∗x +G(R′y∗x)] , where y∗x = arg min y∈Y y′Ux. (7) Directly solving this optimization problem is challenging, because the optimal y∗x is almost surely invariant to small perturbations of U (e.g. when Y is discrete). So a zero valued gradient is witnessed almost everywhere. Therefore a more carefully designed optimization algorithm is in demand."
    }, {
      "heading" : "3 A General Framework of Convexification",
      "text" : "We propose addressing this bi-level optimization by convex relaxation, and it is built upon the first-order optimality conditions of the inner-level optimization. First notice that the set Y participates\nin the problem (7) only via the polar operator at Ux: arg miny∈Y y′Ux. If Y is discrete, this problem is equivalent to optimizing over S := convY , because a linear function on a convex set is always optimized on its extreme points. Clearly, S is convex, bounded, closed, and is PO-tractable. It is important to note that the origin is not necessarily contained in S. To remove the potential non-uniqueness of the minimizer in (7), we next add a small proximal term to the polar operator problem (σ is a small positive number):\nmin w∈S\nw′Ux + σ\n2 ‖w‖2 . (8)\nThis leads to a small change in the problem and makes sure that the minimizer is unique.1 Adding strongly convex terms to the primal and dual objectives is a commonly used technique for accelerated optimization [27], and has been used in graphical model inference [e.g., 28]. We intentionally changed the symbol y into w, because here the optimal w is not necessarily in Y . By the convexity of the problem (8) and noting that the gradient of the objective is Ux + σw, w is optimal if and only if w ∈ S, and (Ux + σw)′(θ̂ −w) ≥ 0, ∀θ̂ ∈ S. (9) These optimality conditions can be plugged into the bi-level optimization problem (7). Introducing “Lagrange multipliers” (γ, θ̂) to enforce the latter condition via a mini-max formulation, we obtain\nmin ‖U‖≤1 min ‖R‖≤1 E (x,z)∼p̃ [ min w max γ≥0,θ̂∈S max v −z′R′w + v′R′w −G∗(v) (10)\n+ ιS(w) + γ(Ux + σw) ′(w − θ̂) ] , (11)\nwhere ιS is the {0,∞}-valued indicator function of the set S. Here we dualized G via G(R′w) = maxv v\n′R′w − G∗(v), and made explicit the Frobenius norm constraints (‖·‖) on U and R.2 Applying change of variable θ = γθ̂, the constraints γ ≥ 0 and θ̂ ∈ S (a convex set) become\n(θ, γ) ∈ N := cone{(θ̂, 1) : θ̂ ∈ S}, where cone stands for the conic hull (convex). Similarly we can dualize ιS(w) = maxπ π′w−σS(π), where σS(π) := maxw∈S π′w is the support function on S. Now swap minw with all the subsequent max (strong duality), we arrive at a form where w can be minimized out analytically\nmin ‖U‖≤1 min ‖R‖≤1 E (x,z)∼p̃ [ max π max (θ,γ)∈N max v min w −z′R′w + v′R′w −G∗(v) (12)\n+ π′w − σS(π) + (Ux + σw)′(γw − θ) ] (13)\n= min ‖U‖≤1 min ‖R‖≤1 E (x,z)∼p̃ [ max π max (θ,γ)∈N max v −G∗(v)− σS(π)− θ′Ux (14)\n− 14σγ ‖R(v − z) + γUx + π − σθ‖ 2 ] . (15)\nGiven (U,R), the optimal (v,π,θ, γ) can be efficiently solved through a concave maximization. However the overall objective is not convex in (U,R) because the quadratic term in (15) is subtracted. Fortunately it turns out not hard to tackle this issue by using semi-definite programming (SDP) relaxation which linearizes the quadratic terms. In particular, let I be the identity matrix, and define\nM := M(U,R) := ( I U ′\nR′\n) (I, U,R) = ( I U R U ′ U ′U U ′R R′ R′U R′R ) =: M1 Mu MrM ′u Mu,u M ′r,u M ′r Mr,u Mr,r  . (16) Then θ′Ux can be replaced by θ′Mux and the quadratic term in (15) can be expanded as\nf(M,π,θ, γ,v;x, z) := tr(Mr,r(v − z)(v − z)′) + γ2 tr(Mu,uxx′) + 2γ tr(Mr,ux(v − z)′) + 2(π − σθ)′(Mr(v − z) + γMux) + ‖π − σθ‖2 . (17)\nSince given (π,θ, γ,v) the objective function becomes linear in M , so after maximizing out these variables the overall objective is convex in M . Although this change of variable turns the objective into convex, it indeed shifts the intractability into the feasible region of M :\nM0 := {M 0 : M1 = I, tr(Mu,u) ≤ 1, tr(Mr,r) ≤ 1}︸ ︷︷ ︸ =:M1 ∩ {M : rank(M) = h} . (18)\n1If p(y|x) ∝ p0(y) exp(−y′Ux− σ2 ‖y‖ 2) (for any σ > 0), then there is no need to add this σ 2 ‖w‖2 term. In this case, all our subsequent developments apply directly. Therefore our approach applies to a broader setting where L2 projection to S is tractable, but here we focus on PO-tractability just for the clarity of presentation.\n2To simplify the presentation, we bound the radius by 1 while in practice it is a hyperparameter to be tuned.\nHere M 0 means M is real symmetric and positive semi-definite. Due to the rank constraint,M0 is not convex. So a natural relaxation—the only relaxation we introduce besides the proximal term in (8)—is to drop this rank constraint and optimize with the resulting convex setM1. This leads to the final convex formulation:\nmin M∈M1 E (x,z)∼p̃ [ max π max (θ,γ)∈N max v −G∗(v)− σS(π)− θ′Mux− 14σγ f(M,π,θ, γ,v;x, z) ] . (19)\nTo summarize, we have achieved a convex model for two-layer conditional models in which the latent structured representation is determined by a polar operator. Instead of bypassing this bi-level optimization via the normal loss based approach [e.g., 19, 29], we addressed it directly by leveraging the optimality conditions of the inner optimization. A convex relaxation is then achieved via SDP."
    }, {
      "heading" : "3.1 Inducing low-rank solutions of relaxation",
      "text" : "Although it is generally hard to provide a theoretical guarantee for nonlinear SDP relaxations, it is interesting to note that the constraint setM1 effectively encourages low-rank solutions (hence tighter relaxations). As a key technical result, we next show that all extreme points ofM1 are rank h (the number of hidden nodes) for all h ≥ 2. Recall that in sparse coding, the atomic norm framework [30] induces low-complexity solutions by setting up the optimization over the convex hull of atoms, or penalize via its gauge function. Therefore the characterization of the extreme points ofM1 might open up the possibility of analyzing our relaxation by leveraging the results in sparse coding. Lemma 1. Let Ai be symmetric matrices. Consider the set of\nR := {X : X 0, tr(AiX) S bi, i = 1, . . . ,m}, (20) where m is the number of linear (in)equality constraints. S means it can be any one of ≤, =, or ≥. Then the rank r of all extreme points ofR is upper bounded by\nr ≤ ⌊ 1 2 ( √ 8m+ 1− 1) ⌋ . (21)\nThis result extends [31] by accommodating inequalities in (20), and its proof is given in Appendix A. Now we show that the feasible regionM1 as defined by (18) has all extreme points with rank h. Theorem 1. If h ≥ 2, then all extreme points ofM1 have rank h, andM1 is the convex hull ofM0. Proof. Let M be an extreme point ofM1. Noting that M 0 already encodes the symmetry of M , the linear constraints forM1 in (18) can be written as 12h(h+ 1) linear equality constraints and two linear inequality constraints. In total m = 12h(h+ 1) + 2. Plug it into (21) in the above lemma\nrank(M) ≤ ⌊ 1 2 ( √ 8m+ 1− 1) ⌋ = ⌊ 1 2 ( √ 4h(h+ 1) + 17− 1) ⌋\n= h+ Jh = 1K. (22) Finally, the identity matrix in the top-left corner of M forces rank(M) ≥ h. So rank(M) = h for all h ≥ 2. It then follows thatM1 = convM0."
    }, {
      "heading" : "4 Application in Machine Learning Problems",
      "text" : "The framework developed above is generic. For example, when Y represents classification for h classes by canonical vectors, S = convY is the h dimensional probability simplex (sum up to 1). Clearly σS(π) = maxi πi, and N = {(x, t) ∈ Rh+1+ : 1′x = t}. In many applications, Y can be characterized as {y ∈ {0, 1}h : Ay ≤ c}, where A is TUM and all entries of c are in {−1, 1, 0}.3 In this case, the convex hull S has all extreme points being integral, and S employs an explicit form: Y = {y ∈ {0, 1}h : Ay ≤ c} =⇒ S = convY = {w ∈ [0, 1]h : Aw ≤ c}, (23)\nreplacing all binary constraints {0, 1} by intervals [0, 1]. Clearly TUM is a sufficient condition for PO-tractability, because miny∈Y d′y is equivalent to minw∈S d′w, an LP. Examples include the above graph matching and linear chain model. We will refer to Aw ≤ c as the non-box constraint."
    }, {
      "heading" : "4.1 Graph matching",
      "text" : "As the first concrete example, we consider convex relaxation for latent graph matching. One task in natural language is transliteration [12, 32]. Suppose we are given an English word e with m letters, and a corresponding Hebrew word h with n letters. The goal is to predict whether e and h are phonetically similar, a binary classification problem with z ∈ {−1, 1}. However it obviously helps to\n3For simplicity, we write equality constraints (handled separately in practice) using two inequality constraints.\nfind, as an intermediate step, the letter-wise matching between e and h. The underlying assumption is that each letter corresponds to at most one letter in the word of the other language. So if we augment both e and h with a sink symbol * at the end (hence making their length m̂ := m+ 1 and n̂ := n+ 1 respectively), we would like to find a matching y ∈ {0, 1}m̂n̂ that minimizes the following cost\nmin Y ∈Y m̂∑ i=1 n̂∑ j=1 Yiju ′φij ,where Y = {0, 1}m̂×n̂ ∩ {Y : Yi,:1 = 1,∀i ≤ m, 1′Y:,j = 1,∀j ≤ n}︸ ︷︷ ︸\n=:G\n. (24)\nHere Yi,: is the i-th row of Y . φij ∈ Rp is a feature vector associated with the pair of i-th letter in e and j-th letter in h, including the dummy *. Our notation omits its dependency on e and h. u is a discriminative weight vector that will be learned from data. After finding the optimal Y ∗, [12] uses the maximal objective value of (24) to make the final binary prediction: − sign( ∑ ij Y ∗ iju ′φij).\nTo pose the problem in our framework, we first notice that the non-box constraints G in (24) are TUM. Therefore, S is simply [0, 1]m̂×n̂ ∩ G. Given the decoded w, the output labeling principle above essentially duplicates u as the output layer weight. A key advantage of our method is to allow the weights of the two layers to be decoupled. By using a weight vector r ∈ Rp, we define the output score as r′Φw, where Φ is a p-by-m̂n̂ matrix whose (i, j)-th column is φij . So Φ depends on e and h. Overall, our model follows by instantiating (12) as:\nmin ‖U‖≤1 min ‖R‖≤1 E (e,h,z)∼p̃ [ max π max (θ,γ)∈N max v∈R min w −zr′Φw + vr′Φw −G∗(v) + π′w (25)\n− σS(π) + ∑\nij (u′φij + σwij)(γwij − θij)\n] . (26)\nOnce more we can minimize out w, which gives rise to a quadratic ‖(v − z)Φ′r + γΦ′u + π − σθ‖2. It is again amenable to SDP relaxation, where (Mu,u,Mr,u,Mr,r) correspond to (uu′, ru′, rr′) resp."
    }, {
      "heading" : "4.2 Homogeneous temporal models",
      "text" : "A variety of structured output problems are formulated with graphical models. We highlight the gist of our technique by using a concrete example: unsupervised structured learning for inpainting. Suppose we are given images of handwritten words, each segmented into p letters, and the latent representation is the corresponding letters. Since letters are correlated in their appearance in words, the recognition problem has long been addressed using linear chain conditional random fields. However imagine no ground truth letter label is available, and instead of predicting labels, we are given images in which a random small patch is occluded. So our goal will be inpainting the patches.\nTo cast the problem in our two-layer latent structure model, let each letter image in the word be denoted as a vector xi ∈ Rn, and the reconstructed image be zi ∈ Rm (m = n here). Let Yi ∈ {0, 1}h×h (h = 26) encode the labels of the letter pair at position i and i + 1 (as rows and columns of Yi respectively). Let Uv ∈ Rh×n be the letter-wise discriminative weights, and Ue ∈ Rh×h be the pairwise weights. Then by (2), the MAP inference can be reformulated as (ref. definition ofH in (3))\nmin {Yi}∈Y ∑p i=1 1′Y ′i Uvxi + ∑p−1 i=1 tr(U ′eYi) where Y = { {Yi} : Yi ∈ {0, 1}C×C } ∩H. (27) Since the non-box constraints in H are TUM, the problem can be cast in our framework with S = convY = { {Yi} : Yi ∈ [0, 1]C×C } ∩ H. Finally to reconstruct the image for each letter, we assume that each letter j has a basis vector rj ∈ Rm. So given Wi, the output of reconstruction is R′Wi1, where R = (r1, . . . , rh)′. To summarize, our model can be instantiated from (12) as\nmin ‖U‖≤1 min ‖R‖≤1 E (x,z)∼p̃\n[ max\nΠ max (Θ,γ)∈N max v min W ∑p i=1 (vi − zi)′R′Wi1−G∗(vi) (28)\n+ tr(Π′W )− σS(Π) + ∑p\ni=1 tr((Uvxi1\n′ + Ji 6= pKUe + σWi)′(γWi −Θi)) ] .\nHere zi is the inpainted images in the training set. If no training image is occluded, then just set zi to xi. The constraints on U and R can be refined, e.g. bounding ‖Uv‖, ‖Ue‖, and ‖rj‖ separately. As before, we can derive a quadratic term ‖R(vi − zi)1′ + γUvxi1′ + γUe + Πi − σΘi‖2 by minimizing out Wi, which again leads to SDP relaxations. Even further, we may allow each letter to employ a set of principal components, whose combination yields the reconstruction (Appendix B).\nBesides modeling flexibility, our method also accommodates problem-specific simplification. For example, the dimension of w is often much higher than the number of non-box constraints. Appendix C shows that for linear chain, the dimension of w can be reduced from C2 to C via partial Lagrangian."
    }, {
      "heading" : "5 Optimization",
      "text" : "The key advantage of our convex relaxation (19) is that the inference depends on S (or equivalently Y) only through the polar operator. Our overall optimization scheme is to perform projected SGD over the function of M . This requires: a) given M , compute its objective value and gradient; and b) project toM1. We next detail the solution to the former, relegating the latter to Appendix D. Given M , we optimize over (π,θ, γ,v) by projected LBFGS [33]. The objective is easy to compute thanks to PO-tractability (for the σS(π) term). The only nontrivial part is to project a point (θ0, γ0) to N , which is actually amenable to conditional gradient (CG). Formally it requires solving\nminθ,γ 1 2 ‖θ − θ0‖ 2 + 12 (γ − γ0) 2, s.t. θ = γs, γ ∈ [0, C], s ∈ S. (29) W.l.o.g., we manually introduced an upper bound4 C := γ0 + √ ‖θ0‖2 + γ20 on γ. At each iteration, CG queries the gradient gθ in θ and gγ in γ, and solves the polar operator problem on N : minθ∈γS,γ∈[0,C] θ ′gθ+γgγ = mins∈S,γ∈[0,C] γs ′gθ + γgγ = min{0, C mins∈S(s′gθ+gγ)}. (30) So it boils down to the polar operator on S, and is hence tractable. If the optimal value in (30) is nonnegative, then the current iterate is already optimal. Otherwise we add a basis (s∗, 1) to the ensemble and a totally corrective update can be performed by CG. More details are available in [34].\nAfter finding the optimal M̂ , we recover the optimal w for each training example based on the optimal w in (12). Using it as the initial point, we locally optimize the two layer models U and R based on (14)."
    }, {
      "heading" : "6 Experimental Results",
      "text" : "To empirically evaluate our convex method (henceforth referred to as CVX), we compared it with the state-of-the-art methods on two prediction problems with latent structure.\nTransliteration The first experiment is based on the English-Hebrew corpus [35]. It consists of 250 positive transliteration pairs for training, and 300 pairs for testing. On average there are 6 characters per word in each of the languages. All these pairs are considered “positive examples\", and for negative examples we followed [12] and randomly sampled t− ∈ {50, 75, 100} pairs from 2502 − 250 mismatched pairings (which are 20%, 30%, and 40% of 250, resp). We did not use many negative examples because, as per [12], our test performance measure will depend mainly on the highest few discriminative values, which are learned largely from the positive examples.\nGiven a pair of words (e,h), the feature representation φij for the i-th letter in e and j-th letter in h is defined as the unigram feature: an n-dimensional vector with all 0’s except a single one in the (ei, hj)-th coordinate. In this dataset, there are n = 655 possible letter pairs (* included). Since our primary objective is to determine whether the convex relaxation of a two-layer model with latent structure can outperform locally trained models, we adopted this simple but effective feature representation (rather than delving into heuristic feature engineering).\nOur test evaluation measurement is the Mean Reciprocal Rank (MRR), which is the average of the reciprocal of the rank of the correct answer. In particular, for each English word e, we calculated the discriminative score of respective methods when e is paired with each Hebrew word in the test set, and then found the rank of the correct word (1 for the highest). The reciprocal of the rank is averaged over all test pairs, giving the MRR. So a higher value is preferred, and 50% means on average the true Hebrew word is the runner-up. For our method, the discriminative score is simply f := r′Φw (using the symbols in (25)), and that for [12] is f := maxY ∈Y u′Φvec(Y ) (vectorization of Y ).\nWe compared our method (with σ = 0.1) against the state-of-the-art approach in [12]. It is a special case of our model with the second-layer weight r tied with the first-layer weight u. They trained it using a local optimization method, and we will refer to it as Local. Both methods employ an output loss function max{0, yf}2 with y ∈ {+1,−1}, and both contain only one parameter—the bound on ‖u‖ (and ‖r‖). We simply tuned it to optimize the performance of Local. The test MRR is shown in Figure 1, where the number of negative examples was varied in 50, 75, and 100. Local was trained with random initialization, and we repeated the random selection of the negative examples for 10 times, yielding 10 dots in each scatter plot. It is clear that CVX in general delivers significantly higher MRR than Local, with the dots lying above or close to the diagonal. Since this dataset is not big, the randomness of the negative set leads to notable variations in the performance (for both methods).\n4For γ to be optimal, we require (γ−γ0)2 ≤ ‖θ−θ0‖2+(γ−γ0)2 ≤ ‖0−θ0‖2+(0−γ0)2, i.e., γ ≤C.\nInpainting for occluded image Our second experiment used structured latent model to inpaint images. We generated 200 sequences of images for training, each with p ∈ {4, 6, 8} digits. In order to introduce structure, each sequence can be either odd (i.e. all digits are either 1 or 3) or even (all digits are 2 or 4). So C = 4. Given the digit label, the corresponding image (x ∈ [0, 1]196) was sampled from the MNIST dataset, downsampled to 14-by-14. 200 test sequences were also generated.\nIn the test data, we randomly set a k× k patch of each image to 0 as occluded (k ∈ {2, 3, 4}), and the task is to inpaint it. This setting is entirely unsupervised, with no digit label available for training. It falls in the framework of X → Y → Z, where X is the occluded input, Y is the latent digit sequence, and Z is the recovered image. In our convex method, we tied Uv with R and so we still have a 3-by-3 block matrix M , corresponding to I , Uv and Ue. We set σ to 10−1 and G(·) = 12 ‖·‖\n2 (Gaussian). Y was predicted using the polar operator, based on which Z was predicted with the Gaussian mean.\nFor comparison, we used CRF-AE, which was proposed very recently by [7]. Although it ties X and Z, extension to our setting is trivial by computing the expected value of Z given X . Here P (Z|Y ) is assumed a Gaussian whose mean is learned by maximizing P (Z = x|X = x), and we initialized all model parameters by unit Gaussian. For the ease of comparison, we introduced regularization by constraining model parameters to L2 norm balls rather than penalizing the squared L2 norm. For both methods, the radius bound was simply chosen as the maximum L2 norm of the images, which produced consistently good results. We did not use higher k because the images are sized 14-by-14.\nThe error of inpainting given by the two methods is shown in Table 1 where we varied the size of the occluded patch with p fixed to 6, and in Table 2 where the length of the sequence p was varied while k was fixed to 4. Each number is the sum of squared error in the occluded patch, averaged over 5 random generations of training and test data (hence producing the mean and standard deviation). Here we can see that CVX gives lower error than CRF-AE. With no surprise, the error grows almost quadratically in k. When the length of sequence grows, the error of both CVX and CRF-AE fluctuates nonmonotonically. This is probably because with more images in each node, the total error is summed over more images, but the error per image decays thanks to the structure."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have presented a new formulation of two-layer models with latent structure, while maintaining a jointly convex training objective. Its effectiveness is demonstrated by the superior empirical performance over local training, along with low-rank characterization of the extreme points of the feasible region. An interesting extension for future investigation is when the latent layer employs submodularity, with its base polytope mirroring the support set S."
    } ],
    "references" : [ {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Learning deep structured models",
      "author" : [ "L.-C. Chen", "A. Schwing", "A. Yuille", "R. Urtasun" ],
      "venue" : "In ICML",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Conditional restricted Boltzmann machines for structured output prediction",
      "author" : [ "V. Mnih", "H. Larochelle", "G.E. Hinton" ],
      "venue" : "In AISTATS",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Sum-product networks for structured prediction: Contextspecific deep conditional random fields",
      "author" : [ "M. Ratajczak", "S. Tschiatschek", "F. Pernkopf" ],
      "venue" : "In Workshop on Learning Tractable Probabilistic Models",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "K. Sohn", "X. Yan", "H. Lee" ],
      "venue" : "In NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Deep learning for efficient discriminative parsing",
      "author" : [ "R. Collobert" ],
      "venue" : "In ICML",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Conditional random field autoencoders for unsupervised structured prediction",
      "author" : [ "W. Ammar", "C. Dyer", "N.A. Smith" ],
      "venue" : "In NIPS",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Discriminative unsupervised learning of structured predictors",
      "author" : [ "L. Xu", "D. Wilkinson", "F. Southey", "D. Schuurmans" ],
      "venue" : "In ICML",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "Unsupervised search-based structured prediction",
      "author" : [ "H. Daumé III" ],
      "venue" : "In ICML",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Contrastive estimation: training log-linear models on unlabeled data",
      "author" : [ "N. Smith", "J. Eisner" ],
      "venue" : "In ACL",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Discriminative learning over constrained latent representations",
      "author" : [ "M.-W. Chang", "D. Goldwasser", "D. Roth", "V. Srikumar" ],
      "venue" : "In NAACL",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Structured output learning with indirect supervision",
      "author" : [ "M.-W. Chang", "V. Srikumar", "D. Goldwasser", "D. Roth" ],
      "venue" : "In ICML",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Large-margin predictive latent subspace learning for multiview data analysis",
      "author" : [ "N. Chen", "J. Zhu", "F. Sun", "E.P. Xing" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Classification using discriminative restricted Boltzmann machines",
      "author" : [ "H. Larochelle", "Y. Bengio" ],
      "venue" : "In ICML",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "On the partition function and random maximum a-posteriori perturbations",
      "author" : [ "T. Hazan", "T. Jaakkola" ],
      "venue" : "In ICML",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Learning with random maximum a-posteriori perturbation models",
      "author" : [ "A. Gane", "T. Hazan", "T. Jaakkola" ],
      "venue" : "In AISTATS",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Learning structured prediction models: A large margin approach",
      "author" : [ "B. Taskar", "V. Chatalbashev", "D. Koller", "C. Guestrin" ],
      "venue" : "In ICML",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Convex deep learning via normalized kernels",
      "author" : [ "O. Aslan", "X. Zhang", "D. Schuurmans" ],
      "venue" : "In NIPS",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Convex neural networks",
      "author" : [ "Y. Bengio", "N.L. Roux", "P. Vincent", "O. Delalleau", "P. Marcotte" ],
      "venue" : "In NIPS",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    }, {
      "title" : "Provable bounds for learning some deep representations",
      "author" : [ "S. Arora", "A. Bhaskara", "R. Ge", "T. Ma" ],
      "venue" : "In ICML",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "An algorithm for training polynomial networks, 2014. ArXiv:1304.7045v2",
      "author" : [ "R. Livni", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Clustering with Bregman divergences",
      "author" : [ "A. Banerjee", "S. Merugu", "I.S. Dhillon", "J. Ghosh" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Sampling from probabilistic submodular models",
      "author" : [ "A. Gotovos", "H. Hassani", "A. Krause" ],
      "venue" : "In NIPS",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Analysis of semi-supervised learning with Yarowsky algorithm",
      "author" : [ "G. Haffari", "A. Sarkar" ],
      "venue" : "In UAI",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2007
    }, {
      "title" : "Optimal reverse prediction: a unified perspective on supervised, unsupervised and semi-supervised learning",
      "author" : [ "L. Xu", "M. White", "D. Schuurmans" ],
      "venue" : "In ICML",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2005
    }, {
      "title" : "Smooth and strong: Map inference with linear convergence",
      "author" : [ "O. Meshi", "M. Mahdavi", "A.G. Schwing" ],
      "venue" : "In NIPS",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Semi-supervised classification with hybrid generative/discriminative methods",
      "author" : [ "G. Druck", "C. Pal", "X. Zhu", "A. McCallum" ],
      "venue" : "In KDD",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2007
    }, {
      "title" : "The convex geometry of linear inverse problems",
      "author" : [ "V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S.Willsky" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2012
    }, {
      "title" : "On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues",
      "author" : [ "G. Pataki" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1998
    }, {
      "title" : "Transliteration as constrained optimization",
      "author" : [ "D. Goldwasser", "D. Roth" ],
      "venue" : "In EMNLP",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2008
    }, {
      "title" : "Revisiting Frank-Wolfe: Projection-free sparse convex optimization",
      "author" : [ "M. Jaggi" ],
      "venue" : "In ICML",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Over the past decade deep learning has achieved significant advances in many application areas [1].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 4,
      "context" : "Improved performance has been achieved in applications such as image recognition and segmentation [5] and natural language parsing [6], amongst others.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "Improved performance has been achieved in applications such as image recognition and segmentation [5] and natural language parsing [6], amongst others.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "[7] proposed a conditional random field auto-encoder (CRF-AE)—a two-layer conditional model—where given the observed data x, the latent structure y is first generated based on p(y|x), and then applied to reconstruct the observations using p(x|y).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[12, 13] addressed transliteration between two languages, where Z is the observed binary label indicating if two words match, and higher accuracy can be achieved if we faithfully recover a letter-wise matching represented by the unobserved structure Y .",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "[12, 13] addressed transliteration between two languages, where Z is the observed binary label indicating if two words match, and higher accuracy can be achieved if we faithfully recover a letter-wise matching represented by the unobserved structure Y .",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 13,
      "context" : "A soft variant adopts the mean of y [14].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "multi-wing harmoniums/RBMs), unless the latter is trained in a discriminative fashion [15].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "Contrastive divergence, instead, samples from p(y|x) [11].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "For some structures like graph matching, neither of them is tractable [16, 17] (unless assuming first-order Markovian).",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "For some structures like graph matching, neither of them is tractable [16, 17] (unless assuming first-order Markovian).",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "In single-layer models, this challenge has been resolved by max-margin estimation, which relies only on the MAP of p(y|x) [18].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "[19] formulated convex conditional models using layer-wise kernels, connected through nonlinear losses.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] used boosting but the underlying oracle is generally intractable.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "Specific global methods were also proposed for polynomial networks [21] and sum-product networks [22].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "Specific global methods were also proposed for polynomial networks [21] and sum-product networks [22].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "Unlike [19], it results in a truly inductive model, and its flexibility is demonstrated with two example structures in the framework of total unimodularity (§4).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 18,
      "context" : "Instead of assuming no interdependency between Yi as in [19], our major goal here is to model the structure in the latent layer Y .",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "Such a parameterization is justified by the equivalence between regular Bregman divergence and regular exponential family [23].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "For example, in graph matching MAP inference is tractable while marginalization is NP-hard [16] and sampling requires MCMC [24].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "For example, in graph matching MAP inference is tractable while marginalization is NP-hard [16] and sampling requires MCMC [24].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 24,
      "context" : "The “bootstrapping\" style estimation [25] optimizes the joint likelihood with the latent y imputed in an optimistic fashion",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 26,
      "context" : "1 Adding strongly convex terms to the primal and dual objectives is a commonly used technique for accelerated optimization [27], and has been used in graphical model inference [e.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 29,
      "context" : "Recall that in sparse coding, the atomic norm framework [30] induces low-complexity solutions by setting up the optimization over the convex hull of atoms, or penalize via its gauge function.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 30,
      "context" : "This result extends [31] by accommodating inequalities in (20), and its proof is given in Appendix A.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "One task in natural language is transliteration [12, 32].",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 31,
      "context" : "One task in natural language is transliteration [12, 32].",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 11,
      "context" : "After finding the optimal Y ∗, [12] uses the maximal objective value of (24) to make the final binary prediction: − sign( ∑ ij Y ∗ iju φij).",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "All these pairs are considered “positive examples\", and for negative examples we followed [12] and randomly sampled t− ∈ {50, 75, 100} pairs from 250(2) − 250 mismatched pairings (which are 20%, 30%, and 40% of 250, resp).",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "We did not use many negative examples because, as per [12], our test performance measure will depend mainly on the highest few discriminative values, which are learned largely from the positive examples.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "For our method, the discriminative score is simply f := r′Φw (using the symbols in (25)), and that for [12] is f := maxY ∈Y u′Φvec(Y ) (vectorization of Y ).",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "1) against the state-of-the-art approach in [12].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "For comparison, we used CRF-AE, which was proposed very recently by [7].",
      "startOffset" : 68,
      "endOffset" : 71
    } ],
    "year" : 2016,
    "abstractText" : "Unsupervised learning of structured predictors has been a long standing pursuit in machine learning. Recently a conditional random field auto-encoder has been proposed in a two-layer setting, allowing latent structured representation to be automatically inferred. Aside from being nonconvex, it also requires the demanding inference of normalization. In this paper, we develop a convex relaxation of two-layer conditional model which captures latent structure and estimates model parameters, jointly and optimally. We further expand its applicability by resorting to a weaker form of inference—maximum a-posteriori. The flexibility of the model is demonstrated on two structures based on total unimodularity—graph matching and linear chain. Experimental results confirm the promise of the method.",
    "creator" : null
  }
}