{
  "name" : "e1696007be4eefb81b1a1d39ce48681b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Scaled Least Squares Estimator for GLMs in Large-Scale Problems",
    "authors" : [ "Murat A. Erdogdu", "Mohsen Bayati" ],
    "emails" : [ "erdogdu@stanford.edu", "bayati@stanford.edu", "ldicker@stat.rutgers.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of efficiently estimating the coefficients of generalized linear models (GLMs) when the number of observations n is much larger than the dimension of the coefficient vector p, (n p 1). GLMs play a crucial role in numerous machine learning and statistics problems, and provide a miscellaneous framework for many regression and classification tasks. Celebrated examples include ordinary least squares, logistic regression, multinomial regression and many applications involving graphical models [MN89, WJ08, KF09].\nThe standard approach to estimating the regression coefficients in a GLM is the maximum likelihood method. Under standard assumptions on the link function, the maximum likelihood estimator (MLE) can be written as the solution to a convex minimization problem [MN89]. Due to the non-linear structure of the MLE problem, the resulting optimization task requires iterative methods. The most commonly used optimization technique for computing the MLE is the Newton-Raphson method, which may be viewed as a reweighted least squares algorithm [MN89]. This method uses a second order approximation to benefit from the curvature of the log-likelihood and achieves locally quadratic convergence. A drawback of this approach is its excessive per-iteration cost of O(np2). To remedy this, Hessian-free Krylov sub-space based methods such as conjugate gradient and minimal residual are used, but the resulting direction is imprecise [HS52, PS75, Mar10]. On the other hand, first order\n⇤Work conducted while at Rutgers University\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\napproximation yields the gradient descent algorithm, which attains a linear convergence rate with O(np) per-iteration cost. Although its convergence rate is slow compared to that of second order methods, its modest per-iteration cost makes it practical for large-scale problems. In the regime n p, another popular optimization technique is the class of Quasi-Newton methods [Bis95, Nes04], which can attain a per-iteration cost of O(np), and the convergence rate is locally super-linear; a well-known member of this class of methods is the BFGS algorithm [Nes04]. There are recent studies that exploit the special structure of GLMs [Erd15], and achieve near-quadratic convergence with a per-iteration cost of O (np), and an additional cost of covariance estimation. In this paper, we take an alternative approach to fitting GLMs, based on an identity that is well-known in some areas of statistics, but appears to have received relatively little attention for its computational implications in large scale problems. Let glm denote the GLM regression coefficients, and let ols denote the corresponding ordinary least squares (OLS) coefficients (this notation will be defined more precisely in Section 2). Then, under certain random predictor (design) models,\nglm / ols. (1) For logistic regression with Gaussian design (which is equivalent to Fisher’s discriminant analysis), (1) was noted by Fisher in the 1930s [Fis36]; a more general formulation for models with Gaussian design is given in [Bri82]. The relationship (1) suggests that if the constant of proportionality is known, then glm can be estimated by computing the OLS estimator, which may be substantially simpler than finding the MLE for the original GLM. Our work in this paper builds on this idea.\nOur contributions can be summarized as follows.\n1. We show that glm is approximately proportional to ols in random design GLMs, regardless of the predictor distribution. That is, we prove\nglm c ⇥ ols 1 . 1\np\n, for some c 2 R.\n2. We design a computationally efficient estimator for glm by first estimating the OLS coefficients, and then estimating the proportionality constant c . We refer to the resulting estimator as the Scaled Least Squares (SLS) estimator and denote it by ˆ sls. After estimating the OLS coefficients, the second step of our algorithm involves finding a root of a real valued function; this can be accomplished using iterative methods with up to a cubic convergence rate and only O(n) per-iteration cost. This is cheaper than the classical batch methods mentioned above by at least a factor of O(p). 3. For random design GLMs with sub-Gaussian predictors, we show that\nˆ\nsls glm 1 . 1 p +\nr\np\nn/max {log(n), p} .\nThis bound characterizes the performance of the proposed estimator in terms of data dimensions, and justifies the use of the algorithm in the regime n p 1.\n4. We study the statistical and computational performance of ˆ sls, and compare it to that of the MLE (using several well-known implementations), on a variety of large-scale datasets.\nThe rest of the paper is organized as follows: Section 1.1 surveys the related work and Section 2 introduces the required background and the notation. In Section 3, we provide the intuition behind the relationship (1), which are based on exact calculations for GLMs with Gaussian design. In Section 4, we propose our algorithm and discuss its computational properties. Section 5 provides a thorough comparison between the proposed algorithm and other existing methods. Theoretical results may be found in Section 6. Finally, we conclude with a brief discussion in Section 7."
    }, {
      "heading" : "1.1 Related work",
      "text" : "As mentioned in Section 1, the relationship (1) is well-known in several forms in statistics. Brillinger [Bri82] derived (1) for models with Gaussian predictors. Li & Duan [LD89] studied model misspecification problems in statistics and derived (1) when the predictor distribution has linear conditional means (this is a slight generalization of Gaussian predictors). More recently, Stein’s lemma [BEM13] and the relationship (1) has been revisited in the context of compressed sensing [PV15, TAH15], where it has been shown that the standard lasso estimator may be very effective when used in models\nwhere the relationship between the expected response and the signal is nonlinear, and the predictors (i.e. the design or sensing matrix) are Gaussian. A common theme for all of this previous work is that it focuses solely on settings where (1) holds exactly and the predictors are Gaussian (or, in [LD89], nearly Gaussian). Two key novelties of the present paper are (i) our focus on the computational benefits following from (1) for large scale problems with n p 1; and (ii) our rigorous analysis of models with non-Gaussian predictors, where (1) is shown to be approximately valid."
    }, {
      "heading" : "2 Preliminaries and notation",
      "text" : "We assume a random design setting, where the observed data consists of n random iid pairs (y1, x1), (y2, x2), . . ., (yn, xn); yi 2 R is the response variable and xi = (xi1, . . . , xip)T 2 Rp is the vector of predictors or covariates. We focus on problems where fitting a GLM is desirable, but we do not need to assume that (yi, xi) are actually drawn from the corresponding statistical model (i.e. we allow for model misspecification).\nThe MLE for GLMs with canonical link is defined by\nˆ\nmle\n= argmax 2Rp 1 n\nn X\ni=1\nyihxi, i (hxi, i). (2)\nwhere h·, ·i denotes the Euclidean inner-product on Rp, and is a sufficiently smooth convex function. The GLM coefficients glm are defined by taking the population average in (2):\nglm\n= argmax 2Rp E [yihxi, i (hxi, i)] . (3)\nWhile we make no assumptions on beyond smoothness, note that if is the cumulant generating function for yi | xi, then we recover the standard GLM with canonical link and regression parameters\nglm [MN89]. Examples of GLMs in this form include logistic regression, with (w) = log{1+ew}; Poisson regression, with (w) = ew; and linear regression (least squares), with (w) = w2/2.\nOur objective is to find a computationally efficient estimator for glm. The alternative estimator for glm proposed in this paper is related to the OLS coefficient vector, which is defined by\nols\n: = E[xixTi ] 1E [xiyi]; the corresponding OLS estimator is ˆ ols := (XTX) 1XT y, where X = (x1, . . . , xn)T is the n⇥ p design matrix and y = (y1, . . . , yn)T 2 Rn. Additionally, throughout the text we let [m]={1, 2, ...,m}, for positive integers m, and we denote the size of a set S by |S|. The m-th derivative of a function g : R ! R is denoted by g(m). For a vector u 2 Rp and a n ⇥ p matrix U, we let kukq and kUkq denote the `q-vector and -operator norms, respectively. If S ✓ [n], let US denote the |S|⇥ p matrix obtained from U by extracting the rows that are indexed by S. For a symmetric matrix M 2 Rp⇥p, max(M) and min(M) denote the maximum and minimum eigenvalues, respectively. ⇢k(M) denotes the condition number of M with respect to k-norm. We denote by Nq the q-variate normal distribution."
    }, {
      "heading" : "3 OLS is equivalent to GLM up to a scalar factor",
      "text" : "To motivate our methodology, we assume in this section that the covariates are multivariate normal, as in [Bri82]. These distributional assumptions will be relaxed in Section 6. Proposition 1. Assume that the covariates are multivariate normal with mean 0 and covariance matrix ⌃ = E ⇥\nxix T i\n⇤ , i.e. xi ⇠ Np(0,⌃). Then glm can be written as\nglm = c ⇥ ols, where c 2 R satisfies the equation 1 = c E ⇥ (2) (hx, olsic ) ⇤ .\nProof of Proposition 1. The optimal point in the optimization problem (3), has to satisfy the following normal equations,\nE [yixi] = E h xi (1) (hxi, i) i . (4)\nNow, denote by (x | ⌃) the multivariate normal density with mean 0 and covariance matrix ⌃. We recall the well-known property of Gaussian density d (x | ⌃)/dx = ⌃ 1x (x | ⌃). Using this\nAlgorithm 1 SLS: Scaled Least Squares Estimator Input: Data (yi, xi)ni=1 Step 1. Compute the least squares estimator: ˆ ols and ŷ = Xˆ ols.\nFor a sub-sampling based OLS estimator, let S ⇢ [n] be a random subset and take ˆ ols = |S|n (X T SXS)\n1XT y. Step 2. Solve the following equation for c 2 R: 1 = cn Pn i=1 (2) (c ŷi).\nUse Newton’s root-finding method: Initialize c = 2/Var (yi); Repeat until convergence:\nc c c 1 n Pn i=1 (2) (c ŷi) 1\n1 n Pn i=1\n(2) (c ŷi) + c (3) (c ŷi)\n.\nOutput: ˆ sls = c⇥ ˆ ols.\nand integration by parts on the right hand side of the above equation, we obtain\nE h xi (1) (hxi, i) i =\nZ\nx (1) (hx, i) (x | ⌃) dx = ⌃ E\nh\n(2) (hxi, i)\ni\n(5)\n(this is basically the Stein’s lemma). Combining this with the identity (4), we conclude the proof.\nProposition 1 and its proof provide the main intuition behind our proposed method. Observe that in our derivation, we only worked with the right hand side of the normal equations (4) which does not depend on the response variable yi. The equivalence holds regardless of the joint distribution of (yi, xi), whereas in [Bri82], yi is assumed to follow a single index model. In Section 6, where we extend the method to non-Gaussian predictors, (5) is generalized via the zero-bias transformations."
    }, {
      "heading" : "3.1 Regularization",
      "text" : ""
    }, {
      "heading" : "A version of Proposition 1 incorporating regularization — an important tool for datasets where p is",
      "text" : "large relative to n or the predictors are highly collinear — is also possible, as outlined briefly in this section. We focus on `2-regularization (ridge regression) in this section; some connections with lasso (`1-regularization) are discussed in Section 6 and Corollary 1.\nFor 0, define the `2-regularized GLM coefficients,\nglm\n= argmax 2Rp E [yihxi, i (hxi, i)] 2 k k22 (6)\nand the corresponding `2-regularized OLS coefficients ols = E ⇥ xix T i ⇤ + I 1 E [xiyi] (so\nglm\n=\nglm 0 and ols = ols0 ). The same argument as above implies that\nglm = c ⇥ ols , where = c . (7) This suggests that the ordinary ridge regression for the linear model can be used to estimate the `\n2-regularized GLM coefficients glm . Further pursuing these ideas for problems where regularization is a critical issue may be an interesting area for future research."
    }, {
      "heading" : "4 SLS: Scaled Least Squares estimator for GLMs",
      "text" : "Motivated by the results in the previous section, we design a computationally efficient algorithm for any GLM task that is as simple as solving the least squares problem; it is described in Algorithm 1. The algorithm has two basic steps. First, we estimate the OLS coefficients, and then in the second step we estimate the proportionality constant via a simple root-finding algorithm.\nThere are numerous fast optimization methods to solve the least squares problem, and even a superficial review of these could go beyond the page limits of this paper. We emphasize that this step (finding the OLS estimator) does not have to be iterative and it is the main computational cost of the proposed algorithm. We suggest using a sub-sampling based estimator for ols, where we only use a subset of the observations to estimate the covariance matrix. Let S ⇢ [n] be a\nrandom sub-sample and denote by XS the sub-matrix formed by the rows of X in S. Then the sub-sampled OLS estimator is given as ˆ ols =\n1 |S|X T SXS 1 1 nX T y. Properties of this estimator\nhave been well-studied [Ver10, DLFU13, EM15]. For sub-Gaussian covariates, it suffices to use a sub-sample size of O (p log(p)) [Ver10]. Hence, this step requires a single time computational cost of O |S|p2 + p3 + np ⇡ O pmax{p2 log(p), n} . For other approaches, we refer reader to [RT08, DLFU13] and the references therein.\nThe second step of Algorithm 1 involves solving a simple root-finding problem. As with the first step of the algorithm, there are numerous methods available for completing this task. Newton’s root-finding method with quadratic convergence or Halley’s method with cubic convergence may be appropriate choices. We highlight that this step costs only O (n) per-iteration and that we can attain up to a cubic rate of convergence. The resulting per-iteration cost is cheaper than other commonly used batch algorithms by at least a factor of O (p) — indeed, the cost of computing the gradient is O (np). For simplicity, we use Newton’s root-finding method initialized at c = 2/Var (yi). Assuming that the GLM is a good approximation to the true conditional distribution, by the law of total variance and basic properties of GLMs, we have\nVar (yi) = E [Var (yi | xi)] + Var (E [yi | xi]) ⇡ c 1 + Var (1) (hxi, i) . (8)\nIt follows that this initialization is reasonable as long as c 1 ⇡ E [Var (yi | xi)] is not much smaller than Var (1) (hxi, i) . Our experiments show that SLS is very robust to initialization.\nIn Figure 1, we compare the performance of our SLS estimator to that of the MLE, when both are used to analyze synthetic data generated from a logistic regression model under general Gaussian design with randomly generated covariance matrix. The left plot shows the computational cost of obtaining both estimators as n increases for fixed p. The right plot shows the accuracy of the estimators. In the regime n p 1 — where the MLE is hard to compute — the MLE and the SLS achieve the same accuracy, yet SLS has significantly smaller computation time. We refer the reader to Section 6 for theoretical results characterizing the finite sample behavior of the SLS."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section contains the results of a variety of numerical studies, which show that the Scaled Least Squares estimator reaches the minimum achievable test error substantially faster than commonly used batch algorithms for finding the MLE. Both logistic and Poisson regression models (two types of GLMs) are utilized in our analyses, which are based on several synthetic and real datasets.\nBelow, we briefly describe the optimization algorithms for the MLE that were used in the experiments.\n1. Newton-Raphson (NR) achieves locally quadratic convergence by scaling the gradient by the inverse of the Hessian evaluated at the current iterate. Computing the Hessian has a per-iteration cost of O np2 , which makes it impractical for large-scale datasets. 2. Newton-Stein (NS) is a recently proposed second-order batch algorithm specifically designed for GLMs [Erd16]. The algorithm uses Stein’s lemma and sub-sampling to efficiently estimate the Hessian with O (np) per-iteration cost, achieving near quadratic rates.\n3. Broyden-Fletcher-Goldfarb-Shanno (BFGS) is the most popular and stable quasi-Newton method [Nes04]. At each iteration, the gradient is scaled by a matrix that is formed by accumulating information from previous iterations and gradient computations. The convergence is locally super-linear with a per-iteration cost of O (np). 4. Limited memory BFGS (LBFGS) is a variant of BFGS, which uses only the recent iterates and gradients to approximate the Hessian, providing significant improvement in terms of memory usage. LBFGS has many variants; we use the formulation given in [Bis95]. 5. Gradient descent (GD) takes a step in the opposite direction of the gradient, evaluated at the current iterate. Its performance strongly depends on the condition number of the design matrix. Under certain assumptions, the convergence is linear with O (np) per-iteration cost. 6. Accelerated gradient descent (AGD) is a modified version of gradient descent with an additional “momentum” term [Nes83]. Its per iteration cost is O (np) and its performance strongly depends on the smoothness of the objective function.\nFor all the algorithms, the step size at each iteration is chosen via the backtracking line search [BV04].\nRecall that the proposed Algorithm 1 is composed of two steps; the first finds an estimate of the OLS coefficients. This up-front computation is not needed for any of the MLE algorithms described above. On the other hand, each of the MLE algorithms requires some initial value for , but no such initialization is needed to find the OLS estimator in Algorithm 1. This raises the question of how the MLE algorithms should be initialized, in order to compare them fairly with the proposed method. We consider two scenarios in our experiments: first, we use the OLS estimator computed for Algorithm 1 to initialize the MLE algorithms; second, we use a random initial value.\nOn each dataset, the main criterion for assessing the performance of the estimators is how rapidly the minimum test error is achieved. The test error is measured as the mean squared error of the estimated mean using the current parameters at each iteration on a test dataset, which is a randomly selected (and set-aside) 10% portion of the entire dataset. As noted previously, the MLE is more accurate for small n (see Figure 1). However, in the regime considered here (n p 1), the MLE and the SLS perform very similarly in terms of their error rates; for instance, on the Higgs dataset, the SLS and MLE have test error rates of 22.40% and 22.38%, respectively. For each dataset, the minimum achievable test error is set to be the maximum of the final test errors, where the maximum is taken over all of the estimation methods. Let ⌃(1) and ⌃(2) be two randomly generated covariance matrices. The datasets we analyzed were: (i) a synthetic dataset generated from a logistic regression model with iid {exponential(1) 1} predictors scaled by ⌃(1); (ii) the Higgs dataset (logistic regression) [BSW14]; (iii) a synthetic dataset generated from a Poisson regression model with iid binary(±1) predictors scaled by ⌃(2); (iv) the Covertype dataset (Poisson regression) [BD99].\nIn all cases, the SLS outperformed the alternative algorithms for finding the MLE by a large margin, in terms of computation. Detailed results may be found in Figure 2 and Table 1. We provide additional experiments with different datasets in the Supplementary Material."
    }, {
      "heading" : "6 Theoretical results",
      "text" : "In this section, we use the zero-bias transformations [GR97] to generalize the equivalence between OLS and GLMs to settings where the covariates are non-Gaussian. Definition 1. Let z be a random variable with mean 0 and variance 2. Then, there exists a random variable z⇤ that satisfies E [zf(z)] = 2E[f (1)(z⇤)], for all differentiable functions f . The distribution of z⇤ is said to be the z-zero-bias distribution.\nThe existence of z⇤ in Definition 1 is a consequence of Riesz representation theorem [GR97]. The normal distribution is the unique distribution whose zero-bias transformation is itself (i.e. the normal distribution is a fixed point of the operation mapping the distribution of z to that of z⇤).\nTo provide some intuition behind the usefulness of the zero-bias transformation, we refer back to the proof of Proposition 1. For simplicity, assume that the covariate vector xi has iid entries with mean 0, and variance 1. Then the zero-bias transformation applied to the j-th normal equation in (4) yields\nE [yixij ] = E h xij (1) xij j + ⌃k 6=jxik k i\n| {z }\nj-th normal equation\n= jE h (2) x ⇤ ij j + ⌃k 6=jxik ik i\n| {z }\nZero-bias transformation\n. (9)\nThe distribution of x⇤ij is the xij-zero-bias distribution and is entirely determined by the distribution of xij ; general properties of x⇤ij can be found, for example, in [CGS10]. If is well spread, it turns out that taken together, with j = 1, . . . , p, the far right-hand side in (9) behaves similar to the right side of (5), with ⌃ = I; that is, the behavior is similar to the Gaussian case, where the proportionality relationship given in Proposition 1 holds. This argument leads to an approximate proportionality relationship for non-Gaussian predictors, which, when carried out rigorously, yields the following. Theorem 1. Suppose that the covariate vector xi has mean 0 and covariance matrix ⌃ and, furthermore, that the random vector ⌃ 1/2xi has independent entries and its sub-Gaussian norm is bounded by . Assume that the function (2) is Lipschitz continuous with constant k. Let k k2 = ⌧ and assume is r-well-spread in the sense that ⌧/ k k1 = r p p for some r 2 (0, 1]. Then, for c = 1/E ⇥ (2) (hxi, glmi) ⇤ , and ⇢ = ⇢1(⌃1/2) denoting the condition number of ⌃1/2, we have\n1 c ⇥ glm ols 1  ⌘ p , where ⌘ = 8k3⇢k⌃1/2k1(⌧/r)2. (10)\nTheorem 1 is proved in the Supplementary Material. It implies that the population parameters ols and glm are approximately equivalent up to a scaling factor, with an error bound of O (1/p). The assumption that glm is well-spread can be relaxed with minor modifications. For example, if we have a sparse coefficient vector, where supp( glm) = {j; glmj 6= 0} is the support set of glm, then Theorem 1 holds with p replaced by the size of the support set."
    }, {
      "heading" : "An interesting consequence of Theorem 1 and the remarks following the theorem is that whenever",
      "text" : "an entry of glm is zero, the corresponding entry of ols has to be small, and conversely. For 0, define the lasso coefficients\nlasso\n= argmin 2Rp\n1\n2\nE ⇥ (yi hxi, i)2 ⇤ + k k1 . (11)\nCorollary 1. For any ⌘/|supp( glm)|, if E [xi] = 0 and E ⇥ xix T i ⇤\n= I, we have supp( lasso\n) ⇢ supp( glm). Further, if and glm also satisfy that 8j 2 supp( glm), | glmj | > c\n+ ⌘/|supp( glm)| , then we have supp( lasso) = supp( glm). So far in this section, we have only discussed properties of the population parameters, such as glm. In the remainder of this section, we turn our attention to results for the estimators that are the main focus of this paper; these results ultimately build on our earlier results, i.e. Theorem 1.\nIn order to precisely describe the performance of ˆ sls, we first need bounds on the OLS estimator. The OLS estimator has been studied extensively in the literature; however, for our purposes, we find it convenient to derive a new bound on its accuracy. While we have not seen this exact bound elsewhere, it is very similar to Theorem 5 of [DLFU13]. Proposition 2. Assume that E [xi] = 0, E ⇥ xix T i ⇤\n= ⌃, and that ⌃ 1/2xi and yi are sub-Gaussian with norms  and , respectively. For min denoting the smallest eigenvalue of ⌃, and |S| > ⌘p,\nˆ\nols ols 2  ⌘ 1/2min\nr\np\n|S| , (12) with probability at least 1 3e p, where ⌘ depends only on and . Proposition 2 is proved in the Supplementary Material. Our main result on the performance of ˆ sls is given next. Theorem 2. Let the assumptions of Theorem 1 and Proposition 2 hold with E[k⌃ 1/2xk2] = µ̃pp. Further assume that the function f(z) = zE ⇥ (2) (hx, olsiz)⇤ satisfies f(c̄) > 1 + ¯ pp for some c̄\nand ¯ such that the derivative of f in the interval [0, c̄] does not change sign, i.e., its absolute value is lower bounded by > 0. Then, for n and |S| sufficiently large, we have\nˆ\nsls glm 1  ⌘1 1 p + ⌘2\nr\np\nmin {n/ log(n), |S|/p} , (13) with probability at least 1 5e p, where the constants ⌘1 and ⌘2 are defined by\n⌘1 =⌘kc̄ 3 ⇢k⌃1/2k1(⌧/r)2 (14)\n⌘2 =⌘c̄ 1/2 min\n⇣\n1 +\n1 1/2 min k olsk1 max {(b+ k/µ̃), kc̄}\n⌘\n, (15)\nand ⌘ > 0 is a constant depending on  and .\nNote that the convergence rate of the upper bound in (13) depends on the sum of the two terms, both of which are functions of the data dimensions n and p. The first term on the right in (13) comes from Theorem 1, which bounds the discrepancy between c ⇥ ols and glm. This term is small when p is large, and it does not depend on the number of observations n.\nThe second term in the upper bound (13) comes from estimating ols and c . This term is increasing in p, which reflects the fact that estimating glm is more challenging when p is large. As expected, this term is decreasing in n and |S|, i.e. larger sample size yields better estimates. When the full OLS solution is used (|S| = n), the second term becomes O(ppmax{log(n), p}/n) = O(p/pn), for p sufficiently large. This suggests that n should be at least of order p2 for good performance."
    }, {
      "heading" : "7 Discussion",
      "text" : "In this paper, we showed that the coefficients of GLMs and OLS are approximately proportional in the general random design setting. Using this relation, we proposed a computationally efficient algorithm for large-scale problems that achieves the same accuracy as the MLE by first estimating the OLS coefficients and then estimating the proportionality constant through iterations that can attain quadratic or cubic convergence rate, with only O (n) per-iteration cost. We briefly mentioned that the proportionality between the coefficients holds even when there is regularization in Section 3.1. Further pursuing this idea may be interesting for large-scale problems where regularization is crucial. Another interesting line of research is to find similar proportionality relations between the parameters in other large-scale optimization problems such as support vector machines. Such relations may reduce the problem complexity significantly."
    } ],
    "references" : [ {
      "title" : "Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables, Comput",
      "author" : [ "J.A. Blackard", "D.J. Dean" ],
      "venue" : "Electron. Agr",
      "citeRegEx" : "Blackard and Dean,? \\Q1999\\E",
      "shortCiteRegEx" : "Blackard and Dean",
      "year" : 1999
    }, {
      "title" : "Estimating lasso risk and noise level",
      "author" : [ "M. Bayati", "M.A. Erdogdu", "A. Montanari" ],
      "venue" : "NIPS 26,",
      "citeRegEx" : "Bayati et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bayati et al\\.",
      "year" : 2013
    }, {
      "title" : "Neural Networks for Pattern Recognition",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Oxford University Press",
      "citeRegEx" : "Bis95",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "A generalized linear model with \"Gaussian\" regressor variables, A Festschrift For Erich L",
      "author" : [ "D. R Brillinger" ],
      "venue" : null,
      "citeRegEx" : "Brillinger,? \\Q1982\\E",
      "shortCiteRegEx" : "Brillinger",
      "year" : 1982
    }, {
      "title" : "Searching for exotic particles in high-energy physics with deep learning",
      "author" : [ "P. Baldi", "P. Sadowski", "D. Whiteson" ],
      "venue" : "Nat. Commun",
      "citeRegEx" : "Baldi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Baldi et al\\.",
      "year" : 2014
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "BV04",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Normal approximation by Stein’s method",
      "author" : [ "L.H.Y. Chen", "L. Goldstein", "Q.-M. Shao" ],
      "venue" : "Springer",
      "citeRegEx" : "CGS10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "New subsampling algorithms for fast least squares regression, NIPS",
      "author" : [ "P. Dhillon", "Y. Lu", "D.P. Foster", "L. Ungar" ],
      "venue" : null,
      "citeRegEx" : "Dhillon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dhillon et al\\.",
      "year" : 2013
    }, {
      "title" : "Convergence rates of sub-sampled newton methods",
      "author" : [ "M.A. Erdogdu", "A. Montanari" ],
      "venue" : "NIPS 28,",
      "citeRegEx" : "Erdogdu and Montanari,? \\Q2015\\E",
      "shortCiteRegEx" : "Erdogdu and Montanari",
      "year" : 2015
    }, {
      "title" : "Newton-Stein method: A second order method for GLMs via Stein’s",
      "author" : [ "M.A. Erdogdu" ],
      "venue" : "lemma, NIPS",
      "citeRegEx" : "Erdogdu,? \\Q2015\\E",
      "shortCiteRegEx" : "Erdogdu",
      "year" : 2015
    }, {
      "title" : "The use of multiple measurements in taxonomic problems, Ann",
      "author" : [ "R.A. Fisher" ],
      "venue" : "Eugenic",
      "citeRegEx" : "Fisher,? \\Q1936\\E",
      "shortCiteRegEx" : "Fisher",
      "year" : 1936
    }, {
      "title" : "l1 bounds in normal approximation",
      "author" : [ "L. Goldstein" ],
      "venue" : "Ann. Probab",
      "citeRegEx" : "Goldstein,? \\Q2007\\E",
      "shortCiteRegEx" : "Goldstein",
      "year" : 2007
    }, {
      "title" : "Stein’s method and the zero bias transformation with application to simple random sampling",
      "author" : [ "L. Goldstein", "G. Reinert" ],
      "venue" : "Ann. Appl. Probab",
      "citeRegEx" : "Goldstein and Reinert,? \\Q1997\\E",
      "shortCiteRegEx" : "Goldstein and Reinert",
      "year" : 1997
    }, {
      "title" : "Methods of conjugate gradients for solving linear systems",
      "author" : [ "M.R. Hestenes", "E. Stiefel" ],
      "venue" : "J. Res. Nat. Bur. Stand",
      "citeRegEx" : "Hestenes and Stiefel,? \\Q1952\\E",
      "shortCiteRegEx" : "Hestenes and Stiefel",
      "year" : 1952
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "MIT press",
      "citeRegEx" : "KF09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Regression analysis under link violation",
      "author" : [ "K.-C. Li", "N. Duan" ],
      "venue" : "Ann. Stat",
      "citeRegEx" : "Li and Duan,? \\Q1989\\E",
      "shortCiteRegEx" : "Li and Duan",
      "year" : 1989
    }, {
      "title" : "Deep learning via Hessian-free optimization, ICML",
      "author" : [ "J. Martens" ],
      "venue" : null,
      "citeRegEx" : "Martens,? \\Q2010\\E",
      "shortCiteRegEx" : "Martens",
      "year" : 2010
    }, {
      "title" : "Generalized Linear Models",
      "author" : [ "P. McCullagh", "J.A. Nelder" ],
      "venue" : "2nd ed., Chapman and Hall",
      "citeRegEx" : "MN89",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/k2)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet Math. Dokl",
      "citeRegEx" : "Nesterov,? \\Q1983\\E",
      "shortCiteRegEx" : "Nesterov",
      "year" : 1983
    }, {
      "title" : "Solution of sparse indefinite systems of linear equations",
      "author" : [ "C.C. Paige", "M.A. Saunders" ],
      "venue" : "SIAM J. Numer. Anal",
      "citeRegEx" : "Paige and Saunders,? \\Q1975\\E",
      "shortCiteRegEx" : "Paige and Saunders",
      "year" : 1975
    }, {
      "title" : "The generalized lasso with non-linear observations, 2015, arXiv preprint arXiv:1502.04071",
      "author" : [ "Y. Plan", "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "Plan and Vershynin,? \\Q2015\\E",
      "shortCiteRegEx" : "Plan and Vershynin",
      "year" : 2015
    }, {
      "title" : "Tygert, A fast randomized algorithm for overdetermined linear least-squares regression, P",
      "author" : [ "M.V. Rokhlin" ],
      "venue" : "Natl. Acad. Sci",
      "citeRegEx" : "Rokhlin,? \\Q2008\\E",
      "shortCiteRegEx" : "Rokhlin",
      "year" : 2008
    }, {
      "title" : "Lasso with non-linear measurements is equivalent to one with linear measurements, NIPS",
      "author" : [ "C. Thrampoulidis", "E. Abbasi", "B. Hassibi" ],
      "venue" : null,
      "citeRegEx" : "Thrampoulidis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Thrampoulidis et al\\.",
      "year" : 2015
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices, 2010, arXiv:1011.3027",
      "author" : [ "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "Vershynin,? \\Q2010\\E",
      "shortCiteRegEx" : "Vershynin",
      "year" : 2010
    }, {
      "title" : "Graphical models, exponential families, and variational inference, Foundations and Trends in Machine Learning",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Wainwright and Jordan,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright and Jordan",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "Under standard assumptions on the link function, the maximum likelihood estimator (MLE) can be written as the solution to a convex minimization problem [MN89].",
      "startOffset" : 152,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "The most commonly used optimization technique for computing the MLE is the Newton-Raphson method, which may be viewed as a reweighted least squares algorithm [MN89].",
      "startOffset" : 158,
      "endOffset" : 164
    }, {
      "referenceID" : 2,
      "context" : "LBFGS has many variants; we use the formulation given in [Bis95].",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "For all the algorithms, the step size at each iteration is chosen via the backtracking line search [BV04].",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "The distribution of xij is the xij-zero-bias distribution and is entirely determined by the distribution of xij ; general properties of xij can be found, for example, in [CGS10].",
      "startOffset" : 170,
      "endOffset" : 177
    } ],
    "year" : 2016,
    "abstractText" : "We study the problem of efficiently estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations n is much larger than the number of predictors p, i.e. n p 1. We show that in GLMs with random (not necessarily Gaussian) design, the GLM coefficients are approximately proportional to the corresponding ordinary least squares (OLS) coefficients. Using this relation, we design an algorithm that achieves the same accuracy as the maximum likelihood estimator (MLE) through iterations that attain up to a cubic convergence rate, and that are cheaper than any batch optimization algorithm by at least a factor of O(p). We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm through extensive numerical studies on large-scale real and synthetic datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.",
    "creator" : null
  }
}