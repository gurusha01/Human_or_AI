{
  "name" : "632cee946db83e7a52ce5e8d6f0fed35.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Large Margin Discriminant Dimensionality Reduction in Prediction Space",
    "authors" : [ "Mohammad Saberian", "Jose Costa Pereira", "Jian Yang" ],
    "emails" : [ "esaberian@netflix.com", "jose.c.pereira@inesctec.pt", "canxu@google.com", "jianyang@yahoo-inc.com", "nvasconcelos@ucsd.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Boosting and support vector machines (SVM) are widely popular techniques for learning classifiers. While both methods are maximizing the margin, there are a number of differences that distinguish them; e.g. while SVM selects a number of examples to assemble the decision boundary, boosting achieves this by combining a set of weak learners. In this work we propose a new duality between boosting and SVM which follows from their multiclass formulations. It shows that both methods seek a linear decision rule by maximizing the margin after transforming input data to an intermediate space. In particular, kernel-SVM (K-SVM) [39] first selects a transformation (induced by the kernel) that maps data points into an intermediate space, and then learns a set of linear decision boundaries that maximize the margin. This is depicted in Figure 1-bottom. In contrast, multiclass boosting (MCBoost) [34] relies on a set of pre-defined codewords in an intermediate space, and then learns a mapping to this space such that it maximizes the margin with respect to the boundaries defined by those codewords. See Figure 1-top. Therefore, both boosting and SVM follow a two-step procedure: (i) mapping data to some intermediate space, and (ii) determine the boundaries that separate the classes. There are, however, two notable differences: 1) while K-SVM aims to learn only the boundaries, MCBoost effort is on learning the mapping and 2) in K-SVM the intermediate space typically has infinite dimensions, while in MCBoost the space has M or M − 1 dimensions, where M is the number of classes.\nSVCL 64\nThe intermediate space (called prediction space) in the exposed duality has some interesting properties. In particular, the final classifier decision is based on the representation of data points in this prediction space where the decision boundaries are linear. An accurate classification by these simple boundaries suggests that the input data points must be very-well separated in this space. Given that in the case of boosting this space has limited dimensions, e.g. M or M − 1, this suggests that we can potentially use the predictor of MCBoost as a discriminant dimensionality reduction operator. However, the dimension of MCBoost is either M or M − 1 which restricts application of this operator as a general dimensionality reduction operator. In\naddition, according to the proposed duality, each of K-SVM or Boosting optimizes only one of the two components, i.e. mapping and decision boundaries. Because of this, extra care needs to be put in manually choosing the right kernel in K-SVM; and in MCBoost, we may not even be able to learn a good mapping if we preset some bad boundaries.\nWe can potentially overcome these limitations by combining boosting and SVM to jointly learn both the mapping and linear classifiers for a prediction space of arbitrary dimension d. We note that this is not a straightforward merge of the two methods as this can lead to a computationally prohibitive method; e.g. imagine having to solve the quadratic optimization of K-SVM before each iteration of boosting. In this paper, we propose a new algorithm, Large-mArgin Discriminant DimEnsionality Reduction (LADDER), to efficiently implement this hybrid approach using a boosting-like method. LADDER is able to learn both the mapping and the decision boundaries in a margin maximizing objective function that is adjustable to any number of dimensions. Experiments show that the resulting embedding can significantly improve tasks such as hashing and image/scene classification.\nRelated works: This paper touches several topics such as dimensionality reduction, classification, embedding and representation learning. Due to space constraints we present only a brief overview and comparison to previous work.\nDimensionality reduction has been studied extensively. Unsupervised techniques, such as principal component analysis (PCA), non-negative matrix factorization (NMF), clustering, or deep autoencoders, are conceptually simple and easy to implement, but may eliminate discriminant dimensions of the data and result in sub-optimal representations for classification. Discriminant methods, such as sequential feature selection techniques [31], neighborhood components analysis [11], large margin nearest neighbors [42] or maximally collapsing metric learning [37] can require extensive computation and/or fail to guarantee large margin discriminant data representations.\nThe idea of jointly optimizing the classifiers and the embedding has been extensively explored in embedding and classification literature, e.g. [7, 41, 45, 43]. These methods, however, typically rely on linear data transformation/classifier, requires more complex semi-definite programming [41] or rely on Error Correcting Output Codes (ECOC) approach [7, 45, 10] which has shown inferior performance compared to direct multiclass boosting methods [34, 27]. In comparison, we note that the proposed method (1) is able to learn a very non-linear transformation through boosting predictor, e.g. boosting deep decision trees; and, (2) relies on direct multiclass boosting that optimizes a margin enforcing loss function. Another example of jointly learning the classifiers and the embedding is multiple kernel learning (MKL) literature, e.g. [12, 36]. In these methods, a new kernel is learned as a linear combination of fixed basis functions. Compared with LADDER, 1) the basis functions are data-driven and not fixed, and 2) our method is also able to combine weak learners and form novel basis functions tailored for the current task. Finally, it is also possible to jointly learn the classifiers and embedding using deep neural networks. This, however, requires large number of training data and can be computationally very intensive. In addition the proposed LADDER method is a meta algorithm that can be used to further improve the deep networks, e.g. by boosting of the deep CNNs."
    }, {
      "heading" : "2 Duality of boosting and SVM",
      "text" : "Consider an M -class classification problem, with training set D = {(xi, zi)}ni=1, where zi ∈ {1 . . .M} is the class of example xi. The goal is to learn a real-valued (multidimensional) function f(x) to predict the class label z of each example x. This is formulated as the predictor f(x) that minimizes the risk defined in terms of the expected loss L(z, f(x)):\nR[f ] = EX,Z{L(z, f(x))} ≈ 1\nn ∑ i L(zi, f(xi)). (1)\nDifferent algorithms vary in their choice of loss functions and numerical optimization procedures. The learned predictor has large margin if the loss L(z, f(x)) encourages large values of the classification margin. For binary classification, f(x) ∈ R, z ∈ {1, 2}, the margin is defined as M(xi, zi) = yif(xi), where yi = y(zi) ∈ {−1, 1} is the codeword of class zi. The classifier is then F (x) = H(sign[f(x)]) whereH(+1) = 1 andH(−1) = 2. The extension to M-ary classification requires M codewords. These are defined in a multidimensional space, i.e. as yk ∈ Rd, k = 1 . . .M where commonly d = M or d = M − 1. The predictor is then f(x) = [f1(x), f2(x) . . . fd(x)] ∈ Rd, and the margin is defined as\nM(xi, zi) = 1\n2\n[ 〈f(xi), yzi〉 −max\nl 6=zi 〈f(xi), yl〉\n] , (2)\nwhere 〈·, ·〉 is the Euclidean dot product. Finally, the classifier is implemented as F (x) = arg max\nk∈{1,...,M} 〈yk, f(x)〉. (3)\nNote that the binary equations are the special cases of (2)-(3) for codewords {−1, 1}. Mutliclass Boosting: MCBoost [34] is a multiclass boosting method that uses a set of unit vectors as codewords – forming a regular simplex in RM−1 –, and the exponential loss\nL(zi, f(xi)) = M∑ j=1,j 6=zi e− 1 2 [〈y zi ,f(xi)〉−〈yj ,f(xi)〉]. (4)\nFor M = 2, this reduces to the loss L(zi, f(xi)) = e−y zif(xi) of AdaBoost [9].\nGiven a set, G, of weak learners g(x) ∈ G : X → RM−1, MCBoost minimizes (1) by gradient descent in function space. In each iteration MCBoost computes the directional derivative of the risk for updating f(x) along the direction of g(x),\nδR[f ; g] = ∂R[f + g]\n∂\n∣∣∣∣ =0 = − 1 2n n∑ i=1 〈g(xi), w(xi)〉, (5)\nwhere w(xi) = ∑M j=1(y\nj − yzi)e− 12 〈yzi−yj ,f(xi)〉 ∈ RM−1. The direction of steepest descent and the optimal step size toward that direction are then\ng∗ = arg min g∈G δR[f ; g] α∗ = arg min α∈R R[f + αg∗]. (6)\nThe predictor is finally updated with f := f + α∗g∗. This method is summarized in Algorithm 1. As previously mentioned, it reduces to AdaBoost [9] for M = 2, in which α∗ has closed form.\nMutliclass Kernel SVM (MC-KSVM) : In the support vector machine (SVM) literature, the margin is defined as\nM(xi,wzi) = 〈Φ(xi),wzi〉 −max l 6=zi 〈Φ(xi),wl〉, (7)\nwhere Φ(x) is a feature transformation, usually defined indirectly through a kernel k(x, x′) = 〈Φ(x),Φ(x′)〉, and wl (l = 1 . . .M ) are a set of discriminative projections. Several algorithms have been proposed for multiclass SVM learning [39, 44, 17, 5]. The classical formulation by Vapnik finds the projections that solve: minw1...wM ∑M l=1 ‖wl‖22 + C ∑ i ξi\ns.t. 〈Φ(xi),wzi〉 − 〈Φ(xi),wl〉 ≥ 1− ξi,∀(xi, zi) ∈ D, l 6= zi, ξi ≥ 0 ∀i.\n(8)\nAlgorithm 1 MCBoost\nInput: Number of classes M , number of iterations Nb, codewords {y1, . . . , yM} ∈ RM−1, and dataset D = {(xi, zi)}ni=1 where zi ∈ {1 . . .M} is label of example xi. Initialization: Set f = 0 ∈ RM−1. for t = 1 to Nb do\nFind the best weak learner g∗(x) and optimal step size α∗ using (6). Update f(x) := f(x) + α∗g∗(x).\nend for Output: F (x) = arg maxk 〈f(x), yk〉\nRewriting the constraints as\nξi ≥ max[0, 1− (〈Φ(xi),wzi〉 −max l 6=zi 〈Φ(xi),wl〉)],\nand using the fact that the objective function is monotonically increasing in ξi, this is identical to solving the problem\nminw1...wM ∑ i b〈Φ(xi),wzi〉 −maxl 6=zi〈Φ(xi),wl〉c+ + λ ∑M l=1 ‖wl‖22, (9)\nwhere bxc+ = max(0, 1 − x) is the hinge loss, and λ = 1/C. Hence, MC-KSVM minimizes the risk R[f ] subject to a regularization constraint on ∑ l ‖wl‖22. The predictor of the multiclass kernel SVM (MC-KSVM) is then defined as\nFMC−KSVM (x) = arg max l=1..M\n〈Φ(x),w∗l 〉. (10)\nDuality: The discussion of the previous sections unveils an interesting duality between multiclass boosting and SVM. Since (7) and (10) are special cases of (2) and (3), respectively, the MC-SVM is a special case of the formulation of Section 2, with predictor f(x) = Φ(x) and codewords yl = wl. This leads to the duality of Figure 1. Both boosting and SVM implement a classifier with a set of linear decision boundaries on a prediction space F . This prediction space is the range space of the predictor f(x). The linear decision boundaries are the planes whose normals are the codewords yl. For both boosting and SVM, the decision boundaries implement a large margin classifier in F . However, the learning procedure is different. For the SVM, examples are first mapped into F by a pre-defined predictor. This is the feature transformation Φ(x) that underlies the SVM kernel. The codewords (linear classifiers) are then learned so as to maximize the margin. On the other hand, for boosting, the codewords are pre-defined and the boosting algorithm learns the predictor f(x) that maximizes the margin. The boosting / SVM duality is summarized in Table 1."
    }, {
      "heading" : "3 Discriminant dimensionality reduction",
      "text" : "In this section, we exploit the multiclass boosting / SVM duality to derive a new family of discriminant dimensionality reduction methods. Many learning problems require dimensionality reduction. This is usually done by mapping the space of features X to some lower dimensional space Z , and then learning a classifier on Z . However, the mapping from X to Z is usually quite difficult to learn. Unsupervised procedures, such as principal component analysis (PCA) or clustering, frequently eliminate discriminant dimensions of the data that are important for classification. On the other hand, supervised procedures tend to lead to complex optimization problems and can be quite difficult to implement. Using the proposed duality we argue that it is possible to use an embedding provided by boosting or SVM. In case of SVM this embedding is usually infinite dimensional which can make it impractical for some applications, e.g. hashing problem [20]. In case of boosting the embedding, f(x), has a finite dimension d. In general, the complexity of learning a predictor f(x) is inversely proportional to this dimension d, and lower dimensional codewords/predictors require more sophisticated predictor learning. For example, convolutional networks such as [22] use the\nend for Output: Codeword set Y\nSVCL 3\ncanonical basis of RM as codeword set, and a predictor composed of M neural network outputs. This is a deep predictor, with multiple layers of feature transformation, using a combination of linear and non-linear operations. Similarly, as discussed in the previous section, MCBoost can be used to learn predictors of dimension M or M − 1, by combining weak learners. A predictor learned by any of these methods can be interpreted as a low-dimensional embedding. Compared to the classic sequential approach of first learning an intermediate low dimensional space Z and then learning a predictor f : Z → F = RM , these methods learn the classifier directly in a low-dimensional prediction space, i.e. F = Z . In the case of boosting, this leverages a classifier that explicitly maximizes the classification margin for the solution of the dimensionality reduction problem.\nThe main limitation of this approach is that current multiclass boosting methods [34, 27] rely on a fixed codeword dimension d, e.g. d = M in [27] or d = M − 1 in [34]. In addition these codewords are pre-defined and are independent of the input data, e.g. vertices of a regular simplex in RM or RM−1 [34]. In summary, the dimensionality of the predictor and codewords are tied to the number of classes. Next, we propose a method that extends current boosting algorithms 1) to use embeddings of arbitrary dimensions and 2) to learn the codewords (linear classifiers) based on the input data.\nIn principle, the formulation of section 2 is applicable to any codeword set and the challenge is to find the optimal codewords for a target dimension d. For this, we propose to leverage the duality between boosting and SVM. First, use boosting to learn the optimal predictor for a given set of codewords, and second use SVM to learn the optimal codewords for the given predictor. This procedure, has two limitations. First, although both are large margin methods, boosting and SVM use different loss functions (exponential vs. hinge). Hence, the procedure is not guaranteed to converge. Second, an algorithm based on multiple iterations of boosting and SVM learning is computationally intensive.\nWe avoid these problems by formulating the codeword learning problem in the boosting framework rather than an SVM formulation. For this, we note that, given a predictor f(x), it is possible to learn a set of codewords Y = {y1 . . . yM} that guarantees large margins, under the exponential loss, by solving {\nminy1...yM R[Y, f ] = 12n ∑n i=1 L(Y, zi, f(xi)) s.t. ‖yk‖ = 1 ∀k (11)\nwhere L(Y, zi, f(xi)) = ∑ j 6=zi e − 12 〈y zi−yj ,f(xi)〉. As is usual in boosting, we propose to solve this optimization by a gradient descent procedure. Each iteration of the proposed codeword boosting algorithm computes the risk derivatives with respect to all codewords and forms the matrix ∂R∂Y =[ ∂R[Y,f ] ∂y1 . . . ∂R[Y,f ] ∂yM ] . The codewords are then updated according to Y = Y − β∗ ∂R∂Y where\nβ∗ = arg min β R\n[ Y − β ∂R\n∂Y , f\n] , (12)\nis found by a line search. Finally, each codeword yl is normalized to satisfy the constraint of (11). This algorithm is summarized in Algorithm 2.\nGiven this, we are ready to introduce an algorithm that jointly optimizes the codeword set Y and predictor f . This is implemented using an alternate minimization procedure that iterates between the following two steps. First, given a codeword set Y , determine the predictor f∗(x) of minimum risk R[Y, f ]. This is implemented with MCBoost (Algorithm 1). Second, given the optimal predictor\nAlgorithm 3 LADDER Input: number of classes M , dataset D = {(xi, zi)}ni=1 where zi ∈ {1 . . .M} is label of example xi, number of predictor and codeword dimension d, number of boosting iteration Nb, number codeword learning iteration Nc and number of interleaving rounds Nr. Initialization: Set f = 0 ∈ Rd and initialize Y . for t = 1 to Nr do\nUse Y and run Nb iterations of MCBoost, Algorithm 1, to update f(x). Use f(x) and run Nc iterations of gradient descent in Algorithm 2 to update Y .\nend for Output: Predictor f(x), codeword set Y and decision rule F (x) = arg maxk 〈f(x), yk〉\nf∗(x), determine the codeword set Y∗ of minimum risk R[Y∗, f∗]. This is implemented with codeword boosting (Algorithm 2). Note that, unlike the combined SVM-Boosting solution, the two steps of this algorithm optimize the common risk of (11). Since this risk encourages predictors of large margin, the algorithm is denoted Large mArgin Discriminant DimEnsionality Reduction (LADDER). The procedure is summarized in Algorithm 3.\nAnalysis: First, note that the sub-problems solved by each step of LADDER, i.e. the minimization of R[Y, f ] given Y or f, are convex. However, the overall optimization of (11) is not convex. Hence, the algorithm will converge to a local optimum, which depends on the initialization conditions. We propose an initialization procedure motivated by the following intuition. If two of the codewords are very close, e.g. yj ≈ yk, then 〈yj , f(x)〉 is very similar to 〈yk, f(x)〉 and small variations of x may change the classification results of (3) from k to j and vice-versa. This suggests that the codewords should be as distant from each other as possible. We thus propose to initialize the MCBoost codewords with the set of unit vectors of maximum pair-wise distance, e.g.\nmax y1...yM min j 6=k ||yj − yk|| ,∀j 6= k (13)\nFor d = M, these codewords can be the canonical basis of RM . We have implemented a barrier method from [18] to obtain maximum pair-wise distance codeword sets for any d < M .\nSecond, Algorithm 2 has interesting intuitions. We start by rewriting the risk derivatives as ∂R[Y,f ]∂yj = 1 2n ∑ i(−1)δijf(xi)Lis (1−δij) ij where Li = L(Y, zi, f(xi)), sij = e 1 2 〈yj,f(xi)〉∑\nk 6=zi e 1 2 〈yk,f(xi)〉\n, and δij = 1\nif zi = j and δij = 0 otherwise. It follows that the update of each codeword along the gradient ascent direction, −∂R[Y,f ]∂yj , is a weighted average of the predictions f(xi). Since δij is an indicator of the examples xi in class j, the term (−1)δij reflects the assignment of examples to the classes. While each xi in class j contributes to the update of yj with a multiple of the prediction f(xi), this contribution is −f(xi) for examples in classes other than j. Hence, each example xi in class j pulls yj towards its current prediction f(xi), while pulling all other codewords in the opposite direction. This is illustrated in Figure 2. The result is an increase of the dot-product 〈yj , f(xi)〉, while the dot-products 〈yk, f(xi)〉 ∀k 6= j decrease. Besides encouraging correct classification, these dot product adjustments maximize the multiclass margin. This effect is modulated by the weight of the contribution of each point. This weight is the factor Lis (1−δij) ij , which has two components. The first, Li, is the loss of the current predictor f(xi) for example xi. This measures how much xi contributes to the current risk and is similar to the example weighting mechanism of AdaBoost. Training examples are weighted, so as to emphasize those poorly classified by the current predictor f(x). The second, s(1−δij)ij , only affects examples xi that do not belong to class j. For these, the weight is multiplied by sij . This computes a softmax-like operation among the codeword projections of f(xi) and is large when the projection along yj is one of the largest, and small otherwise. Hence, among examples xi from classes other than j that have equivalent loss Li, the learning algorithm weights more heavily those most likely to be mistakenly assigned to class j. In result, the emphasis on incorrectly classified examples is modulated by how much class pairs are confused by the current predictor. Examples from classes that are more confusable with class j receive larger weight for the update of the latter."
    }, {
      "heading" : "4 Experiments",
      "text" : "We start with a traffic sign detection problem that allows some insight on the merits of learning codewords from data. This experiment was based on∼ 2K instances from 17 different types of traffic signs in the first set of the Summer traffic sign dataset [25], which was split into training and test set. Examples of traffic signs are shown in the left of figure 3. We also collected about 1, 000 background images, to represent non-traffic sign images, leading to a total of 18 classes. The background class is shown as a black image in figure 3-left and middle. All images were resized to 40× 40 pixels and the integral channel method of [8] was used to extract 810 features per image.\nThe first experiment compared the performance of traditional multiclass boosting to LADDER. The former was implemented by running MCBoost (Algorithm 1) for Nb = 200 iterations, using the optimal solution of (13) as codeword set. LADDER was implemented with Algorithm 3, using Nb = 2, Nc = 4, and Nr = 100. In both cases, codewords were initialized with the solution of (13) and the initial assignment of codewords to classes was random. In each experiment, the learning algorithm was initialized with 5 different random assignments. Figure 3 compares the initial codewords (Left) to those learned by LADDER (Middle) for a 2-D embedding (d = 2). A video showing the evolution of the codewords is available in the supplementary materials. The organization of the learned codewords reflects the semantics of the various classes. Note, for example, how LADDER clusters the codewords associated with speed limit signs, which were initially scattered around the unit circle. On the other hand, all traffic sign codewords are pushed away from that of the background image class. Within the traffic sign class, round signs are positioned in one half-space and signs of other shapes on the other. Regarding discriminant power, a decision rule learned by MCBoost achieved 0.44 ± 0.03 error rate, while LADDER achieved 0.21 ± 0.02. In summary, codeword adaptation produces a significantly more discriminant prediction space.\nThis experiment was repeated for d ∈ [2, 27], with the results of Figure 3-right. For small d, LADDER substantially improves on MCBoost (about half error rate for d ≤ 5). LADDER was also compared to various classical dimensionality reduction techniques that do not operate on the prediction space. These included PCA, LDA, Probabilistic PCA [33], Kernel PCA [35], Locally Preserving Projections (LPP) [16], and Neighborhood Preserving Embedding (NPE) [15]. All implementations were provided by [1]. For each method, the data was mapped to a lower dimension d and classified using MCBoost. LADDER outperformed all methods for all dimensions.\nHashing and retrieval: Image retrieval is a classical problem in Vision [3, 4]. Encoding high dimensional feature vectors into short binary codes to enable large scale retrieval has gained momentum in the last few years [6, 38, 23, 13, 24, 26]. LADDER enables the design of an effective discriminant hash code for retrieval systems. To obtain a d-bit hash, we learn a predictor f(x) ∈ Rd. Each predictor coordinate is then thresholded and mapped to {0, 1}. Retrieval is finally based on the Hamming distance between these hash codes. We compare this hashing method to a number of popular techniques on CIFAR-10 [21], which contains 60K images of ten classes. Evaluation was based on the test settings of [26], using 1, 000 randomly selected images. Learning was based on a random set of 2, 000 images, sampled from the remaining 59K. All images are represented as 512-dimensional GIST feature vectors [28]. The 1, 000 test images were used to query a database containing the remaining 59K images.\nTable 2-Left shows mean average precision (mAP) scores under different code lengths for LSH [6], BRE [23], ITQ [13], MCBoost [34], KSH [26] and LADDER. Several conclusions can be drawn. First, using a multiclass boosting technique with predefined equally spaced codewords of (13), MCBoost, we observe a competitive performance; on par with popular approaches such as ITQ, however slightly worst than KSH. Second, LADDER improves on MCBoost, with mAP gains that range from 6 to 12%. This is due to its ability of LADDER to adjust/learn codewords according to the training data. Finally, LADDER outperformed other popular methods for hash code lengths ≥ 10-bits. These gains are about 5 and 7% as compared to KSH, the second best method. Scene understanding: In this experiment we show that LADDER can provide more efficient dimensionality reduction than regular methods such as PCA. For this we selected the scene understanding pipeline of [30, 14] that is consists of deep CNNs [22, 19], PCA, Fisher Vectors(FV) and SVM. PCA in this setting is necessary as the Fisher Vectors can become extremely high dimensional. We replaced the PCA component by embeddings of MCBoost and LADDER and compared their performance with PCA and other scene classification methods on the MIT Indoor dataset [32]. This is a dataset of 67 indoor scene categories where the standard train/test split contains 80 images for training and 20 images for testing per class. Table 2-Right summarizes performance of different methods. Again even with plain MCBoost predictor we observe a competitive performance; on par with PCA. The performance is then improved by LADDER by learning the embedding and codewords jointly."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this work we present a duality between boosting and SVM. This duality is used to propose a novel discriminant dimensionality reduction method. We show that both boosting and K-SVM maximize the margin, using the combination of a non-linear predictor and linear classification. For K-SVM, the predictor (induced by the kernel) is fixed and the linear classifier is learned. For boosting, the linear classifier is fixed and the predictor is learned. It follows from this duality that 1) the predictor learned by boosting is a discriminant mapping, and 2) by iterating between boosting and SVM it should be possible to design better discriminant mappings. We propose the LADDER algorithm to efficiently implement the two steps and learn an embedding of arbitrary dimension. Experiments show that LADDER learns low-dimensional spaces that are more discriminant."
    } ],
    "references" : [ {
      "title" : "Unsup",
      "author" : [ "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "Feature Learn. for RGB-D Based Object Recognition. In ISER,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the Regularization of Image Semantics by Modal Expansion",
      "author" : [ "J. Costa Pereira", "N. Vasconcelos" ],
      "venue" : "Proc. IEEE CVPR, pages 3093–3099,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Cross-modal domain adaptation for text-based regularization of image semantics in image retrieval systems",
      "author" : [ "J. Costa Pereira", "N. Vasconcelos" ],
      "venue" : "Comput. Vision Image Understand., 124:123–135, July",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "JMLR, MIT Press, 2:265–292,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Locality-sensitive hashing scheme based on p-stable distributions",
      "author" : [ "M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni" ],
      "venue" : "Proc. ACM Symp. on Comp. Geometry, pages 253–262,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multiclass learning by prob",
      "author" : [ "O. Dekel", "Y. Singer" ],
      "venue" : "embeddings. In Adv. NIPS, pages 945–952,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Integral channel features",
      "author" : [ "P. Dollar", "Z. Tu", "P. Perona", "S. Belongie" ],
      "venue" : "Proc. BMVC,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "Journal Comp. and Sys. Science,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Multiclass boosting with hinge loss based on output coding",
      "author" : [ "T. Gao", "D. Koller" ],
      "venue" : "ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Neighbourhood components analysis",
      "author" : [ "J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov" ],
      "venue" : "Adv. NIPS, pages 513–520,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multiple kernel learning algorithms",
      "author" : [ "M. Gonen", "E. Alpaydin" ],
      "venue" : "JMLR, MIT Press, 12:2211–2268, July",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval",
      "author" : [ "Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin" ],
      "venue" : "(99):1–15,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-scale orderless pooling of multi-scale orderless pooling of deep convolutional activation features",
      "author" : [ "Y. Gong", "L. Wang", "R. Guo", "S. LazebniK" ],
      "venue" : "Proc. ECCV,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Neighborhood preserving embedding",
      "author" : [ "X. He", "D. Cai", "S. Yan", "H.-J. Zhang" ],
      "venue" : "Proc. IEEE ICCV,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Locality preserving projections",
      "author" : [ "X. He", "P. Niyogi" ],
      "venue" : "Adv. NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A comparison of methods for multiclass support vector machines",
      "author" : [ "C. Hsu", "C. Lin" ],
      "venue" : "IEEE Trans. Neural Netw., 13(2):415–425,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Numerical Optimization",
      "author" : [ "S.J. Nocedal", "Wright" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1999
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell" ],
      "venue" : "arXiv preprint arXiv:1408.5093,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The art of computer programming",
      "author" : [ "D. Knuth" ],
      "venue" : "Sorting and searching,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1973
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : "Technical report, University of Toronto, Dept. of Computer Science,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Adv. NIPS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning to hash with binary reconstructive embeddings",
      "author" : [ "B. Kulis", "T. Darrell" ],
      "venue" : "Adv. NIPS, volume 22, pages 1042–1050.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Kernelized locality-sensitive hashing",
      "author" : [ "B. Kulis", "K. Grauman" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 34(6):1092–1104,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Correlating Fourier descriptors of local patches for road sign recognition",
      "author" : [ "F. Larsson", "M. Felsberg", "P. Forssen" ],
      "venue" : "IET Computer Vision, 5(4):244–254,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Supervised hashing with kernels",
      "author" : [ "W. Liu", "J. Wang", "R. Ji", "Y.-G. Jiang", "S.-F. Chang" ],
      "venue" : "Proc. IEEE CVPR, pages 2074–2081,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A theory of multiclass boosting",
      "author" : [ "I. Mukherjee", "R.E. Schapire" ],
      "venue" : "Adv. NIPS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Modeling the shape of the scene: A holistic representation of the spatial envelope",
      "author" : [ "A. Oliva", "A. Torralba" ],
      "venue" : "Int. Journal Comput. Vision, 42(3):145–175,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Reconfigurable models for scene recognition",
      "author" : [ "S.N. Parizi", "J.G. Oberlin", "P.F. Felzenszwalb" ],
      "venue" : "Proc. IEEE CVPR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Improving the fisher kernel for large-scale image classification",
      "author" : [ "F. Perronnin", "J. Sánchez", "T. Mensink" ],
      "venue" : "Proc. ECCV,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Floating search methods in feature selection",
      "author" : [ "P. Pudil", "J. Novovičová", "J. Kittler" ],
      "venue" : "Pattern Recogn. Lett., pages 1119–1125,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Recognizing indoor scenes",
      "author" : [ "A. Quattoni", "A. Torralba" ],
      "venue" : "Proc. IEEE CVPR,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "EM Algorithms for PCA and SPCA",
      "author" : [ "S. Roweis" ],
      "venue" : "Adv. NIPS,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Multiclass boosting: Theory and algorithms",
      "author" : [ "M. Saberian", "N. Vasconcelos" ],
      "venue" : "Adv. NIPS,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "B. Schölkopf", "A. Smola", "K.-R. Müller" ],
      "venue" : "Neural Computations, pages 1299–1319,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Large scale multiple kernel learning",
      "author" : [ "S. Sonnenburg", "G. Ratsch", "C. Schafer", "B. Scholkopf" ],
      "venue" : "JMLR, MIT Press, 7:1531–1565, Dec.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Dimensionality reduction of multimodal labeled data by local fisher discriminant analysis",
      "author" : [ "M. Sugiyama" ],
      "venue" : "JMLR, MIT Press, pages 1027–1061,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Small codes and large image databases for recognition",
      "author" : [ "A. Torralba", "R. Fergus", "Y. Weiss" ],
      "venue" : "Proc. IEEE CVPR, pages 1–8,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "John Wiley Sons Inc,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Scene recognition on the semantic manifold",
      "author" : [ "N. Vasconcelos", "N. Rasiwasia" ],
      "venue" : "Proc. ECCV,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Large margin taxonomy embedding for document categorization",
      "author" : [ "K.Q. Weinberger", "O. Chapelle" ],
      "venue" : "Adv. NIPS.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K.Q. Weinberger", "L.K. Saul" ],
      "venue" : "JMLR, MIT Press, pages 207–244,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Large scale image annotation: Learning to rank with joint word-image embeddings",
      "author" : [ "J. Weston", "S. Bengio", "N. Usunier" ],
      "venue" : "Proc. ECML,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Support vector machines for multi-class pattern recognition",
      "author" : [ "J. Weston", "C. Watkins" ],
      "venue" : "Euro. Symp. On Artificial Neural Networks, pages 219–224,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Sparse output coding for large-scale visual recognition",
      "author" : [ "B. Zhao", "E. Xing" ],
      "venue" : "Proc. IEEE CVPR, pages 3350–3357,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 37,
      "context" : "In particular, kernel-SVM (K-SVM) [39] first selects a transformation (induced by the kernel) that maps data points into an intermediate space, and then learns a set of linear decision boundaries that maximize the margin.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 32,
      "context" : "In contrast, multiclass boosting (MCBoost) [34] relies on a set of pre-defined codewords in an intermediate space, and then learns a mapping to this space such that it maximizes the margin with respect to the boundaries defined by those codewords.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 29,
      "context" : "Discriminant methods, such as sequential feature selection techniques [31], neighborhood components analysis [11], large margin nearest neighbors [42] or maximally collapsing metric learning [37] can require extensive computation and/or fail to guarantee large margin discriminant data representations.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Discriminant methods, such as sequential feature selection techniques [31], neighborhood components analysis [11], large margin nearest neighbors [42] or maximally collapsing metric learning [37] can require extensive computation and/or fail to guarantee large margin discriminant data representations.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 40,
      "context" : "Discriminant methods, such as sequential feature selection techniques [31], neighborhood components analysis [11], large margin nearest neighbors [42] or maximally collapsing metric learning [37] can require extensive computation and/or fail to guarantee large margin discriminant data representations.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 35,
      "context" : "Discriminant methods, such as sequential feature selection techniques [31], neighborhood components analysis [11], large margin nearest neighbors [42] or maximally collapsing metric learning [37] can require extensive computation and/or fail to guarantee large margin discriminant data representations.",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 39,
      "context" : "These methods, however, typically rely on linear data transformation/classifier, requires more complex semi-definite programming [41] or rely on Error Correcting Output Codes (ECOC) approach [7, 45, 10] which has shown inferior performance compared to direct multiclass boosting methods [34, 27].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "These methods, however, typically rely on linear data transformation/classifier, requires more complex semi-definite programming [41] or rely on Error Correcting Output Codes (ECOC) approach [7, 45, 10] which has shown inferior performance compared to direct multiclass boosting methods [34, 27].",
      "startOffset" : 191,
      "endOffset" : 202
    }, {
      "referenceID" : 43,
      "context" : "These methods, however, typically rely on linear data transformation/classifier, requires more complex semi-definite programming [41] or rely on Error Correcting Output Codes (ECOC) approach [7, 45, 10] which has shown inferior performance compared to direct multiclass boosting methods [34, 27].",
      "startOffset" : 191,
      "endOffset" : 202
    }, {
      "referenceID" : 8,
      "context" : "These methods, however, typically rely on linear data transformation/classifier, requires more complex semi-definite programming [41] or rely on Error Correcting Output Codes (ECOC) approach [7, 45, 10] which has shown inferior performance compared to direct multiclass boosting methods [34, 27].",
      "startOffset" : 191,
      "endOffset" : 202
    }, {
      "referenceID" : 32,
      "context" : "These methods, however, typically rely on linear data transformation/classifier, requires more complex semi-definite programming [41] or rely on Error Correcting Output Codes (ECOC) approach [7, 45, 10] which has shown inferior performance compared to direct multiclass boosting methods [34, 27].",
      "startOffset" : 287,
      "endOffset" : 295
    }, {
      "referenceID" : 25,
      "context" : "These methods, however, typically rely on linear data transformation/classifier, requires more complex semi-definite programming [41] or rely on Error Correcting Output Codes (ECOC) approach [7, 45, 10] which has shown inferior performance compared to direct multiclass boosting methods [34, 27].",
      "startOffset" : 287,
      "endOffset" : 295
    }, {
      "referenceID" : 32,
      "context" : "Mutliclass Boosting: MCBoost [34] is a multiclass boosting method that uses a set of unit vectors as codewords – forming a regular simplex in RM−1 –, and the exponential loss",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "For M = 2, this reduces to the loss L(zi, f(xi)) = e−y if(xi) of AdaBoost [9].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "As previously mentioned, it reduces to AdaBoost [9] for M = 2, in which α∗ has closed form.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 37,
      "context" : "Several algorithms have been proposed for multiclass SVM learning [39, 44, 17, 5].",
      "startOffset" : 66,
      "endOffset" : 81
    }, {
      "referenceID" : 42,
      "context" : "Several algorithms have been proposed for multiclass SVM learning [39, 44, 17, 5].",
      "startOffset" : 66,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "Several algorithms have been proposed for multiclass SVM learning [39, 44, 17, 5].",
      "startOffset" : 66,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "Several algorithms have been proposed for multiclass SVM learning [39, 44, 17, 5].",
      "startOffset" : 66,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "For example, convolutional networks such as [22] use the",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 32,
      "context" : "The main limitation of this approach is that current multiclass boosting methods [34, 27] rely on a fixed codeword dimension d, e.",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "The main limitation of this approach is that current multiclass boosting methods [34, 27] rely on a fixed codeword dimension d, e.",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 32,
      "context" : "vertices of a regular simplex in R or RM−1 [34].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "We have implemented a barrier method from [18] to obtain maximum pair-wise distance codeword sets for any d < M .",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : "This experiment was based on∼ 2K instances from 17 different types of traffic signs in the first set of the Summer traffic sign dataset [25], which was split into training and test set.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : "All images were resized to 40× 40 pixels and the integral channel method of [8] was used to extract 810 features per image.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "This experiment was repeated for d ∈ [2, 27], with the results of Figure 3-right.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "This experiment was repeated for d ∈ [2, 27], with the results of Figure 3-right.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 31,
      "context" : "These included PCA, LDA, Probabilistic PCA [33], Kernel PCA [35], Locally Preserving Projections (LPP) [16], and Neighborhood Preserving Embedding (NPE) [15].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 33,
      "context" : "These included PCA, LDA, Probabilistic PCA [33], Kernel PCA [35], Locally Preserving Projections (LPP) [16], and Neighborhood Preserving Embedding (NPE) [15].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "These included PCA, LDA, Probabilistic PCA [33], Kernel PCA [35], Locally Preserving Projections (LPP) [16], and Neighborhood Preserving Embedding (NPE) [15].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "These included PCA, LDA, Probabilistic PCA [33], Kernel PCA [35], Locally Preserving Projections (LPP) [16], and Neighborhood Preserving Embedding (NPE) [15].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "Hashing and retrieval: Image retrieval is a classical problem in Vision [3, 4].",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Hashing and retrieval: Image retrieval is a classical problem in Vision [3, 4].",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "Encoding high dimensional feature vectors into short binary codes to enable large scale retrieval has gained momentum in the last few years [6, 38, 23, 13, 24, 26].",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 36,
      "context" : "Encoding high dimensional feature vectors into short binary codes to enable large scale retrieval has gained momentum in the last few years [6, 38, 23, 13, 24, 26].",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : "Encoding high dimensional feature vectors into short binary codes to enable large scale retrieval has gained momentum in the last few years [6, 38, 23, 13, 24, 26].",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "Encoding high dimensional feature vectors into short binary codes to enable large scale retrieval has gained momentum in the last few years [6, 38, 23, 13, 24, 26].",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 22,
      "context" : "Encoding high dimensional feature vectors into short binary codes to enable large scale retrieval has gained momentum in the last few years [6, 38, 23, 13, 24, 26].",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 24,
      "context" : "Encoding high dimensional feature vectors into short binary codes to enable large scale retrieval has gained momentum in the last few years [6, 38, 23, 13, 24, 26].",
      "startOffset" : 140,
      "endOffset" : 163
    }, {
      "referenceID" : 19,
      "context" : "We compare this hashing method to a number of popular techniques on CIFAR-10 [21], which contains 60K images of ten classes.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 24,
      "context" : "Evaluation was based on the test settings of [26], using 1, 000 randomly selected images.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 26,
      "context" : "All images are represented as 512-dimensional GIST feature vectors [28].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "Table 2-Left shows mean average precision (mAP) scores under different code lengths for LSH [6], BRE [23], ITQ [13], MCBoost [34], KSH [26] and LADDER.",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "Table 2-Left shows mean average precision (mAP) scores under different code lengths for LSH [6], BRE [23], ITQ [13], MCBoost [34], KSH [26] and LADDER.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 11,
      "context" : "Table 2-Left shows mean average precision (mAP) scores under different code lengths for LSH [6], BRE [23], ITQ [13], MCBoost [34], KSH [26] and LADDER.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : "Table 2-Left shows mean average precision (mAP) scores under different code lengths for LSH [6], BRE [23], ITQ [13], MCBoost [34], KSH [26] and LADDER.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : "Table 2-Left shows mean average precision (mAP) scores under different code lengths for LSH [6], BRE [23], ITQ [13], MCBoost [34], KSH [26] and LADDER.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 28,
      "context" : "For this we selected the scene understanding pipeline of [30, 14] that is consists of deep CNNs [22, 19], PCA, Fisher Vectors(FV) and SVM.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "For this we selected the scene understanding pipeline of [30, 14] that is consists of deep CNNs [22, 19], PCA, Fisher Vectors(FV) and SVM.",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "For this we selected the scene understanding pipeline of [30, 14] that is consists of deep CNNs [22, 19], PCA, Fisher Vectors(FV) and SVM.",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "For this we selected the scene understanding pipeline of [30, 14] that is consists of deep CNNs [22, 19], PCA, Fisher Vectors(FV) and SVM.",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 30,
      "context" : "We replaced the PCA component by embeddings of MCBoost and LADDER and compared their performance with PCA and other scene classification methods on the MIT Indoor dataset [32].",
      "startOffset" : 171,
      "endOffset" : 175
    } ],
    "year" : 2016,
    "abstractText" : "In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through a combination of weak learners. We argue that the intermediate mapping, i.e. boosting predictor, is preserving the discriminant aspects of the data and that by controlling the dimension of this mapping it is possible to obtain discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.",
    "creator" : null
  }
}