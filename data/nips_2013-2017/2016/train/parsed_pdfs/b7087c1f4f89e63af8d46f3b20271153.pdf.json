{
  "name" : "b7087c1f4f89e63af8d46f3b20271153.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dimensionality Reduction of Massive Sparse Datasets Using Coresets",
    "authors" : [ "Dan Feldman", "Mikhail Volkov", "Daniela Rus" ],
    "emails" : [ "dannyf.post@gmail.com", "mikhail@csail.mit.edu", "rus@csail.mit.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Algorithms for dimensionality reduction usually aim to project an input set of d-dimensional vectors (database records) onto a k ≤ d− 1 dimensional affine subspace that minimizes the sum of squared distances to these vectors, under some constraints. Special cases include the Principle Component Analysis (PCA), Linear regression (k = d− 1), Low-rank approximation (k-SVD), Latent Drichlet Analysis (LDA) and Non-negative matrix factorization (NNMF). Learning algorithms such as kmeans clustering can then be applied on the low-dimensional data to obtain fast approximations with provable guarantees. To our knowledge, unlike SVD, there are no algorithms or coreset constructions with performance guarantees for computing the PCA of sparse n×n matrices in the streaming model, i.e. using memory that is poly-logarithmic in n. Much of the large scale high-dimensional data sets available today (e.g. image streams, text streams, etc.) are sparse. For example, consider the text case of Wikipedia. We can associate a matrix with Wikipedia, where the English words define the columns (approximately 1.4 million) and the individual documents define the rows (approximately 4.4 million documents). This large scale matrix is sparse because most English words do not appear in most documents. The size of this matrix is huge and no existing dimensionality reduction algorithm can compute its eigenvectors. To this point, running the state of the art SVD implementation from GenSim on the Wikipedia document-term matrix crashes the computer very quickly after applying its step of random projection on the first few thousand documents. This is because such dense vectors, each of length 1.4 million, use all of the computer’s RAM capacity.\n1Support for this research has been provided by Hon Hai/Foxconn Technology Group and NSFSaTC-BSF CNC 1526815, and in part by the Singapore MIT Alliance on Research and Technology through the Future of Urban Mobility project and by Toyota Research Institute (TRI). TRI provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. We are grateful for this support.\nSubmitted to 30th Conference on Neural Information Processing Systems (NIPS 2016). Do not distribute.\nIn this paper we present a dimensionality reduction algorithms that can handle very large scale sparse data sets such as Wikipedia and returns provably correct results. A long-open research question has been whether we can have a coreset for PCA that is both small in size and a subset of the original data. In this paper we answer this question affirmatively and provide an efficient construction. We also show that this algorithm provides a practical solution to a long-standing open practical problem: computing the PCA of large matrices such as those associated with Wikipedia."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "Given a matrixA, a coreset C in this paper is defined as a weighted subset of rows ofA such that the sum of squared distances from any given k-dimensional subspace to the rows of A is approximately the same as the sum of squared weighted distances to the rows in C. Formally,\nFor a compact set S ∈ Rd and a vector x in Rd, we denote the Euclidean distance between x and its closest points in S by\ndist2(x, S) := min s∈S ‖x− s‖22\nFor an n×d matrix A whose rows are a1, . . . , an, we define the sum of the squared distances from A to S by\ndist2(A,S) :=\nn∑\ni=1\ndist2(ai, S)\nDefinition 1 ((k, ε)-coreset). Given a n×d matrix A whose rows a1, · · · , an are n points (vectors) in Rd, an error parameter ε ∈ (0, 1], and an integer k ∈ [1, d − 1] = {1, · · · , d− 1} that represents the desired dimensionality reduction, n (k, ε)-coreset for A is a weighted subset C = {wiai | wi > 0 and i ∈ [n]} of the rows of A, where w = (w1, · · · , wn) ∈ [0,∞)n is a non-negative weight vector, such that for every affine k-subspace S in Rd we have\n∣∣dist2(A,S))− dist2(C, S)) ∣∣ ≤ εdist2(A,S)). (1)\nThat is, the sum of squared distances from the n points to S approximates the sum of squared weighted distances ∑n i=1 w 2 i (dist(ai, S))\n2 to S. The approximation is up to a multiplicative factor of 1±ε. By choosing w = (1, · · · , 1) we obtain a trivial (k, 0)-coreset. However, in a more efficient coreset most of the weights will be zero and the corresponding rows in A can be discarded. The cardinality of the coreset is thus the sparsity of w, given by |C| = ‖w‖0 := | {wi 6= 0 | i ∈ [n]} |. If C is small, then the computation is efficient. Because C is a weighted subset of the rows of A, if A is sparse, then C is also sparse. A long-open research question has been whether we can have such a coreset that is both of size independent of the input dimension (n and d) and a subset of the original input rows."
    }, {
      "heading" : "2.1 Related Work",
      "text" : "In [24] it was recently proved that an (k, ε) coreset of size |C| = O(dk3/ε2) exists for every input matrix, and distances to the power of z ≥ 1 where z is constant. The proof is based on a general framework for constructing different kinds of coresets, and is known as sensitivity [10, 17]. This coreset is efficient for tall matrices, since its cardinality is independent of n. However, it is useless for “fat” or square matrices (such as the Wikipedia matrix above), where d is in the order of n, which is the main motivation for our paper. In [5], the Frank-Wolfe algorithm was used to construct different types of coresets than ours, and for different problems. Our approach is based on a solution that we give to an open problem in [5], however we can see how it can be used to compute the coresets in [5] and vice versa. For the special case z = 2 (sum of squared distances), a coreset of size O(k/ε2) was suggested in [7] with a randomized version in [8] for a stream of n points that, unlike the standard approach of using merge-and-reduce trees, returns a coreset of size independent of n with a constant probability. These result minimizes the ‖ ·‖2 error, while our result minimizes the Frobenius norm, which is always higher, and may be higher by a factor of d. After appropriate weighting, we can apply the uniform sampling of size O(k/ε2) to get a coreset with a small Frobenius error [14], as in our paper. However, in this case the probability of success is only constant. Since in the streaming case we compute roughly n coresets (formally, O(n/m) coresets, where m is the size of the coreset) the probability that all these coresets constructions will succeed\nis close to zero (roughly 1/n). Since the probability of failure in [14] reduces linearly with the size of the coreset, getting a constant probability of success in the streaming model for O(n) coresets would require to take coresets of size that is no smaller than the input size.\nThere are many papers, especially in recent years, regarding data compression for computing the SVD of large matrices. None of these works addresses the fundamental problem of computing a sparse approximated PCA for a large matrix (in both rows and columns), such as Wikipedia. The reason is that current results use sketches which do no preserve the sparsity of the data (e.g. because of using random projections). Hence, neither the sketch nor the PCA computed on the sketch is sparse. On the other side, we define coreset as a small weighted subset of rows, which is thus sparse if the input is sparse. Moreover, the low rank approximation of a coreset is sparse, since each of its right singular vectors is a sum of a small set of sparse vectors. While there are coresets constructions as defined in this paper, all of them have cardinality of at least d points, which makes them impractical for large data matrices, where d ≥ n. In what follows we describe these recent results in details.\nThe recent results in [7, 8] suggest coresets that are similar to our definition of coresets (i.e., weighted subsets), and do preserve sparsity. However, as mentioned above they minimize the 2-norm error and not the larger Frobesnius error, and maybe more important, they provide coresets for k-SVD (i.e., k-dimensional subspaces) and not for PCA (k-dimensional affine subspaces that might not intersect the origin). In addition [8] works with constant probability, while our algorithm is deterministic (works with probability 1).\nSoftware. Popular software for computing SVD such as GenSim [21], redsvd [12] or the MATLAB sparse SVD function (svds) use sketches and crash for inputs of a few thousand of documents and a dimensionality reduction (approximation rank) k < 100 on a regular laptop, as expected from the analysis of their algorithms. This is why existing implementations (including Gensim) extract topics from large matrices (e.g. Wikipedia), based on low-rank approximation of only small subset of few thousands of selected words (matrix columns), and not the complete Wikipedia matrix.Even for k = 3, running the implementation of sparse SVD in Hadoop [23] took several days [13]. Next we give a broad overview of the very latest state of the dimensionality reduction methods, such as the Lanczoz algorithm [16] for large matrices, that such systems employ under the hood.\nCoresets. Following a decade of research in [24] it was recently proved that an (ε, k)-coreset for low rank approximation of size |C| = O(dk3/ε2) exists for every input matrix. The proof is based on a general framework for constructing different kinds of coresets, and is known as sensitivity [10, 17]. This coreset is efficient for tall matrices, since its cardinality is independent of n. However, it is useless for “fat” or square matrices (such as the Wikipedia matrix above), where d is in the order of n, which is the main motivation for our paper. In [5], the Frank-Wolfe algorithm was used to construct different types of coresets than ours, and for different problems. Our approach is based on a solution that we give to an open problem in [5].\nSketches. A sketch in the context of matrices is a set of vectors u1, · · · , us in Rd such that the sum of squared distances ∑n i=1(dist(ai, S))\n2 from the input n points to every k-dimensional subspace S in Rd, can be approximated by ∑n i=1(dist(ui, S))\n2 up to a multiplicative factor of 1±ε. Note that even if the input vectors a1, · · · , an are sparse, the sketched vectors u1, · · · , us in general are not sparse, unlike the case of coresets. A sketch of cardinality d can be constructed with no approximation error (ε = 0), by defining u1, · · · , ud to be the d rows of the matrix DV T where UDV T = A is the SVD of A. It was proved in [11] that taking the first O(k/ε) rows of DV T yields such a sketch, i.e. of size independent of n and d.\nThe first sketch for sparse matrices was suggested in [6], but like more recent results, it assumes that the complete matrix fits in memory. Other sketching methods that usually do not support streaming include random projections [2, 1, 9] and randomly combined rows [20, 25, 22, 18].\nThe Lanczoz Algorithm. The Lanczoz method [19] and its variant [15] multiply a large matrix by a vector for a few iterations to get its largest eigenvector v1. Then the computation is done recursively after projecting the matrix on the hyperplane that is orthogonal to v1. However, v1 is in general not sparse even A is sparse. Hence, when we project A on the orthogonal subspace to v1, the resulting matrix is dense for the rest of the computations (k > 1). Indeed, our experimental results show that the MATLAB svds function which uses this method runs faster than the exact SVD, but crashes on large input, even for small k.\nThis paper builds on this extensive body of prior work in dimensionality reduction, and our approach uses coresets to solve the time and space challenges."
    }, {
      "heading" : "2.2 Key Contributions",
      "text" : "Our main result is the first algorithm for computing an (k, ε)-coreset C of size independent of both n and d, for any given n × d input matrix. The algorithm takes as input a finite set of ddimensional vectors, a desired approximation error ε, and an integer k ≥ 0. It returns a weighted subset S (coreset) of k2/ε2 such vectors. This coreset S can be used to approximate the sum of squared distances from the matrix A ∈ Rn×d, whose rows are the n vectors seen so far, to any k-dimensional affine subspace in Rd, up to a factor of 1± ε. For a (possibly unbounded) stream of such input vectors the coreset can be maintained at the cost of an additional factor of log2 n.\nThe polynomial dependency on d of the cardinality of previous coresets made them impractical for fat or square input matrices, such as Wikipedia, images in a sparse feature space representation, or adjacency matrix of a graph. If each row of in input matrix A has O(nnz) non-zeroes entries, then the update time per insertion, the overall memory that is used by our algorithm, and the low rank approximation of the coreset S is O(nnz · k2/ε2), i.e. independent of n and d. We implemented our algorithm to obtain a low-rank approximation for the term-document matrix of Wikipedia with provable error bounds. Since our streaming algorithm is also “embarrassingly parallel” we run it on Amazon Cloud, and receive a significantly better running time and accuracy compared to existing heuristics (e.g. Hadoop/MapReduce) that yield non-sparse solutions.\nThe key contributions in this work are:\n1. A new algorithm for dimensionality reduction of sparse data that uses a weighted subset of the data, and is independent of both the size and dimensionality of the data.\n2. An efficient algorithm for computing such a reduction, with provable bounds on size and running time (cf. http://people.csail.mit.edu/mikhail/NIPS2016).\n3. A system that implements this dimensionality reduction algorithm and an application of the system to compute latent semantic analysis (LSA) of the entire English Wikipedia."
    }, {
      "heading" : "3 Technical Solution",
      "text" : "Given a n×dmatrixA, we propose a construction mechanism for a matrixC of size |C| = O(k2/ε2) and claim that it is a (k, ε)-coreset forA. We use the following corollary for Definition 1 of a coreset, based on simple linear algebra that follows from the geometrical definitions (e.g. see [11]).\nProperty 1 (Coreset for sparse matrix). Let A ∈ Rn×d, k ∈ [1, d − 1] be an integer, and let ε > 0 be an error parameter. For a diagonal matrix W ∈ Rn×n, the matrix C = WA is a (k, ε)-coreset for A if for every matrix X ∈ Rd×(d−k) such that XTX = I , we have\n(i) ∣∣∣∣1− ‖WAX‖ ‖AX‖ ∣∣∣∣ ≤ ε, and (ii) ‖A−WA‖ < ε var(A) (2)\nwhere var(A) is the sum of squared distances from the rows of A to their mean.\nThe goal of this paper is to prove that such a coreset (Definition 1) exists for any matrix A (Property 1) and can be computed efficiently. Formally,\nTheorem 1. For every input matrix A ∈ Rn×d, an error ε ∈ (0, 1] and an integer k ∈ [1, d− 1]:\n(a) there is a (k, ε)-coreset C of size |C| = O(k2/ε2); (b) such a coreset can be constructed in O(k2/ε2) time.\nTheorem 1 is the formal statement for the main technical contribution of this paper. Sections 3–5 constitute a proof for Theorem 1.\nTo establish Theorem 1(a), we first state our two main results (Theorems 2 and 3) axiomatically, and show how they combine such that Property 1 holds. Thereafter we prove the these results in Sections 4 and 5, respectively. To prove Theorem 1(b) (efficient construction) we present an algorithm for\nAlgorithm 1 CORESET-SUMVECS(A, ε) 000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 Algorithm 1 CORESET-SUMVECS(A, ε) 1: Input: A: n input points a1, . . . , an in Rd 2: Input: ε ∈ (0, 1): the approximation error 3: Output: w ∈ [0,∞)n: non-negative weights 4: A← A−mean(A) 5: A← cA where c is a constant s.t. var(A) = 1 6: w ← (1, 0, . . . , 0) 7: j ← 1, p← Aj , J ← {j} 8: Mj = { y2 | y = A ·ATj } 9: for i = 1, . . . , n do 10: j ← argmin {wJ ·MJ} 11: G←W ′ ·AJ where W ′i,i = √ wi 12: ‖c‖ = ‖GTG)‖2F 13: c · p =∑|J|i=1GpT 14: ‖c− p‖ = √ 1 + ‖c‖2 − c · p 15: compp(v) = 1/‖c− p‖ − (c · p) /‖c− p‖ 16: ‖c− c′‖ = ‖c− p‖ − compp(v) 17: α = ‖c− c′‖/‖c− p‖ 18: w ← w(1− |α|) 19: wj ← wj + α 20: w ← w/∑ni=1 wi 21: Mj ← { y2 | y = A ·ATj } 22: J ← J ∪ {j} 23: if ‖c‖2 ≤ ε then 24: break 25: end if 26: end for 27: return w\n1\n(a) Coreset for sum of vectors algorithm\n-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\nc2\na3\nc3\na1 = c1\na2 a4\na5\n(b) Illustration showing first 3 steps of the computation\ncomputing a matrix C, and analyze the running time to show that the C can be constructed in O(k2/ε2) iterations.\nLet A ∈ Rn×d be a matrix of rank d, and let UΣV T = A denote its full SVD. Let W ∈ Rn×n be a diagonal matrix. Let k ∈ [1, d− 1] be an integer. For every i ∈ [n] let\nvi = ( Ui,1, · · · , Ui,k,\nUi,k+1:dΣk+1:d,k+1:d ‖Σk+1:d,k+1:d‖ , 1\n) . (3)\nThen the following two results hold:\nTheorem 2 (Coreset for sum of vectors). For every set of of n vectors v1, · · · , vn in Rd and every ε ∈ (0, 1), a weight vectorw ∈ (0,∞)n of sparsity ‖w‖0 ≤ 1/ε2 can be computed deterministically in O(nd/ε) time such that\n∥∥∥∥∥ n∑\ni=1\nvi − n∑\ni=1\nwivi ∥∥∥∥∥ ≤ ε n∑\ni=1\n‖vi‖2. (4)\nSection 4 establishes a proof for Theorem 2.\nTheorem 3 (Coreset for Low rank approximation). For every X ∈ Rd×(d−k) such that XTX = I , ∣∣∣∣1− ‖WAX‖2 ‖AX‖2 ∣∣∣∣ ≤ 5 ∥∥∥∥∥ n∑\ni=1\nviv T i −Wi,ivivTi ∥∥∥∥∥ . (5)\nSection 5 establishes a proof for Theorem 3."
    }, {
      "heading" : "3.1 Proof of Theorem 1",
      "text" : "Proof of Theorem 1(a). Replacing vi with vivTi , ‖vi‖2 with ‖vivTi ‖, and ε by ε/(5k) in Theorem 2 yields ∥∥∥∥∥ ∑\ni\nviv T i −Wi,ivivTi ∥∥∥∥∥ ≤ (ε/5k) n∑\ni=1\n‖vivTi ‖ = ε.\nCombining this inequality with (4) gives ∣∣∣∣1− ‖WAX‖2 ‖AX‖2 ∣∣∣∣ ≤ 5 ∥∥∥∥∥ n∑\ni=1\nviv T i −Wi,ivivTi ∥∥∥∥∥ ≤ ε.\nThus the left-most term is bounded by the right-most term, which proves (2). This also means that C = WA is a coreset for k-SVD, i.e., (non-affine) k-dimensional subspaces. To support PCA (affine subspaces) the coreset C = WA needs to satisfy the expression in the last line of Property 1 regarding its mean. This holds using the last entry (one) in the definition of vi (3), which implies that the sum of the rows is preserved as in equation (4). Therefore Property 1 holds for C = WA, which proves Theorem 1(a).\nClaim Theorem 1(b) follows from simple analysis of Algorithm 2 that implements this construction."
    }, {
      "heading" : "4 Coreset for Sum of Vectors (k = 0)",
      "text" : "In order to prove the general result Theorem 1(a), that is the existence of a (k, ε)-coreset for any k ∈ [1, d−1], we first establish the special case for k = 0. In this section, we prove Theorem 2 by providing an algorithm for constructing a small weighted subset of points that constitutes a general approximation for the sum of vectors.\nTo this end, we first introduce an intermediate result that shows that given n points on the unit ball with weight distribution z, there exists a small subset of points whose weighted mean is approximately the same as the weighted mean of the original points. Let Dn denote the union over every vector z ∈ [0, 1]n that represent a distribution, i.e., ∑i zi = 1. Our first technical result is that for any finite set of unit vectors a1, . . . , an in Rd, any distribution z ∈ Dn, and every ε ∈ (0, 1], we can compute a sparse weight vector w ∈ Dn of sparsity (nonzeroes entries) ‖w‖0 ≤ 1/ε2. Lemma 1. Let z ∈ Dn be a distribution over n unit vectors a1, · · · , an in Rd. For ε ∈ (0, 1), a sparse weight vector w ∈ Dn of sparsity s ≤ 1/ε2 can be computed in O(nd/ε2) time such that\n∥∥∥∥∥ n∑\ni=1\nzi · ai − n∑\ni=2\nwi ai ∥∥∥∥∥ 2 ≤ ε. (6)\nProof of Lemma 1. Please see Supplementary Material, Section A.\nWe prove Theorem 2 by providing a computation of such a sparse weight vector w. The intuition for this computation is as follows. Given n input points a1,. . . ,an in Rd, with weighted mean∑ i zi ai = 0, we project all the points on the unit sphere. Pick an arbitrary starting point a1 = c1. At each step find the farthest point aj+1 from cj , and compute cj+1 by projecting the origin onto the line segment [cj , aj+1]. Repeat this for j= 1,. . . ,N iterations, where N = 1/ε2. We prove that ‖ci‖2 = 1/i, thus if we iterate 1/ 2 times, this norm will be ‖c1/ 2‖ = 2. The resulting points ci are a weighted linear combination of a small subset of the input points. The output weight vector w ∈ Dn satisfies cN = ∑n i=1 wi ai, and this weighted subset forms the coreset.\nFig. 1a contains the pseudocode for Algorithm 1. Fig. 1b illustrates the first steps of the main computation (lines 9–26). Please see Supplementary Material, Section C for a complete line-by-line analysis of Algorithm 1.\nProof of Theorem 2. The proof of Theorem 2 follows by applying Lemma 1 after normalization of the input points and then post-processing the output."
    }, {
      "heading" : "5 Coreset for Low Rank Approximation (k > 0)",
      "text" : "In Section 4 we presented a new coreset construction for approximating the sum of vectors, showing that given n points on the unit ball there exists a small weighted subset of points that is a coreset for those points. In this section we describe the reduction of Algorithm 1 for k = 0 to an efficient algorithm for any low rank approximation with k ∈ [1, d−1].\nAlgorithm 2 CORESET-LOWRANK(A, k, ε) 000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 Algorithm 1 CORESET-LOWRANK(A, k, ε) 1: Input: A: A sparse n×d matrix 2: Input: k ∈ Z>0: the approximation rank 3: Input: ε ∈ ( 0, 12 ) : the approximation error 4: Output: w ∈ [0,∞)n: non-negative weights 5: Compute UΣV T = A, the SVD of A 6: R← Σk+1:d,k+1:d 7: P ← matrix whose i-th row ∀i ∈ [n] is 8: Pi = (Ui,1:k, Ui,k+1:d · R‖R‖F ) 9: X ← matrix whose i-th row ∀i ∈ [n] is 10: Xi = Pi/‖Pi‖F 11: w ← (1, 0, . . . , 0) 12: for i = 1, . . . , ⌈ k2/ε2 ⌉ do 13: j ← argmini=1,...,n{wXXi} 14: a = ∑n i=1 wi(X T i Xj) 2 15: b = 1− ‖PXj‖2F + ∑n i=1 wi‖PXi‖2F ‖P‖2F 16: c = ‖wX‖2F 17: α = (1− a+ b) / (1 + c− 2a) 18: w ← (1− α)Ij + αw 19: end for 20: return w\n1\n(a) 1/2: Initialization\n000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053\nAlgorithm 1 CORESET-LOWRANK(A, k, ε)\n1: Input: A: A sparse n×d matrix 2: Input: k ∈ Z>0: the approximation rank 3: Input: ε ∈ ( 0, 12 ) : the approximation error 4: Output: w ∈ [0,∞)n: non-negative weights 5: Compute UΣV T = A, the SVD of A 6: R← Σk+1:d,k+1:d 7: P ← matrix whose i-th row ∀i ∈ [n] is 8: Pi = (Ui,1:k, Ui,k+1:d · R‖R‖F ) 9: X ← matrix whose i-th row ∀i ∈ [n] is\n10: Xi = Pi/‖Pi‖F 11: w ← (1, 0, . . . , 0) 12: for i = 1, . . . , ⌈ k2/ε2 ⌉ do 13: j ← argmini=1,...,n{wXXi} 14: a = ∑n i=1 wi(X T i Xj) 2 15: b = 1− ‖PXj‖2F + ∑n i=1 wi‖PXi‖2F ‖P‖2F 16: c = ‖wX‖2F 17: α = (1− a+ b) / (1 + c− 2a) 18: w ← (1− α)Ij + αw 19: end for 20: return w\n1\n(b) 2/2: Computation\nConceptually, we achieve this reduction in two steps. The first step is to show that Algorithm 1 can be reduced to an inefficient computation for low rank approximation for matrices. To this end, we first prove Theorem 3, thus completing the existence clause Theorem 1(a).\nProof of Theorem 3. Let ε = ‖∑ni=1(1 −W 2i,i)vivTi ‖. For every i ∈ [n] let ti = 1 −W 2i,i. Set X ∈ Rd×(d−k) such that XTX = I . Without loss of generality we assume V T = I , i.e. A = UΣ, otherwise we replace X by V TX . It thus suffices to prove that ∣∣∑ i ti‖Ai,:X‖2\n∣∣ ≤ 5ε ‖AX‖2. Using the triangle inequality, we get\n∣∣∣∣∣ ∑\ni\nti‖Ai,:X‖2 ∣∣∣∣∣ ≤ ∣∣∣∣∣ ∑\ni\nti‖Ai,:X‖2 − ∑\ni\nti‖(Ai,1:k,0)X‖2 ∣∣∣∣∣ (7)\n+ ∣∣∣∣∣ ∑\ni\nti‖(Ai,1:k,0)X‖2 ∣∣∣∣∣ . (8)\nWe complete the proof by deriving bounds on (7) and (8), thus proving (5). For the complete proof, please see Supplementary Material, Section B.\nTogether, Theorems 2 and 3 show that the error of the coreset is a 1 ± ε approximation to the true weighted mean. By Theorem 3, we can now simply apply Algorithm 1 to the right hand side of (5) to compute the reduction. The intuition for this inefficient reduction is as follows. We first compute the outer product of each row vector x in the input matrix A ∈ R[n×d]. Each such outer products xTx is a matrix in Rd×d. Next, we expand every such matrix into a vector, in Rd2 by concatenating its entries. Finally, we combine each such vector back to be a vector in the matrix P ∈ Rn×d2 . At this point the reduction is complete, however it is clear that this matrix expansion is inefficient.\nThe second step of the reduction is to transform the slow computation of running Algorithm 1 on the expanded matrix P ∈ Rn×d2 into an equivalent and provably fast computation on the original set of points A ∈ Rd. To this end we make use of the fact that each row of P is a sparse vector in Rd to implicitly run the computation in the original row space Rd. We present Algorithm 2 and prove that it returns the weight vector w=(w1, · · · , wn) of a (k, ε)-coreset for low-rank approximation of the input point set P , and that this coreset is small, namely, only O(k2/ε2) of the weights (entries) in w are non-zeros. Fig. 5 contains the pseudocode for Algorithm 2. Please see Supplementary Material, Section D for a complete line-by-line analysis of Algorithm 2."
    }, {
      "heading" : "6 Evaluation and Experimental Results",
      "text" : "The coreset construction algorithm described in Section 5 was implemented in MATLAB. We make use of the redsvd package [12] to improve performance, but it is not required to run the system. We evaluate our system on two types of data: synthetic data generated with carefully controlled parameters, and real data from the English Wikipedia under the “bag of words” (BOW) model. Synthetic data provides ground-truth to evaluate the quality, efficiency, and scalability of our system, while the Wikipedia data provides us with a grand challenge for latent semantic analysis computation.\nFor our synthetic data experiments, we used a moderate size sparse input of (5000×1000) to evaluate the relationship between the error ε and the number of iterations of the algorithm N . We then compare our coreset against uniform sampling and weighted random sampling using the squared norms of U (A = UΣV T ) as the weights. Finally, we evaluate the efficiency of our algorithm by comparing the running time against the MATLAB svds function and against the most recent state of the art dimensionality reduction algorithm [8]. Figure 1a–1d show the exerimental results. Please see Supplementary Material, Section E for a complete description of the experiments."
    }, {
      "heading" : "6.1 Latent Semantic Analysis of Wikipedia",
      "text" : "For our large-scale grand challenge experiment, we apply our algorithm for computing Latent Semantic Analysis (LSA) on the entire English Wikipedia. The size of the data is n= 3.69M (documents) with a dimensionality d=7.96M (words). We specify a nominal error of ε=0.5, which is a theoretical upper bound for N = 2k/ε iterations, and show that the coreset error remains bounded. Figure 1f shows the log approximation error, i.e. sum of squared distances of the coreset to the subspace for increasing approximation rank k=1, 10, 100. We see that the log error is proportional to k, and as the number of streamed points increases into the millions, coreset error remains bounded by k. Figure 1e shows the running time of our algorithm compared against svds for increasing dimensionality d and a fixed input size n=3.69M (number of documents).\nFinally, we show that our coreset can be used to create a topic model of 100 topics for the entire English Wikipedia. We construct the coreset of size N = 1000 words. Then to generate the topics, we compute a projection of the coreset onto a subspace of rank k= 100. Please see Supplementary Material, Section F for more details, including an example of the topics obtained in our experiments."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We present a new approach for dimensionality reduction using coresets. Our solution is general and can be used to project spaces of dimension d to subspaces of dimension k < d. The key feature of our algorithm is that it computes coresets that are small in size and subsets of the original data. We benchmark our algorithm for quality, efficiency, and scalability using synthetic data. We then apply our algorithm for computing LSA on the entire Wikipedia – a computation task hitherto not possible with state of the art algorithms. We see this work as a theoretical foundation and practical toolbox for a range of dimensionality reduction problems, and we believe that our algorithms will be used to\nconstruct many other coresets in the future. Our project codebase is open-sourced and can be found here: http://people.csail.mit.edu/mikhail/NIPS2016."
    } ],
    "references" : [ {
      "title" : "Fast computation of low-rank matrix approximations",
      "author" : [ "D. Achlioptas", "F. Mcsherry" ],
      "venue" : "Journal of the ACM (JACM), 54(2):9",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A fast random sampling algorithm for sparsifying matrices",
      "author" : [ "S. Arora", "E. Hazan", "S. Kale" ],
      "venue" : "Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 272–279. Springer",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Twice-ramanujan sparsifiers",
      "author" : [ "J. Batson", "D.A. Spielman", "N. Srivastava" ],
      "venue" : "SIAM Journal on Computing, 41(6):1704–1721",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Über den variabilitätsbereich der fourierschen konstanten von positiven harmonischen funktionen",
      "author" : [ "C. Carathéodory" ],
      "venue" : "Rendiconti del Circolo Matematico di Palermo ",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1940
    }, {
      "title" : "Coresets",
      "author" : [ "K.L. Clarkson" ],
      "venue" : "sparse greedy approximation, and the frank-wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):63",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Low rank approximation and regression in input sparsity time",
      "author" : [ "K.L. Clarkson", "D.P. Woodruff" ],
      "venue" : "Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 81–90. ACM",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Dimensionality reduction for k-means clustering and low rank approximation",
      "author" : [ "M.B. Cohen", "S. Elder", "C. Musco", "C. Musco", "M. Persu" ],
      "venue" : "Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, pages 163–172. ACM",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Online row sampling",
      "author" : [ "M.B. Cohen", "C. Musco", "J.W. Pachocki" ],
      "venue" : "CoRR, abs/1604.05448",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A note on element-wise matrix sparsification via a matrix-valued bernstein inequality",
      "author" : [ "P. Drineas", "A. Zouzias" ],
      "venue" : "Information Processing Letters, 111(8):385–389",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A unified framework for approximating and clustering data",
      "author" : [ "D. Feldman", "M. Langberg" ],
      "venue" : "Proc. 41th Ann. ACM Symp. on Theory of Computing (STOC)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Turning big data into tiny data: Constant-size coresets for kmeans",
      "author" : [ "D. Feldman", "M. Schmidt", "C. Sohler" ],
      "venue" : "pca and projective clustering. Proceedings of ACM-SIAM Symposium on Discrete Algorithms (SODA)",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Randomized methods for computing low-rank approximations of matrices",
      "author" : [ "N.P. Halko" ],
      "venue" : "PhD thesis, University of Colorado",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Applications of weighted voronoi diagrams and randomization to variance-based k-clustering",
      "author" : [ "M. Inaba", "N. Katoh", "H. Imai" ],
      "venue" : "Proceedings of the tenth annual symposium on Computational geometry, pages 332–339. ACM",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Generalized power method for sparse principal component analysis",
      "author" : [ "M. Journée", "Y. Nesterov", "P. Richtárik", "R. Sepulchre" ],
      "venue" : "The Journal of Machine Learning Research, 11:517–553",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An iteration method for the solution of the eigenvalue problem of linear differential and integral operators",
      "author" : [ "C. Lanczos" ],
      "venue" : "United States Governm. Press Office Los Angeles, CA",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1950
    }, {
      "title" : "Universal ε approximators for integrals",
      "author" : [ "M. Langberg", "L.J. Schulman" ],
      "venue" : "Proceedings of ACM-SIAM Symposium on Discrete Algorithms (SODA)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Randomized algorithms for the low-rank approximation of matrices",
      "author" : [ "E. Liberty", "F. Woolfe", "P.-G. Martinsson", "V. Rokhlin", "M. Tygert" ],
      "venue" : "Proceedings of the National Academy of Sciences, 104(51):20167– 20172",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Computational variants of the lanczos method for the eigenproblem",
      "author" : [ "C.C. Paige" ],
      "venue" : "IMA Journal of Applied Mathematics, 10(3):373–381",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Latent semantic indexing: A probabilistic analysis",
      "author" : [ "C.H. Papadimitriou", "H. Tamaki", "P. Raghavan", "S. Vempala" ],
      "venue" : "Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems, pages 159–168. ACM",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Gensimstatistical semantics in python",
      "author" : [ "R. Ruvrek", "P. Sojka" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "T. Sarlos" ],
      "venue" : "Foundations of Computer Science, 2006. FOCS’06. 47th Annual IEEE Symposium on, pages 143–152. IEEE",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "The proof is based on a general framework for constructing different kinds of coresets, and is known as sensitivity [10, 17].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "The proof is based on a general framework for constructing different kinds of coresets, and is known as sensitivity [10, 17].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "In [5], the Frank-Wolfe algorithm was used to construct different types of coresets than ours, and for different problems.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "Our approach is based on a solution that we give to an open problem in [5], however we can see how it can be used to compute the coresets in [5] and vice versa.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Our approach is based on a solution that we give to an open problem in [5], however we can see how it can be used to compute the coresets in [5] and vice versa.",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "For the special case z = 2 (sum of squared distances), a coreset of size O(k/ε(2)) was suggested in [7] with a randomized version in [8] for a stream of n points that, unlike the standard approach of using merge-and-reduce trees, returns a coreset of size independent of n with a constant probability.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "For the special case z = 2 (sum of squared distances), a coreset of size O(k/ε(2)) was suggested in [7] with a randomized version in [8] for a stream of n points that, unlike the standard approach of using merge-and-reduce trees, returns a coreset of size independent of n with a constant probability.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "After appropriate weighting, we can apply the uniform sampling of size O(k/ε(2)) to get a coreset with a small Frobenius error [14], as in our paper.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "Since the probability of failure in [14] reduces linearly with the size of the coreset, getting a constant probability of success in the streaming model for O(n) coresets would require to take coresets of size that is no smaller than the input size.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "The recent results in [7, 8] suggest coresets that are similar to our definition of coresets (i.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "The recent results in [7, 8] suggest coresets that are similar to our definition of coresets (i.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "In addition [8] works with constant probability, while our algorithm is deterministic (works with probability 1).",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 19,
      "context" : "Popular software for computing SVD such as GenSim [21], redsvd [12] or the MATLAB sparse SVD function (svds) use sketches and crash for inputs of a few thousand of documents and a dimensionality reduction (approximation rank) k < 100 on a regular laptop, as expected from the analysis of their algorithms.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "Even for k = 3, running the implementation of sparse SVD in Hadoop [23] took several days [13].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "Next we give a broad overview of the very latest state of the dimensionality reduction methods, such as the Lanczoz algorithm [16] for large matrices, that such systems employ under the hood.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "The proof is based on a general framework for constructing different kinds of coresets, and is known as sensitivity [10, 17].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "The proof is based on a general framework for constructing different kinds of coresets, and is known as sensitivity [10, 17].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "In [5], the Frank-Wolfe algorithm was used to construct different types of coresets than ours, and for different problems.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "Our approach is based on a solution that we give to an open problem in [5].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : "It was proved in [11] that taking the first O(k/ε) rows of DV T yields such a sketch, i.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "The first sketch for sparse matrices was suggested in [6], but like more recent results, it assumes that the complete matrix fits in memory.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "Other sketching methods that usually do not support streaming include random projections [2, 1, 9] and randomly combined rows [20, 25, 22, 18].",
      "startOffset" : 89,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Other sketching methods that usually do not support streaming include random projections [2, 1, 9] and randomly combined rows [20, 25, 22, 18].",
      "startOffset" : 89,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "Other sketching methods that usually do not support streaming include random projections [2, 1, 9] and randomly combined rows [20, 25, 22, 18].",
      "startOffset" : 89,
      "endOffset" : 98
    }, {
      "referenceID" : 18,
      "context" : "Other sketching methods that usually do not support streaming include random projections [2, 1, 9] and randomly combined rows [20, 25, 22, 18].",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : "Other sketching methods that usually do not support streaming include random projections [2, 1, 9] and randomly combined rows [20, 25, 22, 18].",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "Other sketching methods that usually do not support streaming include random projections [2, 1, 9] and randomly combined rows [20, 25, 22, 18].",
      "startOffset" : 126,
      "endOffset" : 142
    }, {
      "referenceID" : 17,
      "context" : "The Lanczoz method [19] and its variant [15] multiply a large matrix by a vector for a few iterations to get its largest eigenvector v1.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "The Lanczoz method [19] and its variant [15] multiply a large matrix by a vector for a few iterations to get its largest eigenvector v1.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "Finally, we evaluate the efficiency of our algorithm by comparing the running time against the MATLAB svds function and against the most recent state of the art dimensionality reduction algorithm [8].",
      "startOffset" : 196,
      "endOffset" : 199
    } ],
    "year" : 2016,
    "abstractText" : "In this paper we present a practical solution with performance guarantees to the problem of dimensionality reduction for very large scale sparse matrices. We show applications of our approach to computing the Principle Component Analysis (PCA) of any n × d matrix, using one pass over the stream of its rows. Our solution uses coresets: a scaled subset of the n rows that approximates their sum of squared distances to every k-dimensional affine subspace. An open theoretical problem has been to compute such a coreset that is independent of both n and d. An open practical problem has been to compute a non-trivial approximation to the PCA of very large but sparse databases such as the Wikipedia document-term matrix in a reasonable time. We answer both of these questions affirmatively. Our main technical result is a new framework for deterministic coreset constructions based on a reduction to the problem of counting items in a stream.",
    "creator" : null
  }
}