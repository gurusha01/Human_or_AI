{
  "name" : "a89cf525e1d9f04d16ce31165e139a4b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Relevant sparse codes with variational information bottleneck",
    "authors" : [ "Matthew Chalk", "Olivier Marre" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a ‘relevance’ variable, Y , while constraining the information encoded about the original data, X . Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y ."
    }, {
      "heading" : "1 Introduction",
      "text" : "An important problem, for both humans and machines, is to extract relevant information from complex data. To do so, one must be able to define which aspects of data are relevant and which should be discarded. The ‘information bottleneck’ (IB) approach, developed by Tishby and colleagues [1], provides a principled way to approach this problem. The idea behind the IB approach is to use additional ‘variables of interest’ to determine which aspects of a signal are relevant. For example, for speech signals, variables of interest could be the words being pronounced, or alternatively, the speaker identity. One then seeks a coding scheme that retains maximal information about these variables of interest, constrained on the information encoded about the input.\nThe IB approach has been used to tackle a wide variety of problems, including filtering, prediction and learning [2-5]. However, it quickly becomes intractable with high-dimensional and/or non-gaussian data. Consequently, previous research has primarily focussed on tractable cases, where the data comprises a countably small number of discrete states [1-5], or is gaussian [6].\nHere, we extend the IB algorithm of Tishby et al. [1] using a variational approximation. The algorithm maximizes a lower bound on the IB objective function, and is closely related to variational EM. Using this approach, we derive an IB algorithm that can be effectively applied to ‘sparse’ data in which input and relevance variables are generated by sparsely occurring latent features. The resulting solutions share many properties with previous sparse coding models, used to model early sensory processing [7]. However, unlike these sparse coding models, the learned representation depends on: (i) the relation between the input and variable of interest; (ii) the trade-off between encoding quality and compression. Finally, we present a kernelized version of the algorithm, that can be applied to a large range of problems with non-linear relation between the input data and variables of interest.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain."
    }, {
      "heading" : "2 Variational IB",
      "text" : "Let us define an input variable X , as well as a ‘relevance variable’, Y , with joint distribution p (y, x). The goal of the IB approach is to compress the variable X through another variable R, while conserving information about Y . Mathematically, we seek an encoding model, p (r|x), that maximizes:\nLp(r|x) = I (R;Y )− γI (R;X) ≡ 〈log p (y|r)− log p (y) + γ log p (r)− γ log p (r|x)〉p(r,x,y) , (1)\nwhere 0 < γ < 1 is a Lagrange multiplier that determines the strength of the bottleneck.\nTishby and colleagues showed that the IB loss function can be optimized by applying iterative updates: pt+1 (r|x) ∝ pt (r) exp [ − 1γ ∫ y p (y|x) log p(y|x)pt(y|r) ] , pt+1 (r) = ∫ x p (x) pt+1 (r|x) and\npt+1 (y|r) = ∫ x p (y|x) pt+1 (x|r) [1]. Unfortunately however, when p (x, y) is high-dimensional and/or non-gaussian these updates become intractable, and approximations are required.\nDue to the positivity of the KL divergence, we can write, 〈log q (·)〉p(·) ≤ 〈log p (·)〉p(·) for any approximative distribution q(·). This allows us to formulate a variational lower bound for the IB objective function:\nL̃p(r|x),q(y|r),q(r) = 1\nN N∑ n=1 〈log q (yn|r) + γ log q (r)− γ log p (r|xn)〉p(r|xn) (2)\n≤ Lp(r|x),\nwhere q (yn|r) and q (r) are variational distributions, and we have replaced the expectation over p (x, y) with the empirical expectation over training data. (Note that, for notational simplicity we have also omitted the constant term, HY = −〈log p (y)〉p(y).)\nSetting q (yn|r)← p (yn|r) and q (r)← p (r) fully tightens the bound (so that L̃ = L), and leads to the iterative algorithm of Tishby et al. However, when these exact updates are not possible, one can instead choose a restricted class of distributions q (y|r) ∈ Qy|r and q (r) ∈ Qr for which inference is tractable. Thus, to maximize L̃ with respect to parameters Θ of the encoding distribution p (r|x,Θ), we repeat the following steps until convergence:\n• For fixed Θ, find {qnew (y|r) , qnew (r)} = arg max{q(y|r),q(r)}∈{Qy|r,Qr} L̃\n• For fixed q (y|r) and q (r), find Θ = arg maxΘ L̃.\nWe note that using a simple approximation for the decoding distribution, q(y|r), can carry additional benefits, besides rendering the IB algorithm tractable. Specifically, while an advantage of mutual information is its generality, in certain cases this can also be a drawback. That is, because Shannon information does not make any assumptions about the code, it is not always apparent how information should be best extracted from the responses: just because information is ‘there’ does not mean we know how to get at it.\nIn contrast, using a simple approximation for the decoding distribution, q(y|r) (e.g. linear gaussian), constrains the IB algorithm to find solutions where information about Y can be easily extracted from the responses (e.g. via linear regression)."
    }, {
      "heading" : "3 Sparse IB",
      "text" : "In previous work on gaussian IB [6], responses were equal to a linear projection of the input, plus noise: r = Wx + η, where W is an Nr × Nx matrix of encoding weights, and η ∼ N (η|0,Σ), where Σ is an Nr×Nr covariance matrix. When the joint distribution, p (x, y), is gaussian, it follows that the marginal and decoding distributions, p (r) and p (y|r), are also gaussian, and the parameters of the encoding distribution, W and Σ, can be found analytically.\nTo illustrate the capabilities of the variational algorithm, while permitting comparison to gaussian IB, we begin by adding a single degree of complexity. In common with gaussian IB, we consider\na linear gaussian encoder, p (r|x) = N (r|Wx,Σ), and decoder, q (y|r) = N (y|Ur,Λ). However, unlike gaussian IB, we use a student-t distribution to approximate the response marginal: q (r) =∏ i Student ( ri|0, ω2i , νi ) , with scale and shape parameters, ω2i and νi, respectively. When the shape parameter, νi, is small then the student-t distribution is heavy-tailed, or ‘sparse’, compared to a gaussian distribution. Thus, we call the resulting algorithm ‘sparse IB’. Unlike gaussian IB, the introduction of a student-t marginal means the IB algorithm cannot be solved analytically, and one requires approximations."
    }, {
      "heading" : "3.1 Iterative algorithm",
      "text" : "Recall that the IB objective function consists of two terms: I (R;Y ), and I (R;X). We begin by describing how to optimize the lower and upper bound of each of these two terms with respect to the variational distributions q(y|r) and q(r), respectively. The first term of the IB objective function is bounded from below by:\nI (R;Y ) ≥ −1 2 log |Λ| − 1 2N ∑ n 〈 (yn − Ur)T Λ−1 (yn − Ur) 〉 p(r|xn) + const. (3)\nMaximizing the lower bound on I (R;Y ) with respect to the decoding parameters, U and Λ, gives: Λ = Cyy − UWCxy, U = CTxyWT ( WCxxW T + Σ )−1 (4)\nwhere Cyy = 1N ∑ n yny T n , Cxy = 1 N ∑ n xny T n , and Cxx = 1 N ∑ n xnx T n .\nUnfortunately, it is not straightforward to express the bound on I (R;X) in closed form. Instead, we use an additional variational approximation, utilising the fact that the student-t distribution can be expressed as an infinite mixture of gaussians: Student ( r|0, ω2, ν ) = ∫ η N ( r|0, ω2 ) Gamma ( η|ν2 , ν 2 ) [8]. Following a standard EM procedure [9], one can thus write a tractable lower bound on the loglikelihood, l ≡ log [ Student ( r|0, ω2, ν )] , which corresponds to an upper-bound on the bottleneck term: I (R;X) ≤\n∑ i,n 〈− log q (ri) + log p (ri|xn)〉p(ri|xn) (5)\n≤ ∑ i\n[ 1\n2 logω2i +\n1\n2Nω2i N∑ n=1 ξni 〈 r2ni 〉 + f (νi, ξi, ai)\n] − 1\n2 log |Σ|+ const.\nwhere ξni, and ai denote variational parameters for the ith unit and nth data instance. We used the shorthand notation, 〈 r2ni 〉 = wixnx T nw T i + σ 2 i , where σ 2 i is the i\nth diagonal element of Σ and wi is the ith row of W . For notational simplicity, terms that do not depend on the encoding parameters were pushed into the function, f (νi, ξi, ai)1.\nMinimizing the upper bound on I (R;X) with respect to ω2i , ξni and ai (for fixed νi) gives:\nω2i = 1\nN N∑ n=1 ξni 〈 r2ni 〉 , ξni =\nνi + 1\nνi + 〈r2ni〉 /ω2i , ai =\n1 2 (νi + 1), (6)\nThe shape parameter, νi, is then found numerically on each iteration (for fixed ξni and ai), by solving:\nψ (νi\n2\n) − log (νi 2 ) = 1 + 1 N N∑ n=1 [ ψ(ai)− log ai ξni − ξni ] , (7)\nwhere ψ(·) is the digamma function [9].\nNext we maximize the full variational objective function L̃ with respect to the encoding distribution, p (r|x) (for fixed q(y|r) and q(r)). Maximizing L̃ with respect to the encoding noise covariance, Σ, gives:\nΣ−1 = 1\nγ UTΛ−1U +\n1\nN Ω−1 N∑ n=1 Ξn, (8)\n1 f (νi, ξi, ai) = log Γ ( νi 2 ) − νi 2 log νi 2 − 1 N ∑ n [ νi−1 2 ( ψ(ai) − ln aiξni ) − νi 2 ξni +Hni ] , where Hni\nis the entropy of a gamma distribution with shape and rate parameters: ai, and ai/ξni, respectively [9].\nwhere Ω and Ξn are Nr ×Nr diagonal covariance matrices with diagonal elements Ωii = ω2i , and (Ξn)ii = ξni, respectively.\nFinally, taking the derivative of L̃ with respect to the encoding weights, W , gives:\n∂L̃\n∂W = UTΛ−1CTxy − UTΛ−1UWCxx − γΩ−1\n1\nN ∑ n ΞnWxnx T n , (9)\nSetting the derivative to zero, we can solve for W directly. One may verify that, when variational parameters, ξni, are unity, the above iterative updates are identical to the iterative gaussian IB algorithm described in [6]."
    }, {
      "heading" : "3.2 Simulations",
      "text" : "In our framework, the approximation of the response marginal, q (r), plays an analogous role to the prior distribution in a probabilistic generative model. Thus, we hypothesized that a sparse approximation for the response marginal, q(r), would permit the IB algorithm to recover sparsely occurring input features, analogous to the effect of using a sparse prior.\nA D reconstruction\nFigure 2: spatially correlated noise\nCX\nY\nB\n−90 −45 0 45 90 0\n5\n10\nencoded orientation (relative to vertical)\nno . o\nf u ni\nts\nstim.\nFigure 2: Variant of the task in figure 1, in which the input noise is spatically correlated. (A) Example input X and patch, Y . Spatial noise correlations were aligned along the vertical direction. (B) Subset of decoding filters obtained with the sparse IB algorithm. (C) Distribution of encoded orientations. (D) Example stimulus (left) and reconstruction (right) of bars presented at variable orientations (presented with zero input noise, so that X ≡ Y for this example).\nTo show this, we constructed artificial 9 × 9 image patches from combinations of orientated bar features. Each bar had a gaussian cross-section, with maximum amplitude drawn from a standard normal distribution of width 1.2 pixels. Patches were constructed by linearly combining 3 bars, with uniformly random orientation and position.\nInitially, we considered a simple de-noising task, where the input, X , was a noisy version of the original image patches (gaussian noise, with variance σ2 = 0.005; figure 1A). Training data consisted of 10,000 patches. Figure 1B and 1C show a selection of encoding (W ) and decoding (U ) filters obtained with the gaussian and sparse IB models, respectively. As predicted, only the sparse IB model was able to recover the original bar features. In addition, response histograms were considerably more heavy-tailed for the sparse IB model (fig. 1D).\nThe relevant information, I(R;Y ), encoded by the sparse model was greater than for the gaussian model, over a range of bottleneck strengths (fig. 1E). While the difference may appear small, it is consistent with work showing that sparse coding models achieve only a small improvement in log-likelihood for natural image patches [10]. We also plotted the information curve for a ‘null model’, with responses sampled from p(r|x) = N (r|x, σ2I). Interestingly, the performance of this null model was almost identical to the gaussian IB model.\nFigure 1F plots the fraction of response variance due to the signal, for each unit ( wiCxxw T i\nwiCxxwTi +σ 2 i ). Solid and dashed curves denote strong and weak bottlenecks, respectively. In both cases, the gaussian model gave a smooth spectrum of response magnitudes, while the sparse model was more ‘all-or-nothing’.\nOne way the sparse IB algorithm differs qualitatively from traditional sparse coding algorithms, is that the learned representation depends on the relation between X and Y , rather than just the input statistics. To illustrate this, we conducted simulations with patches corrupted by spatially correlated noise, aligned along the vertical direction (fig. 2A). The spatial covariance of the noise was described by a gaussian envelope, with standard deviation 3 pixels in the vertical direction and 1 pixel in horizontal direction.\nFigure 2B shows a selection of decoding filters obtained from the sparse IB model, with correlated input noise. The shape of individual filters was qualitatively similar to those obtained with uncorrelated noise (fig. 1C). However, with this stimulus, the IB model avoided ‘wasting’ bits by representing features co-orientated with the noise (fig. 2C). Consequently, it was not possible to reconstruct vertical bars from the responses, when bars were presented alone, even with zero noise (fig. 2D)."
    }, {
      "heading" : "4 Kernel IB",
      "text" : "One way to improve the IB algorithm is to consider non-linear encoders. A general choice is: p (r|x) = N (r|Wφ(x),Σ), where φ(x) is an embedding to a high-dimensional non-linear feature space.\nThe variational objective functions for both gaussian and sparse IB algorithms are quadratic in the responses, and thus can be expressed in terms of dot products of the row vector, φ(x). Consequently, every solution forwi can be expressed as an expansion of mapped training data,wi = ∑N n=1 ainφ(xn) [11]. It follows that the variational IB algorithm can be expressed in‘dual space’, with responses to the nth input drawn from r ∼ N (r|Akn,Σ), whereA is anNr×N matrix of expansion coefficients, and kn is the nth column of the N ×N kernel-gram matrix, K, with elements Knm = φ(xn)φ(xm)T . In this formulation, the problem of finding the linear encoding weights, W , is replaced by finding the expansion coefficients, A.\nThe advantage of expressing the algorithm in the dual space is that we never have to deal with φ(x) directly, so are free to consider high- (or even infinite) dimensional feature spaces. However, without additional constraints on the expansion coefficients, A, the IB algorithm becomes degenerate (i.e. the solutions are independent of the input, X). A standard way to deal with this is to add an L2 regularization term that favours solutions with small expansion coefficients. Here, this is achieved here by replacing φTnφn with φ T nφn + λI , where λ is a fixed regularization parameter. Doing so, the derivative of L̃ with respect to A becomes:\n∂L̃ ∂A = UTΛ−1Y K − ∑ n ( UTΛ−1U + γΩ−1Ξn ) A ( knk T n + λK ) (10)\nSetting the derivative to zero and solving for A directly requires inverting an NNr ×NNr matrix, which is expensive. Instead, one can use an iterative solver (we used the conjugate gradients squared\nFigure 5: handwritten digits\nsparse kIB\ngaussian kIB sparse IB gaussian IB\nX\nR Uf(X)\n−10 0 10\n0.001\n0.01\n0.1\n1\ngaussian kIB sparse kIB\nresponse (a.u.)\npr ob\nd en\nsi ty\nA B\nD C\nŶ\nFigure 4: Behaviour of kernel IB algorithm on handwritten digit data. (A) As with figure 4, we considered an occlusion task. This time, units were provided with the left hand side of the image patch, and had to reconstruct the right hand side. (B) Response distribution for 10 neurons with highest variance, for the gaussian (blue) and sparse (green) kIB algorithms. (C) Decoding filters for a subset of units, obtained with the sparse kIB algorithm. Note that, for clearer visualization, we show here the decoding filter for the entire image patch, not just the occluded region. (D) A selection of decoding filters obtained with the alternative IB algorithms.\nmethod). In addition, the computational complexity can be reduced by restricting the solution to lie on a subspace of training instances, such that, wi = ∑M n=1 ainφ(xn), where M < N . The derivation does not change, only now K has dimensions M ×N [11]. When q(r) is gaussian (equivalent to setting Ξn = I), solving for A gives:\nA = ( UTΛ−1U + γΩ−1 )−1 UTΛ−1AKRR (11)\nwhere AKRR = Y (K + λI)−1 are the coefficients obtained from kernel ridge-regression (KRR). This suggests the following two stage algorithm: first, we learn the regularisation constant, λ, and parameters of the kernel matrix, K, to maximize KRR performance on hold-out data; next, we perform variational IB, with fixed K and λ."
    }, {
      "heading" : "4.1 Simulations",
      "text" : "To illustrate the capabilities of the kernel IB algorithm, we considered an ‘occlusion’ task, with the outer columns of each patch presented as input, X (2 columns to the far left and right), and the inner columns as the relevance variable Y , to be reconstructed. Image patches were as before. Note that performing the occlusion task optimally requires detecting combinations of features presented to either side of the occluded region, and is thus inherently nonlinear.\nWe used gaussian kernels, with scale parameter, κ, and regularisation constant, λ, chosen to maximize KRR performance on test data. Both test and training data consisted of 10,000 images. However, A was restricted to lie on a subset of 1000 randomly chosen training patches (see earlier).\nFigure 3B shows a selection of decoding filters (U ) learned by the sparse kernel IB algorithm (‘sparse kIB’). A large fraction of filters resembled near-horizontal bars, traversing the occluded region. This was not the case for the sparse linear IB algorithm, which recovered localized blobs either side of the occluded region, nor the gaussian linear or kernelized models, which recovered non-local features (fig. 3C). Figure 3D shows a small but significant improvement in performance for the sparse kIB versus the gaussian kIB model. Most noticeable, however, is the distribution of responses, which are much more heavy tailed for the sparse kIB algorithm (fig. 3E).\nTo demonstrate the non-linear behaviour of the sparse kIB model, we presented bar segments: first to either side of the occluded patch, then to both sides simultaneously. When bar segments were presented to both sides simultaneously, the sparse KIB model ‘filled in’ the missing bar segment,\nin contrast to the reconstruction obtained with single bar segments (fig. 3F). This behaviour was reflected in the non-linear responses of certain encoding units, which were large when two segments were presented together, but near zero when one segment was presented alone (fig. 3G).\nFinally, we repeated the occlusion task with handwritten digits, taken from the USPS dataset (www. gaussianprocess.org/gpml/data). We used 4649 training and 4649 test patches, of 16×16 pixels. However, expansion coeffecients were restricted to a lie on subset of 500 randomly patches. We set X and Y , to be the left and right side of each patch, respectively (fig. 4A).\nIn common with the artificial data, the response distributions achieved with the sparse kIB algorithm were more heavy-tailed than for the gaussian kIB algorithm (fig. 4B). Likewise, recovered decoding filters closely resembled handwritten digits, and extended far into the occluded region (fig. 4C). This was not the case for the alternative IB algorithms (fig. 4D)."
    }, {
      "heading" : "5 Discussion",
      "text" : "Previous work has shown close parallels between the IB framework and maximum-likelihood estimation in a latent variable model [12, 13]. For the sparse IB algorithm presented here, maximizing the IB objective function is closely related to maximizing the likelihood of a ‘sparse coding’ latent variable model, with student-t prior and linear gaussian likelihood function. However, unlike traditional sparse coding models, the encoding (or ‘recognition’) model p(r|x) is conditioned on a seperate set of inputs, X , distinct from the image patches themselves. Thus, the solutions depend on the relation between X and Y , not just the image statistics (e.g. see fig. 2). Second, an additional parameter, γ, not present in sparse coding models, controls the trade-off between encoding and compression. Finally, in contrast to traditional sparse coding algorithms, IB gives an unambiguous ordering of features, which can be arranged according to the response variance of each unit (fig. 1F).\nOur work is also closely related to the IM algorithm, proposed by Barber et al. to solve the information maximization (‘infomax’) problem [14]. However, a general issue with infomax problems is that they are usually ill-posed, necessitating additional ad hoc constraints on the encoding weights or responses [15]. In contrast, in the IB approach, such constraints emerge automatically from the bottleneck term.\nA related method to find low-dimensional projections of X/Y pairs is canonical correlation analysis (‘CCA’), and its kernel analogue [16]. In fact, the features obtained with gaussian IB are identical to those obtained with CCA [6]. However, unlike CCA, the number and ‘scale’ of the features are not specified in advance, but determined by the bottleneck parameter, γ. Secondly, kernel CCA is symmetric in X and Y , and thus performs nonlinear embedding of both X and Y . In contrast, the IB problem is assymetric: we are interested in recovering Y from an input X . Thus, only X is kernelized, while the decoder remains linear. Finally, the features obtained from gaussian IB (and thus, CCA) differ qualitatively from the sparse IB algorithm, which recovers sparse features that account jointly for X and Y .\nSparse IB can be extended to the nonlinear regime using a kernel expansion. For the gaussian model, the expansion coefficients, A, are a linear projection of the coefficients used for kernel-ridgeregression (‘KRR’). A general disadvantage of KRR, is that it can be difficult to know which aspects of X are relied on to perform the regression. In contrast, the kernel IB framework provides an intermediate representation, allowing one to visualize the features that jointly account for both X and Y (figs. 3B & 4C). Furthermore, this learned representation permits generalisation across different tasks that rely on the same set of latent features; something not possible with KRR.\nFinally, the IB approach has important implications for models of early sensory processing [17, 18]. Notably, ‘efficient coding’ models typically consider the low-noise limit, where the goal is to reduce the neural response redundancy [7]. In contrast, the IB approach provides a natural way to explore the family of solutions that emerge as one varies internal coding constraints (by varying γ) and external constraints (by varying the input, X) [19, 20]. Further, our simulations suggest how the framework can be used to go beyond early sensory processing: for example to explain higher-level cognitive phenomena such as perceptual filling in (fig. 3G). In future, it would be interesting to explore how the IB framework can be used to extend the efficient coding theory, by accounting for modulations in sensory processing that occur due to changing task demands (i.e. via changes to the relevance variable, Y ), rather than just the input statistics (X)."
    } ],
    "references" : [ {
      "title" : "The information bottleneck method",
      "author" : [ "Tishby", "N. Pereira", "F C", "W. Bialek" ],
      "venue" : "The 37th annual Allerton Conference on Communication, Control and Computing",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1999
    }, {
      "title" : "Predictability, complexity, and learning",
      "author" : [ "Bialek", "I.W. Nemenman", "N. Tishby" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Information bottleneck theory and applications",
      "author" : [ "N. Slonim" ],
      "venue" : "PhD thesis, Hebrew University of Jerusalem",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Extracting relevant structures with side information",
      "author" : [ "G. Chechik", "N. Tishby" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "Conditional information bottleneck clustering. In 3rd IEEE International conference in data mining, workshop on clustering large data sets",
      "author" : [ "T. Hofmann", "D. Gondek" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Information bottleneck for gaussian variables",
      "author" : [ "Chechik", "A.G. Globerson", "N. Tishby", "Y. Weiss" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Natural image statistics and neural representation",
      "author" : [ "E.P. Simoncelli", "B.A. Olshausen" ],
      "venue" : "Ann. Rev. Neurosci",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "Scale mixtures of normal distributions",
      "author" : [ "D.F. Andrews", "Mallows C. L" ],
      "venue" : "J. of the Royal Stat. Society. Series B",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1974
    }, {
      "title" : "A derivation of the EM updates for finding the maximum likelihood parameter estimates of the student-t distribution",
      "author" : [ "C. Scheffler" ],
      "venue" : "Technical note. URL www.inference.phy.cam.ac.uk/cs482/publications/",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Natural image coding in V1: how much use is orientation selectivity",
      "author" : [ "Eichhorn", "F.J. Sinz", "M. Bethge" ],
      "venue" : "PLoS Comput Biol,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Invariant Feature Extraction and Classification in Kernel Spaces. In Advances in neural information processing systems",
      "author" : [ "Mika", "S. Ratsch", "G. Weston", "J. Scholkopf", "A.J.B. Smola", "K.R. Muller" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1999
    }, {
      "title" : "Maximum likelihood and the information bottleneck. In Advances in neural information processing systems",
      "author" : [ "N. Slonim", "Y. Weiss" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "The information bottleneck EM algorithm",
      "author" : [ "G. Elidan", "N. Friedman" ],
      "venue" : "In Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "The IM algorithm: a variational approach to information maximization",
      "author" : [ "D. Barber", "F. Agakov" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Efficient Coding of Spatial Information in the Primate Retina",
      "author" : [ "E. Doi", "Gauthier", "J.L. Field", "G.D. Shlens", "J. Sher", "M.A. Greschner" ],
      "venue" : "The Journal of neuroscience",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Canonical correlation analysis: An overview with application to learning methods",
      "author" : [ "D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Efficient representation as a design principle for neural coding and computation",
      "author" : [ "W. Bialek", "R.R. de Ruyter Van Steveninck", "N. Tishby" ],
      "venue" : "In Information Theory,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Predictive information in a sensory population",
      "author" : [ "S.E. Palmer", "O. Marre", "M.J. Berry", "W. Bialek" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Sparse coding of natural images using an overcomplete set of limited capacity units",
      "author" : [ "Doi", "Eizaburo", "M.S. Lewicki" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Optimal population coding by noisy spiking neurons",
      "author" : [ "Tkacik", "G. Prentice", "V.J.S. Balasubramanian", "E. Schneidman" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The ‘information bottleneck’ (IB) approach, developed by Tishby and colleagues [1], provides a principled way to approach this problem.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "The IB approach has been used to tackle a wide variety of problems, including filtering, prediction and learning [2-5].",
      "startOffset" : 113,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "The IB approach has been used to tackle a wide variety of problems, including filtering, prediction and learning [2-5].",
      "startOffset" : 113,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "The IB approach has been used to tackle a wide variety of problems, including filtering, prediction and learning [2-5].",
      "startOffset" : 113,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "The IB approach has been used to tackle a wide variety of problems, including filtering, prediction and learning [2-5].",
      "startOffset" : 113,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "Consequently, previous research has primarily focussed on tractable cases, where the data comprises a countably small number of discrete states [1-5], or is gaussian [6].",
      "startOffset" : 144,
      "endOffset" : 149
    }, {
      "referenceID" : 1,
      "context" : "Consequently, previous research has primarily focussed on tractable cases, where the data comprises a countably small number of discrete states [1-5], or is gaussian [6].",
      "startOffset" : 144,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "Consequently, previous research has primarily focussed on tractable cases, where the data comprises a countably small number of discrete states [1-5], or is gaussian [6].",
      "startOffset" : 144,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "Consequently, previous research has primarily focussed on tractable cases, where the data comprises a countably small number of discrete states [1-5], or is gaussian [6].",
      "startOffset" : 144,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "Consequently, previous research has primarily focussed on tractable cases, where the data comprises a countably small number of discrete states [1-5], or is gaussian [6].",
      "startOffset" : 144,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "Consequently, previous research has primarily focussed on tractable cases, where the data comprises a countably small number of discrete states [1-5], or is gaussian [6].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "[1] using a variational approximation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "The resulting solutions share many properties with previous sparse coding models, used to model early sensory processing [7].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "In previous work on gaussian IB [6], responses were equal to a linear projection of the input, plus noise: r = Wx + η, where W is an Nr × Nx matrix of encoding weights, and η ∼ N (η|0,Σ), where Σ is an Nr×Nr covariance matrix.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "Following a standard EM procedure [9], one can thus write a tractable lower bound on the loglikelihood, l ≡ log [ Student ( r|0, ω(2), ν )] , which corresponds to an upper-bound on the bottleneck term: I (R;X) ≤ ∑",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "where ψ(·) is the digamma function [9].",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "n [ νi−1 2 ( ψ(ai) − ln ai ξni ) − νi 2 ξni +Hni ] , where Hni is the entropy of a gamma distribution with shape and rate parameters: ai, and ai/ξni, respectively [9].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "One may verify that, when variational parameters, ξni, are unity, the above iterative updates are identical to the iterative gaussian IB algorithm described in [6].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "While the difference may appear small, it is consistent with work showing that sparse coding models achieve only a small improvement in log-likelihood for natural image patches [10].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 10,
      "context" : "Consequently, every solution forwi can be expressed as an expansion of mapped training data,wi = ∑N n=1 ainφ(xn) [11].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "The derivation does not change, only now K has dimensions M ×N [11].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "Previous work has shown close parallels between the IB framework and maximum-likelihood estimation in a latent variable model [12, 13].",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 12,
      "context" : "Previous work has shown close parallels between the IB framework and maximum-likelihood estimation in a latent variable model [12, 13].",
      "startOffset" : 126,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "to solve the information maximization (‘infomax’) problem [14].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "However, a general issue with infomax problems is that they are usually ill-posed, necessitating additional ad hoc constraints on the encoding weights or responses [15].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 15,
      "context" : "A related method to find low-dimensional projections of X/Y pairs is canonical correlation analysis (‘CCA’), and its kernel analogue [16].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "In fact, the features obtained with gaussian IB are identical to those obtained with CCA [6].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "Finally, the IB approach has important implications for models of early sensory processing [17, 18].",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 17,
      "context" : "Finally, the IB approach has important implications for models of early sensory processing [17, 18].",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "Notably, ‘efficient coding’ models typically consider the low-noise limit, where the goal is to reduce the neural response redundancy [7].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "In contrast, the IB approach provides a natural way to explore the family of solutions that emerge as one varies internal coding constraints (by varying γ) and external constraints (by varying the input, X) [19, 20].",
      "startOffset" : 207,
      "endOffset" : 215
    }, {
      "referenceID" : 19,
      "context" : "In contrast, the IB approach provides a natural way to explore the family of solutions that emerge as one varies internal coding constraints (by varying γ) and external constraints (by varying the input, X) [19, 20].",
      "startOffset" : 207,
      "endOffset" : 215
    } ],
    "year" : 2016,
    "abstractText" : "In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a ‘relevance’ variable, Y , while constraining the information encoded about the original data, X . Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y .",
    "creator" : null
  }
}