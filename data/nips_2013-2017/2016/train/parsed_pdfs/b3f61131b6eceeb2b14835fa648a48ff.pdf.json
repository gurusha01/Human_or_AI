{
  "name" : "b3f61131b6eceeb2b14835fa648a48ff.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning in Games: Robustness of Fast Convergence",
    "authors" : [ "Dylan J. Foster", "Zhiyuan Li", "Thodoris Lykouris", "Karthik Sridharan", "Éva Tardos" ],
    "emails" : [ "djfoster@cs.cornell.edu.", "teddlyk@cs.cornell.edu.", "sridharan@cs.cornell.edu.", "eva@cs.cornell.edu." ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider players repeatedly playing a game, all acting independently to minimize their cost or maximize their utility. It is natural in this setting for each player to use a learning algorithm that guarantees small regret to decide on their strategy, as the environment is constantly changing due to each player’s choice of strategy. It is well known that such decentralized no-regret dynamics are guaranteed to converge to a form of equilibrium for the game. Furthermore, in a large class of games known as smooth games [23] they converge to outcomes with approximately optimal social welfare matching the worst-case efficiency loss of Nash equilibria (the price of anarchy). In smooth cost minimization games the overall cost is /(1 µ) times the minimum cost, while in smooth mechanisms [29] such as auctions it is /max(1, µ) times the maximum total utility (where and µ are parameters of the smoothness condition). Examples of smooth games and mechanisms include routing games and many forms of auction games (see e.g. [23, 29, 24]).\nThe speed at which the game outcome converges to this approximately optimal welfare is governed by individual players’ regret bounds. There are a large number of simple regret minimization algorithms (Hedge/Multiplicative Weights, Mirror Decent, Follow the Regularized Leader; see e.g. [12]) that\n⇤ Cornell University {djfoster,teddlyk,sridharan,eva}@cs.cornell.edu. Work supported in part under NSF grants CDS&E-MSS 1521544, CCF-1563714, ONR grant N00014-08-1-0031, a Google faculty research award, and an NDSEG fellowship.\n†Tsinghua University, lizhiyuan13@mails.tsinghua.edu.cn. Research performed while author was visiting Cornell University.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nguarantee that the average regret goes down as O(1/ p T ) with time T , which is tight in adversarial settings.\nTaking advantage of the fact that playing a game against opponents who themselves are also using regret minimization is not a truly adversarial setting, a sequence of papers [9, 22, 28] showed that by using specific learning algorithms, the dependence on T of the convergence rate can be improved to O(1/T ) (“fast convergence”). Concretely, Syrgkanis et al. [28] show that all algorithms satisfying the so-called RVU property (Regret by Variation in Utilities), which include Optimistic Mirror Descent [22], converge at a O(1/T ) rate with a fixed number of players.\nOne issue with the works of [9, 22, 28] is that they use expected cost as their feedback model for the players. In each round every player receives the expected cost for each of their available actions, in expectation over the current action distributions of all other players. This clearly represents more information than is realistically available to players in games — at most each player sees the cost of each of their actions given the actions taken by the other players (realized feedback). In fact, even if each player had access to the action distributions of the other players, simply computing this expectation is generally intractable when n, the number of players, is large.\nWe improve the result of [28] on the convergence to approximate optimality in smooth games in a number of different aspects. To achieve this, we relax the quality of approximation from the bound guaranteed by smoothness. Typical smoothness bounds on the price of anarchy in auctions are small constants, such a factor of 1.58 or 2 in item auctions. Increasing the approximation factor by an arbitrarily small constant ✏ > 0 enables the following results:\n• We show that learning algorithms obtaining fast convergence are ubiquitous. • We improve the speed of convergence by a factor of n, the number of players. • For all our results, players only need feedback based on realized outcomes, instead of\nexpected outcomes. • We show that convergence occurs with high probability in most settings. • We extend the results to show that it is enough for the players to observe realized bandit\nfeedback, only seeing the outcome of the action they play. • Our results apply to settings where the set of players in the game changes over time [19].\nWe strengthen previous results by showing that a broader class of algorithms achieve approximate efficiency under significant churn.\nWe achieve these results using a property we term Low Approximate Regret, which simply states that an online learning algorithm achieves good regret against a multiplicative approximation of the best action in hindsight. This property is satisfied by many known algorithms including even the vanilla Hedge algorithm, as well as Optimistic Hedge [21, 28] (via a new analysis). The crux of our analysis technique is the simple observation that for many types of data-dependent regret bounds we can fold part of the regret bound into the comparator term, allowing us to explore the trade-off between additive and multiplicative approximation.\nIn Section 3, we show that Low Approximate Regret implies fast convergence to the social welfare guaranteed by the price of anarchy via the smoothness property. This convergence only requires feedback from the realized actions played by other players, not their action distribution or the expectation over their actions. We further show that this convergence occurs with high probability in most settings. For games with a large number of players we also improve the speed of convergence. [28] shows that players using Optimistic Hedge in a repeated game with n players converge to the approximately optimal outcome guaranteed by smoothness at a rate of O(n2/T ). They also offer an analysis guaranteeing convergence of O(n/T ) , at the expense of a constant factor decrease in the quality of approximation (e.g., a factor of 4 in atomic congestion games with affine congestion). We achieve the convergence bound of O(n/T ) with only an arbitrarily small loss in the approximation.\nAlgorithms that satisfy the Low Approximate Regret property are ubiquitous and include simple, efficient algorithms such as Hedge and variants. The observation that this broad class of algorithms enjoys fast convergence in realistic settings suggests that fast convergence occurs in practice.\nComparing our work to [28] with regard to feedback, Low Approximate Regret algorithms require only realized feedback, while the analysis of the RVU property in [28] requires expected feedback. To see the contrast, consider the load balancing game introduced in [17] with two players and two bins, where each player selects a bin and observes cost given by the number of players in that bin. Initialized at the uniform distribution, any learning algorithm with expectation feedback (e.g. those in [28]) will stay at the uniform distribution forever, because the expected cost vector distributes\ncost equally across the two bins. This gives low regret under expected costs, but suppose we were interested in realized costs: The only “black box” way to lift [28] to this case would be to simply evaluate the regret bound above under realized costs, but here players will experience ⇥(1/ p T ) variation because they select bins uniformly at random, ruining the fast convergence. Our analysis sidesteps this issue because players achieve Low Approximate Regret with high probability.\nIn Section 4 we consider games where players can only observe the cost of the action they played given the actions taken by the other players, and receive no feedback for actions not played (bandit feedback). [22] analyzed zero-sum games with bandit feedback, but assumed that players receive expected cost over the strategies of all other players. In contrast, the Low Approximate Regret property can be satisfied by just observing realizations, even with bandit feedback. We propose a new bandit algorithm based on log-barrier regularization with importance sampling that guarantees fast convergence of O(d log T/✏) where d is the number of actions. Known techniques would either result in a convergence rate of O(d3 log T ) (e.g. adaptations of SCRiBLe [21]) or would not extend to utility maximization settings (e.g. GREEN [2]). Our technique is of independent interest since it improves the dependence of approximate regret bounds on the number of experts while applying to both cost minimization and utility maximization settings.\nFinally, in Section 5, we consider the dynamic population game setting of [19], where players enter and leave the game over time. [19] showed that regret bounds for shifting experts directly influence the rate at which players can turn over and still guarantee close to optimal solutions on average. We show that a number of learning algorithms have the Low Approximate Regret property in the shifting experts setting, allowing us to extend the fast convergence result to dynamic games. Such learning algorithms include a noisy version of Hedge as well as AdaNormalHedge [18], which was previously studied in the dynamic setting in [19]. Low Approximate Regret allows us to increase the turnover rate from the one in [19], while also widening and simplifying the class of learning algorithms that players can use to guarantee the close to optimal average welfare."
    }, {
      "heading" : "2 Repeated Games and Learning Dynamics",
      "text" : "We consider a game G among a set of n players. Each player i has an action space S\ni and a cost function cost\ni : S 1 ⇥ · · · ⇥ S n ! [0, 1] that maps an action profile s = (s 1 , . . . , s n ) to the cost cost\ni (s) that player experiences1. We assume that the action space of each player has cardinality d, i.e. |S\ni | = d. We let w = (w 1 , . . . , w n ) denote a list of probability distributions over all players’ actions, where w\ni 2 (S i ) and w i,x is the probability of action x 2 S i .\nThe game is repeated for T rounds. At each round t each player i picks a probability distribution wt\ni\n2 (S i ) over actions and draws their action st i from this distribution. Depending on the game playing environment under consideration, players will receive different types of feedback after each round. In Sections 3 and 5 we consider feedback where at the end of the round each player i observes the utility they would have received had they played any possible action x 2 S\ni given the actions taken by the other players. More formally let ct\ni,x\n= cost\ni (x, st i), where st i is the set of strategies of all but the ith player at round t, and let ct\ni = (ct i,x ) x2S i . Note that the expected cost of player i at round t (conditioned on the other players’ actions) is simply the inner product hwt\ni , ct i i. We refer to this form of feedback as realized feedback since it only depends on the realized actions st i sampled by the opponents; it does not directly depend on their distributions wt i. This should be contrasted with the expectation feedback used by [28, 9, 22], where player i observes E s t\ni⇠wt i [costi(x, s t i)] for each x.\nSections 4 and 5 consider extensions of our repeated game model. In Section 4 we examine partial information (“bandit”) feedback, where players observe only the cost of their own realized actions. In Section 5 we consider a setting where the player set is evolving over time. Here we use the dynamic population model of [19], where at each round t each player i is replaced (“turns over”) with some probability p. The new player has cost function costt\ni (·) and action space St i which may change arbitrarily subject to certain constraints. We will formalize this notion later on.\nLearning Dynamics We assume that players select their actions using learning algorithms satisfying a property we call Low Approximate Regret, which simply requires that the cumulative cost of the learner multiplicatively approximates the cost of the best action they could have chosen in hindsight.\n1See Appendix D for analogous definitions for utility maximization games.\nWe will see in subsequent sections that this property is ubiquitous and leads to fast convergence in a robust range of settings. Definition 1. (Low Approximate Regret) A learning algorithm for player i satisfies the Low Approximate Regret property for parameter ✏ > 0 and function A(d, T ) if for all action distributions f 2 (S\ni\n),\n(1 ✏) TX\nt=1\nhwti , ctii  TX\nt=1\nhf, ctii+ A(d, T )\n✏ . (1)\nA learning algorithm satisfies Low Approximate Regret against shifting experts if for all sequences f1, . . . , fT 2 (S\ni\n), letting K = |{i > 2 : f t 1 6= f t}| be the number of shifts,\n(1 ✏) TX\nt=1\nhwti , ctii  TX\nt=1\nhf t, ctii+ (1 +K) A(d, T )\n✏ . (2)\nIn the bandit feedback setting, we require (1) to hold in expectation over the realized strategies of player i for any f 2 (S\ni\n) fixed before the game begins.\nWe use the version of the Low Approximate Regret property with shifting experts when considering players in dynamic population games in Section 5. In this case, the game environment is constantly changing due to churn in the population, and we need the players to have low approximate regret with shifting experts to guarantee high social welfare despite the churn.\nWe emphasize that all algorithms we are aware of that satisfy Low Approximate Regret can be made to do so for any fixed choice of the approximation factor ✏ via an appropriate selection of parameters. Many algorithms have an even stronger property: They satisfy (1) or (2) for all ✏ > 0 simultaneously. We say that such algorithms satisfy the Strong Low Approximate Regret property. This property has favorable consequences in the context of repeated games.\nThe Low Approximate Regret property differs from previous properties such as RVU in that it only requires that the learner’s cost be close to a multiplicative approximation to the cost of the best action in hindsight. Consequently, it is always smaller than the regret. For instance, if we consider only uniform (i.e. not data-dependent) regret bounds the Hedge algorithm can only achieve O( p T log d) exact regret, but can achieve Low Approximate Regret with parameters ✏ and A(d, T ) = O(log d) for any ✏ > 0. Low Approximate Regret is analogous to the notion of ↵-regret from [15], with ↵ = (1 + ✏).\nIn Appendix D we show that the Low Approximate Regret property and our subsequent results naturally extend to utility maximization games.\nSmooth Games It is well-known that in a large class of games, termed smooth games by Roughgarden [23], traditional learning dynamics converge to approximately optimal social welfare. In subsequent sections we analyze the convergence of Low Approximate Regret learning dynamics in such smooth games. We will see that Low Approximate Regret (for sufficiently small A(d, T )) coupled with smoothness of the game implies fast convergence of learning dynamics to desirable social welfare under a variety of conditions. Before proving this result we review social welfare and smooth games. For a given action profile s, the social cost is C(s) = P n\ni=1\ncost\ni (s). To bound the efficiency loss due to the selfish behavior of the players we define\nOPT = min so\nnX\ni=1\ncosti(s o ).\nDefinition 2. (Smooth game [23]) A cost minimization game is called ( , µ)-smooth if for all strategy profiles s and s⇤: P i cost i (s⇤ i , s i)  · costi(s⇤) + µ · costi(s).\nThis property is typically applied using a (close to) optimal action profile s⇤ = so. For this case the property implies that if s is an action profile with very high cost, then some player deviating to her share of the optimal profile s⇤\ni\nwill improve her cost.\nFor smooth games, the price of anarchy is at most /(1 µ), meaning that Nash equilibria of the game, as well as no-regret learning outcomes in the limit, have social cost at most a factor of /(1 µ) above the optimum. Smooth cost minimization games include congestion games such\nas routing or load balancing. For example, atomic congestion games with affine cost functions are ( 5\n3 , 1 3 )-smooth [8], non-atomic games are (1, 0.25) smooth [25], implying a price of anarchy of 2.5 and 1.33 respectively. While we focus on cost-minimization games for simplicity of exposition, an analogous definition also applies for utility maximization, including smooth mechanisms [29], which we elaborate on in Appendix D. Smooth mechanisms include most simple auctions. For example, the first price item auction is (1 1/e, 1)-smooth and all-pay actions are (1/2, 1)-smooth, implying a price of anarchy of 1.58 and 2 respectively. All of our results extend to such mechanisms."
    }, {
      "heading" : "3 Learning in Games with Full Information Feedback",
      "text" : "We now analyze the efficiency of algorithms with the Low Approximate Regret property in the full information setting. Our first proposition shows that, for smooth games with full information feedback, learners with the Low Approximate Regret property converge to efficient outcomes. Proposition 1. In any ( , µ)-smooth game, if all players use Low Approximate Regret algorithms satisfying Eq. (1) with parameters ✏ and A(d, T ), then for the action profiles st drawn on round t from the corresponding mixed actions of the players,\n1\nT\nX\nt\nE ⇥ C(st) ⇤ \n1 µ ✏OPT + n T · 1 1 µ ✏ · A(d, T ) ✏ .\nProof. This proof is a straightforward modification of the usual price of anarchy proof for smooth games. We obtain the claimed bound by writing P t E[C(st)] = P i P t E[cost i\n(st)], using the Low Approximate Regret property with f = s⇤\ni for each player i for the optimal solution s⇤, then using the smoothness property for each time t to bound P i cost i (s⇤ i , st i), and finally rearranging terms.\nFor ✏ << (1 µ) the approximation factor of /(1 µ ✏) is very close to the price of anarchy /(1 µ). This shows that Low Approximate Regret learning dynamics quickly converge to outcomes with social welfare arbitrarily close to the welfare guaranteed for exact Nash equilibria by the price of anarchy. A simple corollary of this proposition is that, when players use learning algorithms that satisfy the Strong Low Approximate Regret property, the bound above can be taken to depend on OPT even though this value is unknown to the players.\nWhenever the Low Approximate Regret property is satisfied, a high probability version of the property with similar dependence on ✏ and A(d, T ) is also satisfied. This implies that in addition to quickly converging to efficient outcomes in expectation, Low Approximate Regret learners experience fast convergence with high probability. Proposition 2. In any ( , µ)-smooth game, if all players use Low Approximate Regret algorithms satisfying Eq. (1) for parameters ✏ and A(d, T ), then for the action profile st drawn on round t from the players’ mixed actions and = 2✏/(1 + ✏), we have that 8 > 0, with probability at least 1 ,\n1\nT\nX\nt\nC(st)  1 µ OPT + n T · 1 1 µ ·\n 4A(d, T )\n+\n12 log(n log2(T )/ ))\n,\nExamples of Simple Low Approximate Regret Algorithms Propositions 1 and 2 are informative when applied with algorithms for which A(d, T ) is sufficiently small. One would hope that such algorithms are relatively simple and easy to find. We show now that the well-known Hedge algorithm as well as basic variants such as Optimistic Hedge and Hedge with online learning rate tuning satisfy the property with A(d, T ) = O(log d), which will lead to fast convergence both in terms of n and T . For these algorithms and indeed all that we consider in this paper, we can achieve the Low Approximate Regret property for any fixed ✏ > 0 via an appropriate parameter setting. In Appendix A.2, we provide full descriptions and proofs for these algorithms. Example 1. Hedge satisfies the Low Approximate Regret property with A(d, T ) = log(d). In particular one can achieve the property for any fixed ✏ > 0 by using ✏ as the learning rate. Example 2. Hedge with online learning rate tuning satisfies the Strong Low Approximate Regret property with A(d, T ) = O(log d). Example 3. Optimistic Hedge satisfies the Low Approximate Regret property with A(d, T ) = 8 log(d). As with vanilla Hedge, we can choose the learning rate to achieve the property with any ✏.\nExample 4. Any algorithm satisfying a “small loss” regret bound of the form p (Learner’s cost) ·A or p\n(Cost of best action) ·A satisfies Strong Low Approximate Regret via the AM-GM inequality, i.e. p (Learner’s cost) ·A / inf\n✏>0 [✏ · (Learner’s cost) +A/✏]. In particular, this implies that the following algorithms have Strong Low Approximate Regret: Canonical small loss and self-confident algorithms, e.g. [11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].\nExample 4 shows that the Strong Low Approximate Regret property in fact is ubiquitous, as it is satisfied by any algorithm that provides small loss regret bounds or one of many variants on this type of bound. Moreover, all algorithms that satisfy the Low Approximate Regret property for all fixed ✏ can be made to satisfy the strong property using the doubling trick.\nMain Result for Full Information Games: Theorem 3. In any ( , µ)-smooth game, if all players use Low Approximate Regret algorithms satisfying (1) for parameter ✏ 2 and A(d, T ) = O(log d), then\n1\nT\nX\nt\nE ⇥ C(st) ⇤ \n1 µ ✏OPT + n T · 1 1 µ ✏ · O(log d) ✏ ,\nand furthermore, 8 > 0, with probability at least 1 , 1\nT\nX\nt\nE ⇥ C(st) ⇤ \n1 µ ✏OPT + n T · 1 1 µ ✏ ·\n O(log d)\n✏ + O(log(n log2(T )/ )) ✏ .\nCorollary 4. If all players use Strong Low Approximate Regret algorithms then: 1. The above results hold for all ✏ > 0 simultaneously. 2. Individual players have regret bounded by O(T 1/2), even in adversarial settings. 3. The players approach a coarse correlated equilibrium asymptotically.\nComparison with Syrgkanis et al. [28]. By relaxing the standard /(1 µ) price of anarchy bound, Theorem 3 substantially broadens the class of algorithms that experience fast convergence to include even the common Hedge algorithm. The main result of [28] shows that learning algorithms that satisfy their RVU property converge to the price of anarchy bound /(1 µ) at rate n2 log d/T . They further achieve a worse approximation of (1 + µ)/(µ(1 µ)) at the improved (in terms of n) rate of n log d/T . We converge to an approximation arbitrarily close to /(1 + µ) at a rate of n log d/T . Note that in atomic congestion games with affine congestion function µ = 1/3, so their bound of (1 + µ)/µ(1 µ) loses a factor of 4 compared to the price of anarchy. Strong Low Approximate Regret algorithms such as Hedge with online learning rate tuning simultaneously experience both fast O(n/T ) convergence in games and an O(1/ p T ) bound on individual regret in adversarial settings. In contrast, [28] only shows O(n/ p T ) individual regret and O(n3/T ) convergence to price of anarchy simultaneously.\nLow Approximate Regret algorithms only need realized feedback, whereas [28] require expectation feedback. Having players receive expectation feedback is unrealistic in terms of both information and computation. Indeed, even if the necessary information was available, computing expectations over discrete probability distributions is not tractable unless n is taken to be constant.\nOur results imply that Optimistic Hedge enjoys the best of two worlds: It enjoys fast convergence to the exact /(1 µ) price of anarchy using expectation feedback as well as fast convergence to the ✏-approximate price of anarchy using realized feedback. Our new analysis of Optimistic Hedge (Appendix A.2.2) sheds light on another desirable property of this algorithm: Its regret is bounded in terms of the net cost incurred by Hedge. Figure 1 summarizes the differences between our results.\n2We can also show that the theorem holds if players satisfy the property for different values of ✏, but with a dependence on the worst case value of ✏ across all players."
    }, {
      "heading" : "4 Bandit Feedback",
      "text" : "In many realistic scenarios, the players of a game might not even know what they would have lost or gained if they had deviated from the action they played. We model this lack of information with bandit feedback, in which each player observes a single scalar, cost\ni (st) = hst i , ct i i, per round.3 When the game considered is smooth, one can use the Low Approximate Regret property as in the full information setting to show that players quickly converge to efficient outcomes. Our results here hold with the same generality as in the full information setting: As long as learners satisfy the Low Approximate Regret property (1), an efficiency result analogous to Proposition 1 holds. Proposition 5. Consider a ( , µ)-smooth game. If all players use bandit learning algorithms with Low Approximate Regret A(d, T ) then\n1 T E \" X\nt\nC(st) # \n1 µ ✏OPT + n T · 1 1 µ ✏ · A(d, T ) ✏ .\nBandit Algorithms with Low Approximate Regret The bandit Low Approximate Regret property requires that (1) holds in expectation against any sequence of adaptive and potentially adversarially chosen costs, but only for an obliviously chosen comparator f .4 This is weaker than requiring that an algorithm achieve a true expected regret bound; it is closer to pseudo-regret.\nThe Exp3Light algorithm [27] satisfies Low Approximate Regret with A(d, T ) = d2 log T . The SCRiBLe algorithm introduced in [1] (via the analysis in [21]) enjoys the Low Approximate Regret property with A(d, T ) = d3 log(dT ). The GREEN algorithm [2] achieves the Low Approximate Regret property with A(d, T ) = d log(T ), but only works with costs and not gains. This prevents it from being used in utility settings such as auctions, as in Appendix D.\nWe present a new bandit algorithm (Algorithm 3) that achieves Low Approximate Regret with A(d, T ) = d log(T/d) and thus matches the performance of GREEN, but works in both cost minimization and utility maximization settings. This method is based on Online Mirror Descent with a logarithmic barrier for the positive orthant, but differs from earlier algorithms based on the logarithmic barrier (e.g. [21]) in that it uses the classical importance-weighted estimator for costs instead of sampling based on the Dikin elipsoid. It can be implemented in ˜O(d) time per round, using line search to find . We provide proofs and further discussion of Algorithm 3 in Appendix B.\nAlgorithm 3: Initialize w1 to the uniform distribution. On each round t, perform update:\nAlgorithm 3 update: wtst 1 = w\nt 1 st 1\n1+ ⌘ct st 1 + w t 1 st 1\nand 8j 6= st 1 wtj = w\nt 1 j\n1+ wt 1j , (3)\nwhere  0 is chosen so that wt is a valid probability distribution. Lemma 6. Algorithm 3 with ⌘ = ✏/(1+ ✏) has Low Approximate Regret with A(d, T ) = O(d log T ).\nComparison to Other Algorithms In contrast to the full information setting where the most common algorithm, Hedge, achieves Low Approximate Regret with competitive parameters, the most common adversarial bandit algorithm Exp3 does not seem to satisfy Low Approximate Regret. [3] provide a small loss bound for bandits which would be sufficient for Low Approximate Regret, but their algorithm requires prior knowledge on the loss of the best action (or a bound on it), which is not appropriate in our game setting. Similarly, the small loss bound in [20] is not applicable in our setting as the work assumes an oblivious adversary and so does not apply to the games we consider."
    }, {
      "heading" : "5 Dynamic Population Games",
      "text" : "In this section we consider the dynamic population repeated game setting introduced in [19]. Detailed discussion and proofs are deferred to Appendix C. Given a game G as described in Section 2, a dynamic population game with stage game G is a repeated game where at each round t game G is played and every player i is replaced by a new player with a turnover probability p. Concretely, when a player turns over, their strategy set and cost function are changed arbitrarily subject to the rules\n3With slight abuse of notation, sti denotes the identity vector associated to the strategy player i used at time t. 4This is because we only need to evaluate (1) with the game’s optimal solution s? to prove efficiency results.\nof the game. This models a repeated game setting where players have to adapt to an adversarially changing environment. We denote the cost function of player i at round t as costt\ni (·). As in Section 3, we assume that the players receive full information feedback. At the end of each round they observe the entire cost vector ct\ni\n= cost\nt\ni (·, st i), but are not aware of the costs of other players in the game. Learning in Dynamic Population Games and the Price of Anarchy To guarantee small overall cost using the smoothness analysis from Section 2, players need to exhibit low regret against a shifting benchmark s⇤t\ni\nof socially optimal strategies achieving OPTt = min s\n⇤t P\ni\ncost\nt\ni (s⇤t). Even with a small probability p of change, the sequence of optimal solutions can have too many changes to be able to achieve low regret. In spite of this apparent difficulty, [19] prove that at least a ⇢ /(1 µ ✏) fraction of the optimal welfare is guaranteed if 1. players are using low adaptive regret algorithms (see [14, 18]) and 2. for the underlying optimization problem there exists a relatively stable sequence of solutions which at each step approximate the optimal solution by a factor of ⇢. This holds as long as the turnover probability p is upper bounded by a function of ✏ (and of certain other properties of the game, such as the stability of the close to optimal solution).\nWe consider dynamic population games where each player uses a learning algorithm satisfying Low Approximate Regret for shifting experts (2). This shifting version of Low Approximate Regret implies a dynamic game analog of our main efficiency result, Proposition 1.\nAlgorithms with Low Approximate Regret for Shifting Experts A simple variant of Hedge we term Noisy Hedge, which mixes the Hedge update at each round with a small amount of uniform noise, satisfies the Low Approximate Regret property for shifting experts with A(d, T ) = O(log(dT )). Moreover, algorithms that satisfy a small loss version of the adaptive regret property [14] used in [19] satisfy the Strong Low Approximate Regret property. Proposition 7. Noisy Hedge with learning rate ⌘ = ✏ satisfies the Low Approximate Regret property for shifting experts with A(d, T ) = 2 log(dT ).\nExtending Proposition 1 to the Dynamic Population Game Setting Let s⇤1:T denote a stable sequence of near-optimal solutions s⇤t with\ni\ncost\nt\ni (s⇤t)  ⇢ · OPTt for all rounds t. As discussed in [19], such stable sequences can come from simple greedy algorithms (where each change in the input of one player affects the output of few other players) or via differentially private algorithms (where each change in the input of one player affects the output of all other players with small probability); in the latter case the sequence is randomized. For a deterministic sequence s⇤1:T\ni of player i’s actions, we let the random variable K\ni denote the number of changes in the sequence. For a randomized sequence s⇤1:T\ni\n, we let K i be the sum of total variation distances between subsequent pairs s⇤t 1\ni\nand s⇤t i\n. The stability of a sequence of solutions is determined by E[ P\ni\nK i ]. Proposition 8. (PoA with Dynamic Population) If all players use Low Approximate Regret algorithms satisfying (2) in a dynamic population game, where the stage game is ( , µ)-smooth, and K\ni as defined above then\n1\nT\nX\nt\nE ⇥ C(st) ⇤  1 T · ⇢ 1 µ ✏ X\nt\nE ⇥ OPTt ⇤ +\nn+ E ⇥P i Ki ⇤\nT · 1 1 µ ✏ · A(d, T ) ✏ . (4)\nHere the expectation is taken over the random turnover in the population playing the game, as well as the random choices of the players on the left hand side.\nTo claim a price of anarchy bound, we need to ensure that the additive term in (4) is a small fraction of the optimal cost. The challenge is that high turnover probability reduces stability, increasing E[ P\ni\nK i\n]. By using algorithms with smaller A(d, T ), we can allow for higher E[ P\ni\nK i ] and hence higher turnover probability. Combining Noisy Hedge with Proposition 8 strengthens the results in [19] by both weakening the behavioral assumption on the players, allowing them to use simpler learning algorithms, and allowing a higher turnover probability.\nComparison to Previous Results [19] use the more complex AdaNormalHedge algorithm of [18], which satisfies the adaptive regret property of [14], but has O(dT ) space complexity. In contrast, Noisy Hedge only requires space complexity of just O(d). Moreover, a broader class of algorithms satisfy the Low Approximate Regret property which makes the efficiency guarantees more prescriptive since this property serves as a behavioral assumption. Finally, the our guarantees we provide improve on the turnover probability that can be accommodated as discussed in Appendix C.1.\nAcknowledgements We thank Vasilis Syrgkanis for sharing his simulation software and the NIPS reviewers for pointing out the GREEN algorithm [2]."
    } ],
    "references" : [ {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin" ],
      "venue" : "In Proc. of the 21st Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Consistency in On-Line Learning in Case of Unbounded Losses Under Partial Monitoring, pages 229–243",
      "author" : [ "Chamy Allenberg", "Peter Auer", "László Györfi", "György Ottucsák. Hannan" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Regret bounds and minimax policies under partial monitoring",
      "author" : [ "Jean-Yves Audibert", "Sébastien Bubeck" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Adaptive and self-confident on-line learning algorithms",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Claudio Gentile" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "High-probability regret bounds for bandit online linear optimization",
      "author" : [ "Peter L Bartlett", "Varsha Dani", "Thomas Hayes", "Sham Kakade", "Alexander Rakhlin", "Ambuj Tewari" ],
      "venue" : "In Proceedings of 21st Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gabor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "Nicolo Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "The price of anarchy of finite congestion games",
      "author" : [ "Giorgos Christodoulou", "Elias Koutsoupias" ],
      "venue" : "In Proceedings of the 37th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Near-optimal no-regret algorithms for zero-sum games",
      "author" : [ "Constantinos Daskalakis", "Alan Deckelbaum", "Anthony Kim" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Adaptive online learning",
      "author" : [ "Dylan J Foster", "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E Schapire" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1997
    }, {
      "title" : "Introduction to Online Convex Optimization",
      "author" : [ "Elad Hazan" ],
      "venue" : "Foundations and Trends in Optimization,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Extracting certainty from uncertainty: Regret bounded by variation in costs",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Efficient learning algorithms for changing environments",
      "author" : [ "Elad Hazan", "C. Seshadhri" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning (ICML),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Playing games with approximation algorithms",
      "author" : [ "Sham M. Kakade", "Adam Tauman Kalai", "Katrina Ligett" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Second-order quantile methods for experts and combinatorial games",
      "author" : [ "Wouter M Koolen", "Tim Van Erven" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Achieving all with no parameters: Adanormalhedge",
      "author" : [ "Haipeng Luo", "Robert E Schapire" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory (COLT),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Learning and efficiency in games with dynamic population",
      "author" : [ "Thodoris Lykouris", "Vasilis Syrgkanis", "Éva Tardos" ],
      "venue" : "In Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "First-order regret bounds for combinatorial semi-bandits",
      "author" : [ "Gergely Neu" ],
      "venue" : "In Proceedings of the 27th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Online learning with predictable sequences",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Optimization, learning, and games with predictable sequences",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Intrinsic robustness of the price of anarchy",
      "author" : [ "Tim Roughgarden" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "The price of anarchy in auctions",
      "author" : [ "Tim Roughgarden", "Vasilis Syrgkanis", "Eva Tardos" ],
      "venue" : "Available at https://arxiv.org/abs/1607.07684,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "How bad is selfish routing",
      "author" : [ "Tim Roughgarden", "Eva Tardos" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2002
    }, {
      "title" : "Adaptivity and optimism: An improved exponentiated gradient algorithm",
      "author" : [ "Jacob Steinhardt", "Percy Liang" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning (ICML),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Incomplete information and internal regret in prediction of individual sequences",
      "author" : [ "Gilles Stoltz" ],
      "venue" : "PhD thesis, Universite Paris-Sud,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2005
    }, {
      "title" : "Fast convergence of regularized learning in games",
      "author" : [ "Vasilis Syrgkanis", "Alekh Agarwal", "Haipeng Luo", "Robert E Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Composable and efficient mechanisms",
      "author" : [ "Vasilis Syrgkanis", "Éva Tardos" ],
      "venue" : "In ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "How to better use expert advice",
      "author" : [ "Rani Yaroshinsky", "Ran El-Yaniv", "Steven S. Seiden" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "[19] in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, in a large class of games known as smooth games [23] they converge to outcomes with approximately optimal social welfare matching the worst-case efficiency loss of Nash equilibria (the price of anarchy).",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 27,
      "context" : "In smooth cost minimization games the overall cost is /(1 μ) times the minimum cost, while in smooth mechanisms [29] such as auctions it is /max(1, μ) times the maximum total utility (where and μ are parameters of the smoothness condition).",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "[12]) that ⇤ Cornell University {djfoster,teddlyk,sridharan,eva}@cs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "Taking advantage of the fact that playing a game against opponents who themselves are also using regret minimization is not a truly adversarial setting, a sequence of papers [9, 22, 28] showed that by using specific learning algorithms, the dependence on T of the convergence rate can be improved to O(1/T ) (“fast convergence”).",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 20,
      "context" : "Taking advantage of the fact that playing a game against opponents who themselves are also using regret minimization is not a truly adversarial setting, a sequence of papers [9, 22, 28] showed that by using specific learning algorithms, the dependence on T of the convergence rate can be improved to O(1/T ) (“fast convergence”).",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 26,
      "context" : "Taking advantage of the fact that playing a game against opponents who themselves are also using regret minimization is not a truly adversarial setting, a sequence of papers [9, 22, 28] showed that by using specific learning algorithms, the dependence on T of the convergence rate can be improved to O(1/T ) (“fast convergence”).",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 26,
      "context" : "[28] show that all algorithms satisfying the so-called RVU property (Regret by Variation in Utilities), which include Optimistic Mirror Descent [22], converge at a O(1/T ) rate with a fixed number of players.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[28] show that all algorithms satisfying the so-called RVU property (Regret by Variation in Utilities), which include Optimistic Mirror Descent [22], converge at a O(1/T ) rate with a fixed number of players.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 8,
      "context" : "One issue with the works of [9, 22, 28] is that they use expected cost as their feedback model for the players.",
      "startOffset" : 28,
      "endOffset" : 39
    }, {
      "referenceID" : 20,
      "context" : "One issue with the works of [9, 22, 28] is that they use expected cost as their feedback model for the players.",
      "startOffset" : 28,
      "endOffset" : 39
    }, {
      "referenceID" : 26,
      "context" : "One issue with the works of [9, 22, 28] is that they use expected cost as their feedback model for the players.",
      "startOffset" : 28,
      "endOffset" : 39
    }, {
      "referenceID" : 26,
      "context" : "We improve the result of [28] on the convergence to approximate optimality in smooth games in a number of different aspects.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 17,
      "context" : "• Our results apply to settings where the set of players in the game changes over time [19].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "This property is satisfied by many known algorithms including even the vanilla Hedge algorithm, as well as Optimistic Hedge [21, 28] (via a new analysis).",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "This property is satisfied by many known algorithms including even the vanilla Hedge algorithm, as well as Optimistic Hedge [21, 28] (via a new analysis).",
      "startOffset" : 124,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "[28] shows that players using Optimistic Hedge in a repeated game with n players converge to the approximately optimal outcome guaranteed by smoothness at a rate of O(n2/T ).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "Comparing our work to [28] with regard to feedback, Low Approximate Regret algorithms require only realized feedback, while the analysis of the RVU property in [28] requires expected feedback.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 26,
      "context" : "Comparing our work to [28] with regard to feedback, Low Approximate Regret algorithms require only realized feedback, while the analysis of the RVU property in [28] requires expected feedback.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 26,
      "context" : "those in [28]) will stay at the uniform distribution forever, because the expected cost vector distributes",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 26,
      "context" : "This gives low regret under expected costs, but suppose we were interested in realized costs: The only “black box” way to lift [28] to this case would be to simply evaluate the regret bound above under realized costs, but here players will experience ⇥(1/ p T ) variation because they select bins uniformly at random, ruining the fast convergence.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "[22] analyzed zero-sum games with bandit feedback, but assumed that players receive expected cost over the strategies of all other players.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "adaptations of SCRiBLe [21]) or would not extend to utility maximization settings (e.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : "Finally, in Section 5, we consider the dynamic population game setting of [19], where players enter and leave the game over time.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "[19] showed that regret bounds for shifting experts directly influence the rate at which players can turn over and still guarantee close to optimal solutions on average.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Such learning algorithms include a noisy version of Hedge as well as AdaNormalHedge [18], which was previously studied in the dynamic setting in [19].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "Such learning algorithms include a noisy version of Hedge as well as AdaNormalHedge [18], which was previously studied in the dynamic setting in [19].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "Low Approximate Regret allows us to increase the turnover rate from the one in [19], while also widening and simplifying the class of learning algorithms that players can use to guarantee the close to optimal average welfare.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 26,
      "context" : "This should be contrasted with the expectation feedback used by [28, 9, 22], where player i observes E",
      "startOffset" : 64,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : "This should be contrasted with the expectation feedback used by [28, 9, 22], where player i observes E",
      "startOffset" : 64,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "This should be contrasted with the expectation feedback used by [28, 9, 22], where player i observes E",
      "startOffset" : 64,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "Here we use the dynamic population model of [19], where at each round t each player i is replaced (“turns over”) with some probability p.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "Low Approximate Regret is analogous to the notion of ↵-regret from [15], with ↵ = (1 + ✏).",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "Smooth Games It is well-known that in a large class of games, termed smooth games by Roughgarden [23], traditional learning dynamics converge to approximately optimal social welfare.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : "(Smooth game [23]) A cost minimization game is called ( , μ)-smooth if for all strategy profiles s and s⇤: P",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "3 )-smooth [8], non-atomic games are (1, 0.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 23,
      "context" : "25) smooth [25], implying a price of anarchy of 2.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 27,
      "context" : "While we focus on cost-minimization games for simplicity of exposition, an analogous definition also applies for utility maximization, including smooth mechanisms [29], which we elaborate on in Appendix D.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 10,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 28,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "[11, 4, 30], Algorithm of [7], Variation MW [13], AEG-Path [26], AdaNormalHedge [18], Squint [16], and Optimistic PAC-Bayes [10].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 26,
      "context" : "The main result of [28] shows that learning algorithms that satisfy their RVU property converge to the price of anarchy bound /(1 μ) at rate n2 log d/T .",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 26,
      "context" : "In contrast, [28] only shows O(n/ p T ) individual regret and O(n3/T ) convergence to price of anarchy simultaneously.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 26,
      "context" : "Low Approximate Regret algorithms only need realized feedback, whereas [28] require expectation feedback.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 26,
      "context" : "RVU property [28] Expected costs exact O(n(2) log d/T ) d per round LAR property (section 2) Realized costs ✏-approx O(n log d/(✏T )) O(d) per round Figure 1: Comparison of Low Approximate Regret and RVU properties.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 25,
      "context" : "The Exp3Light algorithm [27] satisfies Low Approximate Regret with A(d, T ) = d2 log T .",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "The SCRiBLe algorithm introduced in [1] (via the analysis in [21]) enjoys the Low Approximate Regret property with A(d, T ) = d3 log(dT ).",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 19,
      "context" : "The SCRiBLe algorithm introduced in [1] (via the analysis in [21]) enjoys the Low Approximate Regret property with A(d, T ) = d3 log(dT ).",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "The GREEN algorithm [2] achieves the Low Approximate Regret property with A(d, T ) = d log(T ), but only works with costs and not gains.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "[21]) in that it uses the classical importance-weighted estimator for costs instead of sampling based on the Dikin elipsoid.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[3] provide a small loss bound for bandits which would be sufficient for Low Approximate Regret, but their algorithm requires prior knowledge on the loss of the best action (or a bound on it), which is not appropriate in our game setting.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "Similarly, the small loss bound in [20] is not applicable in our setting as the work assumes an oblivious adversary and so does not apply to the games we consider.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "5 Dynamic Population Games In this section we consider the dynamic population repeated game setting introduced in [19].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "In spite of this apparent difficulty, [19] prove that at least a ⇢ /(1 μ ✏) fraction of the optimal welfare is guaranteed if 1.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "players are using low adaptive regret algorithms (see [14, 18]) and 2.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "players are using low adaptive regret algorithms (see [14, 18]) and 2.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "Moreover, algorithms that satisfy a small loss version of the adaptive regret property [14] used in [19] satisfy the Strong Low Approximate Regret property.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Moreover, algorithms that satisfy a small loss version of the adaptive regret property [14] used in [19] satisfy the Strong Low Approximate Regret property.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "As discussed in [19], such stable sequences can come from simple greedy algorithms (where each change in the input of one player affects the output of few other players) or via differentially private algorithms (where each change in the input of one player affects the output of all other players with small probability); in the latter case the sequence is randomized.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "Combining Noisy Hedge with Proposition 8 strengthens the results in [19] by both weakening the behavioral assumption on the players, allowing them to use simpler learning algorithms, and allowing a higher turnover probability.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 17,
      "context" : "Comparison to Previous Results [19] use the more complex AdaNormalHedge algorithm of [18], which satisfies the adaptive regret property of [14], but has O(dT ) space complexity.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "Comparison to Previous Results [19] use the more complex AdaNormalHedge algorithm of [18], which satisfies the adaptive regret property of [14], but has O(dT ) space complexity.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "Comparison to Previous Results [19] use the more complex AdaNormalHedge algorithm of [18], which satisfies the adaptive regret property of [14], but has O(dT ) space complexity.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "Acknowledgements We thank Vasilis Syrgkanis for sharing his simulation software and the NIPS reviewers for pointing out the GREEN algorithm [2].",
      "startOffset" : 140,
      "endOffset" : 143
    } ],
    "year" : 2016,
    "abstractText" : "We show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a (1 + ✏)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms; it is satisfied even by the vanilla Hedge forecaster. Our results improve upon recent work of Syrgkanis et al. [28] in a number of ways. We require only that players observe payoffs under other players’ realized actions, as opposed to expected payoffs. We further show that convergence occurs with high probability, and show convergence under bandit feedback. Finally, we improve upon the speed of convergence by a factor of n, the number of players. Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work. Our framework applies to dynamic population games via a low approximate regret property for shifting experts. Here we strengthen the results of Lykouris et al. [19] in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved. In the bandit setting we present a new algorithm which provides a “small loss”-type bound with improved dependence on the number of actions in utility settings, and is both simple and efficient. This result may be of independent interest.",
    "creator" : null
  }
}