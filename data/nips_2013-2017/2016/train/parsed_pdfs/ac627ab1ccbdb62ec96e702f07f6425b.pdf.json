{
  "name" : "ac627ab1ccbdb62ec96e702f07f6425b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Unsupervised Domain Adaptation with Residual Transfer Networks",
    "authors" : [ "Mingsheng Long", "Han Zhu", "Jianmin Wang", "Michael I. Jordan" ],
    "emails" : [ "mingsheng@tsinghua.edu.cn,", "jimwang@tsinghua.edu.cn,", "zhuhan10@gmail.com,", "jordan@berkeley.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep networks have significantly improved the state of the art for a wide variety of machine-learning problems and applications. Unfortunately, these impressive gains in performance come only when massive amounts of labeled data are available for supervised training. Since manual labeling of sufficient training data for diverse application domains on-the-fly is often prohibitive, for problems short of labeled data, there is strong incentive to establishing effective algorithms to reduce the labeling consumption, typically by leveraging off-the-shelf labeled data from a different but related source domain. However, this learning paradigm suffers from the shift in data distributions across different domains, which poses a major obstacle in adapting predictive models for the target task [1].\nDomain adaptation [1] is machine learning under the shift between training and test distributions. A rich line of approaches to domain adaptation aim to bridge the source and target domains by learning domain-invariant feature representations without using target labels, so that the classifier learned from the source domain can be applied to the target domain. Recent studies have shown that deep networks can learn more transferable features for domain adaptation [2, 3], by disentangling explanatory factors of variations behind domains. Latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning which can extract domain-invariant representations [4, 5, 6, 7].\nThe previous deep domain adaptation methods work under the assumption that the source classifier can be directly transferred to the target domain upon the learned domain-invariant feature representations. This assumption is rather strong as in practical applications, it is often infeasible to check whether the source classifier and target classifier can be shared or not. Hence we focus in this paper on a more general, and safe, domain adaptation scenario in which the source classifier and target classifier differ\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nby a small perturbation function. The goal of this paper is to simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain by embedding the adaptations of both classifiers and features in a unified deep architecture.\nMotivated by the state of the art deep residual learning [8], winner of the ImageNet ILSVRC 2015 challenge, we propose a new Residual Transfer Network (RTN) approach to domain adaptation in deep networks which can simultaneously learn adaptive classifiers and transferable features. We relax the shared-classifier assumption made by previous methods and assume that the source and target classifiers differ by a small residual function. We enable classifier adaptation by plugging several layers into deep networks to explicitly learn the residual function with reference to the target classifier. In this way, the source classifier and target classifier can be bridged tightly in the back-propagation procedure. The target classifier is tailored to the target data better by exploiting the low-density separation criterion. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, and can be trained efficiently using standard back-propagation. Extensive evidence suggests that the RTN approach outperforms several state of art methods on standard domain adaptation benchmarks."
    }, {
      "heading" : "2 Related Work",
      "text" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16]. The main technical problem of domain adaptation is that the domain discrepancy in probability distributions of different domains should be formally reduced. Deep neural networks can learn abstract representations that disentangle different explanatory factors of variations behind data samples [17] and manifest invariant factors underlying different populations that transfer well from original tasks to similar novel tasks [3]. Thus deep neural networks have been explored for domain adaptation [18, 19, 15], multimodal and multi-task learning [16, 20], where significant performance gains have been witnessed relative to prior shallow transfer learning methods.\nHowever, recent advances show that deep networks can learn abstract feature representations that can only reduce, but not remove, the cross-domain discrepancy [18, 4]. Dataset shift has posed a bottleneck to the transferability of deep features, resulting in statistically unbounded risk for target tasks [21, 22]. Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation [4, 5, 6, 7]. They extend deep convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched [4, 5], or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm [6, 7]. While performance was significantly improved, these state of the art methods may be restricted by the assumption that under the learned domain-invariant feature representations, the source classifier can be directly transferred to the target domain. In particular, this assumption may not hold when the source classifier and target classifier cannot be shared. As theoretically studied in [22], when the combined error of the ideal joint hypothesis is large, then there is no single classifier that performs well on both source and target domains, so we cannot find a good target classifier by directly transferring from the source domain.\nThis work is primarily motivated by He et al. [8], the winner of the ImageNet ILSVRC 2015 challenge. They present deep residual learning to ease the training of very deep networks (hundreds of layers), termed residual nets. The residual nets explicitly reformulate the layers as learning residual functions ∆F (x) with reference to the layer inputs x, instead of directly learning the unreferenced functions F (x) = ∆F (x) + x. The method focuses on standard deep learning in which training data and test data are drawn from identical distributions, hence it cannot be directly applied to domain adaptation. In this paper, we propose to bridge the source classifier fS(x) and target classifier fT (x) by the residual layers such that the classifier mismatch across domains can be explicitly modeled by the residual functions ∆F (x) in a deep learning architecture. Although the idea of adapting source classifier to target domain by adding a perturbation function has been studied by [23, 24, 25], these methods require target labeled data to learn the perturbation function, which cannot be applied to unsupervised domain adaptation, the focus of this study. Another distinction is that their perturbation function is defined in the input space x, while the input to our residual function is the target classifier fT (x), which can capture the connection between the source and target classifiers more effectively."
    }, {
      "heading" : "3 Residual Transfer Networks",
      "text" : "In unsupervised domain adaptation problem, we are given a source domainDs = {(xsi , ysi )} ns i=1 of ns labeled examples and a target domain Dt = {xtj} nt j=1 of nt unlabeled examples. The source domain and target domain are sampled from different probability distributions p and q respectively, and p 6= q. The goal of this paper is to design a deep neural network that enables learning of transfer classifiers y = fs (x) and y = ft (x) to close the source-target discrepancy, such that the expected target risk Rt (ft) = E(x,y)∼q [ft (x) 6= y] can be bounded by leveraging the source domain supervised data. The challenge of unsupervised domain adaptation arises in that the target domain has no labeled data, while the source classifier fs trained on source domain Ds cannot be directly applied to the target domain Dt due to the distribution discrepancy p(x, y) 6= q(x, y). The distribution discrepancy may give rise to mismatches in both features and classifiers, i.e. p(x) 6= q(x) and fs(x) 6= ft(x). Both mismatches should be fixed by joint adaptation of features and classifiers to enable effective domain adaptation. Classifier adaptation is more difficult than feature adaptation because it is directly related to the labels but the target domain is fully unlabeled. Note that the state of the art deep feature adaptation methods [5, 6, 7] generally assume classifiers can be shared on adapted deep features. This paper assumes fs 6= ft and presents an end-to-end deep learning framework for classifier adaptation. Deep networks [17] can learn distributed, compositional, and abstract representations for natural data such as image and text. This paper addresses unsupervised domain adaptation within deep networks for jointly learning transferable features and adaptive classifiers. We extend deep convolutional networks (CNNs), i.e. AlexNet [26], to novel residual transfer networks (RTNs) as shown in Figure 1. Denote by fs(x) the source classifier, and the empirical error of CNN on source domain data Ds is\nmin fs\n1\nns ns∑ i=1 L (fs (x s i ) ,y s i ), (1)\nwhere L(·, ·) is the cross-entropy loss function. Based on the quantification study of feature transferability in deep convolutional networks [3], convolutional layers can learn generic features that are transferable across domains [3]. Hence we opt to fine-tune, instead of directly adapt, the features of convolutional layers when transferring pre-trained deep models from source domain to target domain."
    }, {
      "heading" : "3.1 Feature Adaptation",
      "text" : "Deep features learned by CNNs can disentangle explanatory factors of variations behind data distributions to boost knowledge transfer [19, 17]. However, the latest literature findings reveal that deep features can reduce, but not remove, the cross-domain distribution discrepancy [3], which motivates the state of the art deep feature adaptation methods [5, 6, 7]. Deep features in standard CNNs must eventually transition from general to specific along the network, and the transferability of features and classifiers will decrease when the cross-domain discrepancy increases [3]. In other words, the shifts in the data distributions linger even after multilayer feature abstractions. In this paper, we perform feature adaptation by matching the feature distributions of multiple layers ` ∈ L across domains. We reduce feature dimensions by adding a bottleneck layer fcb on top of the last feature layer of CNNs, and then fine-tune CNNs on source labeled examples such that the feature distributions of the source and target are made similar under new feature representations in multiple layers L = {fcb, fcc}, as shown in Figure 1. To adapt multiple feature layers effectively, we propose the tensor product between features of multiple layers to perform lossless multi-layer feature fusion, i.e. zsi , ⊗`∈Lxs`i and ztj , ⊗`∈Lxt`j . We then perform feature adaptation by minimizing the Maximum Mean Discrepancy (MMD) [27] between source and target domains using the fusion features (dubbed tensor MMD) as\nmin fs,ft DL (Ds,Dt) = ns∑ i=1 ns∑ j=1\nk ( zsi , z s j ) n2s + nt∑ i=1 nt∑ j=1 k ( zti, z t j ) n2t − 2 ns∑ i=1 nt∑ j=1 k ( zsi , z t j ) nsnt , (2)\nwhere the characteristic kernel k(z, z′) = e−‖vec(z)−vec(z ′)‖2/b is the Gaussian kernel function defined on the vectorization of tensors z and z′ with bandwidth parameter b. Different from DAN [5] that adapts multiple feature layers using multiple MMD penalties, this paper adapts multiple feature layers by first fusing them and then adapting the fused features. The advantage of our method against DAN [5] is that our method can capture full interactions across multilayer features and facilitate easier model selection, while DAN [5] needs |L| independent MMD penalties for adapting |L| layers."
    }, {
      "heading" : "3.2 Classifier Adaptation",
      "text" : "As feature adaptation cannot remove the mismatch in classification models, we further perform classifier adaptation to learn transfer classifiers that make domain adaptation more effective. Although the source classifier fs(x) and target classifier ft(x) are different, fs(x) 6= ft(x), they should be related to ensure the feasibility of domain adaptation. It is reasonable to assume that fs(x) and ft(x) differ only by a small perturbation function ∆f(x). Prior work on classifier adaptation [23, 24, 25] assumes that ft(x) = fs(x) + ∆f(x), where the perturbation ∆f(x) is a function of input feature x. However, these methods require target labeled data to learn the perturbation function, which cannot be applied to unsupervised domain adaptation where target domain has no labeled data. How to bridge fs(x) and ft(x) in a framework is a key challenge of unsupervised domain adaptation. We postulate that the perturbation function ∆f(x) can be learned jointly from the source labeled data and target unlabeled data, given that the source classifier and target classifier are properly connected.\nTo enable classifier adaptation, consider fitting F (x) as an original mapping by a few stacked layers (convolutional or fully connected layers) in Figure 1 (right), where x denotes the inputs to the first of these layers [8]. If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e. F (x)− x. Rather than expecting stacked layers to approximate F (x), one explicitly lets these layers approximate a residual function ∆F (x) , F (x)− x, with the original function being ∆F (x) + x. The operation ∆F (x) + x is performed by a shortcut connection and an element-wise addition, while the residual function is parameterized by residual layers within each residual block. Although both forms are able to asymptotically approximate the desired functions, the ease of learning is different. In reality, it is unlikely that identity mappings are optimal, but it should be easier to find the perturbations with reference to an identity mapping, than to learn the function as new. The residual learning is the key to the successful training of very deep networks. The deep residual network (ResNet) framework [8] bridges the inputs and outputs of the residual layers by the shortcut connection (identity mapping) such that F (x) = ∆F (x) + x, which eases the learning of residual function ∆F (x) (similar to the perturbation function across the source and target classifiers).\nBased on this key observation, we extend the CNN architectures (Figure 1, left) by plugging in the residual block (Figure 1, right). We reformulate the residual block to bridge the source classifier fS(x) and target classifier fT (x) by letting x , fT (x), F (x) , fS(x), and ∆F (x) , ∆f(x). Note that fS(x) is the outputs of the element-wise addition operator and fT (x) is the outputs of the targetclassifier layer fcc, both before softmax activation σ(·), fs (x) , σ (fS (x)) , ft (x) , σ (fT (x)). We can connect the source classifier and target classifier (before activation) by the residual block as\nfS (x) = fT (x) + ∆f (x) , (3)\nwhere we use functions fS and fT before softmax for residual block to ensure that the final classifiers fs and ft will output probabilities. Residual layers fc1–fc2 are fully-connected layers with c× c units, where c is the number of classes. We set the source classifier fS as the outputs of the residual block to make it better trainable from the source-labeled data by deep residual learning [8]. In other words, if we set fT as the outputs of the residual block, then we may be unable to learn it successfully as we do not have target labeled data and thus standard back-propagation will not work. Deep residual learning [8] ensures to output valid classifiers |∆f (x)| |fT (x)| ≈ |fS (x)|, and more importantly,\nmakes the perturbation function ∆f (x) dependent on both the target classifier fT (x) (due to the functional dependency) as well as the source classifier fS(x) (due to the back-propagation pipeline).\nAlthough we successfully cast the classifier adaptation into the residual learning framework while the residual learning framework tends to make the target classifier ft(x) not deviate much from the source classifier fs(x), we still cannot guarantee that ft(x) will fit the target-specific structures well. To address this problem, we further exploit the entropy minimization principle [28] for refining the classifier adaptation, which encourages the low-density separation between classes by minimizing the entropy of class-conditional distribution f tj (x t i) = p(y t i = j|xti; ft) on target domain data Dt as\nmin ft\n1\nnt nt∑ i=1 H ( ft ( xti )) , (4)\nwhere H(·) is the entropy function of class-conditional distribution ft (xti) defined as H (ft (xti)) = − ∑c j=1 f t j (x t i) log f t j (x t i), c is the number of classes, and f t j (x t i) is the probability of predicting point xti to class j. By minimizing entropy penalty (4), the target classifier ft(x) is made directly accessible to target-unlabeled data and will amend itself to pass through the target low-density regions."
    }, {
      "heading" : "3.3 Residual Transfer Network",
      "text" : "To enable effective unsupervised domain adaptation, we propose Residual Transfer Network (RTN), which jointly learns transferable features and adaptive classifiers by integrating deep feature learning (1), feature adaptation (2), and classifier adaptation (3)–(4) in an end-to-end deep learning framework,\nmin fS=fT+∆f\n1\nns ns∑ i=1 L (fs (x s i ) , y s i )\n+ γ\nnt nt∑ i=1 H ( ft ( xti ))\n+ λ DL (Ds,Dt),\n(5)\nwhere λ and γ are the tradeoff parameters for the tensor MMD penalty (2) and entropy penalty (4) respectively. The proposed RTN model (5) is enabled to learn both transferable features and adaptive classifiers. As classifier adaptation proposed in this paper and feature adaptation studied in [5, 6] are tailored to adapt different layers of deep networks, they can complement each other to establish better performance. Since training deep CNNs requires a large amount of labeled data that is prohibitive for many domain adaptation applications, we start with the CNN models pre-trained on ImageNet 2012 data and fine-tune it as [5]. The training of RTN mainly follows standard back-propagation, with the residual transfer layers for classifier adaptation as [8]. Note that, the optimization of tensor MMD penalty (2) requires carefully-designed algorithm to establish linear-time training, as detailed in [5]. We also adopt bilinear pooling [29] to reduce the dimensions of fusion features in tensor MMD (2)."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate the residual transfer network against state of the art transfer learning and deep learning methods. Codes and datasets will be available at https://github.com/thuml/transfer-caffe."
    }, {
      "heading" : "4.1 Setup",
      "text" : "Office-31 [13] is a benchmark for domain adaptation, comprising 4,110 images in 31 classes collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which contain images taken by web camera and digital SLR camera with different photographical settings, respectively. To enable unbiased evaluation, we evaluate all methods on all six transfer tasks A→W, D→W, W→ D, A→ D, D→ A and W→ A as in [5, 7]. Office-Caltech [14] is built by selecting the 10 common categories shared by Office-31 and Caltech256 (C), and is widely used by previous methods [14, 30]. We can build 12 transfer tasks: A→W, D→W, W→ D, A→ D, D→ A, W→ A, A→ C, W→ C, D→ C, C→ A, C→W, and C → D. While Office-31 has more categories and is more difficult for domain adaptation algorithms,\nOffice-Caltech provides more transfer tasks to enable an unbiased look at dataset bias [31]. We adopt DeCAF7 [2] features for shallow transfer methods and original images for deep adaptation methods.\nWe compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [9], Geodesic Flow Kernel (GFK) [14], Deep Convolutional Neural Network (AlexNet [26]), Deep Domain Confusion (DDC) [4], Deep Adaptation Network (DAN) [5], and Reverse Gradient (RevGrad) [6]. TCA is a conventional transfer learning method based on MMD-regularized Kernel PCA. GFK is a manifold learning method that interpolates across an infinite number of intermediate subspaces to bridge domains. DDC is the first method that maximizes domain invariance by adding to AlexNet an adaptation layer using linear-kernel MMD [27]. DAN learns more transferable features by embedding deep features of multiple task-specific layers to reproducing kernel Hilbert spaces (RKHSs) and matching different distributions optimally using multi-kernel MMD. RevGrad improves domain adaptation by making the source and target domains indistinguishable for a discriminative domain classifier via an adversarial training paradigm.\nTo go deeper with the efficacy of classifier adaptation (residual transfer block) and feature adaptation (tensor MMD module), we perform ablation study by evaluating several variants of RTN: (1) RTN (mmd), which adds the tensor MMD module to AlexNet; (2) RTN (mmd+ent), which further adds the entropy penalty to AlexNet; (3) RTN (mmd+ent+res), which further adds the residual module to AlexNet. Note that RTN (mmd) improves DAN [5] by replacing the multiple MMD penalties in DAN by a single tensor MMD penalty in RTN (mmd), which facilitates much easier parameter selection.\nWe follow standard protocols and use all labeled source data and all unlabeled target data for domain adaptation [5]. We compare average classification accuracy of each transfer task using three random experiments. For MMD-based methods (TCA, DDC, DAN, and RTN), we use Gaussian kernel with bandwidth b set to median pairwise squared distances on training data, i.e. median heuristic [27]. As there are no target labeled data in unsupervised domain adaptation, model selection proves difficult. For all methods, we perform cross-valuation on labeled source data to select candidate parameters, then conduct validation on transfer task A→W by requiring one labeled example per category from target domain W as the validation set, and fix the selected parameters throughout all transfer tasks.\nWe implement all deep methods based on the Caffe deep-learning framework, and fine-tune from Caffe-provided models of AlexNet [26] pre-trained on ImageNet. For RTN, We fine-tune all the feature layers, train bottleneck layer fcb, classifier layer fcc and residual layers fc1–fc2, all through standard back-propagation. Since these new layers are trained from scratch, we set their learning rate to be 10 times that of the other layers. We use mini-batch stochastic gradient descent (SGD) with momentum of 0.9 and the learning rate annealing strategy implemented in RevGrad [6]: the learning rate is not selected through a grid search due to high computational cost—it is adjusted during SGD using the following formula: ηp = η0(1+αp)β , where p is the training progress linearly changing from 0 to 1, η0 = 0.01, α = 10 and β = 0.75, which is optimized for low error on the source domain. As RTN can work stably across different transfer tasks, the MMD penalty parameter λ and entropy penalty γ are first selected on A→W and then fixed as λ = 0.3, γ = 0.3 for all other transfer tasks."
    }, {
      "heading" : "4.2 Results",
      "text" : "The classification accuracy results on the six transfer tasks of Office-31 are shown in Table 1, and the results on the twelve transfer tasks of Office-Caltech are shown in Table 2. The RTN model based on AlexNet (Figure 1) outperforms all comparison methods on most transfer tasks. In particular, RTN substantially improves the accuracy on hard transfer tasks, e.g. A→W and C→W, where the source and target domains are very different, and achieves comparable accuracy on easy transfer tasks, D→W and W→ D, where source and target domains are similar [13]. These results suggest that RTN is able to learn more adaptive classifiers and transferable features for safer domain adaptation.\nFrom the results, we can make interesting observations. (1) Standard deep-learning methods (AlexNet) perform comparably with traditional shallow transfer-learning methods with deep DeCAF7 features as input (TCA and GFK). The only difference between these two sets of methods is that AlexNet can take the advantage of supervised fine-tuning on the source-labeled data, while TCA and GFK can take benefits of their domain adaptation procedures. This result confirms the current practice that supervised fine-tuning is important for transferring source classifier to target domain [19], and sustains the recent discovery that deep neural networks learn abstract feature representation, which can only reduce, but not remove, the cross-domain discrepancy [3]. This reveals that the two worlds of deep\nlearning and domain adaptation cannot reinforce each other substantially in the two-step pipeline, which motivates carefully-designed deep adaptation architectures to unify them. (2) Deep-transfer learning methods that reduce the domain discrepancy by domain-adaptive deep networks (DDC, DAN and RevGrad) substantially outperform standard deep learning methods (AlexNet) and traditional shallow transfer-learning methods with deep features as the input (TCA and GFK). This confirms that incorporating domain-adaptation modules into deep networks can improve domain adaptation performance. By adapting source-target distributions in multiple task-specific layers using optimal multi-kernel two-sample matching, DAN performs the best in general among the prior deep-transfer learning methods. (3) The proposed residual transfer network (RTN) performs the best and sets up a new state of the art result on these benchmark datasets. Different from all the previous deep-transfer learning methods that only adapt the feature layers of deep neural networks to learn more transferable features, RTN further adapts the classifier layers to bridge the source and target classifiers in an end-to-end residual learning framework, which can correct the classifier mismatch more effectively.\nTo go deeper into different modules of RTN, we show the results of RTN variants in Tables 1 and 2. (1) RTN (mmd) slightly outperforms DAN, but RTN (mmd) has only one MMD penalty parameter while DAN has two or three. Thus the proposed tensor MMD module is effective for adapting multiple feature layers using a single MMD penalty, which is important for easy model selection. (2) RTN (mmd+ent) performs substantially better than RTN (mmd). This highlights the importance of entropy minimization for low-density separation, which exploits the cluster structure of target-unlabeled data such that the target-classifier can be better adapted to the target data. (3) RTN (mmd+ent+res) performs the best across all variants. This highlights the importance of residual transfer of classifier layers for learning more adaptive classifiers. This is critical as in practical applications, there is no guarantee that the source classifier and target classifier can be safely shared. It is worth noting that, the entropy penalty and the residual module should be used together, otherwise the residual function tends to learn useless zero mapping such that the source and target classifiers are nearly identical [8]."
    }, {
      "heading" : "4.3 Discussion",
      "text" : "Predictions Visualization: We respectively visualize in Figures 2(a)–2(d) the t-SNE embeddings [2] of the predictions by DAN and RTN on transfer task A→W. We can make the following observations. (1) The predictions made by DAN in Figure 2(a)–2(b) show that the target categories are not well discriminated by the source classifier, which implies that target data is not well compatible with the source classifier. Hence the source and target classifiers should not be assumed to be identical, which has been a common assumption made by all prior deep domain adaptation methods [4, 5, 6, 7]. (2) The predictions made by RTN in Figures 2(c)–2(d) show that the target categories are discriminated\nbetter by the target classifier (larger class-to-class distances), which suggests that residual transfer of classifiers is a reasonable extension to previous deep feature adaptation methods. RTN simultaneously learns more adaptive classifiers and more transferable features to enable effective domain adaptation.\nLayer Responses: We show in Figure 3(a) the means and standard deviations of the layer responses [8], which are the outputs of fT (x) (fcc layer), ∆f(x) (fc2 layer), and fS(x) (after element-wise sum operator), respectively. This exposes the response strength of the residual functions. The results show that the residual function ∆f(x) have generally much smaller responses than the shortcut function fT (x). These results support our motivation that the residual functions are generally smaller than the non-residual functions, as they characterize the small gap between the source classifier and target classifier. The small residual function can be learned effectively via deep residual learning [8].\nClassifier Shift: To justify that there exists a classifier shift between source classifier fs and target classifier ft, we train fs on source domain and ft on target domain, both provided with labeled data. By taking A as source domain and W as target domain, the weight parameters of the classifiers (e.g. softmax regression) are shown in Figure 3(b), which shows that fs and ft are substantially different.\nParameter Sensitivity: We check the sensitivity of entropy parameter γ on transfer tasks A→W (31 classes) and C→W (10 classes) by varying the parameter in {0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1.0}. The results are shown in Figures 3(c), with the best results of the baselines shown as dashed lines. The accuracy of RTN first increases and then decreases as γ varies and demonstrates a desirable bell-shaped curve. This justifies our motivation of jointly learning transferable features and adaptive classifiers by the RTN model, as a good trade-off between them can promote transfer performance."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper presented a novel approach to unsupervised domain adaptation in deep networks, which enables end-to-end learning of adaptive classifiers and transferable features. Similar to many prior domain adaptation techniques, feature adaptation is achieved by matching the distributions of features across domains. However, unlike previous work, the proposed approach also supports classifier adaptation, which is implemented through a new residual transfer module that bridges the source classifier and target classifier. This makes the approach a good complement to existing techniques. The approach can be trained by standard back-propagation, which is scalable and can be implemented by most deep learning package. Future work constitutes semi-supervised domain adaptation extensions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Natural Science Foundation of China (61502265, 61325008), National Key R&D Program of China (2016YFB1000701, 2015BAF32B01), and TNList Key Project."
    } ],
    "references" : [ {
      "title" : "A survey on transfer learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "TKDE, 22(10):1345–1359,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Decaf: A deep convolutional activation feature for generic visual recognition",
      "author" : [ "J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell" ],
      "venue" : "ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "How transferable are features in deep neural networks",
      "author" : [ "J. Yosinski", "J. Clune", "Y. Bengio", "H. Lipson" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Deep domain confusion: Maximizing for domain invariance",
      "author" : [ "E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Learning transferable features with deep adaptation networks",
      "author" : [ "M. Long", "Y. Cao", "J. Wang", "M.I. Jordan" ],
      "venue" : "ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Unsupervised domain adaptation by backpropagation",
      "author" : [ "Y. Ganin", "V. Lempitsky" ],
      "venue" : "ICML,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Simultaneous deep transfer across domains and tasks",
      "author" : [ "E. Tzeng", "J. Hoffman", "N. Zhang", "K. Saenko", "T. Darrell" ],
      "venue" : "ICCV,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "K. He", "X. Zhang", "S. Ren", "J. Sun" ],
      "venue" : "CVPR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Domain adaptation via transfer component analysis",
      "author" : [ "S.J. Pan", "I.W. Tsang", "J.T. Kwok", "Q. Yang" ],
      "venue" : "TNNLS, 22(2):199–210,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Domain transfer multiple kernel learning",
      "author" : [ "L. Duan", "I.W. Tsang", "D. Xu" ],
      "venue" : "TPAMI, 34(3):465–479,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Domain adaptation under target and conditional shift",
      "author" : [ "K. Zhang", "B. Schölkopf", "K. Muandet", "Z. Wang" ],
      "venue" : "ICML,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Flexible transfer learning under support and model shift",
      "author" : [ "X. Wang", "J. Schneider" ],
      "venue" : "NIPS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adapting visual category models to new domains",
      "author" : [ "K. Saenko", "B. Kulis", "M. Fritz", "T. Darrell" ],
      "venue" : "ECCV,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Geodesic flow kernel for unsupervised domain adaptation",
      "author" : [ "B. Gong", "Y. Shi", "F. Sha", "K. Grauman" ],
      "venue" : "CVPR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "LSDA: Large scale detection through adaptation",
      "author" : [ "J. Hoffman", "S. Guadarrama", "E. Tzeng", "R. Hu", "J. Donahue", "R. Girshick", "T. Darrell", "K. Saenko" ],
      "venue" : "NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "JMLR, 12:2493–2537,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A. Courville", "P. Vincent" ],
      "venue" : "TPAMI, 35(8):1798–1828,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "ICML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning and transferring mid-level image representations using convolutional neural networks",
      "author" : [ "M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic" ],
      "venue" : "CVPR, June",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multimodal deep learning",
      "author" : [ "J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng" ],
      "venue" : "ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Domain adaptation: Learning bounds and algorithms",
      "author" : [ "Y. Mansour", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : "COLT,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan" ],
      "venue" : "MLJ, 79(1-2):151–175,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Cross-domain video concept detection using adaptive svms",
      "author" : [ "J. Yang", "R. Yan", "A.G. Hauptmann" ],
      "venue" : "MM, pages 188–197. ACM,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Domain adaptation from multiple sources via auxiliary classifiers",
      "author" : [ "Lixin Duan", "Ivor W Tsang", "Dong Xu", "Tat-Seng Chua" ],
      "venue" : "In ICML,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Visual event recognition in videos by learning from web data",
      "author" : [ "L. Duan", "D. Xu", "I.W. Tsang", "J. Luo" ],
      "venue" : "TPAMI, 34(9):1667–1680,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "A. Gretton", "K. Borgwardt", "M. Rasch", "B. Schölkopf", "A. Smola" ],
      "venue" : "JMLR, 13:723–773, March",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Semi-supervised learning by entropy minimization",
      "author" : [ "Y. Grandvalet", "Y. Bengio" ],
      "venue" : "NIPS,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Bilinear cnn models for fine-grained visual recognition",
      "author" : [ "Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Return of frustratingly easy domain adaptation",
      "author" : [ "B. Sun", "J. Feng", "K. Saenko" ],
      "venue" : "AAAI,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Unbiased look at dataset bias",
      "author" : [ "A. Torralba", "A.A. Efros" ],
      "venue" : "CVPR,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "However, this learning paradigm suffers from the shift in data distributions across different domains, which poses a major obstacle in adapting predictive models for the target task [1].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "Domain adaptation [1] is machine learning under the shift between training and test distributions.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "Recent studies have shown that deep networks can learn more transferable features for domain adaptation [2, 3], by disentangling explanatory factors of variations behind domains.",
      "startOffset" : 104,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "Recent studies have shown that deep networks can learn more transferable features for domain adaptation [2, 3], by disentangling explanatory factors of variations behind domains.",
      "startOffset" : 104,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning which can extract domain-invariant representations [4, 5, 6, 7].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 4,
      "context" : "Latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning which can extract domain-invariant representations [4, 5, 6, 7].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 5,
      "context" : "Latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning which can extract domain-invariant representations [4, 5, 6, 7].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "Latest advances have been achieved by embedding domain adaptation in the pipeline of deep feature learning which can extract domain-invariant representations [4, 5, 6, 7].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 7,
      "context" : "Motivated by the state of the art deep residual learning [8], winner of the ImageNet ILSVRC 2015 challenge, we propose a new Residual Transfer Network (RTN) approach to domain adaptation in deep networks which can simultaneously learn adaptive classifiers and transferable features.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 147,
      "endOffset" : 162
    }, {
      "referenceID" : 9,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 147,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 147,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 147,
      "endOffset" : 162
    }, {
      "referenceID" : 12,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 180,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 180,
      "endOffset" : 192
    }, {
      "referenceID" : 14,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 180,
      "endOffset" : 192
    }, {
      "referenceID" : 15,
      "context" : "Domain adaptation [1] builds models that can bridge different domains or tasks, which mitigates the burden of manual labeling for machine learning [9, 10, 11, 12], computer vision [13, 14, 15] and natural language processing [16].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 16,
      "context" : "Deep neural networks can learn abstract representations that disentangle different explanatory factors of variations behind data samples [17] and manifest invariant factors underlying different populations that transfer well from original tasks to similar novel tasks [3].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "Deep neural networks can learn abstract representations that disentangle different explanatory factors of variations behind data samples [17] and manifest invariant factors underlying different populations that transfer well from original tasks to similar novel tasks [3].",
      "startOffset" : 268,
      "endOffset" : 271
    }, {
      "referenceID" : 17,
      "context" : "Thus deep neural networks have been explored for domain adaptation [18, 19, 15], multimodal and multi-task learning [16, 20], where significant performance gains have been witnessed relative to prior shallow transfer learning methods.",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 18,
      "context" : "Thus deep neural networks have been explored for domain adaptation [18, 19, 15], multimodal and multi-task learning [16, 20], where significant performance gains have been witnessed relative to prior shallow transfer learning methods.",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "Thus deep neural networks have been explored for domain adaptation [18, 19, 15], multimodal and multi-task learning [16, 20], where significant performance gains have been witnessed relative to prior shallow transfer learning methods.",
      "startOffset" : 67,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : "Thus deep neural networks have been explored for domain adaptation [18, 19, 15], multimodal and multi-task learning [16, 20], where significant performance gains have been witnessed relative to prior shallow transfer learning methods.",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "Thus deep neural networks have been explored for domain adaptation [18, 19, 15], multimodal and multi-task learning [16, 20], where significant performance gains have been witnessed relative to prior shallow transfer learning methods.",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "However, recent advances show that deep networks can learn abstract feature representations that can only reduce, but not remove, the cross-domain discrepancy [18, 4].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : "However, recent advances show that deep networks can learn abstract feature representations that can only reduce, but not remove, the cross-domain discrepancy [18, 4].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "Dataset shift has posed a bottleneck to the transferability of deep features, resulting in statistically unbounded risk for target tasks [21, 22].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 21,
      "context" : "Dataset shift has posed a bottleneck to the transferability of deep features, resulting in statistically unbounded risk for target tasks [21, 22].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation [4, 5, 6, 7].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation [4, 5, 6, 7].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation [4, 5, 6, 7].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 6,
      "context" : "Some recent work addresses the aforementioned problem by deep domain adaptation, which bridges the two worlds of deep learning and domain adaptation [4, 5, 6, 7].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "They extend deep convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched [4, 5], or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm [6, 7].",
      "startOffset" : 182,
      "endOffset" : 188
    }, {
      "referenceID" : 4,
      "context" : "They extend deep convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched [4, 5], or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm [6, 7].",
      "startOffset" : 182,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "They extend deep convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched [4, 5], or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm [6, 7].",
      "startOffset" : 373,
      "endOffset" : 379
    }, {
      "referenceID" : 6,
      "context" : "They extend deep convolutional networks (CNNs) to domain adaptation either by adding one or multiple adaptation layers through which the mean embeddings of distributions are matched [4, 5], or by adding a fully connected subnetwork as a domain discriminator whilst the deep features are learned to confuse the domain discriminator in a domain-adversarial training paradigm [6, 7].",
      "startOffset" : 373,
      "endOffset" : 379
    }, {
      "referenceID" : 21,
      "context" : "As theoretically studied in [22], when the combined error of the ideal joint hypothesis is large, then there is no single classifier that performs well on both source and target domains, so we cannot find a good target classifier by directly transferring from the source domain.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "[8], the winner of the ImageNet ILSVRC 2015 challenge.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 22,
      "context" : "Although the idea of adapting source classifier to target domain by adding a perturbation function has been studied by [23, 24, 25], these methods require target labeled data to learn the perturbation function, which cannot be applied to unsupervised domain adaptation, the focus of this study.",
      "startOffset" : 119,
      "endOffset" : 131
    }, {
      "referenceID" : 23,
      "context" : "Although the idea of adapting source classifier to target domain by adding a perturbation function has been studied by [23, 24, 25], these methods require target labeled data to learn the perturbation function, which cannot be applied to unsupervised domain adaptation, the focus of this study.",
      "startOffset" : 119,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "Although the idea of adapting source classifier to target domain by adding a perturbation function has been studied by [23, 24, 25], these methods require target labeled data to learn the perturbation function, which cannot be applied to unsupervised domain adaptation, the focus of this study.",
      "startOffset" : 119,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "Note that the state of the art deep feature adaptation methods [5, 6, 7] generally assume classifiers can be shared on adapted deep features.",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "Note that the state of the art deep feature adaptation methods [5, 6, 7] generally assume classifiers can be shared on adapted deep features.",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 6,
      "context" : "Note that the state of the art deep feature adaptation methods [5, 6, 7] generally assume classifiers can be shared on adapted deep features.",
      "startOffset" : 63,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : "Deep networks [17] can learn distributed, compositional, and abstract representations for natural data such as image and text.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 25,
      "context" : "AlexNet [26], to novel residual transfer networks (RTNs) as shown in Figure 1.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : "Based on the quantification study of feature transferability in deep convolutional networks [3], convolutional layers can learn generic features that are transferable across domains [3].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "Based on the quantification study of feature transferability in deep convolutional networks [3], convolutional layers can learn generic features that are transferable across domains [3].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "Deep features learned by CNNs can disentangle explanatory factors of variations behind data distributions to boost knowledge transfer [19, 17].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "Deep features learned by CNNs can disentangle explanatory factors of variations behind data distributions to boost knowledge transfer [19, 17].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "However, the latest literature findings reveal that deep features can reduce, but not remove, the cross-domain distribution discrepancy [3], which motivates the state of the art deep feature adaptation methods [5, 6, 7].",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "However, the latest literature findings reveal that deep features can reduce, but not remove, the cross-domain distribution discrepancy [3], which motivates the state of the art deep feature adaptation methods [5, 6, 7].",
      "startOffset" : 210,
      "endOffset" : 219
    }, {
      "referenceID" : 5,
      "context" : "However, the latest literature findings reveal that deep features can reduce, but not remove, the cross-domain distribution discrepancy [3], which motivates the state of the art deep feature adaptation methods [5, 6, 7].",
      "startOffset" : 210,
      "endOffset" : 219
    }, {
      "referenceID" : 6,
      "context" : "However, the latest literature findings reveal that deep features can reduce, but not remove, the cross-domain distribution discrepancy [3], which motivates the state of the art deep feature adaptation methods [5, 6, 7].",
      "startOffset" : 210,
      "endOffset" : 219
    }, {
      "referenceID" : 2,
      "context" : "Deep features in standard CNNs must eventually transition from general to specific along the network, and the transferability of features and classifiers will decrease when the cross-domain discrepancy increases [3].",
      "startOffset" : 212,
      "endOffset" : 215
    }, {
      "referenceID" : 26,
      "context" : "We then perform feature adaptation by minimizing the Maximum Mean Discrepancy (MMD) [27] between source and target domains using the fusion features (dubbed tensor MMD) as",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "Different from DAN [5] that adapts multiple feature layers using multiple MMD penalties, this paper adapts multiple feature layers by first fusing them and then adapting the fused features.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "The advantage of our method against DAN [5] is that our method can capture full interactions across multilayer features and facilitate easier model selection, while DAN [5] needs |L| independent MMD penalties for adapting |L| layers.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "The advantage of our method against DAN [5] is that our method can capture full interactions across multilayer features and facilitate easier model selection, while DAN [5] needs |L| independent MMD penalties for adapting |L| layers.",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 22,
      "context" : "Prior work on classifier adaptation [23, 24, 25] assumes that ft(x) = fs(x) + ∆f(x), where the perturbation ∆f(x) is a function of input feature x.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "Prior work on classifier adaptation [23, 24, 25] assumes that ft(x) = fs(x) + ∆f(x), where the perturbation ∆f(x) is a function of input feature x.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "Prior work on classifier adaptation [23, 24, 25] assumes that ft(x) = fs(x) + ∆f(x), where the perturbation ∆f(x) is a function of input feature x.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "To enable classifier adaptation, consider fitting F (x) as an original mapping by a few stacked layers (convolutional or fully connected layers) in Figure 1 (right), where x denotes the inputs to the first of these layers [8].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 7,
      "context" : "The deep residual network (ResNet) framework [8] bridges the inputs and outputs of the residual layers by the shortcut connection (identity mapping) such that F (x) = ∆F (x) + x, which eases the learning of residual function ∆F (x) (similar to the perturbation function across the source and target classifiers).",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "We set the source classifier fS as the outputs of the residual block to make it better trainable from the source-labeled data by deep residual learning [8].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : "Deep residual learning [8] ensures to output valid classifiers |∆f (x)| |fT (x)| ≈ |fS (x)|, and more importantly,",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 27,
      "context" : "To address this problem, we further exploit the entropy minimization principle [28] for refining the classifier adaptation, which encourages the low-density separation between classes by minimizing the entropy of class-conditional distribution f t j (x t i) = p(y t i = j|xi; ft) on target domain data Dt as",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "As classifier adaptation proposed in this paper and feature adaptation studied in [5, 6] are tailored to adapt different layers of deep networks, they can complement each other to establish better performance.",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "As classifier adaptation proposed in this paper and feature adaptation studied in [5, 6] are tailored to adapt different layers of deep networks, they can complement each other to establish better performance.",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "Since training deep CNNs requires a large amount of labeled data that is prohibitive for many domain adaptation applications, we start with the CNN models pre-trained on ImageNet 2012 data and fine-tune it as [5].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 7,
      "context" : "The training of RTN mainly follows standard back-propagation, with the residual transfer layers for classifier adaptation as [8].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "Note that, the optimization of tensor MMD penalty (2) requires carefully-designed algorithm to establish linear-time training, as detailed in [5].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 28,
      "context" : "We also adopt bilinear pooling [29] to reduce the dimensions of fusion features in tensor MMD (2).",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 12,
      "context" : "Office-31 [13] is a benchmark for domain adaptation, comprising 4,110 images in 31 classes collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "To enable unbiased evaluation, we evaluate all methods on all six transfer tasks A→W, D→W, W→ D, A→ D, D→ A and W→ A as in [5, 7].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "To enable unbiased evaluation, we evaluate all methods on all six transfer tasks A→W, D→W, W→ D, A→ D, D→ A and W→ A as in [5, 7].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "Office-Caltech [14] is built by selecting the 10 common categories shared by Office-31 and Caltech256 (C), and is widely used by previous methods [14, 30].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "Office-Caltech [14] is built by selecting the 10 common categories shared by Office-31 and Caltech256 (C), and is widely used by previous methods [14, 30].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 29,
      "context" : "Office-Caltech [14] is built by selecting the 10 common categories shared by Office-31 and Caltech256 (C), and is widely used by previous methods [14, 30].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 30,
      "context" : "Office-Caltech provides more transfer tasks to enable an unbiased look at dataset bias [31].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "We adopt DeCAF7 [2] features for shallow transfer methods and original images for deep adaptation methods.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [9], Geodesic Flow Kernel (GFK) [14], Deep Convolutional Neural Network (AlexNet [26]), Deep Domain Confusion (DDC) [4], Deep Adaptation Network (DAN) [5], and Reverse Gradient (RevGrad) [6].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [9], Geodesic Flow Kernel (GFK) [14], Deep Convolutional Neural Network (AlexNet [26]), Deep Domain Confusion (DDC) [4], Deep Adaptation Network (DAN) [5], and Reverse Gradient (RevGrad) [6].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 25,
      "context" : "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [9], Geodesic Flow Kernel (GFK) [14], Deep Convolutional Neural Network (AlexNet [26]), Deep Domain Confusion (DDC) [4], Deep Adaptation Network (DAN) [5], and Reverse Gradient (RevGrad) [6].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 3,
      "context" : "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [9], Geodesic Flow Kernel (GFK) [14], Deep Convolutional Neural Network (AlexNet [26]), Deep Domain Confusion (DDC) [4], Deep Adaptation Network (DAN) [5], and Reverse Gradient (RevGrad) [6].",
      "startOffset" : 254,
      "endOffset" : 257
    }, {
      "referenceID" : 4,
      "context" : "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [9], Geodesic Flow Kernel (GFK) [14], Deep Convolutional Neural Network (AlexNet [26]), Deep Domain Confusion (DDC) [4], Deep Adaptation Network (DAN) [5], and Reverse Gradient (RevGrad) [6].",
      "startOffset" : 289,
      "endOffset" : 292
    }, {
      "referenceID" : 5,
      "context" : "We compare with both conventional and the state of the art transfer learning and deep learning methods: Transfer Component Analysis (TCA) [9], Geodesic Flow Kernel (GFK) [14], Deep Convolutional Neural Network (AlexNet [26]), Deep Domain Confusion (DDC) [4], Deep Adaptation Network (DAN) [5], and Reverse Gradient (RevGrad) [6].",
      "startOffset" : 325,
      "endOffset" : 328
    }, {
      "referenceID" : 26,
      "context" : "DDC is the first method that maximizes domain invariance by adding to AlexNet an adaptation layer using linear-kernel MMD [27].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "Note that RTN (mmd) improves DAN [5] by replacing the multiple MMD penalties in DAN by a single tensor MMD penalty in RTN (mmd), which facilitates much easier parameter selection.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "We follow standard protocols and use all labeled source data and all unlabeled target data for domain adaptation [5].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 25,
      "context" : "We implement all deep methods based on the Caffe deep-learning framework, and fine-tune from Caffe-provided models of AlexNet [26] pre-trained on ImageNet.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "9 and the learning rate annealing strategy implemented in RevGrad [6]: the learning rate is not selected through a grid search due to high computational cost—it is adjusted during SGD using the following formula: ηp = η0 (1+αp) , where p is the training progress linearly changing from 0 to 1, η0 = 0.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "A→W and C→W, where the source and target domains are very different, and achieves comparable accuracy on easy transfer tasks, D→W and W→ D, where source and target domains are similar [13].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "This result confirms the current practice that supervised fine-tuning is important for transferring source classifier to target domain [19], and sustains the recent discovery that deep neural networks learn abstract feature representation, which can only reduce, but not remove, the cross-domain discrepancy [3].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 2,
      "context" : "This result confirms the current practice that supervised fine-tuning is important for transferring source classifier to target domain [19], and sustains the recent discovery that deep neural networks learn abstract feature representation, which can only reduce, but not remove, the cross-domain discrepancy [3].",
      "startOffset" : 308,
      "endOffset" : 311
    }, {
      "referenceID" : 4,
      "context" : "Table 1: Accuracy on Office-31 dataset using standard protocol [5] for unsupervised adaptation.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "Method A→W D→W W→ D A→ D D→ A W→ A Avg TCA [9] 59.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 4,
      "context" : "Table 2: Accuracy on Office-Caltech dataset using standard protocol [5] for unsupervised adaptation.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "Method A→W D→W W→D A→D D→A W→A A→C W→C D→C C→A C→W C→D Avg TCA [9] 84.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "It is worth noting that, the entropy penalty and the residual module should be used together, otherwise the residual function tends to learn useless zero mapping such that the source and target classifiers are nearly identical [8].",
      "startOffset" : 227,
      "endOffset" : 230
    }, {
      "referenceID" : 1,
      "context" : "Predictions Visualization: We respectively visualize in Figures 2(a)–2(d) the t-SNE embeddings [2] of the predictions by DAN and RTN on transfer task A→W.",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "Hence the source and target classifiers should not be assumed to be identical, which has been a common assumption made by all prior deep domain adaptation methods [4, 5, 6, 7].",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "Hence the source and target classifiers should not be assumed to be identical, which has been a common assumption made by all prior deep domain adaptation methods [4, 5, 6, 7].",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 5,
      "context" : "Hence the source and target classifiers should not be assumed to be identical, which has been a common assumption made by all prior deep domain adaptation methods [4, 5, 6, 7].",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 6,
      "context" : "Hence the source and target classifiers should not be assumed to be identical, which has been a common assumption made by all prior deep domain adaptation methods [4, 5, 6, 7].",
      "startOffset" : 163,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "Layer Responses: We show in Figure 3(a) the means and standard deviations of the layer responses [8], which are the outputs of fT (x) (fcc layer), ∆f(x) (fc2 layer), and fS(x) (after element-wise sum operator), respectively.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "The small residual function can be learned effectively via deep residual learning [8].",
      "startOffset" : 82,
      "endOffset" : 85
    } ],
    "year" : 2016,
    "abstractText" : "The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.",
    "creator" : null
  }
}