{
  "name" : "7b4773c039d539af17c883eb9283dd14.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning",
    "authors" : [ "Jean-Bastien Grill", "Michal Valko", "Rémi Munos" ],
    "emails" : [ "jean-bastien.grill@inria.fr", "michal.valko@inria.fr", "munos@google.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of sampling-based planning in a Markov decision process (MDP) when a generative model (oracle) is available. This approach, also called Monte-Carlo planning or MonteCarlo tree search (see e.g., [12]), has been popularized in the game of computer Go [7, 8, 15] and shown impressive performance in many other high dimensional control and game problems [4]. In the present paper, we provide a sample complexity analysis of a new algorithm called TrailBlazer.\nOur assumption about the MDP is that we possess a generative model which can be called from any state-action pair to generate rewards and transition samples. Since making a call to this generative model has a cost, be it a numerical cost expressed in CPU time (in simulated environments) or a financial cost (in real domains), our goal is to use this model as parsimoniously as possible.\nFollowing dynamic programming [2], planning can be reduced to an approximation of the (optimal) value function, defined as the maximum of the expected sum of discounted rewards: E [∑ t≥0 γ trt ] , where γ ∈ [0, 1) is a known discount factor. Indeed, if an ε-optimal approximation of the value function at any state-action pair is available, then the policy corresponding to selecting in each state the action with the highest approximated value will be O (ε/ (1− γ))-optimal [3]. Consequently, in this paper, we focus on a near-optimal approximation of the value function for a single given state (or state-action pair). In order to assess the performance of our algorithm we measure its sample complexity defined as the number of oracle calls, given that we guarantee its consistency, i.e., that with probability at least 1− δ, TrailBlazer returns an ε-approximation of the value function as required by the probably approximately correct (PAC) framework.\n∗on the leave from SequeL team, INRIA Lille - Nord Europe, France\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nWe use a tree representation to represent the set of states that are reachable from any initial state. This tree alternates maximum (MAX) nodes (corresponding to actions) and average (AVG) nodes (corresponding to the random transition to next states). We assume the number K of actions is finite. However, the number N of possible next states is either finite or infinite (which may be the case when the state space is infinite), and we will report results in both the finite N and the infinite case. The root node of this planning tree represents the current state (or a state-action) of the MDP and its value is the maximum (over all policies defined at MAX nodes) of the corresponding expected sum of discounted rewards. Notice that by using a tree representation, we do not use the property that some state of the MDP can be reached by different paths (sequences of states-actions). Therefore, this state will be represented by different nodes in the tree. We could potentially merge such duplicates to form a graph instead. However, for simplicity, we choose not to merge these duplicates and keep a tree, which could make the planning problem harder. To sum up, our goal is to return, with probability 1− δ, an ε-accurate value of the root node of this planning tree while using as low number of calls to the oracle as possible. Our contribution is an algorithm called TrailBlazer whose sampling strategy depends on the specific structure of the MDP and for which we provide sample complexity bounds in terms of a new problem-dependent measure of the quantity of near-optimal nodes. Before describing our contribution in more detail we first relate our setting to what has been around."
    }, {
      "heading" : "1.1 Related work",
      "text" : "In this section we focus on the dependency between ε and the sample complexity and all bound of the style 1/εc are up to a poly-logarithmic multiplicative factor not indicated for clarity. Kocsis and Szepesvári [12] introduced the UCT algorithm (upper-confidence bounds for trees). UCT is efficient in computer Go [7, 8, 15] and a number of other control and game problems [4]. UCT is based on generating trajectories by selecting in each MAX node the action that has the highest upper-confidence bound (computed according to the UCB algorithm of Auer et al. [1]). UCT converges asymptotically to the optimal solution, but its sample complexity can be worst than doubly-exponential in (1/ε) for some MDPs [13]. One reason for this is that the algorithm can expand very deeply the apparently best branches but may lack sufficient exploration, especially when a narrow optimal path is hidden in a suboptimal branch. As a result, this approach works well in some problems with a specific structure but may be much worse than a uniform sampling in other problems.\nOn the other hand, a uniform planning approach is safe for all problems. Kearns et al. [11] generate a sparse look-ahead tree based on expanding all MAX nodes and sampling a finite number of children from AVG nodes up to a fixed depth that depends on the desired accuracy ε. Their sample complexity is2 of the order of (1/ε)log(1/ε), which is non-polynomial in 1/ε. This bound is better than that for UCT in a worst-case sense. However, as their look-ahead tree is built in a uniform and non-adaptive way, this algorithm fails to benefit from a potentially favorable structure of the MDP.\nAn improved version of this sparse-sampling algorithm by Walsh et al. [17] cuts suboptimal branches in an adaptive way but unfortunately does not come with an improved bound and stays non-polynomial even in the simple Monte Carlo setting for which K = 1.\nAlthough the sample complexity is certainly non-polynomial in the worst case, it can be polynomial in some specific problems. First, for the case of finite N , the sample complexity is polynomial and Szörényi et al. [16] show that a uniform sampling algorithm has complexity at most (1/ε)2+log(KN)/(log(1/γ)). Notice that the product KN represents the branching factor of the lookahead planning tree. This bound could be improved for problems with specific reward structure or transition smoothness. In order to do this, we need to design non-uniform, adaptive algorithm that captures the possible structure of the MDP when available, while making sure that in the worst case, we do not perform worse than a uniform sampling algorithm.\nThe case of deterministic dynamics (N = 1) and rewards considered by Hren and Munos [10] has a complexity of order (1/ε)(log κ)/(log(1/γ)), where κ ∈ [1,K] is the branching factor of the subset of near-optimal nodes.3 The case of stochastic rewards has been considered by Bubeck and Munos [5] but with the difference that the goal was not to approximate the optimal value function but the value of the best open-loop policy which consists in a sequence of actions independent of states. Their sample complexity is (1/ε)max(2,(log κ)/(log 1/γ)).\n2neglecting exponential dependence in γ 3nodes that need to be considered in order to return a near-optimal approximation of the value at the root\nIn the case of general MDPs, Buşoniu and Munos [6] consider the case of a fully known model of the MDP. For any state-action, the model returns the expected reward and the set of all next states (assuming N is finite) with their corresponding transition probabilities. In that case, the complexity is (1/ε)log κ/(log(1/γ)), where κ ∈ [0,KN ] can again be interpreted as a branching factor of the subset of near-optimal nodes. These approaches use the optimism in the face of uncertainty principle whose applications to planning have been have been studied by Munos [13]. TrailBlazer is different. It is not optimistic by design: To avoid voracious demand for samples it does not balance the upperconfidence bounds of all possible actions. This is crucial for polynomial sample complexity in the infinite case. The whole Section 3 shines many rays of intuitive light on this single and powerful idea.\nThe work that is most related to ours is StOP by Szörényi et al. [16] which considers the planning problem in MDPs with a generative model. Their complexity bound is of the order of (1/ε)2+log κ/(log(1/γ))+o(1), where κ ∈ [0,KN ] is a problem-dependent quantity. However, their κ defined as limε→0 max(κ1, κ2) (in their Theorem 2) is somehow difficult to interpret as a measure of the quantity of near-optimal nodes. Moreover, StOP is not computationally efficient as it requires to identify the optimistic policy which requires computing an upper bound on the value of any possible policy, whose number is exponential in the number of MAX nodes, which itself is exponential in the planning horizon. Although they suggest (in their Appendix F) a computational improvement, this version is not analyzed. Finally, unlike in the present paper, StOP does not consider the case N =∞ of an unbounded number of states."
    }, {
      "heading" : "1.2 Our contributions",
      "text" : "Our main result is TrailBlazer, an algorithm with a bound on the number of samples required to return a high-probability ε-approximation of the root node whether the number of next states N is finite or infinite. The bounds use a problem-dependent quantity (κ or d) that measures the quantity of near-optimal nodes. We now summarize the results.\nFinite number of next states (N < ∞): The sample complexity of TrailBlazer is of the order of4 (1/ε)max(2,log(Nκ)/ log(1/γ)+o(1)), where κ ∈ [1,K] is related to the branching factor of the set of near-optimal nodes (precisely defined later).\nInfinite number of next states (N =∞): The complexity of TrailBlazer is (1/ε)2+d, where d is a measure of the difficulty to identify the near-optimal nodes. Notice that d can be finite even if the planning problem is very challenging.5 We also state our contributions in specific settings in comparison to previous work.\n• For the case N < ∞, we improve over the best-known previous worst-case bound with an exponent (to 1/ε) of max(2, log(NK)/ log(1/γ)) instead of 2 + log(NK)/ log(1/γ) reported by Szörényi et al. [16]. • For the case N = ∞, we identify properties of the MDP (when d = 0) under which the\nsample complexity is of order (in 1/ε2). This is the case when there are non-vanishing actiongaps6 from any state along near-optimal policies or when the probability of transitionning to nodes with gap ∆ is upper bounded by ∆2. This complexity bound is as good as MonteCarlo sampling and for this reason TrailBlazer is a natural extension of Monte-Carlo sampling (where all nodes are AVG) to stochastic control problems (where MAX and AVG nodes alternate). Also, no previous algorithm reported a polynomial bound when N =∞.\n• In MDPs with deterministic transitions (N = 1) but stochastic rewards our bound is (1/ε)max(2,log κ/(log 1/γ)) which is similar to the bound achieved by Bubeck and Munos [5] in a similar setting (open-loop policies).\n• In the evaluation case without control (K = 1) TrailBlazer behaves exactly as MonteCarlo sampling (thus achieves a complexity of 1/ε2), even in the case N =∞.\n• Finally TrailBlazer is easy to implement and is numerically efficient.\n4neglecting logarithmic terms in ε and δ 5since when N =∞ the actual branching factor of the set of reachable nodes is infinite 6defined as the difference in values of best and second-best actions"
    }, {
      "heading" : "2 Monte-Carlo planning with a generative model",
      "text" : "1: Input: δ, ε 2: Set: η ← γ1/max(2,log(1/ε))\n3: Set: λ← 2 log(ε(1− γ))2 log\n( log(K) (1−η) ) log(γ/η)\n4: Set: m← (log(1/δ) + λ)/((1− γ)2ε2) 5: Use: δ and η as global parameters 6: Output: µ← call the root with parameters (m, ε/2)\nFigure 1: TrailBlazer\nSetup We operate on a planning tree T . Each node of T from the root down is alternatively either an average (AVG) or a maximum (MAX) node. For any node s, C [s] is the set of its children. We consider trees T for which the cardinality of C [s] for any MAX node s is bounded by K. The cardinality N of C [s] for any AVG node s can be either finite, N < ∞, or infinite. We consider both cases. TrailBlazer applies to both situations. We provide performance guarantees for a general case and possibly tighter,\nN -dependent guarantees in the case of N <∞. We assume that we have a generative model of the transitions and rewards: Each AVG node s is associated with a transition, a random variable τs ∈ C [s] and a reward, a random variable rs ∈ [0, 1].\nObjective For any node s, we define the value function V [s] as the optimum over policies π (giving a successor to all MAX nodes) of the sum of discounted expected rewards playing policy π,\nV [s] = sup π E [∑ t≥0 γtrst ∣∣∣s0 = s, π], where γ ∈ (0, 1) is the discount factor. If s is an AVG node, V satisfies the following Bellman equation,\nV [s] = E [rs] + γ ∑ s′∈C[s] p(s′|s)V [s′] .\nIf s is a MAX node, then V [s] = maxs′∈C[s] V [s′] . The planner has access to the oracle which can be called for any AVG node s to either get a reward r or a transition τ which are two independent random variables identically distributed as rs and τs respectively.\nWith the notation above, our goal is to estimate the value V [s0] of the root node s0 using the smallest possible number of oracle calls. More precisely, given any δ and ε, we want to output a value µε,δ such that P [|µε,δ − V [s0]| > ε] ≤ δ using the smallest\npossible number of oracle calls nε,δ . The number of calls is the sample complexity of the algorithm.\n2.1 Blazing the trails with TrailBlazer\nTo fulfill the above objective, our TrailBlazer constructs a planning tree T which is, at any time, a finite subset of the potentially infinite tree. Only the already visited nodes are in T and explicitly represented in memory. Taking the object-oriented paradigm, each node of T is a persistent object with its own memory which can receive and perform calls respectively from and to other nodes. A node can potentially be called several times (with different parameters) during the run of TrailBlazer and may reuse (some of) its stored (transition and reward) samples. In particular, after node s receives a call from its parent, node s may perform internal computation by calling its own children in order to return a real value to its parent.\nPseudocode of TrailBlazer is in Figure 1 along with the subroutines for MAX nodes in Figure 3 and AVG nodes in Figure 2. A node (MAX or AVG) is called with two parameters m and ε, which represent some requested properties of the returned value: m controls the desired variance and ε the desired maximum bias. We now describe the MAX and AVG node subroutines.\nMAX nodes A MAX node s keeps a lower and an upper bound of its children values which with high probability simultaneously hold at all times. It sequentially calls its children with different parameters in order to get more and more precise estimates of their values. Whenever the upper bound of one child becomes lower than the maximum lower bound, this child is discarded. This process can stop in two ways: 1) The set L of the remaining children shrunk enough such that there is a single child b? left. In this case, s calls b? with the same parameters that s received and uses the output of b? as its own output. 2) The precision we have on the value of the remaining children is high enough. In this case, s returns the highest estimate of the children in L. Note that the MAX node is eliminating actions to identify the best. Any other best-arm identification algorithm for bandits can be adapted instead.\nAVG nodes Every AVG node s keeps a list of all the children that it already sampled and a reward estimate r ∈ R. Note that the list may contain the same child multiple times (this is particularly true for N < ∞). After receiving a call with parameters (m, ε), s checks if ε ≥ 1/(1− γ). If this condition is verified, then it returns zero. If not, s considers the first m sampled children and potentially samples more children from the generative model if needed. For every child s′ in this list, s calls it with parameters (k, ε/γ), where k is the number of times a transition toward this child was sampled. It returns r + γµ, where µ is the average of all the children estimates.\nAnytime algorithm TrailBlazer is naturally anytime. It can be called with slowly decreasing ε, such that m is always increased only by 1, without having to throw away any previously collected samples. Executing TrailBlazer with ε′ and then with ε < ε′ leads to the same amount of computation as immediately running TrailBlazer with ε.\nPractical considerations The parameter λ exists so the behavior depends only on the randomness of oracle calls and the parameters (m, ε) that the node has been called with. This is a desirable property because it opens the possibility to extend the algorithm to more general settings, for instance if we have also MIN nodes. However, for practical purposes, we may set λ = 0 and modify the definition of U in Figure 3 by replacing K with the number of oracle calls made so far globally."
    }, {
      "heading" : "3 Cogs whirring behind",
      "text" : "Before diving into the analysis we explain the ideas behind TrailBlazer and the choices made.\nTree-based algorithm The number of policies the planner can consider is exponential in the number of states. This leads to two major challenges. First, reducing the problem to multi-arm bandits on the set of the policies would hurt. When a reward is collected from a state, all the policies which could reach that state are affected. Therefore, it is useful to share the information between the policies. The second challenge is computational as it is infeasible to keep all policies in memory.\nThese two problems immediately vanish with just how TrailBlazer is formulated. Contrary to Szörényi et al. [16], we do not represent the policies explicitly or update them simultaneously to share the information, but we store all the information directly in the planning tree we construct. Indeed, by having all the nodes being separate entities that store their own information, we can share information between policies without explicitly having to enforce it.\nWe steel ourselves for the detailed understanding with the following two arguments. They shed light from two different angles on the very same key point: Do not refine more paths than you need to!\nDelicate treatment of uncertainty First, we give intuition about the two parameters which measure the requested precision of a call. The output estimate µ of any call with parameters (m, ε) verifies the following property (conditioned on a high-probability event),\n∀λ E [ eλ(µ−V[s]) ] ≤ exp ( α+ ε|λ|+ σ 2λ2\n2\n) , with σ2 = O (1/m) and constant α. (1)\nThis awfully looks like the definition of µ being uncentered sub-Gaussian, except that instead of λ in the exponential function, there is |λ| and there is a λ-independent constant α. Inequality 1 implies that the absolute value of the bias of the output estimate µ is bounded by ε,∣∣E [µ]− V [s] ∣∣ ≤ ε. As in the sub-Gaussian case, the second term 12σ\n2λ2 is a variance term. Therefore, ε controls the maximum bias of µ and 1/m control its sub-variance. In some cases, getting high-variance or low-variance estimate matters less as it is going to be averaged later with other independent estimates by an ancestor AVG node. In this case we prefer to query for high variance rather than a low one, in order to decrease sample complexity.\nFrom σ and ε it is possible to deduce a confidence bounds on |µ − V [s]| by typically summing the bias ε and a term proportional to the standard deviation σ = O (1/ √ m). Previous approaches [16, 5] consider a single parameter, representing the width of this high-probability confidence interval. TrailBlazer is different. In TrailBlazer, the nodes can perform high-variance and low-bias queries but can also query for both low-variance and low-bias. TrailBlazer treats these two types of queries differently. This is the whetstone of TrailBlazer and the reason why it is not optimistic.\nRefining few paths In this part we explain the condition |SampledNodes| > m in Figure 2, which is crucial for our approach and results. First notice, that as long as TrailBlazer encounters only AVG nodes, it behaves just like Monte-Carlo sampling — without the MAX nodes we would be just doing a simple averaging of trajectories. However, when TrailBlazer encounters a MAX node it locally uses more samples around this MAX node, temporally moving away from a Monte-Carlo behavior. This enables TrailBlazer to compute the best action at this MAX node. Nevertheless, once this best action is identified with high probability, the algorithm should behave again like Monte-Carlo sampling. Therefore, TrailBlazer forgets the additional nodes, sampled just because of the MAX node, and only keeps in memory the first m ones. This is done with the following line in Figure 2,\nActiveNodes← SampledNodes(1 : m). Again, while additional transitions were useful for some MAX node parents to decide which action to pick, they are discarded once this choice is made. Note that they can become useful again if an ancestor becomes unsure about which action to pick and needs more precision to make a choice. This is an important difference between TrailBlazer and some previous approaches like UCT where all the already sampled transitions are equally refined. This treatment enables us to provide polynomial bounds on the sample complexity for some special cases even in the infinite case (N =∞).\n4 TrailBlazer is good and cheap — consistency and sample complexity\nIn this section, we start by our consistency result, stating that TrailBlazer outputs a correct value in a PAC (probably approximately correct) sense. Later, we define a measure of the problem difficulty which we use to state our sample-complexity results. We remark that the following consistency result holds whether the state space is finite or infinite. Theorem 1. For all ε and δ, the output µε,δ of TrailBlazer called on the root s0 with (ε, δ) verifies\nP [|µε,δ − V [s0]| > ε] < δ."
    }, {
      "heading" : "4.1 Definition of the problem difficulty",
      "text" : "We now define a measure of problem difficulty that we use to provide our sample complexity guarantees. We define a set of near-optimal nodes such that exploring only this set is enough to compute an optimal policy. Let s′ be a MAX node of tree T . For any of its descendants s, let c→s(s\n′) ∈ C [s′] be the child of s′ in the path between s′ and s. For any MAX node s, we define ∆→s(s\n′) = max x∈C[s′] V [x]− V [c→s(s′)] .\n∆→s(s ′) is the difference of the sum of discounted rewards stating from s′ between an agent playing optimally and one playing first the action toward s and then optimally.\nDefinition 1 (near-optimality). We say that a node s of depth h is near-optimal, if for any even depth h′,\n∆→s(sh′) ≤ 16 γ(h−h\n′)/2\nγ(1− γ)\nwith sh′ the ancestor of s of even depth h′. Let Nh be the set of all near-optimal nodes of depth h. Remark 1. Notice that the subset of near-optimal nodes contains all required information to get the value of the root. In the case N =∞, when p(s|s′) = 0 for all s and s′, then our definition of near-optimality nodes leads to the smallest subset in a sense we precise in Appendix C. We prove that with probability 1− δ, TrailBlazer only explores near-optimal nodes. Therefore, the size of the subset of near-optimal nodes directly reflects the sample complexity of TrailBlazer.\nIn Appendix C, we discuss the negatives of other potential definitions of near-optimality."
    }, {
      "heading" : "4.2 Sample complexity in the finite case",
      "text" : "We first state our result where the set of the AVG children nodes is finite and bounded by N .\nDefinition 2. We define κ ∈ [1,K] as the smallest number such that\n∃C ∀h, |N2h| ≤ CNhκh.\nNotice that since the total number of nodes of depth 2h is bounded by (KN)h, κ is upper-bounded by K, the maximum number of MAX’s children. However κ can be as low as 1 in cases when the set of near-optimal nodes is small.\nTheorem 2. There exists C > 0 and K such that for all ε > 0 and δ > 0, with probability 1 − δ, the sample-complexity of TrailBlazer (the number of calls to the generative model before the algorithm terminates) is\nn(ε, δ) ≤ C(1/ε)max(2, log(Nκ) log(1/γ) +o(1)) (log(1/δ) + log(1/ε)) α ,\nwhere α = 5 when log(Nκ)/ log(1/γ) ≥ 2 and α = 3 otherwise.\nThis provides a problem-dependent sample-complexity bound, which already in the worst case (κ = K) improves over the best-known worst-case bound Õ ( (1/ε)2+log(KN)/ log(1/γ) ) [16]. This bound gets better as κ gets smaller and is minimal when κ = 1. This is, for example, the case when the gap (see definition given in Equation 2) at MAX nodes is uniformly lower-bounded by some ∆ > 0. In this case, this theorem provides a bound of order (1/ε)max(2,log(N)/ log(1/γ)). However, we will show in Remark 2 that we can further improve this bound to (1/ε)2."
    }, {
      "heading" : "4.3 Sample complexity in the infinite case",
      "text" : "Since the previous bound depends on N , it does not apply to the infinite case with N =∞. We now provide a sample complexity result in the case N =∞. However, notice that when N is bounded, then both results apply.\nWe first define gap ∆(s) for any MAX node s as the difference between the best and second best arm,\n∆(s) = V [i?]− max i∈C[s],i6=i? V [i] with i? = arg max i∈C[s] V [i] . (2)\nFor any even integer h, we define a random variable Sh taking values among MAX nodes of depth h, in the following way. First, from every AVG nodes from the root to nodes of depth h, we draw a single transition to one of its children according to the corresponding transition probabilities. This defines a subtree with Kh/2 nodes of depth h and we choose Sh to be one of them uniformly at random. Furthermore, for any even integer h′ < h we note Shh′ the MAX node ancestor of S h of depth h′.\nDefinition 3. We define d ≥ 0 as the smallest d such that for all ξ there exists a > 0 for which for all even h > 0,\nE Kh/21 (Sh ∈ Nh) ∏ 0≤h′<h\nh′≡0(mod 2)\n( ξ\nγh−h′\n)1(∆(Sh h′ )<16 γ(h−h ′)/2 γ(1−γ) ) ≤ aγ−dh If no such d exists, we set d =∞.\nThis definition of d takes into account the size of the near-optimality set (just like κ) but unlike κ it also takes into account the difficulty to identify the near-optimal paths.\nIntuitively, the expected number of oracle calls performed by a given AVG node s is proportional to: (1/ε2) × (the product of the inverted squared gaps of the set of MAX nodes in the path from the root to s) × (the probability of reaching s by following a policy which always tries to reach s). Therefore, a near-optimal path with a larger number of small MAX node gaps can be considered difficult. By assigning a larger weight to difficult nodes, we are able to give a better characterization of the actual complexity of the problem and provide polynomial guarantees on the sample complexity for N =∞ when d is finite. Theorem 3. If d is finite then there exists C > 0 such that for all ε > 0 and δ > 0, the expected sample complexity of TrailBlazer satisfies\nE [n(ε, δ)] ≤ C (log(1/δ) + log(1/ε)) 3\nε2+d ·\nNote that this result holds in expectation only, contrary to Theorem 2 which holds in high probability.\nWe now give an example for which d = 0, followed by a special case of it. Lemma 1. If there exists c > 0 and b > 2 such that for any near-optimal AVG node s,\nP [∆ (τs) ≤ x] ≤ cxb,\nwhere the random variable τs is a successor state from s drawn from the MDP’s transition probabilities, then d = 0 and consequently the sample complexity is of order 1/ε2.\nRemark 2. If there exists ∆min such that for any near-optimal MAX node s, ∆(s) ≥ ∆min then d = 0 and the sample complexity is of order 1/ε2. Indeed, in this case as P [∆s ≤ x] ≤ (x/∆min)b for any b > 2 for which d = 0 by Lemma 1."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We provide a new Monte-Carlo planning algorithm TrailBlazer that works for MDPs where the number of next states N can be either finite or infinite. TrailBlazer is easy to implement and is numerically efficient. It comes packaged with a PAC consistency and two problem-dependent sample-complexity guarantees expressed in terms of a measure (defined by κ) of the quantity of near-optimal nodes or a measure (defined by d) of the difficulty to identify the near-optimal paths. The sample complexity of TrailBlazer improves over previous worst-case guarantees. What’s more, TrailBlazer exploits MDPs with specific structure by exploring only a fraction of the whole search space when either κ or d is small. In particular, we showed that if the set of near-optimal nodes have non-vanishing action-gaps, then the sample complexity is Õ(1/ε2), which is the same rate as Monte-Carlo sampling. This is a pretty decent evidence that TrailBlazer is a natural extension of Monte-Carlo sampling to stochastic control problems.\nAcknowledgements The research presented in this paper was supported by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, a doctoral grant of École Normale Supérieure in Paris, Inria and Carnegie Mellon University associated-team project EduBand, and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003)"
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Dynamic Programming",
      "author" : [ "Richard Bellman" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1957
    }, {
      "title" : "Neuro-Dynamic Programming",
      "author" : [ "Dimitri Bertsekas", "John Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1996
    }, {
      "title" : "A survey of Monte Carlo tree search methods",
      "author" : [ "Cameron B. Browne", "Edward Powley", "Daniel Whitehouse", "Simon M. Lucas", "Peter I. Cowling", "Philipp Rohlfshagen", "Stephen Tavener", "Diego Perez", "Spyridon Samothrakis", "Simon Colton" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Open-loop optimistic planning",
      "author" : [ "Sébastien Bubeck", "Rémi Munos" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Optimistic planning for Markov decision processes",
      "author" : [ "Lucian Buşoniu", "Rémi Munos" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Efficient selectivity and backup operators in Monte-Carlo tree search",
      "author" : [ "Rémi Coulom" ],
      "venue" : "Computers and games,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Modification of UCT with patterns in Monte-Carlo Go",
      "author" : [ "Sylvain Gelly", "Wang Yizao", "Rémi Munos", "Olivier Teytaud" ],
      "venue" : "Technical report,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "Efficient Bayes-adaptive reinforcement learning using sample-based search",
      "author" : [ "Arthur Guez", "David Silver", "Peter Dayan" ],
      "venue" : "Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Optimistic Planning of Deterministic Systems",
      "author" : [ "Jean-Francois Hren", "Rémi Munos" ],
      "venue" : "In European Workshop on Reinforcement Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "A sparse sampling algorithm for nearoptimal planning in large Markov decision processes",
      "author" : [ "Michael Kearns", "Yishay Mansour", "Andrew Y. Ng" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1999
    }, {
      "title" : "Bandit-based Monte-Carlo planning",
      "author" : [ "Levente Kocsis", "Csaba Szepesvári" ],
      "venue" : "In European Conference on Machine Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "From bandits to Monte-Carlo tree search: The optimistic principle applied to optimization and planning",
      "author" : [ "Rémi Munos" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Monte-Carlo planning in large POMDPs",
      "author" : [ "David Silver", "Joel Veness" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Optimistic planning in Markov decision processes using a generative model",
      "author" : [ "Balázs Szörényi", "Gunnar Kedenburg", "Rémi Munos" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Integrating sample-based planning and model-based reinforcement learning",
      "author" : [ "Thomas J Walsh", "Sergiu Goschin", "Michael L Littman" ],
      "venue" : "AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : ", [12]), has been popularized in the game of computer Go [7, 8, 15] and shown impressive performance in many other high dimensional control and game problems [4].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : ", [12]), has been popularized in the game of computer Go [7, 8, 15] and shown impressive performance in many other high dimensional control and game problems [4].",
      "startOffset" : 57,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : ", [12]), has been popularized in the game of computer Go [7, 8, 15] and shown impressive performance in many other high dimensional control and game problems [4].",
      "startOffset" : 57,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : ", [12]), has been popularized in the game of computer Go [7, 8, 15] and shown impressive performance in many other high dimensional control and game problems [4].",
      "startOffset" : 57,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : ", [12]), has been popularized in the game of computer Go [7, 8, 15] and shown impressive performance in many other high dimensional control and game problems [4].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "Following dynamic programming [2], planning can be reduced to an approximation of the (optimal) value function, defined as the maximum of the expected sum of discounted rewards: E [∑ t≥0 γ rt ] ,",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "Indeed, if an ε-optimal approximation of the value function at any state-action pair is available, then the policy corresponding to selecting in each state the action with the highest approximated value will be O (ε/ (1− γ))-optimal [3].",
      "startOffset" : 233,
      "endOffset" : 236
    }, {
      "referenceID" : 11,
      "context" : "Kocsis and Szepesvári [12] introduced the UCT algorithm (upper-confidence bounds for trees).",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "UCT is efficient in computer Go [7, 8, 15] and a number of other control and game problems [4].",
      "startOffset" : 32,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "UCT is efficient in computer Go [7, 8, 15] and a number of other control and game problems [4].",
      "startOffset" : 32,
      "endOffset" : 42
    }, {
      "referenceID" : 14,
      "context" : "UCT is efficient in computer Go [7, 8, 15] and a number of other control and game problems [4].",
      "startOffset" : 32,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "UCT is efficient in computer Go [7, 8, 15] and a number of other control and game problems [4].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "UCT converges asymptotically to the optimal solution, but its sample complexity can be worst than doubly-exponential in (1/ε) for some MDPs [13].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "[11] generate a sparse look-ahead tree based on expanding all MAX nodes and sampling a finite number of children from AVG nodes up to a fixed depth that depends on the desired accuracy ε.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] cuts suboptimal branches in an adaptive way but unfortunately does not come with an improved bound and stays non-polynomial even in the simple Monte Carlo setting for which K = 1.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] show that a uniform sampling algorithm has complexity at most (1/ε).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "The case of deterministic dynamics (N = 1) and rewards considered by Hren and Munos [10] has a complexity of order (1/ε) , where κ ∈ [1,K] is the branching factor of the subset of near-optimal nodes.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "3 The case of stochastic rewards has been considered by Bubeck and Munos [5] but with the difference that the goal was not to approximate the optimal value function but the value of the best open-loop policy which consists in a sequence of actions independent of states.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "In the case of general MDPs, Buşoniu and Munos [6] consider the case of a fully known model of the MDP.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 12,
      "context" : "These approaches use the optimism in the face of uncertainty principle whose applications to planning have been have been studied by Munos [13].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "[16] which considers the planning problem in MDPs with a generative model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "• In MDPs with deterministic transitions (N = 1) but stochastic rewards our bound is (1/ε) κ/(log 1/γ)) which is similar to the bound achieved by Bubeck and Munos [5] in a similar setting (open-loop policies).",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "[16], we do not represent the policies explicitly or update them simultaneously to share the information, but we store all the information directly in the planning tree we construct.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Previous approaches [16, 5] consider a single parameter, representing the width of this high-probability confidence interval.",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "Previous approaches [16, 5] consider a single parameter, representing the width of this high-probability confidence interval.",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : "This provides a problem-dependent sample-complexity bound, which already in the worst case (κ = K) improves over the best-known worst-case bound Õ ( (1/ε) log(1/γ) ) [16].",
      "startOffset" : 166,
      "endOffset" : 170
    } ],
    "year" : 2016,
    "abstractText" : "You are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). But you do not want to StOP with exponential running time, you want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.",
    "creator" : null
  }
}