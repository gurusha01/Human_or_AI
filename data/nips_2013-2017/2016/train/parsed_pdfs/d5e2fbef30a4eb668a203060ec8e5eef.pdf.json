{
  "name" : "d5e2fbef30a4eb668a203060ec8e5eef.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Efficient High-Order Interaction-Aware Feature Selection Based on Conditional Mutual Information",
    "authors" : [ "Alexander Shishkin", "Anastasia Bezzubtseva", "Alexey Drutsa", "Ilia Shishkov", "Ekaterina Gladkikh", "Gleb Gusev", "Pavel Serdyukov" ],
    "emails" : [ "sisoid@yandex-team.ru", "nstbezz@yandex-team.ru", "adrutsa@yandex-team.ru", "ishfb@yandex-team.ru", "kglad@yandex-team.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Methods of feature selection is an important topic of machine learning [8, 2, 17], since they improve performance of learning systems while reducing their computational costs. Feature selection methods are usually grouped into three main categories: wrapper, embedded, and filter methods [8]. Filters are computationally cheap and are independent of a particular learning model that make them popular and broadly applicable. In this paper, we focus on most popular filters, which are based on mutual information (MI) and apply the sequential forward selection (SFS) strategy to obtain an optimal subset of features [17]. In such applications as web search, features may be highly relevant only jointly (having a low relevance separately). A challenging task is to account for such interactions [17]. Existing SFS-based filters [18, 3, 24] are able to account for interactions of only up to 3 features.\nIn this study, we fill the gap in the absence of effective SFS-based filters accounting for feature dependences of higher orders. A search of t-way interacting features is turned into a novel saddle point (max-min) optimization problem for MI of the target variable and the candidate feature with its complementary team conditioned on its opposing team of previously selected features. We show that, on the one hand, the saddle value of this conditional MI is a low-dimensional approximation of the CMI score1 and, on the other hand, solving that problem represents two practical challenges: (a) prohibitively high computational complexity and (b) sample complexity, a larger number of instances required to accurately estimate the MI. These issues are addressed by two novel techniques: (a) a two stage greedy search for the approximate solution of the above-mentioned problem whose computational complexity is O(i) at each i-th SFS iteration; and (b) binary representation of features that reduces the dimension of the space of joint distributions by a factor of (q/2)2t for q-value features. Being reasonable and intuitive, these techniques together constitute the main contribution of our study: a novel SFS method CMICOT that is able to identify joint interactions between multiple\n1The CMI filter is believed to be a “north star\" for vast majority of the state-of-the-art filters [2].\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nfeatures. We also empirically validate our approach with 3 state-of-the-art classification models on 10 publicly available benchmark datasets and compare it with known interaction-aware SFS-based filters and several state-of-the-art ones."
    }, {
      "heading" : "2 Preliminaries and related work",
      "text" : "Information-theoretic measures. The mutual information (MI) of two random variables f and g is defined as I(f ; g) = H(f) + H(g) − H(f, g), where H(f) = −E [log P(f)] is Shannon’s entropy [4]2. The conditional mutual information of two random variables f and g given the variable h is I(f ; g | h) = I(f ; g, h) − I(f ;h). The conditional MI measures the amount of additional information about the variable f carried by g compared to the variable h. Given sample data, entropy (and, hence, MI and conditional MI) of discrete variables could be simply estimated using the empirical frequencies (the point estimations) [15] or in a more sophisticated way (e.g., by means of the Bayesian framework [10]). More details on different entropy estimators can be found in [15].\nBackground of the feature selection based on MI. Let F be a set of features that could be used by a classifier to predict a variable c representing a class label. The objective of a feature selection (FS) procedure is to find a feature subset So ⊆ F of a given size k ∈ N that maximizes its joint MI with the class label c, i.e., So = argmax{S:S⊆F,|S|≤k} I(c;S). In our paper, we focus on this simple but commonly studied FS objective in the context of MI-based filters [2], though there is a wide variety of other definitions of optimal subset of features [17] (e.g., the all-relevant problem [13]).\nIn order to avoid an exhaustive search of an optimal subset S , most filters are based on sub-optimal search strategies. The most popular one is the sequential forward selection (SFS) [20, 23, 17], which starts with an empty set (S0 := ∅) and iteratively increases it by adding one currently unselected feature on each step (Si := Si−1 ∪ {fi}, i = 1, . . . , k, and So := Sk). The feature fi is usually selected by maximizing a certain scoring function (also called score) Ji(f) that is calculated with respect to currently selected features Si−1, i.e., fi := argmaxf∈F\\Si−1 Ji(f).\nA trivial feature selection approach is to select top-k features in terms of their MI with the class label c [12]. This technique is referred to as MIM [2] and is a particular case of the SFS strategy based on score JMIMi (f) := I(c; f). Note that the resulting set may contain a lot of redundant features, since the scoring function JMIMi (·) is independent from already selected features Si−1. Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16]. Brown et al. [2] unified these techniques under one framework, where they are different low-order approximations of CMI feature selection approach. This method is based on the score equal to MI of the label with the evaluated feature conditioned on already selected features:\nJCMIi (f) := I(c; f | Si−1). (1)\nThe main drawback of CMI is the sample complexity, namely, the exponential growth of the dimension of the distribution of the tuple (c, f, Si−1) with respect to i. The larger the dimension is, the larger number of instances is required to accurately estimate the conditional MI in Eq. (1). Therefore, this technique is not usable in the case of small samples and in the cases, when a large number of features should be selected [2]. This is also observed in our experiment in Appendix.F2, where empirical score estimated over high dimensions results in drastically low performance of CMI.\nThus, low-dimensional approximations of Eq. (1) are more preferable in practice. For instance, the CMIM approach approximates Eq. (1) by\nJCMIMi (f) := min g∈Si−1 I(c; f | g), (2)\ni.e., one replaces the redundancy of f with respect to the whole subset Si−1 by the worst redundancy with respect to one feature from this subset. The other popular methods (mentioned above) are particular cases of the following approximation of the I(c; f | Si−1):\nJβ,γi (f) := I(c; f)− ∑\ng∈Si−1\n( βI(g; f)− γI(g; f | c) ) , (3)\n2From here on in the paper, variables separated by commas or a set of variables in MI expressions are treated as one random vector variable, e.g., I(f ; g, h) := I ( f ; (g, h) ) and, for F = ∪ni=1{fi}, I(f ;F ) := I(f ; f1, .., fn).\ne.g., MIFS (β ∈ [0, 1], γ = 0), mRMR (β = |Si−1|−1, γ = 0), and JMI (β = γ = |Si−1|−1). An important but usually neglected aspect in FS methods is feature complementariness [8, 3] (also known as synergy [24] and interaction [11]). In general, complementary features are those that appear to have low relevance to the target class c individually, but whose combination is highly relevant [25, 24]. In the next subsection, we provide a brief overview of existing studies on filters that take into account feature interaction. A reader interested in a formalized concept of feature relevance, redundancy, and interaction is referred to [11] and [24].\nRelated work on interaction-aware filters. To the best of our knowledge, existing interaction-aware filters that utilize the pure SFS strategy with a MI-based scoring function are the following ones. RelaxMRMR [18] is a modification of the mRMR method, whose scoring function in Eq. (3) was refined by adding the three-way feature interaction terms ∑ h,g∈Si−1,h6=g I(f ;h | g). The method RCDFS [3] is a special case of Eq. (3), where β = γ are equal to a transformation of the standard deviation of the set {I(f ;h)}h∈Si−1 . The approach IWFS [24] is based on the following idea: at each step i, for each unselected feature f ∈ F \\ Si, one calculates the next step score Ji+1(f) as the current score Ji(f) multiplied by a certain measure of interaction between this feature f and the feature fi selected at the current step. Both RCDFS and IWFS can catch dependences between no more than 2 features, while RelaxMRMR is able to identify an interaction of up to 3 features, but its score’s computational complexity is O(i2) what makes it unusable in real applications. All these methods could not be straightforwardly improved to incorporate interactions of a higher order.\nIn our study, we propose a general methodology that fills the gap between the ideal (“oracle\") but infeasible CMI method, which takes all interactions into account, and the above-described methods that account for up to 3 interacting features. Our method can be effectively used in practice with its score’s computational complexity of a linear growth O(i) (as in most state-of-the-art SFS-filters)."
    }, {
      "heading" : "3 Proposed feature selection",
      "text" : "In this section, we introduce a novel feature selection approach based on the SFS strategy whose score is built by solving from a novel optimization problem and comprises two novel techniques that makes the approach efficient and effective in practice.\n3.1 Score with t-way interacted complementary and opposing teams\nOur FS method has a parameter t ∈ N that is responsible for the desirable number of features whose mutual interaction (referred to as a t-way feature interaction) should be taken into account by the scoring function Ji(·). We build the scoring function according to the following intuitions. First, the amount of relevant information carried by a t-way interaction of a candidate feature f has the form I(c; f,H) for some set of features H of size |H| ≤ t−1. Second, we remove the redundant part of this information w.r.t. the already selected features Si−1 and obtain the non-redundant information part I(c; f,H | Si−1). Following the heuristic of the CMIM method, this could be approximated by use of a small subset G ⊆ Si−1, |G| ≤ s ∈ N, i.e., by the low-dimensional approximation min{G⊆Si−1,|G|≤s} I(c; f,H | G) (assuming s i). Third, since in the SFS strategy one has to select only one feature at an iteration i, this approximated additional information of the candidate f with H w.r.t. Si−1 will be gained by with the feature f at this SFS iteration only if all complementary features H have been already selected (i.e., H ⊆ Si−1). In this way, the score of the candidate f should be equal to the maximal additional information estimated using above reasoning, i.e., we come to the score which is a solution of the following saddle point (max-min) optimization problem\n◦ J (t,s) i (f) := max\nH⊆Si−1, |H|≤t−1 min G⊆Si−1, |G|≤s\nI(c; f,H | G). (4)\nWe refer to the set {f} ∪Hof , where Hof is an optimal set H in Eq. (4), as an optimal complementary team of the feature f ∈ F \\ Si−1, while an optimal set G in Eq. (4) is referred to as an optimal opposing team to this feature f (and, thus, to its complementary team as well) and is denoted by Gof .\nThe described approach is inspired by methods of greedy learning of ensembles of decision trees [7], where an ensemble of trees is built by sequentially adding a decision tree that maximizes the gain in learning quality. In this way, our complementary team corresponds to the features used in a candidate\ndecision tree, while our opposing team corresponds to the features used to build previous trees in the ensemble. Since they are already selected by SFS, they are expectedly stronger than f and we can assume that, at the early iterations, a greedy machine learning algorithm would more likely use these features rather than the new feature f once we add it into the feature set. So, Eq. (4) tries to mimic the maximal amount of information that feature f can provide additionally to the worst-case baseline built on Si−1. Statement 1. For t, s+ 1 ≥ i, the score ◦ J (t,s) i from Eq. (4) is equal to the score J CMI i from Eq. (1).\nThe proof’s sketch is: (a) justify the identity ◦ J (t,s) i (f) = maxH⊆Si−1 minG⊆Si−1\\H I(c; f | H,G) for t, s+ 1 ≥ i; (b) get a contradiction to the assumption that there are no optimal subsets H and G such that Si−1 = H ∪G. Detailed proof of Statement 1 is given in Appendix A. Thus, we argue that the score ◦ J (t,s) i from Eq. (4) is a low-dimensional approximation of the CMI score J CMI i . 3.\nThe score from Eq. (4) is of a general nature and reasonable, but, to the best of our knowledge, was never considered in existing studies. However, this score is not suitable for effective application, since it suffers from two practical issues:\n(PI.a) computational complexity: efficient search of optimal sets Hof and Gof in Eq. (4);\n(PI.b) sample complexity: accurate estimation of the MI over features with a large dimension of its joint distribution.\nWe address these research problems and propose the following solutions to them: in Sec. 3.2, the issue (PI.a) is overcome in a greedy fashion, while, in Sec. 3.3,the issue (PI.b) is mitigated by means of binary representatives."
    }, {
      "heading" : "3.2 Greedy approximation of the score",
      "text" : "Note that an exhaustive search of a saddle point in Eq. (4) requires ( i−1 t−1 )( i−1 s ) MI calculations that can make calculation of the scoring function ◦ J (t,s) i infeasible at a large iteration i even for low team sizes t, s > 1. In order to overcome this issue, we propose the following greedy search for sub-optimal complementary and opposing teams.\nAt the first stage, we start from a greedy search of a sub-optimal set H that cannot be done straightforwardly, since Eq. (4) comprises both max and min operators. The latter one requires a search of an optimal G that we want do at the second stage (after H). Hence, the double optimization problem needs to be replaced by a simpler one which does not utilize a search of G.\nProposition 1. (1) For any H ⊆ Si−1 such that |H| ≤ s, the following holds\nmin G⊆Si−1,|G|≤s\nI(c; f,H | G) ≤ I(c; f | H). (5)\n(2) If s ≥ t− 1, then the score given by the following optimization problem\nmax H⊆Si−1,|H|≤t−1\nI(c; f | H), (6)\nis an upper bound for the score ◦ J (t,s) i from Eq. (4).\nThe optimization problem Eq. (6) seems reasonable due to the following properties: (a) in fact, the search of H in Eq. (6) is maximization of the additional information carried out by the candidate f w.r.t. no more than t− 1 already selected features from Si−1; (b) if a candidate f is a combination of features from H , then the right hand side in Eq. (5) is 0 and the inequality becomes an equality.\nSo, we greedily search the maximum in Eq. (6), obtaining the (greedy) complementary team {f}∪Hf , where Hf := {h1, . . . , ht−1} is defined by4\nhj := argmax h∈Si−1\nI(c; f | h1, . . . , hj−1, h), j = 1, . . . , t− 1. (7)\n3Moreover, the CMIM score from Eq. (2) is a special case of Eq. (4) with s = t = 1 and restriction G 6= ∅. 4If several elements provide an optimum (the case of ties), then we randomly select one of them.\nAt the second stage, given the complementary team {f} ∪ Hf , we greedily search the (greedy) opposing team Gf := {g1, . . . , gs} in the following way:\ngj := argmin g∈Si−1\nI(c; f, h1, . . . , hmin{j,t}−1 | g1, . . . , gj−1, g), j = 1, . . . , s. (8)\nFinally, given the teams {f} ∪Hf and Gf , we get the following greedy approximation of ◦ J (t,s) i (f):\nJ (t,s) i (f) := I(c; f,Hf | Gf ). (9)\nThis score requires (t + s − 1)i MI calculations (see Eq. (7)–(9)), which is a linear dependence on an iteration i as in the most state-of-the-art SFS-based filters [2]. Thus, we built an efficient approximation of the score ◦ J (t,s) i and resolve the issue (PI.a).\nNote that we have two options on the minimization stage: either to search among all members of the set Hf at each step (as in Eq. (A.7) in Appendix A.3), or (what we actually do in Eq. (8)) to use only a few first members of Hf . The latter option demonstrates noticeably better MAUC performance and also results in 0 score for a feature that is a copy of an already selected one (Proposition 2), while the former does not (Remark A.2 in Appendix A.3). That is why we chose this option.\nProposition 2. Let s ≥ t and a candidate feature f ∈ F \\ Si−1 be such that its copy f̃ ≡ f is already selected f̃ ∈ Si−1, then, in the absence of ties in Eq. (8) for j ≤ t, the score J (t,s)i (f) = 0.\nProposition 2 shows that the FS approach based on the greedy score J (t,s)i (f) remains conservative, i.e., a copy of an already selected feature will not be selected, despite that it exploits sub-optimal teams in contrast to the FS approach based on the optimal score ◦ J (t,s) i (f)."
    }, {
      "heading" : "3.3 Binary representatives of features",
      "text" : "As it is mentioned in Sec. 2, a FS method that is based on calculation of MI over more than three features is usually not popular in practice, since a large number of features implies a large dimension of their joint distribution that leads to a large number of instances required to accurately estimate the MI [2]. Both our optimal score ◦ J (t,s) i and our greedy one J (t,s) i suffer from the same issue (PI.b) as well, since they exploit high-dimensional MI in Eq.(4) and Eq. (7)–(9). For instance, if we deal with binary classification and each feature in F has q unique values (e.g., continuous features are usually preprocessed into discrete variables with q ≥ 5 [18]), then the dimension of the joint distribution of features in Eq. (9) is equal to 2 · qt+s (e.g., ≈ 4.9 · 108 for t = s = 6, q = 5). In our method, we cannot reduce the number of features used in MIs (since t-way interaction constitutes the key basis of our approach), but we can mitigate the effect of the sample complexity by the following novel technique, which we demonstrate on our greedy score J (t,s)i . Let F consists of discrete features\n5. Definition 1. For each discrete feature f ∈ F , we denote by B[f ] the binary transformation of f , i.e., the set of binary variables (referred to as the binary representatives (BR) of f ) that constitute all together a vector containing the same information as f 6. For any subset F ′ ⊆ F , the set of binary representatives of all features from F ′ is denoted by B[F ′] = ⋃ f∈F ′ B[f ].\nThen, we replace all features by their binary representatives at each stage of our score calculation. Namely, in Eq. (7) and Eq. (8), (a) the searches are performed for each binary representative b ∈ B[f ] instead of f ; (b) the set Hbinb of the complementary team is found among B[Si−1] ∪B[f ]; while (c) the opposing team Gbinb is found among B[Si−1] (exact formulas could be found in Algorithm 1, lines 12 and 15). Finally, the score of a feature f in this FS approach based on binary representatives is defined as the best score among the binary representatives B[f ] of the candidate f :\nJ (t,s),bin i (f) := max b∈B[f ] I(c; b,Hbinb | Gbinb ). (10)\nNote that, in the previous example with a binary target variable c and q-value features, the dimension of the joint distribution of binary representatives used to calculate MI in J (t,s),bini is equal to 2 1+t+s,\n5If there is a non-discrete feature, then we apply a discretization (e.g., by equal-width, equal-frequency binnings [5], MDL [22, 3], etc.), which is the state-of-the-art preprocessing of continuous features in filters.\n6For instance, for f with values in {xl}ql=1, one could take B[f ] = {I{f=xl}} q−1 l=1 , where IX is X ’s indicator, or take bits of a binary encoding of {xl}ql=1 that is a smallest set (i.e., |B[f ]| = dlog2 qe) among possible B[f ].\nAlgorithm 1 Pseudo-code of the CMICOT feature selection method (an implementation of this algorithm is available at https://github.com/yandex/CMICOT).\n1: Input: F — the set of all features; B[f ], f ∈ F, — set of binary representatives built on f ; 2: c — the target variable; k ∈ N — the number of features to be selected; 3: t ∈ N, s ∈ Z+ — the team sizes (parameters of the algorithm); 4: Output: S — the set of selected features; 5: Initialize: 6: fbest := argmaxf∈F maxb∈B[f ] I(c; b); // Select the first feature 7: S := {fbest}; Sbin := B[fbest]; 8: while |S| < k and |F \\ S| > 0 do 9: for f ∈ F \\ S do\n10: for b ∈ B[f ] do 11: for j := 1 to t− 1 do 12: hj := argmaxh∈Sbin∪B[f ] I(c; b | h1, .., hj−1, h); // Search for complementary feat. 13: end for 14: for j := 1 to s do 15: gj := argming∈Sbin I(c; b, h1, .., hmin{j,t}−1 | g1, .., gj−1, g); // Search for opp. feat. 16: end for 17: Ji[b] := I(c; b, h1, .., ht−1 | g1, .., gs); // Calculate the score of the binary rep. b 18: end for 19: Ji[f ] := maxb∈B[f ] Ji[b]; // Calculate the score of the feature f 20: end for 21: fbest := argmaxf∈F\\S Ji[f ]; // Select the best candidate feature at the current step 22: S := S ∪ {fbest}; Sbin := Sbin ∪B[fbest]; 23: end while\nwhich is (q/2)t+s times smaller (the dimension reduction rate) than for the MI in J (t,s)i . For instance, for t = s = 6, q = 5, the MI from Eq. (10) deals with≈ 8.2 · 103 dimensions, which is ≈ 6 · 104 times lower than≈4.9 ·108 ones for the MI from Eq. (9). The described technique has been inspired by the intuition that probably two binary representatives of two different features interact on average better than two binary representatives of one feature (see App. A.5.1). Therefore, we believe that the BR modification retains the score’s awareness to the most interactions between features.\nSurely, on the one hand, the BR technique can also be applied to any state-of-the-art SFS-filter [2] or any existing interaction-aware one (RelaxMRMR [18], RCDSFS [3], and IWFS [24]), but the effect on them will not be striking breakthrough, since these filters exploit no more than 3 features in one MI, and the dimension reduction rate will thus be not more than (q/2)3 (e.g., ≈ 15.6 for q = 5). On the other hand, this technique is of a general nature and represents a self-contained contribution to ML community, since it may be applied with noticeable profit to SFS-based filters with MIs of higher orders (possibly not yet invented)."
    }, {
      "heading" : "3.4 CMICOT feature selection method",
      "text" : "We summarize Sec. 3.1–Sec. 3.3 in our novel feature selection method that is based on sequential forward selection strategy with the scoring function from Eq. (10). We refer to this FS method as CMICOT (Conditional Mutual Information with Complementary and Opposing Teams) and present its pseudo-code in Algorithm 1, which has a form of a SFS strategy with a specific algorithm to calculate the score (lines 10–19). In order to benefit from Prop. 1 and 2, one has to select s ≥ t, and, for simplicity, from here on in this paper we consider only equally limited teams, i.e., t = s.\nProposition 3. Let |B[f ]| ≤ ν, ∀f ∈ F , |F | ≤ M , and entropies in MIs are calculated over N instances, then O(iν2t2N) simple operations are needed to calculate the score J (t,t),bini and O(k2ν2t2MN) simple operations are needed to select top-k features by CMICOT from Alg. 1.\nLet us remind how each of our techniques contributes to the presented above computational complexity of the score. First, the factor t2 is an expected payment for the ability to be aware of t-way interactions (Sec. 3.1). Second, the two stage greedy technique from Sec. 3.2 makes the score’ computational complexity linearly depend on a SFS iteration i. Third, utilization of the BR technique from Sec. 3.3, on the one hand, seems to increase the computational complexity by the factor ν2, but, on the other\nhand, we know that it drastically reduces the sample complexity (i.e., the number of instances required to accurately estimate the used MIs). For simplicity, let us assume that each feature has 2ν values and is transformed to ν binary ones. If we do not use the BR technique, the complexity will be lower by the factor ν2 for the same number of instances N , but estimation of the MIs will require (2ν/2)2t times more instances to achieve the same level of accuracy as with the BRs. Hence, the BR technique actually reduces the computational complexity by the factor 22t(ν−1)/ν2. Note that the team size t can be used to trade off between the number of instances available in the sample dataset and the maximal number of features whose joint interaction could be taken into account in a SFS manner.\nFinally, for a given dataset and a given team size t, the score’s computational complexity linearly depends on the i-th SFS iteration, on the one hand, as in most state-of-the-art SFS-filters [2] like CMIM, MIFS, mRMR, JMI, etc. (see Eq. (2)–(3)). On the other hand, scores of existing interactionaware ones have either the same (O(i) for RCDFS [3]), or higher (O(M − i) for IWFS [24] and O(i2) for RelaxMRMR [18]) order of complexity w.r.t. i. Thus, we conclude that our FS method is not inferior in efficiency to all baseline filters, but is able to identify feature dependences of higher orders than these baselines."
    }, {
      "heading" : "4 Experimental evaluation",
      "text" : "We compare our CMICOT approach with (a) all known interaction-aware SFS-based filters (RelaxMRMR [18], IWFS [24], and RCDFS [3]); (b) the state-of-the-art filters [2] (MIFS, mRMR, CMIM, JMI, DISR, and FCBF (CBFS)); (c) and the idealistic but practically infeasible CMI method (see Sec. 2 and [2]). In our experiments, we consider t = 1, . . . , 10 to validate that CMICOT is able to detect interactions of a considerably higher order than its competitors.\nEvaluation on synthetic data. First, we study the ability to detect high-order feature dependencies using synthetic datasets where relevant and interacting features are a priory known. A synthetic dataset has feature set F , which contains a group of jointly interacting relevant features Fint, and a its target c is a deterministic function of Fint for half of examples (|F \\Fint| = 15 and |Fint| = 2, . . . , 11 in our experiments). The smaller k0 = min{k | Fint ⊆ Sk}, the more effective the considered FS method, since it builds the smaller set of features needed to construct the best possible classifier. We conduct an experiment where, first, we randomly sample 100 datasets from the predefined joint distribution (more details in Appendix C). Second, we calculate k0 for each of studied FS methods on these datasets. Finally, we average k0 over the datasets and present the results in Figure 1 (a). We see, first, that CMICOT with t ≥ |Fint| significantly outperforms all baselines, except the idealistic CMI method whose results are similar to CMICOT. This is expected, since CMI is infeasible only for large k, and, in App. F.2, we show that CMICOT is the closest approximation of true CMI among all baselines. Second, the team size t definitely responds to the number of interacted features, that provides an experimental evidence for ability of CMICOT to identify high-order feature interactions.\nEvaluation on benchmark real data. Following the state-of-the-art practice [6, 22, 2, 18, 24, 3], we conduct an extensive empirical evaluation of the effectiveness of our CMICOT approach on 10 large public datasets from the UCI ML Repo (that include the NIPS’2003 FS competition) and one private dataset from one of the most popular search engines7. We employ three state-of-theart classifiers: Naive Bayes Classifier (NBC), k-Nearest Neighbor (kNN), and AdaBoost [6] (see App. B). Their performance on a set of features is measured by means of AUC [2] (MAUC [9]) for a binary (multi-class) target variable. First, we apply each of the FS methods to select top-k features Sk for each dataset and for k = 1, .., 50 [2, 24, 3]. Given k ∈ {1, .., 50}, a dataset, and a certain classifier, we measure the performance of a FS method (1) in terms of the (M)AUC of the classifier built on the selected features Sk (2) and in terms of the rank of the FS method among the other FS methods w.r.t. (M)AUC. The resulting (M)AUC and rank averaged over all datasets are shown in Fig. 1(b,c) for kNN and AdaBoost. From these figures we see that our CMICOT for t = 68 method noticeably outperforms all baselines for the classification models kNN and AdaBoost9 starting from approximately k = 10. We reason this frontier by the size of the teams in CMICOT\n7The number of features, instances, and target classes varies from 85 to 5000, from 452 to 105, and from 2 to 26 respectively. More datasets’ characteristics and preprocessing can be found in Appendix D.\n8Our experimentation on CMICOT with different t = 1, . . . , 10 on our datasets showed that t = 5 and 6 are the most reasonable in terms of classifier performance (see Appendix E.1.1).\n9The results of CMICOT on NBC classifier are similar to the ones of other baselines. This is expected since NBC does not exploit high-order feature dependences, which is the key advantage of CMICOT. Note that\nmethod, which should select different teams more likely when |Si−1| > 2t (= 12 for t = 6). The curves on Fig. 1 (b,c) are obtained over a test set, while a 10-fold cross-validation [2, 18] is also applied for several key points (e.g. k = 10, 20, 50) to estimate the significance of differences in classification quality. The detailed results of this CV for k = 50 on representative datasets are given in Appendix E.2. A more comprehensive details on these and other experiments are in App. E and F.\nWe find that our approach either significantly outperforms baselines (most one for kNN and AdaBoost), or have non-significantly different difference with the other (most one for NBC). Note that the interaction awareness of RelaxMRMR, RCDFS and IWFS is apparently not enough to outperform CMIM, our strongest competitor. In fact, there is no comparison of RelaxMRMR and IWFS with CMIM in [3, 24], while RCDFS is outperformed by CMIM on some datasets including the only one utilized in both [18] and our work. One compares CMICOT with and without BR technique: on the one hand, we observed that CMICOT without BRs loses in performance to the one with BRs on the datasets with non-binary features, that emphasizes importance of the problem (PI.b); on the other hand, results on binary datasets (poker, ranking, and semeion; see App. E), where the CMICOT variants are the same, the effectiveness of our approach separately to the BR technique is established."
    }, {
      "heading" : "5 Conclusions",
      "text" : "We proposed a novel feature selection method CMICOT that is based on sequential forward selection and is able to identify high-order feature interactions. The technique based on a two stage greedy search and binary representatives of features makes our approach able to be effectively used on datasets of different sizes for restricted team sized t. We also empirically validated our approach for t up to 10 by means of 3 state-of-the-art classification models (NBC, kNN, and AdaBoost) on 10 publicly available benchmark datasets and compared it with known interaction-aware SFS-based filters (RelaxMRMR, IWFS, and RCDFS) and several state-of-the-art ones (CMIM, JMI, CBFS, and others). We conclude that our FS algorithm, unlike all competitor methods, is capable to detect interactions between up to t features. The overall performance of our algorithm is the best among the state-of-the-art competitors."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are grateful to Mikhail Parakhin for important remarks which resulted in significant improvement of the paper presentation.\nRelaxMRMR also showed its poorest performance on NBC in [18], while IWFS and RCDFS in [3, 24] didn’t consider NBC at all."
    } ],
    "references" : [ {
      "title" : "Using mutual information for selecting features in supervised neural net learning",
      "author" : [ "R. Battiti" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1994
    }, {
      "title" : "Conditional likelihood maximisation: a unifying framework for information theoretic feature selection",
      "author" : [ "G. Brown", "A. Pocock", "M.-J. Zhao", "M. Luján" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Feature selection with redundancycomplementariness dispersion",
      "author" : [ "Z. Chen", "C. Wu", "Y. Zhang", "Z. Huang", "B. Ran", "M. Zhong", "N. Lyu" ],
      "venue" : "arXiv preprint arXiv:1502.00231,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Elements of information theory",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Supervised and unsupervised discretization of continuous features",
      "author" : [ "J. Dougherty", "R. Kohavi", "M. Sahami" ],
      "venue" : "In ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1995
    }, {
      "title" : "Fast binary feature selection with conditional mutual information",
      "author" : [ "F. Fleuret" ],
      "venue" : "JMLR, 5:1531–1555,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Greedy function approximation: a gradient boosting machine",
      "author" : [ "J.H. Friedman" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "An introduction to variable and feature selection",
      "author" : [ "I. Guyon", "A. Elisseeff" ],
      "venue" : "JMLR, 3:1157–1182,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "A simple generalisation of the area under the roc curve for multiple class classification problems",
      "author" : [ "D.J. Hand", "R.J. Till" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Distribution of mutual information",
      "author" : [ "M. Hutter" ],
      "venue" : "NIPS, 1:399–406,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "Analyzing attribute dependencies",
      "author" : [ "A. Jakulin", "I. Bratko" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "Feature selection and feature extraction for text categorization",
      "author" : [ "D.D. Lewis" ],
      "venue" : "In Proceedings of the workshop on Speech and Natural Language,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1992
    }, {
      "title" : "High-dimensional structured feature screening using binary markov random fields",
      "author" : [ "J. Liu", "C. Zhang", "C.A. McCarty", "P.L. Peissig", "E.S. Burnside", "D. Page" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Information-theoretic feature selection in microarray data using variable complementarity",
      "author" : [ "P.E. Meyer", "C. Schretter", "G. Bontempi" ],
      "venue" : "IEEE Journal of STSP,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Estimation of entropy and mutual information",
      "author" : [ "L. Paninski" ],
      "venue" : "Neural comput.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2003
    }, {
      "title" : "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",
      "author" : [ "H. Peng", "F. Long", "C. Ding" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2005
    }, {
      "title" : "A review of feature selection methods based on mutual information",
      "author" : [ "J.R. Vergara", "P.A. Estévez" ],
      "venue" : "Neural Computing and Applications,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Can high-order dependencies improve mutual information based feature selection",
      "author" : [ "N.X. Vinh", "S. Zhou", "J. Chan", "J. Bailey" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Feature selection with conditional mutual information maximin in text categorization",
      "author" : [ "G. Wang", "F.H. Lochovsky" ],
      "venue" : "In ACM CIKM,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "A direct method of nonparametric measurement selection",
      "author" : [ "A.W. Whitney" ],
      "venue" : "Computers, IEEE Transactions on,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1971
    }, {
      "title" : "Feature selection based on joint mutual information",
      "author" : [ "H. Yang", "J. Moody" ],
      "venue" : "In Proceedings of international ICSC symposium on advances in intelligent data analysis,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1999
    }, {
      "title" : "Efficient feature selection via analysis of relevance and redundancy",
      "author" : [ "L. Yu", "H. Liu" ],
      "venue" : "JMLR, 5:1205–1224,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2004
    }, {
      "title" : "Robust feature selection by mutual information distributions. In UAI, pages 577–584",
      "author" : [ "M. Zaffalon", "M. Hutter" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2002
    }, {
      "title" : "A novel feature selection method considering feature interaction",
      "author" : [ "Z. Zeng", "H. Zhang", "R. Zhang", "C. Yin" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Searching for interacting features in subset selection",
      "author" : [ "Z. Zhao", "H. Liu" ],
      "venue" : "Intelligent Data Analysis,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Methods of feature selection is an important topic of machine learning [8, 2, 17], since they improve performance of learning systems while reducing their computational costs.",
      "startOffset" : 71,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Methods of feature selection is an important topic of machine learning [8, 2, 17], since they improve performance of learning systems while reducing their computational costs.",
      "startOffset" : 71,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "Methods of feature selection is an important topic of machine learning [8, 2, 17], since they improve performance of learning systems while reducing their computational costs.",
      "startOffset" : 71,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "Feature selection methods are usually grouped into three main categories: wrapper, embedded, and filter methods [8].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 16,
      "context" : "In this paper, we focus on most popular filters, which are based on mutual information (MI) and apply the sequential forward selection (SFS) strategy to obtain an optimal subset of features [17].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 16,
      "context" : "A challenging task is to account for such interactions [17].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "Existing SFS-based filters [18, 3, 24] are able to account for interactions of only up to 3 features.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "Existing SFS-based filters [18, 3, 24] are able to account for interactions of only up to 3 features.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 23,
      "context" : "Existing SFS-based filters [18, 3, 24] are able to account for interactions of only up to 3 features.",
      "startOffset" : 27,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "The CMI filter is believed to be a “north star\" for vast majority of the state-of-the-art filters [2].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "The mutual information (MI) of two random variables f and g is defined as I(f ; g) = H(f) + H(g) − H(f, g), where H(f) = −E [log P(f)] is Shannon’s entropy [4]2.",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 14,
      "context" : "Given sample data, entropy (and, hence, MI and conditional MI) of discrete variables could be simply estimated using the empirical frequencies (the point estimations) [15] or in a more sophisticated way (e.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 9,
      "context" : ", by means of the Bayesian framework [10]).",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "More details on different entropy estimators can be found in [15].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : "In our paper, we focus on this simple but commonly studied FS objective in the context of MI-based filters [2], though there is a wide variety of other definitions of optimal subset of features [17] (e.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "In our paper, we focus on this simple but commonly studied FS objective in the context of MI-based filters [2], though there is a wide variety of other definitions of optimal subset of features [17] (e.",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 19,
      "context" : "The most popular one is the sequential forward selection (SFS) [20, 23, 17], which starts with an empty set (S0 := ∅) and iteratively increases it by adding one currently unselected feature on each step (Si := Si−1 ∪ {fi}, i = 1, .",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 22,
      "context" : "The most popular one is the sequential forward selection (SFS) [20, 23, 17], which starts with an empty set (S0 := ∅) and iteratively increases it by adding one currently unselected feature on each step (Si := Si−1 ∪ {fi}, i = 1, .",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "The most popular one is the sequential forward selection (SFS) [20, 23, 17], which starts with an empty set (S0 := ∅) and iteratively increases it by adding one currently unselected feature on each step (Si := Si−1 ∪ {fi}, i = 1, .",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "A trivial feature selection approach is to select top-k features in terms of their MI with the class label c [12].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "This technique is referred to as MIM [2] and is a particular case of the SFS strategy based on score J i (f) := I(c; f).",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16].",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16].",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 13,
      "context" : "Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16].",
      "startOffset" : 155,
      "endOffset" : 162
    }, {
      "referenceID" : 18,
      "context" : "Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16].",
      "startOffset" : 155,
      "endOffset" : 162
    }, {
      "referenceID" : 15,
      "context" : "Among methods that take into account the redundancy between features [2, 17], the most popular and widely applicable ones are MIFS [1], JMI [21, 14], CMIM [6, 19], and mRMR [16].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "[2] unified these techniques under one framework, where they are different low-order approximations of CMI feature selection approach.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Therefore, this technique is not usable in the case of small samples and in the cases, when a large number of features should be selected [2].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "An important but usually neglected aspect in FS methods is feature complementariness [8, 3] (also known as synergy [24] and interaction [11]).",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "An important but usually neglected aspect in FS methods is feature complementariness [8, 3] (also known as synergy [24] and interaction [11]).",
      "startOffset" : 85,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "An important but usually neglected aspect in FS methods is feature complementariness [8, 3] (also known as synergy [24] and interaction [11]).",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "An important but usually neglected aspect in FS methods is feature complementariness [8, 3] (also known as synergy [24] and interaction [11]).",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : "In general, complementary features are those that appear to have low relevance to the target class c individually, but whose combination is highly relevant [25, 24].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "In general, complementary features are those that appear to have low relevance to the target class c individually, but whose combination is highly relevant [25, 24].",
      "startOffset" : 156,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : "A reader interested in a formalized concept of feature relevance, redundancy, and interaction is referred to [11] and [24].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 23,
      "context" : "A reader interested in a formalized concept of feature relevance, redundancy, and interaction is referred to [11] and [24].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "RelaxMRMR [18] is a modification of the mRMR method, whose scoring function in Eq.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 2,
      "context" : "The method RCDFS [3] is a special case of Eq.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "The approach IWFS [24] is based on the following idea: at each step i, for each unselected feature f ∈ F \\ Si, one calculates the next step score Ji+1(f) as the current score Ji(f) multiplied by a certain measure of interaction between this feature f and the feature fi selected at the current step.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "The described approach is inspired by methods of greedy learning of ensembles of decision trees [7], where an ensemble of trees is built by sequentially adding a decision tree that maximizes the gain in learning quality.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "(7)–(9)), which is a linear dependence on an iteration i as in the most state-of-the-art SFS-based filters [2].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "2, a FS method that is based on calculation of MI over more than three features is usually not popular in practice, since a large number of features implies a large dimension of their joint distribution that leads to a large number of instances required to accurately estimate the MI [2].",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 17,
      "context" : ", continuous features are usually preprocessed into discrete variables with q ≥ 5 [18]), then the dimension of the joint distribution of features in Eq.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : ", by equal-width, equal-frequency binnings [5], MDL [22, 3], etc.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : ", by equal-width, equal-frequency binnings [5], MDL [22, 3], etc.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : ", by equal-width, equal-frequency binnings [5], MDL [22, 3], etc.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "Surely, on the one hand, the BR technique can also be applied to any state-of-the-art SFS-filter [2] or any existing interaction-aware one (RelaxMRMR [18], RCDSFS [3], and IWFS [24]), but the effect on them will not be striking breakthrough, since these filters exploit no more than 3 features in one MI, and the dimension reduction rate will thus be not more than (q/2)(3) (e.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "Surely, on the one hand, the BR technique can also be applied to any state-of-the-art SFS-filter [2] or any existing interaction-aware one (RelaxMRMR [18], RCDSFS [3], and IWFS [24]), but the effect on them will not be striking breakthrough, since these filters exploit no more than 3 features in one MI, and the dimension reduction rate will thus be not more than (q/2)(3) (e.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 2,
      "context" : "Surely, on the one hand, the BR technique can also be applied to any state-of-the-art SFS-filter [2] or any existing interaction-aware one (RelaxMRMR [18], RCDSFS [3], and IWFS [24]), but the effect on them will not be striking breakthrough, since these filters exploit no more than 3 features in one MI, and the dimension reduction rate will thus be not more than (q/2)(3) (e.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 23,
      "context" : "Surely, on the one hand, the BR technique can also be applied to any state-of-the-art SFS-filter [2] or any existing interaction-aware one (RelaxMRMR [18], RCDSFS [3], and IWFS [24]), but the effect on them will not be striking breakthrough, since these filters exploit no more than 3 features in one MI, and the dimension reduction rate will thus be not more than (q/2)(3) (e.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "Finally, for a given dataset and a given team size t, the score’s computational complexity linearly depends on the i-th SFS iteration, on the one hand, as in most state-of-the-art SFS-filters [2] like CMIM, MIFS, mRMR, JMI, etc.",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, scores of existing interactionaware ones have either the same (O(i) for RCDFS [3]), or higher (O(M − i) for IWFS [24] and O(i(2)) for RelaxMRMR [18]) order of complexity w.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 23,
      "context" : "On the other hand, scores of existing interactionaware ones have either the same (O(i) for RCDFS [3]), or higher (O(M − i) for IWFS [24] and O(i(2)) for RelaxMRMR [18]) order of complexity w.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "On the other hand, scores of existing interactionaware ones have either the same (O(i) for RCDFS [3]), or higher (O(M − i) for IWFS [24] and O(i(2)) for RelaxMRMR [18]) order of complexity w.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : "We compare our CMICOT approach with (a) all known interaction-aware SFS-based filters (RelaxMRMR [18], IWFS [24], and RCDFS [3]); (b) the state-of-the-art filters [2] (MIFS, mRMR, CMIM, JMI, DISR, and FCBF (CBFS)); (c) and the idealistic but practically infeasible CMI method (see Sec.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 23,
      "context" : "We compare our CMICOT approach with (a) all known interaction-aware SFS-based filters (RelaxMRMR [18], IWFS [24], and RCDFS [3]); (b) the state-of-the-art filters [2] (MIFS, mRMR, CMIM, JMI, DISR, and FCBF (CBFS)); (c) and the idealistic but practically infeasible CMI method (see Sec.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "We compare our CMICOT approach with (a) all known interaction-aware SFS-based filters (RelaxMRMR [18], IWFS [24], and RCDFS [3]); (b) the state-of-the-art filters [2] (MIFS, mRMR, CMIM, JMI, DISR, and FCBF (CBFS)); (c) and the idealistic but practically infeasible CMI method (see Sec.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "We compare our CMICOT approach with (a) all known interaction-aware SFS-based filters (RelaxMRMR [18], IWFS [24], and RCDFS [3]); (b) the state-of-the-art filters [2] (MIFS, mRMR, CMIM, JMI, DISR, and FCBF (CBFS)); (c) and the idealistic but practically infeasible CMI method (see Sec.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "Following the state-of-the-art practice [6, 22, 2, 18, 24, 3], we conduct an extensive empirical evaluation of the effectiveness of our CMICOT approach on 10 large public datasets from the UCI ML Repo (that include the NIPS’2003 FS competition) and one private dataset from one of the most popular search engines7.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "Following the state-of-the-art practice [6, 22, 2, 18, 24, 3], we conduct an extensive empirical evaluation of the effectiveness of our CMICOT approach on 10 large public datasets from the UCI ML Repo (that include the NIPS’2003 FS competition) and one private dataset from one of the most popular search engines7.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "Following the state-of-the-art practice [6, 22, 2, 18, 24, 3], we conduct an extensive empirical evaluation of the effectiveness of our CMICOT approach on 10 large public datasets from the UCI ML Repo (that include the NIPS’2003 FS competition) and one private dataset from one of the most popular search engines7.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "Following the state-of-the-art practice [6, 22, 2, 18, 24, 3], we conduct an extensive empirical evaluation of the effectiveness of our CMICOT approach on 10 large public datasets from the UCI ML Repo (that include the NIPS’2003 FS competition) and one private dataset from one of the most popular search engines7.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "Following the state-of-the-art practice [6, 22, 2, 18, 24, 3], we conduct an extensive empirical evaluation of the effectiveness of our CMICOT approach on 10 large public datasets from the UCI ML Repo (that include the NIPS’2003 FS competition) and one private dataset from one of the most popular search engines7.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "Following the state-of-the-art practice [6, 22, 2, 18, 24, 3], we conduct an extensive empirical evaluation of the effectiveness of our CMICOT approach on 10 large public datasets from the UCI ML Repo (that include the NIPS’2003 FS competition) and one private dataset from one of the most popular search engines7.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "We employ three state-of-theart classifiers: Naive Bayes Classifier (NBC), k-Nearest Neighbor (kNN), and AdaBoost [6] (see App.",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "Their performance on a set of features is measured by means of AUC [2] (MAUC [9]) for a binary (multi-class) target variable.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "Their performance on a set of features is measured by means of AUC [2] (MAUC [9]) for a binary (multi-class) target variable.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "1 (b,c) are obtained over a test set, while a 10-fold cross-validation [2, 18] is also applied for several key points (e.",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : "1 (b,c) are obtained over a test set, while a 10-fold cross-validation [2, 18] is also applied for several key points (e.",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "In fact, there is no comparison of RelaxMRMR and IWFS with CMIM in [3, 24], while RCDFS is outperformed by CMIM on some datasets including the only one utilized in both [18] and our work.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 23,
      "context" : "In fact, there is no comparison of RelaxMRMR and IWFS with CMIM in [3, 24], while RCDFS is outperformed by CMIM on some datasets including the only one utilized in both [18] and our work.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "In fact, there is no comparison of RelaxMRMR and IWFS with CMIM in [3, 24], while RCDFS is outperformed by CMIM on some datasets including the only one utilized in both [18] and our work.",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 17,
      "context" : "RelaxMRMR also showed its poorest performance on NBC in [18], while IWFS and RCDFS in [3, 24] didn’t consider NBC at all.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "RelaxMRMR also showed its poorest performance on NBC in [18], while IWFS and RCDFS in [3, 24] didn’t consider NBC at all.",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 23,
      "context" : "RelaxMRMR also showed its poorest performance on NBC in [18], while IWFS and RCDFS in [3, 24] didn’t consider NBC at all.",
      "startOffset" : 86,
      "endOffset" : 93
    } ],
    "year" : 2016,
    "abstractText" : "This study introduces a novel feature selection approach CMICOT, which is a further evolution of filter methods with sequential forward selection (SFS) whose scoring functions are based on conditional mutual information (MI). We state and study a novel saddle point (max-min) optimization problem to build a scoring function that is able to identify joint interactions between several features. This method fills the gap of MI-based SFS techniques with high-order dependencies. In this high-dimensional case, the estimation of MI has prohibitively high sample complexity. We mitigate this cost using a greedy approximation and binary representatives what makes our technique able to be effectively used. The superiority of our approach is demonstrated by comparison with recently proposed interactionaware filters and several interaction-agnostic state-of-the-art ones on ten publicly available benchmark datasets.",
    "creator" : null
  }
}