{
  "name" : "728f206c2a01bf572b5940d7d9a8fa4c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Wasserstein Training of Restricted Boltzmann Machines",
    "authors" : [ "Grégoire Montavon", "Klaus-Robert Müller" ],
    "emails" : [ "gregoire.montavon@tu-berlin.de", "klaus-robert.mueller@tu-berlin.de", "marco.cuturi@ensae.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Boltzmann machines [1] are powerful generative models that can be used to approximate a large class of real-world data distributions, such as handwritten characters [9], speech segments [7], or multimodal data [16]. Boltzmann machines share similarities with neural networks in their capability to extract features at multiple scales, and to build well-generalizing hierarchical data representations [15, 13]. The restricted Boltzmann machine (RBM) is a special type of Boltzmann machine composed of one layer of latent variables, and defining a probability distribution pθ(x) over a set of d binary observed variables whose state is represented by the binary vector x ∈ {0, 1}d, and with a parameter vector θ to be learned.\nGiven an empirical probability distribution p̂(x) = 1N ∑N n=1 δxn where (xn)n is a list of N observations in {0, 1}d, an RBM can be trained using information-theoretic divergences (see for example [12]) by minimizing with respect to θ a divergence ∆(p̂, pθ) between the sample empirical measure p̂ and the modeled distribution pθ. When ∆ is for instance the KL divergence, this approach results in the well-known Maximum Likelihood Estimator (MLE), which yields gradients for the θ of the form\n∇θKL(p̂ ‖ pθ) = − 1\nN N∑ n=1 ∇θ log pθ(xn) = − 〈 ∇θ log pθ(x) 〉 p̂ , (1)\nwhere the bracket notation 〈·〉p indicates an expectation with respect to p. Alternative choices for ∆ are the Bhattacharrya/Hellinger and Euclidean distances between distributions, or more generally\n∗Also with the Department of Brain and Cognitive Engineering, Korea University.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nF -divergences or M -estimators [10]. They all result in comparable gradient terms, that try to adjust θ so that the fitting terms pθ(xn) grow as large as possible.\nWe explore in this work a different scenario: what if θ is chosen so that pθ(x) is large, on average, when x is close to a data point xn in some sense, but not necessarily when x coincides exactly with xn? To adopt such a geometric criterion, we must first define what closeness between observations means. In almost all applications of Boltzmann machines, such a metric between observations is readily available: One can for example consider the Hamming distance between binary vectors, or any other metric motivated by practical considerations2. This being done, the geometric criterion we have drawn can be materialized by considering for ∆ the Wasserstein distance [20] (a.k.a. the Kantorovich or the earth mover’s distance [14]) between measures. This choice was considered in theory by [2], who proved its statistical consistency, but was never considered practically to the best of our knowledge. This paper describes a practical derivation for a minimum Kantorovich distance estimator [2] for Boltzmann machines, which can scale up to tens of thousands of observations. As will be described in this paper, recent advances in the fast approximation of Wasserstein distances [5] and their derivatives [6] play an important role in the practical implementation of these computations. Before describing this approach in detail, we would like to insist that measuring goodness-of-fit with the Wasserstein distance results in a considerably different perspective than that provided by a Kullback-Leibler/MLE approach. This difference is illustrated in Figure 1, where a probability pθ can be close from a KL perspective to a given empirical measure p̂, but far from the same measure p in the Wasserstein sense. Conversely, a different probability pθ′ can miss the mark from a KL viewpoint but achieve a low Wasserstein distance to p̂. Before proceeding to the rest of this paper, let us mention that Wasserstein distances have a broad appeal for machine learning. That distance was for instance introduced in the context of supervised inference by [8], who used it to compute a predictive loss between the output of a multilabel classifier against its ground truth, or for domain adaptation, by [4]."
    }, {
      "heading" : "2 Minimum Wasserstein Distance Estimation",
      "text" : "Consider two probabilities p, q in P(X ), the set of probabilities on X = {0, 1}d. Namely, two maps p, q in RX+ such that ∑ x p(x) = ∑ x q(x) = 1, where we omit x ∈ X under the summation sign. Consider a cost function defined on X ×X , typically a distance D : X ×X → R+. Given a constant γ ≥ 0, the γ-smoothed Wasserstein distance [5] is equal to\nWγ(p, q) = min π∈Π(p,q) 〈D(x,x′)〉π − γH(π), (2)\nwhere Π(p, q) is the set of joint probabilities π on X × X such that ∑x′ π(x,x′) = p(x),∑ x π(x,x\n′) = q(x′) and H(π) = −∑xx′ π(x,x′) log π(x,x′) is the Shannon entropy of π. This optimization problem, a strictly convex program, has an equivalent dual formulation [6] which involves instead two real-valued functions α, β in RX and which plays an important role in this paper:\nWγ(p, q) = max α,β∈RX 〈α(x)〉p + 〈β(x′)〉q − γ ∑ xx′ e 1 γ (α(x)+β(x ′)−D(x,x′))−1. (3)\n2When using the MLE principle, metric considerations play a key role to define densities pθ , e.g. the reliance of Gaussian densities on Euclidean distances. This is the kind of metric we take for granted in this work.\nSmooth Wasserstein Distances The “true” Wasserstein distance corresponds to the case where γ = 0, that is when Equation (2) is stripped of the entropic term. One can easily verify that that definition matches the usual linear program used to describe Wasserstein/EMD distances [14]. When γ → 0 in Equation (3), one also recovers the Kantorovich dual formulation, because the rightmost regularizer converges to the indicator function of the feasible set of the dual optimal transport problem, α(x) + β(x′) ≤ D(x,x′). We consider in this paper the case γ > 0 because it was shown in [5] to considerably facilitate computations, and in [6] to result in a divergenceWγ(p, q) which, unlike the case γ = 0, is differentiable w.r.t to the first variable. Looking at the dual formulation in Equation (3), one can see that this gradient is equal to α?, the centered optimal dual variable (the centering step for α? ensures the orthogonality with respect to the simplex constraint).\nSensitivity analysis gives a clear interpretation to the quantity α?(x): It measures the cost for each unit of mass placed by p at x when computing the Wasserstein distance Wγ(p, q). To decrease Wγ(p, q), it might thus be favorable to transfer mass in p from points where α(x) is high to place it on points where α(x) is low. This idea can be used, by a simple application of the chain rule, to minimize, given a fixed target probability p, the quantityWγ(pθ, p) with respect to θ. Proposition 1. Let pθ(x) = 1Z e\n−Fθ(x) be a parameterized family of probability distributions where Fθ(x) is a differentiable function of θ ∈ Θ and we write Gθ = 〈∇θFθ(x)〉pθ . Let α? be the centered optimal dual solution of Wγ(pθ, p) as described in Equation (3). The gradient of the smoothed Wasserstein distance with respect to θ is given by\n∇θWγ(pθ, p) = 〈 α?(x) 〉 pθ Gθ − 〈 α?(x)∇θFθ(x)) 〉 pθ . (4)\nProof. This result is a direct application of the chain rule: We have ∇θWγ(pθ, p) = (∂pθ ∂θ )T ∂Wγ(pθ, q) ∂pθ .\nAs mentioned in [6], the rightmost term is the optimal dual variable (the Kantorovich potential) ∂Wγ(pθ, q)/∂pθ = α?. The Jacobian (∂pθ/∂θ) is a linear map Θ→ X . For a given x′,\n∂pθ(x ′)/∂θ = pθ(x ′)Gθ −∇Fθ(x′)pθ(x′).\nAs a consequence, ( ∂pθ ∂θ )T α? is the integral w.r.t. x′ of the term above multiplied by α?(x′), which results in Equation (4).\nComparison with the KL Fitting Error The target distribution p plays a direct role in the formation of the gradient of KL(p̂ ‖ pθ) w.r.t. θ through the term 〈∇θFθ(x)〉p in Equation (1). The Wasserstein gradient incorporates the knowledge of p in a different way, by considering, on the support of pθ only, points x that correspond to high potentials (costs) α(x) when computing the distance of pθ to p. A high potential at x means that the probability pθ(x) should be lowered if one were to decreaseWγ(pθ, p), by varying θ accordingly.\nSampling Approximation The gradient in Equation (4) is intractable, since it involves solving an optimal (smoothed) transport problem over probabilities defined on 2d states. In practice, we replace expectations w.r.t pθ by an empirical distribution formed by sampling from the model pθ (e.g. the PCD sample [18]). Given a sample (x̃n)n of size Ñ generated by the model, we define\np̂θ = ∑Ñ n=1 δx̃n/Ñ . The tilde is used to differentiate the sample generated by the model from the empirical observations. Because the dual potential α? is centered and p̂θ is a measure with uniform weights, 〈α?(x)〉p̂θ = 0 which simplifies the approximation of the gradient to\n∇̂θWγ(pθ, p̂) = − 1\nÑ Ñ∑ n=1 α̂?(x̃n)∇θFθ(x̃n) (5)\nwhere α̂? is the solution of the discrete smooth Wasserstein dual between the two empirical distributions p̂ and p̂θ, which have respectively supports of size N and Ñ . In practical terms, α̂? is a vector of size Ñ , one coefficient for each PCD sample, which can be computed by following the algorithm below [6]. To keep notations simple, we describe it in terms of generic probabilities p and q, having in mind these are in practice the training and simulated empirical measures p̂ and p̂θ.\nComputing α? When γ > 0, the optimal variable α? corresponding toWγ(p, q) can be recovered through the Sinkhorn algorithm with a cost which grows as the product |p||q| of the sizes of the support of p and q, where |p| = ∑x 1p(x)>0. The algorithm is well known but we adapt it here to our setting, see [6, Alg.3] for a more precise description. To ease notations, we consider an arbitrary ordering of X , a set of cardinal 2d, and identify its elements with indices 1 ≤ i ≤ 2d. Let I = (i1, · · · , i|p|) be the ordered family of indices in the set {i | p(i) > 0} and define J accordingly for q. I and J have respective lengths |p| and |q|. Form the matrix K = [e−D(i,j)/γ ]i∈I,j∈J of size |p| and |q|. Choose now two positive vectors u ∈ R|p|++ and v ∈ R|q|++ at random, and repeat until u, v converge in some metric the operations u← p/(Kv), v ← q/(KTu). Upon convergence, the optimal variable α? is zero everywhere except for α?(ia) = log(ua/ũ)/γ where 1 ≤ a ≤ |p| and ũ is the geometric mean of vector u (which ensures that α? is centered)."
    }, {
      "heading" : "3 Application to Restricted Boltzmann Machines",
      "text" : "The restricted Boltzmann machine (RBM) is a generative model of binary data that is composed of d binary observed variables and h binary explanatory variables. The vector x ∈ {0, 1}d represents the state of observed variables, and the vector y ∈ {0, 1}h represents the state of explanatory variables. The RBM associates to each configuration x of observed variables a probability pθ(x) defined as pθ(x) = ∑ ye −Eθ(x,y)/Zθ, where Eθ(x,y) = −aTx− ∑h j=1 yj(w T j x + bj) is called the energy\nand Zθ = ∑ x,y e −Eθ(x,y) is the partition function that normalizes the probability distribution to one. The parameters θ = (a, {wj , bj}hj=1) of the RBM are learned from the data. Knowing the state x of the observed variables, the explanatory variables are independent Bernoulli-distributed with Pr(yj = 1|x) = σ(wTj x + bj), where σ is the logistic map z 7→ (1 + e−z)−1. Conversely, knowing the state y of the explanatory variables, the observed variables on which the probability distribution is defined can also be sampled independently, leading to an efficient alternate Gibbs sampling procedure for pθ. In this RBM model, explanatory variables can be analytically marginalized, which yields the following probability model:\npθ(x) = e −Fθ(x)/Z ′θ, where Fθ(x) = −aTx− ∑h j=1 log(1 + exp(w T j x+ bj)) is the free energy associated to this model\nand Z ′θ = ∑ x e −Fθ(x) is the partition function.\nWasserstein Gradient of the RBM Having written the RBM in its free energy form, the Wasserstein gradient can be obtained by computing the gradient of Fθ(x) and injecting it in Equation (5):\n∇̂wjWγ(p̂, pθ) = 〈 α?(x)σ(zj)x 〉 p̂θ ,\nwhere zj = wTj x + bj . Gradients with respect to parameters a and {bj}j can also be obtained by the same means. In comparison, the gradient of the KL divergence is given by\n∇̂wjKL(p̂ ‖ pθ) = 〈 σ(zj)x 〉 p̂θ − 〈 σ(zj)x 〉 p̂ .\nWhile the Wasserstein gradient can in the same way as the KL gradient be expressed in a very simple form, the first one is not sum-decomposable. A simple manifestation of the non-decomposability occurs for Ñ = 1 (smallest possible sample size): In that case, α(x̃n) = 0 due to the centering constraint (see Section 2), thus making the gradient zero.\nStability and KL Regularization Unlike the KL gradient, the Wasserstein gradient depends only indirectly on the data distribution p̂. This is a problem when the sample p̂θ generated by the model strongly differs from the examples coming from p̂, because there is no weighting (α(x̃n))n of the generated sample that can represent the desired direction in the parameter space Θ. In that case, the Wasserstein gradient will point to a bad local minimum. Closeness between the two empirical samples from this optimization perspective can be ensured by adding a regularization term to the objective that incorporates both the usual quadratic containment term, but also the KL term, that forces proximity to p̂ due to the direct dependence of its gradient on it. The optimization problem becomes:\nmin θ∈Θ\nWγ(p̂, pθ) + λ · Ω(θ) with Ω(θ) = KL(p̂ ‖ pθ) + η · (‖a‖2 + ∑ j‖wj‖2)\nstarting at point θ0 = arg minθ∈Θ Ω(θ), and where λ, η are two regularization hyperparameters that must be selected. Determining the starting point θ0 is analogous to having an initial pretraining phase. Thus, the proposed Wasserstein procedure can also be seen as finetuning a standard RBM, and forcing the finetuning not to deviate too much from the pretrained solution."
    }, {
      "heading" : "4 Experiments",
      "text" : "We perform several experiments that demonstrate that Wasserstein-trained RBMs learn distributions that are better from a metric perspective. First, we explore what are the main characteristics of a learned distribution that optimizes the Wasserstein objective. Then, we investigate the usefulness of these learned models on practical problems, such as data completion and denoising, where the metric between observations occurs in the performance evaluation. We consider three datasets: MNIST-small, a subsampled version of the original MNIST dataset [11] with only the digits “0” retained, a subset of the UCI PLANTS dataset [19] containing the geographical spread of plants species, and MNIST-code, 128-dimensional code vectors associated to each MNIST digit (additional details in the supplement)."
    }, {
      "heading" : "4.1 Training, Validation and Evaluation",
      "text" : "All RBM models that we investigate are trained in full batch mode, using for p̂θ the PCD approximation [18] of pθ, where the sample is refreshed at each gradient update by one step of alternate Gibbs sampling, starting from the sample at the previous time step. We choose a PCD sample of same size as the training set (N = Ñ ). The coefficients α1, . . . , αÑ occurring in the Wasserstein gradient are obtained by solving the smoothed Wasserstein dual between p̂ and p̂θ, with smoothing parameter γ = 0.1 and distance D(x,x′) = H(x,x′)/〈H(x,x′)〉p̂, where H denotes the Hamming distance between two binary vectors. We use the centered parameterization of the RBM for gradient descent [13, 3]. We perform holdout validation on the quadratic containment coefficient η ∈ {10−4, 10−3, 10−2}, and on the KL weighting coefficient λ ∈ {0, 10−1, 100, 101,∞}. The number of hidden units of the RBM is set heuristically to 400 for all datasets. The learning rate is set heuristically to 0.01(λ−1) during the pretraining phase and modified to 0.01 min(1, λ−1) when training on the final objective. The Wasserstein distanceWγ(p̂θ, p̂) is computed between the whole test distribution and the PCD sample at the end of the training procedure. This sample is a fast approximation of the true unbiased sample, that would otherwise have to be generated by annealing or enumeration of the states (see the supplement for a comparison of PCD and AIS samples)."
    }, {
      "heading" : "4.2 Results and Analysis",
      "text" : "The contour plots of Figure 2 show the effect of hyperparameters λ and η on the Wasserstein distance. For λ =∞, only the KL regularizer is active, which is equivalent to minimizing a standard RBM. As we reduce the amount of regularization, the Wasserstein distance becomes effectively minimized and thus smaller. If λ is chosen too small, the Wasserstein distance increases again, for the stability reasons mentioned in Section 3. In all our experiments, we observed that KL pretraining was necessary in order to reach low Wasserstein distance. Not doing so leads to degenerate solutions. The relation between hyperparameters and minimization criteria is consistent across datasets: In all cases, the Wasserstein RBM produces lower Wasserstein distance than a standard RBM.\nSamples generated by the standard RBM and the Wasserstein RBM (more precisely their PCD approximation) are shown in Figure 3. The RBM-W produces a reduced set of clean prototypical examples, with less noise than those produced by a regular RBM. All zeros generated by RBM-W\nhave well-defined contours and a round shape but do not reproduce the variety of shapes present in the data. Similarly, the plants territorial spreads generated by the RBM-W form compact and contiguous regions that are prototypical of real spreads, but are less diverse than the data or the sample generated by the standard RBM. Finally, the RBM-W generates codes that, when decoded, are closer to actual MNIST digits.\nThe PCA plots of Figure 4 superimpose to the true data distribution (in gray) the distributions generated by the standard RBM (in blue) and the Wasserstein RBM (in red). In particular, the plots show the projected distributions on the first two PCA components of the true distribution. While the standard RBM distribution uniformly covers the data, the one generated by the RBM-W consists of a finite set of small dense clusters that are scattered across the input distribution. In other words, the Wasserstein model is biased towards these clusters, and systematically ignores other regions.\nAt the bottom of Figure 4, we analyze the effect of the Wasserstein smoothing parameter γ on the learned distribution, with γ = 0.025, 0.05, 0.1, 0.2, 0.4. We observe on all datasets that the stronger the smoothing, the stronger the shrinkage effect. Although the KL-generated distributions shown in blue may look better (the red distribution strongly departs visually from the data distribution), the red distribution is actually superior if considering the smooth Wasserstein distance as a performance metric, as shown in Figure 2."
    }, {
      "heading" : "4.3 Validating the Shrinkage Effect",
      "text" : "To verify that the shrinkage effect observed in Figure 4 is not a training artefact, but a truly expected property of the modeled distribution, we analyze this effect for a simple distribution for which the parameter space can be enumerated. Figure 5 plots the Wasserstein distance between samples of size 100 of a 10-dimensional Gaussian distribution p ∼ N (0, I), and a parameterized model of that distribution pθ ∼ N (0, θ2I), where θ ∈ [0, 1]. The parameter θ can be interpreted as a shrinkage\nparameter. The Wasserstein distance is computed using the cityblock or euclidean metric, both rescaled such that the expected distance between pairs of points is 1.\nInterestingly, for all choices of Wasserstein smoothing parameters γ, and even for the true Wasserstein distance (γ = 0, computed here using the OpenCV library), the best model pθ in the empirical Wasserstein sense is a shrinked version of p (i.e. with θ < 1). When the smoothing is strong enough, the best parameter becomes θ = 0 (i.e. Dirac distribution located at the origin). Overall, this experiment gives a training-independent validation for our observation that Wasserstein RBMs learn shrinked cluster-like distributions. Note that the finite sample size prevents the Wasserstein distance to reach zero, and always favors shrinked models."
    }, {
      "heading" : "4.4 Data Completion and Denoising",
      "text" : "In order to demonstrate the practical relevance of Wasserstein distance minimization, we apply the learned models to the task of data completion and data denoising, for which the use of a metric is crucial: Data completion and data denoising performance is generally measured in terms of distance between the true data and the completed or denoised data (e.g. Euclidean distance for real-valued data, or Hamming distanceH for binary data). Remotely located probability mass that may result from simple KL minimization would incur a severe penalty on the completion and denoising performance metric. Both tasks have useful practical applications: Data completion can be used as a first step when applying discriminative learning (e.g. neural networks or SVM) to data with missing features. Data denoising can be used as a dimensionality reduction step before training a supervised model. Let the input x = [v,h] be composed of d− k visible variables v and k hidden variables h.\nData Completion The setting of the data completion experiment is illustrated in Figure 6 (top). The distribution pθ(x|v) over possible reconstructions can be sampled from using an alternate Gibbs sampler, or by enumeration. The expected Hamming distance between the true state x? and the reconstructed state modeled by the distribution pθ(x|v) is given by iterating on the 2k possible reconstructions:\nE = ∑h pθ(x |v) · H(x,x?) where h ∈ {0, 1}k. Since the reconstruction is a probability distribution, we can compute the expected Hamming error, but also its bias-variance decomposition. On MNIST-small, we hide randomly located image patches of size 3× 3 (i.e. k = 9). On PLANTS and MNIST-code, we hide random subsets of k = 9 variables. Results are shown in Figure 7 (left), where we compare three types of models: Kernel density estimation (KDE), standard RBM (RBM) and Wasserstein RBM (RBM-W). The KDE estimation model uses a Gaussian kernel, with the Gaussian scale parameter chosen such that the KL divergence of the model from the validation data is minimized. The RBM-W is better or comparable the other models. Of particular interest is the structure of the expected Hamming error: For the standard RBM, a large part of the error comes from the variance (or entropy), while for the Wasserstein RBM, the bias term is the most contributing. This can be related to what is observed in Figure 4: For a data point outside the area covered by the red points, the reconstruction is systematically redirected towards the nearest red cluster, thus, incurring a systematic bias.\nData Denoising Here, we consider a simple noise process where for a predefined subset of k variables, denoted by h a known number l of bits flips occur randomly. Remaining d− k variables are denoted by v. The setting of the experiment is illustrated in Figure 6 (bottom). Calling x? the original and x̃ its noisy version resulting from flipping l variables of h, the expected Hamming error\nis given by iterating over the ( k l ) states x with same visible variables v and that are at distance l of x̃:\nE = ∑h pθ(x |v,H(x, x̃) = l) · H(x,x?) where h ∈ {0, 1}k. Note that the original example x? is necessarily part of this set of states under the noise model assumption. For the MNIST-small data, we choose randomly located images patches of size 4 × 3 or 3 × 4 (i.e. k = 12), and generate l = 4 random bit flips within the selected patch. For PLANTS and MNIST-code, we generate l = 4 bit flips in k = 12 randomly preselected input variables. Figure 7 (right) shows the denoising error in terms of expected Hamming distance on the same datasets. The RBM-W is better or comparable to other models. Like for the completion task, the main difference between the two RBMs is the bias/variance ratio, where again the Wasserstein RBM tends to have larger bias. This experiment has considered a very simple noise model consisting of a fixed number of l random bit flips over a small predefined subset of variables. Denoising highly corrupted complex data will however require to combine Wasserstein models with more flexible noise models such as the ones proposed by [17]."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have introduced a new objective for restricted Boltzmann machines (RBM) based on the smooth Wasserstein distance. We derived the gradient of the Wasserstein distance from its dual formulation, and used it to effectively train an RBM. Unlike the usual Kullback-Leibler (KL) divergence, our Wasserstein objective takes into account the metric of the data. In all considered scenarios, the Wasserstein RBM produced distributions that strongly departed from standard RBMs, and outperformed them on practical tasks such as completion or denoising.\nMore generally, we demonstrated empirically, that when learning probability densities, the reliance on distributions that incorporate indirectly the desired metric can be substituted for training procedures that make the desired metric directly part of the learning objective. Thus, Wasserstein training can be seen as a more direct approach to density estimation than regularized KL training. Future work will aim to further explore the interface between Boltzmann learning and Wasserstein minimization, with the aim to scale the newly proposed learning technique to larger and more complex data distributions."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by the Brain Korea 21 Plus Program through the National Research Foundation of Korea funded by the Ministry of Education. This work was also supported by the grant DFG (MU 987/17-1). M. Cuturi gratefully acknowledges the support of JSPS young researcher A grant 26700002. Correspondence to GM, KRM and MC."
    } ],
    "references" : [ {
      "title" : "A learning algorithm for Boltzmann machines",
      "author" : [ "D.H. Ackley", "G.E. Hinton", "T.J. Sejnowski" ],
      "venue" : "Cognitive Science, 9(1):147–169",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "On minimum Kantorovich distance estimators",
      "author" : [ "F. Bassetti", "A. Bodini", "E. Regazzini" ],
      "venue" : "Statistics & Probability Letters, 76(12):1298 – 1302",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Enhanced gradient for training restricted Boltzmann machines",
      "author" : [ "K. Cho", "T. Raiko", "A. Ilin" ],
      "venue" : "Neural Computation, 25(3):805–831",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Optimal transport for domain adaptation",
      "author" : [ "N. Courty", "R. Flamary", "D. Tuia", "A. Rakotomamonjy" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Sinkhorn distances: Lightspeed computation of optimal transport",
      "author" : [ "M. Cuturi" ],
      "venue" : "Advances in Neural Information Processing Systems 26, pages 2292–2300",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Fast computation of Wasserstein barycenters",
      "author" : [ "M. Cuturi", "A. Doucet" ],
      "venue" : "Proceedings of the 31th International Conference on Machine Learning, ICML, pages 685–693",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Phone recognition with the mean-covariance restricted Boltzmann machine",
      "author" : [ "G.E. Dahl", "M. Ranzato", "A. Mohamed", "G.E. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems 23., pages 469–477",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning with a wasserstein loss",
      "author" : [ "C. Frogner", "C. Zhang", "H. Mobahi", "M. Araya", "T. Poggio" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Neural Computation, 14(8):1771–1800",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Robust statistics",
      "author" : [ "P.J. Huber" ],
      "venue" : "Springer",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "and N",
      "author" : [ "B.M. Marlin", "K. Swersky", "B. Chen" ],
      "venue" : "de Freitas. Inductive principles for restricted Boltzmann machine learning. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS, pages 509–516",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Deep Boltzmann machines and the centering trick",
      "author" : [ "G. Montavon", "K.-R. Müller" ],
      "venue" : "Neural Networks: Tricks of the Trade - Second Edition, LNCS, pages 621–637. Springer",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The earth mover’s distance",
      "author" : [ "Y. Rubner", "L. Guibas", "C. Tomasi" ],
      "venue" : "multi-dimensional scaling, and colorbased image retrieval. In Proceedings of the ARPA Image Understanding Workshop, pages 661–668",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Deep Boltzmann machines",
      "author" : [ "R. Salakhutdinov", "G.E. Hinton" ],
      "venue" : "Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, AISTATS, pages 448–455",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Multimodal learning with deep Boltzmann machines",
      "author" : [ "N. Srivastava", "R. Salakhutdinov" ],
      "venue" : "Journal of Machine Learning Research, 15(1):2949–2980",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Robust Boltzmann machines for recognition and denoising",
      "author" : [ "Y. Tang", "R. Salakhutdinov", "G.E. Hinton" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, pages 2264–2271",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Training restricted Boltzmann machines using approximations to the likelihood gradient",
      "author" : [ "T. Tieleman" ],
      "venue" : "Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML), pages 1064–1071",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Optimal transport: old and new",
      "author" : [ "C. Villani" ],
      "venue" : "volume 338. Springer Verlag",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Boltzmann machines [1] are powerful generative models that can be used to approximate a large class of real-world data distributions, such as handwritten characters [9], speech segments [7], or multimodal data [16].",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "Boltzmann machines [1] are powerful generative models that can be used to approximate a large class of real-world data distributions, such as handwritten characters [9], speech segments [7], or multimodal data [16].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 6,
      "context" : "Boltzmann machines [1] are powerful generative models that can be used to approximate a large class of real-world data distributions, such as handwritten characters [9], speech segments [7], or multimodal data [16].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 15,
      "context" : "Boltzmann machines [1] are powerful generative models that can be used to approximate a large class of real-world data distributions, such as handwritten characters [9], speech segments [7], or multimodal data [16].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 14,
      "context" : "Boltzmann machines share similarities with neural networks in their capability to extract features at multiple scales, and to build well-generalizing hierarchical data representations [15, 13].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : "Boltzmann machines share similarities with neural networks in their capability to extract features at multiple scales, and to build well-generalizing hierarchical data representations [15, 13].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "Given an empirical probability distribution p̂(x) = 1 N ∑N n=1 δxn where (xn)n is a list of N observations in {0, 1}d, an RBM can be trained using information-theoretic divergences (see for example [12]) by minimizing with respect to θ a divergence ∆(p̂, pθ) between the sample empirical measure p̂ and the modeled distribution pθ.",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "This being done, the geometric criterion we have drawn can be materialized by considering for ∆ the Wasserstein distance [20] (a.",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "the Kantorovich or the earth mover’s distance [14]) between measures.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 1,
      "context" : "This choice was considered in theory by [2], who proved its statistical consistency, but was never considered practically to the best of our knowledge.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "This paper describes a practical derivation for a minimum Kantorovich distance estimator [2] for Boltzmann machines, which can scale up to tens of thousands of observations.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "As will be described in this paper, recent advances in the fast approximation of Wasserstein distances [5] and their derivatives [6] play an important role in the practical implementation of these computations.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "As will be described in this paper, recent advances in the fast approximation of Wasserstein distances [5] and their derivatives [6] play an important role in the practical implementation of these computations.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "That distance was for instance introduced in the context of supervised inference by [8], who used it to compute a predictive loss between the output of a multilabel classifier against its ground truth, or for domain adaptation, by [4].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "That distance was for instance introduced in the context of supervised inference by [8], who used it to compute a predictive loss between the output of a multilabel classifier against its ground truth, or for domain adaptation, by [4].",
      "startOffset" : 231,
      "endOffset" : 234
    }, {
      "referenceID" : 4,
      "context" : "Given a constant γ ≥ 0, the γ-smoothed Wasserstein distance [5] is equal to Wγ(p, q) = min π∈Π(p,q) 〈D(x,x)〉π − γH(π), (2)",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "This optimization problem, a strictly convex program, has an equivalent dual formulation [6] which involves instead two real-valued functions α, β in RX and which plays an important role in this paper: Wγ(p, q) = max α,β∈RX 〈α(x)〉p + 〈β(x)〉q − γ ∑",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "One can easily verify that that definition matches the usual linear program used to describe Wasserstein/EMD distances [14].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "We consider in this paper the case γ > 0 because it was shown in [5] to considerably facilitate computations, and in [6] to result in a divergenceWγ(p, q) which, unlike the case γ = 0, is differentiable w.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "We consider in this paper the case γ > 0 because it was shown in [5] to considerably facilitate computations, and in [6] to result in a divergenceWγ(p, q) which, unlike the case γ = 0, is differentiable w.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "As mentioned in [6], the rightmost term is the optimal dual variable (the Kantorovich potential) ∂Wγ(pθ, q)/∂pθ = α.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "In practical terms, α̂ is a vector of size Ñ , one coefficient for each PCD sample, which can be computed by following the algorithm below [6].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : "We consider three datasets: MNIST-small, a subsampled version of the original MNIST dataset [11] with only the digits “0” retained, a subset of the UCI PLANTS dataset [19] containing the geographical spread of plants species, and MNIST-code, 128-dimensional code vectors associated to each MNIST digit (additional details in the supplement).",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "1 Training, Validation and Evaluation All RBM models that we investigate are trained in full batch mode, using for p̂θ the PCD approximation [18] of pθ, where the sample is refreshed at each gradient update by one step of alternate Gibbs sampling, starting from the sample at the previous time step.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "We use the centered parameterization of the RBM for gradient descent [13, 3].",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "We use the centered parameterization of the RBM for gradient descent [13, 3].",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 16,
      "context" : "Denoising highly corrupted complex data will however require to combine Wasserstein models with more flexible noise models such as the ones proposed by [17].",
      "startOffset" : 152,
      "endOffset" : 156
    } ],
    "year" : 2016,
    "abstractText" : "Boltzmann machines are able to learn highly complex, multimodal, structured and multiscale real-world data distributions. Parameters of the model are usually learned by minimizing the Kullback-Leibler (KL) divergence from training samples to the learned model. We propose in this work a novel approach for Boltzmann machine training which assumes that a meaningful metric between observations is known. This metric between observations can then be used to define the Wasserstein distance between the distribution induced by the Boltzmann machine on the one hand, and that given by the training sample on the other hand. We derive a gradient of that distance with respect to the model parameters. Minimization of this new objective leads to generative models with different statistical properties. We demonstrate their practical potential on data completion and denoising, for which the metric between observations plays a crucial role.",
    "creator" : null
  }
}