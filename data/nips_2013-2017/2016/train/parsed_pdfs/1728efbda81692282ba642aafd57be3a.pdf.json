{
  "name" : "1728efbda81692282ba642aafd57be3a.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis",
    "authors" : [ "Yoshinobu Kawahara" ],
    "emails" : [ "ykawahara@sanken.osaka-u.ac.jp" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Modeling nonlinear dynamical systems using data is fundamental in a variety of engineering and scientific fields. In machine learning, the problem of learning dynamical systems has been actively discussed, and several Bayesian approaches have been proposed [11, 34]. In the fields of physics, one popular approach for this purpose is the decomposition methods that factorize the dynamics into modes based on some criterion from the data. For example, proper orthogonal decomposition (POD) (see, for example, [12]), which generates orthogonal modes that optimally capture the vector energy of a given dataset, has been extensively applied to complex phenomena in physics [5, 22] even though this method is currently known to have several drawbacks. The so-called spectral method for dynamical systems [15, 31, 17], which is often discussed in machine learning, is closely related to this type of technique, where one aims to estimate a prediction model rather than understand the dynamics by examining the obtained modes.\nAmong the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32]. DMD approximates the spectra of the Koopman operator [16], which is an infinite-dimensional linear operator that represents nonlinear and finite-dimensional dynamics without linearization. While POD just finds the principal directions in a dataset, DMD can yield direct information concerning the dynamics such as growth rates and the frequencies of the dynamics.\nIn this paper, we consider a spectral analysis of the Koopman operator in reproducing kernel Hilbert spaces (RKHSs) for a nonlinear dynamical system\nxt+1 = f(xt), (1) where x∈M is the state vector on a finite-dimensional manifold M⊆Rd, and f is a (possibly, nonlinear) state-transition function. We present a modal decomposition algorithm to perform this,\nwhich is in principle reduced to the calculation of a set of orthogonal bases for the Krylov matrix in RKHS and the eigendecomposition of the projection of the Koopman operator onto the subspace spanned by the bases. Although existing DMD algorithms can conceptually be thought of as producing an approximation of the eigenfunctions of the Koopman operator using a set of linear monomials of observables (or the pre-determined functional maps of observables) as basis functions, which is analogous to a one-term Taylor expansion at each point, our algorithm gives an approximation with a set of nonlinear basis functions due to the expressiveness of kernel functions. The proposed algorithm provides a modal decomposition of the dynamics into a finite number of modes, and thus it could be considered as a feature extraction procedure for a nonlinear dynamical system. Therefore, we consider applications using extracted features from our analysis such as state prediction, sequential change-point detection, and dynamics recognition. We illustrate our method on the applications using synthetic and real-world data.\nThe remainder of this paper is organized as follows. In Section 2, we briefly review the spectral analysis of nonlinear dynamical systems with the Koopman operator and DMD. In Section 3, we extend the analysis with reproducing kernels, and provide a modal decomposition algorithm to perform this analysis based on the equivalent principle of DMD. Although this method is mathematically correct, a practical implementation could yield an ill-conditioned algorithm. Therefore, in Section 4, we describe a way to robustly it by projecting data onto the POD directions. In Section 5, we describe related works. In Section 6, we show some empirical examples by the proposed algorithm and, in Section 7, we describe several applications using extracted features with empirical results. Finally, we conclude the paper in Section 8."
    }, {
      "heading" : "2 The Koopman Operator and Dynamic Mode Decomposition",
      "text" : "Consider a discrete-time nonlinear dynamical system (1). The Koopman operator [16], which we denote here by K, is an infinite-dimensional linear operator that acts on a scalar function gi : M→C, mapping gi to a new function Kgi given as follows:\n(Kgi)(x) = gi ◦ f(x), (2) where ◦ denotes the composition of gi with f . We see that K acts linearly on the function gi, even though the dynamics defined by f may be nonlinear. Since K is a linear operator, it has, in general, an eigendecomposition Kφj(x) = λjφj(x), (3) where λj ∈ C is the j-th eigenvalue (called the Koopman eigenvalue) and φj is the corresponding eigenfunction (called the Koopman eigenfunction). We denote the concatenation of gi as g := [g1, . . . , gp]\n⊤. If each gi lies within the span of the eigenfunctions φj , we can expand the vectorvalued g in terms of these eigenfunctions as\ng(x) = ∑∞\nj=1φj(x)uj , (4)\nwhere uj is a set of vector coefficients called Koopman modes. Then, by the iterative applications of Eqs. (2) and (3), we obtain g ◦ f l(x) = ∑∞\nj=1λ l jφj(x)uj , (5)\nwhere f l is the l-time compositions of f . Therefore, λj characterizes the temporal behavior of the corresponding Koopman mode uj , i.e., the phase of λj determines its frequency, and the magnitude determines the growth rate of the dynamics. Note that, for a system evolving on an attractor, the Koopman eigenvalues always lie on a unit circle [20].\nDMD [25, 26] (and its variants) is a popular approach for estimating the approximations of λj and uj from a finite-length data sequence y0,y1, . . . ,yτ (∈ Rp), where we denote yt := g(xt). DMD can fundamentally be considered as a special use of the Arnoldi method [1]. That is, using the empirical Ritz values λ̃j and vectors vj obtained by the Arnoldi method when regarding the subspace spanned by y0, . . . ,yτ−1 as the Krylov subspace for y0 (and implicitly for some matrix A ∈ Rp×p), it is shown that the observables are expressed as\nyt = ∑τ j=1λ̃ t jvj (t = 0, . . . , τ − 1), and (6a)\nyτ = ∑τ j=1λ̃ τ jvj + r where r ⊥ span{y0, . . . ,yτ−1}. (6b)\nComparing Eq. (6a) with Eq. (5) infers that the empirical Ritz values λ̃j and vectors vj behave in precisely the same manner as the Koopman eigenvalues λj and modes uj (φj(x0)uj), but for the\nfinite sum in Eq. (6a) instead of the infinite sum in Eq. (5). Note that, for r = 0 in Eq. (6b) (which could happen when the data are sufficiently large), the approximate modes are indistinguishable from the true Koopman eigenvalues and modes (as far as the data points are concerned), with the expansion (5) comprising only a finite number of terms."
    }, {
      "heading" : "3 Dynamic Mode Decomposition with Reproducing Kernels",
      "text" : "As described above, the estimation of the Koopman mode by DMD (and its variants) can capture the nonlinear dynamics from finite-length data sequences generated from a dynamical system. Conceptually, DMD can be considered as producing an approximation of the Koopman eigenfunctions using a set of linear monomials of observables as basis functions, which is analogous to a one-term Taylor expansion at each point. In situations where eigenfunctions can be accurately approximated using linear monomials (e.g., in a small neighborhood of a stable fixed point), DMD will produce an accurate local approximation of the Koopman eigenfunctions. However, this is certainly not applicable to all systems (in particular, beyond the region of validity for local linearization). Here, we extend the Koopman spectral analysis with reproducing kernels to approximate the Koopman eigenfunctions with richer basis functions. We provide a modal decomposition algorithm to perform this analysis based on the equivalent principle with DMD.\nLet H be the RKHS embedded with the dot product ⟨·, ·⟩H (we abbreviate ⟨·, ·⟩H as ⟨·, ·⟩ for simplicity) and a positive definite kernel k. Additionally, let ϕ : M → H. Then, we define the Koopman operator on the feature map ϕ by\n(KHϕ)(x) = ϕ ◦ f(x). (7) Thus, the Koopman operator KH is a linear operator in H. Note that almost of the theoretical claims in this and the next sections do not necessarily require ϕ to be in RKHS (it is sufficient that ϕ stays in a Hilbert space). However, this assumption should perform the calculation in practice (as described in the last parts of this and the next sections). Therefore, we proceed with this assumption in the following parts. We denote by φj the j-th eigenfunction of KH with the corresponding eigenvalue λj . Also, we define Φ := span{ϕ(x) : x ∈ M}. We first expand the notions, such as the Ritz values and vectors, that appear in DMD with reproducing kernels. Suppose we have a sequence x0,x1, . . . ,xτ . The Krylov subspace for ϕ(x0) is defined as the subspace spanned by ϕ(x0), (KHϕ)(x0), . . . , (Kτ−1H ϕ)(x0). Note that this is identical to the one spanned by ϕ(x0), . . . , ϕ(xτ−1), whose corresponding Krylov matrix is given by\nMτ = [ϕ(x0) · · · ϕ(xτ−1)]. (8) Therefore, if we denote a set of τ orthogonal bases of the Krylov subspace by q1, . . . , qτ (∈ H) (obtained from the Gram-Schmidt orthogonalization described below), then the orthogonal projection of KH onto Mτ is given by Pτ = Q∗τKHQτ , where Qτ = [q1 · · · qτ ] and Q∗τ indicates the Hermitian transpose of Qτ . Consequently, the empirical Ritz values and vectors are defined as the eigenvalues and vectors of Pτ , respectively. Now, we have the following theorem: Theorem 1. Consider a sequence ϕ(x0), ϕ(x1), . . . , ϕ(xτ ), and let λ̃j and φ̃j be the empirical Ritz values and vectors for this sequence. Assume that λ̃j’s are distinct. Then, we have\nϕ(xt) = ∑τ j=1λ̃ t jφ̃j (t = 0, . . . , τ − 1), and (9a)\nϕ(xτ ) = ∑τ j=1λ̃ τ j φ̃j + ψ where ψ ⊥ span{ϕ(x0), . . . , ϕ(xτ−1)}. (9b)\nProof. Let Mτ = QτR (R ∈ Cτ×τ ) be the Gram-Schmidt QR decomposition of Mτ . Then, the companion matrix (rational canonical form) of Pτ is given as F := R−1PτR. Note that the sets of eigenvalues of Pτ and F are equivalent. Since F is a companion matrix and λ̃j’s are distinct, F can be diagonalized in the form F = T−1Λ̃T , where Λ̃ is a diagonal matrix with λ̃1, . . . , λ̃τ and T is a Vandermonde matrix defined by Tij = λ̃ j−1 i . Therefore, the empirical Ritz vectors φ̃j are obtained as the columns of V = MτT−1. This proves Eq. (9a). Suppose a linear expansion of ϕ(xτ ) is represented as\nϕ(xτ ) = Mτc+ ψ where ψ ⊥ span{ϕ(x0), . . . , ϕ(xτ−1)}. (10) Since F = R−1PτR = M−1τ KHMτ (therefore, MτF = KHMτ ), the first term is given by the last column of MτF = MτT−1Λ̃T = V Λ̃T . This proves Eq. (9b).\nThis theorem gives an extension of DMD via the Gram-Schmidt QR decomposition in the feature space. Although in Step (2), the Gram-Schmidt QR orthogonalization is performed in RKHS, this calculation can be reduced to operations on a Gram matrix due to the reproducing property of kernel functions.\n(1) Define Mτ by Eq. (8) and M+ := [ϕ(x1), . . . , ϕ(xτ )]. (2) Calculate the Gram-Schmidt QR decomposition Mτ = QτR (e.g., refer to Section 5.2 of [29]). (3) Calculate the eigendecomposition of R−1Q∗τM+(=F ) = T−1Λ̃T , where each diagonal ele-\nment of Λ̃ gives λ̃j .\n(4) Define φ̃j to be the columns of MτT−1. The original DMD algorithm (and its variants) produce an approximation of the eigenfunctions of the Koopman operator in Eq. (2) using the set of linear monomials of observables as basis functions. In contrast, because the above algorithm works with operations directly in the functional space, the Koopman operator defined in Eq. (7) is identical to the transition operator on an observable. Therefore, the eigenfunctions of the Koopman operator are fully recovered if the Krylov subspace is sufficiently large, i.e., ϕ(xτ ) is also in span{ϕ(x0), . . . , ϕ(xτ−1)} (or ψ = 0)."
    }, {
      "heading" : "4 Robustifying with POD Bases",
      "text" : "Although the above decomposition based on the Gram-Schmidt orthogonalization is mathematically correct, a practical implementation could yield an ill-conditioned algorithm that is often incapable of extracting multiple modes. A similar issue has been well known for DMD [26], where one needs to adopt a way to robustify DMD by projecting data onto the (truncated) POD directions [8, 33]. Here, we discuss a similar modification of our principle with the POD basis.\nFirst, consider kernel PCA [28] on x0,x1, . . . ,xτ−1: Let Ḡ = BSB∗ be the eigen-decomposition of the centered Gram matrix Ḡ = HGH = G − 1τG − G1τ + 1τG1τ , where G = M∗τMτ is the Gram matrix for the data, H = I − 1τ and 1τ is a τ -by-τ matrix for which each element takes the value 1/τ . Suppose the eigenvalues and eigenvectors can be truncated accordingly based on the magnitudes of the eigenvalues, which results in Ḡ ≈ B̄S̄B̄∗ where p (≤τ) eigenvalues are adopted. Denote the j-th column of B̄ by βj and let ϕ̄(xi)=ϕ(xi)−ϕc, where ϕc= ∑τ−1 j=0 ϕ(xj). A principal\northogonal direction in the feature space is then given by νj = ∑τ−1\ni=0 αj,iϕ̄(xi) = MτHαj (j = 1, . . . , p), where αj = S̄ −1/2 jj βj . Let U = [ν1, . . . , νp] (= MτHB̄S̄−1/2). Since M+ = KHMτ , the projection of KH onto the space spanned by νj is given as F̂ := U∗KHU = S̄−1/2B̄∗H(M∗τM+)HB̄S̄−1/2. (11) Note that the (i, j)-the element of the matrix (M∗τM+) is given by k(xi−1,xj). Then, if we let F̂ = T̂−1Λ̂T̂ be the eigendecomposition of F̂ , then\nφ̄j = Ubj = MτHB̄S̄−1/2bj ,\nwhere bj is the j-th column of T̂−1, can be used as an alternative to the empirical Ritz vector φ̃j . That is, we have the following theorem: Theorem 2. Assume that φj ∈ Φ, so that φj(x) = ⟨ϕ(x), κj⟩ for some κj ∈ H and ∀x ∈ M. If κj is in the subspace spanned by the columns of U , so that κj = Uaj for some aj ∈ Cp, then aj is a left eigenvector of F̂ with eigenvalue λj , and also we have\nϕ(x) = ∑p\nj=1φj(x)φ̄j . (12)\nProof. Since KHφj = λjφj , we have ⟨ϕ(f(x)), κj⟩ = λj ⟨ϕ(x), κj⟩. Thus, from the assumption, ⟨ϕ(f(x)),Uaj⟩ = λj ⟨ϕ(x),Uaj⟩ .\nBy evaluating at x0,x1, . . . , xτ−1 and then stacking into matrices, we have\n(Uaj)∗M+ = λj(Uaj)∗Mτ . If we multiply HḠ−1HM∗τU from the righthand side, this gives\na∗jU∗M+HḠ−1HM∗τU = λja∗jU∗MτHḠ−1HM∗τU = λja∗j .\nSince U∗M+HḠ−1HM∗τU = U∗KHU(= F̂ ), this means aj is a left eigenvector of F̂ with eigenvalue λj . Let bj be a (right) eigenvector of F̂ with eigenvalue λj and the corresponding left eigenvector aj .Assuming these have been normalized so that a∗jbj = δij , then any vector h ∈ Cp can be written as h = ∑p j=1(a\n∗ jh)bj . Applying this to U∗ϕ(x) gives U∗ϕ(x) = ∑p j=1(a ∗ jU∗ϕ(x))bj . = ∑p j=1φj(x)bj\nSince bj = (U∗U)bj = U∗φ̄j , this proves Eq. (12).\nThis theorem clearly gives the connection between the eigenvalues/eigenvectors found by the above procedure and the Koopman eigenvalues/eigenfunctions. The assumptions in the theorem means that the data are sufficiently rich and thus a set of the kernel principal components gives a good approximation of the representation with the Koopman eigenfunctions. As in the case of Eq. (5), by the iterative applications of Eq. (3), we obtain\nϕ(xt) = ∑p j=1λ t jφj(x0)φ̄j . (13)\nThe procedure for the robustified variant of the DMD is summarized as follows.1\n(1) Define Mτ and calculate the centered Gram matrix Ḡ = HM∗τMτH. (2) Calculate the eigendecomposition Ḡ ≈ B̄S̄B̄∗, which gives the kernel principal directions U . (3) Calculate F̂ as in Eq. (11) and its eigendecomposition F̂ = T̂−1Λ̂T̂ , where each diagonal\nelement of Λ̂ gives λj .\n(4) Define φ̄j to be the columns of MτHB̄S̄−1/2T̂−1. Unlike the procedure described in Section 3, the above procedure can perform the truncation of eigenvectors corresponding to small singular values. As well as DMD, this step becomes beneficial in practice when the Gram matrix G, in our case, is rank-deficient or nearly so.\nRemark: Although we assumed that data is a consecutive sequence for demonstrating the correctness of the algorithm, as evident from the above steps, the estimation procedure itself does not necessarily require a sequence but rather a collection of pairs of consecutive observables {(x(i)1 ,x (i) 2 )}τi=1, where each pair is supposed to be x(i)2 = f(x (i) 1 ), with the appropriate definitions of Mτ and M+."
    }, {
      "heading" : "5 Related Works",
      "text" : "Spectral analysis (or, referred as the decomposition technique) for dynamical systems is a popular approach aimed at extracting information concerning (low-dimensional) dynamics from data. Common techniques include global eigenmodes for linearized dynamics (see, e.g., [3]), discrete Fourier transforms, POD for nonlinear dynamics [30, 12], and balancing modes for linear systems [24] as well as multiple variants of these techniques, such as those using shift modes [22] in conjunction with POD modes. In particular, POD, which is in principle equivalent to principal component analysis, has been extensively applied to the analysis of physical phenomena [5, 22] even though it suffers from numerous known issues, including the possibility of principal directions in a set of data may not necessarily correspond to the dynamically important ones.\nDMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32]. Unlike POD (and its variants), DMD yields direct information about the dynamics such as growth rates and frequencies associated with each mode, which can be obtained from the magnitude and phase of each corresponding eigenvalue of the Koopman operator. However, the original DMD has several numerical disadvantages related to the accuracy of the approximate expressions of the Koopman eigenfunctions from data. Therefore, several variants of DMD have been proposed to rectify this point, including exact DMD [33] and optimized DMD [8]. Jovanović et al. proposed sparsity-promoting DMD [13], which provides a framework for the approximation of the Koopman eigenfunctions with fewer bases. Williams et al. proposed extended DMD [35], which works on pre-determined basis functions instead of the monomials of observables. Although in extended DMD the Koopman mode is defined as the eigenvector of the corresponding operator of coefficients on basis functions, the resulting procedure is similar to the robust-version of our algorithm.\n1The Matlab code is available at http://en.44nobu.net/codes/kdmd.zip\n1 2 3 4 5 6 7 0\n0.2\n0.4\n0.6\n0.8\n1\nkernel DMD True\nIndex\nE ig e n v a lu e\n8 -3 -2 -1 0 -1\n0.5\n0\n0.5\n1\nkernel DMD Equilibrium DMD\n1 2 3 Real\nIm a g e\nFigure 1: Estimated eigenvalues with the data from the toy system (left) and the Hénon map (right).\nIn system control, subspace identification [23, 14], or called the eigensystem realization method, has been a popular approach to modeling of dynamical systems. This method basically identifies low-dimensional (hidden) states as canonical vectors determined by canonical correlation analysis, and estimates parameters in the governing system using the state estimates. This type of method is known as a spectral method for dynamical systems in the machine learning community and has recently been applied to several types of systems such as variants of hidden Markov models [31, 19], nonlinear dynamical systems [15], and predictive state-representation [17]. The relation between DMD and other methods, particularly the eigensystem realization method, is an interesting open problem. This is briefly mentioned in [33] but it would require further investigation in future studies."
    }, {
      "heading" : "6 Empirical Example",
      "text" : "To illustrate how our algorithm works, we here consider two examples: a toy nonlinear system given by xt+1= 0.9xt, yt+1= 0.5yt+(0.92−0.5)x2t , and one of the well-known chaotic maps, called the Hénon map (xt+1 = 1 − ax2t + yt, yt+1 = bxt), which was originally presented by Hénon as a simplified model of the Poincaré section of the Lorenz attractor. As for the toy one, the two eigenvalues are 0.5 and 0.9 with the corresponding eigenfunctions φ0.9 = xt and φ0.5 = yt − x2t , respectively. And as for the Hénon map, we set the parameters as a = 1.4, b = 0.3. It is known that this map has two equilibrium points (−1.13135,−0.339406) and (0.631354, 0.189406), whose corresponding eigenvalues are 2.25982 and −1.09203, and −2.92374 and −0.844054. We generated samples according to these systems with several initial conditions and then applied the presented procedure to estimate the Koopman modes. We used the polynomial kernel of degree three for the toy system, and the Gaussian kernel with width 1 for the Hénon map, respectively. The graphs in Fig. 1 show the estimated eigenvalues for two cases. As seen from the left graph, the eigenvalues for the toy system were precisely estimated. Meanwhile, from the right graph, the part of the eigenvalues of the equilibrium points seem to be approximately estimated by the algorithm."
    }, {
      "heading" : "7 Applications",
      "text" : "The above algorithm provides a decomposition of the dynamics into a finite number of modes, and therefore, could be considered as a feature extraction procedure for a nonlinear dynamical system. This would be useful to directly understand dominant characteristics of the dynamics, as done in scientific fields with DMD [2, 10, 21, 25, 27]. However, here we consider some examples of applications using extracted features with the proposed analysis; prediction, sequential change detection, and the recognition of dynamic patterns, with some empirical examples.\nPrediction via Preimage: As is known in physics (nonlinear science), long-term predictions in a nonlinear dynamical system are, in principle, impossible if at least one of its Lyapunov exponents is positive, which would be typically the case of interests. This is true even if the dimension of the system is low because uncertainty involved in the evolution of the system exponentially increases over time. However, it may be possible to predict an observable in the near future (i.e., shortterm prediction) if we could formulate a precise predictive model. Therefore, we here consider a prediction based on estimated Koopman spectra as in Eq. (13). Since Eq. (13) is represented as the linear combination of ϕ(xi) (i = 0, . . . , τ − 1), a prediction can be obtained by considering the pre-image of the predicted observables in the feature space. Even though any method for finding a pre-image of a vector in the feature space can be used for this purpose, here we describe an approach\n-1 -0.5 0 0.5 1 1.5 -0.4\n0\n0.4\n0.8\n1.2\njump\nwalk run\nvaried\nwalk, then turn\nrun, then stop\nrun, then turn\nslow walk, then stop\nFigure 3: MDS embedding with the distance matrix from kernel principal angle between subspaces of the estimated Koopman eigenfunctions for locomotion data. Each point is colored according to its assigned motion (jump, walk, run, and varied).\n0 500 1000 1500 2000\n-20\n-10\n0\n10\n20\n30\n40 50 x1 x2 x3\n1\n0\nkDMD\n1\n0\n1-SVM\nFigure 4: Sample sequence (top) and change scores by our method (green) and the kernel change detection method (blue).\nbased on a similar idea with multidimensional scaling (MDS), as describe in [18], where a pre-image is recovered to preserve the distance between it and other data points in the input space as well as the feature space. The basic steps are (i) find n-neighbors of a new point ϕ̂(xτ+l) in the feature space, (ii) calculate the corresponding distance between the preimage x̂τ+l and each data point xt based on the relation between the feature- and input-space distances, and (iii) calculate the pre-image in order to preserve the input distances. For step (i), we need the distance between the estimated feature and each data point in the feature space, which is calculated as\n∥ϕ̂(xτ+l)− ϕ(xt)∥2 = ∥ϕ̂(xτ+l)∥2 + ∥ϕ(xt)∥2 − 2ϕ̂(xτ+l)∗ϕ(xt) = c∗(M∗τMτ )c+ k(xt,xt)− 2c∗(M∗τϕ(xt)),\nwhere c is from Eq. (10). Note that the first and third terms in the above equation can be calculated using the values in the Gram matrix for the data. Once we obtain n-neighbors based on the feature distances, we can construct the corresponding local coordinate by calculating a set of orthogonal bases (via, for example, singular value decomposition of the data matrix for the neighbors) based on the distances in the input spaces, which are analytically obtained from the feature distances [18]. The graphs in Fig. 2 show empirical examples of the true versus predicted values as described above for the toy nonlinear system and the Hénon map. The setups for the data generation and the kernels etc. are same with the previous section.\nEmbedding and Recognition of Dynamics: A direct but important application of the presented analysis is the embedding and recognition of dynamics with the extracted features. Like (kernel) PCA, a set of Koopman eigenfunctions estimated via the analysis can be used as the bases of a low dimensional subspace that represents the dynamics. For example, the recognition of dynamics based on this representation can be performed as follows. Suppose we are given m collection of data sequences {xt}τit=0 (i=1,. . . ,m) each of which is generated from some known dynamics C (e.g., walks, runs, jumps etc.). Then, a set of estimated Koopman eigenfunctions for each known dynamics, which we denote by Ac = Mτwc for the corresponding complex vector wc, can be regarded as the bases of a low-dimensional embedding of the sequences. Hence, if we let A be a set of the estimated Koopman eigenfunctions for a new sequence, its category of dynamics can be estimated as\nî = argmin c∈C\ndist(A,Ac),\nwhere dist(A,Ac) is a distance between two subspaces spanned by A and Ac. For example, such a distance can be given via the kernel principal angles between two subspaces in the feature space [36]. Fig. 3 shows an empirical example of this application using the locomotion data from CMU Graphics Lab Motion Capture Database.2 We used the RBF Gaussian kernel, where the kernel width was set as the median of the distances from a data matrix. The figure shows an embedding of the sequences via MDS with the distance matrix, which was calculated with kernel principal angles [36] between subspaces spanned by the Koopman eigenfunctions. Each point is colored according to its motion (jump, walk, run, and varied).\n2Available at http://mocap.cs.cmu.edu.\nSequential Change-Point Detection: Another possible application is the sequential detection of change-points in a nonlinear dynamical system based on the prediction via the presented analysis. Here, we give a criterion for this problem based on the so-called cumulative-sum (CUSUM) of likelihood-ratios (see, for example, [9]). Let x0,x1,x2, . . . be a sequence of random vectors distributed according to some distribution ph (h = 0, 1). Then, change-point detection is defined as the sequential decision between hypotheses; H0 : p(xi) = p0(xi) for i = 1, . . . , T , and H1 : p(xi) = p0(xi) for i = 1, . . . , τ and p(xi) = p1(xi) for i = τ + 1, . . . , T , where 1 ≤ τ ≤ T (≤ ∞). In CUSUM, the stopping rule is given as\nT ∗ = inf { T : max1≤τ<T ∑T t=τ+1 log (p1(xt)/p0(xt)) ≥ c } ,\nwhere c > 0 (T ∗ is the stopping time). Although the Koopman operator is, in general, defined for a deterministic system, it is known to be extended to a stochastic system xt+1 = f(xt,vt), where vt is a stochastic disturbance [20]. In that case, the operator works on the expectation. Hence, let us define the distribution of xt as a nonparametric exponential family [7], given by\np(xt) = exp (⟨θ(·), ψ(xt)⟩H − g(θ)) = exp (⟨ϕ ◦ f(xt−1), ϕ(xt)⟩H − g(ϕ ◦ f(xt−1))) ,\nwhere g is the log-partition function. Then, the log-likelihood ratio score is given as log Λτ (x1:T ) := ∑T i=τ+1 log (p1(xt)/p0(xt)) ∝ − ∑T i=τ+1 (∑τ j=1α (0) i k(xj ,xi)− ∑τ j=1α (1) i k(xj ,xi) ) ,\nwhere α(0)i and α (1) i are the coefficients obtained by the proposed algorithm with the data for i = 1, . . . , τ and i = τ + 1, . . . , T , respectively. Here, since the variation of the second term is much smaller than the first one (cf. [7]), the decision rule, log Λ∗ ≥ c, can be simplified by ignoring the second term. As a result, we have the following decision rule with some critical value c̃ ≤ 0:\n− log Λτ (x1:T ) ≈ ∑T\ni=τ+1 ∑τ j=1α (0) i k(xj ,xi) ≤ c̃,\nA change-point is detected if the above rule is satisfied. Otherwise, the procedure will be repeated until a change-point is detected by updating the coefficients using new samples. Fig. 4 shows an empirical example of the (normalized) change score calculated with the proposed algorithm, with comparison with the one by the kernel change detection method (cf. [7]), for the shown data generated from the Lorenz map. We used the RBF Gaussian kernel as in the same way. In the simulation, the parameter of the map changes at 800 and 1200 although the ranges of the data values dramatically change in other areas (where the score by the comparative method has changed correspondingly)."
    }, {
      "heading" : "8 Conclusions",
      "text" : "We presented a spectral analysis method with the Koopman operator in RKHSs, and developed algorithms to perform the analysis using a finite-length data sequence from a nonlinear dynamical system, that is essentially reduced to the calculation of a set of orthogonal bases of the Krylov matrix in RKHSs and the eigendecomposition of the projection of the Koopman operator onto the subspace spanned by the bases. We further considered applications using estimated Koopman spectra with the proposed analysis, which were empirically illustrated using synthetic and real-world data."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by JSPS KAKENHI Grant Number JP16H01548."
    } ],
    "references" : [ {
      "title" : "The principle of minimized iterations in the solution of the matrix eigenvalue problem",
      "author" : [ "W.E. Arnoldi" ],
      "venue" : "Quarterly of Applied Mathematics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1951
    }, {
      "title" : "Koopman-mode decomposition of the cylinder wake",
      "author" : [ "S. Bagheri" ],
      "venue" : "Journal of Fluid Mechanics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Global stability of a jet in cross flow",
      "author" : [ "S. Bagheri", "P. Schlatter", "P.J. Schmid", "D.S. Henningson" ],
      "venue" : "Journal of Fluid Mechanics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Dynamic mode decomposition for perturbation estimation in human robot interaction",
      "author" : [ "E. Berger", "M. Satsuma", "D. Vogt", "B. Jung", "H. Ben Amor" ],
      "venue" : "In Proc. of the 23rd IEEE Int’l Symp. on Robot and Human Interactive Communication,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Ukeiley. Stochastic estimation and proper orthogonal decomposition: Complementary techniques for identifying structure",
      "author" : [ "J.-P. Bonnet", "C.R. Cole", "J. Delville", "M.N. Glauser", "L.S" ],
      "venue" : "Experiments in Fluids,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1994
    }, {
      "title" : "Extracting spatial-temporal coherent patterns in large-scale neural recordings using dynamic mode decomposition",
      "author" : [ "B. Brunton", "L. Aohnson", "J. Ojemann", "J. Nathan Kutz" ],
      "venue" : "Journal of Neuroscience Methods,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "Kernel methods and the exponential family",
      "author" : [ "S. Canu", "A. Smola" ],
      "venue" : "Neurocomputing, 69:714–720,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Variants of dynamic mode decomposition: Boundary condition, Koopman, and Fourier analyses",
      "author" : [ "K.K. Chen", "J.H. Tu", "C.W. Rowley" ],
      "venue" : "Journal of Nonlinear Science,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Limit Theorems in Change-Point Analysis",
      "author" : [ "M. Csörgö", "L. Horváth" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1988
    }, {
      "title" : "Experimental investigation of nonlinear instabilities in annular liquid sheets",
      "author" : [ "D. Duke", "D. Honnery", "J. Soria" ],
      "venue" : "Journal of Fluid Mechanics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Learning nonlinear dynamical systems using an EM algorithm",
      "author" : [ "Z. Ghahramani", "S.T. Roweis" ],
      "venue" : "In Proc. of the 1998 Conf. on Advances in Neural Information Processing Systems II,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Berkooz. Turbulence, Coherent Structures, Dynamical Systems and Symmetry",
      "author" : [ "P. Holmes", "J.L. Lumley" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "Sparsity-promoting dynamic mode decomposition",
      "author" : [ "M.R. Jovanović", "P.J. Schmid", "J.W. Nichols" ],
      "venue" : "Physics of Fluids,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Subspace Methods for System Identification",
      "author" : [ "T. Katayama" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "A kernel subspace method by stochastic realization for learning nonlinear dynamical systems",
      "author" : [ "Y. Kawahara", "T. Yairi", "K. Machida" ],
      "venue" : "In Adv. in Neural Infor. Processing Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Hamiltonian systems and transformation in Hilbert space",
      "author" : [ "B.O. Koopman" ],
      "venue" : "Proc. of the National Academy of Sciences of the United States of America,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1931
    }, {
      "title" : "The pre-image problem in kernel methods",
      "author" : [ "James Tin-Yau Kwok", "Ivor Wai-Hung Tsang" ],
      "venue" : "IEEE Trans. on Neural Networks,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "A spectral algorithm for inference in hidden semi-markov models",
      "author" : [ "I. Melnyk", "A. Banerjee" ],
      "venue" : "In Proc. of the 18th Int’l Conf. on Artificial Intelligence and Statistics",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Spectral properties of dynamical systems, model reduction and decompositions",
      "author" : [ "I. Mezić" ],
      "venue" : "Nonlinear Dynamics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    }, {
      "title" : "Flow structures around a high-speed train extracted using proper orthogonal decomposition and dynamic mode decomposition",
      "author" : [ "T.W. Muld", "G. Efraimsson", "D.S. Henningson" ],
      "venue" : "Computers and Fluids,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "A hierarchy of low-dimensional models for the transient and post-transient cylinder wake",
      "author" : [ "B.R. Noack", "K. Afanasiev", "M. Morzynski", "G. Tadmor", "F. Thiele" ],
      "venue" : "J. of Fluid Mechanics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2003
    }, {
      "title" : "Subspace Identification for Linear Systems: Theory, Implementation, Applications",
      "author" : [ "P. Van Overschee", "B. De Moor" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1996
    }, {
      "title" : "Model reduction for fluids using balanced proper orthogonal decomposition",
      "author" : [ "C.W. Rowley" ],
      "venue" : "International Journal of Bifurcation Chaos,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2005
    }, {
      "title" : "Spectral analysis of nonlinear flows",
      "author" : [ "C.W. Rowley", "I. Mezić", "S. Bagheri", "P. Schlatter", "D.S. Henningson" ],
      "venue" : "Journal of Fluid Mechanics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Dynamic mode decomposition of numerical and experimental data",
      "author" : [ "P.J. Schmid" ],
      "venue" : "Journal of Fluid Mechanics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Dynamic mode decomposition of turbulent cavity flows for self-sustained oscillations",
      "author" : [ "P.J. Schmid", "J. Sesterhenn" ],
      "venue" : "Int’l J. of Heat and Fluid Flow,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "B. Schölkopf", "A. Smola", "K.-R. Müller" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1998
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2004
    }, {
      "title" : "Turbulence and the dynamics of coherent structures",
      "author" : [ "L. Sirovich" ],
      "venue" : "Quarterly of applied mathematics,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1987
    }, {
      "title" : "Nonlinear koopman modes and power system stability assessment without models",
      "author" : [ "Y. Suzuki", "I. Mezić" ],
      "venue" : "IEEE Trans. on Power Systems,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "On dynamic mode decomposition: Theory and applications",
      "author" : [ "J.H. Tu", "C.W. Rowley", "D.M. Luchtenburg", "S.L. Brunton", "J.N. Kutz" ],
      "venue" : "Journal of Computational Dynamics,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    }, {
      "title" : "Gaussian process dynamical models",
      "author" : [ "J. Wang", "A. Hertzmann", "D.M. Blei" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2006
    }, {
      "title" : "A data-driven approximation of the Koopman operator: Extending dynamic mode decomposition",
      "author" : [ "M.O. Williams", "I.G. Kevrekidis", "C.W. Rowley" ],
      "venue" : "Journal of Nonlinear Science,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2015
    }, {
      "title" : "Learning over sets using kernel principal angles",
      "author" : [ "L. Wolf", "A. Shashua" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In machine learning, the problem of learning dynamical systems has been actively discussed, and several Bayesian approaches have been proposed [11, 34].",
      "startOffset" : 143,
      "endOffset" : 151
    }, {
      "referenceID" : 31,
      "context" : "In machine learning, the problem of learning dynamical systems has been actively discussed, and several Bayesian approaches have been proposed [11, 34].",
      "startOffset" : 143,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "For example, proper orthogonal decomposition (POD) (see, for example, [12]), which generates orthogonal modes that optimally capture the vector energy of a given dataset, has been extensively applied to complex phenomena in physics [5, 22] even though this method is currently known to have several drawbacks.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "For example, proper orthogonal decomposition (POD) (see, for example, [12]), which generates orthogonal modes that optimally capture the vector energy of a given dataset, has been extensively applied to complex phenomena in physics [5, 22] even though this method is currently known to have several drawbacks.",
      "startOffset" : 232,
      "endOffset" : 239
    }, {
      "referenceID" : 20,
      "context" : "For example, proper orthogonal decomposition (POD) (see, for example, [12]), which generates orthogonal modes that optimally capture the vector energy of a given dataset, has been extensively applied to complex phenomena in physics [5, 22] even though this method is currently known to have several drawbacks.",
      "startOffset" : 232,
      "endOffset" : 239
    }, {
      "referenceID" : 14,
      "context" : "The so-called spectral method for dynamical systems [15, 31, 17], which is often discussed in machine learning, is closely related to this type of technique, where one aims to estimate a prediction model rather than understand the dynamics by examining the obtained modes.",
      "startOffset" : 52,
      "endOffset" : 64
    }, {
      "referenceID" : 23,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 3,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 5,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 9,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 19,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 23,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 25,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 29,
      "context" : "Among the decomposition techniques, dynamic mode decomposition (DMD) [25, 26] has recently attracted attention in the field of physics, such as flow mechanics, and in engineering, and has been applied to data obtained from complex phenomena [2, 4, 6, 10, 21, 25, 27, 32].",
      "startOffset" : 241,
      "endOffset" : 270
    }, {
      "referenceID" : 15,
      "context" : "DMD approximates the spectra of the Koopman operator [16], which is an infinite-dimensional linear operator that represents nonlinear and finite-dimensional dynamics without linearization.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "The Koopman operator [16], which we denote here by K, is an infinite-dimensional linear operator that acts on a scalar function gi : M→C, mapping gi to a new function Kgi given as follows: (Kgi)(x) = gi ◦ f(x), (2) where ◦ denotes the composition of gi with f .",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 18,
      "context" : "Note that, for a system evolving on an attractor, the Koopman eigenvalues always lie on a unit circle [20].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 23,
      "context" : "DMD [25, 26] (and its variants) is a popular approach for estimating the approximations of λj and uj from a finite-length data sequence y0,y1, .",
      "startOffset" : 4,
      "endOffset" : 12
    }, {
      "referenceID" : 24,
      "context" : "DMD [25, 26] (and its variants) is a popular approach for estimating the approximations of λj and uj from a finite-length data sequence y0,y1, .",
      "startOffset" : 4,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "DMD can fundamentally be considered as a special use of the Arnoldi method [1].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : "A similar issue has been well known for DMD [26], where one needs to adopt a way to robustify DMD by projecting data onto the (truncated) POD directions [8, 33].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "A similar issue has been well known for DMD [26], where one needs to adopt a way to robustify DMD by projecting data onto the (truncated) POD directions [8, 33].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 30,
      "context" : "A similar issue has been well known for DMD [26], where one needs to adopt a way to robustify DMD by projecting data onto the (truncated) POD directions [8, 33].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 26,
      "context" : "First, consider kernel PCA [28] on x0,x1, .",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : ", [3]), discrete Fourier transforms, POD for nonlinear dynamics [30, 12], and balancing modes for linear systems [24] as well as multiple variants of these techniques, such as those using shift modes [22] in conjunction with POD modes.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 28,
      "context" : ", [3]), discrete Fourier transforms, POD for nonlinear dynamics [30, 12], and balancing modes for linear systems [24] as well as multiple variants of these techniques, such as those using shift modes [22] in conjunction with POD modes.",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : ", [3]), discrete Fourier transforms, POD for nonlinear dynamics [30, 12], and balancing modes for linear systems [24] as well as multiple variants of these techniques, such as those using shift modes [22] in conjunction with POD modes.",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : ", [3]), discrete Fourier transforms, POD for nonlinear dynamics [30, 12], and balancing modes for linear systems [24] as well as multiple variants of these techniques, such as those using shift modes [22] in conjunction with POD modes.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : ", [3]), discrete Fourier transforms, POD for nonlinear dynamics [30, 12], and balancing modes for linear systems [24] as well as multiple variants of these techniques, such as those using shift modes [22] in conjunction with POD modes.",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "In particular, POD, which is in principle equivalent to principal component analysis, has been extensively applied to the analysis of physical phenomena [5, 22] even though it suffers from numerous known issues, including the possibility of principal directions in a set of data may not necessarily correspond to the dynamically important ones.",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "In particular, POD, which is in principle equivalent to principal component analysis, has been extensively applied to the analysis of physical phenomena [5, 22] even though it suffers from numerous known issues, including the possibility of principal directions in a set of data may not necessarily correspond to the dynamically important ones.",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "DMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32].",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 9,
      "context" : "DMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32].",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "DMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32].",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "DMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32].",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 25,
      "context" : "DMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32].",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "DMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32].",
      "startOffset" : 131,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "DMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32].",
      "startOffset" : 131,
      "endOffset" : 141
    }, {
      "referenceID" : 29,
      "context" : "DMD has recently attracted considerable attention in physics such as fluid mechanics [2, 10, 21, 25, 27] and in engineering fields [4, 6, 32].",
      "startOffset" : 131,
      "endOffset" : 141
    }, {
      "referenceID" : 30,
      "context" : "Therefore, several variants of DMD have been proposed to rectify this point, including exact DMD [33] and optimized DMD [8].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 7,
      "context" : "Therefore, several variants of DMD have been proposed to rectify this point, including exact DMD [33] and optimized DMD [8].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "proposed sparsity-promoting DMD [13], which provides a framework for the approximation of the Koopman eigenfunctions with fewer bases.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 32,
      "context" : "proposed extended DMD [35], which works on pre-determined basis functions instead of the monomials of observables.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 21,
      "context" : "In system control, subspace identification [23, 14], or called the eigensystem realization method, has been a popular approach to modeling of dynamical systems.",
      "startOffset" : 43,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "In system control, subspace identification [23, 14], or called the eigensystem realization method, has been a popular approach to modeling of dynamical systems.",
      "startOffset" : 43,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "This type of method is known as a spectral method for dynamical systems in the machine learning community and has recently been applied to several types of systems such as variants of hidden Markov models [31, 19], nonlinear dynamical systems [15], and predictive state-representation [17].",
      "startOffset" : 205,
      "endOffset" : 213
    }, {
      "referenceID" : 14,
      "context" : "This type of method is known as a spectral method for dynamical systems in the machine learning community and has recently been applied to several types of systems such as variants of hidden Markov models [31, 19], nonlinear dynamical systems [15], and predictive state-representation [17].",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 30,
      "context" : "This is briefly mentioned in [33] but it would require further investigation in future studies.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "This would be useful to directly understand dominant characteristics of the dynamics, as done in scientific fields with DMD [2, 10, 21, 25, 27].",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "This would be useful to directly understand dominant characteristics of the dynamics, as done in scientific fields with DMD [2, 10, 21, 25, 27].",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 19,
      "context" : "This would be useful to directly understand dominant characteristics of the dynamics, as done in scientific fields with DMD [2, 10, 21, 25, 27].",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 23,
      "context" : "This would be useful to directly understand dominant characteristics of the dynamics, as done in scientific fields with DMD [2, 10, 21, 25, 27].",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 25,
      "context" : "This would be useful to directly understand dominant characteristics of the dynamics, as done in scientific fields with DMD [2, 10, 21, 25, 27].",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 16,
      "context" : "based on a similar idea with multidimensional scaling (MDS), as describe in [18], where a pre-image is recovered to preserve the distance between it and other data points in the input space as well as the feature space.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 16,
      "context" : "Once we obtain n-neighbors based on the feature distances, we can construct the corresponding local coordinate by calculating a set of orthogonal bases (via, for example, singular value decomposition of the data matrix for the neighbors) based on the distances in the input spaces, which are analytically obtained from the feature distances [18].",
      "startOffset" : 341,
      "endOffset" : 345
    }, {
      "referenceID" : 33,
      "context" : "For example, such a distance can be given via the kernel principal angles between two subspaces in the feature space [36].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 33,
      "context" : "The figure shows an embedding of the sequences via MDS with the distance matrix, which was calculated with kernel principal angles [36] between subspaces spanned by the Koopman eigenfunctions.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "Here, we give a criterion for this problem based on the so-called cumulative-sum (CUSUM) of likelihood-ratios (see, for example, [9]).",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : "Although the Koopman operator is, in general, defined for a deterministic system, it is known to be extended to a stochastic system xt+1 = f(xt,vt), where vt is a stochastic disturbance [20].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "Hence, let us define the distribution of xt as a nonparametric exponential family [7], given by p(xt) = exp (⟨θ(·), ψ(xt)⟩H − g(θ)) = exp (⟨φ ◦ f(xt−1), φ(xt)⟩H − g(φ ◦ f(xt−1))) ,",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 6,
      "context" : "[7]), the decision rule, log Λ∗ ≥ c, can be simplified by ignoring the second term.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7]), for the shown data generated from the Lorenz map.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "A spectral analysis of the Koopman operator, which is an infinite dimensional linear operator on an observable, gives a (modal) description of the global behavior of a nonlinear dynamical system without any explicit prior knowledge of its governing equations. In this paper, we consider a spectral analysis of the Koopman operator in a reproducing kernel Hilbert space (RKHS). We propose a modal decomposition algorithm to perform the analysis using finite-length data sequences generated from a nonlinear system. The algorithm is in essence reduced to the calculation of a set of orthogonal bases for the Krylov matrix in RKHS and the eigendecomposition of the projection of the Koopman operator onto the subspace spanned by the bases. The algorithm returns a decomposition of the dynamics into a finite number of modes, and thus it can be thought of as a feature extraction procedure for a nonlinear dynamical system. Therefore, we further consider applications in machine learning using extracted features with the presented analysis. We illustrate the method on the applications using synthetic and real-world data.",
    "creator" : null
  }
}