{
  "name" : "670e8a43b246801ca1eaca97b3e19189.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Flexible Models for Microclustering with Application to Entity Resolution",
    "authors" : [ "Giacomo Zanella", "Brenda Betancourt", "Rebecca C. Steorts" ],
    "emails" : [ "giacomo.zanella@unibocconi.it", "bb222@stat.duke.edu", "hanna@dirichlet.net", "jwmiller@hsph.harvard.edu", "amz19@stat.duke.edu", "beka@stat.duke.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Many clustering applications require models that assume cluster sizes grow linearly with the size of the data set. These applications include topic modeling, inferring population structure, and discriminating among cancer subtypes. Infinitely exchangeable clustering models, including finite mixture models, Dirichlet process mixture models, and Pitman–Yor process mixture models, all make this lineargrowth assumption, and have seen numerous successes when used in these contexts. For other clustering applications, such as entity resolution, this assumption is inappropriate. Entity resolution (including record linkage and de-duplication) involves identifying duplicate2 records in noisy databases [1, 2], traditionally by directly linking records to one another. Unfortunately, this traditional approach is computationally infeasible for large data sets—a serious limitation in “the age of big data” [1, 3]. As a\n∗Giacomo Zanella and Brenda Betancourt are joint first authors. 2In the entity resolution literature, the term “duplicate records” does not mean that the records are identical,\nbut rather that the records are corrupted, degraded, or otherwise noisy representations of the same entity.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nresult, researchers increasingly treat entity resolution as a clustering problem, where each entity is implicitly associated with one or more records and the inference goal is to recover the latent entities (clusters) that correspond to the observed records (data points) [4, 5, 6]. In contrast to other clustering applications, the number of data points in each cluster should remain small, even for large data sets. Applications like this require models that yield clusters whose sizes grow sublinearly with the total number of data points [7]. To address this requirement, we define the microclustering property in section 2 and, in section 3, introduce a new class of models that can exhibit this property. In section 4, we compare two models within this class to two commonly used infinitely exchangeable clustering models."
    }, {
      "heading" : "2 The Microclustering Property",
      "text" : "To cluster N data points x1, . . . , xN using a partition-based Bayesian clustering model, one first places a prior over partitions of [N ] = {1, . . . , N}. Then, given a partitionCN of [N ], one models the data points in each part c ∈ CN as jointly distributed according to some chosen distribution. Finally, one computes the posterior distribution over partitions and, e.g., uses it to identify probable partitions of [N ]. Mixture models are a well-known type of partition-based Bayesian clustering model, in which CN is implicitly represented by a set of cluster assignments z1, . . . , zN . These cluster assignments can be regarded as the first N elements of an infinite sequence z1, z2, . . ., drawn a priori from\nπ ∼ H and z1, z2, . . . |π iid∼ π, (1)\nwhere H is a prior over π and π is a vector of mixture weights with ∑ l πl = 1 and πl ≥ 0 for all l. Commonly used mixture models include (a) finite mixtures where the dimensionality of π is fixed and H is usually a Dirichlet distribution; (b) finite mixtures where the dimensionality of π is a random variable [8, 9]; (c) Dirichlet process (DP) mixtures where the dimensionality of π is infinite [10]; and (d) Pitman–Yor process (PYP) mixtures, which generalize DP mixtures [11].\nEquation 1 implicitly defines a prior over partitions of N = {1, 2, . . .}. Any random partition CN of N induces a sequence of random partitions (CN : N = 1, 2, . . .), where CN is a partition of [N ]. Via the strong law of large numbers, the cluster sizes in any such sequence obtained via equation 1 grow linearly with N because, with probability one, for all l, 1N ∑N n=1 I(zn= l)→ πl as N →∞, where I(·) denotes the indicator function. Unfortunately, this linear growth assumption is not appropriate for entity resolution and other applications that require clusters whose sizes grow sublinearly with N .\nTo address this requirement, we therefore define the microclustering property: A sequence of random partitions (CN : N = 1, 2, . . .) exhibits the microclustering property if MN is op(N), where MN is the size of the largest cluster in CN , or, equivalently, if MN /N → 0 in probability as N →∞. A clustering model exhibits the microclustering property if the sequence of random partitions implied by that model satisfies the above definition. No mixture model can exhibit the microclustering property (unless its parameters are allowed to vary with N ). In fact, Kingman’s paintbox theorem [12, 13] implies that any exchangeable partition of N, such as a partition obtained using equation 1, is either equal to the trivial partition in which each part contains one element or satisfies lim infN→∞MN /N > 0 with positive probability. By Kolmogorov’s extension theorem, a sequence of random partitions (CN : N = 1, 2, . . .) corresponds to an exchangeable random partition of N whenever (a) each CN is finitely exchangeable (i.e., its probability is invariant under permutations of {1, . . . , N}) and (b) the sequence is projective (also known as consistent in distribution)—i.e., if N ′<N , the distribution over CN ′ coincides with the marginal distribution over partitions of [N ′] induced by the distribution over CN . Therefore, to obtain a nontrivial model that exhibits the microclustering property, we must sacrifice either (a) or (b). Previous work [14] sacrificed (a); in this paper, we instead sacrifice (b).\nSacrificing finite exchangeability and sacrificing projectivity have very different consequences. If a partition-based Bayesian clustering model is not finitely exchangeable, then inference will depend on the order of the data points. For most applications, this consequence is undesirable—there is no reason to believe that the order of the data points is meaningful. In contrast, if a model lacks projectivity, then the implied joint distribution over a subset of the data points in a data set will not be the same as the joint distribution obtained by modeling the subset directly. In the context of entity resolution, sacrificing projectivity is a more natural and less restrictive choice than sacrificing finite exchangeability."
    }, {
      "heading" : "3 Kolchin Partition Models for Microclustering",
      "text" : "We introduce a new class of Bayesian models for microclustering by placing a prior on the number of clusters K and, given K, modeling the cluster sizes N1, . . . , NK directly. We start by defining\nK ∼ κ and N1, . . . , NK |K iid∼ µ, (2)\nwhere κ = (κ1, κ2, . . . ) and µ = (µ1, µ2, . . . ) are probability distributions over N = {1, 2, . . .}. We then define N = ∑K k=1Nk and, given N1, . . . , NK , generate a set of cluster assignments z1, . . . , zN by drawing a vector uniformly at random from the set of permutations of (1, . . . , 1︸ ︷︷ ︸ N1 times , 2, . . . , 2︸ ︷︷ ︸ N2 times , . . . . . . ,K, . . . ,K︸ ︷︷ ︸ NK times ). The cluster assignments z1, . . . , zN induce a random partition CN of [N ], where N is itself a random variable—i.e., CN is a random partition of a random number of elements. We refer to the resulting class of marginal distributions over CN as Kolchin partition (KP) models [15, 16] because the form of equation 2 is closely related to Kolchin’s representation theorem for Gibbs-type partitions (see, e.g., 16, theorem 1.2). For appropriate choices of κ and µ, KP models can exhibit the microclustering property (see appendix B for an example).\nIf CN denotes the set of all possible partitions of [N ], then ⋃∞ N=1 CN is the set of all possible\npartitions of [N ] for all N ∈ N. The probability of any given partition CN ∈ ⋃∞ N=1 CN is\nP (CN ) = |CN |!κ|CN |\nN !\n( ∏ c∈CN |c|!µ|c| ) , (3)\nwhere | · | denotes the cardinality of a set, |CN | is the number of clusters in CN , and |c| is the number of elements in cluster c. In practice, however, N is usually observed. Conditioned on N , a KP model implies that P (CN |N) ∝ |CN |!κ|CN | (∏ c∈CN |c|!µ|c| ) . Equation 3 leads to a “reseating algorithm”—much like the Chinese restaurant process (CRP)—derived by sampling from P (CN |N,CN \\n), where CN \\n is the partition obtained by removing element n from CN :\n• for n = 1, . . . , N , reassign element n to – an existing cluster c ∈ CN \\n with probability ∝ (|c|+ 1)\nµ(|c|+1) µ|c|\n– or a new cluster with probability ∝ (|CN \\n|+ 1) κ(|CN\\n|+1) κ|CN\\n| µ1.\nWe can use this reseating algorithm to draw samples from P (CN |N); however, unlike the CRP, it does not produce an exact sample if it is used to incrementally construct a partition from the empty set. In practice, this limitation does not lead to any negative consequences because standard posterior inference sampling methods do not rely on this property. When a KP model is used as the prior in a partition-based clustering model—e.g., as an alternative to equation 1—the resulting Gibbs sampling algorithm for CN is similar to this reseating algorithm, but accompanied by likelihood terms. Unfortunately, this algorithm is slow for large data sets. In appendix C, we therefore propose a faster Gibbs sampling algorithm—the chaperones algorithm—that is particularly well suited to microclustering.\nIn sections 3.1 and 3.2, we introduce two related KP models for microclustering, and in section 3.4 we explain how KP models can be applied in the context of entity resolution with categorical data."
    }, {
      "heading" : "3.1 The NBNB Model",
      "text" : "We start with equation 3 and define\nκ = NegBin (a, q) and µ = NegBin (r, p) , (4)\nwhere NegBin(a, q) and NegBin(r, p) are negative binomial distributions truncated to N = {1, 2, . . . }. We assume that a > 0 and q ∈ (0, 1) are fixed hyperparameters, while r and p are distributed as r ∼ Gam(ηr, sr) and p ∼ Beta(up, vp) for fixed ηr, sr, up and vp.3 We refer to the resulting marginal distribution over CN as the negative binomial–negative binomial (NBNB) model.\n3We use the shape-and-rate parameterization of the gamma distribution.\nBy substituting equation 4 into equation 3, we obtain the probability of CN conditioned N :\nP (CN |N, a, q, r, p) ∝ Γ (|CN |+ a)β|CN | ∏ c∈CN Γ (|c|+ r) Γ (r) , (5)\nwhere β = q (1−p) r\n1−(1−p)r . We provide the complete derivation of equation 5, along with the conditional posterior distributions over r and p, in appendix A.2. Posterior inference for the NBNB model involves alternating between (a) sampling CN from P (CN |N, a, q, r, p) using the chaperones algorithm and (b) sampling r and p from their respective conditional posteriors using, e.g., slice sampling [17]."
    }, {
      "heading" : "3.2 The NBD Model",
      "text" : "Although κ = NegBin (a, q) will yield plausible values of K, µ = NegBin (r, p) may not be sufficiently flexible to capture realistic properties of N1, . . . , NK , especially when K is large. For example, in a record-linkage application involving two otherwise noise-free databases containing thousands of records, K will be large and each Nk will be at most two. A negative binomial distribution cannot capture this property. We therefore define a second KP model—the negative binomial–Dirichlet (NBD) model—by taking a nonparametric approach to modeling N1, . . . , NK and drawing µ from an infinite-dimensional Dirichlet distribution over the positive integers:\nκ = NegBin (a, q) and µ |α,µ(0) ∼ Dir ( α,µ(0) ) , (6)\nwhere α > 0 is a fixed concentration parameter and µ(0) = (µ(0)1 , µ (0) 2 , · · · ) is a fixed base measure with ∑∞ m=1 µ (0) m = 1 and µ (0) m ≥ 0 for all m. The probability of CN conditioned on N and µ is\nP (CN |N, a, q,µ) ∝ Γ (|CN |+ a) q|CN | ∏ c∈CN |c|!µ|c|. (7)\nPosterior inference for the NBD model involves alternating between (a) sampling CN from P (CN |N, a, q,µ) using the chaperones algorithm and (b) sampling µ from its conditional posterior:\nµ |CN , α,µ(0) ∼ Dir ( αµ (0) 1 + L1, α µ (0) 2 + L2, . . . ) , (8)\nwhere Lm is the number of clusters of size m in CN . Although µ is an infinite-dimensional vector, only the first N elements affect P (CN | a, q,µ). Therefore, it is sufficient to sample the (N + 1)-dimensional vector (µ1, . . . , µN , 1− ∑N m=1 µm) from equation 8, modified accordingly, and retain only µ1, . . . , µN . We provide complete derivations of equations 7 and 8 in appendix A.3."
    }, {
      "heading" : "3.3 The Microclustering Property for the NBNB and NBD Models",
      "text" : "Figure 1 contains empirical evidence suggesting that the NBNB and NBD models both exhibit the microclustering property. For each model, we generated samples ofMN /N forN = 100, . . . , 104. For the NBNB model, we set a = 1, q = 0.5, r = 1, and p = 0.5 and generated the samples using rejection sampling. For the NBD model, we set a = 1, q = 0.5, and α = 1 and set µ(0) to be a geometric distribution over N = {1, 2, . . .} with a parameter of 0.5. We generated the samples using MCMC methods. For both models, MN /N appears to converge to zero in probability as N →∞, as desired. In appendix B, we also prove that a variant of the NBNB model exhibits the microclustering property."
    }, {
      "heading" : "3.4 Application to Entity Resolution",
      "text" : "KP models can be used to perform entity resolution. In this context, the data points x1, . . . , xN are observed records and theK clusters are latent entities. If each record consists ofF categorical fields, then\nCN ∼ KP model (9) θfk | δf ,γf ∼ Dir ( δf ,γf ) (10)\nzn ∼ ζ(CN , n) (11) xfn | zn,θf1, . . . ,θfK ∼ Cat (θfzn) (12)\nfor f = 1, . . . , F , k = 1, . . . ,K, and n = 1, . . . , N , where ζ(CN , n) maps the nth record to a latent cluster assignment zn according to CN . We assume that δf > 0 is distributed as δf ∼ Gam (1, 1), while γf is fixed. Via Dirichlet–multinomial conjugacy, we can marginalize over θ11, . . . ,θFK to obtain a closed-form expression for P (x1, . . . , xN | z1, . . . , zN , δf ,γf ). Posterior inference involves alternating between (a) sampling CN from P (CN |x1, . . . , xN , δf ) using the chaperones algorithm accompanied by appropriate likelihood terms, (b) sampling the parameters of the KP model from their conditional posteriors, and (c) sampling δf from its conditional posterior using slice sampling."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we compare two entity resolution models based on the NBNB model and the NBD model to two similar models based on the DP mixture model [10] and the PYP mixture model [11]. All four models use the likelihood in equations 10 and 12. For the NBNB model and the NBD model, we set a and q to reflect a weakly informative prior belief that E[K] = √ Var[K] = N2 . For the NBNB model, we set ηr = sr = 1 and up = vp = 2.4 For the NBD model, we set α = 1 and set µ(0) to be a geometric distribution over N = {1, 2, . . .} with a parameter of 0.5. This base measure reflects a prior belief that E[Nk] = 2. Finally, to ensure a fair comparison between the two different classes of model, we set the DP and PYP concentration parameters to reflect a prior belief that E[K] = N2 . We assess how well each model “fits” four data sets typical of those arising in real-world entity resolution applications. For each data set, we consider four statistics: (a) the number of singleton clusters, (b) the maximum cluster size, (c) the mean cluster size, and (d) the 90th percentile of cluster sizes. We compare each statistic’s true value to its posterior distribution according to each of the models. For each model and data set combination, we also consider five entity-resolution summary statistics: (a) the posterior expected number of clusters, (b) the posterior standard error, (c) the false negative rate, (d) the false discovery rate, and (e) the posterior expected value of δf = δ for f = 1, . . . , F . The false negative and false discovery rates are both invariant under permutations of 1, . . . ,K [5, 18]."
    }, {
      "heading" : "4.1 Data Sets",
      "text" : "We constructed four realistic data sets, each consisting of N records associated with K entities.\nItaly: We derived this data set from the Survey on Household Income and Wealth, conducted by the Bank of Italy every two years. There are nine categorical fields, including year of birth, employment status, and highest level of education attained. Ground truth is available via unique identifiers based upon social security numbers; roughly 74% of the clusters are singletons. We used the 2008 and 2010 databases from the Fruili region to create a record-linkage data set consisting of N = 789 records; each Nk is at most two. We discarded the records themselves, but preserved the number of fields, the empirical distribution of categories for each field, the number of clusters, and the cluster sizes. We then generated synthetic records using equations 10 and 12. We created three variants of this data set, corresponding to δ = 0.02, 0.05, 0.1. For all three, we used the empirical distribution of categories for field f as γf . By generating synthetic records in this fashion, we preserve the pertinent characteristics of the original data, while making it easy to isolate the impacts of the different priors over partitions.\nNLTCS5000: We derived this data set from the National Long Term Care Survey (NLTCS)5—a longitudinal survey of older Americans, conducted roughly every six years. We used four of the\n4We used p ∼ Beta (2, 2) because a uniform prior implies an unrealistic prior belief that E[Nk] =∞. 5http://www.nltcs.aas.duke.edu/\navailable fields: date of birth, sex, state of residence, and regional office. We split date of birth into three separate fields: day, month, and year. Ground truth is available via social security numbers; roughly 68% of the clusters are singletons. We used the 1982, 1989, and 1994 databases and down-sampled the records, preserving the proportion of clusters of each size and the maximum cluster size, to create a record-linkage data set of N = 5, 000 records; each Nk is at most three. We then generated synthetic records using the same approach that we used to create the Italy data set.\nSyria2000 and SyriaSizes: We constructed these data sets from data collected by four human-rights groups between 2011 and 2014 on people killed in the Syrian conflict [19, 20]. Hand-matched ground truth is available from the Human Rights Data Analysis Group. Because the records were hand matched, the data are noisy and potentially biased. Performing entity resolution is non-trivial because there are only three categorical fields: gender, governorate, and date of death. We split date of death, which is present for most records, into three separate fields: day, month, and year. However, because the records only span four years, the year field conveys little information. In addition, most records are male, and there are only fourteen governorates. We created the Syria2000 data set by down-sampling the records, preserving the proportion of clusters of each size, to create a data set of N = 2, 000 records; the maximum cluster size is five. We created the SyriaSizes data set by down-sampling the records, preserving some of the larger clusters (which necessarily contain withindatabase duplications), to create a data set of N = 6, 700 records; the maximum cluster size is ten. We provide the empirical distribution over cluster sizes for each data set in appendix D. We generated synthetic records for both data sets using the same approach that we used to create the Italy data set."
    }, {
      "heading" : "4.2 Results",
      "text" : "We report the results of our experiments in table 1 and figure 2. The NBNB and NBD models outperformed the DP and PYP models for almost all variants of the Italy and NLTCS5000 data sets. In general, the NBD model performed the best of the four, and the differences between the models’ performance grew as the value of δ increased. For the Syria2000 and SyriaSizes data sets, we see no consistent pattern to the models’ abilities to recover the true values of the data-set statistics. Moreover, all four models had poor false negative rates, and false discovery rates—most likely because these data sets are extremely noisy and contain very few fields. We suspect that no entity resolution model would perform well for these data sets. For three of the four data sets, the exception being the Syria2000 data set, the DP model and the PYP model both greatly overestimated the number of clusters for larger values of δ. Taken together, these results suggest that the flexibility of the NBNB and NBD models make them more appropriate choices for most entity resolution applications."
    }, {
      "heading" : "5 Summary",
      "text" : "Infinitely exchangeable clustering models assume that cluster sizes grow linearly with the size of the data set. Although this assumption is reasonable for some applications, it is inappropriate for others. For example, when entity resolution is treated as a clustering problem, the number of data points in each cluster should remain small, even for large data sets. Applications like this require models that yield clusters whose sizes grow sublinearly with the size of the data set. We introduced the microclustering property as one way to characterize models that address this requirement. We then introduced a highly flexible class of models—KP models—that can exhibit this property. We presented two models within this class—the NBNB model and the NBD model—and showed that they are better suited to entity resolution applications than two infinitely exchangeable clustering models. We therefore recommend KP models for applications where the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Tamara Broderick, David Dunson, Merlise Clyde, and Abel Rodriguez for conversations that helped form the ideas in this paper. In particular, Tamara Broderick played a key role in developing the idea of microclustering. We also thank the Human Rights Data Analysis Group for providing us with data. This work was supported in part by NSF grants SBE-0965436, DMS-1045153, and IIS-1320219; NIH grant 5R01ES017436-05; the John Templeton Foundation; the Foerster-Bernstein Postdoctoral Fellowship; the UMass Amherst CIIR; and an EPSRC Doctoral Prize Fellowship."
    } ],
    "references" : [ {
      "title" : "Data Matching: Concepts and Techniques for Record Linkage",
      "author" : [ "P. Christen" ],
      "venue" : "Entity Resolution, and Duplicate Detection. Springer",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A survey of indexing techniques for scalable record linkage and deduplication",
      "author" : [ "P. Christen" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, 24(9)",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Overview of record linkage and current research directions",
      "author" : [ "W.E. Winkler" ],
      "venue" : "Technical report, U.S. Bureau of the Census Statistical Research Division",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Entity resolution with empirically motivated priors",
      "author" : [ "R.C. Steorts" ],
      "venue" : "Bayesian Analysis, 10(4):849– 875",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "SMERED: A Bayesian approach to graphical record linkage and de-duplication",
      "author" : [ "R.C. Steorts", "R. Hall", "S.E. Fienberg" ],
      "venue" : "Journal of Machine Learning Research, 33:922–930",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Variational bayes for merging noisy databases",
      "author" : [ "T. Broderick", "R.C. Steorts" ],
      "venue" : "NIPS 2014 Workshop on Advances in Variational Inference",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On Bayesian analysis of mixtures with an unknown number of components",
      "author" : [ "S. Richardson", "P.J. Green" ],
      "venue" : "Journal of the Royal Statistical Society Series B, pages 731–792",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Mixture models with a prior on the number of components",
      "author" : [ "J.W. Miller", "M.T. Harrison" ],
      "venue" : "arXiv:1502.06241",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A constructive definition of Dirichlet priors",
      "author" : [ "J. Sethuraman" ],
      "venue" : "Statistica Sinica, 4:639–650",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Generalized weighted Chinese restaurant processes for species sampling mixture models",
      "author" : [ "H. Ishwaran", "L.F. James" ],
      "venue" : "Statistica Sinica, 13(4):1211–1236",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The representation of partition structures",
      "author" : [ "J.F. .C Kingman" ],
      "venue" : "Journal of the London Mathematical Society,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1978
    }, {
      "title" : "Exchangeability and related topics",
      "author" : [ "D. Aldous" ],
      "venue" : "École d’Été de Probabilités de Saint-Flour XIII—1983, pages 1–198",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "An alternative prior process for nonparametric Bayesian clustering",
      "author" : [ "H.M. Wallach", "S. Jensen", "L. Dicker", "K.A. Heller" ],
      "venue" : "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A problem of the allocation of particles in cells and cycles of random permutations",
      "author" : [ "V.F. Kolchin" ],
      "venue" : "Theory of Probability & Its Applications, 16(1):74–90",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Combinatorial stochastic processes",
      "author" : [ "J. Pitman" ],
      "venue" : "École d’Été de Probabilités de Saint-Flour XXXII—2002",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Slice sampling",
      "author" : [ "R.M. Neal" ],
      "venue" : "Annals of Statistics, 31:705–767",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A comparison of blocking methods for record linkage",
      "author" : [ "R.C. Steorts", "S.L. Ventura", "M. Sadinle", "S.E. Fienberg" ],
      "venue" : "International Conference on Privacy in Statistical Databases, pages 253–268",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and P",
      "author" : [ "M. Price", "J. Klingner", "A. Qtiesh" ],
      "venue" : "Ball. Updated statistical analysis of documentation of killings in the Syrian Arab Republic",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Updated statistical analysis of documentation of killings in the Syrian Arab Republic",
      "author" : [ "M. Price", "J. Klingner", "A. Qtiesh", "P. Ball" ],
      "venue" : "Human Rights Data Analysis Group, Geneva",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Entity resolution (including record linkage and de-duplication) involves identifying duplicate2 records in noisy databases [1, 2], traditionally by directly linking records to one another.",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "Entity resolution (including record linkage and de-duplication) involves identifying duplicate2 records in noisy databases [1, 2], traditionally by directly linking records to one another.",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "Unfortunately, this traditional approach is computationally infeasible for large data sets—a serious limitation in “the age of big data” [1, 3].",
      "startOffset" : 137,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "Unfortunately, this traditional approach is computationally infeasible for large data sets—a serious limitation in “the age of big data” [1, 3].",
      "startOffset" : 137,
      "endOffset" : 143
    }, {
      "referenceID" : 3,
      "context" : "result, researchers increasingly treat entity resolution as a clustering problem, where each entity is implicitly associated with one or more records and the inference goal is to recover the latent entities (clusters) that correspond to the observed records (data points) [4, 5, 6].",
      "startOffset" : 272,
      "endOffset" : 281
    }, {
      "referenceID" : 4,
      "context" : "result, researchers increasingly treat entity resolution as a clustering problem, where each entity is implicitly associated with one or more records and the inference goal is to recover the latent entities (clusters) that correspond to the observed records (data points) [4, 5, 6].",
      "startOffset" : 272,
      "endOffset" : 281
    }, {
      "referenceID" : 5,
      "context" : "Applications like this require models that yield clusters whose sizes grow sublinearly with the total number of data points [7].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 6,
      "context" : "Commonly used mixture models include (a) finite mixtures where the dimensionality of π is fixed and H is usually a Dirichlet distribution; (b) finite mixtures where the dimensionality of π is a random variable [8, 9]; (c) Dirichlet process (DP) mixtures where the dimensionality of π is infinite [10]; and (d) Pitman–Yor process (PYP) mixtures, which generalize DP mixtures [11].",
      "startOffset" : 210,
      "endOffset" : 216
    }, {
      "referenceID" : 7,
      "context" : "Commonly used mixture models include (a) finite mixtures where the dimensionality of π is fixed and H is usually a Dirichlet distribution; (b) finite mixtures where the dimensionality of π is a random variable [8, 9]; (c) Dirichlet process (DP) mixtures where the dimensionality of π is infinite [10]; and (d) Pitman–Yor process (PYP) mixtures, which generalize DP mixtures [11].",
      "startOffset" : 210,
      "endOffset" : 216
    }, {
      "referenceID" : 8,
      "context" : "Commonly used mixture models include (a) finite mixtures where the dimensionality of π is fixed and H is usually a Dirichlet distribution; (b) finite mixtures where the dimensionality of π is a random variable [8, 9]; (c) Dirichlet process (DP) mixtures where the dimensionality of π is infinite [10]; and (d) Pitman–Yor process (PYP) mixtures, which generalize DP mixtures [11].",
      "startOffset" : 296,
      "endOffset" : 300
    }, {
      "referenceID" : 9,
      "context" : "Commonly used mixture models include (a) finite mixtures where the dimensionality of π is fixed and H is usually a Dirichlet distribution; (b) finite mixtures where the dimensionality of π is a random variable [8, 9]; (c) Dirichlet process (DP) mixtures where the dimensionality of π is infinite [10]; and (d) Pitman–Yor process (PYP) mixtures, which generalize DP mixtures [11].",
      "startOffset" : 374,
      "endOffset" : 378
    }, {
      "referenceID" : 10,
      "context" : "In fact, Kingman’s paintbox theorem [12, 13] implies that any exchangeable partition of N, such as a partition obtained using equation 1, is either equal to the trivial partition in which each part contains one element or satisfies lim infN→∞MN /N > 0 with positive probability.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "In fact, Kingman’s paintbox theorem [12, 13] implies that any exchangeable partition of N, such as a partition obtained using equation 1, is either equal to the trivial partition in which each part contains one element or satisfies lim infN→∞MN /N > 0 with positive probability.",
      "startOffset" : 36,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "Previous work [14] sacrificed (a); in this paper, we instead sacrifice (b).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "We refer to the resulting class of marginal distributions over CN as Kolchin partition (KP) models [15, 16] because the form of equation 2 is closely related to Kolchin’s representation theorem for Gibbs-type partitions (see, e.",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "We refer to the resulting class of marginal distributions over CN as Kolchin partition (KP) models [15, 16] because the form of equation 2 is closely related to Kolchin’s representation theorem for Gibbs-type partitions (see, e.",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "4 Experiments In this section, we compare two entity resolution models based on the NBNB model and the NBD model to two similar models based on the DP mixture model [10] and the PYP mixture model [11].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "4 Experiments In this section, we compare two entity resolution models based on the NBNB model and the NBD model to two similar models based on the DP mixture model [10] and the PYP mixture model [11].",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 17,
      "context" : "Syria2000 and SyriaSizes: We constructed these data sets from data collected by four human-rights groups between 2011 and 2014 on people killed in the Syrian conflict [19, 20].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 18,
      "context" : "Syria2000 and SyriaSizes: We constructed these data sets from data collected by four human-rights groups between 2011 and 2014 on people killed in the Syrian conflict [19, 20].",
      "startOffset" : 167,
      "endOffset" : 175
    } ],
    "year" : 2016,
    "abstractText" : "Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman–Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.",
    "creator" : null
  }
}