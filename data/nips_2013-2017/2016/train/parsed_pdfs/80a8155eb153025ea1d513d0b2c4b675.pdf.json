{
  "name" : "80a8155eb153025ea1d513d0b2c4b675.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Robust k-means: a Theoretical Revisit",
    "authors" : [ "Alexandros Georgogiannis" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Let φ : R → R+ be a lower semi-continuous (lsc) and symmetric function with minimum value φ(0). Given a set of points Xn = {x1, . . . , xn} ⊂ Rp, consider the generalized k-means problem (GKM) [7]\nmin c1,...,ck\nRn(c1, . . . , ck) = n∑\ni=1\nmin 1≤l≤k φ(||xi − cl||2)\nsubject to cl ∈ Rp, l ∈ {1, . . . , k}. (GKM)\nOur aim is to find a set of k centers {c1, . . . , ck} that minimize the clustering risk Rn. These centers define a partition of Xn into k clusters A = {A1, . . . , Ak}, defined as\nAl = { x ∈ Xn : l = argmin1≤j≤k φ(||x− cj ||2) } , (1)\nwhere ties are broken randomly. Varying φ beyond the usual quadratic function (φ(t) = t2) we expect to gain some robustness against the outliers [9]. When φ is upper bounded by δ, the clusters are defined as follows. For l ≤ k, let\nAl = { x ∈ Xn : l = argmin1≤j≤k φ(||x− cj ||2) and φ(||x− cl||2) ≤ δ } , (2)\nand define the extra cluster\nAk+1 = { x ∈ Xn : min\n1≤j≤k φ(||x− cj ||2) > δ\n} . (3)\nThis extra cluster contains points whose distance from their closest center, when measured according to φ(||x−cl||2), is larger than δ and, as will become clear later, it represents the set of outliers. From now on, given a set of centers {c1, . . . , ck}, we write just A = {A1, . . . , Ak} and implicitly mean A ∪Ak+1 when φ is bounded.1\n1 For a similar definition for the set of clusters induced by a bounded φ see also Section 4 in [2].\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nNow, consider the following instance of (GKM), for the same set of points Xn,\nmin c1,...,ck\nR′n(c1, . . . , ck) = n∑\ni=1\nmin 1≤l≤k { min oi 1 2 ||xi − cl − oi||22 + fλ(||oi||2)\n︸ ︷︷ ︸ φ(||xi−cl||2)\n}\nsubject to cl ∈ Rp, l = 1, . . . , k, oi ∈ Rp, i = 1, . . . , n,\n(RKM)\nwhere fλ : R→ R+ is a symmetric, lsc, proper2 and bounded from below function, with minimum value fλ(0), and λ a non-negative parameter. This problem is called robust k-means (RKM) and, as we show later, it takes the form of (GKM) when φ equals the Moreau envelope of fλ. The problem (RKM) [5, 24] describes the following simple model: we allow each observation xi to take on an “error” term oi and we penalize the errors, using a group penalty, in order to encourage most of the observations’ errors to be equal to zero. We consider functions fλ where the parameter λ ≥ 0 has the following effect: for λ = 0, all oi’s may become arbitrary large (all observations are outliers), while, for λ→∞, all oi’s become zero (no outliers); non-trivial cases occur for intermediate values 0 < λ <∞. Our interest is in understanding the robustness and consistency properties of (RKM). Robustness: Although robustness is an important notion, it has not been given a standard technical definition in the literature. Here, we focus on the finite sample breakdown point [18], which counts how many outliers a dataset may contain without causing significant damage in the estimates of the centers. Such damage is reflected to an arbitrarily large magnitude of at least one center. In Section 3, we show that two outliers in a dataset are enough to breakdown some centers. On the other hand, if we restrict our focus on some “well structured” datasets, then (RKM) has some remarkable robustness properties even if there is a considerable amount of contamination.\nConsistency: Much is known about the consistency of (GKM) when the function φ is lsc and increasing [11, 15]. It turns out that this case also includes the case of (RKM) when fλ is convex (see Section 3.1 for details). In Section 4, we show that the known non-asymptotic results about consistency of quadratic k-means may remain valid even when fλ is non-convex."
    }, {
      "heading" : "2 Preliminaries and some technical remarks",
      "text" : "We start our analysis with a few technical tools from variational analysis [19]. Here, we introduce the necessary notation and a lemma (the proofs are in the appendix). The Moreau envelope eµf (x) with parameter µ > 0 (Definition 1.22 in [19]) of an lsc, proper, and bounded from below function f : Rp → R and the associated (possibly multivalued) proximal map Pµf : Rp →→ Rp are\neµf (x) = minz∈Rp 1 2µ ||x− z||22 + f(z) and P µ f (x) = argminz∈Rp 1 2µ ||x− z||22 + f(z), (4)\nrespectively. In order to simplify the notation, in the following, we fix µ to 1 and suppress the superscript. The Moreau envelope is a continuous approximation from below of f having the same set of minimizers while the proximal map gives the (possibly non-unique) minimizing arguments in (4). For (GKM), we define Φ : Rp → R as Φ(x) := φ(||x||2). Accordingly, for (RKM), we define Fλ : Rp → R as Fλ(x) := fλ(||x||2). Thus, we obtain the following pairs:\nefλ(x) := min o∈R\n1 2 (x− o)2 + fλ(o), Pfλ(x) := argmino∈Refλ(x), x ∈ R (5a)\neFλ(x) := min o∈Rp\n1 2 ||x− o||22 + Fλ(o), PFλ(x) := argmino∈RpeFλ(x), x ∈ Rp. (5b)\nObviously, (RKM) is equivalent to (GKM) when Φ(x) = eFλ(x). Every map P : R→→ R throughout the text is assumed to be i) odd, i.e., P(−x) = −P(x), ii) compact-valued, iii) non-decreasing, and iv) have a closed graph. We know that for any such map there exists at least one function fλ such that P = Pfλ (Proposition 3 in [26]).3 Finally, for our purposes (outlier detection), it is natural\n2We call f proper if f(x) < ∞ for at least one x ∈ Rn, and f(x) > −∞ for all x ∈ Rn; in words, if the domain of f is a nonempty set on which f is finite (see page 5 in [19]).\n3 Accordingly, for a general function φ : R → [0,∞) to be a Moreau envelope, i.e., φ(·) = efλ(·) as defined in (5a) for some function fλ, we require that φ(·)− 12 | · | 2 is a concave function (Proposition 1 in [26]).\nto require that v) P is a shrinkage rule, i.e., P(x) ≤ x, ∀x ≥ 0. The following corollary is quite straightforward and useful in the sequel. Corollary 1. Using the notation in definitions (5a) and (5b), we have\nPFλ(x) = x\n||x||2 Pfλ(||x||2) and eFλ(x) = efλ(||x||2). (6)\nPassing from a model of minimization in terms of a single problem, like (GKM), to a model in which a problem is expressed in a particular parametric form, like (RKM) with the Moreau envelope, the description of optimality conditions is opened to the incorporation of the multivalued map PFλ . The next lemma describes the necessary conditions for a center cl to be (local) optimal for (RKM). Since we deal with the general case, well known results, such as smoothness of the Moreau envelope or convexity of its subgradients, can no longer be taken for granted.\nRemark 1. Let Φ(·) = eFλ(·). The usual subgradient, denoted as ∂̂Φ(x), is not sufficient to characterize the differentiability properties of R′n in (RKM). Instead, we use the (generalized) subdifferential ∂Φ(x) (Definition 8.3 in [19]). For all x, we have ∂̂Φ(x) ⊆ ∂Φ(x). Usually, the previous two sets coincide at a point x. In this case, Φ is called regular at x. However, it is common in practice that the sets ∂̂Φ(x) and ∂Φ(x) are different (for a detailed exposition on subgradients see Chapter 8 in [19]; see also Example 1 in Appendix A.9). Lemma 1. Let PFλ : Rp →→ Rp be a proximal map and set Φ(·) = eFλ(·). The necessary (generalized) first order conditions for the centers {c1, . . . , ck} ⊂ Rp to be optimal for (RKM) are\n0 ∈ ∂ {∑\ni∈Al\nΦ(xi − cl) } ⊆ ∑\ni∈Al\n∂Φ(xi − cl) ⊆ ∑\ni∈Al\n(cl − xi + PFλ(xi − cl)) , l ∈ {1, . . . , k}.\n(7)\nThe interpretation of the set inclusion above is the following: for any center cl ∈ Rp, every subgradient vector in ∂Φ(xi − cl) must be a vector associated with a vector in PFλ(xi − cl) (Theorem 10.13 in [19]). However, in general, the converse does not hold true. We note that when the proximal map is single-valued and continuous, which happens for example not only when fλ is convex, but also for many popular non-convex penalties, both set inclusions become equalities and the converse holds, i.e., every vector in PFλ(xi − cl) is a vector associated with a subgradient in ∂Φ(xi − cl) (Theorem 10.13 in [19] and Proposition 7 in [26]).\nWe close this section with some useful remarks on the properties of the Moreau envelope as a map between two spaces of functions. There exist cases where two different functions, fλ ̸= f ′λ, have equal Moreau envelopes, efλ = ef ′λ (Proposition 1 in [26]), implying that two different forms of (RKM) correspond to the same φ in (GKM). For example, the proximal hull of fλ, defined as hµfλ(x) := −e\nµ (−eµfλ ) (x), is a function different from fλ but has the same Moreau envelope as fλ (see also Example 1.44 in [19], Proposition 2 and Example 3 in [26]). This is the main reason we preferred the proximal map instead of the penalty function point of view for the analysis of (RKM).\n3 On the breakdown properties of robust k-means\nIn this section, we study the finite sample breakdown point of (RKM) and, more specifically, its universal breakdown point. Loosely speaking, the breakdown point measures the minimum fraction of outliers that can cause excessive damage in the estimates of the centers. Here, it will become clear how the interplay between the two forms, (GKM) and (RKM), helps the analysis. Given a dataset Xn = {x1, . . . , xn} and a nonnegative integer m ≤ n, we say that Xnm is an m-modification if it arises from Xn after replacing m of its elements by arbitrary elements x′i ∈ Rp [6]. Denote as r(λ) the non-outlier samples, as counted after solving (RKM), for a dataset Xn and some λ ≥ 0, i.e., 4\nr(λ) := ∣∣∣{xi ∈ Xn : ||oi||2 = 0, i = 1, . . . , n} ∣∣∣. (8)\nThen, the number of estimated outliers is q(λ) = n − r(λ). In order to simplify notation, we drop the dependence of r and q on λ. With this notation, we proceed to the following definition.\n4More than one λ can yield the same r, but this does not affect our analysis.\nDefinition 1 (universal breakdown point for the centers [6]). Let n, r, k be such that n ≥ r ≥ k+1. Given a dataset Xnm in Rp, let {c1, . . . , ck} denote the (global) optimal set of centers for (RKM). The universal breakdown value of (RKM) is\nβ(n, r, k) := min Xn min 1≤m≤n {m n\n: sup Xnm max 1≤l≤k\n||cl||2 =∞ } . (9)\nHere, Xn = {x1, . . . , xn} ⊂ Rp while Xnm ⊂ Rp runs over all m-modifications of Xn.\nAccording to the concept of universal breakdown point, (RKM) breaks down at the first integer m for which there exists a set Xn such that the estimates of the cluster centers become arbitrarily bad for a suitable modification Xnm. Our analysis is based on Pfλ and considers two cases: those of biased and unbiased proximal maps. The former corresponds to the class of convex functions fλ, while the latter corresponds to a class of non-convex fλ."
    }, {
      "heading" : "3.1 Biased proximal maps: the case of convex fλ",
      "text" : "If fλ is convex, then Φ = eFλ is also convex while PFλ is continuous, single-valued, and satisfies [19] ||x− PFλ(x)||2 →∞ as ||x||2 →∞. (10) Proximal maps with this property are called biased since, as the l2-norm of x increases, so does the norm of the difference in (10). In this case, for each xi ∈ Al, from Lemma 1 and expression (10), we have\n||∇Φ(xi−cl)||2 = ||∇eFλ(xi−cl)||2 = ||cl−xi+PFλ(xi−cl)||2 →∞ as ||xi−cl||2 →∞. (11)\nThe supremum value of ||∇Φ(x− cl)||2 is closely related to the gross error sensitivity of an estimator [9]. It is interpreted as the worst possible influence which a sample x can have on cl [7]. In view of (11) and the definition of the clusters in (1), (RKM) is extremely sensitive. Although it can detect an outlier, i.e., a sample xi with a nonzero estimate for ||oi||2, it does not reject it since the influence of xi on its closest center never vanishes.5 The l1-norm, fλ(x) = λ|x|, which has Moreau envelope equal to the Huber loss-function [24], is the limiting case for the class of convex penalty functions that, although it keeps the difference ||x − PFλ(x)||2 in (10) constant and equal to λ, introduces a bias term proportional to λ in the estimate cl. The following proposition shows that (RKM) with a biased PFλ has breakdown point equal to 1 n , i.e., one outlier suffices to breakdown a center. Proposition 1. Assume k ≥ 2, k + 1 < r ≤ n. Given a biased proximal map, there exist a dataset Xn and a modification Xn1 such that (RKM) breaks down."
    }, {
      "heading" : "3.2 Unbiased proximal maps: the case of non-convex fλ",
      "text" : "Consider now the l0-(pseudo)norm on R, fλ(z) := λ|z|0 = λ 2\n2 {z ̸=0}, and the associated hardthresholding proximal operator Pλ|·|0 : R→→ R,\nPλ|·|0(x) = argminz∈R 1 2 (x− z) 2 + fλ(z) =\n⎧ ⎨\n⎩ 0, |x| < λ, {0, x}, |x| = λ, x, |x| > λ.\n(12)\nAccording to Lemma 1, for p = 1 (scalar case), we have\n∂Φ(xi − cl) ⊆ cl − xi + Pλ|·|0(xi − cl) (12) = {0} for |xi − cl| > λ, xi ∈ Al, (13)\nimplying that Φ(xi − cl), as a function of cl, remains constant for |xi − cl| > λ. As a consequence of (13), if cl is local optimal, then 0 ∈ ∂{ ∑ i∈Al Φ(xi − cl)} and\n0 ∈ ∑\ni∈Al, |xi−cl|<λ\n(cl − xi) + ∑\ni∈Al, |xi−cl|=λ\n( cl − xi + Pλ|·|(xi − cl) ) .\n(14)\nDepending on the value of λ, (RKM) with the l0-norm is able to ignore samples with distance from their closest center larger than λ. This is done since Pλ|·|0(xi−cl) = xi−cl whenever |xi−cl| > λ\n5See the analysis in [7] about the influence function of (GKM) when φ is convex.\nand the influence of xi vanishes. In fact, there is a whole family of non-convex fλ’s whose proximal map Pfλ satisfies\nPfλ(x) = x, for all |x| > τ, (15)\nfor some τ > 0. These are called unbiased proximal maps [13, 20] and have the useful property that, as one observation is arbitrarily modified, all estimated cluster centers remain bounded by a constant that depends only on the remaining unmodified samples. Under certain circumstances, the proof of the following proposition reveals that, if there exists one outlier in the dataset, then robust k-means will reject it.\nProposition 2. Assume k ≥ 2, k + 1 < r ≤ n, and consider the dataset Xn = {x1, . . . , xn} along with its modification by one replacement y, Xn1 = {x1, . . . , xn−1, y}. If we solve (RKM) with Xn1 and an unbiased proximal map satisfying (15), then all estimates for the cluster centers remain bounded by a constant that depends only on the unmodified samples of Xn.\nNext, we show that, even for this class of maps, there always exists a dataset that causes one of the estimated centers to breakdown as two particular observations are suitably replaced.\nTheorem 1 (Universal breakdown point for (RKM)). Assume k ≥ 2 and n ≥ r ≥ k + 2. Given an unbiased proximal map satisfying (15), there exist a dataset Xn and a modification Xn2 , such that (RKM) breaks down.\nHence, the universal breakdown point of (RKM) with an unbiased proximal map is 2n . In Figure 1, we give a visual interpretation of Theorem 1. The top subfigure depicts the unmodified initial dataset X 9 = {x1, . . . , x9} (black circles) with a clear two-cluster structure; the bottom subfigure shows the modification X 92 (dashed line arrows). Theorem 1 states that (RKM) on X 92 fails to be robust since, every subset of X 92 with r = 8 points has a cluster containing an outlier.\n3.3 Restricted robustness of robust k-means for well-clustered data\nThe result of Theorem 1 is disappointing but it is not (RKM) to be blamed for the poor performance but the tight notion of the definition about the breakdown point [6, 7]; allowing any kind of contamination in a dataset is a very general assumption.\nIn this section, we place two restrictions: i) we consider datasets where inlier samples can be covered by unions of balls with centers that are “far apart” each other, and ii) we ask a question different from the finite sample breakdown point. We want to exploit as much as possible the results of [2] concerning a new quantitative measure of noise robustness which compares the output of (RKM) on a contaminated dataset to its output on the uncontaminated version of the dataset. Our aim is to show that (RKM), with a certain class of proximal maps and datasets that are well-structured ignores the influence of outliers when grouping the inliers.\nFirst, we introduce Corollary 2 which states the form that Pfλ should have in order the results of [2] to apply to (RKM) and, second, we give details about the datasets which we consider as wellstructured. Using this corollary we are able to design proximal maps for which Theorems 3, 4, and 5 in [2] apply; otherwise, it is not clear how the analysis of [2] is valid for (RKM).\nLet h : R→ R be a continuous function with the following properties:\n1. h is odd and non-decreasing (h+(·) is used to denote its restriction on [0,∞));\n2. h is a shrinkage rule: 0 ≤ h+(x) ≤ x, ∀x ∈ [0,∞);\n3. the difference x− h+(x) is non-decreasing, i.e., for 0 ≤ x1 ≤ x2 we have x1 − h+(x1) ≤ x2 − h+(x2).\nDefine the map\nPfλ(x) :=\n⎧ ⎨\n⎩ h(x), |x| < λ, {h(x), x}, |x| = λ, x, |x| > λ.\n(16)\nMultivaluedness of Pfλ at |x| = λ signals that efλ is non-smooth at these points. An immediate consequence for the Moreau envelope associated with the previous map is the following. Corollary 2. Let the function g : [0,∞)→ [0,∞) be defined as\ng(x) :=\n∫ x\n0 (u− h(u))du, x ∈ [0,∞). (17)\nThen, the Moreau envelope associated with Pfλ in (16) is\nefλ(x) = min{g(|x|), g(λ)} = g(min{|x|,λ}). (18)\nNext, we define what it means for a dataset to be (ρ1, ρ2)-balanced; this is the class of datasets which we consider to be well-structured. Definition 2 ((ρ1, ρ2) balanced dataset [2]). Assume that a set Xn ⊂ Rp has a subset I (inliers), with at least n2 samples, and the following properties:\n1. I = ⋃k\nl=1 Bl, where Bl = B(bl, r) is a ball in Rp with bounded radius r and center bl;\n2. ρ1|I| ≤ |Bl| ≤ ρ2|I| for every l, where |Bl| is the number of samples in Bl and ρ1,ρ2 > 0;\n3. ||bl − bl′ ||2 > v for every l ̸= l′, i.e., the centers of the balls are at least v > 0 apart.\nThen, Xn is a (ρ1, ρ2)-balanced dataset.\nWe now state the form that Theorem 3 in [2] takes for (RKM). Theorem 2 (Restricted robustness of (RKM)). If i) efλ is as in Corollary 2, i.e., efλ(||x||2) = g(min{||x||2,λ}), ii) Xn has a (ρ1, ρ2)-balanced subset of samples I with k balls, and iii) the centers of the balls are at least v > 4r + 2g−1(ρ1+ρ2ρ1 g(r)) apart, then for λ ∈[\nv 2 , g\n−1 (\n|I| |Xn\\I| (ρ1g( v 2 − 2r)− (ρ1 + ρ2)g(r))\n)) the set of outliers Xn\\I has no effect on the\ngrouping of inliers I. In other words, if {x, y} ∈ Bl and {c1, . . . , ck} are the optimal centers when solving (RKM) for a λ as described before, then\nl = argmin1≤j≤kefλ(||x− cj ||2) = argmin1≤j≤kefλ(||y − cj ||2).\nFor the sake of completeness, we give a proof of this theorem in the appendix. In a similar way, we can recast the results of Theorems 4 and 5 in [2] to be valid for (RKM).\n4 On the consistency of robust k-means\nLet Xn be a set with n independent and identically distributed random samples xi from a fixed but unknown probability distribution µ. Let Ĉ be the empirical optimal set of centers, i.e.,\nĈ := argminc1...,ck∈RpR′n(c1, . . . , ck). (19) The population optimal set of centers is the set\nC∗ := argminc1...,ck∈RpR ′(c1, . . . , ck), (20)\nwhere R′ is the population clustering risk, defined as\nR′(c1, . . . , ck) := ∫ min 1≤l≤k { min o∈Rp 1 2 ||x− cl − o||22 + fλ(||o||2)\n︸ ︷︷ ︸ φ(||x−cl||2)=efλ (||x−cl||2)\n} µ(dx). (21)\nLoss consistency and (simply) consistency for (RKM) require, respectively, that\nR′n(Ĉ) n→∞−→ R′(C∗) and Ĉ n→∞−→ C∗. (22)\nIn words, as the size n of the dataset Xn increases, the empirical clustering risk R′n(Ĉ) converges almost surely to the minimum population risk R′(C∗) and (for n large enough) Ĉ can effectively replace the optimal set C∗ in quantizing the unknown probability measure µ. For the case of convex fλ, non-asymptotic results describing the rate of convergence of R′n to R in (22) are already known ([11], Theorem 3). Noting that the Moreau envelope of a non-convex fλ belongs to a class of functions with polynomial discrimination [16] (the shatter coefficient of this class is bounded by a polynomial) we give a sketch proof of the following result.\nTheorem 3 (Consistency of (RKM)). Let the samples xi ∈ Xn, i ∈ {1, . . . , n}, come from a fixed but unknown probability measure µ. For any k ≥ 1 and any unbiased proximal map, we have\nlim n→∞ ER′(Ĉ)→ R′(C∗) and lim n→∞ Ĉ → C∗ (convergence in probability). (23)\nTheorem 3 reads like an asymptotic convergence result. However, its proof (given in the appendix) uses combinatorial tools from Vapnik-Chervonenkis theory, revealing that the non-asymptotic rate of convergence of ER′(Ĉ) to R′(C∗) is of order O( √ log n/n) (see Corollary 12.1 in [4]).\n5 Relating (RKM) to trimmed k-means\nAs the effectiveness of robust k-means on real world and synthetic data has already been evaluated [5, 24], the purpose of this section is to relate (RKM) to trimmed k-means (TKM) [7]. Trimmed kmeans is based on the methodology of “impartial trimming”, which is a combinatorial problem fundamentally different from (RKM). Despite their differences, the experiments show that, both (RKM) and (TKM) perform remarkably similar in practice. The solution of (TKM) (which is also a set of k centers) is the solution of quadratic k-means on the subsample containing ⌈n(1 − α)⌉ points with the smallest mean deviation (0 < α < 1). The only common characteristic of (RKM) and (TKM) is that they both have the same universal breakdown point, i.e., 2n , for arbitrary datasets.\nTrimmed k-means takes as input a dataset Xn, the number of clusters k, and a proportion of outliers a ∈ (0, 1) to remove.6 A popular heuristic algorithm for (TKM) is the following. After the initialization, each iteration of (TKM) consists of the following steps: i) the distance of each observation from its closest center is computed, ii) the top ⌈an⌉ observations with larger distance from its closest center are removed, iii) the remaining points are used to update the centers. The previous three steps are repeated untill the centers converge.7 As for robust k-means, we solve the (RKM) problem with a coordinate optimization procedure (see Appendix A.9 for details).\nThe synthetic data for the experiments come from a mixture of Gaussians with 10 components and without any overlap between them.8 The number of inlier samples is 500 and each inlier xi ∈ [−1, 1]10 for i ∈ {1, . . . , 500}. On top of the inliers lie 150 outliers in R10 distributed uniformly in general positions over the entire space. We consider two scenarios: in the first, the outliers lie in [−3, 3]10 (call it mild-contamination), while, in the second, the outliers lie in [−6, 6]10 (call it heavy-contamination). The parameter a in trimmed k-means (the percentage of outliers) is set to a = 0.3, while the value of the parameter λ for which (RKM) yields 150 outliers is found through a search over a grid on the set λ ∈ (0,λmax) (we set λmax as the maximum distance between two points in a dataset). Both algorithms, as they are designed, require as input an initial set of k points; these points form the initial set of centers. In all experiments, both (RKM) and (TKM) take the same k vectors as initial centers, i.e., k points sampled randomly from the dataset.\nThe statistics we use for the comparison are: i) the rand-index for clustering accuracy [17] ii) the cluster estimation error, i.e., the root mean square error between the estimated cluster centers and the sample mean of each cluster, iii) the true positive outlier detection rate, and finally, iv) the false positive outlier detection rate. In Figures 2-3, we plot the results for a proximal map Pf like the one in (16) with h(x) = αx and α = 0.005; with this choice for h, we mimic the hard-thresholding operator. The results for each scenario (accuracy, cluster estimation error, etc) are averages over 150 runs of the experiment. As seen, both algorithms share almost the same statistics in all cases.\n6We use the implementation of trimmed k-means in the R package trimcluster [10]. 7The previous three steps are performed also by another robust variant of k-means, the k-means− (see [3]). 8We use the R toolbox MixSim [14] that guarantees no overlap among the 10 mixtures.\nIn Figure 4, we plot the results for the case of two spherical clusters in R10 with equal radius r, each one with 150 samples, and centers that are at least 4r apart from each other. The inlier samples are in [−3, 3]10. The outliers are 150 (half of the dataset is contaminated) and are uniformly distributed in [−6, 6]10. The results (accuracy, cluster estimation error, etc) are averages over 150 runs of the experiment. This configuration is a heavy contamination scenario but, due to the structure of the dataset, as expected from Theorem 2, (RKM) performs remarkably well; the same holds for (TKM)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We provided a theoretical analysis for the robustness and consistency properties of a variation of the classical quadratic k-means called robust k-means (RKM). As a by-product of the analysis, we derived a detailed description of the optimality conditions for the associated minimization problem. In most cases, (RKM) shares the computational simplicity of quadratic k-means, making it a “computationally cheap” candidate for robust nearest neighbor clustering. We show that (RKM) cannot be robust against any type of contamination and any type of datasets, no matter the form of the proximal map we use. If we restrict our attention to “well-structured” datasets, then the algorithm exhibits some desirable noise robustness. As for the consistency properties, we showed that most general results for consistency of quadratic k-means still remain valid for this robust variant."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The author would like to thank Athanasios P. Liavas for useful comments and suggestions that improved the quality of the article."
    } ],
    "references" : [ {
      "title" : "Regularization of wavelet approximations",
      "author" : [ "Anestis Antoniadis", "Jianqing Fan" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Clustering in the presence of background noise",
      "author" : [ "Shai Ben-David", "Nika Haghtalab" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "A Probabilistic Theory of Pattern Recognition. Stochastic Modelling and Applied Probability",
      "author" : [ "L. Devroye", "L. Györfi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Robust clustering using outlier-sparsity regularization",
      "author" : [ "Pedro A Forero", "Vassilis Kekatos", "Georgios B Giannakis" ],
      "venue" : "Signal Processing, IEEE Transactions on,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "A robust method for cluster analysis",
      "author" : [ "Marı́a Teresa Gallegos", "Gunter Ritter" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Robustness properties of k-means and trimmed k-means",
      "author" : [ "Luis Ángel Garcı́a-Escudero", "Alfonso Gordaliza" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1999
    }, {
      "title" : "Computers and Intractability: A Guide to the Theory of NP- Completeness",
      "author" : [ "Michael R. Garey", "David S. Johnson" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1979
    }, {
      "title" : "Robust statistics: the approach based on influence functions, volume 114",
      "author" : [ "Frank R Hampel", "Elvezio M Ronchetti", "Peter J Rousseeuw", "Werner A Stahel" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "trimcluster: Cluster analysis with trimming, 2012. R package version 0.1-2",
      "author" : [ "Christian Hennig" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Learning-theoretic methods in vector quantization",
      "author" : [ "Tamás Linder" ],
      "venue" : "In Principles of nonparametric learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Least squares quantization in pcm",
      "author" : [ "Stuart P Lloyd" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1982
    }, {
      "title" : "Sparsenet: Coordinate descent with nonconvex penalties",
      "author" : [ "Rahul Mazumder", "Jerome H Friedman", "Trevor Hastie" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "MixSim: An R package for simulating data to study performance of clustering algorithms",
      "author" : [ "Volodymyr Melnykov", "Wei-Chen Chen", "Ranjan Maitra" ],
      "venue" : "Journal of Statistical Software,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Strong consistency of k-means clustering",
      "author" : [ "David Pollard" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1981
    }, {
      "title" : "Convergence of stochastic processes",
      "author" : [ "David Pollard" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1984
    }, {
      "title" : "Objective criteria for the evaluation of clustering methods",
      "author" : [ "William M Rand" ],
      "venue" : "Journal of the American Statistical association,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1971
    }, {
      "title" : "Robust Cluster Analysis and Variable Selection",
      "author" : [ "G. Ritter" ],
      "venue" : "Chapman & Hall/CRC Monographs on Statistics & Applied Probability. CRC Press,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Variational analysis, volume 317",
      "author" : [ "R Tyrrell Rockafellar", "Roger J-B Wets" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Thresholding-based iterative selection procedures for model selection and shrinkage",
      "author" : [ "Yiyuan She" ],
      "venue" : "Electronic Journal of statistics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "A unified continuous optimization framework for center-based clustering methods",
      "author" : [ "Marc Teboulle" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Convergence of a block coordinate descent method for nondifferentiable minimization",
      "author" : [ "Paul Tseng" ],
      "venue" : "Journal of optimization theory and applications,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2001
    }, {
      "title" : "Empirical processes in m-estimation",
      "author" : [ "Sara Van De Geer" ],
      "venue" : "June 13,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "Penalized unsupervised learning with outliers",
      "author" : [ "Daniela M Witten" ],
      "venue" : "Statistics and its Interface,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Coordinate descent algorithms",
      "author" : [ "Stephen J Wright" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Minimizing nonconvex nonseparable functions",
      "author" : [ "Yaoliang Yu", "Xun Zheng", "Micol Marchetti-Bowick", "Eric P Xing" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : ", xn} ⊂ Rp, consider the generalized k-means problem (GKM) [7] min c1,.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "Varying φ beyond the usual quadratic function (φ(t) = t2) we expect to gain some robustness against the outliers [9].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "1 1 For a similar definition for the set of clusters induced by a bounded φ see also Section 4 in [2].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "The problem (RKM) [5, 24] describes the following simple model: we allow each observation xi to take on an “error” term oi and we penalize the errors, using a group penalty, in order to encourage most of the observations’ errors to be equal to zero.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 22,
      "context" : "The problem (RKM) [5, 24] describes the following simple model: we allow each observation xi to take on an “error” term oi and we penalize the errors, using a group penalty, in order to encourage most of the observations’ errors to be equal to zero.",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "Here, we focus on the finite sample breakdown point [18], which counts how many outliers a dataset may contain without causing significant damage in the estimates of the centers.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "Consistency: Much is known about the consistency of (GKM) when the function φ is lsc and increasing [11, 15].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "Consistency: Much is known about the consistency of (GKM) when the function φ is lsc and increasing [11, 15].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "2 Preliminaries and some technical remarks We start our analysis with a few technical tools from variational analysis [19].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "22 in [19]) of an lsc, proper, and bounded from below function f : Rp → R and the associated (possibly multivalued) proximal map P f : Rp →→ Rp are eμf (x) = min z∈Rp 1 2μ ||x− z||(2)2 + f(z) and P μ f (x) = argminz∈Rp 1 2μ ||x− z||(2)2 + f(z), (4) respectively.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 24,
      "context" : "We know that for any such map there exists at least one function fλ such that P = Pfλ (Proposition 3 in [26]).",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "3 Finally, for our purposes (outlier detection), it is natural (2)We call f proper if f(x) < ∞ for at least one x ∈ R, and f(x) > −∞ for all x ∈ R; in words, if the domain of f is a nonempty set on which f is finite (see page 5 in [19]).",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 24,
      "context" : ", φ(·) = efλ(·) as defined in (5a) for some function fλ, we require that φ(·)− 12 | · | 2 is a concave function (Proposition 1 in [26]).",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "However, it is common in practice that the sets ∂̂Φ(x) and ∂Φ(x) are different (for a detailed exposition on subgradients see Chapter 8 in [19]; see also Example 1 in Appendix A.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 24,
      "context" : "There exist cases where two different functions, fλ ̸= f ′ λ, have equal Moreau envelopes, efλ = ef ′ λ (Proposition 1 in [26]), implying that two different forms of (RKM) correspond to the same φ in (GKM).",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "44 in [19], Proposition 2 and Example 3 in [26]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 24,
      "context" : "44 in [19], Proposition 2 and Example 3 in [26]).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : ", xn} and a nonnegative integer m ≤ n, we say that Xn m is an m-modification if it arises from Xn after replacing m of its elements by arbitrary elements xi ∈ Rp [6].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "Definition 1 (universal breakdown point for the centers [6]).",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "1 Biased proximal maps: the case of convex fλ If fλ is convex, then Φ = eFλ is also convex while PFλ is continuous, single-valued, and satisfies [19] ||x− PFλ(x)||2 →∞ as ||x||2 →∞.",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "(11) The supremum value of ||∇Φ(x− cl)||2 is closely related to the gross error sensitivity of an estimator [9].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "It is interpreted as the worst possible influence which a sample x can have on cl [7].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "5 The l1-norm, fλ(x) = λ|x|, which has Moreau envelope equal to the Huber loss-function [24], is the limiting case for the class of convex penalty functions that, although it keeps the difference ||x − PFλ(x)||2 in (10) constant and equal to λ, introduces a bias term proportional to λ in the estimate cl.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "This is done since Pλ|·|0(xi−cl) = xi−cl whenever |xi−cl| > λ (5)See the analysis in [7] about the influence function of (GKM) when φ is convex.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "These are called unbiased proximal maps [13, 20] and have the useful property that, as one observation is arbitrarily modified, all estimated cluster centers remain bounded by a constant that depends only on the remaining unmodified samples.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "These are called unbiased proximal maps [13, 20] and have the useful property that, as one observation is arbitrarily modified, all estimated cluster centers remain bounded by a constant that depends only on the remaining unmodified samples.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "3 Restricted robustness of robust k-means for well-clustered data The result of Theorem 1 is disappointing but it is not (RKM) to be blamed for the poor performance but the tight notion of the definition about the breakdown point [6, 7]; allowing any kind of contamination in a dataset is a very general assumption.",
      "startOffset" : 230,
      "endOffset" : 236
    }, {
      "referenceID" : 5,
      "context" : "3 Restricted robustness of robust k-means for well-clustered data The result of Theorem 1 is disappointing but it is not (RKM) to be blamed for the poor performance but the tight notion of the definition about the breakdown point [6, 7]; allowing any kind of contamination in a dataset is a very general assumption.",
      "startOffset" : 230,
      "endOffset" : 236
    }, {
      "referenceID" : 1,
      "context" : "We want to exploit as much as possible the results of [2] concerning a new quantitative measure of noise robustness which compares the output of (RKM) on a contaminated dataset to its output on the uncontaminated version of the dataset.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "First, we introduce Corollary 2 which states the form that Pfλ should have in order the results of [2] to apply to (RKM) and, second, we give details about the datasets which we consider as wellstructured.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "Using this corollary we are able to design proximal maps for which Theorems 3, 4, and 5 in [2] apply; otherwise, it is not clear how the analysis of [2] is valid for (RKM).",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "Using this corollary we are able to design proximal maps for which Theorems 3, 4, and 5 in [2] apply; otherwise, it is not clear how the analysis of [2] is valid for (RKM).",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "Definition 2 ((ρ1, ρ2) balanced dataset [2]).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "We now state the form that Theorem 3 in [2] takes for (RKM).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "In a similar way, we can recast the results of Theorems 4 and 5 in [2] to be valid for (RKM).",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "For the case of convex fλ, non-asymptotic results describing the rate of convergence of R′ n to R in (22) are already known ([11], Theorem 3).",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "Noting that the Moreau envelope of a non-convex fλ belongs to a class of functions with polynomial discrimination [16] (the shatter coefficient of this class is bounded by a polynomial) we give a sketch proof of the following result.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "5 Relating (RKM) to trimmed k-means As the effectiveness of robust k-means on real world and synthetic data has already been evaluated [5, 24], the purpose of this section is to relate (RKM) to trimmed k-means (TKM) [7].",
      "startOffset" : 135,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "5 Relating (RKM) to trimmed k-means As the effectiveness of robust k-means on real world and synthetic data has already been evaluated [5, 24], the purpose of this section is to relate (RKM) to trimmed k-means (TKM) [7].",
      "startOffset" : 135,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "5 Relating (RKM) to trimmed k-means As the effectiveness of robust k-means on real world and synthetic data has already been evaluated [5, 24], the purpose of this section is to relate (RKM) to trimmed k-means (TKM) [7].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 15,
      "context" : "The statistics we use for the comparison are: i) the rand-index for clustering accuracy [17] ii) the cluster estimation error, i.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : "(6)We use the implementation of trimmed k-means in the R package trimcluster [10].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 12,
      "context" : "(8)We use the R toolbox MixSim [14] that guarantees no overlap among the 10 mixtures.",
      "startOffset" : 31,
      "endOffset" : 35
    } ],
    "year" : 2016,
    "abstractText" : "Over the last years, many variations of the quadratic k-means clustering procedure have been proposed, all aiming to robustify the performance of the algorithm in the presence of outliers. In general terms, two main approaches have been developed: one based on penalized regularization methods, and one based on trimming functions. In this work, we present a theoretical analysis of the robustness and consistency properties of a variant of the classical quadratic k-means algorithm, the robust k-means, which borrows ideas from outlier detection in regression. We show that two outliers in a dataset are enough to breakdown this clustering procedure. However, if we focus on “well-structured” datasets, then robust k-means can recover the underlying cluster structure in spite of the outliers. Finally, we show that, with slight modifications, the most general non-asymptotic results for consistency of quadratic k-means remain valid for this robust variant.",
    "creator" : null
  }
}