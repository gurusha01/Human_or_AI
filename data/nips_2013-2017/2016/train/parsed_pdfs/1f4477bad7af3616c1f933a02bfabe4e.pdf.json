{
  "name" : "1f4477bad7af3616c1f933a02bfabe4e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods",
    "authors" : [ "A. Gautier", "Q. Nguyen" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing. While the theoretical foundations of neural networks have been explored in depth see e.g. [1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9]. On the other hand the parameter search for stochastic gradient descent and variants such as Adagrad and Adam can be quite tedious and there is no guarantee that one converges to the global optimum. In particular, the problem is even for a single hidden layer in general NP hard, see [17] and references therein. This implies that to achieve global optimality efficiently one has to impose certain conditions on the problem. A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11]. The latter two papers are up to our knowledge the first results which provide a globally optimal algorithm for training neural networks. While providing a lot of interesting insights on the relationship of structured matrix factorization and training of neural networks, Haeffele and Vidal admit themselves in their paper [8] that their results are “challenging to apply in practice”. In the work of Janzamin et al. [11] they use a tensor approach and propose a globally optimal algorithm for a feedforward neural network with one hidden layer and squared loss. However, their approach requires the computation of the score function tensor which uses the density of the data-generating measure. However, the data generating measure is unknown and also difficult to estimate for high-dimensional feature spaces. Moreover, one has to check certain non-degeneracy conditions of the tensor decomposition to get the global optimality guarantee.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nIn contrast our nonlinear spectral method just requires that the data is nonnegative which is true for all sorts of count data such as images, word frequencies etc. The condition which guarantees global optimality just depends on the parameters of the architecture of the network and boils down to the computation of the spectral radius of a small nonnegative matrix. The condition can be checked without running the algorithm. Moreover, the nonlinear spectral method has a linear convergence rate and thus the globally optimal training of the network is very fast. The two main changes compared to the standard setting are that we require nonnegativity on the weights of the network and we have to minimize a modified objective function which is the sum of loss and the negative total sum of the outputs. While this model is non-standard, we show in some first experimental results that the resulting classifier is still expressive enough to create complex decision boundaries. As well, we achieve competitive performance on some UCI datasets. As the nonlinear spectral method requires some non-standard techniques, we use the main part of the paper to develop the key steps necessary for the proof. However, some proofs of the intermediate results are moved to the supplementary material."
    }, {
      "heading" : "2 Main result",
      "text" : "0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nIn this section we present the algorithm together with the main theorem providing the convergence guarantee. We limit the presentation to one hidden layer networks to improve the readability of the paper. Our approach can be generalized to feedforward networks of arbitrary depth. In particular, we present in Section 4.1 results for two hidden layers. We consider in this paper multi-class classification where d is the dimension of the feature space and K is the number of classes. We use the negative cross-entropy loss defined for label y ∈ [K] := {1, . . . ,K} and classifier f : Rd → RK as\n( )\nThe function class we are using is a feedforward neural network with one hidden layer with n1 hidden units. As activation functions we use real powers of the form of a generalized polyomial, that is for α ∈ Rn1 with αi ≥ 1, i ∈ [K], we define:\nfr(x) = fr(w, u)(x) = n1∑ l=1 wrl ( d∑ m=1 ulmxm )αl , (1)\nwhere R+ = {x ∈ R |x ≥ 0} and w ∈ RK×n1+ , u ∈ R n1×d + are the parameters of the network which we optimize. The function class in (1) can be seen as a generalized polynomial in the sense that the powers do not have to be integers. Polynomial neural networks have been recently analyzed in [15]. Please note that a ReLU activation function makes no sense in our setting as we require the data as well as the weights to be nonnegative. Even though nonnegativity of the weights is a strong constraint, one can model quite complex decision boundaries (see Figure 1, where we show the outcome of our method for a toy dataset in R2). In order to simplify the notation we use w = (w1, . . . , wK) for the K output units wi ∈ Rn1+ , i = 1, . . . ,K. All output units and the hidden layer are normalized. We optimize over the set\nS+ = { (w, u) ∈ RK×n1+ × R n1×d + ∣∣ ‖u‖pu = ρu, ‖wi‖pw = ρw, ∀i = 1, . . . ,K}. We also introduce S++ where one replaces R+ with R++ = {t ∈ R | t > 0}. The final optimization problem we are going to solve is given as\nmax (w,u)∈S+ Φ(w, u) with (2)\nΦ(w, u) = 1 n n∑ i=1 [ − L ( yi, f(w, u)(xi) ) + K∑ r=1 fr(w, u)(xi) ] + ( K∑ r=1 n1∑ l=1 wr,l + n1∑ l=1 d∑ m=1 ulm ) ,\nwhere (xi, yi) ∈ Rd+ × [K], i = 1, . . . , n is the training data. Note that this is a maximization problem and thus we use minus the loss in the objective so that we are effectively minimizing the loss. The reason to write this as a maximization problem is that our nonlinear spectral method is inspired by the theory of (sub)-homogeneous nonlinear eigenproblems on convex cones [14] which has its origin in the Perron-Frobenius theory for nonnegative matrices. In fact our work is motivated by the closely related Perron-Frobenius theory for multihomogeneous problems developed in [7]. This is also the reason why we have nonnegative weights, as we work on the positive orthant which is a convex cone. Note that > 0 in the objective can be chosen arbitrarily small and is added out of technical reasons. In order to state our main theorem we need some additional notation. For p ∈ (1,∞), we let p′ = p/(p− 1) be the Hölder conjugate of p, and ψp(x) = sign(x)|x|p−1. We apply ψp to scalars and vectors in which case the function is applied componentwise. For a square matrix A we denote its spectral radius by ρ(A). Finally, we write ∇wiΦ(w, u) (resp. ∇uΦ(w, u)) to denote the gradient of Φ with respect to wi (resp. u) at (w, u). The mapping\nGΦ(w, u) = ( ρwψp′w(∇w1Φ(w, u)) ‖ψp′w(∇w1Φ(w, u))‖pw , . . . , ρwψp′w(∇wKΦ(w, u)) ‖ψp′w(∇wKΦ(w, u))‖pw , ρuψp′u(∇uΦ(w, u)) ‖ψp′u(∇uΦ(w, u))‖pu ) ,\n(3) defines a sequence converging to the global optimum of (2). Indeed, we prove: Theorem 1. Let {xi, yi}ni=1 ⊂ Rd+ × [K], pw, pu ∈ (1,∞), ρw, ρu > 0, n1 ∈ N and α ∈ Rn1 with αi ≥ 1 for every i ∈ [n1]. Define ρx, ξ1, ξ2 > 0 as ρx = maxi∈[n] ‖xi‖1, ξ1 = ρw ∑n1 l=1(ρuρx)αl , ξ2 = ρw ∑n1 l=1 αl(ρuρx)αl and let A ∈ R (K+1)×(K+1) ++ be defined as\nAl,m = 4(p′w − 1)ξ1, Al,K+1 = 2(p′w − 1)(2ξ2 + ‖α‖∞), AK+1,m = 2(p′u − 1)(2ξ1 + 1), AK+1,K+1 = 2(p′u − 1)(2ξ2 + ‖α‖∞ − 1),\n∀m, l ∈ [K].\nIf the spectral radius ρ(A) of A satisfies ρ(A) < 1, then (2) has a unique global maximizer (w∗, u∗) ∈ S++. Moreover, for every (w0, u0) ∈ S++, there exists R > 0 such that\nlim k→∞ (wk, uk) = (w∗, u∗) and ‖(wk, uk)− (w∗, u∗)‖∞ ≤ Rρ(A)k ∀k ∈ N,\nwhere (wk+1, uk+1) = GΦ(wk, uk) for every k ∈ N.\nNote that one can check for a given model (number of hidden units n1, choice of α, pw, pu, ρu, ρw) easily if the convergence guarantee to the global optimum holds by computing the spectral radius of a square matrix of size K + 1. As our bounds for the matrix A are very conservative, the “effective” spectral radius is typically much smaller, so that we have very fast convergence in only a few iterations, see Section 5 for a discussion. Up to our knowledge this is the first practically feasible algorithm to achieve global optimality for a non-trivial neural network model. Additionally, compared to stochastic gradient descent, there is no free parameter in the algorithm. Thus no careful tuning of the learning rate is required. The reader might wonder why we add the second term in the objective, where we sum over all outputs. The reason is that we need that the gradient of GΦ is strictly positive in S+, this is why we also have to add the third term for arbitrarily small > 0. In Section 5 we show that this model achieves competitive results on a few UCI datasets.\nChoice of α: It turns out that in order to get a non-trivial classifier one has to choose α1, . . . , αn1 ≥ 1 so that αi 6= αj for every i, j ∈ [n1] with i 6= j. The reason for this lies in certain invariance properties of the network. Suppose that we use a permutation invariant componentwise activation function σ, that is σ(Px) = Pσ(x) for any permutation matrix P and suppose that A,B are globally optimal weight matrices for a one hidden layer architecture, then for any permutation matrix P ,\nAσ(Bx) = APTPσ(Bx) = APTσ(PBx), which implies that A′ = APT and B′ = PB yield the same function and thus are also globally optimal. In our setting we know that the global optimum is unique and thus it has to hold that, A = APT and B = PB for all permutation matrices P . This implies that both A and B have rank one and thus lead to trivial classifiers. This is the reason why one has to use different α for every unit.\nDependence of ρ(A) on the model parameters: Let Q, Q̃ ∈ Rm×m+ and assume 0 ≤ Qi,j ≤ Q̃i,j for every i, j ∈ [m], then ρ(Q) ≤ ρ(Q̃), see Corollary 3.30 [3]. It follows that ρ(A) in Theorem 1 is increasing w.r.t. ρu, ρw, ρx and the number of hidden units n1. Moreover, ρ(A) is decreasing w.r.t. pu, pw and in particular, we note that for any fixed architecture (n1, α, ρu, ρw) it is always possible to find pu, pw large enough so that ρ(A) < 1. Indeed, we know from the Collatz-Wielandt formula (Theorem 8.1.26 in [10]) that ρ(A) = ρ(AT ) ≤ maxi∈[K+1](AT v)i/vi for any v ∈ RK+1++ . We use this to derive lower bounds on pu, pw that ensure ρ(A) < 1. Let v = (pw − 1, . . . , pw − 1, pu − 1), then (AT v)i < vi for every i ∈ [K + 1] guarantees ρ(A) < 1 and is equivalent to\npw > 4(K + 1)ξ1 + 3 and pu > 2(K + 1)(‖α‖∞ + 2ξ2)− 1, (4) where ξ1, ξ2 are defined as in Theorem 1. However, we think that our current bounds are sub-optimal so that this choice is quite conservative. Finally, we note that the constant R in Theorem 1 can be explicitly computed when running the algorithm (see Theorem 3).\nProof Strategy: The following main part of the paper is devoted to the proof of the algorithm. For that we need some further notation. We introduce the sets\nV+ = RK×n1+ × R n1×d + , V++ = R K×n1 ++ × R n1×d ++ B+ = { (w, u) ∈ V+ ∣∣ ‖u‖pu ≤ ρu, ‖wi‖pw ≤ ρw, ∀i = 1, . . . ,K},\nand similarly we define B++ replacing V+ by V++ in the definition. The high-level idea of the proof is that we first show that the global maximum of our optimization problem in (2) is attained in the “interior” of S+, that is S++. Moreover, we prove that any critical point of (2) in S++ is a fixed point of the mapping GΦ. Then we proceed to show that there exists a unique fixed point of GΦ in S++ and thus there is a unique critical point of (2) in S++. As the global maximizer of (2) exists and is attained in the interior, this fixed point has to be the global maximizer. Finally, the proof of the fact that GΦ has a unique fixed point follows by noting that GΦ maps B++ into B++ and the fact that B++ is a complete metric space with respect to the Thompson metric. We provide a characterization of the Lipschitz constant of GΦ and in turn derive conditions under which GΦ is a contraction. Finally, the application of the Banach fixed point theorem yields the uniqueness of the fixed point of GΦ and the linear convergence rate to the global optimum of (2). In Section 4 we show the application of the established framework for our neural networks."
    }, {
      "heading" : "3 From the optimization problem to fixed point theory",
      "text" : "Lemma 1. Let Φ : V → R be differentiable. If ∇Φ(w, u) ∈ V++ for every (w, u) ∈ S+, then the global maximum of Φ on S+ is attained in S++.\nWe now identify critical points of the objective Φ in S++ with fixed points of GΦ in S++. Lemma 2. Let Φ : V → R be differentiable. If ∇Φ(w, u) ∈ V++ for all (w, u) ∈ S++, then (w∗, u∗) is a critical point of Φ in S++ if and only if it is a fixed point of GΦ.\nOur goal is to apply the Banach fixed point theorem to GΦ : B++ → S++ ⊂ B++. We recall this theorem for the convenience of the reader. Theorem 2 (Banach fixed point theorem e.g. [12]). Let (X, d) be a complete metric space with a mapping T : X → X such that d(T (x), T (y)) ≤ q d(x, y) for q ∈ [0, 1) and all x, y ∈ X. Then T has a unique fixed-point x∗ in X, that is T (x∗) = x∗ and the sequence defined as xn+1 = T (xn) with x0 ∈ X converges limn→∞ xn = x∗ with linear convergence rate\nd(xn, x∗) ≤ q n\n1− q d(x 1, x0).\nSo, we need to endow B++ with a metric µ so that (B++, µ) is a complete metric space. A popular metric for the study of nonlinear eigenvalue problems on the positive orthant is the so-called Thompson metric d : Rm++ × Rm++ → R+ [18] defined as\nd(z, z̃) = ‖ ln(z)− ln(z̃)‖∞ where ln(z) = ( ln(z1), . . . , ln(zm) ) .\nUsing the known facts that (Rn++, d) is a complete metric space and its topology coincides with the norm topology (see e.g. Corollary 2.5.6 and Proposition 2.5.2 [14]), we prove: Lemma 3. For p ∈ (1,∞) and ρ > 0, ({z ∈ Rn++ | ‖z‖p ≤ ρ}, d) is a complete metric space.\nNow, the idea is to see B++ as a product of such metric spaces. For i = 1, . . . ,K, let Bi++ = {wi ∈ R n1 ++ | ‖wi‖pw ≤ ρw} and di(wi, w̃i) = γi‖ ln(wi) − ln(w̃i)‖∞ for some constant γi > 0. Furthermore, let BK+1++ = {u ∈ R n1×d ++ | ‖u‖pu ≤ ρu} and dK+1(u, ũ) = γK+1‖ ln(u)− ln(ũ)‖∞. Then (Bi++, di) is a complete metric space for every i ∈ [K + 1] and B++ = B1++ × . . .×BK++ ×BK+1++ . It follows that (B++, µ) is a complete metric space with µ : B++ ×B++ → R+ defined as\nµ ( (w, u), (w̃, ũ) ) = K∑ i=1 γi‖ ln(wi)− ln(w̃i)‖∞ + γK+1‖ ln(u)− ln(ũ)‖∞.\nThe motivation for introducing the weights γ1, . . . , γK+1 > 0 is given by the next theorem. We provide a characterization of the Lipschitz constant of a mapping F : B++ → B++ with respect to µ. Moreover, this Lipschitz constant can be minimized by a smart choice of γ. For i ∈ [K], a, j ∈ [n1], b ∈ [d], we write Fwi,j and Fuab to denote the components of F such that F = (Fw1,1 , . . . , Fw1,n1 , Fw2,1 , . . . , FwK,n1 , Fu11 , . . . , Fun1d).\nLemma 4. Suppose that F ∈ C1(B++, V++) and A ∈ R(K+1)×(K+1)+ satisfies〈 |∇wkFwi,j (w, u)|, wk 〉 ≤ Ai,k Fwi,j (w, u), 〈 |∇uFwi,j (w, u)|, u 〉 ≤ Ai,K+1 Fwi,j (w, u)\nand 〈|∇wkFuab(w, u)|, wk〉 ≤ AK+1,k Fuab(w, u), 〈|∇uFuab(w, u)|, u〉 ≤ AK+1,K+1 Fuab(w, u)\nfor all i, k ∈ [K], a, j ∈ [n1], b ∈ [d] and (w, u) ∈ B++. Then, for every (w, u), (w̃, ũ) ∈ B++ it holds\nµ ( F (w, u), F (w̃, ũ) ) ≤ U µ ( (w, u), (w̃, ũ) ) with U = max\nk∈[K+1] (AT γ)k γk .\nNote that, from the Collatz-Wielandt ratio for nonnegative matrices, we know that the constant U in Lemma 4 is lower bounded by the spectral radius ρ(A) of A. Indeed, by Theorem 8.1.31 in [10], we know that if AT has a positive eigenvector γ ∈ RK+1++ , then\nmax i∈[K+1] (AT γ)i γi = ρ(A) = min γ̃∈RK+1++ max i∈[K+1] (AT γ̃)i γ̃i . (5)\nTherefore, in order to obtain the minimal Lipschitz constant U in Lemma 4, we choose the weights of the metric µ to be the components of γ. A combination of Theorem 2, Lemma 4 and this observation implies the following result. Theorem 3. Let Φ ∈ C1(V,R) ∩ C2(B++,R) with ∇Φ(S+) ⊂ V++. Let GΦ : B++ → B++ be defined as in (3). Suppose that there exists a matrix A ∈ R(K+1)×(K+1)+ such that GΦ and A satisfies the assumptions of Lemma 4 and AT has a positive eigenvector γ ∈ RK+1++ . If ρ(A) < 1, then Φ has a unique critical point (w∗, u∗) in S++ which is the global maximum of the optimization problem (2). Moreover, the sequence ( (wk, uk) ) k defined for any (w0, u0) ∈ S++ as (wk+1, uk+1) = GΦ(wk, uk), k ∈ N, satisfies limk→∞(wk, uk) = (w∗, u∗) and\n‖(wk, uk)− (w∗, u∗)‖∞ ≤ ρ(A)k (\nµ ( (w1, u1), (w0, u0) )( 1− ρ(A) ) min {γK+1 ρu ,mint∈[K] γtρw }) ∀k ∈ N,\nwhere the weights in the definition of µ are the entries of γ."
    }, {
      "heading" : "4 Application to Neural Networks",
      "text" : "In the previous sections we have outlined the proof of our main result for a general objective function satisfying certain properties. The purpose of this section is to prove that the properties hold for our optimization problem for neural networks.\nWe recall our objective function from (2)\nΦ(w, u) = 1 n n∑ i=1 [ − L ( yi, f(w, u)(xi) ) + K∑ r=1 fr(w, u)(xi) ] + ( K∑ r=1 n1∑ l=1 wr,l + n1∑ l=1 d∑ m=1 ulm ) and the function class we are considering from (1)\nfr(x) = fr(w, u)(x) = n1∑ l=1 wr,l ( d∑ m=1 ulmxm )αl ,\nThe arbitrarily small in the objective is needed to make the gradient strictly positive on the boundary of V+. We note that the assumption αi ≥ 1 for every i ∈ [n1] is crucial in the following lemma in order to guarantee that ∇Φ is well defined on S+. Lemma 5. Let Φ be defined as in (2), then ∇Φ(w, u) is strictly positive for any (w, u) ∈ S+.\nNext, we derive the matrix A ∈ R(K+1)×(K+1) in order to apply Theorem 3 to GΦ with Φ defined in (2). As discussed in its proof, the matrix A given in the following theorem has a smaller spectral radius than that of Theorem 1. To express this matrix, we consider Ψαp,q : R n1 ++ × R++ → R++ defined for p, q ∈ (1,∞) and α ∈ R n1 ++ as\nΨαp,q(δ, t) = ([∑\nl∈J\n(δl tαl) p q q−αp ]1−αpq + max j∈Jc (δj tαj )p )1/p , (6)\nwhere J = {l ∈ [n1] | αlp ≤ q}, Jc = {l ∈ [n1] | αlp > q} and α = minl∈J αl. Theorem 4. Let Φ be defined as above and GΦ be as in (3). Set Cw = ρw Ψαp′w,pu(1, ρuρx), Cu = ρw Ψαp′w,pu(α, ρuρx) and ρx = maxi∈[n] ‖x i‖p′u . Then A and G Φ satisfy all assumptions of Lemma 4 with\nA = 2 diag ( p′w − 1, . . . , p′w − 1, p′u − 1 )(Qw,w Qw,u Qu,w Qu,u ) where Qw,w ∈ RK×K++ , Qw,u ∈ RK×1++ , Qu,w ∈ R1×K++ and Qu,u ∈ R++ are defined as\nQw,w = 2Cw11T , Qw,u = (2Cu + ‖α‖∞)1, Qu,w = (2Cw + 1)1T , Qu,u = (2Cu + ‖α‖∞ − 1).\nIn the supplementary material, we prove that Ψαp,q(δ, t) ≤ ∑n1 l=1 δlt\nαl which yields the weaker bounds ξ1, ξ2 given in Theorem 1. In particular, this observation combined with Theorems 3 and 4 implies Theorem 1."
    }, {
      "heading" : "4.1 Neural networks with two hidden layers",
      "text" : "We show how to extend our framework for neural networks with 2 hidden layers. In future work we will consider the general case. We briefly explain the major changes. Let n1, n2 ∈ N and α ∈ Rn1++, β ∈ R n2 ++ with αi, βj ≥ 1 for all i ∈ [n1], j ∈ [n2], our function class is:\nfr(x) = fr(w, v, u)(x) = n2∑ l=1 wr,l ( n1∑ m=1 vlm ( d∑ s=1 umsxs )αm)βl and the optimization problem becomes\nmax (w,v,u)∈S+ Φ(w, v, u) where V+ = RK×n2+ × R n2×n1 + × R n1×d + , (7)\nS+ = {(w1, . . . , wK , v, u) ∈ V+ | ‖wi‖pw = ρw, ‖v‖pv = ρv, ‖u‖pu = ρu} and\nΦ(w, v, u) = 1 n n∑ i=1 [ −L ( yi, f(xi) ) + K∑ r=1 fr(xi) ] + ( K∑ r=1 n2∑ l=1 wr,l+ n2∑ l=1 n1∑ m=1 vlm+ n1∑ m=1 d∑ s=1 ums ) .\nThe map GΦ : S++ → S++ = {z ∈ S+ | z > 0}, GΦ = (GΦw1 , . . . , G Φ wK , G Φ v , G Φ u ), becomes\nGΦwi(w, v, u) = ρw ψp′w(∇wiΦ(w, u))\n‖ψp′w(∇wiΦ(w, v, u))‖pw ∀i ∈ [K] (8)\nand\nGΦv (w, v, u) = ρv ψp′v (∇vΦ(w, v, u)) ‖ψp′v (∇vΦ(w, v, u))‖pv , GΦu (w, v, u) = ρu ψp′u(∇uΦ(w, v, u)) ‖ψp′u(∇uΦ(w, v, u))‖pu .\nWe have the following equivalent of Theorem 1 for 2 hidden layers. Theorem 5. Let {xi, yi}ni=1 ⊂ Rd+× [K], pw, pv, pu ∈ (1,∞), ρw, ρv, ρu > 0, n1, n2 ∈ N and α ∈ Rn1++, β ∈ R n2 ++ with αi, βj ≥ 1 for all i ∈ [n1], j ∈ [n2]. Let ρx = maxi∈[n] ‖xi‖p′u ,\nθ = ρvΨαp′v,pu(1, ρuρx), Cw = ρwΨ β p′w,pv (1, θ), Cv = ρwΨβp′w,pv (β, θ), Cu = ‖α‖∞Cv,\nand define A ∈ R(K+2)×(K+2)++ as\nAm,l = 4(p′w − 1)Cw, Am,K+1 = 2(p′w − 1)(2Cv + ‖β‖∞) Am,K+2 = 2(p′w − 1) ( 2Cu + ‖α‖∞‖β‖∞ ) , AK+1,l = 2(p′v − 1) ( 2Cw + 1 ) AK+1,K+1 = 2(p′v − 1) ( 2Cv + ‖β‖∞ − 1 ) , AK+1,K+2 = 2(p′v − 1) ( 2Cu + ‖α‖∞‖β‖∞\n) AK+2,l = 2(p′u − 1)(2Cw + 1), AK+2,K+1 = 2(p′u − 1)(2Cv + ‖β‖∞),\nAK+2,K+2 = 2(p′u − 1)(2Cu + ‖α‖∞‖β‖∞ − 1) ∀m, l ∈ [K].\nIf ρ(A) < 1, then (7) has a unique global maximizer (w∗, v∗, u∗) ∈ S++. Moreover, for every (w0, v0, u0) ∈ S++, there exists R > 0 such that\nlim k→∞ (wk, vk, uk) = (w∗, v∗, u∗) and ‖(wk, vk, uk)−(w∗, v∗, u∗)‖∞ ≤ Rρ(A)k ∀k ∈ N\nwhere (wk+1, vk+1, uk+1) = GΦ(wk, vk, uk) for every k ∈ N and GΦ is defined as in (8).\nAs for the case with one hidden layer, for any fixed architecture ρw, ρv, ρu > 0, n1, n2 ∈ N and α ∈ Rn1++, β ∈ R n2 ++ with αi, βj ≥ 1 for all i ∈ [n1], j ∈ [n2], it is possible to derive lower bounds on pw, pv, pu that guarantee ρ(A) < 1 in Theorem 5. Indeed, it holds\nCw ≤ ζ1 = ρw n2∑ j=1 [ ρv n1∑ l=1 (ρuρ̃x)αl ]βj and Cv ≤ ζ2 = ρw n2∑ j=1 βj [ ρv n1∑ l=1 (ρuρ̃x)αl ]βj ,\nwith ρ̃x = maxi∈[n] ‖xi‖1. Hence, the two hidden layers equivalent of (4) becomes pw > 4(K+2)ζ1+5, pv > 2(K+2) [ 2ζ2+‖β‖∞ ] −1, pu > 2(K+2)‖α‖∞(2ζ2+‖β‖∞)−1. (9)"
    }, {
      "heading" : "5 Experiments",
      "text" : "10 0 10 1 10 2\nepochs\n10 -14\n10 -12\n10 -10\n10 -8\n(p ∗ − f )/ |p\n∗ |\nNLSM1 SGD 100 SGD 10 SGD 1 SGD 0.1 SGD 0.01\n10 0\n10 1\n10 2\n10 3\nepochs\n0\n20\n40\n60\n80\n100\nte s t\ne rr\no r\nNLSM1 SGD 100 SGD 10 SGD 1 SGD 0.1 SGD 0.01\nFigure 2: Training score (left) w.r.t. the optimal score p∗ and test error (right) of NLSM1 and Batch-SGD with different step-sizes.\nTable 1: Test accuracy on UCI datasets\nDataset NLSM1 NLSM2 ReLU1 ReLU2 SVM\nCancer 96.4 96.4 95.7 93.6 95.7 Iris 90.0 96.7 100 93.3 100 Banknote 97.1 96.4 100 97.8 100 Blood 76.0 76.7 76.0 76.0 77.3 Haberman 75.4 75.4 70.5 72.1 72.1 Seeds 88.1 90.5 90.5 92.9 95.2 Pima 79.2 80.5 76.6 79.2 79.9\nThe shown experiments should be seen as a proof of concept. We do not have yet a good understanding of how one should pick the parameters of our model to achieve good performance. However, the other papers which have up to now discussed global optimality for neural networks [11, 8] have not included any results on real datasets. Thus, up to our\nNonlinear Spectral Method for 1 hidden layer Input: Model n1 ∈ N, pw, pu ∈ (1,∞), ρw, ρu > 0, α1, . . . , αn1 ≥ 1, > 0 so that the matrix A of Theorem 1 satisfies ρ(A) < 1. Accuracy τ > 0 and (w0, u0) ∈ S++. 1 Let (w1, u1) = GΦ(w0, u0) and compute R as in Theorem 3 2 Repeat 3 (wk+1, uk+1) = GΦ(wk, uk) 4 k ← k + 1 5 Until k ≥ ln ( τ/R ) / ln ( ρ(A)\n) Output: (wk, uk) fulfills ‖(wk, uk)− (w∗, u∗)‖∞ < τ . With GΦ defined as in (3). The method for two hidden layers is similar: consider GΦ\nas in (8) instead of (3) and assume that the model satisfies Theorem 5.\nknowledge, we show for the first time a globally optimal algorithm for neural networks that leads to non-trivial classification results. We test our methods on several low dimensional UCI datasets and denote our algorithms as NLSM1 (one hidden layer) and NLSM2 (two hidden layers). We choose the parameters of our model out of 100 randomly generated combinations of (n1, α, ρw, ρu) ∈ [2, 20]× [1, 4]× (0, 1]2 (respectively (n1, n2, α, β, ρw, ρv, ρu) ∈ [2, 10]2 × [1, 4]2 × (0, 1]2) and pick the best one based on 5-fold cross-validation error. We use Equation (4) (resp. Equation (9)) to choose pu, pw (resp. pu, pv, pw) so that every generated model satisfies the conditions of Theorem 1 (resp. Theorem 5), i.e. ρ(A) < 1. Thus, global optimality is guaranteed in all our experiments. For comparison, we use the nonlinear RBF-kernel SVM and implement two versions of the Rectified-Linear Unit network - one for one hidden layer networks (ReLU1) and one for two hidden layers networks (ReLU2). To train ReLU, we use a stochastic gradient descent method which minimizes the sum of logistic loss and L2 regularization term over weight matrices to avoid over-fitting. All parameters of each method are jointly cross validated. More precisely, for ReLU the number of hidden units takes values from 2 to 20, the step-sizes and regularizers are taken in {10−6, 10−5, . . . , 102} and {0, 10−4, 10−3, . . . , 104} respectively. For SVM, the hyperparameter C and the kernel parameter γ of the radius basis function K(xi, xj) = exp(−γ‖xi − xj‖2) are taken from {2−5, 2−4 . . . , 220} and {2−15, 2−14 . . . , 23} respectively. Note that ReLUs allow negative weights while our models do not. The results presented in Table 1 show that overall our nonlinear spectral methods achieve slightly worse performance than kernel SVM while being competitive/slightly better than ReLU networks. Notably in case of Cancer, Haberman and Pima, NLSM2 outperforms all the other models. For Iris and Banknote, we note that without any constraints ReLU1 can easily find an architecture which achieves zero test error while this is difficult for our models as we impose constraints on the architecture in order to prove global optimality. We compare our algorithms with Batch-SGD in order to optimize (2) with batch-size being 5% of the training data while the step-size is fixed and selected between 10−2 and 102. At each iteration of our spectral method and each epoch of Batch-SGD, we compute the objective and test error of each method and show the results in Figure 2. One can see that our method is much faster than SGDs, and has a linear convergence rate. We noted in our experiments that as α is large and our data lies between [0, 1], all units in the network tend to have small values that make the whole objective function relatively small. Thus, a relatively large change in (w, u) might cause only small changes in the objective function but performance may vary significantly as the distance is large in the parameter space. In other words, a small change in the objective may have been caused by a large change in the parameter space, and thus, largely influences the performance - which explains the behavior of SGDs in Figure 2. The magnitude of the entries of the matrix A in Theorems 1 and 5 grows with the number of hidden units and thus the spectral radius ρ(A) also increases with this number. As we expect that the number of required hidden units grows with the dimension of the datasets we have limited ourselves in the experiments to low-dimensional datasets. However, these bounds are likely not to be tight, so that there might be room for improvement in terms of dependency on the number of hidden units."
    }, {
      "heading" : "Acknowledgment",
      "text" : "The authors acknowledge support by the ERC starting grant NOLEPRO 307793."
    } ],
    "references" : [ {
      "title" : "Neural Network Learning: Theoretical Foundations",
      "author" : [ "M. Anthony", "P. Bartlett" ],
      "venue" : "Cambridge University Press, New York",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Provable bounds for learning some deep representations",
      "author" : [ "S. Arora", "A. Bhaskara", "R. Ge", "T. Ma" ],
      "venue" : "ICML",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Nonnegative Matrices in the Mathematical Sciences",
      "author" : [ "A. Berman", "R.J. Plemmons" ],
      "venue" : "SIAM, Philadelphia",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Nonlinear Programming",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific, Belmont, Mass.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "A. Choromanska", "M. Hena", "M. Mathieu", "G.B. Arous", "Y. LeCun" ],
      "venue" : "AISTATS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "and Y",
      "author" : [ "A Daniely", "R. Frostigy" ],
      "venue" : "Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "The Perron-Frobenius Theorem for Multi-Homogeneous Maps",
      "author" : [ "A. Gautier", "F. Tudisco", "M. Hein" ],
      "venue" : "preparation",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Global optimality in tensor factorization, deep learning, and beyond",
      "author" : [ "B.D. Haeffele", "Rene Vidal" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Train faster",
      "author" : [ "M. Hardt", "B. Recht", "Y. Singer" ],
      "venue" : "generalize better: Stability of stochastic gradient descent. In ICML",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Matrix Analysis",
      "author" : [ "R.A. Horn", "C.R. Johnson" ],
      "venue" : "Cambridge University Press, New York, second edition",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and A",
      "author" : [ "M. Janzamin", "H. Sedghi" ],
      "venue" : "Anandkumar. Beating the perils of non-convexity:guaranteed training of neural networks using tensor methods",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An Introduction to Metric Spaces and Fixed Point Theory",
      "author" : [ "W.A. Kirk", "M.A. Khamsi" ],
      "venue" : "John Wiley, New York",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature, 521",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Nonlinear Perron-Frobenius theory",
      "author" : [ "B. Lemmens", "R.D. Nussbaum" ],
      "venue" : "Cambridge University Press, New York, general edition",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the computational efficiency of training neural networks",
      "author" : [ "R. Livni", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : "NIPS, pages 855–863",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep Learning in Neural Networks: An Overview",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Networks, 61:85–117",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Training a single sigmoidal neuron is hard",
      "author" : [ "J. Sima" ],
      "venue" : "Neural Computation, 14:2709–2728",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "On certain contraction mappings in a partially ordered vector space",
      "author" : [ "A.C. Thompson" ],
      "venue" : "Proceedings of the American Mathematical Society, 14:438–443",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1963
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "Deep learning [13, 16] is currently the state of the art machine learning technique in many application areas such as computer vision or natural language processing.",
      "startOffset" : 14,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "[1], the understanding of the success of training deep neural networks is a currently very active research area [5, 6, 9].",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 16,
      "context" : "In particular, the problem is even for a single hidden layer in general NP hard, see [17] and references therein.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 14,
      "context" : "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].",
      "startOffset" : 222,
      "endOffset" : 229
    }, {
      "referenceID" : 10,
      "context" : "A recent line of research has directly tackled the optimization problem of neural networks and provided either certain guarantees [2, 15] in terms of the global optimum or proved directly convergence to the global optimum [8, 11].",
      "startOffset" : 222,
      "endOffset" : 229
    }, {
      "referenceID" : 7,
      "context" : "While providing a lot of interesting insights on the relationship of structured matrix factorization and training of neural networks, Haeffele and Vidal admit themselves in their paper [8] that their results are “challenging to apply in practice”.",
      "startOffset" : 185,
      "endOffset" : 188
    }, {
      "referenceID" : 10,
      "context" : "[11] they use a tensor approach and propose a globally optimal algorithm for a feedforward neural network with one hidden layer and squared loss.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "Polynomial neural networks have been recently analyzed in [15].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "The reason to write this as a maximization problem is that our nonlinear spectral method is inspired by the theory of (sub)-homogeneous nonlinear eigenproblems on convex cones [14] which has its origin in the Perron-Frobenius theory for nonnegative matrices.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 6,
      "context" : "In fact our work is motivated by the closely related Perron-Frobenius theory for multihomogeneous problems developed in [7].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "26 in [10]) that ρ(A) = ρ(A ) ≤ maxi∈[K+1](A v)i/vi for any v ∈ RK+1 ++ .",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "A popular metric for the study of nonlinear eigenvalue problems on the positive orthant is the so-called Thompson metric d : R++ × R++ → R+ [18] defined as d(z, z̃) = ‖ ln(z)− ln(z̃)‖∞ where ln(z) = ( ln(z1), .",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "31 in [10], we know that if A has a positive eigenvector γ ∈ RK+1 ++ , then",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 10,
      "context" : "However, the other papers which have up to now discussed global optimality for neural networks [11, 8] have not included any results on real datasets.",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "However, the other papers which have up to now discussed global optimality for neural networks [11, 8] have not included any results on real datasets.",
      "startOffset" : 95,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "We choose the parameters of our model out of 100 randomly generated combinations of (n1, α, ρw, ρu) ∈ [2, 20]× [1, 4]× (0, 1]2 (respectively (n1, n2, α, β, ρw, ρv, ρu) ∈ [2, 10]2 × [1, 4]2 × (0, 1]2) and pick the best one based on 5-fold cross-validation error.",
      "startOffset" : 102,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "We choose the parameters of our model out of 100 randomly generated combinations of (n1, α, ρw, ρu) ∈ [2, 20]× [1, 4]× (0, 1]2 (respectively (n1, n2, α, β, ρw, ρv, ρu) ∈ [2, 10]2 × [1, 4]2 × (0, 1]2) and pick the best one based on 5-fold cross-validation error.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "We choose the parameters of our model out of 100 randomly generated combinations of (n1, α, ρw, ρu) ∈ [2, 20]× [1, 4]× (0, 1]2 (respectively (n1, n2, α, β, ρw, ρv, ρu) ∈ [2, 10]2 × [1, 4]2 × (0, 1]2) and pick the best one based on 5-fold cross-validation error.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "We choose the parameters of our model out of 100 randomly generated combinations of (n1, α, ρw, ρu) ∈ [2, 20]× [1, 4]× (0, 1]2 (respectively (n1, n2, α, β, ρw, ρv, ρu) ∈ [2, 10]2 × [1, 4]2 × (0, 1]2) and pick the best one based on 5-fold cross-validation error.",
      "startOffset" : 170,
      "endOffset" : 177
    }, {
      "referenceID" : 9,
      "context" : "We choose the parameters of our model out of 100 randomly generated combinations of (n1, α, ρw, ρu) ∈ [2, 20]× [1, 4]× (0, 1]2 (respectively (n1, n2, α, β, ρw, ρv, ρu) ∈ [2, 10]2 × [1, 4]2 × (0, 1]2) and pick the best one based on 5-fold cross-validation error.",
      "startOffset" : 170,
      "endOffset" : 177
    }, {
      "referenceID" : 0,
      "context" : "We choose the parameters of our model out of 100 randomly generated combinations of (n1, α, ρw, ρu) ∈ [2, 20]× [1, 4]× (0, 1]2 (respectively (n1, n2, α, β, ρw, ρv, ρu) ∈ [2, 10]2 × [1, 4]2 × (0, 1]2) and pick the best one based on 5-fold cross-validation error.",
      "startOffset" : 181,
      "endOffset" : 187
    }, {
      "referenceID" : 3,
      "context" : "We choose the parameters of our model out of 100 randomly generated combinations of (n1, α, ρw, ρu) ∈ [2, 20]× [1, 4]× (0, 1]2 (respectively (n1, n2, α, β, ρw, ρv, ρu) ∈ [2, 10]2 × [1, 4]2 × (0, 1]2) and pick the best one based on 5-fold cross-validation error.",
      "startOffset" : 181,
      "endOffset" : 187
    } ],
    "year" : 2016,
    "abstractText" : "The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets.",
    "creator" : null
  }
}