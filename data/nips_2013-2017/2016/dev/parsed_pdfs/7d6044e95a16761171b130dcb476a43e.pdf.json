{
  "name" : "7d6044e95a16761171b130dcb476a43e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Composing graphical models with neural networks for structured representations and fast inference",
    "authors" : [ "Matthew James Johnson", "David Duvenaud", "Ryan P. Adams" ],
    "emails" : [ "mattjj@seas.harvard.edu", "dduvenaud@seas.harvard.edu", "awiltsch@fas.harvard.edu", "srdatta@hms.harvard.edu", "rpa@seas.harvard.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Modeling often has two goals: first, to learn a flexible representation of complex high-dimensional data, such as images or speech recordings, and second, to find structure that is interpretable and generalizes to new tasks. Probabilistic graphical models [1, 2] provide many tools to build structured representations, but often make rigid assumptions and may require significant feature engineering. Alternatively, deep learning methods allow flexible data representations to be learned automatically, but may not directly encode interpretable or tractable probabilistic structure. Here we develop a general modeling and inference framework that combines these complementary strengths.\nConsider learning a generative model for video of a mouse. Learning interpretable representations for such data, and comparing them as the animal’s genes are edited or its brain chemistry altered, gives useful behavioral phenotyping tools for neuroscience and for high-throughput drug discovery [3]. Even though each image is encoded by hundreds of pixels, the data lie near a low-dimensional nonlinear manifold. A useful generative model must not only learn this manifold but also provide an interpretable representation of the mouse’s behavioral dynamics. A natural representation from ethology [3] is that the mouse’s behavior is divided into brief, reused actions, such as darts, rears, and grooming bouts. Therefore an appropriate model might switch between discrete states, with each state representing the dynamics of a particular action. These two learning tasks — identifying an image manifold and a structured dynamics model — are complementary: we want to learn the image manifold in terms of coordinates in which the structured dynamics fit well. A similar challenge arises in speech [4], where high-dimensional spectrographic data lie near a low-dimensional manifold because they are generated by a physical system with relatively few degrees of freedom [5] but also include the discrete latent dynamical structure of phonemes, words, and grammar [6].\nTo address these challenges, we propose a new framework to design and learn models that couple nonlinear likelihoods with structured latent variable representations. Our approach uses graphical models for representing structured probability distributions while enabling fast exact inference subroutines, and uses ideas from variational autoencoders [7, 8] for learning not only the nonlinear\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nfeature manifold but also bottom-up recognition networks to improve inference. Thus our method enables the combination of flexible deep learning feature models with structured Bayesian (and even nonparametric [9]) priors. Our approach yields a single variational inference objective in which all components of the model are learned simultaneously. Furthermore, we develop a scalable fitting algorithm that combines several advances in efficient inference, including stochastic variational inference [10], graphical model message passing [1], and backpropagation with the reparameterization trick [7]. Thus our algorithm can leverage conjugate exponential family structure where it exists to efficiently compute natural gradients with respect to some variational parameters, enabling effective second-order optimization [11], while using backpropagation to compute gradients with respect to all other parameters. We refer to our general approach as the structured variational autoencoder (SVAE)."
    }, {
      "heading" : "2 Latent graphical models with neural net observations",
      "text" : "In this paper we propose a broad family of models. Here we develop three specific examples."
    }, {
      "heading" : "2.1 Warped mixtures for arbitrary cluster shapes",
      "text" : "One particularly natural structure used frequently in graphical models is the discrete mixture model. By fitting a discrete mixture model to data, we can discover natural clusters or units. These discrete structures are difficult to represent directly in neural network models.\nConsider the problem of modeling the data y = {yn}Nn=1 shown in Fig. 1a. A standard approach to finding the clusters in data is to fit a Gaussian mixture model (GMM) with a conjugate prior:\nπ ∼ Dir(α), (µk,Σk) iid∼ NIW(λ), zn |π iid∼ π yn | zn, {(µk,Σk)}Kk=1 iid∼ N (µzn ,Σzn).\nHowever, the fit GMM does not represent the natural clustering of the data (Fig. 1b). Its inflexible Gaussian observation model limits its ability to parsimoniously fit the data and their natural semantics.\nInstead of using a GMM, a more flexible alternative would be a neural network density model:\nγ ∼ p(γ) xn iid∼ N (0, I), yn |xn, γ iid∼ N (µ(xn; γ), Σ(xn; γ)), (1) where µ(xn; γ) and Σ(xn; γ) depend on xn through some smooth parametric function, such as multilayer perceptron (MLP), and where p(γ) is a Gaussian prior [12]. This model fits the data density well (Fig. 1c) but does not explicitly represent discrete mixture components, which might provide insights into the data or natural units for generalization. See Fig. 2a for a graphical model.\nBy composing a latent GMM with nonlinear observations, we can combine the modeling strengths of both [13], learning both discrete clusters along with non-Gaussian cluster shapes:\nπ ∼ Dir(α), (µk,Σk) iid∼ NIW(λ), γ ∼ p(γ) zn |π iid∼ π xn iid∼ N (µ(zn),Σ(zn)), yn |xn, γ iid∼ N (µ(xn; γ), Σ(xn; γ)).\nThis combination of flexibility and structure is shown in Fig. 1d. See Fig. 2b for a graphical model."
    }, {
      "heading" : "2.2 Latent linear dynamical systems for modeling video",
      "text" : "Now we consider a harder problem: generatively modeling video. Since a video is a sequence of image frames, a natural place to start is with a model for images. Kingma et al. [7] shows that the\ndensity network of Eq. (1) can accurately represent a dataset of high-dimensional images {yn}Nn=1 in terms of the low-dimensional latent variables {xn}Nn=1, each with independent Gaussian distributions. To extend this image model into a model for videos, we can introduce dependence through time between the latent Gaussian samples {xn}Nn=1. For instance, we can make each latent variable xn+1 depend on the previous latent variable xn through a Gaussian linear dynamical system, writing\nxn+1 = Axn +Bun, un iid∼ N (0, I), A,B ∈ Rm×m,\nwhere the matrices A and B have a conjugate prior. This model has low-dimensional latent states and dynamics as well as a rich nonlinear generative model of images. In addition, the timescales of the dynamics are represented directly in the eigenvalue spectrum of A, providing both interpretability and a natural way to encode prior information. See Fig. 2c for a graphical model."
    }, {
      "heading" : "2.3 Latent switching linear dynamical systems for parsing behavior from video",
      "text" : "As a final example that combines both time series structure and discrete latent units, consider again the behavioral phenotyping problem described in Section 1. Drawing on graphical modeling tools, we can construct a latent switching linear dynamical system (SLDS) [14] to represent the data in terms of continuous latent states that evolve according to a discrete library of linear dynamics, and drawing on deep learning methods we can generate video frames with a neural network image model.\nAt each time n ∈ {1, 2, . . . , N} there is a discrete-valued latent state zn ∈ {1, 2, . . . ,K} that evolves according to Markovian dynamics. The discrete state indexes a set of linear dynamical parameters, and the continuous-valued latent state xn ∈ Rm evolves according to the corresponding dynamics,\nzn+1 | zn, π ∼ πzn , xn+1 = Aznxn +Bznun, un iid∼ N (0, I),\nwhere π = {πk}Kk=1 denotes the Markov transition matrix and πk ∈ RK+ is its kth row. We use the same neural net observation model as in Section 2.2. This SLDS model combines both continuous and discrete latent variables with rich nonlinear observations. See Fig. 2d for a graphical model."
    }, {
      "heading" : "3 Structured mean field inference and recognition networks",
      "text" : "Why aren’t such rich hybrid models used more frequently? The main difficulty with combining rich latent variable structure and flexible likelihoods is inference. The most efficient inference algorithms used in graphical models, like structured mean field and message passing, depend on conjugate exponential family likelihoods to preserve tractable structure. When the observations are more general, like neural network models, inference must either fall back to general algorithms that do not exploit the model structure or else rely on bespoke algorithms developed for one model at a time.\nIn this section, we review inference ideas from conjugate exponential family probabilistic graphical models and variational autoencoders, which we combine and generalize in the next section."
    }, {
      "heading" : "3.1 Inference in graphical models with conjugacy structure",
      "text" : "Graphical models and exponential families provide many algorithmic tools for efficient inference [15]. Given an exponential family latent variable model, when the observation model is a conjugate exponential family, the conditional distributions stay in the same exponential families as in the prior and hence allow for the same efficient inference algorithms.\nFor example, consider learning a Gaussian linear dynamical system model with linear Gaussian observations. The generative model for latent states x = {xn}Nn=1 and observations y = {yn}Nn=1 is\nxn = Axn−1 +Bun−1, un iid∼ N (0, I), yn = Cxn +Dvn, vn iid∼ N (0, I),\ngiven parameters θ = (A,B,C,D) with a conjugate prior p(θ). To approximate the posterior p(θ, x | y), consider the mean field family q(θ)q(x) and the variational inference objective\nL[ q(θ)q(x) ] = Eq(θ)q(x) [ log\np(θ)p(x | θ)p(y |x, θ) q(θ)q(x)\n] , (2)\nwhere we can optimize the variational family q(θ)q(x) to approximate the posterior p(θ, x | y) by maximizing Eq. (2). Because the observation model p(y |x, θ) is conjugate to the latent variable model p(x | θ), for any fixed q(θ) the optimal factor q∗(x) , arg maxq(x) L[ q(θ)q(x) ] is itself a Gaussian linear dynamical system with parameters that are simple functions of the expected statistics of q(θ) and the data y. As a result, for fixed q(θ) we can easily compute q∗(x) and use message passing algorithms to perform exact inference in it. However, when the observation model is not conjugate to the latent variable model, these algorithmically exploitable structures break down."
    }, {
      "heading" : "3.2 Recognition networks in variational autoencoders",
      "text" : "The variational autoencoder (VAE) [7] handles general non-conjugate observation models by introducing recognition networks. For example, when a Gaussian latent variable model p(x) is paired with a general nonlinear observation model p(y |x, γ), the posterior p(x | y, γ) is non-Gaussian, and it is difficult to compute an optimal Gaussian approximation. The VAE instead learns to directly output a suboptimal Gaussian factor q(x | y) by fitting a parametric map from data y to a mean and covariance, µ(y;φ) and Σ(y;φ), such as an MLP with parameters φ. By optimizing over φ, the VAE effectively learns how to condition on non-conjugate observations y and produce a good approximating factor."
    }, {
      "heading" : "4 Structured variational autoencoders",
      "text" : "We can combine the tractability of conjugate graphical model inference with the flexibility of variational autoencoders. The main idea is to use a conditional random field (CRF) variational family. We learn recognition networks that output conjugate graphical model potentials instead of outputting the complete variational distribution’s parameters directly. These potentials are then used in graphical model inference algorithms in place of the non-conjugate observation likelihoods.\nThe SVAE algorithm computes stochastic gradients of a mean field variational inference objective. It can be viewed as a generalization both of the natural gradient SVI algorithm for conditionally conjugate models [10] and of the AEVB algorithm for variational autoencoders [7]. Intuitively, it proceeds by sampling a data minibatch, applying the recognition model to compute graphical model potentials, and using graphical model inference algorithms to compute the variational factor, combining the evidence from the potentials with the prior structure in the model. This variational factor is then used to compute gradients of the mean field objective. See Fig. 3 for graphical models of the variational families with recognition networks for the models developed in Section 2.\nIn this section, we outline the SVAE model class more formally, write the mean field variational inference objective, and show how to efficiently compute unbiased stochastic estimates of its gradients. The resulting algorithm for computing gradients of the mean field objective, shown in Algorithm 1, is\nAlgorithm 1 Estimate SVAE lower bound and its gradients Input: Variational parameters (ηθ, ηγ , φ), data sample y\nfunction SVAEGRADIENTS(ηθ, ηγ , φ, y) ψ ← r(yn;φ) . Get evidence potentials (x̂, t̄x, KL\nlocal)← PGMINFERENCE(ηθ, ψ) . Combine evidence with prior γ̂ ∼ q(γ) . Sample observation parameters L ← N log p(y | x̂, γ̂)−N KLlocal−KL(q(θ)q(γ)‖p(θ)p(γ)) . Estimate variational bound ∇̃ηθL ← η0θ − ηθ +N(t̄x, 1) +N(∇ηx log p(y | x̂, γ̂), 0) . Compute natural gradient return lower bound L, natural gradient ∇̃ηθL, gradients ∇ηγ ,φL function PGMINFERENCE(ηθ, ψ) q∗(x)← OPTIMIZELOCALFACTORS(ηθ, ψ) . Fast message-passing inference return sample x̂ ∼ q∗(x), statistics Eq∗(x)tx(x), divergence Eq(θ) KL(q∗(x)‖p(x | θ))\nsimple and efficient and can be readily applied to a variety of learning problems and graphical model structures. See the supplementals for details and proofs."
    }, {
      "heading" : "4.1 SVAE model class",
      "text" : "To set up notation for a general SVAE, we first define a conjugate pair of exponential family densities on global latent variables θ and local latent variables x = {xn}Nn=1. Let p(x | θ) be an exponential family and let p(θ) be its corresponding natural exponential family conjugate prior, writing\np(θ) = exp { 〈η0θ , tθ(θ)〉 − logZθ(η0θ) } ,\np(x | θ) = exp { 〈η0x(θ), tx(x)〉 − logZx(η0x(θ)) } = exp {〈tθ(θ), (tx(x), 1)〉} ,\nwhere we used exponential family conjugacy to write tθ(θ) = ( η0x(θ),− logZx(η0x(θ)) ) . The local latent variables x could have additional structure, like including both discrete and continuous latent variables or tractable graph structure, but here we keep the notation simple.\nNext, we define a general likelihood function. Let p(y |x, γ) be a general family of densities and let p(γ) be an exponential family prior on its parameters. For example, each observation yn may depend on the latent value xn through an MLP, as in the density network model of Section 2. This generic non-conjugate observation model provides modeling flexibility, yet the SVAE can still leverage conjugate exponential family structure in inference, as we show next."
    }, {
      "heading" : "4.2 Stochastic variational inference algorithm",
      "text" : "Though the general observation model p(y |x, γ) means that conjugate updates and natural gradient SVI [10] cannot be directly applied, we show that by generalizing the recognition network idea we can still approximately optimize out the local variational factors leveraging conjugacy structure.\nFor fixed y, consider the mean field family q(θ)q(γ)q(x) and the variational inference objective L[ q(θ)q(γ)q(x) ] , Eq(θ)q(γ)q(x) [ log\np(θ)p(γ)p(x | θ)p(y |x, γ) q(θ)q(γ)q(x)\n] . (3)\nWithout loss of generality we can take the global factor q(θ) to be in the same exponential family as the prior p(θ), and we denote its natural parameters by ηθ. We restrict q(γ) to be in the same exponential family as p(γ) with natural parameters ηγ . Finally, we restrict q(x) to be in the same exponential family as p(x | θ), writing its natural parameter as ηx. Using these explicit variational parameters, we write the mean field variational inference objective in Eq. (3) as L(ηθ, ηγ , ηx). To perform efficient optimization of the objective L(ηθ, ηγ , ηx), we consider choosing the variational parameter ηx as a function of the other parameters ηθ and ηγ . One natural choice is to set ηx to be a local partial optimizer of L. However, without conjugacy structure finding a local partial optimizer may be computationally expensive for general densities p(y |x, γ), and in the large data setting this expensive optimization would have to be performed for each stochastic gradient update. Instead, we choose ηx by optimizing over a surrogate objective L̂ with conjugacy structure, given by\nL̂(ηθ, ηx, φ) , Eq(θ)q(x) [ log\np(θ)p(x | θ) exp{ψ(x; y, φ)} q(θ)q(x)\n] , ψ(x; y, φ) , 〈r(y;φ), tx(x)〉,\nwhere {r(y;φ)}φ∈Rm is some parameterized class of functions that serves as the recognition model. Note that the potentials ψ(x; y, φ) have a form conjugate to the exponential family p(x | θ). We define η∗x(ηθ, φ) to be a local partial optimizer of L̂ along with the corresponding factor q∗(x), η∗x(ηθ, φ) , arg min\nηx\nL̂(ηθ, ηx, φ), q∗(x) = exp {〈η∗x(ηθ, φ), tx(x)〉 − logZx(η∗x(ηθ, φ))} .\nAs with the variational autoencoder of Section 3.2, the resulting variational factor q∗(x) is suboptimal for the variational objective L. However, because the surrogate objective has the same form as a variational inference objective for a conjugate observation model, the factor q∗(x) not only is easy to compute but also inherits exponential family and graphical model structure for tractable inference.\nGiven this choice of η∗x(ηθ, φ), the SVAE objective is LSVAE(ηθ, ηγ , φ) , L(ηθ, ηγ , η∗x(ηθ, φ)). This objective is a lower bound for the variational inference objective Eq. (3) in the following sense. Proposition 4.1 (The SVAE objective lower-bounds the mean field objective) The SVAE objective function LSVAE lower-bounds the mean field objective L in the sense that\nmax q(x) L[ q(θ)q(γ)q(x) ] ≥ max ηx L(ηθ, ηγ , ηx) ≥ LSVAE(ηθ, ηγ , φ) ∀φ ∈ Rm,\nfor any parameterized function class {r(y;φ)}φ∈Rm . Furthermore, if there is some φ∗ ∈ Rm such that ψ(x; y, φ∗) = Eq(γ) log p(y |x, γ), then the bound can be made tight in the sense that\nmax q(x) L[ q(θ)q(γ)q(x) ] = max ηx L(ηθ, ηγ , ηx) = max φ LSVAE(ηθ, ηγ , φ).\nThus by using gradient-based optimization to maximize LSVAE(ηθ, ηγ , φ) we are maximizing a lower bound on the model log evidence log p(y). In particular, by optimizing over φ we are effectively learning how to condition on observations so as to best approximate the posterior while maintaining conjugacy structure. Furthermore, to provide the best lower bound we may choose the recognition model function class {r(y;φ)}φ∈Rm to be as rich as possible. Choosing η∗x(ηθ, φ) to be a local partial optimizer of L̂ provides two computational advantages. First, it allows η∗x(ηθ, φ) and expectations with respect to q\n∗(x) to be computed efficiently by exploiting exponential family graphical model structure. Second, it provides a simple expression for an unbiased estimate of the natural gradient with respect to the latent model parameters, as we summarize next. Proposition 4.2 (Natural gradient of the SVAE objective) The natural gradient of the SVAE objective LSVAE with respect to ηθ is ∇̃ηθLSVAE(ηθ, ηγ , φ) = ( η0θ + Eq∗(x) [(tx(x), 1)]− ηθ ) + (∇ηxL(ηθ, ηγ , η∗x(ηθ, φ)), 0). (4)\nNote that the first term in Eq. (4) is the same as the expression for the natural gradient in SVI for conjugate models [10], while a stochastic estimate of the second term is computed automatically as part of the backward pass for computing the gradients with respect to the other parameters, as described next. Thus we have an expression for the natural gradient with respect to the latent model’s parameters that is almost as simple as the one for conjugate models and just as easy to compute. Natural gradients are invariant to smooth invertible reparameterizations of the variational family [16, 17] and provide effective second-order optimization updates [18, 11].\nThe gradients of the objective with respect to the other variational parameters, namely ∇ηγLSVAE(ηθ, ηγ , φ) and∇φLSVAE(ηθ, ηγ , φ), can be computed using the reparameterization trick. To isolate the terms that require the reparameterization trick, we rearrange the objective as\nLSVAE(ηθ, ηγ , φ) = Eq(γ)q∗(x) log p(y |x, γ)−KL(q(θ)q∗(x) ‖ p(θ, x))−KL(q(γ) ‖ p(γ)).\nThe KL divergence terms are between members of the same tractable exponential families. An unbiased estimate of the first term can be computed by sampling x̂ ∼ q∗(x) and γ̂ ∼ q(γ) and computing∇ηγ ,φ log p(y | x̂, γ̂) with automatic differentiation. Note that the second term in Eq. (4) is automatically computed as part of the chain rule in computing∇φ log p(y | x̂, γ̂)."
    }, {
      "heading" : "5 Related work",
      "text" : "In addition to the papers already referenced, there are several recent papers to which this work is related. The two papers closest to this work are Krishnan et al. [19] and Archer et al. [20].\nIn Krishnan et al. [19] the authors consider combining variational autoencoders with continuous state-space models, emphasizing the relationship to linear dynamical systems (also called Kalman filter models). They primarily focus on nonlinear dynamics and an RNN-based variational family, as well as allowing control inputs. However, the approach does not extend to general graphical models or discrete latent variables. It also does not leverage natural gradients or exact inference subroutines.\nIn Archer et al. [20] the authors also consider the problem of variational inference in general continuous state space models but focus on using a structured Gaussian variational family without considering parameter learning. As with Krishnan et al. [19], this approach does not include discrete latent variables (or any latent variables other than the continuous states). However, the method they develop could be used with an SVAE to handle inference with nonlinear dynamics.\nIn addition, both Gregor et al. [21] and Chung et al. [22] extend the variational autoencoder framework to sequential models, though they focus on RNNs rather than probabilistic graphical models."
    }, {
      "heading" : "6 Experiments",
      "text" : "We apply the SVAE to both synthetic and real data and demonstrate its ability to learn feature representations and latent structure. Code is available at github.com/mattjj/svae."
    }, {
      "heading" : "6.1 LDS SVAE for modeling synthetic data",
      "text" : "Consider a sequence of 1D images representing a dot bouncing from one side of the image to the other, as shown at the top of Fig. 4. We use an LDS SVAE to find a low-dimensional latent state space representation along with a nonlinear image model. The model is able to represent the image accurately and to make long-term predictions with uncertainty. See supplementals for details.\nThis experiment also demonstrates the optimization advantages that can be provided by the natural gradient updates. In Fig. 5a we compare natural gradient updates with standard gradient updates at three different learning rates. The natural gradient algorithm not only learns much faster but also is less dependent on parameterization details: while the natural gradient update used an untuned\nstepsize of 0.1, the standard gradient dynamics at step sizes of both 0.1 and 0.05 resulted in some matrix parameters to be updated to indefinite values."
    }, {
      "heading" : "6.2 LDS SVAE for modeling video",
      "text" : "We also apply an LDS SVAE to model depth video recordings of mouse behavior. We use the dataset from Wiltschko et al. [3] in which a mouse is recorded from above using a Microsoft Kinect. We used a subset consisting of 8 recordings, each of a distinct mouse, 20 minutes long at 30 frames per second, for a total of 288000 video fames downsampled to 30× 30 pixels. We use MLP observation and recognition models with two hidden layers of 200 units each and a 10D latent space. Fig. 5b shows images corresponding to a regular grid on a random 2D subspace of the latent space, illustrating that the learned image manifold accurately captures smooth variation in the mouse’s body pose. Fig. 6 shows predictions from the model paired with real data."
    }, {
      "heading" : "6.3 SLDS SVAE for parsing behavior",
      "text" : "Finally, because the LDS SVAE can accurately represent the depth video over short timescales, we apply the latent switching linear dynamical system (SLDS) model to discover the natural units of behavior. Fig. 7 shows some of the discrete states that arise from fitting an SLDS SVAE with 30 discrete states to the depth video data. The discrete states that emerge show a natural clustering of short-timescale patterns into behavioral units. See the supplementals for more."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Structured variational autoencoders provide a general framework that combines some of the strengths of probabilistic graphical models and deep learning methods. In particular, they use graphical models both to give models rich latent representations and to enable fast variational inference with CRF structured approximating distributions. To complement these structured representations, SVAEs use neural networks to produce not only flexible nonlinear observation models but also fast recognition networks that map observations to conjugate graphical model potentials."
    } ],
    "references" : [ {
      "title" : "Probabilistic graphical models: principles and techniques",
      "author" : [ "Daphne Koller", "Nir Friedman" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Machine Learning: a Probabilistic Perspective",
      "author" : [ "Kevin P Murphy" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Mapping Sub-Second Structure in Mouse Behavior",
      "author" : [ "Alexander B. Wiltschko", "Matthew J. Johnson", "Giuliano Iurilli", "Ralph E. Peterson", "Jesse M. Katon", "Stan L. Pashkovski", "Victoria E. Abraira", "Ryan P. Adams", "Sandeep Robert Datta" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "author" : [ "Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Computational models for speech production",
      "author" : [ "Li Deng" ],
      "venue" : "Computational Models of Speech Pattern Processing. Springer,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "Switching dynamic system models for speech articulation and acoustics",
      "author" : [ "Li Deng" ],
      "venue" : "Mathematical Foundations of Speech and Language Processing. Springer,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Auto-Encoding Variational Bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling" ],
      "venue" : "In: International Conference on Learning Representations",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
      "author" : [ "Danilo J Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Stochastic Variational Inference for Bayesian Time Series Models",
      "author" : [ "Matthew J. Johnson", "Alan S. Willsky" ],
      "venue" : "In: International Conference on Machine Learning",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "Matthew D. Hoffman", "David M. Blei", "Chong Wang", "John Paisley" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "New insights and perspectives on the natural gradient method",
      "author" : [ "James Martens" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Density networks”. In: Statistics and neural networks: advances at the interface",
      "author" : [ "David J.C. MacKay", "Mark N. Gibbs" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Warped Mixtures for Nonparametric Cluster Shapes",
      "author" : [ "Tomoharu Iwata", "David Duvenaud", "Zoubin Ghahramani" ],
      "venue" : "In: 29th Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Bayesian Nonparametric Inference of Switching Dynamic Linear Models",
      "author" : [ "E.B. Fox", "E.B. Sudderth", "M.I. Jordan", "A.S. Willsky" ],
      "venue" : "IEEE Transactions on Signal Processing",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Graphical Models, Exponential Families, and Variational Inference",
      "author" : [ "Martin J. Wainwright", "Michael I. Jordan" ],
      "venue" : "In: Foundations and Trends in Machine Learning",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Shun-Ichi Amari" ],
      "venue" : "Neural computation",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1998
    }, {
      "title" : "Methods of Information Geometry",
      "author" : [ "Shun-ichi Amari", "Hiroshi Nagaoka" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Optimizing Neural Networks with Kronecker-factored Approximate Curvature",
      "author" : [ "James Martens", "Roger Grosse" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Deep Kalman Filters",
      "author" : [ "Rahul G Krishnan", "Uri Shalit", "David Sontag" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Black box variational inference for state space models",
      "author" : [ "Evan Archer", "Il Memming Park", "Lars Buesing", "John Cunningham", "Liam Paninski" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "DRAW: A recurrent neural network for image generation",
      "author" : [ "Karol Gregor", "Ivo Danihelka", "Alex Graves", "Daan Wierstra" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "A recurrent latent variable model for sequential data",
      "author" : [ "Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C Courville", "Yoshua Bengio" ],
      "venue" : "Advances in Neural information processing systems",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Probabilistic graphical models [1, 2] provide many tools to build structured representations, but often make rigid assumptions and may require significant feature engineering.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "Probabilistic graphical models [1, 2] provide many tools to build structured representations, but often make rigid assumptions and may require significant feature engineering.",
      "startOffset" : 31,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "Learning interpretable representations for such data, and comparing them as the animal’s genes are edited or its brain chemistry altered, gives useful behavioral phenotyping tools for neuroscience and for high-throughput drug discovery [3].",
      "startOffset" : 236,
      "endOffset" : 239
    }, {
      "referenceID" : 2,
      "context" : "A natural representation from ethology [3] is that the mouse’s behavior is divided into brief, reused actions, such as darts, rears, and grooming bouts.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "A similar challenge arises in speech [4], where high-dimensional spectrographic data lie near a low-dimensional manifold because they are generated by a physical system with relatively few degrees of freedom [5] but also include the discrete latent dynamical structure of phonemes, words, and grammar [6].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "A similar challenge arises in speech [4], where high-dimensional spectrographic data lie near a low-dimensional manifold because they are generated by a physical system with relatively few degrees of freedom [5] but also include the discrete latent dynamical structure of phonemes, words, and grammar [6].",
      "startOffset" : 208,
      "endOffset" : 211
    }, {
      "referenceID" : 5,
      "context" : "A similar challenge arises in speech [4], where high-dimensional spectrographic data lie near a low-dimensional manifold because they are generated by a physical system with relatively few degrees of freedom [5] but also include the discrete latent dynamical structure of phonemes, words, and grammar [6].",
      "startOffset" : 301,
      "endOffset" : 304
    }, {
      "referenceID" : 6,
      "context" : "Our approach uses graphical models for representing structured probability distributions while enabling fast exact inference subroutines, and uses ideas from variational autoencoders [7, 8] for learning not only the nonlinear",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "Our approach uses graphical models for representing structured probability distributions while enabling fast exact inference subroutines, and uses ideas from variational autoencoders [7, 8] for learning not only the nonlinear",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "Thus our method enables the combination of flexible deep learning feature models with structured Bayesian (and even nonparametric [9]) priors.",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, we develop a scalable fitting algorithm that combines several advances in efficient inference, including stochastic variational inference [10], graphical model message passing [1], and backpropagation with the reparameterization trick [7].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, we develop a scalable fitting algorithm that combines several advances in efficient inference, including stochastic variational inference [10], graphical model message passing [1], and backpropagation with the reparameterization trick [7].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, we develop a scalable fitting algorithm that combines several advances in efficient inference, including stochastic variational inference [10], graphical model message passing [1], and backpropagation with the reparameterization trick [7].",
      "startOffset" : 248,
      "endOffset" : 251
    }, {
      "referenceID" : 10,
      "context" : "Thus our algorithm can leverage conjugate exponential family structure where it exists to efficiently compute natural gradients with respect to some variational parameters, enabling effective second-order optimization [11], while using backpropagation to compute gradients with respect to all other parameters.",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 11,
      "context" : "Instead of using a GMM, a more flexible alternative would be a neural network density model: γ ∼ p(γ) xn iid ∼ N (0, I), yn |xn, γ iid ∼ N (μ(xn; γ), Σ(xn; γ)), (1) where μ(xn; γ) and Σ(xn; γ) depend on xn through some smooth parametric function, such as multilayer perceptron (MLP), and where p(γ) is a Gaussian prior [12].",
      "startOffset" : 319,
      "endOffset" : 323
    }, {
      "referenceID" : 12,
      "context" : "By composing a latent GMM with nonlinear observations, we can combine the modeling strengths of both [13], learning both discrete clusters along with non-Gaussian cluster shapes: π ∼ Dir(α), (μk,Σk) iid ∼ NIW(λ), γ ∼ p(γ) zn |π iid ∼ π xn iid ∼ N (μn,Σn), yn |xn, γ iid ∼ N (μ(xn; γ), Σ(xn; γ)).",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : "Drawing on graphical modeling tools, we can construct a latent switching linear dynamical system (SLDS) [14] to represent the data in terms of continuous latent states that evolve according to a discrete library of linear dynamics, and drawing on deep learning methods we can generate video frames with a neural network image model.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 14,
      "context" : "Graphical models and exponential families provide many algorithmic tools for efficient inference [15].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "Figure 3: Variational families and recognition networks for the VAE [7] and three SVAE examples.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "The variational autoencoder (VAE) [7] handles general non-conjugate observation models by introducing recognition networks.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "It can be viewed as a generalization both of the natural gradient SVI algorithm for conditionally conjugate models [10] and of the AEVB algorithm for variational autoencoders [7].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "It can be viewed as a generalization both of the natural gradient SVI algorithm for conditionally conjugate models [10] and of the AEVB algorithm for variational autoencoders [7].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "2 Stochastic variational inference algorithm Though the general observation model p(y |x, γ) means that conjugate updates and natural gradient SVI [10] cannot be directly applied, we show that by generalizing the recognition network idea we can still approximately optimize out the local variational factors leveraging conjugacy structure.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "(4) is the same as the expression for the natural gradient in SVI for conjugate models [10], while a stochastic estimate of the second term is computed automatically as part of the backward pass for computing the gradients with respect to the other parameters, as described next.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Natural gradients are invariant to smooth invertible reparameterizations of the variational family [16, 17] and provide effective second-order optimization updates [18, 11].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "Natural gradients are invariant to smooth invertible reparameterizations of the variational family [16, 17] and provide effective second-order optimization updates [18, 11].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "Natural gradients are invariant to smooth invertible reparameterizations of the variational family [16, 17] and provide effective second-order optimization updates [18, 11].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 10,
      "context" : "Natural gradients are invariant to smooth invertible reparameterizations of the variational family [16, 17] and provide effective second-order optimization updates [18, 11].",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "[19] the authors consider combining variational autoencoders with continuous state-space models, emphasizing the relationship to linear dynamical systems (also called Kalman filter models).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] the authors also consider the problem of variational inference in general continuous state space models but focus on using a structured Gaussian variational family without considering parameter learning.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19], this approach does not include discrete latent variables (or any latent variables other than the continuous states).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] extend the variational autoencoder framework to sequential models, though they focus on RNNs rather than probabilistic graphical models.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[3] in which a mouse is recorded from above using a Microsoft Kinect.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "We propose a general modeling and inference framework that combines the complementary strengths of probabilistic graphical models and deep learning methods. Our model family composes latent graphical models with neural network observation likelihoods. For inference, we use recognition networks to produce local evidence potentials, then combine them with the model distribution using efficient message-passing algorithms. All components are trained simultaneously with a single stochastic variational inference objective. We illustrate this framework by automatically segmenting and categorizing mouse behavior from raw depth video, and demonstrate several other example models.",
    "creator" : null
  }
}