{
  "name" : "5b8add2a5d98b1a652ea7fd72d942dac.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow",
    "authors" : [ "Gang Wang", "Georgios B. Giannakis" ],
    "emails" : [ "gangwang@umn.edu", "georgios@umn.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider a system of m quadratic equations\nyi = |〈ai,x〉|2 , i ∈ [m] := {1, 2, . . . ,m} (1)\nwhere data vector y := [y1 · · · ym]T and feature vectors ai ∈ Rn/Cn, collected in them×n matrix A := [a1 · · · am]H are known, whereas vector x ∈ Rn/Cn is the wanted unknown. When {ai}mi=1 and/or x are complex, their amplitudes are given but phase information is lacking; whereas in the real case only the signs of {〈ai,x〉} are unknown. Supposing that the system of equations in (1) admits a unique solution x (up to a global unimodular constant), our objective is to reconstruct x from m phaseless quadratic equations, or equivalently, recover the missing signs/phases of 〈ai,x〉 in the real-/complex-valued settings. Indeed, it has been established thatm ≥ 2n−1 orm ≥ 4n−4 generic data {(ai; yi)}mi=1 as in (1) suffice for uniqueness of an n-dimensional real- or complex-valued vector x [1, 2], respectively, and the former with equality has also been shown to be necessary [1].\nThe problem in (1) constitutes an instance of nonconvex quadratic programming, that is generally known to be NP-hard [3]. Specifically for real-valued vectors, this can be understood as a combinatorial optimization since one seeks a series of signs si = ±1, such that the solution to the system of linear equations 〈ai,x〉 = siψi, where ψi := √ yi, obeys the given quadratic system (1). Concatenating all amplitudes {ψi}mi=1 to form the vector ψ := [ψ1 · · · ψm] T , apparently there are a total of 2m different combinations of {si}mi=1, among which only two lead to x up to a global sign.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nThe complex case becomes even more complicated, where instead of a set of signs {si}mi=1, one must specify for uniqueness a collection of unimodular complex scalars {σi ∈ C}mi=1. In many fields of physical sciences and engineering, the problem of recovering the phase from intensity/magnitude-only measurements is commonly referred to as phase retrieval [4, 5]. The plethora of applications include X-ray crystallography, optics, as well as array imaging, where due to physical limitations, optical detectors can record only (squared) modulus of the Fresnel or Fraunhofer diffraction pattern, while losing the phase of the incident light reaching the object [5]. It has been shown that reconstructing a discrete, finite-duration signal from its Fourier transform magnitude is NP-complete [6]. Despite its simple form and practical relevance across various fields, tackling the quadratic system (1) under real-/complex-valued settings is challenging and NP-hard in general."
    }, {
      "heading" : "1.1 Nonconvex Optimization",
      "text" : "Adopting the least-squares criterion, the task of recovering x can be recast as that of minimizing the following intensity-based empirical loss\nmin z∈Cn\nf(z) := 1\n2m m∑ i=1 ( yi − ∣∣aHi z∣∣2)2 (2) or, the amplitude-based one\nmin z∈Cn\n`(z) := 1\n2m m∑ i=1 ( ψi − ∣∣aHi z∣∣)2 . (3) Unfortunately, both cost functions (2) and (3) are nonconvex. Minimizing nonconvex objectives, which may exhibit many stationary points, is in general NP-hard [7]. In a nutshell, solving problems of the form (2) or (3) is challenging.\nExisting approaches to solving (2) (or related ones using the Poisson likelihood; see, e.g., [8]) or (3) fall under two categories: nonconvex and convex ones. Popular nonconvex solvers include the alternating projection such as Gerchberg-Saxton [9] and Fineup [10], AltMinPhase [11], and (Truncated) Wirtinger flow (WF/TWF) [12, 8], as well as trust-region methods [13]. Convex approaches on the other hand rely on the so-called matrix-lifting technique to obtain the solvers abbreviated as PhaseLift [14] and PhaseCut [15].\nIn terms of sample complexity for Gaussian {ai} designs, convex approaches enable exact recovery from1 O(n) noiseless measurements [16], while they require solving a semidefinite program of a matrix variable with size n× n, thus incurring worst-case computational complexity on the order of O(n4.5) [15], that does not scale well with dimensionality n. Upon exploiting the underlying problem structure,O(n4.5) can be reduced toO(n3) [15]. Solving for vector variables, nonconvex approaches achieve significantly improved computational performance. Using formulation (3), AltMinPhase adopts a spectral initialization and establishes exact recovery with sample complexity O(n log3 n) under Gaussian {ai} designs with resampling [11]. Concerning formulation (2), WF iteratively refines the spectral initial estimate by means of a gradient-like update [12]. The follow-up TWF improves upon WF through a truncation procedure to separate gradient components of excessively extreme sizes. Likewise, at the initialization stage, since the term (aTi x) 2aia H i responsible for the spectral initialization is heavy-tailed, data {yi}mi=1 are pre-screened in the truncated spectral initialization to yield improved initial estimates [8]. Under Gaussian sampling models, WF allows exact recovery from O(n log n) measurements in O(mn2 log(1/ )) time/flops to yield an -accurate solution for any given > 0 [12], while TWF advances these to O(n) measurements and O(mn log(1/ )) time [8]. Interestingly, the truncation procedure in the gradient stage turns out to be useful in avoiding spurious stationary points in the context of nonconvex optimization. Although for large-scale linear regressions, similar ideas including censoring have been studied [17, 18]. It is worth mentioning that when m ≥ Cn log3 n for sufficiently large C > 0, the objective function in (3) admits benign geometric structure that allows certain iterative algorithms (e.g., trust-region methods) to efficiently find a global minimizer with random initializations [13].\nAlthough achieving a linear (in the number of unknowns n) sample and computational complexity, the state-of-the-art TWF scheme still requires at least 4n ∼ 5n equations to yield a stable empirical success rate (e.g., ≥ 99%) under the real Gaussian model [8, Section 3], which are more than twice the known information-limit of m = 2n − 1 [1]. Similar though less obvious results hold also in\n1The notation φ(n) = O(g(n)) means that there is a constant c > 0 such that |φ(n)| ≤ c|g(n)|.\nthe complex-valued scenario. Even though the truncated spectral initialization improves upon the “plain vallina” spectral initialization, its performance still suffers when the number of measurements is relatively small and its advantage (over the untruncated version) narrows as the number of measurements grows. Further, it is worth stressing that extensive numerical and experimental validation confirms that the amplitude-based cost function performs better than the intensity-based one; that is, formulation (3) is superior over (2) [19]. Hence, besides enhancing initialization, markedly improved performance in the gradient stage could be expected by re-examining the amplitude-based cost function and incorporating judiciously designed truncation rules."
    }, {
      "heading" : "2 Algorithm: Truncated Generalized Gradient Flow",
      "text" : "Along the lines of suitably initialized nonconvex schemes, and building upon the amplitude-based formulation (3), this paper develops a novel linear-time (in both m and n) algorithm, referred to as truncated generalized gradient flow (TGGF), that provably recovers x ∈ Rn/Cn exactly from a near-optimal number of noise-free measurements, while also featuring a near-perfect statistical performance in the noisy setup. Our TGGF proceeds in two stages: s1) A novel orthogonality-promoting initialization that relies on simple power iterations to markedly improve upon spectral initialization; and, s2) a refinement of the initial estimate by successive updates of truncated generalized gradient iterations. Stages s1) and s2) are delineated next in reverse order. For concreteness, our analysis will focus on the real Gaussian model with x ∈ Rn and independently and identically distributed (i.i.d.) design vectors ai ∈ Rn ∼ N (0, In), whereas numerical implementations for the complex Gaussian model having x ∈ Cn and i.i.d. ai ∼ CN (0, In) := N (0, In/2) + jN (0, In/2) will be discussed briefly. To start, define the Euclidean distance of any estimate z to the solution set: dist(z, x) := min ‖z ± x‖ for real signals, and dist(z, x) := minφ∈[0,2π) ‖z−xeiφ‖ for complex ones [12]. Define also the indistinguishable global phase constant in real-valued settings as\nφ(z) := { 0, ‖z − x‖ ≤ ‖z + x‖, π, otherwise.\n(4)\nHenceforth, fixing x to be any solution of the given quadratic system (1), we always assume that φ (z) = 0; otherwise, z is replaced by e−jφ(z)z, but for simplicity of presentation, the constant phase adaptation term e−jφ(z) is dropped whenever it is clear from the context.\nNumerical tests comparing TGGF, TWF, and WF will be presented throughout our analysis, so let us first describe our basic test settings. Simulated estimates will be averaged over 100 independent Monte Carlo (MC) realizations without mentioning this explicitly each time. Performance is evaluated in terms of the relative root mean-square error, i.e., Relative error := dist(z, x)/‖x‖, and the success rate among 100 trials, where a success will be claimed for a trial if the resulting estimate incurs relative error less than 10−5 [8]. Simulated tests under both noiseless and noisy Gaussian models are performed, corresponding to ψi =\n∣∣aHi x+ ηi∣∣ with ηi = 0 and ηi ∼ N (0, σ2) [11], respectively, with i.i.d. ai ∼ N (0, In) or ai ∼ CN (0, In)."
    }, {
      "heading" : "2.1 Truncated generalized gradient stage",
      "text" : "Let us rewrite the amplitude-based cost function in a matrix-vector form as\nmin z∈Rn\n`(z) = 1\n2m ∥∥ψ − |Az|∥∥2 (5) where |Az| := [ |aT1 z| · · · |aTmz| ]T . Apart from being nonconvex, `(z) is nondiffentiable. In the presence of smoothness or convexity, convergence analysis of iterative algorithms relies either on continuity of the gradient (gradient methods) [20], or, on the convexity of the objective functional (subgradient methods) [20]. Although subgradient methods have found widespread applicability in nonsmooth optimization, they are limited to the class of convex functions [20, Page 4]. In nonconvex nonsmooth optimization, the so-termed generalized gradient broadens the scope of the (sub)gradient to the class of almost everywhere differentiable functions [21]. Consider a continuous function h(z) ∈ R defined over an open region S ⊆ Rn.\nDefinition 1 [22, Definition 1.1] The generalized gradient of a function h at z, denoted by ∂h, is the convex hull of the set of limits of the form lim∇h(zk), where zk → z as k → +∞, i.e.,\n∂h(z) := conv {\nlim k→+∞\n∇h(zk) : zk → z, zk /∈ G` }\nwhere the symbol ‘conv’ signifies the convex hull of a set, and G` denotes the set of points in S at which h fails to be differentiable.\nHaving introduced the notion of generalized gradient, and with t denoting the iteration number, our approach to solving (5) amounts to iteratively refining the initial guess z0 by means of the ensuing truncated generalized gradient iterations\nzt+1 = zt − µt∂`tr(zt) (6) where µt > 0 is the stepsize, and a piece of the (truncated) generalized gradient ∂`tr(zt) is given by\n∂`tr(zt) := ∑ i∈It+1 ( aTi zt − ψi aTi zt |aTi zt| ) ai (7)\nfor some index set It+1 ⊆ [m] to be designed shortly; and the convention a T i zt |aTi zt| := 0 is adopted, if aTi zt = 0. Further, it is easy to verify that the update in (6) monotonically decreases the objective value in (5).\nRecall that since they offer descent iterations, the alternating projection variants are guaranteed to converge to a stationary point of `(z), and any limit point z∗ adheres to the following fixed-point equation [23]\nAT ( Az∗ −ψ Az ∗\n|Az∗|\n) = 0 (8)\nfor entry-wise product , which may have many solutions. Clearly, if z∗ is a solution, so is −z∗. Further, both solutions/global minimizers x and −x satisfy (8) due to Ax − ψ Ax|Ax| = 0. Considering any stationary point z∗ 6= ±x that has been adapted such that φ(z∗) = 0, one can write z∗ = x+(ATA)−1AT [ ψ ( Az∗\n|Az∗|− Ax |Ax|\n)] . A necessary\ncondition for z∗ 6= x is Az ∗\n|Az∗| 6= Ax |Ax| . Expressed dif-\nferently, there must be sign differences between Az ∗ |Az∗| and Ax|Ax| whenever one gets stuck with an undesirable\nstationary point z∗. Building on this observation, it is reasonable to devise algorithms that can detect and separate out the generalized gradient components corresponding to mistakenly estimated signs aTi zt |aTi zt| along the iterates {zt}. Precisely, if zt and x lie in different sides of the hyperplane aTi z = 0, then the sign of aTi zt will be different than that of a T i x; that is, aTi x\n|aTi x| 6= a\nT i z\n|aTi z| . Specifically, one\ncan write the i-th generalized gradient component ∂`i(z) = ( aTi z − ψi aTi z\n|aTi z|\n) ai = ( aTi z − ψi aTi x\n|aTi x|\n) ai + ( aTi x |aTi x| − a T i z |aTi z| ) ψiai\n= aia T i h+ ( aTi x |aTi x| − a T i z |aTi z| ) ψiai 4 = aia T i h+ ri (9)\nwhere h := z − x. Apparently, the strong law of large numbers (SLLN) asserts that averaging the first term aiaTi h over m instances approaches h, which qualifies it as a desirable search direction. However, certain generalized gradient entries involve erroneously estimated signs of aTi x; hence, nonzero ri terms exert a negative influence on the search direction h by dragging the iterate away from x, and they typically have sizable magnitudes. To see why, recall that quantities maxi∈[m] ψi and (1/m) ∑m i=1 ψi have magnitudes on the order of √ m‖x‖ and √ π/2‖x‖, respectively, whereas ‖h‖ ≤ ρ‖x‖ for some small constant 0 < ρ ≤ 1/10, to be discussed shortly. To maintain a meaningful search direction, those ‘bad’ generalized gradient entries should be detected and excluded from the search direction.\nNevertheless, it is difficult or even impossible to check whether the sign of aTi zt equals that of aTi x. Fortunately, when the initialization is accurate enough, most spurious gradient entries (those corrupted by nonzero ri terms) provably hover around the watershed hyperplane aTi zt = 0. For this reason, TGGF includes only those components having zt sufficiently away from its watershed\nIt+1 := { 1 ≤ i ≤ m ∣∣∣ |aTi zt||aTi x| ≥ 11 + γ } , t ≥ 0 (10)\nfor an appropriately selected threshold γ > 0. It is worth stressing that our novel truncation rule deviates from the intuition behind TWF. Among its complicated truncation procedures, TWF also throws away large-size gradient components corresponding to (10), which is not the case with TGGF. As demonstrated by our analysis, it rarely happens that a generalized gradient component having a large |aTi zt|/ ‖zt‖ yields an incorrect sign of aTi x. Further, discarding too many samples (those i /∈ Tt+1) introduces large bias into (1/m) ∑m i∈Tt+1 aia T i ht, thus rendering TWF less effective when m/n is small. Numerical comparison depicted in Fig. 1 suggests that even starting with the same truncated spectral initialization, TGGF’s refinement outperforms those of TWF and WF, corroborating the merits of our novel truncation and update rule over TWF/WF.\n2.2 Orthogonality-promoting initialization stage\nLeveraging the SLLN, spectral methods estimate x using the (appropriately scaled) leading eigenvector of Y := 1m ∑ i∈T0 yiaia T i , where T0 is an index set accounting for possible truncation. As asserted in [8], each summand (aTi x) 2aia T i follows a heavytail probability density function lacking a moment generating function. This causes major performance degradation especially when the number of measurements is limited. Instead of spectral initialization, we shall take another route to bypass this hurdle. To gain intuition for selecting our alternate route, a motivating example is presented first that reveals fundamental characteristics among high-dimensional random vectors.\nExample: Fixing any nonzero vector x ∈ Rn, generate data ψi = |〈ai,x〉| using i.i.d. ai ∼ N (0, In), ∀i ∈ [m], and evaluate the squared normalized innerproduct\ncos2 θi := |〈ai,x〉|2\n‖ai‖2‖x‖2 = ψ2i ‖ai‖2‖x‖2 , ∀i ∈ [m] (11)\nwhere θi is the angle between ai and x. Consider ordering all cos2 θi’s in an ascending fashion, and collectively denote them as ξ := [ cos2 θ[m] · · · cos2 θ[1] ]T with cos2 θ[1] ≥ · · · ≥ cos2 θ[m]. Fig. 2 plots the ordered entries in ξ for m/n varying by 2 from 2 to 10 with n = 103. Observe that almost all {ai} vectors have a squared normalized inner-product smaller than 10−2, while half of the inner-products are less than 10−3, which implies that x is nearly orthogonal to many ai’s.\nThis example corroborates that random vectors in high-dimensional spaces are almost always nearly orthogonal to each other [24]. This inspired us to pursue an orthogonality-promoting initialization method. Our key idea is to approximate x by a vector that is most orthogonal to a subset of vectors {ai}i∈I0 , where I0 is a set with cardinality |I0| < m that includes indices of the smallest squared normalized inner-products { cos2 θi } . Since ‖x‖ appears in all inner-products, its exact value does not influence their ordering. Henceforth, we assume without loss of generality that ‖x‖ = 1. Using {(ai; ψi)}, evaluate cos2 θi according to (11) for each pair x and ai. Instrumental for the ensuing derivations is noticing that the summation of cos2 θi over indices i ∈ I0 is very small, while rigorous justification is deferred to Section 3 and supplementary materials. Thus, a meaningful approximation denoted by z0 ∈ Rn can be obtained by solving\nmin ‖z‖=1\nzT\n( 1\n|I0| ∑ i∈I0 aia T i ‖ai‖2\n) z (12)\nwhich amounts to finding the smallest eigenvalue and the associated eigenvector of 1|I0| ∑ i∈I0 aia T i\n‖ai‖2 . Yet finding the smallest eigenvalue calls for eigen-decomposition or matrix inversion, each requiring computational complexity O(n3). Such a computational burden can be intractable when n grows large. Applying a standard concentration result simplifies greatly those computations next [25].\nSince ai/‖ai‖ has unit norm and is uniformly distributed on the unit sphere, it is uniformly spherically distributed.2 Spherical symmetry implies that ai/‖ai‖ has zero mean and covariance matrix In/n [25]. Appealing again to the SLLN, the sample covariance matrix 1m ∑m i=1 aia T i\n‖ai‖2 approaches 1 nIn as m grows. Simple derivations lead to ∑ i∈I0 aia T i ‖ai‖2 = ∑m i=1 aia T i ‖ai‖2 − ∑ i∈I0 aia T i\n‖ai‖2 u m n In − ∑ i∈I0 aia T i ‖ai‖2 , where I0 is the complement of I0 in the set [m].\nDefine S := [a1/‖a1‖ · · · am/‖am‖]T ∈ Rm×n, and form S0 by removing the rows of S if their indices do not belong to I0. The task of seeking the smallest eigenvalue of Y0 := 1|I0|S T 0 S0 reduces to computing the largest eigenvalue of Y0 := 1|I0| ST0 S0, namely,\nz̃0 := arg max ‖z‖=1\nzT Y0z (13)\nwhich can be efficiently solved using simple power iterations. If, on the other hand, ‖x‖ 6= 1, the estimate z̃0 from (13) is further scaled so that its norm matches approximately that of x (which is estimated to be 1m ∑m i=1 yi), thus yielding z0 = √∑m i=1 yi/mz̃0. It is worth stressing that the constructed matrix Y0 does not depend on {yi} explicitly, saving our initialization from suffering heavy-tails of the fourth order of {ai} in spectral initialization schemes.\nFig. 3 compares three initialization schemes showing their relative errors versus the measurement/unknown ratio m/n under the noise-free real Gaussian model, where x ∈ R1,000 and m/n increases by 2 from 2 to 20. Apparently, all schemes enjoy improved performance as m/n increases. In particular, the proposed initialization method outperforms its spectral alternatives. Interestingly, the spectral and truncated spectral schemes exhibit similar performance when m/n is sufficiently large (e.g., m/n ≥ 14). This confirms that truncation helps only if m/n is relatively small. Indeed, truncation is effected by discarding measurements of excessively large sizes emerging from the heavy tails of the data distribution. Hence, its advantage over the untruncated one narrows as the number of measurements increases, thus straightening out the heavy tails. On the contrary, the orthogonalitypromoting initialization method achieves consistently superior performance over its spectral alternatives."
    }, {
      "heading" : "3 Main results",
      "text" : "TGGF is summarized in Algorithm 1 with default values set for pertinent algorithmic parameters. Postulating independent samples {(ai; ψi)}, the following result establishes the performance of our TGGF approach.\n2A random vector z ∈ Rn is said to be spherical (or spherically symmetric) if its distribution does not change under rotations of the coordinate system; that is, the distribution of Pz coincides with that of z for any given orthogonal n× n matrix P .\nAlgorithm 1 Truncated generalized gradient flow (TGGF) solvers 1: Input: Data {ψi}mi=1 and feature vectors {ai}mi=1; the maximum number of iterations T =\n1, 000; by default, take constant step size µ = 0.6/1 for real/complex Gaussian models, truncation thresholds |I0| = d 16me (d·e the ceil operation), and γ = 0.7. 2: Evaluate ψi/‖ai‖, ∀i ∈ [m], and find I0 comprising indices corresponding to the |I0| largest (ψi/‖ai‖)’s.\n3: Initialize z0 to √∑m i=1 ψ 2 i /mz̃0, where z̃0 is the unit leading eigenvector of Y0 :=\n1\n|I0|\n∑ i∈I0 aia T i ‖ai‖2 .\n4: Loop: for t = 0 to T − 1 zt+1 = zt − µ\nm ∑ i∈It+1 ( aTi zt − ψi aTi zt |aTi zt| ) ai\nwhere It+1 := { 1 ≤ i ≤ m ∣∣|aTi zt| ≥ 11+γψi}. 5: Output: zT\nTheorem 1 Let x ∈ Rn be an arbitrary signal vector, and consider (noise-free) measurements ψi = |aTi x|, in which ai\ni.i.d.∼ N (0, In), 1 ≤ i ≤ m. Then with probability at least 1 − (m + 5)e−n/2 − e−c0m − 1/n2 for some universal constant c0 > 0, the initialization z0 returned by the orthogonality-promoting method in Algorithm 1 satisfies\ndist(z0,x) ≤ ρ ‖x‖ (14)\nwith ρ = 1/10 (or any sufficiently small positive constant), provided that m ≥ c1|I0| ≥ c2n for some numerical constants c1, c2 > 0, and sufficiently large n. Further, choosing a constant step size µ ≤ µ0 along with a fixed truncation level γ ≥ 1/2, and starting from any initial guess z0 satisfying (14), successive estimates of the TGGF solver (tabulated in Algorithm 1) obey\ndist (zt,x) ≤ ρ (1− ν)t ‖x‖ , t = 0, 1, . . . (15)\nfor some 0 < ν < 1, which holds with probability exceeding 1− (m+ 5)e−n/2 − 8e−c0m − 1/n2. Typical parameters are µ = 0.6, and γ = 0.7.\nTheorem 1 asserts that: i) TGGF recovers the solution x exactly as soon as the number of equations is about the number of unknowns, which is theoretically order optimal. Our numerical tests demonstrate that for the real Gaussian model, TGGF achieves a success rate of 100% when m/n is as small as 3, which is slightly larger than the information limit of m/n = 2 (Recall that m ≥ 2n− 1 is necessary for a unique solution); this is a significant reduction in the sample complexity ratio, which is 5 for TWF and 7 for WF. Surprisingly, TGGF enjoys also a success rate of over 50% whenm/n is 2, which has not yet been presented for any existing algorithm under Gaussian sampling models and thus, our TGGF bridges the gap; see further discussion in Section 4; and, ii) TGGF converges exponentially fast. Specifically, TGGF requires at most O(log(1/ )) iterations to achieve any given solution accuracy > 0 (a.k.a., dist(zt,x) ≤ ‖x‖), with iteration cost O(mn). Since truncation takes time on the order of O(m), the computational burden of TGGF per iteration is dominated by evaluating the generalized gradients. The latter involves two matrix-vector multiplications that are computable in O(mn) flops, namely,Azt yields ut, andAT vt the generalized gradient, where vt := ut−ψ ut|ut| . Hence, the total running time of TGGF is O(mn log(1/ )), which is proportional to the time taken to read the data O(mn). The proof of Theorem 1 can be found in the supplementary material."
    }, {
      "heading" : "4 Simulated tests and conclusions",
      "text" : "Additional numerical tests evaluating performance of the proposed scheme relative to TWF/WF are presented in this section. For fairness, all pertinent algorithmic parameters involved in each scheme are set to their default values. The Matlab implementations of TGGF are available at http://www.tc.umn.edu/˜gangwang/TAF. The initial estimate was found based on 50 power iterations, and was subsequently refined by T = 103 gradient-like iterations in each scheme. Left panel in Fig. 4 presents average relative error of three initialization methods on a series of noiseless/noisy real Gaussian problems with m/n = 6 fixed, and n varying from 500 to 104,\nwhile those for the corresponding complex Gaussian instances are shown in the right panel. Fig. 5 compares empirical success rate of three schemes under both real and complex Gaussian models with n = 103 and m/n varying by 1 from 1 to 7. Apparently, the proposed initialization method returns more accurate and robust estimates than the spectral ones. Moreover, for real-valued vectors, TGGF achieves a success rate of over 50% when m/n = 2, and guarantees perfect recovery from about 3n measurements; while for complex-valued ones, TGGF enjoys a success rate of 95% when m/n = 3.4, and ensures perfect recovery from about 4.5n measurements. Regarding running times, TGGF converges slightly faster than TWF, while both are markedly faster than WF. Curves in Fig. 5 clearly corroborate the merits of TGGF over Wirtinger alternatives.\nThis paper developed a linear-time algorithm termed TGGF for solving random systems of quadratic equations. TGGF builds on three key ingredients: a novel orthogonality-promoting initialization, along with a simple yet effective truncation rule, as well as simple scalable gradient-like iterations. Numerical tests corroborate the superior performance of TGGF over state-of-the-art solvers."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Work in this paper was supported in part by NSF grants 1500713 and 1514056."
    } ],
    "references" : [ {
      "title" : "On signal reconstruction without phase",
      "author" : [ "R. Balan", "P. Casazza", "D. Edidin" ],
      "venue" : "Appl. Comput. Harmon. Anal., vol. 20, no. 3, pp. 345–356, May 2006.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An algebraic characterization of injectivity in phase retrieval",
      "author" : [ "A. Conca", "D. Edidin", "M. Hering", "C. Vinzant" ],
      "venue" : "Appl. Comput. Harmon. Anal., vol. 38, no. 2, pp. 346–356, Mar. 2015.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Quadratic programming with one negative eigenvalue is NP-hard",
      "author" : [ "P.M. Pardalos", "S.A. Vavasis" ],
      "venue" : "J. Global Optim., vol. 1, no. 1, pp. 15–22, 1991.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "The phase problem of X-ray crystallography",
      "author" : [ "H.A. Hauptman" ],
      "venue" : "Rep. Prog. Phys., vol. 54, no. 11, p. 1427, 1991.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Phase retrieval via matrix completion",
      "author" : [ "E.J. Candès", "Y.C. Eldar", "T. Strohmer", "V. Voroninski" ],
      "venue" : "SIAM Rev., vol. 57, no. 2, pp. 225–251, May 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On phase retrieval of finite-length sequences using the initial time sample",
      "author" : [ "H. Sahinoglou", "S.D. Cabrera" ],
      "venue" : "IEEE Trans. Circuits and Syst., vol. 38, no. 8, pp. 954–958, Aug. 1991.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Some NP-complete problems in quadratic and nonlinear programming",
      "author" : [ "K.G. Murty", "S.N. Kabadi" ],
      "venue" : "Math. Prog., vol. 39, no. 2, pp. 117–129, 1987.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Solving random quadratic systems of equations is nearly as easy as solving linear systems",
      "author" : [ "Y. Chen", "E.J. Candès" ],
      "venue" : "Comm. Pure Appl. Math., 2016 (to appear).",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A practical algorithm for the determination of phase from image and diffraction",
      "author" : [ "R.W. Gerchberg", "W.O. Saxton" ],
      "venue" : "Optik, vol. 35, pp. 237–246, Nov. 1972.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Phase retrieval algorithms: A comparison",
      "author" : [ "J. Fienup" ],
      "venue" : "Appl. Opt., vol. 21, no. 15, pp. 2758–2769, 1982.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Phase retrieval using alternating minimization",
      "author" : [ "P. Netrapalli", "P. Jain", "S. Sanghavi" ],
      "venue" : "IEEE Trans. Signal Process., vol. 63, no. 18, pp. 4814–4826, Sept. 2015.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Phase retrieval via Wirtinger flow: Theory and algorithms",
      "author" : [ "E.J. Candès", "X. Li", "M. Soltanolkotabi" ],
      "venue" : "IEEE Trans. Inf. Theory, vol. 61, no. 4, pp. 1985–2007, Apr. 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "A geometric analysis of phase retrieval",
      "author" : [ "J. Sun", "Q. Qu", "J. Wright" ],
      "venue" : "arXiv:1602.06664, 2016.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "PhaseLift: Exact and stable signal recovery from magnitude measurements via convex programming",
      "author" : [ "E.J. Candès", "T. Strohmer", "V. Voroninski" ],
      "venue" : "Appl. Comput. Harmon. Anal., vol. 66, no. 8, pp. 1241–1274, Nov. 2013.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Phase recovery, maxcut and complex semidefinite programming",
      "author" : [ "I. Waldspurger", "A. d’Aspremont", "S. Mallat" ],
      "venue" : "Math. Prog., vol. 149, no. 1-2, pp. 47–81, 2015.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Solving quadratic equations via PhaseLift when there are about as many equations as unknowns",
      "author" : [ "E.J. Candès", "X. Li" ],
      "venue" : "Found. Comput. Math., vol. 14, no. 5, pp. 1017–1026, 2014.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Online reconstruction from big data via compressive censoring",
      "author" : [ "G. Wang", "D. Berberidis", "V. Kekatos", "G.B. Giannakis" ],
      "venue" : "IEEE Global Conf. Signal and Inf. Process., Atlanta, GA, 2014, pp. 326–330.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adaptive censoring for large-scale regressions",
      "author" : [ "D.K. Berberidis", "V. Kekatos", "G. Wang", "G.B. Giannakis" ],
      "venue" : "IEEE Intl. Conf. Acoustics, Speech and Signal Process., South Brisbane, QLD, Australia, 2015, pp. 5475–5479.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Experimental robustness of Fourier ptychography phase retrieval algorithms",
      "author" : [ "L.-H. Yeh", "J. Dong", "J. Zhong", "L. Tian", "M. Chen", "G. Tang", "M. Soltanolkotabi", "L. Waller" ],
      "venue" : "Opt. Express, vol. 23, no. 26, pp. 33 214– 33 240, Dec. 2015.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Ruszcayǹski, Minimization Methods for Non-differentiable Functions",
      "author" : [ "N.Z. Shor", "K.C. Kiwiel" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1985
    }, {
      "title" : "Generalized gradients and applications",
      "author" : [ "——" ],
      "venue" : "T. Am. Math. Soc., vol. 205, pp. 247–262, 1975.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Phase retrieval with one or two diffraction patterns by alternating projections of the null vector",
      "author" : [ "P. Chen", "A. Fannjiang", "G.-R. Liu" ],
      "venue" : "arXiv:1510.07379v2, 2015.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distributions of angles in random packing on spheres",
      "author" : [ "T. Cai", "J. Fan", "T. Jiang" ],
      "venue" : "J. Mach. Learn. Res., vol. 14, no. 1, pp. 1837–1864, Jan. 2013.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1837
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "arXiv:1011.3027, 2010. 9",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Indeed, it has been established thatm ≥ 2n−1 orm ≥ 4n−4 generic data {(ai; yi)}mi=1 as in (1) suffice for uniqueness of an n-dimensional real- or complex-valued vector x [1, 2], respectively, and the former with equality has also been shown to be necessary [1].",
      "startOffset" : 170,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "Indeed, it has been established thatm ≥ 2n−1 orm ≥ 4n−4 generic data {(ai; yi)}mi=1 as in (1) suffice for uniqueness of an n-dimensional real- or complex-valued vector x [1, 2], respectively, and the former with equality has also been shown to be necessary [1].",
      "startOffset" : 170,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "Indeed, it has been established thatm ≥ 2n−1 orm ≥ 4n−4 generic data {(ai; yi)}mi=1 as in (1) suffice for uniqueness of an n-dimensional real- or complex-valued vector x [1, 2], respectively, and the former with equality has also been shown to be necessary [1].",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 2,
      "context" : "The problem in (1) constitutes an instance of nonconvex quadratic programming, that is generally known to be NP-hard [3].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "In many fields of physical sciences and engineering, the problem of recovering the phase from intensity/magnitude-only measurements is commonly referred to as phase retrieval [4, 5].",
      "startOffset" : 175,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "In many fields of physical sciences and engineering, the problem of recovering the phase from intensity/magnitude-only measurements is commonly referred to as phase retrieval [4, 5].",
      "startOffset" : 175,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "The plethora of applications include X-ray crystallography, optics, as well as array imaging, where due to physical limitations, optical detectors can record only (squared) modulus of the Fresnel or Fraunhofer diffraction pattern, while losing the phase of the incident light reaching the object [5].",
      "startOffset" : 296,
      "endOffset" : 299
    }, {
      "referenceID" : 5,
      "context" : "It has been shown that reconstructing a discrete, finite-duration signal from its Fourier transform magnitude is NP-complete [6].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "Minimizing nonconvex objectives, which may exhibit many stationary points, is in general NP-hard [7].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : ", [8]) or (3) fall under two categories: nonconvex and convex ones.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 8,
      "context" : "Popular nonconvex solvers include the alternating projection such as Gerchberg-Saxton [9] and Fineup [10], AltMinPhase [11], and (Truncated) Wirtinger flow (WF/TWF) [12, 8], as well as trust-region methods [13].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "Popular nonconvex solvers include the alternating projection such as Gerchberg-Saxton [9] and Fineup [10], AltMinPhase [11], and (Truncated) Wirtinger flow (WF/TWF) [12, 8], as well as trust-region methods [13].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "Popular nonconvex solvers include the alternating projection such as Gerchberg-Saxton [9] and Fineup [10], AltMinPhase [11], and (Truncated) Wirtinger flow (WF/TWF) [12, 8], as well as trust-region methods [13].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "Popular nonconvex solvers include the alternating projection such as Gerchberg-Saxton [9] and Fineup [10], AltMinPhase [11], and (Truncated) Wirtinger flow (WF/TWF) [12, 8], as well as trust-region methods [13].",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "Popular nonconvex solvers include the alternating projection such as Gerchberg-Saxton [9] and Fineup [10], AltMinPhase [11], and (Truncated) Wirtinger flow (WF/TWF) [12, 8], as well as trust-region methods [13].",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 12,
      "context" : "Popular nonconvex solvers include the alternating projection such as Gerchberg-Saxton [9] and Fineup [10], AltMinPhase [11], and (Truncated) Wirtinger flow (WF/TWF) [12, 8], as well as trust-region methods [13].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 13,
      "context" : "Convex approaches on the other hand rely on the so-called matrix-lifting technique to obtain the solvers abbreviated as PhaseLift [14] and PhaseCut [15].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "Convex approaches on the other hand rely on the so-called matrix-lifting technique to obtain the solvers abbreviated as PhaseLift [14] and PhaseCut [15].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "In terms of sample complexity for Gaussian {ai} designs, convex approaches enable exact recovery from1 O(n) noiseless measurements [16], while they require solving a semidefinite program of a matrix variable with size n× n, thus incurring worst-case computational complexity on the order of O(n) [15], that does not scale well with dimensionality n.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "In terms of sample complexity for Gaussian {ai} designs, convex approaches enable exact recovery from1 O(n) noiseless measurements [16], while they require solving a semidefinite program of a matrix variable with size n× n, thus incurring worst-case computational complexity on the order of O(n) [15], that does not scale well with dimensionality n.",
      "startOffset" : 296,
      "endOffset" : 300
    }, {
      "referenceID" : 14,
      "context" : "Upon exploiting the underlying problem structure,O(n) can be reduced toO(n(3)) [15].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "Using formulation (3), AltMinPhase adopts a spectral initialization and establishes exact recovery with sample complexity O(n log(3) n) under Gaussian {ai} designs with resampling [11].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 11,
      "context" : "Concerning formulation (2), WF iteratively refines the spectral initial estimate by means of a gradient-like update [12].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Likewise, at the initialization stage, since the term (ai x) (2)aia H i responsible for the spectral initialization is heavy-tailed, data {yi}i=1 are pre-screened in the truncated spectral initialization to yield improved initial estimates [8].",
      "startOffset" : 240,
      "endOffset" : 243
    }, {
      "referenceID" : 11,
      "context" : "Under Gaussian sampling models, WF allows exact recovery from O(n log n) measurements in O(mn(2) log(1/ )) time/flops to yield an -accurate solution for any given > 0 [12], while TWF advances these to O(n) measurements and O(mn log(1/ )) time [8].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 7,
      "context" : "Under Gaussian sampling models, WF allows exact recovery from O(n log n) measurements in O(mn(2) log(1/ )) time/flops to yield an -accurate solution for any given > 0 [12], while TWF advances these to O(n) measurements and O(mn log(1/ )) time [8].",
      "startOffset" : 243,
      "endOffset" : 246
    }, {
      "referenceID" : 16,
      "context" : "Although for large-scale linear regressions, similar ideas including censoring have been studied [17, 18].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "Although for large-scale linear regressions, similar ideas including censoring have been studied [17, 18].",
      "startOffset" : 97,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : ", trust-region methods) to efficiently find a global minimizer with random initializations [13].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : ", ≥ 99%) under the real Gaussian model [8, Section 3], which are more than twice the known information-limit of m = 2n − 1 [1].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "Further, it is worth stressing that extensive numerical and experimental validation confirms that the amplitude-based cost function performs better than the intensity-based one; that is, formulation (3) is superior over (2) [19].",
      "startOffset" : 224,
      "endOffset" : 228
    }, {
      "referenceID" : 11,
      "context" : "To start, define the Euclidean distance of any estimate z to the solution set: dist(z, x) := min ‖z ± x‖ for real signals, and dist(z, x) := minφ∈[0,2π) ‖z−xe‖ for complex ones [12].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : ", Relative error := dist(z, x)/‖x‖, and the success rate among 100 trials, where a success will be claimed for a trial if the resulting estimate incurs relative error less than 10−5 [8].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 10,
      "context" : "Simulated tests under both noiseless and noisy Gaussian models are performed, corresponding to ψi = ∣∣aHi x+ ηi∣∣ with ηi = 0 and ηi ∼ N (0, σ(2)) [11], respectively, with i.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 19,
      "context" : "In the presence of smoothness or convexity, convergence analysis of iterative algorithms relies either on continuity of the gradient (gradient methods) [20], or, on the convexity of the objective functional (subgradient methods) [20].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "In the presence of smoothness or convexity, convergence analysis of iterative algorithms relies either on continuity of the gradient (gradient methods) [20], or, on the convexity of the objective functional (subgradient methods) [20].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 21,
      "context" : "Recall that since they offer descent iterations, the alternating projection variants are guaranteed to converge to a stationary point of `(z), and any limit point z∗ adheres to the following fixed-point equation [23]",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 7,
      "context" : "As asserted in [8], each summand (ai x) (2)aia T i follows a heavytail probability density function lacking a moment generating function.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Consider ordering all cos(2) θi’s in an ascending fashion, and collectively denote them as ξ := [ cos(2) θ[m] · · · cos(2) θ[1] ]T with cos(2) θ[1] ≥ · · · ≥ cos(2) θ[m].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "Consider ordering all cos(2) θi’s in an ascending fashion, and collectively denote them as ξ := [ cos(2) θ[m] · · · cos(2) θ[1] ]T with cos(2) θ[1] ≥ · · · ≥ cos(2) θ[m].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : "This example corroborates that random vectors in high-dimensional spaces are almost always nearly orthogonal to each other [24].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 23,
      "context" : "Applying a standard concentration result simplifies greatly those computations next [25].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "2 Spherical symmetry implies that ai/‖ai‖ has zero mean and covariance matrix In/n [25].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Figure 4: The average relative error using: i) the spectral method [11, 12]; ii) the truncated spectral method [8]; and iii) the proposed orthogonality-promoting method on noise-free (solid) and noisy (dotted) instances with m/n = 6, and n varying from 500/100 to 10, 000/5, 000 for real/complex vectors.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "Figure 4: The average relative error using: i) the spectral method [11, 12]; ii) the truncated spectral method [8]; and iii) the proposed orthogonality-promoting method on noise-free (solid) and noisy (dotted) instances with m/n = 6, and n varying from 500/100 to 10, 000/5, 000 for real/complex vectors.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "Figure 4: The average relative error using: i) the spectral method [11, 12]; ii) the truncated spectral method [8]; and iii) the proposed orthogonality-promoting method on noise-free (solid) and noisy (dotted) instances with m/n = 6, and n varying from 500/100 to 10, 000/5, 000 for real/complex vectors.",
      "startOffset" : 111,
      "endOffset" : 114
    } ],
    "year" : 2016,
    "abstractText" : "This paper puts forth a novel algorithm, termed truncated generalized gradient flow (TGGF), to solve for x ∈ R/C a system of m quadratic equations yi = |〈ai,x〉|, i = 1, 2, . . . ,m, which even for {ai ∈ R/C}i=1 random is known to be NP-hard in general. We prove that as soon as the number of equations m is on the order of the number of unknowns n, TGGF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data {(ai; yi)}i=1. Specifically, TGGF proceeds in two stages: s1) A novel orthogonality-promoting initialization that is obtained with simple power iterations; and, s2) a refinement of the initial estimate by successive updates of scalable truncated generalized gradient iterations. The former is in sharp contrast to the existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. Empirical results demonstrate that: i) The novel orthogonalitypromoting initialization method returns more accurate and robust estimates relative to its spectral counterparts; and, ii) even with the same initialization, our refinement/truncation outperforms Wirtinger-based alternatives, all corroborating the superior performance of TGGF over state-of-the-art algorithms.",
    "creator" : null
  }
}