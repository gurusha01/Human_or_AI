{
  "name" : "7c82fab8c8f89124e2ce92984e04fb40.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Selective inference for group-sparse linear models",
    "authors" : [ "Fan Yang", "Rina Foygel Barber" ],
    "emails" : [ "fyang1@uchicago.edu", "rina@uchicago.edu", "prajain@microsoft.com", "lafferty@galton.uchicago.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Significant progress has been recently made on developing inference tools to complement the feature selection methods that have been intensively studied in the past decade [6, 5, 9]. The goal of selective inference is to make accurate uncertainty assessments for the parameters estimated using a feature selection algorithm, such as the lasso [12]. The fundamental challenge is that after the data have been used to select a set of coefficients to be studied, this selection event must then be accounted for when performing inference, using the same data. A specific goal of selective inference is to provide p-values and confidence intervals for the fitted coefficients. As the sparsity pattern is chosen using nonlinear estimators, the distribution of the estimated coefficients is typically non-Gaussian and multimodal, even under a standard Gaussian noise model, making classical techniques unusable for accurate inference. It is of particular interest to develop finite-sample, non-asymptotic results.\nIn this paper, we present new results for selective inference in the setting of group sparsity [15, 3, 10]. We consider the linear model Y = Xβ +N (0, σ2In) where X ∈ Rn×p is a fixed design matrix. In many applications, the p columns or features of X are naturally grouped into blocks C1, . . . , CG ⊆ {1, . . . , p}. In the high dimensional setting, the working assumption is that only a few of the corresponding blocks of the coefficients β contain nonzero elements; that is, βCg = 0 for most groups g. This group-sparse model can be viewed as an extension of the standard sparse regression model. Algorithms for fitting this model, such as the group lasso [15], extend well-studied methods for sparse linear regression to this grouped setting.\nIn the group-sparse setting, recent results of Loftus and Taylor [9] give a selective inference method for computing p-values for each group chosen by a model selection method such as forward stepwise regression; selection via cross-validation was studied in [9]. More generally, the inference technique of [7] applies to any model selection method whose outcome can be described in terms of quadratic\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nconditions on Y . However, their technique cannot be used to construct confidence intervals for the selected coefficients or for the size of the effects of the selected groups.\nOur main contribution in this work is to provide a tool for constructing confidence intervals as well as p-values for testing selected groups. In contrast to the (non-grouped) sparse regression setting, the confidence interval construction does not follow immediately from the p-value calculation, and requires a careful analysis of non-centered multivariate normal distributions. Our key technical result precisely characterizes the density of ‖PLY ‖2 (the magnitude of the projection of Y onto a given subspace L), conditioned on a particular selection event. This “truncated projection lemma” is the group-wise analogue of the “polyhedral lemma” of Lee et al. [5] for the lasso. This technical result enables us to develop inference tools for a broad class of model selection methods, including the group lasso [15], iterative hard thresholding [1, 4], and forward stepwise group selection [14].\nIn the following section we frame the problem of group-sparse inference more precisely, and present previous results in this direction. We then state our main technical results; the proofs of the results are given in the supplementary material. In Section 3 we show how these results can be used to develop inferential tools for three different model selection algorithms for group sparsity. In Section 4 we give numerical results to illustrate these tools on simulated data, as well as on the California county health data used in previous work [9]. We conclude with a brief discussion of our work."
    }, {
      "heading" : "2 Main results: selective inference over subspaces",
      "text" : "To establish some notation, we will write PL for the projection to any linear subspace L ⊆ Rn, and P⊥L for the projection to its orthogonal complement. For y ∈ Rn, dirL(y) = PLy ‖PLy‖2 ∈ L ∩ S\nn−1 is the unit vector in the direction of PLy. This direction is not defined if PLy = 0. We focus on the linear model Y = Xβ + N (0, σ2In), where X ∈ Rn×p is fixed and σ2 > 0 is assumed to be known. More generally, our model is Y ∼ N (µ, σ2In) with µ ∈ Rn unknown and σ2 known. For a given block of variables Cg ⊆ [p], we write Xg to denote the n× |Cg| submatrix of X consisting of all features of this block. For a set S ⊆ [G] of blocks, XS consists of all features that lie in any of the blocks in S . When we refer to “selective inference,” we are generally interested in the distribution of subsets of parameters that have been chosen by some model selection procedure. After choosing a set of groups S ⊆ [G], we would like to test whether the true mean µ is correlated with a group Xg for each g ∈ S after controlling for the remaining selected groups, i.e. after regressing out all the other groups, indexed by S\\g. Thus, the following question is central to selective inference:\nQuestiong,S : What is the magnitude of the projection of µ onto the span of P⊥XS\\gXg? (1)\nIn particular, we are interested in a hypothesis test to determine if µ is orthogonal to this span, that is, whether block g should be removed from the model with group-sparse support determined by S; this is the question studied by Loftus and Taylor [9] for which they compute p-values. Alternatively, we may be interested in a confidence interval on ‖PLµ‖2, where L = span(P⊥XS\\gXg). Since S and g are themselves determined by the data Y , any inference on these questions must be performed “post-selection,” by conditioning on the event that S is the selected set of groups."
    }, {
      "heading" : "2.1 Background: The polyhedral lemma",
      "text" : "In the more standard sparse regression setting without grouped variables, after selecting a set S ⊆ [p] of features corresponding to columns of X, we might be interested in testing whether the column Xj should be included in the model obtained by regressing Y onto XS\\j . We may want to test the null hypothesis that X>j P⊥XS\\jµ is zero, or to construct a confidence interval for this inner product.\nIn the setting where S is the output of the lasso, Lee et al. [5] and Tibshirani et al. [13] characterize the selection event as a polyhedron in Rn: for any set S ⊆ [p] and any signs s ∈ {±1}S , the event that the lasso (with a fixed regularization parameter λ) selects the given support with the given signs is equivalent to the event Y ∈ A = { y : Ay < b } , where A is a fixed matrix and b is a fixed vector, which are functions of X, S, s, λ. The inequalities are interpreted elementwise, yielding a convex polyhedron A. To test the regression question described above, one then tests η>µ for a fixed unit vector η ∝ P⊥XS\\jXj . The “polyhedral lemma”, found in [5, Theorem 5.2] and [13, Lemma 2], proves that the distribution of η>Y , after conditioning on {Y ∈ A} and on P⊥η Y , is given by a\ntruncated normal distribution, with density f(r) ∝ exp { −(r − η>µ)2/2σ2 } · 1 {a1(Y ) ≤ r ≤ a2(Y )} . (2)\nThe interval endpoints a1(Y ), a2(Y ) depend on Y only through P⊥η Y and are defined to include exactly those values of r that are feasible given the event Y ∈ A. That is, the interval contains all values r such that r · η + P⊥η Y ∈ A.\nExamining (2), we see that under the null hypothesis η>µ = 0, this is a truncated zero-mean normal density, which can be used to construct a p-value testing η>µ = 0. To construct a confidence interval for η>µ, we can instead use (2) with nonzero η>µ, which is a truncated noncentral normal density."
    }, {
      "heading" : "2.2 The group-sparse case",
      "text" : "In the group-sparse regression setting, Loftus and Taylor [9] extend the work of Lee et al. [5] to questions where we would like to test PLµ, the projection of the mean µ to some potentially multidimensional subspace, rather than simply testing η>µ, which can be interpreted as a projection to a one-dimensional subspace, L = span(η). For a fixed set A ⊆ Rn and a fixed subspace L of dimension k, Loftus and Taylor [9, Theorem 3.1] prove that, after conditioning on {Y ∈ A}, on dirL(Y ), and on P⊥L Y , under the null hypothesis PLµ = 0, the distribution of ‖PLY ‖2 is given by a truncated χk distribution,\n‖PLY ‖2 ∼ (σ · χk truncated toRY ) whereRY = { r : r · dirL(Y ) + P⊥L Y ∈ A } . (3)\nIn particular, this means that, if we would like to test the null hypothesis PLµ = 0, we can compute a p-value using the truncated χk distribution as our null distribution. To better understand this null hypothesis, suppose that we run a group-sparse model selection algorithm that chooses a set of blocks S ⊆ [G]. We might then want to test whether some particular block g ∈ S should be retained in this model or removed. In that case, we would set L = span(P⊥XS\\gXg) and test whether PLµ = 0.\nExamining the parallels between this result and the work of Lee et al. [5], where (2) gives either a truncated zero-mean normal or truncated noncentral normal distribution depending on whether the null hypothesis η>µ = 0 is true or false, we might expect that the result (3) of Loftus and Taylor [9] can extend in a straightforward way to the case where PLµ 6= 0. More specifically, we might expect that (3) might then be replaced by a truncated noncentral χk distribution, with its noncentrality parameter determined by ‖PLµ‖2. However, this turns out not to be the case. To understand why, observe that ‖PLY ‖2 and dirL(Y ) are the length and the direction of the vector PLY ; in the inference procedure of Loftus and Taylor [9], they need to condition on the direction dirL(Y ) in order to compute the truncation intervalRY , and then they perform inference on ‖PLY ‖2, the length. These two quantities are independent for a centered multivariate normal, and therefore if PLµ = 0 then ‖PLY ‖2 follows a χk distribution even if we have conditioned on dirL(Y ). However, in the general case where PLµ 6= 0, we do not have independence between the length and the direction of PLY , and so while ‖PLY ‖2 is marginally distributed as a noncentral χk, this is no longer true after conditioning on dirL(Y ).\nIn this work, we consider the problem of computing the distribution of ‖PLY ‖2 after conditioning on dirL(Y ), which is the setting that we require for inference. This leads to the main contribution of this work, where we are able to perform inference on PLµ beyond simply testing the null hypothesis that PLµ = 0."
    }, {
      "heading" : "2.3 Key lemma: Truncated projections of Gaussians",
      "text" : "Before presenting our key lemma, we introduce some further notation. Let A ⊆ Rn be any fixed open set and let L ⊆ Rn be a fixed subspace of dimension k. For any y ∈ A, consider the set\nRy = {r > 0 : r · dirL(y) + P⊥L y ∈ A} ⊆ R+. Note thatRy is an open subset of R+, and its construction does not depend on ‖PLy‖2, but we see that ‖PLy‖2 ∈ Ry by definition. Lemma 1 (Truncated projection). Let A ⊆ Rn be a fixed open set and let L ⊆ Rn be a fixed subspace of dimension k. Suppose that Y ∼ N (µ, σ2In). Then, conditioning on the values of dirL(Y ) and P⊥L Y and on the event Y ∈ A, the conditional distribution of ‖PLY ‖2 has density1\nf(r) ∝ rk−1 exp { − 1 2σ2 ( r2 − 2r · 〈dirL(Y ), µ〉 )} · 1 {r ∈ RY } .\nWe pause to point out two special cases that are treated in the existing literature. 1Here and throughout the paper, we ignore the possibility that Y ⊥ L since this has probability zero.\nSpecial case 1: k = 1 andA is a convex polytope. SupposeA is the convex polytope {y : Ay < b} for fixed A ∈ Rm×n and b ∈ Rm. In this case, this almost exactly yields the “polyhedral lemma” of Lee et al. [5, Theorem 5.2]. Specifically, in their work they perform inference on η>µ for a fixed vector η; this corresponds to taking L = span(η) in our notation. Then since k = 1, Lemma 1 yields a truncated Gaussian distribution, coinciding with Lee et al. [5]’s result (2). The only difference relative to [5] is that our lemma implicitly conditions on sign(η>Y ), which is not required in [5].\nSpecial case 2: the mean µ is orthogonal to the subspace L. In this case, without conditioning on {Y ∈ A}, we have PLY = PL ( µ+N (0, σ2I) ) = PL ( N (0, σ2I) ) , and so ‖PLY ‖2 ∼ σ · χk. Without conditioning on {Y ∈ A} (or equivalently, taking A = Rn), the resulting density is then\nf(r) ∝ rk−1e−r 2/2σ2 · 1 {r > 0}\nwhich is the density of the χk distribution (rescaled by σ), as expected. If we also condition on {Y ∈ A} then this is a truncated χk distribution, as proved in Loftus and Taylor [9, Theorem 3.1]."
    }, {
      "heading" : "2.4 Selective inference on truncated projections",
      "text" : "We now show how the key result in Lemma 1 can be used for group-sparse inference. In particular, we show how to compute a p-value for the null hypothesis H0 : µ ⊥ L, or equivalently, H0 : ‖PLµ‖2 = 0. In addition, we show how to compute a one-sided confidence interval for ‖PLµ‖2, specifically, how to give a lower bound on the size of this projection.\nTheorem 1 (Selective inference for projections). Under the setting and notation of Lemma 1, define\nP =\n∫ r∈RY ,r>‖PLY ‖2 r k−1e−r 2/2σ2 dr∫\nr∈RY r k−1e−r2/2σ2 dr\n. (4)\nIf µ ⊥ L (or, more generally, if 〈dirL(Y ), µ〉 = 0), then P ∼ Uniform[0, 1]. Furthermore, for any desired error level α ∈ (0, 1), there is a unique value Lα ∈ R satisfying∫\nr∈RY ,r>‖PLY ‖2 r k−1e−(r 2−2rLα)/2σ2 dr∫ r∈RY r k−1e−(r2−2rLα)/2σ2 dr = α, (5)\nand we have\nP {‖PLµ‖2 ≥ Lα} ≥ P {〈dirL(Y ), µ〉 ≥ Lα} = 1− α.\nFinally, the p-value and the confidence interval agree in the sense that P < α if and only if Lα > 0.\nFrom the form of Lemma 1, we see that we are actually performing inference on 〈dirL(Y ), µ〉. Since ‖PLµ‖2 ≥ 〈dirL(Y ), µ〉, this means that any lower bound on 〈dirL(Y ), µ〉 also gives a lower bound on ‖PLµ‖2. For the p-value, the statement 〈dirL(Y ), µ〉 = 0 is implied by the stronger null hypothesis µ ⊥ L. We can also use Lemma 1 to give a two-sided confidence interval for 〈dirL(Y ), µ〉; specifically, 〈dirL(Y ), µ〉 lies in the interval [Lα/2, L1−α/2] with probability 1 − α. However, in general this cannot be extended to a two-sided interval for ‖PLµ‖2. To compare to the main results of Loftus and Taylor [9], their work produces the p-value (4) testing the null hypothesis µ ⊥ L, but does not extend to testing PLµ beyond the null hypothesis, which the second part (5) of our main theorem is able to do.2"
    }, {
      "heading" : "3 Applications to group sparse regression methods",
      "text" : "In this section we develop inference tools for three methods for group-sparse model selection: forward stepwise regression (also considered by Loftus and Taylor [9] with results on hypothesis testing), iterative hard thresholding (IHT), and the group lasso.\n2Their work furthermore considers the special case where the conditioning event, Y ∈ A, is determined by a “quadratic selection rule,” that is, A is defined by a set of quadratic constraints on y ∈ Rn. However, extending to the general case is merely a question of computation (as we explore below for performing inference for the group lasso) and this extension should not be viewed as a primary contribution of this work."
    }, {
      "heading" : "3.1 General recipe",
      "text" : "With a fixed design matrix, the outcome of any group-sparse selection method is a function of Y . For example, a forward stepwise procedure determines a particular sequence of groups of variables. We call such an outcome a selection event, and assume that the set of all selection events forms a countable partition of Rn into disjoint open sets: Rn = ∪eAe.3 Each data vector y ∈ Rn determines a selection event, denoted e(y), and thus y ∈ Ae(y).\nLet S(y) ⊆ [G] be the set of groups selected for testing. This is assumed to be a function of e(y), i.e. S(y) = Se for all y ∈ Ae. For any g ∈ Se, let Le,g = span(P⊥XSe\\gXg), the subspace of R n indicating correlation with group Xg beyond what can be explained by the other selected groups.\nWrite RY = {r > 0 : r · U + Y⊥ ∈ Ae(Y )}, where U = dirLe(Y ),g (Y ) and Y⊥ = P⊥Le(Y ),gY . If we condition on the event {Y ∈ Ae} for some e, then as soon as we have calculated the region RY ⊆ R+, Theorem 1 will allow us to perform inference on the quantity of interest ‖PLe,gµ‖2 by evaluating the expressions (4) and (5). In other words, we are testing whether µ is significantly correlated with the group Xg , after controlling for all the other selected groups, S(Y )\\g = Se\\g. To evaluate these expressions accurately, ideally we would like an explicit characterization of the regionRY ⊆ R+. To gain a better intuition for this set, define zY (r) = r · U + Y⊥ ∈ Rn for r > 0, and note that zY (r) = Y when we plug in r = ‖PLe(Y ),gY ‖2. Then we see that\nRY = { r > 0 : e(zY (r)) = e(Y ) } . (6)\nIn other words, we need to find the range of values of r such that, if we replace Y with zY (r), then this does not change the output of the model selection algorithm, i.e. e(zY (r)) = e(Y ). For the forward stepwise and IHT methods, we find that we can calculate RY explicitly. For the group lasso, we cannot calculateRY explicitly, but we can nonetheless compute the integrals required by Theorem 1 through numerical approximations. We now present the details for each of these methods."
    }, {
      "heading" : "3.2 Forward stepwise regression",
      "text" : "Forward stepwise regression [2, 14] is a simple and widely used method. We will use the following version:4 for design matrix X and response Y = y,\n1. Initialize the residual ̂0 = y and the model S0 = ∅. 2. For t = 1, 2, . . . , T ,\n(a) Let gt = argmaxg∈[G]\\St−1{‖X > g ̂t−1‖2}.\n(b) Update the model, St = {g1, . . . , gt}, and update the residual, ̂t = P⊥XSt y. Testing all groups at time T . First we consider the inference procedure where, at time T , we would like to test each selected group gt for t = 1, . . . , T ; inference for this procedure was derived also in [8]. Our selection event e(Y ) is the ordered sequence g1, . . . , gT of selected groups. For a response vector Y = y, this selection event is equivalent to\n‖X>gkP ⊥ XSk−1 y‖2 > ‖X>g P⊥XSk−1 y‖2 for all k = 1, . . . , T , for all g 6∈ Sk. (7) Now we would like to perform inference on the group g = gt, while controlling for the other groups in S(Y ) = ST . Define U , Y⊥, and zY (r) as before. Then, to determine RY = {r > 0 : zY (r) ∈ Ae(Y )}, we check whether all of the inequalities in (7) are satisfied with y = zY (r): for each k = 1, . . . , T and each g 6∈ Sk, the corresponding inequality of (7) can be expressed as\nr2 · ‖X>gkP ⊥ XSk−1 U‖22 + 2r · 〈X > gk P⊥XSk−1U,X > gk P⊥XSk−1Y⊥〉+ ‖X > gk P⊥XSk−1Y⊥‖ 2 2\n> r2 · ‖X>g P⊥XSk−1U‖ 2 2 + 2r · 〈X > g P⊥XSk−1U,X > g P⊥XSk−1Y⊥〉+ ‖X > g P⊥XSk−1Y⊥‖ 2 2.\nSolving this quadratic inequality over r ∈ R+, we obtain a region Ik,g ⊆ R+ which is either a single interval or a union of two disjoint intervals, whose endpoints we can calculate explicitly with the quadratic formula. The setRY is then given by all values r that satisfy the full set of inequalities:\nRY = ⋂\nk=1,...,T ⋂ g∈[G]\\Sk Ik,g.\nThis is a union of finitely many disjoint intervals, whose endpoints are calculated explicitly as above. 3Since the distribution of Y is continuous on Rn, we ignore sets of measure zero without further comment. 4In practice, we would add some correction for the scale of the columns of Xg or for the number of features in group g; this can be accomplished with simple modifications of the forward stepwise procedure.\nSequential testing. Now suppose we carry out a sequential inference procedure, testing group gt at its time of selection, controlling only for the previously selected groups St−1. In fact, this is a special case of the non-sequential procedure above, which shows how to test gT while controlling for ST \\gT = ST−1. Applying this method at each stage of the algorithm yields a sequential testing procedure. (The method developed in [9] computes p-values for this problem, testing whether µ ⊥ P⊥XSt−1Xgt at each time t.) See the supplementary material for detailed pseudo-code."
    }, {
      "heading" : "3.3 Iterative hard thresholding (IHT)",
      "text" : "The iterative hard thresholding algorithm finds a k-group-sparse solution to the linear regression problem, iterating gradient descent steps with hard thresholding to update the model choice as needed [1, 4]. Given k ≥ 1, number of iterations T , step sizes ηt, design matrix X and response Y = y,\n1. Initialize the coefficient vector, β0 = 0 ∈ Rp (or any other desired initial point). 2. For t = 1, 2, . . . , T ,\n(a) Take a gradient step, β̃t = βt−1 − ηtX>(Xβt−1 − y). (b) Compute ‖(β̃t)Cg‖2 for each g ∈ [G] and let St ⊆ [G] index the k largest norms. (c) Update the fitted coefficients βt via (βt)j = (β̃t)j · 1 {j ∈ ∪g∈StCg}.\nHere we are typically interested in testing Questiong,ST for each g ∈ ST . We condition on the selection event, e(Y ), given by the sequence of k-group-sparse models S1, . . . ,ST selected at each stage of the algorithm, which is characterized by the inequalities\n‖(β̃t)Cg‖2 > ‖(β̃t)Ch‖2 for all t = 1, . . . , T , and all g ∈ St, h 6∈ St. (8) Fixing a group g ∈ ST to test, determining RY = {r > 0 : zY (r) ∈ Ae(Y )} involves checking whether all of the inequalities in (8) are satisfied with y = zY (r). First, with the response Y replaced by y = zY (r), we show that we can write β̃t = r · ct + dt for each t = 1, . . . , T , where ct, dt ∈ Rp are independent of r; in the supplementary material, we derive ct, dt inductively as{ c1 = η1 n X >U,\nd1 = (I− η1n X >X)β0 + η1 n X >Y⊥,\n{ ct = (Ip − ηtnX >X)PSt−1ct−1 + ηt nX >U,\ndt = (Ip − ηtnX >X)PSt−1dt−1 + ηt nX >Y⊥ for t ≥ 2.\nNow we compute the regionRY . For each t = 1, . . . , T and each g ∈ St, h 6∈ St, the corresponding inequality in (8), after writing β̃t = r · ct + dt, can be expressed as\nr2·‖(ct)Cg‖22+2r·〈(ct)Cg , (dt)Cg 〉+‖(dt)Cg‖22 > r2·‖(ct)Ch‖22+2r·〈(ct)Ch , (dt)Ch〉+‖(dt)Ch‖22. As for the forward stepwise procedure, solving this quadratic inequality over r ∈ R+, we obtain a region It,g,h ⊆ R+ that is either a single interval or a union of two disjoint intervals whose endpoints we can calculate explicitly. Finally, we obtainRY = ⋂ t=1,...,T ⋂ g∈St ⋂ h∈[G]\\St It,g,h."
    }, {
      "heading" : "3.4 The group lasso",
      "text" : "The group lasso, first introduced by Yuan and Lin [15], is a convex optimization method for linear regression where the form of the penalty is designed to encourage group-wise sparsity of the solution. It is an extension of the lasso method [12] for linear regression. The method is given by\nβ̂ = argminβ { 1 2‖y −Xβ‖ 2 2 + λ ∑ g‖βCg‖2 } ,\nwhere λ > 0 is a penalty parameter. The penalty ∑ g‖βCg‖2 promotes sparsity at the group level.5\nFor this method, we perform inference on the group support S of the fitted model β̂. We would like to test Questiong,S for each g ∈ S. In this setting, for groups of size ≥ 2, we believe that it is not possible to analytically calculateRY , and furthermore, that there is no additional information that we can condition on to make this computation possible, without losing all power to do inference.\nWe thus propose a numerical approximation that circumvents the need for an explicit calculation of RY . Examining the calculation of the p-value P and the lower bound Lα in Theorem 1, we see that we can write P = fY (0) and can find Lα as the unique solution to fY (Lα) = α, where\nfY (t) = Er∼σ·χk\n[ ert/σ 2 · 1 {r ∈ RY , r > ‖PLY ‖2} ]\nEr∼σ·χk [ ert/σ2 · 1 {r ∈ RY } ] , 5Our method can also be applied to a modification of group lasso designed for overlapping groups [3] with a\nnearly identical procedure but we do not give details here.\nwhere we treat Y as fixed in this calculation and set k = dim(L) = rank(XS\\g). Both the numerator and denominator can be approximated by taking a large number B of samples r ∼ σ · χk and taking the empirical expectations. Checking r ∈ RY is equivalent to running the group lasso with the response replaced by y = zY (r), and checking if the resulting selected model remains unchanged.\nThis may be problematic, however, if RY is in the tails of the σ · χk distribution. We implement an importance sampling approach by repeatedly drawing r ∼ ψ for some density ψ; we find that ψ = ‖PLY ‖2 +N (0, σ2) works well in practice. Given samples r1, . . . , rB ∼ ψ we then estimate\nfY (t) ≈ f̂Y (t) := ∑ b ψσ·χk (rb) ψ(rb) · erbt/σ2 · 1 {rb ∈ RY , rb > ‖PLY ‖2}∑\nb\nψσ·χk (rb)\nψ(rb) · erbt/σ2 · 1 {rb ∈ RY }\nwhere ψσ·χk is the density of the σ ·χk distribution. We then estimate P ≈ P̂ = f̂Y (0). Finally, since f̂Y (t) is continuous and strictly increasing in t, we estimate Lα by numerically solving f̂Y (t) = α."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section we present results from experiments on simulated and real data, performed in R [11].6"
    }, {
      "heading" : "4.1 Simulated data",
      "text" : "We fix sample size n = 500 and G = 50 groups each of size 10. For each trial, we generate a design matrix X with i.i.d. N (0, 1/n) entries, set β with its first 50 entries (corresponding to first s = 5 groups) equal to τ and all other entries equal to 0, and set Y = Xβ + N (0, In). We present the result for IHT here; the results for the other two methods can be found in the supplementary material.\nWe run IHT to select k = 10 groups over T = 5 iterations, with step sizes ηt = 2 and initial point β0 = 0. For a moderate signal strength τ = 1.5, we plot the p-values for each selected group in Figure 1; each group displays p-values only for those trials in which it was selected. The histogram of p-values for the s true signals and for the G− s nulls are also shown. We see that the the distribution of p-values for the true signals concentrates near zero while the null p-values are roughly uniform.\nNext we look at the confidence intervals given by our method, examining their empirical coverage across different signal strengths τ in Figure 2. We fix confidence level 0.9 (i.e. α = 0.1) and check empirical coverage with respect to both ‖PLµ‖2 and 〈dirL(Y ), µ〉, with results shown separately for true signals and for nulls. For true signals, the confidence interval for ‖PLµ‖2 is somewhat conservative while the coverage for 〈dirL(Y ), µ〉 is right at the target level, as expected from our theory. As signal strength τ increases, the gap is reduced for the true signals; this is because dirL(Y ) becomes an increasingly more accurate estimate of dirL(µ), and so the gap in the inequality ‖PLµ‖2 ≥ 〈dirL(Y ), µ〉 is reduced. For the nulls, if the set of selected groups contains the support of the true model, which is nearly always true for higher signal levels τ , then the two are equivalent (as ‖PLµ‖2 = 〈dirL(Y ), µ〉 = 0), and coverage is at the target level. At low signal levels τ , however, a true group is occasionally missed, in which case ‖PLµ‖2 > 〈dirL(Y ), µ〉 strictly."
    }, {
      "heading" : "4.2 California health data",
      "text" : "We examine the 2015 California county health data7 which was also studied by Loftus and Taylor [9]. We fit a linear model where the response is the log-years of potential life lost and the covariates\n6Code reproducing experiments: http://www.stat.uchicago.edu/~rina/group_inf.html 7Available at http://www.countyhealthrankings.org\nare the 34 predictors in this data set. We first let each predictor be its own group (i.e., group size 1) and run the three algorithms considered in Section 3. Next, we form a grouped model by expanding each predictor Xj into a group using the first three non-constant Legendre polynomials, (Xj , 1 2 (3X 2 j −1), 12 (5X 3 j −3Xj)). In each case we set parameters so that 8 groups are selected. The selected groups and their p-values are given in Table 1; interestingly, even when the same predictor is selected by multiple methods, its p-value can differ substantially across the different methods."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We develop selective inference tools for group-sparse linear regression methods, where for a datadependent selected set of groups S, we are able to both test each group g ∈ S for inclusion in the model defined by S, and form a confidence interval for the effect size of group g in the model. Our theoretical results can be easily applied to a range of commonly used group-sparse regression methods, thus providing an efficient tool for finite-sample inference that correctly accounts for data-dependent model selection in the group-sparse setting."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Research supported in part by ONR grant N00014-15-1-2379, and NSF grants DMS-1513594 and DMS-1547396."
    } ],
    "references" : [ {
      "title" : "Sampling theorems for signals from the union of finitedimensional linear subspaces",
      "author" : [ "Thomas Blumensath", "Mike E Davies" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Group lasso with overlap and graph lasso",
      "author" : [ "Laurent Jacob", "Guillaume Obozinski", "Jean-Philippe Vert" ],
      "venue" : "In Proceedings of the 26th annual international conference on machine learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Structured sparse regression via greedy hardthresholding",
      "author" : [ "Prateek Jain", "Nikhil Rao", "Inderjit S. Dhillon" ],
      "venue" : "CoRR, abs/1602.06042,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Exact post-selection inference with the lasso",
      "author" : [ "Jason D Lee", "Dennis L Sun", "Yuekai Sun", "Jonathan E Taylor" ],
      "venue" : "arXiv preprint arXiv:1311.6238,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Exact post model selection inference for marginal screening",
      "author" : [ "Jason D. Lee", "Jonathan E. Taylor" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Selective inference after cross-validation",
      "author" : [ "Joshua R Loftus" ],
      "venue" : "arXiv preprint arXiv:1511.08866,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "A significance test for forward stepwise model selection",
      "author" : [ "Joshua R Loftus", "Jonathan E Taylor" ],
      "venue" : "arXiv preprint arXiv:1405.3920,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Selective inference in regression models with groups of variables",
      "author" : [ "Joshua R. Loftus", "Jonathan E. Taylor" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "A primal-dual algorithm for group sparse regularization with overlapping groups",
      "author" : [ "Sofia Mosci", "Silvia Villa", "Alessandro Verri", "Lorenzo Rosasco" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2016",
      "author" : [ "R R Core Team" ],
      "venue" : "URL https://www.R-project.org/",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "Exact postselection inference for sequential regression procedures",
      "author" : [ "Ryan J Tibshirani", "Jonathan Taylor", "Richard Lockhart", "Robert Tibshirani" ],
      "venue" : "arXiv preprint arXiv:1401.3889,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Signal recovery from random measurements via orthogonal matching pursuit",
      "author" : [ "Joel A. Tropp", "Anna C. Gilbert" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Model selection and estimation in regression with grouped variables",
      "author" : [ "Ming Yuan", "Yi Lin" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "1 Introduction Significant progress has been recently made on developing inference tools to complement the feature selection methods that have been intensively studied in the past decade [6, 5, 9].",
      "startOffset" : 187,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction Significant progress has been recently made on developing inference tools to complement the feature selection methods that have been intensively studied in the past decade [6, 5, 9].",
      "startOffset" : 187,
      "endOffset" : 196
    }, {
      "referenceID" : 8,
      "context" : "1 Introduction Significant progress has been recently made on developing inference tools to complement the feature selection methods that have been intensively studied in the past decade [6, 5, 9].",
      "startOffset" : 187,
      "endOffset" : 196
    }, {
      "referenceID" : 11,
      "context" : "The goal of selective inference is to make accurate uncertainty assessments for the parameters estimated using a feature selection algorithm, such as the lasso [12].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : "In this paper, we present new results for selective inference in the setting of group sparsity [15, 3, 10].",
      "startOffset" : 95,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : "In this paper, we present new results for selective inference in the setting of group sparsity [15, 3, 10].",
      "startOffset" : 95,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "In this paper, we present new results for selective inference in the setting of group sparsity [15, 3, 10].",
      "startOffset" : 95,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Algorithms for fitting this model, such as the group lasso [15], extend well-studied methods for sparse linear regression to this grouped setting.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 8,
      "context" : "In the group-sparse setting, recent results of Loftus and Taylor [9] give a selective inference method for computing p-values for each group chosen by a model selection method such as forward stepwise regression; selection via cross-validation was studied in [9].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "In the group-sparse setting, recent results of Loftus and Taylor [9] give a selective inference method for computing p-values for each group chosen by a model selection method such as forward stepwise regression; selection via cross-validation was studied in [9].",
      "startOffset" : 259,
      "endOffset" : 262
    }, {
      "referenceID" : 6,
      "context" : "More generally, the inference technique of [7] applies to any model selection method whose outcome can be described in terms of quadratic 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 14,
      "context" : "This technical result enables us to develop inference tools for a broad class of model selection methods, including the group lasso [15], iterative hard thresholding [1, 4], and forward stepwise group selection [14].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "This technical result enables us to develop inference tools for a broad class of model selection methods, including the group lasso [15], iterative hard thresholding [1, 4], and forward stepwise group selection [14].",
      "startOffset" : 166,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "This technical result enables us to develop inference tools for a broad class of model selection methods, including the group lasso [15], iterative hard thresholding [1, 4], and forward stepwise group selection [14].",
      "startOffset" : 166,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "This technical result enables us to develop inference tools for a broad class of model selection methods, including the group lasso [15], iterative hard thresholding [1, 4], and forward stepwise group selection [14].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : "In Section 4 we give numerical results to illustrate these tools on simulated data, as well as on the California county health data used in previous work [9].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 8,
      "context" : "Thus, the following question is central to selective inference: Questiong,S : What is the magnitude of the projection of μ onto the span of P⊥ XS\\gXg? (1) In particular, we are interested in a hypothesis test to determine if μ is orthogonal to this span, that is, whether block g should be removed from the model with group-sparse support determined by S; this is the question studied by Loftus and Taylor [9] for which they compute p-values.",
      "startOffset" : 406,
      "endOffset" : 409
    }, {
      "referenceID" : 12,
      "context" : "[13] characterize the selection event as a polyhedron in R: for any set S ⊆ [p] and any signs s ∈ {±1}S , the event that the lasso (with a fixed regularization parameter λ) selects the given support with the given signs is equivalent to the event Y ∈ A = { y : Ay < b } , where A is a fixed matrix and b is a fixed vector, which are functions of X, S, s, λ.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "2 The group-sparse case In the group-sparse regression setting, Loftus and Taylor [9] extend the work of Lee et al.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "[5] to questions where we would like to test PLμ, the projection of the mean μ to some potentially multidimensional subspace, rather than simply testing η>μ, which can be interpreted as a projection to a one-dimensional subspace, L = span(η).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5], where (2) gives either a truncated zero-mean normal or truncated noncentral normal distribution depending on whether the null hypothesis η>μ = 0 is true or false, we might expect that the result (3) of Loftus and Taylor [9] can extend in a straightforward way to the case where PLμ 6= 0.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[5], where (2) gives either a truncated zero-mean normal or truncated noncentral normal distribution depending on whether the null hypothesis η>μ = 0 is true or false, we might expect that the result (3) of Loftus and Taylor [9] can extend in a straightforward way to the case where PLμ 6= 0.",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 8,
      "context" : "To understand why, observe that ‖PLY ‖2 and dirL(Y ) are the length and the direction of the vector PLY ; in the inference procedure of Loftus and Taylor [9], they need to condition on the direction dirL(Y ) in order to compute the truncation intervalRY , and then they perform inference on ‖PLY ‖2, the length.",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "The only difference relative to [5] is that our lemma implicitly conditions on sign(η>Y ), which is not required in [5].",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "The only difference relative to [5] is that our lemma implicitly conditions on sign(η>Y ), which is not required in [5].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "To compare to the main results of Loftus and Taylor [9], their work produces the p-value (4) testing the null hypothesis μ ⊥ L, but does not extend to testing PLμ beyond the null hypothesis, which the second part (5) of our main theorem is able to do.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "2 3 Applications to group sparse regression methods In this section we develop inference tools for three methods for group-sparse model selection: forward stepwise regression (also considered by Loftus and Taylor [9] with results on hypothesis testing), iterative hard thresholding (IHT), and the group lasso.",
      "startOffset" : 213,
      "endOffset" : 216
    }, {
      "referenceID" : 1,
      "context" : "2 Forward stepwise regression Forward stepwise regression [2, 14] is a simple and widely used method.",
      "startOffset" : 58,
      "endOffset" : 65
    }, {
      "referenceID" : 13,
      "context" : "2 Forward stepwise regression Forward stepwise regression [2, 14] is a simple and widely used method.",
      "startOffset" : 58,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : ", T ; inference for this procedure was derived also in [8].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "(The method developed in [9] computes p-values for this problem, testing whether μ ⊥ P⊥ XSt−1gt at each time t.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "3 Iterative hard thresholding (IHT) The iterative hard thresholding algorithm finds a k-group-sparse solution to the linear regression problem, iterating gradient descent steps with hard thresholding to update the model choice as needed [1, 4].",
      "startOffset" : 237,
      "endOffset" : 243
    }, {
      "referenceID" : 3,
      "context" : "3 Iterative hard thresholding (IHT) The iterative hard thresholding algorithm finds a k-group-sparse solution to the linear regression problem, iterating gradient descent steps with hard thresholding to update the model choice as needed [1, 4].",
      "startOffset" : 237,
      "endOffset" : 243
    }, {
      "referenceID" : 14,
      "context" : "4 The group lasso The group lasso, first introduced by Yuan and Lin [15], is a convex optimization method for linear regression where the form of the penalty is designed to encourage group-wise sparsity of the solution.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "It is an extension of the lasso method [12] for linear regression.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "Examining the calculation of the p-value P and the lower bound Lα in Theorem 1, we see that we can write P = fY (0) and can find Lα as the unique solution to fY (Lα) = α, where fY (t) = Er∼σ·χk [ e 2 · 1 {r ∈ RY , r > ‖PLY ‖2} ] Er∼σ·χk [ ert/σ(2) · 1 {r ∈ RY } ] , (5)Our method can also be applied to a modification of group lasso designed for overlapping groups [3] with a nearly identical procedure but we do not give details here.",
      "startOffset" : 365,
      "endOffset" : 368
    }, {
      "referenceID" : 10,
      "context" : "4 Experiments In this section we present results from experiments on simulated and real data, performed in R [11].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "2 California health data We examine the 2015 California county health data7 which was also studied by Loftus and Taylor [9].",
      "startOffset" : 120,
      "endOffset" : 123
    } ],
    "year" : 2016,
    "abstractText" : "We develop tools for selective inference in the setting of group sparsity, including the construction of confidence intervals and p-values for testing selected groups of variables. Our main technical result gives the precise distribution of the magnitude of the projection of the data onto a given subspace, and enables us to develop inference procedures for a broad class of group-sparse selection methods, including the group lasso, iterative hard thresholding, and forward stepwise regression. We give numerical results to illustrate these tools on simulated data and on health record data.",
    "creator" : null
  }
}