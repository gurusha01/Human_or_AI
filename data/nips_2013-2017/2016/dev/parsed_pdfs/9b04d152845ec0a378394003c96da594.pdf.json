{
  "name" : "9b04d152845ec0a378394003c96da594.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multimodal Residual Learning for Visual QA",
    "authors" : [ "Jin-Hwa Kim", "Sang-Woo Lee", "Donghyun Kwak", "Min-Oh Heo", "Jeonghee Kim", "Jung-Woo Ha" ],
    "emails" : [ "jhkim@bi.snu.ac.kr", "slee@bi.snu.ac.kr", "dhkwak@bi.snu.ac.kr", "moheo@bi.snu.ac.kr", "jeonghee.kim@navercorp.com", "jungwoo.ha@navercorp.com", "btzhang@bi.snu.ac.kr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Visual question-answering tasks provide a testbed to cultivate the synergistic proposals which handle multidisciplinary problems of vision, language and integrated reasoning. So, the visual questionanswering tasks let the studies in artificial intelligence go beyond narrow tasks. Furthermore, it may help to solve the real world problems which need the integrated reasoning of vision and language.\nDeep residual learning [6] not only advances the studies in object recognition problems, but also gives a general framework for deep neural networks. The existing non-linear layers of neural networks serve to fit another mapping of F(x), which is the residual of identity mapping x. So, with the shortcut connection of identity mapping x, the whole module of layers fit F(x) + x for the desired underlying mappingH(x). In other words, the only residual mapping F(x), defined byH(x)− x, is learned with non-linear layers. In this way, very deep neural networks effectively learn representations in an efficient manner.\nMany attentional models utilize the residual learning to deal with various tasks, including textual reasoning [25, 21] and visual question-answering [29]. They use an attentional mechanism to handle two different information sources, a query and the context of the query (e.g. contextual sentences\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nQ V\nARNN\nCNN\nsoftmax\nMultimodal Residual Networks\nWhat kind of animals are these ?\nsheep\nword embedding\nFigure 1: Inference flow of Multimodal Residual Networks (MRN). Using our visualization method, the attention effects are shown as a sequence of three images. More examples are shown in Figure 4.\nA\nLinear Tanh Linear\nTanhLinear\nTanh Linear\nQ V\nH1\nLinear Tanh Linear\nTanhLinear\nTanh Linear\nH2\nV\nLinear Tanh Linear\nTanhLinear\nTanh Linear\nH3\nV\nLinear Softmax\n⊙ ⊕\n⊙ ⊕\n⊙ ⊕\nf\nFigure 2: A schematic diagram of Multimodal Residual Networks with three-block layers.\nor an image). The query is added to the output of the attentional module, that makes the attentional module learn the residual of query mapping as in deep residual learning.\nIn this paper, we propose Multimodal Residual Networks (MRN) to learn multimodality of visual question-answering tasks exploiting the excellence of deep residual learning [6]. MRN inherently uses shortcuts and residual mappings for multimodality. We explore various models upon the choice of the shortcuts for each modality, and the joint residual mappings based on element-wise multiplication, which effectively learn the multimodal representations not using explicit attention parameters. Figure 1 shows inference flow of the proposed MRN.\nAdditionally, we propose a novel method to visualize the attention effects of each joint residual mapping. The visualization method uses back-propagation algorithm [22] for the difference between the visual input and the output of the joint residual mapping. The difference is back-propagated up to an input image. Since we use the pretrained visual features, the pretrained CNN is augmented for visualization. Based on this, we argue that MRN is an implicit attention model without explicit attention parameters.\nOur contribution is three-fold: 1) extending the deep residual learning for visual question-answering tasks. This method utilizes multimodal inputs, and allows a deeper network structure, 2) achieving the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks, and finally, 3) introducing a novel method to visualize spatial attention effect of joint residual mappings from the collapsed visual feature using back-propagation."
    }, {
      "heading" : "2 Related Works",
      "text" : ""
    }, {
      "heading" : "2.1 Deep Residual Learning",
      "text" : "Deep residual learning [6] allows neural networks to have a deeper structure of over-100 layers. The very deep neural networks are usually hard to be optimized even though the well-known activation functions and regularization techniques are applied [17, 7, 9]. This method consistently shows state-of-the-art results across multiple visual tasks including image classification, object detection, localization and segmentation.\nThis idea assumes that a block of deep neural networks forming a non-linear mapping F(x) may paradoxically fail to fit into an identity mapping. To resolve this, the deep residual learning adds x to F(x) as a shortcut connection. With this idea, the non-linear mapping F(x) can focus on the\nresidual of the shortcut mapping x. Therefore, a learning block is defined as: y = F(x) + x (1)\nwhere x and y are the input and output of the learning block, respectively."
    }, {
      "heading" : "2.2 Stacked Attention Networks",
      "text" : "Stacked Attention Networks (SAN) [29] explicitly learns the weights of visual feature vectors to select a small portion of visual information for a given question vector. Furthermore, this model stacks the attention networks for multi-step reasoning narrowing down the selection of visual information. For example, if the attention networks are asked to find a pink handbag in a scene, they try to find pink objects first, and then, narrow down to the pink handbag.\nFor the attention networks, the weights are learned by a question vector and the corresponding visual feature vectors. These weights are used for the linear combination of multiple visual feature vectors indexing spatial information. Through this, SAN successfully selects a portion of visual information. Finally, an addition of the combined visual feature vector and the previous question vector is transferred as a new input question vector to next learning block.\nqk = F(qk−1,V) + qk−1 (2) Here, ql is a question vector for l-th learning block and V is a visual feature matrix, whose columns indicate the specific spatial indexes. F(q,V) is the attention networks of SAN."
    }, {
      "heading" : "3 Multimodal Residual Networks",
      "text" : "Deep residual learning emphasizes the importance of identity (or linear) shortcuts to have the nonlinear mappings efficiently learn only residuals [6]. In multimodal learning, this idea may not be readily applied. Since the modalities may have correlations, we need to carefully define joint residual functions as the non-linear mappings. Moreover, the shortcuts are undetermined due to its multimodality. Therefore, the characteristics of a given task ought to be considered to determine the model structure."
    }, {
      "heading" : "3.1 Background",
      "text" : "We infer a residual learning in the attention networks of SAN. Since Equation 18 in [29] shows a question vector transferred directly through successive layers of the attention networks. In the case of SAN, the shortcut mapping is for the question vector, and the non-linear mapping is the attention networks.\nIn the attention networks, Yang et al. [29] assume that an appropriate choice of weights on visual feature vectors for a given question vector sufficiently captures the joint representation for answering. However, question information weakly contributes to the joint representation only through coefficients p, which may cause a bottleneck to learn the joint representation.\nF(q,V) = ∑ i piVi (3)\nThe coefficients p are the output of a nonlinear function of a question vector q and a visual feature matrix V (see Equation 15-16 in Yang et al. [29]). The Vi is a visual feature vector of spatial index i in 14× 14 grids. Lu et al. [15] propose an element-wise multiplication of a question vector and a visual feature vector after appropriate embeddings for a joint model. This makes a strong baseline outperforming some of the recent works [19, 2]. We firstly take this approach as a candidate for the joint residual function, since it is simple yet successful for visual question-answering. In this context, we take the global visual feature approach for the element-wise multiplication, instead of the multiple (spatial) visual features approach for the explicit attention mechanism of SAN. (We present a visualization technique exploiting the element-wise multiplication in Section 5.2.)\nBased on these observations, we follow the shortcut mapping and the stacking architecture of SAN [29]; however, the element-wise multiplication is used for the joint residual function F . These updates effectively learn the joint representation of given vision and language information addressing the bottleneck issue of the attention networks of SAN."
    }, {
      "heading" : "3.2 Multimodal Residual Networks",
      "text" : "MRN consists of multiple learning blocks, which are stacked for deep residual learning. Denoting an optimal mapping byH(q,v), we approximate it using\nH1(q,v) = W (1) q′ q + F (1)(q,v). (4)\nThe first (linear) approximation term is W (1)q′ q and the first joint residual function is given by F (1)(q,v). The linear mapping Wq′ is used for matching a feature dimension. We define the joint residual function as\nF (k)(q,v) = σ(W (k)q q) σ(W (k) 2 σ(W (k) 1 v)) (5)\nwhere σ is tanh, and is element-wise multiplication. The question vector and the visual feature vector directly contribute to the joint representation. We justify this choice in Sections 4 and 5.\nFor a deeper residual learning, we replace q with H1(q,v) in the next layer. In more general terms, Equations 4 and 5 can be rewritten as\nHL(q,v) = Wq′q + L∑ l=1 WF(l)F (l)(Hl−1,v) (6)\nwhere L is the number of learning blocks, H0 = q, Wq′ = ΠLl=1W (l) q′ , and WF(l) = Π L m=l+1W (m) q′ . The cascading in Equation 6 can intuitively be represented as shown in Figure 2. Notice that the shortcuts for a visual part are identity mappings to transfer the input visual feature vector to each layer (dashed line). At the end of each block, we denote Hl as the output of the l-th learning block, and ⊕ is element-wise addition."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Visual QA Dataset",
      "text" : "We choose the Visual QA (VQA) dataset [1] for the evaluation of our models. Other datasets may not be ideal, since they have limited number of examples to train and test [16], or have synthesized questions from the image captions [14, 20].\nThe questions and answers of the VQA dataset are collected via Amazon Mechanical Turk from human subjects, who satisfy the experimental requirement. The dataset includes 614,163 questions and 7,984,119 answers, since ten answers are gathered for each question from unique human subjects. Therefore, Agrawal et al. [1] proposed a new accuracy metric as follows:\nmin\n( # of humans that provided that answer\n3 , 1\n) . (7)\nThe questions are answered in two ways: Open-Ended and Multiple-Choice. Unlike Open-Ended, Multiple-Choice allows additional information of eighteen candidate answers for each question. There are three types of answers: yes/no (Y/N), numbers (Num.) and others (Other). Table 3 shows that Other type has the most benefit from Multiple-Choice.\nThe images come from the MS-COCO dataset, 123,287 of them for training and validation, and 81,434 for test. The images are carefully collected to contain multiple objects and natural situations, which is also valid for visual question-answering tasks."
    }, {
      "heading" : "4.2 Implementation",
      "text" : "Torch framework and rnn package [13] are used to build our models. For efficient computation of variable-length questions, TrimZero is used to trim out zero vectors [11]. TrimZero eliminates zero computations at every time-step in mini-batch learning. Its efficiency is affected by a batch size, RNN model size, and the number of zeros in inputs. We found out that TrimZero was suitable for VQA tasks. Approximately, 37.5% of training time is reduced in our experiments using this technique.\nPreprocessing We follow the same preprocessing procedure of DeeperLSTM+NormalizedCNN [15] (Deep Q+I) by default. The number of answers is 1k, 2k, or 3k using the most frequent answers, which covers 86.52%, 90.45% and 92.42% of questions, respectively. The questions are tokenized using Python Natural Language Toolkit (nltk) [3]. Subsequently, the vocabulary sizes are 14,770, 15,031 and 15,169, respectively.\nPretrained Models A question vector q ∈ R2,400 is the last output vector of GRU [4], initialized with the parameters of Skip-Thought Vectors [12]. Based on the study of Noh et al. [19], this method shows effectiveness of question embedding in visual question-answering tasks. A visual feature vector v is an output of the first fully-connected layer of VGG-19 networks [23], whose dimension is 4,096. Alternatively, ResNet-152 [6] is used, whose dimension is of 2,048. The error is back-propagated to the input question for fine-tuning, yet, not for the visual part v due to the heavy computational cost of training.\nPostprocessing Image captioning model [10] is used to improve the accuracy of Other type. Let the intermediate representation v ∈ R|Ω| which is right before applying softmax. |Ω| is the vocabulary size of answers, and vi is corresponding to answer ai. If ai is not a number or yes or no, and appeared at least once in the generated caption, then update vi ← vi + 1. Notice that the pretrained image captioning model is not part of training. This simple procedure improves around 0.1% of the test-dev\noverall accuracy (0.3% for Other type). We attribute this improvement to “tie break” in Other type. For the Multiple-Choice task, we mask the output of softmax layer with the given candidate answers.\nHyperparameters By default, we follow Deep Q+I. The common embedding size of the joint representation is 1,200. The learnable parameters are initialized using a uniform distribution from −0.08 to 0.08 except for the pretrained models. The batch size is 200, and the number of iterations is fixed to 250k. The RMSProp [26] is used for optimization, and dropouts [7, 5] are used for regularization. The hyperparameters are fixed using test-dev results. We compare our method to state-of-the-arts using test-standard results."
    }, {
      "heading" : "4.3 Exploring Alternative Models",
      "text" : "Figure 3 shows alternative models we explored, based on the observations in Section 3. We carefully select alternative models (a)-(c) for the importance of embeddings in multimodal learning [18, 24], (d) for the effectiveness of identity mapping as reported by [6], and (e) for the confirmation of using question-only shortcuts in the multiple blocks as in [29]. For comparison, all models have three-block layers (selected after a pilot test), using VGG-19 features and 1k answers, then, the number of learning blocks is explored to confirm the pilot test. The effect of the pretrained visual feature models and the number of answers are also explored. All validation is performed on the test-dev split."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Quantitative Analysis",
      "text" : "The VQA Challenge, which released the VQA dataset, provides evaluation servers for test-dev and test-standard test splits. For the test-dev, the evaluation server permits unlimited submissions for validation, while the test-standard permits limited submissions for the competition. We report accuracies in percentage.\nAlternative Models The test-dev results of the alternative models for the Open-Ended task are shown in Table 1. (a) shows a significant improvement over SAN. However, (b) is marginally better than (a). As compared to (b), (c) deteriorates the performance. An extra embedding for a question vector may easily cause overfitting leading to the overall degradation. And, the identity shortcuts in (d) cause the degradation problem, too. Extra parameters of the linear mappings may effectively support to do the task.\n(e) shows a reasonable performance, however, the extra shortcut is not essential. The empirical results seem to support this idea. Since the question-only model (50.39%) achieves a competitive result to the joint model (57.75%), while the image-only model gets a poor accuracy (28.13%) (see Table 2 in [1]). Eventually, we chose model (b) as the best performance and relative simplicity.\nexamples examples\nWhat kind of animals are these ? sheep What animal is the picture ? elephant\nWhat is this animal ? zebra What game is this person playing ? tennis\nHow many cats are here ? 2 What color is the bird ? yellow\nWhat sport is this ? surfing Is the horse jumping ? yes\n(a) (b)\n(c) (d)\n(e) (f)\n(g) (h)\nFigure 4: Examples for visualization of the three-block layered MRN. The original images are shown in the first of each group. The next three images show the input gradients of the attention effect for each learning block as described in Section 5.2. The gradients of color channels for each pixel are summed up after taking absolute values of these gradients. Then, these summed absolute values which are greater than the summation of the mean and the standard deviation of these values are visualized as the attention effect (bright color) on the images. The answers (blue) are predicted by MRN.\nThe effects of other various options, Skip-Thought Vectors [12] for parameter initialization, Bayesian Dropout [5] for regularization, image captioning model [10] for postprocessing, and the usage of shortcut connections, are explored in Appendix A.1.\nNumber of Learning Blocks To confirm the effectiveness of the number of learning blocks selected via a pilot test (L = 3), we explore this on the chosen model (b), again. As the depth increases, the overall accuracies are 58.85% (L = 1), 59.44% (L = 2), 60.53% (L = 3) and 60.42% (L = 4).\nVisual Features The ResNet-152 visual features are significantly better than VGG-19 features for Other type in Table 2, even if the dimension of the ResNet features (2,048) is a half of VGG features’ (4,096). The ResNet visual features are also used in the previous work [8]; however, our model achieves a remarkably better performance with a large margin (see Table 3).\nNumber of Target Answers The number of target answers slightly affects the overall accuracies with the trade-off among answer types. So, the decision on the number of target answers is difficult to be made. We chose Res, 2k in Table 2 based on the overall accuracy (for Multiple-Choice task, see Appendix A.1).\nComparisons with State-of-the-arts Our chosen model significantly outperforms other state-ofthe-art methods for both Open-Ended and Multiple-Choice tasks in Table 3. However, the performance of Number and Other types are still not satisfactory compared to Human performance, though the advances in the recent works were mainly for Other-type answers. This fact motivates to study on a counting mechanism in future work. The model comparison is performed on the test-standard results."
    }, {
      "heading" : "5.2 Qualitative Analysis",
      "text" : "In Equation 5, the left term σ(Wqq) can be seen as a masking (attention) vector to select a part of visual information. We assume that the difference between the right term V := σ(W2σ(W1v)) and the masked vector F(q,v) indicates an attention effect caused by the masking vector. Then, the attention effect Latt = 12‖V − F‖\n2 is visualized on the image by calculating the gradient of Latt with respect to a given image I, while treating F as a constant.\n∂Latt ∂I = ∂V ∂I (V − F) (8)\nThis technique can be applied to each learning block in a similar way.\nSince we use the preprocessed visual features, the pretrained CNN is augmented only for this visualization. Note that model (b) in Table 1 is used for this visualization, and the pretrained VGG-19 is used for preprocessing and augmentation. The model is trained using the training set of the VQA dataset, and visualized using the validation set. Examples are shown in Figure 4 (more examples in Appendix A.2-4).\nUnlike the other works [29, 28] that use explicit attention parameters, MRN does not use any explicit attentional mechanism. However, we observe the interpretability of element-wise multiplication as an information masking, which yields a novel method for visualizing the attention effect from this operation. Since MRN does not depend on a few attention parameters (e.g. 14×14), our visualization method shows a higher resolution than others [29, 28]. Based on this, we argue that MRN is an implicit attention model without explicit attention mechanism."
    }, {
      "heading" : "6 Conclusions",
      "text" : "The idea of deep residual learning is applied to visual question-answering tasks. Based on the two observations of the previous works, various alternative models are suggested and validated to propose the three-block layered MRN. Our model achieves the state-of-the-art results on the VQA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we have introduced a novel method to visualize the spatial attention from the collapsed visual features using back-propagation.\nWe believe our visualization method brings implicit attention mechanism to research of attentional models. Using back-propagation of attention effect, extensive research in object detection, segmentation and tracking are worth further investigations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Patrick Emaase for helpful comments and editing. This work was supported by Naver Corp. and partly by the Korea government (IITP-R0126-16-1072-SW.StarLab, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF, ADD-UD130070ID-BMRR)."
    } ],
    "references" : [ {
      "title" : "VQA: Visual Question Answering",
      "author" : [ "Aishwarya Agrawal", "Jiasen Lu", "Stanislaw Antol", "Margaret Mitchell", "C. Lawrence Zitnick", "Dhruv Batra", "Devi Parikh" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Learning to Compose Neural Networks for Question Answering",
      "author" : [ "Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein" ],
      "venue" : "arXiv preprint arXiv:1601.01705,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Natural language processing with Python",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.1259,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks",
      "author" : [ "Yarin Gal" ],
      "venue" : "arXiv preprint arXiv:1512.05287,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Deep Residual Learning for Image Recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "arXiv preprint arXiv:1512.03385,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "A Focused Dynamic Attention Model for Visual Question Answering",
      "author" : [ "Ilija Ilievski", "Shuicheng Yan", "Jiashi Feng" ],
      "venue" : "arXiv preprint arXiv:1604.01485,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift",
      "author" : [ "Sergey Ioffe", "Christian Szegedy" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Deep Visual-Semantic Alignments for Generating Image Descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei" ],
      "venue" : "In 28th IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing",
      "author" : [ "Jin-Hwa Kim", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang" ],
      "venue" : "In Proceedings of KIIS Spring Conference,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Skip-Thought Vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler" ],
      "venue" : "arXiv preprint arXiv:1506.06726,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Recurrent Library for Torch",
      "author" : [ "Nicholas Léonard", "Sagar Waghmare", "Yang Wang", "Jin-Hwa Kim" ],
      "venue" : "arXiv preprint arXiv:1511.07889,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Microsoft COCO: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick" ],
      "venue" : "In European Conference on Computer Vision,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Deeper LSTM and normalized CNN Visual Question Answering model",
      "author" : [ "Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh" ],
      "venue" : "https://github.com/VT-vision-lab/VQA_LSTM_CNN,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Ask Your Neurons: A Neural-based Approach to Answering Questions about Images",
      "author" : [ "Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz" ],
      "venue" : "arXiv preprint arXiv:1505.01121,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Rectified Linear Units Improve Restricted Boltzmann Machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Multimodal Deep Learning",
      "author" : [ "Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng" ],
      "venue" : "In Proceedings of The 28th International Conference on Machine Learning,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Image Question Answering using Convolutional Neural Network with Dynamic Parameter Prediction",
      "author" : [ "Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han" ],
      "venue" : "arXiv preprint arXiv:1511.05756,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Exploring Models and Data for Image Question Answering",
      "author" : [ "Mengye Ren", "Ryan Kiros", "Richard Zemel" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Reasoning about Entailment with Neural Attention",
      "author" : [ "Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiský", "Phil Blunsom" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Learning representations by back-propagating",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "errors. Nature,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1986
    }, {
      "title" : "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "author" : [ "Karen Simonyan", "Andrew Zisserman" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Multimodal Learning with Deep Boltzmann Machines",
      "author" : [ "Nitish Srivastava", "Ruslan R Salakhutdinov" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "End-To-End Memory Networks",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "Tijmen Tieleman", "Geoffrey Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge from External Sources",
      "author" : [ "Qi Wu", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "Dynamic Memory Networks for Visual and Textual Question Answering",
      "author" : [ "Caiming Xiong", "Stephen Merity", "Richard Socher" ],
      "venue" : "arXiv preprint arXiv:1603.01417,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2016
    }, {
      "title" : "Stacked Attention Networks for Image Question Answering",
      "author" : [ "Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola" ],
      "venue" : "arXiv preprint arXiv:1511.02274,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Deep residual learning [6] not only advances the studies in object recognition problems, but also gives a general framework for deep neural networks.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 24,
      "context" : "Many attentional models utilize the residual learning to deal with various tasks, including textual reasoning [25, 21] and visual question-answering [29].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 20,
      "context" : "Many attentional models utilize the residual learning to deal with various tasks, including textual reasoning [25, 21] and visual question-answering [29].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 28,
      "context" : "Many attentional models utilize the residual learning to deal with various tasks, including textual reasoning [25, 21] and visual question-answering [29].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 5,
      "context" : "In this paper, we propose Multimodal Residual Networks (MRN) to learn multimodality of visual question-answering tasks exploiting the excellence of deep residual learning [6].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 21,
      "context" : "The visualization method uses back-propagation algorithm [22] for the difference between the visual input and the output of the joint residual mapping.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "1 Deep Residual Learning Deep residual learning [6] allows neural networks to have a deeper structure of over-100 layers.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : "The very deep neural networks are usually hard to be optimized even though the well-known activation functions and regularization techniques are applied [17, 7, 9].",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "The very deep neural networks are usually hard to be optimized even though the well-known activation functions and regularization techniques are applied [17, 7, 9].",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "The very deep neural networks are usually hard to be optimized even though the well-known activation functions and regularization techniques are applied [17, 7, 9].",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 28,
      "context" : "2 Stacked Attention Networks Stacked Attention Networks (SAN) [29] explicitly learns the weights of visual feature vectors to select a small portion of visual information for a given question vector.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Deep residual learning emphasizes the importance of identity (or linear) shortcuts to have the nonlinear mappings efficiently learn only residuals [6].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 28,
      "context" : "Since Equation 18 in [29] shows a question vector transferred directly through successive layers of the attention networks.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 28,
      "context" : "[29] assume that an appropriate choice of weights on visual feature vectors for a given question vector sufficiently captures the joint representation for answering.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] propose an element-wise multiplication of a question vector and a visual feature vector after appropriate embeddings for a joint model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "This makes a strong baseline outperforming some of the recent works [19, 2].",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "This makes a strong baseline outperforming some of the recent works [19, 2].",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 28,
      "context" : ") Based on these observations, we follow the shortcut mapping and the stacking architecture of SAN [29]; however, the element-wise multiplication is used for the joint residual function F .",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 28,
      "context" : "The base model (a) has a shortcut for a question vector as SAN does [29], and the joint residual function takes the form of the Deep Q+I model’s joint function [15].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "The base model (a) has a shortcut for a question vector as SAN does [29], and the joint residual function takes the form of the Deep Q+I model’s joint function [15].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "1 Visual QA Dataset We choose the Visual QA (VQA) dataset [1] for the evaluation of our models.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "Other datasets may not be ideal, since they have limited number of examples to train and test [16], or have synthesized questions from the image captions [14, 20].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "Other datasets may not be ideal, since they have limited number of examples to train and test [16], or have synthesized questions from the image captions [14, 20].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 19,
      "context" : "Other datasets may not be ideal, since they have limited number of examples to train and test [16], or have synthesized questions from the image captions [14, 20].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "[1] proposed a new accuracy metric as follows:",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "2 Implementation Torch framework and rnn package [13] are used to build our models.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "For efficient computation of variable-length questions, TrimZero is used to trim out zero vectors [11].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "Preprocessing We follow the same preprocessing procedure of DeeperLSTM+NormalizedCNN [15] (Deep Q+I) by default.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "The questions are tokenized using Python Natural Language Toolkit (nltk) [3].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "Pretrained Models A question vector q ∈ R(2,400) is the last output vector of GRU [4], initialized with the parameters of Skip-Thought Vectors [12].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 11,
      "context" : "Pretrained Models A question vector q ∈ R(2,400) is the last output vector of GRU [4], initialized with the parameters of Skip-Thought Vectors [12].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "[19], this method shows effectiveness of question embedding in visual question-answering tasks.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "A visual feature vector v is an output of the first fully-connected layer of VGG-19 networks [23], whose dimension is 4,096.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 5,
      "context" : "Alternatively, ResNet-152 [6] is used, whose dimension is of 2,048.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "Postprocessing Image captioning model [10] is used to improve the accuracy of Other type.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 28,
      "context" : "The precision of some accuracies [29, 2] are one less than others, so, zero-filled to match others.",
      "startOffset" : 33,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "The precision of some accuracies [29, 2] are one less than others, so, zero-filled to match others.",
      "startOffset" : 33,
      "endOffset" : 40
    }, {
      "referenceID" : 25,
      "context" : "The RMSProp [26] is used for optimization, and dropouts [7, 5] are used for regularization.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 6,
      "context" : "The RMSProp [26] is used for optimization, and dropouts [7, 5] are used for regularization.",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 4,
      "context" : "The RMSProp [26] is used for optimization, and dropouts [7, 5] are used for regularization.",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 17,
      "context" : "We carefully select alternative models (a)-(c) for the importance of embeddings in multimodal learning [18, 24], (d) for the effectiveness of identity mapping as reported by [6], and (e) for the confirmation of using question-only shortcuts in the multiple blocks as in [29].",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 23,
      "context" : "We carefully select alternative models (a)-(c) for the importance of embeddings in multimodal learning [18, 24], (d) for the effectiveness of identity mapping as reported by [6], and (e) for the confirmation of using question-only shortcuts in the multiple blocks as in [29].",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "We carefully select alternative models (a)-(c) for the importance of embeddings in multimodal learning [18, 24], (d) for the effectiveness of identity mapping as reported by [6], and (e) for the confirmation of using question-only shortcuts in the multiple blocks as in [29].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 28,
      "context" : "We carefully select alternative models (a)-(c) for the importance of embeddings in multimodal learning [18, 24], (d) for the effectiveness of identity mapping as reported by [6], and (e) for the confirmation of using question-only shortcuts in the multiple blocks as in [29].",
      "startOffset" : 270,
      "endOffset" : 274
    }, {
      "referenceID" : 11,
      "context" : "The effects of other various options, Skip-Thought Vectors [12] for parameter initialization, Bayesian Dropout [5] for regularization, image captioning model [10] for postprocessing, and the usage of shortcut connections, are explored in Appendix A.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "The effects of other various options, Skip-Thought Vectors [12] for parameter initialization, Bayesian Dropout [5] for regularization, image captioning model [10] for postprocessing, and the usage of shortcut connections, are explored in Appendix A.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "The effects of other various options, Skip-Thought Vectors [12] for parameter initialization, Bayesian Dropout [5] for regularization, image captioning model [10] for postprocessing, and the usage of shortcut connections, are explored in Appendix A.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "The ResNet visual features are also used in the previous work [8]; however, our model achieves a remarkably better performance with a large margin (see Table 3).",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 28,
      "context" : "Unlike the other works [29, 28] that use explicit attention parameters, MRN does not use any explicit attentional mechanism.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 27,
      "context" : "Unlike the other works [29, 28] that use explicit attention parameters, MRN does not use any explicit attentional mechanism.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 28,
      "context" : "14×14), our visualization method shows a higher resolution than others [29, 28].",
      "startOffset" : 71,
      "endOffset" : 79
    }, {
      "referenceID" : 27,
      "context" : "14×14), our visualization method shows a higher resolution than others [29, 28].",
      "startOffset" : 71,
      "endOffset" : 79
    } ],
    "year" : 2016,
    "abstractText" : "Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.",
    "creator" : null
  }
}