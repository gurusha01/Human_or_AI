{
  "name" : "c86a7ee3d8ef0b551ed58e354a836f2b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Barzilai-Borwein Step Size for Stochastic Gradient Descent",
    "authors" : [ "Conghui Tan", "Shiqian Ma", "Yuqiu Qian" ],
    "emails" : [ "chtan@se.cuhk.edu.hk", "sqma@se.cuhk.edu.hk", "dyh@lsec.cc.ac.cn", "qyq79@connect.hku.hk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The following optimization problem, which minimizes the sum of cost functions over samples from a finite training set, appears frequently in machine learning:\nmin F (x) ≡ 1 n n∑ i=1 fi(x), (1)\nwhere n is the sample size, and each fi : Rd → R is the cost function corresponding to the i-th sample data. Throughout this paper, we assume that each fi is convex and differentiable, and the function F is strongly convex. Problem (1) is challenging when n is extremely large so that computing F (x) and ∇F (x) for given x is prohibited. Stochastic gradient descent (SGD) method and its variants have been the main approaches for solving (1). In the t-th iteration of SGD, a random training sample it is chosen from {1, 2, . . . , n} and the iterate xt is updated by\nxt+1 = xt − ηt∇fit(xt), (2)\nwhere∇fit(xt) denotes the gradient of the it-th component function at xt, and ηt > 0 is the step size (a.k.a. learning rate). In (2), it is usually assumed that∇fit is an unbiased estimation to∇F , i.e.,\nE[∇fit(xt) | xt] = ∇F (xt). (3)\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nHowever, it is known that the total number of gradient evaluations of SGD depends on the variance of the stochastic gradients and it is of sublinear convergence rate for strongly convex and smooth problem (1). As a result, many works along this line have been focusing on designing variants of SGD that can reduce the variance and improve the complexity. Some popular methods include the stochastic average gradient (SAG) method [16], the SAGA method [7], the stochastic dual coordinate ascent (SDCA) method [17], and the stochastic variance reduced gradient (SVRG) method [10]. These methods are proven to converge linearly on strongly convex problems.\nAs pointed out by Le Roux et al. [16], one important issue regarding to stochastic algorithms that has not been fully addressed in the literature, is how to choose an appropriate step size ηt while running the algorithm. In classical gradient descent method, the step size is usually obtained by employing line search techniques. However, line search is computationally prohibited in stochastic gradient methods because one only has sub-sampled information of function value and gradient. As a result, for SGD and its variants used in practice, people usually use a diminishing step size ηt, or use a best-tuned fixed step size. Neither of these two approaches can be efficient.\nSome recent works that discuss the choice of step size in SGD are summarized as follows. AdaGrad [8] scales the gradient by the square root of the accumulated magnitudes of the gradients in the past iterations, but this still requires to decide a fixed step size η. [16] suggests a line search technique on the component function fik(x) selected in each iteration, to estimate step size for SAG. [12] suggests performing line search for an estimated function, which is evaluated by a Gaussian process with samples fit(xt). [13] suggests to generate the step sizes by a given function with an unknown parameter, and to use the online SGD to update this unknown parameter.\nOur contributions in this paper are in several folds. (i) We propose to use the Barzilai-Borwein (BB) method to compute the step size for SGD and SVRG. The two new methods are named as SGD-BB and SVRG-BB, respectively. The per-iteration computational cost of SGD-BB and SVRG-BB is almost the same as SGD and SVRG, respectively. (ii) We prove the linear convergence of SVRG-BB for strongly convex function. As a by-product, we show the linear convergence of SVRG with Option I (SVRG-I) proposed in [10]. Note that in [10] only convergence of SVRG with Option II (SVRG-II) was given, and the proof for SVRG-I has been missing in the literature. However, SVRG-I is numerically a better choice than SVRG-II, as demonstrated in [10]. (iii) We conduct numerical experiments for SGD-BB and SVRG-BB on solving logistic regression and SVM problems. The numerical results show that SGD-BB and SVRG-BB are comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes. We also compare SGD-BB with some advanced SGD variants, and demonstrate that our method is superior.\nThe rest of this paper is organized as follows. In Section 2 we briefly introduce the BB method in the deterministic setting. In Section 3 we propose our SVRG-BB method, and prove its linear convergence for strongly convex function. As a by-product, we also prove the linear convergence of SVRG-I. In Section 4 we propose our SGD-BB method. A smoothing technique is also implemented to improve the performance of SGD-BB. Finally, we conduct some numerical experiments for SVRG-BB and SGD-BB in Section 5."
    }, {
      "heading" : "2 The Barzilai-Borwein Step Size",
      "text" : "The BB method, proposed by Barzilai and Borwein in [2], has been proven to be very successful in solving nonlinear optimization problems. The key idea behind the BB method is motivated by quasi-Newton methods. Suppose we want to solve the unconstrained minimization problem\nmin x f(x), (4)\nwhere f is differentiable. A typical iteration of quasi-Newton methods for solving (4) is:\nxt+1 = xt −B−1t ∇f(xt), (5)\nwhere Bt is an approximation of the Hessian matrix of f at the current iterate xt. The most important feature of Bt is that it must satisfy the so-called secant equation: Btst = yt, where st = xt − xt−1 and yt = ∇f(xt)−∇f(xt−1) for t ≥ 1. It is noted that in (5) one needs to solve a linear system, which may be time consuming when Bt is large and dense.\nOne way to alleviate this burden is to use the BB method, which replacesBt by a scalar matrix (1/ηt)I . However, one cannot choose a scalar ηt such that the secant equation holds with Bt = (1/ηt)I . Instead, one can find ηt such that the residual of the secant equation, i.e., ‖(1/ηt)st − yt‖22, is minimized, which leads to the following choice of ηt:\nηt = ‖st‖22/ ( s>t yt ) . (6)\nTherefore, a typical iteration of the BB method for solving (4) is\nxt+1 = xt − ηt∇f(xt), (7)\nwhere ηt is computed by (6).\nFor convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [14, 15, 6, 9, 4, 5, 3] and references therein. Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [21], sparse reconstruction [20] and image processing [19]."
    }, {
      "heading" : "3 Barzilai-Borwein Step Size for SVRG",
      "text" : "We see from (7) and (6) that the BB method does not need any parameter and the step size is computed while running the algorithm. This has been the main motivation for us to work out a black-box stochastic gradient descent method that can compute the step size automatically without requiring any parameters. In this section, we propose to incorporate the BB step size to SVRG, which leads to the SVRG-BB method."
    }, {
      "heading" : "3.1 SVRG-BB Method",
      "text" : "Stochastic variance reduced gradient (SVRG) is a variant of SGD proposed in [10], which utilizes a variance reduction technique to alleviate the impact of the random samplings of the gradients. SVRG computes the full gradient ∇F (x) of (1) in every m iterations, where m is a pre-given integer, and the full gradient is then used for generating stochastic gradients with lower variance in the next m iterations (the next epoch). In SVRG, the step size η needs to be provided by the user. According to [10], the choice of η depends on the Lipschitz constant of F , which is usually difficult to estimate in practice.\nOur SVRG-BB algorithm is described in Algorithm 1. The only difference between SVRG and SVRG-BB is that in the latter we use BB method to compute the step size ηk, instead of using a prefixed η as in SVRG.\nAlgorithm 1 SVRG with BB step size (SVRG-BB) Parameters: update frequency m, initial point x̃0, initial step size η0 (only used in the first epoch) for k = 0, 1, · · · do gk = 1 n ∑n i=1∇fi(x̃k)\nif k > 0 then ηk = 1 m · ‖x̃k − x̃k−1‖ 2 2/(x̃k − x̃k−1)>(gk − gk−1) (4) end if x0 = x̃k for t = 0, · · · ,m− 1 do\nRandomly pick it ∈ {1, . . . , n} xt+1 = xt − ηk(∇fit(xt)−∇fit(x̃k) + gk)\nend for Option I: x̃k+1 = xm Option II: x̃k+1 = xt for randomly chosen t ∈ {1, . . . ,m}\nend for\nRemark 1. A few remarks are in demand for the SVRG-BB algorithm. (i) If we always set ηk = η in SVRG-BB instead of using (4), then it reduces to the original SVRG. (ii) One may notice that ηk is equal to the step size computed by the BB formula (6) divided by m. This is because in the inner loop for updating xt, m unbiased gradient estimators are added to x0 to\nget xm. (iii) For the first epoch of SVRG-BB, a step size η0 needs to be specified. However, we observed from our numerical experiments that the performance of SVRG-BB is not sensitive to the choice of η0. (iv) The BB step size can also be naturally incorporated to other SVRG variants, such as SVRG with batching [1]."
    }, {
      "heading" : "3.2 Linear Convergence Analysis",
      "text" : "In this section, we analyze the linear convergence of SVRG-BB (Algorithm 1) for solving (1) with strongly convex objective F (x), and as a by-product, our analysis also proves the linear convergence of SVRG-I. The proofs in this section are provided in the supplementary materials. The following assumption is made throughout this section.\nAssumption 1. We assume that (3) holds for any xt. We assume that the objective function F (x) is µ-strongly convex, i.e.,\nF (y) ≥ F (x) +∇F (x)>(y − x) + µ 2 ‖x− y‖22, ∀x, y ∈ Rd.\nWe also assume that the gradient of each component function fi(x) is L-Lipschitz continuous, i.e.,\n‖∇fi(x)−∇fi(y)‖2 ≤ L‖x− y‖2, ∀x, y ∈ Rd.\nUnder this assumption, it is easy to see that∇F (x) is also L-Lipschitz continuous.\nWe first provide the following lemma, which reveals the relationship between the distances of two consecutive iterates to the optimal point.\nLemma 1. Define\nαk := (1− 2ηkµ(1− ηkL))m + 4ηkL\n2\nµ(1− ηkL) . (8)\nFor both SVRG-I and SVRG-BB, we have the following inequality for the k-th epoch:\nE ‖x̃k+1 − x∗‖22 < αk‖x̃k − x ∗‖22,\nwhere x∗ is the optimal solution to (1).\nThe linear convergence of SVRG-I follows immediately.\nCorollary 1. In SVRG-I, if m and η are chosen such that\nα := (1− 2ηµ(1− ηL))m + 4ηL 2\nµ(1− ηL) < 1, (9)\nthen SVRG-I converges linearly in expectation:\nE ‖x̃k − x∗‖22 < α k‖x̃0 − x∗‖22.\nRemark 2. We now give some remarks on this convergence result. (i) To the best of our knowledge, this is the first time that the linear convergence of SVRG-I is established. (ii) The convergence result given in Corollary 1 is for the iterates x̃k, while the one given in [10] is for the objective function values F (x̃k).\nThe following theorem establishes the linear convergence of SVRG-BB (Algorithm 1).\nTheorem 1. Denote θ = (1− e−2µ/L)/2. Note that θ ∈ (0, 1/2). In SVRG-BB, if m is chosen such that\nm > max\n{ 2\nlog(1− 2θ) + 2µ/L , 4L2 θµ2 + L µ\n} , (10)\nthen SVRG-BB (Algorithm 1) converges linearly in expectation:\nE ‖x̃k − x∗‖22 < (1− θ) k‖x̃0 − x∗‖22."
    }, {
      "heading" : "4 Barzilai-Borwein Step Size for SGD",
      "text" : "In this section, we propose to incorporate the BB method to SGD (2). The BB method does not apply to SGD directly, because SGD never computes the full gradient ∇F (x). One may suggest to use ∇fit+1(xt+1) − ∇fit(xt) to estimate ∇F (xt+1) − ∇F (xt) when computing the BB step size using formula (6). However, this approach does not work well because of the variance of the stochastic gradients. The recent work by Sopyła and Drozda [18] suggested several variants of this idea to compute an estimated BB step size using the stochastic gradients. However, these ideas lack theoretical justifications and the numerical results in [18] show that these approaches are inferior to some existing methods.\nThe SGD-BB algorithm we propose in this paper works in the following manner. We call every m iterations of SGD as one epoch. Following the idea of SVRG-BB, SGD-BB also uses the same step size computed by the BB formula in every epoch. Our SGD-BB algorithm is described as in Algorithm 2.\nAlgorithm 2 SGD with BB step size (SGD-BB) Parameters: update frequency m, initial step sizes η0 and η1 (only used in the first two epochs), weighting parameter β ∈ (0, 1), initial point x̃0 for k = 0, 1, · · · do\nif k > 0 then ηk = 1 m · ‖x̃k − x̃k−1‖ 2 2/|(x̃k − x̃k−1)>(ĝk − ĝk−1)| end if x0 = x̃k ĝk+1 = 0 for t = 0, · · · ,m− 1 do\nRandomly pick it ∈ {1, . . . , n} xt+1 = xt − ηk∇fit(xt) (∗) ĝk+1 = β∇fit(xt) + (1− β)ĝk+1\nend for x̃k+1 = xm\nend for\nRemark 3. We have a few remarks about SGD-BB (Algorithm 2). (i) SGD-BB takes a convex combination of the m stochastic gradients in one epoch as an estimation of the full gradient with parameter β. The performance of SGD-BB on different problems is not sensitive to the choice of β. For example, setting β = 10/m worked well for all test problems in our experiments. (ii) Note that for computing ηk in Algorithm 2, we actually take the absolute value for the BB formula (6). This is because that unlike SVRG-BB, ĝk in Algorithm 2 is not an exact full gradient. As a result, the step size generated by (6) can be negative. This can be seen from the following argument. Consider a simple case in which β = 1/m, approximately we have\nĝk = 1\nm m−1∑ t=0 ∇fit(xt). (11)\nIt is easy to see that x̃k − x̃k−1 = −mηk−1ĝk. By substituting this equality into the equation for computing ηk in Algorithm 2, we have\nηk =(1/m) · ‖x̃k − x̃k−1‖2/|(x̃k − x̃k−1)>(ĝk − ĝk−1)|\n= ηk−1∣∣1− ĝ>k ĝk−1/‖ĝk‖22∣∣ . (12)\nWithout taking the absolute value, the denominator of (12) is ĝ>k ĝk−1/‖ĝk‖22 − 1, which is usually negative in stochastic settings. (iii) Moreover, from (12) we have the following observations. If ĝ>k ĝk−1 < 0, then ηk is smaller than ηk−1. This is reasonable because ĝ>k ĝk−1 < 0 indicates that the step size is too large and we need to shrink it. If ĝ>k ĝk−1 > 0, then it indicates that we should be more aggressive to take larger step size. Hence, the way we compute ηk in Algorithm 2 is in a sense to dynamically adjust the step size, by\nevaluating whether we are moving the iterates along the right direction. This kind of idea can be traced back to [11].\nNote that SGD-BB requires the averaged gradients in two epochs to compute the BB step size. Therefore, we need to specify the step sizes η0 and η1 for the first two epochs. From our numerical experiments, we found that the performance of SGD-BB is not sensitive to choices of η0 and η1."
    }, {
      "heading" : "4.1 Smoothing Technique for the Step Sizes",
      "text" : "Due to the randomness of the stochastic gradients, the step size computed in SGD-BB may vibrate drastically sometimes and this may cause instability of the algorithm. Inspired by [13], we propose the following smoothing technique to stabilize the step size.\nIt is known that in order to guarantee the convergence of SGD, the step sizes are required to be diminishing. Similar as in [13], we assume that the step sizes are in the form of C/φ(k), where C > 0 is an unknown constant that needs to be estimated, φ(k) is a pre-specified function that controls the decreasing rate of the step size, and a typical choice of function φ is φ(k) = k + 1. In the k-th epoch of Algorithm 2, we have all the previous step sizes η2, η3, . . . , ηk generated by the BB method, while the step sizes generated by the function C/φ(k) are given by C/φ(2), C/φ(3), . . . , C/φ(k). In order to ensure that these two sets of step sizes are close to each other, we solve the following optimization problem to determine the unknown parameter C:\nĈk := argmin C k∑ j=2 [ log C φ(j) − log ηj ]2 . (13)\nHere we take the logarithms of the step sizes to ensure that the estimation is not dominated by those ηj’s with large magnitudes. It is easy to verify that the solution to (13) is given by Ĉk =∏k j=2 [ηjφ(j)] 1/(k−1). Therefore, the smoothed step size for the k-th epoch of Algorithm 2 is:\nη̃k = Ĉk/φ(k) = k∏ j=2 [ηjφ(j)] 1/(k−1) /φ(k). (14)\nThat is, we replace the ηk in equation (∗) of Algorithm 2 by η̃k in (14). In practice, we do not need to store all the ηj’s and Ĉk can be computed recursively by Ĉk = Ĉ (k−2)/(k−1) k−1 · [ηkφ(k)] 1/(k−1)."
    }, {
      "heading" : "4.2 Incorporating BB Step Size to SGD Variants",
      "text" : "The BB step size and the smoothing technique we used in SGD-BB (Algorithm 2) can also be used in other variants of SGD, because these methods only require the gradient estimations, which are accessible in all SGD variants. For example, when replacing the stochastic gradient in Algorithm 2 by the averaged gradients in SAG method, we obtain SAG with BB step size (denoted as SAG-BB). Because SAG does not need diminishing step sizes to ensure convergence, in the smoothing technique we just choose φ(k) ≡ 1. The details of SAG-BB are given in the supplementary material."
    }, {
      "heading" : "5 Numerical Experiments",
      "text" : "In this section, we conduct numerical experiments to demonstrate the efficacy of our SVRG-BB (Algorithm 1) and SGD-BB (Algorithm 2) algorithms. In particular, we apply SVRG-BB and SGDBB to solve two standard testing problems in machine learning: logistic regression with `2-norm regularization (LR), and the squared hinge loss SVM with `2-norm regularization (SVM):\n(LR ) min x\nF (x) = 1\nn n∑ i=1 log [ 1 + exp(−bia>i x) ] + λ 2 ‖x‖22, (15)\n(SVM ) min x\nF (x) = 1\nn n∑ i=1 ( [1− bia>i x]+ )2 + λ 2 ‖x‖22, (16)\nwhere ai ∈ Rd and bi ∈ {±1} are the feature vector and class label of the i-th sample, respectively, and λ > 0 is a weighting parameter.\nWe tested SVRG-BB and SGD-BB on three standard real data sets, which were downloaded from the LIBSVM website1. Detailed information of the data sets are given in Table 1."
    }, {
      "heading" : "5.1 Numerical Results of SVRG-BB",
      "text" : "In this section, we compare SVRG-BB (Algorithm 1) and SVRG with fixed step size for solving (15) and (16). We used the best-tuned step size for SVRG, and three different initial step sizes η0 for SVRG-BB. For both SVRG-BB and SVRG, we set m = 2n as suggested in [10].\nThe comparison results of SVRG-BB and SVRG are shown in Figure 1. In all sub-figures, the x-axis denotes the number of epochs k, i.e., the number of outer loops in Algorithm 1. In Figures 1(a), 1(b) and 1(c), the y-axis denotes the sub-optimality F (x̃k)−F (x∗), and in Figures 1(d), 1(e) and 1(f), the y-axis denotes the corresponding step sizes ηk. x∗ is obtained by running SVRG with the best-tuned step size until it converges. In all sub-figures, the dashed lines correspond to SVRG with fixed step sizes given in the legends of the figures. Moreover, the dashed lines in black color always represent SVRG with best-tuned step size, and the green and red lines use a relatively larger and smaller fixed step sizes respectively. The solid lines correspond to SVRG-BB with different initial step sizes η0.\nIt can be seen from Figures 1(a), 1(b) and 1(c) that, SVRG-BB can always achieve the same level of sub-optimality as SVRG with the best-tuned step size. Although SVRG-BB needs slightly more epochs compared with SVRG with the best-tuned step size, it clearly outperforms SVRG with the\n1www.csie.ntu.edu.tw/~cjlin/libsvmtools/.\nother two choices of step sizes. Moreover, from Figures 1(d), 1(e) and 1(f) we see that the step sizes computed by SVRG-BB converge to the best-tuned step sizes after about 10 to 15 epochs. From Figure 1 we also see that SVRG-BB is not sensitive to the choice of η0. Therefore, SVRG-BB has very promising potential in practice because it generates the best step sizes automatically while running the algorithm."
    }, {
      "heading" : "5.2 Numerical Results of SGD-BB",
      "text" : "In this section, we compare SGD-BB with smoothing technique (Algorithm 2) with SGD for solving (15) and (16). We set m = n, β = 10/m and η1 = η0 in our experiments. We used φ(k) = k + 1 when applying the smoothing technique. Since SGD requires diminishing step size to converge, we tested SGD with diminishing step size in the form η/(k + 1) with different constants η. The comparison results are shown in Figure 2. Similar as Figure 1, the dashed line with black color represents SGD with the best-tuned η, and the green and red dashed lines correspond to the other two choices of η; the solid lines represent SGD-BB with different η0.\nFrom Figures 2(a), 2(b) and 2(c) we can see that SGD-BB gives comparable or even better suboptimality than SGD with best-tuned diminishing step size, and SGD-BB is significantly better than SGD with the other two choices of step size. From Figures 2(d), 2(e) and 2(f) we see that after only a few epochs, the step sizes generated by SGD-BB approximately coincide with the best-tuned ones. It can also be seen that after only a few epochs, the step sizes are stabilized by the smoothing technique and they approximately follow the same decreasing trend as the best-tuned diminishing step sizes."
    }, {
      "heading" : "5.3 Comparison with Other Methods",
      "text" : "We also compared our algorithms with many existing related methods. Experimental results also demonstrated the superiority of our methods. The results are given in the supplementary materials."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Research of Shiqian Ma was supported in part by the Hong Kong Research Grants Council General Research Fund (Grant 14205314). Research of Yu-Hong Dai was supported by the Chinese NSF (Nos. 11631013 and 11331012) and the National 973 Program of China (No. 2015CB856000)."
    } ],
    "references" : [ {
      "title" : "Stop wasting my gradients: Practical SVRG",
      "author" : [ "R. Babanezhad", "M.O. Ahmed", "A. Virani", "M. Schmidt", "K. Konečnỳ", "S. Sallinen" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2242–2250,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Two-point step size gradient methods",
      "author" : [ "J. Barzilai", "J.M. Borwein" ],
      "venue" : "IMA Journal of Numerical Analysis, 8(1):141–148,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "A new analysis on the Barzilai-Borwein gradient method",
      "author" : [ "Y.-H. Dai" ],
      "venue" : "Journal of Operations Research Society of China, 1(2):187–198,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Projected Barzilai-Borwein methods for large-scale box-constrained quadratic programming",
      "author" : [ "Y.-H. Dai", "R. Fletcher" ],
      "venue" : "Numerische Mathematik, 100(1):21–47,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The cyclic Barzilai-Borwein method for unconstrained optimization",
      "author" : [ "Y.-H. Dai", "W.W. Hager", "K. Schittkowski", "H. Zhang" ],
      "venue" : "IMA Journal of Numerical Analysis, 26(3):604–627,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "R-linear convergence of the Barzilai and Borwein gradient method",
      "author" : [ "Y.-H. Dai", "L. Liao" ],
      "venue" : "IMA Journal of Numerical Analysis, 22:1–10,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1646–1654,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research, 12:2121–2159,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the Barzilai-Borwein method",
      "author" : [ "R. Fletcher" ],
      "venue" : "Optimization and control with applications, pages 235–256. Springer,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 315–323,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Accelerated stochastic approximation",
      "author" : [ "H. Kesten" ],
      "venue" : "The Annals of Mathematical Statistics, 29(1):41–59,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1958
    }, {
      "title" : "Probabilistic line searches for stochastic optimization",
      "author" : [ "M. Mahsereci", "P. Hennig" ],
      "venue" : "arXiv preprint arXiv:1502.02846,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Speed learning on the fly",
      "author" : [ "P.Y. Massé", "Y. Ollivier" ],
      "venue" : "arXiv preprint arXiv:1511.02540,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the Barzilai and Borwein choice of steplength for the gradient method",
      "author" : [ "M. Raydan" ],
      "venue" : "IMA Journal of Numerical Analysis, 13(3):321–326,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "The Barzilai and Borwein gradient method for the large scale unconstrained minimization problem",
      "author" : [ "M. Raydan" ],
      "venue" : "SIAM Journal on Optimization, 7(1):26–33,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "R.L. Roux", "M. Schmidt", "F. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2663–2671,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Jornal of Machine Learning Research, 14:567–599,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Stochastic gradient descent with Barzilai-Borwein update step for svm",
      "author" : [ "K. Sopyła", "P. Drozda" ],
      "venue" : "Information Sciences, 316:218–233,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Projected Barzilai-Borwein methods for large scale nonnegative image restorations",
      "author" : [ "Y. Wang", "S. Ma" ],
      "venue" : "Inverse Problems in Science and Engineering, 15(6):559–583,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization, and continuation",
      "author" : [ "Z. Wen", "W. Yin", "D. Goldfarb", "Y. Zhang" ],
      "venue" : "SIAM J. SCI. COMPUT, 32(4):1832–1857,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Sparse reconstruction by separable approximation",
      "author" : [ "S.J. Wright", "R.D. Nowak", "M.A.T. Figueiredo" ],
      "venue" : "IEEE Transactions on Signal Processing, 57(7):2479–2493,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result has been missing in the literature.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "Some popular methods include the stochastic average gradient (SAG) method [16], the SAGA method [7], the stochastic dual coordinate ascent (SDCA) method [17], and the stochastic variance reduced gradient (SVRG) method [10].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "Some popular methods include the stochastic average gradient (SAG) method [16], the SAGA method [7], the stochastic dual coordinate ascent (SDCA) method [17], and the stochastic variance reduced gradient (SVRG) method [10].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 16,
      "context" : "Some popular methods include the stochastic average gradient (SAG) method [16], the SAGA method [7], the stochastic dual coordinate ascent (SDCA) method [17], and the stochastic variance reduced gradient (SVRG) method [10].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "Some popular methods include the stochastic average gradient (SAG) method [16], the SAGA method [7], the stochastic dual coordinate ascent (SDCA) method [17], and the stochastic variance reduced gradient (SVRG) method [10].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 15,
      "context" : "[16], one important issue regarding to stochastic algorithms that has not been fully addressed in the literature, is how to choose an appropriate step size ηt while running the algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "AdaGrad [8] scales the gradient by the square root of the accumulated magnitudes of the gradients in the past iterations, but this still requires to decide a fixed step size η.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 15,
      "context" : "[16] suggests a line search technique on the component function fik(x) selected in each iteration, to estimate step size for SAG.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] suggests performing line search for an estimated function, which is evaluated by a Gaussian process with samples fit(xt).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] suggests to generate the step sizes by a given function with an unknown parameter, and to use the online SGD to update this unknown parameter.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "As a by-product, we show the linear convergence of SVRG with Option I (SVRG-I) proposed in [10].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "Note that in [10] only convergence of SVRG with Option II (SVRG-II) was given, and the proof for SVRG-I has been missing in the literature.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "However, SVRG-I is numerically a better choice than SVRG-II, as demonstrated in [10].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 1,
      "context" : "The BB method, proposed by Barzilai and Borwein in [2], has been proven to be very successful in solving nonlinear optimization problems.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [14, 15, 6, 9, 4, 5, 3] and references therein.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [14, 15, 6, 9, 4, 5, 3] and references therein.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [14, 15, 6, 9, 4, 5, 3] and references therein.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [14, 15, 6, 9, 4, 5, 3] and references therein.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [14, 15, 6, 9, 4, 5, 3] and references therein.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [14, 15, 6, 9, 4, 5, 3] and references therein.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "For convergence analysis, generalizations and variants of the BB method, we refer the interested readers to [14, 15, 6, 9, 4, 5, 3] and references therein.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [21], sparse reconstruction [20] and image processing [19].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [21], sparse reconstruction [20] and image processing [19].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 18,
      "context" : "Recently, BB method has been successfully applied for solving problems arising from emerging applications, such as compressed sensing [21], sparse reconstruction [20] and image processing [19].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 9,
      "context" : "Stochastic variance reduced gradient (SVRG) is a variant of SGD proposed in [10], which utilizes a variance reduction technique to alleviate the impact of the random samplings of the gradients.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "According to [10], the choice of η depends on the Lipschitz constant of F , which is usually difficult to estimate in practice.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "(iv) The BB step size can also be naturally incorporated to other SVRG variants, such as SVRG with batching [1].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "(ii) The convergence result given in Corollary 1 is for the iterates x̃k, while the one given in [10] is for the objective function values F (x̃k).",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "The recent work by Sopyła and Drozda [18] suggested several variants of this idea to compute an estimated BB step size using the stochastic gradients.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "However, these ideas lack theoretical justifications and the numerical results in [18] show that these approaches are inferior to some existing methods.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "This kind of idea can be traced back to [11].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "Inspired by [13], we propose the following smoothing technique to stabilize the step size.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 12,
      "context" : "Similar as in [13], we assume that the step sizes are in the form of C/φ(k), where C > 0 is an unknown constant that needs to be estimated, φ(k) is a pre-specified function that controls the decreasing rate of the step size, and a typical choice of function φ is φ(k) = k + 1.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "For both SVRG-BB and SVRG, we set m = 2n as suggested in [10].",
      "startOffset" : 57,
      "endOffset" : 61
    } ],
    "year" : 2016,
    "abstractText" : "One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization methods, the common practice in SGD is either to use a diminishing step size, or to tune a step size by hand, which can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective functions. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result has been missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants.",
    "creator" : null
  }
}