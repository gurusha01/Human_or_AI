{"title": "Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA", "abstract": "", "id": "81e5f81db77c596492e6f1a5a792ed53", "authors": ["Vincent Q. Vu", "Juhee Cho", "Jing Lei", "Karl Rohe"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The authors address the problem of sparse principal component analysis when more than a single component is to be estimated. The standard method computes the principal components one-by-one and uses a heuristic step often called \"deflation\" to switch to the next component. This step-by-step sequential method is fragile and criticizing it is absolutely natural. The authors suggest to estimate the matrix using projection to the Fantope: the convex relaxation of the orthogonal matrices. \n\nThe authors provide an efficient algorithmic scheme to solve the problem and analyse the solution statistically. The paper is well written and will interest people who are working on this very popular topic. \n\nAn important question: why is the orthogonality constraint crucial in sparse PCA? in fact in standard (desne / full) PCA it is a consequence of the fact that the low-rank approximation of a matrix is provided by the eigenvalue decomposition which has orthogonal factors. Somehow when an extra sparsity assumption is maid why should we keep the orthogonality? \n\nThe experimental section is not convincing. I would have wished to see a phase-transition type of diagram to see when the performance really outperforms the rival as the factors supports overlap. The idea of computing multiple principal components at once had already been tackled by Journee et al. (ref [8] in the paper, please name all the authors by the way in [8]). Why is there no comparison with their method? the code for their method is available online. There should be a numerical comparison with a sparse matrix factorization method as well. Comparison of the supports found by each method on real data may be interesting. \n\n\n The paper is about a relevant problem, is well written and suggests a method which seems to be efficient for the goal fixed by the authors. I would like however a more convincing discussion on the orthogonality constraints which seem more embarrassing than useful. The algorithm is an incremental update of DSPCA [1] using Fantope constraint, and theoretical results are also mostly incremental, and not so exciting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors consider the Sparse PCA problem, i.e., PCA with the assumption that the \"principal vectors\" depend on only a few of the variables. They propose a convex relaxation for solving this problem and analyze its performance. An ADMM based method is also developed for solving the resulting SDP efficiently. Finally they show theoretical results when a Kendall's tau matrix is used as input instead of the usual sample covariance matrix. \n\nThe paper is clearly written and the authors have done a good job in explaining various concepts as they become used in their exposition. The results presented in this paper appear to be correct. \n\nI like the convex formulation. The techniques used to achieve it are simple and this might certainly be considered one of the strengths of the paper. It also goes along nicely with the ADMM formulation and Lemma 4.1. I have a minor suggestion : it would be nice if the authors mention the fact that they have a closed form expression for the projection onto the fantope a little earlier in the paper (possibly along with the motivation of a convex relaxation). \n\nHowever, the statistical analysis seems a little weak, particularly in the near low rank and low rank cases. If there were only d non zero eigenvalues, the bound the authors present could potentially be very far away from the minimax bound. On a related note, the authors should consider rephrasing the statement \"It is possible (with more technical work) to tighten the bound...\" in the last paragraph of page 5; it does not appear to contribute constructively without a discussion about what this technical work could be. \n\nEven if the Appendix contains all the proofs, it would be helpful if the authors present the reader with a small proof sketch after each result has been stated (also note that reviewers are not required to read the supplementary material). \n\nThe section on Simulation Results again seems a little weak. The figures need reworking as Figure 1(a) is just too hard to see on a printed version of the paper. On a related note, it will be nice to explain (in the text and/or in the caption) what \"overlapping\" and \"non-overlapping\" sparsity patterns mean precisely. It will also be helpful to insert an intuitive explanation as to why the performance is so different in these cases and when it matters. \n\nMinor point : The authors should consider rephrasing \".. has a wide range of applications - Science, engineering, ... \". It would be much more helpful to point the reader to a few specific applications with references. \n The authors propose a simple convex relaxation (with an efficient solution method) for the Sparse PCA problem. The paper is written well but the results could definitely use some intuitive justification/proof sketch in the main text. The convex relaxation (and the attendant ADMM algorithm) is nice. On the other hand, while the the performance of their algorithm is shown to be near optimal, it is not satisfactory in some natural cases like the low rank or the near low rank settings and the simulation results do not seem extremely persuasive.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper introduced a novel formulation of sparse subspace discovery problem as one of finding sparse matrices in a fanotope, a convex hull of rank-d projection matrices. The proposed formulation gives rise to a convex problem that is solved using an ADMM algorithm. Guarantees for support recovery and error in frobenius norm are provided. Finally, a set of illustrative synthetic experiments are provided demonstrating improved performance, in terms of frobenius norm, in recovering a sparse factorization of a target matrix. \nThe new formulation is elegant and provides a novel and intuitive replacement for the sparse PCA objective. The drop-in replacements for a covariance matrix, Kendal and Pearson correlation, can also be nearly-optimally decomposed enabling an analog of non-linear sparse PCA with the usual constraints familiar from the nonparanormal work. \nA bit lengthier discussion of the synthetic experiments would be helpful. The numbers of selected variables are quite high compared to what they should be for matrix Pi, which seems to be of rank 5 and fairly sparse itself. So, what is the optimal sparse decomposition for the overlap and non-overlap examples and does the method achieve this decomposition with 100s of selected variables? How well does DSPCA work in this respect? The Frobenius norm as an error term is just a part of the story. The claim that FPS gives rise to estimates that \u201care typically sparser\u201d is not supported by the results. Please provide more information here. \nNotation: Please define what \\vee stands for. The meaning can be gleaned from the context, but it is sufficiently non-standard that it would benefit from a clear definition. \nCouple of minor comments: \u201cdifficult to interepretation\u201d -> \u201cdifficult to interpret\u201d, Figure 1 use the same notation for Frobenius norm as in the rest of the paper \n Clearly written paper with novel contribution in recasting sparse PCA problem and providing a straightforward new method for estimating the sparse subspaces.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
