{"title": "Compete to Compute", "abstract": "Local competition among neighboring neurons is common in biological neural networks (NNs). We apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.", "id": "8f1d43620bc6bb580df6e80b0dc05c48", "authors": ["Rupesh K. Srivastava", "Jonathan Masci", "Sohrob Kazerounian", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "Winner-take-all modules are incorporated into an otherwise \nfeed-forward architecture, and trained via backpropagation. The \nimplementation is very straightforward, and the measured performance \nis impressive. \n\nWe should not let the simplicity, and lack of associated mathematics \nor analysis, preclude acceptance. This may be an important discovery, \nand there are a variety of obvious avenues for analysis. For example, \nthe activations in a network of this sort would be extremely sparse, \nso mathematics from the study of sparseness might be brought to bear. \nOther practical extensions also come immediately to mind. \n\nLine 141, grammar, \"only subset\" \n Adds winner-take-all modules to a feedforward architecture, and achieves significant performance improvements.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper introduces a simple form of nonlinearity into neural net architectures - forming groups of (typically two) neurons and zeroing out all the neurons in a group except the one with the highest value. While a very simple idea, it seems to give good results on classification. The authors also give evidence that the network does not forget as much as networks with more standard nonlnearities, however there might be a problem with that experiment (see below). \n\nThis paper introduces a simple form of nonlinearity into neural net architectures - forming groups of (typically two) neurons and zeroing out all the neurons in a group except the one with the highest value. While a very simple idea, it seems to give good results on classification. The authors also give evidence that the network does not forget as much as networks with more standard nonlnearities, however there might be a problem with that experiment (see below). \n\n1) There are better results on permutation invariant mnist, see table 1. in http://arxiv.org/pdf/1302.4389v3.pdf .Some of them are just feedforward networks. Also I don't agree that droput in the input should be considered a data augmentation since it doesn't assume anything about the input structure. You should have tried that experiment too. \n\n2) It is a good property for the network not to forget. However the experiments could have few issues. You wait until network reaches certain likelihood and then change the data/labels. Since the new nonlinearity peforms better on recognition, it doesn't have to work as hard to reach the likelihood and so it doesn't need to do so much training, and so the reason it doesn't forget as much can simply be that it didn't train as much to forget what it has learned. \n\n3) You should have also done a similar and related experiment. You have obtained the perofmance when you train on all digits at the same time (till convergence). Now, train till convergence one digits 1,2,3,4,5, then add the remaining digits and train on all digits. This tests how much is the network stuck in the minimum it found when training in the first phase. Ideally the performance in the second experiment (training on 1-5 and then training on 1-10) is the same as training on 1-10 from the start. It is known that this is not the case for sigmoid networks. \n\nQuality: Good, but more experiments should be there and the forgeting expriment should be better. \nClarity: Very good \nOriginality: I haven't seen it before. On negative side it is just another nonlinearity, related to max pooling, on the positive side it is a simple idea the gives good result. \nSignificance: Somewhat significant - another nonlinearity into neural networks toolbox. \n It is a simple idea that seems to work well for classification. It is also important for network not to forget, however I think the experiment presented there is not quite correct. Few other experiments would also be useful.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a local winner take all approach, where units are grouped in small sets (2 in most of the paper) and only the max of them gets to output something, all the other ones output 0. Experiments on MNIST and a sentiment analysis task show improvements relative to other approaches. \n\nThere are connections to max-pooling (only one unit out of a subset fires) and dropout (some units are shut down), but it is indeed different from both and provides a new nonlinearity to consider in the deep learning toolbox. \n\nResults on MNIST are not very impressive. Either the approach gets 0.02% better than the competitor, which means having correctly classified 2 more images out of 10000, or it is simply not as good as normal CNN (but again, by only 0.02%...). [I must say I stopped being impressed by any results on MNIST long time ago...]. The only good result is on the amazon sentiment analysis task, it seems. \n\nI also liked the experiment of section 6, but wondered if it could be shown to improve performance on a real task instead of this artificial setting. \n\nI would have liked the authors to experiment with variying the \"pool\" size and try to understand when and how it would help: is it better when the model is overfitting, underfitting, noisy, etc. A local winner take all technique is described for deep learning algorithms. It provides yet another simple non-linearity to consider in the toolbox. Results on MNIST are not very impressive, but results on sentiment analysis are.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
