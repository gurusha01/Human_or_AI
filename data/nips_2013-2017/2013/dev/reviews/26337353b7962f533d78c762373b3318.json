{"title": "Learning Multi-level Sparse Representations", "abstract": "", "id": "26337353b7962f533d78c762373b3318", "authors": ["Ferran Diego Andilla", "Fred A. Hamprecht"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper proposes a method of decomposing a matrix into a product of more than \ntwo sparse matrices in a hierarchical manner, where the rank decreases from \nthe lower to higher levels. This method is called multi-level sparse matrix \nfactorization. One benefit of such a decomposition is that different \nlevels of regularization may be performed at different levels. \n\nOverall, the general idea of multi-level sparse factorization is original and \nhas been clearly described. \n\n\n1) Tuning \n\nSome methods for data-driven tuning may be derived as opposed to a user-specified \nvalues. \n\n2) Optimization \n\nCan something be said about convergence properties of the proposed learning \naglorithm, such as convergenec speed or computational complexity?  An interesting and novel method.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The article was motivated by the difficult problem of inferring neurons and their assemblies from Calcium imaging recordings. It proposes a \"bilevel sparse heterarchical matrix factorization\", whereby neurons (i.e. their pixel-correlates), and the assemblies they belong to (potentially >1 per neuron), are inferred jointly. More generally, it offers a decomposition of an observation matrix into a product of a number of sparse matrices, with the rank decreasing from lower to higher levels. It allows for imposing (structured) sparsity constraints at every level, and is not limited to hierarchical structures, as in the previous work (Jenatton and all, 2012; Kim and Xing, 2012). In addition, their method does not require an a priori assumption on the exact number of assemblies (although the authors mention that the number of inferred assemblies depends on the optimisation parameters, that need to be defined by user). \n\nThe method is shown to work well on synthetic data: \n(i) it recovered assembly assignments of neurons with sensitivity surpassing that of MNMMF (Cichocki and Zdunek, 2006), or KSVDS (Rubinstein and all, 2010), all methods, including the proposed one, were initialised with Adina cell sorting (Diego and all, 2013). \n(ii) it detected calcium transients of single neurons with sensitivity higher than MNMMF, Adina or cell sorting. \nAn example from analysing real experimental data (calcium imaging in rat hippocampus) is also given, although the reviewer was not sure what to conclude from these images (other than neurons seem to be less spatially sparse than in the initial state derived from Adina algorithm). \n\nThe parallel inference of structures at every level, including the most basic one (the dictionary), as well as the time courses of at each level, is emphasised throughout the article, it is said to be the reason for the more robust identification of neuronal time courses. However, the inferred dictionary (images of single neurons) is never discussed. Images of such reconstructions (such as supplementary Fig. 6), next to the ground truth and reconstructions from other methods, would be far more interesting to the reviewer than the images of calcium recordings, where the ground truth is unknown (Fig. 6). \n\nQuality \nThis paper is technically sound. It provides an in-depth description of the proposed cost function, with details of the optimisation made available in the Supplemental Material. There might be some details missing, such as how to optimise the trade-off parameters (or, how they chose and what values their used in their simulations), how were the adjacency matrices binarized (to yield true and false positives for their evaluation, l. 308), or how sensitive was the method to levels of noise they tested (l.300), but overall, I find the article a complete piece of work. \n\nClarity \nThe paper is clearly written. \n\nOriginality \nTo the best of my knowledge, the proposed approach is a new one. Relevant literature is adequately referenced. \n\nSignificance \nThe results are potentially important for any lab performing calcium imaging on cell assemblies. As the authors have shown, the parallel optimisation of assembly assignments and time courses at every level of interest improves robustness of the inference. For any experiment that can assume a fairly coherent activation of neurons within an assembly - this method should be of great value. \nOne more advantage is the flexibility in imposing prior knowledge on any level of detail. (It is a pity that the method apparently didn't work well without initialising dictionary by one obtained from Adina. In principle, it should be possible to incorporate the same knowledge that Adina uses at the 0-th level of inference..) \n An interesting, technically sound article, of potential value to anyone working with structured data (i.e., calcium imaging of neural assemblies).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors present an algorithm for learning multilevel sparse representations. This is modeled as a (penalized) low rank matrix factorization problem where the number of factors is typically greater than 2 and each factor represents a different entity in the hierarchy. The factors are estimated using standard block co-ordinate descent methods where each factor is optimized while the others are kept first and sparsity promoting penalties are used at each step. \n\nThe method is discussed in the context of analyzing large scale calcium imaging data where the assumption is that the neurons can be collected into a set of (possibly overlapping) neuronal assemblies giving rise to the hierarchy pixel --> neuron --> assembly. The goal then is to estimate the location of each neuron (in terms of pixels), the membership of each neuronal assembly, and the spikes of each assembly. \n\nThe authors argue that their method performs better than other methods in artificial (but realistic) data and that it can correctly identify neural assemblies in real data. \n\n\nOriginality \n\nThe ideas presented in this paper are novel to the best of my knowledge. The three-way decomposition approach is interesting and I liked the fact that the authors also keep a standard two-way decomposition in their model and enforce consistency between the two. This extra step can make the algorithm more robust to phenomena such as a neuron in an assembly \"misses\" a spike etc. \n\nQuality \n\nMy main concern about this paper is efficiency. Large scale calcium imaging experiments typically involve tens of thousands of pixels and thousand of timesteps. Although the multilevel decomposition reduces significantly the dimensionality, and several steps of the algorithm can bee parallelized, I can imagine that the algorithm might still be quite slow and require a large number of iterations. These points should be discussed. \n\nI am also not convinced by the usage of neural assemblies. It is not clear that this is something that will often appear in practice. Also, if a subset of neurons have the same spikes then this will be reflected in the temporal component of the spatiotemporal matrix. The rank of the matrix will still be small and I don't see a direct benefit from incorporating the proposed multilevel hierarchical decomposition. The authors would probably need to expand their opinion on this issue. \n\nMoreover, the various regularizers \\Omega_U, \\Omega_A should be clarified. For example, what are the convex cell shape priors that are used? \n\nThe authors leave to the user the choice of regularization weights. This seems an important task to me since the number of regularizers is large and their use of critical importance. The authors claim that an accurate prior estimation of the rank is not of great importance since the algorithm will shrink non relevant components to zero. Although this sounds intuitive I can imagine that this behavior depends heavily on the right choice of the various sparsity penalties that will shrink the non relevant components. \n\n\nClarity \n\nI think the paper is not clearly written at all and right now is only accessible to an expert of calcium imaging data analysis. For example, the authors mention very briefly in the results section how the neural spikes are translated into calcium activity through a convolution operator and do not discuss this central aspect of spike deconvolution at all in their algorithm. In fact it seems to me that they sidestep this issue \"by eliminating all transients that overlap more than 20%\" (if I understand correctly). This is a topic of active research (e.g. Vogelstein et al. 2010, Grewe et. al. 2010) and I'd like to see how the authors relate to that literature. \n\nI found the inclusion of the main equations as a figure a bit unusual, although that is a question of style as well. \n\nFinally, I would prefer a more thorough presentation of the real data experiments. Although videos cannot be presented easily in paper, the results as shown in Fig 6 could be more convincing. In general, Fig 6 needs a better explanation. An interesting approach, but the paper needs significant clarification on a few important issues.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
