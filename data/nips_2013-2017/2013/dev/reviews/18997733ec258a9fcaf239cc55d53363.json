{"title": "A Gang of Bandits", "abstract": "Multi-armed bandit problems are receiving a great deal of attention because they adequately formalize the exploration-exploitation trade-offs arising in several industrially relevant applications, such as online advertisement and, more generally, recommendation systems.  In many cases, however, these applications have a strong social component, whose integration in the bandit algorithm could lead to a dramatic performance increase. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. In this paper, we introduce novel algorithmic approaches to the solution of such networked bandit problems. More specifically, we design and analyze a global strategy which allocates a bandit algorithm to each network node (user) and allows it to \u201cshare\u201d signals (contexts and payoffs) with the neghboring nodes. We then derive two more scalable variants of this strategy based on different ways of clustering the graph nodes. We experimentally compare the algorithm and its variants to state-of-the-art methods for contextual bandits that do not use the relational information. Our experiments, carried out on synthetic and real-world datasets, show a marked increase in prediction performance obtained by exploiting the network structure.", "id": "18997733ec258a9fcaf239cc55d53363", "authors": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Giovanni Zappella"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "Thanks to your rebuttal, I think I now understand your algorithm, and I think it is correct. But why did you present in Figure 2 algorithm 2 with CB and not TCB? The algorithm with CB does not work, and it is misleading to put CB in Figure 2. I would recommend changing this and putting TCB in the presentation of your algorithm. \n\nAlso, please comment on the necessity of knowing L(u_1,...,u_n) (or rather an upper bound on this, and rewrite the Thm with an upper bound since it is not realistic to have truly this quantity available). This assumption is not innocent since it implies that you assume some knowledge on the quality of the graph you have (how it relies to the u_1,..., u_n). \n\n\n*********************OLD REVIEW without comments on Thm 1 \n\n\nThis paper considers an extension of linear bandit algorithms, i.e. linear bandits with graph information. For instance a recommendation system can have access to a social network to which the users belong to, and it is reasonable to assume that users that are linked together might share interests. Each user can be modelled as a linear bandit problem, these bandits being linked together by a graph. The graph represents the level of similitude between the parameters of the bandits. \n\nThe authors assume that the graph is known to the algorithm, that at each time t the algorithm receives an user ID and a context, and that it has to recommend an arm according to that. They propose an algorithm, GOB.lin, that solves this problem based on LinUCB. The difference with respect to this last algorithm is that this algorithm exploits the graph information to make use of the similitudes between the users of the recommendation system. The authors provide a bound on the regret of this algorithm, and some numerical experiments that are promising and convincing. \n\n\nAlso, it would be nice if you could add some more intuition on your algorithm. Can it be seen as n bandits where, if some arm i_t is selected at time t, the estimate w_{j,t} of an arm j will be updated as w_{j,t,k} = (1-e(i_t,j)) w_{j,t-1,k} + e(i_t,j) \\tilde r_{t}, \nwhere \\tilde r_t = M_{t-1}^{-1} a_t \\tilde \\phi_{t,k} is the \"classic linear bandit update\" on w_{i_t,t}, and e(i_t,j) is between 0 and 1/t, and is a measure of the distance between i_t and j, that depends on i_t and j only and can be easily computed using the graph (e.g. if two nodes are linked e(i_t,j) = 1/t, if there is no path between them, e(i_t,j) = 0, and then anything in between depending on their proximity level?)? This would make your explanations clearer and your algorithm simpler. \n\nOther remarks: \n- p2 l89: \"....a new model distribuited...\"?? \n- p3 l129: \"We model the similarly among users....\"?? After reading the author rebuttal, I now think the proof is correct and the algorithm works - but please change its presentation, you propose an algorithm that does not work as such (Fig 2), and then analyse a modification of this algorithm, that works (see the TCB in Th 1).I changed completely my score and now recommend acceptance of this paper, that is interesting and on a hot topic. I would however recommend that the authors rewrite their paper in a much clearer way for the final version.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "-------Original Comments------------- \n\nThis paper provides a UCB-based contextual bandit algorithm that can exploit information given in the form of a graph structure. In particular the authors consider a contextual bandit model with linear payoffs and unknown coefficient vectors that depend on the user. A graph structure on the users is given, and an assumption on the relationship between the graph structure and the user coefficient vectors ensures that the graph provides meaningful information about the coefficient vectors. The algorithm then exploits this information, using the Laplacian matrix of the graph to propagate information gained about one user to other users with similar coefficient vectors. The authors provide both a regret bound and numerical experiments to support their proposed approach. \n\nThis is a natural extension of the techniques currently available, and part of an important general direction to integrate bandit ideas in more practical scenarios. Overall the paper is quite well written. However there are some sentences where the English is not quite correct (see below for some of these typos), and I did not find the intuition given in Section 4 to be easy to understand. \n\n--------After Rebuttal-------------- \n\nHaving discussed this submission in detail with the other reviewers, I have decided to lower my quality score. Initially I thought that my lack of familiarity with graph based methods hindered my understanding of the intuition and choices made in the paper. However, now I am convinced that the intuition is not well given. Also, it is strange to see the implementation of CB, but the analysis of TCB. There is no discussion of whether CB can be analysed or not. While it is true that there is a discrepency between LinUCB and SupLinUCB, the former was studied in detail empirically, and the latter in theory in a separate paper. There is nothing wrong (technically) with the elements of the contribution, I no longer find it to be a strong candidate for acceptance. \n\n-----------Some typos:-------------- \n\n129 We model the similary --> We model the similarity \n137 That is, despite (1) may contain ... --> That is, although (1) may contain ... \n207 but it rather contains graph ... --> but contains the graph ... \n371/372 lends itself to be --> turns out to be The paper provides an interesting and natural extension of contextual bandits to incorporate other information available in the form of a graph, a subject of interest to the machine learning community. However the work is not surprising, and there are several parts of the work, such as the intuition, which are not well executed.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers a setting where several contextual linear bandits are connected by the underlying graph. The assumptions is the weight vectors of the respective nodes are smooth over the graph (are close for neighbors). This appears to a be a good and rich enough model for recommendations in social networks. \n\nThe algorithm runs classical contextual linear bandits on each node (which node is being tried is determined by nature), but then shares this information with other nodes as well so they also update their model estimates and confidence widths appropriately via Laplacian. \n\nUnfortunately, the regret analysis provided is disappointing. The authors in fact provide the analysis for a different algorithm than they propose and evaluate in the experiments. This could be perhaps acceptable but I found no reason whatsoever over for this. There is no discussion, \nhow CB vs. TCB would change the algorithm, its evaluation and the deployment. At least the difference between TCB and CB should be discussed better than one could use the results from [1]. It is therefore possible, that the TCB-kind algorithm does not do well in the experiments or that for CB-kind it is not possible to provide learning guarantees. \nMaybe it would be better just to not include such analysis. \n\nRegarding the current analysis, it is not clear how big the terms ln|M_t| and 2L can be. How do they behave with T and n? This is needed in order to get some insight from the provided regret bound. And how does the graph noise influence these quantities? \n\nFurthermore, typically [4,9] the analyses for linear bandits give at least a guideline to set the exploration parameter \\alpha. Unfortunately, we do get this from the analysis provided. \nWhat is the dependence of the upper bound on alpha anyway? Is it hidden in some of the quantities mentioned above? \n\nThe experiment on the other hand are treated very well. The descriptions are very detailed and well explained. It is also great that the authors performed the experiments on real data and described the practical details to make the experiments reproducible. \n\nFinally, L186 - L190 The paper mentions that through graph, the reward influences other nodes in the \"lesser extent\", \"somehow informatively\" and \"similar updates\" are desired. However I do not see a parameter for \"somehow\" or \"similar\" in the algorithm. Could that be done by regularizing L by a multiple of I instead of I. In the experimental section, the authors mention that the clustering acts as a regularizer. Would not regularizing the Laplacian instead be \na more principled way to achieve this regularization? \n\nOther comments \n- Why it is L(u1,...un) in the regret bound and not the regularized version, \nsince the Laplacian is regularized in the algorithm? \n\nAfter the rebuttal: While this is a good work, I believe that including the theoretical analysis of a changed algorithm with not entirely clear relationship between the two, makes it confusing. I would increase my score if the theoretical analysis was taken away. \n This paper extends the contextual bandits to the setting where the different bandits share their weight-vector via provided graph (useful in the social network setting). Very good experiments, but the analysis provided is disappointing and does not appear to be useful.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
