{"title": "The Randomized Dependence Coefficient", "abstract": "We introduce the Randomized Dependence Coefficient (RDC), a measure of non-linear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-R\u00e9nyi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.", "id": "aab3238922bcc25a6f606eb525ffdc56", "authors": ["David Lopez-Paz", "Philipp Hennig", "Bernhard Sch\u00f6lkopf"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper introduces a new method called RDC to measure the statistical dependence between random variables. It combines a copula transform to a variant of kernel CCA using random projections, resulting in a O(n log n) complexity. Results on synthetic and real benchmark data show promising results for feature selection. \n\nThe paper is overall clear and pleasant to read. The good experimental results and simplicity of implementation suggest that the proposed method may be useful in complement to other existing methods. \n\nThe originality is limited, since it mostly combines several \"tricks\" that have been used in the past, namely the copula transformation to make the measure invariant to monotonic transformations (see eg Conover and Iman, The American Statistician, 35(3):124-129, 1981 or more recently the reference [15] cited by the authors), and the random projection trick to define a low-rank approximation of a kernel matrix. \n\nAlthough the work is technically correct, the following points would require clarification: \n\n- the authors insist that RDC is much faster to implement than kCCA, claimed to be in O(n^3) and taking 166s for 1000 points in the experiments. I am surprised by this, since in the original kCCA paper of Bach and Jordan an implementation in O(n) using incomplete Cholevski decomposition is proposed, and the authors claim there that it takes 0.5 seconds on 1000 points (more than 10 years ago). In fact, the incomplete Cholevski decomposition of Bach is a very popular approach to run kernel methods on large numbers of samples, similar to the random projection trick used by the authors. A natural question is then: to perform kCCA with large n, is it really better to use the random projection trick compared to the incomplete Cholevski decomposition? \n\n- as pointed out by the authors, when the dimension k of random projections gets large the method converges to unregularized kCCA, so it is important that k is not too large because \"some\" regularization of kCCA is needed. Hence k seems to play a crucial regularization role, akin to the regularization parameter in kCCA. Is there any theoretical or empirical argument in favor of a regularization by k, compared to the classical kCCA regularization? An argument against using k is that, when k is not too large, the random fluctuations may lead to significant fluctuations in the final score, which is not a good property. In fact, although RDC fulfills all conditions in Table 1, one that is not fulfilled is that it is a stochastic number (ie, compute it twice and you get different values). An interesting work combining several known ideas, but the comparison with the nearest cousin kCCA is not really fair and well-studied.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The RDC is a non-linear dependency estimator that satisfies Renyi's criteria and exploits the very recent FastFood speedup trick (ICML13). This is a straightforward recipe: 1) copularize the data, effectively preserving the dependency structure while ignoring the marginals, 2) sample k non-linear features of each datum (inspired from Bochner's theorem) and 3) solve the regular CCA eigenvalue problem on the resulting paired datasets. Ultimately, RDC feels like a copularised variation of kCCA (misleading as this may sound). Its efficiency is illustrated successfully on a set of classical non-linear bivariate dependency scenarios and 12 real datasets via a forward feature selection procedure. \n\nI found the paper very clear and easy to understand. Even though the idea simply puts together the right pieces, it remains a significant contribution to the literature. \n\nSome notes: \n- I do not like that k was simply 'set to 10'. It seems to play a more important role than the paper implies. \n- line 74: what do you mean by \"same structure as HGR\"? \n- eq.(5): m -> n \n- line 217: which is independent -> by independence RDC is a straightforward and computationally efficient estimator of the HGR coefficient but the choice of k deserves more discussion.I've read the author's rebuttal.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a non-linear measure of dependence between two random variables. This turns out to be the canonical correlation between random, nonlinear projections of the variables after a copula transformation which renders the marginals of the r.vs invariant to linear transformations. \n\nThis work details a simple but seemingly powerful procedure for quantifying non-linear dependancies between random variables. Part of the key idea of the work is the use of non-linear random projections via the \"random fourier features\" of Rahimi and Recht (2007). The authors compare their new randomised dependence coefficient (RDC) against a whole host of other coefficients for measuring both linear and non-linear dependence. They also consider the suitability of the RDC for screening for variable selection in linear prediction problems. I think these are useful comparisons and experiments and they allow the reader to get a decent feel for the behaviour of RDC. \n\nThe overall exposition of the paper is clear and each component part of the procedure is clearly described. In this respect the schematic diagram in figure 1 is particularly useful. Similarly, the comparison between other dependence coefficients combined with the empirical results is very illuminating and suggests that the performance of RDC is very promising. \n\nAlthough the authors choose to use RFFs, recently comparisons have been made with other schemes for generating random features have been made. Yang et al (2012) \"Nystrom Method vs Random Fourier Features: A Theoretical and Empirical Comparison\" flesh out some more of the theoretical properties of the difference between RFF and Nystrom features. One difference in particular is the difference in sampling scheme: Nystrom samples randomly in such a way that it takes the probability distribution of the data into account. In this sense it achieves a better estimate of the underlying kernel. If this is an important property of the proposed RDC, perhaps looking at Nystrom features would be interesting. It also would be interesting to see the effect of different types of non-linear projections where RFFs are limited to shift invariant kernels (with some other extensions), Nystrom features can be computed for any type of kernel function. \n\nAlso, Yang et al show empirically that there is a large improvement in predictive performance when more random features are used - it would be interesting to see what happens when k is increased. \n\nI have some concerns about the theoretical treatment presented. \nThe discussion of the bound in (10) seems to be skipping over the tradeoff involving k. It seems that in order to drive the LC/sqrt(k) term to be small (which could be quite slow?), ||m||_F could grow very large compared with n but this is hidden by referring to the dependancy as O(1/sqrt(n)). As it stands I'm not sure the analysis is sufficient to complement the good empirical performance. \n\nWith regards to the properties of the RFF approximation for linear prediction, one possibility is that the approximation acts as a regulariser. For this reason, I am not sure that the type of generalisation bound used for prediction algorithms are completely appropriate to quantify the performance and analyse the behaviour of a dependence measure. \n\nOne small point: the reference to Recht is incorrectly referred to as \"Brecht\" in the text. \n=============== \n\nI have read the author rebuttal and I am satisfied with the response. The authors should be sure to clarify the constraints in the CCA optimisation problem. I think overall the work is extremely interesting and appears to work well empirically. As it stands I think the theoretical analysis is incomplete although that does detract too much from the impact and importance of the work.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper gives a new approach to nonlinear correlation. The approach consists of three steps: (1) copula transform, (2) random nonlinear projection, and (3) CCA. In each step, the authors give a theoretical guarantee to make the approach convincing. The authors demonstrate the usefulness of the approach using synthetic and real data set. \n\nThis is a well-written paper having high novelty. As the authors describe, the RDC score is easy to compute. Their experiments, Fig.4 in particular, clearly demonstrate the utility of the approach. I enjoyed reading the paper very much. Although calculating nonlinear correlation itself has not been the central topic in the machine learning community, I found it an interesting building block for more realistic tasks such as regression as one of the experiments already suggests. I recommend accepting the paper. \n New and interesting approach to the metric of nonlinear correlation. The novelty is high.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
