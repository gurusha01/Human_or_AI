{"title": "Reward Mapping for Transfer in Long-Lived Agents", "abstract": "We consider how to transfer knowledge from previous tasks to a current task in long-lived and bounded agents that must solve a sequence of MDPs over a finite lifetime.  A novel aspect of our transfer approach is that we reuse reward functions.   While this may seem counterintuitive, we build on the insight of recent work on the optimal rewards problem that guiding an agent's behavior with reward functions other than the task-specifying reward function can help overcome computational  bounds of  the agent.    Specifically, we use good guidance reward functions learned on previous tasks in the sequence to incrementally train a reward mapping function that maps task-specifying reward functions into good initial guidance reward functions for subsequent tasks. We demonstrate that our approach can substantially improve the agent's performance relative to other approaches, including an approach that transfers policies.", "id": "58c54802a9fb9526cd0923353a34a7ae", "authors": ["Xiaoxiao Guo", "Satinder Singh", "Richard L. Lewis"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper presents a model for transfer where synthetic reward functions are learned from previous tasks, and are mapped to new tasks to produce synthetic reward functions which speed up learning. This application of the relatively recent idea of a synthetic reward function that improves the performance of an underlying \"utility\" reward function to the transfer scenario is new and interesting, and the experiments are well thought out, thorough and reasonably convincing. \n\nThe major criticism I had of the paper is that it does not compare to other methods of transfer. The authors are incorrect that policy transfer is not possible in their setting; that is why sub-policies, in the form of options, are often transferred instead of entire policies. (Speaking of which, the authors should probably cite Lisa Torrey's work on policy transfer.) In this case though it's OK to skip that comparison because you could use either, both, or neither types of transfer in combination, so the comparison adds information but is not critical. \n\nHowever, the failure to compare against a similar reward shaping scheme is more problematic. These two methods are effectively solving the same problem, except that reward shaping does not change the ultimate solution and a synthetic reward function does. In my opinion, these two methods (even outside of the transfer scenario) have not been adequately compared, which is odd because its trivial to learn a shaping function (it's just a value function initialization, so you're just learning a value function). This lack of comparison leaves me with significant doubts about the whole synthetic reward function enterprise generally. So I think a comparison here - where the shaping function is mapped in the same way that the reward function is - would significantly improve the paper. But perhaps that is too much to ask for in a single paper, especially since the mapped shaping function could be considered a new (though somewhat obvious) method. \n\nThe paper is very well written and was generally a pleasure to read, though this was spoiled somewhat by the repeated use of parenthetical citations as nouns. The references were poorly formatted in some cases (capitalization on \"mdps\", etc.) This paper describes a novel method for transfer in reinforcement learning domains, and is well written and well executed. A better experimental comparison to reward shaping methods would have been good, but isn't totally necessary.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors consider an agent that will experience a series of sequential decision-making tasks over its lifetime in the same environment (or a similar one). They propose a method for transfering knowledge acquired in previous problems to the current problem. Specifically, they consider transfering knowledge acquired in learning optimal reward functions. The authors demonstrate the utility of their approach in two examples. \n\nThe paper builds primarily on two pieces of earlier work. The first is the optimal rewards formulation in Singh et al. (2010), where the authors define an internal reward function as one that maximizes external reward (this internal reward function may differ from the external reward function if the agent is bounded in its capabilities, for instance if it has a small planning horizon). The second is the algorithm by Sorg et al. (2010) for incremental learning of such a reward function in a single task setting. \n\nThe contribution of the current paper is to place this earlier work in a multi-task setting. As in Sorg et al (2010), the agent learns an internal reward function in each task. In addition, the agent learns a mapping from the external reward functions of the tasks it has experienced in the past to the internal reward functions it has learned by the end of those tasks. The agent uses this mapping to initialize its internal reward function at the beginning of each task. \n\nThe examples in the paper are small but sufficient to demonstrate the potential utility of the approach. In practice, the success of the algorithm will depend on the availability of good features for the mapping from external rewards to internal rewards. \n\nThe paper is well written and easy to follow. The empirical evaluation is well done, informative and useful. Figure 4, in particular, is helpful in concretely showing what the algorithm has done. Including a similar figure (or description) for the network example would be a useful addition to the paper. \n The paper is not particularly innovative but it is a well-executed, useful addition to the literature on transfer learning.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper describes a method to \"conserve\" some earlier experiences \nin solving a RL problem to later runs using different reward \nfunctions. Importantly, the system assumes a limited power for \nlearning, which makes it advantageous to use a surrogate reward \nfunction that allows the agent to learn the concrete reward. The \nidea is to provide the opportunity to learn an inner reward function \nas a function of the external reward function. \n\nThe idea is interesting and attractive, and the prospect of a \n\"bounded rationality\"-type assumption behind the algorithm (although \nthe authors studiously - and wisely - avoid using it) renders the \nmethod a welcome approach to a more practical (and plausible) \nperspective on reinforcement learning in general scenarios. \n\nGenerally well readable, the reviewer found that the paper lost \nclarity in the network routing domain. I'll mention some of the \nissues in the details below. \n\nIn terms of methodology, the paper falls into the general category \nof the \"reward-shaping\" methodology, the success of the methodology \nin the examples is convincing, the general method class is, of \ncourse, already, if not maturing, but consolidating. \n\n- line 323: what are \"trajectory-count\" parameters for UCT? Number \nof sample runs? \n\n- line 332: it seems that either colours, coefficient signs or \nsemantics of the coefficent are inconsistent here. The text says: \n\"negative/dark/discouraging exploration\", but that does not fit \nwith figure 4. \n\n- line 370: I do not understand the point of the decomposition into \nG_1, G_2, G_3? What's the purpose of it? \n\n- line 403: I do not understand how the transition function is \nmodeled. Don't you use reward mapping anymore here? If you use it, \n*and* you modify the transition function, how does that happen? \nPlease reformulate this section, it is completely unclear to me. \n\n- line 418: What is the \"competing policy transfer agent\"? What \nmodel does it use? \n An interesting method for transfer learning under limitedresources. Settled in existing \"reward shaping\" methodologyterritory, the method itself looks sufficiently original andeffective to warrant publication. Some (minor) weaknesses in thedescription of the second example.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
