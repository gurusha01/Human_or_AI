{"title": "Bayesian entropy estimation for binary spike train data using parametric prior knowledge", "abstract": "Shannon's entropy is a basic quantity in information theory, and a  fundamental building block for the analysis of neural codes.   Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable   attention in statistics and theoretical neuroscience.  However,  neural responses have characteristic statistical structure that   generic entropy estimators fail to exploit.  For example, existing  Bayesian entropy estimators make the naive assumption that all spike   words are equally likely a priori, which makes for an  inefficient allocation of prior probability mass in cases where   spikes are sparse.  Here we develop Bayesian estimators for the  entropy of binary spike trains using priors designed to flexibly   exploit the statistical structure of simultaneously-recorded spike  responses.  We define two prior distributions over spike words using   mixtures of Dirichlet distributions centered on simple parametric  models.  The parametric model captures high-level statistical   features of the data, such as the average spike count in a spike  word, which allows the posterior over entropy to concentrate more   rapidly than with standard estimators (e.g., in cases where the  probability of spiking differs strongly from 0.5). Conversely, the   Dirichlet distributions assign prior mass to distributions far from  the parametric model, ensuring consistent estimates for arbitrary   distributions.  We devise a compact representation of the data and  prior that allow for computationally efficient implementations of   Bayesian least squares and empirical Bayes entropy estimators with  large numbers of neurons.  We apply these estimators to simulated   and real neural data and show that they substantially outperform  traditional methods.", "id": "f9a40a4780f5e1306c46f1c8daecee3b", "authors": ["Evan W. Archer", "Il Memming Park", "Jonathan W. Pillow"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "As far as I can see, the method favors descriptions which are mainly based on the spike count. I think, it would be interesting to see, how the method performs in situations, in which there are other higher-order dependencies apart from spike count. Specifically, including a comparison between other methods such as BUB in situations, in which spike-trains are, for example, generated according to a GLM, would be a more convincing argument of the accuracy. \n\nminor points: \nline 163: reference missing \nline 221: \\alpha=0 implies uniform distribution? \nline 296: for -->form The manuscript is well written and the feasibility of the approach is sufficiently demonstrated by the presented experiments.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Paper summary: \n\nThe paper describes two novel entropy estimators for binary neural spike words. The estimators are Bayesian and make use of a mixture-of-Dirichlet prior. The distribution is hierarchical with a count distribution as the base measure of the Dirichlet distribution. The authors evaluate their methods on artificial data and on data recorded simultaneously from retinal ganglion cells and compare them to established entropy estimators. They show that their estimators need less samples for accurate estimations. Finally, they apply their method to quantify temporal synergy in retinal ganglion cell data. \n\n\nQuality: \n\nThe Bayesian entropy estimators are powerful, elegantly evading the curse of dimensionality. By including prior information about the structure of the problem, the method reduces the number of required samples. \n\nOn the synthetic data, it is not surprising that the proposed estimators outperform the alternative estimators, since the distributions of the word data follow the model structure of the estimators. The performance on the data recorded simultaneously from retinal ganglion cells is impressive. \n\nIt is not clear, though, how well the methods would do on other neural data. The method contains the critical underlying assumption that the word distribution is well characterized by the overall count distribution. For the retinal ganglion cell data this is apparently the case, but further evaluations will have to show whether or not this will also hold in general. It might be worth to mention this problem with a sentence in the discussion. In any case, the new estimators are certainly very useful. \n\n\nClarity: \n\nThe paper is nicely written. \n\n\nOriginality: \n\nThe proposed entropy estimators extend the work by Nemenman et al., NIPS 2002 by including prior knowledge about the structure of the spike trains. The general idea is similar to that of the raster marginals model (Okun et al., J Neurosci 2012) in that the total spike count distribution is used as a simplification to evade the curse of dimensionality. \n\n\nSignificance: \n\nEntropy estimation is a very important problem, because information quantification is a central problem of neural coding analyses. The demonstrated performance gain compared to alternative methods is impressive. \n\n\nMinor points: \n080: In Section 3 introduce -> In Section 3, we introduce \n163: Citation missing \n232: Incomplete sentence \n234: the estimator fast-to-compute -> the estimator is fast-to-compute \n290: We only then need only -> We then need only \n295: for -> form \nFigures 4 and 5: DCnt -> DCt \n The paper introduces entropy estimators for neural spike trains that require less samples for accurate estimations. The contribution is important and well implemented.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This submission presents a straightforward improvement on current estimators of entropy in multidimensional binary distributions. Estimating entropy (and related measures like mutual information) is considered important in advancing our understanding of the neural code. This work is especially timely in that an increasing number of multi-electrode recordings are currently taking place whose interpretability depends on better analysis methods like the one presented here. \n\nThe work presented here relies on the fact that population responses, esp. when binned in short time intervals, are very sparse: the most frequent word is one in which no neuron spikes, with a word frequency rapidly decreasing with the number of spikes, or ones, in a word. By incorporating this prior knowledge into their Bayesian estimator, the authors derive an estimator that achieve a bias comparable to that of existing estimators using several orders of magnitude less data - a critical constraint in empirical studies. \n\nIn an example application of their method to real data, the authors find that retinal ganglion cells \"code synergistically\" in time. While interesting, this section would need to be expanded to yield robust and convincing insights. \n\nThe presentation of the derivation and the results is quite clear and I only have minor comments. \n[deleted] \n Important improvement on entropy estimation methods based on a more sophisticated prior than used previously.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
