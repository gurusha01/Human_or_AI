{"title": "Direct 0-1 Loss Minimization and Margin Maximization with Boosting", "abstract": "We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing  empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives consistently better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n'th order bottom sample margin.", "id": "9461cce28ebe3e76fb4b931c35a169b0", "authors": ["Shaodan Zhai", "Tian Xia", "Ming Tan", "Shaojun Wang"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper studies the idea of using a boosting-like approach to directly minimize the training error and various functions of the training margins. The algorithms are explained in detail, and decent experimental results are presented. Theory is fairly minimal. \n\nIn a way, this is a fairly obvious idea, but conventional wisdom says that the idea should not work since classification error is not convex or even continuous. It is great to see someone try it, and to spell out all the issues involved, and the details of how to implement it. I thought the experimental results were especially strong, a nice comparison on 10 datasets against several boosting algorithms. The new method works surprisingly well, especially in the presence of noise. \n\nThe presentation is generally good. I thought the descriptions of algorithms could have been a bit more precise, but the paper gives examples that really help to illustrate what is going on. A good idea presented with algorithmic details, and strong experimental results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this paper the authors provide an algorithm for directly minimzing 0-1 loss and margin maximization. Most existing machine learning techniques have relied on minimizing a convex upper bound on the 0-1 loss in classification problems. In contrast, in this paper the authors propose a simple greedy algorithm for directly minimizing the 0-1 loss via a combination of weak learners. This is followed by a few steps of direct maximization of margin. The proposed algorithm is then evaluated on a few small low dimensional datasets. \n\nI have the following major concerns about this paper: \n\nThe authors claim that their algorithm has a much favorable run-time compared to AdaBoost (Table 2). I do not understand how this is possible. In AdaBoost, in each round, a weight is given to a training example and a weak learner such as a decision tree can be found quite efficiently to minimize a weighted loss (for example using CART as the weak learner). Thus, in each step of boosting one needs to find only one decision tree. However, for the proposed algorithm, one has to iterate over all possible weak learners. I just don't see how the proposed algorithm can be computationally more efficient unless the number of weak learners is really small. In many applications with large datasets it is typical to consider decision trees of depth 5 or 10 and I do not see how the proposed method can be efficient in that case if one has to enumerate all weak learners. I am concerned that the proposed algorithm is limited to small datasets with low dimensions and weak learners with very few instances. \n\nFrom the standard deviations in Table 1, I am not sure that the proposed method results in statistically significant results in Table 1 compared to AdaBoost. If we take the standard error most of the results seem to overlap. \n\nThe proposed method consists of two steps: first, 0-1 loss is minimized using greedy co-ordinate descent. Once a few weak learners are selected, a few more weak learners are added to maximize the average margin of bottom n' examples greedily. How much of the claimed benefit is due to each step? For example, what happens if you run your algorithm 2 after a few iterations of AdaBoost? \"Direct 0-1 loss minimization\" is a bit of a misnomer since it is followed up with a few steps of margin maximization. I am not sure such natural questions are adequately answered in this paper. \n\n While this work could be potentially interesting, I am not completely convinced about this paper at this point.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a boosting method that directly minimizes the empirical classification error that is defined based on the indicator whether predicted y = observed y, the so-called 0-1 loss. The proposed method first runs greedy coordinate descent to search the coordinatewise minimum loss, and then runs coordinate ascent to expand the margin. The method is interesting and novel, and offers some advantages. The following concerns are raised. \n\n1. It seems the authors assume a hypothesis space which has a finite number of hypotheses because in the algorithms 1 and 2, it loops on all hypotheses h at each iteration. For instance, algorithm 1 finds all weak learners that lead to largest classification error reduction at each iteration. What if users choose a hypothesis space, such as linear functions? \n\n2. At the end of Algorithm 1, what is the rationale to update the weight of one weak learner that gives the smallest exponential loss? Isn\u2019t it still using a convex loss function although in the early part of the algorithm, it uses 0-1 loss. \n\n3. It also assumes that data is always separable as long as the combined strong classifier is strong. What if it is not the case? What if the chosen hypothesis space is not complex enough to separate a given dataset. The paper states that algorithm 1 reaches a solution of coordinatewise minimum. It also says the 0-1 loss reaches 0. What is the definition of coordinatewise minimum? Is it actually a global minimum because the lowest error would be 0. \n\n4. When the second step (margin maximization) starts, it starts from a region that would not get the solution out of the 0 loss region, characterized by the value of d. It is not clear how exactly this value of d is calculated instead of simply saying \u201cdetermine the lowest sample whose margin is decreasing\u201d. \n\n5. Given separable cases are assumed, the margin is always positive in their formulation. What happens if there are negative margins for inseparable cases? The second part of the algorithm would not run. \n\n6. In the early paragraph of page 2, it says \u201cit might escape the region of minimum training error in order to achieve a larger margin\u201d. In the later section, the design of algorithm 2 will not allow the margin maximization step go beyond zero 0-1 loss region. \n\n7. Assuming a weak learner can be obtained in a similar computational cost, DirectBoost is certainly more computationally heavy for each iteration than AdaBoost. In their experiments, what is the stopping criterion that they used for AdaBoost? Why does AdaBoost need so many iterations? \n\n8. It is not that clear about the truly convincing advantage of the proposed method over regular boosting methods. \n This paper presents a boosting method that directly minimizes the empirical classification error that is defined based on the indicator whether predicted y = observed y, the so-called 0-1 loss. The method is interesting and novel, and offers some advantages although there are some concerns.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Simple idea with nice results. \nIf this is effectively new, then the paper should be accepted. \nTherefore the novelty aspect should be carefully checked. \n\nIs it really a boosting algorithm? To me it should be called a pursuit algorithm because it does not apparently rely on manipulating the distribution of examples. This is important because there are closely related works (see kernel matching pursuit, http://www.iro.umontreal.ca/~vincentp/Publications/kmp_techreport2000.pdf, section 3) found in the pursuit literature instead of the boosting literature. This is close, but not exactly the same, though... \n\nAbout the overlap with papers 539 and 956. I was initially a reviewer of paper 956 which describes a multiclass variant of the same idea. Paper 956 is harder to understand and could have been a paragraph in paper 481. Paper 539 is a semi-supervised variant od the same idea and could have been a paragraph in paper 481. This splitting of a work into minimal publishable units is unwise. I think that the community will be better served by accepting 481, rejecting 539 and 956, and letting the author know that he can refer to these extensions in the final version of the Nips paper and defer the full discussion to the future journal paper. \n\n\nDetailed comments: \n\n[page 1, line 44] -- Even with surrogate losses, the problem is not always convex. This depends on the family of functions. Recommend accepting only 481 which contains the important idea.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
