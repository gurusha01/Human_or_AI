{"title": "Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition", "abstract": "", "id": "a223c6b3710f85df22e9377d6c4f7553", "authors": ["Adel Javanmard", "Andrea Montanari"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper deals with model selection properties of Gauss-Lasso procedure. This is a two step procedure. In the first step a lasso estimator is fitted. In the second step, the OLS estimator is fitted on the subset of selected variables. The estimated support is given by choosing the largest s components (in absolute value) of the OLS estimator. \n\nThe main contribution of the paper is providing the generalized irrepresentability condition and showing that the Gauss-Lasso procedure correctly recovers the support of the parameter vector under this condition. \n\nFirst, I believe that exposition of the material could be dramatically improved. Section 3.1 does not seem to be needed. One could formulate an optimization procedure that generalizes both (5) and (16). Then state results for this optimization procedure. The way material is currently presented, you are repeating the same things twice. More importantly, you should try to explain how do you improve the results of [1]. In particular, without going through the details of the proof, a reader should get a sense, at a higher level, of what novel tools does one need to use to improve existing results. Maybe provide an outline of the proof and point out where does your work differ from [1]. \n\nThere are other two step procedures that are able to select variables consistently. See for example [2] and [3]. Both papers discuss variable selection under weaker conditions than irrepresentable condition. How does generalized irrepresentable condition compare to conditions imposed in that work. \n\nI believe that there is another question worth answering. How does Gauss-Lasso procedure perform when the unknown parameter vector is approximately sparse? Would the procedure still select the s largest in absolute value components? \n\n[1] M.J. Wainwright, Sharp thresholds for high-dimensional and noisy sparsity recovery using l1-constrained quadratic programming, IEEE Trans. on Inform. Theory 55 (2009) \n\n[2] Fei Ye, Cun-Hui Zhang. Rate Minimaxity of the Lasso and Dantzig Selector for the lq Loss in lr Balls. 11(Dec):3519\u22123540, 2010. \n\n[3] Sara van de Geer, Peter B\u00fchlmann, and Shuheng Zhou. The adaptive and the thresholded Lasso for potentially misspecified models (and a lower bound for the Lasso). Electron. J. Statist. Volume 5 (2011), 688-749. \n The paper studies an important problem. Exposition of the material could be dramatically improved. The authors should also compare their results to other work on two step procedures.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This is a theory paper on model selection in the large p small n scenario. The authors developed a generalized irrepresentability condition and applied it for studying the so-called GAUSS-LASSO selector. This paper is on the theoretical studies of the LASSO type selector, and some experimental results were provided in the appendix. \n\nThe main contribution of this paper is the proposed generalized irrepresentability condition which provides a weaker condition than the widely used irrepresentability condition. It is motivated by studying the KKT conditions of LASSO in the noiseless case. The authors applied this proposed generalized irrepresentability condition to study the GAUSS-LASSO selector. The GAUSS-LASSO selector performs a LASSO selection, then a least squares estimation on the LASSO selected variables, and sets the selected variables as the leading ones of the least squares fit. More specifically, the authors showed that there exists a threshold for the regularization parameter below which the support of the Lasso estimator remains the same and contains the support for the ground truth, and the authors established the theoretical results for LASSO in both deterministic and random designs. \n\nTheorem 2 showed that the support of the signed support of the Lasso estimator is the same as that in the zero-noise problem with high probability. However, it is worthwhile to note that the conditions in Eq. (13-14) might not always hold. For example, when the noisy level is high, one may not find a suitable lambda or c_1 > 1 that satisfies Eq. (13). \n\nMinor comments: \n1. Lemma 2.1, Eq. (16) => Eq. (5) \n2. v_0, T_0 is not easy to understand from the discussion in Section 1.3, although it should denote the restriction of v_0 to the indices in T_0. \n\nAfter reading other reviewers' comments and the author response, the reviewer would like to keep the original recommendation. This an interesting theory paper on model selection in the large p small n scenario. The authors developed a generalized irrepresentability condition and applied it for studying the so-called GAUSS-LASSO selector.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper provides a thorough and comprehensive study of \nthe post-Lasso (the estimator obtained by fitting a \nleast square model on the variables selected by the Lasso) \nin the context of high dimensional sparse regression. \n\nA new theoretical guarantee is provided for the post- \nLasso, as well as simulated experiments on a toy example \nwhere the benefit of the procedure is clear. \n\nThough, there are still a few points that could be improved. \n\nFirst, some important references are missing about other works \nconsidering the post-Lasso on a theoretical level \n(or a variation of it): \n\n\"Pivotal Estimation of Nonparametric Functions via Square-root Lasso\" \nAlexandre Belloni, Victor Chernozhukov, Lie Wang \n(cf. Theorem 7 for instance) \n\n\n\"L1-Penalized Quantile Regression in High-Dimensional Sparse Models\", \nAlexandre Belloni and Victor Chernozhukov, 2011 \n(cf. post-l1-QR Theorem 5) \n\n\n\nMoreover, a recent work focusing on the practical properties of the \npost-Lasso for a particular set of simulated data, challenges, in certain \nsituations, the theoretical benefit illustrated by the authors. \nCan they comment on that? \n\n\"Trust, but verify: benefits and pitfalls of least-squares refitting in high dimensions\",2013 \nJohannes Lederer \n\n\n\n\nPoints to correct: \n\n-l134: without further assumptions the minimizers of G might not be unique. \nthe results is true under some more assumptions on the Gramm matrix, \nas is well known (and in a way proved later by the authors) since \n\n\"On Sparse Representations in Arbitrary Redundant Bases\", J-J. Fuchs,2004 \nand more recently \n\"The Lasso Problem and Uniqueness\", Ryan J. Tibshirani, 2013 \n\nThe problem occurs many time in the proof: the unicity is sometimes used before it \nis proved. Adapting the results from the aforementioned papers, I encourage \nthe authors to show that (under suitable assumptions) unicity holds \n(cf. for instance l1034, where the strict inequality \nis given without any justification, see also l1071 and l1113) and therefore that \nthere proof is right. \n\nI encourage the authors to fix this for the sake of clarity: it could also be better to add \nan assumption mentioning when one needs the (correctly) extracted Gramm matrix to be invertible. \n\n\n-l303: t_0 is defined but nowhere used in this section, and then re-used in the next one... please remove. \n\n-l307: A comment could be added on the fact that the lambda parameter depends on an unknown quantity, eta. \n\n-l317: What is the benefit of the assumptions w.r.t [23]? It does not seem straightforward which one is weaker: \non the one hand we need a matrix invertible of larger size (T_0 contains S) but on the other hand \nonly the sup-norm of a vector should be control is the proposed work. \n\n-Section 3.1 is for me useless, the results are exactly the same as in Section 2.1. \nPlease remove and spare some room for the comments, and the missing references. \nFor instance, further comparisons on the differences between the deterministic case \nand the random case could be investigated. \n\n-l408: Did the authors try to improve the term depending on exp(-t_0/2)? \nIt seems to be the weak part of the probability control provided in Theorem 3.4 \n\n-l441/443: Candes and Cand\\'es are mispelled. It should be Cand\\`es \n\n-l553: it seems there is a sign issue. \n\n\nGeneral questions: \n\n- can the authors comment on the fact that the sparsity level must be known a prior (no adaptivity) in their procedure? \n-When is T_0=S (l267)? That would be interesting to understand when the two are identical. \n\n\n Overall, the paper is clear, sharp and is of high interestfor statisticians and practitioners.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
