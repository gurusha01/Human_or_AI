{"title": "Annealing between distributions by averaging moments", "abstract": "Many powerful Monte Carlo techniques for estimating partition functions, such as annealed importance sampling (AIS), are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and an intractable target distribution. The near-universal practice is to use geometric averages of the initial and target distributions, but alternative paths can perform substantially better. We present a novel sequence of intermediate distributions for exponential families: averaging the moments of the initial and target distributions. We derive an asymptotically optimal piecewise linear schedule for the moments path and show that it performs at least as well as geometric averages with a linear schedule.  Moment averaging performs well empirically at estimating partition functions of restricted Boltzmann machines (RBMs), which form the building blocks of many deep learning models, including Deep Belief Networks and Deep Boltzmann Machines.", "id": "fb60d411a5c5b72b2e7d3527cfc84fd0", "authors": ["Roger B. Grosse", "Chris J. Maddison", "Russ R. Salakhutdinov"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "AIS has become an important tool to evaluate the performance of a learning algorithm for MRFs. In common practice, people use geometric averaged models as intermediate models. This paper raises an interesting question of finding a better alternative to the regular annealing path. It focuses on the bias of the expected log-weight, on which the variance of the AIS output is monotonically related, and compares the asymptotical behavior of different annealing paths. \n\nOverall, I think this paper is well written. The explanation is clear, the example of the two paths between Gaussian distributions in Figure 1 and the visualization of intermediate RBMs in Figure 4 are very helpful. Also the analysis is sound (I didn\u2019t check the proof in the supplementary). \n\nMy only concern is about how to obtain the moment averaged models. The authors use PCD with a relatively short training time to do the moment matching. How sensitive is the outcome of AIS wrt the accuracy of the moments of the target distribution as well as the parameters of those intermediate models? How is the total computational cost of the new method compared to method based on geometric average? \n\nIn the geometric averaging path, it is observed for RBMs that assigning more models at the low temperature end usually obtains much smaller variance than the linear schedule in practice. Is this phenomenon considered or compared in the experiments? \n This paper proposes an alternative to the regular geometric average annealing schedule for AIS, and shows a smaller variance and higher effective sample size on the output weights. The paper is well written and the result is convincing.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary \n======= \nAnnealed importance sampling (AIS) depends on a sequence of distributions between an initial and a target distribution. While most previous applications of AIS have used distributions along paths in parameter space based on geometric averages of the two distributions, the present article proposes using alternative paths based on averaging moments. The authors provide theoretical and empirical evidence for the advantages of their approach. \n\n\nQuality \n======= \nThe paper gives a rigorous analysis of the proposed moment averaging (MA) path. The theoretical claims appear to be sound. Except for typographical errors, I did not find any mistakes in the proofs provided. \n\nThe empirical results on intractable models used a linear annealing schedule for the geometric averaging (GA) path. Since previous work seems to have found other schedules to be more effective (Neal, 1998; Salakhutdinov & Murray, 2008), it would be interesting to also see how well GA does with a different schedule when applied to the larger RBMs \u2013\u00a0despite or especially because of figures 2 and 3 suggesting that it will not make a difference. With the schedules used in the mentioned work, the sampler will spend more time close to the target distribution, so that I suspect that using them will at least lead to more reasonable samples in Figure 4. \n\n\nClarity \n======= \nThe paper is well written and well structured. I had no problems following the main text. \n\nThe supplementary material might benefit from making a few more calculations explicit (for example, the Fisher information matrix used in the integral at the top of page 4). \n\n\nOriginality \n=========== \nThis paper represents a highly original contribution. As pointed out by the authors, virtually all previous applications of AIS used geometric paths. This shows that the proposed alternative is not at all obvious and technically challenging. \n\n\nSignificance \n============ \nAIS is a widely used technique in statistical machine learning. While the MA approach seems to work reasonably well for estimating partition functions of some models (such as RBMs), getting it to produce useful results with more complex models can be difficult. Better annealing strategies have the potential to improve this situation and to allow for better model comparisons. In addition, as also mentioned by the authors, many sampling strategies important for inference and learning can benefit from better annealing strategies. \n\nAlthough I am not convinced that the proposed annealing strategy will find widespread adoption (mainly because of the added complexity of having to estimate intermediate distributions and because the results on intractable RBMs suggest that GA can still compete with or even outperform MA, as measured by the ESS), I think it has great potential to inspire further work in this direction. \n\n\nMinor comments \n============== \n\u2013\u00a0Supp. 1: better use $\\theta_0 = \\theta$ instead of $\\theta = \\theta_0$ \n\u2013\u00a0Supp. 2: better use $p_a$ and $p_b$ for $p_0$ and $p_1$ throughout \n\u2013 Supp. 2.1: the first term of the Lagrangian should be $\\lambda ( 1 - \\sum_x q(x) )$ or $\\lambda ( \\sum_x q(x) - 1 )$ \n\u2013 Supp. 3.1: $\\dot s(\\beta) = t$ should be $\\dot s(\\beta) = 2\\beta$ \n\u2013 Supp. 3.2: a factor 2 is missing in the derivative of $\\lambda(\\beta)$ This is a highly original contribution with potential future impact on learning, inference, and model comparison. While I am not convinced that it will find immediate adoption for the purpose of partition function estimation, I think it will inspire more theoretical work in the direction taken by the authors.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents an alternative method for choosing intermediate distributions when performing AIS on distributions in the exponential family. Experimental results are promising. These results may be of significant practical benefit, as they could improve researchers' ability to make objective comparisons between complex probabilistic models. Overall I liked it. I did think the theoretical motivation for the proposed intermediate distributions was somewhat lacking. \n\nEquation 3 -- cool! \n\n\"high curvature\" --> not high curvature. Rather, large information distance per theta distance. \n\nSection 5.1 -- why is the larger variance of 1437.89 *better*? I would expect larger variance to be worse. \n\nThe experimental results were entirely for second order models, while the technique is proposed for arbitrary exponential family models. It would be nice to see experiments for other choices of sufficient statistic. \n\npage 7 - \"substantially\", say something quantitative. \n\nSome speculation follows: \n\nIt's never really explained why interpolating between the sufficient statistics should be better than interpolating between the parameter values. \n\nEqs. 4 and 5 strongly suggest that the optimal path in parameter space is the geodesic (minimal distance) path between the two parameterizations, with the Fisher information as a metric. \n\nIf I've done my algebra correctly, then $ds/d\\theta = G$. (write down s = \\int dx p(x) dE/dtheta, and take the gradient) If the path is linear in $s$, as you propose, then for small steps $\\Delta s$, $\\Delta \\theta = d\\theta / ds \\Delta s = G^{-1} \\Delta s$. An infinitesimal step along your proposed trajectory in terms of \\theta looks like the step in s times the inverse Fisher information matrix. This is the same as moving towards the final \\theta taking into account the information geometry (with metric = Fisher information matrix). That is, these updates have a functional form which is nearly identical to the natural gradient update. \n\nInterpreting your algorithm in terms of a natural gradient trajectory also opens up some possible approximations/extensions. eg, roughly in order of decreasing promise: \n- Apply the same technique to non-exponential family models, where the sufficient statistics are not defined, but the Fisher information still is. \n- Use well motivated metrics other than the Fisher information -- the matrix natural gradient is likely to best bet here (Amari, S.-I. (1998). Natural Gradient Works Efficiently in Learning. Neural Computation) \n- Use an approximation to G^-1, which can be computed much more cheaply (Sohl-Dickstein, J. (2012). The Natural Gradient by Analogy to Signal Whitening, and Recipes and Tricks for its Use. arXiv:1205.1828v1.) \n- Iterate over choices for the entire trajectory, in order to find the global shortest trajectory (with metric G), rather than just taking the local shortest steps \n\n-- \n\nFinally, here's a plug for reproducible science. You can become more awesome by putting source code for experiments in the supplemental material! When source code is shared for computational experiments, it has a major positive impact on the usefulness of the authors' hard work for future researchers, on the credibility of the work, and on the number of citations earned. And as a reviewer it makes me more positively disposed towards a paper. \n These results may be of significant practical benefit, as they could improve researchers' ability to make objective comparisons between complex probabilistic models. Experimental results are promising, theoretical motivation is not fully satisfying.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
