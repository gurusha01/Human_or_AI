{"title": "Reflection methods for user-friendly submodular optimization", "abstract": "Recently, it has become evident that submodularity naturally captures widely occurring concepts in machine learning, signal processing and computer vision. In consequence, there is need for efficient optimization procedures for submodular functions, in particular for minimization problems. While general submodular minimization is challenging, we propose a new approach that exploits existing decomposability of submodular functions. In contrast to previous approaches, our method is neither approximate, nor impractical, nor does it need any cumbersome parameter tuning. Moreover, it is easy to implement and parallelize. A key component of our approach is a formulation of the discrete submodular minimization problem as a continuous best approximation problem. It is solved through a sequence of reflections and its solution can be  automatically thresholded to obtain an optimal discrete solution. Our method solves both the continuous and discrete formulations of the problem, and therefore has applications in learning, inference, and reconstruction. In our experiments, we show the benefits of our new algorithms for two image segmentation tasks.", "id": "5dd9db5e033da9c6fb5ba83c7a7ebea9", "authors": ["Stefanie Jegelka", "Francis Bach", "Suvrit Sra"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper casts the problem of minimizing decomposable submodular functions as an orthogonal projection problem to obtain easily parallelizable algorithms. Furthermore, by focusing on a special subclass of decomposable functions, they can prove the proximal problem can be solved in fewer iterations than currently possible. \n\nThe authors consider several existing methods for minimizing the Lovasz extension over [0,1]^n, experimentally verify their claims. \n\nUnfortunately there is no empirical verification of the speedups in the case of implementation as a parallel algorithm, which is advertised as a selling point of the paper. \n\nThe paper is well-written and clear. This paper casts the problem of minimizing decomposable submodular functions as an orthogonal projection problem to obtain easily parallelizable algorithms. Furthermore, by focusing on a special subclass of decomposable functions, they can prove the proximal problem can be solved in fewer iterations than currently possible.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "* Summary \n\nThe paper presents an algorithm for exact submodular minimization based on solving a related strongly convex function via Douglas-Rachford splitting. A key insight in the paper is to construct a dual problem whose structure allows Douglas-Rachford splitting to be applied. \n\n* Details \n\n(Quality) The paper is technically strong but lacking in two main respects. First, the experimental evaluation while demonstrating fast convergence of the proposed method over competing approaches does not include sufficient detail for reproducibility. Moreover, the MRF problem can be solved by graphs cuts (for which online code is available). It would be interesting to compare running time of the proposed method against graph cuts for this problem (even with all the standard caveats---Matlab vs C, more general method vs specialized one, etc). I also do not understand why the parallel methods take more iterations to converge than the standard implementations in Figure 2. Can the authors please comment. \n\nSecond, while the paper demonstrates that the proposed method is empirically faster than competing approaches it does not provide any theoretical guarantees. Competing algorithms are shown to be O(1/t) or O(1/sqrt{t}). What is the running time of the Douglas-Rachford approach? \n\n(Clarity) Over all the paper is clear written and (relative to the subject matter) easy to follow. Two suggestions that could improve the readability are: \n1. On line 072 the authors discuss a solution method via level sets. However, details of this are missing here and the method only becomes apparent in section 2 (L150). Some clarification around line 072 would help. \n2. Define y(A) = \\sum_{a \\in A} y_a around line 134. \n\n(Originality) The approach makes use of known techniques but their combination is original. \n\n(Significance) Given the pervasiveness of submodular functions in machine learning a fast and general minimization algorithm is likely to have an impact. The only drawback of the current method is that specialized methods are needed for projecting onto B(F_i) for each subproblem. \n The paper presents a principled approach to minimizing decomposeable submodular functions. The approach is generally well described but some additional details could help improve the paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary of ideas: \nThere is a previously known proximal problem, whose minimization gives at least as much information than minimizing the Lovasz extension. \nWhen a submodular function is written as a sum of submodular functions, each relatively easy to solve, solving the dual for the proximal problem can be done in terms of projections, rather than posing non-smooth functions as previous methods do. \n\nEvaluation: \n\nThe paper presents a refined way of dealing with submodular functions that are additively decomposable into simpler functions, by novel dual decompositions of a less well known use of the Lovasz extension. \nThese allow the decomposition to proceed without some of the complications arising due to non-smoothness in previous approaches. The empirical evaluation using examples from image processing shows promising results. \n\nThe paper is very clear in describing its predecessors; unfortunately, this seems to leave insufficient space to be more than sketchy about the novel techniques. \nFor example, one of the important technical contributions is to show how the DR variant of reference [5] is applicable to dual (9), which is specialized for a sum of 2 functions; but [5] focuses on two functions as well, and how exactly to apply these algorithms for r > 2 is not clear to me, despite such results being reported as useful in the experimental evaluation section under the name DR-para. \nWe certainly do not know iteration complexity depends of r. \nIn another example, any detailed description of the third main contribution is essentially deferred to the supplementary material (which formally states neither an algorithm nor a theorem). \n\nPros: \n- Decomposable submodular functions arise in applications, and more appropriate ways to minimize them are useful. \n- The novel duals might spur further work on algorithms for their solution, beyond the algorithms proposed here. \nCons: \n- While iteration and computational complexity bounds are treated as limitations of previous results, the novel optimization problems and algorithms are not accompanied by corresponding iteration complexity results. While a full analysis can be deferred to future work, some understanding of how r affects complexity under the different algorithms is missing. \n- The presentation of the algorithms (as opposed to duals) is not clear enough. \n\nDetailed comments: \n- 072 \"the level set {k, x_k^* \\ge 0} of ... x^*\" is not very clear, is {k | x_k^* \\ge 0} meant? Ref [2] refers to many different problems, a more specific reference for the fact being mentioned would be useful. \n- 088 Should reference section 3 where you give some examples in which this decomposition holds, and an idea of what types of \"simplicity\" we might encounter for the summand functions would be nice. \n- 116 The notation a(A) where a\\in R^n and A\\subseteq V does not seem to be introduced anywhere. \n- 117 This is misleading: Section 2 punts to the Supplementary material, which also skimps on explicit detail. \n- 138 computed the -> computed by the \n- 156 The term \"discrete gaps\" are used here and elsewhere but not clearly defined. The duality gaps of F(A) and a particular dual problem? Similarly for smooth gaps. \n- 170 \"In the appendix..\" An explicit algorithm should be accompanied by a lemma and proof giving its complexity. Neither is present, in the paper nor in the supplementary material. \n- \"problems have size 640x427. Hence ... have r = 640 + 247\" either there is a typo (427 to 247), or the relationship is not clear enough. Also, it is worth explaining how and why you \"solve as r=2/3\". \n- Figure 2: \n- Inconsistencies of vertical and horizontal axes makes comparisons difficult. \n- \"From top to bottom: four different images\" presumably 2 different images. A wider benchmark \n- Figure 3: \n- Does \"non-smooth problems\" not correspond to the previously existing duals? the legends suggest that the new duals are being used. \n- Empirical evaluation in general: comparisons of convergence in terms of iteration counts are informative only when iteration costs are roughly comparable. Are they? \n Interesting duals are accompanied by not so well presented algorithms.The presentation should focus and give explicit detail on the novel aspects at the expense of some detail on existing methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
