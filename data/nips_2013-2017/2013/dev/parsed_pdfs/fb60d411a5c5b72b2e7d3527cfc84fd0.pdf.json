{
  "name" : "fb60d411a5c5b72b2e7d3527cfc84fd0.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Annealing Between Distributions by Averaging Moments",
    "authors" : [ "Roger Grosse", "Chris J. Maddison" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Many powerful Monte Carlo techniques for estimating partition functions, such as annealed importance sampling (AIS), are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and the intractable target distribution. The near-universal practice is to use geometric averages of the initial and target distributions, but alternative paths can perform substantially better. We present a novel sequence of intermediate distributions for exponential families defined by averaging the moments of the initial and target distributions. We analyze the asymptotic performance of both the geometric and moment averages paths and derive an asymptotically optimal piecewise linear schedule. AIS with moment averaging performs well empirically at estimating partition functions of restricted Boltzmann machines (RBMs), which form the building blocks of many deep learning models."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many generative models are defined in terms of an unnormalized probability distribution, and computing the probability of a state requires computing the (usually intractable) partition function. This is problematic for model selection, since one often wishes to compute the probability assigned to held-out test data. While partition function estimation is intractable in general, there has been extensive research on variational [1, 2, 3] and sampling-based [4, 5, 6] approximations. In the context of model comparison, annealed importance sampling (AIS) [4] is especially widely used because given enough computing time, it can provide high-accuracy estimates. AIS has enabled precise quantitative comparisons of powerful generative models in image statistics [7, 8] and deep learning [9, 10, 11]. Unfortunately, applying AIS in practice can be computationally expensive and require laborious hand-tuning of annealing schedules. Because of this, many generative models still have not been quantitatively compared in terms of held-out likelihood.\nAIS requires defining a sequence of intermediate distributions which interpolate between a tractable initial distribution and the intractable target distribution. Typically, one uses geometric averages of the initial and target distributions. Tantalizingly, [12] derived the optimal paths for some toy models in the context of path sampling and showed that they vastly outperformed geometric averages. However, as choosing an optimal path is generally intractable, geometric averages still predominate.\nIn this paper, we present a theoretical framework for evaluating alternative paths. We propose a novel path defined by averaging moments of the initial and target distributions. We show that geometric averages and moment averages optimize different variational objectives, derive an asymptotically optimal piecewise linear schedule, and analyze the asymptotic performance of both paths. Our proposed path often outperforms geometric averages at estimating partition functions of restricted Boltzmann machines (RBMs)."
    }, {
      "heading" : "2 Estimating Partition Functions",
      "text" : "Suppose we have a probability distribution pb(x) = fb(x)/Zb defined on a space X , where fb(x) can be computed efficiently for a given x ∈ X , and we are interested in estimating the partition function Zb. Annealed importance sampling (AIS) is an algorithm which estimates Zb by gradually changing, or “annealing,” a distribution. In particular, one must specify a sequence of K + 1 intermediate distributions pk(x) = fk(x)/Zk for k = 0, . . .K, where pa(x) = p0(x) is a tractable initial distribution, and pb(x) = pK(x) is the intractable target distribution. For simplicity, assume all distributions are strictly positive on X . For each pk, one must also specify an MCMC transition operator Tk (e.g. Gibbs sampling) which leaves pk invariant. AIS alternates between MCMC transitions and importance sampling updates, as shown in Alg 1.\nAlgorithm 1 Annealed Importance Sampling for i = 1 to M do\nx0 ← sample from p0(x) w(i) ← Za for k = 1 to K do w(i) ← w(i) fk(xk−1)fk−1(xk−1) xk ← sample from Tk (x |xk−1)\nend for end for return Ẑb = ∑M i=1 w (i)/M The output of AIS is an unbiased estimate Ẑb of Zb. Remarkably, unbiasedness holds even in the context of non-equilibrium samples along the chain [4, 13]. However, unless the intermediate distributions and transition operators are carefully chosen, Ẑb may have high variance and be far from Zb with high probability. The mathematical formulation of AIS leaves much flexibility for choosing intermediate distributions. However, one typically defines a path γ : [0, 1] → P through some family P of distributions. The intermediate distributions pk are chosen to be points along this path corresponding to a schedule 0=β0 < β1 < . . . < βK =1. One typically uses the geometric path γGA, defined in terms of geometric averages of pa and pb:\npβ(x) = fβ(x)/Z(β) = fa(x)1−βfb(x)β/Z(β). (1)\nCommonly, fa is the uniform distribution, and (1) reduces to pβ(x) = fb(x)β/Z(β). This motivates the term “annealing,” and β resembles an inverse temperature parameter. As in simulated annealing, the “hotter” distributions often allow faster mixing between modes which are isolated in pb.\nAIS is closely related to a broader family of techniques for posterior inference and partition function estimation, all based on the following identity from statistical physics:\nlogZb − logZa = ∫ 1 0 Ex∼pβ [ d dβ log fβ(x) ] dβ. (2)\nThermodynamic integration [14] estimates (2) using numerical quadrature, and path sampling [12] does so with Monte Carlo integration. The weight update in AIS can be seen as a finite difference approximation. Tempered transitions [15] is a Metropolis-Hastings proposal operator which heats up and cools down the distribution, and computes an acceptance ratio by approximating (2).\nThe choices of a path and a schedule are central to all of these methods. Most work on adapting paths has focused on tuning schedules along a geometric path [15, 16, 17]. [15] showed that the geometric schedule was optimal for annealing the scale parameter of a Gaussian, and [16] extended this result more broadly. The aim of this paper is to propose, analyze, and evaluate a novel alternative to γGA based on averaging moments of the initial and target distributions."
    }, {
      "heading" : "3 Analyzing AIS Paths",
      "text" : "When analyzing AIS, it is common to assume perfect transitions, i.e. that each transition operator Tk returns an independent and exact sample from the distribution pk [4]. This corresponds to the (somewhat idealized) situation where the Markov chains mix quickly. As Neal [4] pointed out, assuming perfect transitions, the Central Limit Theorem shows that the samples w(i) are approximately log-normally distributed. In this case, the variances var(w(i)) and var(logw(i)) are both monotonically related to E[logw(i)]. Therefore, our analysis focuses on E[logw(i)].\nAssuming perfect transitions, the expected log weights are given by:\nE[logw(i)] = logZa + K−1∑ k=0 Epk [log fk+1(x)− log fk(x)] = logZb − K−1∑ k=0 DKL(pk‖pk+1). (3)\nIn other words, each logw(i) can be seen as a biased estimator of logZb, where the bias δ = logZb − E[logw(i)] is given by the sum of KL divergences ∑K−1 k=0 DKL(pk‖pk+1).\nSuppose P is a family of probability distributions parameterized by θ ∈ Θ, and the K + 1 distributions p0, . . . , pK are chosen to be linearly spaced along a path γ : [0, 1] → P . Let θ(β) represent the parameters of the distribution γ(β). As K is increased, the bias δ decays like 1/K, and the asymptotic behavior is determined by a functional F(γ). Theorem 1. Suppose K + 1 distributions pk are linearly spaced along a path γ. Assuming perfect transitions, if θ(β) and the Fisher information matrix Gθ(β) = covx∼pθ (∇θ log pθ(x)) are continuous and piecewise smooth, then as K →∞ the bias δ behaves as follows:\nKδ = K K−1∑ k=0 DKL(pk‖pk+1)→ F(γ) ≡ 1 2 ∫ 1 0 θ̇(β)TGθ(β)θ̇(β)dβ, (4)\nwhere θ̇(β) represents the derivative of θ with respect to β. [See supplementary material for proof.]\nThis result reveals a relationship with path sampling, as [12] showed that the variance of the path sampling estimator is proportional to the same functional. One useful result from their analysis is a derivation of the optimal schedule along a given path. In particular, the value of F(γ) using the optimal schedule is given by `(γ)2/2, where ` is the Riemannian path length defined by\n`(γ) = ∫ 1 0 √ θ̇(β)TGθ(β)θ̇(β)dβ. (5)\nIntuitively, the optimal schedule allocates more distributions to regions where pβ changes quickly. While [12] derived the optimal paths and schedules for some simple examples, they observed that this is intractable in most cases and recommended using geometric paths in practice.\nThe above analysis assumes perfect transitions, which can be unrealistic in practice because many distributions have separated modes between which mixing is difficult. As Neal [4] observed, in such cases, AIS can be viewed as having two sources of variance: that caused by variability within a mode, and that caused by misallocation of samples between modes. The former source of variance is well modeled by the perfect transitions analysis and can be made small by adding more intermediate distributions. The latter, however, can persist even with large numbers of intermediate distributions. While our theoretical analysis assumes perfect transitions, our proposed method often gave substantial improvement empirically in situations with poor mixing."
    }, {
      "heading" : "4 Moment Averaging",
      "text" : "As discussed in Section 2, the typical choice of intermediate distributions for AIS is the geometric averages path γGA given by (1). In this section, we propose an alternative path for exponential family models. An exponential family model is defined as\np(x) = 1\nZ(η) h(x) exp\n( ηTg(x) ) , (6)\nwhere η are the natural parameters and g are the sufficient statistics. Exponential families include a wide variety of statistical models, including Markov random fields.\nIn exponential families, geometric averages correspond to averaging the natural parameters:\nη(β) = (1− β)η(0) + βη(1). (7)\nExponential families can also be parameterized in terms of their moments s = E[g(x)]. For any minimal exponential family (i.e. one whose sufficient statistics are linearly independent), there is a one-to-one mapping between moments and natural parameters [18, p. 64]. We propose an alternative to γGA called the moment averages path, denoted γMA, and defined by averaging the moments of the initial and target distributions:\ns(β) = (1− β)s(0) + βs(1). (8)\nThis path exists for any exponential family model, since the set of realizable moments is convex [18]. It is unique, since g is unique up to affine transformation.\nAs an illustrative example, consider a multivariate Gaussian distribution parameterized by the mean µ and covariance Σ. The moments are E[x] = µ and − 12E[xx T ] = − 12 (Σ + µµ T ). By plugging these into (8), we find that γMA is given by:\nµ(β) = (1− β)µ(0) + βµ(1) (9) Σ(β) = (1− β)Σ(0) + βΣ(1) + β(1− β)(µ(1)− µ(0))(µ(1)− µ(0))T . (10)\nIn other words, the means are linearly interpolated, and the covariances are linearly interpolated and stretched in the direction connecting the two means. Intuitively, this stretching is a useful property, because it increases the overlap between successive distributions with different means. A comparison of the two paths is shown in Figure 1.\nparameters which match the averaged moments s(β). However, much work has been devoted to practical approximations [19, 20], some of which we use in our experiments with intractable models. Since it would be infeasible to moment match every βk even approximately, we introduce the moment averages spline (MAS) path, denoted γMAS . We choose a set ofR values β1, . . . , βR called knots, and solve for the natural parameters η(βj) to match the moments s(βj) for each knot. We then interpolate between the knots using geometric averages. The analysis of Section 4.2 shows that, under the assumption of perfect transitions, using γMAS in place of γMA does not affect the cost functional F defined in Theorem 1."
    }, {
      "heading" : "4.1 Variational Interpretation",
      "text" : "By interpreting γGA and γMA as optimizing different variational objectives, we gain additional insight into their behavior. For geometric averages, the intermediate distribution γGA(β) minimizes a weighted sum of KL divergences to the initial and target distributions:\np (GA) β = arg minq (1− β)DKL(q‖pa) + βDKL(q‖pb). (15)\nOn the other hand, γMA minimizes the weighted sum of KL divergences in the reverse direction:\np (MA) β = arg minq (1− β)DKL(pa‖q) + βDKL(pb‖q). (16)\nSee the supplementary material for the derivations. The objective function (15) is minimized by a distribution which puts significant mass only in the “intersection” of pa and pb, i.e. those regions which are likely under both distributions. By contrast, (16) encourages the distribution to be spread out in order to capture all high probability regions of both pa and pb. This interpretation helps explain why the intermediate distributions in the Gaussian example of Figure 1 take the shape that they do. In our experiments, we found that γMA often gave more accurate results than γGA because the intermediate distributions captured regions of the target distribution which were missed by γGA."
    }, {
      "heading" : "4.2 Asymptotics under Perfect Transitions",
      "text" : "In general, we found that γGA and γMA can look very different. Intriguingly, both paths always result in the same value of the cost functional F(γ) of Theorem 1 for any exponential family model. Furthermore, nothing is lost by using the spline approximation γMAS in place of γMA: Theorem 2. For any exponential family model with natural parameters η and moments s, all three paths share the same value of the cost functional:\nF(γGA) = F(γMA) = F(γMAS) = 1\n2 (η(1)− η(0))T (s(1)− s(0)). (17)\nProof. The two parameterizations of exponential families satisfy the relationship Gηη̇ = ṡ [21, sec. 3.3]. Therefore, F(γ) can be rewritten as 12 ∫ 1 0 η̇(β)T ṡ(β) dβ. Because γGA and γMA linearly interpolate the natural parameters and moments respectively,\nF(γGA) = 1\n2 (η(1)− η(0))T ∫ 1 0 ṡ(β) dβ = 1 2 (η(1)− η(0))T (s(1)− s(0)) (18)\nF(γMA) = 1\n2 (s(1)− s(0))T ∫ 1 0 η̇(β) dβ = 1 2 (s(1)− s(0))T (η(1)− η(0)). (19)\nFinally, to show that F(γMAS) = F(γMA), observe that γMAS uses the geometric path between each pair of knots γ(βj) and γ(βj+1), while γMA uses the moments path. The above analysis shows the costs must be equal for each segment, and therefore equal for the entire path.\nThis analysis shows that all three paths result in the same expected log weights asymptotically, assuming perfect transitions. There are several caveats, however. First, we have noticed experimentally that γMA often yields substantially more accurate estimates of Zb than γGA even when the average log weights are comparable. Second, the two paths can have very different mixing properties, which can strongly affect the results. Third, Theorem 2 assumes linear schedules, and in principle there is room for improvement if one is allowed to tune the schedule.\nFor instance, consider annealing between two Gaussians pa = N (µa, σ) and pb = N (µb, σ). The optimal schedule for γGA is a linear schedule with cost F(γGA) = O(d2), where d = |µb − µa|/σ. Using a linear schedule, the moment path also has costO(d2), consistent with Theorem 2. However, most of the cost of the path results from instability near the endpoints, where the variance changes suddenly. Using an optimal schedule, which allocates more distributions near the endpoints, the cost functional falls to O((log d)2), which is within a constant factor of the optimal path derived by [12]. (See the supplementary material for the derivations.) In other words, while F(γGA) = F(γMA), they achieve this value for different reasons: γGA follows an optimal schedule along a bad path, while γMA follows a bad schedule along a near-optimal path. We speculate that, combined with the procedure of Section 4.3 for choosing a schedule, moment averages may result in large reductions in the cost functional for some models."
    }, {
      "heading" : "4.3 Optimal Binned Schedules",
      "text" : "In general, it is hard to choose a good schedule for a given path. However, consider the set of binned schedules, where the path is divided into segments, some number Kj of intermediate distributions are allocated to each segment, and the distributions are spaced linearly within each segment. Under the assumption of perfect transitions, there is a simple formula for an asymptotically optimal binned schedule which requires only the parameters obtained through moment averaging: Theorem 3. Let γ be any path for an exponential family model defined by a set of knots βj , each with natural parameters ηj and moments sj , connected by segments of either γGA or γMA paths. Then, under the assumption of perfect transitions, an asymptotically optimal allocation of intermediate distributions to segments is given by:\nKj ∝ √ (ηj+1 − ηj)T (sj+1 − sj). (20)\nProof. By Theorem 2, the cost functional for segment j is Fj = 12 (ηj+1−ηj) T (sj+1−sj). Hence, with Kj distributions allocated to it, it contributes Fj/Kj to the total cost. The values of Kj which minimize ∑ j Fj/Kj subject to ∑ j Kj = K and Kj ≥ 0 are given by Kj ∝ √ Fj ."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "In order to compare our proposed path with geometric averages, we ran AIS using each path to estimate partition functions of several probability distributions. For all of our experiments, we report two sets of results. First, we show the estimates of logZ as a function of K, the number of intermediate distributions, in order to visualize the amount of computation necessary to obtain reasonable accuracy. Second, as recommended by [4], we report the effective sample size (ESS) of the weights for a large K. This statistic roughly measures how many independent samples one obtains using AIS.1 All results are based on 5,000 independent AIS runs, so the maximum possible ESS is 5,000."
    }, {
      "heading" : "5.1 Annealing Between Two Distant Gaussians",
      "text" : "In our first experiment, the initial and target distributions were the two Gaussians shown in Fig. 1, whose parameters are N ((−10 0 ) , ( 1 −0.85 −0.85 1 )) and N (( 10 0 ) , ( 1 0.85 0.85 1 )) . As both distributions are normalized, Za = Zb = 1. We compared γGA and γMA both under perfect transitions, and using the Gibbs transition operator. We also compared linear schedules with the optimal binned schedules of Section 4.3, using 10 segments evenly spaced from 0 to 1.\nFigure 2 shows the estimates of logZb for K ranging from 10 to 1,000. Observe that with 1,000 intermediate distributions, all paths yielded accurate estimates of logZb. However, γMA needed fewer intermediate distributions to achieve accurate estimates. For example, with K = 25, γMA resulted in an estimate within one nat of logZb, while the estimate based on γGA was off by 27 nats. This result may seem surprising in light of Theorem 2, which implies that F(γGA) = F(γMA) for linear schedules. In fact, the average log weights for γGA and γMA were similar for all values of K, as the theorem would suggest; e.g., with K = 25, the average was -27.15 for γMA and -28.04 for γGA. However, because the γMA intermediate distributions were broader, enough samples landed in high probability regions to yield reasonable estimates of logZb."
    }, {
      "heading" : "5.2 Partition Function Estimation for RBMs",
      "text" : "Our next set of experiments focused on restricted Boltzmann machines (RBMs), a building block of many deep learning models (see Section 4). We considered RBMs trained with three different methods: contrastive divergence (CD) [19] with one step (CD1), CD with 25 steps (CD25), and persistent contrastive divergence (PCD) [20]. All of the RBMs were trained on the MNIST handwritten digits dataset [22], which has long served as a benchmark for deep learning algorithms. We experimented both with small, tractable RBMs and with full-size, intractable RBMs.\nSince it is hard to compute γMA exactly for RBMs, we used the moments spline path γMAS of Section 4 with the 9 knot locations 0.1, 0.2, . . . , 0.9. We considered the two initial distributions discussed by [9]: (1) the uniform distribution, equivalent to an RBM where all the weights and biases are set to 0, and (2) the base rate RBM, where the weights and hidden biases are set to 0, and the visible biases are set to match the average pixel values over the MNIST training set.\nSmall, Tractable RBMs: To better understand the behavior of γGA and γMAS , we first evaluated the paths on RBMs with only 20 hidden units. In this setting, it is feasible to exactly compute the\n1The ESS is defined as M/(1+ s2(w(i)∗ )) where s2(w (i) ∗ ) is the sample variance of the normalized weights [4]. In general, one should regard ESS estimates cautiously, as they can give misleading results in cases where an algorithm completely misses an important mode of the distribution. However, as we report the ESS in cases where the estimated partition functions are close to the true value (when known) or agree closely with each other, we believe the statistic is meaningful in our comparisons.\npartition function and moments and to generate exact samples by exhaustively summing over all 220 hidden configurations. The moments of the target RBMs were computed exactly, and moment matching was performed with conjugate gradient using the exact gradients.\nThe results are shown in Figure 3 and Table 1. Under perfect transitions, γGA and γMAS were both able to accurately estimate logZb using as few as 100 intermediate distributions. However, using the Gibbs transition operator, γMAS gave accurate estimates using fewer intermediate distributions and achieved a higher ESS at K = 100,000. To check that the improved performance didn’t rely on accurate moments of pb, we repeated the experiment with highly biased moments.2 The differences in log Ẑb and ESS compared to the exact moments condition were not statistically significant. Full-size, Intractable RBMs: For intractable RBMs, moment averaging required approximately solving two intractable problems: moment estimation for the target RBM, and moment matching. We estimated the moments from 1,000 independent Gibbs chains, using 10,000 Gibbs steps with 1,000 steps of burn-in. The moment averaged RBMs were trained using PCD. (We used 50,000 updates with a fixed learning rate of 0.01 and no momentum.) In addition, we ran a cheap, inaccurate moment matching scheme (denoted MAS cheap) where visible moments were estimated from the empirical MNIST base rate and the hidden moments from the conditional distributions of the hidden units given the MNIST digits. Intermediate RBMs were fit using 1,000 PCD updates and 100 particles, for a total computational cost far smaller than that of AIS itself. Results of both methods are\n2In particular, we computed the biased moments from the conditional distributions of the hidden units given the MNIST training examples, where each example of digit class i was counted i+ 1 times.\nshown in Figure 5 and Table 2. Overall, the MAS results compare favorably with those of GA on both of our metrics. Performance was comparable under MAS cheap, suggesting that γMAS can be approximated cheaply and effectively. As with the tractable RBMs, we found that optimal binned schedules made little difference in performance, so we focus here on linear schedules.\nThe most serious failure was γGA for CD1(500) with uniform initialization, which underestimated our best estimates of the log partition function (and hence overestimated held-out likelihood) by nearly 20 nats. The geometric path from uniform to PCD(500) and the moments path from uniform to CD1(500) also resulted in underestimates, though less drastic. The rest of the paths agreed closely with each other on their partition function estimates, although some methods achieved substantially higher ESS values on some RBMs. One conclusion is that it’s worth exploring multiple initializations and paths for a given RBM in order to ensure accurate results.\nFigure 4 compares samples along γGA and γMAS for the PCD(500) RBM using the base rate initialization. For a wide range of β values, the γGA RBMs assigned most of their probability mass to blank images. As discussed in Section 4.1, γGA prefers configurations which are probable under both the initial and target distributions. In this case, the hidden activations were closer to uniform conditioned on a blank image than on a digit, so γGA preferred blank images. By contrast, γMAS yielded diverse, blurry digits which gradually coalesced into crisper ones."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a theoretical analysis of the performance of AIS paths and proposed a novel path for exponential families based on averaging moments. We gave a variational interpretation of this path and derived an asymptotically optimal piecewise linear schedule. Moment averages performed well empirically at estimating partition functions of RBMs. We hope moment averaging can also improve other path-based sampling algorithms which typically use geometric averages, such as path sampling [12], parallel tempering [23], and tempered transitions [15]."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by NSERC and Quanta Computer. We thank Geoffrey Hinton for helpful discussions. We also thank the anonymous reviewers for thorough and helpful feedback."
    } ],
    "references" : [ {
      "title" : "Constructing free-energy approximations and generalized belief propagation algorithms",
      "author" : [ "J.S. Yedidia", "W.T. Freeman", "Y. Weiss" ],
      "venue" : "IEEE Trans. on Inf. Theory,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "A new class of upper bounds on the log partition function",
      "author" : [ "Martin J. Wainwright", "Tommi Jaakkola", "Alan S. Willsky" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Approximate Inference Using Conditional Entropy Decompositions",
      "author" : [ "Amir Globerson", "Tommi Jaakkola" ],
      "venue" : "In 11th International Workshop on AI and Statistics (AISTATS’2007),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Annealed importance sampling",
      "author" : [ "Radford Neal" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Nested sampling for general Bayesian computation",
      "author" : [ "John Skilling" ],
      "venue" : "Bayesian Analysis,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Sequential Monte Carlo samplers",
      "author" : [ "Pierre Del Moral", "Arnaud Doucet", "Ajay Jasra" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Methodology),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Hamiltonian annealed importance sampling for partition function estimation",
      "author" : [ "Jascha Sohl-Dickstein", "Benjamin J. Culpepper" ],
      "venue" : "Technical report, Redwood Center, UC Berkeley,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "In all likelihood, deep belief is not enough",
      "author" : [ "Lucas Theis", "Sebastian Gerwinn", "Fabian Sinz", "Matthias Bethge" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "On the quantitative analysis of deep belief networks",
      "author" : [ "Ruslan Salakhutdinov", "Ian Murray" ],
      "venue" : "In Int’l Conf. on Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "On tracking the partition function",
      "author" : [ "Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : "In NIPS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Products of hidden Markov models: It takes N > 1 to tango",
      "author" : [ "Graham Taylor", "Geoffrey Hinton" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Simulating normalizing constants: From importance sampling to bridge sampling to path sampling",
      "author" : [ "Andrew Gelman", "Xiao-Li Meng" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1998
    }, {
      "title" : "Equilibrium free-energy differences from nonequilibrium measurements: A master-equation approach",
      "author" : [ "Christopher Jarzynski" ],
      "venue" : "Physical Review E,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1997
    }, {
      "title" : "Understanding Molecular Simulation: From Algorithms to Applications",
      "author" : [ "Daan Frenkel", "Berend Smit" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Sampling from multimodal distributions using tempered transitions",
      "author" : [ "Radford Neal" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1996
    }, {
      "title" : "Tuning tempered transitions",
      "author" : [ "Gundula Behrens", "Nial Friel", "Merrilee Hurn" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Estimating Bayes factors via thermodynamic integration and population MCMC",
      "author" : [ "Ben Calderhead", "Mark Girolami" ],
      "venue" : "Computational Statistics and Data Analysis,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "Martin J. Wainwright", "Michael I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "Geoffrey E. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "Training restricted Boltzmann machines using approximations to the likelihood gradient",
      "author" : [ "Tijmen Tieleman" ],
      "venue" : "In Intl. Conf. on Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Methods of Information Geometry",
      "author" : [ "Shun-ichi Amari", "Hiroshi Nagaoka" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2000
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    }, {
      "title" : "Extended ensemble Monte Carlo",
      "author" : [ "Y. Iba" ],
      "venue" : "International Journal of Modern Physics C,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "While partition function estimation is intractable in general, there has been extensive research on variational [1, 2, 3] and sampling-based [4, 5, 6] approximations.",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "While partition function estimation is intractable in general, there has been extensive research on variational [1, 2, 3] and sampling-based [4, 5, 6] approximations.",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "While partition function estimation is intractable in general, there has been extensive research on variational [1, 2, 3] and sampling-based [4, 5, 6] approximations.",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "While partition function estimation is intractable in general, there has been extensive research on variational [1, 2, 3] and sampling-based [4, 5, 6] approximations.",
      "startOffset" : 141,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "While partition function estimation is intractable in general, there has been extensive research on variational [1, 2, 3] and sampling-based [4, 5, 6] approximations.",
      "startOffset" : 141,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "While partition function estimation is intractable in general, there has been extensive research on variational [1, 2, 3] and sampling-based [4, 5, 6] approximations.",
      "startOffset" : 141,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "In the context of model comparison, annealed importance sampling (AIS) [4] is especially widely used because given enough computing time, it can provide high-accuracy estimates.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "AIS has enabled precise quantitative comparisons of powerful generative models in image statistics [7, 8] and deep learning [9, 10, 11].",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "AIS has enabled precise quantitative comparisons of powerful generative models in image statistics [7, 8] and deep learning [9, 10, 11].",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 8,
      "context" : "AIS has enabled precise quantitative comparisons of powerful generative models in image statistics [7, 8] and deep learning [9, 10, 11].",
      "startOffset" : 124,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "AIS has enabled precise quantitative comparisons of powerful generative models in image statistics [7, 8] and deep learning [9, 10, 11].",
      "startOffset" : 124,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : "AIS has enabled precise quantitative comparisons of powerful generative models in image statistics [7, 8] and deep learning [9, 10, 11].",
      "startOffset" : 124,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "Tantalizingly, [12] derived the optimal paths for some toy models in the context of path sampling and showed that they vastly outperformed geometric averages.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "Remarkably, unbiasedness holds even in the context of non-equilibrium samples along the chain [4, 13].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "Remarkably, unbiasedness holds even in the context of non-equilibrium samples along the chain [4, 13].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "Thermodynamic integration [14] estimates (2) using numerical quadrature, and path sampling [12] does so with Monte Carlo integration.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 11,
      "context" : "Thermodynamic integration [14] estimates (2) using numerical quadrature, and path sampling [12] does so with Monte Carlo integration.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Tempered transitions [15] is a Metropolis-Hastings proposal operator which heats up and cools down the distribution, and computes an acceptance ratio by approximating (2).",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 14,
      "context" : "Most work on adapting paths has focused on tuning schedules along a geometric path [15, 16, 17].",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "Most work on adapting paths has focused on tuning schedules along a geometric path [15, 16, 17].",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "Most work on adapting paths has focused on tuning schedules along a geometric path [15, 16, 17].",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "[15] showed that the geometric schedule was optimal for annealing the scale parameter of a Gaussian, and [16] extended this result more broadly.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[15] showed that the geometric schedule was optimal for annealing the scale parameter of a Gaussian, and [16] extended this result more broadly.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "that each transition operator Tk returns an independent and exact sample from the distribution pk [4].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "As Neal [4] pointed out, assuming perfect transitions, the Central Limit Theorem shows that the samples w are approximately log-normally distributed.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 11,
      "context" : "This result reveals a relationship with path sampling, as [12] showed that the variance of the path sampling estimator is proportional to the same functional.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "While [12] derived the optimal paths and schedules for some simple examples, they observed that this is intractable in most cases and recommended using geometric paths in practice.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "As Neal [4] observed, in such cases, AIS can be viewed as having two sources of variance: that caused by variability within a mode, and that caused by misallocation of samples between modes.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 17,
      "context" : "This path exists for any exponential family model, since the set of realizable moments is convex [18].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "However, much work has been devoted to practical approximations [19, 20], some of which we use in our experiments with intractable models.",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 19,
      "context" : "However, much work has been devoted to practical approximations [19, 20], some of which we use in our experiments with intractable models.",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "Using an optimal schedule, which allocates more distributions near the endpoints, the cost functional falls to O((log d)(2)), which is within a constant factor of the optimal path derived by [12].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "Second, as recommended by [4], we report the effective sample size (ESS) of the weights for a large K.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "We considered RBMs trained with three different methods: contrastive divergence (CD) [19] with one step (CD1), CD with 25 steps (CD25), and persistent contrastive divergence (PCD) [20].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : "We considered RBMs trained with three different methods: contrastive divergence (CD) [19] with one step (CD1), CD with 25 steps (CD25), and persistent contrastive divergence (PCD) [20].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 21,
      "context" : "All of the RBMs were trained on the MNIST handwritten digits dataset [22], which has long served as a benchmark for deep learning algorithms.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "We considered the two initial distributions discussed by [9]: (1) the uniform distribution, equivalent to an RBM where all the weights and biases are set to 0, and (2) the base rate RBM, where the weights and hidden biases are set to 0, and the visible biases are set to match the average pixel values over the MNIST training set.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "The ESS is defined as M/(1+ s(2)(w ∗ )) where s(2)(w (i) ∗ ) is the sample variance of the normalized weights [4].",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "We hope moment averaging can also improve other path-based sampling algorithms which typically use geometric averages, such as path sampling [12], parallel tempering [23], and tempered transitions [15].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 22,
      "context" : "We hope moment averaging can also improve other path-based sampling algorithms which typically use geometric averages, such as path sampling [12], parallel tempering [23], and tempered transitions [15].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 14,
      "context" : "We hope moment averaging can also improve other path-based sampling algorithms which typically use geometric averages, such as path sampling [12], parallel tempering [23], and tempered transitions [15].",
      "startOffset" : 197,
      "endOffset" : 201
    } ],
    "year" : 2013,
    "abstractText" : "Many powerful Monte Carlo techniques for estimating partition functions, such as annealed importance sampling (AIS), are based on sampling from a sequence of intermediate distributions which interpolate between a tractable initial distribution and the intractable target distribution. The near-universal practice is to use geometric averages of the initial and target distributions, but alternative paths can perform substantially better. We present a novel sequence of intermediate distributions for exponential families defined by averaging the moments of the initial and target distributions. We analyze the asymptotic performance of both the geometric and moment averages paths and derive an asymptotically optimal piecewise linear schedule. AIS with moment averaging performs well empirically at estimating partition functions of restricted Boltzmann machines (RBMs), which form the building blocks of many deep learning models.",
    "creator" : null
  }
}