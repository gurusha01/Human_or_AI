{
  "name" : "a223c6b3710f85df22e9377d6c4f7553.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition",
    "authors" : [ "Adel Javanmard", "Andrea Montanari" ],
    "emails" : [ "adelj@stanford.edu", "montanar@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In linear regression, we wish to estimate an unknown but fixed vector of parameters θ0 ∈ Rp from n pairs (Y1, X1), (Y2, X2), . . . , (Yn, Xn), with vectors Xi taking values in Rp and response variables Yi given by\nYi = 〈θ0, Xi〉+Wi , Wi ∼ N(0, σ2) , (1)\nwhere 〈 · , · 〉 is the standard scalar product. In matrix form, letting Y = (Y1, . . . , Yn)T and denoting by X the design matrix with rows XT1 , . . . , X T n , we have\nY = X θ0 +W , W ∼ N(0, σ2In×n) . (2)\nIn this paper, we consider the high-dimensional setting in which the number of parameters exceeds the sample size, i.e., p > n, but the number of non-zero entries of θ0 is smaller than p. We denote by S ≡ supp(θ0) ⊆ [p] the support of θ0, and let s0 ≡ |S|. We are interested in the ‘model selection’ problem, namely in the problem of identifying S from data Y , X.\nIn words, there exists a ‘true’ low dimensional linear model that explains the data. We want to identify the set S of covariates that are ‘active’ within this model. This problem has motivated a large body of research, because of its relevance to several modern data analysis tasks, ranging from signal processing [9, 5] to genomics [15, 16]. A crucial step forward has been the development of model-selection techniques based on convex optimization formulations [17, 8, 6]. These formulations have lead to computationally efficient algorithms that can be applied to large scale problems. Such developments pose the following theoretical question: For which vectors θ0, designs X, and\nnoise levels σ, the support S can be identified, with high probability, through computationally efficient procedures? The same question can be asked for random designs X and, in this case, ‘high probability’ will refer both to the noise realization W , and to the design realization X. In the rest of this introduction we shall focus –for the sake of simplicity– on the deterministic settings, and refer to Section 3 for a treatment of Gaussian random designs.\nThe analysis of computationally efficient methods has largely focused on `1-regularized least squares, a.k.a. the Lasso [17]. The Lasso estimator is defined by\nθ̂n(Y,X;λ) ≡ arg min θ∈Rp { 1 2n ‖Y −Xθ‖22 + λ‖θ‖1 } . (3)\nIn case the right hand side has more than one minimizer, one of them can be selected arbitrarily for our purposes. It is worth noting that when columns of X are in general positions (e.g. when the entries of X are drawn form a continuous probability distribution), the Lasso solution is unique [18]. We will often omit the arguments Y , X, as they are clear from the context. (A closely related method is the so-called Dantzig selector [6]: it would be interesting to explore whether our results can be generalized to that approach.)\nIt was understood early on that, even in the large-sample, low-dimensional limit n → ∞ at p constant, supp(θ̂n) 6= S unless the columns of X with index in S are roughly orthogonal to the ones with index outside S [12]. This assumption is formalized by the so-called ‘irrepresentability condition’, that can be stated in terms of the empirical covariance matrix Σ̂ = (XTX/n). Letting Σ̂A,B be the submatrix (Σ̂i,j)i∈A,j∈B , irrepresentability requires\n‖Σ̂Sc,SΣ̂−1S,S sign(θ0,S)‖∞ ≤ 1− η , (4)\nfor some η > 0 (here sign(u)i = +1, 0, −1 if, respectively, ui > 0, = 0, < 0). In an early breakthrough, Zhao and Yu [23] proved that, if this condition holds with η uniformly bounded away from 0, it guarantees correct model selection also in the high-dimensional regime p n. Meinshausen and Bülmann [14] independently established the same result for random Gaussian designs, with applications to learning Gaussian graphical models. These papers applied to very sparse models, requiring in particular s0 = O(nc), c < 1, and parameter vectors with large coefficients. Namely, scaling the columns of X such that Σ̂i,i ≤ 1, for i ∈ [p], they require θmin ≡ mini∈S |θ0,i| ≥ c √ s0/n.\nWainwright [21] strengthened considerably these results by allowing for general scalings of s0, p, n and proving that much smaller non-zero coefficients can be detected. Namely, he proved that for a broad class of empirical covariances it is only necessary that θmin ≥ cσ √ (log p)/n. This scaling of the minimum non-zero entry is optimal up to constants. Also, for a specific classes of random Gaussian designs (including X with i.i.d. standard Gaussian entries), the analysis of [21] provides tight bounds on the minimum sample size for correct model selection. Namely, there exists c`, cu > 0 such that the Lasso fails with high probability if n < c` s0 log p and succeeds with high probability if n ≥ cu s0 log p. While, thanks to these recent works [23, 14, 21], we understand reasonably well model selection via the Lasso, it is fundamentally unknown what model-selection performances can be achieved with general computationally practical methods. Two aspects of of the above theory cannot be improved substantially: (i) The non-zero entries must satisfy the condition θmin ≥ cσ/ √ n to be detected with high probability. Even if n = p and the measurement directions Xi are orthogonal, e.g., X = √ nIn×n, one would need |θ0,i| ≥ cσ/ √ n to distinguish the i-th entry from noise. For instance, in [10], the authors prove a general upper bound on the minimax power of tests for hypothesesH0,i = {θ0,i = 0}. Specializing this bound to the case of standard Gaussian designs, the analysis of [10] shows formally that no test can detect θ0,i 6= 0, with a fixed degree of confidence, unless |θ0,i| ≥ cσ/ √ n. (ii) The sample size must satisfy n ≥ s0. Indeed, if this is not the case, for each θ0 with support of size |S| = s0, there is a one parameter family {θ0(t) = θ0 + t v}t∈R with supp(θ0(t)) ⊆ S, Xθ0(t) = Xθ0 and, for specific values of t, the support of θ0(t) is strictly contained in S.\nOn the other hand, there is no fundamental reason to assume the irrepresentability condition (4). This follows from the requirement that a specific method (the Lasso) succeeds, but is unclear why it should be necessary in general. In this paper we prove that the Gauss-Lasso selector has nearly optimal model selection properties under a condition that is strictly weaker than irrepresentability.\nGAUSS-LASSO SELECTOR: Model selector for high dimensional problems\nInput: Measurement vector y, design model X, regularization parameter λ, support size s0. Output: Estimated support Ŝ.\n1: Let T = supp(θ̂n) be the support of Lasso estimator θ̂n = θ̂n(y,X, λ) given by\nθ̂n(Y,X;λ) ≡ arg min θ∈Rp { 1 2n ‖Y −Xθ‖22 + λ‖θ‖1 } .\n2: Construct the estimator θ̂GL as follows:\nθ̂GLT = (X T TXT ) −1XTT y , θ̂ GL T c = 0 .\n3: Find s0-th largest entry (in modulus) of θ̂GLT , denoted by θ̂ GL (s0) , and let\nŜ ≡ { i ∈ [p] : |θ̂GLi | ≥ |θ̂GL(s0)| } .\nWe call this condition the generalized irrepresentability condition (GIC). The Gauss-Lasso procedure uses the Lasso to estimate a first model T ⊆ {1, . . . , p}. It then constructs a new estimator by ordinary least squares regression of the data Y onto the model T .\nWe prove that the estimated model is, with high probability, correct (i.e., Ŝ = S) under conditions comparable to the ones assumed in [14, 23, 21], while replacing irrepresentability by the weaker generalized irrepresentability condition. In the case of random Gaussian designs, our analysis further assumes the restricted eigenvalue property in order to establish a nearly optimal scaling of the sample size n with the sparsity parameter s0.\nIn order to build some intuition about the difference between irrepresentability and generalized irrepresentability, it is convenient to consider the Lasso cost function at ‘zero noise’:\nG(θ; ξ) ≡ 1 2n ‖X(θ − θ0)‖22 + ξ‖θ‖1 = 1 2 〈(θ − θ0), Σ̂(θ − θ0)〉+ ξ‖θ‖1 .\nLet θ̂ZN(ξ) be the minimizer of G( · ; ξ) and v ≡ limξ→0+ sign(θ̂ZN(ξ)). The limit is well defined by Lemma 2.2 below. The KKT conditions for θ̂ZN imply, for T ≡ supp(v),\n‖Σ̂T c,T Σ̂−1T,T vT ‖∞ ≤ 1 .\nSince G( · ; ξ) has always at least one minimizer, this condition is always satisfied. Generalized irrepresentability requires that the above inequality holds with some small slack η > 0 bounded away from zero, i.e.,\n‖Σ̂T c,T Σ̂−1T,T vT ‖∞ ≤ 1− η .\nNotice that this assumption reduces to standard irrepresentability cf. Eq. (4) if, in addition, we ask that v = sign(θ0). In other words, earlier work [14, 23, 21] required generalized irrepresentability plus sign-consistency in zero noise, and established sign consistency in non-zero noise. In this paper the former condition is shown to be sufficient.\nFrom a different point of view, GIC demands that irrepresentability holds for a superset of the true support S. It was indeed argued in the literature that such a relaxation of irrepresentability allows to cover a significantly broader set of cases (see for instance [3, Section 7.7.6]). However, it was never clarified why such a superset irrepresentability condition should be significantly more general than simple irrepresentability. Further, no precise prescription existed for the superset of the true support.\nOur contributions can therefore be summarized as follows:\n• By tying it to the KKT condition for the zero-noise problem, we justify the expectation that generalized irrepresentability should hold for a broad class of design matrices.\n• We thus provide a specific formulation of superset irrepresentability, prescribing both the superset T and the sign vector vT , that is, by itself, significantly more general than simple irrepresentability.\n• We show that, under GIC, exact support recovery can be guaranteed using the Gauss-Lasso, and formulate the appropriate ‘minimum coefficient’ conditions that guarantee this. As a side remark, even when simple irrepresentability holds, our results strengthen somewhat the estimates of [21] (see below for details).\nThe paper is organized as follows. In the rest of the introduction we illustrate the range of applicability of GIC through a simple example and we discuss further related work. We finally introduce the basic notations to be used throughout the paper. Section 2 treats the case of deterministic designs X, and develops our main results on the basis of the GIC. Section 3 extends our analysis to the case of random designs. In this case GIC is required to hold for the population covariance, and the analysis is more technical as it requires to control the randomness of the design matrix. We refer the reader to the long version of the paper [11] for the proofs of our main results and the technical steps."
    }, {
      "heading" : "1.1 An example",
      "text" : "In order to illustrate the range of new cases covered by our results, it is instructive to consider a simple example. A detailed discussion of this calculation can be found in [11]. The example corresponds to a Gaussian random design, i.e., the rows XT1 , . . .X T n are i.i.d. realizations of a p-variate normal distribution with mean zero. We write Xi = (Xi,1, Xi,2, . . . , Xi,p)T for the components of Xi. The response variable is linearly related to the first s0 covariates:\nYi = θ0,1Xi,1 + θ0,2Xi,2 + · · ·+ θ0,s0Xi,s0 +Wi , where Wi ∼ N(0, σ2) and we assume θ0,i > 0 for all i ≤ s0. In particular S = {1, . . . , s0}. As for the design matrix, first p − 1 covariates are orthogonal at the population level, i.e., Xi,j ∼ N(0, 1) are independent for 1 ≤ j ≤ p−1 (and 1 ≤ i ≤ n). However the p-th covariate is correlated to the s0 relevant ones:\nXi,p = aXi,1 + aXi,2 + · · ·+ aXi,s0 + b X̃i,p .\nHere X̃i,p ∼ N(0, 1) is independent from {Xi,1, . . . , Xi,p−1} and represents the orthogonal component of the p-th covariate. We choose the coefficients a, b ≥ 0 such that s0a2 + b2 = 1, whence E{X2i,p} = 1 and hence the p-th covariate is normalized as the first (p − 1) ones. In other words, the rows of X are i.i.d. Gaussian Xi ∼ N(0,Σ) with covariance given by\nΣij =  1 if i = j, a if i = p, j ∈ S or i ∈ S, j = p, 0 otherwise.\nFor a = 0, this is the standard i.i.d. design and irrepresentability holds. The Lasso correctly recovers the support S from n ≥ c s0 log p samples, provided θmin ≥ c′ √ (log p)/n. It follows from [21] that this remains true as long as a ≤ (1−η)/s0 for some η > 0 bounded away from 0. However, as soon as a > 1/s0, the Lasso includes the p-th covariate in the estimated model, with high probability.\nOn the other hand, Gauss-Lasso is successful for a significantly larger set of values of a. Namely, if a ∈ [ 0,\n1− η s0\n] ∪ ( 1\ns0 , 1− η √ s0\n] ,\nthen it recovers S from n ≥ c s0 log p samples, provided θmin ≥ c′ √\n(log p)/n. While the interval ((1−η)/s0, 1/s0] is not covered by this result, we expect this to be due to the proof technique rather than to an intrinsic limitation of the Gauss-Lasso selector."
    }, {
      "heading" : "1.2 Further related work",
      "text" : "The restricted isometry property [7, 6] (or the related restricted eigenvalue [2] or compatibility conditions [19]) have been used to establish guarantees on the estimation and model selection errors of the Lasso or similar approaches. In particular, Bickel, Ritov and Tsybakov [2] show that, under such conditions, with high probability,\n‖θ̂ − θ0‖22 ≤ Cσ2 s0 log p\nn .\nThe same conditions can be used to prove model-selection guarantees. In particular, Zhou [24] studies a multi-step thresholding procedure whose first steps coincide with the Gauss-Lasso. While the main objective of this work is to prove high-dimensional `2 consistency with a sparse estimated model, the author also proves partial model selection guarantees. Namely, the method correctly recovers a subset of large coefficients SL ⊆ S, provided |θ0,i| ≥ cσ √ s0(log p)/n, for i ∈ SL. This means that the coefficients that are guaranteed to be detected must be a factor √ s0 larger than what is required by our results.\nAn alternative approach to establishing model-selection guarantees assumes a suitable mutual incoherence conditions. Lounici [13] proves correct model selection under the assumption maxi6=j |Σ̂ij | = O(1/s0). This assumption is however stronger than irrepresentability [19]. Candés and Plan [4] also assume mutual incoherence, albeit with a much weaker requirement, namely maxi6=j |Σ̂ij | = O(1/(log p)). Under this condition, they establish model selection guarantees for an ideal scaling of the non-zero coefficients θmin ≥ cσ √ (log p)/n. However, this result only holds with high probability for a ‘random signal model’ in which the non-zero coefficients θ0,i have uniformly random signs.\nThe authors in [22] consider the variable selection problem, and under the same assumptions on the non-zero coefficients as in the present paper, guarantee support recovery under a cone condition. The latter condition however is stronger than the generalized irrepresentability condition. In particular, for the example in Section 1.1 it yields no improvement over the standard irrepresentability. The work [20] studies the adaptive and the thresholded Lasso estimators and proves correct model selection assuming the non-zero coefficients are of order s0 √ (log p)/n.\nFinally, model selection consistency can be obtained without irrepresentability through other methods. For instance [25] develops the adaptive Lasso, using a data-dependent weighted `1 regularization, and [1] proposes the Bolasso, a resampling-based techniques. Unfortunately, both of these approaches are only guaranteed to succeed in the low-dimensional regime of p fixed, and n→∞."
    }, {
      "heading" : "1.3 Notations",
      "text" : "We provide a brief summary of the notations used throughout the paper. For a matrix A and set of indices I, J , we let AJ denote the submatrix containing just the columns in J and AI,J denote the submatrix formed by the rows in I and columns in J . Likewise, for a vector v, vI is the restriction of v to indices in I . Further, the notation A−1I,I represents the inverse of AI,I , i.e., A −1 I,I = (AI,I)\n−1. The maximum and the minimum singular values of A are respectively denoted by σmax(A) and σmin(A). We write ‖v‖p for the standard `p norm of a vector v. Specifically, ‖v‖0 denotes the number of nonzero entries in v. Also, ‖A‖p refers to the induced operator norm on a matrix A. We use ei to refer to the i-th standard basis element, e.g., e1 = (1, 0, . . . , 0). For a vector v, supp(v) represents the positions of nonzero entries of v. Throughout, we denote the rows of the design matrix X byX1, . . . , Xn ∈ Rp and denote its columns by x1, . . . , xp ∈ Rn. Further, for a vector v, sign(v) is the vector with entries sign(v)i = +1 if vi > 0, sign(v)i = −1 if vi < 0, and sign(v)i = 0 otherwise."
    }, {
      "heading" : "2 Deterministic designs",
      "text" : "An outline of this section is as follows: (1) We first consider the zero-noise problem W = 0, and prove several useful properties of the Lasso estimator in this case. In particular, we show that there exists a threshold for the regularization parameter below which the support of the Lasso estimator remains the same and contains supp(θ0). Moreover, the Lasso estimator support is not much larger than supp(θ0). (2) We then turn to the noisy problem, and introduce the generalized irrepresentability condition (GIC) that is motivated by the properties of the Lasso in the zero-noise case. We prove that under GIC (and other technical conditions), with high probability, the signed support of the Lasso estimator is the same as that in the zero-noise problem. (3) We show that the Gauss-Lasso selector correctly recovers the signed support of θ0."
    }, {
      "heading" : "2.1 Zero-noise problem",
      "text" : "Recall that Σ̂ ≡ (XTX/n) denotes the empirical covariance of the rows of the design matrix. Given Σ̂ ∈ Rp×p, Σ̂ 0, θ0 ∈ Rp and ξ ∈ R+, we define the zero-noise Lasso estimator as\nθ̂ZN(ξ) ≡ arg min θ∈Rp { 1 2n 〈(θ − θ0), Σ̂(θ − θ0)〉+ ξ‖θ‖1 } . (5)\nNote that θ̂ZN(ξ) is obtained by letting Y = Xθ0 in the definition of θ̂n(Y,X; ξ).\nFollowing [2], we introduce a restricted eigenvalue constant for the empirical covariance matrix Σ̂:\nκ̂(s, c0) ≡ min J⊆[p] |J|≤s\nmin u∈Rp\n‖uJc‖1≤c0‖uJ‖1\n〈u, Σ̂u〉 ‖u‖22 . (6)\nOur first result states that supp(θ̂ZN(ξ)) is not much larger than the support of θ0, for any ξ > 0.\nLemma 2.1. Let θ̂ZN = θ̂ZN(ξ) be defined as per Eq. (5), with ξ > 0. Then, if s0 = ‖θ0‖0,\n‖θ̂ZN‖0 ≤ ( 1 +\n4‖Σ̂‖2 κ̂(s0, 1)\n) s0 . (7)\nLemma 2.2. Let θ̂ZN = θ̂ZN(ξ) be defined as per Eq. (5), with ξ > 0. Then there exist ξ0 = ξ0(Σ̂, S, θ0) > 0, T0 ⊆ [p], v0 ∈ {−1, 0,+1}p, such that the following happens. For all ξ ∈ (0, ξ0), sign(θ̂ZN(ξ)) = v0 and supp(θ̂ZN(ξ)) = supp(v0) = T0. Further T0 ⊇ S, v0,S = sign(θ0,S) and ξ0 = mini∈S |θ0,i/[Σ̂−1T0,T0v0,T0 ]i|.\nFinally we have the following standard characterization of the solution of the zero-noise problem.\nLemma 2.3. Let θ̂ZN = θ̂ZN(ξ) be defined as per Eq. (5), with ξ > 0. Let T ⊇ S and v ∈ {+1, 0,−1}p be such that supp(v) = T . Then sign(θ̂ZN) = v if and only if∥∥∥Σ̂T c,T Σ̂−1T,T vT∥∥∥∞ ≤ 1 , (8)\nvT = sign ( θ0,T − ξΣ̂−1T,T vT ) . (9)\nFurther, if the above holds, θ̂ZN is given by θ̂ZNT c = 0 and θ̂ ZN T = θ0,T − ξΣ̂ −1 T,T vT .\nMotivated by this result, we introduce the generalized irrepresentability condition (GIC) for deterministic designs.\nGeneralized irrepresentability (deterministic designs). The pair (Σ̂, θ0), Σ̂ ∈ Rp×p, θ0 ∈ Rp satisfy the generalized irrepresentability condition with parameter η > 0 if the following happens. Let v0, T0 be defined as per Lemma 2.2. Then∥∥∥Σ̂T c0 ,T0Σ̂−1T0,T0v0,T0∥∥∥∞ ≤ 1− η . (10) In other words we require the dual feasibility condition (8), which always holds, to hold with a positive slack η."
    }, {
      "heading" : "2.2 Noisy problem",
      "text" : "Consider the noisy linear observation model as described in (2), and let r̂ ≡ (XTW/n). We begin with a standard characterization of sign(θ̂n), the signed support of the Lasso estimator (3).\nLemma 2.4. Let θ̂n = θ̂n(y,X;λ) be defined as per Eq. (3), and let z ∈ {+1, 0,−1}p with supp(z) = T . Further assume T ⊇ S. Then the signed support of the Lasso estimator is given by sign(θ̂n) = z if and only if∥∥∥Σ̂T c,T Σ̂−1T,T zT + 1λ (r̂T c − Σ̂T c,T Σ̂−1T,T r̂T )∥∥∥∞ ≤ 1 , (11)\nzT = sign ( θ0,T − Σ̂−1T,T (λzT − r̂T ) ) . (12)\nTheorem 2.5. Consider the deterministic design model with empirical covariance matrix Σ̂ ≡ (XTX)/n, and assume Σ̂i,i ≤ 1 for i ∈ [p]. Let T0 ⊆ [p], v0 ∈ {+1, 0,−1}p be the set and vector defined in Lemma 2.2. Assume that (i) σmin(Σ̂T0,T0) ≥ Cmin > 0. (ii) The pair (Σ̂, θ0) satisfies the generalized irrepresentability condition with parameter η. Consider the Lasso estimator θ̂n = θ̂n(y,X;λ) defined as per Eq. (3), with λ = (σ/η) √ 2c1 log p/n, for some constant c1 > 1, and suppose that for some c2 > 0:\n|θ0,i| ≥ c2λ+ λ ∣∣[Σ̂−1T0,T0v0,T0 ]i∣∣ for all i ∈ S, (13)∣∣[Σ̂−1T0,T0v0,T0 ]i∣∣ ≥ c2 for all i ∈ T0 \\ S. (14)\nWe further assume, without loss of generality, η ≤ c2 √ Cmin. Then the following holds true:\nP { sign(θ̂n(λ)) = v0 } ≥ 1− 4p1−c1 . (15)\nNote that even under standard irrepresentability, this result improves over [21, Theorem 1.(b)], in that the required lower bound for |θ0,i|, i ∈ S, does not depend on ‖Σ̂−1S,S‖∞.\nRemark 2.6. Condition (i) in Theorem 2.5 requires the submatrix Σ̂T0,T0 to have minimum singular value bounded away form zero. Assuming Σ̂S,S to be non-singular is necessary for identifiability. Requiring the minimum singular value of Σ̂T0,T0 to be bounded away from zero is not much more restrictive since T0 is comparable in size with S, as stated in Lemma 2.1.\nWe next show that the Gauss-Lasso selector correctly recovers the support of θ0.\nTheorem 2.7. Consider the deterministic design model with empirical covariance matrix Σ̂ ≡ (XTX)/n, and assume that Σ̂i,i ≤ 1 for i ∈ [p]. Under the assumptions of Theorem 2.5,\nP ( ‖θ̂GL − θ0‖∞ ≥ µ ) ≤ 4p1−c1 + 2pe−nCminµ 2/2σ2 .\nIn particular, if Ŝ is the model selected by the Gauss-Lasso, then P(Ŝ = S) ≥ 1− 6 p1−c1/4."
    }, {
      "heading" : "3 Random Gaussian designs",
      "text" : "In the previous section, we studied the case of deterministic design models which allowed for a straightforward analysis. Here, we consider the random design model which needs a more involved analysis. Within the random Gaussian design model, the rows Xi are distributed as Xi ∼ N(0,Σ) for some (unknown) covariance matrix Σ 0. In order to study the performance of Gauss-Lasso selector in this case, we first define the population-level estimator. Given Σ ∈ Rp×p, Σ 0, θ0 ∈ Rp and ξ ∈ R+, the population-level estimator θ̂∞(ξ) = θ̂∞(ξ; θ0,Σ) is defined as\nθ̂∞(ξ) ≡ arg min θ∈Rp {1 2 〈(θ − θ0),Σ(θ − θ0)〉+ ξ‖θ‖1 } . (16)\nIn fact, the population-level estimator is obtained by assuming that the response vector Y is noiseless and n =∞, hence replacing the empirical covariance (XTX/n) with the exact covariance Σ in the lasso optimization problem (3). Note that the population-level estimator θ̂∞ is deterministic, albeit X is a random design. We show that under some conditions on the covariance Σ and vector θ0, T ≡ supp(θ̂n) = supp(θ̂∞), i.e., the population-level estimator and the Lasso estimator share the same (signed) support. Further T ⊇ S. Since θ̂∞ (and hence T ) is deterministic, XT is a Gaussian matrix with rows drawn independently from N(0,ΣT,T ). This observation allows for a simple analysis of the Gauss-Lasso selector θ̂GL.\nAn outline of the section is as follows: (1) We begin with noting that the population-level estimator θ̂∞(ξ) has the similar properties to θ̂ZN(ξ) stated in Section 2.1. In particular, there exists a threshold ξ0, such that for all ξ ∈ (0, ξ0), supp(θ̂∞(ξ)) remains the same and contains supp(θ0). Moreover, supp(θ̂∞(ξ)) is not much larger than supp(θ0). (2) We show that under GIC for covariance matrix Σ (and other sufficient conditions), with high probability, the signed support of the Lasso estimator is the same as the signed support of the population-level estimator. (3) Following the previous steps, we show that the Gauss-Lasso selector correctly recovers the signed support of θ0."
    }, {
      "heading" : "3.1 The n =∞ problem",
      "text" : "Comparing Eqs. (5) and (16), the estimators θ̂ZN(ξ) and θ̂∞(ξ) are defined in a very similar manner (the former is defined with respect to Σ̂ and the latter is defined with respect to Σ). It is easy to see that θ̂∞ satisfies the properties stated in Section 2.1 once we replace Σ̂ with Σ."
    }, {
      "heading" : "3.2 The high-dimensional problem",
      "text" : "We now consider the Lasso estimator (3). Recall the notations Σ̂ ≡ (XTX)/n and r̂ ≡ (XTW )/n. Note that Σ̂ ∈ Rp×p, r̂ ∈ Rp are both random quantities in the case of random designs. Theorem 3.1. Consider the Gaussian random design model with covariance matrix Σ 0, and assume that Σi,i ≤ 1 for i ∈ [p]. Let T0 ⊆ [p], v0 ∈ {+1, 0,−1}p be the deterministic set and vector defined in Lemma 2.2 (replacing Σ̂ with Σ), and t0 ≡ |T0|. Assume that (i) σmin(ΣT0,T0) ≥ Cmin > 0. (ii) The pair (Σ, θ0) satisfies the generalized irrepresentability condition with parameter η. Consider the Lasso estimator θ̂n = θ̂n(y,X;λ) defined as per Eq. (3), with λ = (4σ/η) √ c1 log p/n, for some constant c1 > 1, and suppose that for some c2 > 0:\n|θ0,i| ≥ c2λ+ 3 2 λ ∣∣[Σ−1T0,T0v0,T0 ]i∣∣ for all i ∈ S, (17)∣∣[Σ−1T0,T0v0,T0 ]i∣∣ ≥ 2c2 for all i ∈ T0 \\ S. (18)\nWe further assume, without loss of generality, η ≤ c2 √ Cmin. If n ≥ max(M1,M2)t0 log p with M1 ≡ (74c1)/(η2Cmin), and M2 ≡ c1(32/(c2Cmin))2 , then the following holds true:\nP { sign(θ̂n(λ)) = v0 } ≥ 1− pe− n10 − 6e− t0 2 − 8p1−c1 . (19)\nNote that even under standard irrepresentability, this result improves over [21, Theorem 3.(ii)], in that the required lower bound for |θ0,i|, i ∈ S, does not depend on ‖Σ−1/2S,S ‖∞. Remark 3.2. Condition (i) follows readily from the restricted eigenvalue constraint, i.e., κ∞(t0, 0) > 0. This is a reasonable assumption since T0 is not much larger than S0, as stated in Lemma 2.1 (replacing Σ̂ with Σ). Namely, s0 ≤ t0 ≤ (1 + 4‖Σ‖2/κ(s0, 1))s0.\nBelow, we show that the Gauss-Lasso selector correctly recovers the signed support of θ0. Theorem 3.3. Consider the random Gaussian design model with covariance matrix Σ 0, and assume that Σi,i ≤ 1 for i ∈ [p]. Under the assumptions of Theorem 3.1, and for n ≥ max(M1,M2)t0 log p, we have\nP ( ‖θ̂GL − θ0‖∞ ≥ µ ) ≤ pe− n10 + 6e− s0 2 + 8p1−c1 + 2pe−nCminµ 2/2σ2 .\nMoreover, letting Ŝ be the model returned by the Gauss-Lasso selector, we have\nP(Ŝ = S) ≥ 1− p e− n10 − 6 e− s0 2 − 10 p1−c1 .\nRemark 3.4. [Detection level] Let θmin ≡ mini∈S |θ0,i| be the minimum magnitude of the nonzero entries of vector θ0. By Theorem 3.3, Gauss-Lasso selector correctly recovers supp(θ0), with probability greater than 1− p e− n10 − 6 e− s0 2 − 10 p1−c1 , if n ≥ max(M1,M2)t0 log p, and\nθmin ≥ Cσ √ log p\nn\n( 1 + ‖Σ−1T0,T0‖∞ ) , (20)\nfor some constant C. Note that Eq. (20) follows from Eqs. (17) and (18)."
    } ],
    "references" : [ {
      "title" : "Bolasso: model consistent lasso estimation through the bootstrap",
      "author" : [ "F.R. Bach" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "P.J. Bickel", "Y. Ritov", "A.B. Tsybakov" ],
      "venue" : "Amer. J. of Mathematics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Statistics for high-dimensional data",
      "author" : [ "P. Bühlmann", "S. van de Geer" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Near-ideal model selection by `1 minimization",
      "author" : [ "E. Candès", "Y. Plan" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "E. Candes", "J.K. Romberg", "T. Tao" ],
      "venue" : "IEEE Trans. on Inform. Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "The Dantzig selector: statistical estimation when p is much larger than n",
      "author" : [ "E. Candés", "T. Tao" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Decoding by linear programming",
      "author" : [ "E.J. Candés", "T. Tao" ],
      "venue" : "IEEE Trans. on Inform. Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Examples of basis pursuit",
      "author" : [ "S. Chen", "D. Donoho" ],
      "venue" : "In Proceedings of Wavelet Applications in Signal and Image Processing",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1995
    }, {
      "title" : "Compressed sensing",
      "author" : [ "D.L. Donoho" ],
      "venue" : "IEEE Trans. on Inform. Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory",
      "author" : [ "A. Javanmard", "A. Montanari" ],
      "venue" : "arXiv preprint arXiv:1301.4240,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Model selection for high-dimensional regression under the generalized irrepresentability condition",
      "author" : [ "A. Javanmard", "A. Montanari" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Asymptotics for lasso-type estimators",
      "author" : [ "K. Knight", "W. Fu" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2000
    }, {
      "title" : "Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators",
      "author" : [ "K. Lounici" ],
      "venue" : "Electronic Journal of statistics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "High-dimensional graphs and variable selection with the lasso",
      "author" : [ "N. Meinshausen", "P. Bühlmann" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer",
      "author" : [ "J. Peng", "J. Zhu", "A. Bergamaschi", "W. Han", "D.-Y. Noh", "J.R. Pollack", "P. Wang" ],
      "venue" : "The Annals of Applied Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "A simple and efficient algorithm for gene selection using sparse logistic regression",
      "author" : [ "S.K. Shevade", "S.S. Keerthi" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Regression shrinkage and selection with the Lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "J. Royal. Statist. Soc B,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1996
    }, {
      "title" : "The lasso problem and uniqueness",
      "author" : [ "R.J. Tibshirani" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "On the conditions used to prove oracle results for the lasso",
      "author" : [ "S. van de Geer", "P. Bühlmann" ],
      "venue" : "Electron. J. Statist.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "The adaptive and the thresholded Lasso for potentially misspecified models (and a lower bound for the Lasso)",
      "author" : [ "S. van de Geer", "P. Bühlmann", "S. Zhou" ],
      "venue" : "Electron. J. Stat.,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming",
      "author" : [ "M. Wainwright" ],
      "venue" : "IEEE Trans. on Inform. Theory,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Rate minimaxity of the lasso and dantzig selector for the lq loss in lr balls",
      "author" : [ "F. Ye", "C.-H. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "On model selection consistency of Lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2006
    }, {
      "title" : "Thresholded Lasso for high dimensional variable selection and statistical estimation",
      "author" : [ "S. Zhou" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "The adaptive lasso and its oracle properties",
      "author" : [ "H. Zou" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "This problem has motivated a large body of research, because of its relevance to several modern data analysis tasks, ranging from signal processing [9, 5] to genomics [15, 16].",
      "startOffset" : 148,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "This problem has motivated a large body of research, because of its relevance to several modern data analysis tasks, ranging from signal processing [9, 5] to genomics [15, 16].",
      "startOffset" : 148,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "This problem has motivated a large body of research, because of its relevance to several modern data analysis tasks, ranging from signal processing [9, 5] to genomics [15, 16].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "This problem has motivated a large body of research, because of its relevance to several modern data analysis tasks, ranging from signal processing [9, 5] to genomics [15, 16].",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 16,
      "context" : "A crucial step forward has been the development of model-selection techniques based on convex optimization formulations [17, 8, 6].",
      "startOffset" : 120,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "A crucial step forward has been the development of model-selection techniques based on convex optimization formulations [17, 8, 6].",
      "startOffset" : 120,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "A crucial step forward has been the development of model-selection techniques based on convex optimization formulations [17, 8, 6].",
      "startOffset" : 120,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "when the entries of X are drawn form a continuous probability distribution), the Lasso solution is unique [18].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "(A closely related method is the so-called Dantzig selector [6]: it would be interesting to explore whether our results can be generalized to that approach.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "It was understood early on that, even in the large-sample, low-dimensional limit n → ∞ at p constant, supp(θ̂) 6= S unless the columns of X with index in S are roughly orthogonal to the ones with index outside S [12].",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 22,
      "context" : "In an early breakthrough, Zhao and Yu [23] proved that, if this condition holds with η uniformly bounded away from 0, it guarantees correct model selection also in the high-dimensional regime p n.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "Meinshausen and Bülmann [14] independently established the same result for random Gaussian designs, with applications to learning Gaussian graphical models.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : "Wainwright [21] strengthened considerably these results by allowing for general scalings of s0, p, n and proving that much smaller non-zero coefficients can be detected.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 20,
      "context" : "standard Gaussian entries), the analysis of [21] provides tight bounds on the minimum sample size for correct model selection.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 22,
      "context" : "While, thanks to these recent works [23, 14, 21], we understand reasonably well model selection via the Lasso, it is fundamentally unknown what model-selection performances can be achieved with general computationally practical methods.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "While, thanks to these recent works [23, 14, 21], we understand reasonably well model selection via the Lasso, it is fundamentally unknown what model-selection performances can be achieved with general computationally practical methods.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "While, thanks to these recent works [23, 14, 21], we understand reasonably well model selection via the Lasso, it is fundamentally unknown what model-selection performances can be achieved with general computationally practical methods.",
      "startOffset" : 36,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "For instance, in [10], the authors prove a general upper bound on the minimax power of tests for hypothesesH0,i = {θ0,i = 0}.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Specializing this bound to the case of standard Gaussian designs, the analysis of [10] shows formally that no test can detect θ0,i 6= 0, with a fixed degree of confidence, unless |θ0,i| ≥ cσ/ √ n.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : ", Ŝ = S) under conditions comparable to the ones assumed in [14, 23, 21], while replacing irrepresentability by the weaker generalized irrepresentability condition.",
      "startOffset" : 60,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : ", Ŝ = S) under conditions comparable to the ones assumed in [14, 23, 21], while replacing irrepresentability by the weaker generalized irrepresentability condition.",
      "startOffset" : 60,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : ", Ŝ = S) under conditions comparable to the ones assumed in [14, 23, 21], while replacing irrepresentability by the weaker generalized irrepresentability condition.",
      "startOffset" : 60,
      "endOffset" : 72
    }, {
      "referenceID" : 13,
      "context" : "In other words, earlier work [14, 23, 21] required generalized irrepresentability plus sign-consistency in zero noise, and established sign consistency in non-zero noise.",
      "startOffset" : 29,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "In other words, earlier work [14, 23, 21] required generalized irrepresentability plus sign-consistency in zero noise, and established sign consistency in non-zero noise.",
      "startOffset" : 29,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "In other words, earlier work [14, 23, 21] required generalized irrepresentability plus sign-consistency in zero noise, and established sign consistency in non-zero noise.",
      "startOffset" : 29,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "As a side remark, even when simple irrepresentability holds, our results strengthen somewhat the estimates of [21] (see below for details).",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 10,
      "context" : "We refer the reader to the long version of the paper [11] for the proofs of our main results and the technical steps.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "A detailed discussion of this calculation can be found in [11].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 20,
      "context" : "It follows from [21] that this remains true as long as a ≤ (1−η)/s0 for some η > 0 bounded away from 0.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "The restricted isometry property [7, 6] (or the related restricted eigenvalue [2] or compatibility conditions [19]) have been used to establish guarantees on the estimation and model selection errors of the Lasso or similar approaches.",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 5,
      "context" : "The restricted isometry property [7, 6] (or the related restricted eigenvalue [2] or compatibility conditions [19]) have been used to establish guarantees on the estimation and model selection errors of the Lasso or similar approaches.",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "The restricted isometry property [7, 6] (or the related restricted eigenvalue [2] or compatibility conditions [19]) have been used to establish guarantees on the estimation and model selection errors of the Lasso or similar approaches.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "The restricted isometry property [7, 6] (or the related restricted eigenvalue [2] or compatibility conditions [19]) have been used to establish guarantees on the estimation and model selection errors of the Lasso or similar approaches.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "In particular, Bickel, Ritov and Tsybakov [2] show that, under such conditions, with high probability,",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 23,
      "context" : "In particular, Zhou [24] studies a multi-step thresholding procedure whose first steps coincide with the Gauss-Lasso.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : "Lounici [13] proves correct model selection under the assumption maxi6=j |Σ̂ij | = O(1/s0).",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 18,
      "context" : "This assumption is however stronger than irrepresentability [19].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "Candés and Plan [4] also assume mutual incoherence, albeit with a much weaker requirement, namely maxi6=j |Σ̂ij | = O(1/(log p)).",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 21,
      "context" : "The authors in [22] consider the variable selection problem, and under the same assumptions on the non-zero coefficients as in the present paper, guarantee support recovery under a cone condition.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 19,
      "context" : "The work [20] studies the adaptive and the thresholded Lasso estimators and proves correct model selection assuming the non-zero coefficients are of order s0 √ (log p)/n.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 24,
      "context" : "For instance [25] develops the adaptive Lasso, using a data-dependent weighted `1 regularization, and [1] proposes the Bolasso, a resampling-based techniques.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "For instance [25] develops the adaptive Lasso, using a data-dependent weighted `1 regularization, and [1] proposes the Bolasso, a resampling-based techniques.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "Following [2], we introduce a restricted eigenvalue constant for the empirical covariance matrix Σ̂:",
      "startOffset" : 10,
      "endOffset" : 13
    } ],
    "year" : 2013,
    "abstractText" : "In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefficients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefficients through the Lasso (`1-regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantified through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that first solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set.",
    "creator" : null
  }
}