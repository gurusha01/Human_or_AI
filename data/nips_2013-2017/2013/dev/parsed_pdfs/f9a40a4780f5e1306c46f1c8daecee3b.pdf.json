{
  "name" : "f9a40a4780f5e1306c46f1c8daecee3b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian entropy estimation for binary spike train data using parametric prior knowledge",
    "authors" : [ "Evan Archer", "Il Memming Park", "Jonathan W. Pillow" ],
    "emails" : [ "{memming@austin.,", "earcher@,", "pillow@mail.}" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Introduction\nInformation theoretic quantities are popular tools in neuroscience, where they are used to study neural codes whose representation or function is unknown. Neuronal signals take the form of fast (∼ 1 ms) spikes which are frequently modeled as discrete, binary events. While the spiking response of even a single neuron has been the focus of intense research, modern experimental techniques make it possible to study the simultaneous activity of hundreds of neurons. At a given time, the response of a population of n neurons may be represented by a binary vector of length n, where each entry represents the presence (1) or absence (0) of a spike. We refer to such a vector as a spike word. For n much greater than 30, the space of 2n spike words becomes so large that effective modeling and analysis of neural data, with their high dimensionality and relatively low sample size, presents a significant computational and theoretical challenge.\nWe study the problem of estimating the discrete entropy of spike word distributions. This is a difficult problem when the sample size is much less than 2n, the number of spike words. Entropy estimation in general is a well-studied problem with a literature spanning statistics, physics, neuro-\nscience, ecology, and engineering, among others [1–7]. We introduce a novel Bayesian estimator which, by incorporating simple a priori information about spike trains via a carefully-chosen prior, can estimate entropy with remarkable accuracy from few samples. Moreover, we exploit the structure of spike trains to compute efficiently on the full space of 2n spike words.\nWe begin by briefly reviewing entropy estimation in general. In Section 2 we discuss the statistics of spike trains and emphasize a statistic, called the synchrony distribution, which we employ in our model. In Section 3 we introduce two novel estimators, the Dirichlet–Bernoulli (DBer) and Dirichlet–Synchrony (DSyn) entropy estimators, and discuss their properties and computation. We compare ĤDBer and ĤDSyn to other entropy estimation techniques in simulation and on neural data, and show that ĤDBer drastically outperforms other popular techniques when applied to real neural data. Finally, we apply our estimators to study synergy across time of a single neuron."
    }, {
      "heading" : "1 Entropy Estimation",
      "text" : "Let x := {xk}Nk=1 be spike words drawn iid from an unknown word distribution π := {πi} A i=1. There are A = 2n unique words for a population of n neurons, which we index by {1, 2, . . . ,A}. Each sampled word xk is a binary vector of length n, where xki records the presence or absence of a spike from the ith neuron. We wish to estimate the entropy of π,\nH(π) = − A∑ k=1 πk log πk, (1)\nwhere πk > 0 denotes the probability of observing the kth word. A naive method for estimating H is to first estimate π using the count of observed words nk =∑N i=1 1{xi=k} for each word k. This yields the empirical distribution π̂, where π̂k = nk/N . Evaluating eq. 1 on this estimate yields the “plugin” estimator,\nĤplugin = − A∑ i=1 π̂i log π̂i, (2)\nwhich is also the maximum-likelihood estimator under the multinomial likelihood. Although consistent and straightforward to compute, Ĥplugin is in general severely biased when N A. Indeed, all entropy estimators are biased when N A [8]. As a result, many techniques for biascorrection have been proposed in the literature [6, 9–18]. Here, we extend the Bayesian approach of [19], focusing in particular on the problem of entropy estimation for simultaneously-recorded neurons.\nIn a Bayesian paradigm, rather than attempting to directly compute and remove the bias for a given estimator, we instead choose a prior distribution over the space of discrete distributions. Nemenman\net al. showed Dirichlet to be priors highly informative about the entropy, and thus a poor prior for Bayesian entropy estimation [19]. To rectify this problem, they introduced the Nemenman–Shafee– Bialek (NSB) estimator, which uses a mixture of Dirichlet distributions to obtain an approximately flat prior overH . As a prior on π, however, the NSB prior is agnostic about application: all symbols have the same marginal probability under the prior, an assumption that may not hold when the symbols correspond to binary spike words."
    }, {
      "heading" : "2 Spike Statistics and the Synchrony Distribution",
      "text" : "We discretize neural signals by binning multi-neuron spike trains in time, as illustrated in Fig. 1. At a time t, then, the spike response of a population of n neurons is a binary vector ~w = (b1, b2, . . . , bn), where bi ∈ {0, 1} corresponds to the event that the ith neuron spikes within the time window (t, t+ ∆t). We let ~wk be that word such that k = ∑n−1 i=0 bi2\ni. There are a total of A = 2n possible words.\nFor a sufficiently small bin size ∆t, spike words are likely to be sparse, and so our strategy will be to choose priors that place high prior probability on sparse words. To quantify sparsity we use the synchrony distribution: the distribution of population spike counts across all words. In Fig. 2 we compare the empirical synchrony distribution for a population of 8 simultaneously-recorded retinal ganglion cells (RGCs) with the prior synchrony distribution under the NSB model. For real data, the synchrony distribution is asymmetric and sparse, concentrating around words with few simultaneous spikes. No more than 4 synchronous spikes are observed in the data. In contrast, under the NSB model all words are equally likely, and the prior synchrony distribution is symmetric and centered around 4.\nThese deviations in the synchrony distribution are noteworthy: beyond quantifying sparseness, the synchrony distribution provides a surprisingly rich characterization of a neural population. Despite its simplicity, the synchrony distribution carries information about the higher-order correlation structure of a population [20,21]. It uniquely specifies distributions π for which the probability of a word wk depends only on its spike count [k] = [~wk] := ∑ i bi. Equivalently: all words with spike count k, Ek = {w : [w] = k}, have identical probability βk of occurring. For such a π, the synchrony\ndistribution µ is given by,\nµk = ∑ wi∈Ek πi = ( n k ) βk. (3)\nDifferent neural models correspond to different synchrony distributions. Consider an independentlyBernoulli spiking model. Under this model, the number of spikes in a word w is distributed binomially, [~w] ∼ Bin(p, n), where p is a uniform spike probability across neurons. The probability of a word wk is given by,\nP (~wk|p) = β[k] = p[k](1− p)n−[k], (4)\nwhile the probability of observing a word with i spikes is, P (Ei|p) = ( n\ni\n) βi. (5)"
    }, {
      "heading" : "3 Entropy Estimation with Parametric Prior Knowledge",
      "text" : "Although a synchrony distribution may capture our prior knowledge about the structure of spike patterns, our goal is not to estimate the synchrony distribution itself. Rather, we use it to inform a prior on the space of discrete distributions, the (2n−1)-dimensional simplex. Our strategy is to use a synchrony distributionG as the base measure of a Dirichlet distribution. We construct a hierarchical model where π is a mixture of Dir(αG), and counts n of spike train observations are multinomial given π (See Fig. 3(A). Exploiting the conjugacy of Dirichlet and multinomial, and the convenient symmetries of both the Dirichlet distribution and G, we obtain a computationally efficient Bayes least squares estimator for entropy. Finally, we discuss using empirical estimates of the synchrony distribution µ as a base measure."
    }, {
      "heading" : "3.1 Dirichlet–Bernoulli entropy estimator",
      "text" : "We model spike word counts n as drawn iid multinomial given the spike word distribution π. We place a mixture-of-Dirichlets prior on π, which in general takes the form,\nn ∼ Mult(π) (6) π ∼ Dir(α1, α2, . . . , αA︸ ︷︷ ︸\n2n\n), (7)\n~α := (α1, α2, . . . , αA) ∼ P (~α), (8)\nwhere αi > 0 are concentration parameters, and P (~α) is a prior distribution of our choosing. Due to the conjugacy of Dirichlet and multinomial, the posterior distribution given observations and ~α is π|n, ~α ∼ Dir(α1 + n1, . . . , αA + nA), where ni is the number of observations for the i-th spiking pattern. The posterior expected entropy given ~α is given by [22],\nE[H(π)|~α] = ψ0(κ+ 1)− A∑ i=1 αi κ ψ0(αi + 1) (9)\nwhere ψ0 is the digamma function, and κ = ∑A i=1 αi.\nFor large A, ~α is too large to select arbitrarily, and so in practice we center the Dirichlet around a simple, parametric base measure G [23]. We rewrite the vector of concentration parameters as ~α ≡ αG, whereG = Bernoulli(p) is a Bernoulli distribution with spike rate p and α > 0 is a scalar. The general prior of eq. 7 then takes the form,\nπ ∼ Dir(αG) ≡ Dir(αg1, αg2 . . . , αgA), (10) where each gk is the probability of the kth word under the base measure, satisfying ∑ gk = 1.\nWe illustrate the dependency structure of this model schematically in Fig. 3. Intuitively, the base measure incorporates the structure of G into the prior by shifting the Dirichlet’s mean. With a base measure G the prior mean satisfies E[π|p] = G|p. Under the NSB model, G is the uniform distribution; thus, when p = 0.5 the Binomial G corresponds exactly to the NSB model. Since\nin practice choosing a base measure is equivalent to selecting distinct values of the concentration parameter αi, the posterior mean of entropy under this model has the same form as eq. 9, simply replacing αk = αgk. Given hyper-prior distributions P (α) and P (p), we obtain the Bayes least squares estimate, the posterior mean of entropy under our model,\nĤDBer = E[H|x] = ∫∫ E [H|α, p]P (α, p|x) dα dp. (11)\nWe refer to eq. 11 as the Dirichlet–Bernoulli (DBer) entropy estimator, ĤDBer. Thanks to the closedform expression for the conditional mean eq. 9 and the convenient symmetries of the Bernoulli distribution, the estimator is fast to compute using a 2D numerical integral over the hyperparameters α and p."
    }, {
      "heading" : "3.1.1 Hyper-priors on α and p",
      "text" : "Previous work on Bayesian entropy estimation has focused on Dirichlet priors with scalar, constant concentration parameters αi = α. Nemenman et al. [19] noted that these fixed-α priors yield poor estimators for entropy, because p(H|α) is highly concentrated around its mean. To address this problem, [19] proposed a Dirichlet mixture prior on π,\nP (π) = ∫ PDir(π|α)P (α)dα, (12)\nwhere the hyper-prior, P (α) ∝ ddαE[H(π)|α] assures an approximately flat prior distribution over H . We adopt the same strategy here, choosing the prior,\nP (α) ∝ d dα E[H(π)|α, p] = ψ1(α+ 1)− n∑ i=0 ( n i ) β2i ψ1(αβi + 1). (13)\nEntropy estimates are less sensitive to the choice of prior on p. Although we experimented with several priors on p, in all examples we found that the evidence for p was highly concentrated around p̂ = 1Nn ∑ ij xij , the maximum (Bernoulli) likelihood estimate for p. In practice, we found that an empirical Bayes procedure, fitting p̂ from data first and then using the fixed p̂ to perform the integral eq. 11, performed indistinguishably from a P (p) uniform on [0, 1]."
    }, {
      "heading" : "3.1.2 Computation",
      "text" : "For large n, the 2n distinct values of αi render the sum of eq. 9 potentially intractable to compute. We sidestep this exponential scaling of terms by exploiting the redundancy of Bernoulli and binomial distributions. Doing so, we are able to compute eq. 9 without explicitly representing the 2N values of αi.\nUnder the Bernoulli model, each element gk of the base measure takes the value β[k] (eq. 4). Further, there are ( n i ) words for which the value of αi is identical, so that A = ∑n i=0 α ( n i ) βi = α. Applied to eq. 9, we have,\nE[H(π)|α, p] = ψ0(α+ 1)− n∑ i=0 ( n i ) βiψ0(αβi + 1).\nFor the posterior, the sum takes the same form, except that A = n+ α, and the mean is given by,\nE[H(π)|α, p,x] = ψ0(n+ α+ 1)− A∑ i=1 ni + αβ[i] n+ α ψ0(ni + αβ[i] + 1) (14)\n= ψ0(n+ α+ 1)− ∑ i∈I ni + αβ[i] n+ α ψ0(ni + αβ[i] + 1)\n− α n∑ i=0 (( n i ) − ñi ) βi n+ α ψ0(αβi + 1),\nwhere I = {k : nk > 0}, the set of observed characters, and ñi is the count of observed words with i spikes (i.e., observations of the equivalence class Ei). Note that eq. 14 is much more computationally tractable than the mathematically equivalent form given immediately above it. Thus, careful bookkeeping allows us to efficiently evaluate eq. 9 and, in turn, eq. 11.1"
    }, {
      "heading" : "3.2 Empirical Synchrony Distribution as a Base Measure",
      "text" : "While the Bernoulli base measure captures the sparsity structure of multi-neuron recordings, it also imposes unrealistic independence assumptions. In general, the synchrony distribution can capture correlation structure that cannot be represented by a Bernoulli model. For example, in Fig. 2B, a maximum likelihood Bernoulli fit fails to capture the sparsity structure of a simulated Ising model.\nWe might address this by choosing a more flexible parametric base measure. However, since the dimensionality of µ scales only linearly with the number of neurons, the empirical synchrony distribution (ESD),\nµ̂i = 1\nN N∑ j=1 1{[xj ]=i}, (15)\nconverges quickly even when the sample size is inadequate for estimating the full π.\nThis suggests an empirical Bayes procedure where we use the ESD as a base measure (take G = µ̂) for entropy estimation. Computation proceeds exactly as in Section 3.1.2 with the Bernoulli base measure replaced by the ESD by setting gk = µk and βi = µi/ ( m i ) . The resulting Dirichlet– Synchrony (DSyn) estimator may incorporate more varied sparsity and correlation structures into its prior than ĤDBer (see Fig. 4), although it depends on an estimate of the synchrony distribution."
    }, {
      "heading" : "4 Simulations and Comparisons",
      "text" : "We compared ĤDBer and ĤDSyn to the Nemenman–Shafee–Bialek (NSB) [19] and Best Upper Bound (BUB) entropy estimators [8] for several simulated and real neural datasets. For ĤDSyn, we regular-\n1For large n, the binomial coefficient of eq. 14 may be difficult to compute. By writing it in terms of the Bernoulli probability eq. 5, it may be computed using the Normal approximation to the Binomial.\nand 10 spikes\n(\nµi = e −2i + 110e −4(i−2n/3)2\n)\n. (B) Data generated from a power-law synchrony distribution (µi ∝ i−3).\nized the estimated ESD by adding a pseudo-count of 1K , where K is the number of unique words observed in the sample. In Fig. 4 we simulated data from two distinct synchrony distribution models. As is expected, among all estimators, ĤDSyn converges the fastest with increasing sample size N . The ĤDBer estimator converges more slowly, as the Bernoulli base measure is not capable of capturing the correlation structure of the simulated synchrony distributions. In Fig. 5, we show convergence performance on increasing subsamples of 27 simultaneously-recorded retinal ganglion cells. Again, ĤDBer and ĤDSyn show excellent performance. Although the true word distribution is not described by a synchrony distribution, the ESD proves an excellent regularizer for the space of distributions, even for very small sample sizes."
    }, {
      "heading" : "5 Application: Quantification of Temporal Dependence",
      "text" : "We can gain insight into the coding of a single neural time-series by quantifying the amount of information a single time bin contains about another. The correlation function (Fig. 6A) is the statistic most widely used for this purpose. However, correlation cannot capture higher-order dependencies. In neuroscience, mutual information is used to quantify higher-order temporal structure [24]. A re-\nlated quantity, the delayed mutual information (dMI) provides an indication of instantaneous dependence: dMI(s) = I(Xt;Xt+s), whereXt is a binned spike train, and I(X;Y ) = H(X)−H(X|Y ) denotes the mutual information. However, this quantity ignores any temporal dependences in the intervening times: Xt+1, . . . , Xt+s−1. An alternative approach allows us to consider such dependences: the “block mutual information” ν(s) = I(Xt;Xt+1:t+s) − I(Xt;Xt+1:t+s−1) (Fig. 6B,C,D)\nThe relationship between ν(s) and dMI(s) provides insight about the information contained in the recent history of the signal. If each time bin is conditionally independent given Xt, then ν(s) = dMI(s). In contrast, if ν(s) < dMI(s), instantaneous dependence is partially explained by history. Finally, ν(s) > dMI(s) implies that the joint distribution of Xt, Xt+1, . . . , Xt+s contains more information about Xt than the joint distribution of Xt and Xt+s alone. We use the ĤDBer entropy estimator to compute mutual information (by computing H(X) and H(X|Y )) accurately for ∼ 15 bins of history. Surprisingly, individual retinal ganglion cells code synergistically in time (Fig. 6D)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We introduced two novel Bayesian entropy estimators, ĤDBer and ĤDSyn. These estimators use a hierarchical mixture-of-Dirichlets prior with a base measure designed to integrate a priori knowledge about spike trains into the model. By choosing base measures with convenient symmetries, we simultaneously sidestepped potentially intractable computations in the high-dimensional space of spike words. It remains to be seen whether these symmetries, as exemplified in the structure of the synchrony distribution, are applicable across a wide range of neural data. Finally, however, we showed several examples in which these estimators, especially ĤDSyn, perform exceptionally well in application to neural data. A MATLAB implementation of the estimators will be made available at https://github.com/pillowlab/CDMentropy."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank E. J. Chichilnisky, A. M. Litke, A. Sher and J. Shlens for retinal data. This work was supported by a Sloan Research Fellowship, McKnight Scholar’s Award, and NSF CAREER Award IIS-1150186 (JP)."
    } ],
    "references" : [ {
      "title" : "Causality detection based on informationtheoretic approaches in time series analysis",
      "author" : [ "K.H. Schindler", "M. Palus", "M. Vejmelka", "J. Bhattacharya" ],
      "venue" : "Physics Reports, 441:1–46",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On measures of dependence",
      "author" : [ "A. Rényi" ],
      "venue" : "Acta Mathematica Hungarica, 10(3-4):441–451,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1959
    }, {
      "title" : "Approximating discrete probability distributions with dependence trees",
      "author" : [ "C. Chow", "C. Liu" ],
      "venue" : "Information Theory, IEEE Transactions on, 14(3):462–467",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Nonparametric estimation of Shannon’s index of diversity when there are unseen species in sample",
      "author" : [ "A. Chao", "T. Shen" ],
      "venue" : "Environmental and Ecological Statistics, 10(4):429–443",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Estimating the information content of symbol sequences and efficient codes",
      "author" : [ "P. Grassberger" ],
      "venue" : "Information Theory, IEEE Transactions on, 35(3):669–675",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Calculation of entropy from data of motion",
      "author" : [ "S. Ma" ],
      "venue" : "Journal of Statistical Physics, 26(2):221–240",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Correcting for the sampling bias problem in spike train information measures",
      "author" : [ "S. Panzeri", "R. Senatore", "M.A. Montemurro", "R.S. Petersen" ],
      "venue" : "J Neurophysiol,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Estimation of entropy and mutual information",
      "author" : [ "L. Paninski" ],
      "venue" : "Neural Computation, 15:1191–1253",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "R",
      "author" : [ "W. Bialek", "F. Rieke" ],
      "venue" : "R. de Ruyter van Steveninck, R., and D. Warland. Reading a neural code. Science, 252:1854–1857",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "S",
      "author" : [ "R. Strong" ],
      "venue" : "Koberle, de Ruyter van Steveninck R., and W. Bialek. Entropy and information in neural spike trains. Physical Review Letters, 80:197–202",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Estimation of entropy and mutual information",
      "author" : [ "L. Paninski" ],
      "venue" : "Neural Computation, 15:1191–1253",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Dynamic analyses of information encoding in neural ensembles",
      "author" : [ "R. Barbieri", "L. Frank", "D. Nguyen", "M. Quirk", "V. Solo", "M. Wilson", "E. Brown" ],
      "venue" : "Neural Computation, 16:277–307",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Estimating entropy rates with Bayesian confidence intervals",
      "author" : [ "M. Kennel", "J. Shlens", "H. Abarbanel", "E. Chichilnisky" ],
      "venue" : "Neural Computation, 17:1531–1576",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Approaches to information-theoretic analysis of neural activity",
      "author" : [ "J. Victor" ],
      "venue" : "Biological theory, 1(3):302– 316",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Estimating information rates with confidence intervals in neural spike trains",
      "author" : [ "J. Shlens", "M.B. Kennel", "H.D.I. Abarbanel", "E.J. Chichilnisky" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Coverage-adjusted entropy estimation",
      "author" : [ "V.Q. Vu", "B. Yu", "R.E. Kass" ],
      "venue" : "Statistics in medicine, 26(21):4039–4060",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Information in the nonstationary case",
      "author" : [ "V.Q. Vu", "B. Yu", "R.E. Kass" ],
      "venue" : "Neural Computation, 21(3):688– 703",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Bayesian estimation of discrete entropy with mixtures of stickbreaking priors",
      "author" : [ "E. Archer", "I.M. Park", "J. Pillow" ],
      "venue" : "P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2024–2032. MIT Press, Cambridge, MA",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Entropy and inference",
      "author" : [ "I. Nemenman", "F. Shafee", "W. Bialek" ],
      "venue" : "revisited. In Advances in Neural Information Processing Systems 14, pages 471–478. MIT Press, Cambridge, MA",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Population rate dynamics and multineuron firing patterns in sensory cortex",
      "author" : [ "M. Okun", "P. Yger", "S.L. Marguet", "F. Gerard-Mercier", "A. Benucci", "S. Katzner", "L. Busse", "M. Carandini", "K.D. Harris" ],
      "venue" : "The Journal of Neuroscience, 32(48):17108–17119",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The simplest maximum entropy model for collective behavior in a neural network",
      "author" : [ "G. Tkačik", "O. Marre", "T. Mora", "D. Amodei", "M.J. Berry II", "W. Bialek" ],
      "venue" : "Journal of Statistical Mechanics: Theory and Experiment, 2013(03):P03011",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Estimating functions of probability distributions from a finite set of samples",
      "author" : [ "D. Wolpert", "D. Wolf" ],
      "venue" : "Physical Review E, 52(6):6841–6854",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Universal models for binary spike patterns using centered Dirichlet processes",
      "author" : [ "I.M. Park", "E. Archer", "K. Latimer", "J.W. Pillow" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On decoding the responses of a population of neurons from short time windows",
      "author" : [ "A. Panzeri", "S. Treves", "S. Schultz", "E. Rolls" ],
      "venue" : "Neural Computation, 11:1553–1577",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Indeed, all entropy estimators are biased when N A [8].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "Here, we extend the Bayesian approach of [19], focusing in particular on the problem of entropy estimation for simultaneously-recorded neurons.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 18,
      "context" : "showed Dirichlet to be priors highly informative about the entropy, and thus a poor prior for Bayesian entropy estimation [19].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 19,
      "context" : "Despite its simplicity, the synchrony distribution carries information about the higher-order correlation structure of a population [20,21].",
      "startOffset" : 132,
      "endOffset" : 139
    }, {
      "referenceID" : 20,
      "context" : "Despite its simplicity, the synchrony distribution carries information about the higher-order correlation structure of a population [20,21].",
      "startOffset" : 132,
      "endOffset" : 139
    }, {
      "referenceID" : 21,
      "context" : "The posterior expected entropy given ~ α is given by [22],",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "For large A, ~ α is too large to select arbitrarily, and so in practice we center the Dirichlet around a simple, parametric base measure G [23].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 18,
      "context" : "[19] noted that these fixed-α priors yield poor estimators for entropy, because p(H|α) is highly concentrated around its mean.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "To address this problem, [19] proposed a Dirichlet mixture prior on π,",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "We compared ĤDBer and ĤDSyn to the Nemenman–Shafee–Bialek (NSB) [19] and Best Upper Bound (BUB) entropy estimators [8] for several simulated and real neural datasets.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "We compared ĤDBer and ĤDSyn to the Nemenman–Shafee–Bialek (NSB) [19] and Best Upper Bound (BUB) entropy estimators [8] for several simulated and real neural datasets.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "In neuroscience, mutual information is used to quantify higher-order temporal structure [24].",
      "startOffset" : 88,
      "endOffset" : 92
    } ],
    "year" : 2013,
    "abstractText" : "Shannon’s entropy is a basic quantity in information theory, and a fundamental building block for the analysis of neural codes. Estimating the entropy of a discrete distribution from samples is an important and difficult problem that has received considerable attention in statistics and theoretical neuroscience. However, neural responses have characteristic statistical structure that generic entropy estimators fail to exploit. For example, existing Bayesian entropy estimators make the naive assumption that all spike words are equally likely a priori, which makes for an inefficient allocation of prior probability mass in cases where spikes are sparse. Here we develop Bayesian estimators for the entropy of binary spike trains using priors designed to flexibly exploit the statistical structure of simultaneouslyrecorded spike responses. We define two prior distributions over spike words using mixtures of Dirichlet distributions centered on simple parametric models. The parametric model captures high-level statistical features of the data, such as the average spike count in a spike word, which allows the posterior over entropy to concentrate more rapidly than with standard estimators (e.g., in cases where the probability of spiking differs strongly from 0.5). Conversely, the Dirichlet distributions assign prior mass to distributions far from the parametric model, ensuring consistent estimates for arbitrary distributions. We devise a compact representation of the data and prior that allow for computationally efficient implementations of Bayesian least squares and empirical Bayes entropy estimators with large numbers of neurons. We apply these estimators to simulated and real neural data and show that they substantially outperform traditional methods.",
    "creator" : null
  }
}