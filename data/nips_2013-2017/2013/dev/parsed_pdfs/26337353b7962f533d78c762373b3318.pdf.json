{
  "name" : "26337353b7962f533d78c762373b3318.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Multi-level Sparse Representations",
    "authors" : [ "Ferran Diego Fred A. Hamprecht" ],
    "emails" : [ "ferran.diego@iwr.uni-heidelberg.de", "fred.hamprecht@iwr.uni-heidelberg.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "This work was stimulated by a concrete problem, namely the decomposition of state-of-the-art 2D+ time calcium imaging sequences as shown in Fig. 1 into neurons, and assemblies of neurons [20]. Calcium imaging is an increasingly popular tool for unraveling the network structure of local circuits of the brain [11, 6, 7]. Leveraging sparsity constraints seems natural, given that the neural activations are sparse in both space and time. The experimentally achievable optical slice thickness still results in spatial overlap of cells, meaning that each pixel can show intensity from more than one neuron. In addition, it is anticipated that one neuron can be part of more than one assembly. All neurons of an assembly are expected to fire at roughly the same time [20].\nA standard sparse decomposition of the set of vectorized images into a dictionary and a set of coefficients would not conform with prior knowledge that we have entities at three levels: the pixels, the neurons, and the assemblies, see Fig. 2. Also, it would not allow to include structured constraints [10] in a meaningful way. As a consequence, we propose a multi-level decomposition (Fig. 3) that\n• allows enforcing (structured) sparsity constraints at each level, • admits both hierarchical or heterarchical relations between levels (Fig. 2), • can be learned jointly (section 2 and 2.4), and • yields good results on real-world experimental data (Fig. 2)."
    }, {
      "heading" : "1.1 Relation to Previous Work",
      "text" : "Most important unsupervised data analysis methods such as PCA, NMF / pLSA, ICA, cluster analysis, sparse coding and others can be written in terms of a bilinear decomposition of, or approximation to, a two-way matrix of raw data [22]. One natural generalization is to perform multilinear decompositions of multi-way arrays [4] using methods such as higher-order SVD [1]. This is not the direction pursued here, because the image sequence considered does not have a tensorial structure.\nOn the other hand, there is a relation to (hierarchical) topic models (e.g. [8]). These do not use structured sparsity constraints, but go beyond our approach in automatically estimating the appropriate number of levels using nonparametric Bayesian models.\nClosest to our proposal are four lines of work that we build on: Jenatton et al. [10] introduce structured sparsity constraints that we use to find dictionary basis functions representing single neurons. The works [9] and [13] enforce hierarchical (tree-structured) sparsity constraints. These authors find the tree structure using extraneous methods, such as a separate clustering procedure. In contrast, the method proposed here can infer either hierarchical (tree-structured) or heterarchical (directed acyclic graph) relations between entities at different levels. Cichocki and Zdunek [3] proposed a multilayer approach to non-negative matrix factorization. This is a multi-stage procedure which iteratively decomposes the rightmost matrix of the decomposition that was previously found. Similar approaches are explored in [23], [24]. Finally, Rubinstein et al. [21] proposed a novel dictionary structure where each basis function in a dictionary is a linear combination of a few elements from a fixed base dictionary. In contrast to these last two methods, we optimize over all factors (including the base dictionary) jointly. Note that our semantics of “bilevel factorization” (section 2.2) are different from the one in [25].\nNotation. A matrix is a set of columns and rows, respectively, X = [x:1, . . . ,x:n] = [x1:; . . . ;xm:]. The zero matrix or vector is denoted 0, with dimensions inferred from the context. For any vector x ∈ Rm, ‖x‖α = ( ∑m i=1 |xi|α)1/α is the lα (quasi)-norm of x, and ‖ · ‖F is the Frobenius norm."
    }, {
      "heading" : "2 Learning a Sparse Heterarchical Structure",
      "text" : ""
    }, {
      "heading" : "2.1 Dictionary Learning: Single Level Sparse Matrix Factorization",
      "text" : "Let X ∈ Rm×n be a matrix whose n columns represent an m-dimensional observation each. The idea of dictionary learning is to find a decomposition X ≈ D [ U0 ]T\n, see Fig. 3(a). D is called the dictionary, and its columns hold the basis functions in terms of which the sparse coefficients in U0 approximate the original observations. The regularization term ΩU encourages sparsity of the coefficient matrix. ΩD prevents the inflation of dictionary entries to compensate for small coefficients, and induces, if desired, additional structure on the learned basis functions [16]. Interesting theoretical results on support recovery, furthered by an elegantly compact formulation and the ready availability of optimizers [17] have spawned a large number of intriguing and successful applications, e.g. image denoising [19] and detection of unusual events [26]. Dictionary learning is a special instance of our framework, involving only a single-level decomposition. In the following we first generalize to two, then to more levels."
    }, {
      "heading" : "2.2 Bilevel Sparse Matrix Factorization",
      "text" : "We now come to the heart of this work. To build intuition, we first refer to the application that has motivated this development, before giving mathematical details. The relation between the symbols used in the following is sketched in Fig. 3(b), while actual matrix contents are partially visualized in Fig. 2.\nGiven is a sequence of n noisy sparse images which we vectorize and collect in the columns of matrix X. We would like to find the following:\n• a dictionary D of q0 vectorized images comprising m pixels each. Ideally, each basis function should correspond to a single neuron. • a matrix A1 indicating to what extent each of the q0 neurons is associated with any of the q1 neuronal assemblies. We will call this matrix interchangeably assignment or adjacency matrix in the following. It is this matrix which encapsulates the quintessential structure we extract from the raw data, viz., which lower-level concept is associated with which higher-level concept. • a coefficient matrix [U1]T that encodes in its rows the temporal evolution (activation) of the q1 neuronal assemblies across n time steps. • a coefficient matrix [U0]T (shown in the equation, but not in the sketch of Fig. 3(b)) that encodes in its rows the temporal activation of the q0 neuron basis functions across n time steps.\nThe quantities D, A1, [U0], [U1] in this redundant representation need to be consistent.\nLet us now turn to equations. At first sight, it seems like minimizing ‖X − DA1[U1]T ‖2F over D,A1,U1 subject to constraints should do the job. However, this could be too much of a simplification! To illustrate, assume for the moment that only a single neuronal assembly is active at any given time. Then all neurons associated with that assembly would follow an absolutely identical time course. While it is expected that neurons from an assembly show similar activation patterns [20], this is something we want to glean from the data, and not absolutely impose. In response, we introduce an auxiliary matrix U0 ≈ U1[A1]T showing the temporal activation pattern of individual neurons. These two matrices, U0 and U1, are also shown in the false color plots of the collage of Fig. 2, bottom left.\nThe full equation involving coefficient and auxiliary coefficient matrices is shown in Fig. 3(b). The terms involving X are data fidelity terms, while ‖U0−U1[A1]T ‖2F enforces consistency. Parameters η trade off the various terms, and constraints of a different kind can be applied selectively to each of the matrices that we optimize over. Jointly optimizing over D,A1,U0, and U1 is a hard and non-convex problem that we address using a block coordinate descent strategy described in section 2.4 and supplemental material."
    }, {
      "heading" : "2.3 Trilevel and Multi-level Sparse Matrix Factorization",
      "text" : "We now discuss the generalization to an arbitrary number of levels that may be relevant for applications other than calcium imaging. To give a better feeling for the structure of the equations, the trilevel case is spelled out explicitly in Fig. 3(c), while Fig. 3(d) shows the general case of L + 1 levels.\nThe most interesting matrices, in many ways, are the assignment matrices A1,A2, etc. Assume, first, that the relations between lower-level and higher-level concepts obey a strict inclusion hierarchy. Such relations can be expressed in terms of a forest of trees: each highest-level concept is the root of a tree which fans out to all subordinate concepts. Each subordinate concept has a single parent only. Such a forest can also be seen as a (special case of an L + 1-partite) graph, with an adjacency matrix Al specifying the parents of each concept at level l − 1. To impose an inclusion hierarchy, one can enforce the nestedness condition by requiring that ‖alk:‖0 ≤ 1. In general, and in the application considered here, one will not want to impose an inclusion hierarchy. In that case, the relations between concepts can be expressed in terms of a concatenation of bipartite graphs that conform with a directed acyclic graph. Again, the adjacency matrices encode the structure of such a directed acyclic graph.\nIn summary, the general equation in Fig. 3(d) is a principled alternative to simpler approaches that would impose the relations between concepts, or estimate them separately using, for instance, clustering algorithms; and that would then find a sparse factorization subject to this structure. Instead, we simultaneously estimate the relation between concepts at different levels, as well as find a sparse approximation to the raw data."
    }, {
      "heading" : "2.4 Optimization",
      "text" : "The optimization problem in Fig. 3(d) is not jointly convex, but becomes convex w.r.t. one variable while keeping the others fixed provided that the norms ΩU , ΩD, and ΩA are also convex. Indeed, it is possible to define convex norms that not only induce sparse solutions, but also favor non-zero patterns of a specific structure, such as sets of variables in a convex polygon with certain symmetry constraints [10]. Following [5], we use such norms to bias towards neuron basis functions holding a single neuron only. We employ a block coordinate descent strategy [2, Section 2.7] that iteratively optimizes one group of variables while fixing all others. Due to space limitations, the details and implementation of the optimization are described in the supplemental material."
    }, {
      "heading" : "3 Methods",
      "text" : ""
    }, {
      "heading" : "3.1 Decomposition into neurons and their transients only",
      "text" : "Cell Sorting [18] and Adina [5] focus only on the detection of cell centroids and of cell shape, and the estimation and analysis of Calcium transient signals. However, these methods provide no means to detect and identify neuronal co-activation. The key idea is to decompose calcium imaging data into constituent signal sources, i.e. temporal and spatial components. Cell sorting combines principal component analysis (PCA) and independent component analysis (ICA). In contrast, Adina relies on a matrix factorization based on sparse coding and dictionary learning [15], exploiting that neuronal activity is sparsely distributed in both space and time. Both methods are combined with a subsequent image segmentation since the spatial components (basis functions) often contain more than one neuron. Without such a segmentation step, overlapping cells or those with highly correlated activity are often associated with the same basis function."
    }, {
      "heading" : "3.2 Decomposition into neurons, their transients, and assemblies of neurons",
      "text" : "MNNMF+Adina Here, we combine a multilayer extension of non-negative matrix factorization with the segmentation from Adina. MNNMF [3] is a multi-stage procedure that iteratively decomposes the rightmost matrix of the decomposition that was previously found. In the first stage, we decompose the calcium imaging data into spatial and temporal components, just like the methods cited above, but using NMF and a non-negative least squares loss function [12] as implemented in [14]. We then use the segmentation from [5] to obtain single neurons in an updated dictionary1 D. Given this purged dictionary, the temporal components U0 are updated under the NMF criterion. Next, the temporal components U0 are further decomposed into two low-rank matrices, U0 ≈ U1[A1]T , again using NMF. Altogether, this procedure allows identifying neuronal assemblies and their temporal evolution. However, the exact number of assemblies q1 must be defined a priori.\nKSVDS+Adina allows estimating a sparse decomposition [21] X ≈ DA1[U1]T provided that i) a dictionary of basis functions and ii) the exact number of assemblies is supplied as input. In addition, the assignment matrix A1 is typically dense and needs to be thresholded. We obtain good results when supplying the purged dictionary1 of single neurons resulting from Adina [5].\nSHMF – Sparse Heterarchical Matrix Factorization in its bilevel formulation decomposes the raw data simultaneously into neuron basis functions D, a mapping of these to assemblies A1, as well as time courses of neurons U0 and assemblies U1, see equation in Fig. 3. Sparsity is induced by setting ΩU and ΩA to the l1-norm. In addition, we impose the l2-norm at the assembly level Ω1D,\n1Without such a segmentation step, the dictionary atoms often comprise more than one neuron, and overall results (not shown) are poor.\nand let ΩD be the structured sparsity-inducing norm proposed by Jenatton et al. [10]. In contrast to all other approaches described above, this already suffices to produce basis functions that contain, in most cases, only single neurons. Exceptions arise only in the case of cells which both overlap in space and have high temporal correlation. For this reason, and for a fair comparison with the other methods, we again use the segmentation from [5]. For the optimization, D and U0 are initialized with the results from Adina. U1 is initialized randomly with positive-truncated Gaussian noise, and A1 by the identity matrix as in KSVDS [21]. Finally, the number of neurons q0 and neuronal assemblies q1 are set to generous upper bounds of the expected true numbers, and are both set to equal values (here: q0 = q1 = 60) for simplicity. Note that a precise specification as for the above methods is not required."
    }, {
      "heading" : "4 Results",
      "text" : "To obtain quantitative results, we first evaluate the proposed methods on synthetic image sequences designed so as to exhibit similar characteristics as the real data. We also report a qualitative analysis of the performance on real data from [20]. Since neuronal assemblies are still the subject of ongoing research, ground truth is not available for such real-world data."
    }, {
      "heading" : "4.1 Artifical Sequences",
      "text" : "For evaluation, we created 80 synthetic sequences with 450 frames of size 128 × 128 pixels with a frame rate of 30fps. The data is created by randomly selecting cell shapes from 36 different active cells extracted from real data, and locating them in different locations with an overlap of up to 30%. Each cell is randomly assigned to up to three out of a total of five assemblies. Each assembly fires according to a dependent Poisson process, with transient shapes following a one-sided exponential decay with a scale of 500 to 800ms that is convolved by a Gaussian kernel with σ = 50ms. The dependency is induced by eliminating all transients that overlap by more than 20%. Within such a transient, the neurons associated with the assembly fire with a probability of 90% each. The number of cells per assembly varies from 1 to 10, and we use five assemblies in all experiments. Finally, the synthetic movies are distorted by white Gaussian noise with a relative amplitude, (max. intensity − mean intensity)/σnoise ∈ {3, 5, 7, 10, 12, 15, 17, 20}. By construction, the identity, location and activity patterns of all cells along with their membership in assemblies are known. The supplemental material shows one example, and two frames are shown in Fig. 1.\nIdentificaton of assemblies First, we want to quantify the ability to correctly infer assemblies from an image sequence. To that end, we compute the graph edit distance of the estimated assignments of neurons to assemblies, encoded in matrices A1, to the known ground truth. We count the number of false positive and false negative edges in the assignment graphs, where vertices (assemblies) are matched by minimizing the Hamming distance between binarized assignment matrices over all permutations.\nRemember that MNNMF+Adina and KSVDS+Adina require a specification of the precise number of assemblies, which is unknown for real data. Accordingly, adjacency matrices, A1 ∈ Rq0×q1 for different values for the number of assemblies q1 ∈ [3, 7] were estimated. Bilevel SHMF only needs an upper bound on the number of assemblies. Its performance is independent of the precise value, but computational cost increases with the bound. In these experiments, q1 was set to 60.\nFig. 4 shows that all methods from section 3.2 give respectable performance in the task of inferring neuronal assemblies from nontrivial synthetic image sequences. For the true number of assemblies (q1 = 5), Bilevel SHMF reaches a higher sensitivity than the alternative methods, with a median difference of 14%. According to the quartiles, the precisions achieved are broadly comparable, with MNNMF+Adina reaching the highest value.\nAll methods from section 3.2 also infer the temporal activity of all assemblies, U1. We omit a comparison of these matrices for lack of a good metric that would also take into account the correctness of the assemblies themselves: a fine time course has little worth if its associated assembly is deficient, for instance by having lost some neurons with respect to ground truth.\nDetection of calcium transients While the detection of assemblies as evaluated above is completely new in the literature, we now turn to a better studied [18, 5] problem: the detection of calcium transients of individual neurons. Some estimates for these characteristic waveforms are also shown, for real-world data, on the right hand side of Fig. 2.\nTo quantify transient detection performance, we compute the sensitivity and precision as in [20]. Here, sensitivity is the ratio of correctly detected to all neuronal activities; and precision is the ratio of correctly detected to all detected neuronal activities. Results are shown in Fig. 5.\nPerhaps surprisingly, the methods from section 3.2 (MNNMF+Adina and Bilevel SHMF2) fare at least as well as those from section 3.1 (CellSorting and Adina). This is not self-evident, because a bilevel factorization could be expected to be more ill-posed than a single level factorization.\nWe make two observations: Firstly, it seems that using a bilevel representation with suitable regularization constraints helps stabilize the activity estimates also for single neurons. Secondly, the higher sensitivity and similar precision of bilevel SHMF compared to MNNMF+Adina suggest that a joint estimation of neurons, assemblies and their temporal activities as described in section 2 increases the robustness, and compensates errors that may not be corrected in greedy level-per-level estimation.\nIncidentally, the great spread of both sensitivities and precisions results from the great variety of noise levels used in the simulations, and attests to the difficulty of part of the synthetic data sets.\n2KSVDS is not evaluated here because it does not yield activity estimates for individual neurons."
    }, {
      "heading" : "4.2 Real Sequences",
      "text" : "We have applied bilevel SHMF to epifluorescent data sets from mice (C57BL6) hippocampal slice cultures. As shown in Fig. 2, the method is able to distinguish overlapping cells and highly correlated cells, while at the same time estimating neuronal co-activation patterns (assemblies). Exploiting spatio-temporal sparsity and convex cell shape priors allows to accurately infer the transient events."
    }, {
      "heading" : "5 Discussion",
      "text" : "The proposed multi-level sparse factorization essentially combines a clustering of concepts across several levels (expressed by the assignment matrices) with the finding of a basis dictionary, shared by concepts at all levels, and the finding of coefficient matrices for different levels. The formalism allows imposing different regularizers at different levels. Users need to choose tradeoff parameters η, λ that indirectly determine the number of concepts (clusters) found at each level, and the sparsity. The ranks ql, on the other hand, are less important: Figure 2 shows that the ranks of estimated matrices can be lower than their nominal dimensionality: superfluous degrees of freedom are simply not used.\nOn the application side, the proposed method allows to accomplish the detection of neurons, assemblies and their relation in a single framework, exploiting sparseness in the temporal and spatial domain in the process. Bilevel SHMF in particular is able to detect automatically, and differentiate between, overlapping and highly correlated cells, and to estimate the underlying co-activation patterns. As shown in Fig. 6, this approach is able to reconstruct the raw data at both levels of representations, and to make plausible proposals for neuron and assembly identification.\nGiven the experimental importance of calcium imaging, automated methods in the spirit of the one described here can be expected to become an essential tool for the investigation of complex activation patterns in live neural tissue."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We are very grateful for partial financial support by CellNetworks Cluster (EXC81). We also thank Susanne Reichinnek, Martin Both and Andreas Draguhn for their comments on the manuscript."
    } ],
    "references" : [ {
      "title" : "The Higher-Order Singular Value Decomposition Theory and an Application",
      "author" : [ "G. Bergqvist", "E.G. Larsson" ],
      "venue" : "IEEE Signal Processing Magazine, 27(3):151–154,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Nonlinear Programming",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Multilayer nonnegative matrix factorization",
      "author" : [ "A. Cichocki", "R. Zdunek" ],
      "venue" : "Electronics Letters, 42:947– 948,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Nonnegative Matrix and Tensor Factorizations - Applications to Exploratory Multi-way Data Analysis and Blind Source Separation",
      "author" : [ "A. Cichocki", "R. Zdunek", "A.H. Phan", "S. Amari" ],
      "venue" : "Wiley,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Automated identification of neuronal activity from calcium imaging by sparse dictionary learning",
      "author" : [ "F. Diego", "S. Reichinnek", "M. Both", "F.A. Hamprecht" ],
      "venue" : "International Symposium on Biomedical Imaging, in press,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "In vivo calcium imaging of neural network function",
      "author" : [ "W. Goebel", "F. Helmchen" ],
      "venue" : "Physiology,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Imaging calcium in neurons",
      "author" : [ "C. Grienberger", "A. Konnerth" ],
      "venue" : "Neuron,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Document hierarchies from text and links",
      "author" : [ "Q. Ho", "J. Eisenstein", "E.P. Xing" ],
      "venue" : "Proc. of the 21st Int. World Wide Web Conference (WWW 2012), pages 739–748. ACM,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-scale Mining of fMRI data with Hierarchical Structured Sparsity",
      "author" : [ "R. Jenatton", "A. Gramfort", "V. Michel", "G. Obozinski", "E. Eger", "F. Bach", "B. Thirion" ],
      "venue" : "SIAM Journal on Imaging Sciences, 5(3),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Structured sparse principal component analysis",
      "author" : [ "R. Jenatton", "G. Obozinski", "F. Bach" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Imaging in vivo: watching the brain in action",
      "author" : [ "J. Kerr", "W. Denk" ],
      "venue" : "Nature Review Neuroscience,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method",
      "author" : [ "H. Kim", "H. Park" ],
      "venue" : "SIAM J. on Matrix Analysis and Applications,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eQTL mapping",
      "author" : [ "S. Kim", "E.P. Xing" ],
      "venue" : "Ann. Appl. Stat.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The non-negative matrix factorization toolbox for biological data mining",
      "author" : [ "Y. Li", "A. Ngom" ],
      "venue" : "BMC Source Code for Biology and Medicine,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Online dictionary learning for sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Online Learning for Matrix Factorization and Sparse Coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Automated analysis of cellular signals from largescale calcium imaging data",
      "author" : [ "E.A. Mukamel", "A. Nimmerjahn", "M.J. Schnitzer" ],
      "venue" : "Neuron,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Image sequence denoising via sparse and redundant representations",
      "author" : [ "M. Protter", "M. Elad" ],
      "venue" : "IEEE Transactions on Image Processing, 18(1),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reliable optical detection of coherent neuronal activity in fast oscillating networks in vitro",
      "author" : [ "S. Reichinnek", "A. von Kameke", "A.M. Hagenston", "E. Freitag", "F.C. Roth", "H. Bading", "M.T. Hasan", "A. Draguhn", "M. Both" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Double sparsity: Learning sparse dictionaries for sparse signal approximation",
      "author" : [ "R. Rubinstein", "M. Zibulevsky", "M. Elad" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A unified view of matrix factorization models",
      "author" : [ "A.P. Singh", "G.J. Gordon" ],
      "venue" : "ECML PKDD,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A two-layer non-negative matrix factorization model for vocabulary discovery",
      "author" : [ "M. Sun", "H. Van Hamme" ],
      "venue" : "Symposium on machine learning in speech and language processing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Unsupervised multi-level non-negative matrix factorization model: Binary data case",
      "author" : [ "Q. Sun", "P. Wu", "Y. Wu", "M. Guo", "J. Lu" ],
      "venue" : "Journal of Information Security,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bilevel sparse coding for coupled feature spaces",
      "author" : [ "J. Yang", "Z. Wang", "Z. Lin", "X. Shu", "T.S. Huang" ],
      "venue" : "CVPR’12, pages 2360–2367. IEEE,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Online detection of unusual events in videos via dynamic sparse coding",
      "author" : [ "B. Zhao", "L. Fei-Fei", "E.P. Xing" ],
      "venue" : "The Twenty-Fourth IEEE Conference on Computer Vision and Pattern Recognition, Colorado Springs, CO, June",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "1 into neurons, and assemblies of neurons [20].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 10,
      "context" : "Calcium imaging is an increasingly popular tool for unraveling the network structure of local circuits of the brain [11, 6, 7].",
      "startOffset" : 116,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "Calcium imaging is an increasingly popular tool for unraveling the network structure of local circuits of the brain [11, 6, 7].",
      "startOffset" : 116,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "Calcium imaging is an increasingly popular tool for unraveling the network structure of local circuits of the brain [11, 6, 7].",
      "startOffset" : 116,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "All neurons of an assembly are expected to fire at roughly the same time [20].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "Also, it would not allow to include structured constraints [10] in a meaningful way.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 20,
      "context" : "Most important unsupervised data analysis methods such as PCA, NMF / pLSA, ICA, cluster analysis, sparse coding and others can be written in terms of a bilinear decomposition of, or approximation to, a two-way matrix of raw data [22].",
      "startOffset" : 229,
      "endOffset" : 233
    }, {
      "referenceID" : 3,
      "context" : "One natural generalization is to perform multilinear decompositions of multi-way arrays [4] using methods such as higher-order SVD [1].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "One natural generalization is to perform multilinear decompositions of multi-way arrays [4] using methods such as higher-order SVD [1].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 9,
      "context" : "[10] introduce structured sparsity constraints that we use to find dictionary basis functions representing single neurons.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "The works [9] and [13] enforce hierarchical (tree-structured) sparsity constraints.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 12,
      "context" : "The works [9] and [13] enforce hierarchical (tree-structured) sparsity constraints.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "Cichocki and Zdunek [3] proposed a multilayer approach to non-negative matrix factorization.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "Similar approaches are explored in [23], [24].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "Similar approaches are explored in [23], [24].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 19,
      "context" : "[21] proposed a novel dictionary structure where each basis function in a dictionary is a linear combination of a few elements from a fixed base dictionary.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "ΩD prevents the inflation of dictionary entries to compensate for small coefficients, and induces, if desired, additional structure on the learned basis functions [16].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : "image denoising [19] and detection of unusual events [26].",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : "image denoising [19] and detection of unusual events [26].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "The raw data comes from a mouse hippocampal slice, where single neurons can indeed be part of more than one assembly [20].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 18,
      "context" : "While it is expected that neurons from an assembly show similar activation patterns [20], this is something we want to glean from the data, and not absolutely impose.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "Indeed, it is possible to define convex norms that not only induce sparse solutions, but also favor non-zero patterns of a specific structure, such as sets of variables in a convex polygon with certain symmetry constraints [10].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "Following [5], we use such norms to bias towards neuron basis functions holding a single neuron only.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 16,
      "context" : "Cell Sorting [18] and Adina [5] focus only on the detection of cell centroids and of cell shape, and the estimation and analysis of Calcium transient signals.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "Cell Sorting [18] and Adina [5] focus only on the detection of cell centroids and of cell shape, and the estimation and analysis of Calcium transient signals.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "In contrast, Adina relies on a matrix factorization based on sparse coding and dictionary learning [15], exploiting that neuronal activity is sparsely distributed in both space and time.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "MNNMF [3] is a multi-stage procedure that iteratively decomposes the rightmost matrix of the decomposition that was previously found.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "In the first stage, we decompose the calcium imaging data into spatial and temporal components, just like the methods cited above, but using NMF and a non-negative least squares loss function [12] as implemented in [14].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 13,
      "context" : "In the first stage, we decompose the calcium imaging data into spatial and temporal components, just like the methods cited above, but using NMF and a non-negative least squares loss function [12] as implemented in [14].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 4,
      "context" : "We then use the segmentation from [5] to obtain single neurons in an updated dictionary1 D.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 19,
      "context" : "KSVDS+Adina allows estimating a sparse decomposition [21] X ≈ DA(1)[U(1)] provided that i) a dictionary of basis functions and ii) the exact number of assemblies is supplied as input.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "We obtain good results when supplying the purged dictionary1 of single neurons resulting from Adina [5].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 4,
      "context" : "For this reason, and for a fair comparison with the other methods, we again use the segmentation from [5].",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "U(1) is initialized randomly with positive-truncated Gaussian noise, and A(1) by the identity matrix as in KSVDS [21].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 18,
      "context" : "We also report a qualitative analysis of the performance on real data from [20].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "Accordingly, adjacency matrices, A(1) ∈ Rq0×q1 for different values for the number of assemblies q1 ∈ [3, 7] were estimated.",
      "startOffset" : 102,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "Accordingly, adjacency matrices, A(1) ∈ Rq0×q1 for different values for the number of assemblies q1 ∈ [3, 7] were estimated.",
      "startOffset" : 102,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "Detection of calcium transients While the detection of assemblies as evaluated above is completely new in the literature, we now turn to a better studied [18, 5] problem: the detection of calcium transients of individual neurons.",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "Detection of calcium transients While the detection of assemblies as evaluated above is completely new in the literature, we now turn to a better studied [18, 5] problem: the detection of calcium transients of individual neurons.",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 18,
      "context" : "To quantify transient detection performance, we compute the sensitivity and precision as in [20].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "Raw data Cell Sorting [18] Adina [5] Neurons (D[U] ) Assemblies (DA(1)[U(1)] )",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 4,
      "context" : "Raw data Cell Sorting [18] Adina [5] Neurons (D[U] ) Assemblies (DA(1)[U(1)] )",
      "startOffset" : 33,
      "endOffset" : 36
    } ],
    "year" : 2013,
    "abstractText" : "Bilinear approximation of a matrix is a powerful paradigm of unsupervised learning. In some applications, however, there is a natural hierarchy of concepts that ought to be reflected in the unsupervised analysis. For example, in the neurosciences image sequence considered here, there are the semantic concepts of pixel → neuron→ assembly that should find their counterpart in the unsupervised analysis. Driven by this concrete problem, we propose a decomposition of the matrix of observations into a product of more than two sparse matrices, with the rank decreasing from lower to higher levels. In contrast to prior work, we allow for both hierarchical and heterarchical relations of lower-level to higher-level concepts. In addition, we learn the nature of these relations rather than imposing them. Finally, we describe an optimization scheme that allows to optimize the decomposition over all levels jointly, rather than in a greedy level-by-level fashion. The proposed bilevel SHMF (sparse heterarchical matrix factorization) is the first formalism that allows to simultaneously interpret a calcium imaging sequence in terms of the constituent neurons, their membership in assemblies, and the time courses of both neurons and assemblies. Experiments show that the proposed model fully recovers the structure from difficult synthetic data designed to imitate the experimental data. More importantly, bilevel SHMF yields plausible interpretations of real-world Calcium imaging data.",
    "creator" : null
  }
}