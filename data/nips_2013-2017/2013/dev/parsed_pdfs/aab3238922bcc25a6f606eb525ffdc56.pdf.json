{
  "name" : "aab3238922bcc25a6f606eb525ffdc56.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The Randomized Dependence Coefficient",
    "authors" : [ "David Lopez-Paz", "Philipp Hennig", "Bernhard Schölkopf" ],
    "emails" : [ "dlopez@tue.mpg.de", "phennig@tue.mpg.de", "bs@tue.mpg.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We introduce the Randomized Dependence Coefficient (RDC), a measure of nonlinear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper."
    }, {
      "heading" : "1 Introduction",
      "text" : "Measuring statistical dependence between random variables is a fundamental problem in statistics. Commonly used measures of dependence, Pearson’s rho, Spearman’s rank or Kendall’s tau are computationally efficient and theoretically well understood, but consider only a limited class of association patterns, like linear or monotonically increasing functions. The development of non-linear dependence measures is challenging because of the radically larger amount of possible association patterns.\nDespite these difficulties, many non-linear statistical dependence measures have been developed recently. Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18]. However, these methods exhibit high computational demands (at least quadratic costs in the number of samples for KCCA, HSIC, CHSIC, dCor or MIC), are limited to measuring dependencies between scalar random variables (ACE, MIC) or can be difficult to implement (ACE, MIC).\nThis paper develops the Randomized Dependence Coefficient (RDC), an estimator of the HirschfeldGebelein-Rényi Maximum Correlation Coefficient (HGR) addressing the issues listed above. RDC defines dependence between two random variables as the largest canonical correlation between random non-linear projections of their respective empirical copula-transformations. RDC is invariant to monotonically increasing transformations, operates on random variables of arbitrary dimension, and has computational cost of O(n log n) with respect to the sample size. Moreover, it is easy to implement: just five lines of R code, included in Appendix A.\nThe following Section reviews the classic work of Alfréd Rényi [17], who proposed seven desirable fundamental properties of dependence measures, proved to be satisfied by the Hirschfeld-GebeleinRényi’s Maximum Correlation Coefficient (HGR). Section 3 introduces the Randomized Dependence Coefficient as an estimator designed in the spirit of HGR, since HGR itself is computationally intractable. Properties of RDC and its relationship to other non-linear dependence measures are analysed in Section 4. Section 5 validates the empirical performance of RDC on a series of numerical experiments on both synthetic and real-world data."
    }, {
      "heading" : "2 Hirschfeld-Gebelein-Rényi’s Maximum Correlation Coefficient",
      "text" : "In 1959 [17], Alfréd Rényi argued that a measure of dependence ρ∗ : X × Y → [0, 1] between random variables X ∈ X and Y ∈ Y should satisfy seven fundamental properties:\n1. ρ∗(X,Y ) is defined for any pair of non-constant random variables X and Y . 2. ρ∗(X,Y ) = ρ∗(Y,X) 3. 0 ≤ ρ∗(X,Y ) ≤ 1 4. ρ∗(X,Y ) = 0 iff X and Y are statistically independent. 5. For bijective Borel-measurable functions f, g : R→ R, ρ∗(X,Y ) = ρ∗(f(X), g(Y )). 6. ρ∗(X,Y ) = 1 if for Borel-measurable functions f or g, Y = f(X) or X = g(Y ). 7. If (X,Y ) ∼ N (µ,Σ), then ρ∗(X,Y ) = |ρ(X,Y )|, where ρ is the correlation coefficient.\nRényi also showed the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient (HGR) [3, 17] to satisfy all these properties. HGR was defined by Gebelein in 1941 [3] as the supremum of Pearson’s correlation coefficient ρ over all Borel-measurable functions f, g of finite variance:\nhgr(X,Y ) = sup f,g ρ(f(X), g(Y )), (1)\nSince the supremum in (1) is over an infinite-dimensional space, HGR is not computable. It is an abstract concept, not a practical dependence measure. In the following we propose a scalable estimator with the same structure as HGR: the Randomized Dependence Coefficient."
    }, {
      "heading" : "3 Randomized Dependence Coefficient",
      "text" : "The Randomized Dependence Coefficient (RDC) measures the dependence between random samples X ∈ Rp×n and Y ∈ Rq×n as the largest canonical correlation between k randomly chosen nonlinear projections of their copula transformations. Before Section 3.4 defines this concept formally, we describe the three necessary steps to construct the RDC statistic: copula-transformation of each of the two random samples (Section 3.1), projection of the copulas through k randomly chosen nonlinear maps (Section 3.2) and computation of the largest canonical correlation between the two sets of non-linear random projections (Section 3.3). Figure 1 offers a sketch of this process."
    }, {
      "heading" : "3.1 Estimation of Copula-Transformations",
      "text" : "To achieve invariance with respect to transformations on marginal distributions (such as shifts or rescalings), we operate on the empirical copula transformation of the data [14, 15]. Consider a random vector X = (X1, . . . , Xd) with continuous marginal cumulative distribution functions (cdfs) Pi, 1 ≤ i ≤ d. Then the vector U = (U1, . . . , Ud) := P (X) = (P1(X1), . . . , Pd(Xd)), known as the copula transformation, has uniform marginals:\nTheorem 1. (Probability Integral Transform [14]) For a random variableX with cdf P , the random variable U := P (X) is uniformly distributed on [0, 1].\nThe random variables U1, . . . , Ud are known as the observation ranks of X1, . . . , Xd. Crucially, U preserves the dependence structure of the original random vector X , but ignores each of its d marginal forms [14]. The joint distribution of U is known as the copula ofX:\nTheorem 2. (Sklar [20]) Let the random vectorX = (X1, . . . , Xd) have continuous marginal cdfs Pi, 1 ≤ i ≤ d. Then, the joint cumulative distribution ofX is uniquely expressed as:\nP (X1, . . . , Xd) = C(P1(X1), . . . , Pd(Xd)), (2)\nwhere the distribution C is known as the copula ofX .\nA practical estimator of the univariate cdfs P1, . . . , Pd is the empirical cdf :\nPn(x) := 1\nn\nn∑\ni=1\nI(Xi ≤ x), (3)\nwhich gives rise to the empirical copula transformations of a multivariate sample:\nPn(x) = [Pn,1(x1), . . . , Pn,d(xd)]. (4)\nThe Massart-Dvoretzky-Kiefer-Wolfowitz inequality [13] can be used to show that empirical copula transformations converge fast to the true transformation as the sample size increases:\nTheorem 3. (Convergence of the empirical copula, [15, Lemma 7]) Let X1, . . . ,Xn be an i.i.d. sample from a probability distribution over Rd with marginal cdf’s P1, . . . , Pd. Let P (X) be the copula transformation and Pn(X) the empirical copula transformation. Then, for any > 0:\nPr [ sup x∈Rd ‖P (x)− Pn(x)‖2 > ] ≤ 2d exp ( −2n 2 d ) . (5)\nComputing Pn(X) involves sorting the marginals ofX ∈ Rd×n, thus O(dn log(n)) operations."
    }, {
      "heading" : "3.2 Generation of Random Non-Linear Projections",
      "text" : "The second step of the RDC computation is to augment the empirical copula transformations with non-linear projections, so that linear methods can subsequently be used to capture non-linear dependencies on the original data. This is a classic idea also used in other areas, particularly in regression. In an elegant result, Rahimi and Recht [16] proved that linear regression on random, non-linear projections of the original feature space can generate high-performance regressors:\nTheorem 4. (Rahimi-Recht) Let p be a distribution on Ω and |φ(x;w)| ≤ 1. Let F ={ f(x) = ∫ Ω α(w)φ(x;w)dw ∣∣ |α(w)| ≤ Cp(w) }\n. Draw w1, . . . ,wk iid from p. Further let δ > 0, and c be some L-Lipschitz loss function, and consider data {xi, yi}ni=1 drawn iid from some arbitrary P (X, Y ). The α1, . . . , αk for which fk(x) = ∑k i=1 αiφ(x;wi) minimizes the empirical risk c(fk(x), y) has a distance from the c-optimal estimator in F bounded by\nEP [c(fk(x), y)]−min f∈F\nEP [c(f(x), y)] ≤ O ((\n1√ n + 1√ k\n) LC √ log 1\nδ\n) (6)\nwith probability at least 1− 2δ.\nIntuitively, Theorem 4 states that randomly selecting wi in ∑k\ni=1 αiφ(x;wi) instead of optimising them causes only bounded error.\nThe choice of the non-linearities φ : R → R is the main and unavoidable assumption in RDC. This choice is a well-known problem common to all non-linear regression methods and has been studied extensively in the theory of regression as the selection of reproducing kernel Hilbert space [19, §3.13]. The only way to favour one such family and distribution over another is to use prior assumptions about which kind of distributions the method will typically have to analyse.\nWe use random features instead of the Nyström method because of their smaller memory and computation requirements [11]. In our experiments, we will use sinusoidal projections, φ(wTx+ b) := sin(wTx + b). Arguments favouring this choice are that shift-invariant kernels are approximated with these features when using the appropriate random parameter sampling distribution [16], [4, p. 208] [22, p. 24], and that functions with absolutely integrable Fourier transforms are approximated with L2 error below O(1/ √ k) by k of these features [10].\nLet the random parameters wi ∼ N (0, sI), bi ∼ N (0, s). Choosing wi to be Normal is analogous to the use of the Gaussian kernel for HSIC, CHSIC or KCCA [16]. Tuning s is analogous to selecting the kernel width, that is, to regularize the non-linearity of the random projections.\nGiven a data collectionX = (x1, . . . ,xn), we will denote by\nΦ(X; k, s) :=   φ(wT1 x1 + b1) · · · φ(wTk x1 + bk) ... ... ...\nφ(wT1 xn + b1) · · · φ(wTk xn + bk)\n  T\n(7)\nthe k−th order random non-linear projection from X ∈ Rd×n to Φ(X; k, s) ∈ Rk×n. The computational complexity of computing Φ(X; k, s) with naive matrix multiplications is O(kdn). However, recent techniques using fast Walsh-Hadamard transforms [11] allow computing these feature expansions within a computational cost of O(k log(d)n) and O(k) storage."
    }, {
      "heading" : "3.3 Computation of Canonical Correlations",
      "text" : "The final step of RDC is to compute the linear combinations of the augmented empirical copula transformations that have maximal correlation. Canonical Correlation Analysis (CCA, [7]) is the calculation of pairs of basis vectors (α,β) such that the projections αTX and βTY of two random samples X ∈ Rp×n and Y ∈ Rq×n are maximally correlated. The correlations between the projected (or canonical) random samples are referred to as canonical correlations. There exist up to max(rank(X), rank(Y )) of them. Canonical correlations ρ2 are the solutions to the eigenproblem:\n( 0 C−1xxCxy\nC−1yy Cyx 0\n)( α β ) = ρ2 ( α β ) , (8)\nwhere Cxy = cov(X,Y ) and the matrices Cxx and Cyy are assumed to be invertible. Therefore, the largest canonical correlation ρ1 betweenX andY is the supremum of the correlation coefficients over their linear projections, that is: ρ1(X,Y ) = supα,β ρ(α TX,βTY ).\nWhen p, q n, the cost of CCA is dominated by the estimation of the matricesCxx,Cyy andCxy , hence being O((p+ q)2n) for two random variables of dimensions p and q, respectively."
    }, {
      "heading" : "3.4 Formal Definition or RDC",
      "text" : "Given the random samplesX ∈ Rp×n and Y ∈ Rq×n and the parameters k ∈ N+ and s ∈ R+, the Randomized Dependence Coefficient betweenX and Y is defined as:\nrdc(X,Y ; k, s) := sup α,β\nρ ( αTΦ(P (X); k, s),βTΦ(P (Y ); k, s) ) . (9)"
    }, {
      "heading" : "4 Properties of RDC",
      "text" : "Computational complexity: In the typical setup (very large n, large p and q, small k) the computational complexity of RDC is dominated by the calculation of the copula-transformations. Hence, we achieve a cost in terms of the sample size ofO((p+q)n log n+kn log(pq)+k2n) ≈ O(n log n).\nEase of implementation: An implementation of RDC in R is included in the Appendix A.\nRelationship to the HGR coefficient: It is tempting to wonder whether RDC is a consistent, or even an efficient estimator of the HGR coefficient. However, a simple experiment shows that it is not desirable to approximate HGR exactly on finite datasets: Consider p(X,Y ) = N (x; 0, 1)N (y; 0, 1)\nwhich is independent, thus, by both Rényi’s 4th and 7th properties, has hgr(X,Y ) = 0. However, for finitely many N samples from p(X,Y ), almost surely, values in both X and Y are pairwise different and separated by a finite difference. So there exist continuous (thus Borel measurable) functions f(X) and g(Y ) mapping both X and Y to the sorting ranks of Y , i.e. f(xi) = g(yi) ∀(xi, yi) ∈ (X,Y ). Therefore, the finite-sample version of Equation (1) is constant and equal to “1” for continuous random variables. Meaningful measures of dependence from finite samples thus must rely on some form of regularization. RDC achieves this by approximating the space of Borel measurable functions with the restricted function class F from Theorem 4: Assume the optimal transformations f and g (Equation 1) to belong to the Reproducing Kernel Hilbert Space F (Theorem 4), with associated shift-invariant, positive semi-definite kernel function k(x,x′) = 〈φ(x),φ(x′)〉F ≤ 1. Then, with probability greater than 1− 2δ:\nhgr(X,Y ;F)− rdc(X,Y ; k) = O ((‖m‖F√\nn + LC√ k\n)√ log 1\nδ\n) , (10)\nwhere m := ααT + ββT and n, k denote the sample size and number of random features. The bound (10) is the sum of two errors. The error O(1/ √ n) is due to the convergence of CCA’s largest eigenvalue in the finite sample size regime. This result [8, Theorem 6] is originally obtained by posing CCA as a least squares regression on the product space induced by the feature map ψ(x,y) = [φ(x)φ(x)T ,φ(y)φ(y)T , √ 2φ(x)φ(y)T ]T . Because of approximating ψ with k random features, an additional error O(1/ √ k) is introduced in the least squares regression [16, Lemma 3]. Therefore, an equivalence between RDC and KCCA is established if RDC uses an infinite number of sinusoidal features, the random sampling distribution is set to the inverse Fourier transform of the shift-invariant kernel used by KCCA and the copula-transformations are discarded. However, when k ≥ n regularization is needed to avoid spurious perfect correlations, as discussed above.\nRelationship to other estimators: Table 1 summarizes several state-of-the-art dependence measures showing, for each measure, whether it allows for general non-linear dependence estimation, handles multidimensional random variables, is invariant with respect to changes in marginal distributions, returns a statistic in [0, 1], satisfy Rényi’s properties (Section 2), and how many parameters it requires. As parameters, we here count the kernel function for kernel methods, the basis function and number of random features for RDC, the stopping tolerance for ACE and the search-grid size for MIC, respectively. Finally, the table lists computational complexities with respect to sample size.\nWhen using random features φ linear for some neighbourhood around zero (like sinusoids or sigmoids), RDC converges to Spearman’s rank correlation coefficient as s→ 0, for any k.\nTesting for independence with RDC: Consider the hypothesis “the two sets of non-linear projections are mutually uncorrelated”. Under normality assumptions (or large sample sizes), Bartlett’s approximation [12] can be used to show ( 2k+3 2 − n ) log ∏k i=1(1−ρ2i ) ∼ χ2k2 , where ρ1, . . . , ρk are the\ncanonical correlations between Φ(P (X); k, s) and Φ(P (Y ); k, s). Alternatively, non-parametric asymptotic distributions can be obtained from the spectrum of the inner products of the non-linear random projection matrices [25, Theorem 3]."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "We performed experiments on both synthetic and real-world data to validate the empirical performance of RDC versus the non-linear dependence measures listed in Table 1. In some experiments we do not compare against to KCCA because we were unable to find a good set of hyperparameters.\nParameter selection: For RDC, the number of random features is set to k = 20 for both random samples, since no significant improvements were observed for larger values. The random feature sampling parameter s is more crucial, and set as follows: when the marginals of u are standard uniforms, w ∼ N (0, sI) and b ∼ N (0, s), then V[wTu+ b] = s ( 1 + d3 ) ; therefore, we opt to set s to a linear scaling of the input variable dimensionality. In all our experiments s = 16d worked well. The development of better methods to set the parameters of RDC is left as future work.\nHSIC and CHSIC use Gaussian kernels k(z, z′) = exp(−γ‖z−z′‖22) with γ−1 set to the euclidean distance median of each sample [5]. MIC’s search-grid size is set to B(n) = n0.6 as recommended by the authors [18], although speed improvements are achieved when using lower values. ACE’s tolerance is set to = 0.01, default value in the R package acepack."
    }, {
      "heading" : "5.1 Synthetic Data",
      "text" : "Resistance to additive noise: We define the power of a dependence measure as its ability to discern between dependent and independent samples that share equal marginal forms. In the spirit of Simon and Tibshirani1, we conducted experiments to estimate the power of RDC as a measure of non-linear dependence. We chose 8 bivariate association patterns, depicted inside little boxes in Figure 3. For each of the 8 association patterns, 500 repetitions of 500 samples were generated, in which the input sample was uniformly distributed on the unit interval. Next, we regenerated the input sample randomly, to generate independent versions of each sample with equal marginals. Figure 3 shows the power for the discussed non-linear dependence measures as the variance of some zero-mean Gaussian additive noise increases from 1/30 to 3. RDC shows worse performance in the linear association pattern due to overfitting and in the step-function due to the smoothness prior induced by the sinusoidal features, but has good performance in non-functional association patterns.\nRunning times: Table 2 shows running times for the considered non-linear dependence measures on scalar, uniformly distributed, independent samples of sizes {103, . . . , 106} when averaging over 100 runs. Single runs above ten minutes were cancelled. Pearson’s ρ, ACE, dCor, KCCA and MIC are implemented in C, while RDC, HSIC and CHSIC are implemented as interpreted R code. KCCA is approximated using incomplete Cholesky decompositions as described in [1].\nValue of statistic in [0, 1]: Figure 4 shows RDC, ACE, dCor, MIC, Pearson’s ρ, Spearman’s rank and Kendall’s τ dependence estimates for 14 different associations of two scalar random samples. RDC scores values close to one on all the proposed dependent associations, whilst scoring values close to zero for the independent association, depicted last. When the associations are Gaussian (first row), RDC scores values close to the Pearson’s correlation coefficient (Section 2, 7th property).\n1http://www-stat.stanford.edu/˜tibs/reshef/comment.pdf"
    }, {
      "heading" : "5.2 Feature Selection in Real-World Data",
      "text" : "We performed greedy feature selection via dependence maximization [21] on eight real-world datasets. More specifically, we attempted to construct the subset of features G ⊂ X that minimizes the normalized mean squared regression error (NMSE) of a Gaussian process regressor. We do so by selecting the feature x(i) maximizing dependence between the feature set Gi = {Gi−1, x(i)} and the target variable y at each iteration i ∈ {1, . . . 10}, such that G0 = {∅} and x(i) /∈ Gi−1. We considered 12 heterogeneous datasets, obtained from the UCI dataset repository2, the Gaussian process web site Data3 and the Machine Learning data set repository4. Random training/test partitions are computed to be disjoint and equal sized.\nSince G can be multi-dimensional, we compare RDC to the non-linear methods dCor, HSIC and CHSIC. Given their quadratic computational demands, dCor, HSIC and CHSIC use up to 1, 000 points when measuring dependence; this constraint only applied on the sarcos and abalone datasets. Results are averages of 20 random training/test partitions.\nFigure 2 summarizes the results for all datasets and algorithms as the number of selected features increases. RDC performs best in most datasets, with much lower running time than its contenders."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have presented the randomized dependence coefficient, a lightweight non-linear measure of dependence between multivariate random samples. Constructed as a finite-dimensional estimator in the spirit of the Hirschfeld-Gebelein-Rényi maximum correlation coefficient, RDC performs well empirically, is scalable to very large datasets, and is easy to adapt to concrete problems.\nWe thank fruitful discussions with Alberto Suárez, Theofanis Karaletsos and David Reshef.\n2http://www.ics.uci.edu/˜mlearn 3http://www.gaussianprocess.org/gpml/data/ 4http://www.mldata.org"
    }, {
      "heading" : "A R Source Code",
      "text" : "rdc <- function(x,y,k=20,s=1/6,f=sin) { x <- cbind(apply(as.matrix(x),2,function(u)rank(u)/length(u)),1) y <- cbind(apply(as.matrix(y),2,function(u)rank(u)/length(u)),1) x <- s/ncol(x)*x%*%matrix(rnorm(ncol(x)*k),ncol(x)) y <- s/ncol(y)*y%*%matrix(rnorm(ncol(y)*k),ncol(y)) cancor(cbind(f(x),1),cbind(f(y),1))$cor[1] }"
    } ],
    "references" : [ {
      "title" : "Kernel independent component analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "JMLR, 3:1–48",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Estimating Optimal Transformations for Multiple Regression and Correlation",
      "author" : [ "L. Breiman", "J.H. Friedman" ],
      "venue" : "Journal of the American Statistical Association, 80(391):580–598",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "Das statistische Problem der Korrelation als Variations- und Eigenwertproblem und sein Zusammenhang mit der Ausgleichsrechnung",
      "author" : [ "H. Gebelein" ],
      "venue" : "Zeitschrift für Angewandte Mathematik und Mechanik, 21(6):364–379",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1941
    }, {
      "title" : "Skorohod. The Theory of Stochastic Processes, volume 1. Springer, 1974s",
      "author" : [ "A.V.I.I. Gihman" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1974
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "A. Gretton", "K.M. Borgwardt", "M.J. Rasch", "B. Schölkopf", "A. Smola" ],
      "venue" : "JMLR, 13:723–773",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Measuring statistical dependence with Hilbert-Schmidt norms",
      "author" : [ "A. Gretton", "O. Bousquet", "A. Smola", "B. Schölkopf" ],
      "venue" : "Proceedings of the 16th international conference on Algorithmic Learning Theory, pages 63–77. Springer-Verlag",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Applied Multivariate Statistical Analysis",
      "author" : [ "W.K. Härdle", "L. Simar" ],
      "venue" : "Springer, 2nd edition",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Convergence analysis of kernel canonical correlation analysis: theory and practice",
      "author" : [ "D. Hardoon", "J. Shawe-Taylor" ],
      "venue" : "Machine Learning, 74(1):23–38",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Generalized additive models",
      "author" : [ "T. Hastie", "R. Tibshirani" ],
      "venue" : "Statistical Science, 1:297–310",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "A simple lemma on greedy approximation in Hilbert space and convergence rates for projection pursuit regression and neural network training",
      "author" : [ "L.K. Jones" ],
      "venue" : "Annals of Statistics, 20(1):608– 613",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Fastfood – Approximating kernel expansions in loglinear time",
      "author" : [ "Q. Le", "T. Sarlos", "A. Smola" ],
      "venue" : "ICML",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Multivariate Analysis",
      "author" : [ "K.V. Mardia", "J.T. Kent", "J.M. Bibby" ],
      "venue" : "Academic Press",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "The tight constant in the Dvoretzky-Kiefer-wolfowitz inequality",
      "author" : [ "P. Massart" ],
      "venue" : "The Annals of Probability, 18(3)",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "An Introduction to Copulas",
      "author" : [ "R. Nelsen" ],
      "venue" : "Springer Series in Statistics, 2nd edition",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Copula-based kernel dependency measures",
      "author" : [ "B. Poczos", "Z. Ghahramani", "J. Schneider" ],
      "venue" : "ICML",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "On measures of dependence",
      "author" : [ "A. Rényi" ],
      "venue" : "Acta Mathematica Academiae Scientiarum Hungaricae, 10:441–451",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Detecting novel associations in large data sets",
      "author" : [ "D.N. Reshef", "Y.A. Reshef", "H.K. Finucane", "S.R. Grossman", "G. McVean", "P.J. Turnbaugh", "E.S. Lander", "M. Mitzenmacher", "P.C. Sabeti" ],
      "venue" : "Science, 334(6062):1518–1524",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning with Kernels",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : "MIT Press",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Fonctions de repartition à n dimension set leurs marges",
      "author" : [ "A. Sklar" ],
      "venue" : "Publ. Inst. Statis. Univ. Paris, 8(1):229–231",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Feature selection via dependence maximization",
      "author" : [ "L. Song", "A. Smola", "A. Gretton", "J. Bedo", "K. Borgwardt" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Interpolation of Spatial Data",
      "author" : [ "M.L. Stein" ],
      "venue" : "Springer",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Rejoinder: Brownian distance covariance",
      "author" : [ "G.J. Székely", "M.L. Rizzo" ],
      "venue" : "Annals of Applied Statistics, 3(4):1303–1308",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Measuring and testing dependence by correlation of distances",
      "author" : [ "G.J. Székely", "M.L. Rizzo", "N.K. Bakirov" ],
      "venue" : "Annals of Statistics, 35(6)",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "D",
      "author" : [ "K. Zhang", "J. Peters" ],
      "venue" : "Janzing, and B.Schölkopf. Kernel-based conditional independence test and application in causal discovery. CoRR, abs/1202.3775",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 209,
      "endOffset" : 219
    }, {
      "referenceID" : 4,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 209,
      "endOffset" : 219
    }, {
      "referenceID" : 14,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 209,
      "endOffset" : 219
    }, {
      "referenceID" : 23,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 261,
      "endOffset" : 269
    }, {
      "referenceID" : 22,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 261,
      "endOffset" : 269
    }, {
      "referenceID" : 17,
      "context" : "Examples include the Alternating Conditional Expectations or backfitting algorithm (ACE) [2, 9], Kernel Canonical Correlation Analysis (KCCA) [1], (Copula) Hilbert-Schmidt Independence Criterion (CHSIC, HSIC) [6, 5, 15], Distance or Brownian Correlation (dCor) [24, 23] and the Maximal Information Coefficient (MIC) [18].",
      "startOffset" : 316,
      "endOffset" : 320
    }, {
      "referenceID" : 16,
      "context" : "The following Section reviews the classic work of Alfréd Rényi [17], who proposed seven desirable fundamental properties of dependence measures, proved to be satisfied by the Hirschfeld-GebeleinRényi’s Maximum Correlation Coefficient (HGR).",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "2 Hirschfeld-Gebelein-Rényi’s Maximum Correlation Coefficient In 1959 [17], Alfréd Rényi argued that a measure of dependence ρ∗ : X × Y → [0, 1] between random variables X ∈ X and Y ∈ Y should satisfy seven fundamental properties: 1.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Rényi also showed the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient (HGR) [3, 17] to satisfy all these properties.",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "Rényi also showed the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient (HGR) [3, 17] to satisfy all these properties.",
      "startOffset" : 86,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "HGR was defined by Gebelein in 1941 [3] as the supremum of Pearson’s correlation coefficient ρ over all Borel-measurable functions f, g of finite variance: hgr(X,Y ) = sup f,g ρ(f(X), g(Y )), (1)",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "To achieve invariance with respect to transformations on marginal distributions (such as shifts or rescalings), we operate on the empirical copula transformation of the data [14, 15].",
      "startOffset" : 174,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "To achieve invariance with respect to transformations on marginal distributions (such as shifts or rescalings), we operate on the empirical copula transformation of the data [14, 15].",
      "startOffset" : 174,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "(Probability Integral Transform [14]) For a random variableX with cdf P , the random variable U := P (X) is uniformly distributed on [0, 1].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "Crucially, U preserves the dependence structure of the original random vector X , but ignores each of its d marginal forms [14].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "(Sklar [20]) Let the random vectorX = (X1, .",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 12,
      "context" : "(4) The Massart-Dvoretzky-Kiefer-Wolfowitz inequality [13] can be used to show that empirical copula transformations converge fast to the true transformation as the sample size increases: Theorem 3.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "In an elegant result, Rahimi and Recht [16] proved that linear regression on random, non-linear projections of the original feature space can generate high-performance regressors: Theorem 4.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "We use random features instead of the Nyström method because of their smaller memory and computation requirements [11].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "Arguments favouring this choice are that shift-invariant kernels are approximated with these features when using the appropriate random parameter sampling distribution [16], [4, p.",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 9,
      "context" : "24], and that functions with absolutely integrable Fourier transforms are approximated with L2 error below O(1/ √ k) by k of these features [10].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "Choosing wi to be Normal is analogous to the use of the Gaussian kernel for HSIC, CHSIC or KCCA [16].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "However, recent techniques using fast Walsh-Hadamard transforms [11] allow computing these feature expansions within a computational cost of O(k log(d)n) and O(k) storage.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "Canonical Correlation Analysis (CCA, [7]) is the calculation of pairs of basis vectors (α,β) such that the projections αX and βY of two random samples X ∈ Rp×n and Y ∈ Rq×n are maximally correlated.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "Cost Pearson’s ρ × × × × X 0 n Spearman’s ρ × × X × X 0 n log n Kendall’s τ × × X × X 0 n log n CCA × X × × X 0 n KCCA [1] X X × × X 1 n(3) ACE [2] X × × X X 1 n MIC [18] X × × × X 1 n dCor [24] X X × × X 1 n(2) HSIC [5] X X × × × 1 n(2) CHSIC [15] X X X × × 1 n(2) RDC X X X X X 2 n log n",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Cost Pearson’s ρ × × × × X 0 n Spearman’s ρ × × X × X 0 n log n Kendall’s τ × × X × X 0 n log n CCA × X × × X 0 n KCCA [1] X X × × X 1 n(3) ACE [2] X × × X X 1 n MIC [18] X × × × X 1 n dCor [24] X X × × X 1 n(2) HSIC [5] X X × × × 1 n(2) CHSIC [15] X X X × × 1 n(2) RDC X X X X X 2 n log n",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "Cost Pearson’s ρ × × × × X 0 n Spearman’s ρ × × X × X 0 n log n Kendall’s τ × × X × X 0 n log n CCA × X × × X 0 n KCCA [1] X X × × X 1 n(3) ACE [2] X × × X X 1 n MIC [18] X × × × X 1 n dCor [24] X X × × X 1 n(2) HSIC [5] X X × × × 1 n(2) CHSIC [15] X X X × × 1 n(2) RDC X X X X X 2 n log n",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 23,
      "context" : "Cost Pearson’s ρ × × × × X 0 n Spearman’s ρ × × X × X 0 n log n Kendall’s τ × × X × X 0 n log n CCA × X × × X 0 n KCCA [1] X X × × X 1 n(3) ACE [2] X × × X X 1 n MIC [18] X × × × X 1 n dCor [24] X X × × X 1 n(2) HSIC [5] X X × × × 1 n(2) CHSIC [15] X X X × × 1 n(2) RDC X X X X X 2 n log n",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "Cost Pearson’s ρ × × × × X 0 n Spearman’s ρ × × X × X 0 n log n Kendall’s τ × × X × X 0 n log n CCA × X × × X 0 n KCCA [1] X X × × X 1 n(3) ACE [2] X × × X X 1 n MIC [18] X × × × X 1 n dCor [24] X X × × X 1 n(2) HSIC [5] X X × × × 1 n(2) CHSIC [15] X X X × × 1 n(2) RDC X X X X X 2 n log n",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 14,
      "context" : "Cost Pearson’s ρ × × × × X 0 n Spearman’s ρ × × X × X 0 n log n Kendall’s τ × × X × X 0 n log n CCA × X × × X 0 n KCCA [1] X X × × X 1 n(3) ACE [2] X × × X X 1 n MIC [18] X × × × X 1 n dCor [24] X X × × X 1 n(2) HSIC [5] X X × × × 1 n(2) CHSIC [15] X X X × × 1 n(2) RDC X X X X X 2 n log n",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 11,
      "context" : "Under normality assumptions (or large sample sizes), Bartlett’s approximation [12] can be used to show ( 2k+3 2 − n ) log ∏k i=1(1−ρ(2)i ) ∼ χ(2)k2 , where ρ1, .",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "HSIC and CHSIC use Gaussian kernels k(z, z′) = exp(−γ‖z−z‖(2)2) with γ−1 set to the euclidean distance median of each sample [5].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "MIC’s search-grid size is set to B(n) = n as recommended by the authors [18], although speed improvements are achieved when using lower values.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "KCCA is approximated using incomplete Cholesky decompositions as described in [1].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "We performed greedy feature selection via dependence maximization [21] on eight real-world datasets.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "matrix(y),2,function(u)rank(u)/length(u)),1) x <- s/ncol(x)*x%*%matrix(rnorm(ncol(x)*k),ncol(x)) y <- s/ncol(y)*y%*%matrix(rnorm(ncol(y)*k),ncol(y)) cancor(cbind(f(x),1),cbind(f(y),1))$cor[1] }",
      "startOffset" : 188,
      "endOffset" : 191
    } ],
    "year" : 2013,
    "abstractText" : "We introduce the Randomized Dependence Coefficient (RDC), a measure of nonlinear dependence between random variables of arbitrary dimension based on the Hirschfeld-Gebelein-Rényi Maximum Correlation Coefficient. RDC is defined in terms of correlation of random non-linear copula projections; it is invariant with respect to marginal distribution transformations, has low computational cost and is easy to implement: just five lines of R code, included at the end of the paper.",
    "creator" : null
  }
}