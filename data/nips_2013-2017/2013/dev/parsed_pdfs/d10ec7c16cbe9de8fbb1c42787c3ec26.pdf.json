{
  "name" : "d10ec7c16cbe9de8fbb1c42787c3ec26.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian inference as iterated random functions with  applications to sequential inference in graphical models",
    "authors" : [ "Arash A. Amini", "XuanLong Nguyen" ],
    "emails" : [ "aaamini@umich.edu", "xuanlong@umich.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The sequential posterior updates play a central role in many Bayesian inference procedures. As an example, in Bayesian inference one is interested in the posterior probability of variables of interest given the data observed sequentially up to a given time point. As a more specific example which provides the motivation for this work, in a sequential change point detection problem [1], the key quantity is the posterior probability that a change has occurred given the data observed up to present time. When the underlying probability model is complex, e.g., a large-scale graphical model, the calculation of such quantities in a fast and online manner is a formidable challenge. In such situations approximate inference methods are required – for graphical models, message-passing variational inference algorithms present a viable option [2, 3].\nIn this paper we propose to treat Bayesian inference in a complex model as a specific instance of an abstract system of iterated random functions (IRF), a concept that originally arises in the study of Markov chains and stochastic systems [4]. The key technical property of the proposed IRF formalism that enables the connection to Bayesian inference under conditionally independent sampling is the semigroup property, which shall be defined shortly in the sequel. It turns out that most exact and approximate Bayesian inference algorithms may be viewed as specific instances of an IRF system. The goal of this paper is to present a general convergence theory for the IRF with semigroup property. The theory is then applied to the analysis of exact and approximate message-passing inference algorithms, which arise in the context of distributed sequential change point problems using latent variable and directed graphical model as the underlying modeling framework.\nWe wish to note a growing literature on message-passing and sequential inference based on graphical modeling [5, 6, 7, 8]. On the other hand, convergence and error analysis of message-passing algorithms in graphical models is quite rare and challenging, especially for approximate algorithms, and they are typically confined to the specific form of belief propagation (sum-product) algorithm [9, 10, 11]. To the best of our knowledge, there is no existing work on the analysis of messagepassing inference algorithms for calculating conditional (posterior) probabilities for latent random\nvariables present in a graphical model. While such an analysis is a byproduct of this work, the viewpoint we put forward here that equates Bayesian posterior updates to a system of iterated random functions with semigroup property seems to be new and may be of general interest.\nThe paper is organized as follows. In Sections 2– 3, we introduce the general IRF system and provide our main result on its convergence. The proof is deferred to Section 5. As an example of the application of the result, we will provide a convergence analysis for an approximate sequential inference algorithm for the problem of multiple change point detection using graphical models. The problem setup and the results are discussed in Section 4."
    }, {
      "heading" : "2 Bayesian posterior updates as iterated random functions",
      "text" : "In this paper we shall restrict ourselves to multivariate distributions of binary random variables. To describe the general iteration, let Pd := P({0, 1}\nd) be the space of probability measures on {0, 1}d. The iteration under consideration recursively produces a random sequence of elements of Pd, starting from some initial value. We think of Pd as a subset of R 2d equipped with the ℓ1 norm (that is, the total variation norm for discrete probability measures). To simplify, let m := 2d, and for x ∈ Pd, index its coordinates as x = (x\n0, . . . , xm−1). For θ ∈ Rm+ , consider the function qθ : Pd → Pd, defined by\nqθ(x) := x⊙ θ\nxTθ (1)\nwhere xTθ = ∑ i x i θ i is the usual inner product on Rm and x ⊙ θ is pointwise multiplication with coordinates [x ⊙ θ]i := xiθi, for i = 0, 1, . . . ,m − 1. This function models the prior-toposterior update according to the Bayes rule. One can think of θ as the likelihood and x as the prior distribution (or the posterior in the previous stage) and qθ(x) as the (new) posterior based on the two. The division by xTθ can be thought of as the division by the marginal to make a valid probability vector. (See Example 1 below.)\nWe consider the following general iteration\nQn(x) = qθn(T (Qn−1(x)), n ≥ 1, Q0(x) = x, (2)\nfor some deterministic operator T : Pd → Pd and an i.i.d. random sequence {θn}n≥1 ⊂ R m + . By changing operator T , one obtains different iterative algorithms.\nOur goal is to find sufficient conditions on T and {θn} for the convergence of the iteration to an extreme point of Pd, which without loss of generality is taken to be e\n(0) := (1, 0, 0, . . . , 0). Standard techniques for proving the convergence of iterated random functions are usually based on showing some averaged-sense contraction property for the iteration function [4, 12, 13, 14], which in our case is qθn(T (·)). See [15] for a recent survey. These techniques are not applicable to our problem since qθn is not in general Lipschitz, in any suitable sense, precluding qθn(T (·)) from satisfying the aforementioned conditions.\nInstead, the functions {qθn} have another property which can be exploited to prove convergence; namely, they form a semi-group under pointwise multiplication,\nqθ⊙ θ′ = qθ ◦ qθ′ , θ, θ ′ ∈ Rm+ , (3)\nwhere ◦ denotes the composition of functions. If T is the identity, this property allows us to write Qn(x) = q⊙ n\ni=1θi (x) — this is nothing but the Bayesian posterior update equation, under condi-\ntionally independent sampling, while modifying T results in an approximate Bayesian inference procedure. Since after suitable normalization, ⊙ ni=1θi concentrates around a deterministic quantity, by the i.i.d. assumption on {θi}, this representation helps in determining the limit of {Qn(x)}. The main result of this paper, summarized in Theorem 1, is that the same conclusions can be extended to general Lipschitz maps T having the desired fixed point."
    }, {
      "heading" : "3 General convergence theory",
      "text" : "Consider a sequence {θn}n≥1 ⊂ R m + of i.i.d. random elements, where m = 2 d. Let θn = (θ0n, θ 1 n, . . . , θ m−1 n ) with θ 0 n = 1 for all n, and\nθ ∗ n := max i=1,2,...,m−1 θ i n. (4)\nThe normalization θ0n = 1 is convenient for showing convergence to e (0). This is without loss of generality, since qθ is invariant to scaling of θ, that is qθ = qβθ for any β > 0.\nAssume the sequence {log θ∗n} to be i.i.d. sub-Gaussian with mean ≤ −I∗ < 0 and sub-Gaussian norm ≤ σ∗ ∈ (0,∞). The sub-Gaussian norm in can be taken to be the ψ2 Orlicz norm (cf. [16, Section 2.2]), which we denote by ‖ · ‖ψ2 . By definition ‖Y ‖ψ2 := inf{C > 0 : Eψ2(|Y |/C) ≤ 1} where ψ2(x) := e x2 − 1.\nLet ‖ · ‖ denote the ℓ1 norm on R m. Consider the sequence {Qn(x)}n≥0 defined in (2) based on {θn} as above, an initial point x = (x 0, . . . , xm−1) ∈ Pd and a Lipschitz map T : Pd → Pd. Let LipT denote the Lipschitz constant of T , that is LipT := supx 6=y ‖T (x)− T (y)‖/‖x− y‖.\nOur main result regarding iteration (2) is the following.\nTheorem 1. Assume that L := LipT ≤ 1 and that e (0) is a fixed point of T . Then, for all n ≥ 0, and ε > 0,\n‖Qn(x)− e (0)‖ ≤ 2\n1− x0 x0 ( Le−I∗+ε )n (5)\nwith probability at least 1− exp(−c nε2/σ2∗), for some absolute constant c > 0.\nThe proof of Theorem 1 is outlined in Section 5. Our main application of the theorem will be to the study of convergence of stopping rules for a distributed multiple change point problem endowed with latent variable graphical models. Before stating that problem, let us consider the classical (single) change point problem first, and show how the theorem can be applied to analyze the convergence of the optimal Bayes rule.\nExample 1. In the classical Bayesian change point problem [1], one observes a sequence {X1, X2, X3 . . . } of independent data points whose distributions change at some random time λ. More precisely, given λ = k, X1, X2, . . . , Xk−1 are distributed according to g, and Xk+1, Xk+2, . . . according to f . Here, f and g are densities with respect to some underlying measure. One also assumes a prior π on λ, usually taken to be geometric. The goal is to find a stopping rule τ which can predict λ based on the data points observed so far. It is well-known that a rule based on thresholding the posterior probability of λ is optimal (in a Neyman-Pearson sense). To be more specific, let Xn := (X1, X2, . . . , Xn) collect the data up to time n and let γn[n] := P(λ ≤ n|Xn) be the posterior probability of λ having occurred before (or at) time n. Then, the Shiryayev rule\nτ := inf{n ∈ N : γn[n] ≥ 1− α} (6)\nis known to asymptotically have the least expected delay, among all stopping rules with false alarm probability bounded by α.\nTheorem 1 provides a way to quantify how fast the posterior γn[n] approaches 1, once the change point has occurred, hence providing an estimate of the detection delay, even for finite number of samples. We should note that our approach here is somewhat independent of the classical techniques normally used for analyzing stopping rule (6). To cast the problem in the general framework of (2), let us introduce the binary variable Zn := 1{λ ≤ n}, where 1{·} denotes the indicator of an event. Let Qn be the (random) distribution of Z n given Xn, in other words,\nQn := ( P(Zn = 1|Xn), P(Zn = 0|Xn)).\nSince γn[n] = P(Z = 1|Xn), convergence of γn[n] to 1 is equivalent to the convergence of Qn to e (0) = (1, 0). We have\nP (Zn|Xn) ∝Zn P (Z n, Xn|Xn−1) = P (Xn|Zn)P (Zn|Xn−1). (7)\nNote that P (Xn|Zn = 1) = f(Xn) and P (Xn|Zn = 0) = g(Xn). Let θn := ( 1, g(X n) f(Xn) ) and\nRn−1 := ( P(Zn = 1|Xn−1), P(Zn = 0|Xn−1)).\nThen, (7) implies that Qn can be obtained by pointwise multiplication of Rn−1 by f(X n)θn and normalization to make a probability vector. Alternatively, we can multiply by θn, since the procedure is scale-invariant, that is, Qn = qθn(Rn−1) using definition (1). It remains to express Rn−1 in terms of Qn−1. This can be done by using the Bayes rule and the fact that P (X\nn−1|λ = k) is the same for k ∈ {n, n+ 1, . . . }. In particular, after some algebra (see [17]), one arrives at\nγn−1[n] = π(n)\nπ[n− 1]c +\nπ[n]c\nπ[n− 1]c γn−1[n− 1], (8)\nwhere γk[n] := P(λ ≤ n|Xk), π(n) is the prior on λ evaluated at time n, and π[k]c :=∑∞ i=k+1 π(i). For the geometric prior with parameter ρ ∈ [0, 1], we have π(n) := (1− ρ) n−1ρ and π[k]c = ρk. The above recursion then simplifies to γn−1[n] = ρ+ (1− ρ)γn−1[n− 1]. Expressing in terms of Rn−1 and Qn−1, the recursion reads\nRn−1 = T (Qn−1), where T ((x1 x0 )) = ρ (1 0 ) + (1− ρ) (x1 x0 ) .\nIn other words, T (x) = ρe(0) + (1− ρ)x for x ∈ P2.\nThus, we have shown that an iterative algorithm for computing γn[n] (hence determining rule (6)), can be expressed in the form of (2) for appropriate choices of {θn} and operator T . Note that T in this case is Lipschitz with constant 1− ρ which is always guaranteed to be ≤ 1.\nWe can now use Theorem 1 to analyze the convergence of γn[n]. Let us condition on λ = k + 1, that is, we assume that the change point has occurred at time k+1. Then, the sequence {Xn}n≥k+1 is distributed according to f , and we have Eθ∗n = ∫ f log g f = −I , where I is the KL divergence between densities f and g. Noting that ‖Qn − e (0)‖ = 2(1 − γn[n]), we immediately obtain the following corollary.\nCorollary 1. Consider Example 1 and assume that log(g(X)/f(X)), where X ∼ f , is subGaussian with sub-Gaussian norm ≤ σ. Let I := ∫ f log f\ng . Then, conditioned on λ = k + 1,\nwe have for n ≥ 1,\n∣∣γn+k[n+ k]− 1 ∣∣ ≤ [ (1− ρ)e−I+ε ]n( 1 γk[k] − 1 )\nwith probability at least 1− exp(−c nε2/σ2)."
    }, {
      "heading" : "4 Multiple change point problem via latent variable graphical models",
      "text" : "We now turn to our main application for Theorem 1, in the context of a multiple change point problem. In [18], graphical model formalism is used to extend the classical change point problem (cf. Example 1) to cases where multiple distributed latent change points are present. Throughout this section, we will use this setup which we now briefly sketch.\nOne starts with a networkG = (V,E) of d sensors or nodes, each associated with a change point λj . Each node j observes a private sequence of measurements Xj = (X 1 j , X 2 j , . . . ) which undergoes a change in distribution at time λj , that is,\nX1j , X 2 j , . . . , X k−1 j | λj = k iid ∼ gj , X k j , X k+1 j , · · · | λj = k iid ∼ fj,\nfor densities gj and fj (w.r.t. some underlying measure). Each connected pair of nodes share an additional sequence of measurements. For example, if nodes s1 and s2 are connected, that is, e = (s1, s2) ∈ E, then they both observe Xe = (X 1 e , X 2 e , . . . ). The shared sequence undergoes a change in distribution at some point depending on λs1 and λs2 . More specifically, it is assumed that the earlier of the two change points causes a change in the shared sequence, that is, the distribution of Xe conditioned on (λs1 , λs2) only depends on λe := λs1 ∧ λs2 , the minimum of the two, i.e.,\nX1e , X 2 e , . . . , X k e | λe = k iid ∼ ge, X k+1 e , X k+2 e , · · · | λe = k iid ∼ fe.\nLetting λ∗ := {λj}j∈V and X n ∗ = {X n j ,X n e }j∈V,e∈E , we can write the joint density of all random variables as\nP (λ∗,X n ∗ ) =\n∏\nj∈V\nπj(λj) ∏\nj∈V\nP (Xnj |λj) ∏\ne∈E\nP (Xne |λs1 , λs2 ). (9)\nwhere πj is the prior on λj , which we assume to be geometric with parameter ρj . Network G induces a graphical model [2] which encodes the factorization (9) of the joint density. (cf. Fig. 1)\nSuppose now that each node j wants to detect its change point λj , with minimum expected delay, while maintaining a false alarm probability at most α. Inspired by the classical change point problem, one is interested in computing the posterior probability that the change point has occurred up to now, that is,\nγnj [n] := P(λj ≤ n | X n ∗ ). (10)\nThe difference with the classical setting is the conditioning is done on all the data in the network (up to time n). It is easy to verify that the natural stopping rule\nτj = inf{n ∈ N : γ n j [n] ≥ 1− α}\nsatisfy the false alarm constraint. It has also been shown that this rule is asymptotically optimal in terms of expected detection delay. Moreover, an algorithm based on the well-known sum-product [2] has been proposed, which allows the nodes to compute their posterior probabilities 10 by messagepassing. The algorithm is exact when G is a tree, and scales linearly in the number of nodes. More precisely, at time n, the computational complexity is O(nd). The drawback is the linear dependence on n, which makes the algorithm practically infeasible if the change points model rare events (where n could grow large before detecting the change.)\nIn the next section, we propose an approximate message passing algorithm which has computational complexity O(d), at each time step. This circumvents the drawback of the exact algorithm and allows for indefinite run times. We then show how the theory developed in Section 3 can be used to provide convergence guarantees for this approximate algorithm, as well as the exact one."
    }, {
      "heading" : "4.1 Fast approximate message-passing (MP)",
      "text" : "We now turn to an approximate message-passing algorithm which, at each time step, has computational complexity O(d). The derivation is similar to that used for the iterative algorithm in Example 1. Let us define binary variables\nZnj = 1{λj ≤ n}, Z n ∗ = (Z n 1 , . . . , Z n d ). (11)\nThe idea is to computeP (Zn∗ |X n ∗ ) recursively based onP (Z n−1 ∗ |X n−1 ∗ ). By Bayes rule,P (Z n ∗ |X n ∗ ) is proportional in Zn∗ to P (Z n ∗ , X n ∗ |X n−1 ∗ ) = P (X n ∗ |Z n ∗ )P (Z n ∗ |X n−1 ∗ ), hence\nP (Zn∗ |X n ∗ ) ∝Zn∗\n[ ∏\nj∈V\nP (Xnj |Z n j )\n∏\n{i,j}∈E\nP (Xnij |Z n i , Z n j ) ] P (Zn∗ |X n−1 ∗ ), (12)\nwhere we have used the fact that given Zn∗ , X n ∗ is independent of X n−1 ∗ . To simplify notation, let us extend the edge set to Ẽ := E∪{{j} : j ∈ V }. This allows us to treat the private data of node j, i.e., Xj , as shared data of a self-loop in the extended graph (V, Ẽ). Let ue(z; ξ) := [ge(ξ)] 1−z [fe(ξ]) z\nfor e ∈ Ẽ, z ∈ {0, 1}. Then, for i 6= j,\nP (Xnj |Z n j ) = uj(Z n j ;X n j ), P (X n ij |Z n i , Z n j ) = uij(Z n i ∨ Z n j ;X n ij). (13)\nIt remains to express P (Zn∗ |X n−1 ∗ ) in terms of P (Z n−1 ∗ |X n−1 ∗ ). It is possible to do this, exactly, at a cost of O(2|V |). For brevity, we omit the exact expression. (See Lemma 1 for some details.) We term the algorithm that employs the exact relationship, the “exact algorithm”.\nIn practice, however, the exponential complexity makes the exact recursion of little use for large networks. To obtain a fast algorithm (i.e., O(poly(d)), we instead take a mean-field type approximation:\nP (Zn∗ |X n−1 ∗ ) ≈\n∏\nj∈V\nP (Znj |X n−1 ∗ ) =\n∏\nj∈V\nν(Znj ; γ n−1 j [n]), (14)\nwhere ν(z;β) := βz(1− β)1−z . That is, we approximate a multivariate distribution by the product of its marginals. By an argument similar to that used to derive (8), we can obtain a recursion for the marginals,\nγn−1j [n] = πj(n)\nπj [n− 1]c +\nπj [n] c\nπj [n− 1]c γn−1j [n− 1], (15)\nwhere we have used the notation introduced earlier in (8). Thus, at time n, the RHS of (14) is known based on values computed at time n − 1 (with initial value γ0j [0] = 0, j ∈ V ). Inserting this RHS into (12) in place of P (Zn∗ |X n−1 ∗ ), we obtain a graphical model in variables Z n ∗ (instead of λ∗) which has the same form as (9) with ν(Znj ; γ n−1 j [n]) playing the role of the prior π(λj).\nIn order to obtain the marginals γnj [n] = P (Z n j = 1|X n ∗ ) and γ n ij [n] with respect to the approximate version of the joint distribution P (Zn∗ , X n ∗ |X n−1 ∗ ), we need to marginalize out the latent variables Znj ’s, for which a standard sum-product algorithm can be applied (see [2, 3, 18]). The message update equations are similar to those in [18]; the difference is that the messages are now binary and do not grow in size with n."
    }, {
      "heading" : "4.2 Convergence of MP algorithms",
      "text" : "We now turn to the analysis of the approximate algorithm introduced in Section 4.1. In particular, we will look at the evolution of {P̃ (Zn∗ |X n ∗ )}n∈N as a sequence of probability distribution on {0, 1} d. Here, P̃ signifies that this sequence is an approximation. In order to make a meaningful comparison, we also look at the algorithm which computes the exact sequence {P (Zn∗ |X n ∗ )}n∈N, recursively. As mentioned before, this we will call the “exact algorithm”, the details of which are not of concern to us at this point (cf. Prop. 1 for these details.)\nRecall that we take P̃ (Zn∗ |X n ∗ ) and P (Z n ∗ |X n ∗ ), as distributions for Z n ∗ , to be elements of Pd ⊂ R m. To make this correspondence formal and the notation simplified, we use the symbol :≡ as follows\nỹn :≡ P̃ (Z n ∗ |X n ∗ ), yn :≡ P (Z n ∗ |X n ∗ ) (16)\nwhere now ỹn, yn ∈ Pd. Note that ỹn and yn are random elements of Pd, due the randomness of X n ∗ . We have the following description. Proposition 1. The exact and approximate sequences, {yn} and {ỹn}, follow general iteration (2) with the same random sequence {θn}, but with different deterministic operators T , denoted respectively with Tex and Tap. Tex is linear and given by a Markov transition kernel. Tap is a polynomial map of degree d. Both maps are Lipschitz and we have\nLipTex ≤ Lρ := ( 1− d∏\nj=1\nρj ) , LipTap ≤ Kρ := d∑\nj=1\n(1 − ρj). (17)\nDetailed descriptions of the sequence {θn} and the operators Tex and Tap are given in [17]. As suggested by Theorem 1, a key assumption for the convergence of the approximate algorithm will be Kρ ≤ 1. In contrast, we always have Lρ ≤ 1.\nRecall that {λj} are the change points and their priors are geometric with parameters {ρj}. We analyze the algorithms, once all the change points have happened. More precisely, we condition on Mn0 := {maxj λj ≤ n0} for some n0 ∈ N. Then, one expects the (joint) posterior of Z n ∗ to contract to the point Z∞j = 1, for all j ∈ V . In the vectorial notation, we expect both {ỹn} and {yn} to converge to e (0). Theorem 2 below quantifies this convergence in ℓ1 norm (equivalently, total variation for measures).\nRecall pre-change and post-change densities ge and fe, and let Ie denote their KL divergence, that is, Ie := ∫ fe log(fe/ge). We will assume that\nYe := log(ge(X)/fe(X)) with X ∼ fe (18)\nis sub-Gaussian, for all e ∈ Ẽ, where Ẽ is extended edge notation introduced in Section 4.1. The choice X ∼ fe is in accordance with conditioning on Mn0 . Note that EYe = −Ie < 0. We define\nσmax := max e∈Ẽ ‖Ye‖ψ2 , Imin := min e∈Ẽ\nIe, I∗(κ) := Imin − κσmax √ logD..\nwhere D := |V |+ |E|. Theorem 1 and Lemma 1 give us the following. (See [17] for the proof.)\nTheorem 2. There exists an absolute constant κ > 0, such that if I∗(κ) > 0, the exact algorithm converges at least geometrically w.h.p., that is, for all n ≥ 1,\n‖yn+n0 − e (0)‖ ≤ 2 1− yn0 yn0\n( Lρe −I∗(κ)+ε )n\n(19)\nwith probability at least 1 − exp [ −c nε2/(σ2maxD 2 logD) ] , conditioned on Mn0 . If in addition, Kρ ≤ 1, the approximate algorithm also converges at least geometrically w.h.p., i.e., for all n ≥ 1,\n‖ỹn+n0 − e (0)‖ ≤ 2 1− ỹn0 ỹn0\n( Kρe −I∗(κ)+ε )n\n(20)\nwith the same (conditional) probability as the exact algorithm."
    }, {
      "heading" : "4.3 Simulation results",
      "text" : "We present some simulation results to verify the effectiveness of the proposed approximation algorithm in estimating the posterior probabilities γnj [n]. We consider a star graph on d = 4 nodes. This is the subgraph on nodes {1, 2, 3, 4} in Fig. 1. Conditioned on the change points λ∗, all data sequences X∗ are assumed Gaussian with variance 1, pre-change mean 1 and post-change mean zero. All priors are geometric with ρj = 0.1. We note that higher values of ρj yield even faster convergence in the simulations, but we omit these figures due to space constraints. Fig. 1 illustrates typical examples of posterior paths n 7→ γnj [n], for both the exact and approximate MP algorithms. One can observe that the approximate path often closely follows the exact one. In some cases, they might deviate for a while, but as suggested by Theorem 2, they approach one another quickly, once the change points have occurred.\nFrom the theorem and triangle inequality, it follows that under I∗(κ) > 0 and Kρ ≤ 1, ‖yn − ỹn‖ converges to zero, at least geometrically w.h.p. This gives some theoretical explanation for the good tracking behavior of approximate algorithm as observed in Fig. 1."
    }, {
      "heading" : "5 Proof of Theorem 1",
      "text" : "For x ∈ Rm (including Pd), we write x = (x 0, x̃) where x̃ = (x1, . . . , xm−1). Recall that e(0) = (1, 0, . . . , 0) and ‖x‖ = ∑m−1\ni=0 |xi|. For x ∈ Pd, we have 1− x 0 = ‖x̃‖, and\n‖x− e(0)‖ = ‖(x0 − 1, x̃)‖ = 1− x0 + ‖x̃‖ = 2(1− x0). (21)\nFor θ = (θ0, θ̃) ∈ Rm+ , let\nθ ∗ := ‖θ̃‖∞ = max i=1,...,m−1 θ i, θ† :=\n( θ 0, (θ∗L)1m−1 ) ∈ Rm+ (22)\nwhere 1m−1 is a vector in R m−1 whose coordinates are all ones. We start by investigating how ‖qθ(x) − e (0)‖ varies as a function of ‖x− e(0)‖. Lemma 1. For L ≤ 1, θ∗ > 0, and θ0 = 1,\nN := sup x,y∈Pd,\n‖x−e(0)‖≤L‖y−e(0)‖\n‖qθ(x)− e (0)‖ ‖qθ†(y)− e(0)‖ = 1; (23)\nLemma 1 is proved in [17]. We now proceed to the proof of the theorem. Recall that T : Pd → Pd is an L-Lipschitz map, and that e(0) is a fixed point of T , that is, T (e(0)) = e(0). It follows that for any x ∈ Pd, ‖T (x)− e (0)‖ ≤ L‖x− e(0)‖. Applying Lemma 1, we get\n‖qθ(T (x))− e (0)‖ ≤ ‖qθ†(x)− e (0)‖ (24)\nfor θ ∈ Rm+ with θ 0 = 1, and x ∈ Pd. (This holds even if θ ∗ = 0 where both sides are zero.)\nRecall the sequence {θn}n≥1 used in defining functions {Qn} accroding to (2), and the assumption that θ0n = 1, for all n ≥ 1. Inequality (24) is key in allowing us to peel operator T , and bring successive elements of {qθn} together. Then, we can exploit the semi-group property (3) on adjacent elements of {qθn}.\nTo see this, for each θn, let θ ∗ n and θ † n be defined as in (22). Applying (24) with x replaced with Qn−1(x), and θ with θn, we can write\n‖Qn(x) − e (0)‖ ≤ ‖q\nθ † n\n(Qn−1(x)) − e (0)‖ (by (24))\n= ‖q θ † n\n(qθn−1(T (Qn−2(x)))) − e (0)‖\n= ‖q θ † n⊙ θn−1\n(T (Qn−2(x)))) − e (0)‖ (by semi-group property (3))\nWe note that (θ†n ⊙ θn−1) ∗ = Lθ∗nθ ∗ n−1 and\n(θ†n ⊙ θn−1) † = ( 1, L(θ†n ⊙ θn−1) ∗ 1m−1 ) = ( 1, L2θ∗nθ ∗ n−11m−1 ) .\nHere, ∗ and † act on a general vector in the sense of (22). Applying (24) once more, we get\n‖Qn(x) − e (0)‖ ≤ ‖q(1,L2θ∗\nn θ∗ n−11m−1)\n(Qn−2(x)) − e (0)‖.\nThe pattern is clear. Letting ηn := L n ∏n k=1 θ ∗ k, we obtain by induction\n‖Qn(x) − e (0)‖ ≤ ‖q(1,ηn1m−1)(Q0(x))− e (0)‖. (25)\nRecall that Q0(x) := x. Moreover,\n‖q(1,ηn1m−1)(x)− e (0)‖ = 2 ( 1− [q(1,ηn1m−1)(x)] 0 ) = 2 ( 1− gηn(x 0) )\n(26)\nwhere the first inequality is by (21), and the second is easily verified by noting that all the elements of (1, ηn1m−1), except the first, are equal. Putting (25) and (26) together with the bound 1−gθ(r) = θ(1−r) r+θ(1−r) ≤ θ 1−r r , which holds for θ > 0 and r ∈ (0, 1], we obtain ‖Qn(x) − e (0)‖ ≤ 2ηn 1−x0 x0 . By sub-Gaussianity assumption on {log θ∗k}, we have\nP ( 1 n n∑\nk=1\nlog θ∗k − E log θ ∗ 1 > ε ) ≤ exp(−c nε2/σ2∗), (27)\nfor some absolute constant c > 0. (Recall that σ∗ is an upper bound on the sub-Gaussian norm ‖ logθ∗1‖ψ2 .) On the complement of the event in 27, we have ∏n k=1 θ ∗ k ≤ e\nn(−I∗+ε), which completes the proof."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by NSF grants CCF-1115769 and OCI-1047871."
    } ],
    "references" : [ {
      "title" : "Optimal Stopping Rules",
      "author" : [ "A.N. Shiryayev" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : "Morgan Kaufmann",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Graphical models",
      "author" : [ "M.I. Jordan" ],
      "venue" : "Statistical Science, 19:140–155",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Iterated random functions",
      "author" : [ "P. Diaconis", "D. Freedman" ],
      "venue" : "SIAM Rev., 41(1):45–76",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Inference with minimum communication: a decision-theoretic variational approach",
      "author" : [ "O.P. Kreidl", "A. Willsky" ],
      "venue" : "NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Distributed fusion in sensor networks: A graphical models perspective",
      "author" : [ "M. Cetin", "L. Chen", "J.W. Fisher III", "A. Ihler", "R. Moses", "M. Wainwright", "A. Willsky" ],
      "venue" : "IEEE Signal Processing Magazine, July:42–55",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Message-passing sequential detection of multiple change points in networks",
      "author" : [ "X. Nguyen", "A.A. Amini", "R. Rajagopal" ],
      "venue" : "ISIT",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A graphical model representation of the track-oriented multiple hypothesis tracker",
      "author" : [ "A. Frank", "P. Smyth", "A. Ihler" ],
      "venue" : "In Proceedings, IEEE Statistical Signal Processing (SSP)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Loopy belief propagation: Convergence and effects of message errors",
      "author" : [ "A.T. Ihler", "J.W. Fisher III", "A.S. Willsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Accuracy bounds for belief propagation",
      "author" : [ "Alexander Ihler" ],
      "venue" : "In Proceedings of UAI 2007,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "Convergence analysis of reweighted sumproduct algorithms",
      "author" : [ "T.G. Roosta", "M. Wainwright", "S.S. Sastry" ],
      "venue" : "IEEE Trans. Signal Processing, 56(9):4293–4305",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Locally contractive iterated function systems",
      "author" : [ "D. Steinsaltz" ],
      "venue" : "Ann. Probab., 27(4):1952–1979",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A central limit theorem for iterated random functions",
      "author" : [ "W.B. Wu", "M. Woodroofe" ],
      "venue" : "J . Appl. Probab., 37(3):748–755",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Limit theorems for iterated random functions",
      "author" : [ "W.B. Wu", "X. Shao" ],
      "venue" : "J. Appl. Probab.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "A survey of average contractive iterated function systems",
      "author" : [ "Ö. Stenflo" ],
      "venue" : "J. Diff. Equa. and Appl., 18(8):1355–1380",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Weak Convergence and Empirical Processes: With Applications to Statistics",
      "author" : [ "A. van der Vaart", "J. Wellner" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1996
    }, {
      "title" : "Sequential detection of multiple change points in networks: a graphical model approach",
      "author" : [ "A.A. Amini", "X. Nguyen" ],
      "venue" : "IEEE Transactions on Information Theory, 59(9):5824–5841",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "As a more specific example which provides the motivation for this work, in a sequential change point detection problem [1], the key quantity is the posterior probability that a change has occurred given the data observed up to present time.",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "In such situations approximate inference methods are required – for graphical models, message-passing variational inference algorithms present a viable option [2, 3].",
      "startOffset" : 159,
      "endOffset" : 165
    }, {
      "referenceID" : 2,
      "context" : "In such situations approximate inference methods are required – for graphical models, message-passing variational inference algorithms present a viable option [2, 3].",
      "startOffset" : 159,
      "endOffset" : 165
    }, {
      "referenceID" : 3,
      "context" : "In this paper we propose to treat Bayesian inference in a complex model as a specific instance of an abstract system of iterated random functions (IRF), a concept that originally arises in the study of Markov chains and stochastic systems [4].",
      "startOffset" : 239,
      "endOffset" : 242
    }, {
      "referenceID" : 4,
      "context" : "We wish to note a growing literature on message-passing and sequential inference based on graphical modeling [5, 6, 7, 8].",
      "startOffset" : 109,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "We wish to note a growing literature on message-passing and sequential inference based on graphical modeling [5, 6, 7, 8].",
      "startOffset" : 109,
      "endOffset" : 121
    }, {
      "referenceID" : 6,
      "context" : "We wish to note a growing literature on message-passing and sequential inference based on graphical modeling [5, 6, 7, 8].",
      "startOffset" : 109,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "We wish to note a growing literature on message-passing and sequential inference based on graphical modeling [5, 6, 7, 8].",
      "startOffset" : 109,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "On the other hand, convergence and error analysis of message-passing algorithms in graphical models is quite rare and challenging, especially for approximate algorithms, and they are typically confined to the specific form of belief propagation (sum-product) algorithm [9, 10, 11].",
      "startOffset" : 269,
      "endOffset" : 280
    }, {
      "referenceID" : 9,
      "context" : "On the other hand, convergence and error analysis of message-passing algorithms in graphical models is quite rare and challenging, especially for approximate algorithms, and they are typically confined to the specific form of belief propagation (sum-product) algorithm [9, 10, 11].",
      "startOffset" : 269,
      "endOffset" : 280
    }, {
      "referenceID" : 10,
      "context" : "On the other hand, convergence and error analysis of message-passing algorithms in graphical models is quite rare and challenging, especially for approximate algorithms, and they are typically confined to the specific form of belief propagation (sum-product) algorithm [9, 10, 11].",
      "startOffset" : 269,
      "endOffset" : 280
    }, {
      "referenceID" : 3,
      "context" : "Standard techniques for proving the convergence of iterated random functions are usually based on showing some averaged-sense contraction property for the iteration function [4, 12, 13, 14], which in our case is qθn(T (·)).",
      "startOffset" : 174,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : "Standard techniques for proving the convergence of iterated random functions are usually based on showing some averaged-sense contraction property for the iteration function [4, 12, 13, 14], which in our case is qθn(T (·)).",
      "startOffset" : 174,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "Standard techniques for proving the convergence of iterated random functions are usually based on showing some averaged-sense contraction property for the iteration function [4, 12, 13, 14], which in our case is qθn(T (·)).",
      "startOffset" : 174,
      "endOffset" : 189
    }, {
      "referenceID" : 13,
      "context" : "Standard techniques for proving the convergence of iterated random functions are usually based on showing some averaged-sense contraction property for the iteration function [4, 12, 13, 14], which in our case is qθn(T (·)).",
      "startOffset" : 174,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "In the classical Bayesian change point problem [1], one observes a sequence {X(1), X(2), X(3) .",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "In [18], graphical model formalism is used to extend the classical change point problem (cf.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "Network G induces a graphical model [2] which encodes the factorization (9) of the joint density.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "Moreover, an algorithm based on the well-known sum-product [2] has been proposed, which allows the nodes to compute their posterior probabilities 10 by messagepassing.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "In order to obtain the marginals γ j [n] = P (Z n j = 1|X n ∗ ) and γ n ij [n] with respect to the approximate version of the joint distribution P (Z ∗ , X n ∗ |X n−1 ∗ ), we need to marginalize out the latent variables Z j ’s, for which a standard sum-product algorithm can be applied (see [2, 3, 18]).",
      "startOffset" : 291,
      "endOffset" : 301
    }, {
      "referenceID" : 2,
      "context" : "In order to obtain the marginals γ j [n] = P (Z n j = 1|X n ∗ ) and γ n ij [n] with respect to the approximate version of the joint distribution P (Z ∗ , X n ∗ |X n−1 ∗ ), we need to marginalize out the latent variables Z j ’s, for which a standard sum-product algorithm can be applied (see [2, 3, 18]).",
      "startOffset" : 291,
      "endOffset" : 301
    }, {
      "referenceID" : 16,
      "context" : "In order to obtain the marginals γ j [n] = P (Z n j = 1|X n ∗ ) and γ n ij [n] with respect to the approximate version of the joint distribution P (Z ∗ , X n ∗ |X n−1 ∗ ), we need to marginalize out the latent variables Z j ’s, for which a standard sum-product algorithm can be applied (see [2, 3, 18]).",
      "startOffset" : 291,
      "endOffset" : 301
    }, {
      "referenceID" : 16,
      "context" : "The message update equations are similar to those in [18]; the difference is that the messages are now binary and do not grow in size with n.",
      "startOffset" : 53,
      "endOffset" : 57
    } ],
    "year" : 2013,
    "abstractText" : "We propose a general formalism of iterated random functions with semigroup property, under which exact and approximate Bayesian posterior updates can be viewed as specific instances. A convergence theory for iterated random functions is presented. As an application of the general theory we analyze convergence behaviors of exact and approximate message-passing algorithms that arise in a sequential change point detection problem formulated via a latent variable directed graphical model. The sequential inference algorithm and its supporting theory are illustrated by simulated examples.",
    "creator" : null
  }
}