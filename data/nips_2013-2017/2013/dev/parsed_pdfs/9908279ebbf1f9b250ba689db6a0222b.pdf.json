{
  "name" : "9908279ebbf1f9b250ba689db6a0222b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Robust Sparse Principal Component Regression under the High Dimensional Elliptical Model",
    "authors" : [ "Fang Han", "Han Liu" ],
    "emails" : [ "fhan@jhsph.edu", "hanliu@princeton.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Principal component regression (PCR) has been widely used in statistics for years (Kendall, 1968). Take the classical linear regression with random design for example. Let x1, . . . ,xn ∈ Rd be n independent realizations of a random vector X ∈ Rd with mean 0 and covariance matrix Σ. The classical linear regression model and simple principal component regression model can be elaborated as follows:\n(Classical linear regression model) Y = Xβ + ;\n(Principal Component Regression Model) Y = αXu1 + , (1.1)\nwhere X = (x1, . . . ,xn)T ∈ Rn×d, Y ∈ Rn, ui is the i-th leading eigenvector of Σ, and ∈ Nn(0, σ\n2Id) is independent of X, β ∈ Rd and α ∈ R. Here Id ∈ Rd×d is the identity matrix. The principal component regression then can be conducted in two steps: First we obtain an estimator û1 of u1; Secondly we project the data in the direction of û1 and solve a simple linear regression in estimating α.\nBy checking Equation (1.1), it is easy to observe that the principal component regression model is a subset of the general linear regression (LR) model with the constraint that the regression coefficient β is proportional to u1. There has been a lot of discussions on the advantage of principal component regression over classical linear regression. In low dimensional settings, Massy (1965) pointed out that principal component regression can be much more efficient in handling collinearity among predictors compared to the linear regression. More recently, Cook (2007) and Artemiou and Li (2009) argued that principal component regression has potential to play a more important role. In particular, letting ûj be the j-th leading eigenvector of the sample covariance matrix Σ̂ of x1, . . . ,xn,\nArtemiou and Li (2009) show that under mild conditions with high probability the correlation between the response Y and Xûi is higher than or equal to the correlation between Y and Xûj when i < j. This indicates, although not rigorous, there is possibility that principal component regression can borrow strength from the low rank structure of Σ, which motivates our work.\nEven though the statistical performance of principal component regression in low dimensions is not fully understood, there is even less analysis on principal component regression in high dimensions where the dimension d can be even exponentially larger than the sample size n. This is partially due to the fact that estimating the leading eigenvectors of Σ itself has been difficult enough. For example, Johnstone and Lu (2009) show that, even under the Gaussian model, when d/n → γ for some γ > 0, there exist multiple settings under which û1 can be an inconsistent estimator of u1. To attack this “curse of dimensionality”, one solution is adding a sparsity assumption on u1, leading to various versions of the sparse PCA. See, Zou et al. (2006); d’Aspremont et al. (2007); Moghaddam et al. (2006), among others. Under the (sub)Gaussian settings, minimax optimal rates are being established in estimating u1, . . . ,um (Vu and Lei, 2012; Ma, 2013; Cai et al., 2013). Very recently, Han and Liu (2013b) relax the Gaussian assumption in conducting a scale invariant version of the sparse PCA (i.e., estimating the leading eigenvector of the correlation instead of the covariance matrix). However, it can not be easily applied to estimate u1 and the rate of convergence they proved is not the parametric rate.\nThis paper improves upon the aforementioned results in two directions. First, with regard to the classical principal component regression, under a double asymptotic framework in which d is allowed to increase with n, by borrowing the very recent development in principal component analysis (Vershynin, 2010; Lounici, 2012; Bunea and Xiao, 2012), we for the first time explicitly show the advantage of principal component regression over the classical linear regression. We explicitly confirm the following two advantages of principal component regression: (i) Principal component regression is insensitive to collinearity, while linear regression is very sensitive to; (ii) Principal component regression can utilize the low rank structure of the covariance matrix Σ, while linear regression cannot.\nSecondly, in high dimensions where d can increase much faster, even exponentially faster, than n, we propose a robust method in conducting (sparse) principal component regression under a nonGaussian elliptical model. The elliptical distribution is a semiparametric generalization to the Gaussian, relaxing the light tail and zero tail dependence constraints, but preserving the symmetry property. We refer to Klüppelberg et al. (2007) for more details. This distribution family includes many well known distributions such as multivariate Gaussian, rank deficient Gaussian, t, logistic, and many others. Under the elliptical model, we exploit the result in Han and Liu (2013a), who showed that by utilizing a robust covariance matrix estimator, the multivariate Kendall’s tau, we can obtain an estimator ũ1, which can recover u1 in the optimal parametric rate as shown in Vu and Lei (2012). We then exploit ũ1 in conducting principal component regression and show that the obtained estimator β̌ can estimate β in the optimal √ s log d/n rate. The optimal rate in estimating u1 and β, combined with the discussion in the classical principal component regression, indicates that the proposed method has potential to handle high dimensional complex data and has its advantage over high dimensional linear regression methods, such as ridge regression and lasso. These theoretical results are also backed up by numerical experiments on both synthetic and real world equity data."
    }, {
      "heading" : "2 Classical Principal Component Regression",
      "text" : "This section is devoted to the discussion on the advantage of classical principal component regression over the classical linear regression. We start with a brief introduction of notations. Let M = [Mij ] ∈ Rd×d and v = (v1, ..., vd)T ∈ Rd. We denote vI to be the subvector of v whose entries are indexed by a set I . We also denote MI,J to be the submatrix of M whose rows are indexed by I and columns are indexed by J . Let MI∗ and M∗J be the submatrix of M with rows indexed by I , and the submatrix of M with columns indexed by J . Let supp(v) := {j : vj 6= 0}. For 0 < q <∞, we define the `0, `q and `∞ vector norms as\n‖v‖0 := card(supp(v)), ‖v‖q := ( d∑ i=1 |vi|q)1/q and ‖v‖∞ := max 1≤i≤d |vi|.\nLet Tr(M) be the trace of M. Let λj(M) be the j-th largest eigenvalue of M and Θj(M) be the corresponding leading eigenvector. In particular, we let λmax(M) := λ1(M) and λmin(M) :=\nλd(M). We define Sd−1 := {v ∈ Rd : ‖v‖2 = 1} to be the d-dimensional unit sphere. We define the matrix `max norm and `2 norm as ‖M‖max := max{|Mij |} and ‖M‖2 := supv∈Sd−1 ‖Mv‖2. We define diag(M) to be a diagonal matrix with [diag(M)]jj = Mjj for j = 1, . . . , d. We denote\nvec(M) := (MT∗1, . . . ,M T ∗d) T . For any two sequence {an} and {bn}, we denote an c,C bn if there exist two fixed constants c, C such that c ≤ an/bn ≤ C. Let x1, . . . ,xn ∈ Rd be n independent observations of a d-dimensional random vector X ∼ Nd(0,Σ), u1 := Θ1(Σ) and 1, . . . , n ∼ N1(0, σ2) are independent from each other and {Xi}ni=1. We suppose that the following principal component regression model holds:\nY = αXu1 + , (2.1)\nwhere Y = (Y1, . . . , Yn)T ∈ Rn, X = [x1, . . . ,xn]T ∈ Rn×d and = ( 1, . . . , n)T ∈ Rn. We are interested in estimating the regression coefficient β := αu1.\nLet β̂ represent the solution of the classical least square estimator without taking the information that β is proportional to u1 into account. β̂ can be expressed as follows:\nβ̂ := (XTX)−1XTY . (2.2)\nWe then have the following proposition, which shows that the mean square error of β̂ − β is highly related to the scale of λmin(Σ). Proposition 2.1. Under the principal component regression model shown in (2.1), we have\nE‖β̂ − β‖22 = σ2\nn− d− 1\n( 1\nλ1(Σ) + · · ·+ 1 λd(Σ)\n) .\nProposition 2.1 reflects the vulnerability of least square estimator on the collinearity. More specifically, when λd(Σ) is extremely small, going to zero in the scale ofO(1/n), β̂ can be an inconsistent estimator even when d is fixed. On the other hand, using the Markov inequality, when λd(Σ) is lower bounded by a fixed constant and d = o(n), the rate of convergence of β̂ is well known to be OP ( √ d/n).\nMotivated from Equation (2.1), the classical principal component regression estimator can be elaborated as follows.\n(1) We first estimate u1 using the leading eigenvector û1 of the sample covariance Σ̂ := 1n ∑ xix T i .\n(2) We then estimate α ∈ R in Equation (2.1) by the standard least square estimation on the projected data Ẑ := Xû1 ∈ Rn:\nα̃ := (ẐT Ẑ)−1ẐTY ,\nThe final principal component regression estimator β̃ is then obtained as β̃ = α̃û1. We then have the following important theorem, which provides a rate of convergence for β̃ to approximate β. Theorem 2.2. Let r∗(Σ) := Tr(Σ)/λmax(Σ) represent the effective rank of Σ (Vershynin, 2010). Suppose that\n‖Σ‖2 · √ r∗(Σ) log d\nn = o(1).\nUnder the Model (2.1), when λmax(Σ) > c1 and λ2(Σ)/λ1(Σ) < C1 < 1 for some fixed constants C1 and c1, we have\n‖β̃ − β‖2 = OP\n{√ 1\nn +\n( α+\n1√ λmax(Σ)\n) · √ r∗(Σ) log d\nn\n} . (2.3)\nTheorem 2.2, compared to Proposition 2.1, provides several important messages on the performance of principal component regression. First, compared to the least square estimator β̂, β̃ is insensitive to collinearity in the sense that λmin(Σ) plays no role in the rate of convergence of β̃. Secondly, when λmin(Σ) is lower bounded by a fixed constant and α is upper bounded by a fixed constant, the rate of convergence for β̂ is OP ( √ d/n) and for β̃ is OP ( √ r∗(Σ) log d/n), while r∗(Σ) :=\nTr(Σ)/λmax(Σ) ≤ d and is of order o(d) when there exists a low rank structure for Σ. These two observations, combined together, illustrate the advantages of the classical principal component regression over least square estimation. These advantages justify the use of principal component regression. There is one more thing to be noted: the performance of β̃, unlike β̂, depends on α. When α is small, β̃ can predict β more accurately.\nThese three observations are verified in Figure 1. Here the data are generated according to Equation (2.1) and we set n = 100, d = 10, Σ to be a diagonal matrix with descending diagonal values Σii = λi and σ2 = 1. In Figure 1(A), we set α = 1, λ1 = 10, λj = 1 for j = 2, . . . , d − 1, and changing λd from 1 to 1/100; In Figure 1(B), we set α = 1, λj = 1 for j = 2, . . . , d and changing λ1 from 1 to 100; In Figure 1(C), we set λ1 = 10, λj = 1 for j = 2, . . . , d, and changing α from 0.1 to 10. In the three figures, the empirical mean square error is plotted against 1/λd, λ1, and α. It can be observed that the results, each by each, matches the theory."
    }, {
      "heading" : "3 Robust Sparse Principal Component Regression under Elliptical Model",
      "text" : "In this section, we propose a new principal component regression method. We generalize the settings in classical principal component regression discussed in the last section in two directions: (i) We consider the high dimensional settings where the dimension d can be much larger than the sample size n; (ii) In modeling the predictors x1, . . . ,xn, we consider a more general elliptical, instead of the Gaussian distribution family. The elliptical family can capture characteristics such as heavy tails and tail dependence, making it more suitable for analyzing complex datasets in finance, genomics, and biomedical imaging."
    }, {
      "heading" : "3.1 Elliptical Distribution",
      "text" : "In this section we define the elliptical distribution and introduce the basic property of the elliptical distribution. We denote byX d= Y if random vectorsX and Y have the same distribution.\nHere we only consider the continuous random vectors with density existing. To our knowledge, there are essentially four ways to define the continuous elliptical distribution with density. The most intuitive way is as follows: A random vector X ∈ Rd is said to follow an elliptical distribution ECd(µ,Σ, ξ) if and only there exists a random variable ξ > 0 (a.s.) and a Gaussian distribution Z ∼ Nd(0,Σ) such that\nX d = µ+ ξZ. (3.1)\nNote that here ξ is not necessarily independent of Z. Accordingly, elliptical distribution can be regarded as a semiparametric generalization to the Gaussian distribution, with the nonparametric part ξ. Because ξ can be very heavy tailed, X can also be very heavy tailed. Moreover, when Eξ2 exists, we have\nCov(X) = Eξ2Σ and Θj(Cov(X)) = Θj(Σ) for j = 1, . . . , d.\nThis implies that, when Eξ2 exists, to recoveru1 := Θ1(Cov(X)), we only need to recover Θ1(Σ). Here Σ is conventionally called the scatter matrix.\nWe would like to point out that the elliptical family is significantly larger than the Gaussian. In fact, Gaussian is fully parameterized by finite dimensional parameters (mean and variance). In contrast, the elliptical is a semiparametric family (since the elliptical density can be represented as g((x−µ)TΣ−1(x−µ)) where the function g(·) function is completely unspecified.). If we consider the “volumes” of the family of the elliptical family and the Gaussian family with respect to the Lebesgue reference measure, the volume of Gaussian family is zero (like a line in a 3-dimensional space), while the volume of the elliptical family is positive (like a ball in a 3-dimensional space)."
    }, {
      "heading" : "3.2 Multivariate Kendall’s tau",
      "text" : "As a important step in conducting the principal component regression, we need to estimate u1 = Θ1(Cov(X)) = Θ1(Σ) as accurately as possible. Since the random variable ξ in Equation (3.1) can be very heavy tailed, the according elliptical distributed random vector can be heavy tailed. Therefore, as has been pointed out by various authors (Tyler, 1987; Croux et al., 2002; Han and Liu, 2013b), the leading eigenvector of the sample covariance matrix Σ̂ can be a bad estimator in estimating u1 = Θ1(Σ) under the elliptical distribution. This motivates developing robust estimator.\nIn particular, in this paper we consider using the multivariate Kendall’s tau (Choi and Marden, 1998) and recently deeply studied by Han and Liu (2013a). In the following we give a brief description of this estimator. Let X ∼ ECd(µ,Σ, ξ) and X̃ be an independent copy of X . The population multivariate Kendall’s tau matrix, denoted by K ∈ Rd×d, is defined as:\nK := E\n( (X − X̃)(X − X̃)T\n‖X − X̃‖22\n) . (3.2)\nLet x1, . . . ,xn be n independent observations of X . The sample version of multivariate Kendall’s tau is accordingly defined as\nK̂ = 1 n(n− 1) ∑ i 6=j (xi − xj)(xi − xj)T ‖xi − xj‖22 , (3.3)\nand we have that E(K̂) = K. K̂ is a matrix version U statistic and it is easy to see that maxjk |Kjk| ≤ 1,maxjk |K̂jk| ≤ 1. Therefore, K̂ is a bounded matrix and hence can be a nicer statistic than the sample covariance matrix. Moreover, we have the following important proposition, coming from Oja (2010), showing that K has the same eigenspace as Σ and Cov(X). Proposition 3.1 (Oja (2010)). Let X ∼ ECd(µ,Σ, ξ) be a continuous distribution and K be the population multivariate Kendall’s tau statistic. Then if λj(Σ) 6= λk(Σ) for any k 6= j, we have\nΘj(Σ) = Θj(K) and λj(K) = E\n( λj(Σ)U 2 j\nλ1(Σ)U21 + . . .+ λd(Σ)U 2 d\n) , (3.4)\nwhere U := (U1, . . . , Ud)T follows a uniform distribution in Sd−1. In particular, when Eξ2 exists, Θj(Cov(X)) = Θj(K)."
    }, {
      "heading" : "3.3 Model and Method",
      "text" : "In this section we discuss the model we build and the accordingly proposed method in conducting high dimensional (sparse) principal component regression on non-Gaussian data.\nSimilar as in Section 2, we consider the classical simple principal component regression model:\nY = αXu1 + = α[x1, . . . ,xn] Tu1 + .\nTo relax the Gaussian assumption, we assume that both x1, . . . ,xn ∈ Rd and 1, . . . , n ∈ R be elliptically distributed. We assume that xi ∈ ECd(0,Σ, ξ). To allow the dimension d increasing much faster than n, we impose a sparsity structure on u1 = Θ1(Σ). Moreover, to make u1 identifiable, we assume that λ1(Σ) 6= λ2(Σ). Thusly, the formal model of the robust sparse principal component regression considered in this paper is as follows:\nMd(Y , ; Σ, ξ, s) : { Y = αXu1 + ,\nx1, . . . ,xn ∼ ECd(0,Σ, ξ), ‖Θ1(Σ)‖0 ≤ s, λ1(Σ) 6= λ2(Σ). (3.5)\nThen the robust sparse principal component regression can be elaborated as a two step procedure:\n(i) Inspired by the model Md(Y , ; Σ, ξ, s) and Proposition 3.1, we consider the following optimization problem to estimate u1 := Θ1(Σ):\nũ1 = arg max v∈Rd\nvT K̂v, subject to v ∈ Sd−1 ∩ B0(s), (3.6)\nwhere B0(s) := {v ∈ Rd : ‖v‖0 ≤ s} and K̂ is the estimated multivariate Kendall’s tau matrix. The corresponding global optimum is denoted by ũ1. Using Proposition 3.1, ũ1 is also an estimator of Θ1(Cov(X)), whenever the covariance matrix exists.\n(ii) We then estimate α ∈ R in Equation (3.5) by the standard least square estimation on the projected data Z̃ := Xũ1 ∈ Rn:\nα̌ := (Z̃T Z̃)−1Z̃TY ,\nThe final principal component regression estimator β̌ is then obtained as β̌ = α̌ũ1."
    }, {
      "heading" : "3.4 Theoretical Property",
      "text" : "In Theorem 2.2, we show that how to estimate u1 accurately plays an important role in conducting the principal component regression. Following this discussion and the very recent results in Han and Liu (2013a), the following “easiest” and “hardest” conditions are considered. Here κL, κU are two constants larger than 1.\nCondition 1 (“Easiest”): λ1(Σ) 1,κU dλj(Σ) for any j ∈ {2, . . . , d} and λ2(Σ) 1,κU λj(Σ) for any j ∈ {3, . . . , d};\nCondition 2 (“Hardest”): λ1(Σ) κL,κU λj(Σ) for any j ∈ {2, . . . , d}.\nIn the sequel, we say that the modelMd(Y , ; Σ, ξ, s) holds if the data (Y ,X) are generated using the modelMd(Y , ; Σ, ξ, s). Under Conditions 1 and 2, we then have the following theorem, which shows that under certain conditions, ‖β̌ − β‖2 = OP ( √ s log d/n), which is the optimal parametric rate in estimating the regression coefficient (Ravikumar et al., 2008).\nTheorem 3.2. Let the modelMd(Y , ; Σ, ξ, s) hold and |α| in Equation (3.5) are upper bounded by a constant and ‖Σ‖2 is lower bounded by a constant. Then under Condition 1 or Condition 2 and for all random vectorX such that\nmax v∈Sd−1,‖v‖0≤2s\n|vT (Σ̂−Σ)v| = oP (1),\nwe have the robust principal component regression estimator β̌ satisfies that\n‖β̌ − β‖2 = OP\n(√ s log d\nn\n) ."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section we conduct study on both synthetic and real-world data to investigate the empirical performance of the robust sparse principal component regression proposed in this paper. We use the truncated power algorithm proposed in Yuan and Zhang (2013) to approximate the global optimums ũ1 to (3.6). Here the cardinalities of the support sets of the leading eigenvectors are treated as tuning parameters. The following three methods are considered:\nlasso: the classical L1 penalized regression;\nPCR: The sparse principal component regression using the sample covariance matrix as the sufficient statistic and exploiting the truncated power algorithm in estimating u1;\nRPCR: The robust sparse principal component regression proposed in this paper, using the multivariate Kendall’s tau as the sufficient statistic and exploiting the truncated power algorithm to estimate u1."
    }, {
      "heading" : "4.1 Simulation Study",
      "text" : "In this section, we conduct simulation study to back up the theoretical results and further investigate the empirical performance of the proposed robust sparse principal component regression method.\nTo illustrate the empirical usefulness of the proposed method, we first consider generating the data matrix X. To generate X, we need to consider how to generate Σ and ξ. In detail, let ω1 > ω2 > ω3 = . . . = ωd be the eigenvalues and u1, . . . ,ud be the eigenvectors of Σ with uj := (uj1, . . . , ujd)\nT . The top 2 leading eigenvectors u1,u2 of Σ are specified to be sparse with sj := ‖uj‖0 and ujk = 1/ √ sj for k ∈ [1 + ∑j−1 i=1 si, ∑j i=1 si] and zero for all the others. Σ is generated\nas Σ = ∑2 j=1(ωj−ωd)ujuTj +ωdId. Across all settings, we let s1 = s2 = 10, ω1 = 5.5, ω2 = 2.5, and ωj = 0.5 for all j = 3, . . . , d. With Σ, we then consider the following four different elliptical distributions:\n(Normal)X ∼ ECd(0,Σ, ζ1) with ζ1 d = χd. Here χd is the chi-distribution with degree of freedom d. For Y1, . . . , Yd i.i.d.∼ N(0, 1), √ Y 21 + . . .+ Y 2 d d = χd. In this setting, X follows the Gaussian distribution (Fang et al., 1990).\n(Multivariate-t) X ∼ ECd(0,Σ, ζ2) with ζ2 d = √ κξ∗1/ξ ∗ 2 . Here ξ ∗ 1 d = χd and ξ∗2 d = χκ with κ ∈ Z+. In this setting, X follows a multivariate-t distribution with degree of freedom κ (Fang et al., 1990). Here we consider κ = 3.\n(EC1)X ∼ ECd(0,Σ, ζ3) with ζ3 ∼ F (d, 1), an F distribution. (EC2)X ∼ ECd(0,Σ, ζ4) with ζ4 ∼ Exp(1), an exponential distribution. We then simulate x1, . . . ,xn fromX . This forms a data matrix X. Secondly, we let Y = Xu1 + , where ∼ Nn(0, In). This produces the data (Y ,X). We repeatedly generate n data according to the four distributions discussed above for 1,000 times. To show the estimation accuracy, Figure 2 plots the empirical mean square error between the estimate ǔ1 and true regression coefficient β against the numbers of estimated nonzero entries (defined as ‖ǔ1‖0), for PCR and RPCR, under different schemes of (n, d),Σ and different distributions. Here we considered n = 100 and d = 200.\nIt can be seen that we do not plot the results of lasso in Figure 2. As discussed in Section 2, especially as shown in Figure 1, linear regression and principal component regression have their own advantages in different settings. More specifically, we do not plot the results of lasso here simply because it performs so bad under our simulation settings. For example, under the Gaussian settings with n = 100 and d = 200, the lowest mean square error for lasso is 0.53 and the errors are averagely above 1.5, while for RPCR is 0.13 and is averagely below 1.\nFigure 2 shows when the data are non-Gaussian but follow an elliptically distribution, RPCR outperforms PCR constantly in terms of estimation accuracy. Moreover, when the data are indeed normally distributed, there is no obvious difference between RPCR and PCR, indicating that RPCR is a safe alternative to the classical sparse principal component regression."
    }, {
      "heading" : "4.2 Application to Equity Data",
      "text" : "In this section we apply the proposed robust sparse principal component regression and the other two methods to the stock price data from Yahoo! Finance (finance.yahoo.com). We collect the daily closing prices for 452 stocks that are consistently in the S&P 500 index between January 1, 2003 through January 1, 2008. This gives us altogether T=1,257 data points, each data point corresponds to the vector of closing prices on a trading day. Let St = [Stt,j ] denote by the closing price of stock j on day t. We are interested in the log return data X = [Xtj ] with Xtj = log(Stt,j/Stt−1,j).\nWe first show that this data set is non-Gaussian and heavy tailed. This is done first by conducting marginal normality tests (Kolmogorove-Smirnov, Shapiro-Wilk, and Lillifors) on the data. We find that at most 24 out of 452 stocks would pass any of three normality test. With Bonferroni correction there are still over half stocks that fail to pass any normality tests. Moreover, to illustrate the heavy tailed issue, we plot the quantile vs. quantile plot for one stock, “Goldman Sachs”, in Figure 3(A). It can be observed that the log return values for this stock is heavy tailed compared to the Gaussian.\nTo illustrate the power of the proposed method, we pick a subset of the data first. The stocks can be summarized into 10 Global Industry Classification Standard (GICS) sectors and we are focusing on the subcategory “Financial”. This leave us 74 stocks and we denote the resulting data to be F ∈ R1257×74. We are interested in predicting the log return value in day t for each stock indexed by k (i.e., treating Ft,k as the response) using the log return values for all the stocks in day t − 1 to day t − 7 (i.e., treating vec(Ft−7≤t′≤t−1,·) as the predictor). The dimension for the regressor is accordingly 7 × 74 = 518. For each stock indexed by k, to learn the regression coefficient βk, we use Ft′∈{1,...,1256},· as the training data and applying the three different methods on this dataset. For each method, after obtaining an estimator β̂k, we use vec(Ft′∈{1250,...,1256},·)β̂ to estimate F1257,k. This procedure is repeated for each k and the averaged prediction errors are plotted against the number of features selected (i.e., ‖β̂‖0) in Figure 3(B). To visualize the difference more clearly, in the figures we enlarge the scale of the prediction errors by 100 times. It can be observed that RPCR has the universally lowest prediction error with regard to different number of features."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Han’s research is supported by a Google fellowship. Liu is supported by NSF Grants III-1116730 and NSF III-1332109, an NIH sub-award and a FDA sub-award from Johns Hopkins University."
    } ],
    "references" : [ {
      "title" : "On principal components and regression: a statistical explanation of a natural phenomenon",
      "author" : [ "A. Artemiou", "B. Li" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Artemiou and Li,? \\Q2009\\E",
      "shortCiteRegEx" : "Artemiou and Li",
      "year" : 2009
    }, {
      "title" : "On the sample covariance matrix estimator of reduced effective rank population matrices, with applications to fPCA",
      "author" : [ "F. Bunea", "L. Xiao" ],
      "venue" : "arXiv preprint arXiv:1212.5321",
      "citeRegEx" : "Bunea and Xiao,? \\Q2012\\E",
      "shortCiteRegEx" : "Bunea and Xiao",
      "year" : 2012
    }, {
      "title" : "Sparse PCA: Optimal rates and adaptive estimation",
      "author" : [ "T.T. Cai", "Z. Ma", "Y. Wu" ],
      "venue" : "The Annals of Statistics (to appear)",
      "citeRegEx" : "Cai et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2013
    }, {
      "title" : "A multivariate version of kendall’s",
      "author" : [ "K. Choi", "J. Marden" ],
      "venue" : "Journal of Nonparametric Statistics,",
      "citeRegEx" : "Choi and Marden,? \\Q1998\\E",
      "shortCiteRegEx" : "Choi and Marden",
      "year" : 1998
    }, {
      "title" : "Fisher lecture: Dimension reduction in regression",
      "author" : [ "R.D. Cook" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Cook,? \\Q2007\\E",
      "shortCiteRegEx" : "Cook",
      "year" : 2007
    }, {
      "title" : "Sign and rank covariance matrices: statistical properties and application to principal components analysis",
      "author" : [ "C. Croux", "E. Ollila", "H. Oja" ],
      "venue" : "In Statistical data analysis based on the L1-norm and related methods,",
      "citeRegEx" : "Croux et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Croux et al\\.",
      "year" : 2002
    }, {
      "title" : "A direct formulation for sparse PCA using semidefinite programming",
      "author" : [ "A. d’Aspremont", "L. El Ghaoui", "M.I. Jordan", "G.R. Lanckriet" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "d.Aspremont et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "d.Aspremont et al\\.",
      "year" : 2007
    }, {
      "title" : "Symmetric multivariate and related distributions",
      "author" : [ "K. Fang", "S. Kotz", "K. Ng" ],
      "venue" : null,
      "citeRegEx" : "Fang et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 1990
    }, {
      "title" : "Optimal sparse principal component analysis in high dimensional elliptical model",
      "author" : [ "F. Han", "H. Liu" ],
      "venue" : "arXiv preprint arXiv:1310.3561",
      "citeRegEx" : "Han and Liu,? \\Q2013\\E",
      "shortCiteRegEx" : "Han and Liu",
      "year" : 2013
    }, {
      "title" : "Scale-invariant sparse PCA on high dimensional meta-elliptical data",
      "author" : [ "F. Han", "H. Liu" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "Han and Liu,? \\Q2013\\E",
      "shortCiteRegEx" : "Han and Liu",
      "year" : 2013
    }, {
      "title" : "On consistency and sparsity for principal components analysis in high dimensions",
      "author" : [ "I.M. Johnstone", "A.Y. Lu" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Johnstone and Lu,? \\Q2009\\E",
      "shortCiteRegEx" : "Johnstone and Lu",
      "year" : 2009
    }, {
      "title" : "A course in multivariate analysis",
      "author" : [ "M.G. Kendall" ],
      "venue" : null,
      "citeRegEx" : "Kendall,? \\Q1968\\E",
      "shortCiteRegEx" : "Kendall",
      "year" : 1968
    }, {
      "title" : "Estimating the tail dependence function of an elliptical distribution",
      "author" : [ "C. Klüppelberg", "G. Kuhn", "L. Peng" ],
      "venue" : null,
      "citeRegEx" : "Klüppelberg et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Klüppelberg et al\\.",
      "year" : 2007
    }, {
      "title" : "Sparse principal component analysis with missing observations. arXiv preprint arXiv:1205.7060",
      "author" : [ "K. Lounici" ],
      "venue" : null,
      "citeRegEx" : "Lounici,? \\Q2012\\E",
      "shortCiteRegEx" : "Lounici",
      "year" : 2012
    }, {
      "title" : "Sparse principal component analysis and iterative thresholding",
      "author" : [ "Z. Ma" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "Ma,? \\Q2013\\E",
      "shortCiteRegEx" : "Ma",
      "year" : 2013
    }, {
      "title" : "Principal components regression in exploratory statistical research",
      "author" : [ "W.F. Massy" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Massy,? \\Q1965\\E",
      "shortCiteRegEx" : "Massy",
      "year" : 1965
    }, {
      "title" : "Spectral bounds for sparse PCA: Exact and greedy algorithms. Advances in neural information processing",
      "author" : [ "B. Moghaddam", "Y. Weiss", "S. Avidan" ],
      "venue" : null,
      "citeRegEx" : "Moghaddam et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Moghaddam et al\\.",
      "year" : 2006
    }, {
      "title" : "Multivariate Nonparametric Methods with R: An approach based on spatial signs and ranks, volume",
      "author" : [ "H. Oja" ],
      "venue" : null,
      "citeRegEx" : "Oja,? \\Q2010\\E",
      "shortCiteRegEx" : "Oja",
      "year" : 2010
    }, {
      "title" : "Model selection in gaussian graphical models: High-dimensional consistency of l1-regularized mle",
      "author" : [ "P. Ravikumar", "G. Raskutti", "M. Wainwright", "B. Yu" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2008
    }, {
      "title" : "A distribution-freem-estimator of multivariate scatter",
      "author" : [ "D.E. Tyler" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Tyler,? \\Q1987\\E",
      "shortCiteRegEx" : "Tyler",
      "year" : 1987
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027",
      "author" : [ "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "Vershynin,? \\Q2010\\E",
      "shortCiteRegEx" : "Vershynin",
      "year" : 2010
    }, {
      "title" : "Minimax rates of estimation for sparse pca in high dimensions",
      "author" : [ "V.Q. Vu", "J. Lei" ],
      "venue" : "Journal of Machine Learning Research (AIStats Track)",
      "citeRegEx" : "Vu and Lei,? \\Q2012\\E",
      "shortCiteRegEx" : "Vu and Lei",
      "year" : 2012
    }, {
      "title" : "Truncated power method for sparse eigenvalue problems",
      "author" : [ "X. Yuan", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Yuan and Zhang,? \\Q2013\\E",
      "shortCiteRegEx" : "Yuan and Zhang",
      "year" : 2013
    }, {
      "title" : "Sparse principal component analysis",
      "author" : [ "H. Zou", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Journal of computational and graphical statistics,",
      "citeRegEx" : "Zou et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "1 Introduction Principal component regression (PCR) has been widely used in statistics for years (Kendall, 1968).",
      "startOffset" : 97,
      "endOffset" : 112
    }, {
      "referenceID" : 20,
      "context" : "First, with regard to the classical principal component regression, under a double asymptotic framework in which d is allowed to increase with n, by borrowing the very recent development in principal component analysis (Vershynin, 2010; Lounici, 2012; Bunea and Xiao, 2012), we for the first time explicitly show the advantage of principal component regression over the classical linear regression.",
      "startOffset" : 219,
      "endOffset" : 273
    }, {
      "referenceID" : 13,
      "context" : "First, with regard to the classical principal component regression, under a double asymptotic framework in which d is allowed to increase with n, by borrowing the very recent development in principal component analysis (Vershynin, 2010; Lounici, 2012; Bunea and Xiao, 2012), we for the first time explicitly show the advantage of principal component regression over the classical linear regression.",
      "startOffset" : 219,
      "endOffset" : 273
    }, {
      "referenceID" : 1,
      "context" : "First, with regard to the classical principal component regression, under a double asymptotic framework in which d is allowed to increase with n, by borrowing the very recent development in principal component analysis (Vershynin, 2010; Lounici, 2012; Bunea and Xiao, 2012), we for the first time explicitly show the advantage of principal component regression over the classical linear regression.",
      "startOffset" : 219,
      "endOffset" : 273
    }, {
      "referenceID" : 20,
      "context" : "Let r∗(Σ) := Tr(Σ)/λmax(Σ) represent the effective rank of Σ (Vershynin, 2010).",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "Therefore, as has been pointed out by various authors (Tyler, 1987; Croux et al., 2002; Han and Liu, 2013b), the leading eigenvector of the sample covariance matrix Σ̂ can be a bad estimator in estimating u1 = Θ1(Σ) under the elliptical distribution.",
      "startOffset" : 54,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "Therefore, as has been pointed out by various authors (Tyler, 1987; Croux et al., 2002; Han and Liu, 2013b), the leading eigenvector of the sample covariance matrix Σ̂ can be a bad estimator in estimating u1 = Θ1(Σ) under the elliptical distribution.",
      "startOffset" : 54,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "In particular, in this paper we consider using the multivariate Kendall’s tau (Choi and Marden, 1998) and recently deeply studied by Han and Liu (2013a).",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "Under Conditions 1 and 2, we then have the following theorem, which shows that under certain conditions, ‖β̌ − β‖2 = OP ( √ s log d/n), which is the optimal parametric rate in estimating the regression coefficient (Ravikumar et al., 2008).",
      "startOffset" : 214,
      "endOffset" : 238
    }, {
      "referenceID" : 7,
      "context" : "In this setting, X follows the Gaussian distribution (Fang et al., 1990).",
      "startOffset" : 53,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "In this setting, X follows a multivariate-t distribution with degree of freedom κ (Fang et al., 1990).",
      "startOffset" : 82,
      "endOffset" : 101
    } ],
    "year" : 2013,
    "abstractText" : "In this paper we focus on the principal component regression and its application to high dimension non-Gaussian data. The major contributions are two folds. First, in low dimensions and under the Gaussian model, by borrowing the strength from recent development in minimax optimal principal component estimation, we first time sharply characterize the potential advantage of classical principal component regression over least square estimation. Secondly, we propose and analyze a new robust sparse principal component regression on high dimensional elliptically distributed data. The elliptical distribution is a semiparametric generalization of the Gaussian, including many well known distributions such as multivariate Gaussian, rank-deficient Gaussian, t, Cauchy, and logistic. It allows the random vector to be heavy tailed and have tail dependence. These extra flexibilities make it very suitable for modeling finance and biomedical imaging data. Under the elliptical model, we prove that our method can estimate the regression coefficients in the optimal parametric rate and therefore is a good alternative to the Gaussian based methods. Experiments on synthetic and real world data are conducted to illustrate the empirical usefulness of the proposed method.",
    "creator" : null
  }
}