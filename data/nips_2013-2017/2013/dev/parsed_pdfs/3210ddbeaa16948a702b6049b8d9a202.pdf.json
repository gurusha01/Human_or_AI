{
  "name" : "3210ddbeaa16948a702b6049b8d9a202.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Sign Cauchy Projections and Chi-Square Kernel",
    "authors" : [ "Ping Li", "Gennady Samorodnitsky" ],
    "emails" : [ "pingli@stat.rutgers.edu", "gs18@cornell.edu", "jeh@cs.cornell.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "There are many future research problems. For example, when α → 0, the collision probability is a function of the resemblance (of the binary-quantized data). This provides an effective mechanism for resemblance estimation in data streams."
    }, {
      "heading" : "1 Introduction",
      "text" : "High-dimensional representations have become very popular in modern applications of machine learning, computer vision, and information retrieval. For example, Winner of 2009 PASCAL image classification challenge used millions of features [29]. [1, 30] described applications with billion or trillion features. The use of high-dimensional data often achieves good accuracies at the cost of a significant increase in computations, storage, and energy consumptions.\nConsider two data vectors (e.g., two images) u, v ∈ RD. A basic task is to compute their distance or similarity. For example, the correlation (ρ2) and lα distance (dα) are commonly used:\nρ2(u, v) = ∑D i=1 uivi√∑D\ni=1 u 2 i ∑D i=1 v 2 i , dα(u, v) = D∑ i=1 |ui − vi|α (1)\nIn this study, we are particularly interested in the χ2 similarity, denoted by ρχ2 :\nρχ2 = D∑ i=1 2uivi ui + vi , where ui ≥ 0, vi ≥ 0, D∑ i=1 ui = D∑ i=1 vi = 1 (2)\nThe chi-square similarity is closely related to the chi-square distance dχ2 :\ndχ2 = D∑ i=1 (ui − vi)2 ui + vi = D∑ i=1 (ui + vi)− D∑ i=1 4uivi ui + vi = 2− 2ρχ2 (3)\nThe chi-square similarity is an instance of the Hilbertian metrics, which are defined over probability space [10] and suitable for data generated from histograms. Histogram-based features (e.g., bagof-word or bag-of-visual-word models) are extremely popular in computer vision, natural language processing (NLP), and information retrieval. Empirical studies have demonstrated the superiority of the χ2 distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].\nThe method of normal random projections (i.e., α-stable projections with α = 2) has become popular in machine learning (e.g., [7]) for reducing the data dimensions and data sizes, to facilitate\nefficient computations of the l2 distances and correlations. More generally, the method of stable random projections [11, 17] provides an efficient algorithm to compute the lα distances (0 < α ≤ 2). In this paper, we propose to use only the signs of the projected data after applying stable projections."
    }, {
      "heading" : "1.1 Stable Random Projections and Sign (1-Bit) Stable Random Projections",
      "text" : "Consider two high-dimensional data vectors u, v ∈ RD. The basic idea of stable random projections is to multiply u and v by a random matrix R ∈ RD×k: x = uR ∈ Rk, y = vR ∈ Rk, where entries of R are i.i.d. samples from a symmetric α-stable distribution with unit scale. By properties of stable distributions, xj − yj follows a symmetric α-stable distribution with scale dα. Hence, the task of computing dα boils down to estimating the scale dα from k i.i.d. samples. In this paper, we propose to store only the signs of projected data and we study the probability of collision:\nPα = Pr (sign(xj) ̸= sign(yj)) (4) Using only the signs (i.e., 1 bit) has significant advantages for applications in search and learning. When α = 2, this probability can be analytically evaluated [9] (or via a simple geometric argument):\nP2 = Pr (sign(xj) ̸= sign(yj)) = 1\nπ cos−1 ρ2 (5)\nwhich is an important result known as sim-hash [5]. For α < 2, the collision probability is an open problem. When the data are nonnegative, this paper (Theorem 1) will prove a bound of Pα for general 0 < α ≤ 2. The bound is exact at α = 2 and becomes less sharp as α moves away from 2. Furthermore, for α = 1 and nonnegative data, we have the interesting observation that the probability P1 can be well approximated as functions of the χ2 similarity ρχ2 ."
    }, {
      "heading" : "1.2 The Advantages of Sign Stable Random Projections",
      "text" : "1. There is a significant saving in storage space by using only 1 bit instead of (e.g.,) 64 bits. 2. This scheme leads to an efficient linear algorithm (e.g., linear SVM). For example, a nega-\ntive sign can be coded as “01” and a positive sign as “10” (i.e., a vector of length 2). With k projections, we concatenate k short vectors to form a vector of length 2k. This idea is inspired by b-bit minwise hashing [20], which was designed for binary sparse data.\n3. This scheme also leads to an efficient near neighbor search algorithm [8, 12]. We can code a negative sign by “0” and positive sign by “1” and concatenate k such bits to form a hash table of 2k buckets. In the query phase, one only searches for similar vectors in one bucket."
    }, {
      "heading" : "1.3 Data Stream Computations",
      "text" : "Stable random projections are naturally suitable for data streams. In modern applications, massive datasets are often generated in a streaming fashion, which are difficult to transmit and store [22], as the processing is done on the fly in one-pass of the data. In the standard turnstile model [22], a data stream can be viewed as high-dimensional vector with the entry values changing over time.\nHere, we denote a stream at time t by u(t)i , i = 1 to D. At time t, a stream element (it, It) arrives and updates the it-th coordinate as u (t) it = u (t−1) it\n+ It. Clearly, the turnstile data stream model is particularly suitable for describing histograms and it is also a standard model for network traffic summarization and monitoring [31]. Because this stream model is linear, methods based on linear projections (i.e., matrix-vector multiplications) can naturally handle streaming data of this sort. Basically, entries of the projection matrix R ∈ RD×k are (re)generated as needed using pseudo-random number techniques [23]. As (it, It) arrives, only the entries in the it-th row, i.e., rit,j , j = 1 to k, are (re)generated and the projected data are updated as x (t) j = x (t−1) j + It × ritj .\nRecall that, in the definition of χ2 similarity, the data are assumed to be normalized (summing to 1). For nonnegative streams, the sum can be computed error-free by using merely one counter:∑D\ni=1 u (t) i = ∑t s=1 Is. Thus we can still use, without loss of generality, the sum-to-one assumption, even in the streaming environment. This fact was recently exploited by another data stream algorithm named Compressed Counting (CC) [18] for estimating the Shannon entropy of streams.\nBecause the use of the χ2 similarity is popular in (e.g.,) computer vision, recently there are other proposals for estimating the χ2 similarity. For example, [15] proposed a nice technique to approximate ρχ2 by first expanding the data from D dimensions to (e.g.,) 5 ∼ 10×D dimensions through a nonlinear transformation and then applying normal random projections on the expanded data. The nonlinear transformation makes their method not applicable to data streams, unlike our proposal.\nFor notational simplicity, we will drop the superscript (t) for the rest of the paper."
    }, {
      "heading" : "2 An Experimental Study of Chi-Square Kernels",
      "text" : "We provide an experimental study to validate the use of χ2 similarity. Here, the “χ2-kernel” is defined as K(u, v) = ρχ2 and the “acos-χ2-kernel” as K(u, v) = 1 − 1π cos\n−1 ρχ2 . With a slight abuse of terminology, we call both “χ2 kernel” when it is clear in the context.\nWe use the “precomputed kernel” functionality in LIBSVM on two datasets: (i) UCI-PEMS, with 267 training examples and 173 testing examples in 138,672 dimensions; (ii) MNIST-small, a subset of the popular MNIST dataset, with 10,000 training examples and 10,000 testing examples.\nThe results are shown in Figure 1. To compare these two types of χ2 kernels with “linear” kernel, we also test the same data using LIBLINEAR [6] after normalizing the data to have unit Euclidian norm, i.e., we basically use ρ2. For both LIBSVM and LIBLINEAR, we use l2-regularization with a regularization parameter C and we report the test errors for a wide range of C values.\nHere, we should state that it is not the intention of this paper to use these two small examples to conclude the advantage of χ2 kernels over linear kernel. We simply use them to validate our proposed method, which is general-purpose and is not limited to data generated from histograms."
    }, {
      "heading" : "3 Sign Stable Random Projections and the Collision Probability Bound",
      "text" : "We apply stable random projections on two vectors u, v ∈ RD: x = ∑D i=1 uiri, y = ∑D i=1 viri, ri ∼ S(α, 1), i.i.d. Here Z ∼ S(α, γ) denotes a symmetric α-stable distribution with scale γ, whose characteristic function [24] is E ( e √ −1Zt ) = e−γ|t| α . By properties of stable distributions,\nwe know x−y ∼ S ( α, ∑D i=1 |ui − vi|α )\n. Applications including linear learning and near neighbor search will benefit from sign α-stable random projections. When α = 2 (i.e. normal), the collision probability Pr (sign(x) ̸= sign(y)) is known [5, 9]. For α < 2, it is a difficult probability problem. This section provides a bound of Pr (sign(x) ̸= sign(y)), which is fairly accurate for α close to 2."
    }, {
      "heading" : "3.1 Collision Probability Bound",
      "text" : "In this paper, we focus on nonnegative data (as common in practice). We present our first theorem.\nTheorem 1 When the data are nonnegative, i.e., ui ≥ 0, vi ≥ 0, we have\nPr (sign(x) ̸= sign(y)) ≤ 1 π cos−1 ρα, where ρα =  ∑Di=1 uα/2i vα/2i√∑D i=1 u α i ∑D i=1 v α i 2/α (6) For α = 2, this bound is exact [5, 9]. In fact the result for α = 2 leads to the following Lemma:\nLemma 1 The kernel defined as K(u, v) = 1− 1π cos −1 ρ2 is positive definite (PD).\nProof: The indicator function 1 {sign(x) = sign(y)} can be written as an inner product (hence PD) and Pr (sign(x) = sign(y)) = E (1 {sign(x) = sign(y)}) = 1− 1π cos −1 ρ2."
    }, {
      "heading" : "3.2 A Simulation Study to Verify the Bound of the Collision Probability",
      "text" : "We generate the original data u and v by sampling from a bivariate t-distribution, which has two parameters: the correlation and the number of degrees of freedom (which is taken to be 1 in our experiments). We use a full range of the correlation parameter from 0 to 1 (spaced at 0.01). To generate positive data, we simply take the absolute values of the generated data. Then we fix the data as our original data (like u and v), apply sign stable random projections, and report the empirical collision probabilities (after 105 repetitions).\nFigure 2 presents the simulated collision probability Pr (sign(x) ̸= sign(y)) for D = 100 and α ∈ {1.5, 1.2, 1.0, 0.5}. In each panel, the dashed curve is the theoretical upper bound 1π cos\n−1 ρα, and the solid curve is the simulated collision probability. Note that it is expected that the simulated data can not cover the entire range of ρα values, especially as α → 0.\nFigure 2 verifies the theoretical upper bound 1π cos −1 ρα. When α ≥ 1.5, this upper bound is fairly sharp. However, when α ≤ 1, the bound is not tight, especially for small α. Also, the curves of the empirical collision probabilities are not smooth (in terms of ρα).\nReal-world high-dimensional datasets are often sparse. To verify the theoretical upper bound of the collision probability on sparse data, we also simulate sparse data by randomly making 50% of the generated data as used in Figure 2 be zero. With sparse data, it is even more obvious that the theoretical upper bound 1π cos −1 ρα is not sharp when α ≤ 1, as shown in Figure 3.\nIn summary, the collision probability bound: Pr (sign(x) ̸= sign(y)) ≤ 1π cos −1 ρα is fairly sharp when α is close to 2 (e.g., α ≥ 1.5). However, for α ≤ 1, a better approximation is needed."
    }, {
      "heading" : "4 α = 1 and Chi-Square (χ2) Similarity",
      "text" : "In this section, we focus on nonnegative data (ui ≥ 0, vi ≥ 0) and α = 1. This case is important in practice. For example, we can view the data (ui, vi) as empirical probabilities, which are common when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].\nIn this context, we always normalize the data, i.e., ∑D i=1 ui = ∑D i=1 vi = 1. Theorem 1 implies\nPr (sign(x) ̸= sign(y)) ≤ 1 π cos−1 ρ1, where ρ1 = ( D∑ i=1 u 1/2 i v 1/2 i )2 (7)\nWhile the bound is not tight, interestingly, the collision probability can be related to the χ2 similarity. Recall the definitions of the chi-square distance dχ2 = ∑D i=1 (ui−vi)2 ui+vi and the chi-square similarity\nρχ2 = 1− 12dχ2 = ∑D i=1 2uivi ui+vi . In this context, we should view 00 = 0.\nLemma 2 Assume ui ≥ 0, vi ≥ 0, ∑D i=1 ui = 1, ∑D i=1 vi = 1. Then\nρχ2 = D∑ i=1 2uivi ui + vi ≥ ρ1 = ( D∑ i=1 u 1/2 i v 1/2 i )2 (8)\nIt is known that the χ2-kernel is PD [10]. Consequently, we know the acos-χ2-kernel is also PD.\nLemma 3 The kernel defined as K(u, v) = 1− 1π cos −1 ρχ2 is positive definite (PD).\nThe remaining question is how to connect Cauchy random projections with the χ2 similarity."
    }, {
      "heading" : "5 Two Approximations of Collision Probability for Sign Cauchy Projections",
      "text" : "It is a difficult problem to derive the collision probability of sign Cauchy projections if we would like to express the probability only in terms of certain summary statistics (e.g., some distance). Our first observation is that the collision probability can be well approximated using the χ2 similarity:\nPr (sign(x) ̸= sign(y)) ≈ Pχ2(1) = 1\nπ cos−1\n( ρχ2 )\n(9)\nFigure 4 shows this approximation is better than 1π cos −1 (ρ1). Particularly, in sparse data, the approximation 1π cos −1 (ρχ2) is very accurate (except when ρχ2 is close to 1), while the bound 1 π cos −1 (ρ1) is not sharp (and the curve is not smooth in ρ1).\nOur second (and less obvious) approximation is the following integral:\nPr (sign(x) ̸= sign(y)) ≈ Pχ2(2) = 1 2 − 2 π2 ∫ π/2 0 tan−1 ( ρχ2 2− 2ρχ2 tan t ) dt (10)\nFigure 5 illustrates that, for dense data, the second approximation (10) is more accurate than the first (9). The second approximation (10) is also accurate for sparse data. Both approximations, Pχ2(1) and Pχ2(2), are monotone functions of ρχ2 . In practice, we often do not need the ρχ2 values explicitly because it often suffices if the collision probability is a monotone function of the similarity."
    }, {
      "heading" : "5.1 Binary Data",
      "text" : "Interestingly, when the data are binary (before normalization), we can compute the collision probability exactly, which allows us to analytically assess the accuracy of the approximations. In fact, this case inspired us to propose the second approximation (10), which is otherwise not intuitive.\nFor convenience, we define a = |Ia|, b = |Ib|, c = |Ic|, where Ia = {i|ui > 0, vi = 0}, Ib = {i|vi > 0, ui = 0}, Ic = {i|ui > 0, vi > 0}, (11)\nAssume binary data (before normalization, i.e., sum to one). That is,\nui = 1\n|Ia|+ |Ic| =\n1\na+ c , ∀i ∈ Ia ∪ Ic, vi =\n1\n|Ib|+ |Ic| =\n1\nb+ c , ∀i ∈ Ib ∪ Ic (12)\nThe chi-square similarity ρχ2 becomes ρχ2 = ∑D\ni=1 2uivi ui+vi = 2ca+b+2c and hence ρχ2 2−2ρχ2 = ca+b .\nTheorem 2 Assume binary data. When α = 1, the exact collision probability is\nPr (sign(x) ̸= sign(y)) =1 2 − 2 π2 E { tan−1 ( c a |R| ) tan−1 (c b |R| )}\n(13)\nwhere R is a standard Cauchy random variable.\nWhen a = 0 or b = 0, we have E { tan−1 ( c a |R| ) tan−1 ( c b |R| )} = π2E { tan−1 ( c a+b |R| )}\n. This observation inspires us to propose the approximation (10):\nPχ2(2) = 1 2 − 1 π E\n{ tan−1 ( c a+ b |R| )} = 1 2 − 2 π2 ∫ π/2 0 tan−1 ( c a+ b tan t ) dt\nTo validate this approximation for binary data, we study the difference between (13) and (10), i.e.,\nZ(a/c, b/c) = Err = Pr (sign(x) ̸= sign(y))− Pχ2(2)\n=− 2 π2 E\n{ tan−1 ( 1 a/c |R| ) tan−1 ( 1 b/c |R| )} + 1 π E { tan−1 ( 1 a/c+ b/c |R| )}\n(14)\n(14) can be easily computed by simulations. Figure 6 confirms that the errors are larger than zero and very small . The maximum error is smaller than 0.0192, as proved in Lemma 4.\nLemma 4 The error defined in (14) ranges between 0 and Z(t∗): 0 ≤ Z(a/c, b/c) ≤ Z(t∗) = ∫ ∞ 0 { − 2 π2 ( tan−1 ( r t∗ ))2 + 1 π tan−1 ( r 2t∗ )} 2 π 1 1 + r2 dr (15)\nwhere t∗ = 2.77935 is the solution to 1t2−1 log 2t 1+t = log(2t) (2t)2−1 . Numerically, Z(t ∗) = 0.01919."
    }, {
      "heading" : "5.2 An Experiment Based on 3.6 Million English Word Pairs",
      "text" : "To further validate the two χ2 approximations (in non-binary data), we experiment with a word occurrences dataset (which is an example of histogram data) from a chunk of D = 216 web crawl documents. There are in total 2,702 words, i.e., 2,702 vectors and 3,649,051 word pairs. The entries of a vector are the occurrences of the word. This is a typical sparse, non-binary dataset. Interestingly, the errors of the collision probabilities based on two χ2 approximations are still very small. To report the results, we apply sign Cauchy random projections 107 times to evaluate the approximation errors of (9) and (10). The results, as presented in Figure 7, again confirm that the upper bound 1π cos\n−1 ρ1 is not tight and both χ2 approximations, Pχ2(1) and Pχ2(2), are accurate."
    }, {
      "heading" : "6 Sign Cauchy Random Projections for Classification",
      "text" : "Our method provides an effective strategy for classification. For each (high-dimensional) data vector, using k sign Cauchy projections, we encode a negative sign as “01” and a positive as “10” (i.e., a vector of length 2) and concatenate k short vectors to form a new feature vector of length 2k. We then feed the new data into a linear classifier (e.g., LIBLINEAR). Interestingly, this linear classifier approximates a nonlinear kernel classifier based on acos-χ2-kernel: K(u, v) = 1− 1π cos\n−1 ρχ2 . See Figure 8 for the experiments on the same two datasets in Figure 1: UCI-PEMS and MNIST-Small.\nFigure 1 has already shown that, for the UCI-PEMS dataset, the χ2-kernel (ρχ2 ) can produce noticeably better classification results than the acos-χ2-kernel (1 − 1π cos\n−1 ρχ2 ). Although our method does not directly approximate ρχ2 , we can still estimate ρχ2 by assuming the collision probability is exactly Pr (sign(x) ̸= sign(y)) = 1π cos\n−1 ρχ2 and then we can feed the estimated ρχ2 values into LIBSVM “precomputed kernel” for classification. Figure 9 verifies that this method can also approximate the χ2 kernel with enough projections."
    }, {
      "heading" : "7 Conclusion",
      "text" : "The use of χ2 similarity is widespread in machine learning, especially when features are generated from histograms, as common in natural language processing and computer vision. Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using χ2 similarity compared to other measures such as l2 distance. However, for large-scale applications with ultra-high-dimensional datasets, using χ2 similarity becomes challenging for practical reasons. Simply storing (and maneuvering) all the high-dimensional features would be difficult if there are a large number of observations. Computing all pairwise χ2 similarities can be time-consuming and in fact we usually can not materialize an all-pairwise similarity matrix even if there are merely 106 data points. Furthermore, the χ2 similarity is nonlinear, making it difficult to take advantage of modern linear algorithms which are known to be very efficient, e.g., [14, 25, 6, 3]. When data are generated in a streaming fashion, computing χ2 similarities without storing the original data will be even more challenging.\nThe method of α-stable random projections (0 < α ≤ 2) [11, 17] is popular for efficiently computing the lα distances in massive (streaming) data. We propose sign stable random projections by storing only the signs (i.e., 1-bit) of the projected data. Obviously, the saving in storage would be a significant advantage. Also, these bits offer the indexing capability which allows efficient search. For example, we can build hash tables using the bits to achieve sublinear time near neighbor search (although this paper does not focus on near neighbor search). We can also build efficient linear classifiers using these bits, for large-scale high-dimensional machine learning applications.\nA crucial task in analyzing sign stable random projections is to study the probability of collision (i.e., when the two signs differ). We derive a theoretical bound of the collision probability which is exact when α = 2. The bound is fairly sharp for α close to 2. For α = 1 (i.e., Cauchy random projections), we find the χ2 approximation is significantly more accurate. In addition, for binary data, we analytically show that the errors from using the χ2 approximation are less than 0.0192. Experiments on real and simulated data confirm that our proposed χ2 approximations are very accurate.\nWe are enthusiastic about the practicality of sign stable projections in learning and search applications. The previous idea of using the signs from normal random projections has been widely adopted in practice, for approximating correlations. Given the widespread use of the χ2 similarity and the simplicity of our method, we expect the proposed method will be adopted by practitioners.\nFuture research Many interesting future research topics can be studied. (i) The processing cost of conducting stable random projections can be dramatically reduced by very sparse stable random projections [16]. This will make our proposed method even more practical. (ii) We can try to utilize more than just 1-bit of the projected data, i.e., we can study the general coding problem [19]. (iii) Another interesting research would be to study the use of sign stable projections for sparse signal recovery (Compressed Sensing) with stable distributions [21]. (iv) When α → 0, the collision probability becomes Pr (sign(x) ̸= sign(y)) = 12 − 1 2Resemblance, which provides an elegant mechanism for computing resemblance (of the binary-quantized data) in sparse data streams.\nAcknowledgement The work of Ping Li is supported by NSF-III-1360971, NSF-Bigdata1419210, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137. The work of Gennady Samorodnitsky is supported by ARO-W911NF-12-10385.\nReferences [1] http://googleresearch.blogspot.com/2010/04/ lessons-learned-developing-practical.html.\n[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an object? In CVPR, pages 73–80, 2010.\n[3] Leon Bottou. http://leon.bottou.org/projects/sgd.\n[4] Olivier Chapelle, Patrick Haffner, and Vladimir N. Vapnik. Support vector machines for histogram-based image classification. IEEE Trans. Neural Networks, 10(5):1055–1064, 1999.\n[5] Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC, 2002.\n[6] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874, 2008.\n[7] Yoav Freund, Sanjoy Dasgupta, Mayank Kabra, and Nakul Verma. Learning the structure of manifolds using random projections. In NIPS, Vancouver, BC, Canada, 2008.\n[8] Jerome H. Friedman, F. Baskett, and L. Shustek. An algorithm for finding nearest neighbors. IEEE Transactions on Computers, 24:1000–1006, 1975.\n[9] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of ACM, 42(6):1115–1145, 1995.\n[10] Matthias Hein and Olivier Bousquet. Hilbertian metrics and positive definite kernels on probability measures. In AISTATS, pages 136–143, Barbados, 2005.\n[11] Piotr Indyk. Stable distributions, pseudorandom generators, embeddings, and data stream computation. Journal of ACM, 53(3):307–323, 2006.\n[12] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse of dimensionality. In STOC, pages 604–613, Dallas, TX, 1998.\n[13] Yugang Jiang, Chongwah Ngo, and Jun Yang. Towards optimal bag-of-features for object categorization and semantic video retrieval. In CIVR, pages 494–501, Amsterdam, Netherlands, 2007.\n[14] Thorsten Joachims. Training linear svms in linear time. In KDD, pages 217–226, Pittsburgh, PA, 2006.\n[15] Fuxin Li, Guy Lebanon, and Cristian Sminchisescu. A linear approximation to the χ2 kernel with geometric convergence. Technical report, arXiv:1206.4074, 2013.\n[16] Ping Li. Very sparse stable random projections for dimension reduction in lα (0 < α ≤ 2) norm. In KDD, San Jose, CA, 2007.\n[17] Ping Li. Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections. In SODA, pages 10 – 19, San Francisco, CA, 2008.\n[18] Ping Li. Improving compressed counting. In UAI, Montreal, CA, 2009.\n[19] Ping Li, Michael Mitzenmacher, and Anshumali Shrivastava. Coding for random projections. 2013.\n[20] Ping Li, Art B Owen, and Cun-Hui Zhang. One permutation hashing. In NIPS, Lake Tahoe, NV, 2012.\n[21] Ping Li, Cun-Hui Zhang, and Tong Zhang. Compressed counting meets compressed sensing. 2013.\n[22] S. Muthukrishnan. Data streams: Algorithms and applications. Foundations and Trends in Theoretical Computer Science, 1:117–236, 2 2005.\n[23] Noam Nisan. Pseudorandom generators for space-bounded computations. In STOC, 1990.\n[24] Gennady Samorodnitsky and Murad S. Taqqu. Stable Non-Gaussian Random Processes. Chapman & Hall, New York, 1994.\n[25] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In ICML, pages 807–814, Corvalis, Oregon, 2007.\n[26] Andrea Vedaldi and Andrew Zisserman. Efficient additive kernels via explicit feature maps. IEEE Trans. Pattern Anal. Mach. Intell., 34(3):480–492, 2012.\n[27] Sreekanth Vempati, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Generalized rbf feature maps for efficient detection. In BMVC, pages 1–11, Aberystwyth, UK, 2010.\n[28] Gang Wang, Derek Hoiem, and David A. Forsyth. Building text features for object image classification. In CVPR, pages 1367–1374, Miami, Florida, 2009.\n[29] Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv, Thomas S. Huang, and Yihong Gong. Localityconstrained linear coding for image classification. In CVPR, pages 3360–3367, San Francisco, CA, 2010.\n[30] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature hashing for large scale multitask learning. In ICML, pages 1113–1120, 2009.\n[31] Haiquan (Chuck) Zhao, Nan Hua, Ashwin Lall, Ping Li, Jia Wang, and Jun Xu. Towards a universal sketch for origin-destination network measurements. In Network and Parallel Computing, pages 201–213, 2011."
    } ],
    "references" : [ {
      "title" : "What is an object",
      "author" : [ "Bogdan Alexe", "Thomas Deselaers", "Vittorio Ferrari" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Support vector machines for histogram-based image classification",
      "author" : [ "Olivier Chapelle", "Patrick Haffner", "Vladimir N. Vapnik" ],
      "venue" : "IEEE Trans. Neural Networks,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1999
    }, {
      "title" : "Similarity estimation techniques from rounding algorithms",
      "author" : [ "Moses S. Charikar" ],
      "venue" : "In STOC,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "Learning the structure of manifolds using random projections",
      "author" : [ "Yoav Freund", "Sanjoy Dasgupta", "Mayank Kabra", "Nakul Verma" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "An algorithm for finding nearest neighbors",
      "author" : [ "Jerome H. Friedman", "F. Baskett", "L. Shustek" ],
      "venue" : "IEEE Transactions on Computers,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1975
    }, {
      "title" : "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming",
      "author" : [ "Michel X. Goemans", "David P. Williamson" ],
      "venue" : "Journal of ACM,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1995
    }, {
      "title" : "Hilbertian metrics and positive definite kernels on probability measures",
      "author" : [ "Matthias Hein", "Olivier Bousquet" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "Stable distributions, pseudorandom generators, embeddings, and data stream computation",
      "author" : [ "Piotr Indyk" ],
      "venue" : "Journal of ACM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Approximate nearest neighbors: Towards removing the curse of dimensionality",
      "author" : [ "Piotr Indyk", "Rajeev Motwani" ],
      "venue" : "In STOC,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1998
    }, {
      "title" : "Towards optimal bag-of-features for object categorization and semantic video retrieval",
      "author" : [ "Yugang Jiang", "Chongwah Ngo", "Jun Yang" ],
      "venue" : "In CIVR,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Training linear svms in linear time",
      "author" : [ "Thorsten Joachims" ],
      "venue" : "In KDD,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "A linear approximation to the χ2 kernel with geometric convergence",
      "author" : [ "Fuxin Li", "Guy Lebanon", "Cristian Sminchisescu" ],
      "venue" : "Technical report,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Very sparse stable random projections for dimension reduction in lα (0 < α ≤ 2) norm",
      "author" : [ "Ping Li" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections",
      "author" : [ "Ping Li" ],
      "venue" : "In SODA,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Improving compressed counting",
      "author" : [ "Ping Li" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Coding for random projections",
      "author" : [ "Ping Li", "Michael Mitzenmacher", "Anshumali Shrivastava" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "One permutation hashing",
      "author" : [ "Ping Li", "Art B Owen", "Cun-Hui Zhang" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Compressed counting meets compressed sensing",
      "author" : [ "Ping Li", "Cun-Hui Zhang", "Tong Zhang" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Data streams: Algorithms and applications",
      "author" : [ "S. Muthukrishnan" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science, 1:117–236,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Pseudorandom generators for space-bounded computations",
      "author" : [ "Noam Nisan" ],
      "venue" : "In STOC,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1990
    }, {
      "title" : "Stable Non-Gaussian Random Processes",
      "author" : [ "Gennady Samorodnitsky", "Murad S. Taqqu" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1994
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro" ],
      "venue" : "In ICML,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2007
    }, {
      "title" : "Efficient additive kernels via explicit feature maps",
      "author" : [ "Andrea Vedaldi", "Andrew Zisserman" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Generalized rbf feature maps for efficient detection",
      "author" : [ "Sreekanth Vempati", "Andrea Vedaldi", "Andrew Zisserman", "C.V. Jawahar" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Building text features for object image classification",
      "author" : [ "Gang Wang", "Derek Hoiem", "David A. Forsyth" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2009
    }, {
      "title" : "Localityconstrained linear coding for image classification",
      "author" : [ "Jinjun Wang", "Jianchao Yang", "Kai Yu", "Fengjun Lv", "Thomas S. Huang", "Yihong Gong" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "Feature hashing for large scale multitask learning",
      "author" : [ "Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg" ],
      "venue" : "In ICML,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "Towards a universal sketch for origin-destination network measurements",
      "author" : [ "Haiquan (Chuck) Zhao", "Nan Hua", "Ashwin Lall", "Ping Li", "Jia Wang", "Jun Xu" ],
      "venue" : "In Network and Parallel Computing,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "For example, Winner of 2009 PASCAL image classification challenge used millions of features [29].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 27,
      "context" : "[1, 30] described applications with billion or trillion features.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 7,
      "context" : "The chi-square similarity is an instance of the Hilbertian metrics, which are defined over probability space [10] and suitable for data generated from histograms.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "Empirical studies have demonstrated the superiority of the χ(2) distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 7,
      "context" : "Empirical studies have demonstrated the superiority of the χ(2) distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "Empirical studies have demonstrated the superiority of the χ(2) distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "Empirical studies have demonstrated the superiority of the χ(2) distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 25,
      "context" : "Empirical studies have demonstrated the superiority of the χ(2) distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 24,
      "context" : "Empirical studies have demonstrated the superiority of the χ(2) distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "Empirical studies have demonstrated the superiority of the χ(2) distance over l2 or l1 distances for image and text classification tasks [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : ", [7]) for reducing the data dimensions and data sizes, to facilitate",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 8,
      "context" : "More generally, the method of stable random projections [11, 17] provides an efficient algorithm to compute the lα distances (0 < α ≤ 2).",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "More generally, the method of stable random projections [11, 17] provides an efficient algorithm to compute the lα distances (0 < α ≤ 2).",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "When α = 2, this probability can be analytically evaluated [9] (or via a simple geometric argument):",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "P2 = Pr (sign(xj) ̸= sign(yj)) = 1 π cos−1 ρ2 (5) which is an important result known as sim-hash [5].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "This idea is inspired by b-bit minwise hashing [20], which was designed for binary sparse data.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "This scheme also leads to an efficient near neighbor search algorithm [8, 12].",
      "startOffset" : 70,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "This scheme also leads to an efficient near neighbor search algorithm [8, 12].",
      "startOffset" : 70,
      "endOffset" : 77
    }, {
      "referenceID" : 19,
      "context" : "In modern applications, massive datasets are often generated in a streaming fashion, which are difficult to transmit and store [22], as the processing is done on the fly in one-pass of the data.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "In the standard turnstile model [22], a data stream can be viewed as high-dimensional vector with the entry values changing over time.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 28,
      "context" : "Clearly, the turnstile data stream model is particularly suitable for describing histograms and it is also a standard model for network traffic summarization and monitoring [31].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 20,
      "context" : "Basically, entries of the projection matrix R ∈ RD×k are (re)generated as needed using pseudo-random number techniques [23].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "This fact was recently exploited by another data stream algorithm named Compressed Counting (CC) [18] for estimating the Shannon entropy of streams.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "For example, [15] proposed a nice technique to approximate ρχ2 by first expanding the data from D dimensions to (e.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "To compare these two types of χ(2) kernels with “linear” kernel, we also test the same data using LIBLINEAR [6] after normalizing the data to have unit Euclidian norm, i.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 21,
      "context" : "Here Z ∼ S(α, γ) denotes a symmetric α-stable distribution with scale γ, whose characteristic function [24] is E ( e √ −1Zt ) = e−γ|t| α .",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "normal), the collision probability Pr (sign(x) ̸= sign(y)) is known [5, 9].",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "normal), the collision probability Pr (sign(x) ̸= sign(y)) is known [5, 9].",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "For example, we can view the data (ui, vi) as empirical probabilities, which are common when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 159,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "For example, we can view the data (ui, vi) as empirical probabilities, which are common when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 159,
      "endOffset" : 185
    }, {
      "referenceID" : 10,
      "context" : "For example, we can view the data (ui, vi) as empirical probabilities, which are common when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 159,
      "endOffset" : 185
    }, {
      "referenceID" : 0,
      "context" : "For example, we can view the data (ui, vi) as empirical probabilities, which are common when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 159,
      "endOffset" : 185
    }, {
      "referenceID" : 25,
      "context" : "For example, we can view the data (ui, vi) as empirical probabilities, which are common when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 159,
      "endOffset" : 185
    }, {
      "referenceID" : 24,
      "context" : "For example, we can view the data (ui, vi) as empirical probabilities, which are common when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 159,
      "endOffset" : 185
    }, {
      "referenceID" : 23,
      "context" : "For example, we can view the data (ui, vi) as empirical probabilities, which are common when data are generated from histograms (as popular in NLP and vision) [4, 10, 13, 2, 28, 27, 26].",
      "startOffset" : 159,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "It is known that the χ(2)-kernel is PD [10].",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using χ(2) similarity compared to other measures such as l2 distance.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using χ(2) similarity compared to other measures such as l2 distance.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using χ(2) similarity compared to other measures such as l2 distance.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using χ(2) similarity compared to other measures such as l2 distance.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 25,
      "context" : "Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using χ(2) similarity compared to other measures such as l2 distance.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 24,
      "context" : "Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using χ(2) similarity compared to other measures such as l2 distance.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 23,
      "context" : "Many prior studies [4, 10, 13, 2, 28, 27, 26] have shown the advantage of using χ(2) similarity compared to other measures such as l2 distance.",
      "startOffset" : 19,
      "endOffset" : 45
    }, {
      "referenceID" : 8,
      "context" : "The method of α-stable random projections (0 < α ≤ 2) [11, 17] is popular for efficiently computing the lα distances in massive (streaming) data.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "The method of α-stable random projections (0 < α ≤ 2) [11, 17] is popular for efficiently computing the lα distances in massive (streaming) data.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "(i) The processing cost of conducting stable random projections can be dramatically reduced by very sparse stable random projections [16].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : ", we can study the general coding problem [19].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 18,
      "context" : "(iii) Another interesting research would be to study the use of sign stable projections for sparse signal recovery (Compressed Sensing) with stable distributions [21].",
      "startOffset" : 162,
      "endOffset" : 166
    } ],
    "year" : 2013,
    "abstractText" : "The method of stable random projections is useful for efficiently approximating the lα distance (0 < α ≤ 2) in high dimension and it is naturally suitable for data streams. In this paper, we propose to use only the signs of the projected data and we analyze the probability of collision (i.e., when the two signs differ). Interestingly, when α = 1 (i.e., Cauchy random projections), we show that the probability of collision can be accurately approximated as functions of the chi-square (χ) similarity. In text and vision applications, the χ similarity is a popular measure when the features are generated from histograms (which are a typical example of data streams). Experiments confirm that the proposed method is promising for large-scale learning applications. The full paper is available at arXiv:1308.1009. There are many future research problems. For example, when α → 0, the collision probability is a function of the resemblance (of the binary-quantized data). This provides an effective mechanism for resemblance estimation in data streams.",
    "creator" : null
  }
}