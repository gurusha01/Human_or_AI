{
  "name" : "076a0c97d09cf1a0ec3e19c7f2529f2b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Kernel Test for Three-Variable Interactions",
    "authors" : [ "Dino Sejdinovic", "Arthur Gretton", "Wicher Bergsma" ],
    "emails" : [ "arthur.gretton}@gmail.com", "w.p.bergsma@lse.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The problem of nonparametric testing of interaction between variables has been widely treated in the machine learning and statistics literature. Much of the work in this area focuses on measuring or testing pairwise interaction: for instance, the Hilbert-Schmidt Independence Criterion (HSIC) or Distance Covariance [1, 2, 3], kernel canonical correlation [4, 5, 6], and mutual information [7]. In cases where more than two variables interact, however, the questions we can ask about their interaction become significantly more involved. The simplest case we might consider is whether the variables are mutually independent, PX = ∏d i=1 PXi , as considered in R d by [8]. This is already a more general question than pairwise independence, since pairwise independence does not imply total (mutual) independence, while the implication holds in the other direction. For example, if X and Y are i.i.d. uniform on {−1, 1}, then (X,Y,XY ) is a pairwise independent but mutually dependent triplet [9]. Tests of total and pairwise independence are insufficient, however, since they do not rule out all third order factorizations of the joint distribution.\nAn important class of high order interactions occurs when the simultaneous effect of two variables on a third may not be additive. In particular, it may be possible that X ⊥ Z and Y ⊥ Z , whereas ¬ ((X,Y ) ⊥ Z) (for example, neither adding sugar to coffee nor stirring the coffee individually have an effect on its sweetness but the joint presence of the two does). In addition, study of three-variable interactions can elucidate certain switching mechanisms between positive and negative correlation of two genes expressions, as controlled by a third gene [10]. The presence of such interactions is typically tested using some form of analysis of variance (ANOVA) model which includes additional interaction terms, such as products of individual variables. Since each such additional term requires a new hypothesis test, this increases the risk that some hypothesis test will produce a false positive by chance. Therefore, a test that is able to directly detect the presence of any kind of higher-order interaction would be of a broad interest in statistical modeling. In the present work, we provide to our knowledge the first nonparametric test for three-variable interaction. This work generalizes the HSIC test of pairwise independence, and has as its test statistic the norm\nof an embedding of an appropriate signed measure to a reproducing kernel Hilbert space (RKHS). When the statistic is non-zero, all third order factorizations can be ruled out. Moreover, this test is applicable to the cases where X , Y and Z are themselves multivariate objects, and may take values in non-Euclidean or structured domains.1\nOne important application of interaction measures is in learning structure for graphical models. If the graphical model is assumed to be Gaussian, then second order interaction statistics may be used to construct an undirected graph [11, 12]. When the interactions are non-Gaussian, however, other approaches are brought to bear. An alternative approach to structure learning is to employ conditional independence tests. In the PC algorithm [13, 14, 15], a V-structure (a directed graphical model with two independent parents pointing to a single child) is detected when an independence test between the parent variables accepts the null hypothesis, while a test of dependence of the parents conditioned on the child rejects the null hypothesis. The PC algorithm gives a correct equivalence class of structures subject to the causal Markov and faithfulness assumptions, in the absence of hidden common causes. The original implementations of the PC algorithm rely on partial correlations for testing, and assume Gaussianity. A number of algorithms have since extended the basic PC algorithm to arbitrary probability distributions over multivariate random variables [16, 17, 18], by using nonparametric kernel independence tests [19] and conditional dependence tests [20, 18]. We observe that our Lancaster interaction based test provides a strong alternative to the conditional dependence testing approach, and is seen to outperform earlier approaches in detecting cases where independent parent variables weakly influence the child variable when considered individually, but have a strong combined influence.\nWe begin our presentation in Section 2 with a definition of interaction measures, these being the signed measures we will embed in an RKHS. We cover this embedding procedure in Section 3. We then proceed in Section 4 to define pairwise and three way interactions. We describe a statistic to test mutual independence for more than three variables, and provide a brief overview of the more complex high-order interactions that may be observed when four or more variables are considered. Finally, we provide experimental benchmarks in Section 5."
    }, {
      "heading" : "2 Interaction Measure",
      "text" : "An interaction measure [21, 22] associated to a multidimensional probability distribution P of a random vector (X1, . . . , XD) taking values in the product spaceX1×· · ·×XD is a signed measure ∆P that vanishes whenever P can be factorised in a non-trivial way as a product of its (possibly multivariate) marginal distributions. For the cases D = 2, 3 the correct interaction measure coincides with the the notion introduced by Lancaster [21] as a formal product\n∆LP =\nD∏\ni=1\n( P ∗Xi − PXi ) , (1)\nwhere each product ∏D′ j=1 P ∗ Xij\nsignifies the joint probability distribution PXi1 ···XiD′ of a subvector( Xi1 , . . . , XiD′ ) . We will term the signed measure in (1) the Lancaster interaction measure. In the case of a bivariate distribution, the Lancaster interaction measure is simply the difference between the joint probability distribution and the product of the marginal distributions (the only possible non-trivial factorization for D = 2), ∆LP = PXY − PXPY , while in the case D = 3, we obtain\n∆LP = PXY Z − PXY PZ − PY ZPX − PXZPY + 2PXPY PZ . (2)\nIt is readily checked that\n(X,Y ) ⊥ Z ∨ (X,Z) ⊥ Y ∨ (Y, Z) ⊥ X ⇒ ∆LP = 0. (3)\nFor D > 3, however, (1) does not capture all possible factorizations of the joint distribution, e.g., for D = 4, it need not vanish if (X1, X2) ⊥ (X3, X4), but X1 and X2 are dependent and X3 and X4 are dependent. Streitberg [22] corrected this definition using a more complicated construction with the Möbius function on the lattice of partitions, which we describe in Section 4.3. In this\n1As the reader might imagine, the situation becomes more complex again when four or more variables interact simultaneously; we provide a brief technical overview in Section 4.3.\nwork, however, we will focus on the case of three variables and formulate interaction tests based on embedding of (2) into an RKHS.\nThe implication (3) states that the presence of Lancaster interaction rules out the possibility of any factorization of the joint distribution, but the converse is not generally true; see Appendix C for details. In addition, it is important to note the distinction between the absence of Lancaster interaction and the total (mutual) independence of (X,Y, Z), i.e., PXY Z = PXPY PZ . While total independence implies the absence of Lancaster interaction, the signed measure ∆totP = PXY Z−PXPY PZ associated to the total (mutual) independence of (X,Y, Z) does not vanish if, e.g., (X,Y ) ⊥ Z , but X and Y are dependent.\nIn this contribution, we construct the non-parametric test for the hypothesis ∆LP = 0 (no Lancaster interaction), as well as the non-parametric test for the hypothesis ∆totP = 0 (total independence), based on the embeddings of the corresponding signed measures ∆LP and ∆totP into an RKHS. Both tests are particularly suited to the cases where X , Y and Z take values in a high-dimensional space, and, moreover, they remain valid for a variety of non-Euclidean and structured domains, i.e., for all topological spaces where it is possible to construct a valid positive definite function; see [23] for details. In the case of total independence testing, our approach can be viewed as a generalization of the tests proposed in [24] based on the empirical characteristic functions."
    }, {
      "heading" : "3 Kernel Embeddings",
      "text" : "We review the embedding of signed measures to a reproducing kernel Hilbert space. The RKHS norms of such embeddings will then serve as our test statistics. Let Z be a topological space. According to the Moore-Aronszajn theorem [25, p. 19], for every symmetric, positive definite function (henceforth kernel) k : Z × Z → R, there is an associated reproducing kernel Hilbert space (RKHS)Hk of real-valued functions on Z with reproducing kernel k. The map ϕ : Z → Hk, ϕ : z 7→ k(·, z) is called the canonical feature map or the Aronszajn map of k. Denote byM(Z) the Banach space of all finite signed Borel measures on Z . The notion of a feature map can then be extended to kernel embeddings of elements ofM(Z) [25, Chapter 4].\nDefinition 1. (Kernel embedding) Let k be a kernel on Z , and ν ∈M(Z). The kernel embedding of ν into the RKHSHk is µk(ν) ∈ Hk such that ´\nf(z)dν(z) = 〈f, µk(ν)〉Hk for all f ∈ Hk.\nAlternatively, the kernel embedding can be defined by the Bochner integral µk(ν) = ´\nk(·, z) dν(z). If a measurable kernel k is a bounded function, it is straightforward to show using the Riesz representation theorem that µk(ν) exists for all ν ∈ M(Z).2 For many interesting bounded kernels k, including the Gaussian, Laplacian and inverse multiquadratics, the embedding µk :M(Z)→ Hk is injective. Such kernels are said to be integrally strictly positive definite (ISPD) [26, p. 4]. A related but weaker notion is that of a characteristic kernel [20, 27], which requires the kernel embedding to be injective only on the setM1+(Z) of probability measures. In the case that k is ISPD, since Hk is a Hilbert space, we can introduce a notion of an inner product between two signed measures ν, ν′ ∈ M(Z),\n〈〈ν, ν′〉〉k := 〈µk(ν), µk(ν ′)〉Hk =\nˆ\nk(z, z′)dν(z)dν′(z′).\nSince µk is injective, this is a valid inner product and induces a norm on M(Z), for which ‖ν‖k = 〈〈ν, ν〉〉 1/2 k = 0 if and only if ν = 0. This fact has been used extensively in the literature to formulate: (a) a nonparametric two-sample test based on estimation of maximum mean discrepancy ‖P −Q‖k, for samples {Xi} n i=1 i.i.d. ∼ P , {Yi} m i=1 i.i.d. ∼ Q [28] and (b) a nonparametric independence test based on estimation of ‖PXY − PXPY ‖k⊗l, for a joint sample {(Xi, Yi)} n i=1 i.i.d. ∼ PXY [19] (the latter is also called a Hilbert-Schmidt independence criterion), with kernel k ⊗ l on the product space defined as k(x, x′)l(y, y′). When a bounded characteristic kernel is used, the above tests are consistent against all alternatives, and their alternative interpretation is as a generalization [29, 3] of energy distance [30, 31] and distance covariance [2, 32].\n2Unbounded kernels can also be considered, however [3]. In this case, one can still study embeddings\nof the signed measures M 1/2 k (Z) ⊂ M(Z), which satisfy a finite moment condition, i.e., M 1/2 k (Z) = {\nν ∈ M(Z) : ´ k1/2(z, z) d|ν|(z) < ∞ } .\nIn this article, we extend this approach to the three-variable case, and formulate tests for both the Lancaster interaction and for the total independence, using simple consistent estimators of ‖∆LP‖k⊗l⊗m and ‖∆totP‖k⊗l⊗m respectively, which we describe in the next Section. Using the same arguments as in the tests of [28, 19], these tests are also consistent against all alternatives as long as ISPD kernels are used."
    }, {
      "heading" : "4 Interaction Tests",
      "text" : "Notational remarks: Throughout the paper, ◦ denotes an Hadamard (entrywise) product. Let A be an n×n matrix, and K a symmetric n×n matrix. We will fix the following notational conventions: 1 denotes an n × 1 column of ones; A+j = ∑n i=1 Aij denotes the sum of all elements of the j-th column of A; Ai+ = ∑n\nj=1 Aij denotes the sum of all elements of the i-th row of A; A++ = ∑n\ni=1 ∑n j=1 Aij denotes the sum of all elements of A; K+ = 11 ⊤K , i.e., [K+]ij = K+j = Kj+,\nand [ K⊤+ ] ij = Ki+ = K+i."
    }, {
      "heading" : "4.1 Two-Variable (Independence) Test",
      "text" : "We provide a short overview of the kernel independence test of [19], which we write as the RKHS norm of the embedding of a signed measure. While this material is not new (it appears in [28, Section 7.4]), it will help define how to proceed when a third variable is introduced, and the signed measures become more involved. We begin by expanding the squared RKHS norm ‖PXY − PXPY ‖ 2 k⊗l as inner products, and applying the reproducing property,\n‖PXY − PXPY ‖ 2 k⊗l = EXY EX′Y ′k(X,X ′)l(Y, Y ′) + EXEX′k(X,X ′)EY EY ′ l(Y, Y ′)\n− 2EX′Y ′ [EXk(X,X ′)EY l(Y, Y ′)] , (4)\nwhere (X,Y ) and (X ′, Y ′) are independent copies of random variables on X ×Y with distribution PXY .\nGiven a joint sample {(Xi, Yi)} n i=1 i.i.d. ∼ PXY , an empirical estimator of ‖PXY − PXPY ‖ 2 k⊗l is obtained by substituting corresponding empirical means into (4), which can be represented using Gram matrices K and L (Kij = k(Xi, Xj), Lij = l(Yi, Yj)),\nÊXY ÊX′Y ′k(X,X ′)l(Y, Y ′) =\n1\nn2\nn∑\na=1\nn∑\nb=1\nKabLab = 1\nn2 (K ◦ L)++ ,\nÊX ÊX′k(X,X ′)ÊY ÊY ′ l(Y, Y ′) = 1\nn4\nn∑\na=1\nn∑\nb=1\nn∑\nc=1\nn∑\nd=1\nKabLcd = 1\nn4 K++L++,\nÊX′Y ′\n[\nÊXk(X,X ′)ÊY l(Y, Y ′) ] = 1\nn3\nn∑\na=1\nn∑\nb=1\nn∑\nc=1\nKacLbc = 1\nn3 (KL)++ .\nSince these are V-statistics [33, Ch. 5], there is a bias of OP (n −1); U-statistics may be used if an unbiased estimate is needed. Each of the terms above corresponds to an estimate of an inner product 〈〈ν, ν′〉〉k⊗l for probability measures ν and ν\n′ taking values in {PXY , PXPY }, as summarized in Table 1. Even though the second and third terms involve triple and quadruple sums, each of the empirical means can be computed using sums of all terms of certain matrices, where the dominant computational cost is in computing the matrix product KL. In fact, the overall estimator can be\ncomputed in an even simpler form (see Proposition 9 in Appendix F), as ∥ ∥ ∥P̂XY − P̂X P̂Y ∥ ∥ ∥ 2\nk⊗l =\n1 n2 (K ◦HLH)++ , where H = I− 1 n11 ⊤ is the centering matrix. Note that by the idempotence of H , we also have that (K ◦HLH)++ = (HKH ◦HLH)++. In the rest of the paper, for any Gram matrix K , we will denote its corresponding centered matrix HKH by K̃. When three variables are present, a two-variable test already allows us to determine whether for instance (X,Y ) ⊥ Z , i.e., whether PXY Z = PXY PZ . It is sufficient to treat (X,Y ) as a single variable on the product space X ×Y , with the product kernel k⊗ l. Then, the Gram matrix associated to (X,Y ) is simply K ◦L,\nand the corresponding V -statistic is 1n2\n( K ◦ L ◦ M̃ )\n++ .3 What is not obvious, however, is if a\nV-statistic for the Lancaster interaction (which can be thought of as a surrogate for the composite hypothesis of various factorizations) can be obtained in a similar form. We will address this question in the next section."
    }, {
      "heading" : "4.2 Three-Variable Tests",
      "text" : "As in the two-variable case, it suffices to derive V-statistics for inner products 〈〈ν, ν′〉〉k⊗l⊗m, where ν and ν′ take values in all possible combinations of the joint and the products of the marginals, i.e., PXY Z , PXY PZ , etc. Again, it is easy to see that these can be expressed as certain expectations of kernel functions, and thereby can be calculated by an appropriate manipulation of the three Gram matrices. We summarize the resulting expressions in Table 2 - their derivation is a tedious but straightforward linear algebra exercise. For compactness, the appropriate normalizing terms are moved inside the measures considered.\nBased on the individual RKHS inner product estimators, we can now easily derive estimators for various signed measures arising as linear combinations of PXY Z , PXY PZ , and so on. The first such measure is an “incomplete” Lancaster interaction measure∆(Z)P = PXY Z+PXPY PZ−PY ZPX− PXZPY , which vanishes if (Y, Z) ⊥ X or (X,Z) ⊥ Y , but not necessarily if (X,Y ) ⊥ Z . We obtain the following result for the empirical measure P̂ .\nProposition 2 (Incomplete Lancaster interaction). ∥ ∥ ∥∆(Z)P̂ ∥ ∥ ∥ 2\nk⊗l⊗m = 1n2\n( K̃ ◦ L̃ ◦M )\n++ .\nAnalogous expressions hold for ∆(X)P̂ and ∆(Y )P̂ . Unlike in the two-variable case where either matrix or both can be centered, centering of each matrix in the three-variable case has a different meaning. In particular, one requires centering of all three kernel matrices to perform a “complete” Lancaster interaction test, as given by the following Proposition.\nProposition 3 (Lancaster interaction). ∥ ∥ ∥∆LP̂ ∥ ∥ ∥ 2\nk⊗l⊗m = 1n2\n( K̃ ◦ L̃ ◦ M̃ )\n++ .\nThe proofs of these Propositions are given in Appendix A. We summarize various hypotheses and the associated V-statistics in the Appendix B. As we will demonstrate in the experiments in Section 5, while particularly useful for testing the factorization hypothesis, i.e., for (X,Y ) ⊥ Z ∨ (X,Z) ⊥ Y ∨ (Y, Z) ⊥ X , the statistic ∥ ∥ ∥∆LP̂ ∥ ∥ ∥ 2\nk⊗l⊗m can also be used for powerful tests of either the\nindividual hypotheses (Y, Z) ⊥ X , (X,Z) ⊥ Y , or (X,Y ) ⊥ Z , or for total independence testing,\n3In general, however, this approach would require some care since, e.g., X and Y could be measured on very different scales, and the choice of kernels k and l needs to take this into account.\ni.e., PXY Z = PXPY PZ , as it vanishes in all of these cases. The null distribution under each of these hypotheses can be estimated using a standard permutation-based approach described in Appendix D.\nAnother way to obtain the Lancaster interaction statistic is as the RKHS norm of the joint “central moment” ΣXY Z = EXY Z [(kX − µX) ⊗ (lY − µY ) ⊗ (mZ − µZ)] of RKHS-valued random variables kX , lY and mZ (understood as an element of the tensor RKHS Hk ⊗ Hl ⊗Hm). This is related to a classical characterization of the Lancaster interaction [21, Ch. XII]: there is no Lancaster interaction between X , Y and Z if and only if cov [f(X), g(Y ), h(Z)] = 0 for all L2 functions f , g and h. There is an analogous result in our case (proof is given in Appendix A), which states\nProposition 4. ‖∆LP‖k⊗l⊗m = 0 if and only if cov [f(X), g(Y ), h(Z)] = 0 for all f ∈ Hk, g ∈ Hl, h ∈ Hm.\nAnd finally, we give an estimator of the RKHS norm of the total independence measure ∆totP .\nProposition 5 (Total independence). Let ∆totP̂ = P̂XY Z − P̂X P̂Y P̂Z . Then: ∥ ∥ ∥∆totP̂ ∥ ∥ ∥ 2\nk⊗l⊗m =\n1\nn2 (K ◦ L ◦M)++ −\n2\nn4 tr(K+ ◦ L+ ◦M+) +\n1\nn6 K++L++M++.\nThe proof follows simply from reading off the corresponding inner-product V-statistics from the Table 2. While the test statistic for total independence has a somewhat more complicated form than that of Lancaster interaction, it can also be computed in quadratic time."
    }, {
      "heading" : "4.3 Interaction for D > 3",
      "text" : "Streitberg’s correction of the interaction measure for D > 3 has the form\n∆SP = ∑\nπ\n(−1)|π|−1 (|π| − 1)!JπP, (5)\nwhere the sum is taken over all partitions of the set {1, 2, . . . , n}, |π| denotes the size of the partition (number of blocks), and Jπ : P 7→ Pπ is the partition operator on probability measures, which for a fixed partition π = π1|π2| . . . |πr maps the probability measure P to the product measure Pπ =∏r\nj=1 Pπj , where Pπj is the marginal distribution of the subvector (Xi : i ∈ πj) . The coefficients correspond to the Möbius inversion on the partition lattice [34]. While the Lancaster interaction has an interpretation in terms of joint central moments, Streitberg’s correction corresponds to joint cumulants [22, Section 4]. Therefore, a central moment expression like EX1...Xn [ ( k (1) X1 − µX1 ) ⊗ · · · ⊗ (\nk (n) Xn − µXn\n)\n] does not capture the correct notion of the interaction measure. Thus, while\none can in principle construct RKHS embeddings of higher-order interaction measures, and compute RKHS norms using a calculus of V -statistics and Gram-matrices analogous to that of Table 2, it does not seem possible to avoid summing over all partitions when computing the corresponding statistics, yielding a computationally prohibitive approach in general. This can be viewed by analogy with the scalar case, where it is well known that the second and third cumulants coincide with the second and third central moments, whereas the higher order cumulants are neither moments nor central moments, but some other polynomials of the moments."
    }, {
      "heading" : "4.4 Total independence for D > 3",
      "text" : "In general, the test statistic for total independence in the D-variable case is ∥ ∥ ∥ ∥ ∥ P̂X1:D − D∏\ni=1\nP̂Xi ∥ ∥ ∥ ∥ ∥ 2\n⊗\nD i=1 k (i)\n= 1\nn2\nn∑\na=1\nn∑\nb=1\nD∏\ni=1\nK (i) ab −\n2\nnD+1\nn∑\na=1\nD∏\ni=1\nn∑\nb=1\nK (i) ab\n+ 1\nn2D\nD∏\ni=1\nn∑\na=1\nn∑\nb=1\nK (i) ab .\nA similar statistic for total independence is discussed by [24] where testing of total independence based on empirical characteristic functions is considered. Our test has a direct interpretation in terms of characteristic functions as well, which is straightforward to see in the case of translation invariant kernels on Euclidean spaces, using their Bochner representation, similarly as in [27, Corollary 4]."
    }, {
      "heading" : "5 Experiments",
      "text" : "We investigate the performance of various permutation based tests that use the Lancaster statistic ∥ ∥ ∥∆LP̂ ∥ ∥ ∥ 2\nk⊗l⊗m and the total independence statistic\n∥ ∥ ∥∆totP̂ ∥ ∥ ∥ 2\nk⊗l⊗m on two synthetic datasets where\nX , Y and Z are random vectors of increasing dimensionality:\nDataset A: Pairwise independent, mutually dependent data. Our first dataset is a triplet of random vectors (X,Y, Z) on Rp × Rp × Rp, with X,Y i.i.d. ∼ N (0, Ip), W ∼ Exp(\n1√ 2 ),\nZ1 = sign(X1Y1)W , and Z2:p ∼ N (0, Ip−1), i.e., the product of X1Y1 determines the sign of Z1, while the remaining p − 1 dimensions are independent (and serve as noise in this example).4 In this case, (X,Y, Z) is clearly a pairwise independent but mutually dependent triplet. The mutual dependence becomes increasingly difficult to detect as the dimensionality p increases.\nDataset B: Joint dependence can be easier to detect. In this example, we consider a triplet of random vectors (X,Y, Z) on Rp × Rp × Rp, with X,Y i.i.d. ∼ N (0, Ip), Z2:p ∼ N (0, Ip−1), and\nZ1 =\n\n \n \nX21 + ǫ, w.p. 1/3, Y 21 + ǫ, w.p. 1/3, X1Y1 + ǫ, w.p. 1/3,\nwhere ǫ ∼ N (0, 0.12). Thus, dependence of Z on pair (X,Y ) is stronger than on X and Y individually.\n4Note that there is no reason for X , Y and Z to have the same dimensionality p - this is done for simplicity of exposition.\nIn all cases, we use permutation tests as described in Appendix D. The test level is set to α = 0.05, sample size to n = 500, and we use gaussian kernels with bandwidth set to the interpoint median distance. In Figure 1, we plot the null hypothesis acceptance rates of the standard kernel twovariable tests for X ⊥ Y (which is true for both datasets A and B, and accepted at the correct rate across all dimensions) and for X ⊥ Z (which is true only for dataset A), as well as of the standard kernel two-variable test for (X,Y ) ⊥ Z , and the test for (X,Y ) ⊥ Z using the Lancaster statistic. As expected, in dataset B, we see that dependence of Z on pair (X,Y ) is somewhat easier to detect than on X individually with two-variable tests. In both datasets, however, the Lancaster interaction appears significantly more sensitive in detecting this dependence as dimensionality p\nincreases. Figure 2 plots the Type II error of total independence tests with statistics ∥ ∥ ∥∆LP̂ ∥ ∥ ∥ 2\nk⊗l⊗m\nand ∥ ∥ ∥∆totP̂ ∥ ∥ ∥ 2\nk⊗l⊗m . The Lancaster statistic outperforms the total independence statistic everywhere\napart from the Dataset B when the number of dimensions is small (between 1 and 5). Figure 3 plots the Type II error of the factorization test, i.e., test for (X,Y ) ⊥ Z ∨ (X,Z) ⊥ Y ∨ (Y, Z) ⊥ X with Lancaster statistic with Holm-Bonferroni correction as described in Appendix D, as well as the two-variable based test (which performs three standard two-variable tests and applies the HolmBonferroni correction). We also plot the Type II error for the conditional independence test for X ⊥ Y |Z from [18]. Under assumption that X ⊥ Y (correct on both datasets), negation of each of these three hypotheses is equivalent to the presence of V-structure X → Z ← Y , so the rejection of the null can be viewed as a V-structure detection procedure. As dimensionality increases, the Lancaster statistic appears significantly more sensitive to the interactions present than the competing approaches, which is particularly pronounced in Dataset A."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We have constructed permutation-based nonparametric tests for three-variable interactions, including the Lancaster interaction and total independence. The tests can be used in datasets where only higher-order interactions persist, i.e., variables are pairwise independent; as well as in cases where joint dependence may be easier to detect than pairwise dependence, for instance when the effect of two variables on a third is not additive. The flexibility of the framework of RKHS embeddings of signed measures allows us to consider variables that are themselves multidimensional. While the total independence case readily generalizes to more than three dimensions, the combinatorial nature of joint cumulants implies that detecting interactions of higher order requires significantly more costly computation."
    } ],
    "references" : [ {
      "title" : "Measuring statistical dependence with Hilbert- Schmidt norms",
      "author" : [ "A. Gretton", "O. Bousquet", "A. Smola", "B. Schölkopf" ],
      "venue" : "ALT, pages 63–78",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Measuring and testing dependence by correlation of distances",
      "author" : [ "G. Székely", "M. Rizzo", "N.K. Bakirov" ],
      "venue" : "Ann. Stat., 35(6):2769–2794",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Equivalence of distance-based and RKHS-based statistics in hypothesis testing",
      "author" : [ "D. Sejdinovic", "B. Sriperumbudur", "A. Gretton", "K. Fukumizu" ],
      "venue" : "Ann. Stat., 41(5):2263–2291",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Kernel independent component analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "J. Mach. Learn. Res., 3:1–48",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Statistical consistency of kernel canonical correlation analysis",
      "author" : [ "K. Fukumizu", "F. Bach", "A. Gretton" ],
      "venue" : "J. Mach. Learn. Res., 8:361–383",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Nonlinear canonical analysis and independence tests",
      "author" : [ "J. Dauxois", "G.M. Nkiet" ],
      "venue" : "Ann. Stat., 26(4):1254– 1278",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "and Cs",
      "author" : [ "D. Pal", "B. Poczos" ],
      "venue" : "Szepesvari. Estimation of renyi entropy and mutual information based on generalized nearest-neighbor graphs. In NIPS 23",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Consistent Testing of Total Independence Based on the Empirical Characteristic Function",
      "author" : [ "A. Kankainen" ],
      "venue" : "PhD thesis, University of Jyväskylä",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "The Theory of Probabilities",
      "author" : [ "S. Bernstein" ],
      "venue" : "Gastehizdat Publishing House, Moscow",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1946
    }, {
      "title" : "Efficiently finding genome-wide threeway gene interactions from transcript- and genotype-data",
      "author" : [ "M. Kayano", "I. Takigawa", "M. Shiga", "K. Tsuda", "H. Mamitsuka" ],
      "venue" : "Bioinformatics, 25(21):2735–2743",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "High dimensional graphs and variable selection with the lasso",
      "author" : [ "N. Meinshausen", "P. Buhlmann" ],
      "venue" : "Ann. Stat., 34(3):1436–1462",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "High-dimensional covariance estimation by minimizing  l1-penalized log-determinant divergence",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu" ],
      "venue" : "Electron. J. Stat., 4:935–980",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Causality: Models",
      "author" : [ "J. Pearl" ],
      "venue" : "Reasoning and Inference. Cambridge University Press",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Causation",
      "author" : [ "P. Spirtes", "C. Glymour", "R. Scheines" ],
      "venue" : "Prediction, and Search. 2nd edition",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Estimating high-dimensional directed acyclic graphs with the PC algorithm",
      "author" : [ "M. Kalisch", "P. Buhlmann" ],
      "venue" : "J. Mach. Learn. Res., 8:613–636",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A kernel-based causal learning algorithm",
      "author" : [ "X. Sun", "D. Janzing", "B. Schölkopf", "K. Fukumizu" ],
      "venue" : "ICML, pages 855–862",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Nonlinear directed acyclic structure learning with weakly additive noise models",
      "author" : [ "R. Tillman", "A. Gretton", "P. Spirtes" ],
      "venue" : "NIPS 22",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Kernel-based conditional independence test and application in causal discovery",
      "author" : [ "K. Zhang", "J. Peters", "D. Janzing", "B. Schoelkopf" ],
      "venue" : "UAI, pages 804–813",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A kernel statistical test of independence",
      "author" : [ "A. Gretton", "K. Fukumizu", "C.-H. Teo", "L. Song", "B. Schölkopf", "A. Smola" ],
      "venue" : "NIPS 20, pages 585–592, Cambridge, MA",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Kernel measures of conditional dependence",
      "author" : [ "K. Fukumizu", "A. Gretton", "X. Sun", "B. Schölkopf" ],
      "venue" : "NIPS 20, pages 489–496",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The Chi-Squared Distribution",
      "author" : [ "H.O. Lancaster" ],
      "venue" : "Wiley, London",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Lancaster interactions revisited",
      "author" : [ "B. Streitberg" ],
      "venue" : "Ann. Stat., 18(4):1878–1885",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Characteristic kernels on groups and semigroups",
      "author" : [ "K. Fukumizu", "B. Sriperumbudur", "A. Gretton", "B. Schoelkopf" ],
      "venue" : "NIPS 21, pages 473–480",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Consistent Testing of Total Independence Based on the Empirical Characteristic Function",
      "author" : [ "A. Kankainen" ],
      "venue" : "PhD thesis, University of Jyväskylä",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Reproducing Kernel Hilbert Spaces in Probability and Statistics",
      "author" : [ "A. Berlinet", "C. Thomas-Agnan" ],
      "venue" : "Kluwer",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Universality",
      "author" : [ "B. Sriperumbudur", "K. Fukumizu", "G. Lanckriet" ],
      "venue" : "characteristic kernels and rkhs embedding of measures. J. Mach. Learn. Res., 12:2389–2410",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Hilbert space embeddings and metrics on probability measures",
      "author" : [ "B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "G. Lanckriet", "B. Schölkopf" ],
      "venue" : "J. Mach. Learn. Res., 11:1517–1561",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A kernel two-sample test",
      "author" : [ "A. Gretton", "K. Borgwardt", "M. Rasch", "B. Schölkopf", "A. Smola" ],
      "venue" : "J. Mach. Learn. Res., 13:723–773",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Hypothesis testing using pairwise distances and associated kernels",
      "author" : [ "D. Sejdinovic", "A. Gretton", "B. Sriperumbudur", "K. Fukumizu" ],
      "venue" : "ICML",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Testing for equal distributions in high dimension. InterStat",
      "author" : [ "G. Székely", "M. Rizzo" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2004
    }, {
      "title" : "On a new multivariate two-sample test",
      "author" : [ "L. Baringhaus", "C. Franz" ],
      "venue" : "J. Multivariate Anal., 88(1):190–206",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Brownian distance covariance",
      "author" : [ "G. Székely", "M. Rizzo" ],
      "venue" : "Ann. Appl. Stat., 4(3):1233–1303",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Approximation Theorems of Mathematical Statistics",
      "author" : [ "R. Serfling" ],
      "venue" : "Wiley, New York",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Cumulants and partition lattices",
      "author" : [ "T.P. Speed" ],
      "venue" : "Austral. J. Statist., 25:378–388",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "A simple sequentially rejective multiple test procedure",
      "author" : [ "S. Holm" ],
      "venue" : "Scand. J. Statist., 6(2):65–70",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "A fast",
      "author" : [ "A. Gretton", "K. Fukumizu", "Z. Harchaoui", "B. Sriperumbudur" ],
      "venue" : "consistent kernel two-sample test. In NIPS 22, Red Hook, NY",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Much of the work in this area focuses on measuring or testing pairwise interaction: for instance, the Hilbert-Schmidt Independence Criterion (HSIC) or Distance Covariance [1, 2, 3], kernel canonical correlation [4, 5, 6], and mutual information [7].",
      "startOffset" : 171,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : "Much of the work in this area focuses on measuring or testing pairwise interaction: for instance, the Hilbert-Schmidt Independence Criterion (HSIC) or Distance Covariance [1, 2, 3], kernel canonical correlation [4, 5, 6], and mutual information [7].",
      "startOffset" : 171,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : "Much of the work in this area focuses on measuring or testing pairwise interaction: for instance, the Hilbert-Schmidt Independence Criterion (HSIC) or Distance Covariance [1, 2, 3], kernel canonical correlation [4, 5, 6], and mutual information [7].",
      "startOffset" : 171,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "Much of the work in this area focuses on measuring or testing pairwise interaction: for instance, the Hilbert-Schmidt Independence Criterion (HSIC) or Distance Covariance [1, 2, 3], kernel canonical correlation [4, 5, 6], and mutual information [7].",
      "startOffset" : 211,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : "Much of the work in this area focuses on measuring or testing pairwise interaction: for instance, the Hilbert-Schmidt Independence Criterion (HSIC) or Distance Covariance [1, 2, 3], kernel canonical correlation [4, 5, 6], and mutual information [7].",
      "startOffset" : 211,
      "endOffset" : 220
    }, {
      "referenceID" : 5,
      "context" : "Much of the work in this area focuses on measuring or testing pairwise interaction: for instance, the Hilbert-Schmidt Independence Criterion (HSIC) or Distance Covariance [1, 2, 3], kernel canonical correlation [4, 5, 6], and mutual information [7].",
      "startOffset" : 211,
      "endOffset" : 220
    }, {
      "referenceID" : 6,
      "context" : "Much of the work in this area focuses on measuring or testing pairwise interaction: for instance, the Hilbert-Schmidt Independence Criterion (HSIC) or Distance Covariance [1, 2, 3], kernel canonical correlation [4, 5, 6], and mutual information [7].",
      "startOffset" : 245,
      "endOffset" : 248
    }, {
      "referenceID" : 7,
      "context" : "The simplest case we might consider is whether the variables are mutually independent, PX = ∏d i=1 PXi , as considered in R d by [8].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "uniform on {−1, 1}, then (X,Y,XY ) is a pairwise independent but mutually dependent triplet [9].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "In addition, study of three-variable interactions can elucidate certain switching mechanisms between positive and negative correlation of two genes expressions, as controlled by a third gene [10].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 10,
      "context" : "If the graphical model is assumed to be Gaussian, then second order interaction statistics may be used to construct an undirected graph [11, 12].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 11,
      "context" : "If the graphical model is assumed to be Gaussian, then second order interaction statistics may be used to construct an undirected graph [11, 12].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "In the PC algorithm [13, 14, 15], a V-structure (a directed graphical model with two independent parents pointing to a single child) is detected when an independence test between the parent variables accepts the null hypothesis, while a test of dependence of the parents conditioned on the child rejects the null hypothesis.",
      "startOffset" : 20,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "In the PC algorithm [13, 14, 15], a V-structure (a directed graphical model with two independent parents pointing to a single child) is detected when an independence test between the parent variables accepts the null hypothesis, while a test of dependence of the parents conditioned on the child rejects the null hypothesis.",
      "startOffset" : 20,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "In the PC algorithm [13, 14, 15], a V-structure (a directed graphical model with two independent parents pointing to a single child) is detected when an independence test between the parent variables accepts the null hypothesis, while a test of dependence of the parents conditioned on the child rejects the null hypothesis.",
      "startOffset" : 20,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "A number of algorithms have since extended the basic PC algorithm to arbitrary probability distributions over multivariate random variables [16, 17, 18], by using nonparametric kernel independence tests [19] and conditional dependence tests [20, 18].",
      "startOffset" : 140,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "A number of algorithms have since extended the basic PC algorithm to arbitrary probability distributions over multivariate random variables [16, 17, 18], by using nonparametric kernel independence tests [19] and conditional dependence tests [20, 18].",
      "startOffset" : 140,
      "endOffset" : 152
    }, {
      "referenceID" : 17,
      "context" : "A number of algorithms have since extended the basic PC algorithm to arbitrary probability distributions over multivariate random variables [16, 17, 18], by using nonparametric kernel independence tests [19] and conditional dependence tests [20, 18].",
      "startOffset" : 140,
      "endOffset" : 152
    }, {
      "referenceID" : 18,
      "context" : "A number of algorithms have since extended the basic PC algorithm to arbitrary probability distributions over multivariate random variables [16, 17, 18], by using nonparametric kernel independence tests [19] and conditional dependence tests [20, 18].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 19,
      "context" : "A number of algorithms have since extended the basic PC algorithm to arbitrary probability distributions over multivariate random variables [16, 17, 18], by using nonparametric kernel independence tests [19] and conditional dependence tests [20, 18].",
      "startOffset" : 241,
      "endOffset" : 249
    }, {
      "referenceID" : 17,
      "context" : "A number of algorithms have since extended the basic PC algorithm to arbitrary probability distributions over multivariate random variables [16, 17, 18], by using nonparametric kernel independence tests [19] and conditional dependence tests [20, 18].",
      "startOffset" : 241,
      "endOffset" : 249
    }, {
      "referenceID" : 20,
      "context" : "An interaction measure [21, 22] associated to a multidimensional probability distribution P of a random vector (X1, .",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 21,
      "context" : "An interaction measure [21, 22] associated to a multidimensional probability distribution P of a random vector (X1, .",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 20,
      "context" : "For the cases D = 2, 3 the correct interaction measure coincides with the the notion introduced by Lancaster [21] as a formal product",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 21,
      "context" : "Streitberg [22] corrected this definition using a more complicated construction with the Möbius function on the lattice of partitions, which we describe in Section 4.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 22,
      "context" : ", for all topological spaces where it is possible to construct a valid positive definite function; see [23] for details.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "In the case of total independence testing, our approach can be viewed as a generalization of the tests proposed in [24] based on the empirical characteristic functions.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 19,
      "context" : "A related but weaker notion is that of a characteristic kernel [20, 27], which requires the kernel embedding to be injective only on the setM(1)+(Z) of probability measures.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 26,
      "context" : "A related but weaker notion is that of a characteristic kernel [20, 27], which requires the kernel embedding to be injective only on the setM(1)+(Z) of probability measures.",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 27,
      "context" : "∼ Q [28] and (b) a nonparametric independence test based on estimation of ‖PXY − PXPY ‖k⊗l, for a joint sample {(Xi, Yi)} n i=1 i.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 18,
      "context" : "∼ PXY [19] (the latter is also called a Hilbert-Schmidt independence criterion), with kernel k ⊗ l on the product space defined as k(x, x′)l(y, y′).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 28,
      "context" : "When a bounded characteristic kernel is used, the above tests are consistent against all alternatives, and their alternative interpretation is as a generalization [29, 3] of energy distance [30, 31] and distance covariance [2, 32].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "When a bounded characteristic kernel is used, the above tests are consistent against all alternatives, and their alternative interpretation is as a generalization [29, 3] of energy distance [30, 31] and distance covariance [2, 32].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 29,
      "context" : "When a bounded characteristic kernel is used, the above tests are consistent against all alternatives, and their alternative interpretation is as a generalization [29, 3] of energy distance [30, 31] and distance covariance [2, 32].",
      "startOffset" : 190,
      "endOffset" : 198
    }, {
      "referenceID" : 30,
      "context" : "When a bounded characteristic kernel is used, the above tests are consistent against all alternatives, and their alternative interpretation is as a generalization [29, 3] of energy distance [30, 31] and distance covariance [2, 32].",
      "startOffset" : 190,
      "endOffset" : 198
    }, {
      "referenceID" : 1,
      "context" : "When a bounded characteristic kernel is used, the above tests are consistent against all alternatives, and their alternative interpretation is as a generalization [29, 3] of energy distance [30, 31] and distance covariance [2, 32].",
      "startOffset" : 223,
      "endOffset" : 230
    }, {
      "referenceID" : 31,
      "context" : "When a bounded characteristic kernel is used, the above tests are consistent against all alternatives, and their alternative interpretation is as a generalization [29, 3] of energy distance [30, 31] and distance covariance [2, 32].",
      "startOffset" : 223,
      "endOffset" : 230
    }, {
      "referenceID" : 2,
      "context" : "(2)Unbounded kernels can also be considered, however [3].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 27,
      "context" : "Using the same arguments as in the tests of [28, 19], these tests are also consistent against all alternatives as long as ISPD kernels are used.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : "Using the same arguments as in the tests of [28, 19], these tests are also consistent against all alternatives as long as ISPD kernels are used.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 18,
      "context" : "We provide a short overview of the kernel independence test of [19], which we write as the RKHS norm of the embedding of a signed measure.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 33,
      "context" : "The coefficients correspond to the Möbius inversion on the partition lattice [34].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : "A similar statistic for total independence is discussed by [24] where testing of total independence based on empirical characteristic functions is considered.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 17,
      "context" : "a two-variable based test; Test for X ⊥ Y |Z from [18]",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "We also plot the Type II error for the conditional independence test for X ⊥ Y |Z from [18].",
      "startOffset" : 87,
      "endOffset" : 91
    } ],
    "year" : 2013,
    "abstractText" : "We introduce kernel nonparametric tests for Lancaster three-variable interaction and for total independence, using embeddings of signed measures into a reproducing kernel Hilbert space. The resulting test statistics are straightforward to compute, and are used in powerful interaction tests, which are consistent against all alternatives for a large family of reproducing kernels. We show the Lancaster test to be sensitive to cases where two independent causes individually have weak influence on a third dependent variable, but their combined effect has a strong influence. This makes the Lancaster test especially suited to finding structure in directed graphical models, where it outperforms competing nonparametric tests in detecting such V-structures.",
    "creator" : null
  }
}