{
  "name" : "8f1d43620bc6bb580df6e80b0dc05c48.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Compete to Compute",
    "authors" : [ "Rupesh Kumar Srivastava", "Jonathan Masci", "Sohrob Kazerounian", "Faustino Gomez", "Jürgen Schmidhuber" ],
    "emails" : [ "juergen}@idsia.ch" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Although it is often useful for machine learning methods to consider how nature has arrived at a particular solution, it is perhaps more instructive to first understand the functional role of such biological constraints. Indeed, artificial neural networks, which now represent the state-of-the-art in many pattern recognition tasks, not only resemble the brain in a superficial sense, but also draw on many of its computational and functional properties. One of the long-studied properties of biological neural circuits which has yet to fully impact the machine learning community is the nature of local competition. That is, a common finding across brain regions is that neurons exhibit on-center, off-surround organization [1, 2, 3], and this organization has been argued to give rise to a number of interesting properties across networks of neurons, such as winner-take-all dynamics, automatic gain control, and noise suppression [4]. In this paper, we propose a biologically inspired mechanism for artificial neural networks that is based on local competition, and ultimately relies on local winner-take-all (LWTA) behavior. We demonstrate the benefit of LWTA across a number of different networks and pattern recognition tasks by showing that LWTA not only enables performance comparable to the state-of-the-art, but moreover, helps to prevent catastrophic forgetting [5, 6] common to artificial neural networks when they are first trained on a particular task, then abruptly trained on a new task. This property is desirable in continual learning wherein learning regimes are not clearly delineated [7]. Our experiments also show evidence that a type of modularity emerges in LWTA networks trained in a supervised setting, such that different modules (subnetworks) respond to different inputs. This is beneficial when learning from multimodal data distributions as compared to learning a monolithic model. In the following, we first discuss some of the relevant neuroscience background motivating local competition, then show how we incorporate it into artificial neural networks, and how LWTA, as implemented here, compares to alternative methods. We then show how LWTA networks perform on a variety of tasks, and how it helps buffer against catastrophic forgetting."
    }, {
      "heading" : "2 Neuroscience Background",
      "text" : "Competitive interactions between neurons and neural circuits have long played an important role in biological models of brain processes. This is largely due to early studies showing that\nmany cortical [3] and sub-cortical (e.g., hippocampal [1] and cerebellar [2]) regions of the brain exhibit a recurrent on-center, off-surround anatomy, where cells provide excitatory feedback to nearby cells, while scattering inhibitory signals over a broader range. Biological modeling has since tried to uncover the functional properties of this sort of organization, and its role in the behavioral success of animals. The earliest models to describe the emergence of winner-take-all (WTA) behavior from local competition were based on Grossberg’s shunting short-term memory equations [4], which showed that a center-surround structure not only enables WTA dynamics, but also contrast enhancement, and normalization. Analysis of their dynamics showed that networks with slower-than-linear signal functions uniformize input patterns; linear signal functions preserve and normalize input patterns; and faster-than-linear signal functions enable WTA dynamics. Sigmoidal signal functions which contain slower-than-linear, linear, and faster-than-linear regions enable the supression of noise in input patterns, while contrast-enhancing, normalizing and storing the relevant portions of an input pattern (a form of soft WTA). The functional properties of competitive interactions have been further studied to show, among other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13]. Biological models have also been extended to show how competitive interactions in spiking neural networks give rise to (soft) WTA dynamics [14], as well as how they may be efficiently constructed in VLSI [15, 16]. Although competitive interactions, and WTA dynamics have been studied extensively in the biological literature, it is only more recently that they have been considered from computational or machine learning perspectives. For example, Maas [17, 18] showed that feedforward neural networks with WTA dynamics as the only non-linearity are as computationally powerful as networks with threshold or sigmoidal gates; and, networks employing only soft WTA competition are universal function approximators. Moreover, these results hold, even when the network weights are strictly positive—a finding which has ramifications for our understanding of biological neural circuits, as well as the development of neural networks for pattern recognition. The large body of evidence supporting the advantages of locally competitive interactions makes it noteworthy that this simple mechanism has not provoked more study by the machine learning community. Nonetheless, networks employing local competition have existed since the late 80s [21], and, along with [22], serve as a primary inspiration for the present work. More recently, maxout networks [19] have leveraged locally competitive interactions in combination with a technique known as dropout [20] to obtain the best results on certain benchmark problems."
    }, {
      "heading" : "3 Networks with local winner-take-all blocks",
      "text" : "This section describes the general network architecture with locally competing neurons. The network consists of B blocks which are organized into layers (Figure 1). Each block, bi, i = 1..B, contains n computational units (neurons), and produces an output vector yi, determined by the local interactions between the individual neuron activations in the block:\nyji = g(h 1 i , h 2 i ..., h n i ), (1)\nwhere g(·) is the competition/interaction function, encoding the effect of local interactions in each block, and hji , j = 1..n, is the activation of the j-th neuron in block i computed by:\nhi = f(wTijx), (2) where x is the input vector from neurons in the previous layer, wij is the weight vector of neuron j in block i, and f(·) is a (generally non-linear) activation function. The output activations y are passed as inputs to the next layer. In this paper we use the winner-take-all interaction function, inspired by studies in computational neuroscience. In particular, we use the hard winner-take-all function:\nyji = { hji if h j i ≥ hki , ∀k = 1..n\n0 otherwise. In the case of multiple winners, ties are broken by index precedence. In order to investigate the capabilities of the hard winner-take-all interaction function in isolation, f(x) = x\n(identity) is used for the activation function in equation (2). The difference between this Local Winner Take All (LWTA) network and a standard multilayer perceptron is that no non-linear activation functions are used, and during the forward propagation of inputs, local competition between the neurons in each block turns off the activation of all neurons except the one with the highest activation. During training the error signal is only backpropagated through the winning neurons. In a LWTA layer, there are as many neurons as there are blocks active at any one time for a given input pattern1. We denote a layer with blocks of size n as LWTA-n. For each input pattern presented to a network, only a subgraph of the full network is active, e.g. the highlighted neurons and synapses in figure 1. Training on a dataset consists of simultaneously training an exponential number of models that share parameters, as well as learning which model should be active for each pattern. Unlike networks with sigmoidal units, where all of the free parameters need to be set properly for all input patterns, only a subset is used for any given input, so that patterns coming from very different sub-distributions can potentially be modelled more efficiently through specialization. This modular property is similar to that of networks with rectified linear units (ReLU) which have recently been shown to be very good at several learning tasks (links with ReLU are discussed in section 4.3)."
    }, {
      "heading" : "4 Comparison with related methods",
      "text" : ""
    }, {
      "heading" : "4.1 Max-pooling",
      "text" : "Neural networks with max-pooling layers [23] have been found to be very useful, especially for image classification tasks where they have achieved state-of-the-art performance [24, 25]. These layers are usually used in convolutional neural networks to subsample the representation obtained after convolving the input with a learned filter, by dividing the representation into pools and selecting the maximum in each one. Max-pooling lowers the computational burden by reducing the number of connections in subsequent convolutional layers, and adds translational/rotational invariance.\n1However, there is always the possibility that the winning neuron in a block has an activation of exactly zero, so that the block has no output.\nAt first glance, the max-pooling seems very similar to a WTA operation, however, the two differ substantially: there is no downsampling in a WTA operation and thus the number of features is not reduced, instead the representation is \"sparsified\" (see figure 2)."
    }, {
      "heading" : "4.2 Dropout",
      "text" : "Dropout [20] can be interpreted as a model-averaging technique that jointly trains several models sharing subsets of parameters and input dimensions, or as data augmentation when applied to the input layer [19, 20]. This is achieved by probabilistically omitting (“dropping”) units from a network for each example during training, so that those neurons do not participate in forward/backward propagation. Consider, hypothetically, training an LWTA network with blocks of size two, and selecting the winner in each block at random. This is similar to training a neural network with a dropout probability of 0.5. Nonetheless, the two are fundamentally different. Dropout is a regularization technique while in LWTA the interaction between neurons in a block replaces the per-neuron non-linear activation. Dropout is believed to improve generalization performance since it forces the units to learn independent features, without relying on other units being active. During testing, when propagating an input through the network, all units in a layer trained with dropout are used with their output weights suitably scaled. In an LWTA network, no output scaling is required. A fraction of the units will be inactive for each input pattern depending on their total inputs. Viewed this way, WTA is restrictive in that only a fraction of the parameters are utilized for each input pattern. However, we hypothesize that the freedom to use different subsets of parameters for different inputs allows the architecture to learn from multimodal data distributions more accurately."
    }, {
      "heading" : "4.3 Rectified Linear units",
      "text" : "Rectified Linear Units (ReLU) are simply linear neurons that clamp negative activations to zero (f(x) = x if x > 0, f(x) = 0 otherwise). ReLU networks were shown to be useful for Restricted Boltzmann Machines [26], outperformed sigmoidal activation functions in deep neural networks [27], and have been used to obtain the best results on several benchmark problems across multiple domains [24, 28]. Consider an LWTA block with two neurons compared to two ReLU neurons, where x1 and x2 are the weighted sum of the inputs to each neuron. Table 1 shows the outputs y1 and y2 in all combinations of positive and negative x1 and x2, for ReLU and LWTA neurons. For both ReLU and LWTA neurons, x1 and x2 are passed through as output in half of the possible cases. The difference is that in LWTA both neurons are never active or inactive at the same time, and the activations and errors flow through exactly one neuron in the block. For ReLU neurons, being inactive (saturation) is a potential drawback since neurons that\ndo not get activated will not get trained, leading to wasted capacity. However, previous work suggests that there is no negative impact on optimization, leading to the hypothesis that such hard saturation helps in credit assignment, and, as long as errors flow through certain paths, optimization is not affected adversely [27]. Continued research along these lines validates this hypothesis [29], but it is expected that it is possible to train ReLU networks better. While many of the above arguments for and against ReLU networks apply to LWTA networks, there is a notable difference. During training of an LWTA network, inactive neurons can become active due to training of the other neurons in the same block. This suggests that LWTA nets may be less sensitive to weight initialization, and a greater portion of the network’s capacity may be utilized."
    }, {
      "heading" : "5 Experiments",
      "text" : "In the following experiments, LWTA networks were tested on various supervised learning datasets, demonstrating their ability to learn useful internal representations without utilizing any other non-linearities. In order to clearly assess the utility of local competition, no special strategies such as augmenting data with transformations, noise or dropout were used. We also did not encourage sparse representations in the hidden layers by adding activation penalties to the objective function, a common technique also for ReLU units. Thus, our objective is to evaluate the value of using LWTA rather than achieving the absolute best testing scores. Blocks of size two are used in all the experiments.2\nAll networks were trained using stochastic gradient descent with mini-batches, learning rate lt and momentum mt at epoch t given by\nαt = { α0λ\nt if αt > αmin αmin otherwise\nmt = { t T mi + (1− t T )mf if t < T\npf if t ≥ T\nwhere λ is the learning rate annealing factor, αmin is the lower learning rate limit, and momentum is scaled from mi to mf over T epochs after which it remains constant at mf . L2 weight decay was used for the convolutional network (section 5.2), and max-norm normalization for other experiments. This setup is similar to that of [20]."
    }, {
      "heading" : "5.1 Permutation Invariant MNIST",
      "text" : "The MNIST handwritten digit recognition task consists of 70,000 28x28 images (60,000 training, 10,000 test) of the 10 digits centered by their center of mass [33]. In the permutation invariant setting of this task, we attempted to classify the digits without utilizing the 2D structure of the images, e.g. every digit is a vector of pixels. The last 10,000 examples in the training set were used for hyperparameter tuning. The model with the best hyperparameter setting was trained until convergence on the full training set. Mini-batches of size 20 were\n2To speed up our experiments, the Gnumpy [30] and CUDAMat [31] libraries were used.\nused, the pixel values were rescaled to [0, 1] (no further preprocessing). The best model obtained, which gave a test set error of 1.28%, consisted of three LWTA layers of 500 blocks followed by a 10-way softmax layer. To our knowledge, this is the best reported error, without utilizing implicit/explicit model averaging, for this setting which does not use deformations/noise to enhance the dataset or unsupervised pretraining. Table 2 compares our results with other methods which do not use unsupervised pre-training. The performance of LWTA is comparable to that of a ReLU network with dropout in the hidden layers. Using dropout in input layers as well, lower error rates of 1.1% using ReLU [20] and 0.94% using maxout [19] have been obtained."
    }, {
      "heading" : "5.2 Convolutional Network on MNIST",
      "text" : "For this experiment, a convolutional network (CNN) was used consisting of 7 × 7 filters in the first layer followed by a second layer of 6 × 6, with 16 and 32 maps respectively, and ReLU activation. Every convolutional layer is followed by a 2 × 2 max-pooling operation. We then use two LWTA-2 layers each with 64 blocks and finally a 10-way softmax output layer. A weight decay of 0.05 was found to be beneficial to improve generalization. The results are summarized in Table 3 along with other state-of-the-art approaches which do not use data augmentation (for details of convolutional architectures, see [33])."
    }, {
      "heading" : "5.3 Amazon Sentiment Analysis",
      "text" : "LWTA networks were tested on the Amazon sentiment analysis dataset [37] since ReLU units have been shown to perform well in this domain [27, 38]. We used the balanced subset of the dataset consisting of reviews of four categories of products: Books, DVDs, Electronics and Kitchen appliances. The task is to classify the reviews as positive or negative. The dataset consists of 1000 positive and 1000 negative reviews in each category. The text of each review was converted into a binary feature vector encoding the presence or absence of unigrams and bigrams. Following [27], the 5000 most frequent vocabulary entries were retained as features for classification. We then divided the data into 10 equal balanced folds, and tested our network with cross-validation, reporting the mean test error over all folds. ReLU activation was used on this dataset in the context of unsupervised learning with denoising autoencoders to obtain sparse feature representations which were used for classification. We trained an LWTA-2 network with three layers of 500 blocks each in a supervised setting to directly classify each review as positive or negative using a 2-way softmax output layer. We obtained mean accuracies of Books: 80%, DVDs: 81.05%, Electronics: 84.45% and Kitchen: 85.8%, giving a mean accuracy of 82.82%, compared to 78.95% reported in [27] for denoising autoencoders using ReLU and unsupervised pre-training to find a good initialization."
    }, {
      "heading" : "6 Implicit long term memory",
      "text" : "This section examines the effect of the LWTA architecture on catastrophic forgetting. That is, does the fact that the network implements multiple models allow it to retain information about dataset A, even after being trained on a different dataset B? To test for this implicit long term memory, the MNIST training and test sets were each divided into two parts, P1 containing only digits {0, 1, 2, 3, 4}, and P2 consisting of the remaining digits {5, 6, 7, 8, 9}. Three different network architectures were compared: (1) three LWTA layers each with 500 blocks of size 2, (2) three layers each with 1000 sigmoidal neurons, and (3) three layers each of 1000 ReLU neurons. All networks have a 5-way softmax output layer representing the probability of an example belonging to each of the five classes. All networks were initialized with the same parameters, and trained with a fixed learning rate and momentum. Each network was first trained to reach a 0.03 log-likelihood error on the P1 training set. This value was chosen heuristically to produce low test set errors in reasonable time for all three network types. The weights for the output layer (corresponding to the softmax classifier) were then stored, and the network was trained further, starting with new initial random output layer weights, to reach the same log-likelihood value on P2. Finally, the output layer weights saved from P1 were restored, and the network was evaluated on the P1 test set. The experiment was repeated for 10 different initializations. Table 4 shows that the LWTA network remembers what was learned from P1 much better than sigmoid and ReLU networks, though it is notable that the sigmoid network performs much worse than both LWTA and ReLU. While the test error values depend on the learning rate and momentum used, LWTA networks tended to remember better than the ReLU network by about a factor of two in most cases, and sigmoid networks always performed much worse. Although standard network architectures are known to suffer from catastrophic forgetting, we not only show here, for the first time, that ReLU networks are actually quite good in this regard, and moreover, that they are outperformed by LWTA. We expect this behavior to manifest itself in competitive models in general, and to become more pronounced with increasingly complex datasets. The neurons encoding specific features in one dataset are not affected much during training on another dataset, whereas neurons encoding common features can be reused. Thus, LWTA may be a step forward towards models that do not forget easily."
    }, {
      "heading" : "7 Analysis of subnetworks",
      "text" : "A network with a single LWTA-m of N blocks consists of mN subnetworks which can be selected and trained for individual examples while training over a dataset. After training, we expect the subnetworks consisting of active neurons for examples from the same class to have more neurons in common compared to subnetworks being activated for different classes. In the case of relatively simple datasets like MNIST, it is possible to examine the number of common neurons between mean subnetworks which are used for each class. To do this, which neurons were active in the layer for each example in a subset of 10,000 examples were recorded. For each class, the subnetwork consisting of neurons active for at least 90% of the examples was designated the representative mean subnetwork, which was then compared to all other class subnetworks by counting the number of neurons in common. Figure 3a shows the fraction of neurons in common between the mean subnetworks of each pair of digits. Digits that are morphologically similar such as “3” and “8” have subnetworks with more neurons in common than the subnetworks for digits “1” and “2” or “1” and “5” which are intuitively less similar. To verify that this subnetwork specialization is a result of training, we looked at the fraction of common neurons between all pairs of digits for the\nsame 10000 examples both before and after training (Figure 3b). Clearly, the subnetworks were much more similar prior to training, and the full network has learned to partition its parameters to reflect the structure of the data."
    }, {
      "heading" : "8 Conclusion and future research directions",
      "text" : "Our LWTA networks automatically self-modularize into multiple parameter-sharing subnetworks responding to different input representations. Without significant degradation of state-of-the-art results on digit recognition and sentiment analysis, LWTA networks also avoid catastrophic forgetting, thus retaining useful representations of one set of inputs even after being trained to classify another. This has implications for continual learning agents that should not forget representations of parts of their environment when being exposed to other parts. We hope to explore many promising applications of these ideas in the future."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was funded by EU projects WAY (FP7-ICT-288551), NeuralDynamics (FP7ICT-270247), and NASCENCE (FP7-ICT-317662); additional funding from ArcelorMittal."
    } ],
    "references" : [ {
      "title" : "Participation of inhibitory and excitatory interneurones in the control of hippocampal cortical output",
      "author" : [ "Per Anderson", "Gary N. Gross", "Terje Lømo", "Ola Sveen" ],
      "venue" : "The Interneuron,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1969
    }, {
      "title" : "Szentágothai. The cerebellum as a neuronal machine",
      "author" : [ "John Carew Eccles", "Masao Ito", "János" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1967
    }, {
      "title" : "Interneuronal mechanisms in the cortex",
      "author" : [ "Costas Stefanis" ],
      "venue" : "The Interneuron,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1969
    }, {
      "title" : "Contour enhancement, short-term memory, and constancies in reverberating neural networks",
      "author" : [ "Stephen Grossberg" ],
      "venue" : "Studies in Applied Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1973
    }, {
      "title" : "Catastrophic interference in connectionist networks: The sequential learning problem",
      "author" : [ "Michael McCloskey", "Neal J. Cohen" ],
      "venue" : "The Psychology of Learning and Motivation,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1989
    }, {
      "title" : "The art of adaptive pattern recognition by a self-organising neural network",
      "author" : [ "Gail A. Carpenter", "Stephen Grossberg" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1988
    }, {
      "title" : "Continual Learning in Reinforcement Environments. PhD thesis, Department of Computer Sciences, The University of Texas at Austin, Austin, Texas",
      "author" : [ "Mark B. Ring" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1994
    }, {
      "title" : "Pattern formation, contrast control, and oscillations in the short term memory of shunting on-center off-surround networks",
      "author" : [ "Samuel A. Ellias", "Stephen Grossberg" ],
      "venue" : "Bio. Cybernetics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1975
    }, {
      "title" : "Complex dynamics in winner-take-all neural nets with slow inhibition",
      "author" : [ "Brad Ermentrout" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1992
    }, {
      "title" : "Self-organization of orientation sensitive cells",
      "author" : [ "Christoph von der Malsburg" ],
      "venue" : "in the striate cortex. Kybernetik,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1973
    }, {
      "title" : "Self-organized formation of topologically correct feature maps",
      "author" : [ "Teuvo Kohonen" ],
      "venue" : "Biological cybernetics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1982
    }, {
      "title" : "Computational maps in the visual cortex",
      "author" : [ "Risto Mikkulainen", "James A. Bednar", "Yoonsuck Choe", "Joseph Sirosh" ],
      "venue" : "Springer Science+ Business Media,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "Attention activates winner-takeall competition among visual filters",
      "author" : [ "Dale K. Lee", "Laurent Itti", "Christof Koch", "Jochen Braun" ],
      "venue" : "Nature Neuroscience,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Spiking inputs to a winner-take-all network",
      "author" : [ "Matthias Oster", "Shih-Chii Liu" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "Winnertake-all networks of O(n) complexity",
      "author" : [ "John P. Lazzaro", "Sylvie Ryckebusch", "Misha Anne Mahowald", "Caver A. Mead" ],
      "venue" : "Technical report,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1988
    }, {
      "title" : "Modeling selective attention using a neuromorphic analog VLSI device",
      "author" : [ "Giacomo Indiveri" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Neural computation with winner-take-all as the only nonlinear operation",
      "author" : [ "Wolfgang Maass" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1999
    }, {
      "title" : "On the computational power of winner-take-all",
      "author" : [ "Wolfgang Maass" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "A local learning algorithm for dynamic feedforward and recurrent networks",
      "author" : [ "Juergen Schmidhuber" ],
      "venue" : "Connection Science,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1989
    }, {
      "title" : "First experiments with powerplay",
      "author" : [ "Rupesh K. Srivastava", "Bas R. Steunebrink", "Juergen Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Hierarchical models of object recognition in cortex",
      "author" : [ "Maximillian Riesenhuber", "Tomaso Poggio" ],
      "venue" : "Nature Neuroscience,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1999
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Goeffrey E. Hinton" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Multi-column deep neural networks for image classification",
      "author" : [ "Dan Ciresan", "Ueli Meier", "Jürgen Schmidhuber" ],
      "venue" : "Proceeedings of the CVPR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E. Hinton" ],
      "venue" : "In Proceedings of the ICML, number",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In AIS- TATS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Improving Deep Neural Networks for LVCSR using Rectified Linear Units and Dropout",
      "author" : [ "George E. Dahl", "Tara N. Sainath", "Geoffrey E. Hinton" ],
      "venue" : "In Proceedings of ICASSP,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Rectifier nonlinearities improve neural network acoustic models",
      "author" : [ "Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng" ],
      "venue" : "In Proceedings of the ICML,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "Gnumpy: an easy way to use GPU boards in Python",
      "author" : [ "Tijmen Tieleman" ],
      "venue" : "Department of Computer Science, University of Toronto,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2010
    }, {
      "title" : "CUDAMat: a CUDA-based matrix class for Python",
      "author" : [ "Volodymyr Mnih" ],
      "venue" : "Department of Computer Science, University of Toronto, Tech. Rep. UTML TR,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Best practices for convolutional neural networks applied to visual document analysis",
      "author" : [ "Patrice Y. Simard", "Dave Steinkraus", "John C. Platt" ],
      "venue" : "In International Conference on Document Analysis and Recognition (ICDAR),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2003
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1998
    }, {
      "title" : "Efficient learning of sparse representations with an energy-based model",
      "author" : [ "Marc’Aurelio Ranzato", "Christopher Poultney", "Sumit Chopra", "Yann LeCun" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2007
    }, {
      "title" : "Stochastic pooling for regularization of deep convolutional neural networks",
      "author" : [ "Matthew D. Zeiler", "Rob Fergus" ],
      "venue" : "In Proceedings of the ICLR,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2013
    }, {
      "title" : "What is the best multi-stage architecture for object recognition",
      "author" : [ "Kevin Jarrett", "Koray Kavukcuoglu", "Marc’Aurelio Ranzato", "Yann LeCun" ],
      "venue" : "In Proc. of the ICCV,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2009
    }, {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira" ],
      "venue" : "Annual Meeting-ACL,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2007
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the ICML,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "That is, a common finding across brain regions is that neurons exhibit on-center, off-surround organization [1, 2, 3], and this organization has been argued to give rise to a number of interesting properties across networks of neurons, such as winner-take-all dynamics, automatic gain control, and noise suppression [4].",
      "startOffset" : 108,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "That is, a common finding across brain regions is that neurons exhibit on-center, off-surround organization [1, 2, 3], and this organization has been argued to give rise to a number of interesting properties across networks of neurons, such as winner-take-all dynamics, automatic gain control, and noise suppression [4].",
      "startOffset" : 108,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "That is, a common finding across brain regions is that neurons exhibit on-center, off-surround organization [1, 2, 3], and this organization has been argued to give rise to a number of interesting properties across networks of neurons, such as winner-take-all dynamics, automatic gain control, and noise suppression [4].",
      "startOffset" : 108,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "That is, a common finding across brain regions is that neurons exhibit on-center, off-surround organization [1, 2, 3], and this organization has been argued to give rise to a number of interesting properties across networks of neurons, such as winner-take-all dynamics, automatic gain control, and noise suppression [4].",
      "startOffset" : 316,
      "endOffset" : 319
    }, {
      "referenceID" : 4,
      "context" : "We demonstrate the benefit of LWTA across a number of different networks and pattern recognition tasks by showing that LWTA not only enables performance comparable to the state-of-the-art, but moreover, helps to prevent catastrophic forgetting [5, 6] common to artificial neural networks when they are first trained on a particular task, then abruptly trained on a new task.",
      "startOffset" : 244,
      "endOffset" : 250
    }, {
      "referenceID" : 5,
      "context" : "We demonstrate the benefit of LWTA across a number of different networks and pattern recognition tasks by showing that LWTA not only enables performance comparable to the state-of-the-art, but moreover, helps to prevent catastrophic forgetting [5, 6] common to artificial neural networks when they are first trained on a particular task, then abruptly trained on a new task.",
      "startOffset" : 244,
      "endOffset" : 250
    }, {
      "referenceID" : 6,
      "context" : "This property is desirable in continual learning wherein learning regimes are not clearly delineated [7].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "many cortical [3] and sub-cortical (e.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : ", hippocampal [1] and cerebellar [2]) regions of the brain exhibit a recurrent on-center, off-surround anatomy, where cells provide excitatory feedback to nearby cells, while scattering inhibitory signals over a broader range.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : ", hippocampal [1] and cerebellar [2]) regions of the brain exhibit a recurrent on-center, off-surround anatomy, where cells provide excitatory feedback to nearby cells, while scattering inhibitory signals over a broader range.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "The earliest models to describe the emergence of winner-take-all (WTA) behavior from local competition were based on Grossberg’s shunting short-term memory equations [4], which showed that a center-surround structure not only enables WTA dynamics, but also contrast enhancement, and normalization.",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 7,
      "context" : "The functional properties of competitive interactions have been further studied to show, among other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "The functional properties of competitive interactions have been further studied to show, among other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].",
      "startOffset" : 177,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "The functional properties of competitive interactions have been further studied to show, among other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].",
      "startOffset" : 177,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "The functional properties of competitive interactions have been further studied to show, among other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].",
      "startOffset" : 221,
      "endOffset" : 233
    }, {
      "referenceID" : 10,
      "context" : "The functional properties of competitive interactions have been further studied to show, among other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].",
      "startOffset" : 221,
      "endOffset" : 233
    }, {
      "referenceID" : 11,
      "context" : "The functional properties of competitive interactions have been further studied to show, among other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].",
      "startOffset" : 221,
      "endOffset" : 233
    }, {
      "referenceID" : 12,
      "context" : "The functional properties of competitive interactions have been further studied to show, among other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].",
      "startOffset" : 277,
      "endOffset" : 281
    }, {
      "referenceID" : 13,
      "context" : "Biological models have also been extended to show how competitive interactions in spiking neural networks give rise to (soft) WTA dynamics [14], as well as how they may be efficiently constructed in VLSI [15, 16].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "Biological models have also been extended to show how competitive interactions in spiking neural networks give rise to (soft) WTA dynamics [14], as well as how they may be efficiently constructed in VLSI [15, 16].",
      "startOffset" : 204,
      "endOffset" : 212
    }, {
      "referenceID" : 15,
      "context" : "Biological models have also been extended to show how competitive interactions in spiking neural networks give rise to (soft) WTA dynamics [14], as well as how they may be efficiently constructed in VLSI [15, 16].",
      "startOffset" : 204,
      "endOffset" : 212
    }, {
      "referenceID" : 16,
      "context" : "For example, Maas [17, 18] showed that feedforward neural networks with WTA dynamics as the only non-linearity are as computationally powerful as networks with threshold or sigmoidal gates; and, networks employing only soft WTA competition are universal function approximators.",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 17,
      "context" : "For example, Maas [17, 18] showed that feedforward neural networks with WTA dynamics as the only non-linearity are as computationally powerful as networks with threshold or sigmoidal gates; and, networks employing only soft WTA competition are universal function approximators.",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "Nonetheless, networks employing local competition have existed since the late 80s [21], and, along with [22], serve as a primary inspiration for the present work.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 20,
      "context" : "Nonetheless, networks employing local competition have existed since the late 80s [21], and, along with [22], serve as a primary inspiration for the present work.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 18,
      "context" : "More recently, maxout networks [19] have leveraged locally competitive interactions in combination with a technique known as dropout [20] to obtain the best results on certain benchmark problems.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 21,
      "context" : "Neural networks with max-pooling layers [23] have been found to be very useful, especially for image classification tasks where they have achieved state-of-the-art performance [24, 25].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "Neural networks with max-pooling layers [23] have been found to be very useful, especially for image classification tasks where they have achieved state-of-the-art performance [24, 25].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 23,
      "context" : "Neural networks with max-pooling layers [23] have been found to be very useful, especially for image classification tasks where they have achieved state-of-the-art performance [24, 25].",
      "startOffset" : 176,
      "endOffset" : 184
    }, {
      "referenceID" : 18,
      "context" : "Dropout [20] can be interpreted as a model-averaging technique that jointly trains several models sharing subsets of parameters and input dimensions, or as data augmentation when applied to the input layer [19, 20].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 18,
      "context" : "Dropout [20] can be interpreted as a model-averaging technique that jointly trains several models sharing subsets of parameters and input dimensions, or as data augmentation when applied to the input layer [19, 20].",
      "startOffset" : 206,
      "endOffset" : 214
    }, {
      "referenceID" : 24,
      "context" : "ReLU networks were shown to be useful for Restricted Boltzmann Machines [26], outperformed sigmoidal activation functions in deep neural networks [27], and have been used to obtain the best results on several benchmark problems across multiple domains [24, 28].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : "ReLU networks were shown to be useful for Restricted Boltzmann Machines [26], outperformed sigmoidal activation functions in deep neural networks [27], and have been used to obtain the best results on several benchmark problems across multiple domains [24, 28].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 22,
      "context" : "ReLU networks were shown to be useful for Restricted Boltzmann Machines [26], outperformed sigmoidal activation functions in deep neural networks [27], and have been used to obtain the best results on several benchmark problems across multiple domains [24, 28].",
      "startOffset" : 252,
      "endOffset" : 260
    }, {
      "referenceID" : 26,
      "context" : "ReLU networks were shown to be useful for Restricted Boltzmann Machines [26], outperformed sigmoidal activation functions in deep neural networks [27], and have been used to obtain the best results on several benchmark problems across multiple domains [24, 28].",
      "startOffset" : 252,
      "endOffset" : 260
    }, {
      "referenceID" : 25,
      "context" : "However, previous work suggests that there is no negative impact on optimization, leading to the hypothesis that such hard saturation helps in credit assignment, and, as long as errors flow through certain paths, optimization is not affected adversely [27].",
      "startOffset" : 252,
      "endOffset" : 256
    }, {
      "referenceID" : 27,
      "context" : "Continued research along these lines validates this hypothesis [29], but it is expected that it is possible to train ReLU networks better.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 31,
      "context" : "The MNIST handwritten digit recognition task consists of 70,000 28x28 images (60,000 training, 10,000 test) of the 10 digits centered by their center of mass [33].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 28,
      "context" : "2To speed up our experiments, the Gnumpy [30] and CUDAMat [31] libraries were used.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 29,
      "context" : "2To speed up our experiments, the Gnumpy [30] and CUDAMat [31] libraries were used.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 18,
      "context" : "43% ReLU + dropout in hidden layers [20] 1.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 32,
      "context" : "Architecture Test Error 2-layer CNN + 2 layer MLP [34] * 0.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 34,
      "context" : "55% 2-layer CNN + 2 layer MLP [36] * 0.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 31,
      "context" : "53% 3-layer ReLU CNN + stochastic pooling [33] 0.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 31,
      "context" : "The results are summarized in Table 3 along with other state-of-the-art approaches which do not use data augmentation (for details of convolutional architectures, see [33]).",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 35,
      "context" : "LWTA networks were tested on the Amazon sentiment analysis dataset [37] since ReLU units have been shown to perform well in this domain [27, 38].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "LWTA networks were tested on the Amazon sentiment analysis dataset [37] since ReLU units have been shown to perform well in this domain [27, 38].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 36,
      "context" : "LWTA networks were tested on the Amazon sentiment analysis dataset [37] since ReLU units have been shown to perform well in this domain [27, 38].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 25,
      "context" : "Following [27], the 5000 most frequent vocabulary entries were retained as features for classification.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 25,
      "context" : "95% reported in [27] for denoising autoencoders using ReLU and unsupervised pre-training to find a good initialization.",
      "startOffset" : 16,
      "endOffset" : 20
    } ],
    "year" : 2013,
    "abstractText" : "Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.",
    "creator" : null
  }
}