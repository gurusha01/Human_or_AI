{
  "name" : "598b3e71ec378bd83e0a727608b5db01.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Distributed Exploration in Multi-Armed Bandits",
    "authors" : [ "Eshcar Hillel", "Zohar Karnin", "Tomer Koren" ],
    "emails" : [ "eshcar@yahoo-inc.com", "zkarnin@yahoo-inc.com", "tomerk@technion.ac.il", "rlempel@yahoo-inc.com", "orens@yahoo-inc.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ k times faster than a single player.\nThat is, distributing learning to k players gives rise to a factor √ k parallel speedup. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/ε."
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the past years, multi-armed bandit (MAB) algorithms have been employed in an increasing amount of large-scale applications. MAB algorithms rank results of search engines [23, 24], choose between stories or ads to showcase on web sites [2, 8], accelerate model selection and stochastic optimization tasks [21, 22], and more. In many of these applications, the workload is simply too high to be handled by a single processor. In the web context, for example, the sheer volume of user requests and the high rate at which they arrive, require websites to use many front-end machines that run in multiple data centers. In the case of model selection tasks, a single evaluation of a certain model or configuration might require considerable computation time, so that distributing the exploration process across several nodes may result with a significant gain in performance. In this paper, we study such large-scale MAB problems in a distributed environment where learning is performed by several independent nodes that may take actions and observe rewards in parallel.\nFollowing recent MAB literature [14, 3, 15, 18], we focus on the problem of identifying a “good” bandit arm with high confidence. In this problem, we may repeatedly choose one arm (corresponding to an action) and observe a reward drawn from a probability distribution associated with that arm. Our goal is to find an arm with an (almost) optimal expected reward, with as few arm pulls as possible (that is, minimize the simple regret [7]). Our objective is thus explorative in nature, and in\n∗Most of this work was done while the author was at Yahoo Labs, Haifa.\nparticular we do not mind the incurred costs or the involved regret. This is indeed the natural goal in many applications, such as in the case of model selection problems mentioned above. In our setup, a distributed strategy is evaluated by the number of arm pulls per node required for the task, which correlates with the parallel speed-up obtained by distributing the learning process.\nWe abstract a distributed MAB system as follows. In our model, there are k players that correspond to k independent machines in a cluster. The players are presented with a set of arms, with a common goal of identifying a good arm. Each player receives a stream of queries upon each it chooses an arm to pull. This stream is usually regulated by some load balancer ensuring the load is roughly divided evenly across players. To collaborate, the players may communicate with each other. We assume that the bandwidth of the underlying network is limited, so that players cannot simply share every piece of information. Also, communicating over the network might incur substantial latencies, so players should refrain from doing so as much as possible. When measuring communication of a certain multi-player protocol we consider the number of communication rounds it requires, where in a round of communication each player broadcasts a single message (of arbitrary size) to all other players. Round-based models are natural in distributed learning scenarios, where frameworks such as MapReduce [11] are ubiquitous.\nWhat is the tradeoff between the learning performance of the players, and the communication between them? At one extreme, if all players broadcast to each other each and every arm reward as it is observed, they can simply simulate the decisions of a serial, optimal algorithm. However, the communication load of this strategy is of course prohibitive. At the other extreme, if the players never communicate, each will suffer the learning curve of a single player, thereby avoiding any possible speed-up the distributed system may provide. Our goal in this work is to better understand this tradeoff between inter-player communication and learning performance.\nConsidering the high cost of communication, perhaps the simplest and most important question that arises is how well can the players learn while keeping communication to the very minimum. More specifically, is there a non-trivial strategy by which the players can identify a “good” arm while communicating only once, at the end of the process? As we discuss later on, this is a non-trivial question. On the positive side, we present a k-player algorithm that attains an asymptotic parallel speed-up of √ k factor, as compared to the conventional, serial setting. In fact, our approach demonstrates how to convert virtually any serial exploration strategy to a distributed algorithm enjoying such speed-up. Ideally, one could hope for a factor k speed-up in learning performance; however, we show a lower bound on the required number of pulls in this case, implying that our √ k speed-up is essentially optimal.\nAt the other end of the trade-off, we investigate how much communication is necessary for obtaining the ideal factor k parallel speed-up. We present a k-player strategy achieving such speed-up, with communication only logarithmic in 1/ε. As a corollary, we derive an algorithm that demonstrates an explicit trade-off between the number of arm pulls and the amount of inter-player communication."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Recently there has been an increasing interest in distributed and collaborative learning problems. In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually compete with each other, either on arm-pulls resources [15] or on the rewards received [19]. In contrast, we study a collaborative multi-player problem and investigate how sharing observations helps players achieve their common goal. The related work of Kanade et al. [17] in the context of non-stochastic (i.e. adversarial) experts also deals with a collaborative problem in a similar distributed setup, and examine the trade-off between communication and the cumulative regret.\nAnother line of recent work was focused on distributed stochastic optimization [13, 1, 12] and distributed PAC models [6, 10, 9], investigating the involved communication trade-offs. The techniques developed there, however, are inherently “batch” learning methods and thus are not directly applicable to our MAB problem which is online in nature. Questions involving network topology [13, 12] and delays [1] are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting."
    }, {
      "heading" : "2 Problem Setup and Statement of Results",
      "text" : "In our model of the Distributed Multi-Armed Bandit problem, there are k ≥ 1 individual players. The players are given n arms, enumerated by [n] := {1, 2, . . . , n}. Each arm i ∈ [n] is associated with a reward, which is a [0, 1]-valued random variable with expectation pi. For convenience, we assume that the arms are ordered by their expected rewards, that is p1 ≥ p2 ≥ · · · ≥ pn. At every time step t = 1, 2, . . . , T , each player pulls one arm of his choice and observes an independent sample of its reward. Each player may choose any of the arms, regardless of the other players and their actions. At the end of the game, each player must commit to a single arm. In a communication round, that may take place at any predefined time step, each player may broadcast a message to all other players. While we do not restrict the size of each message, in a reasonable implementation a message should not be larger than Õ(n) bits.\nIn the best-arm identification version of the problem, the goal of a multi-player algorithm given some target confidence level δ > 0, is that with probability at least 1− δ all players correctly identify the best arm (i.e. the arm having the maximal expected reward). For simplicity, we assume in this setting that the best arm is unique. Similarly, in the (ε, δ)-PAC variant the goal is that each player finds an ε-optimal (or “ε-best”) arm, that is an arm i with pi ≥ p1−ε, with high probability. In this paper we focus on the more general (ε, δ)-PAC setup, which also includes best-arm identification for ε = 0.\nWe use the notation ∆i := p1 − pi to denote the suboptimality gap of arm i, and occasionally use ∆? := ∆2 for denoting the minimal gap. In the best-arm version of the problem, where we assume that the best arm is unique, we have ∆i > 0 for all i > 1. When dealing with the (ε, δ)-PAC setup, we also consider the truncated gaps ∆εi := max{∆i, ε}. In the context of MAB problems, we are interested in deriving distribution-dependent bounds, namely, bounds that are stated as a function of ε, δ and also the distribution-specific values ∆ := (∆2, . . . ,∆n). The Õ notation in our bounds hides polylogarithmic factors in n, k, ε, δ, and also in ∆2, . . . ,∆n. In the case of serial exploration algorithms (i.e., when there is only one player), the lower bounds of Mannor and Tsitsiklis [20] and Audibert et al. [3] show that in general Ω̃(Hε) pulls are necessary for identifying an ε-arm, where\nHε := n∑ i=2 1 (∆εi ) 2 . (1)\nIntuitively, the hardness of the task is therefore captured by the quantity Hε, which is roughly the number of arm pulls needed to find an ε-best arm with a reasonable probability; see also [3] for a discussion. Our goal in this work is therefore to establish bounds in the distributed model that are expressed as a function of Hε, in the same vein of the bounds known in the classic MAB setup."
    }, {
      "heading" : "2.1 Baseline approaches",
      "text" : "We now discuss several baseline approaches for the problem, starting with our main focus—the single round setting. The first obvious approach, already mentioned earlier, is the no-communication strategy: just let each player explore the arms in isolation of the other players, following an independent instance of some serial strategy; at the end of the executions, all players hold an ε-best arm. Clearly, this approach performs poorly in terms of learning performance, needing Ω̃(Hε) pulls per player in the worst case and not leading to any parallel speed-up.\nAnother straightforward approach is to employ a majority vote among the players: let each player independently identify an arm, and choose the arm having most of the votes (alternatively, at least half of the votes). However, this approach does not lead to any improvement in performance: for this vote to work, each player has to solve the problem correctly with reasonable probability, which already require Ω̃(Hε) pulls of each. Even if we somehow split the arms between players and let each player explore a share of them, a majority vote would still fail since those players getting the “good” arms might have to pull arms Ω̃(Hε) times—a small MAB instance might be as hard as the full-sized problem (in terms of the complexity measure Hε).\nWhen considering algorithms employing multiple communication rounds, we use an ideal simulated serial algorithm (i.e., a full-communication approach) as our baseline. This approach is of course prohibited in our context, but is able to achieve the optimal parallel speed-up, linear in the number of players k."
    }, {
      "heading" : "2.2 Our results",
      "text" : "We now discuss our approach and overview our algorithmic results. These are summarized in Table 1 below, that compares the different algorithms in terms of parallel speed-up and communication.\nOur approach for the one-round case is based on the idea of majority vote. For the best-arm identification task, our observation is that by letting each player explore a smaller set of n/ √ k arms chosen at random and choose one of them as “best”, about √ k of the players would come up with the global best arm. This (partial) consensus on a single arm is a key aspect in our approach, since it allows the players to identify the correct best arm among the votes of all k players, after sharing information only once. Our approach leads to a factor √ k parallel speed-up which, as we demonstrate in our lower bound, is the optimal factor in this setting. Although our goal here is pure exploration, in our algorithms each player follows an explore-exploit strategy. The idea is that a player should sample his recommended arm as much as his budget permits, even if it was easy to identify in his smallsized problem. This way we can guarantee that the top arms are sampled to a sufficient precision by the time each of the players has to choose a single best arm.\nThe algorithm for the (ε, δ)-PAC setup is similar, but its analysis is more challenging. As mentioned above, an agreement on a single arm is essential for a vote to work. Here, however, there might be several ε-best arms, so arriving at a consensus on a single one is more difficult. Nonetheless, by examining two different regimes, namely when there are “many” ε-best arms and when there are “few” of them, our analysis shows that a vote can still work and achieve the √ k multiplicative speed-up.\nIn the case of multiple communication rounds, we present a distributed elimination-based algorithm that discards arms right after each communication round. Between rounds, we share the work load between players uniformly. We show that the number of such rounds can be reduced to as low as O(log(1/ε)), by eliminating all 2−r-suboptimal arms in the r’th round. A similar idea was employed in [4] for improving the regret bound of UCB with respect to the parameters ∆i. We also use this technique to develop an algorithm that performs only R communication rounds, for any given parameter R ≥ 1, that achieves a slightly worse multiplicative ε2/Rk speed-up."
    }, {
      "heading" : "3 One Communication Round",
      "text" : "This section considers the most basic variant of the multi-player MAB problem, where each player is only allowed a single transmission, when finishing her queries. For the clarity of exposition, we first consider the best-arm identification setting in Section 3.1. Section 3.2 deals with the (ε, δ)-PAC setup. We demonstrate the tightness of our result in Section 3.3 with a lower bound for the required budget of arm pulls in this setting.\nOur algorithms in this section assume the availability of a serial algorithm A(A, ε), that given a set of arms A and target accuracy ε, identifies an ε-best arm in A with probability at least 2/3 using no more than\ncA ∑ i∈A 1 (∆εi ) 2 log |A| ∆εi\n(2)\narm pulls, for some constant cA > 1. For example, the Successive Elimination algorithm [14] and the Exp-Gap Elimination algorithm [18] provide a guarantee of this form. Essentially, any exploration strategy whose guarantee is expressed as a function of Hε can be used as the procedure A, with technical modifications in our analysis."
    }, {
      "heading" : "3.1 Best-arm Identification Algorithm",
      "text" : "We now describe our one-round best-arm identification algorithm. For simplicity, we present a version matching δ = 1/3, meaning that the algorithm produces the correct arm with probability at least 2/3; we later explain how to extend it to deal with arbitrary values of δ.\nOur algorithm is akin to a majority vote among the multiple players, in which each player pulls arms in two stages. In the first EXPLORE stage, each player independently solves a “smaller” MAB instance on a random subset of the arms using the exploration strategy A. In the second EXPLOIT stage, each player exploits the arm identified as “best” in the first stage, and communicates that arm and its observed average reward. See Algorithm 1 below for a precise description. An appealing feature of our algorithm is that it requires each player to transmit a single message of constant size (up to logarithmic factors).\nAlgorithm 1 ONE-ROUND BEST-ARM input time horizon T output an arm\n1: for player j = 1 to k do 2: choose a subset Aj of 6n/ √ k arms uniformly at random 3: EXPLORE: execute ij ← A(Aj , 0) using\nat most 12T pulls (and halting the algorithm early if necessary); if the algorithm fails to identify any arm or does not terminate gracefully, let ij be an arbitrary arm\n4: EXPLOIT: pull arm ij for 12T times and let q̂j be its average reward 5: communicate the numbers ij , q̂j 6: end for 7: let ki be the number of players j with ij = i,\nand define A = {i : ki > √ k} 8: let p̂i = (1/ki) ∑ {j : ij=i} q̂j for all i 9: return arg maxi∈A p̂i; if the set A is empty, output an arbitrary arm.\nIn Theorem 3.1 we prove that Algorithm 1 indeed achieves the promised upper bound. Theorem 3.1. Algorithm 1 identifies the best arm correctly with probability at least 2/3 using no more than\nO ( 1√ k · n∑ i=2 1 ∆2i log n ∆i ) arm pulls per player, provided that 6 ≤ √ k ≤ n. The algorithm uses a single communication round, in which each player communicates Õ(1) bits.\nBy repeating the algorithm O(log(1/δ)) times and taking the majority vote of the independent runs, we can amplify the success probability to 1 − δ for any given δ > 0. Note that we can still do that with one communication round (at the end of all executions), but each player now has to communicate O(log(1/δ)) values1. Theorem 3.2. There exists a k-player algorithm that given Õ ( 1√ k ∑n i=2 1/∆ 2 i ) arm pulls, identifies the best arm correctly with probability at least 1 − δ. The algorithm uses a single communication round, in which each player communicates O(log(1/δ)) numerical values.\nWe now prove Theorem 3.1. We show that a budget T of samples (arm pulls) per player, where\nT ≥ 24cA√ k · n∑ i=2 1 ∆2i ln n ∆i , (3)\nsuffices for the players to jointly identify the best arm i? with the desired probability. Clearly, this would imply the bound stated in Theorem 3.1. We note that we did not try to optimize the constants in the above expression.\nWe begin by analyzing the EXPLORE phase of the algorithm. Our first lemma shows that each player chooses the global best arm and identifies it as the local best arm with sufficiently large probability.\n1In fact, by letting each player pick a slightly larger subset of O( √ log(1/δ) ·n/ √ k) arms, we can amplify the success probability to 1− δ without needing to communicate more than 2 values per player. However, this approach only works when k = Ω(log(1/δ)).\nLemma 3.3. When (3) holds, each player identifies the (global) best arm correctly after the EXPLORE phase with probability at least 2/ √ k.\nWe next address the EXPLOIT phase. The next simple lemma shows that the popular arms (i.e. those selected by many players) are estimated to a sufficient precision. Lemma 3.4. Provided that (3) holds, we have |p̂i − pi| ≤ 12∆? for all arms i ∈ A with probability at least 5/6.\nDue to lack of space, the proofs of the above lemmas are omitted and can be found in [16]. We can now prove Theorem 3.1.\nProof (of Theorem 3.1). Let us first show that with probability at least 5/6, the best arm i is contained in the set A. To this end, notice that ki? is the sum of k i.i.d. Bernoulli random variables {Ij}j where Ij is the indicator of whether player j chooses arm i? after the EXPLORE phase. By Lemma 3.3 we have that E[Ij ] ≥ 2/ √ k for all j, hence by Hoeffding’s inequality, Pr[ki? ≤ √ k] ≤ Pr[ki? − E[ki? ] ≤ − √ k] ≤ exp(−2k/k) ≤ 1/6 which implies that i? ∈ A with probability at least 5/6.\nNext, note that with probability at least 5/6 the arm i ∈ A having the highest empirical reward p̂i is the one with the highest expected reward pi. Indeed, this follows directly from Lemma 3.4 that shows that with probability at least 5/6, for all arms i ∈ A the estimate p̂i is within 12∆ of the true bias pi. Hence, via a union bound we conclude that with probability at least 2/3, the best arm is in A and has the highest empirical reward. In other words, with probability at least 2/3 the algorithm outputs the best arm i?."
    }, {
      "heading" : "3.2 (ε, δ)-PAC Algorithm",
      "text" : "We now present an algorithm whose purpose is to recover an ε-optimal arm. Here, there might be more than one ε-best arm, so each “successful” player might come up with a different ε-best arm. Nevertheless, our analysis below shows that with high probability, a subset of the players can still agree on a single ε-best arm, which makes it possible to identify it among the votes of all players. Our algorithm is described in Algorithm 2, and the following theorem states its guarantees. Theorem 3.5. Algorithm 2 identifies a 2ε-best arm with probability at least 2/3 using no more than\nO ( 1√ k · n∑ i=2 1 (∆εi ) 2 log n ∆εi ) arm pulls per player, provided that 24 ≤ √ k ≤ n. The algorithm uses a single communication\nround, in which each player communicates Õ(1) bits.\nBefore proving the theorem, we first state several key lemmas. In the following, let nε and n2ε denote the number of ε-best and 2ε-best arms respectively. Our analysis considers two different regimes: n2ε ≤ 150 √ k and n2ε > 150 √ k, and shows that in any case,\nT ≥ 400cA√ k n∑ i=2 1 (∆εi ) 2 ln 24n ∆εi (4)\nsuffices for identifying a 2ε-best arm with the desired probability. Clearly, this implies the bound stated in Theorem 3.5.\nThe first lemma shows that at least one of the players is able to find an ε-best arm. As we later show, this is sufficient for the success of the algorithm in case there are many 2ε-best arms. Lemma 3.6. When (4) holds, at least one player successfully identifies an ε-best arm in the EXPLORE phase, with probability at least 5/6.\nThe next lemma is more refined and states that in case there are few 2ε-best arms, the probability of each player to successfully identify an ε-best arm grows linearly with nε. Lemma 3.7. Assume that n2ε ≤ 150 √ k. When (4) holds, each player identifies an ε-best arm in the\nEXPLORE phase, with probability at least 2nε/ √ k.\nAlgorithm 2 ONE-ROUND ε-ARM input time horizon T , accuracy ε output an arm\n1: for player j = 1 to k do 2: choose a subset Aj of 12n/ √ k arms uniformly at random 3: EXPLORE: execute ij ← A(Aj , ε) using\nat most 12T pulls (and halting the algorithm early if necessary); if the algorithm fails to identify any arm or does not terminate gracefully, let ij be an arbitrary arm\n4: EXPLOIT: pull arm ij for 12T times, and let q̂j be the average reward 5: communicate the numbers ij , q̂j 6: end for 7: let ki be the number of players j with ij = i 8: let ti = 12kiT and p̂i = (1/ki) ∑ {j : ij=i} q̂j\nfor all i 9: define A = {i ∈ [n] : ti ≥ (1/ε2) ln(12n)}\n10: return arg maxi∈A p̂i; if the set A is empty, output an arbitrary arm.\nThe last lemma we need analyzes the accuracy of the estimated rewards of arms in the set A. Lemma 3.8. With probability at least 5/6, we have |p̂i − pi| ≤ ε/2 for all arms i ∈ A.\nFor the proofs of the above lemmas, refer to [16]. We now turn to prove Theorem 3.5.\nProof. We shall prove that with probability 5/6 the set A contains at least one ε-best arm. This would complete the proof, since Lemma 3.8 assures that with probability 5/6, the estimates p̂i of all arms i ∈ A are at most ε/2-away from the true reward pi, and in turn implies (via a union bound) that with probability 2/3 the arm i ∈ A having the maximal empirical reward p̂i must be a 2ε-best arm. First, consider the case n2ε > 150 √ k. Lemma 3.6 shows that with probability 5/6 there exists a player j that identifies an ε-best arm ij . Since for at least n2ε arms ∆i ≤ 2ε, we have\ntij ≥ 12T ≥ 400 2 √ k · n2ε − 1 (2ε)2 ln 24n 2ε\n≥ 1 ε2 ln(12n) ,\nthat is, ij ∈ A. Next, consider the case n2ε ≤ 150 √ k. Let N denote the number of players that identified some ε-best arm. The random variable N is a sum of Bernoulli random variables {Ij}j where Ij indicates whether player j identified some ε-best arm. By Lemma 3.7, E[Ij ] ≥ 2nε/ √ k and thus by Hoeffding’s inequality, Pr[N < nε √ k] = Pr[N − E[N ] ≤ −nε √ k] ≤ exp(−2n2ε) ≤ 1/6 .\nThat is, with probability 5/6, at least nε √ k players found an ε-best arm. A pigeon-hole argument\nnow shows that in this case there exists an ε-best arm i? selected by at least √ k players. Hence, with probability 5/6 the number of samples of this arm collected in the EXPLOIT phase is at least ti? ≥ √ kT/2 > (1/ε2) ln(12n), which means that i? ∈ A."
    }, {
      "heading" : "3.3 Lower Bound",
      "text" : "The following theorem suggests that in general, for identifying the best arm k players achieve a multiplicative speed-up of at most Õ( √ k) when allowing one transmission per player (at the end of the game). Clearly, this also implies that a similar lower bound holds in the PAC setup, and proves that our algorithmic results for the one-round case are essentially tight. Theorem 3.9. For any k-player strategy that uses a single round of communication, there exist rewards p1, . . . , pn ∈ [0, 1] and integer T such that\n• each individual player must use at least T/ √ k arm pulls for them to collectively identify the\nbest arm with probability at least 2/3; • there exist a single-player algorithm that needs at most Õ(T ) pulls for identifying the best arm\nwith probability at least 2/3.\nThe proof of the theorem is omitted due to space constraints and can be found in [16]."
    }, {
      "heading" : "4 Multiple Communication Rounds",
      "text" : "In this section we establish an explicit tradeoff between the performance of a multi-player algorithm and the number of communication rounds it uses, in terms of the accuracy ε. Our observation is that\nby allowing O(log(1/ε)) rounds of communication, it is possible to achieve the optimal speedup of factor k. That is, we do not gain any improvement in learning performance by allowing more than O(log(1/ε)) rounds.\nAlgorithm 3 MULTI-ROUND ε-ARM input (ε, δ) output an arm\n1: initialize S0 ← [n], r ← 0, t0 ← 0 2: repeat 3: set r ← r + 1 4: let εr ← 2−r, tr ← (2/kε2r) ln(4nr2/δ) 5: for player j = 1 to k do 6: sample each arm i ∈ Sr−1 for tr − tr−1 times 7: let p̂rj,i be the average reward of arm i (in all rounds so far of player j) 8: communicate the numbers p̂rj,1, . . . , p̂ r j,n\n9: end for 10: let p̂ri = (1/k) ∑k j=1 p̂ r j,i for all i ∈ Sr−1,\nand let p̂r? = maxi∈Sr−1 p̂ r i\n11: set Sr ← Sr−1\\{i ∈ Sr−1 : p̂ri < p̂r?−εr} 12: until εr ≤ ε/2 or |Sr| = 1 13: return an arm from Sr\nOur algorithm is given in Algorithm 3. The idea is to eliminate in each round r (i.e., right after the rth communication round) all 2−rsuboptimal arms. We accomplish this by letting each player sample uniformly all remaining arms and communicate the results to other players. Then, players are able to eliminate suboptimal arms with high confidence. If each such round is successful, after log2(1/ε) rounds only ε-best arms survive. Theorem 4.1 below bounds the number of arm pulls used by this algorithm (a proof can be found in [16]). Theorem 4.1. With probability at least 1 − δ, Algorithm 3\n• identifies the optimal arm using\nO\n( 1\nk · n∑ i=2 1 (∆εi ) 2 log ( n δ log 1 ∆εi )) arm pulls per player;\n• terminates after at most 1 + dlog2(1/ε)e rounds of communication (or after 1 + dlog2(1/∆?)e rounds for ε = 0).\nBy properly tuning the elimination thresholds εr of Algorithm 3 in accordance with the target accuracy ε, we can establish an explicit trade-off between the number of communication rounds and the number of arm pulls each player needs. In particular, we can design a multi-player algorithm that terminates after at most R communication rounds, for any given parameter R > 0. This, however, comes at the cost of a compromise in learning performance as quantified in the following corollary.\nCorollary 4.2. Given a parameter R > 0, set εr ← εr/R for all r ≥ 1 in Algorithm 3. With probability at least 1− δ, the modified algorithm\n• identifies an ε-best arm using Õ((ε−2/R/k) · ∑n\ni=2(1/∆ ε i ) 2) arm pulls per player; • terminates after at most R rounds of communication."
    }, {
      "heading" : "5 Conclusions and Further Research",
      "text" : "We have considered a collaborative MAB exploration problem, in which several independent players explore a set of arms with a common goal, and obtained the first non-trivial results in such setting. Our main results apply for the specifically interesting regime where each of the players is allowed a single transmission; this setting fits naturally to common distributed frameworks such as MapReduce. An interesting open question in this context is whether one can obtain a strictly better speed-up result (which, in particular, is independent of ε) by allowing more than a single round. Even when allowing merely two communication rounds, it is unclear whether the √ k speed-up can be improved. Intuitively, the difficulty here is that in the second phase of a reasonable strategy each player should focus on the arms that excelled in the first phase; this makes the sub-problems being faced in the second phase as hard as the entire MAB instance, in terms of the quantity Hε. Nevertheless, we expect our one-round approach to serve as a building-block in the design of future distributed exploration algorithms, that are applicable in more complex communication models.\nAn additional interesting problem for future research is how to translate our results to the regret minimization setting. In particular, it would be nice to see a conversion of algorithms like UCB [5] to a distributed setting. In this respect, perhaps a more natural distributed model is a one resembling that of Kanade et al. [17], that have established a regret vs. communication trade-off in the nonstochastic setting."
    } ],
    "references" : [ {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "A. Agarwal", "J.C. Duchi" ],
      "venue" : "In NIPS, pages 873–881,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Online models for content optimization",
      "author" : [ "D. Agarwal", "B.-C. Chen", "P. Elango", "N. Motgi", "S.-T. Park", "R. Ramakrishnan", "S. Roy", "J. Zachariah" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Best arm identification in multi-armed bandits",
      "author" : [ "J.-Y. Audibert", "S. Bubeck", "R. Munos" ],
      "venue" : "In COLT, pages",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem",
      "author" : [ "P. Auer", "R. Ortner" ],
      "venue" : "Periodica Mathematica Hungarica,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "M. Balcan", "A. Blum", "S. Fine", "Y. Mansour" ],
      "venue" : "Arxiv preprint arXiv:1204.3514,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Pure exploration in multi-armed bandits problems",
      "author" : [ "S. Bubeck", "R. Munos", "G. Stoltz" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Mortal multi-armed bandits",
      "author" : [ "D. Chakrabarti", "R. Kumar", "F. Radlinski", "E. Upfal" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Efficient protocols for distributed classification and optimization",
      "author" : [ "H. Daumé III", "J.M. Phillips", "A. Saha", "S. Venkatasubramanian" ],
      "venue" : "In ALT,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Protocols for learning classifiers on distributed data",
      "author" : [ "H. Daumé III", "J.M. Phillips", "A. Saha", "S. Venkatasubramanian" ],
      "venue" : "AISTAT,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "MapReduce: simplified data processing on large clusters",
      "author" : [ "J. Dean", "S. Ghemawat" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Distributed dual averaging",
      "author" : [ "J. Duchi", "A. Agarwal", "M.J. Wainwright" ],
      "venue" : "in networks. NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems",
      "author" : [ "E. Even-Dar", "S. Mannor", "Y. Mansour" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Multi-bandit best arm identification",
      "author" : [ "V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Distributed exploration in multiarmed bandits",
      "author" : [ "E. Hillel", "Z. Karnin", "T. Koren", "R. Lempel", "O. Somekh" ],
      "venue" : "arXiv preprint arXiv:1311.0800,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Distributed non-stochastic experts",
      "author" : [ "V. Kanade", "Z. Liu", "B. Radunovic" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Almost optimal exploration in multi-armed bandits",
      "author" : [ "Z. Karnin", "T. Koren", "O. Somekh" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Distributed learning in multi-armed bandit with multiple players",
      "author" : [ "K. Liu", "Q. Zhao" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2010
    }, {
      "title" : "The sample complexity of exploration in the multi-armed bandit problem",
      "author" : [ "S. Mannor", "J. Tsitsiklis" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Hoeffding races: Accelerating model selection search for classification and function approximation",
      "author" : [ "O. Maron", "A.W. Moore" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1994
    }, {
      "title" : "Empirical bernstein stopping",
      "author" : [ "V. Mnih", "C. Szepesvári", "J.-Y. Audibert" ],
      "venue" : "In ICML,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "How does clickthrough data reflect retrieval quality",
      "author" : [ "F. Radlinski", "M. Kurup", "T. Joachims" ],
      "venue" : "In CIKM,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "MAB algorithms rank results of search engines [23, 24], choose between stories or ads to showcase on web sites [2, 8], accelerate model selection and stochastic optimization tasks [21, 22], and more.",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 23,
      "context" : "MAB algorithms rank results of search engines [23, 24], choose between stories or ads to showcase on web sites [2, 8], accelerate model selection and stochastic optimization tasks [21, 22], and more.",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "MAB algorithms rank results of search engines [23, 24], choose between stories or ads to showcase on web sites [2, 8], accelerate model selection and stochastic optimization tasks [21, 22], and more.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "MAB algorithms rank results of search engines [23, 24], choose between stories or ads to showcase on web sites [2, 8], accelerate model selection and stochastic optimization tasks [21, 22], and more.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "MAB algorithms rank results of search engines [23, 24], choose between stories or ads to showcase on web sites [2, 8], accelerate model selection and stochastic optimization tasks [21, 22], and more.",
      "startOffset" : 180,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "MAB algorithms rank results of search engines [23, 24], choose between stories or ads to showcase on web sites [2, 8], accelerate model selection and stochastic optimization tasks [21, 22], and more.",
      "startOffset" : 180,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "Following recent MAB literature [14, 3, 15, 18], we focus on the problem of identifying a “good” bandit arm with high confidence.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "Following recent MAB literature [14, 3, 15, 18], we focus on the problem of identifying a “good” bandit arm with high confidence.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "Following recent MAB literature [14, 3, 15, 18], we focus on the problem of identifying a “good” bandit arm with high confidence.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "Following recent MAB literature [14, 3, 15, 18], we focus on the problem of identifying a “good” bandit arm with high confidence.",
      "startOffset" : 32,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "Our goal is to find an arm with an (almost) optimal expected reward, with as few arm pulls as possible (that is, minimize the simple regret [7]).",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "Round-based models are natural in distributed learning scenarios, where frameworks such as MapReduce [11] are ubiquitous.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually compete with each other, either on arm-pulls resources [15] or on the rewards received [19].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually compete with each other, either on arm-pulls resources [15] or on the rewards received [19].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 16,
      "context" : "[17] in the context of non-stochastic (i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "Another line of recent work was focused on distributed stochastic optimization [13, 1, 12] and distributed PAC models [6, 10, 9], investigating the involved communication trade-offs.",
      "startOffset" : 79,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "Another line of recent work was focused on distributed stochastic optimization [13, 1, 12] and distributed PAC models [6, 10, 9], investigating the involved communication trade-offs.",
      "startOffset" : 79,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "Another line of recent work was focused on distributed stochastic optimization [13, 1, 12] and distributed PAC models [6, 10, 9], investigating the involved communication trade-offs.",
      "startOffset" : 79,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "Another line of recent work was focused on distributed stochastic optimization [13, 1, 12] and distributed PAC models [6, 10, 9], investigating the involved communication trade-offs.",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "Another line of recent work was focused on distributed stochastic optimization [13, 1, 12] and distributed PAC models [6, 10, 9], investigating the involved communication trade-offs.",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "Another line of recent work was focused on distributed stochastic optimization [13, 1, 12] and distributed PAC models [6, 10, 9], investigating the involved communication trade-offs.",
      "startOffset" : 118,
      "endOffset" : 128
    }, {
      "referenceID" : 12,
      "context" : "Questions involving network topology [13, 12] and delays [1] are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 11,
      "context" : "Questions involving network topology [13, 12] and delays [1] are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Questions involving network topology [13, 12] and delays [1] are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : ", when there is only one player), the lower bounds of Mannor and Tsitsiklis [20] and Audibert et al.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "[3] show that in general Ω̃(Hε) pulls are necessary for identifying an ε-arm, where",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "Intuitively, the hardness of the task is therefore captured by the quantity Hε, which is roughly the number of arm pulls needed to find an ε-best arm with a reasonable probability; see also [3] for a discussion.",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 3,
      "context" : "A similar idea was employed in [4] for improving the regret bound of UCB with respect to the parameters ∆i.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "For example, the Successive Elimination algorithm [14] and the Exp-Gap Elimination algorithm [18] provide a guarantee of this form.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 17,
      "context" : "For example, the Successive Elimination algorithm [14] and the Exp-Gap Elimination algorithm [18] provide a guarantee of this form.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "Due to lack of space, the proofs of the above lemmas are omitted and can be found in [16].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "For the proofs of the above lemmas, refer to [16].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "The proof of the theorem is omitted due to space constraints and can be found in [16].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "1 below bounds the number of arm pulls used by this algorithm (a proof can be found in [16]).",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "In particular, it would be nice to see a conversion of algorithms like UCB [5] to a distributed setting.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "[17], that have established a regret vs.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2013,
    "abstractText" : "We study exploration in Multi-Armed Bandits in a setting where k players collaborate in order to identify an ε-optimal arm. Our motivation comes from recent employment of bandit algorithms in computationally intensive, large-scale applications. Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them. In particular, our main result shows that by allowing the k players to communicate only once, they are able to learn √ k times faster than a single player. That is, distributing learning to k players gives rise to a factor √ k parallel speedup. We complement this result with a lower bound showing this is in general the best possible. On the other extreme, we present an algorithm that achieves the ideal factor k speed-up in learning performance, with communication only logarithmic in 1/ε.",
    "creator" : null
  }
}