{
  "name" : "9461cce28ebe3e76fb4b931c35a169b0.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Direct 0-1 Loss Minimization and Margin Maximization with Boosting",
    "authors" : [ "Shaodan Zhai", "Tian Xia", "Ming Tan", "Shaojun Wang" ],
    "emails" : [ "zhai.6@wright.edu", "xia.7@wright.edu", "tan.6@wright.edu", "shaojun.wang@wright.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The classification problem in machine learning and data mining is to predict an unobserved discrete output value y based on an observed input vector x. In the spirit of the model-free framework, it is always assumed that the relationship between the input vector and the output value is stochastic and described by a fixed but unknown probability distribution p(X,Y ) [7]. The goal is to learn a classifier, i.e., a mapping function f(x) from x to y ∈ Y such that the probability of the classification error is small. As it is well known, the optimal choice is the Bayes classifier [7]. However, since p(X,Y ) is unknown, we cannot learn the Bayes classifier directly. Instead, following Vapnik’s general setting of the empirical risk minimization [7, 24], we focus on a more realistic goal: Given a set of training data D = {(x1, y1), · · · , (xn, yn)} independently drawn from p(X,Y ), we consider finding f(x) in a function classH that minimizes the empirical classification error,\n1\nn\nn ∑\ni=1\n1(ŷi 6= yi) (1)\nwhere ŷi = argmaxy∈Y yf(xi), Y = {−1, 1} and 1(·) is an indicator function. Under certain conditions, direct empirical classification error minimization is consistent [24] and under low noise situations it has a fast convergence rate [15, 23]. However, due to the nonconvexity, nondifferentiability and discontinuity of the classification error function, the minimization of (1) is typically NP-hard for general linear models [13]. The common approach is to minimize a surrogate function which is usually a convex upper bound of the classification error function. The problem of minimizing the empirical surrogate loss turns out to be a convex programming problem with considerable computational advantages and learned classifiers remain consistent to Bayes classifier [1, 20, 28, 29], however clearly there is a mismatch between “desired” loss function used in inference and “training” loss function during the training process [16]. Moreover, it has been shown that all boosting algorithms based on convex functions are susceptible to random classification noise [14].\nBoosting is a machine-learning method based on the idea of creating a single, highly accurate classifier by combining many weak and inaccurate “rules of thumb.” A remarkably rich theory and a record of empirical success [18] have evolved around boosting, nevertheless it is still not clear how to best exploit what is known about how boosting operates, even for binary classification. In\nthis paper, we propose a boosting method for binary classification – DirectBoost – a greedy coordinate descent algorithm that directly minimizes classification error over labeled training examples to build an ensemble linear classifier of weak classifiers. Once the training error is reduced to a (local coordinatewise) minimum, DirectBoost runs a coordinate ascent algorithm that greedily adds weak classifiers by directly maximizing any targeted arbitrarily defined margins, it might escape the region of minimum training error in order to achieve a larger margin. The algorithm stops once a (local coordinatewise) maximum of the margins is reached. In the next section, we first present a coordinate descent algorithm that directly minimizes 0-1 loss over labeled training examples. We then describe coordinate ascent algorithms that aims to directly maximize any targeted arbitrarily defined margins right after we reach a (local coordinatewise) minimum of 0-1 loss. In Section 3, we show experimental results on a collection of machine-learning benchmark data sets for DirectBoost, AdaBoost [9], LogitBoost [11], LPBoost with column generation [6] and BrownBoost [10], and discuss our findings. Due to space limitation, the proofs of theorems, related works, technical details as well as conclustions and future works are given in the full version of this paper [27]."
    }, {
      "heading" : "2 DirectBoost: Minimizing 0-1 Loss and Maximizing Margins",
      "text" : "Let H = {h1, ..., hl} denote the set of all possible weak classifiers that can be produced by the weak learning algorithm, where a weak classifier hj ∈ H is a mapping from an instance space X to Y = {−1, 1}. The hjs are not assumed to be linearly independent, and H is closed under negation, i.e., both h and −h belong to H. We assume that the training set consists of examples with labels {(xi, yi)}, i = 1, · · · , n, where (xi, yi) ∈ X × Y that are generated independently from p(X,Y ). We define C of H as the set of mappings that can be generated by taking a weighted average of classifiers fromH:\nC =\n{\nf : x → ∑\nh∈H\nαhh(x) | αh ≥ 0\n}\n, (2)\nThe goal here is to find f ∈ C that minimizes the empirical classification error (1), and has good generalization performance."
    }, {
      "heading" : "2.1 Minimizing 0-1 Loss",
      "text" : "Similar to AdaBoost, DirectBoost works by sequentially running an iterative greedy coordinate descent algorithm, each time directly minimizing true empirical classification error (1) instead of a weighted empirical classification error in AdaBoost. That is, for each iteration, only the parameter of a weak classifier that leads to the most significant true classification error reduction is updated, while the weights of all other weak classifiers are kept unchanged. The rationale is that the inference used to predict the label of a sample can be written as a linear function with a single parameter.\nConsider the tth iteration, the ensemble classifier is\nft(x) =\nt ∑\nk=1\nαkhk(x) (3)\nwhere previous t− 1 weak classifiers hk(x) and corresponding weights αk, k = 1, · · · , t− 1 have been selected and determined. The inference function for sample xi is defined as\nFt(xi, y) = yft(xi) = y (\nt−1 ∑\nk=1\nαkhk(xi)) + αtyht(xi) (4)\nSince a(xi) = ∑t−1\nk=1 αkhk(xi) is constant and hk(xi) is either +1 or -1 depending on sample xi, we re-write the equation above as,\nFt(xi, y) = y ht(xi)αt + ya(xi) (5)\nNote that for each label y of sample xi, there is a linear function of αt with the slope to be either +1 or -1 and intercept to be ya(xi). Given an input of αt, each example xi has two linear scoring functions, Ft(xi,+1) and Ft(xi,−1), i = 1, · · · , n, one for the positive label y = +1 and one for the negative label y = −1. From these two linear scoring functions, the one with the higher score determines the predicted label ŷi of the ensemble classifier ft(xi). The intersection point ei of these two linear scoring functions is the critical point that the predicted label ŷi switches its sign, the intersection point satisfies the condition that Ft(xi,+1) = Ft(xi,−1) = 0, i.e. a(xi) + αtht(xi) = 0, and can be computed as ei = −\na(xi) ht(xi)\n, i = 1, · · · , n. These points divide αt into (at most) n + 1 intervals, each interval has the value of a true classification error, thus the classification error is a stepwise\nAlgorithm 1 Greedy coordinate descent algorithm that minimizes a 0-1 loss. 1: D = {(xi, yi), i = 1, · · · , n} 2: Sort |a(xi)|, i = 1, · · · , n in an increasing order. 3: for a weak classifier hk ∈ H do 4: Visit each sample in the order that |a(xi)| is increasing. 5: Compute the slope and intercept of F (xi, yi) = yihk(xi)α+ yia(xi). 6: Let êi = |a(xi)|. 7: If (slope > 0 and intercept < 0), error update on the righthand side of êi is -1. 8: If (slope < 0 and intercept > 0), error update on the righthand side of êi is +1. 9: Incrementally calculate classification error on intervals of êis. 10: Get the interval that has minimum classification error. 11: end for 12: Pick the weak classifiers that lead to largest classification error reduction. 13: Among selected these weak classifiers, only update the weight of one weak classifier that gives\nthe smallest exponential loss. 14: Repeat 2-13 until training error reaches minimum.\nfunction of αt. The value of ei, i = 1, · · · , n can be negative or positive, however sinceH is closed in negation, we only care about these that are positive.\nThe greedy coordinate descent algorithm that sequentially minimizes a 0-1 loss is described in Algorithm 1, lines 3-11 are the weak learning steps and the rest are boosting steps. Consider an example with 4 samples to illustrate this procedure. Suppose for a weak classifier, we have Ft(xi, yi), i = 1, 2, 3, 4 as shown in Figure 1. At αt = 0, samples x1 and x2 have negative margins, thus they are misclassified, the error rate is 50%. We incrementally update the classification error on intervals of êi, i = 1, 2, 3, 4: For Ft(x1, y1), its slope is negative and its intercept is negative, sample x1 always has a negative margin for αt > 0, thus there is no error update on the right-hand side of ê1. For Ft(x2, y2), its slope is positive and its intercept is negative, then when αt is at the right side of ê2, sample x2 has positive margin and becomes correctly classified, so we update the error by -1, the error rate is reduced to 25%. For Ft(x3, y3), its slope is negative and its intercept is positive, then when αt is at the right side of ê3, sample x3 has a negative margin and becomes misclassified, so we update the error rate changes to 50% again. For Ft(x4, y4), its slope is positive and its intercept is positive, sample x4 always has positive margin for αt > 0, thus there is no error update on the right-hand side of ê4. We finally have the minimum error rate of 25% on the interval of [ê2, ê3].\n0 a1 a2\na3,|a3|\na4,|a4|\n|a1| |a2|\nê1 ê2 ê3 ê4\n0\nFt(x2, y2)\nFt(x3, y3)\nFt(x4, y4)\nFt(x1, y1)\n25%\n50% Classification error\nαt\nαtê2 ê3\nTheorem 1 The region of zero training error, if exists, is a cone, and it is not a set of isolated cones.\nAlgorithm 1 is a heuristic procedure that minimizes 0-1 loss, it is not guaranteed to find the global minimum, it may trap to a coordinatewise local minimum [22] of 0-1 loss. Nevertheless, we switch to algorithms that directly maximize the margins we present below."
    }, {
      "heading" : "2.2 Maximizing Margins",
      "text" : "The margins theory [17] provides an insightful analysis for the success of AdaBoost where the authors proved that the generalization error of any ensemble classifiers is bounded in terms of the\nentire distribution of margins of training examples, as well as the number of training examples and the complexity of the base classifiers, and AdaBoost’s dynamics has a strong tendency to increase the margins of training examples. Instead, we can prove that the generalization error of any ensemble classifier is bounded in terms of the average margin of bottom n′ samples or n′th order margin of training examples, as well as the number of training examples and the complexity of the base classifiers. This view motivates us to propose a coordinate ascent algorithm to directly maximize several types of margins just right after the training error reaches a (local coordinatewise) minimum.\nThe margin of a labeled example (xi, yi) with respect to an ensemble classifier ft(x) =∑t k=1 αkhk(xi) is defined to be\nmi = yi\n∑t\nk=1 αkhk(xi) ∑t\nk=1 αk\n(6)\nThis is a real number between -1 and +1 that intuitively measures the confidence of the classifier in its prediction on the ith example. It is equal to the weighted fraction of base classifiers voting for the correct label minus the weighted fraction voting for the incorrect label [17].\nWe denote the minimum margin, the average margin, and median margin over the training examples as gmin = mini∈{1,··· ,n} mi, gaverage = 1 n ∑n i=1 mi, and gmedian = median{mi, i = 1, · · · , n}. Furthermore, we can sort the margins over all training examples in an increasing order, and consider n′ worst training examples n′ ≤ n that have smaller margins, and compute the average margin over those n′ training examples. We call this the average margin of the bottom n′ samples, and denote it as gaverage n′ = 1n′ ∑ i∈Bn′\nmi, where Bn′ denotes the set of n′ samples having the smallest margins.\nThe margin maximization method described below is a greedy coordinate ascent algorithm that adds a weak classifier achieving maximum margin. It allows us to continuously maximize the margin while keeping the training error at a minimum by running the greedy coordinate descent algorithm presented in the previous section. The margin mi is a linear fractional function of α, and it is quasiconvex, and quasiconcave, i.e., quasilinear [2, 5]. Theorem 2 shows that the average margin of bottom n′ examples is quasiconcave in the region of the zero training error. Theorem 2 Denote the average margin of bottom n′ samples as\ngaverage n′(α) = ∑\ni∈{B n′ |α}\nyi ∑t\nk=1 αkhk(xi) ∑t\nk=1 αk\nwhere {Bn′ |α} denotes the set of n′ samples whose margins are at the bottom for fixed α. Then gaverage n′(α) in the region of zero training error is quasiconcave. We denote ai = ∑t−1 k=1 yiαkhk(xi), bi,t = yiht(xi) ∈ {−1,+1} and c = ∑t−1\nk=1 αk, then the margin on the ith example (xi, yi) can be rewritten as,\nillustration.\nConsider a greedy coordinate ascent algorithm that maximizes the average margin gaverage over all training examples. The derivative of gaverage can be written as,\n∂gaverage\n∂αt =\n∑n\ni=1 bi,tc−\n∑n\ni=1 ai\n(c+ αt)2 (9)\nAlgorithm 2 Greedy coordinate ascent algorithm that maximizes the average margin of bottom n′ examples. 1: Input: ai=1,··· ,n and c from previous round. 2: Sort ai=1,··· ,n in an increasing order. Bn′ ← {n′ samples having the smallest ai at αt = 0}. 3: for a weak classifier do 4: Determine the lowest sample whose margin is decreasing and determine d. 5: Compute Dn′ ← ∑ i∈Bn′\n(bi,tc− ai). 6: j ← 0, qj ← 0. 7: Compute the intersection qj+1 of the j+1th highest increasing margin in Bn′ and the j+1th smallest decreasing margin in Bcn′ (the complement of the set Bn′). 8: if qj+1 < d and Dn′ > 0 then 9: Incrementally update Bn′ , Bcn′ and Dn′ at αt = qj+1; j ← j + 1. 10: Go back to Line 7. 11: else 12: if Dn′ > 0 then q∗ ← d; otherwise q∗ ← qj . 13: Compute the average margin of the bottom n′ examples at q∗. 14: end if 15: end for 16: Pick the weak classifier with the largest increment of the average margin of bottom n′ examples\nwith weight being q∗. 17: Repeat 2-16 until no increment in average margin of bottom n′ examples.\nTherefore, the maximum average margin can only happen at two ends of the interval. As shown in Figure 2, the maximum average margin is either at the origin or at point d, which depends on the sign of the derivative in (9). If it is positive, the average margin is monotonically increasing, we set αt = d − ǫ, otherwise we set αt = 0. The greedy coordinate ascent algorithm found by: looking at all weak classifiers in H, if the nominator in (9) is positive, we let its weight ǫ close to the right value on the interval where the training error is minimum, and compute the value of the average margin. We add the weak classifier which has the largest average margin increment. We iterate this procedure until convergence. Its convergence is given by Theorem 3 shown below.\nTheorem 3 When constrained to the region of zero training error, the greedy coordinate ascent algorithm that maximizes the average margin over all examples converges to an optimal solution.\nNow consider a greedy coordinate ascent algorithm maximizing the average margin of bottom n′ training examples, gaverage n′ . Apparently maximizing the minimum margin is a special case by choosing n′ = 1. Figure 2 is a simple illustration with six training examples. Our aim is to maximize the average margin of the bottom 3 examples. The interval [0, d] of αt indicates an interval where the training error is zero. On the point of d, the sample margin m3 alters from positive to negative, which causes the training error jump from 0 to 1/6. As shown in Figure 2, the margin of each of six training examples is either monotonically increasing or decreasing.\nIf we know a fixed set of bottom n′ training examples having smaller margins for an interval of αt with a minimum training error, it is straightforward to compute the derivative of the average margin of bottom n′ training examples as\n∂gaverage n′\n∂αt =\n∑\ni∈B n′\nbi,tc− ∑\ni∈B n′\nai\n(c+ αt)2 (10)\nAgain gaverage n′ is a monotonic function of αt, depending on the sign of the derivative in (10), it is maximized either on the left side or on the right side of the interval.\nIn general, the set of bottom n′ training examples for an interval of αt with a minimum training error varies over αt, it is required to precisely search for any snapshot of bottom n′ examples with a different value of α.\nTo address this, we first examine when the margins of two examples intersect. Consider the ith example (xi, yi) with margin mi =\nai+bi,tαt c+αt\nand the jth example (xj , yj) with margin mj = aj+bj,tαt\nc+αt . Notice bi, bj is either -1 or +1. Assume bi = bj , then because mi 6= mj (since ai 6= aj), the margins of example i and example j never intersect; assume bi 6= bj , then because mi = mj\nat αt = |ai−aj | 2 , the margins of example i and example j might intersect with each other if |ai−aj | 2 belongs to the interval of αt with the minimum training error. In summary, given any two samples, we can decide whether they intersect by checking whether b terms have the same sign, if not, they do intersect, and we can determine the intersection point.\nThe greedy coordinate ascent algorithm that sequentially maximizes the average margin of bottom n′ examples is described in Algorithm 2, lines 3-15 are the weak learning steps and the rest are boosting steps. At line 5 we compute Dn′ which can be used to check the sign of the derivative in (10). Since the function of the average margin of bottom n′ examples is quasiconcave, we can determine the optimal point q∗ by Dn′ , and only need to compute the margin value at q∗. We add the weak learner, which has the largest increment of the average margin over bottom n′ examples, into the ensembled classifier. This procedure terminates if there is no increment in the average margin of bottom n′ examples over the considered weak classifiers. If M weak learners are considered, the computational complexity of Algorithm 2 in the training stage is O (max(n log n,Mn′)) for each iteration. The convergence analysis of Algorithm 2 is given by Theorem 4.\nTheorem 4 When constrained to the region of zero training error, the greedy coordinate ascent algorithm that maximizes average margin of bottom n′ samples converges to a coordinatewise maximum solution, but it is not guaranteed to converge to an optimal solution due to the non-smoothness of the average margin of bottom n′ samples.\nǫ-relaxation: Unfortunately, there is a fundamental difficulty in the greedy coordinate ascent algorithm that maximizes the average margin of bottom n′ samples: It gets stuck at a corner, from which it is impossible to make progress along any coordinate direction. We propose an ǫ-relaxation method to overcome this difficulty. This method was first proposed by [3] for the assignment problem, and was extended to the linear cost network flow problem and strictly convex costs and linear constraints [4, 21]. The main idea is to allow a single coordinate to change even if this worsens the margin function. When a coordinate is changed, it is set to ǫ plus or ǫ minus the value that maximizes the margin function along that coordinate, where ǫ is a positive number.\nWe can design a similar greedy coordinate ascent algorithm to directly maximize the bottom n′th sample margin by only making a slight modification to Algorithm 2: for a weak classifier, we choose the intersection point that led to the largest increasing of the bottom n′th margin. When combined with ǫ-relaxation, this algorithm will eventually approach a small neighbourhood of a local optimal solution that maximizes the bottom n′th sample margin. As shown in Figure 2, bottom n′th margin is a multimodal function, this algorithm with ǫ-relaxation is very sensitive to the choice of n′, and it usually gets stuck in a bad coordinatewise point without using ǫ-relaxation. However, an impressive advantage is that this method is tolerant to noise, which will be shown in Section 3."
    }, {
      "heading" : "3 Experimental Results",
      "text" : "In the experiments below, we first evaluate the performance of DirectBoost on 10 UCI data sets. We then evaluate noise robustness of DirectBoost. For all the algorithms in our comparison, we use decision trees with depth of either 1 or 3 as weak learners since for the small datasets, decision stumps (tree depth of 1) is already strong enough. DirectBoost with decision trees is implemented by a greedy top-down recursive partition algorithm to find the tree but differently from AdaBoost and LPBoost, since DirectBoost does not maintain a distribution over training samples. Instead, for each splitting node, DirectBoost simply chooses the attribute to split on by minimizing 0-1 loss or maximizing the predefined margin value. In all the experiments that ǫ-relaxation is used, the value of ǫ is 0.01. Note that our empirical study is focused on whether the proposed boosting algorithm is able to effectively improve the accuracy of state-of-the-art boosting algorithms with the same weak learner space H, thus we restrict our comparison to boosting algorithms with the same weak learners, rather than a wide range of classification algorithms, such as SVMs and KNN."
    }, {
      "heading" : "3.1 Experiments on UCI data",
      "text" : "We first compare DirectBoost with AdaBoost, LogitBoost, soft margin LPBoost and BrownBoost on 10 UCI data sets1 from the UCI Machine Learning Repository [8]. We partition each UCI dataset into five parts with the same number of samples for five-fold cross validation. In each fold, we use three parts for training, one part for validation, and the remaining part for testing. The validation\n1For Adult data, where we use a subset a5a in LIBSVM set http://www.csie.ntu.edu.tw/˜cjlin/libsvm. We do not use the original Adult data which has 48842 examples since LPBoost runs very slow on it.\nset is used to choose the optimal model for each algorithm: For AdaBoost and LogitBoost, the validation data is used to perform early stopping since there is no nature stopping criteria for these algorithms. We run the algorithms until convergence where the stopping criterion is that the change of loss is less than 1e-6, and then choose the ensemble classifier from the round with minimum error on the validation data. For BrownBoost, we select the optimal cutoff parameters by the validation set, which are chosen from {0.0001, 0.001, 0.01, 0.03, 0.05, 0.08, 0.1, 0.14, 0.17, 0.2}. LPBoost maximizes the soft margin subject to linear constraints, its objective is equivalent to DirectBoost with maximizing the average margin of bottom n′ samples [19], thus we set the same candidate parameters n′/n = {0.01, 0.05, 0.1, 0.2, 0.5, 0.8} for them. For LPBoost, the termination rule we use is same to the one in [6], and we select the optimal regularization parameter by the validation set. For DirectBoost, the algorithm terminates when there is no increment in the targeted margin value, and we select the model with the optimal n′ by the validation set.\nWe use DirectBoostavg to denote our method that runs Algorithm 1 first and then maximizes the average of bottom n′ margins without ǫ-relaxation, DirectBoostǫavg to denote our method that runs Algorithm 1 first and then maximizes the average margin of bottom n′ samples with ǫ-relaxation, and DirectBoostorder to denote our method that runs Algorithm 1 first and then maximizes the bottom n′th margin with ǫ-relaxation. The means and standard deviations of test errors are given in Table 1. Clearly DirectBoostavg, DirectBoostǫavg and DirectBoostorder outperform other boosting algorithms in general, specially DirectBoostǫavg is better than AdaBoost, LogitBoost, LPBoost and BrownBoost over all data sets except Cancer-wdbc. Among the family of DirectBoost algorithms, DirectBoostavg wins on two datasets where it searches the optimal margin solution in the region of zero training error, this means that keeping the training error at zero may lead to good performance in some cases. DirectBoostorder wins on three other datasets, but its results are unstable and sensitive to n′. With ǫ-relaxation, DirectBoostǫavg searches the optimal margin solution in the whole parameter space and gives the best performance on the remaining 5 data sets. It is well known that AdaBoost performs well on the datasets with a small test error such as Tic-tac-toe and Fourclass, it is extremely hard for other boosting algorithms to beat AdaBoost. Nevertheless, DirectBoost is still able to give even better results in this case. For example, on Tic-tac-toe data set, the test error becomes 0.63%, more than half the error rate reduction. Our method would be more valuable for those who value prediction accuracy, which might be the case in areas of medical and genetic research.\nure in Figure 3, even though no theoretical justification is known for this observed phenomenon.\nTable 2 shows the number of iterations and total run times (in seconds) for AdaBoost, LPBoost and DirectBoostǫavg at the training stage, where we use the Adult dataset with 10000 training samples. All these three algorithms employ decision trees with a depth of 3 as weak learners. The experiments are conducted on a PC with Core2 Duo 2.6GHz CPU and 2G RAM. Clearly\nDirectBoostǫavg takes less time for the entire training stage since it converges much faster. LPBoost converges in less than three hundred rounds, but as a total corrective algorithm, it has a greater computational cost on each round. To handle large scale data sets in practice, similar to AdaBoost, we can use many tricks. For example, we can partition the data into many parts and use distributed algorithms to select the weak classifier."
    }, {
      "heading" : "3.2 Evaluate noise robustness",
      "text" : "In the experiments conducted below, we evaluate the noise robustness of each boosting method. First, we run the above algorithms on a synthetic example created by [14]. This is a simple counterexample to show that for a broad class of convex loss functions, no boosting algorithm is provably robust to random label noise, this class includes AdaBoost, LogitBoost, etc. For LPBoost and its variations [25, 26], they do not satisfy the preconditions of the theorem presented by [14], but Glocer [12] showed experimentally that these soft margin boosting methods have the same problem as the AdaBoost and LogitBoost to handle random noise.\nWe repeat the synthetic learning problem with binary-valued weak classifiers that is described in [14]. We set the number of training examples to 1000 and the labels are corrupted with a noise rate η at 0%, 5%, and 20% respectively. Examples in this setting are binary vectors of length 2l+11. Table 3 reports the error rates on a clean test data set with size 5000, that is, the labels of test data are uncorrupted, and a same size clean data is generated as validation data. AdaBoost performs very poor on this problem. This result is not surprising at all since [14] designed this example on purpose to explain the inadequacy of convex optimization methods. LogitBoost, LPBoost with column generation, and DirectBoostǫavg perform better in the case that l = 5 and η = 5%, but for the other cases they do as bad as AdaBoost. BrownBoost is designed for noise tolerance, and it does well in the case of l = 5, but it also cannot handle the case of l = 20 and η > 0%. On the other hand, DirectBoostorder performs very well for all cases, showing DirectBoostorder’s impressive noise tolerance property since the most difficult examples are given up without any penalty.\nThese algorithms are also tested on two UCI datasets, randomly corrupted with additional label noise on training data at rates of 5% and 20% respectively. Again, we keep the validation and the test data are clean. The results are reported in Table 4 by five-fold cross validation, the same as Experiment 1. LPBoost with column generation, DirectBoostǫavg and DirectBoostorder do well in the case of η = 5%, and their performance is better than AdaBoost, LogitBoost, and BrownBoost. For the case of η = 20%, all the algorithms perform much worse than the corresponding noise-free case, except DirectBoostorder which still generates a good performance close to the noise-free case."
    }, {
      "heading" : "4 Acknowledgements",
      "text" : "This research is supported in part by AFOSR under grant FA9550-10-1-0335, NSF under grant IIS:RI-small 1218863, DoD under grant FA2386-13-1-3023, and a Google research award."
    } ],
    "references" : [ {
      "title" : "AdaBoost is consistent",
      "author" : [ "P. Bartlett", "M. Traskin" ],
      "venue" : "Journal of Machine Learning Research, 8:2347–2368",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Nonlinear Programming: Theory and Algorithms",
      "author" : [ "M. Bazaraa", "H. Sherali", "C. Shetty" ],
      "venue" : "3rd Edition. Wiley-Interscience",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A distributed algorithm for the assignment problem",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Technical Report, MIT",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Network Optimization: Continuous and Discrete Models",
      "author" : [ "D. Bertsekas" ],
      "venue" : "Athena Scientific",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Linear programming boosting via column generation",
      "author" : [ "A. Demiriz", "K. Bennett", "J. Shawe-Taylor" ],
      "venue" : "Machine Learning, 46:225–254",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "A Probabilistic Theory of Pattern Recognition Springer",
      "author" : [ "L. Devroye", "L. Györfi", "G. Lugosi" ],
      "venue" : "New York",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "UCI Machine Learning Repository",
      "author" : [ "A. Frank", "A. Asuncion" ],
      "venue" : "School of Information and Computer Science, University of California at Irvine",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R. Schapire" ],
      "venue" : "Journal of Computer and System Sciences, 55(1):119–139",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "An adaptive version of the boost by majority algorithm",
      "author" : [ "Y. Freund" ],
      "venue" : "Machine Learning, 43(3):293–318",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Additive logistic regression: A statistical view of boosting",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "The Annals of Statistics, 28(2):337–374",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Entropy regularization and soft margin maximization",
      "author" : [ "K. Glocer" ],
      "venue" : "Ph.D. Dissertation, UCSC",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "H",
      "author" : [ "K. Hoffgen" ],
      "venue" : "Simon and K. van Horn. Robust trainability of single neurons. Journal of Computer and System Sciences, 50(1):114–125",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Random classification noise defeats all convex potential boosters",
      "author" : [ "P. Long", "R. Servedio" ],
      "venue" : "Machine Learning, 78:287-304",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Smooth discrimination analysis",
      "author" : [ "E. Mammen", "A. Tsybakov" ],
      "venue" : "The Annals of Statistics, 27, 1808-1829",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Direct loss minimization for structured prediction",
      "author" : [ "D. McAllester", "T. Hazan", "J. Keshet" ],
      "venue" : "Neural Information Processing Systems (NIPS), 1594-1602",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Boosting the margin: A new explanation for the effectiveness of voting methods",
      "author" : [ "R. Schapire", "Y. Freund", "P. Bartlett", "W. Lee" ],
      "venue" : "The Annals of Statistics, 26(5):1651–1686",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Boosting: Foundations and Algorithms",
      "author" : [ "R. Schapire", "Y. Freund" ],
      "venue" : "MIT Press",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the equivalence of weak learnability and linear separability: new relaxations and efficient boosting algorithms",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer" ],
      "venue" : "Machine Learning, 80(2-3): 141-163",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Consistency of support vector machines and other regularized kernel classifiers",
      "author" : [ "I. Steinwart" ],
      "venue" : "IEEE Transactions on Information Theory, 51(1):128-142",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Relaxation methods for strictly convex costs and linear constraints",
      "author" : [ "P. Tseng", "D. Bertsekas" ],
      "venue" : "Mathematics of Operations Research, 16:462-481",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Convergence of block coordinate descent method for nondifferentiable minimization",
      "author" : [ "P. Tseng" ],
      "venue" : "Journal of Optimization Theory and Applications, 109(3):475–494",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Optimal aggregation of classifiers in statistical learning",
      "author" : [ "A. Tsybakov" ],
      "venue" : "The Annals of Statistics, 32(1):135- 166",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : "John Wiley",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Boosting algorithms for maximizing the soft margin",
      "author" : [ "M. Warmuth", "K. Glocer", "G. Ratsch" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 21, 1585-1592",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Entropy regularized LPBoost",
      "author" : [ "M. Warmuth", "K. Glocer", "S. Vishwanathan" ],
      "venue" : "The 19th International conference on Algorithmic Learning Theory (ALT), 256-271",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Direct 0-1 loss minimization and margin maximization with boosting",
      "author" : [ "S. Zhai", "T. Xia", "M. Tan", "S. Wang" ],
      "venue" : "Technical Report",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Statistical behavior and consistency of classification methods based on convex risk minimization",
      "author" : [ "T. Zhang" ],
      "venue" : "The Annals of Statistics, 32(1):56–85",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Boosting with early stopping: Convergence and consistency",
      "author" : [ "T. Zhang", "B. Yu" ],
      "venue" : "The Annals of Statistics, 33:1538–1579",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "In the spirit of the model-free framework, it is always assumed that the relationship between the input vector and the output value is stochastic and described by a fixed but unknown probability distribution p(X,Y ) [7].",
      "startOffset" : 216,
      "endOffset" : 219
    }, {
      "referenceID" : 6,
      "context" : "As it is well known, the optimal choice is the Bayes classifier [7].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Instead, following Vapnik’s general setting of the empirical risk minimization [7, 24], we focus on a more realistic goal: Given a set of training data D = {(x1, y1), · · · , (xn, yn)} independently drawn from p(X,Y ), we consider finding f(x) in a function classH that minimizes the empirical classification error,",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "Instead, following Vapnik’s general setting of the empirical risk minimization [7, 24], we focus on a more realistic goal: Given a set of training data D = {(x1, y1), · · · , (xn, yn)} independently drawn from p(X,Y ), we consider finding f(x) in a function classH that minimizes the empirical classification error,",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "Under certain conditions, direct empirical classification error minimization is consistent [24] and under low noise situations it has a fast convergence rate [15, 23].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Under certain conditions, direct empirical classification error minimization is consistent [24] and under low noise situations it has a fast convergence rate [15, 23].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "Under certain conditions, direct empirical classification error minimization is consistent [24] and under low noise situations it has a fast convergence rate [15, 23].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "However, due to the nonconvexity, nondifferentiability and discontinuity of the classification error function, the minimization of (1) is typically NP-hard for general linear models [13].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : "The problem of minimizing the empirical surrogate loss turns out to be a convex programming problem with considerable computational advantages and learned classifiers remain consistent to Bayes classifier [1, 20, 28, 29], however clearly there is a mismatch between “desired” loss function used in inference and “training” loss function during the training process [16].",
      "startOffset" : 205,
      "endOffset" : 220
    }, {
      "referenceID" : 19,
      "context" : "The problem of minimizing the empirical surrogate loss turns out to be a convex programming problem with considerable computational advantages and learned classifiers remain consistent to Bayes classifier [1, 20, 28, 29], however clearly there is a mismatch between “desired” loss function used in inference and “training” loss function during the training process [16].",
      "startOffset" : 205,
      "endOffset" : 220
    }, {
      "referenceID" : 27,
      "context" : "The problem of minimizing the empirical surrogate loss turns out to be a convex programming problem with considerable computational advantages and learned classifiers remain consistent to Bayes classifier [1, 20, 28, 29], however clearly there is a mismatch between “desired” loss function used in inference and “training” loss function during the training process [16].",
      "startOffset" : 205,
      "endOffset" : 220
    }, {
      "referenceID" : 28,
      "context" : "The problem of minimizing the empirical surrogate loss turns out to be a convex programming problem with considerable computational advantages and learned classifiers remain consistent to Bayes classifier [1, 20, 28, 29], however clearly there is a mismatch between “desired” loss function used in inference and “training” loss function during the training process [16].",
      "startOffset" : 205,
      "endOffset" : 220
    }, {
      "referenceID" : 15,
      "context" : "The problem of minimizing the empirical surrogate loss turns out to be a convex programming problem with considerable computational advantages and learned classifiers remain consistent to Bayes classifier [1, 20, 28, 29], however clearly there is a mismatch between “desired” loss function used in inference and “training” loss function during the training process [16].",
      "startOffset" : 365,
      "endOffset" : 369
    }, {
      "referenceID" : 13,
      "context" : "Moreover, it has been shown that all boosting algorithms based on convex functions are susceptible to random classification noise [14].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "” A remarkably rich theory and a record of empirical success [18] have evolved around boosting, nevertheless it is still not clear how to best exploit what is known about how boosting operates, even for binary classification.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "In Section 3, we show experimental results on a collection of machine-learning benchmark data sets for DirectBoost, AdaBoost [9], LogitBoost [11], LPBoost with column generation [6] and BrownBoost [10], and discuss our findings.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "In Section 3, we show experimental results on a collection of machine-learning benchmark data sets for DirectBoost, AdaBoost [9], LogitBoost [11], LPBoost with column generation [6] and BrownBoost [10], and discuss our findings.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "In Section 3, we show experimental results on a collection of machine-learning benchmark data sets for DirectBoost, AdaBoost [9], LogitBoost [11], LPBoost with column generation [6] and BrownBoost [10], and discuss our findings.",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 9,
      "context" : "In Section 3, we show experimental results on a collection of machine-learning benchmark data sets for DirectBoost, AdaBoost [9], LogitBoost [11], LPBoost with column generation [6] and BrownBoost [10], and discuss our findings.",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 26,
      "context" : "Due to space limitation, the proofs of theorems, related works, technical details as well as conclustions and future works are given in the full version of this paper [27].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 21,
      "context" : "Algorithm 1 is a heuristic procedure that minimizes 0-1 loss, it is not guaranteed to find the global minimum, it may trap to a coordinatewise local minimum [22] of 0-1 loss.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 16,
      "context" : "2 Maximizing Margins The margins theory [17] provides an insightful analysis for the success of AdaBoost where the authors proved that the generalization error of any ensemble classifiers is bounded in terms of the",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "It is equal to the weighted fraction of base classifiers voting for the correct label minus the weighted fraction voting for the incorrect label [17].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "This method was first proposed by [3] for the assignment problem, and was extended to the linear cost network flow problem and strictly convex costs and linear constraints [4, 21].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "This method was first proposed by [3] for the assignment problem, and was extended to the linear cost network flow problem and strictly convex costs and linear constraints [4, 21].",
      "startOffset" : 172,
      "endOffset" : 179
    }, {
      "referenceID" : 20,
      "context" : "This method was first proposed by [3] for the assignment problem, and was extended to the linear cost network flow problem and strictly convex costs and linear constraints [4, 21].",
      "startOffset" : 172,
      "endOffset" : 179
    }, {
      "referenceID" : 7,
      "context" : "1 Experiments on UCI data We first compare DirectBoost with AdaBoost, LogitBoost, soft margin LPBoost and BrownBoost on 10 UCI data sets1 from the UCI Machine Learning Repository [8].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 18,
      "context" : "LPBoost maximizes the soft margin subject to linear constraints, its objective is equivalent to DirectBoost with maximizing the average margin of bottom n samples [19], thus we set the same candidate parameters n/n = {0.",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : "For LPBoost, the termination rule we use is same to the one in [6], and we select the optimal regularization parameter by the validation set.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 18,
      "context" : "DirectBoostavg and LPBoost are both designed to maximize the average margin over bottom n samples [19], but as shown by the left figure in Figure 3, DirectBoostavg generates a larger margin value than LPBoost when decision trees with depth greater than 1 are used as weak learners, this may explain why DirectBoostavg outperforms LPBoost.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "First, we run the above algorithms on a synthetic example created by [14].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 24,
      "context" : "For LPBoost and its variations [25, 26], they do not satisfy the preconditions of the theorem presented by [14], but Glocer [12] showed experimentally that these soft margin boosting methods have the same problem as the AdaBoost and LogitBoost to handle random noise.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 25,
      "context" : "For LPBoost and its variations [25, 26], they do not satisfy the preconditions of the theorem presented by [14], but Glocer [12] showed experimentally that these soft margin boosting methods have the same problem as the AdaBoost and LogitBoost to handle random noise.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 13,
      "context" : "For LPBoost and its variations [25, 26], they do not satisfy the preconditions of the theorem presented by [14], but Glocer [12] showed experimentally that these soft margin boosting methods have the same problem as the AdaBoost and LogitBoost to handle random noise.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "For LPBoost and its variations [25, 26], they do not satisfy the preconditions of the theorem presented by [14], but Glocer [12] showed experimentally that these soft margin boosting methods have the same problem as the AdaBoost and LogitBoost to handle random noise.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 13,
      "context" : "We repeat the synthetic learning problem with binary-valued weak classifiers that is described in [14].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "This result is not surprising at all since [14] designed this example on purpose to explain the inadequacy of convex optimization methods.",
      "startOffset" : 43,
      "endOffset" : 47
    } ],
    "year" : 2013,
    "abstractText" : "We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classifier of weak classifiers through directly minimizing empirical classification error over labeled training examples; once the training classification error is reduced to a local coordinatewise minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classifiers to maximize any targeted arbitrarily defined margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an nth order bottom sample margin.",
    "creator" : null
  }
}