{"title": "Policy Shaping: Integrating Human Feedback with Reinforcement Learning", "abstract": "A long term goal of Interactive Reinforcement Learning is to incorporate non-expert human feedback to solve complex tasks. State-of-the-art methods have approached this problem by mapping human information to reward and value signals to indicate preferences and then iterating over them to compute the necessary control policy. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it as direct labels on the policy. We compare Advise to state-of-the-art approaches and highlight scenarios where it outperforms them and importantly is robust to infrequent and inconsistent human feedback.", "id": "e034fb6b66aacc1d48f445ddfb08da98", "authors": ["Shane Griffith", "Kaushik Subramanian", "Jonathan Scholz", "Charles L. Isbell", "Andrea L. Thomaz"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper presents an approach to interactive reinforcement learning there human feddback is interpreted directly as policy labels. \nThe paper is clearly written and easy to understand. The method is sound and based (to my understanding) well on the existing literature. \nIn my opinion the papers strongest point is that the method presented (named Advise) is simple, needs less meta parameters than state of the art methods and this single meta parameter C (that depicts the estimated consistency of the human feedback) is also not very sensitiv. In combination with the results that show that Advise performs better or equal to the state of the art approaches, Advise seems to me to be an very interesting method. \n\nBut the paper has also some weaknesses, especially for a NIPS submission: \nThe examples that were used as benchmarks seem too easy. \nAlso the theoretical delta of the method to the state of the art is not very large. \n\nBecause the idea is interesting and the method itself is compelling I still tend, however, slightly to suggesting acceptance of the paper. \n\nThere are also some minor points: \nPage 1, line 32 or 33 (the numbering is a bit off in the PDF): \"In this paper WE introduce...\" \nPage 2, line 75 or 76: \"This is THE most common...\" \nPage 5, Table 1: This table is in my opinion too small. \nPages 6-8, Figures 2-5: This figures are definitively too small (at least in the printout). I know its hard to meet the page-limit in NIPS, but the ticks are not readable and the plots themselves are too close on top of each other. \nPage 7, Line 373 or 374: \"interpret feedback is as a direction\" - please rephrase. The paper presents an interesting method for interactive reinforcement learning that is simpler, with less meta parameters by showing equal or better performance than state of the art methods.It lacks however involved theoretical innovation and demonstrates the performance only on two simple benchmarks.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a new method for using human feedback to improve a reinforcement-learning agent. The novelty of the approach is to transform human feedback into a potentially inconsistent estimate on the optimality of an action, instead of a reward as is often the case. The resultant algorithm outperforms the previous state of the art in a pair of toy domains. I thought this was an excellent paper, which appropriately motivated the problem, clearly introduced a new idea and then compared performance to other state-of-the-art algorithms (and not just strawmen). I mostly have suggestions for improvement. \n\n- I really liked the use of simulated human teacher, which could be manipulated systematically to change likelihood and consistency of feedback. One thing I would have liked to see is much lower likelihoods of feedback (< 1%) \n\n- Something that worries me is that people may be systematically inconsistent in their feedback. In psychology, one of the most common uses of reward shaping is in the process of training a new behaviour through successive approximation. That is, let\u2019s say you want a rat to pull a chain. First, you would reward the rat for getting near the chain, then for touching it, and finally for pulling on it. At each step, you deliberately stop rewarding the earlier step. How would Advise deal with this type of systematic inconsistency in the human feedback (which is the type of feedback they might get from an expert human trainer)? \n\n- Sec 4.2: I found the assumption that humans only know one optimal action to be a bit too strong. What happens to the algorithm if that assumption is relaxed? Is performance compromised if the human teacher vacillates between shaping two different optimal actions? Maybe it should? A few words on this issue would be nice. \n\n- One other issue that arises in working with human feedback is delay. Much inconsistency may simple be due to people not responding at the same rate each time\u2014i.e., giving positive feedback only after another intervening action. I think this might actually be another reason that the Advise approach (which allows for inconsistency) is stronger than the other alternatives considered. \n\nMinor things: \n\nline 053: \u201cfrom MDP reward\u201d is an awkward construction \nsec 5.1. How do you win in Pac-Man? Eat both dots? Not specified. \nTable 1 (and figures). Second column would be clearer as \u201cReduced Frequency\u201d instead of \u201cReduced Feedback\u201d. Also, the ordering of conditions (from left to right) is different in Table 1 than the subsequent figures. \nlines 234-247. The relation between control sharing and action biasing could be made a little clearer. \nlines 294. prior estimate of the (missing of) \nFigure 2. Other than the ideal case, why choose to plot only those cases where Advise does not help (if I am reading Table 1 correctly)? \nline 369-370. More accurately, it\u2019s probably best to take the closest overestimate of C (i.e., err upward). \nFigures 4 and 5: The text in the figures (esp the axis labels) was way too small.  This paper presents a new method for using human feedback to improve a reinforcement-learning agent. The approach was novel, and the experiments nicely showed improvement performance against other state-of-the-art approaches.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes a Bayes-optimal approach for integrating human feedback with reinforcement learning. The method extends and is compared to the baseline of Bayesian Q-learning. The approach is tested experimentally on two domains (Pac-Man and Frogger). \n\n\nQuality \n------- \nOverall, the paper is sound and logical. However, there are a few problems: \n\n1) The authors claim that their approach does not convert the feedback to rewards or values. But, by calculating delta_{s,a} from the count of the labels (right/wrong) they essentially convert the labels to values. \n\n2) The name of the proposed approach (policy shaping) is a bit misleading. In fact, the feedback is given per action, not per whole policy/episode during the learning. Therefore, a more appropriate name would have been, maybe, \"action shaping\". \n\n\nClarity \n------- \nThe paper is generally well written and flows logically. There are a few parts, though, that are a bit confusing: \n\n1) Section 4.2. In it, the authors at first state the assumption that the optimality of an action a in a given state s is independent of the labels provided to other actions in the same state. This leads to formula (1). However, the following formula (2) violates this assumption by relying on the values delta_{s,j} from other actions in the same state. \n\n2) The first time the authors clearly state how the human feedback is given (as binary right/wrong labels immediately following an action) comes too late in the text (around line 112, on page 3). It should have been much earlier in the text. \n\n3) Section 5.3. It is not entirely clear to me how the pre-recorded human demonstrations are used to produce a simulated oracle. \n\n\nOriginality \n----------- \nUnfortunately, some of the most interesting problems are left for future work (e.g. the credit assignment problem, mentioned on line 125, as well as the case when there is more than one optimal action per state). \n\nThe proposed method for resolution of the multiple sources does not seem to be elaborate enough. By multiplying the two probabilities, both of them are taken into account with equal weight, even if one of them is less reliable than the other. A better approach would have been to use the value of C to evaluate the reliability of the human feedback and take this into consideration. \n\n\nSignificance \n------------ \nIn my opinion, the demonstrated improvement by using the additional human feedback is not sufficiently significant to justify the large amount of additional information needed by the algorithm. In fact, if the \"oracle\" information is directly provided to the agent in the form of \"demonstrations\", the agent would be able to \"jump-start\" the learning from a very high-return initial policy, and further improve it during the episodes. \n\n\n The paper proposes a Bayes-optimal method for inclusion of binary right/wrong action labels provided by human into reinforcement learning. The paper is well written, but could be further improved in terms of clarity, originality and significance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
