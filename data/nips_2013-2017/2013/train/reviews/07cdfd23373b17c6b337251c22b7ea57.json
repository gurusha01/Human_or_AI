{"title": "A Scalable Approach to Probabilistic Latent Space Inference of Large-Scale Networks", "abstract": "We propose a scalable approach for making inference about latent spaces of large networks. With a succinct representation of networks as a bag of triangular motifs, a parsimonious statistical model, and an efficient stochastic variational inference algorithm, we are able to analyze real networks with over a million vertices and hundreds of latent roles on a single machine in a matter of hours, a setting that is out of reach for many existing methods. When compared to the state-of-the-art probabilistic approaches, our method is several orders of magnitude faster, with competitive or improved accuracy for latent space recovery and link prediction.", "id": "07cdfd23373b17c6b337251c22b7ea57", "authors": ["Junming Yin", "Qirong Ho", "Eric P. Xing"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper proposes parsimonious triangular model (PTM), which constrains the O(K^3) parameter space of mixed-membership triangular model (MMTM) to O(K) for faster inference. Authors develop a stochastic variational inference algorithm for PTM and additional approximation tricks to make it further scalable. It is shown from synthetic dataset that the reduction of the number of variables may lead to stronger statistical power, and from real-world datasets that the proposed method is competitive with existing methods in terms of accuracy. \n\nQuality: \n\nPTM seems to be an interesting specialization of MMTM, but it is questionable what is the practical advantage of achieving good scalability in terms of K (the number of possible roles). To empirically evaluate the value of such a method, it is critical for us to answer \"how does it help if we can learn MMTM with large K?\" Since MMSB and MMTM are mixed-membership models, using small K may not be as troublesome as it is in single-membership models! For real network experiments, what would be really interesting is to see the performance of both MMTM and PTM as a function of K: if we can really see that PTM with large K outperforms MMTM with small K, it will show the power of PTM. Unfortunately such experiments were not done. Actually I could not find good description about how K's were chosen in real data experiments for both PTM and its competitor MMSB; the method of model selection could've biased experimental results. \n\nAuthors are also a bit reticent on what are the impact of simplifying assumptions they make. It is clear that we can achieve good scalability by constraining the parameter space, but it would be also nice to hear about when would such model fail. If there really are O(K^3) distinct values in the parameter matrix B (following authors' notation) of MMSB and MMTM, PTM should perform poorly, right? In this regard, it is unsatisfactory that the choice of true B in synthetic data experiments were not clearly described; authors' should've experimented with various B's which some are in favor of PTM and some are against the model. It would be more beneficial for readers to know when would PTM succeed and when would it fail, than to know only when it would succeed. \n\nOn the other hand, authors introduce a number of tricks to make PTM scale; while I find them to be very interesting and effective, this paper does a poor job in describing and analyzing the impact of such an approximation. 1) What is the impact of choosing the threshold \\delta? On real-world and synthetic data experiments, does it help to choose larger or smaller \\delta? How much can we lower \\delta to improve scalability while not sacrificing much of accuracy? 2) Authors claim that a-MMSB does not work since downsampling compromises accuracy; why would PTM free of such problem? MMTM and PTM are actually ignoring triplets with only one edge. Why can't we ignore non-edges in MMSB while we can in PTM? 3) What is the impact of O(K) approximation to local update (in page 5)? What happens if we sample 2K, 4K, or 100K of triples? Would the quality of approximation suffer when K gets larger? \n\nTo sum up, I am not convinced experimental setup was fair to competitors, and the description of potential weakness of the algorithm/model is poor. \n\nClarity: the paper is very clearly written and well-organized. \n\nOriginality: In terms of modeling it is a bit incremental, but achieving scalability via constraining parameters is an interesting and original direction to pursue, as recently in this topic a lot of work has been focused on generalizing existing models. \n\nSignificance: Provided simplifying assumptions in the model and the inference algorithm do hold in practice, it would have a good impact on the practice of latent-space models on networks, as not many of them actually scale to large data. \n\nAdditional Comments: Personally I think the title is too broad. There are other scalable approaches to other latent space models, and authors propose a scalable algorithm to _a_ latent space model- PTM. \n\nAlso, I am not sure that PTM is really a probabilistic method. Authors argue in the last paragraph of Section 3 that this is same in spirit to bag-of-words assumption, but I am not convinced. When LDA generates a bag-of-words, there always exists a document corresponding to it; so it has proper likelihood function, no matter how unrealistically simple it might be. On the other hand, with very high probability there does not exist any network corresponding to a bag-of-triples generated by MMTM and PTM. Therefore MMTM/PTM is not really a generative model for networks, and majority of posterior probability mass is placed on modeling non-networks! It might be dangerous to abandon basic principles of likelihood in \"probabilistic\" methods (what does probability mean in PTM?)... I am curious what authors and other reviewers think. Authors propose modeling assumptions and inference tricks that would make inference on MMTM scalable. Side-effects of such decision, however, are poorly investigated.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a technique for learning latent structure in social network data. The model is fit to triangle count statistics obtained from a preprocessing step, analogous to a bag-of-words representation for text documents. The main modeling advance is motivated by aMMSB: rather than parameterize all between-\"community\" interactions they consider the configurations of the triangles to identify a set of K roles to parameterize. This results in fewer parameters in the model and, in turn, a more scalable implementation. They apply recent advances stochastic variational inference -- deriving a global update and local updates for the proposed model. \n\nThe novel modeling contribution is a straightforward combination of the aMMSB with the MMTM, though somewhat tedious bookkeeping was required. The presentation of this material could be improved. \n\nThough most of the learning algorithm is a direct application of stochastic variational inference, the authors provide an intuitive and intriguing approximation for the local update rule. \n\nGiven the prevalence of directed networks, a detailed discussion of how these methods might extend would be welcome. \n\nOngoing concerns: \n- \"our Parsimonious Triangular Model (PTM) is essentially a latent space counterpart to ERGMs, that represents the processes of generating triangular motifs\". This seems like a curious statement: 1) ERGMs can include much more varied statistics than just triangles and 2) I don't see how the PTM is a \"latent space counterpart\", unless all latent space network models are \"latent space counterparts\". \n- Are the inferred latent space interpretable? If so, what do we learn about the similarities and differences among these graphs? \nThe difference between Delta^1 and Delta^2 is not clear (at least to this reader). \n- It is not made clear (in this paper) why subsampling the triangles for each node (using delta) provides an adequate approximation. \n- If the goal is to recover the \"set of non-zero roles\" then perhaps there should be more of a binary notion of \"role\" in the model. \n- How robust are the latent space recovery results to the threshold of .125 (which seems somewhat arbitrary)? \n- A variety of latent space models can produce power-law networks. Which one did you use for the experiments? \n- Why is PTM CGS have K^3 complexity? Didn't you argue that there are O(K) parameters rather than O(K^3)? \n- One concern is how this will perform when fewer edges are observed. In the experiments they only hold out 10% of the edges, presumably because the distribution of adjacent triangles will be quite biased if too many edges are removed prior to the preprocessing step. \n- It is counterintuitive that the method beats MMSB on a synthetic network generated via MMSB (Figure 1, left 4 panels). \n This work uses stochastic variational inference to learn a mixed-membership blockmodel (with a restricted set of parameters) on the triangle counts in a graph. Their modeling choices and approximations allow for a more scalable O(NK) algorithm.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors develop a scalable approximate inference algorithm for the \n\"triangle model\" of networks. They develop a powerful subclass of the \nmodel that shares parameters in a clever way, and they use stochastic \nvariational inference to infer the posterior with very large networks. \nThe paper is well written, and well exexcuted. I think it is acceptable. \n\np1: What is \"parsimonious\" modeling? I don't think this is a \nrun-of-the-mill expression in the context of probabilistic models. (I \nmight be wrong about that.) I suggest unpacking that term in a \nsentence since it is important to the paper. \n\np1: From my reading of Gopalan et al., down-sampling the zeros does \nnot comprimise accuracy. Your sentence in the introduction is \nmisleading. \n\np4: I'm sympathetic to the triangle model being OK from a pragmatic \nperspective, but I don't think appealing to LDA is fair. If we run the \ngenerative process of LDA, we will obtain something that \"is\" a \ndocument, albeit a silly one, even if it's not the original. Running \nyour model, however, can produce objects that are not networks. Again, \nthis does not bother me---it's a good and practical model---but the \ndefense by means of LDA is not appropriate. \n\np5: You should cite Hoffman et al. with Amari and Sato. In your \nscenario, where there are \"local variables\" (in Hoffman's language), \nSato's algorithm does not apply. The needed theory is in Hoffman. \n\np6: How did you generate power law networks in the latent space? The \npaper is vague around this point. \n\np6: Earlier you cite Gopalan et al. as a way of speeding up MMSB. You \nshould compare to their algorithm, in addition to (or instead of) MMSB \nbatch. \n\nSmall comments \n\np2: mixture-membership -> mixed-membership \n\np8: I suggest removing the last sentence. The second-to-last sentence \nis a stronger way to end the paper. \n\nIn the bibliography, cite the JMLR version of \"Stochastic Variational \nInference\". \n The authors develop a scalable approximate inference algorithm for the\"triangle model\" of networks. They develop a powerful subclass of themodel that shares parameters in a clever way, and they use stochasticvariational inference to infer the posterior with very large networks.The paper is well written, and well exexcuted. I think it is acceptable.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
