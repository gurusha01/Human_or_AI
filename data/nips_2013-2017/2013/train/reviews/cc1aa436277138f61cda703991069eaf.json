{"title": "Scoring Workers in Crowdsourcing: How Many Control Questions are Enough?", "abstract": "We study the problem of estimating continuous quantities, such as prices, probabilities, and point spreads, using a crowdsourcing approach.  A challenging aspect of combining the crowd's answers is that workers' reliabilities and biases are usually unknown and highly diverse.  Control items with known answers can be used to evaluate workers' performance, and hence improve the combined results on the target items with unknown answers.  This raises the problem of how many control items to use when the total number of items each workers can answer is limited: more control items evaluates the workers better, but leaves fewer resources for the target items that are of direct interest, and vice versa. We give theoretical results for this problem under different scenarios, and provide a simple rule of thumb for crowdsourcing practitioners.  As a byproduct, we also provide theoretical analysis of the accuracy of different consensus methods.", "id": "cc1aa436277138f61cda703991069eaf", "authors": ["Qiang Liu", "Alexander T. Ihler", "Mark Steyvers"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The authors consider the natural problem of bounding the number of control questions needed to evaluate workers\u2019 performance in a crowdsourcing task. They posit two methods of leveraging control questions. One is a two-stage estimator that produces two estimates, one for the biases of the workers and another for the bias-neutralized estimate for the true label. For this method they show that to minimize the MSE of the estimator one needs O(sqrt(L)) control questions to each worker where L is the number of labels provided by each worker. \n\nThe other method they consider is a joint-estimator of the biases and true labels. This model turns out to be more complex to analyze and the authors solve the problem by connecting the MSE to the eigenvalues of the adjacency matrix of the bipartite graph connecting items to workers. Here the bound on the number of control items turns out to be O(L/sqrt(n)), where n is the total number of items labeled. Since the number of items given to each worker is <= n this bound is O(sqrt(L)) and is much better than the two-stage estimator in terms of the number of control questions needed as n --> \\infty. The joint-estimator crucially relies on the structure of the assignment graph of items to workers. In particular, the bound mentioned above holds if the assignment graph is an expander. Happily, a random graph is an expander almost surely and a random L-regular assignment scheme works. \n\nThe paper concludes with some realistic experiments which show that the performance follows the theoretical results. I found it interesting (and appreciate) that the authors also investigated what happens if some of the assumptions made in the derivations doesn\u2019t hold and they discover that the joint-estimator method, though better in control question utilization, is not as robust as the two-stage method when the model is mis-specified. The paper is well written and though the proofs are a little too concise for my taste, the reviewer understands that this might be because of the page limits. \n\nMinor typo: In the conclusion section line #371, O(L) should be O(sqrt(L)).  The reviewer feels that this paper contains important and interesting results and recommends the paper for acceptance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper considers various methods of achieving consensus labeling in a crowdsourcing setting, specifically the special case where some real-valued quantity has to be estimated by e.g., averaging estimates from multiple users. \nIf individuals have a fixed bias, and some truth values are available, the bias could be estimated using only the true values, or using all labels provided by the user. \n\nThe paper provides theoretical results under this specific data model for these two schemes, in an effort to estimate how many true values are needed. \n\nThe theoretical work seems solid, and matches up fairly well with empirical data in the simulations. In particular, as one might intuitively expect, the joint estimation scheme is asymptotically much preferred as more questions need to be answered. \nOverall, it's a nice combination of a theoretical contribution and empirical evaluation on a current topic. Many questions remain on the model itself, and perhaps the authors could discuss some of these details in the paper. \n\n\n\nSome general questions on the model: \n1. relevance: are there many such \"real-valued-estimation\" problems that could in fact benefit from crowdsourcing? The authors mention forecasting as a possible application.would this bias- or bias-variance model be empirically appropriate for those settings? \n2. model structure: consider an extremely simplified model where all workers share a bias, i.e., crowd-average is always off-by the same quantity irrespective of teh crowd. Then these estimation schemes are inappropriate/can be vastly simplified. \nhow would such a model work in practice? e.g., the football dataset suggests that a \"variance-only\" model may in fact work out better. \n3. in general, model appropriateness is a challenge - what would the authors suggest for figuring out the appropriate model? a larger control experiment, or other strategies? The paper provides theoretical results and empirical evaluation of two specific models of consensus labeling, addressing the question of how many \"pre-labeled\" items are needed to achieve robust consensus labeling. The paper is a nice combination of theory and evaluation on a current, relevant problem.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper examines the problem of determining what fraction of \ncontrol questions (with known answers) vs. target answers \n(unknown) to use when using crowdsourcing to estimate continuous \nquantities (i.e., not categorical judgments). The authors \ndescribe two models for using control questions to estimate \nworker parameters (bias and variance from true answers); a \ntwo-stage estimator which estimates worker parameters from the \ncontrol items alone, and a joint estimator which comes up with \nan ML estimate using both control and target items. The authors \nderive expressions for the optimal number of target items (k) \nfor each case, beginning with a clear statement of the results \nand then going through detailed but clear derivations. They \nthen show their results empirically on both synthetic and real \ndata, showing how the estimates align with the true optimal \nk in cases where model is a perfect match to the data \nvs. misspecified (for synthetic) and then show how the practical \neffects of misspecification when dealing with real data. They \nclose with valuable recommendations for practitioners in terms \nof choosing between the models. \n\nFirst of all, this is a very important and highly practical \nsetting - as someone who has run many crowdsourced tasks as well \nas read/heard many accounts from others, using control questions \nis a tried and true method of estimating bias and variance in \njudges; much of the past theoretical work has ignored this and \nassumed no such control questions are available. While control \nquestions could be used in these other methods in principle, I \nknow of no previous paper that has examined the *value* of \ncontrol items and how many should be used in a given setting. \n\nI have often wondered about this question in my own experiments, \nand have considered working on the problem myself; as such I was \ndelighted to read this thorough treatment by the authors. This \npaper is excellent in so many ways: it is unusually clear in its \nwriting, from the motivation and setup to the explanation of \ntheir the strategy and purpose of their approach before diving \ninto the derivations, to the setup and explanation of the \nexperiments and their implications. The past literature is \nwell-covered, the figures are clear, the notation and \ndevelopment are easy to follow. The estimation algorithms and \nthe optimal k are clear, and the discussion of the effects of \nmodel mismatch and recommendations for real \nsettings/applications are insightful and valuable. I would \nrecommend this paper not only to my colleagues who work on the \nanalysis of crowdsourcing, but also to many others who are users \nof crowdsourcing: an excllent paper all around, that I expect \nwill be well-regarded at the conference and well-cited in future years. \n This excellent paper examines the relative value of a givennumber/fraction of control items (i.e., with knownanswers) to estimate worker parameters when estimatingcontinuous quantities via crowdsourcing. This is a novel andextremely practical investigation, as control items are widelyused in an ad-hoc manner in practice. The paper is exceedinglyclear and well-structured, and well-supported by carefulexperiments on synthetic and real datasets showing the practicalperformance of the derived estimates.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers the recently popular problem of aggregating low-quality answers collected from crowds to obtain more accurate results. \nThe central question here is how many control examples (whose ground truth labels are known) are required to obtain the most accurate results. \nWith a simple Gaussian model with worker ability parameters, the authors evaluate expected errors for two estimation strategies: two-stage estimation and joint estimation, from which the optimal numbers of control items are derived. \n\nAlthough I found no apparent flaw in the analysis and the experiments support the claims as far as several assumptions hold, the main concern is the assumption of uniform task assignments to workers. \nIn most crowdsourcing situations, the assumption is not so realistic; some workers complete many tasks, but most workers do only a few. \nWhether or not the proposed method is robust to such situations is not evaluated in the experiments since all of the datasets used in the experiments follow the assumption. \n\nIt would be nice if extension to discrete values were discussed. \nAlso, the authors should mention several existing work incorporating control items into statistical quality control such as Tang&Lease(CIR11) and Kajino&Kashima(HCOMP12), \n The problem is interesting, but the assumption of random task assignments might limit the applicability of the proposed method.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
