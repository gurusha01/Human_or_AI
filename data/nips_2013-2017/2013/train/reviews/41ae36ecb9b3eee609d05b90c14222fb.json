{"title": "On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization", "abstract": "Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately.  Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence.  In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear.  Our result is established without any strong convexity assumption on the loss function.  A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm-regularized problem, which may be of independent interest.", "id": "41ae36ecb9b3eee609d05b90c14222fb", "authors": ["Ke Hou", "Zirui Zhou", "Anthony Man-Cho So", "Zhi-Quan Luo"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "******** Added after authors' feedback ********* \nPlease include a simulation where the loss is not strongly convex but PG algorithm converges linearly (for different levels of stability), and importantly discuss when the theory fails. Please also discuss on how this can be extended to the analysis of ADMM. \n\n************************************************* \n\nThis paper proves the linear convergence of the proximal gradient method applied to trace-norm regularized learning problem when the loss function has the form of f(X)=h(A(X)), where A is linear and h is strongly convex on any compact set and has Lipschitz gradient. \n\nThis paper is an extension of Tseng [20], Tseng and Yun \"A coordinate gradient descent method for nonsmooth separable minimization\" and Zhang et al. [22], which established the same result using the \"error-bound condition\" for lasso and group lasso, to the trace norm. This is a non-trivial extension but the contribution seems purely technical. \n\nThe presentation of the proofs is mostly clear. \n\nStrength: \n- Shows the linear convergence of the proximal gradient algorithm extending the result of Tseng et al. \nWeakness: \n- The contribution is purely technical. \n- I would like to see a numerical example showing the linear convergence. \n\n\n\nMore details: \n1. The outlines of the proofs of Theorem 3.1 and Lemma 3.2 seem very similar to those of Tseng. The authors should refer to the original work more precisely to make their contribution clearer. \n2. I would like to see a numerical example that indeed shows the linear convergence (a semilog plot of function value vs. the number of iterations). \n3. It is not clear how the constants \\kappa_1, ..., \\kappa_4 depend on the choice of \\underbar{\\alpha} and \\bar{\\alpha}. \n\nMinor issue: \n4. The sequence of inequalities before inequality (13) is confusing. The last inequality follows due to the convexity of the trace norm and the fact that -\\bar{G}\\in \\tau\\partial\\|X\\|_{\\ast} and not because of the intermediate inequality (see p289 in [20]). \n Extension of previous linear convergence result based on the \"error-bound condition\" from lasso and group lasso to the trace norm.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proves linear convergence of the standard proximal gradient method for trace norm regularized problems, under weaker assumptions than strong convexity of the loss function, which is a crucial problem for many ML and signal processing applications. \n\nThe paper is carefully and clearly written. It closely follows the Lipschizian error bound technique of Tseng [20], whose application to the nuclear norm case was still left as an open question. The main part of the proof seems correct as up to my understanding. \n\n* What I miss in this paper is a clear comparison/analogy to the vector case of l1-norm regularization, as e.g. in [20,22] and related work. \nIt would be very helpful to relate to the results and techniques used in that case, and to explain if/why the l1-case techniques do apparently not directly translate to trace norm. (Also now the l1-norm case should follow from the trace norm one as a corollary). \n\nAs for judging the value of the paper, this should be taken into account, as some people might find it not extremely surprising that the l1-result also holds for trace norm (I still find it a very valuable contribution though). \n\n* Independent of that I would suggest a slight change of the title of the paper, as the main new contribution affects the loss part more than making the trace norm regularization a *structured* one. (Since the newly gained flexibility by the linear operator A is on the loss side, not in the regularizer). \n\n*** Minor issues \n- Notation: Calling P a sampling or mask operator? (sampling might hint at randomness, but here P is fixed) \n-l337: Broken reference \n- The meanings of \"Q-\" and \"R-\" linear convergence are maybe not that widely known, and could be repeated for clarity. \n- the residual R(X) defined in (6) could be additionally explained in relation to the original definition (46) by [20] for better interpretation. \n\nIn general, I'd vote to include a bit more discussion and maybe conclusions in favour of the long technical part of the proof of Lemma 3.2 in Section 3. \n\n*** Update after author feedback: \nThanks for answering most questions. As for the title, i was referring to the word *structured*, which i found unnecessary in the context here, but of course i leave that to the authors. The paper proves linear convergence of the standard proximal gradient method for trace norm regularized problems, under weaker assumptions than strong convexity of the loss function, which is a crucial problem for many ML and signal processing applications.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "--------added after authors' rebuttal-------- \nThe proof for |R_k| <= kappa |X_k - X_{k+1}| has been explicitly established in \n\"Scalable nonconvex inexact proximal splitting\", Suvrit Sra, NIPS'12. \n(Lemma 4) \n--------end-------- \n\nFor trace norm regularized risk minimization, a popular method is the proximal gradient algorithm. The sublinear global convergence rate of this algorithm is well-understood while it is also known that the rate is linear if the loss is additionally strongly convex. This paper relaxes the strong convexity assumption by following the framework established by Luo and Tseng. In particular, the authors proved a local error bound that allows them to show that the proximal gradient converges asymptotically at a linear rate for trace norm regularization. The paper more or less follows the usual pattern to prove the local error bound, with a new observation that allows simultaneous diagonalization of the matrices. \n\nQuality: \nThis paper appears to be technically sound. The only gap I had in mind is in the appendix, line 085. Please explain why such a choice of kappa_3 (say alpha < 1) leads to the claimed inequality R(X^k) <= kappa_3 |X^k - X^{k+1}|_F ? \n\nClarity: \nI could follow the paper without much difficulty. Although due to the technical nature, it might be a good idea to lay out the main claims first and then go to details one by one. Particularly, after stating the local error bound, the authors might want to postpone the very technical proof of the local error bound but turn immediately to prove the linear rate (which by the way, has hardly anything new in it). \n\nOriginality: \nThe originality of this paper is moderate. The observation that allows the authors to simultaneously diagonalize matrices seems to be new and interesting; other than that, the authors more or less follow the existing approach to establish the local error bound. The rest steps are not new. \n\nSignificance: \nThe significance of this paper is mainly in theory, with little impact on algorithm design though. While it is certainly great to extend the linear convergence rate of the proximal gradient algorithm to more practical scenarios (i.e., relaxed form of strong convexity), the significance is nevertheless lowered by the asymptotic nature of the proof (and claim). The way the authors handle the diagonalization of matrices might be of some independent interest. This work proved that the proximal gradient algorithm, when applied to trace norm regularized risk minimization, converges asymptotically at a linear rate (under a relaxed form of strong convexity). The authors mostly follow a well-established framework due to Luo and Tseng, with a new observation that allows them to (simultaneously) diagonalize matrices hence (more or less) reduce to the vector case. Overall, this work is incremental and is moderately interesting in the theoretical aspect.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
