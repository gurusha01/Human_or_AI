{"title": "Compressive Feature Learning", "abstract": "", "id": "5e1b18c4c6a6d31695acbae3fd70ecc6", "authors": ["Hristo S. Paskov", "Robert West", "John C. Mitchell", "Trevor Hastie"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The method consists of compressing documents for accelerating the \ndocument processing in other tasks, such as classification. The \ncoding scheme is related to the one used in the Lempel-Ziv algorithm, \nstoring pointers of substrings appearing at several locations in the \ndocument. The proposed approach is formulated as a combinatorial \noptimization problem, whose solution is approximated by a sequence \nof convex problems solved by ADMM. \nThe experiments are carried on text classification problems, where \ncompressing the documents leads to some gains in memory and \ncomputational efficiency, at a minor loss in terms of precision. \n\nI found the approach interesting, even though I am not familiar enough \nwith the NLP literature to exactly judge the novelty of the approach. \nI have only a few minor remarks to make \n- the sentence ``an optimal lossless compression of D...'' requires some \nclarifiation. Is the coding scheme optimal in terms of minimum entropy? \n- it is not clear that the reweighting scheme can be interpreted here as \na majorization-minimization procedure. Is it really the case here? \n- minimizing (4) with respect to w amounts to computing the Fenchel conjugate \nof the (weighted) l_infty-norm, which involves a projection on a weighted \nl1-norm (the dual norm of the l_infty-norm). When adding the non-negativity \nconstraint, this involves a projection on the simplex. Algorithms for \nprojecting on the simplex have been known for a while, and are similar to \nthe approach described in the paper. See \nBrucker. An O(n) algorithm for quadratic knapsack problems. 1984. \nsee also \nBach et al. Optimization with Sparsity-Inducing Penalties. 2012, \nfor the computation of Fenchel conjugates for norms. \n The paper proposes a way to compress documents to accelerate the document processing in various tasks. It seems that the approach is novel and performs well.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors proposed to compress the features by optimizing a formulation that takes into consideration both the pointer cost and the dictionary cost. The non-convex formulation is relaxed by replacing the binary constraints with box constraints in the interval of 0 and 1, and ADMM is applied for solving the relaxed problem. Experimental results were reported in comparison with several approaches. \n\nThe proposed compressive feature learning utilizes a losses compression formulation that considers both the pointer cost and the dictionary cost. The resulting problem is non-convex due to the binary constraints. The proposed formulation seems new. \n\nIn solving the relaxation, ADMM is adopted. The proposed approach requires solving a series of relaxed problems that are dependent on the parameter d. These series of solutions adopt a reweighting scheme. How about the efficiency of the proposed compression approach? For subsequent classification, it was reported that the classification speed can be improved with the proposed approach, which is reasonable. \n\nAfter the features are extracted, the elastic net method is used for classification. The elastic net method has two tuning parameters, how are these two parameters tuned via cross validation? Two-dimensional grid search? \n\nAfter reading other reviewers' comments and the author response, the reviewer would like to keep the original recommendation. The authors proposed to compress the features by optimizing a formulation that takes into consideration both the pointer cost and the dictionary cost. The non-convex formulation is relaxed by replacing the binary constraints with box constraints in the interval of 0 and 1, and ADMM is applied for solving the relaxed problem.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper formulates the problem of selecting a subset of informative n-grams for document classfication as a lossless compression problem solved by iterative relaxation of he original hard combinatorial problem. While unsupervised feature learning as compression is not a new idea, this particular formulation is interesting, seems novel, and performs fairly well in small-scale experiments. The paper is very well written, the ideas are clear and reasonably motivated. The algorithm presentation, while not fully self-contained (there's a substantial supplement), is understandable as given. I would have liked more analysis of the algorithm's computational properties, or at least some experiments on computational cost-accuracy tradeoffs to help understand the scalability of the method (including the claims of parallelizability). Still in the experimental side, I'd also liked to see comparisons with popular lossy representation methods such as embeddings (Bengio et all 2006, Collobert and Weston 2008, Mikolov et al 2010, inter alia). And I'd like to see the tradeoffs between model size and accuracy obtained with this method compared with sparsifying regularization over the uncompressed n-gram features.  Formalizes n-gram feature selection for document classification as lossless compression, with an efficient relaxation algorithm for the original hard combinatorial problem. Elegant and well-motivated approach, with basic positive results but in some need of more analysis and experimentation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
