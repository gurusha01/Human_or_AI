{"title": "Which Space Partitioning Tree to Use for Search?", "abstract": "We consider the task of nearest-neighbor search with the class of binary-space-partitioning trees, which includes kd-trees, principal axis trees and random projection trees, and try to rigorously answer the question which tree to use for nearest-neighbor search?'' To this end, we present the theoretical results which imply that trees with better vector quantization performance have better search performance guarantees. We also explore another factor affecting the search performance -- margins of the partitions in these trees. We demonstrate, both theoretically and empirically, that large margin partitions can improve the search performance of a space-partitioning tree. \"", "id": "a01a0380ca3c61428c26a231f0e49a09", "authors": ["Parikshit Ram", "Alexander Gray"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper presents bounds on the search performance of a simple, \ntree-based nearest neighbor search algorithm. The bounds depend on the \nvector quantization performance on the tree. It is argued that this \nresult implies that trees with good vector quantization performance \nare advantageous for nearest neighbor search. The statement is extended \nto large margin splits. \n\nThe title of the paper asks \"which space partitioning tree to use for \nsearch\"? It should better ask \"which tree results in the strongest \nperformance guarantees\"? The paper says almost nothing about practical \nperformance. This is mostly due to the choice of an artificially \nsimplified search procedure. More often than not, a better guarantee is \nan artifact of a certain flavor of analysis or a proof technique, since \nwe are only talking about upper bounds. If the bounds are not tight \nthen the bounds say little about which tree to \"use\" (in practice!). \nThis paper makes the common mistake of confusing a better performance \nguarantee with a guarantee for better performance. This happens at \nseveral spots, e.g., first sentence of section 4. \n\nAlgorithm 1 is analyzed in depth. However, I am unsure how relevant \nthis algorithm is. It descends the tree without backtracking. At the \ntarget depth l it performs exhaustive search. Although this is not \ntaken into account in the analysis, the final search can be performed \nwith efficient exact tree search, so, this time with backtracking. \n\nThis algorithm does not find the exact nearest neighbor. The obvious \nalternative is to apply a heuristic for skipping some of the branches \non the fly. The decisive difference is that in Algorithm 1 the decision \nwhich branches to traverse is not made in a data dependent manner, but \ninstead based on a pre-specified parameter. This is why personally \nI would never use this algorithm. Since all results are restricted to \nthis algorithm I question the relevance of this paper. \n\nI see that analyzing the computational complexity of a method with \nbacktracking is surely much harder. I argue that doing so would be a \nprerequisite for understanding the behavior of realistic algorithms. \nI cannot get rid of the impression that this analysis was conducted \nsimply because it is possible, and not because it is relevant. \n\nThe logic of the conclusion is as follows: \nAlgorithm 1: good VQ performance => good search performance. \nNow Algorithm 1 is not a good search algorithm by itself. When using \nmore elaborate tree search procedures there remains little to nothing \nto conclude from the present analysis. However, the title as well as \nother statements in the paper (e.g., top of page 5) indicate that the \nconclusion is rather general. I want to emphasize that this is not the \ncase, and thus this paper does NOT answer the question which search \ntree to use for search in practice. \n\nI would appreciate the result if it could help the analysis of more \nrealistic search procedures. However, I am not an expert on the \nanalysis of tree search and thus I cannot judge the present paper from \nthis perspective. Also, this paper does not claim to provide new \nmethods for analysis, it is all about the theorems. And this makes me \nquestion its value. \n\nThe empirical evaluation is weak. Only four data sets are used, and \nthey are even non-standard. E.g., why is MNIST sub-sampled to 6000 \ntraining and 1000 test samples? This is arbitrary, an no reason is \ngiven. This does not help my trust in the evaluation. With low numbers \nof training points there is no real need for tree search at all. \nI see that the empirical results are nicely in line with the analysis. \nHowever, how about computation time? E.g., a single decision in a \nkd-tree is cheaper than in a PA-tree by a factor of O(dim(X)). The \nadded computation time could be used for backtracking, which could \nwell give the kd-tree an advantage. So once more, this analysis says \nnothing about which tree to use for search with a better algorithm. \n I don't have trust that this simplified analysis will actually answerthe question posed in the title for practical purposes. This is becausea too much simplified search algorithm is considered. This reduces therelevance of the analysis to nearly zero.I have just read the author feedback. I find my points addressedvery well and convincingly. I have changed my decision accordingly.Thanks for the good feedback!", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Paper Summary: \nThe authors formalize and prove the long standing assumption that partition trees with better quantization rates also have better nearest neighbor search performance. Inspired by this, they propose the \u2018max margin\u2019 partition tree and relate its search performance with the margin. Experiments with some real world data validate that \u2018max margin\u2019 and partition trees with good quantization errors yield good nearest neighbors. \n\nReview: \nI really like that the authors were able to formalize the long standing assumption that partition trees with better quantization rates tend to have better nearest neighbor search performance. The formalism and conditions proposed in Theorem 3.1 are intuitive; and the performance is nicely related to the the \u2018expansion constant\u2019 (a formalism of intrinsic dimension of the input sample). Although the bound provided in Eq. 4 can be made tighter (see below). \n\n\nI do have a few suggestions that authors should consider adding to the current text: \n\n- while using the expansion dimension is intuitive, this results in making a strong assumption like condition C1 to hold over _every_ convex set in the underlying space. I believe (I haven't really checked it) that if the authors consider doubling dimension instead, they dont have to explicitly assume C1 to hold, thus improving the overall readability of the text. (perhaps make a note that bounded doubling dimension, also implies the result?) \n\n- perhaps the authors want to add a short discussion on what happens if the input S comes from an underlying distribution. \n\n- why the notation \u2018\\tilde c\u2019 for expansion constant? consider changing it to \u2018c\u2019. (Theorem 3.1) \n\n- I believe that the bound provided in Eq. 4 can be made tighter. Shouldn't the ratio in lines 531--532 always less than 1? This can significantly improve Eq. 14. \n\n\nQuality: \nThe paper systematically explores the connection between vector quantization and nearest neighbor search, providing both a sound theory and convincing experiments. The bound seems to be looser than expected. \n\nClarity: \nThe paper is written clearly. \n\nOriginality: \nThe work is original. \n\nSignificance: \nI believe this work is very significant as it formalizes the long standing belief that partition trees with better quantization rates tend to have better nearest neighbor search performance. This work can encourage formal analysis of other fast nearest neighbor schemes. \n\n\nUpdate after the author response: \nThanks for the clarifications. In the past week, I came across the following COLT2013 paper that is highly relevant to this submission: \"Randomized Partition Trees for Exact Nearest Neighbor Search\". The authors should update their discussion and compare their results with this paper as well. Authors have done a good job in formalizing the long standing intuition that partition trees with better quantization rates tend to have better nearest neighbor search performance. The authors can do a tighter analysis to improve the bound. Experiments with real data corroborate that the theoretical intuition also works well in practice.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors study space partioning trees with the nearest-neighbor search. More \nspecifically, they study the error caused by dfs search (without backtracking) \nand show connection between the error and quantization error of the tree. \n\nThe authors work is interesting, however, there seems to be some weaknesses. \nAuthors demonstate the connection between the search error and the quantization \nerror by bounding the search error with a (rather complex) bound that has \nparameters derived from the quantization error. However, this connection \nis implied only if the bound is tight, which doesn't seem to be the case: \n\nConsider the expansion coefficient c in in Theorem 3.1. This coefficient \ndepends on q. If we select q to be far enough from S: let's say that that \nmin_{x \\in S} |x - p| is bigger than the distance of any two points in S. This \nimplies that expansion coefficient is n, which forces L in C2 to be 0, making \nTheorem 3.1. unusable for this case. The problem with this one is that Theorem \n3.1. analysies only upstairs of the distance error. I think a more solid result \ncould be obtained is the downstairs term is also taken into account \nsimultaneosly. \n\nThe fact that c depends on q implies that Theorem 3.1 cannot be used in \npractice to obtain the actual error bounds while doing the actual search since \ncomputing expansion coefficient seems to be very computationally expensive. \nTheorem 3.1. can be only used as a theoretical evidence for the connection \nbetween the search error and the quantization error. \n\nLet's make an assumption now that q is actually \"within\" S, that the expansion \ncoefficient doesn't change that much when we add q into S. Consider the \nfollowing case: copy S and transpose the copy from S such that they both copies \nare fully separated. The expansion coefficient should stay about the same, as \nwell as \\omega (since any reasonable tree would first separate the copies from \neach other, giving \\omega = 0, for the first level). While the quantization \nimprovement rate will be excellent on the first level, it will be as bad on the \nnext levels as with original S. Consequently, \\beta \\beta will stay the same. \nAs we move the copies from each other away. \\psi will get larger, and the bound \nwill get loose. I think a better approach would not to consider \\psi and \\beta \nglobally but instead try to work on a \"local level\". \n\nWhile these cases may not happen in practice, I would like to see authors \ndemonstrating how tight is the bound, for example, empirically, similar to \nFigure 2. \n\nTechical writing could be improved. The authors tend to use accents and \nindices in cases where they are not really needed. For example, \n\n\\tilde{c} -> c \n\nB_{l_2} -> B \n\nuse less exotic caligraphy for intrinsic dimenstion d and full dimension D \n\n027: - -> --- \n\nDefinition 2.1. is a bit messy. It would be better to just say that \nquantization error improvement rate is equal to \nbeta = V_S(A) / (V_S(A) - V_S(A_l, A_r)) \n\nC2: what do you mean by complete? \nC4: in Theorem 3.1. are all nodes in T included or only leaves? \n The authors bound the search error of non-backtracking dfs of space-partitioning tree (for NN search) with a quantization error.Interesting paper and while the connection makes sense, I am not convinced that the provided bound is tight, which dilutes the impact of the paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
