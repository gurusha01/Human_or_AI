{"title": "Approximate Dynamic Programming Finally Performs Well in the Game of Tetris", "abstract": "", "id": "7504adad8bb96320eb3afdd4df6e1f60", "authors": ["Victor Gabillon", "Mohammad Ghavamzadeh", "Bruno Scherrer"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper empirically evaluates classification based policy iteration (CBPI) on the Tetris benchmark problem. There are no new technical results; instead the authors analyze the impact of various parameters of the algorithm on its performance. \n\nThe results show that direct policy iteration (CBPI that does not use value functions) performs essentially as well as the cross-entropy method and classification-based policy iteration. In some sense, I do not find this too surprising since the DPI is very similar to the cross-entropy method in the way that policies are represented and optimized. \n\nI do find the analysis and comparison convincing in that the standard Tetris features are not suitable for representing value functions optimized by approximate policy iteration. However, the paper comes short of identifying why this may be the case or how one can design better features. This limits the importance of the results and their implication for other problems. In addition, I would like to see included other algorithms that compute approximate value functions in a different way from policy iteration. As it stands, the results show that good value functions cannot be computed using policy iteration, but that does not mean that other algorithms cannot find a good representation. For example, Smoothed Approximate Linear Programming [Desai, Farias, et al, 2012] has been used to obtain moderately encouraging results on this test domain and should be included in the comparison. The paper presents thorough evaluation of some major algorithms on Tetris, which is an important RL benchmark problem. While the evaluation shows that classification-based policy iteration performs better than DP, the paper lacks additional insights into the reasons for the difference in solution quality which limits the applicability of the results to other (practical) domains.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper studies the application of a classifier-based approximate policy iteration algorithm called CBMPI to the game of Tetris, a much studied benchmark task in reinforcement learning. CBMPI is compared to DPI, a variant which omits the regression step used to estimate the value function in CBMPI; lambda-PI, a related policy iteration method which uses eligibility-trace-like backups and omits the classification step of CPMPI; and CE, a policy-search method that is currently the state of the art in Tetris. Experiments are presented on small boards and large boards using different state features, and the best policies across runs for each method are further evaluated across 10,000 games. \n\nThough this paper makes no theoretical or algorithmic contribution, the empirical analysis is potentially highly significant. Because this benchmark task is well studied, significantly improving the state of the art is a nontrivial endeavor. In addition, since the state-of-the-art CE approach has high sample complexity, progress getting strong performance from more sample-efficient value-function methods is of substantial interest, especially if the results shed light on *why* previous efforts with value-function methods performed poorly. \n\nHowever, I have two main concerns. First, I find the paper's efforts to address the \"why\" unsatisfactory. Second, the experimental comparisons are plagued by confounding factors that substantially undermine the validity of the conclusions drawn from them. \n\nRegarding the \"why\", the authors propose that the success of CE suggests that in Tetris, policies may be easier to represent than value functions. That's a reasonable hypothesis, and it is consistent with the folk wisdom that good policies are sometimes simpler than accurate value functions, and that this can give policy search an advantage. However, the authors then suggest that, if this hypothesis holds, a consequence thereof would be that we should employ ADP methods that search in policy space rather than value function space, and they propose CBMPI as a candidate. However, this is essentially an actor-critic method that explicitly represents both the value function and the policy and thus the difficulties of representing a value function have in no way been circumvented. (This is of course true of all ADP methods because if they didn't explicitly represent the value function they would by definition be policy-search methods). It's true that CBMPI conducts a search in policy space for a classifier-based policy that is consistent with estimated Q-values, and that this is different from many ADP methods, but we are never given any argument why this should make Tetris easier to solve and it does not follow from the hypothesis that Tetris policies are easier to represent than value functions that this would be the case. The superior performance of CBMPI to lambda-PI, which does not explicitly search in policy space, would seem to lend credence to the authors' claim (though without addressing the \"why\") but there are confounding factors in these comparisons (see below) that leave me unconvinced. \n\nRegarding the confounding factors, I have the following concerns about the experiments: \n\n1. In 2.2, we are told that CBMPI depends on rollouts starting from a set of states sampled from a distribution mu. This already raises questions about comparing to CE, because CBMPI requires a generative model whereas a CE requires only a trajectory model. Then, in 3.1, we learn that the set of states are not sampled from a stationary mu but from the trajectories generated by a strong Tetris policy DU which is assumed to be available as prior knowledge. However, in Appendix A, we are told that lambda-PI was deprived of this prior knowledge and instead used a Dirac on the empty board state. In addition, CE was also deprived of this prior knowledge. Though it is less obvious how CE should make use of this initial policy, one could certainly imagining seeding the initial Gaussian distribution on the basis of it. In my opinion, this raises serious questions about the fairness of the comparisons between CBMPI and lambda-PI and CE. \n\n2. In 3.2.1, we are told that in the small-board experiments using the D-T features, CBMPI was optimized for the feature settings and the best performance was obtained using D-T plus the 5 RBF height features. While the paper is not 100% clear on this matter, as far as I can tell, the other methods used only the D-T features, making the comparisons in Figure 4 potentially very unfair. In Figure 5a-c, where all methods use the same Bertsekas features, we see no performance improvement for CBMPI. In Figure 5d, we again see some performance improvement but here again, though the paper is not explicit about it, as far as I can tell CBMPI is using D-T+5 while the other methods are using only D-T. \n\n3. Also regarding 5d: while it's true that CBMPI does better than DPI after 4 iterations, it's also true that DPI does better from 2-4 iterations. It's awkward to argue that final performance is what matters (and therefore CBMPI is better than DPI) given that CE outperforms both in the long run. It's fair to say that CBMPI learns better than CE in a short term, but it's misleading to conclude that CBMPI achieved equal performance with fewer samples, because this is true at only specific points on the learning curve. CBMPI is faster but does not equal CE's final performance. \n\n4. Because a few random events can mean the difference between a game that ends quickly and one that lasts hours, Tetris is notorious for having high variance in scores for a given policy. However, none of the experimental results in this paper include any variance analysis, so we have no way of assessing the statistical significance of the performance differences observed in Figures 4 and 5. I'm particularly concerned about the differences presented in Table 1, since I suspect that, for policies that can clear millions of lines per game, the cross-game variance is probably huge. \n\nMinor comment: Figures 4 and 5 would be much easier to decipher if the x-axis was samples, not iterations, thereby making it directly possible to fairly compare the methods. \n\n\n\n\n\n\n\n This paper does not make a theoretical or algorithmic contribution but presents strong results for value-function methods on Tetris, an important reinforcement-learning benchmark on which to date only policy-search methods have done well. However, the authors' explanation for why this result occurs is unsatisfying and the empirical comparisons are plagued by confounding factors that render the conclusions unconvincing.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper describes the first RL controller in the literature that has comparable performance to black-box optimization techniques in the game of Tetris. The topic is relevant to the RL community, as Tetris has always been a difficult benchmark domain for RL algorithms. \n\nQuality \nThe contribution of the paper is to apply a previously published algorithm (CBMPI) for solving the game of Tetris. The authors present competitive results with the previous state of the art optimization algorithm for Tetris - the Cross-Entropy method (CE). The empirical results are convincing and the boost in performance as compared to previous attempts to apply RL controllers to this domain is remarkable. \n\nThat being said, the paper lacks a detailed discussion regarding the (possible) reasons for this improvement. While the authors mention in the introduction that the key conjecture (that was positively tested by the paper) is that one should use techniques that search in the policy space instead of the value function space, more details would improve the paper. To be concrete, here are two examples of topics: \n\n1. In lines 252-253 the authors describe the state distribution used for sampling rollout states. This state distribution comes from executing a very good Tetris controller. It would be interesting to describe how strongly this choice of distribution influenced the performance of CBMPI. If the authors tried other solutions that didn't work well, reporting it would be useful for anybody trying to replicate the results. For example, it would be interesting to know whether the states CBMPI visits while learning constitute a reasonable rollout distribution. \n\n2. The optimization algorithm used for the \"Classifier\" stage of CBMPI is CMA-ES. As discussed in \"Path Integral Policy Improvement with Covariance Matrix Adaptation\" (ICML 2012) for example, CMA-ES is part of the same family of algorithms as CE. It is thus interesting that an ADP algorithm that uses a CE-type algorithm in its \"inner loop\" becomes comparable to CE as applied to searching directly in the policy space. So one natural question is: what would happen if CMA-ES would be replaced by other types of optimizers (gradient-based for instance). \n\nClarity \nThe paper is well written, easy to follow and the contributions are clearly stated. \nI have several minor comments: \n- The definition of the \"Classifier\" in line 222 is confusing: shouldn't the output be \"labels\" and not differences of Q values? \n- In line 266/267, CMA-ES is named \"classifier\" - but it is optimization algorithm. \n- It's not clear why graphs 4a and 4d are disjoint but 5d contains the performance comparison for all algorithms. \n- Section 3.1 - \"Experimental\" is spelled incorrectly. \n\nOriginality and Significance \nThe paper solves a difficult problem in a novel way and provides interesting insights about how to apply RL algorithms to non-toy domains. It doesn't introduce new algorithmic or theoretical tools. As discussed above, I think the results are significant for the RL community. \n\nFinally, I have several questions: \n1. If the paper is accepted, are the authors willing to share the code for reproducing their experiments? \n2. The variance of scores tends to be high in Tetris so the graphs should be modified to include confidence intervals for the mean values. So, are the differences we see in the graphs (Figure 4D for example) statistically relevant? \n3. Regarding the claim in the sentence in lines 393-395, what happens if CBMPI receives the same number of samples as CE? Will it improve, or is the learning curve already at a plateau so more samples wouldn't help? \n4. Regarding the comparison of the best policies - how did you compute the scores of the policies DU and BDU? Did you take the values from the referred paper ([20]), or did you execute those policies using your code base? I'm wondering what is the performance of the best policy discovered by CE in your experiments, and I wanted to make sure such a result is reported. The paper needs a more detailed discussion of the results to maximize its impact and usefulness to the community. But it is a valuable contribution to the literature that sheds new light on a previously unknown phenomenon: the poor performance of RL algorithms in the game Tetris.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
