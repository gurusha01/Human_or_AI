{"title": "Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC", "abstract": "State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning in nonlinear nonparametric state-space models. We place a Gaussian process prior over the transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. However, to enable efficient inference, we marginalize over the dynamics of the model and instead infer directly the joint smoothing distribution through the use of specially tailored Particle Markov Chain Monte Carlo samplers. Once an approximation of the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. We make use of sparse Gaussian process models to greatly reduce the computational complexity of the approach.", "id": "2dffbc474aa176b6dc957938c15d0c8b", "authors": ["Roger Frigola", "Fredrik Lindsten", "Thomas B. Sch\u00f6n", "Carl Edward Rasmussen"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper presents a Bayesian approach to state and parameter estimation in nonlinear state-space models, while also learning the transition dynamics through the use of a Gaussian process (GP) prior. The inference mechanism is based on particle Markov chain Monte Carlo (PMCMC) with the recently-introduced idea of ancestor sampling. The paper also discusses computational efficiencies to be had with respect to sparsity and low-rank Cholesky updates. \n\nThis is a technically sound and strong paper with clear and accessible presentation. The online marginalisation of the transition dynamics and the use of ancestor sampling to achieve this is novel. The consideration of computational issues such as sparsity and low-rank updates/downdates to the Cholesky factors of covariance matrices strengthens the paper further. The empirical results, while brief, are sufficient (further suggestions below). \n\nIn addition to its stated aims, the paper will likely stimulate discussion around inference methods for non-Markovian state-space models and the potential advantages/disadvantages of learning the transition dynamics in this way rather than specifying a parametric model a priori. \n\nWhile space is slight, the authors may like to consider some further discussion around the differences between using a parametric transition model given a priori against the use of a similar model as the mean function of the GP. For example in out of sample prediction (e.g. forecasting). \n\nThe results in Table 1 and the description in the preceding paragraph are slightly unclear to me. I am unsure as to whether the RMSE is against a withheld set of data points or the same set of data points that is conditioned upon (the *|data in the column headings). My main interest would be an RMSE against an out-of-sample prediction, especially a forecast forward in time against withheld data. It is in this scenario that I would expect to see the largest differences between the learnt dynamics and the ground truth model. If Table 1 is not already showing this, an extra column that does so would be a great addition. \n\nOne minor point: the abbreviation CPF-AS is used in Algorithm 1 before being defined in the first paragraph of Section 3.3.1 below. A strong and novel paper that should stimulate some interesting discussion.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose to apply particle MCMC to perform inference in Gaussian process state-space models. In particular, they focus on the recent ancestral sampling particle Gibbs algorithm of Lindsten et al. The paper is clear and it is an interesting and original application of particle MCMC. There are also some useful model-specific methodology developed in the paper, namely sparse GP-SSM. \n\nOne thing I find truly regrettable is the lack of comparisons to other particle MCMC schemes, in particular the particle marginal MH (PMMH) scheme and the particle Gibbs with backward sampling (as in Whiteley et al.). They could have been straightforwardly implemented and it would be of interest to know how those variants compared to the proposed scheme (and it would not be much work for the authors either). \n\nAdditionally I would like to see graphs displaying the performance of the algorithms (e.g. in terms of ACF or ESS) as a function of N and T. As they stand the results are not very informative. Do I need to scale N linearly with T, sublinearly? \nI believe that for such models the PMMH would require a number of particles increasing quadratically with T as observed in Whiteley et al. whereas both particle Gibbs require a number of particle growing sublinearly with T. \n\n\n\n A well-written application of particle MCMC to GP state-space models. The paper could be significantly improved if the proposed algorithm was compared to the PMMH and the particle Gibbs with backward sampling.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "PMCMC sampling is exploited in an ssm with GP process prior to extend to actual parameters rather than just the usual filtering and smoothing. Nice straightforward application of PMCMC methodology. Pity that a proper evaluation of the challenge tp get the PMCMC scheme to work is not described and evaluated in more detail as this would really have been the important contribution. A more detailed and critical evaluation of the strengths and weaknesses of the approach would have made the paper of value given it is an application of PMCMC methodology. It is probably too much to ask that the experimental section be revised to provide more evaluation than demonstration. Application of PMCMC methodology to ssmwould have been more useful assessing the practicial difficulties in getting such a scheme to work - and how well it actually works", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
