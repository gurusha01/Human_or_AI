{"title": "Projecting Ising Model Parameters for Fast Mixing", "abstract": "Inference in general Ising models is difficult, due to high treewidth making tree-based algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We find that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.", "id": "5a4b25aaed25c2ee1b74de72dc03c14e", "authors": ["Justin Domke", "Xianghang Liu"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "Aims to improve the mixing rate of Gibbs sampling in pairwise Ising models with strong interactions, which are known to be \"slow-mixing\". Several projections to \"fast-mixing\" models are proposed; essentially, parameters are identified which are as close as possible to the original model, but weak enough to satisfy a spectral bound establishing rapid mixing. Experiments show some regimes where this leads to improved marginal estimates for a given computation time, compared to variational methods (mean field and belief propagation variants) and Gibbs sampling in the original model. \n\nThe technical content builds heavily on prior results establishing conditions for rapid mixing [4,8]. But I haven't seen the idea of projecting onto the nearest rapidly mixing model, as an approximate inference method, explored before. \n\nOn the domain of \"toy tiny Ising models\", results offer some nice improvements over a good set of baselines (standard sampling and contemporary variational methods). My main concern is that I think there should be more than the usual amount of skepticism as to whether these results will scale to larger models. Experiments involve comparisons of mixing times of samplers on different models, and it is hard to judge how these will scale with problem size. Also there is a (from all appearances) computationally expensive projection step required to build the fast-mixing model, the cost of which seems not to be accounted for. Finally, it is not at all clear that generalization beyond binary states will be possible, since establishing convergence bounds more generally is far more challenging. \n\nCLARITY: \nIn general the presentation is clear and reasonably accessible, given the technical content. But there are some places where reorganization and clarification is needed: \n* The paper refers several times to \"standard results\" and even states these in the form of a theorem (e.g. Theorem 6) without reference or proof sketch. Lemma 5 is left unproven, the reference proves for a special case of zero-field. \n* Discuss properties of the dependency matrix R. For instance, it does not appear to be tractable due to the maximization, please state whether this is the case. (I presume this is why lemma 5 is invoked.) \n* The second half of Sec. 4 is nearly impossible to follow. Before Theorem 7 the text references g, M, and Lambda before they are introduced. Then the statement of Theorem 4 includes notation that is not really explained. If this optimization is going to be discussed, more explanation is needed. (Perhaps less time could be spent restating results from [8] which are not really used.) \n* In Sec. 1, KL(q||p) is used for both directions of KL divergence when the notation needs to be shifted for one. In the first paragraph, q is used for the true distribution and p for the tractable approximation; this is the opposite of almost all related literature. \n\nEXPERIMENTS: \nIt appears that the time comparison in Figure 2 does *not* include the computation required for projection. A somewhat ambiguous statement to this effect appears in Sec 6.1 but is unclear; please clarify and if it is the case, show results with projection time included. As it stands, the proposed methods essentially get to use the output of a sophisticated variational optimization without penalty, which certainly makes the improvement over standard Gibbs less convincing. \n\nIt was disappointing not to see experiments on larger models, given that Gibbs mixing times often depend on problem size. There are certainly options for running experiments in regimes where junction tree is intractable, like using a model where symmetries let the true marginals be computed, or taking the output of a very long sampling run as truth. \n\nI understand that KL(\\theta||\\psi) is intractable in general, but it would still be interesting to explore here as a potential \"best case\" for how sampling in an approximate model would perform. (Junction tree could be used for the toy models used in the submission.) \n\nMean field is a degenerate case of the reverse KL projection, as the paper points out, yet there is a large difference between mean field error and the error from reverse KL projection. This deserves discussion. \n The idea of approximating slow-mixing models by projecting to the closest fast-mixing model is a nice one, and recent work on mixing bounds is leveraged in an elegant way. But there are some concerns about experimental comparisons, and the limited range of models to which this approach is potentially applicable.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a method for improving the mixing rate of Gibbs sampling in Ising models by projecting the models original parameterization onto a new parameter setting that satisfies the Dobrushin criterion needed for fast mixing. They formulate the projection as a constrained optimization problem, where the constraints are needed to ensure that the new parameters are defined over the same space as the original parameters, and examine this projection under several distance/divergence measures. \n\nIn my opinion, methods that combine principles from stochastic and deterministic inference is an under-explored area and as a result, I think this is an interesting idea. While the idea of augmenting the original parameters to improve mixing time is straightforward, I found the description of the dual of the projection problem to be a little unclear - e.g. how do the z_ij*d_ij =0 constraints ensure that the new parameterization is over the same space (can't it be over a smaller space)? I also was unsure of the overall procedure - do you perform the projected gradient operations to completion and then run a Gibbs sampler, or do you somehow interleave sampling with the gradient updates? An algorithm description box would help to clarify. Also, is the proposed projected gradient descent strategy guaranteed to converge? \n\nI also found the experiments to be a little unconvincing. In the first set of experiments, why was the Gibbs sampler run for 30K iterations? Since you are comparing a sampling method to deterministic methods, a comparison on the basis of time would seem more fair. Also, where are the error bars on these charts - the reader cannot tell if these results are significant. The second set of results compare the original/naive gibbs sampler with gibbs samplers under different projections and show that the projection does lead to faster mixing. However, it takes time to performing the projection operation and this is not accounted for in this comparison (e.g. if the projection takes 1 minute, and the naive Gibbs sampler can generate 10k samples in that time, then the projection might not be worthwhile). Last, why was there no comparison to blocked Gibbs samplers or Gogate's \"Lifted Gibbs Sampler\"? \n All in all, a very nice idea. However, the development of the projection problem and proposed method could use a little work and a bolstered set of experiments are needed to convince me of the utility of the method.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Success of inference by Gibbs sampling in MRF (here, only with two \nlabels, ie, ising model) depends strongly on the mixing rate of the \nunderlying Monte Carlo Markov chain. The paper suggests the following \napproach to inference: \n\n1) Project the input model (which possibly does not mix fast) on the set \nof models that do mix fast. \n\n2) Do inference on the obtained model that mixes fast. \n\nThe space of fast-mixing models is defined by bounding the spectral \nnorm of the matrix of absolute values of Ising edge strengths. \n\"Projection\" is defined by divergences of Gibbs distribution. It is \nforced to preserve the graph structure. Projection in Euclid distance \nis obtained by dualizing the initial task and using LBFGS-B. For \nother divergences (KL, piecewise KL, and reversed KL divergences are \nimplemented), projected gradient algorithm is used. In reversed KL \ndivergence, Gibbs sampling (but on a fast mixing model) must be done \nto compute the projection. \n\nExtensive experiments on small random models are presented compare the \napproximated marginals with the true marginals. The methods tested are \nthe proposed one (with all the above divergences) and loopy BP, TRW, \nMF and Gibbs sampling on the original model. Not only accuracy but also \nruntime-vs-accuracy evaluation is done. The experiments show that the \nproposed methods consistently outperform TRW, MF and LBP in accuracy, \nand for reasonable range of runtimes also Gibbs sampling on the \noriginal model. Of the proposed methods, the one with reversed KL \ndivergence performs consistently best. \n\nComments: \n\nThe projected gradient algorithm from section 5.1 n fact has two \nnested loops, the inner loop being LBFGS-B. Pls give details on when \nthe inner iterations are stopped. \n\nIt is not clear what the horizontal axis in the plots in Figure 2 (and \nthe supplement) means. It is titled \"number of samples\" but sampling \nis used only for reversed KL divergence. I believe the horizontal \naxis should be runtime of the algorithm. Similarly, why not to report \nalso runtime of LBP, TRW and MF. This would ensure fair \naccuracy-runtime comparison of all tested algorithms. Please, clarify \nthis issue - without that it is hard to interpret the experiments. Give absolute running time in seconds. \n\nPlease consider experimental comparison with larger models. An interesting option is to use models from the paper \n\n[Boris Flach: A class of random fields on complete graphs with tractable partition function, \nto appear in TPAMI, available online] \n\nwhich allow polynomial inference. \n\n222: replace \"onto the tree\" with \"onto graph T\" \n\n226: Shouldn't we ssy \"subgradient\" rather than \"derivative\"? Interesting paper, convincing empirical results. Practical utility can be limited though due to high runtime (this needs clarification in rebuttal).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
