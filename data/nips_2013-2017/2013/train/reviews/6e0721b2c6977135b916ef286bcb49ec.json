{"title": "Near-Optimal Entrywise Sampling for Data Matrices", "abstract": "", "id": "6e0721b2c6977135b916ef286bcb49ec", "authors": ["Dimitris Achlioptas", "Zohar S. Karnin", "Edo Liberty"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper describes a novel procedure to sample from a sparse data matrix using the spectral norm as a measurement of quality. The procedure is described for the \"streaming\" setting in which in which a fixed matrix is read from disk and sampled without requiring the entire matrix to reside in memory at any one time. \n\nThe paper is overall well written and assumptions and implications are explained clearly. For instance, while the definition of the data matrix in 2.1 appearing to be quite arbitrary at first glance, the authors made a good case that these conditions should hold for most typical inputs. The background sections do a good job of positioning this work in relation to other techniques and the experiments are described clearly and well done. The resultant procedure is quite unique, relying upon an unusually weighted data-dependent sampling procedure and the authors demonstrate that the procedure can match or outperform other matrix sampling methods. \n\nHowever, the Proof Outline (section 5) is difficult to follow. It is not clear what proof it is trying to outline (for Theorem 4.1?) and it is not clear how Lemma 5.1, 5.2, 5.3 relates, and how the proof outline concludes. \n\n\nOther issues: \n- Equation 7 is written in a rather confusing way. Perhaps split it into two sections: P_ij = \\theta |A_{ij}| and \\theta = ... \n- Page 4, line 210 \"matrices B_i\". Should this be B^{(l)} ? \n- Page 6, line 315. It is not clear where that quadratic equation came from. \n- Page 7 line 349. 62 sqrt(2) / 29 ~= 3.02 which is > 3 \n- The blue \"Bernstein\" line is completely invisible in many of the plots. Adding some arrows to indicate the location of the Bernstein line will be helpful. \n Few complaints, a well-written paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper introduces a new sampling method for sampling the entries of large matrices. Unlike deterministic approaches, which only select matrix entries that exceed a given threshold, the authors propose a probabilistic approach for selecting entries of a matrix that are candidates for storage and further analysis. From that perspective, the authors are looking for a matrix B that approximates an input matrix A in an optimal way, where optimality is measured in terms of the loss ||B-A||. \n\nThe approach of the authors is supported by precise mathematical arguments, but I was not convinced by this paper. I found the introduction utterly confusing. For example, the simple formula in (1) does not lead to a probability, in the sense that it yields a score between zero and one. Maybe I am totally confused, but I also did not get why B_ij should be in the set (-theta_i, 0, theta_i). I guess it should be in the set (-1/theta_i, 0, 1/theta_i). Moreover, I also did not get why B should be an unbiased estimator of A. The same small mathematical inconsistencies continue to appear in later sections. For example, in Theorem 1.1, the expected value of a matrix is zero. This should probably be the n x m zero matrix? \n\nMost importantly, the motivation for this work was not convincing to me. Why should the machine learning community care about sampling models for matrix data? The authors mention the \u201cstreaming model\u201d motivation, where one must decide for every entry whether to keep it or not upon presentation. In such a scenario, probably a deterministic solution based on retaining entries that exceed a threshold would suffice. From that perspective, the last paragraph of Section 3 could not convince me that such a deterministic approach is less useful. I would like to read at least one concrete application where one would need sampling models for matrix data. Probably, such applications exist, but then they should be mentioned in a paper of this kind. \n Interesting problem, but a better justification for this work is needed.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Near-optimal Distribution for Data Matrix Sampling \n\nThe paper presents a new sampling methods for generating (random) sparse matrices that best approximate a given matrix in terms of the spectral norm. Several assumptions are made and well motivated, such as the \"data matrix\" properties and the constraints of the computational model (i.e. streaming data). The exposition is very readable, although some topics such as compressibility and fast indexing are brought up that are then never related back to the method proposed. More to the heart of the matter, the author(s) start with discussing the commonly known L1 and L2 sampling schemes, which serve as reference points throughout the development of the paper and then move on to the matrix Berstein inequality and explain its significance in the context of sparse matrix sampling. A lot of attention is given to proper explanation and intuitive arguments, which makes the paper accessible for non-specialists. The only thing that has to be said is that the text becomes somewhat \"talkative\" and lengthy, in particular in comparison to the very condensed experimental section. I would recommend shortening these passage, while keeping the gist of the arguments and explanations. \n\nThe main contributions of the paper are algorithm 1 together theorem 2.2, which justifies it. I find eq 6 and what feeds into it somewhat hard to digest. Some properties (e.g. certain regimes for s: small, large) are explained in the surrounding text, but it would be nice to find a way of relating this back to the Bernstein inequality that supposedly motivates it. Some of this happens in section 5, but overall the algebraic complexity and notation make this difficult and it would be nice to preserve the crucial link between how even the simplified row-L1 distribution and the Bernstein inequality. With all these caveats around the structure and clarity of the presentation of the main contributions, I deem the result to be significant and the key idea as well as the execution of the proof and algorithmic detail to be highly non-trivial. \n\nThe experimental section and the empirical analysis are somewhat disappointing by comparison. First, I cannot even read the graphs in the printed paper (which is a nuisance). Second, there is really no diving deeper on any aspect. Third, no connection to any relevant application is made. Fourth, the spectral norm criteria is thrown out (\"it turns out to be more fruitful to consider\u2026\") and replaced by a new criterion based on projections to the top singular vectors. The spectral norm objective is not even plotted or mentioned any more. Given how much care has been spent on the theory part, how detailed things have been motivated and then worked through in the proof, this feels like quite a surprise. Shouldn't one go back and change the objective to address the \"scaling\" issue, for instance? \n Good overview of literature and nice idea to take the Bernstein inequality as a starting point to seek for better sampling distributions. However, in the light of the experimental section and without a somewhat deeper analysis (beyond the proofs), I am not 100% convinced of the superiority of this approach.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
