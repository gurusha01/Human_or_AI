{"title": "Large Scale Distributed Sparse Precision Estimation", "abstract": "We consider the problem of sparse precision matrix estimation in high dimensions using the CLIME estimator, which has several desirable theoretical properties. We present an inexact alternating direction method of multiplier (ADMM) algorithm for CLIME, and establish rates of convergence for both the objective and optimality conditions. Further, we develop a large scale distributed framework for the computations, which scales to millions of dimensions and trillions of parameters, using hundreds of cores. The proposed framework solves CLIME in column-blocks and only involves elementwise operations and parallel matrix multiplications.  We evaluate our algorithm on both shared-memory and distributed-memory architectures, which can use block cyclic distribution of data and parameters to achieve load balance and improve the efficiency in the use of memory hierarchies. Experimental results show that our algorithm is substantially more scalable than state-of-the-art methods and scales almost linearly with the number of cores.", "id": "52720e003547c70561bf5e03b95aa99f", "authors": ["Huahua Wang", "Arindam Banerjee", "Cho-Jui Hsieh", "Pradeep K. Ravikumar", "Inderjit S. Dhillon"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The authors discuss the problem of sparse precision matrix estimation using CLIME and present a scalable variant CLIME-ADMM along with a distributed framework for computations. Empirical results comparing the CLIME-ADMM algorithm to state-of-the-art techniques such as DC-QUIC, Tiger and Flare are presented.\n\nNotation remains scattered in various parts of Section 1, 2 and 3. It may help instead to have a table with all the notations for ease of readability. For example, \\rho and \\eta do not seem to be defined in the text. \n\nThe block cyclic data distribution appears to be interesting- it is however not obvious why this scheme would achieve load balance and scalability. There are also no empirical results to prove this point.\n\nIt appears that the algorithm currently sets the column block size on an adhoc basis -- is there some intuition or theory to guide the choice of k?\nA more general concern is that this is perhaps not a 'distributed' algorithm as no explicit mechanism of message passing and communication are discussed. Perhaps it would make sense to consider this a parallel implementation.\n\nMinor comments:\nSection 1: ' where \\lambda is a tunning parameter -> Replace with 'tuning'\nSection 5.1: 'As Tiger is parameter tunning free' -> Replace with 'tuning'\nSection 2: 'CLIME is summerized in' -> 'CLIME is summarized in' \n\n A large scale parallel algorithm for estimation of sparse precision matrix using CLIME is presented. The novelty involves the estimation of the precision matrix by column blocks, instead of column-by-column. Empricial analysis using OpenMPI and Scalapak is presented.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This authors proposed an inexact alternating direction method of multiplier (ADMM) for solving the linear program problem in the CLIME estimator. A convergence rate of O(1/T) is established. The algorithm is implemented using both shared-memory and distributed-memory architectures. Numerical comparisons with other methods, such as DC-QUIC, Tiger and CLIME-Flare, are provided. \n\n\nQuality: The paper is technically sound. \n\nminor comments: \n1) The derivation of the inexact ADMM for CLIME is standard. See, for example: \nhttp://math.nju.edu.cn/~hebma/English%20Version.htm \n\n2) Due to the structures of CLIME, the solution of each subproblem can be computed componentwise and the most expensive operation is matrix-matrix multiplication. Hence, ADMM can be parallelized ideally for CLIME. It is well known that matrix-matrix multiplications can be parallelized well. \n\n3) The set up of the numerical experiments can be further specified. For example, what is the accuracy achieved by each solver? \n\nClarity: The paper is well-organized. \n\nOriginality: The evaluation of ADMM in shared-memory and distributed-memory architectures is interesting. \n\nSignificance: Solving large scale sparse precision estimation problems is challenging. A simple yet robust distributed algorithm is helpful. The evaluation of ADMM in shared-memory and distributed-memory architectures is interesting. The derivation of inexact ADMM is standard and the good scalability is due to the simple structure of CLIME.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary of the paper \n\nThis paper is concerned with the resolution of covariance selection problems in a very large scale setup (up to millions of features and trillions of parameters). The estimator considered is the CLIME, which is known to provide an estimator of the precision matrix of a Gaussian vector that can be solved column by column, thus more easily amenable to distributed computations. Theoretical analysis is also claimed to be easier than for the Graphical-Lasso. The authors developed a new algorithm to fit the CLIME by solving the problem block wise rather than column wise, couple to an inexact direction method of multipliers. Theoretical results on the convergence rate of this algorithm are stated, in term of both objective function and distance to optimality. Special attention is paid for special matrix structures (sparse and low rank) along the computation to reduce the computational burden. The algorithm is implemented within a scalable parallel computation framework for both shared and distributed memory systems. In the numerical studies, comparison are made in term of runtime with its direct competitors on both synthetic and real data. A detailed study concerning this new algorithm is also provided to evaluate the speedup regarding the block sizes and the number of cores on both shared memory and distributed memory systems. \n\nComments \n\nThis paper is well written and very clear. Novelty and contributions brought by the method are clearly stated in the introduction and connexion to existing works clearly established, with sounded bibliographical references. The algorithm is nicely introduced with a good balance between technical points of numerical analysis/algebra and theoretical guarantees for convergence rates. The numerical analysis is appropriate and the implementation of the algorithm shows very impressive performances. \n\nMy only concern is the availability of such a method: if the source code (mostly based upon open source libraries) is not made available to the community, their is no point in such a work. \n A sound algorithmic paper which deals with the important practical problem of covariance selection when the dimension is very large (millions of variables). Such a powerful tool should be rewarded by publication if it is made available to the community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
