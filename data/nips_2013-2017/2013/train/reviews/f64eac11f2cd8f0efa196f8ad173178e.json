{"title": "Adaptive Step-Size for Policy Gradient Methods", "abstract": "In the last decade, policy gradient methods have significantly grown in popularity in the reinforcement--learning field. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly influenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step--size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second--order polynomial of the step size, and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear--quadratic regulator problem.", "id": "f64eac11f2cd8f0efa196f8ad173178e", "authors": ["Matteo Pirotta", "Marcello Restelli", "Luca Bascetta"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper provides a theoretically grounded adaptive step size for policy gradient methods in which the lower bound of the expected policy improvement is maximized. Approximated version of step size can be computed only by estimated gradient and thus the computational complexity would be same as ordinary policy gradient methods. There is a mild assumption that the standard deviation of Gaussian policy is fixed for deriving the lower bound. Experimental results show that the proposed method works well when the fixed SD is large enough. \n\nQuality, clarity and originality: \nThe motivation of the paper is clearly described and its organization is reasonable. The paper provides potentially useful theoretical analyses such as Theorem 4.3 and Corollary 4.4 although I cannot check the derivation of lemmas and theorems in detail. The originality is definitely high. \n\nSignificance: \nNewton method would be a standard approach for adaptive step-size but the second-order derivative of expected return with Gaussian policy is not guaranteed to be semi-positive definite. Thus, adaptive step-size based on the maximization of lower-bound would be a good alternative of ordinary policy gradient methods. \nHowever, as the author(s) review in the introduction, there are several extensions of policy gradients for stable policy update, such as EM policy search. Thus, it would be much more convincing if there are more experimental comparisons with these methods. \n\nSmall comments and questions: \n* The section of conclusion should be added. \n* Is it really possible to converge to the optimal policy with only one update for \\sigma=5 (in Table 1)? The paper provides potentially useful theoretical analyses and good alternative of policy gradient methods. It would be much more convincing if there are more experimental comparisons with state-of-art methods such as EM policy search.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper investigates how to automatically set the step size of policy gradient methods, in particular focusing on the Gaussian policy case. The paper provides a lower bound on the difference in performance when taking a gradient step, and optimizes this lower bound with respect to the step size. \n\nOverall, I like the approach of thoroughly studying the interplay between step size and policy performance. While the paper could be improved with clearer, more direct results, I think this is a fine, albeit incremental result for NIPS. \n\nQuality. The paper contains a hefty amount of theoretical work, but sometimes lacks a certain focus, e.g. Theorem 3.3 seems extraneous to the main result. I think the paper would be improved with a clear directing line throughout and less \"beating around the bush\". \n\nThe experimental results are somewhat disappointing. While a meaningful comparison, they don't strongly confirm the superiority of the approach. \n\nClarity. The paper is sometimes hard to follow because some of the steps are relegated to the supplemental material or ignored altogether. For example, the authors argue repeatedly using stationary points without providing formal details. \n\nOriginality. This is original work. \n\nSignificance. I appreciate the idea of improving policy gradient methods by refining the step size, and doubly so by obtaining a theoretical grasp of the problem. But I think the results presented here are marginally significant. \n\nSuggestion. The algorithmic aspect of this work (Section 5) comes in late, and seems mostly an afterthought. I think the paper could be much improved by emphasizing the algorithm, and subsequently deriving the quantities needed in the bound. Overall, I like the approach of thoroughly studying the interplay between step size and policy performance. While the paper could be improved with clearer, more direct results, I think this is a fine, albeit incremental result for NIPS.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Overview \n======== \nThe paper proposes a lower bound on the expected performance gain for policy gradient methods. This lower bound can then used to optimize the step-size. The bound is given in general, specialized for Gaussian policies, and finally a version for REINFORCE and G(PO)MDP/PGT is given, which is also evaluated empirically on a simple LQG problem. \n\nQuality \n======= \nThe theoretical part is very nice with a lot of material. The experiment nicely illustrates the approach. It would be highly interesting to evaluate how well the adaptive step-size performs in motor control and robotic applications (but that is clearly out of the scope of this paper). \n\nClarity \n======= \nThe paper reads well. The paper itself nicely manages to convey a notion of where the theorems and lemmas come from/how they are proven. I like the structure of the paper that goes from more theoretical to more and more applied. However, towards the end (Sect. 6) I got the feeling that it gets a bit crammed... \n\nOriginality \n=========== \nTo my knowledge nobody has looked into adaptive step-sizes for policy gradient methods from a theoretical point of view yet. \n\nSignificance \n============ \nEven though tighter bounds would be nice (as mentioned in Sect. 6), the results are interesting from a more theoretical point of view and even show promise in experiments. \n\nMinor Comments: \n=============== \nTable 1: maybe you could add another line with the optimal step-size (determined by line search as discussed in Sect. 1.) \n\nl 75: \"do not need to tune a free parameter\" is a bit misleading. It is true that this type of algorithms does not have a step-size but still e.g. the exploration magnitude needs tuning... \n\nTable 1: maybe you could mark the \"winners\" \n\nTable 1: 1 step for alpha* with sigma=5. is hard to believe/a very lucky coincidence \n\nTable 2: How do you count the iterations here? The number of update steps or (number of update steps)*(number of trajectories)? \n\nTable 2: maybe use e notation instead of >100,000 \n\nTable 2: especially here a comparison to the heuristics would have been interesting \n\nSupplementary Material Fig. 1&2: maybe you could use a logarithmic iteration axis (such that we can see better what happens at the beginning with alpha=1e-5/t). Axis labels would also be nice. \n\nPublic Review: \n============== \nThe author response addresses most of the above points. The notes on spelling and grammar mistakes as well as minor formatting/LaTeX suggestions were removed. The paper tackles a very interesting unsolved problem from a theoretical point of view and manges to get results that have great potential for practical applications.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a new policy-gradient reinforcement learning method in which the step size is set adaptively by maximizing a lower bound on the expected performance gain. The authors first consider the case where the policy gradient is known and establish a lower bound on the difference in performance between the original policy and the modified policy resulting from an update that follows the policy gradient. Since the above bound is a fourth-order polynomial, the authors then consider the special case in which the stochastic policy is represented as a Gaussian distribution with a fixed standard deviation and a mean that is a linear combination of the state features. In this case, they derive a lower bound that is quadratic in the step size and has a single maximum for positive step sizes. Next, they remove the assumption that the policy gradient is known and establish a high-probability lower bound given an epsilon-optimal estimate of the policy gradient. Finally, they show how two existing techniques, REINFORCE and G{PO)MDP/PGT, can provide such policy gradient estimates, leading to an adaptive step size policy gradient method that does not require a model of the environment and can learn only from sample trajectories. The proposed method is evaluated in a simple simulation experiment using a 1-dimensional toy problem. \n\nOverall, this is a high-quality, well written paper. The authors make a good argument that most research on policy-gradient methods has focused on finding better gradient estimators and thus there remains room for improvement in automating the step size in such algorithms. The theoretical results in the paper make a substantial contribution by providing a principled framework for the automatic selection of such step sizes by maximizing lower bounds on expected performance gain. The paper contributes not only a specific algorithm but more general results that could be used to derive algorithms with different policy representations and gradient estimators. \n\nThe main weakness in the paper is the empirical evaluation, which considers only a toy problem involving the optimization of one policy parameter. Results obtained using the true gradient show that manual choices of the step size can lead to slow learning (too small) or divergence (too big) whereas the adaptive method always converges within the threshold. For large values of the standard deviation of the Gaussian policy, the adaptive method performs better than any of the fixed step-sizes tested. However, for small standard deviations it performs worse, which the authors suggest reflects an inherent trade-off between the determinism of the policy and tightness of the lower bound. \n\nThe authors also present results for the case where the true gradient is not known and the REINFORCE and PGT estimators are used instead. However, it is not clear what these results demonstrate, other than that, unsurprisingly, increasing the number of trajectories used to estimate the gradient improves the gradient estimate and thus learning performance. More worryingly, however, the results also show that even with many trajectories, the errors are quite large, leading to loose bounds that prevent strong performance. This casts doubt on whether the results have practical implications or are only of theoretical interest. \n\nFurthermore, the empirical results compare only to fixed-step size variants and not to other baseline methods. In particular, the EM-based policy-gradient-like methods of Kobers & Peters and Vlassis et al. mentioned in the introduction seem like particularly relevant baselines since they do not need to tune a free parameter. \n\n\n\n\n\n Well written paper with a substantial theoretical contribution. The empirical evaluation is preliminary and yields mixed results for the proposed method.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
