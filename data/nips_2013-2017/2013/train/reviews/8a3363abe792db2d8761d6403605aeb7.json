{"title": "The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited", "abstract": "Hypergraphs allow to encode higher-order relationships in data and are thus a very flexible modeling tool. Current learning methods are either based on approximations of the hypergraphs via graphs or  on tensor methods which are only applicable under special conditions. In this paper we present a new learning framework on hypergraphs which fully uses the hypergraph structure. The key element  is a family of regularization functionals based on  the total variation on hypergraphs.", "id": "8a3363abe792db2d8761d6403605aeb7", "authors": ["Matthias Hein", "Simon Setzer", "Leonardo Jost", "Syama Sundar Rangapuram"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper proposes an algorithm for normalized cuts of hypergraphs by \nformulating the cut as the minimization of a ratio of two convex \nfunctions which can be solved using existing methods (RatioDCA, with \nan inner problem solved using a primal-dual method). Semi-supervised \nlearning on a hypergraph is formulated as a related optimization \nproblem and solved with a similar primal-dual method. The proposed \napproach is shown on several datasets to outperform an alternative \ntechnique based on a transformation of the hypergraph to a regular \ngraph for a semi-supervised learning, a clustering and a cut \nobjective. \n\nThe paper is clear and well written. It is technically sound and \nprovides a significant contribution to the problem of hypergraph cut, \nand possibly to semi-supervised learning and clustering --- assuming a \nhypergraph based approach is relevant to the problem. \n\nConcerning this last point, not much is said about the relevance of \nthe hypergraph approach. In all examples, the hypergraph is not \nprovided as a separate structure, but is built from the covariates by \nmaking one hyperedge for all samples which share the same value of one \ncovariate. if the data points are originally represented as vectors of \nfeatures, other semi-supervised and clustering techniques (eg based on \nscalar products or on a regular graph built from the features) would \nmake sense and should be compared against. \n\nSimilarly, the relevance of semi-supervised learning to the datasets \nis not discussed: what kind of performance would be obtained without \nusing the unlabeled samples? Admittedly, the point of this paper is \nhow the new formulation improves semi-supervised, not whether \nsemi-supervised is relevant. \n\nA few minor points: \n\n- Total variation seems to be accessory in the paper, whose main \nachievement is to provide a better algorithm for hypergraph \nnormalized cuts. The current title is a little misleading from this \npoint of view. \n\n- The semi-supervised problem (3) is itself a prox of \\lambda\\Omega at \npoint Y. There may not be a simpler way to compute the prox than the \nproposed algorithm but it could be useful to point it out in order \nto avoid confusion between prox (3) and the prox of its first term \nand the conjugate of the second term which are used to compute it. \n - Clear presentation, well written paper.- Significant contribution, technically sound.- Little discussion of the relevance of using hypergraphs to representthis data (as opposed to vectors or graphs).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Graph cut methods for semi-supervised classification and clustering are dominant in the last decade. Hypergraphs can incorporate higher order information about data than ordinary graphs and thus should be more preferable. Existing methods all have their own sets of limitation, as discusses in Section 1. \n\nIn contrast, this paper studies how to directly deal with the hypergraph cut using the total variation of the Lovasz extension. Two frameworks are proposed in Sections 3 and 4, and an algorithm for solving the involved problems is presented in Section 5. Experimental results in Section 6 are promising. \n\nThe idea of this paper sounds quite interesting. However, the derivation is not easy to follow. I have not checked the technical details since I am not familiar with these optimization problems in Sections 4 and 5. Anyway, the extension in Section 3 for semi-supervised learning from graph Laplacian matrices to the proposed functionals is natural, and the theoretical results seem correct. \n\nI have two minor questions. Firstly, in Definition 2.1, it is required that f_1<=...<=f_n. It seems later in the optimizations f as an optimization variable may not always satisfy this constraint. So when is it valid? Secondly, in the experiments, numerical features are converted into categorical by 10 equal size bins. Will this cause certain loss of information? \n\n** Comment after authors' feedback ** \n\nThe authors have clarified my concern about the experimental setup. This paper studies how to directly deal with the hypergraph cut using the total variation of the Lovasz extension. The idea is quite interesting, and the experimental results are promising.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper defines a notion of total variation on hypergraphs, proves its convexity and relation to cuts, discusses applications to learning and normalized cuts (for clustering), computes proximity operators as part of optimization algorithms, and presents numerical experiments comparing with a popular method of Zhou from 2006. Overall, the paper tours several sub-disciplines. There are a lot of ideas and a few theorems (though of course they cannot say much about a global solution in the case of the normalized cut problem, but no one can). The paper stays focused despite the amount of material and it is well written, with only occasional grammar issues. \n\nSection 5 falls most into my area of expertise, and I am satisfied with it. I am less familiar with existing work on spectral clustering and existing approaches for hypergraphs. This paper is basically claiming to be the first major advance in 7 years (since Zhou [11]), and I am inclined to believe this, but not completely certain. If it is true, then this paper should definitely be accepted and will likely become well-known. But of course I am a bit skeptical that the authors have not found any more recent work than [11], given the amount of attention to the subject (or is it because everyone is working on tensors and ignoring hypergraphs?) \n\nOther comments: \n- Comparisons were always with [11], which uses the same hypergraph framework, and then reduces it with the CE technique to a graph problem. But these problems (SSL, clustering) can be attacked from quite different perspectives. A comparison with an \"outside\" approach would make this paper stronger, even a simple algorithm (such as k-means for clustering... or explain why it is not applicable). Your clustering approach which recursively splits clusters is clearly not optimal (though I do not doubt that it is a standard technique). \n\n- Line 121: \"the optimal cut\". Shouldn't this be \"an optimal cut\", since it is symmetric so and the mirror-image cut would work as well? \n\n- The section around Thm 4.1 seemed vague to me and could have benefited from more explanation. For example, it's not clear to me how to apply the second equation from the theorem. \n\n- I'm not sure what the authors mean by \"tight relaxation\" (section 4, and mentioned in the intro). Is it the \"the tightest\"? (in that case, define what is your class of \"relaxations\", since it's not the class of convex functions in this case). Or do you mean \"tighter\" than the linear eigenproblem relaxation? (in this case, how do you justify this? For graphs, it is justified by empirical evidence, so for hypergraphs, you should derive the linear relaxation and test it). \n\n- Section 5. The method in [24] has since been extended a lot; see, for example, Condat 2011 (http://www.optimization-online.org/DB_HTML/2011/12/3284.html). In particular (and I think this was already in [24]), it can exploit smooth functions. Usually, it is better (in terms of convergence rates) to take derivatives of smooth functions when possible, as opposed to calculating their proximity operators; so for the first G term in Table 1, why not treat it as the smooth term? For line 276, \"the main idea\" (also, note that this phrase is repeated three times on this page) is not really described that well, since this is not unique to the method of [24]. Rather [24] allows one to separate many terms. \n\n- I am quite familiar with TV on a grid (for 2D). In this case, people prefer using the isotropic TV, but this does not have a closed-form proximity operator nor an efficient algorithm to compute it. Alternatively, the most common form of anisotropic TV is much simpler to work with. It appears that your TV definition generalizes this anisotropic TV. It's not clear, but it would be very interesting to explore, if it is possible to define an isotropic TV on a hypergraph. \n\n- Prop 5.1. This is O( n log n) due to the sort. Is it not possible to avoid the sort and just find the largest and smallest entries of the input? This would bring it down to O(n). I realize you may have to worry about cases when the answer will have several values at the max/min, but it still may be possible to deal with this. \n\n- Experiments. Overall, well done. For making the hypergraph edges using data points that have the same value of a feature, is it not beneficial to make edges (but with much lower weight) when data points have *similar* values? \n\n- The table on page 8: what are the entries? Some kind of error? This was not explained \n\n- Line 415: \"Our method... minimizes the normalized cut on the hypergraph.\" Unless I have misunderstood, this is not true. You apply a convex method to a nonconvex problem, so in the best case (and this you would need to show) you reach a stationary point or local minima. So at least say that your method \"attempts\" to minimize... \n\nGrammar: \n- Line 276 and 301, \"proximum\" and \"proxima\" are not commonly used; just use \"proximity operator\". \n\n- Line 416-7 is awkward. \n\n- The authors use the construction \"allow to\" a lot, as well as variants \"suggest/recommend to use\", \"favor to split off\", \"propose to solve\", (e.g., line 17, 69, 107, 192, 201, 385). Since the English is otherwise nearly perfect, it's worth correcting this. In these cases, the infinite should be a gerund, e.g. \"suggest to use\" should be \"suggest using\". Rarely, both forms are correct (e.g. \"I like to play tennis\" and \"I like playing tennis\" are both fine). In the case of \"Hypergraphs allow to encode\", it should be \"Hypergraphs allow one to encode\". \n\n- \"However\" is not used correctly a few times. E.g. line 199, it should be \"Hence\" not \"However\", and line 408 it should be \"Thus\". This seems like a significant paper. It introduces a new topic and explores it quite a bit, with good results. The numerics are done well, and it compares quite favorably over a similar approach from 2006. My only reservation is whether they have missed some recent work from the past 7 years that has further explored hypergraphs and would improve on [11].", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
