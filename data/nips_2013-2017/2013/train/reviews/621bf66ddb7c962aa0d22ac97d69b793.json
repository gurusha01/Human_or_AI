{"title": "Faster Ridge Regression via the Subsampled Randomized Hadamard Transform", "abstract": "", "id": "621bf66ddb7c962aa0d22ac97d69b793", "authors": ["Yichao Lu", "Paramveer Dhillon", "Dean P. Foster", "Lyle Ungar"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The authors propose a method (SHRT) to accelerate Ridge Regression when the \nnumber of features p is much greater than the number of samples n (p >> n). \n\nThe main idea is to reduce the dimensionality of the design input matrix X to \nmake the computation of XX^T faster, from O(n^2) to O(nplogn). This is done via \nWalsh-Hadamard transforms, which can be computed in log-time. \n\nThe research idea is of high-quality and great use. The paper is well written \nand technically correct. However, my main concern is the significant overlap to \nthe recently published paper: \n\n\"Fastfood \u2013 Computing Hilbert Space Expansions in loglinear time, \nQuoc Le; Tamas Sarlos; Alexander Smola\", ICML 2013. \n\nBoth submissions are very close in time, so I believe they were developed \nindependently. Fastfood seems completely analogous to the here proposed \nSHRT algorithm, but more general: \n\nSHRT(X) := RHDX \nFASTFOOD(X) := exp(SHGPHBX) \n\n1) The basis matrix R in SRHT is the matrix P in Fastfood. \n2) The Walsh-Hadamard matrix H in SRHT is the matrix H in Fastfood. \n3) The Rademacher matrix D is the matrix B in Fastfood. \n4) The main difference is the setup of p >> n for SHRT. In Fastfood, \nthe number of random features p_subs to generate can be greater than p, \nso multiple \"SHRT blocks\" can be built. \n5) In SHRT there exists no matrix \"G\", which corresponds to spectral samples \nof a Gaussian kernel in Fastfood. This is because in SHRT a huge feature \nspace is assumed, so linear kernel is used (that is, Fastfood's G and S \nmatrices are diagonal in SHRT). This is also the reason why SHRT \nis not exponentiated. \n\nThe theoretical developments in Lemma 2 and Theorem 1 are mathematically well \nexecuted and are novel material. This is also the case for the application of \nthe method to the PCA algorithm. Therefore, there is enough novelty in the \nsubmission. \n\nThe authors should cite Rahimi & Brecht works (2007, 2008) and the Fastfood \npaper. \n\nI believe that the admission of this work demands a better connection with \nexisting work and (ideally) a comparison to other randomized methods. \n\nI have read the Author Feedback. A method to accelerate the computation of XX^T in ridge regression from O(n^2p)to O(nplog(n)). The idea is to reduce the dimensionality of X via subsampledrandomized Hadamard tranforms. The paper exhibits some overlap with the ICML2013work \"Fastfood \u2013 Computing Hilbert Space Expansions in loglinear time\", but noveltheoretical developments.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a fast algorithm for ridge regression by applying subsampled randomized Hadamard transform (SRHT), which can reduce the dimensionality and reduce the computational cost significantly. The basic idea of this algorithm is first applying the randomized Hadamard transform, and then sampling features uniformly. Also the theoretical results on the risk of the proposed algorithm is analyzed. Based on the proved theoretical results, the order of subsampled features p_subs is estimated. And the comparison between the proposed algorithm and the principal component regression is also discussed. The empirical studies on both synthetic and UCI data sets are performed to demonstrate the good performance of the proposed algorithm. \nThe paper presents an interesting algorithm and is easy to read. The idea of the proposed algorithm is fairly simple. The key is the efficient implementation of the subsampled randomized Hadamard transform. The idea is simple but interesting to me. The disadvantage of other random projection algorithms is the cost to generate a random projection matrix and compute the product of the projection matrix and the design matrix. The key of this paper is to integrate these two steps into one step which is implemented efficiently. \nThe empirical studies are a little weak. For the UCI data sets, the classification accuracy is reported. As this paper focuses on ridge regression and the theoretical bound of risk is proved, it is more reasonable to investigate the error in regression instead of classification. \nA typo: in Eq. (3), the right hand side of the inequality is sqrt{c*log(2k/delta)*k)/p_subs}, while in the following discussions there no coefficient 2. I have not checked the cited references, but it seems a typo. \n A simple and efficient implementation of ridge regression using the subsampled randomized Hadamard transform is proposed. The idea is simple but interesting to me; the theoretical results and the empirical results are a little weak.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
