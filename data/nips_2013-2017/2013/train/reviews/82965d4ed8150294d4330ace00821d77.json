{"title": "Fast Template Evaluation with Vector Quantization", "abstract": "Applying linear templates is an integral part of many object detection systems and accounts for a significant portion of computation time. We describe a method that achieves a substantial end-to-end speedup over the best current methods, without loss of accuracy. Our method is a combination of approximating scores by vector quantizing feature windows and a number of speedup techniques including cascade. Our procedure allows speed and accuracy to be traded off in two ways: by choosing the number of Vector Quantization levels, and by choosing to rescore windows or not. Our method can be directly plugged into any recognition system that relies on linear templates. We demonstrate our method to speed up the original Exemplar SVM detector [1] by an order of magnitude and Deformable Part models [2] by two orders of magnitude with no loss of accuracy.", "id": "82965d4ed8150294d4330ace00821d77", "authors": ["Mohammad Amin Sadeghi", "David Forsyth"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper explores a way of speeding up template evaluations in object detection by using vector quantization. The essential idea is to replace each HOG cell by an index into a dictionary. Template evaluation then amounts to a series of table lookups followed by a simple addition. In conjunction with previous work on speeding up detection [5], this paper introduces a speed up of two orders of magnitude. \n\nQuality : The paper is well written and thoroughly evaluated. Section 4, the computational cost model, is especially well done and clears up the air vis-a-vis previous speedup papers. The timing comparisons are convincing. \n\nClarity : The paper is clearly written and well organized. \nA suggestion (this doesn't matter for the review) : Section 4 might also gain from a graph showing how the time taken for end-to-end evaluation on a single image varies as the number of categories increases. This might be an alternative way of conveying Table 1 \n\nOriginality : This is the single negative point in this review. A lot of the ideas used in this paper have actually been done before. The idea of quantizing HOG cells was actually put forward in [10] (see section 5.2). Using just the idea of quantization (without all the other additional speedups that are shown here), [10] achieves a speed-up factor of 2, while the final speedup factor that this paper demonstrates is much larger, ~100. This is a huge difference. Still, in terms of novelty, this paper is more of a somewhat novel combination of not-so-novel techniques achieving a big gain, rather than one single original idea. \n\nSignificance : This paper is extremely significant for the computer vision community, because the magnitude of the speedup is huge, and can have a big impact on downstream locations. The authors promise to release the code; if they do, then the code will very likely be quickly adopted throughout the vision community. \n This paper doesn't introduce any novel machine learning techniques, but it convincingly demonstrates big speedups in an important computer vision task.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper considers HOG-based image detection. The bottleneck of such detection is the scalar product operation between the pairs of HOG cells coming from the image and the template. The central idea of the paper is to replace this scalar product with a single look up, by vector quantizing the HOG-cells on the image side (and precomputing the scalar products with cluster centers on the template side). \n\nPros: \n+ The idea is simple, is sound and is clearly explained. \n\n+ As HOG-based detection (including DPMs) is very popular, and the authors promice to release their code as a software library, this may become a widely used and cited paper. \n\n+ The experimental evaluation is thorough and the results are very good for the proposed method (several-fold speedup compared to already good codes for evaluating DPMs). This good performance however required considerable micro-level optimization. \n\nCons: \n- I can see only one major problem with the paper, but it is a big one: the central idea is already present in [10]. It is true that [10] has a different emphasis, and does retrain its models, but the central idea is there. The main difference between the proposed approach and [10] seems to be that [10] vector-quantizes both the image and the template side (ADC vs SDC in terms of [13]), hence obtaining coarser approximation. \n\n\n--- \nif accepted, please fix the author names in the ref. [13] A well-written paper with a natural and thouroughly-evaluated idea, which is likely to be appealing for many CV researchers. This is undermined by a strong overlap with the previously published (CVPR12) work.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Authors have proposed a series of 'tricks' to reduce the memory storage and computational cost of applying linear templates. The presented approximation methods achieve speed-up without significant loss of accuracy. Moreover, the authors aim for methods that trade off between speed-up and accuracy by: choosing number of quantization levels, 16-bit or 32-bit arithmetic, and finally choosing to re-score windows or not. More precisely, the key idea is to replace convolution by a look-up table. To make the latter efficient, the authors use k-means as a quantization method effectively reducing dimensionality (authors also report excellence of k-means over PCA in Fig. 2). Finally, the authors introduce different computational models showing achieved speed-up of the proposed methods over the state-of-the-art detection algorithms on Pascal VOC 2007. Similar conclusion is drawn based on the experiments on exemplar SVM. \n\nReference [2] (see below) seems the most related reference. It is not cited in the paper an therefore a comparison is absent. A lot of the ground is already covered in that paper - which takes away from the novelty. [2] proposes an approximation if the filters in a sparse coding way. Also nearest neightbor and pca are considered in [2]. \n\nAlthough the presented idea is interesting and impactful as it contributes to the speeding-up the detection task, the work is too incremental. A replacement of dot-product in the convolution operator has already been introduced in [1]. Considerable speed-up has also been shown in [2] with a sparse intermediate representation and a nearest neighbor approach where the closest part filters are retrieved. Also a pca representation has been considered there as a baseline already. The presented method scales up well as the number of classes grows. Also [3] shows significant speed-up, however [3] seems to be complementary and fits into the 'cascade' section of the paper. \n\nPros: \n+ Authors tackle important problem of speeding up the convolution step that is at heart in many detection systems, but not only. \n+ The proposed methods trades off between accuracy and speed-up \n+ Simplicity of the method \n+ Authors have promised publishing source code. \n+ Paper is generally well written. \nCons: \n- Incremental nature of the paper. \n- Experiments shown in Table 1, Table 2, and Table 3 are confusing. In particular, why the paper reports different total time for the same method in different tables? Authors should also be more explicit how they have estimated the running time of unaccessible algorithms. \n- It is not clear which approximation 'trick' contributed the most to the speed-up (simple look-up table, packed look-up table, 16-bit arithmetic, deformation estimates, ...). Authors should make up experiments showing the importance of every 'trick'. \n- Authors should be more explicit about implementation details. For instance about the programming language being used in the experiments for every method together with the 'programming tweaks'. \n\n\n[1] 'Fast, Accurate Detection of 100.000 Object Classes on a Single Machine' by T. Dean et. al. \n[2] 'Sparselet Models for Efficient Multiclass Object Detection' by H. Song et. al. \n[3] 'Branch and Rank: Efficient, Non-linear Object Detection' by A. Lehmann et. al. \n\n The paper is significant as it speeds up the convolution step, however the presented idea seems to be incremental.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
