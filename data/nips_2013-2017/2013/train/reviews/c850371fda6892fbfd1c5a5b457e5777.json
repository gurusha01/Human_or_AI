{"title": "(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings", "abstract": "", "id": "c850371fda6892fbfd1c5a5b457e5777", "authors": ["Abhradeep Guha Thakurta", "Adam Smith"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper provides a generic way to perform parameter tuning of a given training algorithm in a differentially private manner. \nGiven a set of examples (for training and validation), a set of model parameters, a training algorithm, and a performance measure, the proposed procedure outputs a differentially private hypothesis with respect to prescribed privacy parameters. \nThe basic idea behind the procedure is the definition of (\\beta_1, \\beta_2, \\delta)-stability; which describes the stability of the performance with respect to change in the training set and the validation set. \nThe procedure basically follows the exponential mechanism, and the utility bound is also provided. \n\nAt each step, the gradient is set to a tree such that each node of the tree represents the differentially private partial sum of values held by the descendent nodes. In execution of the differentially private FTAL, the learner issues a query to the tree to learn a differentially private parietal sum of previous gradients. Noting the fact that only O(log T) accesses are needed to learn the partial sum, the variance of the noise added to each value is O(log T). \n\nThe authors provides two algorithms, one is for the full information model and the other is for the bandit setting. In the full information setting, the proposed algorithm improves the regret bounds. The regret bound in the full information model with strongly convex and L-Lipschitz cost functions is significant; O(poly log T) is achieved as FTAL does. In the bandit setting, the authors introduces a technique called \"one-shot gradient\" to evaluate the gradient and shows regret bounds in several settings; the regret bound is optimal with respect to T when the cost function is strongly convex and L-Lipschitz. \n\nThe clarity can be improved. It is hard to follows the problem settings. It is not clearly stated what are private instances to be protected by differential privacy. Definitions of the oblivious adversary and the adaptive adversary in the bandit setting is unclear, too. \n\nline 107. What is \"stronger setting\"? Does this mean \"with stronger assumptions\"? \nline 229-232. The sentence was hard to follow. \nline 305. This technique was referred to as one-point gradient, not as one-shot gradient. \n\n\n Significant improvement of regret bounds of differentially private online learning in the full information model is shown.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This work provides new algorithms for differentially private online learning in basically all settings where results are known in the non-private case. The results very closely parallel (with some additional small factors and dependences) the best non-private bounds. There has been significant recent interest in differentially private learning, and this paper gives a much more comprehensive and compelling suite of results than previous work. \n\nThe approach is actually quite simple at its heart---do follow the approximate leader using a private (noisy) history that you maintain using standard algorithms for maintaining a differentially private counter. The natural suitability of follow the leader algorithms for privacy has been observed before, but the treatment here and the actual bounds obtained are very nice. \n\nI'd prefer that you describe this work as giving a class of private learning algorithms, or a technique for constructing such algorithms, rather than a \"general technique for making online algorithms differentially private\" (since it's not the case that you can take an arbitrary online algorithm and make it private using this technique---right?). \n\nThe practice of citing references without listing authors made this paper hard to read---I was constantly flipping to the bibliography, since in many cases knowing the reference was important to understanding the ideas (e.g. in cases where the text did not make sufficiently clear whether the referenced work was in the private or non-private case). \n\nIn Table 1, it's probably worth reminding the reader that the delta in the first column comes from (eps, delta)-DP. \n\nWhy aren't there citations for the non-private results in Table 2? And also, the caption says it's the full-information setting, but the results are clearly for the bandit setting. And it seems your results for the adaptive case should have a T^3/4, not T^2/3, no? \n\nDo you think the explicit dependences you see on the dimensionality are necessary when preserving privacy? Do you think there's a provable (small) gap between what is achievable in the dependence on T between private and non-private algorithms (accounting for your different poly-log factors)? \n\nThe second to last paragraph on p.6 has strange redundancy. \n\n\n A simple and appealing approach and set of results on private online learning.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents a general method to design differentially private online algorithms, for the full information and bandit settings. \n\nThe paper is clear and well motivated. The relation with the relevant literature is thoroughly discussed. \nI did not check all the details in the appendix, but the results appear correct to me. \n\nI think the paper is probably not a breakthrough, but the ideas presented are interesting and I think that it still deserves to be published. \n\nMinor things: \n- please move the discussion to the main text, I find that the open problems are actually interesting, especially the comparison with the results in [1]. A general method to design differentially private online algorithms, for the full information and bandit settings. Clear and complete theoretical results are shown.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
