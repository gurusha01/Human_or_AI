{"title": "Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent", "abstract": "We present and study a distributed optimization algorithm by employing  a stochastic dual coordinate ascent method. Stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems. It still lacks of efforts in studying them in a distributed framework. We make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network, with an analysis of the tradeoff between  computation and  communication. We verify  our analysis by experiments on real data sets. Moreover, we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing SVMs in the same distributed framework, and observe competitive performances.", "id": "dc912a253d1e9ba40e2c597ed2376640", "authors": ["Tianbao Yang"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper propose a new parallel dual coordinate descent method for solving regularized risk minimization problems. In the proposed algorithm, each machine/core updates a subset of dual variables using the current parameter, and then conduct the \"delayed\" update to the shared parameter. \n\nThe similar idea has been used in yahoo-LDA and matrix factorization (Hogwild), but to my knowledge this is the first applying to dual coordinate descent method. Nice Theoretical guarantee is provided in Theorem 1. Experiments show that the method outperforms other parallel algorithms on linear SVM problems. My only complain is that Figure 3 is too small and hard to read; also, I suggest to use log-scale on primal obj and duality gap.  This is a good paper and I vote for acceptance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper combines a recent advance in stochastic dual coordinate ascent with the standard approach to parallelize training which is to assign a mini-batch of example to each process and average the resulting gradients. While this combination is novel, its most original contribution is the trade-off bounds between communication and computation. This is likely to be an influential work, introducing a new methodology. The main limitation to its significance are poorly designed experiments. In particular, the authors do not discuss the impact of the input dimension in the bounds, in particular in the case where the data is sparse, which is nearly always the case for big data. \n\n2 major limitations appear in the significance of this trade-off: \n- In the discussion on page 5, the authors omit the number of dimensions d. If the data is dense, this could be treated as a constant factor as both the communication and computation cost are linear in (I assume the main loop in computation is W.x which is in O(d)). However the situation becomes very different when the data is sparse: the computation cost for a single dot product scales as the number of non-zero features we denote d\u2019. Thus the total computation cost per iteration is O(m*d\u2019). In the worst case, where no features from the m examples overlap, the communication cost will also be O(m*d\u2019). We then lose this m ratio between them that was critical to the analysis on page 5. Note that in the KDD cup data, one feature out of 1 million is non zero. \n- Experiments only count the number of iterations, not the total running time that includes the communication latency. On top of that, the time per iteration depends on the parameters. So experiments reported on figure 3 that show that increasing m always reduce the number of iteration are very misleading, as each single iteration scales as O(m*d) in computation cost, and O(d) in communication cost. A curve showing the total training time, with values for m and K for which this time is minimized would be much more convincing. \nIn summary, the experiments are so far of little significance, as they only show a reduction of the number of iterations as a function of m and K, which is a trivial observation that does not need theorem 1. Note also that the authors only say they use openMPI, which does say anything about the architecture: \n- Cluster: how many nodes, how many core per node? \n- Single multicore? Shared memory would mean no communication costs. \n\nThe most interesting observation from theorem 1 is the presence of an \u201ceffective region\u201d for K and m. But the only thing the experiment show is that decreasing lambda gives more room to choose m and K. A effective upper threshold of the mK product, supported by actual training times, would be a very significant result. \n\nDetailed comments: \nTens of hundreds of CPU cores: do you mean thousands of cores of tens of clusters with hundreds of cores? If communication costs are involved, the target should be clusters, not multicores with shared memory. \n\nProof of theorem 1: while the proof is correct and trivially obtained from the bound E(epsilon_D^t), the rest of the proof in not needed. In particular, the last sentence is confusing, as there is no T0. \n\u201cWe can complete the proof of Theorem 1 for the smooth loss function by plugging the values of T and T0.\u201d \nThere seems to be several lines that have nothing to do with the proof?? \n\nWhy do the authors repeat twice DisDCA is parameter free? The choice of lambda is critical. \n\nP3, l128: I thought alpha was the associated dual of w, not x, but this may be terminology. \nFigure 3: varing->varying. \nPlots in Figure 3 are very hard to read. \n An very interesting new algorithm with a theoretical derivation of a communication/computation trade-off: too bad the experiments do not properly demonstrate the trade-off.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper investigates a distributed implementation of stochastic dual coordinate ascent (SDCA). The DisDCA algorithm is shown to enjoy the same convergence guarrantees as SDCA, and compared to ADMM it has no parameters that need tuning. Empirically it performs as well as a well-tuned ADMM implementation. \n\nThe paper is well-written and technically sound. The algorithm is novel but straightforward and I really enjoyed the section on the tradeoffs between computation and communication. Distributed learning is a very active topic yet not many papers try to analyze the regime at which the algorithms are competitive. In general I don't have any major complaints with the paper except that the figures, both in the main paper, as well as the supplementary material are extremely hard to read. I suggest that Figure 3 contains a sample of the results and the rest are over *multiple pages* in the supplementary material. This is a solid paper and relevant to the NIPS community. Distributed DCA is a nice alternative to ADMM.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
