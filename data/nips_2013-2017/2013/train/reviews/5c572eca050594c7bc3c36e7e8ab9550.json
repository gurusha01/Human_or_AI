{"title": "Inverse Density as an Inverse Problem: the Fredholm Equation Approach", "abstract": "", "id": "5c572eca050594c7bc3c36e7e8ab9550", "authors": ["Qichao Que", "Mikhail Belkin"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper proposes a new method to estimate the density ratio p/q. There are a couple of other methods already proposed to this problem. The novelty of this paper is that the authors reformulate the density ratio estimation as a Fredholm integral equation. \n\nStrong points of the paper: \n\nThe paper is well written; I enjoyed reading it. The density ratio estimation problem is important and interesting. \n\nWeak points of the paper, comments, and questions: \n\n(i) Some details are missing from the paper. For example, why \n\\int k_t(x,y)f(x)dx=f(y)+ O(t)? Either a proof (in the paper or in the supplementary material) or a reference needed for all these kind of statements, even if they are easy to prove. \n\n(ii) As a special case (q=1), this method can also be used to density estimation. It is interesting that this approach doesn\u2019t require the bandwidth to converge to zero. The standard kde density estimation is not consistent in that case. It would be nice to see more discussion about this, so the readers could clearly see that the bandwidth parameters in the paper have different roles than that of in kde. \n\n(iii) I recommend adding a few sentences about Tikhonov regularization. The majority of the NIPS community might be not familiar with it. \n\n(iv) All the proofs are in the supplementary material. I haven\u2019t checked if they are correct. \n\n(v) It would be nice to read a few sentences about why a naive plug-in approach, that is separately estimate densities p and q, is worse than the method proposed in this paper. \n\n(vi) It\u2019s also not clear how this method performs compared to other methods. For example, if I remember correctly [12] derives minimax bounds too. Corollary 2 and 4 provide upper bounds on the convergence rate, but it is not clear how far they are from the optimal minimax rate. \n\n(vii) Please provide references for the datasets used in the paper (CPUsmall, Kin8nm, USPS). \n\n(viii) I missed the demonstration on how the proposed method would work on toy problems for some fixed and known p and q. For example, for 1 dimensional p and q, a plot of the true p/q and the estimated p/q could help the readers assess the quality of the estimation. \n\n It\u2019s a nice paper about density ratio estimation. It is not clear to me how this method would perform compared to other algorithms (e,g. [12]), and I missed the demonstration of how this approach would work on toy problems, e.g. using simple 1-dimensional p and q densities.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper discusses two new methods for estimating the density ratio of two probabilities. The first method uses the equation of importance sampling, and formulates it in the form of a regularization problem where L^2 norm of the deviation from the equality and RKHS regularization term are used (Type I Fredholm equation). The second method considers a kernel where the locality parameter should go to zero for consistency. The latter is also expressed in the form of regularization (Type II Fredholm equation). The paper shows some theoretical results on convergence, and demonstrates the practical performance experimentally. \n\nWhile the method has similarity to existing methods such as KMM, it is a strong advantage of the proposed one that the L^2 formulation enables us to apply cross-validation (CV) approach to choose the parameters in the algorithm. Unlike supervised problems, where CV is dominant as a method for choosing parameters in practice, there are no general methods for unsupervised learning. The paper solves this issue for density ratio estimation with a clever idea of using L^2 norm. \nIt is nicer to make a remark that the objective function given by RKHS, such as in KMM, is not appropriate for choosing a kernel with CV, since the values of the objective function depend on the kernel, and thus not comparable. \n\nIn Theorem 1, by the (-t/log \\lambda) factor, the derived convergence rate is slow with fixed t. In many literatures of kernel methods, with fixed t, one can derive a polynomial rate with respect to the sample size after fixing the rate of \\lambda. Are there any reasonable additional assumptions that give a polynomial rate for the proposed estimator? Caponnetto and De Vito (2007), for example, make assumptions on the spectrum of the operator K_{p,t} to derive the optimal rate for kernel ridge regression. I guess that making a stronger assumption on q/p will also derive a polynomial rate. It will be more interesting if the authors discuss such polynomial rate under stronger assumptions with fixed t. \n\nIn estimating a density ratio, we usually need to make an assumption that q/p is in a nice function class such as Sobolev. It is important to note that this assumption is very strong, requiring some knowledge on the ratio or tails of p and q. Under such an assumption on q/p, the opposite ratio p/q is usually behaves badly. It is necessary to include some discussions somewhere on this limitation of density ratio estimation. \n\nThis is not a requirement, but the supplementary materials should be improved. There are many typos and minor mistakes, which make it hard to check the correctness of the theoretical results in the main paper. For instance, in Lemma 5, W_2^2 should be W_s^2, and the bound should be D_1 (t/-log \\lambda)^2 + \\lambda^{1-\\alpha} D_2 \\|f\\|_2^2. The expressions at line 675 and 686 are incorrect, while Eq.(31) is correct. In the proof of Lemma 5, the domain of the integral at line 1147 must be \\| \\xi\\|^2 \\geq. Since the theoretical results of convergence consist of main contributions of this paper, I advise the authors to revise the supplementary material. \n\n\nReferences: \nCaponnetto A., De Vito E. Optimal Rates for Regularized Least-Squares Algorithm. Foundations of Computational Mathematics, 7 331-368 (2007). \n\n\n The paper proposes a new approach to the density ratio estimation. The strong point is that CV can be effectively applied to choose parameters.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a solution to the problem how to estimate the ratio of two probability density functions. The approach is based on using a kernel and a Fredholm integral equation. This yields \nan inverse problem.  From the viewpoint of machine learning, the assumption of densities is a very hard one, because it is well-known that in any epsilon neighbourhood of a probability measure P which has a \\nu-density there are probability measures which have no \\nu-densities.Therefore, the estimation of a ratio of two densities is problematic from the viewpoint of ML.As far as I can see, the authors do not describe which dominating measure \\nu is used (Lebesgue measure, counting measure, something else) and it is unclear to me, if their results are rigorous because of sets of \\nu-measure zero. I also miss a clear statement which assumptions are made. E.g., does x belong to $R^d$ or to some general topological space? Is the density p positive for all x ?", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors use two equalities to derive two estimators of the Radon-Nikodym derivative f= p/q: \n\n(i) Ep ( k(x,.) f) = E_q k(x,.) and (ii) Ep (k(x,.) f ) ~ q(x) \n\nThe integral operators are replaced with empirical versions. {k(x,.)} can be seen as a set of test functions over which the difference between the right and left side is minimised. The choice of {k(x,.)} as the family of test functions has the advantage that standard kernel methods can be used for optimisation. \n\nConvergence guarantees are given for both settings under smoothness & boundedness assumptions on the densities. The rate of convergence depends crucially on the dimension of the underlying space -- as one would expect for a density estimate. \n\nI think it is nice and well executed work. \n\nA couple of comments: \n\n- For the convergence rates. Optimising over the set {k(x,.)} is sufficient to guarantee convergence to the ratio. For (ii) this is intuitive given the bandwidth dependence. Would be nice to have a short intuitive argument why this also works out in (i). \n\n-You optimise on page 2 over L_2,p. Later in (9) you have a combination of L_2,p and L_2,q. Some motivation for the choice of cost function would be nice. I guess ultimately these need to stem from the tasks one is interested in. So would be nice to link the cost function choice to certain important applications. \n\n- Speaking of applications, one example you got is importance sampling. Ie q/p is relevant for this to move from Eq to Ep. I'm wondering here: you are effectively estimating E_p and E_q to get your empirical cost function. So why not use directly the E_q you got there? Can E_p (q/p) be any better than directly E_q? I guess I'm asking here for a motivation of the setup since you need to produce estimates E_p and E_q to estimate q/p. You have a sentence on p 5 about it. That is sampling is difficult from E_q. Is this the kind of main application you have in mind? \n\n- The regulariser: You enforce smoothness for the ratio q/p. Later in the theorem you got smoothness assumptions on q and p ( which I would say is the natural thing). So I'm wondering if there is any sort of relation between smoothness on q and p and smoothness of the ratio? I guess an answer is here already that you have many different q and p which produce the same ratio; ie just change on countable or on uncountable many points q and p to make it extremely irregular. So it might be more p ,q smooth (possibly bounded away from 0) => ratio smooth (?) or ratio smooth => there exist a smooth p and q which produces the ratio. \n\n- Would be nice to discuss the d dependence of your rates -- for what dimensions would you expect your method to be useful? \n\n- Is there any good motivation for your family of test functions U? \n\n- A more principled question: For this q/p ; covariate shift etc setting one of the main difficulties seems to be to come up with a generic cost function. One approach is to go through expectation operators and throw different test functions at these to extract the underlying densities. Do you feel that you got here \"the right\" approach with the {k(x,.)} ? There is also some similarity to [1] in terms of playing around with transformations to get hold of the quantities of interest. Certainly, your approach for the Radon Nikodym derivative is nicer by working with test functions. But the overall approach has some similarity. \n\n\n[1] Smooth Operators; Grunewalder, Gretton, Shawe-Taylor \n I think it is a timely and strong paper addressing a relevant problem, providing a convergence study and experimental results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
