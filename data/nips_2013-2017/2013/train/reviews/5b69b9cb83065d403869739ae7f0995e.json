{"title": "Low-rank matrix reconstruction and clustering via approximate message passing", "abstract": "We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by formulating the problem of clustering as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd's K-means algorithm.", "id": "5b69b9cb83065d403869739ae7f0995e", "authors": ["Ryosuke Matsushita", "Toshiyuki Tanaka"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The authors present an algorithm for matrix reconstruction under noisy observations. The particular setting looks at low-rank matrices with additional assumptions on the factors and uses an Approximate Message Passing approach in order to speed up the classical, computationally expensive, Bayesian approach. The authors also connect matrix reconstruction with K-means clustering, which is an interesting application domain for the proposed algorithms. \n\nTo the best of my knowledge, the Approximate Message Passing approach for matrix reconstruction is novel and interesting. The connections between low-rank matrix factorizations and K-means are fairly well-known (e.g., PCA provides a factor two approximation algorithm for K-means). However, this allows the authors to provide a nice experimental evaluation of their algorithms and compare them to k-means and k-means ++. Interestingly, their approach seems faster and more efficient than classical k-means and k-means ++ according to their empirical data. The authors compare both the Frobenius norm residual, as well as the actual clusterings, which is a nice feature of their experimental evaluations. \n\nThe main weak point of the paper is that the proposed algorithm comes with few theoretical guarantees in terms of convergence. This is to be expected, since many other algorithms for K-means also have only weak properties. The authors might want to at least cite more papers in Theoretical Computer Science that provide provably accurate algorithms for the K-means objective function. A solid paper presenting Approximate Message Passing algorithms for low-rank matrix reconstruction and k-means. Promising experimental evaluation, somewhat weak theoretical results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Review of \"Low-rank matrix reconstruction and clustering\" \n\nThis paper contributes a new algorithm for low-rank matrix reconstruction which is based on an application of Belief Propagation (BP) message-passing to a Bayesian model of the reconstruction problem. The algorithm, as described in the \"Supplementary Material\", incorporates two simplifying approximations, based on assuming a large number of rows and columns, respectively, in the input matrix. The algorithm is evaluated in a novel manner against Lloyd's K-means algorithm by formulating clustering as a matrix reconstruction problem. It is also compared against Variational Bayes Matrix Factorization (VBMF), which seems to be the only previous message-passing reconstruction algorithm. \n\nCons \n\nThere are some arguments against accepting the paper. Because a new algorithm is being evaluated on a non-standard problem (clustering encoded as matrix factorization), it is not easy to interpolate the experimental results to predict how the algorithm would perform on more conventional matrix reconstruction problems. For instance, two references appear to be cited for VBMF, which are Lin and Teh (2007); and Raiko, Ilin and Karhunen (2007). Both of these papers use the Netflix dataset to evaluate their algorithm against predecessors. It would be ideal if the present paper had used the same dataset. Although BP is usually more accurate than Variational, evaluating the present BP variant using a new criterion creates doubt surrounding its actual competitiveness. Even if Netflix or a similar dataset can't be used, the authors should explain in the paper why this is the case. \n\nThe algorithm itself appears to be a more or less straightforward application of BP to a problem which had been previously addressed with Variational Message Passing. Although new, it is not exactly groundbreaking. The most interesting part of it, to me, is the approximations which are introduced in the limit $N\\to\\infty$ and $m\\to\\infty$, where $m \\times N$ is the dimensions of the input matrix. However, the validity of these approximations, which are only described in supplementary material, is never directly tested, and I think they could be explained a bit more clearly. \n\nThere are some serious problems regarding the citation of prior work. When I first read the paper, I thought that it was introducing the application of matrix factorization to clustering as an original contribution. The text of the abstract and Section 2.2 give this impression. I felt betrayed when I learned from other reviewers that the connection is well-known. I don't see a good reason why the paper would not make this clear to the reader. If it is too well-known to cite any particular reference, then one should just say that it is well-known. Otherwise, cite prior work. \n\nAlso, EP should be cited, since that is usually the name people give to applying BP to continuous classes of distributions, and the relationship with EP to the paper's algorithm should be explained. Relatedly, the main algorithm is most plainly understood as an application of Belief Propagation, but this fact is not mentioned until Section 4.1. It should be mentioned in the abstract. \n\nPros \n\nThe paper was interesting to read, and presents a new and potentially useful algorithm. The mathematics of the paper was possible to follow, and although I did not replicate it by hand, I got the sense that it would be possible to do so. Although I think it is reasonable to be suspicious of new evaluation criteria, the K-means problem may be sufficiently general to give a fair comparison of the algorithms, and certainly shows a benefit for the new algorithm in the experiments. \n\nClarity \n\nI found the early presentation fairly easy to follow. The introduction was clear, as was the summary of earlier work. The fact that the derivation of the algorithm only appears in the Supplemental Material is a drawback. I wish that the derivation could be outlined in the main paper. The experiments section was clear, and although some of the plots show up poorly in black and white, they were still readable. \n\nI found it difficult to understand the description of the algorithm, and I was not able to check the correctness of the derivation. The mathematics was the weakest part of this presentation. Even at a very superficial level it was difficult to parse. For instance, I don't understand why factors of $1 / m \\tau_w$ appear before each term in equations 9a, 9b, 9d, and 9e. These could be factored out, to make the expressions easier to read. Also, the last two terms in 9b and 9e, respectively, could be combined. \n\nThere seem to be an excess of hats, tildes, subscripts, and superscripts. For instance, there is a $\\tau_w$ but no $\\tau$, why not just replace $\\tau_w$ by $\\tau$? Also, the most important Section 4 contains no p, q, or r, but only \\hat{p}, \\hat{q}, \\hat{r} - why not give readers a break and say at the beginning of the section \"We'll be dropping the hats here for brevity\"? And the 't' superscripts in the messages seem to be wholly unnecessary. When the left hand side has a \"t+1\" you just need to change \"=\" to \"\\gets\" and then you can drop all of the t's. I have a hard time imagining that the algorithm was originally derived by the authors using such cumbersome notation. I would suggest going back to the first notation you used and looking to see if it is simpler. \n\nThe exposition could be improved: Why not explain that equation 5 is the negative logarithm of equation 2? Or that equation 8 is just equation 2 to the power of $\\beta$? Algorithm 2 seems to be almost row-column symmetrical, why not point this out? And even make it half as long, by saying, \"then copy and paste these equations, switching u and v\"? \n\nThe meaning of functions in 10 should be explained near their definition. It would be clearer to say \"f_\\beta is the mean of q_\\beta\", rather than giving an equation; but if you decide to give an equation, then why not say what it means? At the end of Section 4.2, it says \"G_\\beta(b, \\Lambda; p) is a scaled covariance matrix of q_\\beta(u; b, \\Lambda, p)\", but this was not obvious at first, and wasn't even mentioned at the definition of G. \n\nNone of the messages have \\beta subscripts, even though they depend on f and G which have \\beta subscripts. But f and G don't depend directly on \\beta, only on q_\\beta. So it's not clear why these subscripts are propagated only as far as f and G and no further. I would suggest eliminating them entirely, even from q. The parameter \\beta is simply a global variable, which is just fine. Before (15) and (16), it will just be necessary to say something like \"in the limit \\beta \\to \\infty, f and G take the following form:\". \n\nOn a deeper level, there were other things about the algorithm that I would like to understand better. What is the significance of the m factor in the additive noise variance? Does it play a role in applications of matrix factorization? Does it play a role in the approximation used to derive the algorithm? It seems to be the only thing making the algorithm fail to be row-column symmetrical, is this true? I think the authors know the answer to these questions, but do not comment on them. \n\nOther questions: \n\nIs it standard to use a tilde to denote column vectors? I didn't understand this at first. \n\nJust curious - why is N capitalized, but m lowercase? \n\nIn section 5, I would like a citation after VBMF-MA to indicate the primary reference guiding the implementation of this competing algorithm. There are two citations for variational matrix factorization appearing earlier in the paper, and it is not clear which is intended (or if it is both). \n\nIn the supplementary material: For equation 5, I think it would be clearer to write a product of exponentials for the two terms involving $z$, to make it more obvious that one is the pdf. For section 2, I had trouble with the step from equation 14 to 15, regarding big-O of $m$ terms. I am not sure if I just need to think harder, but this is where I got stuck. \n\nAlso in the supplementary material, at the top of page 2 it says \"A technique used in ... is to give an approximate representation of these message in terms of a few number of real-valued parameters\". Without reading these references, I am not sure how exactly the approximation described below this text relates to what has been published before. It would be good to clarify the novelty of the algorithm's derivation in the text itself, and even in the main paper. \n\nThe English is very good, and the meaning always gets across, but there are some places where it reads like it was written by someone not entirely skilled in the use of articles. This can be distracting for some readers. Even in the title - I think it should be something like \"Low-rank matrix reconstruction and clustering using an approximate message passing algorithm\". For other places, I will make a list of suggestions which may be of use to the authors: \n\np. 1 \n\n\"Since properties\" -> \"Since the properties\" \n\n\"according to problems\" -. \"according to the problem\" \n\n\"has flexibility\" - \"has enough flexibility\" \n\n\"motivate use of\" -> \"motivate the use of\" \n\np. 2 \n\n\"We present results\" -> \"We present the results\" \n\np. 3 \n\n\"We regard that\" -> \"We see that\" \n\n\"maximum accuracy clustering\" could be italicized \n\n\"Previous works\" -> \"Previous work\" \n\n\"ICM algorithm; It\" -> \"ICM algorithm: it\" \n\np. 4 \n\n\"particularized\" -> \"specialized\" \n\np. 5 \n\n\"plausible properties\" -> \"discernable properties\"? \n\np. 6 \n\n\"stuck to bad local minima\" -> \"stuck in bad local minima\"? \n\np. 7 \n\n\"This is contrastive to that\" -> \"This is in contrast to the fact that\" \n\nThe Supplementary Material also has some language issues, for instance: \n\np. 2 \n\n\"of these message\" -> \"of these messages\" \n\n\"a few number\" -> \"a few\" \n\nI liked the diagram in Figure 1 in the Supplementary Material, which I found very helpful. If it is possible to create more diagrams, that would add to the paper. \n The mathematics seems interesting, and the algorithm should be published somewhere - as the first effort to apply belief propagation to matrix factorization, it fills a certain important role. But the paper needs more work before it can be accepted. The paper is sufficiently lacking in clarity and scholarship as to put an unacceptable burden on readers, which would reflect poorly on a conference which accepted it in its present state.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes an approximate message passing algorithm for matrix factorization. After showing that the matrix factorization problem can be seen as a generalization of clustering problem, the message passing algorithm derived for matrix factorization is specialized to clustering. Lastly, experiments were conducted to compare the proposed algorithm with the k-means++ algorithm. \n\nQuality: Mathematical derivations in this paper are very sketchy and omits many non-trivial steps even in appendix. \n\n1) In line 139 of Appendix, why is the second term of (11) O(1/\\sqrt(m))? \\beta seems to be omitted in Section 2. \n2) Some of the analysis, including derivation of (19), seems to assume that m and n grows in the same order. After all, asymptotic analysis in this section are a bit terse and I am concerned about its mathematical rigor. \n3) In line 167, why is m \\tau_w^2 the expectation of A_{il}^2? A is observed data, and m\\tau_w^2 is only variance of the likelihood; the rational for such approximation should be verified. \n4) How would the message passing algorithm derived for finite \\beta converge to the MAP problem? Message passing algorithm minimizes the KL divergence, but \\lim_\\beta \\min KL = \\min \\lim_\\beta KL does not necessarily hold. \n5) The proof of Proposition 1 is too brief for me. How is (17) derived? \n\nClarity: The paper is well-structured and it is not difficult to grasp what is the main point of the paper. \n\nOriginality: Since authors are mostly concerned about estimating first and second moments of the marginal posterior, isn't the whole algorithm just application of Expectation-Propagation (EP) to matrix factorization? EP was already used in matrix factorization in the following paper: http://research.microsoft.com/pubs/79460/www09.pdf Section 1 of Appendix seems to be standard EP updated procedures; please correct me if I am wrong. \n\nAlso, it seems that there should be an interesting connection between the clustering algorithm proposed by authors and the clutter problem of original EP paper: http://research.microsoft.com/en-us/um/people/minka/papers/ep/minka-ep-uai.pdf , if the likelihood of clutter problem is switched to the uniform mixture of gaussian distributions. Just to clarify, in this paragraph I am just suggesting a connection and not attacking the originality of the paper. \n\nSignificance: Authors mainly focus on its application to clustering, but considering the vast amount of literature on clustering, comparing only to k-means++ may not be sufficient to prove the practical usefulness of the algorithm.  Proofs in this paper, even in appendix, are very sketchy and thus it is hard to evaluate its technical correctness. Also, I suspect that it is an application of expectation-propagation to matrix factorization problem, which was already done by Stern et al. http://research.microsoft.com/pubs/79460/www09.pdf", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
