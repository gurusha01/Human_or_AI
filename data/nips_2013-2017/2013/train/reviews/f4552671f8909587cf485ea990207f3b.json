{"title": "Noise-Enhanced Associative Memories", "abstract": "Recent advances in associative memory design through structured pattern sets and graph-based inference algorithms have allowed reliable learning and recall of an exponential number of patterns. Although these designs correct external errors in recall, they assume neurons that compute noiselessly, in contrast to the highly variable neurons in hippocampus and olfactory cortex.  Here we consider associative memories with noisy internal computations and analytically characterize performance. As long as the internal noise level is below a specified threshold, the error probability in the recall phase can be made exceedingly small. More surprisingly, we show that internal noise actually improves the performance of the recall phase. Computational experiments lend additional support to our theoretical analysis. This work suggests a functional benefit to noisy neurons in biological neuronal networks.", "id": "f4552671f8909587cf485ea990207f3b", "authors": ["Amin Karbasi", "Amir Hesam Salavati", "Amin Shokrollahi", "Lav R. Varshney"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "Classic work on associative memories following Hopfield 1982 focused on issues of capacity and performance, usually considering random memories embedded as stable attractors of a dynamical system. Such work usually led to capacities which scale linearly with the size of the network. The present work proposes a neural architecture which is able to reach exponential capacities at the cost of introducing specific low-dimensional structure into the stored patterns. \nThe authors propose a bi-partite architecture of pattern and constraint neurons corresponding to patterns and clusters, and a two-tiered algorithm, based on within and between-module processing aimed at retrieving clean versions of noise corrupted the patterns. The intra-module algorithm operates iteratively based on forward and backward iterations, based on a belief variable. The inter-module algorithm is based on iterations taking place until all constraints are satisfied. The authors present theoretical results relating to convergence, showing that there is a noise range for the internal dynamics, which leads to potentially improved performance relative to noise-free dynamics. Simulation results seem to corroborate these theoretical claims, also demonstrating an interesting threshold effect as a function of the noise parameter. \nThe results presented here are interesting and, to the best of my knowledge, novel. The fact that noise improves performance in such a nonlinear dynamical system is indeed surprising and non-trivial. While the authors have tried to present some intuition about their results (the proof of which appears in the appendix), I was not able to get a good feeling for \u2018what makes the system tick\u2019. \nSpecific comments: The authors do not seem to be aware that a great deal of early work on associative memories dealt with stochastic retrieval dynamics, as well as in structured patterns leading to higher capacities. AN early review of many of these results can be found in Daniel Amit\u2019s \u201cModeling Brain Function\u201d, and later work. In fact, the advantageous effect of noisy dynamics on improving retrieval by eliminating so-called spurious states was noted there. \nAlthough the authors have shown that noise may assist in improving retrieval, it would be nice to understand the robustness of this result. For example, what would happen if the weights W_{ij} were corrupted by noise? Such robustness had been shown in early neural network studies (again, see Amit\u2019s book). \nFinally, the dynamics given by eq. (2) and (3) seem to be at odds with the requirement that the patterns be restricted to a fixed finite range {0,\u2026,S-1}. How is this guaranteed by this update rule? \n A work which suggests how to increase the capacity of associative memories exponentially at the price of introducing specific structure into the stored patters. The authors do not relate to earlier work on the effect of noise, but present an interesting result on possible merits of noise.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "An associative memory mechanism for pattern completion of non-binary integer valued data is proposed for which the authors claim that the number of stored patterns can be exponential in the number of neurons. The main result of this paper is an analysis of the performance of this memory model in the presence of intrinsic neural noise. Interestingly it is shown by the analysis and simulations that intrinsic noise can increase the performance of the model. \n\nComments: \n\n1) It would be important to add results supporting that the model can store exponentially many patterns. Either theoretical capacity results or simulation curves how recall error behaves as a function of the number of stored patterns. \n\n2) To make the paper stand-alone, it would be important to describe the learning algorithm from [10] at least the main principle \n\n3) For integer-valued data it is somewhat artificial to confine the (external) additive noise to be -1, 0 or 1. Can the model handle larger additive errors? \n\n4) The discussion stresses the neurobiological relevance of this model. You also should discuss that many elements of your algorithm are not biologically plausible, e.g., the negative activity in the control neurons, the algorithm loops Alg1 and Alg 2 etc. \n\n5) It would be important in the final version of the paper to include the essential proof lines from the appendix. \n\n6) To gain space, you could omit the 3D figure, Fig 4, which is redundant with Fig. 5. \n\n\n An associative memory mechanism for pattern completion of non-binary integer valued data is proposed for which the authors claim that the number of stored patterns can be exponential in the number of neurons.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "SUMMARY \n\nIn this article, the authors study a model for associative memory where the patterns lie in some low-dimensional manifold. The structure of the patterns is captured by the so-called \"constraint-neurons\". Those constraint-neurons together with the pattern neurons form the memory network. The authors propose a recall algorithm for this network and show that intrinsic noise actually improves recall performance. \n\nThe paper is well written and as far as I can judge, it is technically sound. \n\nMAJOR COMMENTS \n\n1. Biological plausibility. The paper starts with considerations on associative memory and the brain. However, the associative memory model seems very distant to any biological implementation. \n- For example, how would step 4 in Algorithm 2 be implemented in some biologically realistic network? This step assumes that some memory should be kept over the tmax iterations of Algo 1 (c.f. Step 3 in Algo 3). \n- How should we think of the integer-valued state of the neuron (are states spike vs non-spike? this binary code (S = 2) would be different from the code in the constraint neurons.) \n\n\n2. As mentioned by the authors, the architecture is similar to the one of Restricted Boltzman machines (RBM), the pattern neurons being the visible neurons and the constraint neuron being the hidden neurons. Now, with RBM, neurons are also intrinsically noisy and the level of intrinsic noise is adapted (through synaptic strength) such that the marginal distribution over visible units is consistent with the data. So I would also expect that the performance would be poorer with RBM if we forced neurons to be deterministic. Therefore, it is not clear to what extend it is a breaking news that intrinsic noise is good. Furthermore, the authors state that Deep Belief Nets (and therefore RBM as well) are used for classification and not pattern completion, but this is not correct. RBM can be used for pattern completion. \n\n\n3 Other approaches to associative memories have been proposed. For example in [R1], Lengyel et al. see recall as a Bayesian inference process which exploits knowledge about the structure of the stored patterns (prior) and the knowledge about the learning rule as well as knowledge about the corrupting process of the inputs (the likelihood). In this perspective, the posterior distribution is a deterministic function of the prior and the likelihood and does not require additional noise. So how is it possible that the \"best\" recall rule (in a Bayesian sense) does not need noise (except if one want to sample from the posterior distribution) and the recall rule presented here needs noise? \n\n[R1] Lengyel, M., Kwag, J., Paulsen, O., & Dayan, P. (2005). Matching storage and recall: hippocampal spike timing\u2013dependent plasticity and phase response curves. Nature Neuroscience, 8(12), 1677\u20131683. \n\n The paper is well written and as far as I can judge, it is technically sound.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
