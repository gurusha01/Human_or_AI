{"title": "More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server", "abstract": "We propose a parameter server system for distributed ML, which follows a Stale Synchronous Parallel (SSP) model of computation that maximizes the time computational workers spend doing useful work on ML algorithms, while still providing correctness guarantees. The parameter server provides an easy-to-use shared interface for read/write access to an ML model's values (parameters and variables), and the SSP model allows distributed workers to read older, stale versions of these values from a local cache, instead of waiting to get them from a central storage. This significantly increases the proportion of time workers spend computing, as opposed to waiting. Furthermore, the SSP model ensures ML algorithm correctness by limiting the maximum age of the stale values. We provide a proof of correctness under SSP, as well as empirical results demonstrating that the SSP model achieves faster algorithm convergence on several different ML problems, compared to fully-synchronous and asynchronous schemes.", "id": "b7bb35b9c6ca2aee2df08cf09d7016c2", "authors": ["Qirong Ho", "James Cipar", "Henggang Cui", "Seunghak Lee", "Jin Kyu Kim", "Phillip B. Gibbons", "Garth A. Gibson", "Greg Ganger", "Eric P. Xing"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper describes a bounded-staleness design for a parameter server for iterative distributed learning based on vector clocks. The idea is clear, intuitive and provides a nice parameterized range between the previous designed for pure asynchronous updates (e.g., Hogwild or Yahoo LDA) and approaches that have a hard synchronization barrier (e.g., most recent SGD papers). While there are no breaking theoretical insights, the paper correctly adapts the analysis from [17]. THe paper is very well-written, and the SSPtable design description is exemplary. \n\nThe experimental results are convincing, with computation-vs-communication and clocks/worker/time results particularly encouraging . A few areas for improvement/details that would help: \n- Lasso results are painfully limited to a toy synthetic dataset -- a more comprehensive evaluation on it would be quite useful. \n- A direct comparison with state-of-the-art distributed learning packages (Yahoo LDA, Hogwild) would be illustrative. \n- There should be a single-worker baseline. \n- How exactly is data partitioned? This should be mentioned. \n\nA couple suggestions: \n- For matrix factorization, could the idea be combined with Gemulla et al's approach for sub-epochs that operate on non-overlapping parameter blocks sequentially? \n- It would be helpful to discuss the possibility of forcing the slowest stragglers to sync with an incomplete iteration if that prevents blocking for current staleness level. \n The paper is well-written, describes an intuitive idea, and provides convincing experimental results with some relatively minor areas for improvement.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents an approach to building a parameter server for distribute ML systems that presents a view to each client where parameters have a bounded degree of staleness. Using a combination of caches, the client interface guarantees that all updates to the parameter array occurring after a fixed deadline (the current clock/iteration/tick minus a fixed delay) are visible along with more recent updates if possible. Thus the interface presents a view of parameters that integrates most updates along with best-effort service for more recent updates. It is shown that this simple semantic preserves the theoretical guarantees of cyclic delay methods while being significantly faster in practice. Empirical analysis on several problems with multiple cluster configurations show that the advantage is due to a combination of increased efficiency (over BSP) and optimization progress per update (over Asynchronous). \n\nThis paper presents a simple set of semantics that improve on \"parameter server\" schemes currently showing up in large scale machine learning applications. Along with straight-forwardly carrying over simple theoretical guarantees, the method is apparently significantly faster for reasonably-sized tests than the obvious competitors and on those points alone I think this is pretty positive. \n\nThere are a lot of well-known problems in loaded clusters (stragglers, etc.) and as far as I can tell this approach should deal with them well. A saving grace may be that the caching mechanism reduces overhead dramatically for the slowest nodes thus giving a degree of \"load balancing\" that is annoying hard to get in other ways. Is it possible to show that this is actually happens to a degree sufficient to enable the \"catch up\" phenomenon claimed in the paper? \n\nSome discussion of the read semantics is given with details on the cache policy (which just falls through when the local cache is stale). Due to the read-my-writes policy, is it correct that all local writes are written through to all caches/server? A sentence of clarification on the writing semantics might help for reproduction of the work. \n\nAs far as systems papers go the authors make a good case for the approach. \n\nPros: \nFairly simple semantics. \nPreserves basic theoretical guarantees for typical methods (e.g., SGD). \nExperimental analysis shows actual speedups on multiple distributed problems and the speedup comes from the sources expected. \n\nCons: \nA comparison to cyclic delay would be nice. \nPotentially complex to re-implement in comparison to asynchronous/BSP; is it worth it for the moderate speedup? \n\n**Update \nThanks to the authors for their clarifications and additions. I think these are reasonable arguments and resolve my [very limited] qualms. A framework for parameter servers in machine learning applications is presented with moderate speedups and appealing theoretical and practical properties.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes Stale Synchronous Parallel(SSP) computation for machine learning. It also discusses an implementation thereof and, most importantly, provides the first discussion of its relationship to machine learning quantities. The latter analysis is done for SGD. Comprehensive experiments show the empirical validity of the approach. \n\nThe paper is very well written and easy to follow. It has been a pleasure to read and review. Thanks for that! \n\nWhile my overall verdict is to accept the paper, I'd like point out a few things that might improve the clarity of exposition and the embedding into the related work. To my mind, the main contribution of the work is not to _suggest_ to use SSP for machine learning. That (arguably somewhat obvious) connection has been made before, if only amongst practitioners in the field and not in the academic discourse. \n\nThe key contribution in my mind is to (a) have done it with rigor and (b), more importantly, to provide analysis that connects systems level parameters like stale-ness with machine learning concepts such as convergence of the learner. If at all possible, more space should be devoted to that analysis, as there is a suggestion in the paper that space was a consideration when leaving examples beyond SGD out. Please add them to the paper or to the appendix. \n\nLastly, some nitpicking: The statement that SSPtable is more general than Hadoop on page 7, Line 376 is a bit naive in the following sense: SSPtable is optimized for small incremental updates, while Hadoop can deal with massive objects being aggregated. Also, Hadoop has a (rather good) shuffle phase, while SSPtable doesn't need one. I'd thus suggest to weaken that statement a bit as it does stand out a bit in an otherwise clean paper. Excellent paper that makes the connection between SSP and machine learning. The presentation and content are great, an easy accept.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
