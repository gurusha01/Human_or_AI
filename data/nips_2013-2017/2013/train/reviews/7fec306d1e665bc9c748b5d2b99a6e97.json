{"title": "Predicting Parameters in Deep Learning", "abstract": "We demonstrate that there is significant redundancy in the parameterization of   several deep learning models.  Given only a few weight values for each feature   it is possible to accurately predict the remaining values.  Moreover, we show   that not only can the parameter values be predicted, but many of them need not   be learned at all.  We train several different architectures by learning only   a small number of weights and predicting the rest.  In the best case we are   able to predict more than 95% of the weights of a network without any drop in   accuracy.", "id": "7fec306d1e665bc9c748b5d2b99a6e97", "authors": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc&#x27;Aurelio Ranzato", "Nando de Freitas"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "Summary: \n\n\"Predicting Parameters in Deep Learning\" explores the hypothesis that there is \nsignificant redundancy in the naive (and universally used) representation of \nthe parameters of neural networks. \n\nPro: \n* Interesting observation and interpretation \n* Experiments show promising evidence \n\nCon: \n* Empirical results do not support central claims \n* Missing comparisons with common weight matrix factoring like PCA preprocessing \n\n\nQuality: \n\nAt a few points in the paper the authors remark on how difficult it is to \ntrain with a parameter matrix W factored as a product UV. Could the authors \noffer some intuition for why? One reason might be that if U = V = 0 at the outset, \nabsolutely naive backprop will not move them. On the other hand if W is \ntrained normally but simply constrained to be a product of some U and V, some \ncomputational overhead during updates can be traded for lower communication \ncost. \n\nA more fundamental concern is that the paper's central observation that \nweights are redundant, together with the use of a spatial exponential kernel \nsuggest that similar gains could be had by either doing a PCA of the input, or \ndownsampling the images and the kernels for convolution. These are both common \npractice. PCA pre-processing also represents a low-rank factorization of the \nfirst layer's weights. \n\nThe technique of maintaining a PCA of intermediate layers is not common practice, but \nit might be helpful, and it certainly represents a conceptual starting point for your \nwork on \"data driven expander matrices\". \n\nThe discussion of columnar architectures and convolutional nets seems \nreasonable, but largely hypothetical. Pointing out various ways to \nrefactor matrices in various architectures without more extensive empirical \nfollow-through seems like a departure from the core thesis of the paper. \n\nThe empirical aspect of this paper is important, and I think it needs more \nwork. The claim that the authors test is that \"there exists models whose \nparameters can be predicted\", but that is a weak claim. The claim the authors are aiming for \nfrom the tone of their introduction and which I agree they *should* be aiming for is that: \n\"there exist large models that work better than naively down-sized versions \nand using our techniques we do less damage to the large models than simply \ndown-sizing them\". This second point is the central claim of the text of the \npaper, but it is not supported by empirical results. \n\nThe claim that parallel training can actually be accelerated by the use of \nthese techniques also requires more justification. The proposed method \nintroduces a few new sorts of computational overhead. Particular \nencoder/decoder algorithms for transmitting and rebuilding weight matrices \nwould need to be implemented and tested to complete this argument. \n\n\nClarity: \n\n\"collimation\" - mis-used word; it is actually a word, but not about making columns \n\n\"the the\" \n\n\"redundancy\" is probably the wrong term to describe the symmetry / equivalence \nset around line 115. \n\nSection 2 \"Low rank weight matrices\" is a short bit of background material, it \nshould probably be merged into the introduction. \n\nFigure 1 is only referenced in the introduction, but it actually illustrates \nthe method of section 3.2 in action right? It would be clearer if Figure \n1 were put into Section 3.2. \n\nPlease number your equations, if only for the sake of your reviewers. \n\n\nOriginality: \n\nThe idea of looking at weight matrices in neural networks as a continuous \nfunction has precedent, as does the observation that weights have redundancy, \nbut the idea that training could be accelerated by communicating a small fraction of \nrandomly chosen matrix values is new. \n\n\nSignificance: \n\nAs the authors point out in their introduction, this work could be very \ninteresting to researchers trying to parallelize neural network training across \nvery low-bandwidth channels. \n\n\nEdit: After reading the authors' rebuttal I have raised my quality score. I apparently did not understand the discussion of columns, and how it related to the experimental work. I hope the authors' defense of their experimental work is included into future revisions. The paper has some good ideas and gets you thinking, but the empirical resultsdo not really support the most important and interesting claims. The algorithmfor actually accelerating parallel training is only sketched.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presented a reduction approach to reducing the size of deep models, in order to improve the training efficiency of deep learning. The main contribution is: (1) the exploration of prior knowledge for the redundancy of deep models, such as spatial parameter smoothness for images; (2) the use of kernel ridge regression for parameter interpolation from a subset; \n\nThe paper is clearly written. The work seems to be original. \n\nThe authors claimed the approach should be very general, i.e., even applicable to non-image tasks. They described some extension of the methods to handle non-image data, such as autoencoder pertaining. But in the experiments there is nothing for non-image datasets. Therefore the point is very weak. \n\nAll the experiments on images are on pretty small datasets with simpler patterns. It's hard to believe any methods that are good for data like CIFAR will be supposed good for more realistic datasets such as ImageNet. Therefore the value of this work is not entirely convincing. \n\nI have read the authors' rebuttal. I don't think I would change my recommendation.  This work is novel, with some quite interesting ideas to reduce the size of deep networks. The value of the work has not been convincingly demonstrated in the experiments.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Motivated by recent attempts to learn very large networks this work proposes an approach for reducing the number of free parameters in neural-network type architectures. The method is based on the intuition that there is typically strong redundancy in the learned parameters (for instance, the first layer filters of of NNs applied to images are smooth): The authors suggest to learn only a subset of the parameter values and to then predicted the remaining ones through some form of interpolation. The proposed approach is evaluated for several architectures (MLP, convolutional NN, reconstruction-ICA) and different vision datasets (MNIST, CIFAR, STL-10). The results suggest that in general it is sufficient to learn fewer than 50% of the parameters without any loss in performance (significantly fewer parameters seem sufficient for MNIST). \n\nThe paper is clear and easy to follow. The method is relatively simple: The authors assume a low-rank decomposition of the weight matrix and then further fix one of the two matrices using prior knowledge about the data (e.g., in the vision case, exploiting the fact that nearby pixels - and weights - tend to be correlated). This can be interpreted as predicting the \"unobserved\" parameters from the subset of learned filter weights via kernel ridge regression, where the kernel captures prior knowledge about the topology / \"smoothness\" of the weights. For the situation when such prior knowledge is not available the authors describe a way to learn a suitable kernel from data. \n\nThe idea of reducing the number of parameters in NN-like architectures through connectivity constraints in itself is of course not novel, and the authors provide a pretty good discussion of related work in section 5. Their method is very closely related to the idea of factorizing weight matrices as is, for instance, commonly done for 3-way RBMs (e.g. ref [22] in the paper), but also occasionally for standard RBMs (e.g. [R1], missing in the paper). The present papers differs from these in that the authors propose to exploit prior knowledge to constrain one of the matrices. As also discussed by the authors, the approach can further be interpreted as a particular type of pooling -- a strategy commonly employed in convolutional neural networks. Another view of the proposed approach is that the filters are represented as a linear combination of basis functions (in the paper, the particular form of the basis functions is determined by the choice of kernel). Such representations have been explored in various forms and to various ends in the computer vision and signal processing literature (see e.g. [R2,R3,R4,R5]). [R4,R5], for instance, represent filters in terms of a linear combination of basis functions that reduce the computational complexity of the filtering process). \n\nIn the experimental section the authors make an effort to demonstrate that the proposed method is practically useful and widely applicable, considering multiple datasets and several different architectures. I think, however, that this section could have been stronger in several ways: \n\nFirstly, it would have been nice if the paper had not focused exclusively on vision applications, and had put generally more emphasis on scenarios where there is a less obvious topolgy in the data that can be exploited when predicting weights (which will pose more of a challenge to the method). The data-dependent kernel is not very well evaluated. \n\nSecondly, especially for the vision case, I am wondering why the authors are limiting themselves to the particular set of basis functions derived from the kernel regression view. It would seem natural to explore other linear basis representations of the filters (a simple choice would be a PCA basis, for instance). These might be more efficient in terms of reducing the number of free parameters, and might have other desirable properties (e.g. [R4,R5]). \n\nFinally, I would have hoped for a truly compelling experimental use case demonstrating the impact of the approach in practice. Since the work is motivated as a way of reducing computational and memory complexity, I think it would have been useful to include a more detailed discussion and empirical demonstration how the reduction in number of learned parameters translates into such savings and consequently allows the training of larger networks that achieve better performance than otherwise possible. At the moment, the evaluation appears to be focused on moderately large networks, and the authors show that a moderate reduction in parameters can be achieved that way, without loosing performance (for MNIST the potential reduction in parameters seems to be very large, but for the more interesting CIFAR / STL-10 datasets it seems that at least 40% of the parameters are required to avoid a loss in accuracy). Why is computational complexity and speed of convergence not considered at all in the evaluation? I think that Fig. 6-right (which plots performance against # of free parameters for a standard network and a reduced parameterization) goes in the right direction, but it seems that there is really only a clear advantage of the reduced parameterization for CIFAR (much less so for STL), and I'm wondering whether the performance difference on CIFAR would disappear even for currently realistic network sizes. \n\nAll in all, I think the paper takes an interesting perspective although related approaches have appeared in the literature in various guises (see discussion above). I think that the ideas can have practical impact, and to my knowledge, they are currently not widely used in the NN/deep learning community. A stronger case could probably be made, however, by further exploring alternative implementations of the idea and by having a more compelling demonstration of the effectiveness and versatility of the approach. \n\nFurther remarks: \n\n** It would have been really helpful to include state of the art results from the literature for the datasets / and specific evaluation regimes considered. This would put the reported performance into perspective and make it easier to judge whether savings in learned parameters can be achieved for competitive architectures. \n\n** I find the evaluation of the data-dependent kernel somewhat limited: it is only explored for a single dataset / architecture combination (the MNIST, MLP experiments in section 4.1). As mentioned above, it would have been nice to inlcude some other type of data that requires the use of the empirical kernel. \n\nFor the experiments in section 4.1 it would have further been useful to also consider the case SE-rand. At the moment it's not clear that the learned kernel really outperforms the random approaches. The difference might simply be due to the fact that for the empirical kernel you're using the SE kernel in the first layer. Another interesting control for the effectiveness of the empirical kernel would be to have a setting where you're using the empirical kernel in both layers. \n\n** Do you have any insights as to why the naive low-rank scheme, where both matrices are learned, is working so poorly? Have you made any attempt to remove the redundancy inherent in the parameterization? Would it be possible (and potentially advantageous) to start of with a fixed U (e.g. the empirical kernel), but allow U to change later at some point, possibly in an optimization scheme where U and V are updated in an alternating manner? \n\n** In your conv-net experiments I suppose you are not using any pooling layers as described in [18]? This would also reduce the dimensionality of the representation in the higher layers. Furthermore, based on Fig. 5 I am wondering whether you can really argue that predicting 75% of the parameters has negligible impact on performance: when you're predicting 50% of the parameters the loss seems to be about 5-6 percentage points which does not seem so negligible to me (I guess it would help to have some errorbars here). \n\n** How did you optimize the hyperparameters (learning rate etc.) for your comparisons? Shouldn't the kernel width vary with the size of the subset of learned parameters alpha? \n\n** In section 4.1, for the MLP experiments, how do the columns differ? Do they use the same empirical kernel and differ only in the set of sampled pixels? Is the great advantage of using multiple columns here due to the fact that otherwise you don't sample the space well? \n\n** Fig. 6 right: I find it curious that the reduced parameterization is advantageous for for CIFAR but there seems to be very little difference for STL - given the higher resolution of STL wouldn't one expect the opposite? \n\n** For conv-nets where, in the higher-layers, there are many input channels relative to the spatial extent of each filter, would it be possible to use a data-dependent kernel to subsample across channels (if I understand correctly you are currently performing the subsampling only spatially, i.e. the elements of alphas are tied across channels)? \n\n** Is there a way to extend your scheme such as to start off with a small fraction of learned parameters, but to then increase the fraction of learned parameters slowly until a desired performance is reached / no further improvement is observed? \n\n\n\nReferences: \n\n\n[R1] Dahl et al., 2012: Training Restricted Boltzmann Machines on Word Observations \n\n[R2] Roth & Black, IJCV, 2009: Fields of Experts (Section 5.3) \n\n[R3] Freeman & Adelson, 1991: The design and use of steerable filters \n\n[R4] Rubinstein et al., 2010: Double Sparsity: Learning Sparse Dictionaries for \nSparse Signal Approximation \n\n[R5] Rigamonti et al., 2013: Learning Separable Filters \n The paper proposes a way of reducing free parameters in NN and I think this can be a useful practical contribution. There is, however, some similarity to existing work, and a stronger case for the approach could probably be made by considering alternative implementations and providing a more compelling evaluation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
