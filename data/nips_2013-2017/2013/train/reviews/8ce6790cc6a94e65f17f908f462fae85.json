{"title": "A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables", "abstract": "We address the problem of learning a sparse Bayesian network  structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure  must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the first stage and then searches for a network structure that satisfies the DAG constraint in the second stage. Although this approach is effective in a low-dimensional setting, it is difficult to ensure that the correct network structure is not pruned in the first stage in a high-dimensional setting.  In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal  sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efficiency of the well-known exact methods  based on dynamic programming. We also present a heuristic scheme that further improves the efficiency of A* lasso without significantly compromising the quality of solutions and   demonstrate this on benchmark Bayesian networks and real data.", "id": "8ce6790cc6a94e65f17f908f462fae85", "authors": ["Jing Xiang", "Seyoung Kim"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper introduces a method for finding Bayesian networks for continuous variables in high-dimensional spaces. The paper assumes a Gaussian distribution of any particular random variable when conditioned on its parent nodes. A LASSO objective function is used to construct a sparse set of parent nodes for each random variable, subject to an additional constraint that the resulting structure be an acyclic graph. The network structure constraint is framed as an ordering problem, and an A* search algorithm is proposed which finds a directed acyclic graph which maximizes the LASSO objective function. The LASSO objective function, minus the DAG constraint, is used as an admissible heuristic in the A* search. This search is still not fast enough for graphs with large numbers of nodes, so a method for further pruning the search space is introduced. \n\nThe paper is written clearly, with clear motivation and sufficient detail for implementation by a third party. \n\nThe use of the LassoScore for optimization in the dynamic programming algorithm, and using the unconstrained LassoScore as a heuristic function, is clever. The simulation studies adequately show that the approximate version of their algorithm still greatly outperforms conventional greedy approaches, while exhibiting fast runtimes even on large networks of nodes. The performance graphs indicate that even substantial pruning the search graph represents only minimal losses relative to the optimal solution. \n\nWhile the paper describes the process as learning network structure for continuous variables, using a linear regression model with fixed noise error (section 2.1) for the conditional distributions would seem to suggest assuming a network in which each random variable is itself linear Gaussian. Offhand this appears to be a fairly strong assumption; however, as noted in the rebuttal, recovering structure is a difficult problem even in Gaussian networks. \n\nThe application described in section 3.1 is not very clear, and it is not described how stock price data is represented as a Bayesian network. This reviewer is not familiar with the use of Bayesian networks for stock market analysis, and it is not immediately clear to the uninitiated what sort of model is used, or why stock market data would exhibit a particular network structure. This is addressed by references in the rebuttal, which should be added to section 3.1. \n\n This is a nice paper, which exploits the relationship between a DAG structure and an ordering of random variable dependencies to implement a clever A* algorithm for optimizing a constrained LASSO objective function; the resulting algorithm performs quickly and accurately, uncovering network structure in the synthetic data generated from this model.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors put forward a new class of approaches for learning sparse Bayesian Networks. Their first method is a DP approach that integrates the Lasso as a scoring method. Since this approach requires searching over a prohibitively large space, they modify it with an A* search (that still uses the Lasso scoring method). This method uses a heuristic function that is admissible and consistent (and thus is guaranteed to find the optimum and allows pruning). Their final method is an approximation of A* that works by limiting the queue size. While very small queues degrade the quality of solution, they find that for moderate limits, great speed improvements are possible with little degradation of quality. \n\nThe authors note that many other methods prune the search space in a first step and then find the best DAG in this reduced space as a separate step. This practice may exclude the optimum. Therefore, their approach seems like a good way to avoid doing this. \n\nThe experimental section seems pretty thorough. \n\nThis paper is well-written and well-organized. \n\nIn Figure 3, what was varied to generate the curves? Lambda? I ask because the non-monotonicity in the Hailfinder 2 plot is a little surprising. Also, does SBN really drop below the \"random guessing\" line in four places? This might indicate something fishy. \n\nMinor comments: \n- Line 21: Missing space: \"two-stageapproach\" \n- Line 130: \"U\\in V\" should be \"U\\subseteq V\" \n- Line 140: empty citation? [] \n- Lines 157 and 175: \"and and\" \n- Line 169 and 171: Don't start both with \"On the other hand\" \n- Line 180: heuristic misspelled \n- Line 258: CLOSE should be CLOSED \n- Line 356: least square misspelled \n- Line 357: no \"a\" before prediction \n- Line 430: Not a sentence. This paper presents a promising set of methods for learning Bayesian Networks through an A* search with Lasso-based scoring function.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper considers the problem of learning sparse bayesian networks. \nThe paper follows the literature in addressing the problem through a dynamic programming based approach to finding the optimal ordering to determine the network \nand learn the parameters of the distribution. \n\nThe contributions of the paper seem to be the use of a consistent and admissible heuristic inside A* search. The paper also proposes heuristic schemes to improve scalability of the DP A* search based approach. \n\nThe scalability gains are observable for small data sets in the numerical experiments. However, the complexity of the algorithm in the worst-case still seems exponential. So, for problems with large number of nodes in the network, the proposed algorithm doesn't seem practical. On the other hand, the SBN algorithm of Huang et al \navoids DP and guarantees to find the optimal solution in polynomial time. \nThe SBN algorithm is significantly slower than the algorithm in the paper. \nHowever, the SBN algorithm comes with a polynomial time guarantee and the algorithm in the paper, though fast in experiments, is based on heuristics without guarantees. \n\nIn summary, the paper proposes fast heuristics inside a DP approach to learn sparse bayesian networks. However these heuristics don't come with any guarantees of optimality and the overall algorithm is also not guaranteed to be polynomial time. \nThe algorithm in the paper would be attractive if there are some theoretical results \nto back up the performance of the fast heuristics in the algorithm. Also, the experiments could be more convincing when the algorithms are compared on data sets with large number of nodes. The paper proposes a DP-based approach with fast heuristics embedded that are consistent and admissible. While the algorithm is empirically faster than state of the art, the heuristics used for scalability in the algorithm doesn't come with any theoretical guarantees.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
