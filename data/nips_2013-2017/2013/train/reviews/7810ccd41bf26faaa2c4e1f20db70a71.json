{"title": "\u03a3-Optimality for Active Learning on Gaussian Random Fields", "abstract": "A common classifier for unlabeled nodes on undirected graphs uses label propagation from the labeled nodes, equivalent to the harmonic predictor on Gaussian random fields (GRFs). For active learning on GRFs, the commonly used V-optimality criterion queries nodes that reduce the L2 (regression) loss. V-optimality satisfies a submodularity property showing that greedy reduction produces a (1 \u2212 1/e) globally optimal solution. However, L2 loss may not characterise the true nature of 0/1 loss in classification problems and thus may not be the best choice for active learning. We consider a new criterion we call \u03a3-optimality, which queries the node that minimizes the sum of the elements in the predictive covariance. \u03a3-optimality directly optimizes the risk of the surveying problem, which is to determine the proportion of nodes belonging to one class. In this paper we extend submodularity guarantees from V-optimality to \u03a3-optimality using properties specific to GRFs. We further show that GRFs satisfy the suppressor-free condition in addition to the conditional independence inherited from Markov random fields. We test \u03a3-optimality on real-world graphs with both synthetic and real data and show that it outperforms V-optimality and other related methods on classification.", "id": "7810ccd41bf26faaa2c4e1f20db70a71", "authors": ["Yifei Ma", "Roman Garnett", "Jeff Schneider"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "A new heauristic criterion is provided for active label acquisition for the Gaussian Random Field (GRF) model for semi-supervised classification. The authors show that so-called E-optimal criterion minimizes the survery risk as described by Garnett et al. I very much enjoyed the nice theoretical properties (sub-moudlarity, leading to near optimality guarantees for greedy applications; suppresor free conditions) and insights from analysis (cauchy scwartz inequality based arguments indicate that the criterion is likely to prefer graph cluster centers as opposed to outliers). Experimentally the proposed method outperforms most of the obvious alternatives. \n\nOverall a very nice paper, with sufficient novelty, theoretical rigor, clarity of explanation and reasonable experiments for a good NIPS paper A new heauristic criterion is provided for active label acquisition for the Gaussian Random Field (GRF) model for semi-supervised classification. Overall a very nice paper, with sufficient novelty, theoretical rigor, clarity of explanation and reasonable experiments for a good NIPS paper", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors suggest the use of a criterion, \u03a3-Optimality, for active \nlearning in Gauss-Markov random fields. The criterion itself was \noriginally proposed by Garnett et al for active surveying, but it does \nnot appear that the submodular property was recognized in that \nprevious work. \n\nLabeled and unlabeled are embedded in a graph nodes represent both \nlabeled and unlabled data and edge weights, computed via a kernel, \ncapture similarity. The motivation for an active approach is that \nacquiring labels on the full data set may incur some cost (presumably \ngreater than computing the edge weights over all data) so a criterion \nis used to determine which of the remaining unlabeled data should be \nlabeled. The authors establish that the criterion satifies the \nsubmodular monotone property and as such greedy selection achieve \n(1-1/e) performance relative to optimal selection. The authors should \nbe careful to note that this optimality is with respect to the \ncriterion itself and not with respect to classification \naccuracy. While the empirical results do give good classification \nperformance, the criterion itself is only a surrogate. \n\nSeveral existing criterion are mentioned in the introduction (Settles, \nKrause et al, Ji and Han) to which the authors compare their \ncriterion. One item which is only given limited discusion is the \ncomputational complexity of computing the reward. For example, both \nV-optimality and Sigma-optimality require computing the inverse of the \ngraph Laplacian over the remaining unlabeled data. Some of the other \ncriterion have lower complexity. This is an important issue since the \nproblem is cast in terms of costs and since performance of all \ncriterion will eventually approach each other (since they are \npresumably solving the same classification problem following \nselection) a fairer comparison would include the cost of computing the \nreward. It may be that the proposed method wins here, but it at least \nbears some mention. \n\nWhy do the authors cite Streeter and Golovin for results regarding \nsubmodular set functions? Shouldn't this be Nemhauser et al? \n\nSection 2.3 misstates several things. The intractability of subset \nselection is not a consequence of submodularity, it is simply a \nconsequence of the combinatorics. Furthermore, this does not mean \nthat a greedy solution is \"required\". Other strategies may have lower \ncomplexity and outperform the greedy selection. It is merely, that by \nestablishing the property, greedy selection has certain guarantees \nrelative to the optimal selection. \n\nThe fact that the criterion of interest was originally proposed by \nGarnett et al in 2012 should be mentioned much earlier. This changes \nthe abstract from \"We propos a new criterion\" to something more \naccurate, such as \"We analyze a previously suggested criterion and \ndemonstrate its utility...\". \n\nEstablishing the sumodular property as well as the suppressor free \nproperties are interesting. \n\nEmpirical results are suffcient. \n\nModulo the comments above, the paper is fairly clear. The results are \ninteresting and the analysis, though limited, represents a \ncontribution. \n\nminor comments: \n\nFigure 3 lacks a horizontal axis. I assume it is probability of \ncorrect classification, but the authors give an ambiguous description \nin the caption. \n\n The authors establish that a previously proposed criterion is a montonoe submodular function and as such greedy selection achieves performance guarantees relative to optimal selection for an acitve learning problem. Experimental results demonstrate superior performance as compared to previously proposed criterion.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes a criterion called \\Sigma-optimality for active learning in Gaussian Random Fields (GRFs). V-optimality (proposed before), queries nodes that minimize the L2 loss. The paper argues that for classification problems V-optimality is not ideal (as a surrogate for the 0/1 more appropriate loss). \n\n\\Sigma-optimality was proposed before but not for active learning. The paper shows that \\Sigma optimality leads to a submodular function and provides a (1 \u2212 1/e) approximation bound to the global optimum. It also shows that experimentally it outperforms V-optimality and other active learning criteria, when a greedy algorithm is used. \n\nLike V-optimality, \\Sigma-optimality tend to pick nodes that have high variance or are correlated with nodes with high variance. However, the paper makes the observation based on Eq. 3.7 that \\Sigma optimality also prefers nodes that have \u2018more influence\u2019, usually nodes in the cluster centers. \n\nExperiments show that on synthetic datasets (generated according to the assumed model) \\Sigma-opt outperforms V-opt and random. The graphs are shown for specific model parameters \\beta and \\delta. It would be interesting to see when \\Sigma optimality breaks, in particular when random or V-opt are close (or better) than the proposed approaches. What happens on sparse graphs or highly connected ones? \n\nI am surprised by how badly MIG works and to a lesser extend also for the terrible performance of Uncertainty-based active learning. Is there any explanation for this? It would be useful to include in the paper how MIG was employed and include a discussion to contrast these methods (together with Unc) with the proposed approach. \n\nThis paper is clearly written. It is a small extension of previous ideas, in particular the use of V-optimality for the same problem and borrowing the idea of \\Sigma-optimality from recent previous work. Its significance is primarily based on the improved performance shown in the experimental evaluation. However, it is not very clear why it outperforms other methods with such ease. Overall, the paper is based on using \\Sigma optimality as active learning criterion in GRFs (for classification), a well-known problem. \\Sigma optimality have been proposed before, thus the mild novelty is in its use for active learning. The proven submodularity guarantee is an incremental contribution.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper analyses two optimization objectives for active learning on GRFs - Sigma-Optimality and a V-Objective (that was proposed before, but not analyzed). The authors show that the maximization versions of both the objectives are submodular and hence obtain approximation guarantees on the performance of greedy algorithms. Furthermore, they show that the covariance matrix for GRFs satisfy the Suppressor-free condition. \n\nThe paper has some nice theoretical insights, the most interesting of which being that the covariance matrix of GRFs is suppressor-free. The notion of how suppressors play an adverse role in subset selection, and how the absence of suppressors enables good performance guarantees for greedy algorithms has been studied previously, but it was not known whether there was a general class of covariance matrices that satisfied this condition. The authors' result that GRFs do satisfy this condition is quite interesting, both theoretically and practically. \n\nIt was not clear how novel the Sigma-Optimality criteria is, given that it seems to have been proposed before in an Active Surveying context. But I liked the fact that the authors analyzed this optimization problem rigorously, and provide approximation guarantees using submodularity. \n\nOn the other hand, I think the paper could improve in its writing - there were a few places where it was technically imprecise (though these do not affect correctness of their analysis) \n-I found it annoying that the paper was imprecise in distinguishing between minimization and maximization versions of the problem, and kept switching back and forth. The greedy multiplicative performance guarantees and submodularity of the objective are only for the maximization objective, whereas the paper seemed to suggest several times that it worked for the minimization objective (eg. beginning of section 2.3). \n-In the first para of page 4, calculating the global optimum is not intractable because of submodularity, and a greedy-solution is not \"required\". \n-In the last para of page 4, the suppressor-free property does not \"prove\" 2.3 (2.3 is just a definition) \n-The proper citation for the greedy bound for submodular-maximization is [Nemhauser et al 1978], not Streeter and Golovin. \n\n\nRegarding the experiments section, it is very surprising that the Mutual-Information criteria performs even worse than Random. \n The authors analyze two active-learning criteria for GRFs and show performance bounds of greedy algorithms using submodularity of the objectives. They also show that GRFs obey the supressor-free condition. Overall it is a nice paper, and has useful theoretical contributions - though it should fix the imprecise notation/sentences mentioned above.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
