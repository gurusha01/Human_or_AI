{"title": "Decision Jungles: Compact and Rich Models for Classification", "abstract": "Randomized decision trees and forests have a rich history in machine learning and have seen considerable success in application, perhaps particularly so for computer vision. However, they face a fundamental limitation: given enough data, the number of nodes in decision trees will grow exponentially with depth. For certain applications, for example on mobile or embedded processors, memory is a limited resource, and so the exponential growth of trees limits their depth, and thus their potential accuracy. This paper proposes decision jungles, revisiting the idea of ensembles of rooted decision directed acyclic graphs (DAGs), and shows these to be compact and powerful discriminative models for classification. Unlike conventional decision trees that only allow one path to every node, a DAG in a decision jungle allows multiple paths from the root to each leaf. We present and compare two new node merging algorithms that jointly optimize both the features and the structure of the DAGs efficiently. During training, node splitting and node merging are driven by the minimization of exactly the same objective function, here the weighted sum of entropies at the leaves. Results on varied datasets show that, compared to decision forests and several other baselines, decision jungles require dramatically less memory while considerably improving generalization.", "id": "69adc1e107f7f7d035d7baf04342e1ca", "authors": ["Jamie Shotton", "Toby Sharp", "Pushmeet Kohli", "Sebastian Nowozin", "John Winn", "Antonio Criminisi"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "the paper starts with the observation that rooted DAG and binary decision trees are quite similar. Fixing the number of child notes in a rooted DAG (RDAG) the authors then show in a series of experiments that these RDAG (and ensembles of those) compare favorably to a number of baselines. As this is a somewhat different type of regularization than is typically used in randomized trees this is a worthwhile empirical observation. In my view the key contribution is the experimentation showing indeed that these RDAG might well have practical importance. While any experimentation can be extended the authors have done a good job in my view covering various aspects. \n\nthe authors argue in section 2 that binary decision trees and rooted binary decision DAGs share quite some similarity - this is also what is used in the remainder of the paper. In the abstract, title and introduction however the writing suggests that a much more general similarity is used which I find at least confusing if not even annoying - in that sense the authors should tone down their statements at the beginning of their paper as this is not justified by the rest of the paper. \n\nanother detail: the abbreviation DAG is quite standard - but nevertheless the meaning of the abbreviation should be given the first of its occurrence in the text (that is the abstract in this case)  RDAG (rooted directed acyclic graphs) are explored in this paper (and ensembles) thereof. Experiments show that fixing the number of child notes obtains good results w.r.t. other randomized tree classifiers.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary of the paper: \nThis paper revisits the idea of decision DAGs for classification. Unlike a decision tree, a decision DAG is able to merge nodes at each layer, preventing the tree from growing exponentially with depth. This represents an alternative to decision-trees utilizing pruning methods as a means of controlling model size and preventing overfitting. The paper casts learning with this model as an empirical risk minimization problem, where the idea is to learn both the DAG structure along with the split parameters of each node. Two algorithms are presented to learn the structure and parameters in a greedy layer-wise manner using an information-gain based objective. Compared to several baseline approaches using ensembles of fixed-size decision trees, ensembles of decision DAGs seem to provide improved generalization performance for a given model size (as measured by the total number of nodes in the ensemble). \n\nQuality: \nThis paper is of generally good quality. The related work seems to have been throughly investigated, the model and algorithms make intuitive sense, and the experiments are fairly compelling and do a good job of investigating the effects of varying different design parameters. One aspect that seems to be missing is a comparison of the training and evaluation times of different approaches. This is an important consideration in these models that I think should be addressed. \n\nClarity: \nThe paper is quite clear for the most part. The writing is good, the model is well-presented, and the experiments are fairly easy to comprehend. One minor detail that I am not quite clear on is whether the ensemble is trained with bagging in addition to the other randomized elements of the learning algorithm. Also, the authors suddenly refer to energy in section 3.1, is this the same as the empirical risk? Finally, there is a minor grammatical error at the end of the LSearch description: \"for considerably less compute.\" \n\nOriginality: \nThe ideas underlying this paper are fairly well established (decision DAGs, ensembles, empirical risk minimization and information gain). However, the novelty of this paper is in combining these ideas into a cohesive model, and providing intuitive algorithms to learn it. \n\nSignificance: \nRandom forests are a heavy favorite among machine learning practitioners, and therefore any related method that improves upon them without significantly increasing their computational or implementation overhead could have a large impact. The authors present ensembles of randomized decision DAGs in order to improve classification under memory constraints, and cast the learning problem in terms of empirical risk minimization. Relatively thorough experiments on several datasets demonstrate the potential of the method.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a modication to the random forests to make the models more memory-efficient. Instead of using trees, it use DAGs. The paper proposes two methods for automatically learning the DAGs. \n\nThe overall quality of this paper is clearly below the threshold for NIPS. First of all, the novelty wrt random forests is very limited. The only modification is that this paper uses DAGs instead trees. Although this paper claims that the advantage of DAG is that it is more memory efficent, but this point is not sufficently demonstrated (more on this later). \n\nSecond, the proposed technique is very ad-hoc. Although Section 3 starts with an optimization problem in Eq 2-4. The optimization in Sec 3.1 ends up being some ad-hoc local search methods. The description of the algorithms in Sec 3.1 is extremely hand-waving. I doubt anyone can implement the algorithm with the given description. Also, does the algorithm converge at all (even to some local minimum)? \n\nThe experimental results did not really demonstrate the benefit of the proposed approach. The main claim of this paper is that it is more memory efficent. However, the experiments in the paper only try to demonstrate that the proposed method generates models with less nodes. But the number of nodes is only one of the factors of model size (you also need to store other information of the model, e.g. the spliting function, the class distribution at leaf nodes, etc). It is not clear to me how the number of nodes will translate to memory efficency. For instance, if the number of nodes only takes a small percetange of the overall model size, perhapse 3000 nodes and 22000 nodes will take rougly the same amount of memory. In addition, memories are getting cheaper everyday. If the proposed method only reduces the memory by a small amount (say a few KB), it is not relevant for real applications. \n\n This paper seems to be trivial modification of existing techniques. It is unlikely to have any impact and is clearly below the NIPS standard.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
