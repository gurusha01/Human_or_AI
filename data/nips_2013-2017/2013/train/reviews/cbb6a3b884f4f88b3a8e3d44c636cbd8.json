{"title": "On Flat versus Hierarchical Classification in Large-Scale Taxonomies", "abstract": "We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. We then introduce another type of bounds targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.", "id": "cbb6a3b884f4f88b3a8e3d44c636cbd8", "authors": ["Rohit Babbar", "Ioannis Partalas", "Eric Gaussier", "Massih R. Amini"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "Summary: The paper concerns hierarchical multi-class classification. The authors study whether and when a hierarchical classifier can be more beneficial than its flat counterpart. They proof a generalization bound that provides an explanation when a flat and when a hierarchical classifier should be used. Additionally, the authors provide an approach for logistic regression and naive Bayes classifiers, which enables pruning of nodes in large-scale hierarchies. \n\nQuality: The authors consider a very interesting and up-to-date problem. Therefore I was very glad to read this paper. The first bound obtained by the authors is very interesting and indeed provides an explanation of existing empirical results. The rest of the paper is less clear. It seems that Lemma 1 is quite similar to standard results in statistics. At least, I suppose that a similar result has been published elsewhere. I am also not entirely convinced by the pruning strategy introduced by the authors. A better explanation is required here. It is also not clear why AdaBoost with Random Forest is used as the meta-classifier. What are the reasons to use such a complex classifier here, while the main classification problem is solved by rather simple linear algorithms? It is also not clear how the results concerning pruning of the hierarchy can be generalized to other algorithms. \n\nClarity: The paper is quite dense, therefore not easy to read. It contains many theoretical results, which are not easy to verify in the limited time of the reviewing period. Personally, I would prefer a paper that would be constructed around one of the results, for example, the first theorem. Then the verification of the results would be easier, as well as the reading of the paper. Unfortunately, the second part of the paper concerning logistic regression and pruning is not so clear as the first part. The description of the meta-classifier could be improved. It seems that there is missing something around Theorem 3. At least it is not clear when Theorem 3 ends. \n\nOriginality: The results are in my opinion very original. I do not know any paper that considers a similar problem. \n\nSignificance: The obtained results can be very significant. At least the first theorem (if the proof is correct) gives a nice explanation of the performance of flat and hierarchical classifiers.  Summary: It is an interesting paper that concerns an up-to-date problem. Personally, I like very much the first part of the paper around theorem 1. The rest of the paper is unfortunately less understandable.Update (after rebuttal): I thank the authors for their clarifications. The second part of the paper is now a little bit clearer for me, however, still I think that this part could be written in a more comprehensible way.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper investigates a tradeoff in hierarchical classification: when a hierarchy is deep, a classifier needs to make a decision at every level of the tree which is likely to propagate errors. When a hierarchy is flat(ter), less decision need to be made so there is less room for errors to propagate, however, the decisions are harder (because the cardinality of is higher). \n\nThis paper introduces a data-dependent generalization error bound for kernel based hypotheses. The main theorem of the paper states an upper bound on the generalization error of a hierarchical classifier in terms of the empirical error and the Rademacher complexity of the classifier. The former encouraging flat classifiers, the latter encouraging deep classifiers. \n\nThe paper uses the insight from the generalization bound to come up with a strategy to prune hierarchical classifiers. The paragraph starting on line 307 was a bit unclear to me; I missed the motivation of the methodology using the metaclassifier and how it relates to improvements in the generalization bound. \n\nThe paper is clearly written and solves an interesting and important problem. On the theoretical side, the paper contributes to the literature on hierarchical classification. My main issue with the paper is that it is unclear how to use their insights for anyone doing hierarchical classification. I think the authors can do a better job of describing the practical take-away around how to best do pruning of a hierarchy to improve hierarchical classification. A well written theoretical paper on the important topic of hierarchical classification. The writeup falls short on taking the theoretical insight and deriving practical procedures for improving hierarchical classification.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary \n======= \n\nThis paper proposes a bound on the generalization error of hierarchical multi-classifiers for large-scale taxonomies. Such bound provides a justification of empirical results of flat and hierarchical classifiers. The paper also proposes a bound on the approximation error of a family of classifiers. This is used to define a node-classifier for pruning large-scale taxonomy and thus improving the overall accuracy. Some experiments illustrate the findings of the proposed theory on two well-known taxonomies. \n\nEvaluation \n======== \n\nThe paper is well written and presented. \nIt contains interesting technical content, which appears to be sound. \nIn general the paper is of high quality. \n\nHowever, this reviewer sees two limitations of the proposed methods: \n\n1. Considering hierarchical classification as the task of only classifying the leaf nodes is rather limitative for two reasons: \n\na. In real cases, category nodes can contain documents that do not belong to any of their children. This is very common as, when new documents of new types have to be classified, there is no specific child node to accommodate them. Thus the best choice is to assign them to the father nodes. \n\nb. This reviewer is not so sure that there is an actual debate on using flat and hierarchical models in the setting proposed by the authors (i.e., only leaf classification). Indeed, when working in the authors' setting, the flat model seems superior. \n\nIn contrast, top-down methods may get better accuracy when classification is also carried out in the internal nodes. For example, the following paper clearly shows that the top-down method is more accurate than the flat one in such setting: \n\nAlessandro Moschitti, Qi Ju, Richard Johansson: Modeling Topic Dependencies in Hierarchical Text Categorization. ACL 2012: 759-767. \n\nIn this respect a close comparison with other methods, e.g., the one above, those cited by the authors (e.g., Gopal et al.) and the following \n\nZhou, D., Xiao, L., Wu, M.: Hierarchical classification via orthogonal transfer. ICML11. \n\nwould be appreciated. \n\n2. The proposed bounds do not look very strict: the Rademacher complexity term is a rough approximation and does not consider feature distribution/relevance, which has a major impact in text categorization. \nFor example, in Reuters 21578, there are very rare categories. They may contain about 0.1% of the entire training data, i.e., about 10 documents but, for them, systems can reach accuracy of about 90%, larger than for other more populated categories. \nThis suggests that the data imbalance of categories is an important factor but it is not enough. For example, also category ambiguity (how much a category is similar to others) should be considered. \n\n\nAfter the authors' response \n==================== \n\nIn addition to the reviewer comments, the concepts expressed in the authors' answer should be included in the final version of the paper. \nMoreover, the authors' answer does not completely solve all the reviewer's doubts, thus the authors should inform the reader about the possible theory's flaws. \n - The technical content is very good but the proposed bounds may result not enough strict. The authors should inform the reader about this.- The paper claims should be lowered in strength and remodulated according to the reviewers' comments and the authors' response.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
