{"title": "Symbolic Opportunistic Policy Iteration for Factored-Action MDPs", "abstract": "We address the scalability of symbolic planning under uncertainty with factored states and actions. Prior work has focused almost exclusively on factored states but not factored actions, and on value iteration (VI) compared to policy iteration (PI). Our \ufb01rst contribution is a novel method for symbolic policy backups via the application of constraints, which is used to yield a new ef\ufb01cient symbolic imple- mentation of modi\ufb01ed PI (MPI) for factored action spaces. While this approach improves scalability in some cases, naive handling of policy constraints comes with its own scalability issues. This leads to our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent al- gorithm lying between VI and MPI. The core idea is a symbolic procedure that applies policy constraints only when they reduce the space and time complexity of the update, and otherwise performs full Bellman backups, thus automatically adjusting the backup per state. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show signi\ufb01cantly improved scalability over the state-of-the-art.", "id": "7d771e0e8f3633ab54856925ecdefc5d", "authors": ["Aswin Raghavan", "Roni Khardon", "Alan Fern", "Prasad Tadepalli"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper presents a novel policy iteration algorithm for symbolic \nMDPs with factored-action (in addition to factored-state) \ndynamics. The algorithm, MB-OPI, yields a way to trade \nrepresentational complexity between value and policy iteration for the \nclass of MDPs defined over algebraic decision diagrams, just as MPI \ngives a way to smoothly trade computational complexity. In doing so, \nthe authors generalize several existing algorithms which consider \nfactored actions and memory constraints independently. \n\nThe main technical challenge is that ADD policy iteration requires \nmultiplying an explicit policy representation into the current value \nfunction, which can significantly increase its size. The solution is \nto control this increase in size by defining a procedure to \nconservatively combine policy and value diagrams using a pruning \nprocedure, rather than naively multiplying them. Results are \npresented in terms of solution time, and show a ~2-6x improvement over \nexisting approaches. \n\nThis paper is technically sound and well written. The authors make a \ntheoretical contribution to the literature on symbolic MDP planning by \nintroducing the concept of pruning as an alternative to ADD products, \nand proving that this satisfies the guarantees of MPI. Also couched \nas generalization to existing work in symbolic dynamic programming, \nand appears to be state of the art for planning with factored actions \n\nEmpirical results support the idea that pruning offers an MPI approach \nto SDP planning that avoids representational bloat, and offers a \nseveral factor speed up \n\nThe paper is also generally well written and easy to follow. \n\nIf possible, I would suggest adding more background on SDP solving \nusing ADDs for representing value and DBNs, the basic policy iteration \napproach using ADD product, and the difference between multiplying pi \ninto V vs. pruning V with pi. \n\nIt would also be nice to have (a) discussion of practical problems \nwith many parallel actions for which factoring actions is critical, \nand (b) a toy test case with large parallel actions that highlights \nthe best-case improvement over SPUDD and FAR. \n\n\nSome notes on clarity that might be helpful: \n\n053, \"enforcement of policy constraint\": 'constraint' hasn't been defined yet, and only makes sense if you remember to view policy iteration as policy-constrained value iteration \n\n060, Figure 1: ordering of state and action nodes would be more readable if they were interleaved or stacked (something consistent) \n\n060, Figure 1: as state variables these are propositions, not predicates, so might be better to use underscores (e.g. reboot_c1) \n\n095, \"marginalization operators\": examples include marginalization but also max. should reword for correctness \n\n110, equation 1: this was confusing without referring back to SPUDD paper. I suggest adding 2 things: (a) explanation of how expectation for factored models turns into a product of DBNs, and that sums can be pushed in, and (b) simple explanation that \"primed\" literally adds a prime to each state variable, and is necessary to make the ADD operations well defined (saying \"swaps the state variables X in the diagram V with next state variables X\u2032\" can be confused with more complicated mapping issues) \n\n152, policy ADD: Would be helpful to have a sentence like \"Intuitively, this representation makes it possible to express 1-step policy backup as the ADD product of a policy with a value function\". \n\n179, Figure 4: caption uses C to denote policy, but figures use \\pi. other places too \n\n206, \"size of a Bellman backup\": usually think of backups in terms of computational complexity, so should clarify that \"size\" refers to the data structure that results from applying eq1. also would be helpful to intuitively explain what this looks like, as opposed to pi (both have action variables, so why is pi generally bigger??) \n\n206, (due to\u2026): for clarity, would help to clarify that pi is bigger because it represents joint-actions, whereas backup only represents value and (product of) factored actions \n\n212, \"only those paths in D that\": add \"in D\" to make clear that policy is being applied to value function. otherwise can be confused with original definition of policy \n\n247, eq3: two extra close parens \n\n252, \"sandwiched\": intuitive, but perhaps too informal (though who am I to say such a thing?) \n\n278, \"parameterized by k, \u2026\": missing comma \n\n327, Figure 6: colors for OPI 2 and 5 are reversed in b... I think. \n This paper presents a well-defined improvement to decision-diagrambased planning in symbolic MDPs. Empirical and theoretical resultssuggest that their algorithm is the state of the art for planning withfactored actions.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper introduces an improvement to symbolic policy iteration for domains with factored actions. The core idea seems to be that we can take some liberties with the symbolic backup operations to reduce the size of the resulting ADDs, and that the particular way that this is done is by performing a more general backup (rather than an on-policy backup) for some actions, when doing so does not increase the resulting expression. This is proven to converge, and some thorough and reasonable impressive experimental results are given, though I do not know enough about symbolic policy iteration to determine whether or not they are exhaustive. \n\nBoth symbolic approaches and factored actions are interesting and under-explored, so I am positively disposed toward the paper. \nThe main difficulty I had was that it is not explained how \\pi is represented as an ADD, so that \\pi is introduced as a constraint in section 3. Some more explanatory material here - perhaps giving an example of an on-policy vs. off-policy action backup links to the result of the pruning, would really help. As it is the reader has to piece together what the pruning operator does from some math and some diagrams, before its high-level explanation-which as far as I can understand is actually quite simple - is given in passing. This is made extra confusing in Figure 4, when D and C presumably mean D and \\pi. \n\nUnfortunately this, combined with only passing familiarity with symbolic approaches, made the paper quite hard to understand, when it probably should not be. \n\nOtherwise I only have small writing comments: \no \"flat actions\" might better be described as \"atomic actions\". \"Flat\" is often the opposite of hierarchical. \no \"assume a small flat action space\" -> \"assumes a small flat ...\" \no \"Factored State and Actions Spaces\" -> \"Factored State and Action Spaces\" \no A parenthetical citation is not a noun. \no assignments OF n boolean variables to A real value \no \"interestingly, the first approach to symbolic planning in MDPs, was a version\" (delete comma) \no The graphs are all tiny. I appreciate the space constraint, but it looks like you can eliminate some whitespace. \no Please use full citation information and properly format your references (including capitalization). \n An interesting paper that presents a technical improvement to symbolic backups that improve their performance. Often difficult to understand.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper describes a pruning technique that enables symbolic modified policy \niteration in large factored action MDPs. Their technique, like regular MPI, \ngeneralizes policy iteration and valuation, incorporates prior (orthogonal) \nwork on partially bound actions, and is experimentally validated. \n\nI weakly recommend this paper for acceptance. The main contribution appears to \nbe a non-trivial implementation detail, but their experiments show that (a) \npruning by itself is helpful for value iteration, (b) pruning is required for \nmodified policy iteration, which is often not possible for memory reasons, and \n(c) that modified policy iteration improves convergence in factored action \nMDPs. \n\nThe paper is well motivated, but the notation is inconsistent in places and \noften hard to follow. e.g., Alg 3.2 is called Prune, but it is used as \\cal P \nelsewhere, it is not obvious from the text that T^Q(V) is a function of \nstates and actions, or even that the variables are binary. \n\nMy main concern with the paper is that I could not follow the details enough to \ncompletely understand the statement of theorem 1. In particular, it is not \nclear why \\hat T^Q_\\pi can be different than T^Q_\\pi. Is it necessary to prune \nat every step, or is it sufficient to prune only once? Is it the repeated \npruning that causes the overestimation? or is the convergence theorem the same \nfor FA-MPI and OPI? \n\nProposition 2 seems trivial. Is there any guarantee on how much smaller the \npruned tree will be? \n\n I recommend that this paper be accepted. From a high level it is well motivated and clearly written, and the experiments demonstrate its ability to tackle previously intractable problems.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
