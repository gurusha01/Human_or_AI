{"title": "Learning Multiple Models via Regularized Weighting", "abstract": "We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd's algorithm  for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models.   We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufficiently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efficiently. We demonstrate the robustness benefits of our approach with some experimental results and prove for the important  case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers.", "id": "9ab0d88431732957a618d4a469a0d4c3", "authors": ["Daniel Vainsencher", "Shie Mannor", "Huan Xu"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The authors propose in this paper a regularized weighting technique for multi-model learning. Multi-model learning consists in fitting a \"mixture\" of simple models to a data set in a quite generic sense where each model is associated to a convex loss and the mixture is optimized with respect to both the model parameters and their weights (each model is associated to a set of weights that describe how much the model claims to explain each data point). This model generalizes several classical approaches such as k-means and classical probabilistic mixture modeling. The authors propose to penalize the weights of each model so as to favor uniform weight distributions as a way to bring robustness of the mixture with respect to outliers. The paper contains several theoretical results and some limited experimental validation. \n\nThe paper is interesting and well written. In particular, the generality of the approach is very well presented. The authors also managed to pack a nice collection of results in a limited space. They pushed all the proofs in supplementary material, but nevertheless, the paper feels quite self contained. Limitations of the technique are also clearly presented (see Figure 2.2 for instance). \n\nMy overall impression is therefore very positive. That said, the paper is not without flaws. A very minor one concerns notations: \n- \\Delta^n is introduced a bit too late (in footnote 1 which falls on page 3 while the notation is used on page 2). \n- P_C (the orthogonal projection operator) is defined only in the supplementary material while it is used in Lemma 2 (in the s.m. the definition is also a bit too delayed). \n- while the gamma parameter used in the discussion below definition 2 is indirectly defined, I think the discussion would be clearer by being more explicit (for instance by saying that the proportion of zero weights (i.e., ignored data points) should be between this and that). \n\nWhile the overall paper is very clear, Section 3.1 is a bit too cryptic and lacks too much details: how was the noise generated? what is the value of alpha? what is the effect of said value on the performances? does MAD in the legend stands for MD in the text? if the base learning is PCA, is there any dimensionality reduction done? if yes how? All in one, this unique experimental evaluation raises more questions that it brings answers or insights on the proposed method. \n\nComing back to the choice of alpha, it is obvious, as discussed in Section 2 and 3.2, that its actual value will have at least some impact on the performances of the final model. And of course, any practitioner would want to optimize said alpha. Yet the experimental section does not show the effects of alpha (whose value is not specified) and the authors offer almost no clue on how to tune it. This is quite a shortcoming in the paper, in my opinion. As alpha appears explicitly in Theorem 3, it is quite clear that even on a theoretical point of view the parameter has some complex effect on the results. \n\nAnother point is the upper bound assumption (B everywhere). I'm familiar with clipping techniques used in rather old papers (for instance the Zeger and Lugosi's 1995 paper, see http://www.code.ucsd.edu/~zeger/publications/journals/LuZe95-IT-Nonparametric/LuZe95-IT-Nonparametric.pdf) but I'm not sure how to understand the assumption in the present context. In particular, it seems too me that the very idea of fighting arbitrary outliers might prevent this assumption to be true in practice. In other words, moving from Theorem 3 to a more general one which applies when the bound assumption is not valid (as in done in the paper referenced above) does not seem completely obvious to me. I might be missing something that should then be emphasized more in the paper. \n\nA final point is that despite the (short) Section 1.1, the discussion on related works seem quite limited. In particular, mixture models can accommodate many types of reasonable outliers by modeling them explicitly, for instance via some Student component instead of Gaussian ones, or via some explicit noisy components added to the mixture. Bayesian priors can be included into the mix to avoid some strong effects of very badly behaving outliers. I'm not saying that such an approach might lead to non zero breakdown point, but in practice, such variants do work very well.  A very complete paper on a new way of mixing simple models in a regularized way that brings guaranteed robustness against outliers which lacks only a better experimental evaluation. It would be however quite impossible to pack this evaluation into the paper if all theoretical results were to be kept.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a general formulation of multiple model learning, which introduces the weight of each data point with a regularization term and is robust to outliers. This paper provides theoretical support of the formulation with some empirical results. \n\nQuality, clarity, and significance: \nAlthough this paper contains some interesting results, there are many unclear descriptions in both texts and mathematical notations, which significantly deteriorate the quality and the significance of this paper. \n\n- Definition 1 is not clear. Why the tuple (actually, it's a triplet) with two sets and one function is called a \"problem\"? I cannot see any problem from here. I think the problem is minimizing the weighted loss. \n- l.109: What is the bold style m ? I think it's a vector of m_j from j=1 to k, but such a vector is defined as M in l.104. \n- In Example 1, it is better to show that for each example what X, M, and l are. \n- l.147: Why an outlier tends to infinity? \n- Lemma 2. What is the definition of P_{\\Delta^n} ? \n- In 3.1 (empirical results), what is the actual alpha and how to set it? \nAs the authors say in l.73, this alpha should be set appropriately and I think this is important for this formulation. But how? For example, cross-validation cannot be used in unsupervised learning. So some strategy to set alpha is needed and it is also valuable to analyze the sensitivity with respect to changes in alpha. \n- In Figure 3.1, I do not understand \"Dataset\" of x-axis. Why RW MAD monotonically increases if datasets change? \n- l.320: It is better to state the mathematical definition of the breakdown point. \n- l.336: In l(m_j, x_i), m_j and x_j are opposite (see Definition 1). \n- l.337: It would be better for readability to add some intuitive explanation of the number \"22\". \n- l.375: What is l(\\cdot, m_j)? I think some x is needed for the place \\cdot to define this value. \n\nOriginality: \nThis paper presents a generalization of multi-model learning with some regularization term and the originality is not so high. But for me it is ok since this approach is important for development of this area. This paper contains interesting theoretical results, but the description is not sophisticated enough and empirical evaluation is not sufficient.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Multiple model learning is a generalization of clustering, and in this framework cluster centers can be learning models. \nFor each learning model, weighted data are assigned, and the weights averaged over learning models are restricted uniform weights with l2 regularization. \nThanks to this regularization term, a simple alternating optimization algorithm is derived. \nAlso some theoretical bounds of performance are obtained, which support robustness to outliers. \n\nThe motivation is clearly stated. \nTheoretical analysis looks mathematically sound. \n(some notations such as P_delta is defined in appendix, \nbut not defined in the main body. therefore the authors should carefully check the main manuscript is self-contained.) \nEach set of weighted data can be related with (empirical) distribution, so it might be nice to discuss the properties of regularization from the viewpoint of probabilistic mixture models, not only from optimization perspective. \n Based on l2 regularization of average weights of data, a new method of multiple model learning is proposed.Numerical experiments support its efficiency and good performance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This work proposes a new robust method and a new unified framework \nfor some learning task generalizing clustering models. It \nallows to robustly deal with problems such as clustering, subspace \nclustering or multiple regression by encompassing weights on the data \nsamples. \nTo better handle outliers, probability distributions over the data \n(in classical settings Dirac distributions are mostly considered) \nare attached to each \"model\". Moreover, the distributions are obtained \nby minimizing a trade-off between two terms. \nThe first one is the (Euclidean) \ndistance between the average weight distribution to the uniform distribution. \nIt aims at producing spread out weights inside each \nmodel (it is a regularization term). \nThe second one is a weighted loss function that take into account the \nimportance of each samples (it is a data fitting term). \n\nGlobally the notation, formulae and mathematics details \nare really loose. The English level is also rather poor. \nWorst, very often the clarity of the paper is at stake. \nThough the motivation and the method proposed are interesting, \nI feel that this version of the paper is rather a preliminary investigation, \nthan a camera-ready paper. More work is needed to make it clear. \n\nFor instance, to clarify their point, the authors should \nprovide, with careful details, what the proposed method does in the simplest \ncase of their framework, i.e., in the context of clustering (with possibly experiments \nin the presence of outliers). \nThis could be better developed by moving Section 2.1 into the Appendix. \n\nThe experiment section is even worse than the theoretical part. \nNot enough details are provided to understand the figures, and section \n3.1 is particularly uninformative in term of practical performance. \nCan the author compare with other methods for the specific task proposed, \nas, for instance, for the clustering task mentioned earlier. \n\nLast but not least, a longer discussion on the way the trade-off parameter \n\\alpha is chosen (both in theory and in practice) should be given. \n\n\nAdditional line by line comments: \n\nl086: other generalizations of the the k-means algorithm could be referenced \nhere as the framework proposed by \n\nBanerjee et al. \"Clustering with Bregman divergences\", JMLR, 2005. \n\nl103--106: the notations are not introduced before they are used: X, M, Delta_n^k, etc. \n\nl109: \\Delta^{n 1} is not consistent with the notation defined just before. \n\nl147: a a \n\nl161: the footnote should be given on the previous page. \n\nl185: mention that u=(1/n, ... , 1/n) (if I understand correctly) \n\nl186: indices in the sum are not consistent... \n\nl194: is the optimization over W or over one single w_j. Please remove \nthis ambiguity. \n\nl200: so why using an unusual closeness index? can you motivate your choice? \n\nl207: \"average penalty\" what is this referring to precisely? \n\nl232: \" has 1/3 the point\"? I don't understand this sentence, neither do I get the \nfigure. \n\nl254: P_\\Delta^n: is never defined (only afterwards in the Appendix!!!). \n\nl299-314: What is MAD referring to in the legend? it is nowhere defined before. \n\nl368: the rhs depends on i, the lhs does not. Please correct. \n\nl402: requires->required \n\nl403: converge->converges \n\nl457: Candes -> Cand\\`es \n\nl468: what is the nature of this work?article, conference, etc... \n\nl553: \"with the corresponding variable ...\": this sentence is unclear \n\nl778: is the equality true for all U? It seems false without more assumptions \nthat should be reminded. \n\n\n This papers proposes a general robust formulation for handling multiple model learning (e.g. extending clustering algorithms) by considering optimized data distributions.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
