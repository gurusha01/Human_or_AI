{"title": "Convex Two-Layer Modeling", "abstract": "Latent variable prediction models, such as multi-layer networks, impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction.  Unfortunately, such models are difficult to train because inference over latent variables must be performed concurrently with parameter optimization---creating a highly non-convex problem.  Instead of proposing another local training method, we develop a convex relaxation of hidden-layer conditional models that admits global training.  Our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer.  The resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features, while improving training quality over local heuristics.", "id": "a9be4c2a4041cadbf9d61ae16dd1389e", "authors": ["\u00d6zlem Aslan", "Hao Cheng", "Xinhua Zhang", "Dale Schuurmans"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper addresses problem of training two-layer network for classification with latent variables. The authors propose a convex SDP relaxation of originally non-convex training problem. They also provide an approximate optimization algorithm to solve their SDP formulation. A proof of concept experiments show promising results, namely, that the algorithm outperforms both globally optimized single-layer models as well as the same two-layer model optimized with local alternating minimization. \n\nThe proposed reformulation which allows SDP relaxation is interesting and \nnovel. Overall the paper is sufficiently clear though some parts of the text are \ndense. The paper seems to be technically sound. \n\nThe main weakness seems to be complexity of the resulting SDP problem (21). The \nauthors could mention basic properties of the proposed optimization algorithm \nfor solving (21), e.g computational time required for the benchmark problems and \nwhether the algorithm provides a precise solution (i.e. what was the stopping \ncondition used). This is an important information because convex problem does \nnot immediately mean easy to solve problem, i.e. a convex relaxation can be \nintractable in practice and it should be clear if it is the case or not. However, the proposed relaxation would be valuable even in this case. \n\nMinor comments: \n- equ (17): variable is missing below the first minimum \n- equ (18): I think N=\\Phi'*\\Phi should appear in the conditions defining the set. \n- line 331: (d) Synhetic results \n- line 356: It is unclear why the used transductive evaluation does not require \ncomputing responses of the network and thus knowledge of W and V. \n- I could not find the actual size of instances of the problem (21) solved \nin the experiments. \n The proposed SDP relaxation is an interesting attempt to find a betterapproximation of an important instance of non-convex training problems. Thoughthe current algorithm may not be practical it can inspire development of moreefficient methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a convex approach to train a two-layer model in supervised learning. This is achieved by incorporating large margin losses in the training objectives, adopting indmax transfer for the second layer and multi-label perception model with a step transfer for the first layer; finally, convex relaxations are applied that make global training of a two-layer model possible. \n\nThe paper is well written, and the process of linking all the factors together and deriving a convex objective is interesting and insightful. The authors did a good job presenting the technical steps taken to the ultimate convex formulation, with some nice intermediate results. The global training methodology proposed in this paper is meaningful from a deep learning perspective, and experimental results are convincing to demonstrate the significance of global training. \n The paper presents a convex approach to train a conditional two-layer model. The technical work in this paper is solid, while the conclusion is significant and insightful.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "A convex relaxation of two-layer neural network is proposed in the \npaper. This paper is well-written. The experiments show good \nperformance on the \"real\" datasets. But my main concern is the \nscalability of this approach. \n\nThe approach of using SDP for convex relaxation was widely used more \nthan 10 years ago, nothing new here. Though it has a nice form, the \nscalability is the major issue of this type of relaxation. Here, we \nneed to optimize a problem of t^2, the square of the size of \ninstances, which is probably only feasible for toy datasets. For the scalability issue, it would be better to compare the training time among algorithms. \n\nAlgorithm 2 takes advantage of low rank of N. However, the rank of N \nis not guaranteed to be small. \n\nFor those synthetic experiments, RBF SVM probably can achieve a quite \ngood. A fair comparison would be Nystrom approximation to RBF SVM with \nrandom selected bases, instead of one-layer linear SVM. \n Well-written. But SDP convex relaxation is not novel.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
