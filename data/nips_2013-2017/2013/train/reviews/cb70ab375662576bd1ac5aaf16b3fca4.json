{"title": "Learning Hidden Markov Models from Non-sequence Data via Tensor Decomposition", "abstract": "Learning dynamic models from observed data has been a central issue in many scientific  studies or engineering tasks. The usual setting is that data are collected sequentially  from trajectories of some dynamical system operation.   In quite a few modern scientific modeling tasks, however, it turns out that  reliable sequential data are rather difficult to gather, whereas out-of-order snapshots are much easier to obtain. Examples include the modeling of galaxies, chronic diseases such Alzheimer's, or certain biological processes.  Existing methods for learning dynamic model from non-sequence data are mostly based on Expectation-Maximization, which involves non-convex optimization and is thus hard to analyze.  Inspired by recent advances in spectral learning methods, we propose to study this problem  from a different perspective: moment matching and spectral decomposition. Under that framework,  we identify reasonable assumptions on the generative process of non-sequence data,  and propose learning algorithms based on the tensor decomposition method  \\cite{anandkumar2012tensor} to \\textit{provably} recover first-order Markov models and hidden Markov models. To the best of our knowledge, this is the first formal guarantee on learning from non-sequence data. Preliminary simulation results confirm our theoretical findings.", "id": "cb70ab375662576bd1ac5aaf16b3fca4", "authors": ["Tzu-Kuo Huang", "Jeff Schneider"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper gives a spectral algorithm for learning HMM from non-sequential observations. Motivated by several scientific examples, the authors define a sampling model for non-sequential observations that shares some similarities with the generative model of Latent Dirichlet Allocation. Then, resorting to recent spectral techniques for learning LDA, HMM, and mixture models, they prove sample bounds for recovering the parameters of an HMM with continuous output from data sampled according to this model. The last section provides a simple simulation that illustrates the behavior of the algorithm in a synthetic example. Proofs of all results are given in the supplementary material. \n\nThe main contribution of the paper is to identify a sampling model for non-sequential data generated from an HMM which is amenable to theoretical analysis using recent results on spectral learning, and to prove finite-sample bounds under this model. The experimental evaluation is extremely limited, and perhaps unnecessary given the nature of the paper -- the space used by this part could probably be put to a better use, e.g. highlighting the novel points in the proofs found in the appendix. \n\nIn general the paper is well written. The authors explain most of the intuitions behind their results in the text. However, the mathematical style is rather dry -- specially the sample bounds, which favor precision over asymptotic behavior, are quite hard to grasp. \n\nThe content of the paper is not extremely original, though there is some novelty in the generative model for non-sequential data and the way it is analyzed. Proof techniques, though involved, look very similar to those in other spectral learning analyses. Besides, the problem of using non-sequence data in a spectral learning algorithm for HMM was already considered in (Spectral Learning of Hidden Markov Models from Dynamic and Static Data, T. Huang and J. Schneider, ICML 2013). \n\nThe significance of the method depends on whether the algorithm performs well on real data which, most likely, won't be generated according to the sampling model defined in the paper. The authors leave this as future work, but the sample sizes for which they report good results on a synthetic target suggest that it may not be applicable to problems where only small samples are available. \n\n*** Typo *** \n[line 107] V_2 -> X_2 The paper gives the first finite-sample bounds for learning HMM from non-sequence data from a reasonable sampling model. The interest of this approach depends on whether the algorithm will behave well on the real problems that motivate the model.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a tensor factorization approach for parameter \nestimation in settings where there is an underlying hidden Markov \nmodel, but we only see a small random fraction of the observations. \nThe contribution of the paper involves identifying the tensor \nfactorization structure in the problem (which extends ideas of \nAnandkumar et al.), which is relatively straightforward. An \ninteresting part is showing that you can estimate recover the \ntransition distribution from an expectation over the sums of the \ntransition distribution. Sample complexity results and some toy \nsimulations are provided as well. Overall, a reasonable paper with \nsome new ideas. \n\nPresentation suggestion: I would suggest defining the actual \nmodel/problem (which is currently in section 3) much earlier - for \nexample, one shouldn't be subjected to Theorem 1 before even having a \nformal definition of what the paper is trying to do. Too much space is \nspent reviewing the tensor decomposition framework; a citation to that \nwork and a brief description of the key ideas suffices, or relegate to \nthe appendix. \n\nIn the definition of the model in section 3.1, please make it explicit \nthat we are getting $N$ i.i.d. replicates of this model. For a while, \nI was confused at how this was even possible if you only have one \nrandom draw from $\\pi_0$. \n\nCurrently, the theorems very dryly write down true facts, but the text \ndoesn't really provide any guidance about what are the important \nproperties to watch out for. For example, line 062 of Appendix A.1 \nnicely lays out the factorization structure, which I think can be \nimported into the main body of the paper. Also, I'd appreciate more \nintuition about Theorem 3. \n\nExperiments: it's nice to see some relationship (even if it's just \nqualitative) between the empirical findings and the theoretical bounds; \nthe fact that $U$ is easier to learn is not surprising. I'd be curious \nto see how this algorithm compares with EM, since the original \nmotivation is that EM presumably gets stuck in local optima and this \nmethod does not suffer from that problem. And of course, needless to \nsay, experiments on real data would be good too, especially taken from \nthe cited that look at learning from non-sequence data. \n\n318,320: primes should be placed above the subscripts in $M_2$ and $M_3$ \n This paper applies recently developed tensor factorization techniquesto the new setting of learning a HMM from non-sequence data. The papercould be written to convey more insight, but overall it's a reasonablepaper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "- In this paper, a spectral learning based algorithm for learning Markov Model and HMM in a non-sequential setting is derived. The paper deals with learning Markov models and hidden Markov models from sparse realizations obtained at random times. \n\nProper proofs for empirical moments are given. Also, a sample complexity bound has been provided. The paper is generally well-written and understandable. The comments are as follows: \n\n-Learning a sequential model in a non-sequential setting is not a concept that everybody is familiar of. It may be beneficial to briefly review the existing methods (possibly maximum likelihood based). Author(s) may argue that this can\u2019t be done because of space constraints. However, I believe that, it would be better to motivate the non-sequential sequential model learning rather than to reproduce the tensor decomposition algorithm of Anandkumar et al. I think that the tensor algorithm can be introduced only by giving the general idea of expressing the parameters as symmetric observable tensors. (equation 2) That is, author(s) can exclude the algorithm 2 and theorem 1. \n\n-The proper moment equations for learning a Markov model in a non-sequential setting are defined in page 5. Using these moment equations, it possible to recover the expected transition probability matrix T, and parameters \\pi. of the Dirichlet prior, using the symmetric orthogonal tensor decomposition algorithm of Anandkumar et al. from 2012. However, in order to recover the transition matrix itself (P), author(s) propose a search heuristic. In my opinion, this search algorithm can be made clearer to the reader by writing it as a pseudo-code. The assumption of presence of a zero entry in P makes sense if the number of states in the Markov chain is large, which may induce sparsity. \nMinor comment: isn\u2019t \\pi = \\alpha / \\alpha_0? Instead of using the proportional (at the beggining of page 5), it is better to give the exact equality to make the reader understand the moment equation proofs more easily. \n\n-Experimental results are limited to synthetic data. Since learning a sequential model in a non-sequential setting is the primal motivation of this paper, I think it is essential to validate the algorithm on a real life data. Moreover, a performance comparison with a more conventional learning algorithm is vital for the justification of the algorithm. \n\n-In section 4, it is indicated that as the number of data items N increases, the take off point in Figure1(a) gets closer to the true value r=0.3. I personally can not understand why the projection error is not zero when r is less than 0.3 and it is zero when r is larger. Shouldn\u2019t it be different from zero whenever r is not equal to 0.3? A comment on this issue would be helpful. \nMinor comment: On legend of figure 1, shouldn\u2019t the logarithms be of base 10, instead of 2? \n\n-The general motivation behind choosing spectral learning algorithms for learning latent variable models over ML based approaches is also due to their speed. There is no mention regarding the proposed algorithm on this aspect. A speed comparison with a ML based approach (possibly EM) would be beneficial. It is also possible to use matrix eigendecomposition based algorithm of Anandkumar et al. (A method of moments for mixture models and HMMs, 2012) which would be computationally cheaper than the tensor decomposition approach.  This is a well written and executed paper for a fairly interesting problem.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
