{
  "name" : "621bf66ddb7c962aa0d22ac97d69b793.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Faster Ridge Regression via the Subsampled Randomized Hadamard Transform",
    "authors" : [ "Yichao Lu", "Paramveer S. Dhillon", "Dean Foster", "Lyle Ungar" ],
    "emails" : [ "dhillon@cis.upenn.edu", "ungar@cis.upenn.edu", "foster@wharton.upenn.edu,", "yichaolu@sas.upenn.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a fast algorithm for ridge regression when the number of features is much larger than the number of observations (p n). The standard way to solve ridge regression in this setting works in the dual space and gives a running time of O(n2p). Our algorithm Subsampled Randomized Hadamard Transform- Dual Ridge Regression (SRHT-DRR) runs in time O(np log(n)) and works by preconditioning the design matrix by a Randomized Walsh-Hadamard Transform with a subsequent subsampling of features. We provide risk bounds for our SRHT-DRR algorithm in the fixed design setting and show experimental results on synthetic and real datasets."
    }, {
      "heading" : "1 Introduction",
      "text" : "Ridge Regression, which penalizes the `2 norm of the weight vector and shrinks it towards zero, is the most widely used penalized regression method. It is of particular interest in the p > n case (p is the number of features and n is the number of observations), as the standard ordinary least squares regression (OLS) breaks in this setting. This setting is even more relevant in today’s age of ‘Big Data’, where it is common to have p n. Thus efficient algorithms to solve ridge regression are highly desirable.\nThe current method of choice for efficiently solving RR is [19], which works in the dual space and has a running time of O(n2p), which can be slow for huge p. As the runtime suggests, the bottleneck is the computation of XX> where X is the design matrix. An obvious way to speed up the algorithm is to subsample the columns of X. For example, suppose X has rank k, if we randomly subsample psubs of the p (k < psubs p ) features, then the matrix multiplication can be performed in O(n2psubs) time, which is very fast! However, this speed-up comes with a big caveat. If all the signal in the problem were to be carried in just one of the p features, and if we missed this feature while sampling, we would miss all the signal.\nA parallel and recently popular line of research for solving large scale regression involves using some kind of random projections, for instance, transforming the data with a randomized Hadamard transform [1] or Fourier transform and then uniformly sampling observations from the resulting transformed matrix and estimating OLS on this smaller data set. The intuition behind this approach is that these frequency domain transformations uniformlize the data and smear the signal across all the observations so that there are no longer any high leverage points whose omission could unduly influence the parameter estimates. Hence, a uniform sampling in this transformed space suffices. This approach can also be viewed as preconditioning the design matrix with a carefully constructed data-independent random matrix. This transformation followed by subsampling has been used in a variety of variations, including Subsampled Randomized Hadamard Transform (SRHT) [4, 6] and Subsampled Randomized Fourier Transform (SRFT) [22, 17].\nIn this paper, we build on the above line of research and provide a fast algorithm for ridge regression (RR) which applies a Randomized Hadamard transform to the columns of the X matrix and then samples psubs = O(n) columns. This allows the bottleneck matrix multiplication in the dual RR to be computed in O(np log(n)) time, so we call our algorithm Subsampled Randomized Hadamard Transform-Dual Ridge Regression (SRHT-DRR).\nIn addition to being computationally efficient, we also prove that in the fixed design setting SRHTDRR only increases the risk by a factor of (1 + C √ k\npsubs ) (where k is the rank of the data matrix)\nw.r.t. the true RR solution."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Using randomized algorithms to handle large matrices is an active area of research, and has been used in a variety of setups. Most of these algorithms involve a step that randomly projects the original large matrix down to lower dimensions [9, 16, 8]. [14] uses a matrix of i.i.d Gaussian elements to construct a preconditioner for least square which makes the problem well conditioned. However, computing a random projection is still expensive as it requires multiplying a huge data matrix by another random dense matrix. [18] introduced the idea of using structured random projection for making matrix multiplication substantially faster.\nRecently, several randomized algorithms have been developed for kernel approximation. [3] provided a fast method for low rank kernel approximation by randomly selecting q samples to construct a rank q approximation of the original kernel matrix. Their approximation can reduce the cost to O(nq2). [15] introduced a random sampling scheme to approximate symmetric kernels and [12] accelerates [15] by applying Hadamard Walsh transform. Although our paper and these papers can all be understood from a kernel approximation point of view, we are working in the p n 1 case while they focus on large n.\nAlso, it is worth distinguishing our setup from standard kernel learning. Kernel methods enable the learning models to take into account a much richer feature space than the original space and at the same time compute the inner product in these high dimensional space efficiently. In our p n 1 setup, we already have a rich enough feature space and it suffices to consider the linear kernel XX> 1. Therefore, in this paper we propose a randomized scheme to reduce the dimension of X and accelerate the computation of XX>."
    }, {
      "heading" : "2 Faster Ridge Regression via SRHT",
      "text" : "In this section we firstly review the traditional solution of solving RR in the dual and it’s computational cost. Then we introduce our algorithm SRHT-DRR for faster estimation of RR."
    }, {
      "heading" : "2.1 Ridge Regression",
      "text" : "Let X be the n × p design matrix containing n i.i.d. samples from the p dimensional independent variable (a.k.a. “covariates” or “predictors”) X such that p n. Y is the real valued n × 1 response vector which contains n corresponding values of the dependent variable Y . is the n × 1 homoskedastic noise vector with common variance σ2. Let β̂λ be the solution of the RR problem, i.e.\nβ̂λ = arg min β∈p×1\n1 n ‖Y −Xβ‖2 + λ‖β‖2 (1)\nThe solution to Equation (1) is β̂λ = (X>X + nλIp)−1X>Y . The step that dominates the computational cost is the matrix inversion which takes O(p3) flops and will be extremely slow when p n 1. A straight forward improvement to this is to solve the Equation (1) in the dual space. By change of variables β = X>α where α ∈ n× 1 and further letting K = XX> the optimization problem becomes\nα̂λ = arg min α∈n×1\n1 n ‖Y −Kα‖2 + λα>Kα (2)\n1For this reason, it is standard in natural language processing applications to just use linear kernels.\nand the solution is α̂λ = (K + nλIn)−1Y which directly gives β̂λ = X>α̂λ. Please see [19] for a detailed derivation of this dual solution. In the p n case the step that dominates computational cost in the dual solution is computing the linear kernel matrix K = XX> which takesO(n2p) flops. This is regarded as the computational cost of the true RR solution in our setup.\nSince our algorithm SRHT-DRR uses Subsampled Randomized Hadamard Transform (SRHT), some introduction to SRHT is warranted."
    }, {
      "heading" : "2.2 Definition and Properties of SRHT",
      "text" : "Following [20], for p = 2q where q is any positive integer, a SRHT can be defined as a psubs × p (p > psubs) matrix of the form:\nΘ =\n√ p\npsubs RHD\nwhere\n• R is a random psubs × p matrix the rows of which are psubs uniform samples (without replacement) from the standard basis of Rp. • H ∈ Rp×p is a normalized Walsh-Hadamard matrix. The Walsh-Hadamard matrix of size p× p is defined recursively: Hp = [ Hp/2 Hp/2 Hp/2 −Hp/2 ] with H2 = [ +1 +1 +1 −1 ] .\nH = 1√pHp is a rescaled version of Hp.\n• D is a p × p diagonal matrix and the diagonal elements are i.i.d. Rademacher random variables.\nThere are two key features that makes SRHT a nice candidate for accelerating RR when p n. Firstly, due to the recursive structure of the H matrix, it takes only O(p log(psubs)) FLOPS to compute Θv where v is a generic p × 1 dense vector while for arbitrary unstructured psubs × p dense matrix A, the cost for computing Av is O(psubsp) flops. Secondly, after projecting any matrix W ∈ p× k with orthonormal columns down to low dimensions with SRHT, the columns of ΘW ∈ psubs × k are still about orthonormal. The following lemma characterizes this property: Lemma 1. Let W be an p × k (p > k) matrix where W>W = Ik. Let Θ be a psubs × p SRHT matrix where p > psubs > k. Then with probability at least 1− (δ + pek ),\n‖(ΘW)>ΘW − Ik‖2 ≤\n√ c log( 2kδ )k\npsubs (3)\nThe bound is in terms of the spectral norm of the matrix. The proof of this lemma is in the Appendix. The tools for the random matrix theory part of the proof come from [20] and [21]. [10] also provided similar results."
    }, {
      "heading" : "2.3 The Algorithm",
      "text" : "Our fast algorithm for SRHT-DRR is described below:\nSRHT-DRR Input: Dataset X ∈ n× p, response Y ∈ n× 1, and subsampling size psubs. Output: The weight parameter β ∈ psubs × 1.\n• Compute the SRHT of the data: XH = XΘ>. • Compute KH = XHX>H • Compute αH,λ = (KH + nλIn)−1Y , which is the solution of Equation (2) obtained by\nreplacing K with KH . • Compute βH,λ = X>HαH,λ\nSince, SRHT is only defined for p = 2q for any integer q, so, if the dimension p is not a power of 2, we can concatenate a block of zero matrix to the feature matrix X to make the dimension a power of 2.\nRemark 1. Let’s look at the computational cost of SRHT-DRR. Computing XH takes O(np log(psubs)) FLOPS [2, 6]. Once we have XH , computing αH,λ costs O(n2psubs) FLOPS, with the dominating step being computing KH = XHX>H . So the computational cost for computing αH,λ is O(np log(psubs) + n2psubs), compared to the true RR which costs O(n2p). We will discuss how large psubs should be later after stating the main theorem."
    }, {
      "heading" : "3 Theory",
      "text" : "In this section we bound the risk of SRHT-DRR and compare it with the risk of the true dual ridge estimator in fixed design setting.\nAs earlier, let X be an arbitrary n× p design matrix such that p n. Also, we have Y = Xβ + , where is the n× 1 homoskedastic noise vector with common mean 0 and variance σ2. [5] and [3] did similar analysis for the risk of RR under similar fixed design setups.\nFirstly, we provide a corollary to Lemma 1 which will be helpful in the subsequent theory.\nCorollary 1. Let k be the rank of X. With probability at least 1− (δ + p ek )\n(1−∆)K KH (1 + ∆)K (4)\nwhere ∆ = C √\nk log(2k/δ) psubs . ( as for p.s.d. matrices G L means G− L is p.s.d.)\nProof. Let X = UDV> be the SVD of X where U ∈ n × k, V ∈ p × k has orthonormal columns and D ∈ k×k is diagonal. Then KH = UD(V>ΘΘV)DU>. Lemma 1 directly implies Ik(1 −∆) (V>ΘΘV) Ik(1 + ∆) with probability at least 1 − (δ + pek ). Left multiply UD and right multiply DU> to the above inequality complete the proof."
    }, {
      "heading" : "3.1 Risk Function for Ridge Regression",
      "text" : "Let Z = E (Y ) = Xβ. The risk for any prediction Ŷ ∈ n× 1 is 1nE ‖Ŷ − Z‖ 2. For any n× n positive symmetric definite matrix M, define the following risk function.\nR(M) = σ2\nn Tr[M2(M + nλIn)−2] + nλ2Z>(M + nλIn)−2Z (5)\nLemma 2. Under the fixed design setting, the risk for the true RR solution is R(K) and the risk for SRHT-DRR is R(KH).\nProof. The risk of the SRHT-DRR estimator is\n1 n E ‖KHαH,λ − Z‖2 = 1 n E ‖KH(KH + nλIn)−1Y − Z‖2\n= 1\nn E ‖KH(KH + nλIn)−1Y − E (KH(KH + nλIn)−1Y )‖2 + 1\nn ‖E (KH(KH + nλIn)−1Y )− Z‖2\n= 1\nn E ‖KH(KH + nλIn)−1 ‖2 + 1\nn ‖(KH(KH + nλIn)−1Z − Z‖2\n= 1\nn Tr[K2H(KH + nλIn) −2 >] + 1\nn Z>(In −KH(KH + nλIn)−1)2Z\n= σ2\nn Tr[K2H(KH + nλIn) −2] +nλ2Z>(KH + nλIn) −2Z\n(6)\nNote that the expectation here is only over the random noise and it is conditional on the Randomized Hadamard Transform. The calculation is the same for the ordinary estimator. In the risk function, the first term is the variance and the second term is the bias."
    }, {
      "heading" : "3.2 Risk Inflation Bound",
      "text" : "The following theorem bounds the risk inflation of SRHT-DRR compared with the true RR solution. Theorem 1. Let k be the rank of the X matrix. With probability at least 1− (δ + p\nek )\nR(KH) ≤ (1−∆)−2R(K) (7) where ∆ = C √\nk log(2k/δ) psubs\nProof. Define\nB(M) = nλ2Z>(M + nλIn) −2Z\nV (M) = σ2\nn Tr[K2H(KH + nλIn) −2]\nfor any p.s.d matrix M ∈ n × n. Therefore, R(M) = V (M) + B(M). Now, due to [3] we know that B(M) is non-increasing in M and V (M) is non-decreasing in M. When Equation(4) holds,\nR(KH) = V (KH) +B(KH)\n≤ V ((1 + ∆)K) +B((1−∆)K) ≤ (1 + ∆)2V (K) + (1−∆)−2B(K) ≤ (1−∆)−2(V (K) +B(K)) = (1−∆)−2R(K)\nRemark 2. Theorem 1 gives us an idea of how large psubs should be. Assuming ∆ (the risk inflation ratio) is fixed, we get psubs = C k log(2k/δ) ∆2 = O(k). If we further assume that X is full rank, i.e. k = n, then, it suffices to choose psubs = O(n). Combining this with Remark 1, we can see that the cost of computing XH is O(np log(n)). Hence, under the ideal setup where p is huge so that the dominating step of SRHT-DRR is computing XH , the computational cost of SRHT-DRR O(np log(n)) FLOPS.\nComparison with PCA Another way to handle high dimensional features is to use PCA and run regression only on the top few principal components (this procedure is called PCR), as illustrated by [13] and many other papers. RR falls in the family of “shrinkage” estimators as it shrinks the weight parameter towards zero. On the other hand, PCA is a “keep-or-kill” estimator as it kills components with smaller eigenvalues. Recently, [5] have shown that the risk of PCR and RR are related and that the risk of PCR is bounded by four times the risk of RR. However, we believe that both PCR and RR are parallel approaches and one can be better than the other depending on the structure of the problem, so it is hard to compare SRHT-DRR with PCR theoretically.\nMoreover, PCA under our p n 1 setup is itself a non-trivial problem both statistically and computationally. Firstly, in the p n case we do not have enough samples to estimate the huge p × p covariance matrix. Therefore the eigenvectors of the sample covariance matrix obtained by PCA maybe very different from the truth. (See [11] for a theoretical study on the consistency of the principal directions for the high p low n case.) Secondly, PCA requires one to compute an SVD of the X matrix, which is extremely slow when p n 1. An alternative is to use a randomized algorithm such as [16] or [9] to compute PCA. Again, whether randomized PCA is better than our SRHT-DRR algorithm depends on the problem. With that in mind, we compare SRHT-DRR against standard as well as Randomized PCA in our experiments section; We find that SRHT-DRR beats both of them in speed as well as accuracy."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section we show experimental results on synthetic as well as real-world data highlighting the merits of SRHT, namely, lower computational cost compared to the true Ridge Regression (RR) solution, without any significant loss of accuracy. We also compare our approach against “standard” PCA as well as randomized PCA [16].\nIn all our experiments, we choose the regularization constant λ via cross-validation on the training set. As far as PCA algorithms are concerned, we implemented standard PCA using the built in SVD function in MATLAB and for randomized PCA we used the block power iteration like approach proposed by [16]. We always achieved convergence in three power iterations of randomized PCA."
    }, {
      "heading" : "4.1 Measures of Performance",
      "text" : "Since we know the true β which generated the synthetic data, we report MSE/Risk for the fixed design setting (they are equivalent for squared loss) as measure of accuracy. It is computed as ‖Ŷ − Xβ‖2, where Ŷ is the prediction corresponding to different methods being compared. For real-world data we report the classification error on the test set.\nIn order to compare the computational cost of SHRT-DRR with true RR, we need to estimate the number of FLOPS used by them. As reported by other papers, e.g. [4, 6], the theoretical cost of applying Randomized Hadamard Transform is O(np log(psubs)). However, the MATLAB implementation we used took about np log(p) FLOPS to compute XH . So, for SRHT-DRR, the total computational cost is np log(p) for getting XH and a further 2n2psubs FLOPS to compute KH . As mentioned earlier, the true dual RR solution takes≈ 2n2p. So, in our experiments, we report relative computational cost which is computed as the ratio of the two.\nRelative Computational Cost = p log(p) · n+ 2n2psubs\n2n2p"
    }, {
      "heading" : "4.2 Synthetic Data",
      "text" : "We generated synthetic data with p = 8192 and varied the number of observations n = 20, 100, 200. We generated a n × n matrix R ∼ MVN(0, I) where MVN(µ,Σ) is the Multivariate Normal Distribution with mean vector µ, variance-covariance matrix Σ and βj ∼ N (0, 1) ∀j = 1, . . . , p. The final X matrix was generated by rotating R with a randomly generated n × p rotation matrix. Finally, we generated the Ys as Y = Xβ + where i ∼ N (0, 1) ∀i = 1, . . . , n.\nFor PCA and randomized PCA, we tried keeping r PCs in the range 10 to n and finally chose the value of r which gave the minimum error on the training set. We tried 10 different values for psubs from n+ 10 to 2000 . All the results were averaged over 50 random trials.\nThe results are shown in Figure 1. There are two main things worth noticing. Firstly, in all the cases, SRHT-DRR gets very close in accuracy to the true RR with only ≈ 30% of its computational cost. SRHT-DRR also cost much fewer FLOPS than the Randomized PCA for our experiments. Secondly, as we mentioned earlier, RR and PCA are parallel approaches. Either one might be better than the other depending on the structure of the problem. As can be seen, for our data, RR approaches are always better than PCA based approaches. We hypothesize that PCA might perform better relative to RR for larger n."
    }, {
      "heading" : "4.3 Real world Data",
      "text" : "We took the UCI ARCENE dataset which has 200 samples with 10000 features as our real world dataset. ARCENE is a binary classification dataset which consists of 88 cancer individuals and 112 healthy individuals (see [7] for more details about this dataset). We split the dataset into 100 training and 100 testing samples and repeated this procedure 50 times (so n = 100, p = 10000 for this dataset). For PCA and randomized PCA, we tried keeping r = 10, 20, 30, 40, 50, 60, 70, 80, 90 PCs and finally chose the value of r which gave the minimum error on the training set (r = 30). As earlier, we tried 10 different values for psubs: 150, 250, 400, 600, 800, 1000, 1200, 1600, 2000, 2500.\nStandard PCA is known to be slow for this size datasets, so the comparison with it is just for accuracy. Randomized PCA is fast but less accurate than standard (“true”) PCA; its computational cost for r = 30 can be approximately calculated as about 240np (see [9] for details), which in this case is roughly the same as computing XX> (≈ 2n2p). The results are shown in Figure 2. As can be seen, SRHT-DRR comes very close in accuracy to the true RR solution with just ≈ 30% of its computational cost. SRHT-DRR beats PCA and Randomized PCA even more comprehensively, achieving the same or better accuracy at just≈ 18% of their computational cost."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper we proposed a fast algorithm, SRHT-DRR, for ridge regression in the p n 1 setting SRHT-DRR preconditions the design matrix by a Randomized Walsh-Hadamard Transform with a subsequent subsampling of features. In addition to being significantly faster than the true dual ridge regression solution, SRHT-DRR only inflates the risk w.r.t. the true solution by a small amount. Experiments on both synthetic and real data show that SRHT-DRR gives significant speeds up with only small loss of accuracy. We believe similar techniques can be developed for other statistical methods such as logistic regression.\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.13 0.14 0.155 0.175 0.195 0.215 0.235 0.275 0.315 0.365\nRelative Computational Cost\nC la\ns s\nif ic\na ti\no n\nE rr\no r\nTrue RR Solution\nPCA\nRandomized PCA"
    } ],
    "references" : [ {
      "title" : "Approximate nearest neighbors and the fast johnsonlindenstrauss transform",
      "author" : [ "Nir Ailon", "Bernard Chazelle" ],
      "venue" : "In STOC,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Fast dimension reduction using rademacher series on dual bch codes",
      "author" : [ "Nir Ailon", "Edo Liberty" ],
      "venue" : "Technical report,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Sharp analysis of low-rank kernel matrix approximations",
      "author" : [ "Francis Bach" ],
      "venue" : "CoRR, abs/1208.2015,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Improved matrix algorithms via the subsampled randomized hadamard transform",
      "author" : [ "Christos Boutsidis", "Alex Gittens" ],
      "venue" : "CoRR, abs/1204.0062,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "A risk comparison of ordinary least squares vs ridge regression",
      "author" : [ "Paramveer S. Dhillon", "Dean P. Foster", "Sham M. Kakade", "Lyle H. Ungar" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Faster least squares approximation",
      "author" : [ "Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan", "Tamás Sarlós" ],
      "venue" : "CoRR, abs/0710.1435,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Design of experiments for the nips 2003 variable selection benchmark",
      "author" : [ "Isabelle Guyon" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P.G. Martinsson", "J.A. Tropp" ],
      "venue" : "SIAM Rev.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "An algorithm for the principal component analysis of large data sets",
      "author" : [ "Nathan Halko", "Per-Gunnar Martinsson", "Yoel Shkolnisky", "Mark Tygert" ],
      "venue" : "SIAM J. Scientific Computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Analysis of a randomized approximation scheme for matrix multiplication",
      "author" : [ "Daniel Hsu", "Sham M. Kakade", "Tong Zhang" ],
      "venue" : "CoRR, abs/1211.5414,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "PCA consistency in high dimension, low sample size context",
      "author" : [ "S. Jung", "J.S. Marron" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Fastfood -approximating kernel expansions in loglinear time",
      "author" : [ "Quoc Le", "Tamas Sarlos", "Alex Smola" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Principal components regression in exploratory statistical research",
      "author" : [ "W.F. Massy" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1965
    }, {
      "title" : "Lsrn: A parallel iterative solver for strongly over- or under-determined systems",
      "author" : [ "Xiangrui Meng", "Michael A. Saunders", "Michael W. Mahoney" ],
      "venue" : "CoRR, abs/1109.5981,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "Ali Rahimi", "Ben Recht" ],
      "venue" : "Neural Infomration Processing Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "A randomized algorithm for principal component analysis",
      "author" : [ "Vladimir Rokhlin", "Arthur Szlam", "Mark Tygert" ],
      "venue" : "SIAM J. Matrix Analysis Applications,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "A fast randomized algorithm for overdetermined linear least-squares regression",
      "author" : [ "Vladimir Rokhlin", "Mark Tygert" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "Tamas Sarlos" ],
      "venue" : "Proc. 47th Annu. IEEE Sympos. Found. Comput. Sci,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Ridge regression learning algorithm in dual variables",
      "author" : [ "G. Saunders", "A. Gammerman", "V. Vovk" ],
      "venue" : "In Proc. 15th International Conf. on Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1998
    }, {
      "title" : "Improved analysis of the subsampled randomized hadamard transform",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "CoRR, abs/1011.1595,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "A fast algorithm for computing minimal-norm solutions to underdetermined systems of linear equations",
      "author" : [ "Mark Tygert" ],
      "venue" : "CoRR, abs/0905.4745,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "The current method of choice for efficiently solving RR is [19], which works in the dual space and has a running time of O(n(2)p), which can be slow for huge p.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "A parallel and recently popular line of research for solving large scale regression involves using some kind of random projections, for instance, transforming the data with a randomized Hadamard transform [1] or Fourier transform and then uniformly sampling observations from the resulting transformed matrix and estimating OLS on this smaller data set.",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 3,
      "context" : "This transformation followed by subsampling has been used in a variety of variations, including Subsampled Randomized Hadamard Transform (SRHT) [4, 6] and Subsampled Randomized Fourier Transform (SRFT) [22, 17].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 5,
      "context" : "This transformation followed by subsampling has been used in a variety of variations, including Subsampled Randomized Hadamard Transform (SRHT) [4, 6] and Subsampled Randomized Fourier Transform (SRFT) [22, 17].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 21,
      "context" : "This transformation followed by subsampling has been used in a variety of variations, including Subsampled Randomized Hadamard Transform (SRHT) [4, 6] and Subsampled Randomized Fourier Transform (SRFT) [22, 17].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 16,
      "context" : "This transformation followed by subsampling has been used in a variety of variations, including Subsampled Randomized Hadamard Transform (SRHT) [4, 6] and Subsampled Randomized Fourier Transform (SRFT) [22, 17].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 8,
      "context" : "Most of these algorithms involve a step that randomly projects the original large matrix down to lower dimensions [9, 16, 8].",
      "startOffset" : 114,
      "endOffset" : 124
    }, {
      "referenceID" : 15,
      "context" : "Most of these algorithms involve a step that randomly projects the original large matrix down to lower dimensions [9, 16, 8].",
      "startOffset" : 114,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "Most of these algorithms involve a step that randomly projects the original large matrix down to lower dimensions [9, 16, 8].",
      "startOffset" : 114,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "[18] introduced the idea of using structured random projection for making matrix multiplication substantially faster.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[3] provided a fast method for low rank kernel approximation by randomly selecting q samples to construct a rank q approximation of the original kernel matrix.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "[15] introduced a random sampling scheme to approximate symmetric kernels and [12] accelerates [15] by applying Hadamard Walsh transform.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[15] introduced a random sampling scheme to approximate symmetric kernels and [12] accelerates [15] by applying Hadamard Walsh transform.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 14,
      "context" : "[15] introduced a random sampling scheme to approximate symmetric kernels and [12] accelerates [15] by applying Hadamard Walsh transform.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "Please see [19] for a detailed derivation of this dual solution.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 19,
      "context" : "Following [20], for p = 2 where q is any positive integer, a SRHT can be defined as a psubs × p (p > psubs) matrix of the form:",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "The tools for the random matrix theory part of the proof come from [20] and [21].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 20,
      "context" : "The tools for the random matrix theory part of the proof come from [20] and [21].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "Computing XH takes O(np log(psubs)) FLOPS [2, 6].",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "Computing XH takes O(np log(psubs)) FLOPS [2, 6].",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "[5] and [3] did similar analysis for the risk of RR under similar fixed design setups.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[5] and [3] did similar analysis for the risk of RR under similar fixed design setups.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 2,
      "context" : "Now, due to [3] we know that B(M) is non-increasing in M and V (M) is non-decreasing in M.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "Comparison with PCA Another way to handle high dimensional features is to use PCA and run regression only on the top few principal components (this procedure is called PCR), as illustrated by [13] and many other papers.",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "Recently, [5] have shown that the risk of PCR and RR are related and that the risk of PCR is bounded by four times the risk of RR.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : "(See [11] for a theoretical study on the consistency of the principal directions for the high p low n case.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 15,
      "context" : "An alternative is to use a randomized algorithm such as [16] or [9] to compute PCA.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "An alternative is to use a randomized algorithm such as [16] or [9] to compute PCA.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 15,
      "context" : "We also compare our approach against “standard” PCA as well as randomized PCA [16].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 15,
      "context" : "As far as PCA algorithms are concerned, we implemented standard PCA using the built in SVD function in MATLAB and for randomized PCA we used the block power iteration like approach proposed by [16].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "[4, 6], the theoretical cost of applying Randomized Hadamard Transform is O(np log(psubs)).",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "[4, 6], the theoretical cost of applying Randomized Hadamard Transform is O(np log(psubs)).",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "ARCENE is a binary classification dataset which consists of 88 cancer individuals and 112 healthy individuals (see [7] for more details about this dataset).",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "Randomized PCA is fast but less accurate than standard (“true”) PCA; its computational cost for r = 30 can be approximately calculated as about 240np (see [9] for details), which in this case is roughly the same as computing XX> (≈ 2n(2)p).",
      "startOffset" : 155,
      "endOffset" : 158
    } ],
    "year" : 2013,
    "abstractText" : "We propose a fast algorithm for ridge regression when the number of features is much larger than the number of observations (p n). The standard way to solve ridge regression in this setting works in the dual space and gives a running time of O(np). Our algorithm Subsampled Randomized Hadamard TransformDual Ridge Regression (SRHT-DRR) runs in time O(np log(n)) and works by preconditioning the design matrix by a Randomized Walsh-Hadamard Transform with a subsequent subsampling of features. We provide risk bounds for our SRHT-DRR algorithm in the fixed design setting and show experimental results on synthetic and real datasets.",
    "creator" : null
  }
}