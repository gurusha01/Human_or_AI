{
  "name" : "5c572eca050594c7bc3c36e7e8ab9550.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Inverse Density as an Inverse Problem: the Fredholm Equation Approach",
    "authors" : [ "Qichao Que", "Mikhail Belkin" ],
    "emails" : [ "que@cse.ohio-state.edu", "mbelkin@cse.ohio-state.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In this paper we address the problem of estimating the ratio of two functions, q(x)p(x) where p is given by a sample and q(x) is either a known function or another probability density function given by a sample. This estimation problem arises naturally when one attempts to integrate a function with respect to one density, given its values on a sample obtained from another distribution. Recently there have been a significant amount of work on estimating the density ratio (also known as the importance function) from sampled data, e.g., [6, 10, 9, 22, 2]. Many of these papers consider this problem in the context of covariate shift assumption [19] or the so-called selection bias [27]. The approach taken in our paper is based on reformulating the density ratio estimation as an integral equation, known as the Fredholm equation of the first kind, and solving it using the tools of regularization and Reproducing Kernel Hilbert Spaces. That allows us to develop simple and flexible algorithms for density ratio estimation within the popular kernel learning framework. The connection to the classical operator theory setting makes it easier to apply the standard tools of spectral and Fourier analysis to obtain theoretical results.\nWe start with the following simple equality underlying the importance sampling method: Eq(h(x)) = ∫ h(x)q(x)dx = ∫\nh(x) q(x) p(x) p(x)dx = Ep\n( h(x)\nq(x) p(x)\n) (1)\nBy replacing the function h(x) with a kernel k(x, y), we obtain\nKp q\np (x) :=\n∫ k(x, y)\nq(y) p(y)\np(y)dy = ∫ k(x, y)q(y)dy := Kq1(x). (2)\nThinking of the function q(x)p(x) as an unknown quantity and assuming that the right hand side is known this becomes a Fredholm integral equation. Note that the right-hand side can be estimated given a sample from q while the operator on the left can be estimated using a sample from p.\nTo push this idea further, suppose kt(x, y) is a “local” kernel, (e.g., the Gaussian, kt(x, y) = 1\n(2πt)d/2 e−\n‖x−y‖2 2t ) such that ∫ Rd kt(x, y)dx = 1. When we use δ-kernels, like Gaussian, and f\nsatisfies some smoothness conditions, we have ∫\nRd kt(x, y)f(x)dx = f(y) + O(t) (see [24], Ch. 1). Thus we get another (approximate) integral equality:\nKt,p q\np (y) := ∫ Rd kt(x, y) q(x) p(x) p(x)dx ≈ q(y). (3)\nIt becomes an integral equation for q(x)p(x) , assuming that q is known or can be approximated.\nWe address these inverse problems by formulating them within the classical framework of TiknonovPhilips regularization with the penalty term corresponding to the norm of the function in the Reproducing Kernel Hilbert Space H with kernel kH used in many machine learning algorithms.\n[Type I]: q\np ≈ arg min f∈H ‖Kpf−Kq1(x)‖2L2,p +λ‖f‖ 2 H [II]:\nq p ≈ arg min f∈H ‖Kt,pf−q‖2L2,p +λ‖f‖ 2 H\nImportantly, given a sample x1, . . . , xn from p, the integral operator Kpf applied to a function f can be approximated by the corresponding discrete sum Kpf(x) ≈ 1n ∑ i f(xi)K(xi, x), while L2,p norm is approximated by an average: ‖f‖2L2,p ≈ 1 n ∑ i f(xi)\n2. Of course, the same holds for a sample from q. We see that the Type I formulation is useful when q is a density and samples from both p and q are available, while the Type II is useful, when the values of q (which does not have to be a density function at all1) are known at the data points sampled from p.\nSince all of these involve only function evaluations at the sample points, an application of the usual representer theorem for Reproducing Kernel Hilbert Spaces, leads to simple, explicit and easily implementable algorithms, representing the solution of the optimization problem as linear combinations of the kernels over the points of the sample ∑ i αikH(xi, x) (see Section 2). We call the resulting algorithms FIRE for Fredholm Inverse Regularized Estimator.\nRemark: Other norms and loss functions. Norms and loss functions other that L2,p can also be used in our setting as long as they can be approximated from a sample using function evaluations. 1. Perhaps, the most interesting is L2,q norm available in the Type I setting, when a sample from the probability distribution q is available. In fact, given a sample from both p and q we can use the combined empirical norm γ‖ · ‖L2,p + (1 − γ)‖ · ‖L2,q . Optimization using those norms leads to some interesting kernel algorithms described in Section 2. We note that the solution is still a linear combination of kernel functions centered on the sample from p and can still be written explicitly. 2. In Type I formulation, if the kernels k(x, y) and kH(x, y) coincide, it is possible to use the RKHS norm ‖ · ‖H instead of L2,p. This formulation (see Section 2) also yields an explicit formula and is related to the Kernel Mean Matching [9] , although with a different optimization procedure.\nSince we are dealing with a classical inverse problem for integral operators, our formulation allows for theoretical analysis using the methods of spectral theory. In Section 3 we present concentration and error bounds as well as convergence rates for our algorithms when data are sampled from a distribution defined in Rd, a domain in Rd with boundary or a compact d-dimensional sub-manifold of a Euclidean space RN for the case of the Gaussian kernel.\nIn Section 4 we introduce a unsupervised method, referred as CD-CV (for cross-density crossvalidation) for model selection and discuss the experimental results on several data sets comparing our method FIRE with the available alternatives, Kernel Mean Matching (KMM) [9] and LSIF [10] as well as the base-line thresholded inverse kernel density estimator2 (TIKDE) and importance sampling (when available).\n1This could be useful in sampling procedures, when the normalizing coefficients are hard to estimate. 2The standard kernel density estimator for q divided by a thresholded kernel density estimator for p.\nWe summarize the contributions of the paper as follows: 1. We provide a formulation of estimating the density ratio (importance function) as a classical inverse problem, known as the Fredholm equation, establishing a connections to the methods of classical analysis. The underlying idea is to “linearize” the properties of the density by studying an associated integral operator. 2. To solve the resulting inverse problems we apply regularization with an RKHS norm penalty. This provides a flexible and principled framework, with a variety of different norms and regularization techniques available. It separates the underlying inverse problem from the necessary regularization and leads to a family of very simple and direct algorithms within the kernel learning framework in machine learning. 3. Using the techniques of spectral analysis and concentration, we provide a detailed theoretical analysis for the case of the Gaussian kernel, for Euclidean case as well as for distributions supported on a sub-manifold. We prove error bounds and as well as the convergence rates. 4. We also propose a completely unsupervised technique, CD-CV, for cross-validating the parameters of our algorithm and demonstrate its usefulness, thus addressing in our setting one of the most thorny issues in unsupervised/semi-supervised learning. We evaluate and compare our methods on several different data sets and in various settings and demonstrate strong performance and better computational efficiency compared to the alternatives. Related work. Recently the problem of density ratio estimation has received significant attention due in part to the increased interest in transfer learning [15] and, in particular to the form of transfer learning known as covariate shift [19]. To give a brief summary, given the feature space X and the label space Y , two probability distributions p and q on X × Y satisfy the covariate assumption if for all x, y, p(y|x) = q(y|x). It is easy to see that training a classifier to minimize the error for q, given a sample from p requires estimating the ratio of the marginal distributions qX(x)pX(x) . The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].\nThe algorithm most closely related to ours is Kernel Mean Matching [9]. It is based on the equation: Eq(Φ(x)) = Ep( qpΦ(x)), where Φ is the feature map corresponding to an RKHS H. It is rewritten as an optimization problem q(x)p(x) ≈ arg minβ∈L2,β(x)>0,Ep(β)=1 ‖Eq(Φ(x)) − Ep(β(x)Φ(x))‖H. The quantity on the right can be estimated given a sample from p and a sample from q and the minimization becomes a quadratic optimization problem over the values of β at the points sampled from p. Writing down the feature map explicitly, i.e., recalling that Φ(x) = KH(x, ·), we see that the equality Eq(Φ(x)) = Ep( qpΦ(x)) is equivalent to the integral equation Eq. 2 considered as an identity in the Hilbert space H. Thus the problem of KMM can be viewed within our setting Type I (see the Remark 2 in the introduction), with a RKHS norm but a different optimization algorithm.\nHowever, while the KMM optimization problem uses the RKHS norm, the weight function β itself is not in the RKHS. Thus, unlike most other algorithms in the RKHS framework (in particular, FIRE), the empirical optimization problem does not have a natural out-of-sample extension. Also, since there is no regularizing term, the problem is less stable (see Section 4 for some experimental comparisons) and the theoretical analysis is harder (however, see [6] and the recent paper [26] for some nice theoretical analysis of KMM in certain settings).\nAnother related recent algorithm is Least Squares Importance Sampling (LSIF) [10], which attempts to estimate the density ratio by choosing a parametric linear family of functions and choosing a function from this family to minimize the L2,p distance to the density ratio. A similar setting with the Kullback-Leibler distance (KLIEP) was proposed in [23]. This has an advantage of a natural out-of-sample extension property. We note that our method for unsupervised parameter selection in Section 4 is related to their ideas. However, in our case the set of test functions does not need to form a good basis since no approximation is required.\nWe note that our methods are closely related to a large body of work on kernel methods in machine learning and statistical estimation (e.g., [21, 17, 16]). Many of these algorithms can be interpreted as inverse problems, e.g., [3, 20] in the Tikhonov regularization or other regularization frameworks. In particular, we note interesting methods for density estimation proposed in [12] and estimating the support of density through spectral regularization in [4], as well as robust density estimation using RKHS formulations [11] and conditional density [8]. We also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [25, 18, 1]. Among those works that provide theoretical analysis of algorithms for estimating density ratios,\n[14] establishes minimax rates for likelihood ratio estimation. Another recent theoretical analysis of KMM in [26] contains bounds for the output of the corresponding integral operators."
    }, {
      "heading" : "2 Settings and Algorithms",
      "text" : "Settings and objects. We start by introducing objects and function spaces important for our development. As usual, the norm in space of square-integrable functions with respect to a measure ρ, is defined as follows: L2,ρ = { f : ∫ Ω |f(x)|2dρ < ∞ } . This is a Hilbert space with the inner\nproduct defined in the usual way by 〈f, g〉2,ρ = ∫ Ω\nf(x)g(x)dρ. Given a kernel k(x, y) we define the operator Kρ: Kρf(y) := ∫ Ω\nk(x, y)f(x)dρ(x). We will use the notation Kt,ρ to explicitly refer to the parameter of the kernel function kt(x, y), when it is a δ-family. If the function k(x, y) is symmetric and positive definite, then there is a corresponding Reproducing Kernel Hilbert space (RKHS) H. We recall the key property of the kernel kH: for any f ∈ H, 〈f, kH(x, ·)〉H = f(x). The Representer Theorem allows us to write solutions to various optimization problems over H in terms of linear combinations of kernels supported on sample points (see [21] for an in-depth discussion or the RKHS theory and the issues related to learning). Given a sample x1, . . . , xn from p, one can approximate the L2,p norm of a sufficiently smooth functionf by ‖f‖22,p ≈ 1n ∑ i |f(xi)|2, and\nsimilarly, the integral operator Kpf(x) ≈ 1n ∑\ni k(xi, x)f(xi). These approximate equalities can be made precise by using appropriate concentration inequalities.\nThe FIRE Algorithms. As discussed in the introduction, the starting point for our development is the two integral equalities,\n[I]: Kp q\np (·) =\n∫ k(·, y)q(y)\np(y) dp(y) = Kq1(·) [II]:Kt,p\nq p (·) =\n∫ kt(·, y)\nq(y) p(y) dp(y) = q(·)+ o(1)\n(4) Notice that in the Type I setting, the kernel does not have to be in a δ-family. For example, a linear kernel is admissible. Type II setting comes from the fact Kt,qf(x) ≈ f(x)p(x) + O(t) for a “δfunction-like” kernel and we keep t in the notation in that case. Assuming that either Kq1 or q are (approximately) known (Type I and II settings, respectively) equalities in Eqs. 4 become integral equations for pq , known as Fredholm equations of the first kind. To estimate p q , we need to obtain an approximation to the solution which (a) can be obtained computationally from sampled data, (b) is stable with respect to sampling and other perturbation of the input function, (c) can be analyzed using the standard machinery of functional analysis.\nTo provide a framework for solving these inverse problems, we apply the classical techniques of regularization combined with the RKHS norm popular in machine learning. In particular a simple formulation of Type I using Tikhonov regularization, ([5], Ch. 5), with the L2,p norm is as follows:\n[Type I]: f Iλ = arg min f∈H ‖Kpf −Kq1‖22,p + λ‖f‖2H (5)\nHere H is an appropriate Reproducing Kernel Hilbert Space. Similarly Type II can be solved by [Type II]: f IIλ = arg min\nf∈H ‖Kt,pf − q‖22,p + λ‖f‖2H (6)\nWe will now discuss the empirical versions of these equations and the resulting algorithms.\nType I setting. Algorithm for L2,p norm. Given an iid sample from p, zp = {xi}ni=1 and an iid sample from q, zq = {x′j}mj=1 (z for the combined sample), we can approximate the integral operators Kp and Kq by Kzpf(x) = 1n ∑ xi∈zp k(xi, x)f(xi) and Kzqf(x) = 1 m ∑ x′i∈zq k(x′i, x)f(x ′ i). Thus the empirical version of Eq. 5 becomes\nf Iλ,z = arg min f∈H 1 n ∑ xi∈zp ((Kzpf)(xi)− (Kzq1)(xi))2 + λ‖f‖2H (7)\nThe first term of the optimization problem involves only evaluations of the function f at the points of the sample. From Representer Theorem and matrix manipulation, we obtain the following:\nf Iλ,z(x) = ∑\nxi∈zp\nkH(xi, x)vi and v = ( K2p,pKH + nλI )−1 Kp,pKp,q1zq . (8)\nwhere the kernel matrices are defined as follows: (Kp,p)ij = 1nk(xi, xj), (KH)ij = kH(xi, xj) for xi, xj ∈ zp and Kp,q is defined as (Kp,q)ij = 1mk(xi, x ′ j) for xi ∈ zp and x′j ∈ zq.\nIf KH and Kp,p are the same kernel we simply have: v = 1n ( K3p,p + λI )−1 Kp,pKp,q1zq .\nAlgorithms for γL2,p +(1−γ)L2,q norm. Depending on the setting, we may want to minimize the error of the estimate over the probability distribution p, q or over some linear combination of these. A significant potential benefit of using a linear combination is that both samples can be used at the same time in the loss function. First we state the continuous version of the problem:\nf *λ = arg min f∈H γ‖Kpf −Kq1‖22,p + (1− γ)‖Kpf −Kq1‖22,q + λ‖f‖2H (9)\nGiven a sample from p, zp = {x1, x2, . . . , xn} and a sample from q, zq = {x′1, x′2, . . . , x′m} we obtain an empirical version of the Eq. 9: f∗λ,z(x) =\narg min f∈H\nγ\nn ∑ xi∈zp ( Kzpf(xi)−Kzq1(x p i ) )2 + 1− γ m ∑ x′i∈zq ( (Kzpf)(x′i)− (Kzq1)(x′i) )2 +λ‖f‖2H From the Representer Theorem f∗λ,z(x) = ∑ xi∈zp vikH(xi, x) v = (K + nλI) −1 K11zq\nK = ( γ\nn (Kp,p)2 + 1− γ m KTq,pKq,p\n) KH and K1 = ( γ\nn Kp,pKp,q + 1− γ m KTq,pKq,q ) where (Kp,p)ij = 1nk(xi, xj), (KH)ij = kH(xi, xj) for xi, xj ∈ zp, and (Kp,q)ij = 1 mk(xi, x ′ j) and (Kq,p)ji = 1nk(x ′ j , xi) for xi ∈ zp,x′j ∈ zq. Despite the loss function combining both samples, the solution is still a summation of kernels over the points in the sample from p.\nAlgorithms for the RKHS norm. In addition to using the RKHS norm for regularization norm, we can also use it as a loss function: f *λ = arg minf∈H ‖Kpf − Kq1‖2H′ + λ‖f‖2H Here the Hilbert space H′ must correspond to the kernel k and can potentially be different from the space H used for regularization. Note that this formulation is only applicable in the Type I setting since it requires the function q to belong to the RKHS H′. Given two samples zp,zq, it is easy to write down the empirical version of this problem, leading to the following formula:\nf∗λ,z(x) = ∑\nxi∈zp\nvikH(xi, x) v = (Kp,pKH + nλI) −1 Kp,q1zq . (10)\nThe result is somewhat similar to our Type I formulation with the L2,p norm. We note the connection between this formulation of using the RKHS norm as a loss function and the KMM algorithm [9]. When the kernels K and KH are the same, Eq. 10 can be viewed as a regularized version of KMM (with a different optimization procedure).\nType II setting. In Type II setting we assume that we have a sample z = {xi}ni=1 drawn from p and that we know the function values q(xi) at the points of the sample. Replacing the norm and the integral operator with their empirical versions, we obtain the following optimization problem:\nf IIλ,z = arg min f∈H 1 n ∑ xi∈z (Kt,zpf(xi)− q(xi))2 + λ‖f‖2H (11)\nAs before, using the Representer Theorem we obtain an analytical formula for the solution: f IIλ,z(x) = ∑ xi∈z kH(xi, x)vi where v = ( K2KH + nλI )−1 Kq.\nwhere the kernel matrix K is defined by Kij = 1nkt(xi, xj), (KH)ij = kH(xi, xj) and qi = q(xi).\nComparison of type I and type II settings. 1. In Type II setting q does not have to be a density function (i.e., non-negative and integrate to one). 2. Eq. 7 of the Type I setting cannot be easily solved in the absence of a sample zq from q, since estimating Kq requires either sampling from q (if it is a density) or estimating the integral in some other way, which may be difficult in high dimension but perhaps of interest in certain low-dimensional application domains. 3. There are a number of problems (e.g., many problems involving MCMC) where q(x) is known explicitly (possibly up to a multiplicative constant), while sampling from q is expensive or even impossible computationally [13]. 4. Unlike Eq. 5, Eq. 6 has an error term depending on the kernel. For example, in the important case of the Gaussian kernel, the error is of the order O(t), where t is the variance of Gaussian. 5. Several norms are available in the Type I setting, but only the L2,p norm is available for Type II."
    }, {
      "heading" : "3 Theoretical analysis: bounds and convergence rates for Gaussian Kernels",
      "text" : "In this section, we state our main results on bounds and convergence rates for our algorithm based on Tikhonov regularization with a Gaussian kernel. We consider both Type I and Type II settings for the Euclidean and manifold cases and make a remark on the Euclidean domains with boundary. To simplify the theoretical development, the integral operator and the RKHS H will correspond to the same Gaussian kernel kt(x, y). The proofs will be found in the supplemental material. Assumptions: The set Ω, where the density function p is defined, could be one of the following: (1) the whole Rd; (2) a compact smooth Riemannian sub-manifold M of d-dimension in Rn. We also need p(x) < Γ, q(x) < Γ for any x ∈ Ω and that qp , q p2 are in Sobolev space W 2 2 (Ω).\nTheorem 1. ( Type I setting.) Let p and q be two density functions on Ω. Given n points, zp = {x1, x2, . . . , xn}, i.i.d. sampled from p and m points, zq = {x′1, x′2, . . . , x′m}, i.i.d. sampled from q, and for small enough t, for the solution to the optimization problem in (7), with confidence at least 1− 2e−τ , we have (1) If the domain Ω is Rd, for some constants C1, C2, C3 independent of t, λ.∥∥∥∥f Iλ,z − qp ∥∥∥∥ 2,p ≤ C1t + C2λ 1 2 + C3 √ τ λtd/2 ( 1√ m + 1 λ1/6 √ n ) (12)\n(2) If the domain Ω is a compact sub-manifold without boundary of d dimension, for some 0 < ε < 1, C1, C2, C3 independent of t, λ.∥∥∥∥f Iλ,z − qp ∥∥∥∥ 2,p ≤ C1t1−ε + C2λ 1 2 + C3 √ τ λtd/2 ( 1√ m + 1 λ1/6 √ n ) (13)\nCorollary 2. ( Type I setting.) Assuming m > λ1/3n, with confidence at least 1− 2e−τ , when (1) Ω = Rd, (2) Ω is a d-dimensional sub-manifold of a Euclidean space, we have\n(1) ∥∥∥∥f Iλ,z − qp ∥∥∥∥2 2,p = O (√ τn− 1 3.5+d/2 ) (2) ∥∥∥∥f Iλ,z − qp ∥∥∥∥2 2,p = O (√ τn− 1 3.5(1−ε)+d/2 ) ∀ε ∈ (0, 1)\nTheorem 3. ( Type II setting.) Let p be a density function on Ω and q be a function satisfying the assumptions. Given n points z = {x1, x2, . . . , xn} sampled i.i.d. from p, and for sufficiently small t, for the solution to the optimization problem in (11), with confidence at least 1− 2e−τ , we have (1) If the domain Ω is Rd,∥∥∥∥f IIλ,z − qp ∥∥∥∥ 2,p ≤C1t + C2λ 1 2 + C3λ− 1 3 ‖Kt,q1− q‖2,p + C4 √ τ λ3/2td/2 √ n , (14)\nwhere C1, C2, C3, C4 are constants independent of t, λ. Moreover, ‖Kt,q1− q‖2,p = O(t).\n(2) If Ω is a d-dimensional sub-manifold of a Euclidean space, for any 0 < ε < 1∥∥∥∥f IIλ,z − qp ∥∥∥∥\n2,p\n≤C1t1−ε + C2λ 1 2 + C3λ− 1 3 ‖Kt,q1− q‖2,p + C4\n√ τ\nλ3/2td/2 √ n , (15)\nwhere C1, C2, C3, C4 are independent of t, λ. Moreover, ‖Kt,q1− q‖2,p = O(t 1−η),∀η > 0.\nCorollary 4. ( Type II setting.) With confidence at least 1− 2e−τ , when (1) Ω = Rd, (2) Ω is a d-dimensional sub-manifold of a Euclidean space, we have\n(1) ∥∥∥∥f IIλ,z − qp ∥∥∥∥2 2,p = O (√ τn − 1 4+ 56 d ) (2) ∥∥∥∥f IIλ,z − qp ∥∥∥∥2 2,p = O (√ τn − 1−η 4−4η+ 56 d ) ∀η ∈ (0, 1)"
    }, {
      "heading" : "4 Model Selection and Experiments",
      "text" : "We describe an unsupervised technique for parameter selection, Cross-Density Cross-Validation (CD-CV) based on a performance measure unique to our setting. We proceed to evaluate our method.\nThe setting. In our experiments, we have Xp = {xp1, . . . , xpn} and Xq = {x q 1, . . . , x q m}. The goal is to estimate qp , assuming that X p, Xq are i.i.d. sampled from p, q respectively. Note that\nlearning qp is unsupervised and our algorithms typically have two parameters: the kernel width t and regularization parameter λ. Performance Measures and CD-CV Model Selection. We describe a set of performance measures used for parameter selection. For a given function u, we have the following importance sampling equality (Eq. 1): Ep(u(x)) = Eq ( u(x)p(x)q(x) ) . If f is an approximation of the true ratio qp , and Xp, Xq are samples from p, q respectively, we will have the following approximation to the previous equation: 1n ∑n i=1 u(x p i )f(x p i ) ≈ 1m ∑m j=1 u(x q j). So after obtaining an estimate f of the ratio, we can validate it using the following performance measure:\nJCD(f ;Xp, Xq, U) = 1 F F∑ l=1  n∑ i=1 ul(x p i )f(x p i )− m∑ j=1 ul(x q j) 2 (16) where U = {u1, . . . , uF } is a collection of test functions. Using this performance measure allows various cross-validation procedures to be used for parameter selection. We note that this way to measure error is related to the LSIF [10] and KLIEP [23] algorithms. However, there a similar measure is used to construct an approximation to the ratio qp using functions u1, . . . , uF as a basis. In our setting, we can use test functions (e.g., linear functions) which are poorly suited as a basis for approximating the density ratio.\nWe will use the following two families of test functions for parameter selection: (1) Sets of random linear functions ui(x) = βT x where β ∼ N(0, Id); (2) Sets of random half-space indicator functions, ui(x) = 1βT x>0.\nProcedures for parameter selection. The performance is optimized using five-fold cross-validation by splitting the data set into two parts Xptrain and X q train for training and X p cv and X q cv for validation. The range we use for kernel width t is (t0, 2t0, . . . , 29t0), where t0 is the average distance of the 10 nearest neighbors. The range for regularization parameter λ is (1e− 5, 1e− 6, . . . , 1e− 10). Data sets and Resampling We use two datasets, CPUsmall and Kin8nm, for regression; and USPS handwritten digits for classification. And we draw the first 500 or 1000 points from the original data set as Xp. To obtain Xq, the following two ways of resampling, using the features or the label information, are used (along the lines of those in [6]). Given a set of data with labels {(x1, y1), (x2, y2), . . . , (xn, yn)} and denoting Pi the probability of i’th instance being chosen, we resample as follows: (1) Resampling using features (labels yi are not used). Pi = e (a〈xi,e1〉−b)/σv\n1+e(a〈xi,e1〉−b)/σv , where a, b are the\nresampling parameters, e1 is the first principal component, and σv is the standard deviation of the projections to e1. This resampling method will be denoted by PCA(a, b). (2) Resampling using labels. Pi = {\n1 yi ∈ Lq 0 Otherwise. where yi ∈ L = {1, 2, . . . , k} and Lq is a\nsubset of the whole label set L. It only applies to binary problems obtained by aggregating different classes in multi-class setting.\nTesting the FIRE algorithm. In the first experiment, we test our method for selecting parameters by focusing on the error JCD(f ;Xp, Xq, U) in Eq. 16 for different function classes U . Parameters are chosen using a family of functions U1, while the performance of the parameter is measured using an independent function family U2. This measure is important because in practice the functions we are interested in may not be the ones chosen for validation. We use the USPS data sets for this experiment. As a basis for comparison we use TIKDE (Thresholded Inverse Kernel Density Estimator). TIKDE estimates p̂ and q̂ respectively using Kernel Density Estimation (KDE), and assigns p̂(x) = α to any x satisfying p̂(x) < α. TIKDE then outputs q̂/p̂. We note that chosen threshold α is key to reasonable performance. One issue of this heuristic is that it could underestimate at the region with high density ratio, due to the uniform thresholding. We also compare our methods to LSIF [10]. In these experiments we do not compare with KMM as out-of-sample extension is necessary for fair comparison.\nTable 1 shows the average errors of various methods, defined in Eq. 16 on held-out set Xerr over 5 trials. We use different validation functions f cv(Columns) and error-measuring functions f err(Row). N is the number of random functions used for validation. The error-measuring function families U2 are as follows: (1) Linear(L.): random linear functions f(x) = βT x where β ∼ N(0, Id); (2) Half-\nspace(H.S.): Sets of random half-space indicator functions, f(x) = 1βT x; (3) Kernel(K.): random linear combinations of kernel functions centered at training data, f(x) = γT K where γ ∼ N(0, Id) and Kij = k(xi, xj) for xi from training set; (4) Kernel indicator(K.I.) functions f(x) = 1g(x)>0, where g is as in (3).\nSupervised Learning: Regression and Classification. We compare our FIRE algorithm with several other methods in regression and classification tasks. We consider the situation where part of the data set Xp are labeled and all of Xq are unlabeled. We use weighted ordinary least-square for regression and weighted linear SVM for classification. Regression. Square loss function is used for regression. The performance is measured using normalized square loss, ∑n i=1 (ŷi−yi)2 Var(ŷ−y) . X\nq is resampled using PCA resampler, described before. L is for Linear, and HS is for Half-Space function families for parameter selection.\nClassification. Weighted linear SVM. Percentage of incorrectly labeled test set instances.\nAcknowledgements. The work was partially supported by NSF Grants IIS 0643916, IIS 1117707."
    } ],
    "references" : [ {
      "title" : "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "M. Belkin", "P. Niyogi", "V. Sindhwani" ],
      "venue" : "JMLR, 7:2399–2434",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Discriminative learning for differing training and test distributions",
      "author" : [ "S. Bickel", "M. Brückner", "T. Scheffer" ],
      "venue" : "ICML",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Learning from examples as an inverse problem",
      "author" : [ "E. De Vito", "L. Rosasco", "A. Caponnetto", "U. De Giovannini", "F. Odone" ],
      "venue" : "JMLR, 6:883",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Spectral regularization for support estimation",
      "author" : [ "E. De Vito", "L. Rosasco", "A. Toigo" ],
      "venue" : "NIPS, pages 487–495",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Regularization of inverse problems",
      "author" : [ "H.W. Engl", "M. Hanke", "A. Neubauer" ],
      "venue" : "Springer",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Covariate shift by kernel mean matching",
      "author" : [ "A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Schölkopf" ],
      "venue" : "Dataset shift in machine learning, pages 131–160",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Smooth operators",
      "author" : [ "S. Grünewälder", "A. Gretton", "J. Shawe-Taylor" ],
      "venue" : "ICML",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Conditional mean embeddings as regressors",
      "author" : [ "S. Grünewälder", "G. Lever", "L. Baldassarre", "S. Patterson", "A. Gretton", "M. Pontil" ],
      "venue" : "ICML",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Correcting sample selection bias by unlabeled data",
      "author" : [ "J. Huang", "A. Gretton", "K.M. Borgwardt", "B. Schölkopf", "A. Smola" ],
      "venue" : "NIPS, pages 601–608",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A least-squares approach to direct importance estimation",
      "author" : [ "T. Kanamori", "S. Hido", "M. Sugiyama" ],
      "venue" : "JMLR, 10:1391–1445",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Robust kernel density estimation",
      "author" : [ "J.S. Kim", "C. Scott" ],
      "venue" : "ICASSP, pages 3381–3384",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Support vector method for multivariate density estimation",
      "author" : [ "S. Mukherjee", "V. Vapnik" ],
      "venue" : "Center for Biological and Computational Learning. Department of Brain and Cognitive Sciences, MIT. CBCL, volume 170",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Annealed importance sampling",
      "author" : [ "R.M. Neal" ],
      "venue" : "Statistics and Computing, 11(2):125–139",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization",
      "author" : [ "X. Nguyen", "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "NIPS, 20:1089–1096",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning with kernels: Support vector machines",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : "regularization, optimization, and beyond. MIT press",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Kernel methods for pattern analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : "Cambridge university press",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Data spectroscopy: Eigenspaces of convolution operators and clustering",
      "author" : [ "T. Shi", "M. Belkin", "B. Yu" ],
      "venue" : "The Annals of Statistics, 37(6B):3960–3984",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Improving predictive inference under covariate shift by weighting the loglikelihood function",
      "author" : [ "H. Shimodaira" ],
      "venue" : "Journal of Statistical Planning and Inference, 90(2):227–244",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "On a kernel-based method for pattern recognition",
      "author" : [ "A.J. Smola", "B. Schölkopf" ],
      "venue" : "regression, approximation, and operator inversion. Algorithmica, 22(1):211–231",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Support vector machines",
      "author" : [ "I. Steinwart", "A. Christmann" ],
      "venue" : "Springer",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Covariate shift adaptation by importance weighted cross validation",
      "author" : [ "M. Sugiyama", "M. Krauledat", "K. Müller" ],
      "venue" : "JMLR, 8:985–1005",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Direct importance estimation with model selection and its application to covariate shift adaptation",
      "author" : [ "Masashi Sugiyama", "Shinichi Nakajima", "Hisashi Kashima", "Paul Von Buenau", "Motoaki Kawanabe" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Introduction to nonparametric estimation",
      "author" : [ "A. Tsybakov" ],
      "venue" : "Springer",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The effect of the input density distribution on kernel-based classifiers",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "ICML",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Analysis of kernel mean matching under covariate shift",
      "author" : [ "Y. Yu", "C. Szepesvári" ],
      "venue" : "ICML",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning and evaluating classifiers under sample selection bias",
      "author" : [ "B. Zadrozny" ],
      "venue" : "ICML",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Many of these papers consider this problem in the context of covariate shift assumption [19] or the so-called selection bias [27].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 26,
      "context" : "Many of these papers consider this problem in the context of covariate shift assumption [19] or the so-called selection bias [27].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 23,
      "context" : "When we use δ-kernels, like Gaussian, and f satisfies some smoothness conditions, we have ∫ Rd kt(x, y)f(x)dx = f(y) + O(t) (see [24], Ch.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "This formulation (see Section 2) also yields an explicit formula and is related to the Kernel Mean Matching [9] , although with a different optimization procedure.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "In Section 4 we introduce a unsupervised method, referred as CD-CV (for cross-density crossvalidation) for model selection and discuss the experimental results on several data sets comparing our method FIRE with the available alternatives, Kernel Mean Matching (KMM) [9] and LSIF [10] as well as the base-line thresholded inverse kernel density estimator2 (TIKDE) and importance sampling (when available).",
      "startOffset" : 267,
      "endOffset" : 270
    }, {
      "referenceID" : 9,
      "context" : "In Section 4 we introduce a unsupervised method, referred as CD-CV (for cross-density crossvalidation) for model selection and discuss the experimental results on several data sets comparing our method FIRE with the available alternatives, Kernel Mean Matching (KMM) [9] and LSIF [10] as well as the base-line thresholded inverse kernel density estimator2 (TIKDE) and importance sampling (when available).",
      "startOffset" : 280,
      "endOffset" : 284
    }, {
      "referenceID" : 14,
      "context" : "Recently the problem of density ratio estimation has received significant attention due in part to the increased interest in transfer learning [15] and, in particular to the form of transfer learning known as covariate shift [19].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "Recently the problem of density ratio estimation has received significant attention due in part to the increased interest in transfer learning [15] and, in particular to the form of transfer learning known as covariate shift [19].",
      "startOffset" : 225,
      "endOffset" : 229
    }, {
      "referenceID" : 26,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 9,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 22,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "The work on covariate shift, density ratio estimation and related settings includes [27, 2, 6, 10, 22, 9, 23, 14, 7].",
      "startOffset" : 84,
      "endOffset" : 116
    }, {
      "referenceID" : 8,
      "context" : "The algorithm most closely related to ours is Kernel Mean Matching [9].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "Also, since there is no regularizing term, the problem is less stable (see Section 4 for some experimental comparisons) and the theoretical analysis is harder (however, see [6] and the recent paper [26] for some nice theoretical analysis of KMM in certain settings).",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 25,
      "context" : "Also, since there is no regularizing term, the problem is less stable (see Section 4 for some experimental comparisons) and the theoretical analysis is harder (however, see [6] and the recent paper [26] for some nice theoretical analysis of KMM in certain settings).",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "Another related recent algorithm is Least Squares Importance Sampling (LSIF) [10], which attempts to estimate the density ratio by choosing a parametric linear family of functions and choosing a function from this family to minimize the L2,p distance to the density ratio.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "A similar setting with the Kullback-Leibler distance (KLIEP) was proposed in [23].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : ", [3, 20] in the Tikhonov regularization or other regularization frameworks.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 19,
      "context" : ", [3, 20] in the Tikhonov regularization or other regularization frameworks.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : "In particular, we note interesting methods for density estimation proposed in [12] and estimating the support of density through spectral regularization in [4], as well as robust density estimation using RKHS formulations [11] and conditional density [8].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : "In particular, we note interesting methods for density estimation proposed in [12] and estimating the support of density through spectral regularization in [4], as well as robust density estimation using RKHS formulations [11] and conditional density [8].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 10,
      "context" : "In particular, we note interesting methods for density estimation proposed in [12] and estimating the support of density through spectral regularization in [4], as well as robust density estimation using RKHS formulations [11] and conditional density [8].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 7,
      "context" : "In particular, we note interesting methods for density estimation proposed in [12] and estimating the support of density through spectral regularization in [4], as well as robust density estimation using RKHS formulations [11] and conditional density [8].",
      "startOffset" : 251,
      "endOffset" : 254
    }, {
      "referenceID" : 24,
      "context" : "We also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [25, 18, 1].",
      "startOffset" : 136,
      "endOffset" : 147
    }, {
      "referenceID" : 17,
      "context" : "We also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [25, 18, 1].",
      "startOffset" : 136,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "We also note the connections of the methods in this paper to properties of density-dependent operators in classification and clustering [25, 18, 1].",
      "startOffset" : 136,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "[14] establishes minimax rates for likelihood ratio estimation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "Another recent theoretical analysis of KMM in [26] contains bounds for the output of the corresponding integral operators.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 20,
      "context" : "The Representer Theorem allows us to write solutions to various optimization problems over H in terms of linear combinations of kernels supported on sample points (see [21] for an in-depth discussion or the RKHS theory and the issues related to learning).",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "In particular a simple formulation of Type I using Tikhonov regularization, ([5], Ch.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "We note the connection between this formulation of using the RKHS norm as a loss function and the KMM algorithm [9].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : ", many problems involving MCMC) where q(x) is known explicitly (possibly up to a multiplicative constant), while sampling from q is expensive or even impossible computationally [13].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 9,
      "context" : "We note that this way to measure error is related to the LSIF [10] and KLIEP [23] algorithms.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "We note that this way to measure error is related to the LSIF [10] and KLIEP [23] algorithms.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "To obtain X, the following two ways of resampling, using the features or the label information, are used (along the lines of those in [6]).",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "We also compare our methods to LSIF [10].",
      "startOffset" : 36,
      "endOffset" : 40
    } ],
    "year" : 2013,
    "abstractText" : "We address the problem of estimating the ratio q p where p is a density function and q is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration often referred to as importance sampling in statistical inference. It is also closely related to the problem of covariate shift in transfer learning. Our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel, known as the Fredholm problem of the first kind. This formulation, combined with the techniques of regularization leads to a principled framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is flexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel for densities defined on R and smooth d-dimensional sub-manifolds of the Euclidean space. Model selection for unsupervised or semi-supervised inference is generally a difficult problem. It turns out that in the density ratio estimation setting, when samples from both distributions are available, simple completely unsupervised model selection methods are available. We call this mechanism CD-CV for Cross-Density Cross-Validation. We show encouraging experimental results including applications to classification within the covariate shift framework.",
    "creator" : null
  }
}