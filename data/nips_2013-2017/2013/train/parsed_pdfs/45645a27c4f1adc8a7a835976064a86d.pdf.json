{
  "name" : "45645a27c4f1adc8a7a835976064a86d.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Factorized Asymptotic Bayesian Inference for Latent Feature Models",
    "authors" : [ "Kohei Hayashi", "Ryohei Fujimaki" ],
    "emails" : [ "kohei-h@nii.ac.jp", "rfujimaki@nec-labs.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Factorized asymptotic Bayesian (FAB) inference is a recently-developed Bayesian approximation inference method for model selection of latent variable models [5, 6]. FAB inference maximizes a computationally tractable lower bound of a “factorized information criterion” (FIC) which converges to a marginal log-likelihood for a large sample limit. In application with respect to mixture models (MMs) and hidden Markov models, previous work has shown that FAB inference achieves as good or even better model selection accuracy as state-of-the-art non-parametric Bayesian (NPB) methods and variational Bayesian (VB) methods with less computational cost. One of the interesting characteristics of FAB inference is that it estimates both models (e.g., the number of mixed components for MMs) and parameter values without priors (i.e., it asymptotically ignores priors), and it does not have a hand-tunable hyper-parameter. With respect to the trade-off between controllability and automation, FAB inference places more importance on automation.\nAlthough FAB inference is a promising model selection method, as yet it has only been applicable to models satisfying a specific condition that the Hessian matrix of a complete log-likelihood (i.e., of a log-likelihood over both observed and latent variables) must be block diagonal, with only a part of the observed samples contributing individual sub-blocks. Such models include basic latent variable models as MMs [6]. The application of FAB inference to more advanced models that do not satisfy the condition remains to be accomplished.\nThis paper extends an FAB framework to latent feature models (LFMs) [9, 17]. Model selection for LFMs (i.e., determination of the dimensionality of latent features) has been addressed by NBP and VB methods [10, 3]. Although they have shown promising performance in such applications as link prediction [16], their high computational costs restrict their applications to large-scale data.\nOur asymptotic analysis of the Hessian matrix of the log-likelihood shows that FICs for LFMs have the same form as those for MMs, despite the fact that LFMs do not satisfy the condition explained above (see Lemma 1). Eventually, as FAB/MMs, FAB/LFMs offer several desirable properties, such as FIC convergence to a marginal log-likelihood, automatic hidden states selection, and monotonic increase in the lower FIC bound through iterative optimization. Further we conduct two analysis in\nSection 3: 1) we relate FAB E-steps to a convex concave procedure (CCCP) [29]. Inspired by this analysis, we propose a shrinkage acceleration method which drastically reduces computational cost in practice, and 2) we show that FAB/LFMs have parameter identifiability. This analysis offers a natural guide to the merging post-processing of latent features. Rigorous proofs and assumptions with respect to the main results are given in the supplementary materials. Notation In this paper, we denote the (i, j)-th element, the i-th row vector, and the j-th column vector of A by aij , ai, and a·j , respectively."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "FIC for MMs Suppose we have N × D observed data X and N × K latent variables Z. FIC considers the following alternative representation of the marginal log-likelihood:\nlog p(X|M) = max q {∑ Z q(Z) log p(X,Z|M) q(Z) } , p(X,Z|M) = ∫ p(X,Z|P)p(P|M)dP, (1) where q(Z) is a variational distribution on Z; M and P are a model and its parameter, respectively. In the case of MMs, log p(X,Z|P) can be factorized into log p(Z) and log p(X|Z) =∑\nk log pk(X|z·k), where pk is the k-th observation distribution (we here omit parameters for notational simplicity.) We can then approximate p(X,Z|M) by individually applying Laplace’s method [28] to log p(Z) and log pk(X|z·k):\np(X,Z|M) ≈ p(X,Z|P̂) (2π) DZ/2 NDZ/2 det |FZ |1/2 K∏\nk=1\n(2π)Dk/2 ( ∑\nn znk) Dk/2 det |Fk|1/2\n, (2)\nwhere P̂ is the maximum likelihood estimator (MLE) of p(X,Z|P).1 DZ and Dk are the parameter dimensionalities of p(Z) and pk(X|z·k), respectively. FZ and Fk are −∇∇ log p(Z)|P̂/N and −∇∇ log pk(X|z·k)|P̂/( ∑ n znk), respectively. Under conditions for asymptotic ignoring of log det |FZ | and log det |Fk|, substituting Eq.(2) into (1) gives the FIC for MMs as follows:\nFICMM ≡ max q\nEq [ log p(X,Z|P̂)− DZ\n2 logN − ∑ k Dk 2 log ∑ n znk ] +H(q), (3)\nwhere H(q) is the entropy of q(Z). The most important term in FICMM (3) is log( ∑\nn znk), which offers such theoretically desirable properties for FAB inference as automatic shrinkage of irrelevant latent variables and parameter identifiability [6].\nDirect optimization of FICMM is difficult because: (i) evaluation of Eq[log ∑\nn znk] is computationally infeasible, and (ii) the MLE is not available in principle. Instead, FAB optimizes a tractable lower bound of an FIC [6]. For (i), since− log ∑ n znk is a convex function, its linear approximation at Nπ̃k > 0 yields the lower bound:\n− ∑ k Dk 2 Eq [ log ∑ n znk ] ≥ − ∑ k Dk 2 ( logNπ̃k + ∑ n Eq[znk]/N − π̃k π̃k ) , (4)\nwhere 0 < π̃k ≤ 1 is a linearization parameter. For (ii), since, from the definition of the MLE, the inequality log p(X,Z|P̂) ≥ log p(X,Z|P) holds for any P , we optimize P along with q. Alternating maximization of the lower bound with respect to q, P , and π̃ guarantees a monotonic increase in the FIC lower bound [6].\nInfinite LFMs and Indian Buffet Process The IBP [10, 11] is a nonparametric prior over infinite LFMs. It enables us to express an infinite number of latent features, and making it possible to adjust model complexity on the basis of observations. Infinite IBPs have still been actively studied in terms of both applications (e.g., link prediction [16]) and model representations (e.g., latent attribute models [19]). Since naive Gibbs sampling requires unrealistic computational cost, acceleration algorithms such as accelerated sampling [2] and VB [3] have been developed. Reed and Ghahramani [22] have recently proposed an efficient MAP estimation framework of an IBP model via submodular optimization, which is referred to as maximum-expectation IBP (MEIBP). As similar to FIC, “MAD-Bayes” [1] considers asymptotics of MMs and LFMs, but it is based on a limiting case that the noise variance goes to zero, which yields a prior-derived regularization term.\n1While p(X|P) is a non-regular model, P (X,Z|P) is a regular model (i.e., the Fisher information is nonsingular at the ML estimator,) and Fk and FZ have their inversions at P̂ ."
    }, {
      "heading" : "2 FIC and FAB Algorithm for LFMs",
      "text" : "LFMs assume underlying relationships for X with binary features Z ∈ {0, 1}N×K and linear bases W ∈ RD×K such that, for n = 1, . . . , N ,\nxn = Wzn + b+ εn, (5)\nwhere εn ∼ N(0,Λ−1) is the Gaussian noise having the diagonal precision matrix Λ ≡ diag(λ), and b ∈ RD is a bias term. For later convenience, we define the centered observation X̄ = X − 1b>. Z follows a Bernoulli prior distribution znk ∼ Bern(πk) with a mean parameter πk. The parameter set P is defined as P ≡ {W,b,λ,π}. Also, we denote parameters with respect to the d-th dimension as θd = (wd, bd, λd). Similarly with other FAB frameworks, the log-priors of P are assumed to be constant with respect to N , i.e., limN→∞ log p(P|M) N = 0\nIn the case of MMs, we implicitly use the fact that: A1) parameters of pk(X|z·k) are mutually independent for k = 1, . . . ,K (in other words,∇∇ log p(X|Z) is block diagonal having K blocks), and A2) the number of observations which contribute ∇∇ log pk(X|z·k) is ∑ n znk. These conditions\nnaturally yield the FAB regularization term log ∑\nn znk by the Laplace approximation of MMs (2). However, since θd is shared by all latent features in LFMs, A1 and A2 are not satisfied. In the next section, we address this issue and derive FIC for LFMs."
    }, {
      "heading" : "2.1 FICs for LFMs",
      "text" : "The following lemma plays the most important role in our derivation of FICs for LFMs.\nLemma 1. Let F(d) be the Hessian matrix of the negated log-likelihood with respect to θd, i.e., −∇∇ log p(x·d|Z,θd). Under some mild assumptions (see the supplementary materials), the following equality holds:\nlog det |F(d)| = ∑ k log ∑ n znk N +Op(1). (6)\nAn important fact is that the log ∑\nn znk term naturally appears in log det |F (d)| without A1 and A2.\nLemma 1 induces the following theorem, which states an asymptotic approximation of a marginal complete log-likelihood, log p(X,Z|M). Theorem 2. If Lemma 1 holds and the joint marginal log-likelihood is bounded for a sufficiently large N , it can be asymptotically approximated as:\nlog p(X,Z|M) = J(Z, P̂) +Op(1), (7)\nJ(Z,P) ≡ log p(X,Z|P)− |P| −DK 2 logN − D 2 ∑ k log ∑ n znk. (8)\nIt is worth noting that, if we evaluate the model complexity of θd (log det |F(d)|) by N , i.e., if we apply Laplace’s method without Lemma 1, Eq. (7) falls into Bayesian Information Criterion [23], which tells us that the model complexity relevant to θd increases not O(K logN) but O( ∑ k log ∑ n znk).\nBy substituting approximation (7) into Eq. (1), we obtain the FIC of the LFM as follows:\nFICLFM ≡ max q Eq[J(Z, P̂)] +H(q). (9)\nIt is interesting that FICLFM (9) and FICMM (3) have exactly the same representation despite the fact that LFMs do not satisfy A1 and A2. This indicates the wide applicability of FICs and suggests that FIC representation of approximated marginal log-likelihoods is feasible not only for MMs but also for more general (discrete) latent variable models.\nSince the asymptotic constant terms of Eq. (7) are not affected by the expectation of q(Z), the difference between the FIC and the marginal log-likelihood is asymptotically constant; in other words, the distance between log p(X|M)/N and FICLFM/N is asymptotically small. Corollary 3. For N →∞, log p(X|M) = FICLFM +Op(1) holds."
    }, {
      "heading" : "2.2 FAB/LFM Algorithm",
      "text" : "As with the case of MMs (3), FICLFM is not available in practice, and we employ the lower bounding techniques (i) and (ii). For LFMs, we further introduce a mean-filed approximation on Z, i.e., we restrict the class of q(zn) to a factorized form: q(zn) = ∏ k q̃(znk|µnk), where q̃(z|µ) is a Bernoulli distribution with a mean parameter µ = Eq[z]. Rather than this approximation’s making the FIC lower bound looser (the equality (1) no longer holds), the variational distribution has a closed-form solution. Note that this approximation does not cause significant performance degradation in VB contexts [20, 25]. The VB-extension of IBP [3] also uses this factorized assumption.\nBy applying (i), (ii), and the mean-field approximation, we obtain the lower bound: L(q,P, π̃) =\nEq [log p(X|Z,Θ) + log p(Z|π) + RHS of (4)]− 2D +K\n2 logN + ∑ n H(q(zn)). (10)\nAn FAB algorithm alternatingly maximizes L(q,P, π̃) with respect to {{µn},P, π̃}. Notice that the algorithm described below monotonically increases L in every single step, and therefore we are guaranteed to obtain a local maximum. This monotonic increase in L gives us a natural stopping condition with a tolerance δ: if (Lt − Lt−1)/N < δ then stop the algorithm, where we denote the value of L at the t-th iteration by Lt.\nFAB E-step In the FAB E-step, we update µn in a way similar to that with the variational meanfield inference in a restricted Boltzmann machine [20]. Taking the gradient of L with respect to µn and setting it to zero yields the following fixed-point equations:\nµnk = g (cnk + η(πk)−D/2Nπ̃k) (11) where g(x) = (1+exp(−x))−1 is the sigmoid function, cnk = w>·kΛ(x̄n− ∑ l 6=k µnlw·l− 1 2w·k), and η(πk) = log πk1−πk is a natural parameter of the prior of z·k. Update equation (11) is a form of coordinate descent, and every update is guaranteed to increase the lower bound [25]. After several iterations of Eq. (11) over k = 1, . . . ,K, we are able to obtain a local maximum of Eq[zn] = µn and Eq[znz>n ] = µnµ>n + diag(µn − µ2n).\nOne unique term in Eq. (11) is − D2Nπ̃k , which originated in the log ∑\nn znk term in Eq. (8). In updating µnk (11), the smaller π̃k (or equivalent to πk by Eq. (12)) is, the smaller µnk is. And a smaller µnk is likely to induce a smaller π̃k (see Eq. (12)). This results in the shrinking of irrelevant features, and therefore FAB/LFMs are capable of automatically selecting feature dimensionality K. This regularization effect is induced independently of prior (i.e., asymptotic ignorance of prior) and is known as “model induced regularization” which is caused by Bayesian marginalization in singular models [18]. Notice that Eq. (11) offers another shrinking effect, by means of η(πk), which is a prior-based regularization. We empirically show that the latter shrinking effect is too weak to mitigate over-fitting and the FAB algorithm achieves faster convergence, with respect to N , to the true model (see Section 4.) Note that if we only use the effect of η(πk) (i.e. setting D/2Nπ̃k = 0), then update equation (11) is equivalent to that of variational EM.\nFAB M-step The FAB M-step is equivalent to the M-step in the EM algorithm of LFMs; the solutions of W,Λ and b are given as in closed form and is exactly the same as those of PPCA [24] (see the supplementary materials.) For π̃ and π, we obtain the following solutions:\nπk = π̃k = ∑ n µnk/N. (12)\nShrinkage step As we have explained, in principle, the FAB regularization term D2Nπ̃k in Eq. (11) automatically eliminates irrelevant latent features. While the elimination does not change the value of Eq[log(X|Z,P)], removing them from the model increases L due to a decrease in model complexity. We eliminate shrunken features after FAB E-step in terms of that LFMs approximate X by∑\nk µ·kw > ·k + 1b\n>. When ∑\nn µnk/N = 0, the k-th feature does not affect to the approximation ( ∑\nl z·lw > ·l = ∑ l 6=k z·lw > ·l ), and we simply remove it. When ∑ n µnk/N = 1, wk can be seen as a\nbias ( ∑\nl z·lw > ·l = ∑ l 6=k z·lw > ·l + 1w > ·k), and we update b new = b+wk and then remove it.\nAlgorithm 1 The FAB algorithm for LFMs. 1: Initialize {µn} 2: while Convergence do 3: Update P 4: accelerateShrinkage({µn}) 5: for k = 1, . . . ,K do 6: Update {µnk} by Eq. (11) 7: end for 8: Shrink unnecessary latent features 9: if (Lt − Lt−1)/N < δ then 10: {{µ′n},W ′} ← merge({µn},W) 11: if dim(W′) = dim(W) then Converge 12: else {µn} ← {µ′n},W←W ′ 13: end if 14: end while\nAlgorithm 2 accelerateShrinkage input {µn}\n1: for k = 1, . . . ,K do 2: ck ← (X̄− ∑ l 6=k µ·lw > ·l− 121w > ·k)Λw·k 3: for t = 1, . . . , Tshrink do 4: Update {µnk} by Eq. (11) 5: Update π and π̃ by Eq. (12) 6: end for 7: end for\nThis model shrinkage also works to avoid the ill-conditioning of the FIC; if there are latent features that are never activated ( ∑ n µnk/N = 0) or always activated ( ∑ n µnk/N = 1), the FIC will no longer be an approximation of the marginal log-likelihood. Algorithm 1 summarizes whole procedures with respect to the FAB/LFMs. Note that details regarding sub-routines accelerateShrinkage() and merge() are explained in Section 3."
    }, {
      "heading" : "3 Analysis and Refinements",
      "text" : "CCCP Interpretation and Shrinkage Acceleration Here we interpret the alternating updates of µ and π̃ as a convex concave procedure (CCCP) [29] and consider to eliminate irrelevant features in early steps to reduce computational cost. By substituting an optimality condition π̃k = ∑ n µnk/N (12) into the lower bound, we obtain\nL(q) = −D 2 ∑ k log ∑ n µnk + (∑ n (cn + η) >µn +H(q) ) + const. (13)\nThe first and second terms are convex and concave with respect to µnk, respectively. The CCCP solves Eq.(13) by iteratively linearizing the first term around µt−1nk . By setting the derivative of the “linearized” objective to be zero, we obtain the CCCP update as follows:\nµtnk = g ( cnk + η(πk)− D\n2 ∑ n µt−1nk\n) . (14)\nBy taking Nπ̃k = ∑ n µ t−1 nk into account, Eq.(14) is equivalent to Eq.(11).\nThis new view of the FAB optimization gives us an important insight to accelerate the algorithm. By considering the FAB optimization as the alternating maximization in terms of P and µ (π̃ is removed), it is natural to take multiple CCCP steps (14). Such multiple CCCP steps in each FABEM step is expected to accelerate the shrinkage effect discussed in the previous section because the\nregularization in terms of −D/2( ∑\nn µnk) causes the effect. Eventually, it is expected to reduce the total computational cost since we may be able to remove irrelevant latent features in earlier iterations. We summarize the whole routine of accelerateShrinkage() based on the CCCP in Algorithm 2. Note that, in practice, we update π along with π̃ for further acceleration of the shrinkage. We empirically confirmed that Algorithm 2 significantly reduced computational costs (see Section 4 and Figure 1.) Further discussion of this this update (an exponentiated gradient descent interpretation) can be found in the supplementary materials.\nIdentifiability and Merge Post-processing Parameter identifiability is an important theoretical aspect in learning algorithms for latent variable models. It has been known [26, 27] that generalization error significantly worsens if the mapping between parameters and functions is not one-toone (i.e., is non-identifiable.) Let us consider the LFM case of K = 2. If w·1 = w·2, then any combination of µn1 and µn2 = 2µ − µn1 will have the same representation: Eq[Ex[x̄nd|θd]] = wd1(µn1 + µn2) = 2wd1µ, and therefore the MLE is non-identifiable.\nThe following theorem shows that FAB inference resolves such non-identifiability in LFMs. Theorem 4. Let P∗ and q∗ be stationary points of L such that 0 < ∑\nn µ ∗ nk/N < 1 for k ="
    }, {
      "heading" : "1, . . . ,K and |x̄>nΛ",
      "text" : "∗w∗·k| < ∞ for k = 1, . . . ,K, n = 1, . . . , N . Then, w∗·k = w∗·l is a sufficient condition of ∑\nn µ ∗ nk/N = ∑ n µ ∗ nl/N .\nFor the ill-conditioned situation described above, the FAB algorithm has a unique solution that balances the sizes of latent features. In large sample limit, both FAB and EM reach the same ML value. The point is, for LFMs, ML solutions are not unique and EM is likely to choose large-Ksolutions because of this non-identifiability issue. On the other hands, FAB prefers to small-K ML solutions on the basis of the regularizer. In addition, Theorem 4 gives us an important insight about post-processing of latent features. If w∗·k = w ∗ ·l, then Eq[log p(X,Z|M∗)] is equivalent without relation to µnk and µnl, while model complexity is smaller if we only have one latent feature. Therefore, if w∗·k = w ∗ ·l, merging these two latent features increases L, i.e., w∗·k = 2w∗·k and µ∗·k = µ∗·k+µ ∗ ·l\n2 . In practice, we search for such overlapping features on the basis of a Euclidean distance matrix of W∗ and w∗·k for k = 1, . . . ,K and merge them if the lower bound increases after the post-processing. We empirically found that a few merging operations were likely to occur in real world data sets. The algorithm of merge() is summarized in the supplementary materials."
    }, {
      "heading" : "4 Experiments",
      "text" : "We have evaluated FAB/LFMs in terms of computational speed, model selection accuracy, and prediction performance with respect to missing values. We compared FAB inference and the variational EM algorithm (see Section 2.2) with an IBP that utilized fast Gibbs sampling [2], a VB [3] having a finite K, and MEIBP [22]. IBP and MEIBP select a model which maximizes posterior probability. For VB, we performed inference with K = 2, . . . , D and selected the model having the highest free energy. EM selects K using the shrinkage effect of η as we have explained in Section 2.2.\nAll the methods were implemented in Matlab (for IBP, VB, and MEIBP, we used original codes released by the authors), and the computational performance were fairly compared. For FAB and EM, we set δ = 10−4 (this was not sensitive) and Tshrink = 100 (FAB only); {µn} were randomly and uniformly initialized by 0 and 1; the initial number of latent features was set to min(N,D) as well as MEIBP. Since the softwares of IBP, VB, and MEIBP did not learn the standard deviation of the noise (1/ √ λ in FAB), we fixed it to 1 for artificial simulations, which is the true standard deviation of toy data, and 0.75 for real data by following the original papers [2, 22]. We set other parameters with software default values. For example, α, a hyperparameter of IBP, was set to 3, which might cause overestimation of K. As common preprocessing, we normalized X (i.e., the sample variance is 1) in all experiments.\nArtificial Simulations We first conducted artificial simulations with fully-observed synthetic data generated by model (5) having a fixed λk = 1 and πk = 0.5. Figure 1 shows the results of a comparison between FAB with and without shrinkage acceleration.2 Clearly, our shrinkage acceleration\n2We also investigated the effect of merge post-processing, but none was observed in this small example.\nsignificantly reduced computational cost by eliminating irrelevant features in the early steps, while both algorithms achieved roughly the same objective value L and model selection performance at the convergence. Figure 2 shows the results of a comparison between FAB (with acceleration) and the other methods. While MEIBP was much faster than FAB in terms of elapsed computational time, FAB achieved the most accurate estimation of K, especially for large N .\nBlock Data We next demonstrate performance of FAB/LFMs in terms of learning features. We used the block data, a synthetic data originally used in [10]. Observations were generated by combining four distinct patterns (i.e., K = 4, see Figure 3) with Gaussian noise, on 6 by 6 pixels (i.e., D = 36). We prepared the results of N = 2000 samples with the noise standard deviation 0.3 and no missing values (more results can be found in the supplementary materials.) Figure 3 compares estimated features of each method on early learning phase (at the 5th iteration) and after the convergence (the result displayed is the example which has the median log-likelihood over 10 trials.) Note that, we omitted MEIPB since we observed that its parameter setting was very sensitive for this data. While EM and IBP retain irrelevant features, FAB successfully extracts the true patterns without irrelevant features.\nReal World Data We finally evaluated predictive performance by using the real data sets described in Table 1. We randomly removed 30% of data with 5 different random seeds and treated them as missing values, and we measured predictive and training log-likelihood (PLL and TLL) for them. Table 1 summarizes the results with respect to elapsed computational time (hours), selected K, PLL, and TLL. Note that, for cases when the computational time for a method exceeded 50 hours, we stopped the program after that iteration.3 Since MEIBP is the method for non-negative data, we omitted the results of those containing negative values. Also, since MEIBP did not finish the first iteration within 50 hours for yaleB and USPS data, we set the initial K as 100. FAB consistently achieved good predictive performance (higher PLL) with low computational cost. Although MEIBP performed faster than FAB with appropriately set the initial value of K (i.e., yaleB and USPS), PLLs of FAB were much better than those of MEIBP. In terms of K, FAB typically achieved a more compact and better model representation than the others (smaller K). Another important observation is that FAB have much smaller differences between TLL and PLL than the others. This suggests that FAB’s unique regularization worked well for mitigating over-fitting. For the large sample data sets (EEG, Piano, USPS), PLLs of FAB and EM were competitive with one another;\n3We totally omitted VB because of its long computational time.\nthis is reasonable, for large N , both of them ideally achieve the maximum likelihood while FAB achieved much smaller K (see identifiability discussion in Section 3). In small N scenarios, on the other hand, FIC approximation would be not accurate, and FAB would perform worse than NPBs (while we observed such case only in Libras.)"
    }, {
      "heading" : "5 Summary",
      "text" : "We have considered here an FAB framework for LFMs that offers fully automated model selection, i.e., selecting the number of latent features. While LFMs do not satisfy the assumptions that naturally induce FIC/FAB on MMs, we have shown that they have the same “degree” of model complexity as the approximated marginal log-likelihood, and we have derived FIC/FAB in a form similar to that for MMs. In addition, our proposed accelerating mechanism for shrinking models drastically reduces total computational time. Experimental comparisons of FAB inference with existing methods, including state-of-the-art IBP methods, have demonstrated the superiority of FAB/LFM."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Finale Doshi-Velez for providing Piano and EEG data sets. This work was supported by JSPS KAKENHI Grant Number 25880028."
    } ],
    "references" : [ {
      "title" : "MAD-Bayes: MAP-based Asymptotic Derivations from Bayes",
      "author" : [ "T. Broderick", "B. Kulis", "M.I. Jordan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Accelerated sampling for the indian buffet process",
      "author" : [ "F. Doshi-Velez", "Z. Ghahramani" ],
      "venue" : "In ICML,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Variational inference for the Indian buffet process",
      "author" : [ "F. Doshi-Velez", "K.T. Miller", "J. Van Gael", "Y.W. Teh" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Factorized asymptotic bayesian hidden markov model",
      "author" : [ "R. Fujimaki", "K. Hayashi" ],
      "venue" : "In ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Factorized asymptotic bayesian inference for mixture modeling",
      "author" : [ "R. Fujimaki", "S. Morinaga" ],
      "venue" : "In AIS- TATS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "From few to many: Illumination cone models for face recognition under variable lighting and pose",
      "author" : [ "A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "Factorial learning and the EM algorithm",
      "author" : [ "Z. Ghahramani" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1995
    }, {
      "title" : "Bayesian nonparametric latent feature models (with discussion)",
      "author" : [ "Z. Ghahramani", "T.L. Griffiths", "P. Sollich" ],
      "venue" : "In 8th Valencia International Meeting on Bayesian Statistics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Infinite latent feature models and the indian buffet",
      "author" : [ "T. Griffiths", "Z. Ghahramani" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2005
    }, {
      "title" : "The indian buffet process: An introduction and review",
      "author" : [ "T.L. Griffiths", "Z. Ghahramani" ],
      "venue" : "JMLR, 12:1185–1224,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "A boosting approach to p300 detection with application to brain-computer interfaces",
      "author" : [ "U. Hoffmann", "G. Garcia", "J.M. Vesin", "K. Diserens", "T. Ebrahimi" ],
      "venue" : "In International IEEE EMBS Conference on Neural Engineering,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "A database for handwritten text recognition research",
      "author" : [ "J.J. Hull" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1994
    }, {
      "title" : "Temporal Classification: Extending the Classification Paradigm to Multivariate Time Series",
      "author" : [ "M.W. Kadous" ],
      "venue" : "PhD thesis, School of Computer Science & Engineering,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Exponentiated gradient versus gradient descent for linear predictors",
      "author" : [ "J. Kivinen", "M.K. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1997
    }, {
      "title" : "Nonparametric latent feature models for link prediction",
      "author" : [ "K. Miller", "T. Griffiths", "M. Jordan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Bayesian Nonparametric Latent Feature Models",
      "author" : [ "K.T. Miller" ],
      "venue" : "PhD thesis, University of California, Berkeley,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "On bayesian PCA: Automatic dimensionality selection and analytic solution",
      "author" : [ "S. Nakajima", "M. Sugiyama", "D. Babacan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "An infinite latent attribute model for network data",
      "author" : [ "K. Palla", "D.A. Knowles", "Z. Ghahramani" ],
      "venue" : "In ICML,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "A mean field theory learning algorithm for neural networks",
      "author" : [ "C. Peterson", "J. Anderson" ],
      "venue" : "Complex systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1987
    }, {
      "title" : "A discriminative model for polyphonic piano transcription",
      "author" : [ "G.E. Poliner", "D.P.W. Ellis" ],
      "venue" : "EURASIP Journal of Advances in Signal Processing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Scaling the indian buffet process via submodular maximization",
      "author" : [ "C. Reed", "Z. Ghahramani" ],
      "venue" : "In ICML,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Estimating the dimension of a model",
      "author" : [ "G. Schwarz" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1978
    }, {
      "title" : "Probabilistic principal component analysis",
      "author" : [ "M. Tipping", "C. Bishop" ],
      "venue" : "Journal of the Royal Statistical Society. Series B,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1999
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "Algebraic analysis for nonidentifiable learning machines",
      "author" : [ "S. Watanabe" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2001
    }, {
      "title" : "Algebraic Geometry and Statistical Learning Theory (Cambridge Monographs on Applied and Computational Mathematics)",
      "author" : [ "S. Watanabe" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "Asymptotic Approximation of Integrals (Classics in Applied Mathematics)",
      "author" : [ "R. Wong" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2001
    }, {
      "title" : "The Concave-Convex procedure",
      "author" : [ "A.L. Yuille", "A. Rangarajan" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2003
    }, {
      "title" : "Learning population codes by minimizing description length",
      "author" : [ "R.S. Zemel", "G.E. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Factorized asymptotic Bayesian (FAB) inference is a recently-developed Bayesian approximation inference method for model selection of latent variable models [5, 6].",
      "startOffset" : 157,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "Factorized asymptotic Bayesian (FAB) inference is a recently-developed Bayesian approximation inference method for model selection of latent variable models [5, 6].",
      "startOffset" : 157,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "Such models include basic latent variable models as MMs [6].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "This paper extends an FAB framework to latent feature models (LFMs) [9, 17].",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "This paper extends an FAB framework to latent feature models (LFMs) [9, 17].",
      "startOffset" : 68,
      "endOffset" : 75
    }, {
      "referenceID" : 8,
      "context" : ", determination of the dimensionality of latent features) has been addressed by NBP and VB methods [10, 3].",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : ", determination of the dimensionality of latent features) has been addressed by NBP and VB methods [10, 3].",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "Although they have shown promising performance in such applications as link prediction [16], their high computational costs restrict their applications to large-scale data.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 27,
      "context" : "Section 3: 1) we relate FAB E-steps to a convex concave procedure (CCCP) [29].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 26,
      "context" : ") We can then approximate p(X,Z|M) by individually applying Laplace’s method [28] to log p(Z) and log pk(X|z·k): p(X,Z|M) ≈ p(X,Z|P̂) (2π) DZ/2 NDZ/2 det |FZ |1/2 K ∏",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "The most important term in FICMM (3) is log( ∑ n znk), which offers such theoretically desirable properties for FAB inference as automatic shrinkage of irrelevant latent variables and parameter identifiability [6].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 4,
      "context" : "Instead, FAB optimizes a tractable lower bound of an FIC [6].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "Alternating maximization of the lower bound with respect to q, P , and π̃ guarantees a monotonic increase in the FIC lower bound [6].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "Infinite LFMs and Indian Buffet Process The IBP [10, 11] is a nonparametric prior over infinite LFMs.",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "Infinite LFMs and Indian Buffet Process The IBP [10, 11] is a nonparametric prior over infinite LFMs.",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : ", link prediction [16]) and model representations (e.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "Since naive Gibbs sampling requires unrealistic computational cost, acceleration algorithms such as accelerated sampling [2] and VB [3] have been developed.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "Since naive Gibbs sampling requires unrealistic computational cost, acceleration algorithms such as accelerated sampling [2] and VB [3] have been developed.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 20,
      "context" : "Reed and Ghahramani [22] have recently proposed an efficient MAP estimation framework of an IBP model via submodular optimization, which is referred to as maximum-expectation IBP (MEIBP).",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "As similar to FIC, “MAD-Bayes” [1] considers asymptotics of MMs and LFMs, but it is based on a limiting case that the noise variance goes to zero, which yields a prior-derived regularization term.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 21,
      "context" : "(7) falls into Bayesian Information Criterion [23], which tells us that the model complexity relevant to θd increases not O(K logN) but O( ∑ k log ∑ n znk).",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : "Note that this approximation does not cause significant performance degradation in VB contexts [20, 25].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 23,
      "context" : "Note that this approximation does not cause significant performance degradation in VB contexts [20, 25].",
      "startOffset" : 95,
      "endOffset" : 103
    }, {
      "referenceID" : 2,
      "context" : "The VB-extension of IBP [3] also uses this factorized assumption.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 18,
      "context" : "FAB E-step In the FAB E-step, we update μn in a way similar to that with the variational meanfield inference in a restricted Boltzmann machine [20].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : "Update equation (11) is a form of coordinate descent, and every update is guaranteed to increase the lower bound [25].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : ", asymptotic ignorance of prior) and is known as “model induced regularization” which is caused by Bayesian marginalization in singular models [18].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : "FAB M-step The FAB M-step is equivalent to the M-step in the EM algorithm of LFMs; the solutions of W,Λ and b are given as in closed form and is exactly the same as those of PPCA [24] (see the supplementary materials.",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 27,
      "context" : "CCCP Interpretation and Shrinkage Acceleration Here we interpret the alternating updates of μ and π̃ as a convex concave procedure (CCCP) [29] and consider to eliminate irrelevant features in early steps to reduce computational cost.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 24,
      "context" : "It has been known [26, 27] that generalization error significantly worsens if the mapping between parameters and functions is not one-toone (i.",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 25,
      "context" : "It has been known [26, 27] that generalization error significantly worsens if the mapping between parameters and functions is not one-toone (i.",
      "startOffset" : 18,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "2) with an IBP that utilized fast Gibbs sampling [2], a VB [3] having a finite K, and MEIBP [22].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "2) with an IBP that utilized fast Gibbs sampling [2], a VB [3] having a finite K, and MEIBP [22].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 20,
      "context" : "2) with an IBP that utilized fast Gibbs sampling [2], a VB [3] having a finite K, and MEIBP [22].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "75 for real data by following the original papers [2, 22].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 20,
      "context" : "75 for real data by following the original papers [2, 22].",
      "startOffset" : 50,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "We used the block data, a synthetic data originally used in [10].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "01 MEIBP N/A N/A N/A N/A EEG [12] FAB 1.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "05 MEIBP N/A N/A N/A N/A Piano [21] FAB 19.",
      "startOffset" : 31,
      "endOffset" : 35
    } ],
    "year" : 2013,
    "abstractText" : "This paper extends factorized asymptotic Bayesian (FAB) inference for latent feature models (LFMs). FAB inference has not been applicable to models, including LFMs, without a specific condition on the Hessian matrix of a complete loglikelihood, which is required to derive a “factorized information criterion” (FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models. FAB/LFMs have several desirable properties (e.g., automatic hidden states selection and parameter identifiability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection, prediction, and computational efficiency.",
    "creator" : null
  }
}