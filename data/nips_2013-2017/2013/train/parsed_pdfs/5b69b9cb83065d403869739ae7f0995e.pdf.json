{
  "name" : "5b69b9cb83065d403869739ae7f0995e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Low-rank matrix reconstruction and clustering via approximate message passing",
    "authors" : [ "Ryosuke Matsushita", "Toshiyuki Tanaka" ],
    "emails" : [ "matsur8@gmail.com", "tt@i.kyoto-u.ac.jp" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Low-rankedness of matrices has frequently been exploited when one reconstructs a matrix from its noisy observations. In such problems, there are often demands to incorporate additional structural properties of matrices in addition to the low-rankedness. In this paper, we consider the case where a matrix A0 ∈ Rm×N to be reconstructed is factored as A0 = U0V ⊤0 , U0 ∈ Rm×r, V0 ∈ RN×r (r ≪ m, N ), and where one knows structural properties of the factors U0 and V0 a priori. Sparseness and non-negativity of the factors are popular examples of such structural properties [1, 2].\nSince the properties of the factors to be exploited vary according to the problem, it is desirable that a reconstruction method has enough flexibility to incorporate a wide variety of properties. The Bayesian approach achieves such flexibility by allowing us to select prior distributions of U0 and V0 reflecting a priori knowledge on the structural properties. The Bayesian approach, however, often involves computationally expensive processes such as high-dimensional integrations, thereby requiring approximate inference methods in practical implementations. Monte Carlo sampling methods and variational Bayes methods have been proposed for low-rank matrix reconstruction to meet this requirement [3–5].\nWe present in this paper an approximate message passing (AMP) based algorithm for Bayesian lowrank matrix reconstruction. Developed in the context of compressed sensing, the AMP algorithm reconstructs sparse vectors from their linear measurements with low computational cost, and achieves a certain theoretical limit [6]. AMP algorithms can also be used for approximating Bayesian inference with a large class of prior distributions of signal vectors and noise distributions [7]. These successes of AMP algorithms motivate the use of the same idea for low-rank matrix reconstruction. The IterFac algorithm for the rank-one case [8] has been derived as an AMP algorithm. An AMP algorithm for the general-rank case is proposed in [9], which, however, can only treat estimation of posterior means. We extend their algorithm so that one can deal with other estimations such as the maximum a posteriori (MAP) estimation. It is the first contribution of this paper.\nAs the second contribution, we apply the derived AMP algorithm to K-means type clustering to obtain a novel efficient clustering algorithm. It is based on the observation that our formulation of the low-rank matrix reconstruction problem includes the clustering problem as a special case. Although the idea of applying low-rank matrix reconstruction to clustering is not new [10, 11], our proposed algorithm is, to our knowledge, the first that directly deals with the constraint that each datum should be assigned to exactly one cluster in the framework of low-rank matrix reconstruction. We present results of numerical experiments, which show that the proposed algorithm outperforms Lloyd’s K-means algorithm [12] when data are high-dimensional.\nRecently, AMP algorithms for dictionary learning and blind calibration [13] and for matrix reconstruction with a generalized observation model [14] were proposed. Although our work has some similarities to these studies, it differs in that we fix the rank r rather than the ratio r/m when taking the limit m,N → ∞ in the derivation of the algorithm. Another difference is that our formulation, explained in the next section, does not assume statistical independence among the components of each row of U0 and V0. A detailed comparison among these algorithms remains to be made."
    }, {
      "heading" : "2 Problem setting",
      "text" : ""
    }, {
      "heading" : "2.1 Low-rank matrix reconstruction",
      "text" : "We consider the following problem setting. A matrix A0 ∈ Rm×N to be estimated is defined by two matrices U0 := (u0,1, . . . ,u0,m)⊤ ∈ Rm×r and V0 := (v0,1, . . . ,v0,N )⊤ ∈ RN×r as A0 := U0V ⊤ 0 , where u0,i,v0,j ∈ Rr. We consider the case where r ≪ m, N . Observations of A0 are corrupted by additive noise W ∈ Rm×N , whose components Wi,j are i.i.d. Gaussian random variables following N(0,mτ). Here τ > 0 is a noise variance parameter and N(a, σ2) denotes the Gaussian distribution with mean a and variance σ2. The factor m in the noise variance is introduced to allow a proper scaling in the limit where m and N go to infinity in the same order, which is employed in deriving the algorithm. An observed matrix A ∈ Rm×N is given by A := A0 + W . Reconstructing A0 and (U0, V0) from A is the problem considered in this paper.\nWe take the Bayesian approach to address this problem, in which one requires prior distributions of variables to be estimated, as well as conditional distributions relating observations with variables to be estimated. These distributions need not be the true ones because in some cases they are not available so that one has to assume them arbitrarily, and in some other cases one expects advantages by assuming them in some specific manner in view of computational efficiencies. In this paper, we suppose that one uses the true conditional distribution\np(A|U0, V0) = 1\n(2πmτ) mN 2 exp ( − 1 2mτ ∥A− U0V ⊤0 ∥2F ) , (1)\nwhere ∥ · ∥F denotes the Frobenius norm. Meanwhile, we suppose that the assumed prior distributions of U0 and V0, denoted by p̂U and p̂V, respectively, may be different from the true distributions pU and pV, respectively. We restrict p̂U and p̂V to distributions of the form p̂U(U0) = ∏ i p̂u(u0,i)\nand p̂V(V0) = ∏\nj p̂v(v0,j), respectively, which allows us to construct computationally efficient algorithms. When U ∼ p̂U(U) and V ∼ p̂V(V ), the posterior distribution of (U, V ) given A is\np̂(U, V |A) ∝ exp ( − 1 2mτ ∥A− UV ⊤∥2F ) p̂U(U)p̂V(V ). (2)\nPrior probability density functions (p.d.f.s) p̂u and p̂v can be improper, that is, they can integrate to infinity, as long as the posterior p.d.f. (2) is proper. We also consider cases where the assumed rank r̂ may be different from the true rank r. We thus suppose that estimates U and V are of size m× r̂ and N × r̂, respectively. We consider two problems appearing in the Bayesian approach. The first problem, which we call the marginalization problem, is to calculate the marginal posterior distributions given A,\np̂i,j(ui,vj |A) := ∫ p̂(U, V |A) ∏ k ̸=i duk ∏ l ̸=j dvl. (3)\nThese are used to calculate the posterior mean E[UV ⊤|A] and the marginal MAP estimates uMMAPi := argmaxu ∫ p̂i,j(u,v|A)dv and vMMAPj := argmaxv ∫ p̂i,j(u,v|A)du. Because\ncalculation of p̂i,j(ui,vj |A) typically involves high-dimensional integrations requiring high computational cost, approximation methods are needed.\nThe second problem, which we call the MAP problem, is to calculate the MAP estimate argmaxU,V p̂(U, V |A). It is formulated as the following optimization problem:\nmin U,V\nCMAP(U, V ), (4)\nwhere CMAP(U, V ) is the negative logarithm of (2):\nCMAP(U, V ) := 1\n2mτ ∥A− UV ⊤∥2F − m∑ i=1 log p̂u(ui)− N∑ j=1 log p̂v(vj). (5)\nBecause ∥A − UV ⊤∥2F is a non-convex function of (U, V ), it is generally hard to find the global optimal solutions of (4) and therefore approximation methods are needed in this problem as well."
    }, {
      "heading" : "2.2 Clustering as low-rank matrix reconstruction",
      "text" : "A clustering problem can be formulated as a problem of low-rank matrix reconstruction [11]. Suppose that v0,j ∈ {e1, . . . , er}, j = 1, . . . , N , where el ∈ {0, 1}r is the vector whose lth component is 1 and the others are 0. When V0 and U0 are fixed, aj follows one of the r Gaussian distributions N(ũ0,l,mτI), l = 1, . . . , r, where ũ0,l is the lth column of U0. We regard that each Gaussian distribution defines a cluster, ũ0,l being the center of cluster l and v0,j representing the cluster assignment of the datum aj . One can then perform clustering on the dataset {a1, . . . ,aN} by reconstructing U0 and V0 from A = (a1, . . . ,aN ) under the structural constraint that every row of V0 should belong to {e1, . . . , er̂}, where r̂ is an assumed number of clusters. Let us consider maximum likelihood estimation argmaxU,V p(A|U, V ), or equivalently, MAP estimation with the (improper) uniform prior distributions p̂u(u) = 1 and p̂v(v) = r̂−1 ∑r̂ l=1 δ(v−el). The corresponding MAP problem is\nmin U∈Rm×r̂,V ∈{0,1}N×r̂\n∥A− UV ⊤∥2F subject to vj ∈ {e1, . . . , er̂}. (6)\nWhen V satisfies the constraints, the objective function ∥A − UV ⊤∥2F = ∑N j=1 ∑r̂ l=1 ∥aj − ũl∥22I(vj = el) is the sum of squared distances, each of which is between a datum and the center of the cluster that the datum is assigned to. The optimization problem (6), its objective function, and clustering based on it are called in this paper the K-means problem, the K-means loss function, and the K-means clustering, respectively.\nOne can also use the marginal MAP estimation for clustering. If U0 and V0 follow p̂U and p̂V, respectively, the marginal MAP estimation is optimal in the sense that it maximizes the expectation of accuracy with respect to p̂(V0|A). Here, accuracy is defined as the fraction of correctly assigned data among all data. We call the clustering using approximate marginal MAP estimation the maximum accuracy clustering, even when incorrect prior distributions are used."
    }, {
      "heading" : "3 Previous work",
      "text" : "Existing methods for approximately solving the marginalization problem and the MAP problem are divided into stochastic methods such as Markov-Chain Monte-Carlo methods and deterministic ones. A popular deterministic method is to use the variational Bayesian formalism. The variational Bayes matrix factorization [4, 5] approximates the posterior distribution p(U, V |A) as the product of two functions pVBU (U) and p VB V (V ), which are determined so that the Kullback-Leibler (KL) divergence from pVBU (U)p VB V (V ) to p(U, V |A) is minimized. Global minimization of the KL divergence is difficult except for some special cases [15], so that an iterative method to obtain a local minimum is usually adopted. Applying the variational Bayes matrix factorization to the MAP problem, one obtains the iterated conditional modes (ICM) algorithm, which alternates minimization of CMAP(U, V ) over U for fixed V and minimization over V for fixed U .\nThe representative algorithm to solve the K-means problem approximately is Lloyd’s K-means algorithm [12]. Lloyd’s K-means algorithm is regarded as the ICM algorithm: It alternates minimization of the K-means loss function over U for fixed V and minimization over V for fixed U iteratively.\nAlgorithm 1 (Lloyd’s K-means algorithm).\nntl = N∑ j=1 I(vtj = el), ũ t l = 1 ntl N∑ j=1 ajI(v t j = el), (7a)\nlt+1j = arg min l∈{1,...,r̂} ∥aj − ũtl∥22, vt+1j = elt+1j . (7b)\nThroughout this paper, we represent an algorithm by a set of equations as in the above. This representation means that the algorithm begins with a set of initial values and repeats the update of the variables using the equations presented until it satisfies some stopping criteria. Lloyd’s K-means algorithm begins with a set of initial assignments V 0 ∈ {e1, . . . , er̂}N . This algorithm easily gets stuck in local minima and its performance heavily depends on the initial values of the algorithm. Some methods for initialization to obtain a better local minimum are proposed [16].\nMaximum accuracy clustering can be solved approximately by using the variational Bayes matrix factorization, since it gives an approximation to the marginal posterior distribution of vj given A."
    }, {
      "heading" : "4 Proposed algorithm",
      "text" : ""
    }, {
      "heading" : "4.1 Approximate message passing algorithm for low-rank matrix reconstruction",
      "text" : "We first discuss the general idea of the AMP algorithm and advantages of the AMP algorithm compared with the variational Bayes matrix factorization. The AMP algorithm is derived by approximating the belief propagation message passing algorithm in a way thought to be asymptotically exact for large-scale problems with appropriate randomness. Fixed points of the belief propagation message passing algorithm correspond to local minima of the KL divergence between a kind of trial function and the posterior distribution [17]. Therefore, the belief propagation message passing algorithm can be regarded as an iterative algorithm based on an approximation of the posterior distribution, which is called the Bethe approximation. The Bethe approximation can reflect dependence of random variables (dependence between U and V in p̂(U, V |A) in our problem) to some extent. Therefore, one can intuitively expect that performance of the AMP algorithm is better than that of the variational Bayes matrix factorization, which treats U and V as if they were independent in p̂(U, V |A). An important property of the AMP algorithm, aside from its efficiency and effectiveness, is that one can predict performance of the algorithm accurately for large-scale problems by using a set of equations, called the state evolution [6]. Analysis with the state evolution also shows that required iteration numbers are O(1) even when the problem size is large. Although we can present the state evolution for the algorithm proposed in this paper and give a proof of its validity like [8, 18], we do not discuss the state evolution here due to the limited space available.\nWe introduce a one-parameter extension of the posterior distribution p̂(U, V |A) to treat the marginalization problem and the MAP problem in a unified manner. It is defined as follows:\np̂(U, V |A;β) ∝ exp ( − β 2mτ ∥A− UV ⊤∥2F )( p̂U(U)p̂V(V ) )β , (8)\nwhich is proportional to p̂(U, V |A)β , where β > 0 is the parameter. When β = 1, p̂(U, V |A;β) is reduced to p̂(U, V |A). In the limit β → ∞, the distribution p̂(U, V |A;β) concentrates on the maxima of p̂(U, V |A). An algorithm for the marginalization problem on p̂(U, V |A;β) is particularized to the algorithms for the marginalization problem and for the MAP problem for the original posterior distribution p̂(U, V |A) by letting β = 1 and β → ∞, respectively. The AMP algorithm for the marginalization problem on p̂(U, V |A;β) is derived in a way similar to that described in [9], as detailed in the Supplementary Material.\nIn the derived algorithm, the values of variables Btu = (b t u,1, . . . , b t u,m) ⊤ ∈ Rm×r̂, Btv = (btv,1, . . . , b t v,N )\n⊤ ∈ RN×r̂, Λtu ∈ Rr̂×r̂, Λtv ∈ Rr̂×r̂, U t = (ut1, . . . ,utm)⊤ ∈ Rm×r̂, V t = (vt1, . . . ,v t N )\n⊤ ∈ RN×r̂, St1, . . . , Stm ∈ Rr̂×r̂, and T t1 , . . . , T tN ∈ Rr̂×r̂ are calculated iteratively, where the superscript t ∈ N ∪ {0} represents iteration numbers. Variables with a negative iteration number are defined as 0. The algorithm is as follows:\nAlgorithm 2.\nBtu = 1 mτ AV t − 1 mτ U t−1 N∑ j=1 T tj , Λ t u = 1 mτ (V t)⊤V t + 1 βmτ N∑ j=1 T tj − 1 mτ N∑ j=1 T tj , (9a)\nuti = f(b t u,i,Λ t u; p̂u), S t i = G(b t u,i,Λ t u; p̂u), (9b)\nBtv = 1 mτ A⊤U t − 1 mτ V t m∑ i=1 Sti , Λ t v = 1 mτ (U t)⊤U t + 1 βmτ m∑ i=1 Sti − 1 mτ m∑ i=1 Sti , (9c)\nvt+1j = f(b t v,j ,Λ t v; p̂v), T t+1 j = G(b t v,j ,Λ t v; p̂v). (9d)\nAlgorithm 2 is almost symmetric in U and V . Equations (9a)–(9b) and (9c)–(9d) update quantities related to the estimates of U0 and V0, respectively. The algorithm requires an initial value V 0 and begins with T 0j = O. The functions f(·, ·; p̂) : Rr̂×Rr̂×r̂ → Rr̂ and G(·, ·; p̂) : Rr̂×Rr̂×r̂ → Rr̂×r̂, which have a p.d.f. p̂ : Rr̂ → R as a parameter, are defined by\nf(b,Λ; p̂) := ∫ uq̂(u; b,Λ, p̂)du, G(b,Λ; p̂) := ∂f(b,Λ; p̂)\n∂b , (10)\nwhere q̂(u; b,Λ, p̂) is the normalized p.d.f. of u defined by q̂(u; b,Λ, p̂) ∝ exp ( −β (1 2 u⊤Λu− b⊤u− log p̂(u) )) . (11)\nOne can see that f(b,Λ; p̂) is the mean of the distribution q̂(u; b,Λ, p̂) and that G(b,Λ; p̂) is its covariance matrix scaled by β. The function f(b,Λ; p̂) need not be differentiable everywhere; Algorithm 2 works if f(b,Λ; p̂) is differentiable at b for which one needs to calculate G(b,Λ; p̂) in running the algorithm.\nWe assume in the rest of this section the convergence of Algorithm 2, although the convergence is not guaranteed in general. Let B∞u , B ∞ v , Λ ∞ u , Λ ∞ v , S ∞ i , T ∞ j , U\n∞, and V ∞ be the converged values of the respective variables. First, consider running Algorithm 2 with β = 1. The marginal posterior distribution is then approximated as\np̂i,j(ui,vj |A) ≈ q̂(ui; b∞u,i,Λ∞u , p̂u)q̂(vj ; b∞v,j ,Λ∞v , p̂v). (12)\nSince u∞i and v ∞ j are the means of q̂(u; b ∞ u,i,Λ ∞ u , p̂u) and q̂(v; b ∞ v,j ,Λ ∞ v , p̂v), respectively, the posterior mean E[UV ⊤|A] = ∫ UV ⊤p̂(U, V |A)dUdV is approximated as\nE[UV ⊤|A] ≈ U∞(V ∞)⊤. (13)\nThe marginal MAP estimates uMMAPi and v MMAP j are approximated as\nuMMAPi ≈ argmax u q̂(u; b∞u,i,Λ ∞ u , p̂u), v MMAP j ≈ argmax v q̂(v; b∞v,j ,Λ ∞ v , p̂v). (14)\nTaking the limit β → ∞ in Algorithm 2 yields an algorithm for the MAP problem (4). In this case, the functions f and G are replaced with\nf∞(b,Λ; p̂) := argmin u [1 2 u⊤Λu− b⊤u− log p̂(u) ] , G∞(b,Λ; p̂) := ∂f∞(b,Λ; p̂) ∂b . (15)\nOne may calculate G∞(b,Λ; p̂) from the Hessian of log p̂(u) at u = f∞(b,Λ; p̂), denoted by H , via the identity G∞(b,Λ; p̂) = ( Λ−H )−1 . This identity follows from the implicit function theorem under some additional assumptions and helps in the case where the explicit form of f∞(b,Λ; p̂) is not available. The MAP estimate is approximated by (U∞, V ∞)."
    }, {
      "heading" : "4.2 Properties of the algorithm",
      "text" : "Algorithm 2 has several plausible properties. First, it has a low computational cost. The computational cost per iteration is O(mN), which is linear in the number of components of the matrix A. Calculation of f(·, ·; p̂) and G(·, ·; p̂) is performed O(N +m) times per iteration. The constant\nfactor depends on p̂ and β. Calculation of f for β < ∞ generally involves an r̂-dimensional numerical integration, although they are not needed in cases where an analytic expression of the integral is available and cases where the variables take only discrete values. Calculation of f∞ involves minimization over an r̂-dimensional vector. When − log p̂ is a convex function and Λ is positive semidefinite, this minimization problem is convex and can be solved at relatively low cost.\nSecond, Algorithm 2 has a form similar to that of an algorithm based on the variational Bayesian matrix factorization. In fact, if the last terms on the right-hand sides of the four equations in (9a) and (9c) are removed, the resulting algorithm is the same as an algorithm based on the variational Bayesian matrix factorization proposed in [4] and, in particular, the same as the ICM algorithm when β → ∞. (Note, however, that [4] only treats the case where the priors p̂u and p̂v are multivariate Gaussian distributions.) Note that additional computational cost for these extra terms is O(m+N), which is insignificant compared with the cost of the whole algorithm, which is O(mN).\nThird, when one deals with the MAP problem, the value of CMAP(U, V ) may increase in iterations of Algorithm 2. The following proposition, however, guarantees optimality of the output of Algorithm 2 in a certain sense, if it has converged. Proposition 1. Let (U∞, V ∞, S∞1 , . . . , S∞m , T∞1 , . . . , T∞N ) be a fixed point of the AMP algorithm for the MAP problem and suppose that ∑m i=1 S ∞ i and ∑N j=1 T ∞ j are positive semidefinite. Then U∞ is a global minimum of CMAP(U, V ∞) and V ∞ is a global minimum of CMAP(U∞, V ).\nThe proof is in the Supplementary Material. The key to the proof is the following reformulation:\nU t = argmin U\n[ CMAP(U, V t)− tr ( (U − U t−1) ( 1 2mτ N∑ j=1 T tj ) (U − U t−1)⊤ )] (16)\nIf ∑N\nj=1 T t j is positive semidefinite, the second term of the minimand is the negative squared pseudometric between U and U t−1, which is interpreted as a penalty on nearness to the temporal estimate. Positive semidefiniteness of ∑m i=1 S t i and ∑N j=1 T t j holds in almost all cases. In fact, we only have to assume limβ→∞ G(b,Λ; p̂) = G∞(b,Λ; p̂), since G(b,Λ; p̂) is a scaled covariance matrix of q̂(u; b,Λ, p̂), which is positive semidefinite. It follows from Proposition 1 that any fixed point of the AMP algorithm is also a fixed point of the ICM algorithm. It has two implications: (i) Execution of the ICM algorithm initialized with the converged values of the AMP algorithm does not improve CMAP(U t, V t). (ii) The AMP algorithm has not more fixed points than the ICM algorithm. The second implication may help the AMP algorithm avoid getting stuck in bad local minima."
    }, {
      "heading" : "4.3 Clustering via AMP algorithm",
      "text" : "One can use the AMP algorithm for the MAP problem to perform the K-means clustering by letting p̂u(u) = 1 and p̂v(v) = r̂−1 ∑r̂ l=1 δ(v − el). Noting that f∞(b,Λ; p̂v) is piecewise constant with respect to b and hence G∞(b,Λ; p̂v) is O almost everywhere, we obtain the following algorithm: Algorithm 3 (AMP algorithm for the K-means clustering).\nBtu = 1\nmτ AV t, Λtu =\n1\nmτ (V t)⊤V t, U t = Btu(Λ t u) −1, St = (Λtu) −1, (17a)\nBtv = 1 mτ A⊤U t − 1 τ V tSt, Λtv = 1 mτ (U t)⊤U t − 1 τ St, (17b)\nvt+1j = arg min v∈{e1,...,er̂} [1 2 v⊤Λtvv − v⊤btv,j ] . (17c)\nIt is initialized with an assignment V 0 ∈ {e1, . . . , er̂}N . Algorithm 3 is rewritten as follows:\nntl = N∑ j=1 I(vtj = el), ũ t l = 1 ntl N∑ j=1 ajI(v t j = el), (18a)\nlt+1j = arg min l∈{1,...,r̂} [ 1 mτ ∥aj − ũtl∥22 + 2m ntl I(vtj = el)− m ntl ] , vt+1j = elt+1j . (18b)\nThe parameter τ appearing in the algorithm does not exist in the K-means clustering problem. In fact, τ appears because m−2 ∑m i=1 A 2 ijS t i was estimated by τm −1 ∑m i=1 S t i in deriving Algorithm 2, which can be justified for large-sized problems. In practice, we propose using m−2N−1∥A − U t(V t)⊤∥2F as a temporary estimate of τ at tth iteration. While the AMP algorithm for the Kmeans clustering updates the value of U in the same way as Lloyd’s K-means algorithm, it performs assignments of data to clusters in a different way. In the AMP algorithm, in addition to distances from data to centers of clusters, the assignment at present is taken into consideration in two ways: (i) A datum is less likely to be assigned to the cluster that it is assigned to at present. (ii) Data are more likely to be assigned to a cluster whose size at present is smaller. The former can intuitively be understood by observing that if vtj = el, one should take account of the fact that the cluster center ũtl is biased toward aj . The term 2m(n t l)\n−1I(vtj = el) in (18b) corrects this bias, which, as it should be, is inversely proportional to the cluster size.\nThe AMP algorithm for maximum accuracy clustering is obtained by letting β = 1 and p̂v(v) be a discrete distribution on {e1, . . . , er̂}. After the algorithm converges, argmaxv q̂(v;v∞j ,Λ∞v , p̂v) gives the final cluster assignment of the jth datum and U∞ gives the estimate of the cluster centers."
    }, {
      "heading" : "5 Numerical experiments",
      "text" : "We conducted numerical experiments on both artificial and real data sets to evaluate performance of the proposed algorithms for clustering. In the experiment on artificial data sets, we set m = 800 and N = 1600 and let r̂ = r. Cluster centers ũ0,l, l = 1, . . . , r, were generated according to the multivariate Gaussian distribution N(0, I). Cluster assignments v0,j , j = 1, . . . , N, were generated according to the uniform distribution on {e1, . . . , er}. For fixed τ = 0.1 and r, we generated 500 problem instances and solved them with five algorithms: Lloyd’s K-means algorithm (K-means), the AMP algorithm for the K-means clustering (AMP-KM), the variational Bayes matrix factorization [4] for maximum accuracy clustering (VBMF-MA), the AMP algorithm for maximum accuracy clustering (AMP-MA), and the K-means++ [16]. The K-means++ updates the variables in the same way as Lloyd’s K-means algorithm with an initial value chosen in a sophisticated manner. For the other algorithms, initial values v0j , j = 1, . . . , N, were randomly generated from the same distribution as v0,j . We used the true prior distributions of U and V for maximum accuracy clustering.\nWe ran Lloyd’s K-means algorithm and the K-means++ until no change was observed. We ran the AMP algorithm for the K-means clustering until either V t = V t−1 or V t = V t−2 is satisfied. This is because we observed oscillations of assignments of a small number of data. For the other two algorithms, we terminated the iteration when ∥U t − U t−1∥2F < 10−15∥U t−1∥2F and ∥V t − V t−1∥2F < 10−15∥V t−1∥2F were met or the number of iterations exceeded 3000. We then evaluated the following performance measures for the obtained solution (U∗, V ∗):\n• Normalized K-means loss ∥A−U∗(V ∗)⊤∥2F /( ∑N j=1 ∥aj−ā∥22), where ā := 1 N ∑N j=1 aj .\n• Accuracy maxP N−1 ∑N j=1 I(Pv ∗ j = v0,j), where the maximization is taken over all\nr-by-r permutation matrices. We used the Hungarian algorithm [19] to solve this maximization problem efficiently.\n• Number of iterations needed to converge.\nWe calculated the averages and the standard deviations of these performance measures over 500 instances. We conducted the above experiments for various values of r.\nFigure 1 shows the results. The AMP algorithm for the K-means clustering achieves the smallest Kmeans loss among the five algorithms, while the Lloyd’s K-means algorithm and K-means++ show large K-means losses for r ≥ 5. We emphasize that all the three algorithms are aimed to minimize the same K-means loss and the differences lie in the algorithms for minimization. The AMP algorithm for maximum accuracy clustering achieves the highest accuracy among the five algorithms. It also shows fast convergence. In particular, the convergence speed of the AMP algorithm for maximum accuracy clustering is comparable to that of the AMP algorithm for the K-means clustering when the two algorithms show similar accuracy (r < 9). This is in contrast to the common observation that the variational Bayes method often shows slower convergence than the ICM algorithm.\nIn the experiment on real data, we used the ORL Database of Faces [20], which contains 400 images of human faces, ten different images of each of 40 distinct subjects. Each image consists of 112 × 92 = 10304 pixels whose value ranges from 0 to 255. We divided N = 400 images into r̂ = 40 clusters with the K-means++ and the AMP algorithm for the K-means clustering. We adopted the initialization method of the K-means++ also for the AMP algorithm, because random initialization often yielded empty clusters and almost all data were assigned to only one cluster. The parameter τ was estimated in the way proposed in Subsection 4.3. We ran 50 trials with different initial values, and Figure 2 summarizes the results.\nThe AMP algorithm for the K-means clustering outperformed the standard K-means++ algorithm in 48 out of the 50 trials in terms of the K-means loss and in 47 trials in terms of the accuracy. The AMP algorithm yielded just one cluster with all data assigned to it in two trials. The attained minimum value of K-means loss is 0.412 with the K-means++ and 0.400 with the AMP algorithm. The accuracies at these trials are 0.635 with the K-means++ and 0.690 with the AMP algorithm. The average number of iterations was 6.6 with the K-means++ and 8.8 with the AMP algorithm. These results demonstrate efficiency of the proposed algorithm on real data."
    } ],
    "references" : [ {
      "title" : "Least squares formulation of robust non-negative factor analysis",
      "author" : [ "P. Paatero" ],
      "venue" : "Chemometrics and Intelligent Laboratory Systems, vol. 37, no. 1, pp. 23–35, May 1997.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Non-negative matrix factorization with sparseness constraints",
      "author" : [ "P.O. Hoyer" ],
      "venue" : "The Journal of Machine Learning Research, vol. 5, pp. 1457–1469, Dec. 2004.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo",
      "author" : [ "R. Salakhutdinov", "A. Mnih" ],
      "venue" : "Proceedings of the 25th International Conference on Machine Learning, New York, NY, Jul. 5– Aug. 9, 2008, pp. 880–887.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Variational Bayesian approach to movie rating prediction",
      "author" : [ "Y.J. Lim", "Y.W. Teh" ],
      "venue" : "Proceedings of KDD Cup and Workshop, San Jose, CA, Aug. 12, 2007.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Principal component analysis for large scale problems with lots of missing values",
      "author" : [ "T. Raiko", "A. Ilin", "J. Karhunen" ],
      "venue" : "Machine Learning: ECML 2007, ser. Lecture Notes in Computer Science, J. N. Kok, J. Koronacki, R. L. de Mantaras, S. Matwin, D. Mladenič, and A. Skowron, Eds. Springer Berlin Heidelberg, 2007, vol. 4701, pp. 691–698.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Message-passing algorithms for compressed sensing",
      "author" : [ "D.L. Donoho", "A. Maleki", "A. Montanari" ],
      "venue" : "Proceedings of the National Academy of Sciences USA, vol. 106, no. 45, pp. 18 914–18 919, Nov. 2009.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Generalized approximate message passing for estimation with random linear mixing",
      "author" : [ "S. Rangan" ],
      "venue" : "Proceedings of 2011 IEEE International Symposium on Information Theory, St. Petersburg, Russia, Jul. 31– Aug. 5, 2011, pp. 2168–2172.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Iterative estimation of constrained rank-one matrices in noise",
      "author" : [ "S. Rangan", "A.K. Fletcher" ],
      "venue" : "Proceedings of 2012 IEEE International Symposium on Information Theory, Cambridge, MA, Jul. 1–6, 2012, pp. 1246–1250.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Approximate message passing algorithm for low-rank matrix reconstruction",
      "author" : [ "R. Matsushita", "T. Tanaka" ],
      "venue" : "Proceedings of the 35th Symposium on Information Theory and its Applications, Oita, Japan, Dec. 11–14, 2012, pp. 314–319.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Document clustering based on non-negative matrix factorization",
      "author" : [ "W. Xu", "X. Liu", "Y. Gong" ],
      "venue" : "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, Toronto, Canada, Jul. 28–Aug. 1, 2003, pp. 267–273.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Convex and semi-nonnegative matrix factorizations",
      "author" : [ "C. Ding", "T. Li", "M. Jordan" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 1, pp. 45–55, Jan. 2010.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Least squares quantization in PCM",
      "author" : [ "S.P. Lloyd" ],
      "venue" : "IEEE Transactions on Information Theory, vol. IT-28, no. 2, pp. 129–137, Mar. 1982.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "Phase diagram and approximate message passing for blind calibration and dictionary learning",
      "author" : [ "F. Krzakala", "M. Mézard", "L. Zdeborová" ],
      "venue" : "preprint, Jan. 2013, arXiv:1301.5898v1 [cs.IT].",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Bilinear generalized approximate message passing",
      "author" : [ "J.T. Parker", "P. Schniter", "V. Cevher" ],
      "venue" : "preprint, Oct. 2013, arXiv:1310.2632v1 [cs.IT].",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Theoretical analysis of Bayesian matrix factorization",
      "author" : [ "S. Nakajima", "M. Sugiyama" ],
      "venue" : "Journal of Machine Learning Research, vol. 12, pp. 2583–2648, Sep. 2011.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "k-means++: the advantages of careful seeding",
      "author" : [ "D. Arthur", "S. Vassilvitskii" ],
      "venue" : "SODA ’07 Proceedings of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, New Orleans, Louisiana, Jan. 7–9, 2007, pp. 1027–1035.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Constructing free-energy approximations and generalized belief propagation algorithms",
      "author" : [ "J.S. Yedidia", "W.T. Freeman", "Y. Weiss" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 51, no. 7, pp. 2282–2312, Jul. 2005.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The dynamics of message passing on dense graphs, with applications to compressed sensing",
      "author" : [ "M. Bayati", "A. Montanari" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 57, no. 2, pp. 764–785, Feb. 2011.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The Hungarian method for the assignment problem",
      "author" : [ "H.W. Kuhn" ],
      "venue" : "Naval Research Logistics Quarterly, vol. 2, no. 1–2, pp. 83–97, Mar. 1955.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1955
    }, {
      "title" : "Parameterisation of a stochastic model for human face identification",
      "author" : [ "F.S. Samaria", "A.C. Harter" ],
      "venue" : "Proceedings of 2nd IEEE Workshop on Applications of Computer Vision, Sarasota FL, Dec. 1994, pp. 138–142. [Online]. Available: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html 9",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Sparseness and non-negativity of the factors are popular examples of such structural properties [1, 2].",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 1,
      "context" : "Sparseness and non-negativity of the factors are popular examples of such structural properties [1, 2].",
      "startOffset" : 96,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "Developed in the context of compressed sensing, the AMP algorithm reconstructs sparse vectors from their linear measurements with low computational cost, and achieves a certain theoretical limit [6].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 6,
      "context" : "AMP algorithms can also be used for approximating Bayesian inference with a large class of prior distributions of signal vectors and noise distributions [7].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 7,
      "context" : "The IterFac algorithm for the rank-one case [8] has been derived as an AMP algorithm.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "An AMP algorithm for the general-rank case is proposed in [9], which, however, can only treat estimation of posterior means.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "Although the idea of applying low-rank matrix reconstruction to clustering is not new [10, 11], our proposed algorithm is, to our knowledge, the first that directly deals with the constraint that each datum should be assigned to exactly one cluster in the framework of low-rank matrix reconstruction.",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "Although the idea of applying low-rank matrix reconstruction to clustering is not new [10, 11], our proposed algorithm is, to our knowledge, the first that directly deals with the constraint that each datum should be assigned to exactly one cluster in the framework of low-rank matrix reconstruction.",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "We present results of numerical experiments, which show that the proposed algorithm outperforms Lloyd’s K-means algorithm [12] when data are high-dimensional.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "Recently, AMP algorithms for dictionary learning and blind calibration [13] and for matrix reconstruction with a generalized observation model [14] were proposed.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "Recently, AMP algorithms for dictionary learning and blind calibration [13] and for matrix reconstruction with a generalized observation model [14] were proposed.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "A clustering problem can be formulated as a problem of low-rank matrix reconstruction [11].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "The variational Bayes matrix factorization [4, 5] approximates the posterior distribution p(U, V |A) as the product of two functions p U (U) and p VB V (V ), which are determined so that the Kullback-Leibler (KL) divergence from p U (U)p VB V (V ) to p(U, V |A) is minimized.",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 4,
      "context" : "The variational Bayes matrix factorization [4, 5] approximates the posterior distribution p(U, V |A) as the product of two functions p U (U) and p VB V (V ), which are determined so that the Kullback-Leibler (KL) divergence from p U (U)p VB V (V ) to p(U, V |A) is minimized.",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "Global minimization of the KL divergence is difficult except for some special cases [15], so that an iterative method to obtain a local minimum is usually adopted.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "The representative algorithm to solve the K-means problem approximately is Lloyd’s K-means algorithm [12].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "Some methods for initialization to obtain a better local minimum are proposed [16].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 16,
      "context" : "Fixed points of the belief propagation message passing algorithm correspond to local minima of the KL divergence between a kind of trial function and the posterior distribution [17].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 5,
      "context" : "An important property of the AMP algorithm, aside from its efficiency and effectiveness, is that one can predict performance of the algorithm accurately for large-scale problems by using a set of equations, called the state evolution [6].",
      "startOffset" : 234,
      "endOffset" : 237
    }, {
      "referenceID" : 7,
      "context" : "Although we can present the state evolution for the algorithm proposed in this paper and give a proof of its validity like [8, 18], we do not discuss the state evolution here due to the limited space available.",
      "startOffset" : 123,
      "endOffset" : 130
    }, {
      "referenceID" : 17,
      "context" : "Although we can present the state evolution for the algorithm proposed in this paper and give a proof of its validity like [8, 18], we do not discuss the state evolution here due to the limited space available.",
      "startOffset" : 123,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "The AMP algorithm for the marginalization problem on p̂(U, V |A;β) is derived in a way similar to that described in [9], as detailed in the Supplementary Material.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "In fact, if the last terms on the right-hand sides of the four equations in (9a) and (9c) are removed, the resulting algorithm is the same as an algorithm based on the variational Bayesian matrix factorization proposed in [4] and, in particular, the same as the ICM algorithm when β → ∞.",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 3,
      "context" : "(Note, however, that [4] only treats the case where the priors p̂u and p̂v are multivariate Gaussian distributions.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "1 and r, we generated 500 problem instances and solved them with five algorithms: Lloyd’s K-means algorithm (K-means), the AMP algorithm for the K-means clustering (AMP-KM), the variational Bayes matrix factorization [4] for maximum accuracy clustering (VBMF-MA), the AMP algorithm for maximum accuracy clustering (AMP-MA), and the K-means++ [16].",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 15,
      "context" : "1 and r, we generated 500 problem instances and solved them with five algorithms: Lloyd’s K-means algorithm (K-means), the AMP algorithm for the K-means clustering (AMP-KM), the variational Bayes matrix factorization [4] for maximum accuracy clustering (VBMF-MA), the AMP algorithm for maximum accuracy clustering (AMP-MA), and the K-means++ [16].",
      "startOffset" : 342,
      "endOffset" : 346
    }, {
      "referenceID" : 18,
      "context" : "We used the Hungarian algorithm [19] to solve this maximization problem efficiently.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 19,
      "context" : "In the experiment on real data, we used the ORL Database of Faces [20], which contains 400 images of human faces, ten different images of each of 40 distinct subjects.",
      "startOffset" : 66,
      "endOffset" : 70
    } ],
    "year" : 2013,
    "abstractText" : "We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows us to exploit structural properties of matrices in addition to low-rankedness, such as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for matrix reconstruction. We have also successfully applied the proposed algorithm to a clustering problem, by reformulating it as a low-rank matrix reconstruction problem with an additional structural property. Numerical experiments show that the proposed algorithm outperforms Lloyd’s K-means algorithm.",
    "creator" : null
  }
}