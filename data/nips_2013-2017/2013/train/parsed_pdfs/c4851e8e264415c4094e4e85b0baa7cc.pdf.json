{
  "name" : "c4851e8e264415c4094e4e85b0baa7cc.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Relevance Topic Model for Unstructured Social Group Activity Recognition",
    "authors" : [ "Fang Zhao", "Yongzhen Huang", "Liang Wang", "Tieniu Tan" ],
    "emails" : [ "fang.zhao@nlpr.ia.ac.cn", "yzhuang@nlpr.ia.ac.cn", "wangliang@nlpr.ia.ac.cn", "tnt@nlpr.ia.ac.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The explosive growth of web videos makes automatic video classification important for online video search and indexing. Classifying short video clips which contain simple motions and actions has been solved well in standard datasets (such as KTH [1], UCF-Sports [2] and UCF50 [3]). However, detecting complex activities, specially social group activities [4], in web videos is a more difficult task because of unstructured activity context and complex multi-object interaction.\nIn this paper, we focus on the task of automatic classification of unstructured social group activities (e.g., wedding dance, birthday party and graduation ceremony in Figure 1), where the low-level features have innate limitations in semantic description of the underlying video data and only a few labeled training videos are available. Thus, a common method is to learn human-defined (or semi-human-defined) semantic concepts as mid-level representations to help video classification [4]. However, those human defined concepts are hardly generalized to a larger or new dataset. To discover more powerful representations for classification, we propose a novel supervised topic model called “relevance topic model” to automatically extract latent “relevance” topics from bag-of-words (BoW) video representations and simultaneously learn a classifier with sparse weights.\nOur model is built on Replicated Softmax [5], an undirected topic model which can be viewed as a family of different-sized Restricted Boltzmann Machines that share parameters. Sparse Bayesian learning [6] is incorporated to guide the topic model towards learning more predictive topics which are associated with sparse classifier weights. We refer to those topics corresponding to non-zero weights as “relevance” topics. Meanwhile, binary stochastic units in Replicated Softmax are replaced by rectified linear units [7], which allows each unit to express more information for better\nexplaining video data containing complex content and also makes variational inference tractable for the proposed model. Furthermore, by using a simple quadratic bound on the log-sum-exp function [8], an efficient variational EM algorithm is developed for parameter estimation and inference. Our model is able to be naturally extended to deal with multi-modal data without changing the learning and inference procedures, which is beneficial for video classification tasks."
    }, {
      "heading" : "2 Related work",
      "text" : "The problems of activity analysis and recognition have been widely studied. However, most of the existing works [9, 10] were done on constrained videos with limited contents (e.g., clean background and little camera motions). Complex activity recognition in web videos, such as social group activity, is not much explored. Most relevant to our work is a recent work that learns video attributes to analyze social group activity [4]. In [4], a semi-latent attribute space is introduced, which consists of user-defined attributes, class-conditional and background latent attributes, and an extended Latent Dirichlet Allocation (LDA) [11] is used to model those attributes as topics. Different from that, our work discovers a set of discriminative latent topics without extra human annotations on videos.\nFrom the view of graphical models, most similar to our model are the maximum entropy discrimination LDA (MedLDA) [12] and the generative Classification Restricted Boltzmann Machines (gClassRBM) [13], both of which have been successfully applied to document semantic analysis. MedLDA integrates the max-margin learning and hierarchical directed topic models by optimizing a single objective function with a set of expected margin constraints. MedLDA tries to estimate parameters and find latent topics in a max-margin sense, which is different from our model relying on the principle of automatic relevance determination [14]. The gClassRBM used to model word count data is actually a supervised Replicated Softmax. Different from the gClassRBM, instead of point estimation of classifier parameters, our proposed model learns a sparse posterior distribution over parameters within a Bayesian paradigm."
    }, {
      "heading" : "3 Models and algorithms",
      "text" : "We start with the description of Replicated Softmax, and then by integrating it with sparse Bayesian learning, propose the relevance topic model for videos. Finally, we develop an efficient variational algorithm for inference and parameter estimation."
    }, {
      "heading" : "3.1 Replicated Softmax",
      "text" : "The Replicated Softmax model is a two-layer undirected graphical model, which can be used to model sparse word count data and extract latent semantic topics from document collections. Replicated Softmax allows for very efficient inference and learning, and outperforms LDA in terms of both the generalization performance and the retrieval accuracy on text datasets.\nAs shown in Figure 2 (left), this model is a generalization of the restricted Boltzmann machine (RBM). The bottom layer represents a multinomial visible unit sampled K times (K is the total number of words in a document) and the top layer represents binary stochastic hidden units.\nLet a word count vector v ∈ NN be the visible unit (N is the size of the vocabulary), and a binary topic vector h ∈ {0, 1}F be the hidden units. Then the energy function of the state {v,h} is defined as follows:\nE(v,h; θ) = − N∑ i=1 F∑ j=1 Wijvihj − N∑ i=1 aivi −K F∑ j=1 bjhj , (1)\nwhere θ = {W,a,b}, Wij is the weight connected with vi and hj , ai and bj are the bias terms of visible and hidden units respectively. The joint distribution over the visible and hidden units is defined by:\nP (v,h; θ) = 1\nZ(θ) exp(−E(v,h; θ)), Z(θ) = ∑ v ∑ h exp(−E(v,h; θ)), (2)\nwhere Z(θ) is the partition function. Since exact maximum likelihood learning is intractable, the contrastive divergence [15] approximation is often used to estimate model parameters in practice."
    }, {
      "heading" : "3.2 Relevance topic model",
      "text" : "The relevance topic model (RTM) is an integration of sparse Bayesian learning and Replicated Softmax, the main idea of which is to jointly learn discriminative topics as mid-level video representations and sparse discriminant function as a video classifier.\nWe represent the video dataset with class labels y ∈ {1, ..., C} as D = {(vm, ym)}Mm=1, where each video is represented as a BoW vector v ∈ NN . Consider modeling video BoW vectors using the Replicated Softmax. Let tr = [tr1, ..., t r F ] denotes a F-dimensional topic vector of one video. According to Equation 2, the marginal distribution over the BoW vector v is given by:\nP (v; θ) = 1 Z(θ) ∑ tr exp(−E(v, tr; θ)), (3)\nSince videos contain more complex and diverse contents than documents, binary topics are far from ideal to explain video data. We replace binary hidden units in the original Replicated Softmax with rectified linear units which are given by:\ntrj = max(0, tj), P (tj |v; θ) = N (tj |Kbj + N∑ i=1 Wijvi, 1), (4)\nwhere N (·|µ, τ) denotes a Gaussian distribution with mean µ and variance τ . The rectified linear units taking nonnegative real values can preserve information about relative importance of topics. Meanwhile, the rectified Gaussian distribution is semi-conjugate to the Gaussian likelihood. This facilitates the development of variational algorithms for posterior inference and parameter estimation, which we describe in Section 3.3.\nLet η = {ηy}Cy=1 denote a set of class-specific weight vectors. We define the discriminant function as a linear combination of topics: F (y, tr,η) = ηTy tr. The conditional distribution of classes is\ndefined as follows:\nP (y|tr,η) = exp(F (y, t r,η))∑C\ny′=1 exp(F (y ′, tr,η))\n, (5)\nand the classifier is given by:\nŷ = arg max y∈C\nE[F (y, tr,η)|v]. (6)\nThe weights η are given a zero-mean Gaussian prior:\nP (η|α) = C∏ y=1 F∏ j=1 P (ηyj |αyj) = C∏ y=1 F∏ j=1 N(ηyj |0, α−1yj ), (7)\nwhere α = {αy}Cy=1 is a set of hyperparameter vectors, and each hyperparameter αyj is assigned independently to each weight ηyj . The hyperpriors over α are given by Gamma distributions:\nP (α) = C∏ y=1 F∏ j=1 P (αyj) = C∏ y=1 F∏ j=1 Γ(c) −1 dcαc−1yj e −dα, (8)\nwhere Γ(c) is the Gamma function. To obtain broad hyperpriors, we set c and d to small values, e.g., c = d = 10−4. This hierarchical prior, which is a type of automatic relevance determination prior [14], enables the posterior probability of the weights η to be concentrated at zero and thus effectively switch off the corresponding topics that are considered to be irrelevant to classification.\nFinally, given the parameters θ, RTM defines the joint distribution: P (v, y, tr,η,α; θ) = P (v; θ)P (y|tr,η) ( F∏ j=1 P (tj |v; θ) )( C∏ y=1 F∏ j=1 P (ηyj |αyj)P (αyj) ) . (9)\nFigure 2 (right) illustrates RTM as a mixed graphical model with undirected and directed edges. The undirected part models the marginal distribution of video data and the directed part models the conditional distribution of classes given latent topics. We can naturally extend RTM to Multimodal RTM by using the undirected part to model the multimodal data v = {vmodl}Ll=1. Accordingly, P (v; θ) in Equation 9 is replaced with ∏L l=1 P (v\nmodl; θmodl). In Section 3.3, we can see that it will not change learning and inference rules."
    }, {
      "heading" : "3.3 Parameter estimation and inference",
      "text" : "For RTM, we wish to find parameters θ = {W,a,b} that maximize the log likelihood on D: logP (D; θ) = log ∫ P ({vm, ym, trm}Mm=1,η,α; θ)d{tm}Mm=1dηdα, (10)\nand learn the posterior distribution P (η,α|D; θ) = P (η,α,D; θ)/P (D; θ). Since exactly computing P (D; θ) is intractable, we employ variational methods to optimize a lower bound L on the log likelihood by introducing a variational distribution to approximate P ({tm}Mm=1,η,α|D; θ):\nQ({tm}Mm=1,η,α) = ( M∏ m=1 F∏ j=1 q(tmj) ) q(η)q(α). (11)\nUsing Jensens inequality, we have: logP (D; θ) > ∫ Q({tm}Mm=1,η,α)\nlog\n(∏M m=1 P (vm; θ)P (ym|trm,η)P (tm|vm; θ) ) P (η|α)P (α)\nQ({tm}Mm=1,η,α) d{tm}Mm=1dηdα. (12)\nNote that P (ym|trm,η) is not conjugate to the Gaussian prior, which makes it intractable to compute the variational factors q(η) and q(tmj). Here we use a quadratic bound on the log-sum-exp (LSE) function [8] to derive a further bound. We rewrite P (ym|trm,η) as follows:\nP (ym|trm,η) = exp(yTmTrmη − lse(Trmη)), (13)\nwhere Trmη = [(trm)Tη1, ..., (trm)TηC−1], ym = I(ym = c) is the one-of-C encoding of class label ym and lse(x) , log(1 + ∑C−1 y′=1 exp(xy′)) (we set ηC = 0 to ensure identifiability). In [8], the LSE function is expanded as a second order Taylor series around a point ϕ, and an upper bound is found by replacing the Hessian matrix H(ϕ) with a fixed matrix A = 12 [IC∗ − 1 C∗+11C∗1 T C∗ ] such that A H(ϕ), where C∗ = C − 1, IC∗ is the identity matrix of size M ×M and 1C∗ is a M -vector of ones. Thus, similar to [16], we have:\nlogP (ym|trm,η) > J(ym, trm,η,ϕm) = y T mT r mη −\n1 2 (Trmη) TATrmη + s T mT r mη − κi, (14)\nsm = Aϕm − exp(ϕm − lse(ϕm)), (15)\nκi = 1\n2 ϕTmAϕm − ϕ T m exp(ϕm − lse(ϕm)) + lse(ϕm), (16)\nwhere ϕm ∈ RC ∗\nis a vector of variational parameters. Substituting J(ym, trm,η,ϕm) into Equation 11, we can obtain a further lower bound:\nlogP (D; θ) > L(θ,ϕ) = M∑ m=1 logP (vm; θ) + EQ [ M∑ m=1 J(ym, t r m,η,ϕm)\n+ M∑ m=1 logP (tm|vm; θ) + logP (η|α) + logP (α)−Q({tm}Mm=1,η,α) ] . (17)\nNow we convert the problem of model training into maximizing the lower bound L(θ,ϕ) with respect to the variational posteriors q(η), q(α) and q(t) = {q(tmj)} as well as the parameters θ and ϕ = {ϕm}. We can give some insights into the objective function L(θ,ϕ): the first term is exactly the marginal log likelihood of video data and the second term is a variational bound of the conditional log likelihood of classes, thus maximizing L(θ,ϕ) is equivalent to finding a set of model parameters and latent topics which could fit video data well and simultaneously make good predictions for video classes.\nDue to the conjugacy properties of the chosen distributions, we can directly calculate free-form variational posteriors q(η), q(α) and parameters ϕ:\nq(η) = N (η|Eη,Vη), (18)\nq(α) = C∏ y=1 F∏ j=1 Gamma(αyj |ĉ, d̂yj), (19)\nϕ = 〈Trm〉q(t)Eη, (20)\nwhere 〈·〉q denotes an exception with respect to the distribution q and\nVη = ( M∑ m=1 〈 (Trm) TATrm 〉 q(t) + diag〈αyj〉q(α) )−1 ,Eη = Vη M∑ m=1 〈 (Trm) T 〉 q(t) (ym + sm),\n(21)\nĉ = c+ 1\n2 , d̂yj = d+\n1\n2\n〈 η2yj 〉 q(η). (22)\nFor q(t), the calculation is not immediate because of the rectification. Inspired by [17], we have the following free-form solution:\nq(tmj) = ωpos Z N (tmj |µpos, σ2pos)u(tmj) + ωneg Z N (tmj |µneg, σ2neg)u(−tmj), (23)\nwhere u(·) is the unit step function. See Appendix A for parameters of q(tmj). Given θ, through repeating the updates of Equations 18-20 and 23 to maximize L(θ,ϕ), we can obtain the variational posteriors q(η), q(α) and q(t). Then given q(η), q(α) and q(t), we estimate θ by using stochastic gradient descent to maximize L(θ,ϕ), and the derivatives of L(θ,ϕ) with\nrespect to θ are given by:\n∂L(θ,ϕ) ∂Wij\n= 〈 vit r j 〉 data − 〈 vit r j 〉 model + 1\nM M∑ m=1 vmi ( 〈tmj〉q(t) − N∑ i=1 Wijvmi −Kbj ) , (24)\n∂L(θ,ϕ) ∂ai = 〈vi〉data − 〈vi〉model, (25)\n∂L(θ,ϕ) ∂bj = 〈 trj 〉 data − 〈 trj 〉 model + K M M∑ m=1 ( 〈tmj〉q(t) − N∑ i=1 Wijvmi −Kbj ) , (26)\nwhere the derivatives of ∑M m=1 logP (vm; θ) are the same as those in [5].\nThis leads to the following variational EM algorithm:\nE-step: Calculate variational posteriors q(η), q(α) and q(t). M-step: Estimate parameters θ = {W,a,b} through maximizing L(θ,ϕ).\nThese two steps are repeated until L(θ,ϕ) converges. For the Multimodal RTM learning, we just additionally calculate the gradients of θmodl for each modality l in the M-step while the updating rules are not changed.\nAfter the learning is completed, according to Equation 6 the prediction for new videos can be easily obtained:\nŷ = arg max y∈C\n〈 ηTy 〉 q(η)〈t r〉p(t|v;θ). (27)"
    }, {
      "heading" : "4 Experiments",
      "text" : "We test our models on the Unstructured Social Activity Attribute (USAA) dataset 1 for social group activity recognition. Firstly, we present quantitative evaluations of RTM in the case of different modalities and comparisons with other supervised topic models (namely MedLDA and gClassRBM). Secondly, we compare Multimodal RTM with some baselines in the case of plentiful and sparse training data respectively. In all experiments, the contrastive divergence is used to efficiently approximate the derivatives of the marginal log likelihood and the unsupervised training on Replicated Softmax is used to initialize θ."
    }, {
      "heading" : "4.1 Dataset and video representation",
      "text" : "The USAA dataset consists of 8 semantic classes of social activity videos collected from the Internet. The eight classes are: birthday party, graduation party, music performance, non-music performance, parade, wedding ceremony, wedding dance and wedding reception. The dataset contains a total of 1466 videos and approximate 100 videos per-class for training and testing respectively. These videos range from 20 seconds to 8 minutes averaging 3 minutes and contain very complex and diverse contents, which brings significant challenges for content analysis.\nEach video is represented using three modalities, i.e., static appearance, motion, and auditory. Specifically, three visual and audio local keypoint features are extracted for each video: scaleinvariant feature transform (SIFT) [18], spatial-temporal interest points (STIP) [19] and melfrequency cepstral coefficients (MFCC) [20]. Then the three features are collected into a BoW vector (5000 dimensions for SIFT and STIP, and 4000 dimensions for MFCC) using a soft-weighting clustering algorithm, respectively."
    }, {
      "heading" : "4.2 Model comparisons",
      "text" : "To evaluate the discriminative power of video topics learned by RTM, we present quantitative classification results compared with other supervised topic models (MedLDA and gClassRBM) in the case of different modalities. We have tried our best to tune these compared models and report the best results.\n1Available at http://www.eecs.qmul.ac.uk/˜yf300/USAA/download/.\nTable 1 shows the classification accuracy of different models for three single-modal features: SIFT, STIP and MFCC. We can see that RTM achieves higher classification accuracy than MedLDA and gClassRBM in all cases, which demonstrates that through leveraging sparse Bayesian learning to incorporate class label information into topic modeling, RTM can find more discriminative topical representations for complex video data."
    }, {
      "heading" : "4.3 Baseline comparisons",
      "text" : "We compare Multimodal RTM with the baselines in [4] which are the best results on the USAA dataset:\nDirect Direct SVM or KNN classification on raw video BoW vectors (14000 dimensions), where SVM is used for experiments with more than 10 instances and KNN otherwise.\nSVM-UD+LR SVM attribute classifiers learn 69 user-defined attributes, and then a logistic regression (LR) classifier is performed according to the attribute classifier outputs.\nSLAS+LR Semi-latent attribute space is learned, and then a LR classifier is performed based on the 69 user-defined, 8 class-conditional and 8 latent topics.\nBesides, we also perform a comparison with another baseline where different modal topics extracted by Replicated Softmax are connected together as video representations, and then a multi-class SVM classifier [21] is learned from the representations. This baseline is denoted by RS+SVM.\nThe results are illustrated in Table 2. Here the number of topics of each modality is assumed to be the same. When the labeled training data is plentiful (100 instances per class), the classification performance of Multimodal RTM is similar to the baselines in [4]. However, We argue that our model learns a lower dimensional latent semantic space which provides efficient video representations and is able to be better generalized to a larger or new dataset because extra human defined concepts are not required in our model. When considering the classification scenario where only a very small number of training data are available (10 instances per class), Multimodal RTM can achieve better performance with an appropriate number (e.g., > 90) of topics because the sparsity of relevance topics learned by RTM can effectively prevent overfitting to specific training instances. In addition, our model outperforms RS+SVM in both cases, which demonstrates the advantage of jointly learning topics and classifier weights through sparse Bayesian learning.\nIt is also interesting to examine the sparsity of relevance topics. Figure 3 illustrates the degree of correlation between topics and two different classes. We can see that the learned relevance topics are very sparse, which leads to good generalisation for new instances and robustness for small datasets."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper has proposed a supervised topic model, the relevance topic model (RTM), to jointly learn discriminative latent topical representations and a sparse classifer for recognizing unstructured social group activity. In RTM, sparse Bayisian learning is integrated with an undirected topic model to discover sparse relevance topics. Rectified linear units are employed to better fit complex video data and facilitate the learning of the model. Efficient variational methods are developed for parameter estimation and inference. To further improve video classification performance, RTM is also extended to deal with multimodal data. Experimental results demonstrate that RTM can find more predictive video topics than other supervised topic models and achieve state of the art classification performance, particularly in the scenario of lacking labeled training videos.\nAppendix A. Parameters of free-form variational posterior q(tmj)\nThe expressions of parameters in q(tmj) (Equation 23) are listed as follows:\nωpos = N (α|β, γ + 1), σ2pos = (γ−1 + 1)−1, µpos = σ2pos( α\nγ + β), (28)\nωneg = N (α|0, γ), σ2neg = 1, µneg = β, (29)\nZ = 1\n2 ωposerfc\n( −µpos√\n2σ2pos\n) + 1\n2 ωnegerfc ( µneg√ 2σ2neg ) , (30)\nwhere erfc(·) is the complementary error function and\nα =\n〈 η·j ( ym + sm − ∑ j′ 6=j η·j′At r mj′ ) η·jAη T ·j 〉 q(η)q(t) , (31)\nγ = 〈\nη·jAη T ·j 〉−1 q(η) , β = N∑ i=1 Wijvmi +Kbj . (32)\nWe can see that q(tmj) depends on expectations over η and {tmj′}j′ 6=j , which is consistent with the graphical model representation of RTM in Figure 2."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by the National Basic Research Program of China (2012CB316300), Hundred Talents Program of CAS, National Natural Science Foundation of China (61175003, 61135002, 61203252), and Tsinghua National Laboratory for Information Science and Technology Crossdiscipline Foundation."
    } ],
    "references" : [ {
      "title" : "Recognizing human actions: a local svm approach",
      "author" : [ "C. Schuldt", "I. Laptev", "B. Caputo" ],
      "venue" : "ICPR",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Action mach a spatio-temporal maximum average correlation height filter for action recognition",
      "author" : [ "M. Rodriguez", "J. Ahmed", "M. Shah" ],
      "venue" : "CVPR",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Attribute learning for understanding unstructured social activity",
      "author" : [ "Y.W. Fu", "T.M. Hospedales", "T. Xiang", "S.G. Gong" ],
      "venue" : "ECCV",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Replicated softmax: an undirected topic model",
      "author" : [ "R. Salakhutdinov", "G.E. Hinton" ],
      "venue" : "NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sparse bayesian learning and the relevance vector machine",
      "author" : [ "M.E. Tipping" ],
      "venue" : "JMLR",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "ICML",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multinomial logistic regression algorithm",
      "author" : [ "D. Bohning" ],
      "venue" : "AISM",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Machine recognition of human activities: a survey",
      "author" : [ "P. Turaga", "R. Chellappa", "V.S. Subrahmanian", "O. Udrea" ],
      "venue" : "TCSVT",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A sequential topic model for mining recurrent activities from long term video logs",
      "author" : [ "J. Varadarajan", "R. Emonet", "J.-M. Odobez" ],
      "venue" : "IJCV",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "D. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "JMLR",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Medlda: Maximum margin supervised topic models",
      "author" : [ "J. Zhu", "A. Ahmed", "E.P. Xing" ],
      "venue" : "JMLR",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning algorithms for the classification restricted boltzmann machine",
      "author" : [ "H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio" ],
      "venue" : "JMLR",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Bayesian learning for neural networks",
      "author" : [ "R.M. Neal" ],
      "venue" : "PhD thesis, University of Toronto",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Neural Computation",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Machine learning: a probabilistic perspective",
      "author" : [ "K.P. Murphy" ],
      "venue" : "MIT Press",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Variational learning for rectified factor analysis",
      "author" : [ "M. Harva", "A. Kaban" ],
      "venue" : "Signal Processing",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Distinctive image features from scale-invariant keypoints",
      "author" : [ "D.G. Lowe" ],
      "venue" : "IJCV",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "On space-time interest points",
      "author" : [ "I. Laptev" ],
      "venue" : "IJCV",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Mel frequency cepstral coefficients for music modeling",
      "author" : [ "B. Logan" ],
      "venue" : "ISMIR",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Support vector learning for interdependent and structured output spaces",
      "author" : [ "I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun" ],
      "venue" : "ICML",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Classifying short video clips which contain simple motions and actions has been solved well in standard datasets (such as KTH [1], UCF-Sports [2] and UCF50 [3]).",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "Classifying short video clips which contain simple motions and actions has been solved well in standard datasets (such as KTH [1], UCF-Sports [2] and UCF50 [3]).",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "However, detecting complex activities, specially social group activities [4], in web videos is a more difficult task because of unstructured activity context and complex multi-object interaction.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "Thus, a common method is to learn human-defined (or semi-human-defined) semantic concepts as mid-level representations to help video classification [4].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "Our model is built on Replicated Softmax [5], an undirected topic model which can be viewed as a family of different-sized Restricted Boltzmann Machines that share parameters.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "Sparse Bayesian learning [6] is incorporated to guide the topic model towards learning more predictive topics which are associated with sparse classifier weights.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "Meanwhile, binary stochastic units in Replicated Softmax are replaced by rectified linear units [7], which allows each unit to express more information for better",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Figure 1: Example videos of the “Wedding Dance”, “Birthday Party” and “Graduation Ceremony” classes taken from the USAA dataset [4].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, by using a simple quadratic bound on the log-sum-exp function [8], an efficient variational EM algorithm is developed for parameter estimation and inference.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "However, most of the existing works [9, 10] were done on constrained videos with limited contents (e.",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "However, most of the existing works [9, 10] were done on constrained videos with limited contents (e.",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : "Most relevant to our work is a recent work that learns video attributes to analyze social group activity [4].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "In [4], a semi-latent attribute space is introduced, which consists of user-defined attributes, class-conditional and background latent attributes, and an extended Latent Dirichlet Allocation (LDA) [11] is used to model those attributes as topics.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 9,
      "context" : "In [4], a semi-latent attribute space is introduced, which consists of user-defined attributes, class-conditional and background latent attributes, and an extended Latent Dirichlet Allocation (LDA) [11] is used to model those attributes as topics.",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "From the view of graphical models, most similar to our model are the maximum entropy discrimination LDA (MedLDA) [12] and the generative Classification Restricted Boltzmann Machines (gClassRBM) [13], both of which have been successfully applied to document semantic analysis.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "From the view of graphical models, most similar to our model are the maximum entropy discrimination LDA (MedLDA) [12] and the generative Classification Restricted Boltzmann Machines (gClassRBM) [13], both of which have been successfully applied to document semantic analysis.",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "MedLDA tries to estimate parameters and find latent topics in a max-margin sense, which is different from our model relying on the principle of automatic relevance determination [14].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Since exact maximum likelihood learning is intractable, the contrastive divergence [15] approximation is often used to estimate model parameters in practice.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "This hierarchical prior, which is a type of automatic relevance determination prior [14], enables the posterior probability of the weights η to be concentrated at zero and thus effectively switch off the corresponding topics that are considered to be irrelevant to classification.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "Here we use a quadratic bound on the log-sum-exp (LSE) function [8] to derive a further bound.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "In [8], the LSE function is expanded as a second order Taylor series around a point φ, and an upper bound is found by replacing the Hessian matrix H(φ) with a fixed matrix A = 12 [IC∗ − 1 C∗+11C∗1 T C∗ ] such that A H(φ), where C∗ = C − 1, IC∗ is the identity matrix of size M ×M and 1C∗ is a M -vector of ones.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "Inspired by [17], we have the following free-form solution: q(tmj) = ωpos Z N (tmj |μpos, σ(2) pos)u(tmj) + ωneg Z N (tmj |μneg, σ(2) neg)u(−tmj), (23) where u(·) is the unit step function.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "i=1 Wijvmi −Kbj ) , (26) where the derivatives of ∑M m=1 logP (vm; θ) are the same as those in [5].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "Specifically, three visual and audio local keypoint features are extracted for each video: scaleinvariant feature transform (SIFT) [18], spatial-temporal interest points (STIP) [19] and melfrequency cepstral coefficients (MFCC) [20].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 17,
      "context" : "Specifically, three visual and audio local keypoint features are extracted for each video: scaleinvariant feature transform (SIFT) [18], spatial-temporal interest points (STIP) [19] and melfrequency cepstral coefficients (MFCC) [20].",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "Specifically, three visual and audio local keypoint features are extracted for each video: scaleinvariant feature transform (SIFT) [18], spatial-temporal interest points (STIP) [19] and melfrequency cepstral coefficients (MFCC) [20].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 2,
      "context" : "3 Baseline comparisons We compare Multimodal RTM with the baselines in [4] which are the best results on the USAA dataset: Direct Direct SVM or KNN classification on raw video BoW vectors (14000 dimensions), where SVM is used for experiments with more than 10 instances and KNN otherwise.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : "Besides, we also perform a comparison with another baseline where different modal topics extracted by Replicated Softmax are connected together as video representations, and then a multi-class SVM classifier [21] is learned from the representations.",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 2,
      "context" : "When the labeled training data is plentiful (100 instances per class), the classification performance of Multimodal RTM is similar to the baselines in [4].",
      "startOffset" : 151,
      "endOffset" : 154
    } ],
    "year" : 2013,
    "abstractText" : "Unstructured social group activity recognition in web videos is a challenging task due to 1) the semantic gap between class labels and low-level visual features and 2) the lack of labeled training data. To tackle this problem, we propose a “relevance topic model” for jointly learning meaningful mid-level representations upon bagof-words (BoW) video representations and a classifier with sparse weights. In our approach, sparse Bayesian learning is incorporated into an undirected topic model (i.e., Replicated Softmax) to discover topics which are relevant to video classes and suitable for prediction. Rectified linear units are utilized to increase the expressive power of topics so as to explain better video data containing complex contents and make variational inference tractable for the proposed model. An efficient variational EM algorithm is presented for model parameter estimation and inference. Experimental results on the Unstructured Social Activity Attribute dataset show that our model achieves state of the art performance and outperforms other supervised topic model in terms of classification accuracy, particularly in the case of a very small number of labeled training videos.",
    "creator" : null
  }
}