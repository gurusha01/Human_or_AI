{
  "name" : "9766527f2b5d3e95d4a733fcfb77bd7e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Variance Reduction for Stochastic Gradient Optimization",
    "authors" : [ "Chong Wang", "Xi Chen", "Alex Smola", "Eric P. Xing" ],
    "emails" : [ "chongw@cs.cmu.edu", "xichen@cs.cmu.edu", "epxing@cs.cmu.edu", "alex@smola.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Stochastic gradient (SG) optimization [1, 2] is widely used for training machine learning models with very large-scale datasets. It uses the noisy gradient (a.k.a. stochastic gradient) estimated from random data samples rather than that from the entire data. Thus, stochastic gradient algorithms can run many more iterations in a limited time budget. However, if the noisy gradient has a large variance, the stochastic gradient algorithm might spend much time bouncing around, leading to slower convergence and worse performance. Taking a mini-batch with a larger size for computing the noisy gradient could help to reduce its variance; but if the mini-batch size is too large, it can undermine the advantage in efficiency of stochastic gradient optimization.\nIn this paper, we propose a general remedy to the “noisy gradient” problem ubiquitous to all stochastic gradient optimization algorithms for different models. Our approach builds on a variance reduction technique, which makes use of control variates [3] to augment the noisy gradient and thereby reduce its variance. The augmented “stochastic gradient” can be shown to remain an unbiased estimate of the true gradient, a necessary condition that ensures the convergence. For such control variates to be effective and sound, they must satisfy the following key requirements: 1) they have a high correlation with the noisy gradient, and 2) their expectation (with respect to random data samples) is inexpensive to compute. We show that such control variates can be constructed via low-order approximations to the noisy gradient so that their expectation only depends on low-order moments of the data. The intuition is that these low-order moments roughly characterize the empirical data distribution, and can be used to form the control variate to correct the noisy gradient to a better direction. In other words, the variance of the augmented “stochastic gradient” becomes smaller as it is derived with more information about the data.\nThe rest of the paper is organized as follows. In §2, we describe the general formulation and the theoretical property of variance reduction via control variates in stochastic gradient optimization.\nIn §3, we present two examples to show how one can construct control variates for practical algorithms. (More examples are provided in the supplementary material.) These include a convex problem—the MAP estimation for logistic regression, and a non-convex problem—stochastic variational inference for latent Dirichlet allocation [22]. Finally, we demonstrate the empirical performance of our algorithms under these two examples in §4. We conclude with a discussion on some future work."
    }, {
      "heading" : "2 Variance reduction for general stochastic gradient optimization",
      "text" : "We begin with a description of the general formulation of variance reduction via control variate for stochastic gradient optimization. Consider a general optimization problem over a finite set of training data D = {xd}Dd=1 with each xd ∈ Rp. Here D is the number of the training data. We want to maximize the following function with respect to a p-dimensional vector w,\nmaximize w\nL(w) := R(w) + (1/D) ∑D d=1 f(w;xd),\nwhereR(w) is a regularization function.1 Gradient-based algorithms can be used to maximize L(w) at the expense of computing the gradient over the entire training set. Instead, stochastic gradient (SG) methods use the noisy gradient estimated from random data samples. Suppose data index d is selected uniformly from {1, · · · , D} at step t,\ng(w;xd) = ∇wR(w) +∇wf(w;xd), (1) wt+1 = wt + ρtg(w;xd), (2)\nwhere g(w;xd) is the noisy gradient that only depends on xd and ρt is a proper step size. To make notation simple, we use gd(w) , g(w;xd).\nFollowing the standard stochastic optimization literature [1, 4], we require the expectation of the noisy gradient gd equals to the true gradient,\nEd[gd(w)] = ∇wL(w), (3) to ensure the convergence of the stochastic gradient algorithm. When the variance of gd(w) is large, the algorithm could suffer from slow convergence.\nThe basic idea of using control variates for variance reduction is to construct a new random vector that has the same expectation as the target expectation but with smaller variance. In previous work [5], control variates were used to improve the estimate of the intractable integral in variational Bayesian inference which was then used to compute the gradient of the variational lower bound. In our context, we employ a random vector hd(w) of length p to reduce the variance of the sampled gradient,\ng̃d(w) = gd(w)−AT (hd(w)− h(w)), (4)\nwhere A is a p× p matrix and h(w) , Ed[hd(w)]. (We will show how to choose hd(w) later, but it usually depends on the form of gd(w).) The random vector g̃d(w) has the same expectation as the noisy gradient gd(w) in Eq. 1, and thus can be used to replace gd(w) in the SG update in Eq. 2. To reduce the variance of the noisy gradient, the trace of the covariance matrix of g̃d(w),\nVard[g̃d(w)] , Covd[g̃d(w), g̃d(w)] = Vard[gd(w)]\n− (Covd[hd(w), gd(w)] + Covd[gd(w), hd(w)])A+ATVard[hd(w)]A, (5) must be necessarily small; therefore we set A to be the minimizer of Tr (Vard[g̃d(w)]). That is,\nA∗ = argminATr (Vard[g̃d(w)])\n= (Vard[hd(w)]) −1 (Covd[gd(w), hd(w)] + Covd[hd(w), gd(w)]) /2. (6)\nThe optimal A∗ is a function of w.\nWhy is g̃d(w) a better choice? Now we show that g̃d(w) is a better “stochastic gradient” under the `2-norm. In the first-order stochastic oracle model, we normally assume that there exists a constant σ such that for any estimate w in its domain [6, 7]:\nEd [ ‖gd(w)− Ed[gd(w)]‖22 ] = Tr(Vard[gd(w)]) ≤ σ2.\n1We follow the convention of maximizing a function f : when we mention a convex problem, we actually mean the objective function −f is convex.\nUnder this assumption, the dominating term in the optimal convergence rate is O(σ/ √ t) for convex problems and O(σ2/(µt)) for strongly convex problems, where µ is the strong convexity parameter (see the definition of strong convexity on Page 459 in [8]).\nNow suppose that we can find a random vector hd(w) and compute A∗ according to Eq. 6. By plugging A∗ back into Eq. 5,\nEd [ ‖g̃d(w)− Ed[g̃d(w)]‖22 ] = Tr(Vard[g̃d(w)]),\nwhere Vard[g̃d(w)] = Vard[gd(w)]− Covd[gd(w), hd(w)](Vard[hd(w)])−1Covd[hd(w), gd(w)].\nFor any estimate w, Covd(gd, hd) (Covd(hd, hd)) −1\nCovd(hd, gd) is a semi-positive definite matrix. Therefore, its trace, which equals to the sum of the eigenvalues, is positive (or zero when hd and gd are uncorrelated) and hence,\nEd [ ‖g̃d(w)− Ed[g̃d(w)]‖22 ] ≤ Ed [ ‖gd(w)− Ed[gd(w)]‖22 ] .\nIn other words, it is possible to find a constant τ ≤ σ such that Ed [ ‖g̃d(w)− Ed[g̃d(w)]‖22 ] ≤ τ2 for all w. Therefore, when applying stochastic gradient methods, we could improve the optimal convergence rate from O(σ/ √ t) to O(τ/ √ t) for convex problems; and from O(σ2/(µt)) to O(τ2/(µt)) for strongly convex problems.\nEstimating optimal A∗. When estimating A∗ according to Eq. 6, one needs to compute the inverse of Vard[hd(w)], which could be computationally expensive. In practice, we could constrain A to be a diagonal matrix. According to Eq. 5, when A = Diag(a11, . . . , app), its optimal value is:\na∗ii = [Covd(gd(w),hd(w))]ii [Vard(hd(w))]ii . (7)\nThis formulation avoids the computation of the matrix inverse, and leads to significant reduction of computational cost since only the diagonal elements of Covd(gd(w), hd(w)) and Vard(hd(w)), instead of the full matrices, need to be evaluated. It can be shown that, this simpler surrogate to the A∗ due to Eq. 6 still leads to a better convergence rate. Specifically:\nEd [ ‖g̃d(w)− Ed[g̃d(w)]‖22 ] = Tr(Vard(g̃d(w))) = Tr (Vard(gd(w)))− ∑p i=1 ([Covd(gd(w),hd(w))]ii) 2 [Vard(hd(w))]ii ,\n= ∑p\ni=1(1− ρ 2 ii)Var(gd(w))ii ≤ Tr (Vard(gd(w))) = Ed [ ‖gd(w)− Ed[gd(w)]‖22 ] , (8)\nwhere ρii is the Pearson’s correlation coefficient between [gd(w)]i and [hd(w)]i.\nIndeed, an even simpler surrogate to the A∗, by reducing A to a single real number a, can also improve convergence rate of SG. In this case, according to Eq. 5, the optimal a∗ is simply:\na∗ = Tr (Covd(gd(w), hd(w)))/Tr (Vard(hd(w))). (9)\nTo estimate the optimal A∗ or its surrogates, we need to evaluate Covd(gd(w), hd(w)) and Vard(hd(w)) (or their diagonal elements), which can be approximated by the sample covariance and variance from mini-batch samples while running the stochastic gradient algorithm. If we can not always obtain mini-batch samples, we may use strategies like moving average across iterations, as those used in [9, 10].\nFrom Eq. 8, we observe that when the Pearson’s correlation coefficient between gd(w) and hd(w) is higher, the control variate hd(w) will lead to a more significant level of variance reduction and hence faster convergence. In the maximal correlation case, one could set hd(w) = gd(w) to obtain zero variance. But obviously, we cannot compute Ed[hd(w)] efficiently in this case. In practice, one should construct hd(w) such that it is highly correlated with gd(w). In next section, we will show how to construct control variates for both convex and non-convex problems."
    }, {
      "heading" : "3 Practicing variance reduction on convex and non-convex problems",
      "text" : "In this section, we apply the variance reduction technique presented above to two exemplary but practical problems: MAP estimation for logistic regression—a convex problem; and stochastic variational inference for latent Dirichlet allocation [11, 22]—a non-convex problem. In the supplement,\nwe show that the same principle can be applied to more problems, such as hierarchical Dirichlet process [12, 13] and nonnegative matrix factorization [14].\nAs we discussed in §2, the higher the correlation between gd(w) and hd(w), the lower the variance is. Therefore, to apply the variance reduction technique in practice, the key is to construct a random vector hd(w) such that it has high correlations with gd(w), but its expectation h(w) = Ed[hd(w)] is inexpensive to compute. The principle behind our choice of h(w) is that we construct h(w) based on some data statistics, such as low-order moments. These low-order moments roughly characterize the data distribution which does not depend on parameter w. Thus they can be pre-computed when processing the data or estimated online while running the stochastic gradient algorithm. Figure 1 illustrates this idea. We will use this principle throughout the paper to construct control variates for variance reduction under different scenarios."
    }, {
      "heading" : "3.1 SG with variance reduction for logistic regression",
      "text" : "Logistic regression is widely used for classification [15]. Given a set of training examples (xd, yd), d = 1, ..., D, where yd = 1 or yd = −1 indicates class labels, the probability of yd is\np(yd |xd, w) = σ(ydw>xd), where σ(z) = 1/(1 + exp(−z)) is the logistic function. The averaged log likelihood of the training data is\n`(w) = 1D ∑D d=1 { ydw >xd − log ( 1 + exp(ydw >xd) )} . (10)\nAn SG algorithm employs the following noisy gradient:\ngd(w) = ydxdσ(−ydw>xd). (11)\nNow we show how to construct our control variate for logistic regression. We begin with the first-order Taylor expansion around ẑ for the sigmoid function,\nσ(z) ≈ σ(ẑ) (1 + σ(−ẑ)(z − ẑ)) . We then apply this approximation to σ(−ydw>xd) in Eq. 11 to obtain our control variate.2 For logistic regression, we consider two classes separately, since data samples within each class are more likely to be similar. We consider positive data samples first. Let z = −w>xd, and we define our control variate hd(w) for yd = 1 as\nh (1) d (w) , xdσ(ẑ) (1 + σ(−ẑ)(z − ẑ)) = xdσ(ẑ) ( 1 + σ(−ẑ)(−w>xd − ẑ) ) .\nIts expectation given yd = 1 can be computed in closed-form as Ed[h(1)d (w) | yd = 1] = σ(ẑ) ( x̄(1) (1− σ(−ẑ)ẑ)− σ(−ẑ) ( Var(1)[xd] + x̄ (1)(x̄(1))> ) w ) ,\n2Taylor expansion is not the only way to obtain control variates. Lower bounds or upper bounds of the objective function [16] can also provide alternatives. But we will not explore those solutions in this paper.\nwhere x̄(1) and Var(1)[xd] are the mean and variance of the input features for the positive examples. In our experiments, we choose ẑ = −w>x̄(1), which is the center of the positive examples. We can similarly derive the control variate h(−1)d (w) for negative examples and we omit the details. Given the random sample regardless its label, the expectation of the control variate is computed as\nEd[hd(w)] = (D(1)/D)Ed[h(1)d (w) | yd = 1] + (D(−1)/D)Ed[h (−1) d (w) | yd = −1],\nwhereD(1) andD(−1) are the number of positive and negative examples andD(1)/D is the probability of choosing a positive example from the training set. With Taylor approximation, we would expect our control variate is highly correlated with the noisy gradient. See our experiments in §4 for details."
    }, {
      "heading" : "3.2 SVI with variance reduction for latent Dirichlet allocation",
      "text" : "The stochastic variational inference (SVI) algorithm used for latent Dirichlet allocation (LDA) [22] is also a form of stochastic gradient optimization, therefore it can also benefit from variance reduction. The basic idea is to stochastically optimize the variational objective for LDA, using stochastic mean field updates augmented by control variates derived from low-order moments on the data.\nLatent Dirichlet allocation (LDA). LDA is the simplest topic model for discrete data such as text collections [17, 18]. Assume there are K topics. The generative process of LDA is as follows.\n1. Draw topics βk ∼ DirV (η) for k ∈ {1, . . . ,K}. 2. For each document d ∈ {1, . . . , D}:\n(a) Draw topic proportions θd ∼ DirK(α). (b) For each word wdn ∈ {1, . . . , N}:\ni. Draw topic assignment zdn ∼ Mult(θd). ii. Draw word wdn ∼ Mult(βzdn).\nGiven the observed words w , w1:D, we want to estimate the posterior distribution of the latent variables, including topics β , β1:K , topic proportions θ , θ1:D and topic assignments z , z1:D,\np(β, θ, z |w) ∝ ∏K k=1 p(βk | η) ∏D d=1 p(θd |α) ∏N n=1 p(zdn | θd)p(wdn |βzdn). (12)\nHowever, this posterior is intractable. We must resort to approximation methods. Mean-field variational inference is a popular approach for the approximation [19].\nMean-field variational inference for LDA. Mean-field variational inference posits a family of distributions (called variational distributions) indexed by free variational parameters and then optimizes these parameters to minimize the KL divergence between the variational distribution and the true posterior. For LDA, the variational distribution is\nq(β, θ, z) = ∏K k=1 q(βk |λk) ∏D d=1 q(θd | γd) ∏N n=1 q(zdn |φdn), (13)\nwhere the variational parameters are λk (Dirichlet), θd (Dirichlet), and φdn (multinomial). We seek the variational distribution (Eq. 13) that minimizes the KL divergence to the true posterior (Eq. 12). This is equivalent to maximizing the lower bound of the log marginal likelihood of the data,\nlog p(w) ≥ Eq [log p(β, θ, z, w)]− Eq [log q(β, θ, z)] , L(q), (14)\nwhere Eq [·] denotes the expectation with respect to the variational distribution q(β, θ, z). Setting the gradient of the lower bound L(q) with respect to the variational parameters to zero gives the following coordinate ascent algorithm [17]. For each document d ∈ {1, . . . , D}, we run local variational inference using the following updates until convergence,\nφkdv ∝ exp {Ψ(γdk) + Ψ(λk,v)−Ψ ( ∑ v λkv)} for v ∈ {1, . . . , V } (15)\nγd = α+ ∑V v=1 ndvφdv. (16)\nwhere Ψ(·) is the digamma function and ndv is the number of term v in document d. Note that here we use φdv instead of φdn in Eq. 13 since the same term v have the same φdn. After finding the variational parameters for each document, we update the variational Dirichlet for each topic,\nλkv = η + ∑D d=1 ndvφ k dv. (17)\nThe whole coordinate ascent variational algorithm iterates over Eq. 15, 16 and 17 until convergence. However, this also reveals the drawback of this algorithm—updating the topic parameter λ in Eq. 17 depends on the variational parameters φ from every document. This is especially inefficient for largescale datasets. Stochastic variational inference solves this problem using stochastic optimization.\nStochastic variational inference (SVI). Instead of using the coordinate ascent algorithm, SVI optimizes the variational lower bound L(q) using stochastic optimization [22]. It draws random samples from the corpus and use these samples to form the noisy estimate of the natural gradient [20]. Then the algorithm follows that noisy natural gradient with a decreasing step size until convergence. The noisy gradient only depends on the sampled data and it is inexpensive to compute. This leads to a much faster algorithm than the traditional coordinate ascent variational inference algorithm.\nLet d be a random document index, d ∼ Unif(1, ..., D) and Ld(q) be the sampled lower bound. The sampled lower bound Ld(q) has the same form as the L(q) in Eq. 14 except that the sampled lower bound uses a virtual corpus that only contains document d replicated D times. According to [22], for LDA the noisy natural gradient with respect to the topic variational parameters is\ngd(λkv) , −λkv + η +Dndvφkdv, (18) where the φkdv are obtained from the local variational inference by iterating over Eq. 15 and 16 until convergence.3 With a step size ρt, SVI uses the following update λkv ← λkv + ρtgd(λkv). However, the sampled natural gradient gd(λkv) in Eq. 18 might have a large variance when the number of documents is large. This could lead to slow convergence or a poor local mode.\nControl variate. Now we show how to construct control variates for the noisy gradient to reduce its variance. According to Eq. 18, the noisy gradient gd(λkv) is a function of topic assignment parameters φdv , which in turn depends on wd, the words in document d, through the iterative updates in Eq. 15 and 16. This is different from the case in Eq. 11. In logistic regression, the gradient is an analytical function of the training data (Eq. 11), while in LDA, the natural gradient directly depends on the optimal local variational parameters (Eq. 18), which then depends on the training data through the local variational inference (Eq. 15). However, by carefully exploring the structure of the iterations, we can create effective control variates.\nThe key idea is to run Eq. 15 and 16 only up to a fixed number of iterations, together with some additional approximations to maintain analytical tractability. Starting the iteration with γdk having the same value, we have φk(0)v ∝ exp {Ψ(λkv)−Ψ ( ∑ v λkv)}.4 Note that φ k(0) v does not depend on document d. Intuitively, φk(0)v is the probability of term v belonging to topic k out of K topics.\nNext we use γdk − α to approximate exp(Ψ(γdk)) in Eq. 15.5 Plugging this approximation into Eq. 15 and 16 leads to the update,\nφ k(1) dv =\n( ∑V u=1 fduφ k(0) u )φ\nk(0) v∑K\nk=1 (∑V u=1 fduφ k(0) u ) φ k(0) v\n≈ ( ∑V u=1 fduφ k(0) u )φ\nk(0) v∑K\nk=1 (∑V u=1 f̄uφ k(0) u ) φ k(0) v , (19)\nwhere fdv = ndv/nd is the empirical frequency of term v in document d. In addition, we replace fdu with f̄u , (1/D) ∑ d fdu, the averaged frequency of term u in the corpus, making the denominator\nof Eq. 19, m(1)v , ∑K k=1 (∑V u=1 f̄uφ k(0) u ) φ k(0) v , independent of documents. This approximation does not change the relative importance for the topics from term v. We define our control variate as\nhd(λkv) , Dndvφ k(1) dv , whose expectation is Ed[hd(λkv)] = ( D/m (1) v ){(∑V u=1 nvfuφ k(0) u ) φ k(0) v } , where nvfu ,\n(1/D) ∑ d ndufdv = (1/D) ∑ d ndundv/nd. This depends on up to the second-order moments of data, which is usually sparse. We can continue to compute φk(2)dv (or higher) given φ k(1) dv , which turns out using the third-order (or higher) moments. We omit the details here. Similar ideas can be used in deriving control variates for hierarchical Dirichlet process [12, 13] and nonnegative matrix factorization [14]. We outline these in the supplementary material.\n3Running to convergence is essential to ensure the natural gradient is valid in Eq. 18 [22]. 4In our experiments, we set φk(0)v = 0 if φ k(0) v is less than 0.02. This leaves φ(0) very sparse, since a term usually belongs to a small set of topics. For example, in Nature data, only 6% entries are non-zero. 5The scale of the approximation does not matter—C(γdk − α), where C is a constant, has the same effect as γdk − α. Other approximations to exp(Ψ(γdk)) can also be used as long as it is linear in term of γdk.\n(a) Optimum minus Objective on training data (b) Test Accuracy on testing data"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we conducted experiments on the MAP estimation for logistic regression and stochastic variational inference for LDA.6 In our experiments, we chose to estimate the optimal a∗ as a scalar shown in Eq. 9 for simplicity."
    }, {
      "heading" : "4.1 Logistic regression",
      "text" : "We evaluate our algorithm on stochastic gradient (SG) for logistic regression. For the standard SG algorithm, we also evaluated the version with averaged output (ASG), although we did not find it outperforms the standard SG algorithm much. Our regularization added to Eq. 10 for the MAP estimation is − 12Dw\n>w. Our dataset contains covtype (D = 581, 012, p = 54), obtained from the LIBSVM data website.7 We separate 5K examples as the test set. We test two types of learning rates, constant and decayed. For constant rates, we explore ρt ∈ {0.01, 0.05, 0.1, 0.2, 0.5, 1}. For decayed rates, we explore ρt ∈ {t−1/2, t−0.75, t−1}. We use a mini-batch size of 100. Results. We found that the decayed learning rates we tested did not work well compared with the constant ones on this data. So we focus on the results using the constant rates. We plot three cases in Figure 2 for ρt ∈ {0.05, 0.2, 1} to show the trend by comparing the objective function on the training data and the test accuracy on the testing data. (The best result for variance reduction is obtained when ρt = 1.0 and for standard SGD is when ρt = 0.2.) These contain the best results of\n6Code will be available on authors’ websites. 7http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets\neach. With variance reduction, a large learning rate is possible to allow faster convergence without sacrificing performance. Figure 3 shows the mean of Pearson’s correlation coefficient between the control variate and noisy gradient8, which is quite high—the control variate is highly correlated with the noisy gradient, leading to a large variance reduction."
    }, {
      "heading" : "4.2 Stochastic variational inference for LDA",
      "text" : "We evaluate our algorithm on stochastic variational inference for LDA. [10] has shown that the adaptive learning rate algorithm for SVI performed better than the manually tuned ones. So we use their algorithm to estimate adaptive learning rate. For LDA, we set the number of topics K = 100, hyperparameters α = 0.1 and η = 0.01. We tested mini-batch sizes as 100 and 500.\nData sets. We analyzed three large corpora: Nature, New York Times, and Wikipedia. The Nature corpus contains 340K documents and a vocabulary of 4,500 terms; the New York Times corpus contains 1.8M documents and a vocabulary vocabulary of 8,000 terms; the Wikipedia corpus contains 3.6M documents and a vocabulary of 7,700 terms.\nEvaluation metric and results. To evaluate our models, we held out 10K documents from each corpus and calculated its predictive likelihood. We follow the metric used in recent topic modeling literature [21, 22]. For a document wd in Dtest, we split it in into halves, wd = (wd1, wd2), and computed the predictive log likelihood of the words in wd2 conditioned on wd1 and Dtrain. The per-word predictive log likelihood is defined as\nlikelihoodpw , ∑ d∈Dtest log p(wd2|wd1,Dtrain)/ ∑ d∈Dtest |wd2|.\nHere | · | is the number of words. A better predictive distribution given the first half gives higher likelihood to the second half. We used the same strategy as in [22] to approximate its computation. Figure 4 shows the results. On all three corpora, our algorithm gives better predictive distributions."
    }, {
      "heading" : "5 Discussions and future work",
      "text" : "In this paper, we show that variance reduction with control variates can be used to improve stochastic gradient optimization. We further demonstrate its usage on convex and non-convex problems, showing improved performance on both. In future work, we would like to explore how to use second-order methods (such as Newton’s method) or better line search algorithms to further improve the performance of stochastic optimization. This is because, for example, with variance reduction, second-order methods are able to capture the local curvature much better.\nAcknowledgement. We thank anonymous reviewers for their helpful comments. We also thank Dani Yogatama for helping with some experiments on LDA. Chong Wang and Eric P. Xing are supported by NSF DBI-0546594 and NIH 1R01GM093156.\n8Since the control variate and noisy gradient are vectors, we use the mean of the Pearson’s coefficients computed for each dimension between these two vectors."
    } ],
    "references" : [ {
      "title" : "Introduction to stochastic search and optimization: Estimation, simulation, and control",
      "author" : [ "J. Spall" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Variational Bayesian inference with stochastic search",
      "author" : [ "J. Paisley", "D. Blei", "M. Jordan" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "An optimal method for stochastic composite optimization",
      "author" : [ "G. Lan" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Optimal regularized dual averaging methods for stochastic optimization",
      "author" : [ "X. Chen", "Q. Lin", "J. Pena" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "An adaptive learning rate for stochastic variational inference",
      "author" : [ "R. Ranganath", "C. Wang", "D.M. Blei" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Online inference for latent Drichlet allocation",
      "author" : [ "M. Hoffman", "D. Blei", "F. Bach" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Hierarchical Dirichlet processes",
      "author" : [ "Y. Teh", "M. Jordan", "M. Beal" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Online variational inference for the hierarchical Dirichlet process",
      "author" : [ "C. Wang", "J. Paisley", "D. Blei" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Algorithms for non-negative matrix factorization",
      "author" : [ "D. Seung", "L. Lee" ],
      "venue" : "In Neural Information Processing Systems",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2001
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "C. Bishop" ],
      "venue" : "Springer New York.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "A variational approach to Bayesian logistic regression models and their extensions",
      "author" : [ "T. Jaakkola", "M. Jordan" ],
      "venue" : "In International Workshop on Artificial Intelligence and Statistics",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1996
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Introduction to variational methods for graphical models",
      "author" : [ "M. Jordan", "Z. Ghahramani", "T. Jaakkola" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1999
    }, {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "S. Amari" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "On smoothing and inference for topic models",
      "author" : [ "A. Asuncion", "M. Welling", "P. Smyth" ],
      "venue" : "In Uncertainty in Artificial Intelligence",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Stochastic Variational Inference",
      "author" : [ "M. Hoffman", "D. Blei", "C. Wang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Stochastic gradient (SG) optimization [1, 2] is widely used for training machine learning models with very large-scale datasets.",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 17,
      "context" : ") These include a convex problem—the MAP estimation for logistic regression, and a non-convex problem—stochastic variational inference for latent Dirichlet allocation [22].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 0,
      "context" : "Following the standard stochastic optimization literature [1, 4], we require the expectation of the noisy gradient gd equals to the true gradient, Ed[gd(w)] = ∇wL(w), (3) to ensure the convergence of the stochastic gradient algorithm.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Following the standard stochastic optimization literature [1, 4], we require the expectation of the noisy gradient gd equals to the true gradient, Ed[gd(w)] = ∇wL(w), (3) to ensure the convergence of the stochastic gradient algorithm.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "In previous work [5], control variates were used to improve the estimate of the intractable integral in variational Bayesian inference which was then used to compute the gradient of the variational lower bound.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "In the first-order stochastic oracle model, we normally assume that there exists a constant σ such that for any estimate w in its domain [6, 7]:",
      "startOffset" : 137,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "In the first-order stochastic oracle model, we normally assume that there exists a constant σ such that for any estimate w in its domain [6, 7]:",
      "startOffset" : 137,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "Under this assumption, the dominating term in the optimal convergence rate is O(σ/ √ t) for convex problems and O(σ(2)/(μt)) for strongly convex problems, where μ is the strong convexity parameter (see the definition of strong convexity on Page 459 in [8]).",
      "startOffset" : 252,
      "endOffset" : 255
    }, {
      "referenceID" : 6,
      "context" : "If we can not always obtain mini-batch samples, we may use strategies like moving average across iterations, as those used in [9, 10].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "In this section, we apply the variance reduction technique presented above to two exemplary but practical problems: MAP estimation for logistic regression—a convex problem; and stochastic variational inference for latent Dirichlet allocation [11, 22]—a non-convex problem.",
      "startOffset" : 242,
      "endOffset" : 250
    }, {
      "referenceID" : 17,
      "context" : "In this section, we apply the variance reduction technique presented above to two exemplary but practical problems: MAP estimation for logistic regression—a convex problem; and stochastic variational inference for latent Dirichlet allocation [11, 22]—a non-convex problem.",
      "startOffset" : 242,
      "endOffset" : 250
    }, {
      "referenceID" : 8,
      "context" : "we show that the same principle can be applied to more problems, such as hierarchical Dirichlet process [12, 13] and nonnegative matrix factorization [14].",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "we show that the same principle can be applied to more problems, such as hierarchical Dirichlet process [12, 13] and nonnegative matrix factorization [14].",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "we show that the same principle can be applied to more problems, such as hierarchical Dirichlet process [12, 13] and nonnegative matrix factorization [14].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 11,
      "context" : "Logistic regression is widely used for classification [15].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "Lower bounds or upper bounds of the objective function [16] can also provide alternatives.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "The stochastic variational inference (SVI) algorithm used for latent Dirichlet allocation (LDA) [22] is also a form of stochastic gradient optimization, therefore it can also benefit from variance reduction.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "LDA is the simplest topic model for discrete data such as text collections [17, 18].",
      "startOffset" : 75,
      "endOffset" : 83
    }, {
      "referenceID" : 14,
      "context" : "Mean-field variational inference is a popular approach for the approximation [19].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "Setting the gradient of the lower bound L(q) with respect to the variational parameters to zero gives the following coordinate ascent algorithm [17].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 17,
      "context" : "Instead of using the coordinate ascent algorithm, SVI optimizes the variational lower bound L(q) using stochastic optimization [22].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 15,
      "context" : "It draws random samples from the corpus and use these samples to form the noisy estimate of the natural gradient [20].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "According to [22], for LDA the noisy natural gradient with respect to the topic variational parameters is gd(λkv) , −λkv + η +Dndvφdv, (18) where the φdv are obtained from the local variational inference by iterating over Eq.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "Similar ideas can be used in deriving control variates for hierarchical Dirichlet process [12, 13] and nonnegative matrix factorization [14].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "Similar ideas can be used in deriving control variates for hierarchical Dirichlet process [12, 13] and nonnegative matrix factorization [14].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "Similar ideas can be used in deriving control variates for hierarchical Dirichlet process [12, 13] and nonnegative matrix factorization [14].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 6,
      "context" : ") Legend “Standard-100” indicates the stochastic algorithm in [10] with the batch size as 100.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "[10] has shown that the adaptive learning rate algorithm for SVI performed better than the manually tuned ones.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "We follow the metric used in recent topic modeling literature [21, 22].",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "We follow the metric used in recent topic modeling literature [21, 22].",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "We used the same strategy as in [22] to approximate its computation.",
      "startOffset" : 32,
      "endOffset" : 36
    } ],
    "year" : 2013,
    "abstractText" : "Stochastic gradient optimization is a class of widely used algorithms for training machine learning models. To optimize an objective, it uses the noisy gradient computed from the random data samples instead of the true gradient computed from the entire dataset. However, when the variance of the noisy gradient is large, the algorithm might spend much time bouncing around, leading to slower convergence and worse performance. In this paper, we develop a general approach of using control variate for variance reduction in stochastic gradient. Data statistics such as low-order moments (pre-computed or estimated online) is used to form the control variate. We demonstrate how to construct the control variate for two practical problems using stochastic gradient optimization. One is convex—the MAP estimation for logistic regression, and the other is non-convex—stochastic variational inference for latent Dirichlet allocation. On both problems, our approach shows faster convergence and better performance than the classical approach.",
    "creator" : null
  }
}