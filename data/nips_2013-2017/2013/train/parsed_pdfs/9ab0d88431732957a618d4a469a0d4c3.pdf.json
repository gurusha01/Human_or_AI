{
  "name" : "9ab0d88431732957a618d4a469a0d4c3.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Multiple Models via Regularized Weighting",
    "authors" : [ "Daniel Vainsencher", "Shie Mannor" ],
    "emails" : [ "danielv@tx.technion.ac.il", "shie@ee.technion.ac.il", "mpexuh@nus.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufficiently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efficiently. We demonstrate the robustness benefits of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers."
    }, {
      "heading" : "1 Introduction",
      "text" : "The standard approach to learning models from data assumes that the data were generated by a certain model, and the goal of learning is to recover this generative model. For example, in linear regression, an unknown linear functional, which we want to recover, is believed to have generated covariate-response pairs. Similarly, in principal component analysis, a random variable in some unknown low-dimensional subspace generated the observed data, and the goal is to recover this low-dimensional subspace. Yet, in practice, it is common to encounter data that were generated by a mixture of several models rather than a single one, and the goal is to learn a number of models such that any given data can be explained by at least one of the learned models. It is also common for the data to contain outliers: data-points that are not well explained by any of the models to be learned, possibly inserted by external processes.\nWe briefly explain our approach (presented in detail in the next section). At its center is the problem of assigning data points to models, with the main consideration that every model be consistent with many of the data points. Thus we seek for each model a distribution of weights over the data points, and encourage even weights by regularizing these distributions (hence our approach is called Regularized Weighting; abbreviated as RW). A data point that is inconsistent with all available models will receive lower weight and even sometimes be ignored. The value of ignoring difficult points is illustrated by contrast with the common approach, which we consider next.\nThe arguably most widely applied approach for multiple model learning is the minimum loss approach, also known as Lloyd’s algorithm [1] in clustering, where the goal is to find a set of models, associate each data point to one model (in so called “soft” variations, one or more models), such that the sum of losses over data points is minimal. Notice that in this approach, every data point must be explained by some model. This leaves the minimum loss approach vulnerable to outliers and corruptions: If one data point goes to infinity, so must at least one model.\nOur remedy to this is relaxing the requirement that each data point must be explained. Indeed, as we show later, the RW formulation is provably robust in the case of clustering, in the sense of having non-zero breakdown point [2]. Moreover, we also establish other desirable properties, both computational and statistical, of the proposed method. Our main contributions are:\n1. A new formulation of the sub-task of associating data points to models as a convex optimization problem for setting weights. This problem favors broadly based models, and may ignore difficult data points entirely. We formalize such properties of optimal solutions through analysis of a strongly dual problem. The remaining results are characteristics of this approach.\n2. Outlier robustness. We show that the breakdown point of the proposed method is bounded away from zero for the clustering case. The breakdown point is a concept from robust statistics: it is the fraction of adversarial outliers that an algorithm can sustain without having its output arbitrarily changed.\n3. Robustness to fat tailed noise. We show, empirically on a synthetic and real world datasets, that our formulation is more resistant to fat tailed additive noise.\n4. Generalization. Ignoring some of the data, in general, may lead to overfitting. We show that when the parameter α (defined in Section 2) is appropriately set, this essentially does not occur. We prove this through uniform convergence bounds resilient to the lack of efficient algorithms to find near-optimal solutions in multiple model learning.\n5. Computational complexity. As almost every method to tackle the multiple model learning problem, we use alternating optimization of the models and the association (weights), i.e., we iteratively optimize one of them while fixing the other. Our formulation for optimizing the association requires solving a quadratic problem in kn variables, where k is the number of models and n is the number of points. Compared to O(kn) steps for some formulations, this seems expensive. We show how to take advantage of the special problem structure and repetition in the alternating optimization subproblems to reduce this cost."
    }, {
      "heading" : "1.1 Relation to previous work",
      "text" : "Learning multiple models is by no means a new problem. Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details. Fewer studies attempt to cross problem type boundaries. A general treatment of the sample complexity of problems that can be interpreted as learning a code book (which encompasses some types of multiple model learning) is [11]. Slightly closer to our approach is [12], whose formulation generalizes a common approach to different model types and permits for problem specific regularization, giving both generalization results and algorithmic iteration complexity results. A probabilistic and generic algorithmic approach to learning multiple models is Expectation Maximization [13].\nAlgorithms for dealing with outliers and multiple models together have been proposed in the context of clustering [14]. Reference [15] provides an example of an algorithm for outlier resistance in learning a single subspace, and partly inspires the current work. In contrast, we abstract almost completely over the class of models, allowing both algorithms and analysis to be easily reused to address new classes."
    }, {
      "heading" : "2 Formulation",
      "text" : "In this section we show how multi-model learning problems can be formed from simple estimation problem (where we seek to explain weighted data points by a single model), and imposing a par-\nticular joint loss. We contrast the joint loss proposed here to a common one through the weights assigned by each and their effects on robustness.\nWe refer throughout to n data points from X by (xi) n i=1 = X ∈ X n, which we seek to explain by k models from M denoted (mj) k j=1 = M ∈ M k. A data set may be weighted by a set of k distributions (wj) k j=1 = W ∈ (△ n) k where△n ⊂ Rn is the simplex.\nDefinition 1. A base weighted learning problem is a tuple (X ,M, ℓ,A), where ℓ : X ×M→ R+ is a non-negative convex function, which we call a base loss function and A : △n × Xn → M defines an efficient algorithm for choosing a model. Given the weight w and data X , A obtains low weighted empirical loss ∑n\ni=1 wiℓ (xi,m) (the weighted empirical loss need not be minimal, allowing for regularization which we do not discuss further).\nWe will often denote the losses of a model m over X as a vector l = (ℓ(xi,m)) n\ni=1. In the context of a set of models M , we similarly associate the loss vector lj and the weight vector wj with the model mj ; this allows us to use the terse notation w ⊤ j lj for the weighted loss of model j.\nGiven a base weighted learning problem, one may pose a multi-model learning problem\nExample 1. The multi-model learning problem covers many examples, here we list a few:\n• In k-means clustering, the goal is to partition the training samples into k subsets, where each subset of samples is “close” to their mean. In our terminology, a multi-model learning\nproblem where the base learning problem is ( R d,Rd, (x,m) 7→ ‖x−m‖\n2 2 ,A\n)\nwhere A\nfinds the weighted mean of the data. The weights allow us to compute each cluster center according to the relevant subset of points.\n• In subspace clustering, also known as subspace segmentation, the objective is to group the training samples into subsets, such that each subset can be well approximated by a low-dimensional affine subspace. This is a multi-model learning problem where the corresponding single-model learning problem is PCA.\n• Regression clustering [16] extends the standard linear regression problem in that the training samples cannot be explained by one linear function. Instead, multiple linear function are sought, so that the training samples can be split into groups, and each group can be approximated by one linear function.\n• Gaussian Mixture Model considers the case where data points are generated by a mixture of a finite number of Gaussian distributions, and seeks to estimate the mean and variance of each of these distribution, and simultaneously to group the data points according to the distribution that generates it. This is a multi-model learning problem where the respective single model learning problem is estimating the mean and variance of a distribution.\nThe most common way to tackle the multiple model learning problem is the minimum loss approach, i.e, to minimize the following joint loss\nL (X,M) = 1\nn\n∑\nx∈X\nmin m∈M ℓ (x,m) . (2.1)\nIn terms of weighted base learning problems, each model gives equal weight to all points for which it is the best (lowest loss) model. For example, when M = X = Rn with ℓ(x,m) = ‖x−m‖ 2 2 the squared Euclidean distance loss yields k means clustering. In this context, alternating between choosing for each x its loss minimizing model, and adjusting each model to minimized the squared Euclidean loss, yields Lloyd’s algorithm (and its generalizations for other problems).\nThe minimum loss approach requires that every point is assigned to a model, this can potentially cause problems in the presence of outliers. For example, consider the clustering case where the data contain a single outlier point xi. Let xi tend to infinity; there will always be some mj that is closest to xi, and is therefore (at equilibrium) the average of xi and some other data points. Then mj will tend to infinity also. We call this phenomenon mode I of sensitivity to outliers; it is common also\nto such simple estimators as the mean. Mode II of sensitivity is more particular: as mj follows xi to infinity, it stops being the closest to any other points, until the model is associated only to the outlier and thus matches it perfectly. Thus under Eq. (2.1) outliers tend to take over models. Mode II of sensitivity is not clustering specific, and Fig. 2.1 provides an example in multiple regression. Neither mode is avoided by spreading a point’s weight among models as in mixture models [6].\nTo overcome both modes of sensitivity, we propose a different joint loss, in which the hard constraint is only that for each model we produce a distribution over data points. A penalty term discourages the concentration of a model on few points and thus mode II sensitivity. Deweighting difficult points helps mitigate mode I. For clustering this robustness is formalized in Theorem 2.\nDefinition 2. Let u ∈ △n be the uniform distribution. Given k weight vectors, we denote their average v (W ) = k−1 ∑k\nj=1 wj , and just v when W is clear from context. The Regularized Weighting\nmultiple model learning loss is a function Lα : X n ×Mk × (△n) k → R defined as\nLα (X,M,W ) = α ‖u− v (W )‖ 2 2 + k\n−1 k ∑\nj=1\nl ⊤ j wj (2.2)\nwhich in particular defines the weight setting subproblem:\nLα (X,M) = min W∈(△n)k Lα (X,M,W ) . (2.3)\nAs its name suggests, our formulation regularizes distributions of weight over data points; specifically, wj are controlled by forcing their average v to be close to the uniform distribution u. Our goal is for each model to represent many data points, so weights should not be concentrated. We avoid this by penalizing squared Euclidean distance from uniformity, which emphasizes points receiving weight much higher than the natural n−1, and essentially ignores small variations around n−1. The effect is later formalized in Lemma 1, but to illustrate we next calculate the penalties for two stylized cases. This will also produce the first of several hints about the appropriate range of values for α.\nIn the following examples, we will consider a set of γnk−1 data points, recalling that nk−1 is the natural number of points per model. To avoid letting a few high loss outliers skew our models (mode I of sensitivity), we prefer instead to give them zero weight. Take γ ≪ k/2, then the cost of ignoring some γnk−1 points in all models is at most αn−1 · 2γk−1 ≪ αn−1. In contrast, basing a model\non very few points (mode II of sensitivity) should be avoided. If the jth model is fit to only γnk−1 points for γ ≪ 1, the penalty from those points will be at least (approximately) αn−1 · γ−1k−1. We can make the first situation cheap and the second expensive (per model) in comparison to the empirical weighted loss term by choosing\nαn−1 ≈ k−1 k ∑\nj=1\nw ⊤ j lj . (2.4)\nOn the flip side, highly unbalanced classes in the data can be challenging to our approach. Consider the case where a model has low loss for fewer than n/(2k) points: spreading its weight only over them can incur very high costs due to the regularization term, which might be lowered by including some higher-loss points that are indeed better explained by another model (see Figure 2.2 on page 5 for an illustration). This challenge might be solved by explicitly and separately estimating the relative frequencies of the classes, and penalizing deviations from the estimates rather than from equal frequencies, as is done in mixture models [6]; this is left for future study."
    }, {
      "heading" : "2.1 Two properties of Regularized Weighting",
      "text" : "Two properties of our formulation result from an analysis (in Appendix A for lack of space) of a dual problem of the weight setting problem (2.3). These provide the basis for later theory by relating v, losses and α. The first illustrates the uniform control of v:\nLemma 1. Let all losses be in [0, B], then in an optimal solution to (2.3), we have\n‖v − u‖ ∞ ≤ B/ (2α) .\nThis strengthens the conclusion of (2.4): if outliers are present and αn−1 > 2B where B bounds losses on all points including outliers, weights will be almost uniform (enabling mode I of sensi-\ntivity). On the positive side, this lemma plays an important role in the generalization and iteration complexity results presented in the sequel. A more detailed view of vi for individual points is provided by the second property.\nBy PC we denote the orthogonal projection mapping into a convex set C. Lemma 2. For an optimal solution to (2.3), there exists t ∈ Rk such that:\nv = P△n\n(\nu−min j (lj − tj) / (2α)\n)\n,\nwhere minj should be read as operating element-wise, and in particular wj,i > 0 implies that j minimizes the ith element.\nThis establishes that average weight (when positive) is affine in the loss; the concave parabolas visible in Figure 2.2 on page 5 are an example. We also learn the role of α in solutions is determining the coefficient in the affine relation. Distinct t allow for different densities of points around different models. One observation from this lemma is that if a particular model j gives weight to some point i, then every point with lower loss ℓ (xi′ ,mj) under that model will receive at least that much weight. This property plays a key role in the proof of robustness to outliers in clustering."
    }, {
      "heading" : "2.2 An alternating optimization algorithm",
      "text" : "The RW multiple model learning loss, like other MML losses, is not convex. However the weight setting problem (2.3) is convex when we fix the models, and an efficient procedure A is assumed for solving a weighted base learning problem for a model, supporting an alternating optimization approach, as in Algorithm 1; see Section 5 for further discussion.\nData: X Result: The model-set M M ← initialModels (X); repeat\nM ′ ←M ; W ← argminW ′ Lα (X,M,W\n′); mj ← A (wj , X) (∀j ∈ [k]) ;\nuntil L (X,M ′)− L (X,M) < ε;\nAlgorithm 1: Alternating optimization for Regularized Weighting"
    }, {
      "heading" : "3 Breakdown point in clustering",
      "text" : "Our formulation allows a few difficult outliers to be ignored if the right models are found; does this happen in practice? Figure 2.1 on page 4 provides a positive example in regression clustering, and a more substantial empirical evaluation on subspace clustering is in Appendix B. In the particular case of clustering with the squared Euclidean loss, robustness benefits can be proved.\nWe use “breakdown point” – the standard robustness measure in the literature of robust statistics [2], see also [17, 18] and many others – to quantify the robustness property of the proposed formulation. The breakdown point of an estimator is the smallest fraction of bad observations that can cause the estimator to take arbitrarily aberrant values, i.e., the smallest fraction of outliers needed to completely break an estimator.\nFor the case of clustering with the squared Euclidean distance base loss, the min-loss approach corresponds to k-means clustering which is not robust in this sense; its breakdown point is 0. The non robustness of k-means has led to the development of many formulations of robust clustering, see a review by [14]. In contrast, we show that our joint loss yields an estimator that has a non-zero breakdown point, and is hence robust.\nIn general, a squared loss clustering formulation that assigns equal weight to different data points cannot be robust – as one data point tends to infinity so must at least one model. This applies to our model if α is allowed to tend to infinity. On the other hand if α is too low, it becomes possible\nfor each model to assign all of its weight to a single point, which may well be an outlier tending to infinity. Thus, it is well expected that the robustness result below requires α to belong to a data dependent range.\nTheorem 2. Let X = M be a Euclidean space in which we perform clustering with the loss ℓ (xi,mj) = ‖mj − xi‖ 2\nand k centers. Denote by R the radius of any ball containing the inliers, and η < k−2/22 the proportion of outliers allowed to be outside the ball. Denote also by r a radius such that there exists M ′ = {m′1, · · · ,m ′ k} such that each inlier is within a distance r of some model m′j and each mj approximates (i.e., within a distance r) at least n/(2k) inliers; this always holds for some r ≤ R.\nFor any α ∈ n [ r2, 13R2 ]\nlet (M,W ) be minimizers of Lα (X,M,W ). Then we have ‖mj − xi‖2 ≤ 6R for every model mj and inlier xi.\nTheorem 2 shows that when the number of outliers is not too high, then the learned model, regardless of the magnitude of the outliers, is close to the inliers and hence cannot be arbitrarily bad. In particular, the theorem implies a non-zero breakdown point for any α > nr2; taking too high an α merely forces a larger but still finite R. If the inliers are amenable to balanced clustering so that r ≪ R, the regime of non-zero breakdown is extended to smaller α.\nThe proof follows three steps. First, due to the regularization term, for any model, the total weight on the few outliers is at most 1/3. Second, an optimal model must thus be at least twice as close to the weighted average of its inlier as it is to the weighted average of its outliers. This step depends critically on squared Euclidean loss being used. Lastly, this gap in distances cannot be large in absolute terms, due to Lemma 2; an outlier that is much farther from the model than the inliers must receive weight zero. For the proof see Appendix C of the supplementary material."
    }, {
      "heading" : "4 Regularized Weighting formulation sample complexity",
      "text" : "An important consideration in learning algorithms is controlling overfitting, in which a model is found that is appropriate for some data, rather than for the source that generates the data. The current formulation seems to be particularly vulnerable since it allows data to be ignored, in contrast to most generalization bounds that assume equal weight is given to all data.\nOur loss Lα(X,M) differs from common losses in allowing data points to be differently weighted. Thus, to obtain the sample complexity of our formulation we need to bound the difference that a single sample can make to the loss. For a common empirical average loss this is bounded by Bn−1 where B is the maximal value of the non-negative loss on a single data point, and in our case by B ‖v‖\n∞ , because if X,X ′ differ only on the ith element, then:\n|Lα (X ′,M,W )− Lα (X,M,W )| =\n∣ ∣ ∣ ∣ ∣ ∣ k−1 k ∑\nj=1\n(\nwj,i\n(\nlj,i − l ′ j,i\n))\n∣ ∣ ∣ ∣ ∣ ∣ ≤ Bk−1 k ∑\nj=1\nwj,i ≤ Bvi.\nWhenever W is optimal with respect to either X or X ′, Lemma 1 provides the necessary bound on ‖v‖\n∞ . Along with covering numbers as defined next and standard arguments (found in the\nsupplementary material), this bound on differences provides us with the desired generalization result.\nDefinition 3 (Covering numbers for multiple models). We shall endowMk with the metric\nd∞ (M,M ′) = max\nj∈[k]\n∥ ∥ℓ (·,mj)− ℓ ( ·,m′j )∥ ∥\n∞\nand define its covering number Nε ( Mk ) as the minimal cardinality of a setMkε such thatM k ⊆ ⋃\nM∈Mk ε\nB(M, ε).\nThe bound depends on an upper bound on base losses denoted B; this should be viewed as fixing a scale for the losses and is standard where losses are not naturally bounded (e.g., classical bounds on SVM kernel regression [19] use bounded kernels). Thus, we have the following generalization result, whose proof can be found in Appendix D of the supplementary material.\nTheorem 3. Let the base losses be bounded in the interval [0, B], let Mk have covering numbers Nε ( Mk ) ≤ (C/ε) dk and let γ = nB/ (2α). Then we have with probability at least 1− exp { dk log (\n2C τ\n) − 2nτ 2\nB2(1+γ)2\n}\n:\n∀M ∈Mk |Lα (X,M)− EX′∼DnLα (X ′,M)| ≤ 3τ."
    }, {
      "heading" : "5 The weight assignment optimization step",
      "text" : "As is typical in multi-model learning, simultaneously optimizing the model and the association of the data (in our formulation, the weight) is computationally hard [20], thus Algorithm 1 alternates between optimizing the weight with the model fixed, and optimizing the model with the weights fixed. Thus we show how to efficiently solve a sequence of weight setting problems, minimizing Lα(X,Mi,W ) over W , where Mi typically converge.\nWe propose to solve each instance of weight setting using gradient methods, and in particular FISTA [21]. This has two advantages compared to Interior Point methods: First, the use of memory for gradient methods depends only linearly with respect to the dimension, which is O(kn) in problem (2.3), allowing scaling to large data sets. Second, gradient methods have “warm start” properties: the number of iterations required is proportional to the distance between the initial and optimal solutions, which is useful both due to bounds on ‖v − u‖\n∞ and when Mi converge.\nTheorem 4. Given data and models (X,M) there exists an algorithm that finds a weight matrix W such that Lα(X,M,W )− Lα(X,M) ≤ ε using O ( √ kα/ε ) iterations, each costing O(kn) time and memory. If α ≥ Bn/4 then O ( k √ αn−1/ε ) iterations suffice.\nThe first bound might suggest that typical settings of α ∝ n requires iterations to increase with the number of points n; the second bounds shows this is not always necessary.\nThis result can be realized by applying the algorithm FISTA, with a starting point wj = u, with 2αk−2 as a bound on the Lipschitz constant for the gradient. For the first bound we estimate the distance from u by the radius of the product of k simplices; for the second we use Lemma 1 in Appendix E."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we proposed and analyzed, from a general perspective, a new formulation for learning multiple models that explain well much of the data. This is based on associating to each model a regularized weight distribution over the data it explains well. A main advantage of the new formulation is its robustness to fat tailed noise and outliers: we demonstrated this empirically for regression clustering and subspace clustering tasks, and proved that for the important case of clustering, the proposed method has a non-trivial breakdown point, which is in sharp contrast to standard methods such as k-means. We further provided generalization bounds and explained an optimization procedure to solve the formulation in scale.\nOur main motivation comes from the fast growing attention to analyzing data using multiple models, under the names of k-means clustering, subspace segmentation, and Gaussian mixture models, to list a few. While all these learning schemes share common properties, they are largely studied separately, partly because these problems come from different sub-fields of machine learning. We believe general methods with desirable properties such as generalization and robustness will supply ready tools for new applications using other model types."
    }, {
      "heading" : "Acknowledgments",
      "text" : "H. Xu is partially supported by the Ministry of Education of Singapore through AcRF Tier Two grant R-265-000-443-112 and NUS startup grant R-265-000-384-133. This research was funded (in part) by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."
    } ],
    "references" : [ {
      "title" : "Least squares quantization in PCM",
      "author" : [ "S. Lloyd" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1982
    }, {
      "title" : "Robust Statistics",
      "author" : [ "P.J. Huber" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1981
    }, {
      "title" : "Algorithm AS 136: A k-means clustering algorithm",
      "author" : [ "J.A. Hartigan", "M.A. Wong" ],
      "venue" : "Journal of the Royal Statistical Society. Series C (Applied Statistics),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1979
    }, {
      "title" : "The effectiveness of Lloyd-type methods for the k-means problem",
      "author" : [ "R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Analysis of global k-means, an incremental heuristic for minimum sum-of-squares clustering",
      "author" : [ "P. Hansen", "E. Ngai", "B.K. Cheung", "N. Mladenovic" ],
      "venue" : "Journal of classification,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Mixture Models: Inference and Applications to Clustering",
      "author" : [ "G.J. McLachlan", "K.E. Basford" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "Mikhail Belkin", "Kaushik Sinha" ],
      "venue" : "In FOCS 2010: Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Multiscale geometric and spectral analysis of plane arrangements",
      "author" : [ "G. Chen", "M. Maggioni" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Rank/norm regularization with closed-form solutions: Application to subspace clustering",
      "author" : [ "Yaoliang Yu", "Dale Schuurmans" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "A geometric analysis of subspace clustering with outliers",
      "author" : [ "M. Soltanolkotabi", "E.J. Candès" ],
      "venue" : "Arxiv preprint arXiv:1112.4258,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "k-dimensional coding schemes in hilbert spaces",
      "author" : [ "A. Maurer", "M. Pontil" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Regularized principal manifolds",
      "author" : [ "A.J. Smola", "S. Mika", "B. Schölkopf", "R.C. Williamson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2001
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A. Dempster", "N. Laird", "D. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society. Series B,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1977
    }, {
      "title" : "Robust clustering methods: a unified view",
      "author" : [ "R.N. Davé", "R. Krishnapuram" ],
      "venue" : "Fuzzy Systems, IEEE Transactions on,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "Outlier-robust PCA: The highdimensional case",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Shie Mannor" ],
      "venue" : "IEEE transactions on information theory,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Regression clustering",
      "author" : [ "B. Zhang" ],
      "venue" : "In Data Mining,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Robust Regression and Outlier Detection",
      "author" : [ "P.J. Rousseeuw", "A.M. Leroy" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1987
    }, {
      "title" : "Robust Statistics: Theory and Methods",
      "author" : [ "R.A. Maronna", "R.D. Martin", "V.J. Yohai" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Stability and generalization",
      "author" : [ "Olivier Bousquet", "André Elisseeff" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "The planar k-means problem is np-hard",
      "author" : [ "M. Mahajan", "P. Nimbhorkar", "K. Varadarajan" ],
      "venue" : "WALCOM: Algorithms and Computation,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "A benchmark for the comparison of 3-d motion segmentation algorithms",
      "author" : [ "Roberto Tron", "René Vidal" ],
      "venue" : "In CVPR. IEEE Computer Society,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Efficient projections onto the l1ball for learning in high dimensions",
      "author" : [ "J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The arguably most widely applied approach for multiple model learning is the minimum loss approach, also known as Lloyd’s algorithm [1] in clustering, where the goal is to find a set of models, associate each data point to one model (in so called “soft” variations, one or more models), such that the sum of losses over data points is minimal.",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "Indeed, as we show later, the RW formulation is provably robust in the case of clustering, in the sense of having non-zero breakdown point [2].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details.",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details.",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details.",
      "startOffset" : 97,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details.",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 6,
      "context" : "Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details.",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details.",
      "startOffset" : 224,
      "endOffset" : 234
    }, {
      "referenceID" : 8,
      "context" : "Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details.",
      "startOffset" : 224,
      "endOffset" : 234
    }, {
      "referenceID" : 9,
      "context" : "Indeed, special examples of multi-model learning have been studied, including k-means clustering [3, 4, 5] (and many other variants thereof), Gaussian mixture models (and extensions) [6, 7] and subspace segmentation problem [8, 9, 10]; see Section 2 for details.",
      "startOffset" : 224,
      "endOffset" : 234
    }, {
      "referenceID" : 10,
      "context" : "A general treatment of the sample complexity of problems that can be interpreted as learning a code book (which encompasses some types of multiple model learning) is [11].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 11,
      "context" : "Slightly closer to our approach is [12], whose formulation generalizes a common approach to different model types and permits for problem specific regularization, giving both generalization results and algorithmic iteration complexity results.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 12,
      "context" : "A probabilistic and generic algorithmic approach to learning multiple models is Expectation Maximization [13].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Algorithms for dealing with outliers and multiple models together have been proposed in the context of clustering [14].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "Reference [15] provides an example of an algorithm for outlier resistance in learning a single subspace, and partly inspires the current work.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "• Regression clustering [16] extends the standard linear regression problem in that the training samples cannot be explained by one linear function.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "Neither mode is avoided by spreading a point’s weight among models as in mixture models [6].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "This challenge might be solved by explicitly and separately estimating the relative frequencies of the classes, and penalizing deviations from the estimates rather than from equal frequencies, as is done in mixture models [6]; this is left for future study.",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 1,
      "context" : "We use “breakdown point” – the standard robustness measure in the literature of robust statistics [2], see also [17, 18] and many others – to quantify the robustness property of the proposed formulation.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "We use “breakdown point” – the standard robustness measure in the literature of robust statistics [2], see also [17, 18] and many others – to quantify the robustness property of the proposed formulation.",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 17,
      "context" : "We use “breakdown point” – the standard robustness measure in the literature of robust statistics [2], see also [17, 18] and many others – to quantify the robustness property of the proposed formulation.",
      "startOffset" : 112,
      "endOffset" : 120
    }, {
      "referenceID" : 13,
      "context" : "The non robustness of k-means has led to the development of many formulations of robust clustering, see a review by [14].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : ", classical bounds on SVM kernel regression [19] use bounded kernels).",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "As is typical in multi-model learning, simultaneously optimizing the model and the association of the data (in our formulation, the weight) is computationally hard [20], thus Algorithm 1 alternates between optimizing the weight with the model fixed, and optimizing the model with the weights fixed.",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : "We propose to solve each instance of weight setting using gradient methods, and in particular FISTA [21].",
      "startOffset" : 100,
      "endOffset" : 104
    } ],
    "year" : 2013,
    "abstractText" : "We consider the general problem of Multiple Model Learning (MML) from data, from the statistical and algorithmic perspectives; this problem includes clustering, multiple regression and subspace clustering as special cases. A common approach to solving new MML problems is to generalize Lloyd’s algorithm for clustering (or Expectation-Maximization for soft clustering). However this approach is unfortunately sensitive to outliers and large noise: a single exceptional point may take over one of the models. We propose a different general formulation that seeks for each model a distribution over data points; the weights are regularized to be sufficiently spread out. This enhances robustness by making assumptions on class balance. We further provide generalization bounds and explain how the new iterations may be computed efficiently. We demonstrate the robustness benefits of our approach with some experimental results and prove for the important case of clustering that our approach has a non-trivial breakdown point, i.e., is guaranteed to be robust to a fixed percentage of adversarial unbounded outliers.",
    "creator" : null
  }
}