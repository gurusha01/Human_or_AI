{
  "name" : "dc4c44f624d600aa568390f1f1104aa0.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Generalized Method-of-Moments for Rank Aggregation",
    "authors" : [ "Hossein Azari Soufiani", "William Z. Chen" ],
    "emails" : [ "azari@fas.harvard.edu", "wchen@college.harvard.edu", "parkes@eecs.harvard.edu", "xial@cs.rpi.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we propose a class of efficient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run significantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efficiency."
    }, {
      "heading" : "1 Introduction",
      "text" : "In many applications, we need to aggregate the preferences of agents over a set of alternatives to produce a joint ranking. For example, in systems for ranking the quality of products, restaurants, or other services, we can generate an aggregate rank through feedback from individual users. This idea of rank aggregation also plays an important role in multiagent systems, meta-search engines [4], belief merging [5], crowdsourcing [15], and many other e-commerce applications.\nA standard approach towards rank aggregation is to treat input rankings as data generated from a probabilistic model, and then learn the MLE of the input data. This idea has been explored in both the machine learning community and the (computational) social choice community. The most popular statistical models are the Bradley-Terry-Luce model (BTL for short) [2, 13], the PlackettLuce model (PL for short) [17, 13], the random utility model [18], and the Mallows (Condorcet) model [14, 3]. In machine learning, researchers have focused on designing efficient algorithms to estimate parameters for popular models; e.g. [8, 12, 1]. This line of research is sometimes referred to as learning to rank [11].\nRecently, Negahban et al. [16] proposed a rank aggregation algorithm, called Rank Centrality (RC), based on computing the stationary distribution of a Markov chain whose transition matrix is defined according to the data (pairwise comparisons among alternatives). The authors describe the approach as being model independent, and prove that for data generated according to BTL, the output of RC converges to the ground truth, and the performance of RC is almost identical to the performance of\nMLE for BTL. Moreover, they characterized the convergence rate and showed experimental comparisons.\nOur Contributions. In this paper, we take a generalized method-of-moments (GMM) point of view towards rank aggregation. We first reveal a new and natural connection between the RC algorithm [16] and the BTL model by showing that RC algorithm can be interpreted as a GMM estimator applied to the BTL model.\nThe main technical contribution of this paper is a class of GMMs for parameter estimation under the PL model, which generalizes BTL and the input consists of full rankings instead of pairwise comparisons as in the case of BTL and RC algorithm.\nOur algorithms first break full rankings into pairwise comparisons, and then solve the generalized moment conditions to find the parameters. Each of our GMMs is characterized by a way of breaking full rankings. We characterize conditions for the output of the algorithm to be unique, and we also obtain some general characterizations that help us to determine which method of breaking leads to a consistent GMM. Specifically, full breaking (which uses all pairwise comparisons in the ranking) is consistent, but adjacent breaking (which only uses pairwise comparisons in adjacent positions) is inconsistent.\nWe characterize the computational complexity of our GMMs, and show that the asymptotic complexity is better than for the classical Minorize-Maximization (MM) algorithm for PL [8]. We also compare statistical efficiency and running time of these methods experimentally using both synthetic and real-world data, showing that all GMMs run much faster than the MM algorithm.\nFor the synthetic data, we observe that many consistent GMMs converge as fast as the MM algorithm, while there exists a clear tradeoff between computational complexity and statistical efficiency among consistent GMMs.\nTechnically our technique is related to the random walk approach [16]. However, we note that our algorithms aggregate full rankings under PL, while the RC algorithm aggregates pairwise comparisons. Therefore, it is quite hard to directly compare our GMMs and RC fairly since they are designed for different types of data. Moreover, by taking a GMM point of view, we prove the consistency of our algorithms on top of theories for GMMs, while Negahban et al. proved the consistency of RC directly."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Let C = {c1, .., cm} denote the set of m alternatives. Let D = {d1, . . . , dn} denote the data, where each dj is a full ranking over C. The PL model is a parametric model where each alternative ci is parameterized by γi ∈ (0, 1), such that ∑m i=1 γi = 1. Let ~γ = (γ1, . . . , γm) and Ω denote the\nparameter space. Let Ω̄ denote the closure of Ω. That is, Ω̄ = {~γ : ∀i, γi ≥ 0 and ∑m i=1 γi = 1}. Given ~γ∗ ∈ Ω, the probability for a ranking d = [ci1 ci2 · · · cim ] is defined as follows.\nPrPL(d|~γ) = γi1∑m l=1 γil × γi2∑m l=2 γil × · · · × γim−1 γim−1 + γim\nIn the BTL model, the data is composed of pairwise comparisons instead of rankings, and the model is parameterized in the same way as PL, such that PrBTL(ci1 ci2 |~γ) =\nγi1 γi1 + γi2 .\nBTL can be thought of as a special case of PL via marginalization, since PrBTL(ci1 ci2 |~γ) =∑ d:ci1 cc2 PrPL(d|~γ). In the rest of the paper, we denote Pr = PrPL.\nGeneralized Method-of-Moments (GMM) provides a wide class of algorithms for parameter estimation. In GMM, we are given a parametric model whose parametric space is Ω ⊆ Rm, an infinite series of q × q matrices W = {Wt : t ≥ 1}, and a column-vector-valued function g(d,~γ) ∈ Rq . For any vector ~a ∈ Rq and any q × q matrix W , we let ‖~a‖W = (~a)TW~a. For any data D, let g(D,~γ) = 1n ∑ d∈D g(d,~γ), and the GMM method computes parameters ~γ\n′ ∈ Ω that minimize ‖g(D,~γ′)‖Wn , formally defined as follows:\nGMMg(D,W) = {~γ′ ∈ Ω : ‖g(D,~γ′)‖Wn = inf ~γ∈Ω ‖g(D,~γ)‖Wn} (1)\nSince Ω may not be compact (as is the case for PL), the set of parameters GMMg(D,W) can be empty. A GMM is consistent if and only if for any ~γ∗ ∈ Ω, GMMg(D,W) converges in probability to ~γ∗ as n→∞ and the data is drawn i.i.d. given ~γ∗. Consistency is a desirable property for GMMs.\nIt is well-known that GMMg(D,W) is consistent if it satisfies some regularity conditions plus the following condition [7]:\nCondition 1. Ed|~γ∗ [g(d,~γ)] = 0 if and only if ~γ = ~γ∗. Example 1. MLE as a consistent GMM: Suppose the likelihood function is twice-differentiable, then the MLE is a consistent GMM where g(d,~γ) = 5~γ log Pr(d|~γ) and Wn = I . Example 2. Negahban et al. [16] proposed the Rank Centrality (RC) algorithm that aggregates pairwise comparisons DP = {Y1, . . . , Yn}.1 Let aij denote the number of ci cj in DP and it is assumed that for any i 6= j, aij + aji = k. Let dmax denote the maximum pairwise defeats for an alternative. RC first computes the following m×m column stochastic matrix:\nPRC(DP )ij =\n{ aij/(kdmax) if i 6= j\n1− ∑ l 6=i ali/(kdmax) if i = j\nThen, RC computes (PRC(DP ))T ’s stationary distribution ~γ as the output. Let Xci cj (Y ) = {\n1 if Y = [ci cj ] 0 otherwise and P ∗ RC(Y ) =\n{ Xci cj if i 6= j\n− ∑ l 6=iX cl ci if i = j .\nLet gRC(d,~γ) = P ∗RC(d) ·~γ. It is not hard to check that the output of RC is the output of GMMgRC . Moreover, GMMgRC satisfies Condition 1 under the BTL model, and as we will show later in Corollary 4, GMMgRC is consistent for BTL."
    }, {
      "heading" : "3 Generalized Method-of-Moments for the Plakett-Luce model",
      "text" : "In this section we introduce our GMMs for rank aggregation under PL. In our methods, q = m, Wn = I and g is linear in ~γ. We start with a simple special case to illustrate the idea.\nExample 3. For any full ranking d over C, we let • Xci cj (d) = {\n1 ci d cj 0 otherwise\n• P (d) be an m×m matrix where P (d)ij = {\nXci cj (d) if i 6= j − ∑ l 6=iX cl ci(d) if i = j\n• gF (d,~γ) = P (d) · ~γ and P (D) = 1n ∑ d∈D P (d)\nFor example, let m = 3, D = {[c1 c2 c3], [c2 c3 c1]}. Then P (D) =[ −1 1/2 1/2 1/2 −1/2 1 1/2 0 −3/2 ] . The corresponding GMM seeks to minimize ‖P (D) · ~γ‖22 for ~γ ∈ Ω.\nIt is not hard to verify that (Ed|~γ∗ [P (d)])ij =  γ∗i γ∗i +γ ∗ j if i 6= j − ∑ l 6=i γ∗l γ∗i +γ ∗ l if i = j , which means that Ed|~γ∗ [gF (d,~γ ∗)] = Ed|~γ∗ [P (d)] · ~γ∗ = 0. It is not hard to verify that ~γ∗ is the only solution to Ed|~γ∗ [gF (d,~γ)] = 0. Therefore, GMMgF satisfies Condition 1. Moreover, we will show in Corollary 3 that GMMgF is consistent for PL.\nIn the above example, we count all pairwise comparisons in a full ranking d to build P (d), and define g = P (D) · ~γ to be linear in ~γ. In general, we may consider some subset of pairwise comparisons. This leads to the definition of our class of GMMs based on the notion of breakings. Intuitively, a breaking is an undirected graph over the m positions in a ranking, such that for any full ranking d, the pairwise comparisons between alternatives in the ith position and jth position are counted to construct PG(d) if and only if {i, j} ∈ G. Definition 1. A breaking is a non-empty undirected graph G whose vertices are {1, . . . ,m}. Given any breaking G, any full ranking d over C, and any ci, cj ∈ C, we let\n1The BTL model in [16] is slightly different from that in this paper. Therefore, in this example we adopt an equivalent description of the RC algorithm.\n• Xci cjG (d) = { 1 {Pos(ci, d),Pos(cj , d)} ∈ G and ci d cj 0 otherwise , where Pos(ci, d) is the posi-\ntion of ci in d. • PG(d) be an m×m matrix where PG(d)ij = {\nX ci cj G (d) if i 6= j − ∑ l 6=iX cl ci G (d) if i = j\n• gG(d,~γ) = PG(d) · ~γ • GMMG(D) be the GMM method that solves Equation (1) for gG and Wn = I .2\nIn this paper, we focus on the following breakings, illustrated in Figure 1.\n• Full breaking: GF is the complete graph. Example 3 is the GMM with full breaking. • Top-k breaking: for any k ≤ m, GkT = {{i, j} : i ≤ k, j 6= i}.\n• Bottom-k breaking: for any k ≥ 2, GkB = {{i, j} : i, j ≥ m+ 1− k, j 6= i}.3\n• Adjacent breaking: GA = {{1, 2}, {2, 3}, . . . , {m− 1,m}}. • Position-k breaking: for any k ≥ 2, GkP = {{k, i} : i 6= k}.\nIntuitively, the full breaking contains all the pairwise comparisons that can be extracted from each agent’s full rank information in the ranking; the top-k breaking contains all pairwise comparisons that can be extracted from the rank provided by an agent when she only reveals her top k alternatives and the ranking among them; the bottom-k breaking can be computed when an agent only reveals her bottom k alternatives and the ranking among them; and the position-k breaking can be computed when the agent only reveals the alternative that is ranked at the kth position and the set of alternatives ranked in lower positions.\nWe note that GmT = G m B = GF , G 1 T = G 1 P , and for any k ≤ m − 1, GkT ∪ G m−k B = GF , and GkT = ⋃k l=1G l P .\nWe are now ready to present our GMM algorithm (Algorithm 1) parameterized by a breaking G.\n2To simplify notation, we use GMMG instead of GMMgG . 3We need k ≥ 2 since GkB is empty.\nAlgorithm 1: GMMG(D) Input: A breaking G and data D = {d1, . . . , dn} composed of full rankings. Output: Estimation GMMG(D) of parameters under PL.\n1 Compute PG(D) = 1n ∑ d∈D PG(d) in Definition 1. 2 Compute GMMG(D) according to (1). 3 return GMMG(D).\nStep 2 can be further simplified according to the following theorem. Due to the space constraints, most proofs are relegated to the supplementary materials. Theorem 1. For any breaking G and any data D, there exists ~γ ∈ Ω̄ such that PG(D) · ~γ = 0.\nTheorem 1 implies that in Equation (1), inf~γ∈Ω g(D,~γ)TWng(D,~γ)} = 0. Therefore, Step 2 can be replaced by: 2∗ Let GMMG = {~γ ∈ Ω : PG(D) · ~γ = 0}."
    }, {
      "heading" : "3.1 Uniqueness of Solution",
      "text" : "It is possible that for some data D, GMMG(D) is empty or non-unique. Our next theorem characterizes conditions for |GMMG(D)| = 1 and |GMMG(D)| 6= ∅. A Markov chain (row stochastic matrix) M is irreducible, if any state can be reached from any other state. That is, M only has one communicating class. Theorem 2. Among the following three conditions, 1 and 2 are equivalent for any breaking G and any dataD. Moreover, conditions 1 and 2 are equivalent to condition 3 if and only ifG is connected.\n1. (I + PG(D)/m)T is irreducible.\n2. |GMMG(D)| = 1. 3. GMMG(D) 6= ∅.\nCorollary 1. For the full breaking, adjacent breaking, and any top-k breaking, the three statements in Theorem 2 are equivalent for any data D. For any position-k (with k ≥ 2) and any bottom-k (with k ≤ m− 1), 1 and 2 are not equivalent to 3 for some data D.\nFord, Jr. [6] identified a necessary and sufficient condition on data D for the MLE under PL to be unique, which is equivalent to condition 1 in Theorem 2. Therefore, we have the following corollary. Corollary 2. For the full breaking GF , |GMMGF (D)| = 1 if and only if |MLEPL(D)| = 1."
    }, {
      "heading" : "3.2 Consistency",
      "text" : "We say a breaking G is consistent (for PL), if GMMG is consistent (for PL). Below, we show that some breakings defined in the last subsection are consistent. We start with general results. Theorem 3. A breaking G is consistent if and only if Ed|~γ∗ [g(d,~γ∗)] = 0, which is equivalent to the following equalities:\nfor all i 6= j, Pr(ci cj |{Pos(ci, d),Pos(cj , d)} ∈ G) Pr(cj ci|{Pos(ci),Pos(cj)} ∈ G) = γ∗i γ∗j . (2)\nTheorem 4. Let G1, G2 be a pair of consistent breakings.\n1. If G1 ∩G2 = ∅, then G1 ∪G2 is also consistent. 2. If G1 ( G2 and (G2 \\G1) 6= ∅, then (G2 \\G1) is also consistent.\nContinuing, we show that position-k breakings are consistent, then use this and Theorem 4 as building blocks to prove additional consistency results. Proposition 1. For any k ≥ 1, the position-k breaking GkP is consistent.\nWe recall that GkT = ⋃k l=1G l P , GF = G m T , and G k B = GF \\ G m−k T . Therefore, we have the following corollary. Corollary 3. The full breaking GF is consistent; for any k, GkT is consistent, and for any k ≥ 2, GkB is consistent. Theorem 5. Adjacent breaking GA is consistent if and only if all components in ~γ∗ are the same.\nLastly, the technique developed in this section can also provide an independent proof that the RC algorithm is consistent for BTL, which is implied by the main theorem in [16]:\nCorollary 4. [16] The RC algorithm is consistent for BTL.\nRC is equivalent to GMMgRC that satisfies Condition 1. By checking similar conditions as we did in the proof of Theorem 3, we can prove that GMMgRC is consistent for BTL.\nThe results in this section suggest that if we want to learn the parameters of PL, we should use consistent breakings, including full breaking, top-k breakings, bottom-k breakings, and position-k breakings. The adjacent breaking seems quite natural, but it is not consistent, thus will not provide a good estimate to the parameters of PL. This will also be verified by experimental results in Section 4."
    }, {
      "heading" : "3.3 Complexity",
      "text" : "We first characterize the computational complexity of our GMMs.\nProposition 2. The computational complexity of the MM algorithm for PL [8] and our GMMs are listed below.\n•MM: O(m3n) per iteration. • GMM (Algorithm 1) with full breaking: O(m2n + m2.376), with O(m2n) for breaking and\nO(m2.376) for computing step 2∗ in Algorithm 1 (matrix inversion).\n• GMM with adjacent breaking: O(mn + m2.376), with O(mn) for breaking and O(m2.376) for computing step 2∗ in Algorithm 1.\n• GMM with top-k breaking: O((m + k)kn + m2.376), with O((m + k)kn) for breaking and O(m2.376) for computing step 2∗ in Algorithm 1.\nIt follows that the asymptotic complexity of the GMM algorithms is better than for the classical MM algorithm. In particular, the GMM with adjacent breaking and top-k breaking for constant k’s are the fastest. However, we recall that the GMM with adjacent breaking is not consistent, while the other algorithms are consistent. We would expect that as data size grows, the GMM with adjacent breaking will provide a relatively poor estimation to ~γ∗ compared to the other methods.\nMoreover in the statistical setting in order to gain consistency we need regimes that m = o(n) and large ns are going to lead to major computational bottlenecks. All the above algorithms (MM and different GMMs) have linear complexity in n, hence, the coefficient for n is essential in determining the tradeoffs between these methods. As it can be seen above the coefficient for n is linear in m for top-k breaking and quadratic for full breaking while it is cubic in m for the MM algorithm. This difference is illustrated through experiments in Figure 5.\nAmong GMMs with top-k breakings, the larger the k is, the more information we use in a single ranking, which comes at a higher computational cost. Therefore, it is natural to conjecture that for the same data, GMMGkT with large k converges faster than GMMGkT with small k. In other words, we expect to see the following time-efficiency tradeoff among GMMGkT for different k’s, which is verified by the experimental results in the next section.\nConjecture 1. (time-efficiency tradeoff) for any k1 < k2, GMMGk1T runs faster, while GMMGk2T provides a better estimate to the ground truth."
    }, {
      "heading" : "4 Experiments",
      "text" : "The running time and statistical efficiency of MM and our GMMs are examined for both synthetic data and a real-world sushi dataset [9]. The synthetic datasets are generated as follows.\n• Generating the ground truth: for m ≤ 300, the ground truth ~γ∗ is generated from the Dirichlet distribution Dir(~1).\n• Generating data: given a ground truth ~γ∗, we generate up to 1000 full rankings from PL. We implemented MM [8] for 1, 3, 10 iterations, as well as GMMs with full breaking, adjacent breaking, and top-k breaking for all k ≤ m− 1.\nWe focus on the following representative criteria. Let ~γ denote the output of the algorithm.\n• Mean Squared Error: MSE = E(‖~γ − ~γ∗‖22). • Kendall Rank Correlation Coefficient: Let K(~γ,~γ∗) denote the Kendall tau distance between the ranking over components in ~γ and the ranking over components in ~γ∗. The Kendall correlation is 1− 2 K(~γ,~γ\n∗) m(m−1)/2 .\nAll experiments are run on a 1.86 GHz Intel Core 2 Duo MacBook Air. The multiple repetitions for the statistical efficiency experiments in Figure 3 and experiments for sushi data in Figure 5 have been done using the odyssey cluster. All the codes are written in R project and they are available as a part of the package ”StatRank”."
    }, {
      "heading" : "4.1 Synthetic Data",
      "text" : "In this subsection we focus on comparisons among MM, GMM-F (full breaking), and GMM-A (adjacent breaking). The running time is presented in Figure 2. We observe that GMM-A (adjacent breaking) is the fastest and MM is the slowest, even for one iteration.\nThe statistical efficiency is shown in Figure 3. We observe that in regard to the MSE criterion, GMM-F (full breaking) performs as well as MM for 10 iterations (which converges), and that these are both better than GMM-A (adjacent breaking). For the Kendall correlation criterion, GMM-F (full breaking) has the best performance and GMM-A (adjacent breaking) has the worst performance. Statistics are calculated over 1840 trials. In all cases except one, GMM-F (full breaking) outperforms MM which outperforms GMM-A (adjacent breaking) with statistical significance at 95% confidence. The only exception is between GMM-F (full breaking) and MM for Kendall correlation at n = 1000.\n4.2 Time-Efficiency Tradeoff among Top-k Breakings Results on the running time and statistical efficiency for top-k breakings are shown in Figure 4. We recall that top-1 is equivalent to position-1, and top-(m− 1) is equivalent to the full breaking. For n = 100, MSE comparisons between successive top-k breakings are statistically significant at 95% level from (top-1, top-2) to (top-6, top-7). The comparisons in running time are all significant at 95% confidence level. On average, we observe that top-k breakings with smaller k run faster, while top-k breakings with larger k have higher statistical efficiency in both MSE and Kendall correlation. This justifies Conjecture 1."
    }, {
      "heading" : "4.3 Experiments for Real Data",
      "text" : "In the sushi dataset [9], there are 10 kinds of sushi (m = 10) and the amount of data n is varied, randomly sampling with replacement. We set the ground truth to be the output of MM applied to all 5000 data points. For the running time, we observe the same as for the synthetic data: GMM (adjacent breaking) runs faster than GMM (full breaking), which runs faster than MM (The results on running time can be found in supplementary material B).\nComparisons for MSE and Kendall correlation are shown in Figure 5. In both figures, 95% confidence intervals are plotted but too small to be seen. Statistics are calculated over 1970 trials.\nFor MSE and Kendall correlation, we observe that MM converges fastest, followed by GMM (full breaking), which outperforms GMM (adjacent breaking) which does not converge. Differences between performances are all statistically significant with 95% confidence (with exception of Kendall correlation and both GMM methods for n = 200, where p = 0.07). This is different from comparisons for synthetic data (Figure 3). We believe that the main reason is because PL does not fit sushi data well, which is a fact recently observed by Azari et al. [1]. Therefore, we cannot expect that GMM converges to the output of MM on the sushi dataset, since the consistency results (Corollary 3) assumes that the data is generated under PL."
    }, {
      "heading" : "5 Future Work",
      "text" : "We plan to work on the connection between consistent breakings and preference elicitation. For example, even though the theory in this paper is developed for full ranks, the notion of top-k and bottom-k breaking are implicitly allowing some partial order settings. More specifically, top-k breaking can be achieved from partial orders that include full rankings for the top-k alternatives."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is supported in part by NSF Grants No. CCF- 0915016 and No. AF-1301976. Lirong Xia acknowledges NSF under Grant No. 1136996 to the Computing Research Association for the CIFellows project and an RPI startup fund. We thank Joseph K. Blitzstein, Edoardo M. Airoldi, Ryan P. Adams, Devavrat Shah, Yiling Chen, Gábor Cárdi and members of Harvard EconCS group for their comments on different aspects of this work. We thank anonymous NIPS-13 reviewers, for helpful comments and suggestions."
    } ],
    "references" : [ {
      "title" : "Random utility theory for social choice",
      "author" : [ "Hossein Azari Soufiani", "David C. Parkes", "Lirong Xia" ],
      "venue" : "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Rank analysis of incomplete block designs: I",
      "author" : [ "Ralph Allan Bradley", "Milton E. Terry" ],
      "venue" : "The method of paired comparisons. Biometrika,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1952
    }, {
      "title" : "Rank aggregation methods for the web",
      "author" : [ "Cynthia Dwork", "Ravi Kumar", "Moni Naor", "D. Sivakumar" ],
      "venue" : "In Proceedings of the 10th World Wide Web Conference,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "The strategy-proofness landscape of merging",
      "author" : [ "Patricia Everaere", "Sébastien Konieczny", "Pierre Marquis" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Solution of a ranking problem from binary comparisons",
      "author" : [ "Lester R. Ford", "Jr." ],
      "venue" : "The American Mathematical Monthly,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1957
    }, {
      "title" : "Large Sample Properties of Generalized Method of Moments",
      "author" : [ "Lars Peter Hansen" ],
      "venue" : "Estimators. Econometrica,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1982
    }, {
      "title" : "MM algorithms for generalized Bradley-Terry models",
      "author" : [ "David R. Hunter" ],
      "venue" : "In The Annals of Statistics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Nantonac collaborative filtering: Recommendation based on order responses",
      "author" : [ "Toshihiro Kamishima" ],
      "venue" : "In Proceedings of the Ninth International Conference on Knowledge Discovery and Data Mining (KDD),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "Learning to Rank for Information",
      "author" : [ "Tie-Yan Liu" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Learning Mallows models with pairwise preferences",
      "author" : [ "Tyler Lu", "Craig Boutilier" ],
      "venue" : "In Proceedings of the Twenty-Eighth International Conference on Machine Learning (ICML",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Individual Choice Behavior: A Theoretical Analysis",
      "author" : [ "Robert Duncan Luce" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1959
    }, {
      "title" : "Non-null ranking model",
      "author" : [ "Colin L. Mallows" ],
      "venue" : "Biometrika, 44(1/2):114–130,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1957
    }, {
      "title" : "Better human computation through principled voting",
      "author" : [ "Andrew Mao", "Ariel D. Procaccia", "Yiling Chen" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Iterative ranking from pair-wise comparisons",
      "author" : [ "Sahand Negahban", "Sewoong Oh", "Devavrat Shah" ],
      "venue" : "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "The analysis of permutations",
      "author" : [ "Robin L. Plackett" ],
      "venue" : "Journal of the Royal Statistical Society. Series C (Applied Statistics),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1975
    }, {
      "title" : "Thurstone. A law of comparative judgement",
      "author" : [ "Louis Leon" ],
      "venue" : "Psychological Review,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1927
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "This idea of rank aggregation also plays an important role in multiagent systems, meta-search engines [4], belief merging [5], crowdsourcing [15], and many other e-commerce applications.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "This idea of rank aggregation also plays an important role in multiagent systems, meta-search engines [4], belief merging [5], crowdsourcing [15], and many other e-commerce applications.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 12,
      "context" : "This idea of rank aggregation also plays an important role in multiagent systems, meta-search engines [4], belief merging [5], crowdsourcing [15], and many other e-commerce applications.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "The most popular statistical models are the Bradley-Terry-Luce model (BTL for short) [2, 13], the PlackettLuce model (PL for short) [17, 13], the random utility model [18], and the Mallows (Condorcet) model [14, 3].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "The most popular statistical models are the Bradley-Terry-Luce model (BTL for short) [2, 13], the PlackettLuce model (PL for short) [17, 13], the random utility model [18], and the Mallows (Condorcet) model [14, 3].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "The most popular statistical models are the Bradley-Terry-Luce model (BTL for short) [2, 13], the PlackettLuce model (PL for short) [17, 13], the random utility model [18], and the Mallows (Condorcet) model [14, 3].",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 10,
      "context" : "The most popular statistical models are the Bradley-Terry-Luce model (BTL for short) [2, 13], the PlackettLuce model (PL for short) [17, 13], the random utility model [18], and the Mallows (Condorcet) model [14, 3].",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "The most popular statistical models are the Bradley-Terry-Luce model (BTL for short) [2, 13], the PlackettLuce model (PL for short) [17, 13], the random utility model [18], and the Mallows (Condorcet) model [14, 3].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 11,
      "context" : "The most popular statistical models are the Bradley-Terry-Luce model (BTL for short) [2, 13], the PlackettLuce model (PL for short) [17, 13], the random utility model [18], and the Mallows (Condorcet) model [14, 3].",
      "startOffset" : 207,
      "endOffset" : 214
    }, {
      "referenceID" : 8,
      "context" : "This line of research is sometimes referred to as learning to rank [11].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "[16] proposed a rank aggregation algorithm, called Rank Centrality (RC), based on computing the stationary distribution of a Markov chain whose transition matrix is defined according to the data (pairwise comparisons among alternatives).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "We first reveal a new and natural connection between the RC algorithm [16] and the BTL model by showing that RC algorithm can be interpreted as a GMM estimator applied to the BTL model.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "We characterize the computational complexity of our GMMs, and show that the asymptotic complexity is better than for the classical Minorize-Maximization (MM) algorithm for PL [8].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 13,
      "context" : "Technically our technique is related to the random walk approach [16].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "It is well-known that GMMg(D,W) is consistent if it satisfies some regularity conditions plus the following condition [7]: Condition 1.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "[16] proposed the Rank Centrality (RC) algorithm that aggregates pairwise comparisons DP = {Y1, .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Given any breaking G, any full ranking d over C, and any ci, cj ∈ C, we let (1)The BTL model in [16] is slightly different from that in this paper.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : "[6] identified a necessary and sufficient condition on data D for the MLE under PL to be unique, which is equivalent to condition 1 in Theorem 2.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "Lastly, the technique developed in this section can also provide an independent proof that the RC algorithm is consistent for BTL, which is implied by the main theorem in [16]: Corollary 4.",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : "[16] The RC algorithm is consistent for BTL.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "The computational complexity of the MM algorithm for PL [8] and our GMMs are listed below.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "The running time and statistical efficiency of MM and our GMMs are examined for both synthetic data and a real-world sushi dataset [9].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "We implemented MM [8] for 1, 3, 10 iterations, as well as GMMs with full breaking, adjacent breaking, and top-k breaking for all k ≤ m− 1.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "3 Experiments for Real Data In the sushi dataset [9], there are 10 kinds of sushi (m = 10) and the amount of data n is varied, randomly sampling with replacement.",
      "startOffset" : 49,
      "endOffset" : 52
    } ],
    "year" : 2013,
    "abstractText" : "In this paper we propose a class of efficient Generalized Method-of-Moments (GMM) algorithms for computing parameters of the Plackett-Luce model, where the data consists of full rankings over alternatives. Our technique is based on breaking the full rankings into pairwise comparisons, and then computing parameters that satisfy a set of generalized moment conditions. We identify conditions for the output of GMM to be unique, and identify a general class of consistent and inconsistent breakings. We then show by theory and experiments that our algorithms run significantly faster than the classical Minorize-Maximization (MM) algorithm, while achieving competitive statistical efficiency.",
    "creator" : null
  }
}