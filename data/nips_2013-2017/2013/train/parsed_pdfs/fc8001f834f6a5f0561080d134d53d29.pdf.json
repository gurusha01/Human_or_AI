{
  "name" : "fc8001f834f6a5f0561080d134d53d29.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning to Prune in Metric and Non-Metric Spaces",
    "authors" : [ "Leonid Boytsov", "Bilegsaikhan Naidan" ],
    "emails" : [ "srchvrs@cmu.edu", "bileg@idi.ntnu.no" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Similarity search algorithms are essential to multimedia retrieval, computational biology, and statistical machine learning. Resemblance between objects x and y is typically expressed in the form of a distance function d(x, y), where smaller values indicate less dissimilarity. In our work we use the Euclidean distance (L2), the KL-divergence ( ∑ xi log xi/yi), and the Itakura-Saito distance\n( ∑ xi/yi − log xi/yi − 1). KL-divergence is commonly used in text analysis, image classification, and machine learning [6]. Both KL-divergence and the Itakura-Saito distance belong to a class of distances called Bregman divergences.\nOur interest is in the nearest neighbor (NN) search, i.e., we aim to retrieve the object o that is closest to the query q. For the KL-divergence and other non-symmetric distances two types of NN-queries are defined. The left NN-query returns the object o that minimizes the distance d(o, q), while the right NN-query finds o that minimizes d(q, o).\nThe distance function can be computationally expensive. There was a considerable effort to reduce computational costs through approximating the distance function, projecting data in a lowdimensional space, and/or applying a hierarchical space decomposition. In the case of the hierarchical space decomposition, a retrieval process is a recursion that employs an “oracle” procedure. At each step of the recursion, retrieval can continue in one or more partitions. The oracle allows one to prune partitions without directly comparing the query against data points in these partitions. To this end, the oracle assesses the query and estimates which partitions may contain an answer and, therefore, should be recursively analyzed. A pruning algorithm is essentially a binary classifier. In metric spaces, one can use the classifier based on the triangle inequality. In non-metric spaces, a classifier can be learned from data.\nThere are numerous data structures that speedup the NN-search by creating hierarchies of partitions at index time, most notably the VP-tree [28, 31] and the KD-tree [4]. A comprehensive review of these approaches can be found in books by Zezula et al. [32] and Samet [27]. As dimensionality\nincreases, the filtering efficiency of space-partitioning methods decreases rapidly, which is known as the “curse of dimensionality” [30]. This happens because in high-dimensional spaces histograms of distances and 1-Lipschitz function values become concentrated [25]. The negative effect can be partially offset by creating overlapping partitions (see, e.g., [21]) and, thus, trading index size for retrieval time. The approximate NN-queries are less affected by the curse of the dimensionality, because it is possible to reduce retrieval time at the cost of missing some relevant answers [18, 9, 25]. Low-dimensional data sets embedded into a high-dimensional space do not exhibit high concentration of distances, i.e., their intrinsic dimensionality is low. In metric spaces, it was proposed to compute the intrinsic dimensionality as the half of the squared signal to noise ratio (for the distance distribution) [10].\nA well-known approximate NN-search method is the locality sensitive hashing (LSH) [18, 17]. It is based on the idea of random projections [18, 20]. There is also an extension of the LSH for symmetric non-metric distances [23]. The LSH employs several hash functions: It is likely that close objects have same hash values and distant objects have different hash values. In the classic LSH index, the probability of finding an element in one hash table is small and, consequently, many hash tables are to be created during indexing. To reduce space requirements, Lv et al. proposed a multi-probe version of the LSH, which can query multiple buckets of the same hash table [22]. Performance of the LSH depends on the choice of parameters, which can be tuned to fit the distribution of data [11].\nFor approximate searching it was demonstrated that an early termination strategy could rely on information about distances from typical queries to their respective nearest neighbors [33, 1]. Amato et al. [1] showed that density estimates can be used to approximate a pruning function in metric spaces. They relied on a hierarchical decomposition method (an M-tree) and proposed to visit partitions in the order defined by density estimates. Chávez and Navarro [9] proposed to relax triangle-inequality based lower bounds for distances to potential nearest neighbors. The approach, which they dubbed as stretching of the triangle inequality, involves multiplying an exact bound by α > 1.\nFew methods were designed to work in non-metric spaces. One common indexing approach involves mapping the data to a low-dimensional Euclidean space. The goal is to find the mapping without large distortions of the original similarity measure [19, 16]. Jacobs et al. [19] review various projection methods and argue that such a coercion is often against the nature of a similarity measure, which can be, e.g., intrinsically non-symmetric. A mapping can be found using machine learning methods. This can be done either separately for each data point [12, 24] or by computing one global model [3]. There are also a number of approaches, where machine learning is used to estimate optimal parameters of classic search methods [7]. Vermorel [29] applied VP-trees to searching in undisclosed non-metric spaces without trying to learn a pruning function. Like Amato et al. [1], he proposed to visit partitions in the order defined by density estimates and employed the same early termination method as Zezula et al. [33].\nCayton [6] proposed a Bregman ball tree (bbtree), which is an exact search method for Bregman divergences. The bbtree divides data into two clusters (each covered by a Bregman ball) and recursively repeats this procedure for each cluster until the number of data points in a cluster falls below a threshold (a bucket size). At search time, the method relies on properties of Bregman divergences to compute the shortest distances to covering balls. This is an expensive iterative procedure that may require several computations of direct and inverse gradients, as well as of several distances. Additionally, Cayton [6] employed an early termination method: The algorithm can be told to stop after processing a pre-specified number of buckets. The resulting method is an approximate search procedure. Zhang et al. [34] proposed an exact search method based on estimating the maximum distance to a bounding rectangle, but it works with left queries only. The most efficient variant of this method relies on an optimization technique applicable only to certain decomposable Bregman divergences (a decomposable distance is a sum of values computed separately for each coordinate).\nChávez et al. [8] as well as Amato and Savino [2] independently proposed permutation-based search methods. These approximate methods do not involve learning, but, nevertheless, are applicable to non-metric spaces. At index time, k pivots are selected. For every data point, we create a list, called a permutation, where pivots are sorted in the order of increasing distances from the data point. At query time, a rank correlation (e.g., Spearman’s) is computed between the permutation of the query and permutations of data points. Candidate points, which have sufficiently small correlation values, are then compared directly with the query (by computing the original distance function). One can sequentially scan the list of permutations and compute the rank correlation between the\npermutation of the query and the permutation of every data point [8]. Data points are then sorted by rank-correlation values. This approach can be improved by incremental sorting [14], storing permutations as inverted files [2], or prefix trees [13].\nIn this work we experiment with two approaches to learning a pruning function of the VP-tree, which to our knowledge was not attempted previously. We compare the resulting method, which can be applied to both metric and non-metric spaces, with the following state-of-the-art methods: the multi-probe LSH, permutation methods, and the bbtree."
    }, {
      "heading" : "2 Proposed Method",
      "text" : ""
    }, {
      "heading" : "2.1 Classic VP-tree",
      "text" : "In the VP-tree (also known as a ball tree) the space is partitioned with respect to a (usually randomly) chosen pivot π [28, 31]. Assume that we have computed distances from all points to the pivot π and R is a median of these distances. The sphere centered at π with the radius R divides the space into two partitions, each of which contains approximately half of all points. Points inside the pivotcentered sphere are placed into the left subtree, while points outside the pivot-centered sphere are placed into the right subtree (points on the border may be placed arbitrarily). The search algorithm proceeds recursively. When the number of data points is below a certain threshold (the bucket size), these data points are stored as a single bucket. The obtained hierarchical partition is represented by the binary tree, where buckets are leaves.\nThe NN-search is a recursive traversal procedure that starts from the root of the tree and iteratively updates the distance r to the closest object found. When it reaches a bucket (i.e., a leaf), bucket elements are searched sequentially. Each internal node stores the pivot π and the radius R. In a metric space with the distance d(x, y), we use the triangle inequality to prune the search space. We visit:\n• only the left subtree if d(π, q) < R− r; • only the right subtree if d(π, q) > R+ r; • both subtrees if R− r ≤ d(π, q) ≤ R+ r.\nIn the third case, we first visit the partition that contains q. These three cases are illustrated in Fig. 1. Let Dπ,R(x) = |R − x|. Then we need to visit both partitions if and only if r ≥ Dπ,R(d(π, q)). If r < Dπ,R(d(π, q)), we visit only the partition containing the query point. In this case, we prune the other partition. Pruning is a classification task with three classes, where the prediction function is defined through Dπ,R(x). The only argument of this function is a distance between the pivot and the query, i.e., d(π, q). The function value is equal to the maximum radius of the query ball that fits inside the partition containing the query (see the red and the blue sample balls in Fig. 1)."
    }, {
      "heading" : "2.2 Approximating Dπ,R(x) with a Piece-wise Linear Function",
      "text" : "In Section 2 of the supplemental materials, we describe a straightforward sampling algorithm to learn the decision function Dπ,R(x) for every pivot π. This method turned out to be inferior to most state-of-the-art approaches. It is, nevertheless, instructive to examine the decision functions Dπ,R(x) learned by sampling for the Euclidean distance and KL-divergence (see Table 1 for details on data sets).\nEach point in Fig. 2a-2c is a value of the decision function obtained by sampling. Blue curves are fit to these points. For the Euclidean data (Fig. 2a), Dπ,R(x) resembles a piece-wise linear function approximately equal to |R− x|. For the KL-divergence data (Fig. 2b and 2c), Dπ,R(x) looks like a U-shape and a hockey-stick curve, respectively. Yet, most data points concentrate around the median (denoted by a dashed red line). In this area, a piece-wise linear approximation of Dπ,R(x) could\nstill be reasonable. Formally, we define the decision function as:\nDπ,R(x) =  αleft|x−R|, if x ≤ Rαright|x−R|, if x ≥ R (1) Once we obtain the values of αleft and αright that permit near exact searching, we can induce more aggressive pruning by increasing αleft and/or αright, thus, exploring trade-offs between retrieval efficiency and effectiveness. This is similar to stretching of the triangle inequality proposed by Chávez and Navarro [9].\nOptimal αleft and αright are determined using a grid search. To this end, we index a small subset of the data points and seek to obtain parameters that give the shortest retrieval time at a specified recall threshold. The grid search is initialized by values a and b. Then, recall values and retrieval times for all αleft = aρi/m−0.5 and αright = bρj/m−0.5 are obtained (1 ≤ i, j ≤ m). The values of m and ρ are chosen so that: (1) the grid step is reasonably small (i.e., ρ1/m is close to one); (2) the search space is manageable (i.e., m is not large).\nIf the obtained recall values are considerably larger than a specified threshold, the procedure repeats the grid search using larger values of a and b. Similarly, if the recall is not sufficient, the values of a and b are decreased and the grid search is repeated. One can see that the perfect recall can be achieved with αleft = 0 and αright = 0. In this case, no pruning is done and the data set is searched sequentially. Values of αleft = ∞ and αright = ∞ represent an (almost) zero recall, because one of the partitions is always pruned."
    }, {
      "heading" : "2.3 Applicability Conditions",
      "text" : "It is possible to apply the classic VP-tree algorithm only to data sets such that Dπ,R(d(π, q)) > 0 when d(π, q) 6= R. In a relaxed version of this applicability condition, we require that Dπ,R(d(π, q)) > 0 for almost all queries and a large subset of data points. More formally: Property 1. For any pivot π, probability α, and distance x 6= R, there exists a radius r > 0 such that, if two randomly selected points q (a potential query) and u (a potential nearest neighbor) satisfy d(π, q) = x and d(u, q) ≤ r, then both p and q belong to the same partition (defined by π and R) with a probability at least α.\nThe Property 1, which is true for all metric spaces due to the triangle inequality, holds in the case of the KL-divergence and data points u sampled randomly and uniformly from the simplex {xi|xi ≥ 0, ∑ xi = 1}. The proof, which is given in Section 1 of supplemental materials, can be trivially extended to other non-negative distance functions d(x, y) ≥ 0 (e.g., to the Itakura-Saito distance) that satisfy (additional compactness requirements may be required): (1) d(x, y) = 0 ⇔ x = y; (2) the set of discontinuities of d(x, y) has measure zero in L2. This suggests that the VP-tree could be applicable to a wide class of non-metric spaces."
    }, {
      "heading" : "3 Experiments",
      "text" : "We run experiments on a Linux server equipped with Intel Core i7 2600 (3.40 GHz, 8192 KB of L3 CPU cache) and 16 GB of DDR3 RAM (transfer rate is 20GB/sec). The software (including scripts that can be used to reproduce our results) is available online, as a part of the Non-Metric Space Library2 [5]. The code was written in C++, compiled using GNU C++ 4.7 (optimization flag -Ofast), and executed in a single thread. SIMD instructions were enabled using the flags -msse2 -msse4.1 -mssse3.\nAll distance and rank correlation functions are highly optimized and employ SIMD instructions. Vector elements were single-precision numbers. For the KL-divergence, though, we also implemented a slower version, which computes logarithms on-line, i.e., for each distance computation. The faster version computes logarithms of vector elements off-line, i.e., during indexing, and stores with the vectors. Additionally, we need to compute logarithms of query vector elements, but this is done only once per query. The optimized implementation is about 30x times faster than the slower one.\nThe data sets are described in Table 1. Each data set is randomly divided into two parts. The smaller part (containing 1,000 elements) is used as queries, while the larger part is indexed. This procedure is repeated 5 times (for each data sets) and results are aggregated using a classic fixedeffect model [15]. Improvement in efficiency due to indexing is measured as a reduction in retrieval time compared to a sequential, i.e., exhaustive, search. The effectiveness was measured using a simple rank error metric proposed by Cayton [6]. It is equal to the number of NN-points closer to the query than the nearest point returned by the search method. This metric is appropriate mostly for 1-NN queries. We present results only for left queries, but we also verified that in the case of right queries the VP-tree provides similar effectiveness/efficiency trade-offs. We ran benchmarks for L2, the KL-divergence,3 and the Itakura-Saito distance. Implemented methods included:\n• The novel search algorithm based on the VP-tree and a piece-wise linear approximation for Dπ,R(x) as described in Section 2.2. The parameters of the grid search algorithm were: m = 7 and ρ = 8.\n• The permutation method with incremental sorting [14]. The near-optimal performance was obtained by using 16 pivots.\n• The permutation prefix index, where permutation profiles are stored in a prefix tree of limited depth [13]. We used 16 pivots and the maximal prefix length 4 (again selected for best performance).\n• The bbtree [6], which is designed for Bregman divergences, and, thus, it was not used with L2.\n• The multi-probe LSH, which is designed to work only forL2. The implementation employs the LSHKit, 4 which is embedded in the Non-Metric Space Library. The best-performing configuration that we could find used 10 probes and 50 hash tables. The remaining parameters were selected automatically using the cost model proposed by Dong et al. [11].\n2https://github.com/searchivarius/NonMetricSpaceLib 3In the case of SIFT signatures, we use generalized KL-divergence (similarly to Cayton). 4Downloaded from http://lshkit.sourceforge.net/\nFor the bbtree and the VP-tree, vectors in the same bucket were stored in contiguous chunks of memory (allowing for about 1.5-2x reduction in retrieval times). It is typically more efficient to search elements of a small bucket sequentially, rather than using an index. A near-optimal performance was obtained with 50 elements in a bucket. The same optimization approach was also used for both permutation methods.\nSeveral parameters were manually selected to achieve various effectiveness/efficiency trade-offs. They included: the minimal number/percentage of candidates in permutation methods, the desired\nrecall in the multi-probe LSH and in the VP-tree, as well as the maximum number of processed buckets in the bbtree.\nThe results for L2 are given in Fig. 3. Even though a representational dimensionality of the Uniform data set is only 64, it has the highest intrinsic dimensionality among all sets in Table 1 (according to the definition of Chávez et al. [10]). Thus, for the Uniform data set, no method achieved more than a 10x speedup over sequential searching without substantial quality degradation. For instance, for the VP-tree, a 160x speedup was only possible, when a retrieved object was a 40-th nearest neighbor (on average) instead of the first one. The multi-probe LSH can be twice as fast as the VP-tree at the expense of having a 4.7x larger index. All the remaining data sets have low or moderate intrinsic dimensionality (smaller than eight). For example, the SIFT signatures have the representational dimensionality of 1111, but the intrinsic dimensionality is only four. For data sets with low and moderate intrinsic dimensionality, the VP-tree is faster than the other methods most of the time. For the data sets Colors and RCV-16 there is a two orders of magnitude difference.\nThe results for the KL-divergence and Itakura-Saito distance are summarized in Fig. 4. The bbtree is never substantially faster than the VP-tree, while being up to an order of magnitude slower for RCV-16 and RCV-256 in the case of Itakura-Saito distance. Similar to results in L2, in most cases, the VP-tree is at least as fast as other methods. Yet, for the SIFT signatures data set and the Itakura-Saito distance, permutation methods can be twice as fast.\nAdditional analysis has showed that the VP-tree is a good rank-approximation method, but it is not necessarily the best approach in terms of recall. When the VP-tree misses the nearest neighbor, it often returns the second nearest or the third nearest neighbor instead. However, when other examined methods miss the nearest neighbor, they frequently return elements that are far from the true result. For example, the multi-probe LSH may return a true nearest neighbor 50% of the time, and 50% of the time it would return the 100-th nearest neighbor. This observation about the LSH is in line with previous findings [26].\nFinally, we measured improvement in efficiency (over exhaustive search) for the bbtree, where the early termination algorithm was disabled. This was done using both the slow and the fast implementation of the KL-divergence. The results are given in Table 2. Improvements in efficiency for the case of the slower KL-divergence (reported in the first row) are consistent with those reported by Cayton [6]. The second row shows improvements in efficiency for the case of the faster KL-divergence and these improvements are substantially smaller than those reported in the first row, despite the fact that using the faster KL-divergence greatly reduces retrieval times. The reason is that the pruning algorithm of the bbtree is quite expensive. It involves computations of logarithms/exponents for coordinates of unknown vectors, and, thus, these computations cannot be deferred to index time."
    }, {
      "heading" : "4 Discussion and conclusions",
      "text" : "We evaluated two simple yet effective learning-to-prune methods and showed that the resulting approach was competitive against state-of-the-art methods in both metric and non-metric spaces. In most cases, this method provided better trade-offs between rank approximation quality and retrieval speed. For datasets with low or moderate intrinsic dimensionality, the VP-tree could be one-two orders of magnitude faster than other methods (for the same rank approximation quality). We discussed applicability of our method (a VP-tree with the learned pruner) and proved a theorem supporting the point of view that our method can be applicable to a class of non-metric distances, which includes\nthe KL-divergence. We also showed that a simple trick of pre-computing logarithms at index time substantially improved performance of existing methods (e.g., bbtree) for the studied distances.\nIt should be possible to improve over basic learning-to-prune methods (employed in this work) using: (1) a better pivot-selection strategy [31]; (2) a more sophisticated sampling strategy; (3) a more accurate (non-linear) approximation for the decision function Dπ,R(x) (see section 2.1)."
    }, {
      "heading" : "5 Acknowledgements",
      "text" : "We thank Lawrence Cayton for providing the data sets, the bbtree code, and answering our questions; Anna Belova for checking the proof of Property 1 (supplemental materials) and editing the paper."
    } ],
    "references" : [ {
      "title" : "Region proximity in metric spaces and its use for approximate similarity search",
      "author" : [ "G. Amato", "F. Rabitti", "P. Savino", "P. Zezula" ],
      "venue" : "ACM Trans. Inf. Syst.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Approximate similarity search in metric spaces using inverted files",
      "author" : [ "G. Amato", "P. Savino" ],
      "venue" : "In Proceedings of the 3rd international conference on Scalable information systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "BoostMap: A method for efficient approximate similarity rankings",
      "author" : [ "V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Multidimensional binary search trees used for associative searching",
      "author" : [ "J. Bentley" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1975
    }, {
      "title" : "Engineering efficient and effective Non-Metric Space Library",
      "author" : [ "L. Boytsov", "B. Naidan" ],
      "venue" : "Similarity Search and Applications,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Fast nearest neighbor retrieval for Bregman divergences",
      "author" : [ "L. Cayton" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "A learning framework for nearest neighbor search",
      "author" : [ "L. Cayton", "S. Dasgupta" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Effective proximity retrieval by ordering permutations",
      "author" : [ "E. Chávez", "K. Figueroa", "G. Navarro" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Probabilistic proximity search: Fighting the curse of dimensionality in metric spaces",
      "author" : [ "E. Chávez", "G. Navarro" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "Searching in metric spaces",
      "author" : [ "E. Chávez", "G. Navarro", "R. Baeza-Yates", "J.L. Marroquin" ],
      "venue" : "ACM Computing Surveys,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "Modeling LSH for performance tuning",
      "author" : [ "W. Dong", "Z. Wang", "W. Josephson", "M. Charikar", "K. Li" ],
      "venue" : "In Proceedings of the 17th ACM conference on Information and knowledge management,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Indexing inexact proximity search with distance regression in pivot space",
      "author" : [ "O. Edsberg", "M.L. Hetland" ],
      "venue" : "In Proceedings of the Third International Conference on SImilarity Search and APplications,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Use of permutation prefixes for efficient and scalable approximate similarity search",
      "author" : [ "A. Esuli" ],
      "venue" : "Inf. Process. Manage.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Effective proximity retrieval by ordering permutations",
      "author" : [ "E. Gonzalez", "K. Figueroa", "G. Navarro" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Fixed-and random-effects models in meta-analysis",
      "author" : [ "L.V. Hedges", "J.L. Vevea" ],
      "venue" : "Psychological methods,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1998
    }, {
      "title" : "Properties of embedding methods for similarity searching in metric spaces",
      "author" : [ "G. Hjaltason", "H. Samet" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Nearest neighbors in high-dimensional spaces",
      "author" : [ "P. Indyk" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Approximate nearest neighbors: towards removing the curse of dimensionality",
      "author" : [ "P. Indyk", "R. Motwani" ],
      "venue" : "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Classification with nonmetric distances: Image retrieval and class representation",
      "author" : [ "D. Jacobs", "D. Weinshall", "Y. Gdalyahu" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2000
    }, {
      "title" : "Efficient search for approximate nearest neighbor in high dimensional spaces",
      "author" : [ "E. Kushilevitz", "R. Ostrovsky", "Y. Rabani" ],
      "venue" : "In Proceedings of the 30th annual ACM symposium on Theory of computing,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "NV-Tree: An efficient disk-based index for approximate search in very large high-dimensional collections",
      "author" : [ "H. Lejsek", "F. Ásmundsson", "B. Jónsson", "L. Amsaleg" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Multi-probe LSH: efficient indexing for high-dimensional similarity search",
      "author" : [ "Q. Lv", "W. Josephson", "Z. Wang", "M. Charikar", "K. Li" ],
      "venue" : "In Proceedings of the 33rd international conference on Very large data bases,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Non-metric locality-sensitive hashing",
      "author" : [ "Y. Mu", "S. Yan" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Versatile probability-based indexing for approximate similarity search",
      "author" : [ "T. Murakami", "K. Takahashi", "S. Serita", "Y. Fujii" ],
      "venue" : "In Proceedings of the Fourth International Conference on SImilarity Search and APplications,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Indexability, concentration, and {VC} theory",
      "author" : [ "V. Pestov" ],
      "venue" : "Journal of Discrete Algorithms,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Rank-approximate nearest neighbor search: Retaining meaning and speed in high dimensions",
      "author" : [ "P. Ram", "D. Lee", "H. Ouyang", "A.G. Gray" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "Foundations of Multidimensional and Metric Data Structures",
      "author" : [ "H. Samet" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2005
    }, {
      "title" : "Satisfying general proximity similarity queries with metric trees",
      "author" : [ "J. Uhlmann" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1991
    }, {
      "title" : "Near neighbor search in metric and nonmetric space, 2005",
      "author" : [ "J. Vermorel" ],
      "venue" : "http:// hal.archives-ouvertes.fr/docs/00/03/04/85/PDF/densitree.pdf last accessed on Nov",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces",
      "author" : [ "R. Weber", "H.J. Schek", "S. Blott" ],
      "venue" : "In Proceedings of the 24th International Conference on Very Large Data Bases,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1998
    }, {
      "title" : "Data structures and algorithms for nearest neighbor search in general metric spaces",
      "author" : [ "P.N. Yianilos" ],
      "venue" : "In Proceedings of the fourth annual ACM-SIAM Symposium on Discrete algorithms,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 1993
    }, {
      "title" : "Similarity Search: The Metric Space Approach (Advances in Database Systems)",
      "author" : [ "P. Zezula", "G. Amato", "V. Dohnal", "M. Batko" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2005
    }, {
      "title" : "Approximate similarity retrieval with M-trees",
      "author" : [ "P. Zezula", "P. Savino", "G. Amato", "F. Rabitti" ],
      "venue" : "The VLDB Journal,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1998
    }, {
      "title" : "Similarity search on Bregman divergence: towards non-metric indexing",
      "author" : [ "Z. Zhang", "B.C. Ooi", "S. Parthasarathy", "A.K.H. Tung" ],
      "venue" : "Proc. VLDB Endow.,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "KL-divergence is commonly used in text analysis, image classification, and machine learning [6].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 27,
      "context" : "There are numerous data structures that speedup the NN-search by creating hierarchies of partitions at index time, most notably the VP-tree [28, 31] and the KD-tree [4].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 30,
      "context" : "There are numerous data structures that speedup the NN-search by creating hierarchies of partitions at index time, most notably the VP-tree [28, 31] and the KD-tree [4].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 3,
      "context" : "There are numerous data structures that speedup the NN-search by creating hierarchies of partitions at index time, most notably the VP-tree [28, 31] and the KD-tree [4].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 29,
      "context" : "increases, the filtering efficiency of space-partitioning methods decreases rapidly, which is known as the “curse of dimensionality” [30].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 24,
      "context" : "This happens because in high-dimensional spaces histograms of distances and 1-Lipschitz function values become concentrated [25].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 20,
      "context" : ", [21]) and, thus, trading index size for retrieval time.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 17,
      "context" : "The approximate NN-queries are less affected by the curse of the dimensionality, because it is possible to reduce retrieval time at the cost of missing some relevant answers [18, 9, 25].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "The approximate NN-queries are less affected by the curse of the dimensionality, because it is possible to reduce retrieval time at the cost of missing some relevant answers [18, 9, 25].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 24,
      "context" : "The approximate NN-queries are less affected by the curse of the dimensionality, because it is possible to reduce retrieval time at the cost of missing some relevant answers [18, 9, 25].",
      "startOffset" : 174,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "In metric spaces, it was proposed to compute the intrinsic dimensionality as the half of the squared signal to noise ratio (for the distance distribution) [10].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 17,
      "context" : "A well-known approximate NN-search method is the locality sensitive hashing (LSH) [18, 17].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : "A well-known approximate NN-search method is the locality sensitive hashing (LSH) [18, 17].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : "It is based on the idea of random projections [18, 20].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : "It is based on the idea of random projections [18, 20].",
      "startOffset" : 46,
      "endOffset" : 54
    }, {
      "referenceID" : 22,
      "context" : "There is also an extension of the LSH for symmetric non-metric distances [23].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "proposed a multi-probe version of the LSH, which can query multiple buckets of the same hash table [22].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Performance of the LSH depends on the choice of parameters, which can be tuned to fit the distribution of data [11].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 32,
      "context" : "For approximate searching it was demonstrated that an early termination strategy could rely on information about distances from typical queries to their respective nearest neighbors [33, 1].",
      "startOffset" : 182,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "For approximate searching it was demonstrated that an early termination strategy could rely on information about distances from typical queries to their respective nearest neighbors [33, 1].",
      "startOffset" : 182,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "[1] showed that density estimates can be used to approximate a pruning function in metric spaces.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "Chávez and Navarro [9] proposed to relax triangle-inequality based lower bounds for distances to potential nearest neighbors.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 18,
      "context" : "The goal is to find the mapping without large distortions of the original similarity measure [19, 16].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "The goal is to find the mapping without large distortions of the original similarity measure [19, 16].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "[19] review various projection methods and argue that such a coercion is often against the nature of a similarity measure, which can be, e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "This can be done either separately for each data point [12, 24] or by computing one global model [3].",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 23,
      "context" : "This can be done either separately for each data point [12, 24] or by computing one global model [3].",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "This can be done either separately for each data point [12, 24] or by computing one global model [3].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "There are also a number of approaches, where machine learning is used to estimate optimal parameters of classic search methods [7].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 28,
      "context" : "Vermorel [29] applied VP-trees to searching in undisclosed non-metric spaces without trying to learn a pruning function.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "[1], he proposed to visit partitions in the order defined by density estimates and employed the same early termination method as Zezula et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "Cayton [6] proposed a Bregman ball tree (bbtree), which is an exact search method for Bregman divergences.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "Additionally, Cayton [6] employed an early termination method: The algorithm can be told to stop after processing a pre-specified number of buckets.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 33,
      "context" : "[34] proposed an exact search method based on estimating the maximum distance to a bounding rectangle, but it works with left queries only.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[8] as well as Amato and Savino [2] independently proposed permutation-based search methods.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[8] as well as Amato and Savino [2] independently proposed permutation-based search methods.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 7,
      "context" : "permutation of the query and the permutation of every data point [8].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "This approach can be improved by incremental sorting [14], storing permutations as inverted files [2], or prefix trees [13].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "This approach can be improved by incremental sorting [14], storing permutations as inverted files [2], or prefix trees [13].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "This approach can be improved by incremental sorting [14], storing permutations as inverted files [2], or prefix trees [13].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 27,
      "context" : "1 Classic VP-tree In the VP-tree (also known as a ball tree) the space is partitioned with respect to a (usually randomly) chosen pivot π [28, 31].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 30,
      "context" : "1 Classic VP-tree In the VP-tree (also known as a ball tree) the space is partitioned with respect to a (usually randomly) chosen pivot π [28, 31].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "This is similar to stretching of the triangle inequality proposed by Chávez and Navarro [9].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "5 · 10(6) i ∈ {8, 16, 32, 128, 256} Cayton [6] SIFT-signat.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "KL-div, L2 1 · 10(4) 1111 Cayton [6] Uniform L2 0.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "The software (including scripts that can be used to reproduce our results) is available online, as a part of the Non-Metric Space Library2 [5].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 14,
      "context" : "This procedure is repeated 5 times (for each data sets) and results are aggregated using a classic fixedeffect model [15].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "The effectiveness was measured using a simple rank error metric proposed by Cayton [6].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "• The permutation method with incremental sorting [14].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "• The permutation prefix index, where permutation profiles are stored in a prefix tree of limited depth [13].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "• The bbtree [6], which is designed for Bregman divergences, and, thus, it was not used with L2.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 25,
      "context" : "This observation about the LSH is in line with previous findings [26].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "Improvements in efficiency for the case of the slower KL-divergence (reported in the first row) are consistent with those reported by Cayton [6].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "It should be possible to improve over basic learning-to-prune methods (employed in this work) using: (1) a better pivot-selection strategy [31]; (2) a more sophisticated sampling strategy; (3) a more accurate (non-linear) approximation for the decision function Dπ,R(x) (see section 2.",
      "startOffset" : 139,
      "endOffset" : 143
    } ],
    "year" : 2013,
    "abstractText" : "Our focus is on approximate nearest neighbor retrieval in metric and non-metric spaces. We employ a VP-tree and explore two simple yet effective learning-toprune approaches: density estimation through sampling and “stretching” of the triangle inequality. Both methods are evaluated using data sets with metric (Euclidean) and non-metric (KL-divergence and Itakura-Saito) distance functions. Conditions on spaces where the VP-tree is applicable are discussed. The VP-tree with a learned pruner is compared against the recently proposed state-of-the-art approaches: the bbtree, the multi-probe locality sensitive hashing (LSH), and permutation methods. Our method was competitive against state-of-the-art methods and, in most cases, was more efficient for the same rank approximation quality.",
    "creator" : null
  }
}