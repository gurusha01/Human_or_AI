{
  "name" : "37f0e884fbad9667e38940169d0a3c95.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Linear Convergence with Condition Number Independent Access of Full Gradients",
    "authors" : [ "Lijun Zhang", "Mehrdad Mahdavi", "Rong Jin" ],
    "emails" : [ "zhanglij@msu.edu", "mahdavim@msu.edu", "rongjin@msu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "For smooth and strongly convex optimizations, the optimal iteration complexity of the gradient-based algorithm is O( √ κ log 1/ǫ), where κ is the condition number. In the case that the optimization problem is ill-conditioned, we need to evaluate a large number of full gradients, which could be computationally expensive. In this paper, we propose to remove the dependence on the condition number by allowing the algorithm to access stochastic gradients of the objective function. To this end, we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that is able to utilize two kinds of gradients. A distinctive step in EMGD is the mixed gradient descent, where we use a combination of the full and stochastic gradients to update the intermediate solution. Theoretical analysis shows that EMGD is able to find an ǫ-optimal solution by computing O(log 1/ǫ) full gradients and O(κ2 log 1/ǫ) stochastic gradients."
    }, {
      "heading" : "1 Introduction",
      "text" : "Convex optimization has become a tool central to many areas of engineering and applied sciences, such as signal processing [20] and machine learning [24]. The problem of convex optimization is typically given as\nmin w∈W F (w),\nwhere W is a convex domain, and F (·) is a convex function. In most cases, the optimization algorithm for solving the above problem is an iterative process, and the convergence rate is characterized by the iteration complexity, i.e., the number of iterations needed to find an ǫ-optimal solution [3,17]. In this study, we focus on first order methods, where we only have the access to the (stochastic) gradient of the objective function. For most convex optimization problems, the iteration complexity of an optimization algorithm depends on the following two factors.\n1. The analytical properties of the objective function. For example, is F (·) smooth or strongly convex?\n2. The information that can be elicited about the objective function. For example, do we have access to the full gradient or the stochastic gradient of F (·)?\nThe optimal iteration complexities for some popular combinations of the above two factors are summarized in Table 1 and elaborated in the related work section. We observe that when the objective function is smooth (and strongly convex), the convergence rate for full gradients is much faster than that for stochastic gradients. On the other hand, the evaluation of a stochastic gradient is usually significantly more efficient than that of a full gradient. Thus, replacing full gradients with stochastic gradients essentially trades the number of iterations with a low computational cost per iteration.\nIn this work, we consider the case when the objective function is both smooth and strongly convex, where the optimal iteration complexity is O( √ κ log 1ǫ ) if the optimization method is first order and has access to the full gradients [17]. For the optimization problems that are ill-conditioned, the condition number κ can be very large, leading to many evaluations of full gradients, an operation that is computationally expensive for large data sets. To reduce the computational cost, we are interested in the possibility of making the number of full gradients required independent from κ. Although the O( √ κ log 1ǫ ) rate is in general not improvable for any first order method, we bypass this difficulty by allowing the algorithm to have access to both full and stochastic gradients. Our objective is to reduce the iteration complexity from O( √ κ log 1ǫ ) to O(log 1 ǫ ) by replacing most of the evaluations of full gradients with the evaluations of stochastic gradients. Under the assumption that stochastic gradients can be computed efficiently, this tradeoff could lead to a significant improvement in computational efficiency.\nTo this end, we developed a novel optimization algorithm named Epoch Mixed Gradient Descent (EMGD). It divides the optimization process into a sequence of epochs, an idea that is borrowed from the epoch gradient descent [9]. At each epoch, the proposed algorithm performs mixed gradient descent by evaluating one full gradient and O(κ2) stochastic gradients. It achieves a constant reduction in the optimization error for every epoch, leading to a linear convergence rate. Our analysis shows that EMGD is able to find an ǫ-optimal solution by computing O(log 1ǫ ) full gradients and O(κ2 log 1ǫ ) stochastic gradients. In other words, with the help of stochastic gradients, the number\nof full gradients required is reduced from O( √ κ log 1ǫ ) to O(log 1 ǫ ), independent from the condition number."
    }, {
      "heading" : "2 Related Work",
      "text" : "During the last three decades, there have been significant advances in convex optimization [3,15,17]. In this section, we provide a brief review of the first order optimization methods.\nWe first discuss deterministic optimization, where the gradient of the objective function is available. For the general convex and Lipschitz continuous optimization problem, the iteration complexity of gradient (subgradient) descent is O( 1ǫ2 ), which is optimal up to constant factors [15]. When the objective function is convex and smooth, the optimal optimization scheme is the accelerated gradient descent developed by Nesterov, whose iteration complexity is O( L√\nǫ ) [16, 18]. With slight\nmodifications, the accelerated gradient descent algorithm can also be applied to optimize the smooth and strongly convex objective function, whose iteration complexity is O( √ κ log 1ǫ ) and is in general not improvable [17, 19]. The objective of our work is to reduce the number of accesses to the full gradients by exploiting the availability of stochastic gradients.\nIn stochastic optimization, we have access to the stochastic gradient, which is an unbiased estimate of the full gradient [14]. Similar to the case in deterministic optimization, if the objective function is convex and Lipschitz continuous, stochastic gradient (subgradient) descent is the optimal algorithm and the iteration complexity is also O( 1ǫ2 ) [14, 15]. When the objective function is λ-strongly convex, the algorithms proposed in very recent works [9, 10, 21, 26] achieve the optimal O( 1λǫ ) iteration complexity [1]. Since the convergence rate of stochastic optimization is dominated by the randomness in the gradient [6,11], smoothness usually does not lead to a faster convergence rate for stochastic optimization. A variant of stochastic optimization is the “semi-stochastic” approximation, which interleave stochastic gradient descent and full gradient descent [12]. In the strongly convex case, if the stochastic gradients are taken at a decreasing rate, the convergence rate can be improved to approach O( 1\nλ √ ǫ ) [13].\nFrom the above discussion, we observe that the iteration complexity in stochastic optimization is polynomial in 1ǫ , making it difficult to find high-precision solutions. However, when the objective function is strongly convex and can be written as a sum of a finite number of functions, i.e.,\nF (w) = 1\nn\nn∑\ni=1\nfi(w), (1)\nwhere each fi(·) is smooth, the iteration complexity of some specific algorithms may exhibit a logarithmic dependence on 1ǫ , i.e., a linear convergence rate. The two very recent works are the stochastic average gradient (SAG) [22], whose iteration complexity is O(n log 1ǫ ), provided n ≥ 8κ, and the stochastic dual coordinate ascent (SDCA) [23], whose iteration complexity is O((n + κ) log 1ǫ ). 1 Under approximate conditions, the incremental gradient method [2] and the hybrid method [5] can also minimize the function in (1) with a linear convergence rate. But those algorithms usually treat one pass of all fi’s (or the subset of fi’s) as one iteration, and thus have high computational cost per iteration."
    }, {
      "heading" : "3 Epoch Mixed Gradient Descent",
      "text" : ""
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "In this paper, we assume there exist two oracles.\n1. The first one is a gradient oracle Og , which for a given input point w ∈ W returns the gradient ∇F (w), that is, Og(w) = ∇F (w). 2. The second one is a function oracle Of , each call of which returns a random function f(·),\nsuch that F (w) = Ef [f(w)], ∀w ∈ W, and f(·) is L-smooth, that is,\n‖∇f(w)−∇f(w′)‖ ≤ L‖w −w′‖, ∀w,w′ ∈ W. (2)\nAlthough we do not define a stochastic gradient oracle directly, the function oracle Of allows us to evaluate the stochastic gradient of F (·) at any point w ∈ W . Notice that the assumption about the function oracle Of implies that the objective function F (·) is also L-smooth. Since ∇F (w) = Ef∇f(w), by Jensen’s inequality, we have\n‖∇F (w)−∇F (w′)‖ ≤ Ef‖∇f(w)−∇f(w′)‖ (2)\n≤ L‖w −w′‖, ∀w,w′ ∈ W. (3) Besides, we further assume F (·) is λ-strongly convex, that is,\n‖∇F (w)−∇F (w′)‖ ≥ λ‖w −w′‖, ∀w,w′ ∈ W. (4) From (3) and (4), it is obvious that L ≥ λ. The condition number κ is defined as the ratio between them. i.e., κ = L/λ ≥ 1."
    }, {
      "heading" : "3.2 The Algorithm",
      "text" : "The detailed steps of the proposed Epoch Mixed Gradient Descent (EMGD) are shown in Algorithm 1, where we use the superscript for the index of epoches, and the subscript for the index of iterations at each epoch. We denote by B(x; r) the ℓ2 ball of radius r around the point x. Similar to the epoch gradient descent (EGD) [9], we divided the optimization process into a sequence of epochs (step 3 to step 10). While the number of accesses to the gradient oracle in EGD increases exponentially over the epoches, the number of accesses to the two oracles in EMGD is fixed.\n1In order to apply SDCA, we need to assume each function fi is λ-strongly convex, so that we can rewrite fi(w) as gi(w) +\nλ 2 ‖w‖2, where gi(w) = fi(w)− λ 2 ‖w‖2 is convex.\nAlgorithm 1 Epoch Mixed Gradient Descent (EMGD)\nInput: step size η, the initial domain size ∆1, the number of iterations T per epoch, and the number of epoches m\n1: Initialize w̄1 = 0 2: for k = 1, . . . ,m do 3: Set wk1 = w̄ k 4: Call the gradient oracle Og to obtain ∇F (w̄k) 5: for t = 1, . . . , T do 6: Call the function oracle Of to obtain a random function fkt (·) 7: Compute the mixed gradient as\ng̃kt = ∇F (w̄k) +∇fkt (wkt )−∇fkt (w̄k)\n8: Update the solution by\nwkt+1 = argmin w∈W∩B(w̄k;∆k)\nη〈w −wkt , g̃kt 〉+ 1\n2 ‖w −wkt ‖2\n9: end for 10: Set w̄k+1 = 1T+1 ∑T+1 t=1 w k t and ∆ k+1 = ∆k/ √ 2 11: end for Return w̄m+1\nAt the beginning of each epoch, we initialize the solution wk1 to be the average solution w̄ k obtained from the last epoch, and then call the gradient oracle Og to obtain ∇F (w̄k). At each iteration t of epoch k, we call the function oracle Of to obtain a random function fkt (·) and define the mixed gradient at the current solution wkt as\ng̃kt = ∇F (w̄k) +∇fkt (wkt )−∇fkt (w̄k),\nwhich involves both the full gradient and the stochastic gradient. The mixed gradient can be divided into two parts: the deterministic part ∇F (w̄k) and the stochastic part ∇fkt (wkt ) −∇fkt (w̄k). Due to the smoothness property of fkt (·) and the shrinkage of the domain size, the norm of the stochastic part is well bounded, which is the reason why our algorithm can achieve linear convergence.\nBased on the mixed gradient, we update wkt by a gradient mapping over a shrinking domain (i.e., W ∩B(w̄k; ∆k)) in step 8. Since the updating is similar to the standard gradient descent except for the domain constraint, we refer to it as mixed gradient descent for short. At the end of the iteration for epoch k, we compute the average value of T + 1 solutions, instead of T solutions, and update the domain size by reducing a factor of √ 2."
    }, {
      "heading" : "3.3 The Convergence Rate",
      "text" : "The following theorem shows the convergence rate of the proposed algorithm.\nTheorem 1. Assume\nδ ≤ e−1/2, T ≥ 1152L 2\nλ2 ln\n1 δ , and ∆1 ≥ max\n√ 2\nλ (F (0)− F (w∗)). (5)\nSet η = 1/[L √ T ]. Let w̄m+1 be the solution returned by Algorithm 1 after m epoches that has m accesses to oracle Og and mT accesses to oracle Of . Then, with a probability at least 1−mδ, we have\nF (w̄m+1)− F (w∗) ≤ λ[∆ 1]2 2m+1 , and ‖w̄m+1 −w∗‖2 ≤ [∆ 1]2 2m .\nTheorem 1 immediately implies that EMGD is able to achieve an ǫ optimization error by computing O(log 1ǫ ) full gradients and O(κ 2 log 1ǫ ) stochastic gradients."
    }, {
      "heading" : "3.4 Comparisons",
      "text" : "Compared to the optimization algorithms that only rely on full gradients [17], the number of full gradients needed in EMGD is O(log 1ǫ ) instead of O( √ κ log 1ǫ ). Compared to the optimization algorithms that only rely on stochastic gradients [9,10,21], EMGD is more efficient since it achieves a linear convergence rate.\nThe proposed EMGD algorithm can also be applied to the special optimization problem considered in [22, 23], where F (w) = 1n ∑n i=1 fi(w). To make quantitative comparisons, let’s assume the full gradient is n times more expensive to compute than the stochastic gradient. Table 2 lists the computational complexities of the algorithms that enjoy linear convergence. As can be seen, the computational complexity of EMGD is lower than Nesterov’s algorithm [17] as long as the condition number κ ≤ n2/3, the complexity of SAG [22] is lower than Nesterov’s algorithm if κ ≤ n/8, and the complexity of SDCA [23] is lower than Nesterov’s algorithm if κ ≤ n2.2 The complexity of EMGD is on the same order as SAG and SDCA when κ ≤ n1/2, but higher in other cases. Thus, in terms of computational cost, EMGD may not be the best one, but it has advantages in other aspects.\n1. Unlike SAG and SDCA that only work for unconstrained optimization problem, the proposed algorithm works for both constrained and unconstrained optimization problems, provided that the constrained problem in Step 8 can be solved efficiently.\n2. Unlike the SAG and SDCA that require an Ω(n) storage space, the proposed algorithm only requires the storage space of Ω(d), where d is the dimension of w.\n3. The only step in Algorithm 1 that has dependence on n is step 4 for computing the gradient ∇F (w̄k). By utilizing distributed computing, the running time of this step can be reduced to O(n/k), where k is the number of computers, and the convergence rate remains the same. For SAG and SDCA , it is unclear whether they can reduce the running time without affecting the convergence rate.\n4. The linear convergence of SAG and SDCA only holds in expectation, whereas the linear convergence of EMGD holds with a high probability, which is much stronger."
    }, {
      "heading" : "4 The Analysis",
      "text" : "In the proof, we frequently use the following property of strongly convex functions [9]. Lemma 1. Let f(x) be a λ-strongly convex function over the domain X , and x∗ = argmin\nx∈X f(x). Then, for any x ∈ X , we have\nf(x)− f(x∗) ≥ λ 2 ‖x− x∗‖2. (6)"
    }, {
      "heading" : "4.1 The Main Idea",
      "text" : "The Proof of Theorem 1 is based on induction. From the assumption about ∆1 in (5), we have\nF (w̄1)− F (w∗) (5) ≤ λ[∆ 1]2\n2 , and ‖w̄1 −w∗‖2\n(5), (6)\n≤ [∆1]2,\n2In machine learning, we usually face a regularized optimization problem minw∈W 1\nn ∑n i=1 ℓ(yi;x ⊤ i w)+\nτ 2 ‖w‖2, where ℓ(·; ·) is some loss function. When the norm of the data is bounded, the smoothness parameter L can be treated as a constant. The strong convexity parameter λ is lower bounded by τ . Thus, as long as τ > Ω(n−2/3), which is a reasonable scenario [25], we have κ < O(n2/3), indicating our proposed EMGD can be applied.\nwhich means Theorem 1 is true for m = 0. Suppose Theorem 1 is true for m = k. That is, with a probability at least 1− kδ, we have\nF (w̄k+1)− F (w∗) ≤ λ[∆ 1]2 2k+1 , and ‖w̄k+1 −w∗‖2 ≤ [∆ 1]2 2k .\nOur goal is to show that after running the k+1-th epoch, with a probability at least 1− (k+1)δ, we have\nF (w̄k+2)− F (w∗) ≤ λ[∆ 1]2 2k+2 , and ‖w̄k+2 −w∗‖2 ≤ [∆ 1]2 2k+1 ."
    }, {
      "heading" : "4.2 The Details",
      "text" : "For the simplicity of presentation, we drop the index k for epoch. Let w̄ be the solution obtained from the epoch k. Given the condition\nF (w̄)− F (w∗) ≤ λ 2 ∆2, and ‖w̄ −w∗‖2 ≤ ∆2, (7)\nwe will show that after running the T iterations in one epoch, the new solution, denoted by ŵ, satisfies\nF (ŵ)− F (w∗) ≤ λ 4 ∆2, and ‖ŵ −w∗‖2 ≤ 1 2 ∆2, (8)\nwith a probability at least 1− δ. Define g = ∇F (w̄), F̂ (w) = F (w)− 〈w,g〉, and gt(w) = ft(w)− 〈w,∇ft(w̄)〉. (9) The objective function can be rewritten as\nF (w) = 〈w,g〉+ F̂ (w). (10) And the mixed gradient can be rewritten as\ng̃k = g +∇gt(wt). Then, the updating rule given in Algorithm 1 becomes\nwt+1 = argmin w∈W∩B(w̄,∆)\nη〈w −wt,g +∇gt(wt)〉+ 1\n2 ‖w −wt‖2. (11)\nNotice that the objective function in (11) is 1-strongly convex. Using the fact that w∗ ∈ W ∩ B(w̄; ∆) and Lemma 1 (with x∗ = wt+1 and x = w∗), we have\nη〈wt+1 −wt,g +∇gt(wt)〉+ 1\n2 ‖wt+1 −wt‖2\n≤η〈w∗ −wt,g +∇gt(wt)〉+ 1\n2 ‖w∗ −wt‖2 −\n1 2 ‖w∗ −wt+1‖2.\n(12)\nFor each iteration t in the current epoch, we have\nF (wt)− F (w∗) (4) ≤〈∇F (wt),wt −w∗〉 − λ\n2 ‖wt −w∗‖2\n(10) = 〈g +∇gt(wt),wt −w∗〉+ 〈 ∇F̂ (wt)−∇gt(wt),wt −w∗ 〉 − λ\n2 ‖wt −w∗‖2,\n(13)\nand\n〈g +∇gt(wt),wt −w∗〉 (12)\n≤ 〈g +∇gt(wt),wt −wt+1〉+ ‖wt −w∗‖2 2η − ‖wt+1 −w ∗‖2 2η − ‖wt −wt+1‖ 2 2η\n≤〈g,wt −wt+1〉+ ‖wt −w∗‖2 2η − ‖wt+1 −w ∗‖2 2η\n+max w\n( 〈∇gt(wt),wt −w〉 −\n‖wt −w‖2 2η\n)\n=〈g,wt −wt+1〉+ ‖wt −w∗‖2 2η − ‖wt+1 −w ∗‖2 2η + η 2 ‖∇gt(wt)‖2.\n(14)\nCombining (13) and (14), we have\nF (wt)− F (w∗)\n≤‖wt −w ∗‖2 2η − ‖wt+1 −w ∗‖2 2η − λ 2 ‖wt −w∗‖2\n+ 〈g,wt −wt+1〉+ η\n2 ‖∇gt(wt)‖2 +\n〈 ∇F̂ (wt)−∇gt(wt),wt −w∗ 〉 .\nBy adding the inequalities of all iterations, we have\nT∑\nt=1\n(F (wt)− F (w∗))\n≤‖w̄ −w ∗‖2 2η − ‖wT+1 −w ∗‖2 2η − λ 2\nT∑\nt=1\n‖wt −w∗‖2 + 〈g, w̄ −wT+1〉\n+ η\n2\nT∑\nt=1\n‖∇gt(wt)‖2\n︸ ︷︷ ︸ ,AT\n+\nT∑\nt=1 〈∇F̂ (wt)−∇gt(wt),wt −w∗〉 ︸ ︷︷ ︸\n,BT\n.\n(15)\nSince F (·) is L-smooth, we have\nF (wT+1)− F (w̄) ≤ 〈∇F (w̄),wT+1 − w̄〉+ L\n2 ‖w̄ −wT+1‖2,\nwhich implies\n〈g, w̄ −wT+1〉 ≤ F (w̄)− F (wT+1) + L\n2 ∆2\n(7) ≤ F (w∗)− F (wT+1) + λ\n2 ∆2 +\nL 2 ∆2 ≤ F (w∗)− F (wT+1) + L∆2.\n(16)\nFrom (15) and (16), we have\nT+1∑\nt=1\n(F (wt)− F (w∗)) ≤ ∆2 ( 1\n2η + L\n) + η\n2 AT +BT . (17)\nNext, we consider how to bound AT and BT . The upper bound of AT is given by\nAT =\nT∑\nt=1\n‖∇gt(wt)‖2 = T∑\nt=1\n‖∇ft(wt)−∇ft(w̄)‖2 (2) ≤ L2 T∑\nt=1\n‖wt − w̄‖2 ≤ TL2∆2. (18)\nTo bound BT , we need the Hoeffding-Azuma inequality stated below [4].\nLemma 2. Let V1, V2, . . . be a martingale difference sequence with respect to some sequence X1, X2, . . . such that Vi ∈ [Ai, Ai + ci] for some random variable Ai, measurable with respect to X1, . . . , Xi−1 and a positive constant ci. If Sn = ∑n i=1 Vi, then for any t > 0,\nPr[Sn > t] ≤ exp ( − 2t 2\n∑n i=1 c 2 i\n) .\nDefine Vt = 〈∇F̂ (wt)−∇gt(wt),wt −w∗〉, t = 1, . . . , T. Recall the definition of F̂ (·) and gt(·) in (9). Based on our assumption about the function oracle Of , it is straightforward to check that V1, . . . is a martingale difference with respect to g1, . . .. The value of Vt can be bounded by\n|Vt| ≤ ∥∥∥∇F̂ (wt)−∇gt(wt) ∥∥∥ ‖wt −w∗‖ ≤ 2∆ (‖∇F (wt)−∇F (w̄)‖+ ‖∇ft(wt)−∇ft(w̄)‖)\n(2), (3)\n≤ 4L∆‖wt − w̄‖ ≤ 4L∆2.\nFollowing Lemma 2, with a probability at least 1− δ, we have\nBT ≤ 4L∆2 √ 2T ln 1\nδ . (19)\nBy adding the inequalities in (17), (18) and (19) together, with a probability at least 1− δ, we have T+1∑\nt=1\n(F (wt)− F (w∗)) ≤ ∆2 ( 1\n2η + L+\nηTL2\n2 + 4L\n√ 2T ln 1\nδ\n) .\nBy choosing η = 1/[L √ T ], we have\nT+1∑\nt=1\n(F (wt)− F (w∗)) ≤ L∆2 ( √ T + 1 + 4 √ 2T ln 1\nδ\n) (5) ≤ 6L∆2 √ 2T ln 1\nδ , (20)\nwhere in the second inequality we use the condition δ ≤ e−1/2 in (5). By Jensen’s inequality, we have\nF (ŵ)− F (w∗) ≤ 1 T + 1\nT+1∑\nt=1\n(F (wt)− F (w∗)) (20) ≤ ∆2 6L √\n2 ln 1/δ√ T + 1 ,\nand therefore\n‖ŵ −w∗‖2 (6) ≤ 2 λ F (ŵ)− F (w∗) ≤ ∆2 12L\n√ 2 ln 1/δ\nλ √ T + 1 .\nThus, when\nT ≥ 1152L 2\nλ2 ln\n1 δ ,\nwith a probability at least 1− δ, we have\nF (ŵ)− F (w∗) ≤ λ 4 ∆2, and ‖ŵ −w∗‖2 ≤ 1 2 ∆2."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we consider how to reduce the number of full gradients needed for smooth and strongly convex optimization problems. Under the assumption that both the gradient and the stochastic gradient are available, a novel algorithm named Epoch Mixed Gradient Descent (EMGD) is proposed. Theoretical analysis shows that with the help of stochastic gradients, we are able to reduce the number of gradients needed from O( √ κ log 1ǫ ) to O(log 1 ǫ ). In the case that the objective function is in the form of (1), i.e., a sum of n smooth functions, EMGD has lower computational cost than the full gradient method [17], if the condition number κ ≤ n2/3. In practice, a drawback of EMGD is that it requires the condition number κ is known beforehand. We will interstage how to find a good estimation of κ in future. When the objective function is a sum of some special functions, such as the square loss (i.e., (yi − x⊤i w)2), we can estimate the condition number by sampling. In particular, the Hessian matrix estimated from a subset of functions, combined with the concentration inequalities for matrix [7], can be used to bound the eigenvalues of the true Hessian matrix and consequentially κ. Furthermore, if there exists a strongly convex regularizer in the objective function, which happens in many machine learning problems [8], the knowledge of the regularizer itself allows us to find an upper bound of κ."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partially supported by ONR Award N000141210431 and NSF (IIS-1251031)."
    } ],
    "references" : [ {
      "title" : "Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization",
      "author" : [ "A. Agarwal", "P.L. Bartlett", "P. Ravikumar", "M.J. Wainwright" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "A new class of incremental gradient methods for least squares problems",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Hybrid deterministic-stochastic methods for data fitting",
      "author" : [ "M. Friedlander", "M. Schmidt" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: a generic algorithmic framework",
      "author" : [ "S. Ghadimi", "G. Lan" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Tail bounds for all eigenvalues of a sum of random matrices",
      "author" : [ "A. Gittens", "J.A. Tropp" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "In Proceedings of the 24th Annual Conference on Learning Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Primal-dual subgradient methods for minimizing uniformly convex functions",
      "author" : [ "A. Juditsky", "Y. Nesterov" ],
      "venue" : "Technical report,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "An optimal method for stochastic composite optimization",
      "author" : [ "G. Lan" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "On solutions of stochastic programming problems by descent procedures with stochastic and deterministic directions",
      "author" : [ "K. Marti" ],
      "venue" : "Methods of Operations Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1979
    }, {
      "title" : "Rates of convergence of semi-stochastic approximation procedures for solving stochastic optimization problems",
      "author" : [ "K. Marti", "E. Fuchs" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1986
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Problem complexity and method efficiency in optimization",
      "author" : [ "A. Nemirovski", "D.B. Yudin" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1983
    }, {
      "title" : "A method for unconstrained convex minimization problem with the rate of convergence O(1/k2)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Doklady AN SSSR (translated as Soviet. Math. Docl.),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1983
    }, {
      "title" : "Introductory lectures on convex optimization: a basic course, volume 87 of Applied optimization",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Core discussion papers,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "A. Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "N.L. Roux", "M. Schmidt", "F. Bach" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Optimization for Machine Learning",
      "author" : [ "S. Sra", "S. Nowozin", "S.J. Wright", "editors" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Svm soft margin classifiers: Linear programming versus quadratic programming",
      "author" : [ "Q. Wu", "D.-X. Zhou" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2005
    }, {
      "title" : "O(log T ) projections for stochastic optimization of smooth and strongly convex functions",
      "author" : [ "L. Zhang", "T. Yang", "R. Jin", "X. He" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Convex optimization has become a tool central to many areas of engineering and applied sciences, such as signal processing [20] and machine learning [24].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : ", the number of iterations needed to find an ǫ-optimal solution [3,17].",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : ", the number of iterations needed to find an ǫ-optimal solution [3,17].",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "In this work, we consider the case when the objective function is both smooth and strongly convex, where the optimal iteration complexity is O( √ κ log 1ǫ ) if the optimization method is first order and has access to the full gradients [17].",
      "startOffset" : 236,
      "endOffset" : 240
    }, {
      "referenceID" : 8,
      "context" : "It divides the optimization process into a sequence of epochs, an idea that is borrowed from the epoch gradient descent [9].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "During the last three decades, there have been significant advances in convex optimization [3,15,17].",
      "startOffset" : 91,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "During the last three decades, there have been significant advances in convex optimization [3,15,17].",
      "startOffset" : 91,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "During the last three decades, there have been significant advances in convex optimization [3,15,17].",
      "startOffset" : 91,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "For the general convex and Lipschitz continuous optimization problem, the iteration complexity of gradient (subgradient) descent is O( 1 ǫ(2) ), which is optimal up to constant factors [15].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 15,
      "context" : "When the objective function is convex and smooth, the optimal optimization scheme is the accelerated gradient descent developed by Nesterov, whose iteration complexity is O( L √ ǫ ) [16, 18].",
      "startOffset" : 182,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "When the objective function is convex and smooth, the optimal optimization scheme is the accelerated gradient descent developed by Nesterov, whose iteration complexity is O( L √ ǫ ) [16, 18].",
      "startOffset" : 182,
      "endOffset" : 190
    }, {
      "referenceID" : 16,
      "context" : "With slight modifications, the accelerated gradient descent algorithm can also be applied to optimize the smooth and strongly convex objective function, whose iteration complexity is O( √ κ log 1ǫ ) and is in general not improvable [17, 19].",
      "startOffset" : 232,
      "endOffset" : 240
    }, {
      "referenceID" : 18,
      "context" : "With slight modifications, the accelerated gradient descent algorithm can also be applied to optimize the smooth and strongly convex objective function, whose iteration complexity is O( √ κ log 1ǫ ) and is in general not improvable [17, 19].",
      "startOffset" : 232,
      "endOffset" : 240
    }, {
      "referenceID" : 13,
      "context" : "In stochastic optimization, we have access to the stochastic gradient, which is an unbiased estimate of the full gradient [14].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 13,
      "context" : "Similar to the case in deterministic optimization, if the objective function is convex and Lipschitz continuous, stochastic gradient (subgradient) descent is the optimal algorithm and the iteration complexity is also O( 1 ǫ(2) ) [14, 15].",
      "startOffset" : 229,
      "endOffset" : 237
    }, {
      "referenceID" : 14,
      "context" : "Similar to the case in deterministic optimization, if the objective function is convex and Lipschitz continuous, stochastic gradient (subgradient) descent is the optimal algorithm and the iteration complexity is also O( 1 ǫ(2) ) [14, 15].",
      "startOffset" : 229,
      "endOffset" : 237
    }, {
      "referenceID" : 8,
      "context" : "When the objective function is λ-strongly convex, the algorithms proposed in very recent works [9, 10, 21, 26] achieve the optimal O( 1 λǫ ) iteration complexity [1].",
      "startOffset" : 95,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "When the objective function is λ-strongly convex, the algorithms proposed in very recent works [9, 10, 21, 26] achieve the optimal O( 1 λǫ ) iteration complexity [1].",
      "startOffset" : 95,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : "When the objective function is λ-strongly convex, the algorithms proposed in very recent works [9, 10, 21, 26] achieve the optimal O( 1 λǫ ) iteration complexity [1].",
      "startOffset" : 95,
      "endOffset" : 110
    }, {
      "referenceID" : 24,
      "context" : "When the objective function is λ-strongly convex, the algorithms proposed in very recent works [9, 10, 21, 26] achieve the optimal O( 1 λǫ ) iteration complexity [1].",
      "startOffset" : 95,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "When the objective function is λ-strongly convex, the algorithms proposed in very recent works [9, 10, 21, 26] achieve the optimal O( 1 λǫ ) iteration complexity [1].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "Since the convergence rate of stochastic optimization is dominated by the randomness in the gradient [6,11], smoothness usually does not lead to a faster convergence rate for stochastic optimization.",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "Since the convergence rate of stochastic optimization is dominated by the randomness in the gradient [6,11], smoothness usually does not lead to a faster convergence rate for stochastic optimization.",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "A variant of stochastic optimization is the “semi-stochastic” approximation, which interleave stochastic gradient descent and full gradient descent [12].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : "In the strongly convex case, if the stochastic gradients are taken at a decreasing rate, the convergence rate can be improved to approach O( 1 λ √ ǫ ) [13].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 20,
      "context" : "The two very recent works are the stochastic average gradient (SAG) [22], whose iteration complexity is O(n log 1ǫ ), provided n ≥ 8κ, and the stochastic dual coordinate ascent (SDCA) [23], whose iteration complexity is O((n + κ) log 1ǫ ).",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "The two very recent works are the stochastic average gradient (SAG) [22], whose iteration complexity is O(n log 1ǫ ), provided n ≥ 8κ, and the stochastic dual coordinate ascent (SDCA) [23], whose iteration complexity is O((n + κ) log 1ǫ ).",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "1 Under approximate conditions, the incremental gradient method [2] and the hybrid method [5] can also minimize the function in (1) with a linear convergence rate.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "1 Under approximate conditions, the incremental gradient method [2] and the hybrid method [5] can also minimize the function in (1) with a linear convergence rate.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 8,
      "context" : "Similar to the epoch gradient descent (EGD) [9], we divided the optimization process into a sequence of epochs (step 3 to step 10).",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "Table 2: The computational complexity for minimizing 1 n ∑n i=1 fi(w) Nesterov’s algorithm [17] EMGD SAG (n ≥ 8κ) [22] SDCA [23] O (√ κn log 1ǫ ) O ( (n+ κ(2)) log 1ǫ ) O ( n log 1ǫ ) O ( (n+ κ) log 1ǫ )",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 20,
      "context" : "Table 2: The computational complexity for minimizing 1 n ∑n i=1 fi(w) Nesterov’s algorithm [17] EMGD SAG (n ≥ 8κ) [22] SDCA [23] O (√ κn log 1ǫ ) O ( (n+ κ(2)) log 1ǫ ) O ( n log 1ǫ ) O ( (n+ κ) log 1ǫ )",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "Table 2: The computational complexity for minimizing 1 n ∑n i=1 fi(w) Nesterov’s algorithm [17] EMGD SAG (n ≥ 8κ) [22] SDCA [23] O (√ κn log 1ǫ ) O ( (n+ κ(2)) log 1ǫ ) O ( n log 1ǫ ) O ( (n+ κ) log 1ǫ )",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 16,
      "context" : "Compared to the optimization algorithms that only rely on full gradients [17], the number of full gradients needed in EMGD is O(log 1ǫ ) instead of O( √ κ log 1ǫ ).",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "Compared to the optimization algorithms that only rely on stochastic gradients [9,10,21], EMGD is more efficient since it achieves a linear convergence rate.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "Compared to the optimization algorithms that only rely on stochastic gradients [9,10,21], EMGD is more efficient since it achieves a linear convergence rate.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "Compared to the optimization algorithms that only rely on stochastic gradients [9,10,21], EMGD is more efficient since it achieves a linear convergence rate.",
      "startOffset" : 79,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "The proposed EMGD algorithm can also be applied to the special optimization problem considered in [22, 23], where F (w) = 1 n ∑n i=1 fi(w).",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 21,
      "context" : "The proposed EMGD algorithm can also be applied to the special optimization problem considered in [22, 23], where F (w) = 1 n ∑n i=1 fi(w).",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "As can be seen, the computational complexity of EMGD is lower than Nesterov’s algorithm [17] as long as the condition number κ ≤ n, the complexity of SAG [22] is lower than Nesterov’s algorithm if κ ≤ n/8, and the complexity of SDCA [23] is lower than Nesterov’s algorithm if κ ≤ n(2).",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "As can be seen, the computational complexity of EMGD is lower than Nesterov’s algorithm [17] as long as the condition number κ ≤ n, the complexity of SAG [22] is lower than Nesterov’s algorithm if κ ≤ n/8, and the complexity of SDCA [23] is lower than Nesterov’s algorithm if κ ≤ n(2).",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 21,
      "context" : "As can be seen, the computational complexity of EMGD is lower than Nesterov’s algorithm [17] as long as the condition number κ ≤ n, the complexity of SAG [22] is lower than Nesterov’s algorithm if κ ≤ n/8, and the complexity of SDCA [23] is lower than Nesterov’s algorithm if κ ≤ n(2).",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 8,
      "context" : "In the proof, we frequently use the following property of strongly convex functions [9].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "Thus, as long as τ > Ω(n), which is a reasonable scenario [25], we have κ < O(n), indicating our proposed EMGD can be applied.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "To bound BT , we need the Hoeffding-Azuma inequality stated below [4].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : ", a sum of n smooth functions, EMGD has lower computational cost than the full gradient method [17], if the condition number κ ≤ n.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "In particular, the Hessian matrix estimated from a subset of functions, combined with the concentration inequalities for matrix [7], can be used to bound the eigenvalues of the true Hessian matrix and consequentially κ.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, if there exists a strongly convex regularizer in the objective function, which happens in many machine learning problems [8], the knowledge of the regularizer itself allows us to find an upper bound of κ.",
      "startOffset" : 134,
      "endOffset" : 137
    } ],
    "year" : 2013,
    "abstractText" : "For smooth and strongly convex optimizations, the optimal iteration complexity of the gradient-based algorithm is O( √ κ log 1/ǫ), where κ is the condition number. In the case that the optimization problem is ill-conditioned, we need to evaluate a large number of full gradients, which could be computationally expensive. In this paper, we propose to remove the dependence on the condition number by allowing the algorithm to access stochastic gradients of the objective function. To this end, we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that is able to utilize two kinds of gradients. A distinctive step in EMGD is the mixed gradient descent, where we use a combination of the full and stochastic gradients to update the intermediate solution. Theoretical analysis shows that EMGD is able to find an ǫ-optimal solution by computing O(log 1/ǫ) full gradients and O(κ log 1/ǫ) stochastic gradients.",
    "creator" : null
  }
}