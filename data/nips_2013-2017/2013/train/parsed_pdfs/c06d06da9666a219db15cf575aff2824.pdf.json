{
  "name" : "c06d06da9666a219db15cf575aff2824.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Chordal Markov Networks by Constraint Satisfaction",
    "authors" : [ "Jukka Corander", "Tomi Janhunen", "Jussi Rintanen", "Henrik Nyman" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Graphical models (GMs) represent the backbone of the generic statistical toolbox for encoding dependence structures in multivariate distributions. Using Markov networks or Bayesian networks conditional independencies between variables can be readily communicated and used for various computational purposes. The development of the statistical theory of GMs is largely set by the seminal works of Darroch et al. [1] and Lauritzen and Wermuth [2]. Although various approaches have been developed to generalize the theory of graphical models to allow for modeling of more complex dependence structures, Markov networks and Bayesian networks are still widely used in applications ranging from genetic mapping of diseases to machine learning and expert systems.\nBayesian learning of undirected GMs, also known as Markov random fields, from databases has attained a considerable interest, both in the statistical and computer science literature [3, 4, 5, 6, 7, 8, 9]. The cardinality and complex topology of GM space pose difficulties with respect to both the computational complexity of the learning task and the reliability of reaching representative model structures. Solutions to these problems have been proposed in earlier work. Della Pietra et al. [10] present a greedy local search algorithm for Markov network learning and apply it to discovering word morphology. Lee et al. [11] reduce the learning problem to a convex optimization problem that is solved by gradient descent. Related methods have been investigated later [12, 13]. ∗This work was funded by the Academy of Finland, project 251170. †Funded by ERC grant 239784. ‡Also affiliated with the Helsinki Institute of Information Technology, Finland. §Also affiliated with Griffith University, Brisbane, Australia. ¶This work was funded by the Foundation of Åbo Akademi University, as part of the grant for the Center of Excellence in Optimization and Systems Engineering.\nCertain types of stochastic search methods, such as Markov Chain Monte Carlo (MCMC) or simulated annealing, can be proven to be consistent with respect to the identification of a structure maximizing posterior probability [4, 5, 6, 7]. However, convergence of such methods towards the areas associated with high posterior probabilities may still be slow when the number of nodes increases [4, 6]. In addition, it is challenging to guarantee that the identified model indeed truly represents the global optimum since the consistency of MCMC estimates is by definition a limit result. To the best of our knowledge, strict constraint-based search methods have not been previously applied in learning of Markov random fields. In this article, we formalize the structure of Markov networks using constraints at a general level. This enables the development of reductions from the structure learning problem to propositional satisfiability (SAT) [14] and its generalizations such as maximum satisfiability (MAXSAT) [15], and satisfiability modulo theories (SMT) [16], as well as answer-set programming (ASP) [17]. A main novelty is the recognition of maximum weight spanning trees of the clique graph by a condition on the cardinalities of occurrences of variables in cliques and separators, which we call the balancing condition.\nThe article is structured as follows. We first review some details of Markov networks and the respective structure learning problem in Section 2. To enable efficient encodings of Markov network learning as a constraint satisfaction problem, in Section 3 we establish a new characterization of the separators of a Markov network based on a balancing condition. In Section 4, we provide a high-level description how the learning problem can be expressed using constraints and sketch the actual translations into propositional satisfiability (SAT) and its generalizations. We have implemented these translations and conducted experiments to study the performance of existing solver technology on structure learning problems in Section 5 using two widely used datasets [18]. Finally, some conclusions and possibilities for further research in this area are presented in Section 6."
    }, {
      "heading" : "2 Structure Learning for Markov Networks",
      "text" : "An undirected graph G = 〈V,E〉 consists of a set of nodes V which represents a set of random variables and a set of undirected edges E ⊆ {{n, n′} | n, n′ ∈ V and n 6= n′}. A path in a graph is a sequence of nodes such that every two consecutive nodes are connected by an edge. Two sets of nodes A and B are said to be separated by a third set of nodes D if every path between a node in A and a node in B contains at least one node in D. An undirected graph is chordal if for all paths n0, . . . nk with k ≥ 4 and n0 = nk there exist two nodes ni, nj in the path connected by an edge such that j 6= i ± 1. A clique in a graph is a set of nodes c such that every two nodes in it are connected by an edge. In addition, there may not exist a set of nodes c′ such that c ⊂ c′ and every two nodes in c′ are connected by an edge. Given the set of cliques C in a chordal graph, the set of separators S can be obtained through intersections of the cliques ordered in terms of a junction tree [19], this operation is considered thoroughly in Section 3.\nA Markov network is defined as a pair consisting of a graph G and a joint distribution PV over the variables in V . The graph specifies the dependence structure of the variables and PV factorizes according to G (see below). Given G it is possible to ascertain if two sets of variables A and B are conditionally independent given another set of variables D, due to the global Markov property\nA ⊥⊥ B | D, if D separates A from B.\nFor a Markov network with a chordal graph G, the probability of a joint outcome x factorizes as\nPV (x) = ∏ ci∈C Pci(xci)∏ si∈S Psi(xsi) .\nFollowing this factorization the marginal likelihood of a dataset X given a Markov network with a chordal graph G can be written\nP (X|G) = ∏\nci∈C Pci(Xci)∏ si∈S Psi(Xsi) .\nBy a suitable choice of prior distribution, the terms Pci(Xci) and Psi(Xsi) can be calculated analytically. Let a denote an arbitrary clique or separator containing the variables Xa whose outcome space has the cardinality k. Further, let n(j)a denote the number of occurrences where Xa = x (j) a in\nthe dataset Xa. Now assign the Dirichlet(αa1 , . . . , αak) distribution as prior over the probabilities Pa(Xa = x (j) a ) = θj , determining the distribution Pa(Xa). Now Pa(Xa) can be calculated as\nPa(Xa) = ∫ Θ k∏ j=1 (θj) n(j)a · πa(θ)dθ\nwhere πa(θ) is the density function of the Dirichlet prior distribution. By the standard properties of the Dirichlet integral, Pa(Xa) can be reduced to the form\nPa(Xa) = Γ(α)\nΓ(na + α) k∏ j=1 Γ(n (j) a + αaj ) Γ(αaj )\nwhere Γ(·) denotes the gamma function and\nα = k∑ j=1 αaj and na = k∑ j=1 n(j)a .\nWhen dealing with the marginal likelihood of a dataset it is most often necessary to use the logarithmic value logP (X|G). Introducing the notations v(ci) = logPci(Xci) the logarithmic value of the marginal likelihood can be written\nlogP (X|G) = ∑ ci∈C logPci(Xci)− ∑ si∈S logPsi(Xsi) = ∑ ci∈C v(ci)− ∑ si∈S v(si). (1)\nThe learning problem is to find a graph G that optimizes the posterior distribution\nP (G|X) = P (X|G)P (G)∑ G∈G P (X|G)P (G) .\nHere G denotes the set of all graphs under consideration and P (G) is the prior probability assigned to G. In the case where a uniform prior is used for the graphs the optimization problem reduces to finding the graph with the largest marginal likelihood."
    }, {
      "heading" : "3 Fundamental Properties and Characterization Results",
      "text" : "In this section, we point out some properties of chordal graphs and clique graphs that can be utilized in the encodings of the learning problem. In particular, we develop a characterization of maximum weight spanning trees in terms of a balancing condition on separators.\nThe separators needed for determining the score (1) of a candidate Markov network are defined as follows. Given the cliques, we can form the clique graph, in which the nodes are the cliques and there is an edge between two nodes if the corresponding cliques have a non-empty intersection. We label each of the edges with this intersection and consider the cardinality of the label as its weight. The separators are the edge labels of a maximum weight spanning tree of the clique graph. Maximum weight spanning trees of arbitrary graphs can be found in polynomial time by reducing the problem to finding minimum weight spanning trees. This reduction consists of negating all the edge weights and then using any of the polynomial time algorithms for the latter problem [20]. There may be several maximum weight spanning trees, but they induce exactly the same separators, and they only differ in terms of which pairs of cliques induce the separators.\nTo restrict the search space we can observe that a chordal graph with n nodes has at most nmaximal cliques [19]. This gives an immediate upper bound on the number of cliques chosen to build a Markov network, which can be encoded as a simple cardinality constraint."
    }, {
      "heading" : "3.1 Characterization of Maximum Weight Spanning Trees",
      "text" : "To simplify the encoding of maximum weight spanning trees (and forests) of chordal clique graphs, we introduce the notion of balanced spanning trees (respectively, forests), and show that these two concepts coincide for chordal graphs. Then separators can be identified more effectively: rather than encoding an algorithm for finding maximum-weight spanning trees as constraints, it is sufficient to select a subset of the edges of the clique graph that is acyclic and satisfies the balancing condition expressible as a cardinality constraint over occurrences of nodes in cliques and separators.\nDefinition 1 (Balancing) A spanning tree (or forest) of a clique graph is balanced if for every node n, the number of cliques containing n is one higher than the number of labeled edges containing n.\nWhile in the following we state many results for spanning trees only, they can be straightforwardly generalized to spanning forests as well (in case the Markov networks are disconnected.)\nLemma 2 For any clique graph, all its balanced spanning trees have the same weight.\nProof: This holds in general because the balancing condition requires exactly the same number of occurrences of any node in the separator edges for any balanced spanning tree, and the weight is defined as the sum of the occurrences of nodes in the edge labels.\nLemma 3 ([21, 22]) Any maximum weight spanning tree of the clique graph is a junction tree, and hence satisfies the running intersection property: for every pair of nodes c and c′, (c ∩ c′) ⊆ c′′ for all nodes c′′ on the unique path between c and c′.\nLemma 4 Let T = 〈V,ET 〉 be a maximum weight spanning tree of the clique graph 〈V,E〉 of a connected chordal graph. Then T is balanced.\nProof: We order the tree by choosing an arbitrary clique as the root and by assigning a depth to all nodes according to their distance from the root node. The rest of the proof proceeds by induction on the height of subtrees starting from the leaf nodes as the base case. The induction hypothesis says that all subtrees satisfy the balancing condition. The base cases are trivial: each leaf node (clique) trivially satisfies the balancing condition, as there are no separators to consider.\nIn the inductive cases, we have a clique c at depth d, connected to one or more subtrees rooted at neighboring cliques c1, . . . , ck at depth d + 1, with the subtrees satisfying the balancing condition. We show that the tree consisting of the clique c, the labeled edges connecting c respectively to cliques c1, . . . , ck, and the subtrees rooted at c1, . . . , ck, satisfies the balancing condition.\nFirst note that by Lemma 3, any maximum weight spanning tree of the clique graph is a junction tree and hence satisfies the running intersection property, meaning that for any two cliques c1 and c2 in the tree, every clique on the unique path connecting them includes c1 ∩ c2. We have to show that the subtree rooted at c is balanced, given that its subtrees are balanced. We show that the balancing condition is satisfied for each node separately. So let n be one of the nodes in the original graph. Now each of the subtrees rooted at some ci has either 0 occurrences of n, or ki ≤ 1 occurrences in the cliques and ki−1 occurrences in the edge labels, because by the induction hypothesis the balancing condition is satisfied. Four cases arise:\n1. The node n does not occur in any of the subtrees. Now the balancing condition is trivially satisfied for the subtree rooted at c, because n either does not occur in c, or it occurs in c but does not occur in the label of any of the edges to the subtrees.\n2. The node n occurs in more than one subtree. Since any maximum weight spanning tree is a junction tree by Lemma 3, n must occur also in c and in the labels of the edges between c and the cliques in which the subtrees with n are rooted. Let s1, . . . , sj be the numbers of occurrences of n in the edge labels in the subtrees with at least one occurrence of n, and t1, . . . , tj the numbers of occurrences of n in the cliques in the same subtrees. By the induction hypothesis, these subtrees are balanced, and hence ti − si = 1 for all i ∈ {1, . . . , j}. The subtree rooted at c now has 1 + ∑k i=1 ti occurrences of n in the nodes\n(once in c itself and then the subtrees) and j + ∑j\ni=1 si occurrences in the edge labels, where the j occurrences are in the edges between c and the j subtrees.\nWe establish the balancing condition through a sequence of equalities. The first and the last expression are the two sides of the condition.\n(1 + ∑j i=1 ti)− (j + ∑k i=1 si)\n= 1− j + ∑j\ni=1(ti − si) reordering the terms = 1− j + j since ti − si = 1 for every subtree = 1\nHence also the subtree rooted at c is balanced.\n3. The node n occurs in one subtree and in c. Let i be the index of the subtree in which n occurs. Since any maximum weight spanning tree is a junction tree by Lemma 3, n must occur also in the clique ci. Hence n occurs in the label of the edge from ci to c. Since the subtree is balanced, the new graph obtained by adding the clique c and the edge with a label containing n is also balanced. Further, adding all the other subtrees that do not contain n will not affect the balancing of n.\n4. The node n occurs in one subtree but not in c. Since there are n occurrences of n in any of the other subtrees, in c, or in the edge labels between c and any of the subtrees, the balancing condition holds.\nThis completes the induction step and consequently, the whole spanning tree is balanced.\nLemma 5 Assume T = 〈V,EB〉 is a spanning tree of the clique graph GC = 〈V,E〉 of a chordal graph that satisfies the balancing condition. Then T is a maximum weight spanning tree of GC .\nProof: Let TM be one of the spanning trees of GC with the maximum weight w. By Lemma 4, this maximum weight spanning tree is balanced. By Lemma 2, T has the same weight w as TM . Hence also T is a maximum weight spanning tree of GC .\nTheorem 6 For any clique graph of a chordal graph, any of its subgraphs is a maximum weight spanning tree if and only if it is a balanced acyclic subgraph."
    }, {
      "heading" : "4 Representation as Constraints",
      "text" : "In this section we first show how the structure learning problem of Markov networks is cast as a constraint satisfaction problem, and then formalize it concretely in the language of propositional logic, as directly supported by SMT solvers and easily translatable into conjunctive normal form as used by SAT and MAXSAT solvers. In ASP slightly different rule-based formulations are used.\nThe learning problem is formalized as follows. The goal is to find a balanced spanning tree (cf. Definition 1) for a set C of cliques forming a Markov network and the set S of separators induced by the tree structure. In addition, C and S are supposed to be optimal in the sense of (1), i.e., the overall score v(C, S) = ∑ c∈C v(c) − ∑ s∈S v(s) is maximized. The individual score v(c) for any set of nodes c describes how well it reflects the interdependencies of the variables in c in the data.\nDefinition 7 Let V be a set of nodes representing random variables and v : 2V → R a scoring function. A solution to the Markov network learning problem is a set of cliques C = {c1, . . . , ck} satisfying the following requirements viewed as abstract constraints:\n1. Every node is included in at least one of the chosen cliques in C, i.e., ⋃k\ni=1 ci = V .\n2. Cliques in C are maximal, i.e.,\n(a) for every c, c′ ∈ C, if c ⊆ c′, then c = c′; and (b) for every c ⊆ V , if edges(c) ⊆ ⋃ c′∈C edges(c ′), then c ⊆ c′ for some c′ ∈ C\nwhere edges(c) = {{n, n′} ⊆ c | n 6= n′} is defined for each c ⊆ V . 3. The graph 〈V,E〉 with the set of edges E = ⋃\nc∈C edges(c) is chordal.\n4. The set C has a balanced spanning tree labeled by a set of separators S = {s1, . . . , sl}.\nMoreover, the solution is optimal if it maximizes the overall score v(C, S).\nThe encodings of basic graph properties (conditions 1 and 2 above) are presented Section 4.1. The more complex properties (3 and 4) are addressed in Sections 4.2 and 4.3."
    }, {
      "heading" : "4.1 Graph Properties",
      "text" : "We assume that clique candidates – which are the non-empty subsets of V – are indexed from 1 to 2|V |. We often identify a clique with its index. Each clique candidate c ⊆ V has an associated score v(c). To encode the search space for Markov networks, we introduce, for every clique candidate c, a propositional variable xc denoting that c is part of the learned network. We also introduce propositional variables en,m that represent edges {n,m} that are in at least one chosen clique.1\nTo formalize condition 1 of Definition 7, for every node n we have the constraint\nxc1 ∨ · · · ∨ xck (2)\nwhere c1, . . . , ck are all cliques c with n ∈ c. To satisfy the maximality condition 2(a), we require that if a clique is chosen, then at least one edge in each of its super-cliques is not chosen. We first make the edges of the chosen cliques explicit by the next constraint for all {n,m} ⊆ V and cliques c1, . . . , ck such that {n,m} ⊆ ci.\nen,m ↔ (xc1 ∨ · · · ∨ xck) (3)\nThen for every clique candidate c = {n1, . . . , nk} and every node n ∈ V \\c we have the constraint\nxc → (¬en1,n ∨ · · · ∨ ¬enk,n) (4)\nwhere en1,n, . . . , enk,n represent all additional edges that would turn c ∪ {n} into a clique. For each pair of clique candidates c and c′ such that c ⊂ c′, ¬xc ∨ ¬xc′ is a logical consequence of the constraints (4). They are useful for strengthening the inferences made by SAT solvers.\nFor condition 2(b) we use propositional variables zc which mean that either c or one of its supercliques is chosen, and propositional variables wc which mean that all edges of c are chosen. For 2-element cliques c = {n1, n2} we have\nwc ↔ en1,n2 . (5)\nFor larger cliques c we have wc ↔ wc1 ∧ · · · ∧ wck (6)\nwhere c1, . . . , ck are all subcliques of c with one less node than c. Hence wc is true iff all edges of c are chosen. If all edges of a clique are chosen, then the clique itself or one of its super-cliques must be chosen. If c1, . . . , ck are all cliques that extend c by one node, this is encoded as follows.\nwc → zc (7) zc ↔ (xc ∨ zc1 ∨ · · · ∨ zck) (8)"
    }, {
      "heading" : "4.2 Chordality",
      "text" : "We use a straightforward encoding of the chordality condition (3) of Definition 7. The idea is to generate constraints corresponding to every k ≥ 4 element subset S = {n1, . . . , nk} of V . Let us consider all cycles these nodes could form in the graph 〈V,E〉 of condition 3 in Definition 7. A cycle starts from a given node, goes through all other nodes, with (undirected) edges between two consecutive nodes, and ends in the starting node. The number of constraints can be reduced by two observations. First, the same cycle could be generated from different starting nodes, e.g., cycles n1, n2, n3, n4, n1 and n2, n3, n4, n1, n2 are the same. Second, generating the same cycle in two opposite directions, as in n1, n2, n3, n4, n1 and n1, n4, n3, n2, n1, is unnecessary. To avoid\n1As the edges are undirected, we limit to en,m such that the ordering of n and m according to some fixed ordering is increasing, i.e., n < m. Under this assumption, em,n for n < m denotes en,m.\nredundant cycle constraints, we arbitrarily fix the starting node and require that the index of the second node in the cycle is lower than the index of the second last node. These restrictions guarantee that every cycle associated with S is considered exactly once. Now, the chordality constraint says that if there is an edge between every pair of consecutive nodes in n1, . . . , nk, n1, then there also has to be an edge between at least one pair of two non-consecutive nodes. In the case k = 4, for instance, this leads to formulas of the form\nen1,n2 ∧ en2,n3 ∧ en3,n4 ∧ en4,n1 → en1,n3 ∨ en2,n4 . (9) This encoding of chordality constraints is exponential in |V | and therefore not scalable to large numbers of nodes. However, the datasets considered in Section 5 have only 6 or 8 variables, and in these cases the exponentiality is not an issue."
    }, {
      "heading" : "4.3 Separators",
      "text" : "Separators for pairs c and c′ of clique candidates can be formalized as propositional variables sc,c′ , meaning that c ∩ c′ is a separator and there is an edge in the spanning tree between c and c′ labeled by c ∩ c′. The corresponding constraint is\nsc,c′ → xc ∧ xc′ . (10) The lack of the converse implication formalizes the choice of the spanning tree, i.e., sc,c′ can be false even if xc and xc′ are true. The remaining constraints on separators fall into two cases.\nFirst, we have cardinality constraints encoding the balancing condition (cf. Section 3.1): each variable occurs in the chosen cliques one more time than it occurs in the separators which label the spanning tree. Cardinality constraints are natively supported by some constraint solvers, or they can be reduced to Boolean constraints [23]. Second, the graph formed by the cliques with the separators as edges must be acyclic. We encode this through an inductive definition of trees: repeatedly remove leaf nodes, i.e., nodes with at most one neighbor, until all nodes have been removed. When applying this definition to a cyclic graph, some nodes will remain in the end. We define the leaf level for each node. A node is a level 0 leaf iff it has 0 or 1 neighbors in the graph. A node is a level l + 1 leaf iff all its neighbors except possibly one are level j ≤ l leaves. This definition is directly expressible as Boolean constraints. A graph with m nodes is acyclic iff all its nodes are level bm2 c leaves."
    }, {
      "heading" : "5 Experimental Evaluation",
      "text" : "The constraints described in Section 4 can be alternatively expressed as MAXSAT, SMT, or ASP problems. We have used respective solvers in computing optimal Markov networks for datasets from the literature. The test runs were with an Intel Xeon E3-1230 CPU running at 3.20 GHz.\n1. For the MAXSAT encodings, we tried out SAT4J (version 2.3.2) [24] and PWBO (version 2.2) [25]. The latter was run in its default configuration as well as in the UB configuration.\n2. For SMT, we used the OPTIMATHSAT solver (version 5) [26]. 3. For ASP, we used the CLASP (version 2.1.3) [27] and HCLASP (also v. 2.1.3) [28]\nsolvers. The latter allows declaratively specifying search heuristics. We also tried the LP2NORMAL tool that reduces cardinality constraints to more basic constraints [29].\nWe consider two datasets, one containing risk factors in heart diseases and the other variables related to economical behavior [18], to be abbreviated by heart and econ in the sequel. For heart, the globally optimal network has been verified via (expensive) exhaustive enumeration. For econ, however, exhaustive enumeration is impractical due to the extremely large search space, and consequently the optimality of the Markov network found by stochastic search in [4] had been open until now. For both datasets, we computed the respective score file that specifies the score of each clique candidate, i.e., the log-value of its potential function, and the list of variables involved in that clique. The score files were then translated to be run with the different solvers. The MAXSAT and ASP solvers only support integer scores obtained by multiplying the original scores by 1000 and rounding. The SMT solver OptiMathSAT used the original floating point scores. The results are given in Table 1.\nThe heart data involves 6 variables giving rise to 26 = 64 clique candidates in total and a search space of 215 undirected networks of which a subset are decomposable. For instance, the ASP solver\nHCLASP traversed a considerably smaller search space that consisted of 26651 (partial) networks. This illustrates the power of branch-and-bound type algorithms behind the solvers and their ability to prune the search space. On the other hand, the econ dataset is based on 8 variables giving rise to a much larger search space 228. We were able to solve this instance optimally with one solver only, HCLASP, which allows for a more refined control of the search heuristic: we forced HCLASP to try cliques in an ascending order by size, with greatest cliques first. This allowed us to find the global optimum in about 14 hours, after which 3 days is spent on the proof of optimality."
    }, {
      "heading" : "6 Conclusions",
      "text" : "Boolean constraint methods appear not to have been earlier applied to learning of undirected Markov networks. We have introduced a generic approach in which the learning problem is expressed in terms of constraints on variables that determine the structure of the learned network. The related problem of structure learning of Bayesian networks has been addressed by general-purpose combinatorial search methods, including MAXSAT [30] and a constraint-programming solver with a linear-programming solver as a subprocedure [31, 32]. We introduced explicit translations of the generic constraints to the languages of MAXSAT, SMT and ASP, and demonstrated their use through existing solver technology. Our method thus opens up a novel venue of research to further develop and optimize the use of such technology for network learning. A wide variety of possibilities does exist also for using these methods in combination with stochastic or heuristic search."
    } ],
    "references" : [ {
      "title" : "Markov fields and log-linear interaction models for contingency tables",
      "author" : [ "J.N. Darroch", "Steffen L. Lauritzen", "T.P. Speed" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1980
    }, {
      "title" : "Graphical models for associations between variables, some of which are qualitative and some quantitative",
      "author" : [ "Steffen L. Lauritzen", "Nanny Wermuth" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1989
    }, {
      "title" : "Bayesian graphical model determination using decision theory",
      "author" : [ "Jukka Corander" ],
      "venue" : "Journal of Multivariate Analysis,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Parallel interacting MCMC for learning of topologies of graphical models",
      "author" : [ "Jukka Corander", "Magnus Ekdahl", "Timo Koski" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Markov chain Monte Carlo model determination for hierarchical and graphical log-linear models",
      "author" : [ "Petros Dellaportas", "Jonathan J. Forster" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "Improving Markov chain Monte Carlo model search for data mining",
      "author" : [ "Paolo Giudici", "Robert Castello" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Decomposable graphical Gaussian model determination",
      "author" : [ "Paolo Giudici", "Peter J. Green" ],
      "venue" : "Biometrika, 86:785–801,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1999
    }, {
      "title" : "Exact Bayesian structure discovery in Bayesian networks",
      "author" : [ "Mikko Koivisto", "Kismat Sood" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Model selection and accounting for model uncertainty in graphical models using Occam’s window",
      "author" : [ "David Madigan", "Adrian E. Raftery" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1994
    }, {
      "title" : "Inducing features of random fields",
      "author" : [ "Stephen Della Pietra", "Vincent Della Pietra", "John Lafferty" ],
      "venue" : "IEEE Trans. on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "Efficient structure learning of Markov networks using L1-regularization",
      "author" : [ "Su-In Lee", "Varun Ganapathi", "Daphne Koller" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Learning graphical model structure using L1-regularization paths",
      "author" : [ "M. Schmidt", "A. Niculescu-Mizil", "K. Murphy" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Estimation of sparse binary pairwise Markov networks using pseudo-likelihoods",
      "author" : [ "Holger Höfling", "Robert Tibshirani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Handbook of Satisfiability",
      "author" : [ "Armin Biere", "Marijn J.H. Heule", "Hans van Maaren", "Toby Walsh", "editors" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "MaxSAT, Hard and Soft Constraints, chapter 19, pages 613–631",
      "author" : [ "Chu Min Li", "Felip Manyà" ],
      "venue" : "In Biere et al",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Satisfiability Modulo Theories, chapter 26, pages 825–885",
      "author" : [ "Clark Barrett", "Roberto Sebastiani", "Sanjit A. Seshia", "Cesare Tinelli" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Answer set programming at a glance",
      "author" : [ "Gerhard Brewka", "Thomas Eiter", "Miroslaw Truszczynski" ],
      "venue" : "Commun. ACM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Graphical Models in Applied Multivariate Statistics",
      "author" : [ "Joe Whittaker" ],
      "venue" : "Wiley Publishing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1990
    }, {
      "title" : "Algorithmic Graph Theory and Perfect Graphs",
      "author" : [ "Martin C. Golumbic" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1980
    }, {
      "title" : "On the history of the minimum spanning tree problem",
      "author" : [ "Ronald L. Graham", "Pavol Hell" ],
      "venue" : "Annals of the History of Computing,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1985
    }, {
      "title" : "On the tree representation of chordal graphs",
      "author" : [ "Yukio Shibata" ],
      "venue" : "Journal of Graph Theory,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1988
    }, {
      "title" : "Optimal junction trees",
      "author" : [ "Finn V. Jensen", "Frank Jensen" ],
      "venue" : "In Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1994
    }, {
      "title" : "Towards an optimal CNF encoding of Boolean cardinality constraints. In Principles and Practice of Constraint Programming – CP 2005, number 3709 in Lecture",
      "author" : [ "Carsten Sinz" ],
      "venue" : "Notes in Computer Science,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "The Sat4j library, release 2.2 system description",
      "author" : [ "Daniel Le Berre", "Anne Parrain" ],
      "venue" : "Journal on Satisfiability, Boolean Modeling and Computation,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "Parallel search for maximum satisfiability",
      "author" : [ "Ruben Martins", "Vasco Manquinho", "Inês Lynce" ],
      "venue" : "AI Communications,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2012
    }, {
      "title" : "Optimization in SMT with LA(Q) cost functions",
      "author" : [ "Roberto Sebastiani", "Silvia Tomasi" ],
      "venue" : "In Automated Reasoning,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "Conflict-driven answer set solving: From theory to practice",
      "author" : [ "Martin Gebser", "Benjamin Kaufmann", "Torsten Schaub" ],
      "venue" : "Artif. Intell.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Domain-specific heuristics in answer set programming",
      "author" : [ "Martin Gebser", "Benjamin Kaufmann", "Ramón Otero", "Javier Romero", "Torsten. Schaub", "Philipp Wanko" ],
      "venue" : "In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence. AAAI Press,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Compact translations of non-disjunctive answer set programs to propositional clauses",
      "author" : [ "Tomi Janhunen", "Ilkka Niemelä" ],
      "venue" : "In Gelfond Festschrift,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    }, {
      "title" : "Bayesian network learning by compiling to weighted MAX-SAT",
      "author" : [ "James Cussens" ],
      "venue" : "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Bayesian network learning with cutting planes",
      "author" : [ "James Cussens" ],
      "venue" : "In Proceedings of the Twenty- Seventh Conference Annual Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Advances in Bayesian network learning using integer programming",
      "author" : [ "Mark Bartlett", "James Cussens" ],
      "venue" : "In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Bayesian learning of undirected GMs, also known as Markov random fields, from databases has attained a considerable interest, both in the statistical and computer science literature [3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "Bayesian learning of undirected GMs, also known as Markov random fields, from databases has attained a considerable interest, both in the statistical and computer science literature [3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 4,
      "context" : "Bayesian learning of undirected GMs, also known as Markov random fields, from databases has attained a considerable interest, both in the statistical and computer science literature [3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 5,
      "context" : "Bayesian learning of undirected GMs, also known as Markov random fields, from databases has attained a considerable interest, both in the statistical and computer science literature [3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 6,
      "context" : "Bayesian learning of undirected GMs, also known as Markov random fields, from databases has attained a considerable interest, both in the statistical and computer science literature [3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 7,
      "context" : "Bayesian learning of undirected GMs, also known as Markov random fields, from databases has attained a considerable interest, both in the statistical and computer science literature [3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 8,
      "context" : "Bayesian learning of undirected GMs, also known as Markov random fields, from databases has attained a considerable interest, both in the statistical and computer science literature [3, 4, 5, 6, 7, 8, 9].",
      "startOffset" : 182,
      "endOffset" : 203
    }, {
      "referenceID" : 9,
      "context" : "[10] present a greedy local search algorithm for Markov network learning and apply it to discovering word morphology.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] reduce the learning problem to a convex optimization problem that is solved by gradient descent.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Related methods have been investigated later [12, 13].",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Related methods have been investigated later [12, 13].",
      "startOffset" : 45,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Certain types of stochastic search methods, such as Markov Chain Monte Carlo (MCMC) or simulated annealing, can be proven to be consistent with respect to the identification of a structure maximizing posterior probability [4, 5, 6, 7].",
      "startOffset" : 222,
      "endOffset" : 234
    }, {
      "referenceID" : 4,
      "context" : "Certain types of stochastic search methods, such as Markov Chain Monte Carlo (MCMC) or simulated annealing, can be proven to be consistent with respect to the identification of a structure maximizing posterior probability [4, 5, 6, 7].",
      "startOffset" : 222,
      "endOffset" : 234
    }, {
      "referenceID" : 5,
      "context" : "Certain types of stochastic search methods, such as Markov Chain Monte Carlo (MCMC) or simulated annealing, can be proven to be consistent with respect to the identification of a structure maximizing posterior probability [4, 5, 6, 7].",
      "startOffset" : 222,
      "endOffset" : 234
    }, {
      "referenceID" : 6,
      "context" : "Certain types of stochastic search methods, such as Markov Chain Monte Carlo (MCMC) or simulated annealing, can be proven to be consistent with respect to the identification of a structure maximizing posterior probability [4, 5, 6, 7].",
      "startOffset" : 222,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : "However, convergence of such methods towards the areas associated with high posterior probabilities may still be slow when the number of nodes increases [4, 6].",
      "startOffset" : 153,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "However, convergence of such methods towards the areas associated with high posterior probabilities may still be slow when the number of nodes increases [4, 6].",
      "startOffset" : 153,
      "endOffset" : 159
    }, {
      "referenceID" : 13,
      "context" : "This enables the development of reductions from the structure learning problem to propositional satisfiability (SAT) [14] and its generalizations such as maximum satisfiability (MAXSAT) [15], and satisfiability modulo theories (SMT) [16], as well as answer-set programming (ASP) [17].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "This enables the development of reductions from the structure learning problem to propositional satisfiability (SAT) [14] and its generalizations such as maximum satisfiability (MAXSAT) [15], and satisfiability modulo theories (SMT) [16], as well as answer-set programming (ASP) [17].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 15,
      "context" : "This enables the development of reductions from the structure learning problem to propositional satisfiability (SAT) [14] and its generalizations such as maximum satisfiability (MAXSAT) [15], and satisfiability modulo theories (SMT) [16], as well as answer-set programming (ASP) [17].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 16,
      "context" : "This enables the development of reductions from the structure learning problem to propositional satisfiability (SAT) [14] and its generalizations such as maximum satisfiability (MAXSAT) [15], and satisfiability modulo theories (SMT) [16], as well as answer-set programming (ASP) [17].",
      "startOffset" : 279,
      "endOffset" : 283
    }, {
      "referenceID" : 17,
      "context" : "We have implemented these translations and conducted experiments to study the performance of existing solver technology on structure learning problems in Section 5 using two widely used datasets [18].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 18,
      "context" : "Given the set of cliques C in a chordal graph, the set of separators S can be obtained through intersections of the cliques ordered in terms of a junction tree [19], this operation is considered thoroughly in Section 3.",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 19,
      "context" : "This reduction consists of negating all the edge weights and then using any of the polynomial time algorithms for the latter problem [20].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 18,
      "context" : "To restrict the search space we can observe that a chordal graph with n nodes has at most nmaximal cliques [19].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "Lemma 3 ([21, 22]) Any maximum weight spanning tree of the clique graph is a junction tree, and hence satisfies the running intersection property: for every pair of nodes c and c′, (c ∩ c′) ⊆ c′′ for all nodes c′′ on the unique path between c and c′.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 21,
      "context" : "Lemma 3 ([21, 22]) Any maximum weight spanning tree of the clique graph is a junction tree, and hence satisfies the running intersection property: for every pair of nodes c and c′, (c ∩ c′) ⊆ c′′ for all nodes c′′ on the unique path between c and c′.",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 22,
      "context" : "Cardinality constraints are natively supported by some constraint solvers, or they can be reduced to Boolean constraints [23].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "For SMT, we used the OPTIMATHSAT solver (version 5) [26].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 28,
      "context" : "We also tried the LP2NORMAL tool that reduces cardinality constraints to more basic constraints [29].",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "We consider two datasets, one containing risk factors in heart diseases and the other variables related to economical behavior [18], to be abbreviated by heart and econ in the sequel.",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "For econ, however, exhaustive enumeration is impractical due to the extremely large search space, and consequently the optimality of the Markov network found by stochastic search in [4] had been open until now.",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 29,
      "context" : "The related problem of structure learning of Bayesian networks has been addressed by general-purpose combinatorial search methods, including MAXSAT [30] and a constraint-programming solver with a linear-programming solver as a subprocedure [31, 32].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 30,
      "context" : "The related problem of structure learning of Bayesian networks has been addressed by general-purpose combinatorial search methods, including MAXSAT [30] and a constraint-programming solver with a linear-programming solver as a subprocedure [31, 32].",
      "startOffset" : 240,
      "endOffset" : 248
    }, {
      "referenceID" : 31,
      "context" : "The related problem of structure learning of Bayesian networks has been addressed by general-purpose combinatorial search methods, including MAXSAT [30] and a constraint-programming solver with a linear-programming solver as a subprocedure [31, 32].",
      "startOffset" : 240,
      "endOffset" : 248
    } ],
    "year" : 2013,
    "abstractText" : "We investigate the problem of learning the structure of a Markov network from data. It is shown that the structure of such networks can be described in terms of constraints which enables the use of existing solver technology with optimization capabilities to compute optimal networks starting from initial scores computed from the data. To achieve efficient encodings, we develop a novel characterization of Markov network structure using a balancing condition on the separators between cliques forming the network. The resulting translations into propositional satisfiability and its extensions such as maximum satisfiability, satisfiability modulo theories, and answer set programming, enable us to prove optimal certain networks which have been previously found by stochastic search.",
    "creator" : null
  }
}