{
  "name" : "cbb6a3b884f4f88b3a8e3d44c636cbd8.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On Flat versus Hierarchical Classification in Large-Scale Taxonomies",
    "authors" : [ "Rohit Babbar", "Ioannis Partalas", "Eric Gaussier", "Massih-Reza Amini" ],
    "emails" : [ "firstname.lastname@imag.fr" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Large-scale classification of textual and visual data into a large number of target classes has been the focus of several studies, from researchers and developers in industry and academia alike. The target classes in such large-scale scenarios typically have an inherent hierarchical structure, usually in the form of a rooted tree, as in Directory Mozilla1, or a directed acyclic graph, with a parentchild relationship. Various classification techniques have been proposed for deploying classifiers in such large-scale taxonomies, from flat (sometimes referred to as big bang) approaches to fully hierarchical one adopting a complete top-down strategy. Several attempts have also been made in order to develop new classification techniques that integrate, at least partly, the hierarchy into the objective function being optimized (as [3, 5, 10, 11] among others). These techniques are however costly in practice and most studies either rely on a flat classifier, or a hierarchical one either deployed on the original hierarchy or a simplified version of it obtained by pruning some nodes (as [15, 18])2.\nHierarchical models for large scale classification however suffer from the fact that they have to make many decisions prior to reach a final category. This intermediate decision making leads to the error propagation phenomenon causing a decrease in accuracy. On the other hand, flat classifiers rely on a single decision including all the final categories, a single decision that is however difficult to make as it involves many categories, potentially unbalanced. It is thus very difficult to assess which strategy is best and there is no consensus, at the time being, on to which approach, flat or hierarchical, should be preferred on a particular category system.\nIn this paper, we address this problem and introduce new bounds on the generalization errors of classifiers deployed in large-scale taxonomies. These bounds make explicit the trade-off that both flat and hierarchical classifiers encounter in large-scale taxonomies and provide an explanation to\n1www.dmoz.org 2The study in [19] introduces a slightly different simplification, through an embedding of both categories\nand documents into a common space.\nseveral empirical findings reported in previous studies. To our knowledge, this is the first time that such bounds are introduced and that an explanation of the behavior of flat and hierarchical classifiers is based on theoretical grounds. We also propose a well-founded way to select nodes that should be pruned so as to derive a taxonomy better suited to the classification problem. Contrary to [4] that reweighs the edges in a taxonomy through a cost sensitive loss function to achieve this goal, we use here a simple pruning strategy that modifies the taxonomy in an explicit way.\nThe remainder of the paper is organized as follows: Section 2 introduces the notations used and presents the generalization error bounds for classification in large-scale taxonomies. It also presents the meta-classifier we designed to select those nodes that should be pruned in the original taxonomy. Section 3 illustrates these developments via experiments conducted on several taxonomies extracted from DMOZ and the International Patent Classification. The experimental results are in line with results reported in previous studies, as well as with our theoretical developments. Finally, Section 4 concludes this study."
    }, {
      "heading" : "2 Generalization Error Analyses",
      "text" : "Let X ⊆ Rd be the input space and let V be a finite set of class labels. We further assume that examples are pairs (x, v) drawn according to a fixed but unknown distribution D over X × V . In the case of hierarchical classification, the hierarchy of classes H = (V,E) is defined in the form of a rooted tree, with a root ⊥ and a parent relationship π : V \\ {⊥} → V where π(v) is the parent of node v ∈ V \\ {⊥}, and E denotes the set of edges with parent to child orientation. For each node v ∈ V \\{⊥}, we further define the set of its sisters S(v) = {v′ ∈ V \\{⊥}; v 6= v′∧π(v) = π(v′)} and its daughters D(v) = {v′ ∈ V \\ {⊥};π(v′) = v}. The nodes at the intermediary levels of the hierarchy define general class labels while the specialized nodes at the leaf level, denoted by Y = {y ∈ V : @v ∈ V, (y, v) ∈ E} ⊂ V , constitute the set of target classes. Finally for each class y in Y we define the set of its ancestors P(y) defined as\nP(y) = {vy1 , . . . , v y ky ; vy1 = π(y) ∧ ∀l ∈ {1, . . . , ky − 1}, v y l+1 = π(v y l ) ∧ π(v y ky ) =⊥}\nFor classifying an example x, we consider a top-down classifier making decisions at each level of the hierarchy, this process sometimes referred to as the Pachinko machine selects the best class at each level of the hierarchy and iteratively proceeds down the hierarchy. In the case of flat classification, the hierarchy H is ignored, Y = V , and the problem reduces to the classical supervised multiclass classification problem."
    }, {
      "heading" : "2.1 A hierarchical Rademacher data-dependent bound",
      "text" : "Our main result is the following theorem which provides a data-dependent bound on the generalization error of a top-down multiclass hierarchical classifier. We consider here kernel-based hypotheses, withK : X×X → R a PDS kernel and Φ : X → H its associated feature mapping function, defined as : FB = {f : (x, v) ∈ X × V 7→ 〈Φ(x),wv〉 |W = (w1 . . . , w|V |), ||W||H ≤ B} where W = (w1 . . . , w|V |) is the matrix formed by the |V | weight vectors defining the kernel-based hypotheses, 〈., .〉 denotes the dot product, and ||W||H = (∑ v∈V ||wv||2 )1/2 is the L2H group norm of W. We further define the following associated function class:\nGFB = {gf : (x, y) ∈ X × Y 7→ min v∈P(y) (f(x, v)− max v′∈S(v) f(x, v′)) | f ∈ FB}\nFor a given hypothesis f ∈ FB , the sign of its associated function gf ∈ GFB directly defines a hierarchical classification rule for f as the top-down classification scheme outlined before simply amounts to: assign x to y iff gf (x, y) > 0. The learning problem we address is then to find a hypothesis f from FB such that the generalization error of gf ∈ GFB , E(gf ) = E(x,y)∼D [ 1gf (x,y)≤0 ] , is minimal (1gf (x,y)≤0 is the 0/1 loss, equal to 1 if gf (x, y) ≤ 0 and 0 otherwise). The following theorem sheds light on the trade-off between flat versus hierarchical classification. The notion of function class capacity used here is the empirical Rademacher complexity [1]. The proof of the theorem is given in the supplementary material.\nTheorem 1 Let S = ((x(i), y(i)))mi=1 be a dataset of m examples drawn i.i.d. according to a probability distributionD over X ×Y , and letA be a Lipschitz function with constant L dominating the 0/1 loss; further let K : X × X → R be a PDS kernel and let Φ : X → H be the associated feature mapping function. Assume that there exists R > 0 such that K(x,x) ≤ R2 for all x ∈ X . Then, for all 1 > δ > 0, with probability at least (1 − δ) the following hierarchical multiclass classification generalization bound holds for all gf ∈ GFB :\nE(gf ) ≤ 1\nm m∑ i=1 A(gf (x(i), y(i))) + 8BRL√ m ∑ v∈V \\Y |D(v)|(|D(v)| − 1) + 3 √ ln(2/δ) 2m (1)\nwhere |D(v)| denotes the number of daughters of node v.\nFor flat multiclass classification, we recover the bounds of [12] by considering a hierarchy containing a root node with as many daughters as there are categories. Note that the definition of functions in GFB subsumes the definition of the margin function used for the flat multiclass classification problems in [12], and that the factor 8L in the complexity term of the bound, instead of 4 in [12], is due to the fact that we are using an L-Lipschitz loss function dominating the 0/1 loss in the empirical Rademacher complexity.\nFlat vs hierarchical classification on large-scale taxonomies. The generalization error is controlled in inequality (1) by a trade-off between the empirical error and the Rademacher complexity of the class of classifiers. The Rademacher complexity term favors hierarchical classifiers over flat ones, as any split of a set of category of size n in k parts n1, · · · , nk (\n∑k i=1 ni = n) is such that∑k\ni=1 n 2 i ≤ n2. On the other hand, the empirical error term is likely to favor flat classifiers vs hierarchical ones, as the latter rely on a series of decisions (as many as the length of the path from the root to the chosen category in Y) and are thus more likely to make mistakes. This fact is often referred to as the propagation error problem in hierarchical classification.\nOn the contrary, flat classifiers rely on a single decision and are not prone to this problem (even though the decision to be made is harder). When the classification problem in Y is highly unbalanced, then the decision that a flat classifier has to make is difficult; hierarchical classifiers still have to make several decisions, but the imbalance problem is less severe on each of them. So, in this case, even though the empirical error of hierarchical classifiers may be higher than the one of flat ones, the difference can be counterbalanced by the Rademacher complexity term, and the bound in Theorem 1 suggests that hierarchical classifiers should be preferred over flat ones.\nOn the other hand, when the data is well balanced, the Rademacher complexity term may not be sufficient to overcome the difference in empirical errors due to the propagation error in hierarchical classifiers; in this case, Theorem 1 suggests that flat classifiers should be preferred to hierarchical ones. These results have been empirically observed in different studies on classification in largescale taxonomies and are further discussed in Section 3.\nSimilarly, one way to improve the accuracy of classifiers deployed in large-scale taxonomies is to modify the taxonomy by pruning (sets of) nodes [18]. By doing so, one is flattening part of the taxonomy and is once again trading-off the two terms in inequality (1): pruning nodes leads to reduce the number of decisions made by the hierarchical classifier while maintaining a reasonable Rademacher complexity. Even though it can explain several empirical results obtained so far, the bound displayed in Theorem 1 does not provide a practical way to decide on whether to prune a node or not, as it would involve the training of many classifiers which is impractical with large-scale taxonomies. We thus turn towards another bound in the next section that will help us design a direct and simple strategy to prune nodes in a taxonomy."
    }, {
      "heading" : "2.2 Asymptotic approximation error bounds",
      "text" : "We now propose an asymptotic approximation error bound for a multiclass logistic regression (MLR) classifier. We first consider the flat, multiclass case (V = Y), and then show how the bounds can be combined in a typical top-down cascade, leading to the identification of important features that control the variation of these bounds.\nConsidering a pivot class y? ∈ Y , a MLR classifier, with parameters β = {βy0 , β y j ; y ∈ Y \\{y?}, j ∈ {1, . . . , d}}, models the class posterior probabilities via a linear function in x = (xj)dj=1 (see for example [13] p. 96) :\nP (y|x;β)y 6=y? = exp(βy0 +\n∑d j=1 β y j xj)\n1 + ∑ y′∈Y,y′ 6=y? exp(β y′ 0 + ∑d j=1 β y′ j xj)\nP (y?|x;β) = 1 1 + ∑ y′∈Y,y′ 6=y? exp(β y′ 0 + ∑d j=1 β y′ j xj)\nThe parameters β are usually fit by maximum likelihood over a training set S of size m (denoted by β̂m in the following) and the decision rule for this classifier consists in choosing the class with the highest class posterior probability :\nhm(x) = argmax y∈Y P (y|x, β̂m) (2)\nThe following lemma states to which extent the posterior probabilities with maximum likelihood estimates β̂m may deviate from their asymptotic values obtained with maximum likelihood estimates when the training size m tends to infinity (denoted by β̂∞).\nLemma 1 Let S be a training set of size m and let β̂m be the maximum likelihood estimates of the MLR classifier over S. Further, let β̂∞ be the maximum likelihood estimates of parameters of MLR when m tends to infinity. For all examples x, let R > 0 be the bound such that ∀y ∈ Y\\{y?}, exp(βy0 + ∑d j=1 β y j xj) < √ R; then for all 1 > δ > 0, with probability at least (1− δ) we have:\n∀y ∈ Y, ∣∣∣P (y|x, β̂m)− P (y|x, β̂∞)∣∣∣ < d √ R|Y|σ0 δm\nwhere σ0 = maxj,y σ y j and (σ y j )y,j represent the components of the inverse (diagonal) Fisher infor-\nmation matrix at β̂∞.\nProof (sketch) By denoting the sets of parameters β̂m = {β̂ y j ; j ∈ {0, . . . , d}, y ∈ Y \\{y?}}, and β̂∞ = {β y j ; j ∈ {0, . . . , d}, y ∈ Y \\{y?}}, and using the independence assumption and the asymptotic normality of maximum likelihood estimates (see for example [17], p. 421), we have, for 0 ≤ j ≤ d and ∀y ∈ Y \\ {y?}: √ m(β̂yj − β y j ) ∼ N(0, σ y j ) where the (σ y j )y,i represent the components of the inverse (diagonal) Fisher information matrix at β̂∞. Let σ0 = maxj,y σ y j . Then using Chebyshev’s inequality, for 0 ≤ j ≤ d and ∀y ∈ Y\\{y?} we have with probability at least 1− σ0/ 2, |β̂yj − β y j | < √m . Further ∀x and ∀y ∈ Y\\{y ?}, exp(βy0 + ∑d j=1 β y j xj) < √ R; using a Taylor development of the functions exp(x+ ) and (1+x+ x)−1 and the union bound, one obtains\nthat, ∀ > 0 and y ∈ Y with probability at least 1− |Y|σ0 2 : ∣∣∣P (y|x, β̂m)− P (y|x, β̂∞)∣∣∣ < d√Rm . Setting |Y|σ0 2 to δ, and solving for gives the result.\nLemma 1 suggests that the predicted and asymptotic posterior probabilities are close to each other, as the quantities they are based on are close to each other. Thus, provided that the asymptotic posterior probabilities between the best two classes, for any given x, are not too close to each other, the generalization error of the MLR classifier and the one of its asymptotic version should be similar. Theorem 2 below states such a relationship, using the following function that measures the confusion between the best two classes for the asymptotic MLR classifier defined as :\nh∞(x) = argmax y∈Y P (y|x, β̂∞) (3)\nFor any given x ∈ X , the confusion between the best two classes is defined as follows.\nDefinition 1 Let f1∞(x) = maxy∈Y P (y|x, β̂∞) be the best class posterior probability for x by the asymptotic MLR classifier, and let f2∞(x) = maxy∈Y\\h∞(x) P (y|x, β̂∞) be the second best class posterior probability for x. We define the confusion of the asymptotic MLR classifier for a category set Y as: GY(τ) = P(x,y)∼D(|f1∞(x)− f2∞(x)| < 2τ) for a given τ > 0.\nThe following theorem states a relationship between the generalization error of a trained MLR classifier and its asymptotic version.\nTheorem 2 For a multi-class classification problem in d dimensional feature space with a training set of sizem, {x(i), y(i)}mi=1, x(i) ∈ X , y(i) ∈ Y , sampled i.i.d. from a probability distributionD, let hm and h∞ denote the multiclass logistic regression classifiers learned from a training set of finite size m and its asymptotic version respectively, and let E(hm) and E(h∞) be their generalization errors. Then, for all 1 > δ > 0, with probability at least (1− δ) we have:\nE(hm) ≤ E(h∞) +GY ( d √ R|Y|σ0 δm ) (4)\nwhere √ R is a bound on the function exp(βy0 + ∑d j=1 β y j xj), ∀x ∈ X and ∀y ∈ Y , and σ0 is a constant.\nProof (sketch) The difference E(hm) − E(h∞) is bounded by the probability that the asymptotic MLR classifier h∞ correctly classifies an example (x, y) ∈ X × Y randomly chosen from D, while hm misclassifies it. Using Lemma 1, for all δ ∈ (0, 1),∀x ∈ X ,∀y ∈ Y , with probability at least 1− δ, we have: ∣∣∣P (y|x, β̂m)− P (y|x, β̂∞)∣∣∣ < d √ R|Y|σ0 δm\nThus, the decision made by the trained MLR and its asymptotic version on an example (x, y) differs only if the distance between the two predicted classes of the asymptotic classifier is less than two times the distance between the posterior probabilities obtained with β̂m and β̂∞ on that example;\nand the probability of this is exactly GY ( d √ R|Y|σ0 δm ) , which upper-bounds E(hm)− E(h∞).\nNote that the quantity σ0 in Theorem 2 represents the largest value of the inverse (diagonal) Fisher information matrix ([17]). It is thus the smallest value of the (diagonal) Fisher information matrix, and is related to the smallest amount of information one has on the estimation of each parameter β̂kj . This smallest amount of information is in turn related to the length (in number of occurrences) of the longest (resp. shortest) class in Y denoted respectively by nmax and nmin as, the smaller they are, the larger σ0 is likely to be."
    }, {
      "heading" : "2.3 A learning based node pruning strategy",
      "text" : "Let us now consider a hierarchy of classes and a top-down classifier making decisions at each level of the hierarchy. A node-based pruning strategy can be easily derived from the approximation bounds above. Indeed, any node v in the hierarchy H = (V,E) is associated with three category sets: its sister categories with the node itself S′(v) = S(v) ∪ {v}, its daughter categories, D(v), and the union of its sister and daughter categories, denoted F(v) = S(v) ∪ D(v).\n⊥\nv\n...\n...\n...\n⊥\n...\n...\n... Pruning\nS(v) ∪ {v}\nD(v)\nF(v) These three sets of categories are the ones involved before and after the pruning of node v. Let us now denote the MLR classifier by hS ′ v\nm learned from a set of sister categories of node v and the node itself, and by hDvm a MLR classifier learned from the set of daughter categories of node v (hS ′ v∞ and hDv∞ respectively denote their asymptotic versions). The following theorem is a direct extension of Theorem 2 to this setting.\nTheorem 3 With the notations defined above, for MLR classifiers, ∀ > 0, v ∈ V \\ Y , one has, with probability at least 1− ( Rd2|S′(v)|σS ′(v) 0\nmS′(v) 2 + Rd2|D(v)|σD(v)0 mD(v) 2\n) :\nE(hS ′ v m ) + E(hDvm ) ≤ E(h S′v∞ ) + E(hDv∞ ) +GS′(v)( ) +GD(v)( )\n{|Y`|,mY` , σY ` 0 ;Y` ∈ {S′(v),D(v)}} are constants related to the set of categories Y` ∈ {S′(v),D(v)} and involved in the respective bounds stated in Theorem 2. Denoting by hFvm the MLR classifier trained on the set F(v) and by hFv∞ its asymptotic version, Theorem 3 suggests that one should prune node v if:\nGF(v)( ) ≤ GS′(v)( ) +GD(v)( ) and |F(v)|σF(v)0 mF(v) ≤ |S ′(v)|σS\n′(v) 0\nmS′(v) + |D(v)|σD(v)0 mD(v)\n(5)\nFurthermore, the bounds obtained rely on the union bound and thus are not likely to be exploitable in practice. They nevertheless exhibit the factors that play an important role in assessing whether a particular trained classifier in the logistic regression family is close or not to its asymptotic version. Each node v ∈ V can then be characterized by factors in the set {|Y`|,mY` , nY ` max, n Y` min, GY`(.)|Y` ∈ {S′(v),D(v),F(v)}} which are involved in the estimation of inequalities (5) above. We propose to estimate the confusion term GY`(.) with two simple quantities: the average cosine similarity of all the pairs of classes in Y`, and the average symmetric Kullback-Leibler divergences between all the pairs in Y` of class conditional multinomial distributions.\nThe procedure for collecting training data associates a positive (resp. negative) class to a node if the pruning of that node leads to a final performance increase (resp. decrease). A meta-classifier is then trained on these features using a training set from a selected class hierarchy. After the learning phase, the meta-classifier is applied to each node of a new hierarchy of classes so as to identify which nodes should be pruned. A simple strategy to adopt is then to prune nodes in sequence: starting from the root node, the algorithm checks which children of a given node v should be pruned by creating the corresponding meta-instance and feeding the meta-classifier; the child that maximizes the probability of the positive class is then pruned; as the set of categories has changed, we recalculate which children of v can be pruned, prune the best one (as above) and iterate this process till no more children of v can be pruned; we then proceed to the children of v and repeat the process."
    }, {
      "heading" : "3 Discussion",
      "text" : "We start our discussion by presenting results on different hierarchical datasets with different characteristics using MLR and SVM classifiers. The datasets we used in these experiments are two large datasets extracted from the International Patent Classification (IPC) dataset3 and the publicly available DMOZ dataset from the second PASCAL large scale hierarchical text classification challenge (LSHTC2)4. Both datasets are multi-class; IPC is single-label and LSHTC2 multi-label with an average of 1.02 categories per class. We created 4 datasets from LSHTC2 by splitting randomly the first layer nodes (11 in total) of the original hierarchy in disjoint subsets. The classes for the IPC and LSHTC2 datasets are organized in a hierarchy in which the documents are assigned to the leaf categories only. Table 1 presents the characteristics of the datasets.\nCR denotes the complexity ratio between hierarchical and flat classification, given by the Rademacher complexity term in Theorem 1: (∑ v∈V \\Y |D(v)|(|D(v)| − 1) ) / (|Y|(|Y| − 1)); the same constants B, R and L are used in the two cases. As one can note, this complexity ratio always goes in favor of the hierarchal strategy, although it is 2 to 10 times higher on the IPC dataset, compared to LSHTC2-1,2,3,4,5. On the other hand, the ratio of empirical errors (last column of Table 1) obtained with top-down hierarchical classification over flat classification when using SVM\n3http://www.wipo.int/classifications/ipc/en/support/ 4http://lshtc.iit.demokritos.gr/\nwith a linear kernel is this time higher than 1, suggesting the opposite conclusion. The error ratio is furthermore really important on IPC compared to LSHTC2-1,2,3,4,5. The comparison of the complexity and error ratios on all the datasets thus suggests that the flat classification strategy may be preferred on IPC, whereas the hierarchical one is more likely to be efficient on the LSHTC datasets. This is indeed the case, as is shown below.\nTo test our simple node pruning strategy, we learned binary classifiers aiming at deciding whether to prune a node, based on the node features described in the previous section. The label associated to each node in this training set is defined as +1 if pruning the node increases the accuracy of the hierarchical classifier by at least 0.1, and -1 if pruning the node decreases the accuracy by more than 0.1. The threshold at 0.1 is used to avoid too much noise in the training set. The meta-classifier is then trained to learn a mapping from the vector representation of a node (based on the above features) and the labels {+1;−1}. We used the first two datasets of LSHTC2 to extract the training data while LSHTC2-3, 4, 5 and IPC were employed for testing.\nThe procedure for collecting training data is repeated for the MLR and SVM classifiers resulting in three meta-datasets of 119 (19 positive and 100 negative), 89 (34 positive and 55 negative) and 94 (32 positive and 62 negative) examples respectively. For the binary classifiers, we used AdaBoost with random forest as a base classifier, setting the number of trees to 20, 50 and 50 for the MLR and SVM classifiers respectively and leaving the other parameters at their default values. Several values have been tested for the number of trees ({10, 20, 50, 100 and 200}), the depth of the trees ({unrestricted, 5, 10, 15, 30, 60}), as well as the number of iterations in AdaBoost ({10, 20, 30}). The final values were selected by cross-validation on the training set (LSHTC2-1 and LSHTC2-2) as the ones that maximized accuracy and minimized false-positive rate in order to prevent degradation of accuracy.\nWe compare the fully flat classifier (FL) with the fully hierarchical (FH) top-down Pachinko machine, a random pruning (RN) and the proposed pruning method (PR) . For the random pruning we restrict the procedure to the first two levels and perform 4 random prunings (this is the average number of prunings that are performed in our approach). For each dataset we perform 5 independent runs for the random pruning and we record the best performance. For MLR and SVM, we use the LibLinear library [8] and apply the L2-regularized versions, setting the penalty parameter C by cross-validation.\nThe results on LSHTC2-3,4,5 and IPC are reported in Table 2. On all LSHTC datasets flat classification performs worse than the fully hierarchy top-down classification, for all classifiers. These results are in line with complexity and empirical error ratios for SVM estimated on different collections and shown in table 1 as well as with the results obtained in [14, 7] over the same type of taxonomies. Further, the work by [14] demonstrated that class hierarchies on LSHTC datasets suffer from rare categories problem, i.e., 80% of the target categories in such hierarchies have less than 5 documents assigned to them.\nAs a result, flat methods on such datasets face unbalanced classification problems which results in smaller error ratios; hierarchical classification should be preferred in this case. On the other hand, for hierarchies such as the one of IPC, which are relatively well balanced and do not suffer from the rare categories phenomenon, flat classification performs at par or even better than hierarchical\nclassification. This is in agreement with the conclusions obtained in recent studies, as [2, 9, 16, 6], in which the datasets considered do not have rare categories and are more well-balanced.\nThe proposed hierarchy pruning strategy aims to adapt the given taxonomy structure for better classification while maintaining the ancestor-descendant relationship between a given pair of nodes. As shown in Table 2, this simple learning based pruning strategy leads to statistically significant better results for all three classifiers compared to both the original taxonomy and a randomly pruned one. A similar result is reported in [18] through a pruning of an entire layer of the hierarchy, which can be seen as a generalization, even though empirical in nature, of the pruning strategy retained here. Another interesting approach to modify the original taxonomy is presented in [21]. In this study, three other elementary modification operations are considered, again with an increase of performance."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We have studied in this paper flat and hierarchical classification strategies in the context of largescale taxonomies, through error generalization bounds of multiclass, hierarchical classifiers. The first theorem we have introduced provides an explanation to several empirical results related to the performance of such classifiers. We have also introduced a well-founded way to simplify a taxonomy by selectively pruning some of its nodes, through a meta-classifier. The features retained in this meta-classifier derive from the error generalization bounds we have proposed. The experimental results reported here (as well as in other papers) are in line with our theoretical developments and justify the pruning strategy adopted.\nThis is the first time, to our knowledge, that a data dependent error generalization bound is proposed for multiclass, hierarchical classifiers and that a theoretical explanation is provided for the performance of flat and hierarchical classification strategies in large-scale taxonomies. In particular, there is, up to now, no consensus on which classification scheme, flat or hierarchical, to use on a particular category system. One of our main conclusions is that top-down hierarchical classifiers are well suited to unbalanced, large-scale taxonomies, whereas flat ones should be preferred for well-balanced taxonomies.\nLastly, our theoretical development also suggests possibilities to grow a hierarchy of classes from a (large) set of categories, as has been done in several studies (e.g. [2]). We plan to explore this in future work."
    }, {
      "heading" : "5 Acknowledgments",
      "text" : "This work was supported in part by the ANR project Class-Y, the Mastodons project Garguantua, the LabEx PERSYVAL-Lab ANR-11-LABX-0025 and the European project BioASQ (grant agreement no. 318652)."
    } ],
    "references" : [ {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Label embedding trees for large multi-class tasks",
      "author" : [ "S. Bengio", "J. Weston", "D. Grangier" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Hierarchical document categorization with support vector machines",
      "author" : [ "L. Cai", "T. Hofmann" ],
      "venue" : "In Proceedings 13 ACM International Conference on Information and Knowledge Management (CIKM),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Distribution-calibrated hierarchical classification",
      "author" : [ "O. Dekel" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Large margin hierarchical classification",
      "author" : [ "O. Dekel", "J. Keshet", "Y. Singer" ],
      "venue" : "In Proceedings of the 21 International Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Fast and balanced: Efficient label tree learning for large scale object recognition",
      "author" : [ "J. Deng", "S. Satheesh", "A.C. Berg", "F.-F. Li" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Hierarchical classification of web content",
      "author" : [ "S. Dumais", "H. Chen" ],
      "venue" : "In Proceedings of the 23 annual international ACM SIGIR conference,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2000
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Discriminative learning of relaxed hierarchy for large-scale visual recognition",
      "author" : [ "T. Gao", "D. Koller" ],
      "venue" : "In IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Regularization framework for large scale hierarchical classification",
      "author" : [ "S. Gopal", "Y.Y.A. Niculescu-Mizil" ],
      "venue" : "In Large Scale Hierarchical Classification, ECML/PKDD Discovery Challenge Workshop,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Bayesian models for large-scale hierarchical classification",
      "author" : [ "S. Gopal", "Y. Yang", "B. Bai", "A. Niculescu-Mizil" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Sample complexity of classifiers taking values in R , application to multi-class SVMs",
      "author" : [ "Y. Guermeur" ],
      "venue" : "Communications in Statistics - Theory and Methods,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "Support vector machines classification with a very large-scale taxonomy",
      "author" : [ "T.-Y. Liu", "Y. Yang", "H. Wan", "H.-J. Zeng", "Z. Chen", "W.-Y. Ma" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2005
    }, {
      "title" : "Improving hierarchical SVMs by hierarchy flattening and lazy classification",
      "author" : [ "H. Malik" ],
      "venue" : "In 1st Pascal Workshop on Large Scale Hierarchical Classification,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Towards good practice in large-scale learning for image classification",
      "author" : [ "F. Perronnin", "Z. Akata", "Z. Harchaoui", "C. Schmid" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Theory of Statistics",
      "author" : [ "M. Schervish" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1995
    }, {
      "title" : "Flatten hierarchies for large-scale hierarchical text categorization",
      "author" : [ "X. Wang", "B.-L. Lu" ],
      "venue" : "In 5 International Conference on Digital Information Management,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Large margin taxonomy embedding for document categorization",
      "author" : [ "K.Q. Weinberger", "O. Chapelle" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2008
    }, {
      "title" : "A re-examination of text categorization methods",
      "author" : [ "Y. Yang", "X. Liu" ],
      "venue" : "In Proceedings of the 22 annual International ACM SIGIR conference,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "Automatically adjusting content taxonomies for hierarchical classification",
      "author" : [ "J. Zhang", "L. Tang", "H. Liu" ],
      "venue" : "In Proceedings of the 4 Workshop on Text Mining,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Several attempts have also been made in order to develop new classification techniques that integrate, at least partly, the hierarchy into the objective function being optimized (as [3, 5, 10, 11] among others).",
      "startOffset" : 182,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "Several attempts have also been made in order to develop new classification techniques that integrate, at least partly, the hierarchy into the objective function being optimized (as [3, 5, 10, 11] among others).",
      "startOffset" : 182,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "Several attempts have also been made in order to develop new classification techniques that integrate, at least partly, the hierarchy into the objective function being optimized (as [3, 5, 10, 11] among others).",
      "startOffset" : 182,
      "endOffset" : 196
    }, {
      "referenceID" : 10,
      "context" : "Several attempts have also been made in order to develop new classification techniques that integrate, at least partly, the hierarchy into the objective function being optimized (as [3, 5, 10, 11] among others).",
      "startOffset" : 182,
      "endOffset" : 196
    }, {
      "referenceID" : 14,
      "context" : "These techniques are however costly in practice and most studies either rely on a flat classifier, or a hierarchical one either deployed on the original hierarchy or a simplified version of it obtained by pruning some nodes (as [15, 18])2.",
      "startOffset" : 228,
      "endOffset" : 236
    }, {
      "referenceID" : 17,
      "context" : "These techniques are however costly in practice and most studies either rely on a flat classifier, or a hierarchical one either deployed on the original hierarchy or a simplified version of it obtained by pruning some nodes (as [15, 18])2.",
      "startOffset" : 228,
      "endOffset" : 236
    }, {
      "referenceID" : 18,
      "context" : "org (2)The study in [19] introduces a slightly different simplification, through an embedding of both categories and documents into a common space.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "Contrary to [4] that reweighs the edges in a taxonomy through a cost sensitive loss function to achieve this goal, we use here a simple pruning strategy that modifies the taxonomy in an explicit way.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "The notion of function class capacity used here is the empirical Rademacher complexity [1].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "For flat multiclass classification, we recover the bounds of [12] by considering a hierarchy containing a root node with as many daughters as there are categories.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "Note that the definition of functions in GFB subsumes the definition of the margin function used for the flat multiclass classification problems in [12], and that the factor 8L in the complexity term of the bound, instead of 4 in [12], is due to the fact that we are using an L-Lipschitz loss function dominating the 0/1 loss in the empirical Rademacher complexity.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 11,
      "context" : "Note that the definition of functions in GFB subsumes the definition of the margin function used for the flat multiclass classification problems in [12], and that the factor 8L in the complexity term of the bound, instead of 4 in [12], is due to the fact that we are using an L-Lipschitz loss function dominating the 0/1 loss in the empirical Rademacher complexity.",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 17,
      "context" : "Similarly, one way to improve the accuracy of classifiers deployed in large-scale taxonomies is to modify the taxonomy by pruning (sets of) nodes [18].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 12,
      "context" : ", d}}, models the class posterior probabilities via a linear function in x = (xj)j=1 (see for example [13] p.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : ", d}, y ∈ Y \\{y}}, and using the independence assumption and the asymptotic normality of maximum likelihood estimates (see for example [17], p.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 16,
      "context" : "Note that the quantity σ0 in Theorem 2 represents the largest value of the inverse (diagonal) Fisher information matrix ([17]).",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "For MLR and SVM, we use the LibLinear library [8] and apply the L2-regularized versions, setting the penalty parameter C by cross-validation.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 13,
      "context" : "These results are in line with complexity and empirical error ratios for SVM estimated on different collections and shown in table 1 as well as with the results obtained in [14, 7] over the same type of taxonomies.",
      "startOffset" : 173,
      "endOffset" : 180
    }, {
      "referenceID" : 6,
      "context" : "These results are in line with complexity and empirical error ratios for SVM estimated on different collections and shown in table 1 as well as with the results obtained in [14, 7] over the same type of taxonomies.",
      "startOffset" : 173,
      "endOffset" : 180
    }, {
      "referenceID" : 13,
      "context" : "Further, the work by [14] demonstrated that class hierarchies on LSHTC datasets suffer from rare categories problem, i.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 19,
      "context" : "Statistical significance (using micro sign test (s-test) as proposed in [20]) is denoted with ↓ for p-value<0.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "This is in agreement with the conclusions obtained in recent studies, as [2, 9, 16, 6], in which the datasets considered do not have rare categories and are more well-balanced.",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "This is in agreement with the conclusions obtained in recent studies, as [2, 9, 16, 6], in which the datasets considered do not have rare categories and are more well-balanced.",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 15,
      "context" : "This is in agreement with the conclusions obtained in recent studies, as [2, 9, 16, 6], in which the datasets considered do not have rare categories and are more well-balanced.",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "This is in agreement with the conclusions obtained in recent studies, as [2, 9, 16, 6], in which the datasets considered do not have rare categories and are more well-balanced.",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "A similar result is reported in [18] through a pruning of an entire layer of the hierarchy, which can be seen as a generalization, even though empirical in nature, of the pruning strategy retained here.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "Another interesting approach to modify the original taxonomy is presented in [21].",
      "startOffset" : 77,
      "endOffset" : 81
    } ],
    "year" : 2013,
    "abstractText" : "We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end, we first propose a multiclass, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. We then introduce another type of bound targeting the approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.",
    "creator" : null
  }
}