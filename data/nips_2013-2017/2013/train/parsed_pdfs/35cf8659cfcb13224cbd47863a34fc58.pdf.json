{
  "name" : "35cf8659cfcb13224cbd47863a34fc58.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian Hierarchical Community Discovery",
    "authors" : [ "Charles Blundell", "Yee Whye Teh" ],
    "emails" : [ "charles@deepmind.com", "y.w.teh@stats.ox.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "People often organise themselves into groups or communities. For example, friends may form cliques, scientists may have recurring collaborations, and politicians may form factions. Consequently the structure found in social networks is often studied by inferring these groups. Using community membership one may then make predictions about the presence or absence of unobserved connectivity in the social network. Sometimes these communities possess hierarchical structure. For example, within science, the community of physicists may be split into those working on various branches of physics, and each branch refined repeatedly until finally reaching the particular specialisation of an individual physicist.\nMuch previous work on social networks has focused on discovering flat community structure. The stochastic blockmodel [1] places each individual in a community according to the block structure of the social network’s adjacency matrix, whilst the mixed membership stochastic blockmodel [2] extends the stochastic blockmodel to allow individuals to belong to several flat communities simultaneously. Both models require the number of flat communities to be known and are parametric methods.\nGreedy hierarchical clustering has previously been applied directly to discovering hierarchical community structure [3]. These methods do not require the community structure to be flat or the number of communities to be known. Such schemes are often computationally efficient, scaling quadratically in the number of individuals for a dense network, or linearly in the number of edges for a sparse network [4]. These methods do not yield a full probabilistic account of the data, in terms of parameters and the discovered structure.\nSeveral authors have also proposed Bayesian approaches to inferring community structure. The Infinite Relational Model (IRM; [5, 6, 7]) infers a flat community structure. The IRM has been extended to infer hierarchies [8], by augmenting it with a tree, but comes at considerable computational cost. [9] and [10] propose methods limited to hierarchies of depth two, whilst [11] propose methods limited to hierarchies of known depth.. The Mondrian process [12] propose a flexible prior on trees and a likelihood model for relational data. Current Bayesian nonparametric methods do not scale well to larger networks because the inference algorithms used make many small changes to the model.\n∗Part of the work was done whilst at the Gatsby Unit, University College London.\nSuch schemes can take a large number of iterations to converge on an adequate solution whilst each iteration often scales unfavourably in the number of communities or vertices.\nWe shall describe a greedy Bayesian hierarchical clustering method for discovering community structure in social networks. Our work builds upon Bayesian approaches to greedy hierarchical clustering [13, 14] extending these approaches to relational data. We call our method Bayesian Hierarchical Community Discovery (BHCD). BHCD produces good results two orders of magnitude faster than a single iteration of the IRM, and its worst case run-time is quadratic in the number of vertices of the graph and independent of the number of communities.\nThe remainder of the paper is organised as follows. Section 2 describes the stochastic blockmodel. In Section 3 we introduce our model as a hierarchical mixture of stochastic blockmodels. In Section 4 we describe an efficient scheme for inferring hierarchical community structure with our model. Section 5 demonstrates BHCD on several data sets. We conclude with a brief discussion in Section 6"
    }, {
      "heading" : "2 Stochastic Blockmodels",
      "text" : "A stochastic blockmodel [1] consists of a partition, φ, of vertices V and for each pair of clusters p and q in φ, a parameter, θpq , giving the probability of a presence or absence of an edge between nodes of the clusters. Suppose V = {a, b, c, d}, then one way to partition the vertices would be to form clusters ab, c and d, which we shall write as φ = ab|c|d, where | denotes a split between clusters. The probability of an adjacency matrix, D, given a stochastic blockmodel, is as follows:\nP (D|φ, {θpq}p,q∈φ) = ∏ p,q∈φ θ n1pq pq (1− θpq)n 0 pq (1)\nwhere n1pq is the total number of edges in D between the vertices in clusters p and q, and n0pq is the total number of observed absent edges in D between the vertices in clusters p and q. When modelling communities, we expect the edge appearance probabilities within a cluster to be different to those between different clusters. Hence we place a different prior on each of these cases. Similar approaches have been taken to adapt the IRM to community detection [7], where non-conjugate priors were used at increased computational cost. In the interest of computational efficiency, our model will instead use conjugate priors but with differing hyperparameters. θpp will have a Beta(α, β) prior and θpq , p 6= q, will have a Beta(δ, λ) prior. The hyperparameters are picked such that α > β and δ < λ, which correspond to a prior belief of a higher density of edges within a community than across communities. Integrating out the edge appearance parameters, we obtain the following likelihood of a particular partition φ:\nP (D|φ) = ∏ p∈φ B(α+ n1pp, β + n 0 pp) B(α, β) ∏ p,q∈φ p 6=q B(δ + n1pq, λ+ n 0 pq) B(δ, λ) (2)\nwhere B(·, ·) is the Beta function. We may generalise this to use any exponential family: p(D|φ) = ∏ p∈φ f(σpp) ∏ p,q∈φ, p 6=q g(σpq) (3)\nwhere f(·) is the marginal likelihood of the on-diagonal blocks, and g(·) is the marginal likelihood of the off-diagonal blocks. We use σpq to denote the sufficient statistics from a (p, q)-block of the adjacency matrix: all of the elements whose row indices are in cluster p and whose column indices are in cluster q. For the remainder of the paper, we shall focus on the beta-Bernoulli case given in (2) for concreteness. i.e., σpq = (n1pq, n 0 pq), with f(x, y) = B(α+x,β+y) B(α,β) and g(x, y) = B(δ+x,λ+y) B(δ,λ) . For clarity of exposition, we shall focus on modelling undirected or symmetric networks with no self-edges, so σpq = σqp and σ{x}{x} = 0 for each vertex x, but in general this restriction is not necessary.\nOne approach to inferring φ is to fix the number of communities K then use maximum likelihood estimation or Bayesian inference to assign vertices to each of the communities [1, 15]. Another approach is to use variational Bayes, combined with an upper bound on the number of communities, to determine the number of communities and community assignments [16]."
    }, {
      "heading" : "3 Bayesian Hierarchical Communities",
      "text" : "In this section we shall develop a Bayesian nonparametric approach to community discovery. Our model organises the communities into a nested hierarchy T , with all vertices in one community at the root and singleton vertices at the leaves. Each vertex belongs to all communities along the path from the root to the leaf containing it. We describe the probabilistic model relating the hierarchy of communities to the observed network connectivity data here, whilst in the next section we will develop a greedy model selection procedure for learning the hierarchy T from data.\nWe begin with the marginal probability of the adjacency matrix D under a stochastic blockmodel: p(D) = ∑ φ p(φ)p(D|φ) (4)\nIf the Chinese restaurant process (CRP) is used as the prior on partitions p(φ), then (4) corresponds to the marginal likelihood of the IRM. Computing (4) typically requires an approximation: the space of partitions φ is large and so the number of partitions in the above sum grows at least exponentially in the number of vertices.\nWe shall take a different approach: we use a tree to define a prior on partitions, where only partitions that are consistent with the tree are included in the sum. This allows us to evaluate (4) exactly. The tree will represent the hierarchical community structure discovered in the network. Each internal node of the tree corresponds to a community and the leaves of the tree are the vertices of the adjacency matrix, D. Figure 1 shows how a tree defines a collection of partitions for inclusion in the sum in (4). The adjacency matrix on the left is explained by our model, conditioned upon the tree on the upper left, by its five tree-consistent partitions. Various blocks within the adjacency matrix are explained either by the on-diagonal model f or the off-diagonal model g, according to each partition. Note that the block structure of the off-diagonal model depends on the structure of the tree T , not just on the partition φ. The model always includes the trivial partition of all vertices in a single community and also the singleton partition of all vertices in separate communities.\nMore precisely, we shall denote trees as a nested collection of sets of vertices. For example, the tree in Figure 1 is T = {{a, b}, {c, d, e}, f}. The set of of partitions consistent with a tree T may be expressed formally as in [14]:\nΦ(T ) = {leaves(T )} ∪ {φ1|. . . |φnT : φi ∈ Φ(Ti), Ti ∈ ch(T )} (5)\nwhere leaves(T ) are the leaves of the tree T , ch(T ) are its children, and so Ti is the ith subtree of tree T . The marginal likelihood of the tree T can be written as:\np(D|T ) = p(DTT |T ) = ∑ φ p(φ|T )p(DTT |φ, T ) (6)\nwhere the notation DTT is short for Dleaves(T ),leaves(T ), the block of D whose rows and columns correspond to the leaves of T .\nOur prior on partitions p(φ|T ) is motivated by the following generative process: Begin at the root of the tree, S = T . With probability πS , stop and generate DSS according to the on-diagonal model f . Otherwise, with probability 1 − πS , generate all inter-cluster edges between the children of the current node according to g, and recurse on each child of the current tree S. The resulting prior on\ntree-consistent partitions p(φ|T ) factorises as: p(φ|T ) = ∏\nS∈ancestorT (φ)\n(1− πS) ∏\nS∈subtreeT (φ)\nπS (7)\nwhere subtreeT (φ) are the subtrees in T corresponding to the clusters in partition φ and ancestorT (φ) are the ancestors of trees in subtreeT (φ). The prior probability of partitions not consistent with T is zero. Following [14], we define πS = 1− (1− γ)|ch(S)|, where γ is a parameter of the model. This choice of πS gives higher likelihood to non-binary trees over cascading binary trees when the data has no hierarchical structure [14]. Similarly, the likelihood of each partition p(D|φ, T ) factorises as:\np(DTT |φ, T ) = ∏\nS∈ancestorT (φ)\ng ( σ¬chSS ) ∏ S∈subtreeT (φ) f(σSS) (8)\nwhere σSS are the sufficient statistics of the adjacency matrix D among the leaves of tree S, and σ¬chSS are the sufficient statistics of the edges between different children of S:\nσ¬chSS = σSS − ∑\nC∈ch(S)\nσCC (9)\nThe set of tree consistent partitions given in (5) has at mostO(2n) partitions, for n vertices. However due to the structure of the prior on partitions (7) and the block model (8), the marginal likelihood (6) may be calculated by dynamic programming, in O(n + m) time where n is the number of vertices and m is the number of edges. Combining (7) and (8) and expanding (6) by breadth-first traversal of the tree, yields the following recursion for the marginal likelihood of the generative process given at the beginning of the section:\np(DTT |T ) = πT f(σTT ) + (1− πT )g ( σ¬chTT ) ∏ C∈ch(T ) p(DCC |C) (10)"
    }, {
      "heading" : "4 Agglomerative Model Selection",
      "text" : "In this section we describe how to learn the hierarchy of communities T . The problem is treated as one of greedy model selection: each tree T is a different model, and we wish to find the model that best explains the data. The tree is built in a bottom-up greedy agglomerative fashion, starting from a forest consisting of n trivial trees, each corresponding to exactly one vertex. Each iteration then merges two of the trees in the forest. At each iteration, each vertex in the network is a leaf of exactly one tree in the forest. The algorithm finishes when just one tree remains. We define the likelihood of the forest F as the probability of data described by each tree in the forest times that for the data corresponding to edges between different trees:\np(D|F ) = g(σ¬chFF ) ∏ T∈F p(DTT |T ) (11)\nwhere σ¬chFF are the sufficient statistics of the edges between different trees in the forest.\nThe initial forest, F (0), consists a singleton tree for each vertex of the network. At each iteration a pair of trees in the forest F is chosen to be merged, resulting in forest F ?. Which pair of tree to merge, and how to merge these trees, is determined by considering which pair and type of merger yields the largest Bayes factor improvement over the current model. If the trees I and J are merged to form the tree M , then the Bayes factor score is:\nSCORE(M ; I, J) = p(DMM |F ?) p(DMM |F ) = p(DMM |M) p(DII |I)p(DJJ |J)g(σIJ) (12)\nwhere p(DMM |M), p(DII |I) and p(DJJ |J) are given by (10) and σIJ are the sufficient statistics of the edges connecting leaves(I) and leaves(J). Note that the Bayes factor score is based on data local to the merge—i.e., by considering the probability of the connectivity data only among the leaves of the newly merged tree. This permits efficient local computations and makes the assumption that local community structure should depend only on the local connectivity structure.\nWe consider three possible mergers of two trees I and J into M . See Figure 2, where for concreteness we take I = {Ta, Tb, Tc} and J = {Td, Te} where Ta, Tb, Tc, Td, Te are subtrees. M may be\nAlgorithm 1: Bayesian hierarchical community discovery.\nformed by joining I and J together using a new node, giving M = {I, J}. Alternatively M may be formed by absorbing J as a child of I , yielding M = {J}∪ ch(I), or vice versa, M = {I}∪ ch(J). The algorithm for finding T is described in Algorithm 1. The algorithm maintains a forest F of trees, the likelihood pI = p(DII |I) of each tree I ∈ F according to (10), and the sufficient statistics {σ¬chII }I∈F , {σIJ}I,J∈F needed for efficient computation. It also maintains a heap of potential merges ordered by the SCORE (12), used for determining the ordering of merges. At each iteration, the best potential merge, say of treesX and Y resulting in tree I , is picked off the heap. If eitherX or Y is not in F , this means that the tree has been used in a previous merge, so that the potential merge is discarded and the next potential merge is considered. After a successful merge, the sufficient statistics associated with the new tree I are computed using the previously computed ones:\nσIJ = σXJ + σY J for J ∈ F, J 6= I . σMM = σII + σJJ + σIJ\nσ¬chMM =  σIJ if M is formed by joining I and J , σ¬chII + σIJ if M is formed by J absorbed into I , σ¬chJJ + σIJ if M is formed by I absorbed into J .\n(13)\nThese sufficient statistics are computed based on previous cached values, allowing each inner loop of the algorithm to takeO(1) time. Finally, potential mergers of I with other trees J in the forest are considered and added onto the heap. In the algorithm, MERGE(I; J) denotes the best of the three possible merges of I and J .\nAlgorithm 1 is structurally the same as that in [14], and so has time complexity in O(n2 log(n)). The difference is that additional care is needed to cache the sufficient statistics allowing for O(1) computation per inner loop of the algorithm. We shall refer to Algorithm 1 as BHCD."
    }, {
      "heading" : "4.1 Variations",
      "text" : "BHCD will consider merging trees that have no edges between them if the merge score (12) is high enough. This does not seem to be a reasonable behaviour as communities that are completely disconnected should not be merged. We can alter BHCD by simply prohibiting such merges between trees that have no edges between them. The resulting algorithm we call BHCD sparse, as it only needs to perform computations on the parts of the network with edges present. Empirically, we have found that it produces better results than BHCD and runs faster for sparse networks, although in the worst case it has the same time complexity O(n2 log n) as BHCD.\nAs BHCD runs, several merges can have the same score. In particular, at the first iteration all pairs of vertices connected by an edge have the same score. In such situations, we break the ties at random. Different tie breaks can yield different results and so different runs on the same data may yield\ndifferent trees. Where we want a single tree, we use R (R = 50 in experiments) restarts and pick the tree with the highest likelihood according to (10). Where we wish to make predictions, we will construct predictive probabilities (see next section) by averaging all R trees."
    }, {
      "heading" : "4.2 Predictions",
      "text" : "For link prediction, we wish to obtain the predictive distribution of a previously unobserved element of the adjacency matrix. This is easily achieved by traversing one path of the tree from the root towards the leaves, hence the computational complexity is linear in the depth of the tree. In particular, suppose we wish to predict the edge between x and y, Dxy , given the observed edges D, then the predictive distribution can be computed recursively starting with S = T :\np(Dxy|DSS , S) = rSf(Dxy|σSS) + (1− rS) { p(Dxy|DCC , C) if ∃C ∈ ch(S) : x, y ∈ leaves(C), g(Dxy|σ¬chSS ) if ∀C ∈ ch(S) : x, y 6∈ leaves(C).\nrS = πSf(σSS)\np(DSS |S) (14)\nwhere rS is the probability that the cluster consisting of leaves(S) is present if the cluster corresponding to its parent is not present, given the data D and the tree T . The predictive distribution is a mixture of a number of on-diagonal posterior f terms (weighted by the responsibility rT ), and finally an off-diagonal posterior g term. Hence the computational complexity is Θ(n)."
    }, {
      "heading" : "5 Experiments",
      "text" : "We now demonstrate BHCD on three data sets. Firstly we examine qualitative performance on Sampson’s monastery network. Then we demonstrate the speed and accuracy of our method on a subset of the NIPS 1–17 co-authorship network compared to IRM—one of the fastest Bayesian nonparametric models for these data. Finally we show hierarchical structure found examining the full NIPS 1–17 co-authorship network. In our experiments we set the model hyperparameters α = δ = 1.0, β = λ = 0.2, and γ = 0.4 which we found to work well. In the first two experiments we shall compare four variations of BHCD: BHCD, BHCD sparse, BHCD restricted to binary trees, and BHCD sparse restricted to binary trees. Binary-only variations of BHCD only consider joins, not absorptions, and so may run faster. They also tend to produce better predictive results as they average over a larger number of partitions. But, as we shall see below, the hierarchies found can be more difficult to interpret than the non-binary hierarchies found.\nSampson’s Monastery Network Figure 3 shows the result of running six variants of BHCD on time four of Sampson’s monastery network [17]. Sampson observed the monastery at five times—time four is the most interesting time as it was before four of the monks were expelled. We treated positive affiliations as edges, and negative affiliations as observed absent edges, and unknown affiliation as missing data. [17], using a variety of methods, found four flat groups, shown at the top of Figure 3: Young Turks (Albert, Boniface, Gregory, Hugh, John Bosco, Mark, Winfrid), Loyal Opposition (Ambrose, Berthold, Bonaventure, Louis, Peter), Outcasts (Basil, Elias, Simplicius), and Interstitial group (Amand, Ramuald, Victor).\nAs can be seen in Figure 3, most BHCD variants find clear block diagonal structure in the adjacency matrix. The binary versions find similar structure to the non-binary versions, up to permutations of the children of the non-binary trees. BHCD global is lead astray by out of date scores on its heap and so finds less coherent structure. The log likelihoods of the trees in Figure 3 are −6.62 (BHCD) and −22.80 (BHCD sparse). Whilst the log likelihoods of the binary trees in Figure 3 are −8.32 (BHCD binary) and −24.68 (BHCD sparse binary). BHCD finds the most likely tree, and rose trees typically better explain the data than binary trees.\nBHCD finds the Young Turks and Loyal Opposition groups and chooses to merge some members of the Interstitial group into the Loyal Opposition and Amand into the Outcasts. Mark, however, is placed in a separate community: although Mark has a positive affiliation with Gregory, Mark also has a negative affiliation with John Bosco and so BHCD elects to create a new community to account for this discrepancy.\nNIPS-234 Next we applied BHCD to a subset of the NIPS co-authorship dataset [19]. We compared its predictive performance to both IRM using MCMC and also inference in the IRM using greedy\nMethod Time complexity IRM (naı̈ve) O(n2K2IR)\nIRM (sparse) O(mK2IR) LFRM [18] O(n2F 2IR) IMMM [9] O(n2K2IR)\nILA [10] O(n2(F +K2)IR) [8] O(n2K2IR)\nBHCD O(n2 log(n)R)\nsearch, using the publicly available C implementation[20]. Our implementation of BHCD is also in C. As can be seen from Table 1, BHCD has significantly lower computational complexity than other Bayesian nonparametric methods even than those inferring flat hierarchies. This is because it is a simpler model and uses a simpler inference method—thus we do not expect it to yield better predictive results, but instead to get good results quickly. Unlike the other listed methods, BHCD’s worst case complexity does not depend upon the number of communities, and BHCD always terminates after a fixed number of steps so has no I factor. This latter factor, I , corresponds to the number of MCMC steps or the number of greedy search steps, may be large and may need to scale as the number of vertices increases.\nFollowing [18, 10] we restricted the network to the 234 most connected individuals. Figure 4 shows the average log predictive probability of held out data, accuracy and Area under the receiver operating Curve (AUC) over time for both BHCD and IRM. For the IRM, each point represents a single Gibbs step (for MCMC) or a search step (for greedy search). For BHCD, each point represents a complete run of the inference algorithm. BHCD is able to make reasonable predictions before the IRM has completed a single Gibbs scan. We used the same 10 cross-validation folds as used in [10] and so our results are quantitatively comparable to their results for the Latent Factor Relational Model (LFRM [18]) and their model, the Infinite Latent Attributes model (ILA). BHCD performs similarly to LFRM, worse than ILA, and better IRM. After about 10 seconds, the sparse variants of BHCD make as good predictions on NIPS-234 as the IRM after about 1000 seconds. Notably the sparse variations are faster than the non-sparse variants of BHCD, as the NIPS co-authorship network is sparse.\nFull NIPS The full NIPS dataset has 2864 vertices and 9466 edges. Figure 5 shows part of the hierarchy discovered by BHCD. The full inferred hierarchy is large, having 646 internal nodes. We cut the tree and retained the top portion of the hierarchy, shown above the clusters. We merged all the leaves of a subtree T into a flat cluster when rT ∏ A∈ancestorT (1−rA) > 0.5 where rT is given in (14). This quantity corresponds to the probability of picking that particular subtree in the predictive distribution. Amongst these clusters we included only those with at least 15 members in Figure 5. We include hierarchies with a lower cut-off in the supplementary."
    }, {
      "heading" : "6 Discussion and Future Work",
      "text" : "We proposed an efficient Bayesian procedure for discovering hierarchical communities in social networks. Experimentally our procedure discovers reasonable hierarchies and is able to make predictions about two orders of magnitude faster than one of the fastest existing Bayesian nonparametric schemes, whilst attaining comparable performance. Our inference procedure scales as O(n2 log n) through a novel caching scheme, where n is the number of vertices, making the procedure suitable for large dense networks. However our likelihood can be computed in O(n+m) time, where m are the number of edges. This disparity between inference and likelihood suggests that in future it may be possible to improve the scalability of the model on sparse networks, wherem n2. Another way to scale up the model would be to investigate parameterising the network using the sufficient statistics of triangles, instead of edges as in [21]. Others [7] have found that non-conjugate likelihoods can yield improved predictions—thus adapting our scheme to work with non-conjugate likelihoods and doing hyperparameter inference could also be fruitful next steps.\nAcknowledgements We thank the Gatsby Charitable Foundation for generous funding."
    } ],
    "references" : [ {
      "title" : "Stochastic blockmodels: Some first steps",
      "author" : [ "P. Holland", "K.B. Laskey", "S. Leinhardt" ],
      "venue" : "Social Networks, 5:109137",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Mixed membership stochastic blockmodel",
      "author" : [ "Edoardo M. Airoldi", "David M. Blei", "Stephen E. Fienberg", "Eric P. Xing" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Community structure in social and biological networks",
      "author" : [ "M. Girvan", "M.E.J. Newman" ],
      "venue" : "PNAS, 99:7821–7826",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Finding community structure in very large networks",
      "author" : [ "A. Clauset", "M.E.J. Newman", "C. Moore" ],
      "venue" : "Physics Review E, 70",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning systems of concepts with an infinite relational model",
      "author" : [ "Charles Kemp", "Joshua B. Tenenbaum", "Thomas L. Griffiths", "Takeshi Yamada", "Naonori Ueda" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Infinite hidden relational models",
      "author" : [ "Zhao Xu", "Volker Tresp", "Kai Yu", "Hans-Peter Kriegel" ],
      "venue" : "Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Bayesian community detection",
      "author" : [ "Morten Mørup", "Mikkel N. Schmidt" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Detecting hierarchical structure in networks",
      "author" : [ "T. Herlau", "M. Mørup", "M.N. Schmidt", "L.K. Hansen" ],
      "venue" : "Cognitive Information Processing",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Finding mixed-memberships in social networks",
      "author" : [ "Phaedon-Stelios Koutsourelakis", "Tina Eliassi-Rad" ],
      "venue" : "AAAI Spring Symposium on Social Information Processing (AAAI-SS’08),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "An infinite latent attribute model for network data",
      "author" : [ "Konstantina Palla", "David A. Knowles", "Zoubin Ghahramani" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Multiscale community blockmodel for network exploration",
      "author" : [ "Qirong Ho", "Ankur P. Parikh", "Le Song", "Erix P. Xing" ],
      "venue" : "Proceedings of the Fourteenth International Workshop on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "The Mondrian process",
      "author" : [ "D.M. Roy", "Y.W. Teh" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 21",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Bayesian hierarchical clustering",
      "author" : [ "K.A. Heller", "Z. Ghahramani" ],
      "venue" : "Proceedings of the International Conference on Machine Learning, volume 22",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Bayesian Rose trees",
      "author" : [ "C. Blundell", "Y. Teh", "K.A. Heller" ],
      "venue" : "UAI",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Estimation and prediction for stochastic blockmodels for graphs with latent block structure",
      "author" : [ "T. Snijders", "K. Nowicki" ],
      "venue" : "Journal of Classification, 14:75–100",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Bayesian approach to network modularity",
      "author" : [ "Jake M. Hofman", "Chris H. Wiggins" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "A novitiate in a period of change. an experimental and case study of social relationships",
      "author" : [ "S.F. Sampson" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1968
    }, {
      "title" : "Nonparametric latent feature models for link prediction",
      "author" : [ "Kurt T. Miller", "Thomas L. Griffiths", "Michael I. Jordan" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Euclidean embedding of co-occurrence data",
      "author" : [ "A. Globerson", "G. Chechik", "F. Pereira", "N. Tishby" ],
      "venue" : "Journal of Machine Learning Research, 8:2265–2295",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Infinite relational model implementation. http://www.psy.cmu.edu/ ̃ckemp/code/irm.html",
      "author" : [ "Charles Kemp" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "On triangular versus edge representations — towards scalable modeling of networks",
      "author" : [ "Q. Ho", "J. Yin", "E.P. Xing" ],
      "venue" : "Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The stochastic blockmodel [1] places each individual in a community according to the block structure of the social network’s adjacency matrix, whilst the mixed membership stochastic blockmodel [2] extends the stochastic blockmodel to allow individuals to belong to several flat communities simultaneously.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "The stochastic blockmodel [1] places each individual in a community according to the block structure of the social network’s adjacency matrix, whilst the mixed membership stochastic blockmodel [2] extends the stochastic blockmodel to allow individuals to belong to several flat communities simultaneously.",
      "startOffset" : 193,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Greedy hierarchical clustering has previously been applied directly to discovering hierarchical community structure [3].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "Such schemes are often computationally efficient, scaling quadratically in the number of individuals for a dense network, or linearly in the number of edges for a sparse network [4].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "The Infinite Relational Model (IRM; [5, 6, 7]) infers a flat community structure.",
      "startOffset" : 36,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "The Infinite Relational Model (IRM; [5, 6, 7]) infers a flat community structure.",
      "startOffset" : 36,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "The Infinite Relational Model (IRM; [5, 6, 7]) infers a flat community structure.",
      "startOffset" : 36,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "The IRM has been extended to infer hierarchies [8], by augmenting it with a tree, but comes at considerable computational cost.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 8,
      "context" : "[9] and [10] propose methods limited to hierarchies of depth two, whilst [11] propose methods limited to hierarchies of known depth.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[9] and [10] propose methods limited to hierarchies of depth two, whilst [11] propose methods limited to hierarchies of known depth.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 10,
      "context" : "[9] and [10] propose methods limited to hierarchies of depth two, whilst [11] propose methods limited to hierarchies of known depth.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "The Mondrian process [12] propose a flexible prior on trees and a likelihood model for relational data.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "Our work builds upon Bayesian approaches to greedy hierarchical clustering [13, 14] extending these approaches to relational data.",
      "startOffset" : 75,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : "Our work builds upon Bayesian approaches to greedy hierarchical clustering [13, 14] extending these approaches to relational data.",
      "startOffset" : 75,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "A stochastic blockmodel [1] consists of a partition, φ, of vertices V and for each pair of clusters p and q in φ, a parameter, θpq , giving the probability of a presence or absence of an edge between nodes of the clusters.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "Similar approaches have been taken to adapt the IRM to community detection [7], where non-conjugate priors were used at increased computational cost.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "One approach to inferring φ is to fix the number of communities K then use maximum likelihood estimation or Bayesian inference to assign vertices to each of the communities [1, 15].",
      "startOffset" : 173,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "One approach to inferring φ is to fix the number of communities K then use maximum likelihood estimation or Bayesian inference to assign vertices to each of the communities [1, 15].",
      "startOffset" : 173,
      "endOffset" : 180
    }, {
      "referenceID" : 15,
      "context" : "Another approach is to use variational Bayes, combined with an upper bound on the number of communities, to determine the number of communities and community assignments [16].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "The set of of partitions consistent with a tree T may be expressed formally as in [14]:",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "Following [14], we define πS = 1− (1− γ)|ch(S)|, where γ is a parameter of the model.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 13,
      "context" : "This choice of πS gives higher likelihood to non-binary trees over cascading binary trees when the data has no hierarchical structure [14].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Algorithm 1 is structurally the same as that in [14], and so has time complexity in O(n(2) log(n)).",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "Sampson’s Monastery Network Figure 3 shows the result of running six variants of BHCD on time four of Sampson’s monastery network [17].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 16,
      "context" : "[17], using a variety of methods, found four flat groups, shown at the top of Figure 3: Young Turks (Albert, Boniface, Gregory, Hugh, John Bosco, Mark, Winfrid), Loyal Opposition (Ambrose, Berthold, Bonaventure, Louis, Peter), Outcasts (Basil, Elias, Simplicius), and Interstitial group (Amand, Ramuald, Victor).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "NIPS-234 Next we applied BHCD to a subset of the NIPS co-authorship dataset [19].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : "Method Time complexity IRM (naı̈ve) O(n(2)K(2)IR) IRM (sparse) O(mK(2)IR) LFRM [18] O(n(2)F (2)IR) IMMM [9] O(n(2)K(2)IR) ILA [10] O(n(2)(F +K(2))IR) [8] O(n(2)K(2)IR) BHCD O(n(2) log(n)R)",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "Method Time complexity IRM (naı̈ve) O(n(2)K(2)IR) IRM (sparse) O(mK(2)IR) LFRM [18] O(n(2)F (2)IR) IMMM [9] O(n(2)K(2)IR) ILA [10] O(n(2)(F +K(2))IR) [8] O(n(2)K(2)IR) BHCD O(n(2) log(n)R)",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "Method Time complexity IRM (naı̈ve) O(n(2)K(2)IR) IRM (sparse) O(mK(2)IR) LFRM [18] O(n(2)F (2)IR) IMMM [9] O(n(2)K(2)IR) ILA [10] O(n(2)(F +K(2))IR) [8] O(n(2)K(2)IR) BHCD O(n(2) log(n)R)",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "Method Time complexity IRM (naı̈ve) O(n(2)K(2)IR) IRM (sparse) O(mK(2)IR) LFRM [18] O(n(2)F (2)IR) IMMM [9] O(n(2)K(2)IR) ILA [10] O(n(2)(F +K(2))IR) [8] O(n(2)K(2)IR) BHCD O(n(2) log(n)R)",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 19,
      "context" : "search, using the publicly available C implementation[20].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 17,
      "context" : "Following [18, 10] we restricted the network to the 234 most connected individuals.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "Following [18, 10] we restricted the network to the 234 most connected individuals.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "We used the same 10 cross-validation folds as used in [10] and so our results are quantitatively comparable to their results for the Latent Factor Relational Model (LFRM [18]) and their model, the Infinite Latent Attributes model (ILA).",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "We used the same 10 cross-validation folds as used in [10] and so our results are quantitatively comparable to their results for the Latent Factor Relational Model (LFRM [18]) and their model, the Infinite Latent Attributes model (ILA).",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 20,
      "context" : "Another way to scale up the model would be to investigate parameterising the network using the sufficient statistics of triangles, instead of edges as in [21].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "Others [7] have found that non-conjugate likelihoods can yield improved predictions—thus adapting our scheme to work with non-conjugate likelihoods and doing hyperparameter inference could also be fruitful next steps.",
      "startOffset" : 7,
      "endOffset" : 10
    } ],
    "year" : 2013,
    "abstractText" : "We propose an efficient Bayesian nonparametric model for discovering hierarchical community structure in social networks. Our model is a tree-structured mixture of potentially exponentially many stochastic blockmodels. We describe a family of greedy agglomerative model selection algorithms that take just one pass through the data to learn a fully probabilistic, hierarchical community model. In the worst case, Our algorithms scale quadratically in the number of vertices of the network, but independent of the number of nested communities. In practice, the run time of our algorithms are two orders of magnitude faster than the Infinite Relational Model, achieving comparable or better accuracy.",
    "creator" : null
  }
}