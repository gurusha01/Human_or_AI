{
  "name" : "74071a673307ca7459bcf75fbd024e09.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The Pareto Regret Frontier",
    "authors" : [ "Wouter M. Koolen" ],
    "emails" : [ "wouter.koolen@qut.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "One of the central problems studied in online learning is prediction with expert advice. In this task a learner is given access to K strategies, customarily referred to as experts. He needs to make a sequence of T decisions with the objective of performing as well as the best expert in hindsight. This goal can be achieved with modest overhead, called regret. Typical algorithms, e.g. Hedge [1] with learning rate η = √ 8/T lnK, guarantee\nLT − LkT ≤ √ T/2 lnK for each expert k. (1)\nwhere LT and LkT are the cumulative losses of the learner and expert k after all T rounds.\nHere we take a closer look at that right-hand side. For it is not always desirable to have a uniform regret bound w.r.t. all experts. Instead, we may want to single out a few special experts and demand to be really close to them, at the cost of increased overhead compared to the rest. When the number of experts K is large or infinite, such favouritism even seems unavoidable for non-trivial regret bounds. The typical proof of the regret bound (1) suggests that the following can be guaranteed as well. For each choice of probability distribution q on experts, there is an algorithm that guarantees\nLT − LkT ≤ √ T/2(− ln q(k)) for each expert k. (2)\nHowever, it is not immediately obvious how this can be achieved. For example, the Hedge learning rate η would need to be tuned differently for different experts. We are only aware of a single (complex) algorithm that achieves something along these lines [2]. On the flip side, it is also not obvious that this trade-off profile is optimal.\nIn this paper we study the Pareto (achievable and non-dominated) regret trade-offs. Let us say that a candidate trade-off 〈r1, . . . , rK〉 ∈ RK is T -realisable if there is an algorithm that guarantees\nLT − LkT ≤ rk for each expert k.\nWhich trade-offs are realisable? Among them, which are optimal? And what is the strategy that witnesses these realisable strategies?"
    }, {
      "heading" : "1.1 This paper",
      "text" : "We resolve the preceding questions for the simplest case of absolute loss, where K = 2. We first obtain an exact characterisation of the set of realisable trade-offs. We then construct for each realisable profile a witnessing strategy. We also give a randomised procedure for optimal play that extends the randomised procedures for balanced regret profiles from [3] and later [4, 5].\nWe then focus on the relation between priors and regret bounds, to see if the particular form (2) is achievable, and if so, whether it is optimal. To this end, we characterise the asymptotic Pareto frontier as T → ∞. We find that the form (2) is indeed achievable but fundamentally sub-optimal. This is of philosophical interest as it hints that approaching absolute loss by essentially reducing it to information theory (including Bayesian and Minimum Description Length methods, relative entropy based optimisation (instance of Mirror Descent), Defensive Forecasting etc.) is lossy.\nFinally, we show that our solution for absolute loss equals that ofK = 2 experts with bounded linear loss. We then show how to obtain the bound (1) for K ≥ 2 experts using a recursive combination of two-expert predictors. Counter-intuitively, this cannot be achieved with a balanced binary tree of predictors, but requires the most unbalanced tree possible. Recursive combination with non-uniform prior weights allows us to obtain (2) (with higher constant) for any prior q."
    }, {
      "heading" : "1.2 Related work",
      "text" : "Our work lies in the intersection of two lines of work, and uses ideas from both. On the one hand there are the game-theoretic (minimax) approaches to prediction with expert advice. In [6] CesaBianchi, Freund, Haussler, Helmbold, Schapire and Warmuth analysed the minimax strategy for absolute loss with a known time horizon T . In [5] Cesa-Bianchi and Shamir used random walks to implement it efficiently for K = 2 experts or K ≥ 2 static experts. A similar analysis was given by Koolen in [4] with an application to tracking. In [7] Abernethy, Langford and Warmuth obtained the optimal strategy for absolute loss with experts that issue binary predictions, now controlling the game complexity by imposing a bound on the loss of the best expert. Then in [3] Abernethy, Warmuth and Yellin obtained the worst case optimal algorithm for K ≥ 2 arbitrary experts. More general budgets were subsequently analysed by Abernethy and Warmuth in [8]. Connections between minimax values and algorithms were studied by Rakhlin, Shamir and Sridharan in [9].\nOn the other hand there are the approaches that do not treat all experts equally. Freund and Schapire obtain a non-uniform bound for Hedge in [1] using priors, although they leave the tuning problem open. The tuning problem was addressed by Hutter and Poland in [2] using two-stages of Follow the Perturbed Leader. Even-Dar, Kearns, Mansour and Wortman characterise the achievable tradeoffs when we desire especially small regret compared to a fixed average of the experts’ losses in [10]. Their bounds were subsequently tightened by Kapralov and Panigrahy in [11]. An at least tangentially related problem is to ensure smaller regret when there are several good experts. This was achieved by Chaudhuri, Freund and Hsu in [12], and later refined by Chernov and Vovk in [13]."
    }, {
      "heading" : "2 Setup",
      "text" : "The absolute loss game is one of the core decision problems studied in online learning [14]. In it, the learner sequentially predicts T binary outcomes. Each round t ∈ {1, . . . , T} the learner assigns a probability pt ∈ [0, 1] to the next outcome being a 1, after which the actual outcome xt ∈ {0, 1} is revealed, and the learner suffers absolute loss |pt− xt|. Note that absolute loss equals expected 0/1 loss, that is, the probability of a mistake if a “hard” prediction in {0, 1} is sampled with bias p on 1. Realising that the learner cannot avoid high cumulative loss without assumptions on the origin of the outcomes, the learner’s objective is defined to ensure low cumulative loss compared to a fixed set of baseline strategies. Meeting this goal ensures that the easier the outcome sequence (i.e. for which some reference strategy has low loss), the lower the cumulative loss incurred by the learner.\nThe regret w.r.t. the strategy k ∈ {0, 1} that always predicts k is given by 1\nRkT := T∑ t=1 ( |pt − xt| − |k − xt| ) .\nMinimising regret, defined in this way, is a multi-objective optimisation problem. The classical approach is to “scalarise” it into the single objective RT := maxk RkT , that is, to ensure small regret compared to the best expert in hindsight. In this paper we study the full Pareto trade-off curve.\nDefinition 1. A candidate trade-off 〈r0, r1〉 ∈ R2 is called T -realisable for the T -round absolute loss game if there is a strategy that keeps the regret w.r.t. each k ∈ {0, 1} below rk, i.e. if\n∃p1∀x1 · · · ∃pT∀xT : R0T ≤ r0 and R1T ≤ r1\nwhere pt ∈ [0, 1] and xt ∈ {0, 1} in each round t. We denote the set of all T -realisable pairs by GT .\nThis definition extends easily to other losses, many experts, fancy reference combinations of experts (e.g. shifts, drift, mixtures), protocols with side information etc. We consider some of these extension in Section 5, but for now our goal is to keep it as simple as possible."
    }, {
      "heading" : "3 The exact regret trade-off profile",
      "text" : "In this section we characterise the set GT ⊂ R2 of T -realisable trade-offs. We show that it is a convex polygon, that we subsequently characterise by its vertices and edges. We also exhibit the optimal strategy witnessing each Pareto optimal trade-off and discuss the connection with random walks. We first present some useful observations about GT .\nThe linearity of the loss as a function of the prediction already renders GT highly regular. Lemma 2. The set GT of T -realisable trade-offs is convex for each T .\nProof. Take rA and rB in GT . We need to show that αrA+ (1−α)rB ∈ GT for all α ∈ [0, 1]. Let A and B be strategies witnessing the T -realisability of these points. Now consider the strategy that in each round t plays the mixture αpAt + (1− α)pBt . As the absolute loss is linear in the prediction, this strategy guaranteesLT = αLAT +(1−α)LBT ≤ LkT+αrAk +(1−α)rBk for each k ∈ {0, 1}.\nGuarantees violated early cannot be restored later.\nLemma 3. A strategy that guarantees RkT ≤ rk must maintain Rkt ≤ rk for all 0 ≤ t ≤ T .\n1One could define the regret RkT for all static reference probabilities k ∈ [0, 1], but as the loss is minimised by either k = 0 or k = 1, we immediately restrict to only comparing against these two.\nProof. Suppose toward contradiction that Rkt > rk at some t < T . An adversary may set all xt+1 . . . xT to k to fix LkT = L k t . As LT ≥ Lt, we haveRkT = LT−LkT ≥ Lt−Lkt = Rkt > rk.\nThe two extreme trade-offs 〈0, T 〉 and 〈T, 0〉 are Pareto optimal. Lemma 4. Fix horizon T and r1 ∈ R. The candidate profile 〈0, r1〉 is T -realisable iff r1 ≥ T .\nProof. The static strategy pt = 0 witnesses 〈0, T 〉 ∈ GT for every horizon T . To ensure R1T < T , any strategy will have to play pt > 0 at some time t ≤ T . But then it cannot maintain R0t = 0.\nIt is also intuitive that maintaining low regret becomes progressively harder with T . Lemma 5. G0 ⊃ G1 ⊃ . . .\nProof. Lemma 3 establishes ⊇, whereas Lemma 4 establishes 6=.\nWe now come to our first main result, the characterisation of GT . We will directly characterise its south-west frontier, that is, the set of Pareto optimal trade-offs. These frontiers are graphed up to T = 10 in Figure 1a. The vertex numbering we introduce below is illustrated by Figure 1b. Theorem 6. The Pareto frontier of GT is the piece-wise linear curve through the T + 1 vertices〈\nfT (i), fT (T − i) 〉 for i ∈ {0, . . . , T} where fT (i) := i∑\nj=0\nj2j−T ( T − j − 1 T − i− 1 ) .\nMoreover, for T > 0 the optimal strategy at vertex i assigns to the outcome x = 1 the probability\npT (0) := 0, pT (T ) := 1, and pT (i) := fT−1(i)− fT−1(i− 1)\n2 for 0 < i < T,\nand the optimal probability interpolates linearly in between consecutive vertices.\nProof. By induction on T . We first consider the base case T = 0. By Definition 1 G0 = { 〈r0, r1〉 ∣∣ r0 ≥ 0 and r1 ≥ 0} is the positive orthant, which has the origin as its single Pareto optimal vertex, and indeed 〈f0(0), f0(0)〉 = 〈0, 0〉. We now turn to T ≥ 1. Again by Definition 1 〈r0, r1〉 ∈ GT if\n∃p ∈ [0, 1]∀x ∈ {0, 1} : 〈 r0 − |p− x|+ |0− x|, r1 − |p− x|+ |1− x| 〉 ∈ GT−1,\nthat is if ∃p ∈ [0, 1] : 〈 r0 − p, r1 − p+ 1 〉 ∈ GT−1 and 〈 r0 + p, r1 + p− 1 〉 ∈ GT−1.\nBy the induction hypothesis we know that the south-west frontier curve for GT−1 is piecewise linear. We will characterise GT via its frontier as well. For each r0, let r1(r0) and p(r0) denote the value and minimiser of the optimisation problem\nmin p∈[0,1]\n{ r1 ∣∣ both 〈r0, r1〉 ± 〈p, p− 1〉 ∈ GT−1}.\nWe also refer to 〈r0, r1(r0)〉 ± 〈p(r0), p(r0) − 1〉 as the rear(−) and front(+) contact points. For r0 = 0 we find r1(0) = T , with witness p(0) = 0 and rear/front contact points 〈0, T + 1〉 and 〈0, T − 1〉, and for r0 = T we find r1(T ) = 0 with witness p(T ) = 1 and rear/front contact points 〈T − 1, 0〉 and 〈T + 1, 0〉. It remains to consider the intermediate trajectory of r1(r0) as r0 runs from 0 to T . Initially at r0 = 0 the rear contact point lies on the edge of GT−1 entering vertex i = 0 of GT−1, while the front contact point lies on the edge emanating from that same vertex. So if we increase r0 slightly, the contact points will slide along their respective lines. By Lemma 11 (supplementary material), r1(r0) will trace along a straight line as a result. Once we increase r0 enough, both the rear and front contact point will hit the vertex at the end of their edges simultaneously (a fortunate fact that greatly simplifies our analysis), as shown in Lemma 12 (supplementary material). The contact points then transition to tracing the next pair of edges of GT−1. At this point r0 the slope of r1(r0) changes, and we have discovered a vertex of GT .\nGiven that at each such transition 〈r0, r1(r0)〉 is the midpoint between both contact points, this implies that all midpoints between successive vertices of GT−1 are vertices of GT . And in addition, there are the two boundary vertices 〈0, T 〉 and 〈T, 0〉.\ncurve\n〈√\n− ln(q),\n√\n− ln(1− q)\n〉\nfor all q ∈ [0, 1]."
    }, {
      "heading" : "3.1 The optimal strategy and random walks",
      "text" : "In this section we describe how to follow the optimal strategy. First suppose we desire to witness a T -realisable trade-off that happens to be a vertex of GT , say vertex i at 〈fT (i), fT (T − i)〉. With T rounds remaining and in state i, the strategy predicts with pT (i). Then the outcome x ∈ {0, 1} is revealed. If x = 0, we need to witness in the remaining T − 1 rounds the trade-off 〈fT (i), fT (T − i)〉 − 〈pT (i), pT (i) + 1〉 = 〈fT−1(i − 1), fT−1(T − 1)〉, which is vertex i − 1 of GT−1. So the strategy transition to state i− 1. Similarly upon x = 1 we update our internal state to i. If the state ever either exceeds the number of rounds remaining or goes negative we simply clamp it.\nSecond, if we desire to witness a T -realisable trade-off that is a convex combination of successive vertices, we simply follow the mixture strategy as constructed in Lemma 2. Third, if we desire to witness a sub-optimal element of GT , we may follow any strategy that witnesses a Pareto optimal dominating trade-off.\nThe probability p issued by the algorithm is sometimes used to randomly sample a “hard prediction” from {0, 1}. The expression |p − x| then denotes the expected loss, which equals the probability of making a mistake. We present, following [3], a random-walk based method to sample a 1 with probability pT (i). Our random walk starts in state 〈T, i〉. In each round it transitions from state 〈T, i〉 to either state 〈T − 1, i〉 or state 〈T − 1, i − 1〉 with equal probability. It is stopped when the state 〈T, i〉 becomes extreme in the sense that i ∈ {0, T}. Note that this process always terminates. Then the probability that this process is stopped with i = T equals pT (i). In our case of absolute loss, evaluating pT (i) and performing the random walk both take T units of time. The random walks considered in [3] for K ≥ 2 experts still take T steps, whereas direct evaluation of the optimal strategy scales rather badly with K."
    }, {
      "heading" : "4 The asymptotic regret rate trade-off profile",
      "text" : "In the previous section we obtained for each time horizon T a combinatorial characterisation of the set GT of T -realisable trade-offs. In this section we show that properly normalised Pareto frontiers for increasing T are better and better approximations of a certain intrinsic smooth limit curve. We obtain a formula for this curve, and use it to study the question of realisability for large T .\nDefinition 7. Let us define the set G of asymptotically realisable regret rate trade-offs by\nG := lim T→∞ GT√ T .\nDespite the disappearance of the horizon T from the notation, the set G still captures the trade-offs that can be achieved with prior knowledge of T . Each achievable regret rate trade-off 〈ρ0, ρ1〉 ∈ G\nmay be witnessed by a different strategy for each T . This is fine for our intended interpretation of√ T G as a proxy for GT . We briefly mention horizon-free algorithms at the end of this section.\nThe literature [2] suggests that, for some constant c, 〈 √ −c ln(q), √ −c ln(1− q)〉 should be asymptotically realisable for each q ∈ [0, 1]. We indeed confirm this below, and determine the optimal constant to be c = 1. We then discuss the philosophical implications of the quality of this bound.\nWe now come to our second main result, the characterisation of the asymptotically realisable tradeoff rates. The Pareto frontier is graphed in Figure 2 both on normal axes for comparison to Figure 1a, and on a log-log scale to show its tails. Note the remarkable quality of approximation to GT / √ T .\nTheorem 8. The Pareto frontier of the set G of asymptotically realisable trade-offs is the curve〈 f(u), f(−u) 〉 for u ∈ R, where f(u) := u erf (√ 2u ) + e−2u 2\n√ 2π + u,\nand erf(u) = 2√ π ∫ u 0 e−v 2 dv is the error function. Moreover, the optimal strategy converges to\np(u) = 1− erf\n(√ 2u )\n2 .\nProof. We calculate the limit of normalised Pareto frontiers at vertex i = T/2 + u √ T , and obtain\nlim T→∞\nfT ( T/2 + u √ T )\n√ T\n= lim T→∞ 1√ T\nT/2+u √ T∑\nj=0\nj2j−T (\nT − j − 1 T/2− u √ T − 1\n)\n= lim T→∞ 1√ T ∫ T/2+u√T 0 j2j−T ( T − j − 1 T/2− u √ T − 1 ) dj\n= lim T→∞ ∫ u − √ T/2 (u− v)2(u−v) √ T−T ( T − (u− v) √ T − 1 T/2− u √ T − 1 )√ T dv\n= ∫ u −∞ (u− v) lim T→∞ 2(u−v) √ T−T ( T − (u− v) √ T − 1 T/2− u √ T − 1 )√ T dv\n= ∫ u −∞ (u− v)e − 12 (u+v) 2 √ 2π dv = u erf (√ 2u ) + e−2u 2 √ 2π + u\nIn the first step we replace the sum by an integral. We can do this as the summand is continuous in j, and the approximation error is multiplied by 2−T and hence goes to 0 with T . In the second step we perform the variable substitution v = u− j/ √ T . We then exchange limit and integral, subsequently evaluate the limit, and in the final step we evaluate the integral.\nTo obtain the optimal strategy, we observe the following relation between the slope of the Pareto curve and the optimal strategy for each horizon T . Let g and h denote the Pareto curves at times T and T +1 as a function of r0. The optimal strategy p for T +1 at r0 satisfied the system of equations\nh(r0) + p− 1 = g(u+ p) h(r0)− p+ 1 = g(u− p)\nto which the solution satisfies\n1− 1 p = g(r0 + p)− g(r0 − p) 2p ≈ dg(r0) dr0 , so that p ≈ 1\n1− dg(r0)dr0 .\nSince slope is invariant under normalisation, this relation between slope and optimal strategy becomes exact as T tends to infinity, and we find\np(u) = 1\n1 + df(r0(u))dr0(u) =\n1\n1 + f ′(u)\nf ′(−u)\n= 1− erf\n(√ 2u )\n2 .\nWe believe this last argument is more insightful than a direct evaluation of the limit."
    }, {
      "heading" : "4.1 Square root of min log prior",
      "text" : "Results for Hedge suggest — modulo a daunting tuning problem — that a trade-off featuring square root negative log prior akin to (2) should be realisable. We first show that this is indeed the case, we then determine the optimal leading constant and we finally discuss its sub-optimality.\nTheorem 9. The parametric curve 〈√ −c ln(q), √ −c ln(1− q) 〉 for q ∈ [0, 1] is contained in G (i.e. asymptotically realisable) iff c ≥ 1.\nProof. By Theorem 8, the frontier of G is of the form 〈f(u), f(−u)〉. Our argument revolves around the tails (extreme u) of G. For large u 0, we find that f(u) ≈ 2u. For small u 0, we find that f(u) ≈ e −2u2\n4 √ 2πu2 . This is obtained by a 3rd order Taylor series expansion around u = −∞. We need to go to 3rd order since all prior orders evaluate to 0. The additive approximation error is of order e−2u 2 u−4, which is negligible. So for large r0 0, the least realisable r1 is approximately\nr1 ≈ e− r20 2 −2 ln r0 √\n2π . (3)\nWith the candidate relation r0 = √ −c ln(q) and r1 = √ −c ln(1− q), still for large r0 0 so that q is small and − ln(1− q) ≈ q, we would instead find least realisable r1 approximately equal to\nr1 ≈ √ ce− r20 2c . (4)\nThe candidate tail (4) must be at least the actual tail (3) for all large r0. The minimal c for which this holds is c = 1. The graphs of Figure 2 illustrate this tail behaviour for c = 1, and at the same time verify that there are no violations for moderate u.\nEven though the sqrt-min-log-prior trade-off is realisable, we see that its tail (4) exceeds the actual tail (3) by the factor r20 √ 2π, which gets progressively worse with the extremity of the tail r0. Figure 2a shows that its behaviour for moderate 〈r0, r1〉 is also not brilliant. For example it gives us a symmetric bound of √ ln 2 ≈ 0.833, whereas f(0) = 1/ √ 2π ≈ 0.399 is optimal.\nFor certain log loss games, each Pareto regret trade-off is witnessed uniquely by the Bayesian mixture of expert predictions w.r.t. a certain non-uniform prior and vice versa (not shown). In this sense the Bayesian method is the ideal answer to data compression/investment/gambling. Be that as it may, we conclude that the world of absolute loss is not information theory: simply putting a prior is not the definitive answer to non-uniform guarantees. It is a useful intuition that leads to the convenient sqrt-min-log-prior bounds. We hope that our results contribute to obtaining tighter bounds that remain manageable."
    }, {
      "heading" : "4.2 The asymptotic algorithm",
      "text" : "The previous theorem immediately suggests an approximate algorithm for finite horizon T . To approximately witness 〈r0, r1〉, find the value of u for which √ T 〈f(u), f(−u)〉 is closest to it. Then play p(u). This will not guarantee 〈r0, r1〉 exactly, but intuitively it will be close. We leave analysing this idea to the journal version. Conversely, by taking the limit of the game protocol, which involves the absolute loss function, we might obtain an interesting protocol and “asymptotic” loss function2, for which u is the natural state, p(u) is the optimal strategy, and u is updated in a certain way. Investigating such questions will probably lead to interesting insights, for example horizon-free strategies that maintain RkT / √ T ≤ ρk for all T simultaneously. Again this will be pursued for the journal version.\n2 We have seen an instance of this before. When the Hedge algorithm with learning rate η plays weights w and faces loss vector `, its dot loss is given by wT `. Now consider the same loss vector handed out in identical pieces `/n over the course of n trials, during which the weights w update as usual. In the limit of n→∞, the resulting loss becomes the mix loss − 1\nη ln ∑ k w(k)e −η`k ."
    }, {
      "heading" : "5 Extension",
      "text" : ""
    }, {
      "heading" : "5.1 Beyond absolute loss",
      "text" : "In this section we consider the general setting with K = 2 expert, that we still refer to as 0 and 1. Here the learner plays p ∈ [0, 1] which is now interpreted as the weight allocated to expert 1, the adversary chooses a loss vector ` = 〈`0, `1〉 ∈ [0, 1]2, and the learner incurs dot loss given by (1− p)`0 + p`1. The regrets are now redefined as follows\nRkT := T∑ t=1 pt` 1 t + (1− pt)`0t − T∑ t=1 `kt for each expert k ∈ {0, 1}.\nTheorem 10. The T -realisable trade-offs for absolute loss and K = 2 expert dot loss coincide.\nProof. By induction on T . The loss is irrelevant in the base case T = 0. For T > 0, a trade-off 〈r0, r1〉 is T -realisable for dot loss if\n∃p ∈ [0, 1]∀` ∈ [0, 1]2 : 〈r0 + p`1 + (1− p)`0 − `0, r1 + p`1 + (1− p)`0 − `1〉 ∈ GT−1 that is if ∃p ∈ [0, 1]∀δ ∈ [−1, 1] : 〈r0 − pδ, r1 + (1− p)δ〉 ∈ GT−1 . We recover the absolute loss case by restricting δ to {−1, 1}. These requirements are equivalent since GT is convex by Lemma 2."
    }, {
      "heading" : "5.2 More than 2 experts",
      "text" : "In the general experts problem we compete with K instead of 2 experts. We now argue that an algorithm guaranteeing RkT ≤ √ cT lnK w.r.t. each expert k can be obtained. The intuitive approach, combining the K experts in a balanced binary tree of two-expert predictors, does not achieve this goal: each internal node contributes the optimal symmetric regret of √ T/(2π). This accumulates to RkT ≤ lnK √ cT , where the log sits outside the square root. Counter-intuitively, the maximally unbalanced binary tree does result in a √\nlnK factor when the internal nodes are properly skewed. At each level we combineK experts one-vs-all, permitting large regret w.r.t. the first expert but tiny regret w.r.t. the recursive combination of the remaining K − 1 experts. The argument can be found in Appendix A.1. The same argument shows that, for any prior q on k = 1, 2, . . ., combining the expert with the smallest prior with the recursive combination of the rest guarantees regret √ −cT ln q(k) w.r.t. each expert k."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We studied asymmetric regret guarantees for the fundamental online learning setting of the absolute loss game. We obtained exactly the achievable skewed regret guarantees, and the corresponding optimal algorithm. We then studied the profile in the limit of large T . We conclude that the expected√ T 〈 √ − ln(q), √ − ln(1− q)〉 trade-off is achievable for any prior probability q ∈ [0, 1], but that it is not tight. We then showed how our results transfer from absolute loss to general linear losses, and to more than two experts.\nMajor next steps are to determine the optimal trade-offs forK > 2 experts, to replace our traditional √ T budget by modern variants √ LkT [15], √ LkT (T−LkT ) T [16], √ VarmaxT [17], √ D∞ [18], ∆T [19] etc. and to find the Pareto frontier for horizon-free strategies maintaining RkT ≤ ρk √ T at any T ."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work benefited substantially from discussions with Peter Grünwald."
    } ],
    "references" : [ {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1997
    }, {
      "title" : "Adaptive online prediction by following the perturbed leader",
      "author" : [ "Marcus Hutter", "Jan Poland" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "When random play is optimal against an adversary",
      "author" : [ "Jacob Abernethy", "Manfred K. Warmuth", "Joel Yellin" ],
      "venue" : "Omnipress,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Combining Strategies Efficiently: High-quality Decisions from Conflicting Advice. PhD thesis, Institute of Logic, Language and Computation (ILLC)",
      "author" : [ "Wouter M. Koolen" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Efficient online learning via randomized rounding",
      "author" : [ "Nicolò Cesa-Bianchi", "Ohad Shamir" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "How to use expert advice",
      "author" : [ "Nicolò Cesa-Bianchi", "Yoav Freund", "David Haussler", "David P. Helmbold", "Robert E. Schapire", "Manfred K. Warmuth" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Continuous experts and the Binning algorithm",
      "author" : [ "Jacob Abernethy", "John Langford", "Manfred K Warmuth" ],
      "venue" : "In Learning Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Repeated games against budgeted adversaries",
      "author" : [ "Jacob Abernethy", "Manfred K. Warmuth" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Relax and randomize : From value to algorithms",
      "author" : [ "Sasha Rakhlin", "Ohad Shamir", "Karthik Sridharan" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Regret to the best vs. regret to the average",
      "author" : [ "Eyal Even-Dar", "Michael Kearns", "Yishay Mansour", "Jennifer Wortman" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Prediction strategies without loss",
      "author" : [ "Michael Kapralov", "Rina Panigrahy" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "A parameter-free hedging algorithm",
      "author" : [ "Kamalika Chaudhuri", "Yoav Freund", "Daniel Hsu" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Prediction with advice of unknown number of experts",
      "author" : [ "Alexey V. Chernov", "Vladimir Vovk" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Adaptive and self-confident on-line learning algorithms",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Claudio Gentile" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "Nicolò Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Extracting certainty from uncertainty: Regret bounded by variation in costs",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Online optimization with gradual variations",
      "author" : [ "Chao-Kai Chiang", "Shenghuo Zhu" ],
      "venue" : "In Proceedings of the 25th Annual Conference on Learning Theory, number 23 in JMLR W&CP,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Follow the leader if you can, Hedge if you must",
      "author" : [ "Steven de Rooij", "Tim van Erven", "Peter D. Grünwald", "Wouter M. Koolen" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Hedge [1] with learning rate η = √ 8/T lnK, guarantee",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "We are only aware of a single (complex) algorithm that achieves something along these lines [2].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "We also give a randomised procedure for optimal play that extends the randomised procedures for balanced regret profiles from [3] and later [4, 5].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "We also give a randomised procedure for optimal play that extends the randomised procedures for balanced regret profiles from [3] and later [4, 5].",
      "startOffset" : 140,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "We also give a randomised procedure for optimal play that extends the randomised procedures for balanced regret profiles from [3] and later [4, 5].",
      "startOffset" : 140,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "In [6] CesaBianchi, Freund, Haussler, Helmbold, Schapire and Warmuth analysed the minimax strategy for absolute loss with a known time horizon T .",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "In [5] Cesa-Bianchi and Shamir used random walks to implement it efficiently for K = 2 experts or K ≥ 2 static experts.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "A similar analysis was given by Koolen in [4] with an application to tracking.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 6,
      "context" : "In [7] Abernethy, Langford and Warmuth obtained the optimal strategy for absolute loss with experts that issue binary predictions, now controlling the game complexity by imposing a bound on the loss of the best expert.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "Then in [3] Abernethy, Warmuth and Yellin obtained the worst case optimal algorithm for K ≥ 2 arbitrary experts.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 7,
      "context" : "More general budgets were subsequently analysed by Abernethy and Warmuth in [8].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "Connections between minimax values and algorithms were studied by Rakhlin, Shamir and Sridharan in [9].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "Freund and Schapire obtain a non-uniform bound for Hedge in [1] using priors, although they leave the tuning problem open.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "The tuning problem was addressed by Hutter and Poland in [2] using two-stages of Follow the Perturbed Leader.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "Even-Dar, Kearns, Mansour and Wortman characterise the achievable tradeoffs when we desire especially small regret compared to a fixed average of the experts’ losses in [10].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 10,
      "context" : "Their bounds were subsequently tightened by Kapralov and Panigrahy in [11].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : "This was achieved by Chaudhuri, Freund and Hsu in [12], and later refined by Chernov and Vovk in [13].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "This was achieved by Chaudhuri, Freund and Hsu in [12], and later refined by Chernov and Vovk in [13].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "The absolute loss game is one of the core decision problems studied in online learning [14].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "We present, following [3], a random-walk based method to sample a 1 with probability pT (i).",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "The random walks considered in [3] for K ≥ 2 experts still take T steps, whereas direct evaluation of the optimal strategy scales rather badly with K.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 1,
      "context" : "The literature [2] suggests that, for some constant c, 〈 √ −c ln(q), √ −c ln(1− q)〉 should be asymptotically realisable for each q ∈ [0, 1].",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "Major next steps are to determine the optimal trade-offs forK > 2 experts, to replace our traditional √ T budget by modern variants √ LT [15], √ LT (T−LT ) T [16], √ Varmax T [17], √ D∞ [18], ∆T [19] etc.",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "Major next steps are to determine the optimal trade-offs forK > 2 experts, to replace our traditional √ T budget by modern variants √ LT [15], √ LT (T−LT ) T [16], √ Varmax T [17], √ D∞ [18], ∆T [19] etc.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 16,
      "context" : "Major next steps are to determine the optimal trade-offs forK > 2 experts, to replace our traditional √ T budget by modern variants √ LT [15], √ LT (T−LT ) T [16], √ Varmax T [17], √ D∞ [18], ∆T [19] etc.",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "Major next steps are to determine the optimal trade-offs forK > 2 experts, to replace our traditional √ T budget by modern variants √ LT [15], √ LT (T−LT ) T [16], √ Varmax T [17], √ D∞ [18], ∆T [19] etc.",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 18,
      "context" : "Major next steps are to determine the optimal trade-offs forK > 2 experts, to replace our traditional √ T budget by modern variants √ LT [15], √ LT (T−LT ) T [16], √ Varmax T [17], √ D∞ [18], ∆T [19] etc.",
      "startOffset" : 195,
      "endOffset" : 199
    } ],
    "year" : 2013,
    "abstractText" : "Performance guarantees for online learning algorithms typically take the form of regret bounds, which express that the cumulative loss overhead compared to the best expert in hindsight is small. In the common case of large but structured expert sets we typically wish to keep the regret especially small compared to simple experts, at the cost of modest additional overhead compared to more complex others. We study which such regret trade-offs can be achieved, and how. We analyse regret w.r.t. each individual expert as a multi-objective criterion in the simple but fundamental case of absolute loss. We characterise the achievable and Pareto optimal trade-offs, and the corresponding optimal strategies for each sample size both exactly for each finite horizon and asymptotically.",
    "creator" : null
  }
}