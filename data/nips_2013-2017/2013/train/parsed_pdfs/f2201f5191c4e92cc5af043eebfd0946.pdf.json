{
  "name" : "f2201f5191c4e92cc5af043eebfd0946.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Firing rate predictions in optimal balanced networks",
    "authors" : [ "David G.T. Barrett", "Sophie Denève", "Christian K. Machens" ],
    "emails" : [ "david.barrett@ens.fr", "sophie.deneve@ens.fr", "christian.machens@neuro.fchampalimaud.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The firing rate of a neuron is arguably the most important characterisation of both neural network dynamics and neural computation, and has been ever since the seminal recordings of Adrian and Zotterman [1] in which the firing rate of a neuron was observed to increase with muscle tension. A large, sometimes bewildering, diversity of firing rate responses to stimuli have since been observed [2], ranging from sigmoidal-shaped tuning curves [3, 4], to bump-shaped tuning curves [5], with much diversity in between [6]. What is the computational role of these firing rate responses and how are firing rates determined by neuron dynamics, network connectivity and neural input?\nThere have been many attempts to answer these questions, using a variety of experimental and theoretical techniques. However, most approaches have struggled to deal with the non-linearity of neural spike-generation mechanisms and the strong interaction between neurons as mediated through network connectivity. Significant progress has been made using linear approximations. For example, experimentally recorded firing rates in a variety of systems have been described using the linear receptive field, which captures the linear relationship between stimulus and firing rate response [7]. However, in recent years, it has been found that this linear approximation often fails to capture important aspects of neural activity [8]. Similarly, in theoretical studies, linear approximations\nhave been used to simplify non-linear firing rate calculations in a variety of network models, using Taylor Series approximations [9], and more recently, using linear response theory [10, 11]. These calculations have led to important insights into how neural network connectivity and input determine firing rates. Again, however, these calculations only apply to a restricted subset of situations, where the linearising assumptions apply.\nWe develop a new technique for calculating firing rates, by directly identifying the non-linear structure of tightly balanced networks. Balanced network theory has come to be regarded as the standard model of cortical activity [12, 13], accounting for a large proportion of observed activity through a dynamic balance of excitation and inhibition [14]. Recently, it was found that tightly balanced networks are synonymous with efficient coding, in which a signal is represented optimally subject to metabolic costs [15]. This observation allows us, here, to interpret balanced network activity as an optimisation algorithm. We can then directly identify that the non-linear relationship between firing rates, input, connectivity and neural computation is provided by this algorithm. We use this technique to calculate firing rates in a variety of balanced network models, thereby exploring the computational role and underlying network mechanisms of monotonic firing rate tuning curves, bump-shaped tuning curves and tuning curve inhomogeneity."
    }, {
      "heading" : "2 Optimal balanced network models",
      "text" : "We calculate firing rates in a balanced network consisting of N recurrently connected leaky integrate-and-fire neurons (Fig. 1a). The network is driven by an input signal I = (I1, . . . , Ik, . . . IM ), where Ik is the kth input and M is the dimension of the input. In response to this input, neurons produce spike trains, denoted by s = (s1, . . . , si, . . . , sN ), where si(t) = ∑ k δ(t − tik) is the spike train of neuron i with spike times { tik }\n. A spike is produced whenever the membrane potential Vi exceeds the spiking threshold Ti of neuron i. This simple spike rule captures the essence of a neural spike-generation mechanism. The membrane potential has the following dynamics:\ndVi dt = −λVi + N∑ k=1 Ωiksk + M∑ j=1 FijIj , (1)\nwhere λ is the neuron leak, Ωik is connection strength from neuron k to neuron i and Fij is the connection strength from input j to neuron i [16]. When a neuron spikes, the membrane potential is reset to Ri ≡ Ti + Ωii. This is written in equation 1 as a self-connection. Throughout this work, we focus on networks where connectivity Ω is symmetric - this simplifies our analysis, although in certain cases we can generalise to non-symmetric matrices.\nWe are interested in networks where a balance of excitation and inhibition coincides with optimal signal representation. Not all choices of network connectivity and spiking thresholds will give both [12, 13], but if certain conditions are satisfied, this can be possible. Before we proceed to our firing rate calculation, we must derive these conditions.\nWe begin by calculating the sum total of excitatory and inhibitory input received by neurons in our network. This is given by solving equation 1 implicitly:\nVi = N∑ k=1 Ωikrk + M∑ j=1 Fijxj , (2)\nwhere rk is a temporal filtering of the kth neuron’s spike train\nrk = ∫ ∞ 0 e−λt ′ sk(t− t′) dt′ , (3)\nand xj is a temporal filtering of the jth input\nxj = ∫ ∞ 0 e−λt ′ Ij(t− t′) dt′ . (4)\nAll the excitatory and inhibitory inputs received by neuron i are included in this summation (Eqn. 2). This can be rewritten as the slope of a loss function as follows:\nVi = − 1\n2\ndE(r)\ndri , (5)\nwhere E(r) = −rTΩr− 2rTFx + c (6)\nand c is a constant.\nNow, we can use this expression to derive the conditions that connectivity must satisfy so that the network operates in an optimal balanced state. In balanced networks, excitation and inhibition cancel to produce an input that is the same order of magnitude as the spiking threshold. This is very small, relative to the magnitude of excitation or inhibition alone [12, 13]. In tightly balanced networks, which we consider, this cancellation is so precise that Vi → 0 in the large network limit (for all active neurons) [15, 17, 18]. Now, using equation 5, we can see that this tight balance condition is equivalent to saying that our loss function (Eqn. 6) is minimised.\nThis has two implications for our choice of network connectivity and spiking thresholds. First, the loss function must have a minimum. To guarantee this, we require −Ω to be positive definite. Secondly, the spiking threshold of each neuron must be chosen so that each spike acts to minimise the cost function. This spiking condition can be written as E(no spike) > E(with spike). Using equation 6, this can be rewritten as E(no spike) > E(no spike)− 2[Ωr]k − 2[Fx]k −Ωkk. Finally,\ncancelling terms, and using equation 2, we can write our spiking condition as Vk > −Ωkk/2. Therefore, the spiking threshold for each neuron must be set to Tk ≡ −Ωkk/2, though this condition can be relaxed considerably if our loss function has an additional linear cost term1. Once these conditions are satisfied, our network is tightly balanced.\nWe are interested in networks that are both tightly balanced and optimal. Now, we can see from equation 5 that the balance of excitation and inhibition coincides with the optimisation of our loss function (Eqn. 6). This is an important result, because it relates balanced network dynamics to a neural computation. Specifically, it allows us to interpret the spiking activity of our tightly balanced network as an algorithm that optimises a loss function (Eqn. 6).\nThis is interesting because this optimisation can be easily mapped onto many useful computations. A particularly interesting example is given by Ω = −FFT − βI, where I is the identity matrix [15, 17, 18]. In recent work, it was shown that this connectivity can be learnt using a spike timingdependent plasticity rule [15]. Here, we use this connectivity to rewrite our loss function (Eqn. 6) as follows:\nE = (x− x̂)2 + β N∑ i=1 r2i , (7)\nwhere x̂ = FT r . (8)\nThe second term of equation 7 is a metabolic cost term that penalises neurons for spiking excessively, and the first term quantifies the difference between the signal value x and a linear read-out, x̂, where x̂ is computed using the linear decoder FT (Eqn. 8). Therefore, a network with this connectivity produces spike trains that optimise equation 7, thereby producing an output x̂ that is close to the signal value x. Throughout the remainder of this work, we will focus on optimal balanced networks with this form of connectivity.\nWe illustrate the properties of this system by simulating a network of 30 neurons. We find that our network produces spike trains (Fig. 1 b, middle panel) that represent x with great accuracy, across a broad range of signal values (Fig. 1 b, top panel). As expected, this optimal performance coincides with a tight balance of excitation and inhibition (Fig. 1 b, bottom panel), reminiscent of cortical observations [14]. In this example, our network has been optimised to represent a 2- dimensional signal x = (x1, x2). We measure firing rate tuning curves using a fixed value of x2 while varying x1. We use this signal because it can produce interesting, non-linear tuning curves (Fig. 1 c), especially at signal values where neurons fall silent. In the next section, we will attempt to understand this tuning curve non-linearity by calculating firing rates analytically."
    }, {
      "heading" : "3 Firing rate analysis with quadratic programming",
      "text" : "Our goal is to calculate the firing rates f of all the neurons in these tightly balanced network models as a function of the network input, the recurrent network connectivity Ω, and the feedforward connectivity F. On the surface, this may seem to be a difficult problem, because individual neurons have complicated non-linear integrate-and-fire dynamics and they interact strongly through network connectivity. However, the loss function relationship that we developed above allows us now to circumvent these problems.\nThere are many possible firing rate measures used in experiments and theoretical studies. Usually, a box-shaped temporal averaging window is used. We define the firing rate of a neuron to be:\nfk = λ ∫ ∞ 0 e−λt ′ sk(t− t′) dt′ . (9)\nThis is an exponentially weighted temporal average2, with timescale λ−1. We have chosen this temporal average because it matches the dynamics of synaptic filters in our neural network (Eqn. 3),\n1 Suppose that our network optimises the following cost function: E(r) = −rTΩr − 2rTFx + c+ bT r, where b is a vector of positive linear weights. Then, we find that the optimal spiking thresholds for this network are given by Ti ≡ (−Ωii + bi)/2 ≥ −Ωii/2. Therefore, we can apply our techniques to all networks with thresholds Ti ≥ −Ωii/2.\n2In this case, the firing rate timescale is very short, because λ is the membrane potential leak. However, we can easily generalise our framework so that this timescale can be as long as the slowest synaptic process [17, 18].\nallowing us to write fi(t) = λri(t). Here, we need to multiply by λ to ensure that our firing rates are reported in units of spikes per second.\nWe can now calculate firing rates using this relationship and by exploiting the algorithmic nature of tightly balanced networks. These networks produce spike trains that minimise our loss function E(r) (Eqn. 6). Therefore, the firing rates of our network are those that minimise E(f/λ), under the constraint that firing rates must be positive:\n{fi} = arg min fi≥0 E(f/λ) . (10)\nThis firing rate prediction is the solution to a constrained optimisation problem known as quadratic programming [19]. The optimisation is quadratic, because our loss function is a quadratic function of f , and it is constrained because firing rates are positive valued quantities, by definition.\nWe illustrate this firing rate prediction using a simple two-neuron network, with recurrent connectivity given by Ω = −FTF−βI as before. We simulate this system and measure the spike-train firing rates for both neurons (Fig. 2 a, left panel). We then use equation 10 to obtain a theoretical prediction for firing rates. We find that our firing rate prediction matches the spike-train measurement with great accuracy (Fig.2 a, middle panel and right panel).\nWe can now use our firing rate solution to understand the relationship between firing rates, input, connectivity and function. When both neurons are active, we can solve equation 10 exactly, to see that firing rates are related to network connectivity according to f = −λΩ−1Fx. When one of the neurons becomes silent, the other neuron must compensate by adjusting its firing rate slope. For example, when neuron 1 becomes silent, we have f1 = 0 and the firing rate of neuron 2 increases to f2 = λF2x/(F2FT2 + βI), where F2 denotes the second row of F. Similarly, when neuron 2\nbecomes silent, we have f2 = 0, and the firing rate of neuron 1 increases to f1 = λF1x/(F1FT1 + βI), where F1 is the first row of F. This non-linear change in firing rates is caused by the positivity constraint. It can be understood functionally, as an attempt by the network to represent x accurately, within the constraints of the system.\nIn larger networks, our firing rate prediction is more difficult to write down analytically because there are so many interactions between individual neurons and the positivity constraint. Nonetheless, we can make a number of general observations about tuning curve shape. In general, we can interpret tuning curve shape to be the solution of a quadratic programming problem, which can be written as a piece-wise linear function f = M (x) · x, where M(x) is a matrix whose entries depend on the region of signal space occupied by x. For example, in the two-neuron system that we just discussed, the signal space is partitioned into three regions: one region where neuron 1 is active and where neuron 2 is silent, a second region where both neurons are active and a third region where neuron 1 is silent and neuron 2 is active (Fig. 2 a, left panel). In each region there is a different linear relationship between the signal and the firing rates. The boundaries of these regions occur at points in signal space where an active neuron becomes silent (or where a silent neuron becomes active). At most, there will be N + 1 such regions.\nWe can also use quadratic programming to describe the spiking dynamics underlying these nonlinear networks. Returning to our two-neuron example, we measure the temporal evolution of the firing rates f1 and f2. We find that if we initialise the network to a sub-optimal state, the firing rates rapidly evolve toward the optimum in a series of discrete steps of size λ (Fig. 2 b, left panel). The step-size is λ because when neuron i spikes, ri → ri + 1, according to equation 3, and therefore, fi → fi+λ, according to equation 9. Once the network has reached the optimal state, it is impossible for it to remain there. The firing rates begin to decay exponentially, because our firing rate definition is an exponentially weighted summation (Eqn. 9) (Fig. 2 b, middle panel). Eventually, when the firing rate has decayed too far from the optimal solution, another spike is fired and the network moves closer to the optimum. In this way, spiking dynamics can be interpreted as a quadratic programming algorithm. The firing rate continues to fluctuate around the optimal spiking value. These fluctuations are noisy, in that they are dependent on initial conditions of the network. However, this noise has an unusual algorithmic structure that it is not well characterised by standard probabilistic descriptions of spiking irregularity."
    }, {
      "heading" : "4 Analysing tuning curve shape with quadratic programming",
      "text" : "Now that we have a framework for relating firing rates to network connectivity and input, we can explore the computational function of tuning curve shapes and the network mechanisms that generate these tuning curves. We will investigate systems that have monotonic tuning curves and systems that have bump-shaped tuning curves, which together constitute a large proportion of firing rate observations [2, 3, 4, 5].\nWe begin by considering a system of monotonic tuning curves, similar to the examples that we have considered already where recurrent connectivity is given by Ω = −FFT − βI. In these systems, the recurrent connectivity and hence the tuning curve shape is largely determined by the form of the feedforward matrix F. This matrix also determines the contribution of tuning curves to computational function, through its role as a linear decoder for signal representation (Eqn. 8). We illustrate this by simulating the response of our network to a 2-dimensional signal x = (x1, x2), where x1 is varied and x2 is fixed, using three different configurations of F (Fig. 3). This system produces monotonically increasing and decreasing tuning curves (Fig. 3a). We find that neurons with positive values of F have positive firing rate slopes (Fig. 3, blue tuning curves), and neurons with negative F values have negative firing rate slopes (Fig. 3, red tuning curves). If the values of F are regularly spaced, then the tuning curves of individual neurons are regularly spaced, and, if we manipulate this regularity by adding some random noise to the connectivity, we obtain inhomogeneous and highly irregular tuning curves (Fig.3 b). This inhomogeneity has little effect on the representation error.\nThis inhomogeneous monotonic tuning is reminiscent of tuning in many neural systems, including the oculomotor system [4]. The oculomotor system represents eye position, using neurons with negative slopes to represent left side eye positions and neurons with positive slopes to represent right side eye positions. To relate our model to this system, the signal variable x1 can be interpreted as eye-position, with zero representing the central eye position, and with positive and negative values\nof x1 representing right and left side eye positions, respectively. Now, we can use the relationship that we have developed between tuning curves and computational function to interpret oculomotor tuning as an attempt to represent eye positions optimally.\nBump-shaped tuning curves can be produced by networks representing circular variables x1 = cos θ, x2 = sin θ, where θ is the orientation of the signal (Fig. 3 c). As before, the tuning curves of individual neurons are regularly spaced if the values of F are regularly spaced. If we add some noise to the connectivity F, the tuning curves become inhomogeneous and highly irregular. Again, this inhomogeneity has little effect on the signal representation error.\nIn all the above examples, our firing rate predictions closely match firing rate measurements from network simulations (Fig. 3). The success of our algorithmic approach in calculating firing rates depends on the success of spiking networks in algorithmically optimising a cost function. The resolution of this spiking algorithm is determined by the leak λ and membrane potential noise. If λ is large, the firing rate prediction error will have large fluctuations about the optimal firing rate value (Fig. 4 a). However, the average prediction error (averaged over time and neurons) remains small. Similarly, membrane potential noise3 increases fluctuations about the optimal firing rate but the average prediction error remains small (until the noise is large enough to generate spikes without any input) (Fig. 4 b)."
    }, {
      "heading" : "5 Discussion and Conclusions",
      "text" : "We have developed a new algorithmic technique for calculating firing rates in tightly balanced networks. Our approach does not require us to make any linearising approximations. Rather, we directly identify the non-linear relationship between firing rates, connectivity, input and optimal signal representation. Identifying such relationships is a long-standing problem in systems neuroscience, largely because the mathematical language that we use to describe information representation is very different to the language that we use to describe neural network spiking statistics. For tightly balanced networks, we have essentially solved this problem, by matching the firing rate statistics of neural activity to the structure of neural signal representation. The non-linear relationship that we identify is the solution to a quadratic programming problem.\nPrevious studies have also interpreted firing rates to be the result of a constrained optimisation problem [21], but for a population coding model, not for a network of spiking neurons. In a more recent study, a spiking network was used to solve an optimisation problem, although this network required positive and negative spikes, which is difficult to reconcile with biological spiking [22].\nThe firing rate tuning curves that we calculate have allowed us to investigate poorly understood features of experimentally recorded tuning curves. In particular, we have been able to evaluate the impact of tuning curve inhomogeneity on neural computation. This inhomogeneity often goes unreported in experimental studies because it is difficulty to interpret [6], and in theoretical studies, it is often treated as a form of noise that must be averaged out. We find that tuning curve inhomogeneity is not necessarily noise because it does not necessarily harm signal representation. Therefore, we propose that tuning curves are inhomogeneous simply because they can be.\nBeyond the interpretation of tuning curve shape, our quadratic programming approach to firing rate calculations promises to be useful in other areas of neuroscience - from data analysis, where it may be possible to train our framework using neural data so as to predict firing rate responses to sensory stimuli - to the study of computational neurodegeneration, where the impact of neural damage on tuning curves and computation may be characterised."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank Nuno Calaim for helpful comments on the manuscript. Also, we are grateful for generous funding from the Emmy-Noether grant of the Deutsche Forschungs-gemeinschaft (CKM) and the Chaire dexcellence of the Agence National de la Recherche (CKM, DB), as well as a James Mcdonnell Foundation Award (SD) and EU grants BACS FP6-IST-027140, BIND MECTCT-20095-024831, and ERC FP7-PREDSPIKE (SD).\n3Membrane potential noise can be included in our network model by adding a Wiener process noise term to our membrane potential equation (Eqn. 1). We parametrise this noise with a constant η."
    } ],
    "references" : [ {
      "title" : "The impulses produced by sensory nerve endings. The Journal of physiology",
      "author" : [ "E.D. Adrian", "Y. Zotterman" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1926
    }, {
      "title" : "Population-wide distributions of neural activity during perceptual decision-making",
      "author" : [ "A. Wohrer", "M.D. Humphries", "C.K. Machens" ],
      "venue" : "Progress in neurobiology",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Orientation selectivity in the cat’s striate cortex is invariant with stimulus contrast",
      "author" : [ "G. Sclar", "R.D. Freeman" ],
      "venue" : "Experimental brain research",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1982
    }, {
      "title" : "Functional dissection of circuitry in a neural integrator",
      "author" : [ "E. Aksay", "I. Olasagasti", "B.D. Mensh", "R. Baker", "Goldman M.S", "Tank D.W" ],
      "venue" : "Nature neuroscience",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex",
      "author" : [ "D.H. Hubel", "T.N. Wiesel" ],
      "venue" : "Physiological Soc",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1962
    }, {
      "title" : "How close are we to understanding V1",
      "author" : [ "B.A. Olshausen", "D.J. Field" ],
      "venue" : "Neural computation",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Spectro-temporal receptive fields of auditory neurons in the grassfrog",
      "author" : [ "A. Aertsen", "P.I.M. Johannesma", "D.J. Hermes" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1980
    }, {
      "title" : "Linearity of cortical receptive fields measured with natural sounds",
      "author" : [ "C.K. Machens", "M.S. Wehr", "A.M. Zador" ],
      "venue" : "The Journal of neuroscience : the official journal of the Society for Neuroscience",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Theory of correlations in stochastic neural networks",
      "author" : [ "I. Ginzburg", "H. Sompolinsky" ],
      "venue" : "Physical Review E",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1994
    }, {
      "title" : "Impact of network structure and cellular response on spike time correlations. PLoS computational biology",
      "author" : [ "J. Trousdale", "Y. Hu", "E. Shea-Brown", "K. Josić" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Insights from a simple expression for linear fisher information in a recurrently connected population of spiking neurons",
      "author" : [ "J. Beck", "V.R. Bejjanki", "A. Pouget" ],
      "venue" : "Neural computation",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Chaos in neuronal networks with balanced excitatory and inhibitory activity",
      "author" : [ "C. van Vreeswijk", "H. Sompolinsky" ],
      "venue" : "Neural computation",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    }, {
      "title" : "Chaotic balanced state in a model of cortical circuits",
      "author" : [ "C. van Vreeswijk", "H. Sompolinsky" ],
      "venue" : "Neural computation",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Neocortical network activity in vivo is generated through a dynamic balance of excitation and inhibition",
      "author" : [ "B. Haider", "A. Duque", "A.R. Hasenstaub", "D.A. McCormick" ],
      "venue" : "The Journal of neuroscience : the official journal of the Society for Neuroscience",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Learning optimal spikebased representations Advances in Neural Information",
      "author" : [ "R. Bourdoukan", "D.G.T. Barrett", "C. Machens", "S. Deneve" ],
      "venue" : "Processing Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Dynamics of encoding in a population of neurons. The Journal of general physiology",
      "author" : [ "B.W. Knight" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1972
    }, {
      "title" : "Predictive coding of dynamical variables in balanced spiking networks",
      "author" : [ "M. Boerlin", "Machens C.K", "S. Deneve" ],
      "venue" : "PLoS computational biology,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Spike-based population coding and working memory",
      "author" : [ "M. Boerlin", "S. Deneve" ],
      "venue" : "PLoS Comput Biol",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Anatomy of the cortex. Statistics and Geometry",
      "author" : [ "V. Braitenber", "A. Schuz" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1991
    }, {
      "title" : "How behavioral constraints may determine optimal sensory representations PLoS biolog",
      "author" : [ "E. Salinas" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Spike-based population coding and working memory",
      "author" : [ "C.J. Rozell", "D.H. Johnson", "R.G. Baraniuk", "B.A. Olshausen" ],
      "venue" : "PLoS Comput Biol 7,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The firing rate of a neuron is arguably the most important characterisation of both neural network dynamics and neural computation, and has been ever since the seminal recordings of Adrian and Zotterman [1] in which the firing rate of a neuron was observed to increase with muscle tension.",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 1,
      "context" : "A large, sometimes bewildering, diversity of firing rate responses to stimuli have since been observed [2], ranging from sigmoidal-shaped tuning curves [3, 4], to bump-shaped tuning curves [5], with much diversity in between [6].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : "A large, sometimes bewildering, diversity of firing rate responses to stimuli have since been observed [2], ranging from sigmoidal-shaped tuning curves [3, 4], to bump-shaped tuning curves [5], with much diversity in between [6].",
      "startOffset" : 152,
      "endOffset" : 158
    }, {
      "referenceID" : 3,
      "context" : "A large, sometimes bewildering, diversity of firing rate responses to stimuli have since been observed [2], ranging from sigmoidal-shaped tuning curves [3, 4], to bump-shaped tuning curves [5], with much diversity in between [6].",
      "startOffset" : 152,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "A large, sometimes bewildering, diversity of firing rate responses to stimuli have since been observed [2], ranging from sigmoidal-shaped tuning curves [3, 4], to bump-shaped tuning curves [5], with much diversity in between [6].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "A large, sometimes bewildering, diversity of firing rate responses to stimuli have since been observed [2], ranging from sigmoidal-shaped tuning curves [3, 4], to bump-shaped tuning curves [5], with much diversity in between [6].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 6,
      "context" : "For example, experimentally recorded firing rates in a variety of systems have been described using the linear receptive field, which captures the linear relationship between stimulus and firing rate response [7].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 7,
      "context" : "However, in recent years, it has been found that this linear approximation often fails to capture important aspects of neural activity [8].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "have been used to simplify non-linear firing rate calculations in a variety of network models, using Taylor Series approximations [9], and more recently, using linear response theory [10, 11].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "have been used to simplify non-linear firing rate calculations in a variety of network models, using Taylor Series approximations [9], and more recently, using linear response theory [10, 11].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 10,
      "context" : "have been used to simplify non-linear firing rate calculations in a variety of network models, using Taylor Series approximations [9], and more recently, using linear response theory [10, 11].",
      "startOffset" : 183,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "Balanced network theory has come to be regarded as the standard model of cortical activity [12, 13], accounting for a large proportion of observed activity through a dynamic balance of excitation and inhibition [14].",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Balanced network theory has come to be regarded as the standard model of cortical activity [12, 13], accounting for a large proportion of observed activity through a dynamic balance of excitation and inhibition [14].",
      "startOffset" : 91,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : "Balanced network theory has come to be regarded as the standard model of cortical activity [12, 13], accounting for a large proportion of observed activity through a dynamic balance of excitation and inhibition [14].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 14,
      "context" : "Recently, it was found that tightly balanced networks are synonymous with efficient coding, in which a signal is represented optimally subject to metabolic costs [15].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 15,
      "context" : "where λ is the neuron leak, Ωik is connection strength from neuron k to neuron i and Fij is the connection strength from input j to neuron i [16].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 11,
      "context" : "Not all choices of network connectivity and spiking thresholds will give both [12, 13], but if certain conditions are satisfied, this can be possible.",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "Not all choices of network connectivity and spiking thresholds will give both [12, 13], but if certain conditions are satisfied, this can be possible.",
      "startOffset" : 78,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "This is very small, relative to the magnitude of excitation or inhibition alone [12, 13].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "This is very small, relative to the magnitude of excitation or inhibition alone [12, 13].",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "In tightly balanced networks, which we consider, this cancellation is so precise that Vi → 0 in the large network limit (for all active neurons) [15, 17, 18].",
      "startOffset" : 145,
      "endOffset" : 157
    }, {
      "referenceID" : 16,
      "context" : "In tightly balanced networks, which we consider, this cancellation is so precise that Vi → 0 in the large network limit (for all active neurons) [15, 17, 18].",
      "startOffset" : 145,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : "In tightly balanced networks, which we consider, this cancellation is so precise that Vi → 0 in the large network limit (for all active neurons) [15, 17, 18].",
      "startOffset" : 145,
      "endOffset" : 157
    }, {
      "referenceID" : 14,
      "context" : "A particularly interesting example is given by Ω = −FF − βI, where I is the identity matrix [15, 17, 18].",
      "startOffset" : 92,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "A particularly interesting example is given by Ω = −FF − βI, where I is the identity matrix [15, 17, 18].",
      "startOffset" : 92,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "A particularly interesting example is given by Ω = −FF − βI, where I is the identity matrix [15, 17, 18].",
      "startOffset" : 92,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "In recent work, it was shown that this connectivity can be learnt using a spike timingdependent plasticity rule [15].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "1 b, bottom panel), reminiscent of cortical observations [14].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "However, we can easily generalise our framework so that this timescale can be as long as the slowest synaptic process [17, 18].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "However, we can easily generalise our framework so that this timescale can be as long as the slowest synaptic process [17, 18].",
      "startOffset" : 118,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "We will investigate systems that have monotonic tuning curves and systems that have bump-shaped tuning curves, which together constitute a large proportion of firing rate observations [2, 3, 4, 5].",
      "startOffset" : 184,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "We will investigate systems that have monotonic tuning curves and systems that have bump-shaped tuning curves, which together constitute a large proportion of firing rate observations [2, 3, 4, 5].",
      "startOffset" : 184,
      "endOffset" : 196
    }, {
      "referenceID" : 3,
      "context" : "We will investigate systems that have monotonic tuning curves and systems that have bump-shaped tuning curves, which together constitute a large proportion of firing rate observations [2, 3, 4, 5].",
      "startOffset" : 184,
      "endOffset" : 196
    }, {
      "referenceID" : 4,
      "context" : "We will investigate systems that have monotonic tuning curves and systems that have bump-shaped tuning curves, which together constitute a large proportion of firing rate observations [2, 3, 4, 5].",
      "startOffset" : 184,
      "endOffset" : 196
    }, {
      "referenceID" : 3,
      "context" : "This inhomogeneous monotonic tuning is reminiscent of tuning in many neural systems, including the oculomotor system [4].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 19,
      "context" : "Previous studies have also interpreted firing rates to be the result of a constrained optimisation problem [21], but for a population coding model, not for a network of spiking neurons.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 20,
      "context" : "In a more recent study, a spiking network was used to solve an optimisation problem, although this network required positive and negative spikes, which is difficult to reconcile with biological spiking [22].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "This inhomogeneity often goes unreported in experimental studies because it is difficulty to interpret [6], and in theoretical studies, it is often treated as a form of noise that must be averaged out.",
      "startOffset" : 103,
      "endOffset" : 106
    } ],
    "year" : 2013,
    "abstractText" : "How are firing rates in a spiking network related to neural input, connectivity and network function? This is an important problem because firing rates are a key measure of network activity, in both the study of neural computation and neural network dynamics. However, it is a difficult problem, because the spiking mechanism of individual neurons is highly non-linear, and these individual neurons interact strongly through connectivity. We develop a new technique for calculating firing rates in optimal balanced networks. These are particularly interesting networks because they provide an optimal spike-based signal representation while producing cortex-like spiking activity through a dynamic balance of excitation and inhibition. We can calculate firing rates by treating balanced network dynamics as an algorithm for optimising signal representation. We identify this algorithm and then calculate firing rates by finding the solution to the algorithm. Our firing rate calculation relates network firing rates directly to network input, connectivity and function. This allows us to explain the function and underlying mechanism of tuning curves in a variety of systems.",
    "creator" : null
  }
}