{
  "name" : "6f3ef77ac0e3619e98159e9b6febf557.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "What Are the Invariant Occlusive Components of Image Patches? A Probabilistic Generative Approach",
    "authors" : [ "Zhenwen Dai", "Georgios Exarchakis" ],
    "emails" : [ "z.dai@sheffield.ac.uk", "exarchakis@berkeley.edu", "joerg.luecke@uni-oldenburg.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Probabilistic generative models are used to mathematically formulate the generation process of observed data. Based on a good probabilistic model of the data, we can infer the processes that have generated a given data point, i.e., we can estimate the hidden causes of the generation. These hidden causes are usually the objects we want to infer knowledge about, be it for medical data, biological processes, or sensory data such as acoustic or visual data. However, real data are usually very complex, which makes the formulation of an exact data model infeasible. Image data are a typical example of such complex data. The true generation process of images involves, for instance, different objects with different features at different positions, mutual occlusions, object shades, lighting\nconditions and reflections due to self-structure and nearby objects. Even if a generative model can capture some of these features, an inversion of the model using Bayes’ rule very rapidly becomes analytically and computationally intractable. As a consequence, generative modelers make compromises to allow for trainability and applicability of their generative approaches.\nTwo properties that have, since long, been identified as crucial for models of images are object occlusions [1–5] and the invariance of object identity to translations [6–13]. However, models incorporating both occlusions and invariances suffer from a very pronounced combinatorial complexity. They could, so far, only be trained with very low dimensional hidden spaces [2, 14, 15]. At first glance, occlusion modeling is, furthermore, mathematically more inconvenient. For these reasons, many studies including style and content models [16], other bi-linear models [17, 18], invariant sparse coding [19, 20], or invariant NMF [21] do not model occlusions. Analytical and computation reasons are often explicitly stated as the main motivation for the use of the linear superposition of components (see, e.g., [16, 17]).\nIn this work, we for the first time study the encoding of natural image patches using a model with both non-linear feature combinations and translation invariances."
    }, {
      "heading" : "2 A Generative Model with Non-linear and Invariant Components",
      "text" : "The model used to study image patch encoding assumes an exclusive component combination, i.e., for each pixel exclusively one cause is made responsible. It thus shares the property of exclusiveness with visual occlusions. The model will later be shown to capture occluding components. We will, however, not model explicit occlusion using a depth variable (compare [2]) but will focus on the exclusiveness property. The applied model is a novel version of the invariant occlusive components model studied for mid-level vision earlier [22]. We first briefly reiterate the basic model in the following and discuss the main differences of the new version afterwards.\nWe consider image patches ~y with D2 observed scalar variables, ~y = (y1, . . . , yD2). An image patch is assumed to contain a subset from a set of H components. Each component h can be located at a different position denoted by an index variable xh ∈ {1, . . . , D2}, which is associated with a set of permutation matrices that covers all the possible planar translations {T1, . . . , TD2} (similar formulations have also been used in sprite models [14, 15]). Each component h is modeled to appear in an image patch with probability πh ∈ (0, 1). Following [22], we do not model component presence and absence explicitly but, for mathematical convenience, assign the special ‘position’ −1 to all the components which are not chosen to generate the patch. Assuming a uniform distribution for the positions, the prior distribution for components and their positions is thus given by:\np(~x|~π) = ∏ h p(xh|πh), p(xh|πh) = { 1− πh, xh = −1 πh D2 , otherwise , (1)\nwhere the hidden variable ~x = (x1, . . . , xH) contains the information on presence/absence and position of all the image components.\nIn contrast to linear models, the studied approach requires two sets of parameters for the encoding of image components: component masks and component features. Component masks describe where an image component is located, and component features describe what a component encodes (compare [2, 3, 14, 15]). High values of mask parameters ~αh encode the pixels most associated with a component h but the encoding has to be understood relative to a global component position. The feature parameters ~wh encode the values of a component’s features. Fig. 1 shows an example\nof the mask and feature parameters for two typical low-level visual features. Given a particular position, the mask and feature parameters of the component are transformed to the target position by multiplying a corresponding translation matrix like Txh~αh and Txh ~wh. When generating an image patch, two or more components may occupy the same pixel, but according to occlusion the pixel value is exclusively determined by only one of them. This exclusiveness is formulated by defining a mask variable ~m = (m1, . . . ,mD2). For a pixel at a position d, md determines which component is responsible for the pixel value. Therefore, md takes a value from the set of present components Γ = {h|xh 6= −1} plus a special value “0” indicating background, and the prior distribution of ~m is defined as:\np(~m|~x,A) = D2∏ d=1 p(md|~x,A), p(md = h|~x,A) =  α0 α0+ ∑ h′∈Γ(Txh′ ~αh′ )d , h = 0 (Txh ~αh)d α0+ ∑\nh′∈Γ(Txh′ ~αh′ )d\n, h ∈ Γ , (2)\nwhere A = (~α1, . . . , ~αH) contains the mask parameters for all the components, and α0 defines the mask parameter for background. The mask variable md chooses the component h with a high likelihood if the translated mask parameter of the corresponding component is high at the position d. Note that md follows a mixture model given the presence/absence and positions of all the components ~x. This can be thought of as an approximation to the distribution of mask variables marginalizing the depth orderings and pixel transparency in the exact occlusive model (see Supplement A for a comparison). After drawing the values of the hidden variables ~x and ~m, an image patch can be generated with a Gaussian noise model, which is given by:\np(~y |~m, ~x,Θ) = D2∏ d=1 p(yd|md, ~x,Θ), p(yd|md = h, ~x,Θ) = { N (yd;B, σ2B), h = 0 N (yd; (Txh ~wh)d, σ 2), h ∈ Γ , (3) where σ2 is the variance of components, and Θ = (~π,W,A, σ2, α0, B, σ2B) are all the model parameters. The background distribution is a Gaussian distribution with mean B and variance σ2B . Compared to an occlusive model with exact EM (see Supplement A), our approach will use the exclusiveness approximation and a truncated posterior approximation in order to make learning tractable.\nThe model described in (1) to (3) has been optimized for the encoding of image patches. First, feature variables are scalar to encode light intensities or input by the lateral geniculus nucleus (LGN) rather than color features for mid-level vision. Second, to capture the frequency of presence for individual components, we implement the learning of the prior parameter of presence ~π. Third, the pre-selection function in the variational approximation (see below) has been adapted to the usage of scalar valued features. As a scalar value is much less distinctive than the sophisticated image features used in [22], the pre-selection of components has been changed to the complete component instead of only salient features."
    }, {
      "heading" : "3 Efficient Likelihood Optimization",
      "text" : "Given a set of image patches Y = (~y(1), . . . , ~y(n)), learning is formulated as inferring the best model parameters w.r.t. the log-likelihood L = p(Y |Θ). Following the Expectation Maximization (EM) approach, the parameter update equations are derived. The equations of the mask parameter ~αh, and feature parameter ~wh are the same as in [22]. Additionally, we derived the update equation for the prior parameter of presence:\nπh = 1\nN N∑ n=1 ∑ ~x∈{xh 6=−1} p(~x|~y(n),Θ). (4)\nBy learning the prior parameters πh, the probabilities of individual components’ presence can be estimated. This allows us to gain more insights about the statistics of image components. In the update equations, a posterior distribution has been estimated for each data point, which corresponds to the E-step of an EM algorithm. The posterior distribution of our model can be decomposed as:\np(~m, ~x|~y,Θ) = p(~x|~y,Θ) ∏D2 d=1 p(md|~x, ~y,Θ), (5)\nin which p(~x|~y,Θ) and p(md|~x, ~y,Θ) are estimated separately. Computing the exact distribution of p(~x|~y,Θ) is intractable, as it includes the combinatorics of the presence/absence of components and their positions. An efficient posterior approximation, Expectation Truncation (ET), has been successfully employed. ET approximates the posterior distribution as a truncated distribution [23]:\np(~x|~y,Θ) ≈ p(~y, ~x|Θ)∑ ~x′∈Kn p(~y, ~x ′|Θ) , if ~x ∈ Kn, (6)\nand zero otherwise. If Kn is chosen to be small but to contain the states with most posterior probability mass, the computation of the posterior distribution becomes tractable while a high accuracy\nof the approximations can be maintained [23]. To select a proper subspace Kn, τ features (pixel intensities) are chosen according to their mask parameters. Based on the chosen features, a score value S(xh) is computed for each component at each position (see [22]). We select H ′ components, denoted as H, for the candidates that may appear in the given image according to the probability p(~y, x̌h|Θ). x̌h corresponds to the vector ~x with xh = x∗h and the rest components absent (xh′ = −1, h′ 6= h), where x∗h is the best position of the component h w.r.t. S(xh). This is different from the earlier work [22], where Kn is constructed directly according to S(xh). For each component, we select the set of its candidate positions Xh, xh ∈ Xh, which contains the p best positions w.r.t. S(xh). Then the truncated subspace Kn is defined as:\nKn = {~x | ( ∑ j sj ≤ γ and si = 0, ∀i /∈ H) or ∑ j′ sj′ ≤ 1}, (7)\nwhere sh represents the presence/absence state of the component h (sh = 0 if xh = −1∪ xh /∈ Xh and sh = 1 if xh ∈ Xh). To avoid converging to local optima, we used the directional annealing scheme [22] for our learning algorithm."
    }, {
      "heading" : "4 Numerical Experiments on Artificial Data",
      "text" : "The goal of the experiment on artificial data is to verify that the model and inference method can recover the correct parameters, and to investigate inference on the data generated according to occlusions with explicit depth variable. We generated 4×4 gray-scale image patches. In the data set, eight different components are used, which are four vertical ‘bars’ and four horizontal ‘bars’, and each bar has a different intensity and has a binary vector indicating its ‘transparency’ (1 for non-transparent and 0 for transparent, see Fig. 2b) . When generating an image patch, a subset of components is selected according to their prior probabilities πh = 0.25, and the selected components are combined according to a random depth ordering (flat priors on the ordering). A component with smaller depth will occlude the components with larger depth, and for each image patch we sample a new depthordering. For the pixels in which all the selected components are transparent, the value is determined according to the background with zero intensity (B = 0). All the pixels generated by components are subject to a Gaussian noise with σ = 0.02 and the pixels belonging to the background have a Gaussian noise with σB = 0.001. In total, we generated N = 1, 000 image patches. Fig. 2a shows eight samples. The artificial data is similar to data generated by the occlusive components analysis model (OCA; [2]), except of the use of scalar features and the assumption of shift-invariance.\nFig. 2c shows the learned model parameters on the generated data set. We learned nine components (H = 9). The initial feature value W was set to randomly selected data points. The initial mask parameter A was independently and uniformly drawn from the interval (0, 1). The initial annealing temperature was set to T = 5. After keeping constant for 20 iterations, the temperature linearly decreased to 1 in 100 iterations. For the robustness of learning, σ decreased together with the temperature from 0.2 to 0.02, and an additive Gaussian noise with zero mean and σw = 0.04 was\ninjected intoW and σw gradually decreased to zero. The algorithm terminated when the temperature was equal to 1 and the difference of the pseudo data log-likelihood of two consecutive iterations was sufficiently small (less than 0.1%). The approximation parameters used in learning was H ′ = 8, γ = 4, p = 2 and τ = 3. In this result, all the eight generative components have been successfully learned. The 2nd to last component (see Fig. 2c) is a dumpy component (low πh, i.e., very rarely used). Its single pixel structure is therefore an artifact. With the learned parameters, the model could infer the present components, their positions and the pixel-to-component assignment. Fig. 2d shows a typical example. Given an image patch on the left, the present components and their positions are correctly inferred. Furthermore, as shown on the 3rd row, the posterior probabilities of the mask variable p(md|~x, ~y,Θ) give a clear assignment of the contributing component for each pixel. This information is potentially very valuable for tasks like parts-based object segmentation or to infer the depth ordering among the components. We assess the reliability of our learning algorithm by repeating the learning procedure with the same configuration but different random parameter initializations. The algorithm recovers all the generative components in 11 out of 20 repetitive runs. The 9 runs not recovering all bars did still recover reasonable solutions with usually 7 bars out of 8 bars represented. In general, optima of bar stimuli seem to have much more pronounced local optima, e.g., compared to image patches."
    }, {
      "heading" : "5 Numerical Experiments on Image Patches",
      "text" : "After we verified the inference and learning algorithm on artificial data, it was applied to patches of natural images. As training set we used N = 100, 000 patches of size 16 × 16 pixels extracted at random positions from random images of the van Hateren natural image database [24]. We modeled the sensitivity of neurons in the LGN using a difference-of-Gaussians (DoG) filter for different positions, i.e., we processed all patches by convolving them with a DoG kernel. Following earlier studies (see [5] for references), the ratio between the standard deviation of the positive and the negative Gaussian was chosen to be 1/3 and the amplitudes chosen to obtain a mean-free centersurround filter. Fig. 3a shows some samples of the image patches after preprocessing.\nOur algorithm learnedH = 100 components from the natural image data set. The model parameters were initialized in the same way as for artificial data. The annealing temperature was initialized with T = 10, kept constant for 10 iterations, the temperature linearly decreased to 1 in 100 iterations. σ decreased together with the temperature from 0.5 to 0.2, and an additive Gaussian noise with zero mean and σw = 0.2 was injected into W and σw gradually decreased to zero. The approximation parameters used for learning were H ′ = 6, γ = 4, p = 2 and τ = 50. After 134 iterations, the model parameters had essentially converged.\nFigs. 3bc show the learned mask parameters and the learned feature values for all the 100 components. Mask parameters define the frequently used areas within a component, and feature parameters reveal the appearance of a component on image patches. As can be observed, image components are very differently represented from linear models. See the component in Fig. 3d as an example: mask parameters are localized and all positive; feature parameters have positive and negative values across the whole patch. Masks and features can be combined to resemble a familiar Gabor function via point-wise multiplication (see Fig. 3d). All the above shown component representations are sorted in descending order according to the learned prior probabilities of occurrence ~π (see Fig. 3e)."
    }, {
      "heading" : "6 Estimation of Receptive Fields",
      "text" : "For visualization, mask and feature parameters can be combined via point-wise multiplication. To more systematically and quantitatively interpret the learned components and to compare them to biological experimental findings, we estimated the predicted receptive fields (RFs). RFs estimates were computed with reverse correlation based on the model inference results. Reverse correlation can be defined as procedure to find the best linear approximation of the components’ presence given an image patch ~y(n). More formally, we search for a set of predicted receptive fields ~Rh, h ∈ {1, . . . ,H} that minimize the following cost function:\nf = 1 N ∑ n ∑ ~x∈Kn p(~x |~y (n),Θ) ∑ h( ~RTh T̄xh~y (n) − sh)2 + λ ∑ h ~RTh ~Rh, (8)\nwhere ~y(n) is the nth stimulus and λ is the coefficient for L2 regularization. sh is a binary variable representing the presence/absence state of the component h, where sh = 0 if xh = −1, and sh = 1\notherwise. As our model allows the components to be at different locations, the reverse correlation is computed by shifting the stimuli according to the inferred location of each components. T̄xh represents the transformation matrix applied to the stimulus for the component h, which is the opposite transformation of the inferred transformation Txh (T̄xhTxh = 1). For the absent components, the stimulus is used without any transformations (T−1 = 1).\nDue to the intractability of computing an exact posterior distribution, given a data point, the cost function only sums across the truncated subspace Kn in the variational approximation (see Sec. 3). By setting the derivative of the cost function to zero, ~Rh can be estimated as:\n~Rh = ( λN1 + ∑ n〈T̄xh~y (n)(T̄xh~y (n))T 〉qn )−1(∑ n〈~s(T̄xh~y (n))T 〉qn )\n(9)\nwhere 〈·〉qn denotes the expectation value w.r.t. the posterior distribution p(~x |~y(n),Θ) and 1 is an identity matrix. When solving ~Rh, we often observe that many of the eigenvalues of the data covariance matrix ∑N n=1〈T̄xh~y(n)(T̄xh~y(n))T 〉qn are close to zero, which makes the solution of ~Rh very unstable. Therefore, we introduce a L2 regularization to the cost function. The regularization coefficient λ is chosen between the minimum and maximum element of the data covariance matrix. The estimated receptive fields are not sensitive to the value of the regularization coefficient λ as long as λ is large enough to resolve the numerical instability (see Supplement for a comparison of the receptive fields estimated with different λ values). From the experiments with artificial data and\nnatural image patches, we observed that the L2 regularization successfully eliminated the numerical stability problem.\nFig. 3f shows the RFs estimated according to our model. For further analysis, we matched the RFs using Gabor functions and DoG functions as was suggested in [5]. If we factored in the occurrence probabilities, we found that the model considered about 17% of all components of the patches to be globular, 56% to be Gabor-like and 27% to have another structure (see Supplement for details). The prevalence of ‘center-on’ globular fields may be a consequence of the prevalence of convex object shapes."
    }, {
      "heading" : "7 Discussion",
      "text" : "The encoding of image patches investigated in this study separates feature and position information of visual components. Functionally, such an encoding has been found very useful, e.g., for the construction of object recognition systems. Many state-of-the-art systems for visual object classification make use of convolutional neural networks [12, 25, 26]. Such networks compute the responses of a set of filters for all positions in a predefined area and use the maximal response for further processing ([12] for a review). If we identify the predefined area with one image patch as processed by our approach, then the encoding studied here is to some extent similar to convolutional networks: (A) it uses like convolutional networks one set of component parameters for all positions; and (B) a hidden component variable of the generative model integrates or ‘pools’ the information across all positions. As the here studied approach is based on a generative data model, the integration across positions can directly be interpreted as inversion of the generation process. Crucially, the inversion can take occlusions of visual features into account while convolutional networks do not model occlusions. Furthermore, the generative model uses a probabilistic encoding, i.e., it assigns probabilities to positions and features of a joint feature and position space. Ambiguous visual input can therefore be represented appropriately. In contrast, convolutional networks use one position for each feature as representation. In this sense a convolutional encoding could be regarded as MAP estimate for the feature position while the generative integration could be interpreted as probabilistic pooling. Many bilinear models have also been applied to image patches, e.g., [17, 18]. Such studies do report that neurally plausible receptive fields (RFs) in the form of Gabor functions emerge [17, 18]. Likewise, invariant versions of NMF [21] or ICA (in the form of ISA [9] have been applied to image patches.\nIn addition to Gabors, we observed in our study a large variety of further types of RFs. Gabor filters with different orientations, phase and frequencies, as well as globular fields and fields with more complex structures (Fig. 3f). Gabors have been studied since several decades, globular and more complex fields have attracted attention in the last couple of years. In particular, globular fields have attracted attention [5, 27, 28] as they have been reported together with Gabors in macaques and other species ([29] and [5] for further references). Such fields have been associated with occlusions before [5, 28, 30]; and our study now for the first time reports globular fields for an occlusive and translation invariant approach. The results may be taken as further evidence of the connection between occlusions and globular fields. However, also linear convolutional approaches have recently reported such fields [19, 31]. Linear approaches seem to require a high degree of overcompleteness or specific priors while globular fields naturally emerge for occlusion-like non-linearities. More concretely: for non-invariant linear sparse coding, globular fields only emerged from a sufficiently high degree of overcompleteness onwards [32, 33] or with specific prior settings and overcompleteness [27]; for non-invariant occlusive models [5, 30] globular fields always emerge alongside Gabors for any overcompleteness. The results reported here can be taken as confirming this observation for position invariant encoding. The invariant non-linear model assigns high degrees of occurrences (high πh) to Gabor-like and to globular fields (first rows in Fig. 3f). Components with more complex structures are assigned lower occurrence frequencies. In total the model assumes a fraction between 10 and 20% of all data components to be globular. Such high percentages may be related to the high percentages of globular fields (∼16-23%) measured in vivo ([29] and [5] for references). In contrast, the highest degrees of occurrences, e.g., for convolutional matching pursuit [31] seems to be assigned exclusively to Gabor features. Globular fields only emerge (alongside other non-Gabor fields) for higher degrees of overcompleteness. A direct comparison in terms of occurrence frequencies is difficult because the linear models to not infer occurrence frequencies from data. The closest match to such frequencies would be an (inverse) sparsity which is set by hand for almost all linear approaches. The reason is the use of MAP-based point-estimates while our approach uses a more probabilistic posterior estimate.\nBecause of their separate encoding of features and positions, all models with separate position encoding can represent high degrees of over-completeness. Convolutional matching pursuit [31] shows results for up to 64 filters of size 8 × 8. With 8 horizontal and 8 vertical shifts, the number of noninvariant components would amount to 8 × 8 × 64 = 3136. Convolutional sparse coding [19] reports results by assuming 128 components for 9 × 9 patches.The number of non-invariant components would therefore amount to 10, 368. For our network we obtained results for up to 100 components of size 16 × 16. With 16 horizontal and 16 vertical shift this amounts to 25, 600 noninvariant components. In terms of components per observed variable, invariant models are therefore now computationally feasible in a regime the visual cortex is estimated to operate in [33].\nThe hidden units associated with component feature are fully translation invariant. In terms of neural encoding, their insensitivity to stimulus shifts would therefore place them into the category of V1 complex cells. Also globular fields or fields that seem sensitive to structures such as corners would warrant such units the label ‘complex cell’. No hidden variable in the model can directly be associated with simple cell responses. However, a possible neural network implementation of the model is an explicit representation of component features at different positions. The weight sharing of the model would be lost but units with explicit non-invariant representation could correspond to simple cells. While such a correspondence can connect our predictions to experimental studies of simple cells, recently developed approaches for the estimation of translation invariant cell responses [34, 35] can represent a more direct connection. To approximately implement the non-linear generative model neurally, the integration of information would have to be a very active process. In contrast to passive pooling mechanisms across units representing linear filters (such as simple cells), it would involve neural units with explicit position encoding. Such units would control or ‘gate’ the information transfer from simple cells to downstream complex cells. As such our probabilistic model can be related to ideas of active control units for individual components [6, 7, 10, 11, 36] (also compare [37]). A notable difference to all these models is that the here studied approach allows to interpret active control as optimal inference w.r.t. a generative model of translations and occlusions.\nFuture work can go in different directions. Different transformations could be considered or learned [37], explicit modeling in time could be incorporated (compare [17]), and/or further hierarchical stages could be considered. The crucial challenge all such developments face are computational intractabilities due to large combinatorial hidden spaces. Base on the presented results, we believe, however, that advances in analytical and computational training technology will enable an increasingly sophisticated modeling of image patches in the future.\nAcknowledgement. We thank Richard E. Turner for helpful discussions and acknowledge funding by DFG grant LU 1196/4-2."
    } ],
    "references" : [ {
      "title" : "Stochastic models for generic images",
      "author" : [ "D. Mumford", "B. Gidas" ],
      "venue" : "Q. Appl. Math., 59:85–111",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Occlusive Components Analysis",
      "author" : [ "J. Lücke", "R. Turner", "M. Sahani", "M. Henniges" ],
      "venue" : "NIPS, 22:1069–77",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning a generative model of images by factoring appearance and shape",
      "author" : [ "Nicolas LeRoux", "Nicolas Heess", "Jamie Shotton", "John Winn" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Natural images",
      "author" : [ "D. Zoran", "Y. Weiss" ],
      "venue" : "Gaussian mixtures and dead leaves. NIPS, 25:1745–1753",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A parallel computation that assigns canonical object-based frames of reference",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Proc. IJCAI, pages 683–685",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Shifter circuits: a computational strategy for dynamic aspects of visual processing",
      "author" : [ "C.H. Anderson", "D.C. Van Essen" ],
      "venue" : "PNAS, 84(17):6297–6301",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "C",
      "author" : [ "M. Lades", "J. Vorbrüggen", "J. Buhmann", "J. Lange" ],
      "venue" : "v. d. Malsburg, R. Würtz, and W. Konen. Distortion invariant object recognition in the dynamic link architecture. IEEE Transactions on Computers, 42(3):300–311",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Emergence of phase- and shift-invariant features by decomposition of natural images into independent feature subspaces",
      "author" : [ "A. Hyvärinen", "P. Hoyer" ],
      "venue" : "Neural Computation, 12(7):1705–20",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Map-Seeking circuits in Visual Cognition — A Computational Mechanism for Biological and Machine Vision",
      "author" : [ "D.W. Arathorn" ],
      "venue" : "Standford Univ. Press, Stanford, California",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "and C",
      "author" : [ "J. Lücke", "C. Keck" ],
      "venue" : "von der Malsburg. Rapid convergence to feature layer correspondences. Neural Computation, 20(10):2441–2463",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Convolutional networks and applications in vision",
      "author" : [ "Y. LeCun", "K. Kavukcuoglu", "C. Farabet" ],
      "venue" : "Proceedings of 2010 IEEE International Symposium on Circuits and Systems, pages 253–6",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Modeling Images using Transformed Indian Buffet Processes",
      "author" : [ "Y. Hu", "K. Zhai", "S. Williamson", "J. Boyd-Graber" ],
      "venue" : "ICML",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning flexible sprites in video layers",
      "author" : [ "N. Jojic", "B. Frey" ],
      "venue" : "CVPR",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Greedy learning of multiple objects in images using robust statistics and factorial learning",
      "author" : [ "C.K.I. Williams", "M.K. Titsias" ],
      "venue" : "Neural Computation, 16:1039–62",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Separating Style and Content with Bilinear Models",
      "author" : [ "J.B. Tenenbaum", "W.T. Freeman" ],
      "venue" : "Neural Computation, 12(6):1247–83",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A structured model of video reproduces primary visual cortical organisation",
      "author" : [ "P. Berkes", "R.E. Turner", "M. Sahani" ],
      "venue" : "PLoS Computational Biology, 5(9):e1000495",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning intermediate-level representations of form and motion from natural movies",
      "author" : [ "C.F. Cadieu", "B.A. Olshausen" ],
      "venue" : "Neural Computation, 24(4):827–866",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning convolutional feature hierarchies for visual recognition",
      "author" : [ "K. Kavukcuoglu", "P. Sermanet", "Y.L. Boureau", "K. Gregor", "M. Mathieu", "Y. LeCun" ],
      "venue" : "NIPS, 23:14",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Efficient learning of sparse invariant representations",
      "author" : [ "K. Gregor", "Y. LeCun" ],
      "venue" : "CoRR, abs/1105.5307",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Transformation-invariant representation and NMF",
      "author" : [ "J. Eggert", "H. Wersing", "E. Körner" ],
      "venue" : "2004 IEEE International Joint Conference on Neural Networks, pages 2535–39",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Unsupervised learning of translation invariant occlusive components",
      "author" : [ "Z. Dai", "J. Lücke" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Expectation truncation and the benefits of preselection in training generative models",
      "author" : [ "J. Lücke", "J. Eggert" ],
      "venue" : "Journal of Machine Learning Research, 11:2855–900",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Independent component filters of natural images compared with simple cells in primary visual cortex",
      "author" : [ "J.H. van Hateren", "A. van der Schaaf" ],
      "venue" : "Proceedings of the Royal Society of London B,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1998
    }, {
      "title" : "Hierarchical models of object recognition in cortex",
      "author" : [ "M. Riesenhuber", "T. Poggio" ],
      "venue" : "Nature Neuroscience, 211(11):1019 – 1025",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS, volume 25, pages 1106–1114",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive fields",
      "author" : [ "M. Rehn", "F.T. Sommer" ],
      "venue" : "Journal of Computational Neuroscience, 22(2):135–46",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Receptive field self-organization in a model of the fine-structure in V1 cortical columns",
      "author" : [ "J. Lücke" ],
      "venue" : "Neural Computation, 21(10):2805–45",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Spatial structure and symmetry of simple-cell receptive fields in macaque primary visual cortex",
      "author" : [ "D.L. Ringach" ],
      "venue" : "Journal of Neurophysiology, 88:455–63",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The maximal causes of natural scenes are edge filters",
      "author" : [ "G. Puertas", "J. Bornschein", "J. Lücke" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2010
    }, {
      "title" : "Convolutional matching pursuit and dictionary training",
      "author" : [ "A. Szlam", "K. Kavukcuoglu", "Y. LeCun" ],
      "venue" : "arXiv preprint arXiv:1010.0422",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning real and complex overcomplete representations from the statistics of natural images",
      "author" : [ "B.A. Olshausen", "C.F. Cadieu", "D.K. Warland" ],
      "venue" : "volume 7446, page 74460S. SPIE",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Highly overcomplete sparse coding",
      "author" : [ "B.A. Olshausen" ],
      "venue" : "Proc. of HVEI, page 86510S",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Characterizing responses of translationinvariant neurons to natural stimuli: maximally informative invariant dimensions",
      "author" : [ "M. Eickenberg", "R.J. Rowekamp", "M. Kouh", "T.O. Sharpee" ],
      "venue" : "Neural Computation, 24(9):2384–421",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficient and direct estimation of a neural subunit model for sensory coding",
      "author" : [ "B. Vintch", "A. Zaharia", "J.A. Movshon", "E.P. Simoncelli" ],
      "venue" : "Proc. of NIPS, pages 3113–3121",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information",
      "author" : [ "B. Olshausen", "C. Anderson", "D. Van Essen" ],
      "venue" : "J Neuroscience, 13(11):4700–4719",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Learning to represent spatial transformations with factored higher-order Boltzmann machines",
      "author" : [ "R. Memisevic", "G.E. Hinton" ],
      "venue" : "Neural Computation, 22(6):1473–1492",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An efficient method for finding the minimum of a function of several variables without calculating derivatives",
      "author" : [ "M.J.D. Powell" ],
      "venue" : "The Computer Journal, 7(2):155–162",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1964
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "They could, so far, only be trained with very low dimensional hidden spaces [2, 14, 15].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "They could, so far, only be trained with very low dimensional hidden spaces [2, 14, 15].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "They could, so far, only be trained with very low dimensional hidden spaces [2, 14, 15].",
      "startOffset" : 76,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "For these reasons, many studies including style and content models [16], other bi-linear models [17, 18], invariant sparse coding [19, 20], or invariant NMF [21] do not model occlusions.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 15,
      "context" : "For these reasons, many studies including style and content models [16], other bi-linear models [17, 18], invariant sparse coding [19, 20], or invariant NMF [21] do not model occlusions.",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 16,
      "context" : "For these reasons, many studies including style and content models [16], other bi-linear models [17, 18], invariant sparse coding [19, 20], or invariant NMF [21] do not model occlusions.",
      "startOffset" : 96,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "For these reasons, many studies including style and content models [16], other bi-linear models [17, 18], invariant sparse coding [19, 20], or invariant NMF [21] do not model occlusions.",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 18,
      "context" : "For these reasons, many studies including style and content models [16], other bi-linear models [17, 18], invariant sparse coding [19, 20], or invariant NMF [21] do not model occlusions.",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "For these reasons, many studies including style and content models [16], other bi-linear models [17, 18], invariant sparse coding [19, 20], or invariant NMF [21] do not model occlusions.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "We will, however, not model explicit occlusion using a depth variable (compare [2]) but will focus on the exclusiveness property.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "The applied model is a novel version of the invariant occlusive components model studied for mid-level vision earlier [22].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 12,
      "context" : ", TD2} (similar formulations have also been used in sprite models [14, 15]).",
      "startOffset" : 66,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : ", TD2} (similar formulations have also been used in sprite models [14, 15]).",
      "startOffset" : 66,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : "Following [22], we do not model component presence and absence explicitly but, for mathematical convenience, assign the special ‘position’ −1 to all the components which are not chosen to generate the patch.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "Component masks describe where an image component is located, and component features describe what a component encodes (compare [2, 3, 14, 15]).",
      "startOffset" : 128,
      "endOffset" : 142
    }, {
      "referenceID" : 2,
      "context" : "Component masks describe where an image component is located, and component features describe what a component encodes (compare [2, 3, 14, 15]).",
      "startOffset" : 128,
      "endOffset" : 142
    }, {
      "referenceID" : 12,
      "context" : "Component masks describe where an image component is located, and component features describe what a component encodes (compare [2, 3, 14, 15]).",
      "startOffset" : 128,
      "endOffset" : 142
    }, {
      "referenceID" : 13,
      "context" : "Component masks describe where an image component is located, and component features describe what a component encodes (compare [2, 3, 14, 15]).",
      "startOffset" : 128,
      "endOffset" : 142
    }, {
      "referenceID" : 20,
      "context" : "As a scalar value is much less distinctive than the sophisticated image features used in [22], the pre-selection of components has been changed to the complete component instead of only salient features.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "The equations of the mask parameter ~ αh, and feature parameter ~ wh are the same as in [22].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "ET approximates the posterior distribution as a truncated distribution [23]: p(~x|~y,Θ) ≈ p(~y, ~x|Θ) ∑ ~x′∈Kn p(~y, ~x ′|Θ) , if ~x ∈ Kn, (6)",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "of the approximations can be maintained [23].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "Based on the chosen features, a score value S(xh) is computed for each component at each position (see [22]).",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "This is different from the earlier work [22], where Kn is constructed directly according to S(xh).",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "To avoid converging to local optima, we used the directional annealing scheme [22] for our learning algorithm.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 1,
      "context" : "The artificial data is similar to data generated by the occlusive components analysis model (OCA; [2]), except of the use of scalar features and the assumption of shift-invariance.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 22,
      "context" : "As training set we used N = 100, 000 patches of size 16 × 16 pixels extracted at random positions from random images of the van Hateren natural image database [24].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "Many state-of-the-art systems for visual object classification make use of convolutional neural networks [12, 25, 26].",
      "startOffset" : 105,
      "endOffset" : 117
    }, {
      "referenceID" : 23,
      "context" : "Many state-of-the-art systems for visual object classification make use of convolutional neural networks [12, 25, 26].",
      "startOffset" : 105,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "Many state-of-the-art systems for visual object classification make use of convolutional neural networks [12, 25, 26].",
      "startOffset" : 105,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "Such networks compute the responses of a set of filters for all positions in a predefined area and use the maximal response for further processing ([12] for a review).",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "Such studies do report that neurally plausible receptive fields (RFs) in the form of Gabor functions emerge [17, 18].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : "Such studies do report that neurally plausible receptive fields (RFs) in the form of Gabor functions emerge [17, 18].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 19,
      "context" : "Likewise, invariant versions of NMF [21] or ICA (in the form of ISA [9] have been applied to image patches.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "Likewise, invariant versions of NMF [21] or ICA (in the form of ISA [9] have been applied to image patches.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "In particular, globular fields have attracted attention [5, 27, 28] as they have been reported together with Gabors in macaques and other species ([29] and [5] for further references).",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 26,
      "context" : "In particular, globular fields have attracted attention [5, 27, 28] as they have been reported together with Gabors in macaques and other species ([29] and [5] for further references).",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 27,
      "context" : "In particular, globular fields have attracted attention [5, 27, 28] as they have been reported together with Gabors in macaques and other species ([29] and [5] for further references).",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 26,
      "context" : "Such fields have been associated with occlusions before [5, 28, 30]; and our study now for the first time reports globular fields for an occlusive and translation invariant approach.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 28,
      "context" : "Such fields have been associated with occlusions before [5, 28, 30]; and our study now for the first time reports globular fields for an occlusive and translation invariant approach.",
      "startOffset" : 56,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "However, also linear convolutional approaches have recently reported such fields [19, 31].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 29,
      "context" : "However, also linear convolutional approaches have recently reported such fields [19, 31].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 30,
      "context" : "More concretely: for non-invariant linear sparse coding, globular fields only emerged from a sufficiently high degree of overcompleteness onwards [32, 33] or with specific prior settings and overcompleteness [27]; for non-invariant occlusive models [5, 30] globular fields always emerge alongside Gabors for any overcompleteness.",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 31,
      "context" : "More concretely: for non-invariant linear sparse coding, globular fields only emerged from a sufficiently high degree of overcompleteness onwards [32, 33] or with specific prior settings and overcompleteness [27]; for non-invariant occlusive models [5, 30] globular fields always emerge alongside Gabors for any overcompleteness.",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 25,
      "context" : "More concretely: for non-invariant linear sparse coding, globular fields only emerged from a sufficiently high degree of overcompleteness onwards [32, 33] or with specific prior settings and overcompleteness [27]; for non-invariant occlusive models [5, 30] globular fields always emerge alongside Gabors for any overcompleteness.",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 28,
      "context" : "More concretely: for non-invariant linear sparse coding, globular fields only emerged from a sufficiently high degree of overcompleteness onwards [32, 33] or with specific prior settings and overcompleteness [27]; for non-invariant occlusive models [5, 30] globular fields always emerge alongside Gabors for any overcompleteness.",
      "startOffset" : 249,
      "endOffset" : 256
    }, {
      "referenceID" : 27,
      "context" : "Such high percentages may be related to the high percentages of globular fields (∼16-23%) measured in vivo ([29] and [5] for references).",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 29,
      "context" : ", for convolutional matching pursuit [31] seems to be assigned exclusively to Gabor features.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 29,
      "context" : "Convolutional matching pursuit [31] shows results for up to 64 filters of size 8 × 8.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "Convolutional sparse coding [19] reports results by assuming 128 components for 9 × 9 patches.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 31,
      "context" : "In terms of components per observed variable, invariant models are therefore now computationally feasible in a regime the visual cortex is estimated to operate in [33].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 32,
      "context" : "While such a correspondence can connect our predictions to experimental studies of simple cells, recently developed approaches for the estimation of translation invariant cell responses [34, 35] can represent a more direct connection.",
      "startOffset" : 186,
      "endOffset" : 194
    }, {
      "referenceID" : 33,
      "context" : "While such a correspondence can connect our predictions to experimental studies of simple cells, recently developed approaches for the estimation of translation invariant cell responses [34, 35] can represent a more direct connection.",
      "startOffset" : 186,
      "endOffset" : 194
    }, {
      "referenceID" : 4,
      "context" : "As such our probabilistic model can be related to ideas of active control units for individual components [6, 7, 10, 11, 36] (also compare [37]).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "As such our probabilistic model can be related to ideas of active control units for individual components [6, 7, 10, 11, 36] (also compare [37]).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "As such our probabilistic model can be related to ideas of active control units for individual components [6, 7, 10, 11, 36] (also compare [37]).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "As such our probabilistic model can be related to ideas of active control units for individual components [6, 7, 10, 11, 36] (also compare [37]).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 34,
      "context" : "As such our probabilistic model can be related to ideas of active control units for individual components [6, 7, 10, 11, 36] (also compare [37]).",
      "startOffset" : 106,
      "endOffset" : 124
    }, {
      "referenceID" : 35,
      "context" : "As such our probabilistic model can be related to ideas of active control units for individual components [6, 7, 10, 11, 36] (also compare [37]).",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 35,
      "context" : "Different transformations could be considered or learned [37], explicit modeling in time could be incorporated (compare [17]), and/or further hierarchical stages could be considered.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "Different transformations could be considered or learned [37], explicit modeling in time could be incorporated (compare [17]), and/or further hierarchical stages could be considered.",
      "startOffset" : 120,
      "endOffset" : 124
    } ],
    "year" : 2013,
    "abstractText" : "We study optimal image encoding based on a generative approach with non-linear feature combinations and explicit position encoding. By far most approaches to unsupervised learning of visual features, such as sparse coding or ICA, account for translations by representing the same features at different positions. Some earlier models used a separate encoding of features and their positions to facilitate invariant data encoding and recognition. All probabilistic generative models with explicit position encoding have so far assumed a linear superposition of components to encode image patches. Here, we for the first time apply a model with non-linear feature superposition and explicit position encoding for patches. By avoiding linear superpositions, the studied model represents a closer match to component occlusions which are ubiquitous in natural images. In order to account for occlusions, the non-linear model encodes patches qualitatively very different from linear models by using component representations separated into mask and feature parameters. We first investigated encodings learned by the model using artificial data with mutually occluding components. We find that the model extracts the components, and that it can correctly identify the occlusive components with the hidden variables of the model. On natural image patches, the model learns component masks and features for typical image components. By using reverse correlation, we estimate the receptive fields associated with the model’s hidden units. We find many Gabor-like or globular receptive fields as well as fields sensitive to more complex structures. Our results show that probabilistic models that capture occlusions and invariances can be trained efficiently on image patches, and that the resulting encoding represents an alternative model for the neural encoding of images in the primary visual cortex.",
    "creator" : null
  }
}