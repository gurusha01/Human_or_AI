{
  "name" : "c850371fda6892fbfd1c5a5b457e5777.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings",
    "authors" : [ "Adam Smith", "Abhradeep Thakurta" ],
    "emails" : [ "asmith@cse.psu.edu", "b-abhrag@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "This paper looks at the information leaked by online learning algorithms, and seeks to design accurate learning algorithms with rigorous privacy guarantees – that is, algorithms that provably leak very little about individual inputs.\nEven the output of offline (batch) learning algorithms can leak private information. The dual form of a support vector machine’s solution, for example, is described in terms of a small number of exact data points, revealing these individuals’ data in the clear. Considerable effort has been devoted to designing batch learning algorithms satisfying differential privacy (a rigorous notion of privacy that emerged from the cryptography literature [DMNS06, Dwo06]), for example [BDMN05, KLN+08, CM08, CMS11, Smi11, KST12, JT13, DJW13].\nIn this work we provide a general technique for making a large class of online learning algorithms differentially private, in both the full information and bandit settings. Our technique applies to algorithms that aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. We modify the popular mirror descent approach (or rather a variant called follow the approximate leader) [Sha11, HAK07].\nIn most cases, the modified algorithms provide similar accuracy guarantees to their nonprivate counterparts, with a small (logarithmic in the stream length) blowup in space and time complexity.\nOnline (Convex) Learning: We begin with the full information setting. Consider an algorithm that receives a stream of inputs F = 〈f1, ...., fT 〉, each corresponding to one individual’s data. We interpret each input as a loss function on a parameter space C (for example, it might be one term ∗Supported by NSF awards #0941553 and #0747294. †Supported by Sloan Foundation fellowship and Microsoft Research.\nin a convex program such as the one for logistic regression). The algorithm’s goal is to output a sequence of parameter estimates w1, w2, ..., with each wt in C, that roughly minimizes the errors∑ t ft(wt). The difficulty for the algorithm is that it computes wt based only on f1, ..., ft−1. We seek to minimize the a posteriori regret,\nRegret(T ) = T∑ t=1 ft(wt)−min w∈C T∑ t=1 ft(w) (1)\nIn the bandit setting, the input to the algorithms consists only of f1(w1), f2(w2), .... That is, at each time step t, the algorithm learns only the cost ft−1(wt−1) of the choice wt−1 it made at the previous time step, rather than the full cost function ft−1.\nWe consider three types of adversarial input selection: An oblivious adversary selects the input stream f1, ..., fT ahead of time, based on knowledge of the algorithm but not of the algorithm’s random coins. A (strongly) adaptive adversary selects ft based on the output so far w1, w2, ..., wt (but not on the algorithm’s internal random coins).\nBoth the full-information and bandit settings are extensively studied in the literature (see, e.g., [Sha11, BCB12] for recent surveys). Most of this effort has been spent on online learning problems are convex, meaning that the loss functions ft are convex (in w) and the parameter set C ⊆ Rp is a convex set (note that one can typically “convexify” the parameter space by randomization). The problem dimension p is the dimension of the ambient space containing C. We consider various restrictions on the cost functions, such as Lipschitz continuity and strong convexity. A function f : C → R is L-Lipschitz with respect to the `2 metric if |f(x) − f(y)| ≤ L‖x − y‖2 for all x, y ∈ C. Equivalently, for every x ∈ C0 (the interior of C) and every subgradient z ∈ ∂f(x), we have ‖z‖2 ≤ L. (Recall that z is a subgradient of f at x if the function f̃(y) = f(x) + 〈z, y − x〉 is a lower bound for f on all of C. If f is convex, then a subgradient exists at every point, and the subgradient is unique if and only if f is differentiable at that point.) The function f is H-strongly convex w.r.t. `2 if for every y ∈ C, we can bound f below on C by a quadratic function of the form f̃(y) = f(x) + 〈z, y − x〉+ H2 ‖y− x‖ 2 2. If f is twice differentiable, H-strong convexity is equivalent to the requirement that all eigenvalues of ∇2f(w) be at least H for all w ∈ C. We denote by D the set of allowable cost functions; the input sequence thus lies in DT .\nDifferential Privacy, and Challenges for Privacy in the Online Setting: We seek to design online learning algorithms that satisfy differential privacy [DMNS06, Dwo06], which ensures that the amount of information an adversary learns about a particular cost function ft in the function sequence F is almost independent of its presence or absence in F . Each ft can be thought as private information belonging to an individual. The appropriate notion of privacy here is when the entire sequence of outputs of the algorithms (ŵ1, ..., ŵT ) is revealed to an attacker (the continual observation setting [DNPR10]). Formally, we say two input sequences F, F ′ ∈ DT are neighbors if they differ only in one entry (say, replacing ft by f ′t). Definition 2 (Differential privacy [DMNS06, Dwo06, DNPR10]). A randomized algorithm A is ( , δ)-differentially private if for every two neighboring sequences F, F ′ ∈ DT , and for every event O in the output space CT ,\nPr[A(F ) ∈ O] ≤ e Pr[A(F ′) ∈ O] + δ. (2) If δ is zero, then we simply say A is -differentially private.\nHereA(F ) refers to the entire sequence of outputs produced by the algorithm during its execution.1 Our protocols all satisfy -differential privacy (that is, with δ = 0). We include δ in the definition for comparison with previous work.\n1As defined, differential privacy requires indistinguishable outputs only for nonadaptively chosen sequences (that is, sequences where the inputs at time t are fixed ahead of time and do not depend on the outputs at times 1, ..., t − 1). The algorithms in our paper (and in previous work) in fact satisfy a stronger adaptive variant, in which an adversary selects the input online as the computation proceeds. When δ = 0, the nonadaptive and adaptive variants are equivalent [DNPR10]. Moreover, protocols based on “randomized response” or the “tree-based sum” protocol of [DNPR10, CSS10] are adaptively secure, even when δ > 0. We do not define the adaptive variant here explicitly, but we use it implicitly when proving privacy.\nDifferential privacy provides meaningful guarantees in against an attacker who has access to considerable side information: the attacker learns the same things about someone whether or not their data were actually used (see [KS08, DN10, KM12] for further discussion).\nDifferential privacy is particularly challenging to analyze for online learning algorithms, since a change in a single input at the beginning of the sequence may affect outputs at all future times in ways that are hard to predict. For example, a popular algorithm for online learning is online gradient descent: at each time step, the parameter is updated as wt+1 = ΠC(wt−1 − ηt∇ft−1(wt−1)), where ΠC(x) the nearest point to x in C, and ηt > 0 is a parameter called the learning rate. A change in an input fi (replacing it with f ′i ) leads to changes in all subsequent outputswi+1, wi+2, ..., roughly pushing them in the direction of ∇fi(wi) − ∇f ′i(wi). The effect is amplified by the fact that the gradient of subsequent functions fi+1, fi+2, ... will be evaluated at different points in the two streams.\nPrevious Approaches: Despite the challenges, there are several results on differentially private online learning. A special case, “learning from experts” in the full information setting, was discussed in the seminal paper of Dwork, Naor, Pitassi and Rothblum [DNPR10] on privacy under continual observation. In this case, the set of available actions is the simplex ∆({1, ..., p}) and the functions fi are linear with coefficients in {0, 1} (that is, ft(w) = 〈w, ct〉 where ct ∈ {0, 1}p). Their algorithm guarantees a weaker notion of privacy than the one we consider2 but, when adapted to our stronger setting, it yields a regret bound of O(p √ T/ ).\nJain, Kothari and Thakurta [JKT12] defined the general problem of private online learning, and gave algorithms for learning convex functions over convex domains in the full information setting. They gave algorithms that satisfy ( , δ)-differential privacy with δ > 0 (our algorithms satisfy the stronger variant with δ = 0). Specifically, their algorithms have regret Õ( √ T log(1/δ)/ ) for Lipshitz-\nbounded, strongly convex cost functions and Õ(T 2/3 log(1/δ)/ ) for general Lipshitz convex costs. The idea of [JKT12] for learning strongly convex functions is to bound the sensitivity of the entire vector of outputs w1, w2, ... to a change in one input (roughly, they show that when fi is changed, a subsequent output wj changes by O(1/|j − i|)). Unfortunately, the regret bounds obtained by previous work remain far from the best nonprivate bounds. [Zin03] gave an algorithm with regret O( √ T ) for general Lipshitz functions, assuming L and the diameter ‖C‖2 of C are constants. Ω( √ T ) regret is necessary (see, e.g., [HAK07]), so the dependence on T of [Zin03] is tight. When cost functions in F are H-strongly convex for constant H , then the regret can be improved to O(log T ) [HAK07], which is also tight. In this work, we give new algorithms that match these nonprivate bounds’ dependence on T , up to (poly log T )/ factors.\nWe note that [JKT12] give one algorithm for a specific strongly convex problem, online linear regression, with regret poly(log T ). One can view that algorithm as a special case of our results.\nWe are not aware of any previous work on privacy in the bandit setting. One might expect that bandit learning algorithms are easier to make private, since they access data in a much more limited way. However, even nonprivate algorithms for bandit learning are very delicate, and private versions had until now proved elusive.\nOur Results: In this work we provide a technique for making a large class of online learning algorithms differentially private, in both the full information and bandit settings. In both cases, the idea is to search for algorithms whose decisions at time t depend only on previous time steps through a sum of observations made at times 1, 2, ..., t. Specifically, our algorithms work by measuring the gradient ∇ft(wt) when ft is learned, and maintaining a differentially private running sum of the gradients observed so far. We maintain this sum using the tree-based sum protocol of [DNPR10, CSS10]. We then show that a class of learning algorithms known collectively as follow the approximate leader (the version we use is due to [HAK07]) can be run given only these noisy sums, and that their regret can be bounded even when these sums are inaccurate.\nOur algorithms can be run with space O(log T ), and require O(log T ) running time at each step.\n2Specifically, Dwork et al. [DNPR10] provide single-entry-level privacy, in the sense that a neighboring data set may only differ in one entry of the cost vector for one round. In contrast, we allow the entire cost vector to change at one round. Hiding that larger set of possible changes is more difficult, so our algorithms also satisfy the weaker notion of Dwork et al.\nOur contributions for the full information setting and their relation to previous work is summarized in Table 1. Our main algorithm, for strongly convex functions, achieves regret O( log 2.5 T ), ignoring factors of the dimension p, Lipschitz continuity L and strong convexity H . When strong convexity is not guaranteed, we use regularization to ensure it (similar to what is done in nonprivate settings,\ne.g. [Sha11]). Setting parameters carefully, we get regret of O( √ T log2.5 T ). These bounds essentially match the nonprivate lower bounds of Ω(log T ) and Ω( √ T ), respectively.\nThe results in the full information setting apply even when the input stream is chosen adaptively as a function of the algorithm’s choices at previous time steps. In the bandit setting, we distinguish between oblivious and adaptive adversaries.\nFurthermore, in the bandit setting, we assume that C is sandwiched between two concentric L2-balls of radii r and R (where r < R). We also assume that for all w ∈ C, |ft(w)| ≤ B for all t ∈ [T ]. Similar assumption were made in [FKM05, ADX10].\nOur results are summarized in Table 2. For most of the settings we consider, we match the dependence on T of the best nonprivate algorithm, though generally not the dependence on the dimension p.\nIn the remainder of the text, we refer to appendices for many of the details of algorithms and proofs. The appendices can be found in the “Supplementary Materials” associated to this paper."
    }, {
      "heading" : "2 Private Online Learning: Full-information Setting",
      "text" : "In this section we adapt the Follow The Approximate Leader (FTAL) algorithm of [HAK07] to design a differentially private variant. Our modified algorithm, which we call Private Follow The\nApproximate Leader (PFTAL), needs a new regret analysis as we have to deal with randomness due to differential privacy."
    }, {
      "heading" : "2.1 Private Follow The Approximate Leader (PFTAL) with Strongly Convex Costs",
      "text" : "Algorithm 1 Differentially Private Follow the Approximate Leader (PFTAL) Input: Cost functions: 〈f1, · · · , fT 〉 (in an online sequence), strong convexity parameter: H , Lip-\nschitz constant: L, convex set: C ⊆ Rp and privacy parameter: . 1: ŵ1 ← Any vector from C. Output ŵ1. 2: Pass5f1(ŵ1), L2-bound L and privacy parameter to the tree based aggregation protocol and\nreceive the current partial sum in v̂1. 3: for time steps t ∈ {1, · · · , T − 1} do\n4: ŵt+1 ← arg min w∈C 〈v̂t, w〉+ H2 t∑ τ=1 ‖w − ŵτ‖22. Output ŵt. 5: Pass 5ft+1(ŵt+1), L2-bound L and privacy parameter to the tree-based protocol (Algorithm 2) and receive the current partial sum in v̂t+1. 6: end for\nThe main idea in PFTAL algorithm is to execute the well-known Follow The Leader algorithm (FTL) algorithm [Han57] using quadratic approximations f̃1, · · · , f̃T of the cost functions f1, · · · , fT . Roughly, at every time step (t + 1), PFTAL outputs a vector w that approximately minimizes the sum of the approximations f̃1, · · · , f̃t over the convex set C. Let ŵ1, · · · , ŵt be the sequence of outputs produced in the first t time steps, and let ft be the costfunction at step t. Consider the following quadratic approximation to ft (as in [HAK07]). Define\nf̃t(w) = ft(ŵt) + 〈5ft(ŵt), w − ŵt〉+ H2 ‖w − ŵt‖ 2 2 (3)\nwhere H is the strong convexity parameter. Notice that ft and f̃t have the same value and gradient at ŵt (that is, ft(ŵt) = f̃t(ŵt) and 5ft(ŵt) = 5f̃t(ŵt)). Moreover, f̃t is a lower bound for ft everywhere on C.\nLet w̃t+1 = arg min w∈C t∑ τ=1 f̃τ (w) be the “leader” corresponding to the cost functions f̃1, · · · , f̃t. Minimizing the sum of f̃t(w) is the same as minimizing the sum of f̃t(w)−ft(ŵt), since subtracting a constant term won’t change the minimizer. We can thus write w̃t+1 as\nw̃t+1 = arg min w∈C 〈 t∑\nτ=1\n5ft(ŵτ ), w〉+ H2 t∑\nτ=1\n‖w − ŵτ‖22 (4)\nSuppose, ŵ1, · · · , ŵt have been released so far. To release a private approximation to w̃t+1, it suffices to approximate vt+1 = ∑t τ=15ft(ŵτ ) while ensuring differential privacy. If we fix the previously released information ŵτ , then changing any one cost function will only change one of the summands in vt+1.\nWith the above observation, we abstract out the following problem: Given a set of vectors z1, · · · , zT ∈ Rp, compute all the partial sums vt = t∑\nτ=1 zτ , while preserving privacy. This problem\nis well studied in the privacy literature. Assuming each zt has L2-norm of at most L′, the following tree-based aggregation scheme will ensure that in expectation, the noise (in terms of L2-error) in each of vt is O ( pL′ log1.5 T/ ) and the whole sequence v1, · · · , vT is -differentially private. We now describe the tree-based scheme.\nTree-based Aggregation [DNPR10, CSS10]: Consider a complete binary tree. The leaf nodes are the vectors z1, · · · , zT . (For the ease of exposition, assume T to be a power of two. In general, we can work with the smallest power of two greater than T ). Each internal node in the tree stores the sum of all the leaves in its sub-tree. In a differentially private version of this tree, we ensure that each node’s sub-tree sum is ( /log2T )-differentially private, by adding a noise vector b ∈ Rp\nwhose L2-norm is Gamma distributed and has standard deviation O( √ pL′ log T\n). Since each zt only affects log2T nodes in the tree, by the composition property [DMNS06], the complete tree will be -differentially private. Moreover, the algorithm’s error in estimating any partial sum vt = ∑t τ=1 zτ grows as O( √ pL′ log2 T\n), since one can compute vt from at most log T nodes in the tree. A formal description of the tree based aggregation scheme in given in Appendix A.\nNow we complete the PFTAL algorithm by computing the private version ŵt+1 of w̃t+1 in (4) as the minimizer of the perturbed loss function:\nŵt+1 = arg min w∈C 〈v̂t, w〉+ H2 t∑ τ=1 ‖w − ŵτ‖22 (5)\nHere v̂t is the noisy version of vt, computed using the tree-based aggregation scheme. A formal description of the algorithm is given in Algorithm 1.\nNote on space complexity: For simplicity, in the description of tree based aggregation scheme (Algorithm 2 in Appendix A) we maintain the complete binary tree. However, it is not hard to show at any time step t, it suffices to keep track of the vectors (of partial sums) in the path from zt to the root of the tree. So, the amount of space required by the algorithm is O(log T )."
    }, {
      "heading" : "2.1.1 Privacy and Utility Guarantees for PFTAL (Algorithm 1)",
      "text" : "In this section we provide the privacy and regret guarantees for the PFTAL algorithm (Algorithm 1). For detailed proofs of the theorem statements, see Appendix B. Theorem 3 (Privacy guarantee). Algorithm 1 is -differentially private.\nProof Sketch. Given the binary tree, the sequence ŵ2, · · · , ŵT is completely determined. Hence, it suffices to argue privacy for the collection of noisy sums associated to nodes in the binary tree. At first glance, it seems that each loss function affects only one leaf in the tree, and hence at most log T of the nodes’ partial sums. If it were true, that statement would make the analysis simple. The analysis is delicate, however, since the value (gradient zτ ) at a leaf τ in the tree depends on the partial sums that are released before time τ . Hence, changing one loss function ft actually affects all subsequent partial sums. One can get around this by using the fact that differential privacy composes adaptively [DMNS06]: we can write the computations done on a particular loss function ft as a sequence of log T smaller differentially private computations, where the each computation in the sequence depends on the outcome of previous ones. See Appendix B for details.\nIn terms of regret guarantee, we show that our algorithm enjoys regret of O(p log2.5 T ) (assuming other parameters to be constants). Compared to the non-private regret bound of O(log T ), our regret bound has an extra log1.5 T factor and an explicit dependence on the dimensionality (p). A formal regret bound for PFTAL algorithm is given in Theorem 4. Theorem 4 (Regret guarantee). Let f1, · · · , fT be L-Lipschitz, H-strongly convex functions and let C ⊆ Rp be a fixed convex set. For adaptive adversaries, the expected regret satisfies:\nE [Regret(T )] = O ( p(L+H‖C‖2)2 log2.5 T\nH\n) .\nHere expectation is taken over the random coins of the algorithm and adversary.\nResults for Lipschitz Convex Costs: Our algorithm for strongly convex costs can be adapted to arbitrary Lipschitz convex costs by executing Algorithm 1 on functions ht(w) = ft(w) + H2 ‖w‖ 2 2\ninstead of the ft’s. Setting H = O(p log2.5 T/( √ T )) will give us a regret bound of Õ( √ pT/ ). See Appendix C for details."
    }, {
      "heading" : "3 Private Online Learning: Bandit Setting",
      "text" : "In this section we adapt the Private Follow the Approximate Leader (PFTAL) from Section 2 to the bandit setting. Existing (nonprivate) bandit algorithms for online convex optimization follow\na generic reduction to the full-information setting [FKM05, ADX10], called the “one-point” (or “one-shot”) gradient trick. Our adaptation of PFTAL to the bandit setting also uses this technique. Specifically, to define the quadratic lower bounds to the input cost functions (as in (3)), we replace the exact gradient of ft at ŵt with a one-point approximation.\nIn this section we describe our results for strongly convex costs. Specifically, to define the quadratic lower bounds to the input cost functions (as in (3)), we replace the exact gradient of ft at ŵt with a one-point approximation. As in the full information setting, one may obtain regret bounds for general convex functions in the bandit setting by adding a strongly convex regularizer to the cost functions.\nOne-point Gradient Estimates [FKM05]: Suppose one has to estimate the gradient of a function f : Rp → R at a point w ∈ Rp via a single query access to f . [FKM05] showed that one can approximate5f(w) by pβ f(w + βu)u, where β > 0 is a small real parameter and u is a uniformly random vector from the p-dimensional unit sphere Sp−1 = {a ∈ Rp : ‖a‖2 = 1}. More precisely, 5f(w) = lim β→0 Eu [ p β f(w + βu)u ] .\nFor finite, nonzero values of β, one can view this technique as estimating the gradient of a smoothed version of f . Given β > 0, define f̂(w) = Ev∼Bp [f(w + βv)] where Bp is the unit ball in Rp. That is, f̂ = f ∗ UβBp is the convolution of f with the uniform distribution on the ball βBp of radius β. By Stokes’ theorem, we have Eu∼Sp−1 [ p β f(w + βu)u ] = 5f̂(w)."
    }, {
      "heading" : "3.1 Follow the Approximate Leader (Bandit version): Non-private Algorithm",
      "text" : "Let W̃ = 〈w̃1, · · · , w̃T 〉 be a sequence of vectors in C (the outputs of the algorithm). Corresponding to the smoothed function f̂t = f ∗ UβBp , we define a quadratic lower bound ĝt:\nĝt(w) = f̂t(w̃t) + 〈5f̂t(w̃t), w − w̃t〉+ H2 ‖w − w̃t‖ 2 2 (6)\nNotice that ĝt is a uniform lower bound on f̂t satisfying ĝt(w̃t) = f̂t(w̃t) and5ĝt(w̃t) = 5f̂t(w̃t).\nTo define ĝt, one needs access to 5f̂t(w̃t). As suggested above, we replace the true gradient with the one-point estimate. Consider the following proxy g̃t for ĝt:\ng̃t(w) = f̂t(w̃t)− 〈5f̂t(w̃t), w̃t〉︸ ︷︷ ︸ A +〈 p β ft(w̃t + βut)ut, w〉+ H 2 ‖w − w̃t‖22 (7)\nwhere uT is drawn uniformly from the unit sphere Sp−1. Note that in (7) we replaced the gradient of f̂t with its one-point approximation only in one of its two occurrences (the inner product with w).\nWe would like to define w̃t+1 as the minimizer of the sum of proxies ∑t τ=1 g̃τ (w). One difficulty remains: because ft is only assumed to be defined on C, the approximation pβ ft(w̃t+βut)ut is only defined when w̃t is sufficiently far inside C. Recall from the introduction that we assume C contains rBp (the ball of radius r). To ensure that we only evaluate f on C, we actually minimize over a smaller set (1− ξ)C, where ξ = βr . We obtain:\nw̃t+1 = arg min w∈(1−ξ)C t∑ τ=1 g̃τ (w) = arg min w∈(1−ξ)C 〈 t∑ τ=1 ( p β ft(w̃t + βut)ut ) , w〉+H2 t∑ τ=1 ‖w−w̃τ‖22 (8) (We have use the fact that to minimize g̃t, one can ignore the constant term A in (7).)\nWe can now state the bandit version of FTAL. At each step t = 1, ..., T :\n1. Compute w̃t+1 using (8). 2. Output ŵt = w̃t + βut.\nTheorem 12 (in Appendix D) gives the precise regret guarantees for this algorithm. For adaptive adversaries the regret is bounded by Õ(p2/3T 3/4) and for oblivious adversaries the regret is bounded by Õ(p2/3T 2/3)."
    }, {
      "heading" : "3.2 Follow the Approximate Leader (Bandit version): Private Algorithm",
      "text" : "To make the bandit version of FTAL -differentially private, we replace the value vt =∑t τ=1 ( p β ft(w † t + βut)ut ) with a private approximation v†t computed using the tree-based sum protocol. Specifically, at each time step t we output\nw†t+1 = arg min w∈(1−ξ)C 〈v†t , w〉+ H 2 t∑ τ=1 ‖w − w†τ‖22 . (9)\nSee Algorithm 3 (Appendix E.1) for details. Theorem 5 (Privacy guarantee). The bandit version of Private Follow The Approximate Leader (Algorithm 3) is -differentially private.\nThe proof of Theorem 5 is exactly the same as of Theorem 3, and hence we omit the details.\nIn the following theorem we provide the regret guarantee of the Private FTAL (bandit version). For a complete proof, see Appendix E.2. Theorem 6 (Regret guarantee). Let Bp be the p-dimensional unit ball centered at the origin and C ⊆ Rp be a convex set such that rBp ⊆ C ⊆ RBp (where 0 < r < R). Let f1, · · · , fT be LLipschitz, H-strongly convex functions such that for all w ∈ C, |fi(w)| ≤ B. Setting ξ = β/r in the bandit version of Private Follow The Approximate Leader (Algorithm 3 in Appendix E.1), we obtain the following regret guarantees.\n1. (Oblivious adversary) With β = p T 1/3\n, E [Regret(T )] ≤ Õ ( pT 2/3χ ) 2. (Adaptive adversary) With β = p\nT 1/4 , E [Regret(T )] ≤ Õ\n( pT 3/4χ ) Here χ = ( BR+ (1 +R/r)L+ (H‖C‖2+B) 2\nH\n( 1 + B )) . The expectations are taken over the ran-\ndomness of the algorithm and the adversary.\nOne can remove the dependence on r in Thm. 6 by rescaling C to isotropic position. This increases the expected regret bound by a factor of (LR+ ‖C‖2). See [FKM05] for details.\nBound for general convex functions: Our results in this section can be extended to the setting of arbitrary Lipshitz convex costs via regularization, as in Section C (by adding H2 ‖w‖ 2 2 to each cost function ft) . With the appropriate choice of H the regret scales as Õ(T 3/4/ ) for both oblivious and adaptive adversaries. See Appendix E.3 for details."
    }, {
      "heading" : "4 Open Questions",
      "text" : "Our work raises several interesting open questions: First, our regret bounds with general convex functions have the form Õ( √ T/ ). We would like to have a regret bound where the parameter 1/ is factored out with lower order terms in the regret, i.e., we would like to have regret bound of the form O( √ T ) + o( √ T/ ).\nSecond, our regret bounds for convex bandits are worse than the non-private bounds for linear and multi-arm bandits. For multi-arm bandits [ACBF02] and for linear bandits [AHR08], the non-private regret bound is known to be O( √ T ). If we use our private algorithm in this setting, we will incur a regret of Õ(T 2/3). Can we get O( √ T ) regret for multi-arm or linear bandits?\nFinally, bandit algorithms require internal randomness to get reasonable regret guarantees. Can we harness the randomness of non-private bandit algorithms in the design private bandit algorithms? Our current privacy analysis ignores this additional source of randomness."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Optimal algorithms for online convex optimization with multi-point bandit feedback",
      "author" : [ "Alekh Agarwal", "Ofer Dekel", "Lin Xiao" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2010
    }, {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2008
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multiarmed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi" ],
      "venue" : "arXiv preprint arXiv:1204.5721,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Practical privacy: The SuLQ framework",
      "author" : [ "Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim" ],
      "venue" : "In PODS,",
      "citeRegEx" : "Blum et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Blum et al\\.",
      "year" : 2005
    }, {
      "title" : "Privacy-preserving logistic regression",
      "author" : [ "Kamalika Chaudhuri", "Claire Monteleoni" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Chaudhuri and Monteleoni.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chaudhuri and Monteleoni.",
      "year" : 2008
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2011
    }, {
      "title" : "Private and continual release of statistics",
      "author" : [ "TH Hubert Chan", "Elaine Shi", "Dawn Song" ],
      "venue" : "In ICALP,",
      "citeRegEx" : "Chan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2010
    }, {
      "title" : "Local privacy and statistical minimax rates",
      "author" : [ "John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright" ],
      "venue" : "In IEEE Symp. on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Duchi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2013
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith" ],
      "venue" : "In TCC,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "On the difficulties of disclosure prevention in statistical databases or the case for differential privacy",
      "author" : [ "Cynthia Dwork", "Moni Naor" ],
      "venue" : "J. Privacy and Confidentiality,",
      "citeRegEx" : "Dwork and Naor.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dwork and Naor.",
      "year" : 2010
    }, {
      "title" : "Differential privacy under continual observation",
      "author" : [ "Cynthia Dwork", "Moni Naor", "Toniann Pitassi", "Guy N Rothblum" ],
      "venue" : "In Proceedings of the 42nd ACM symposium on Theory of computing,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2010
    }, {
      "title" : "Online convex optimization in the bandit setting: gradient descent without a gradient",
      "author" : [ "Abraham D Flaxman", "Adam Tauman Kalai", "H Brendan McMahan" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "Elad Hazan", "Amit Agarwal", "Satyen Kale" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2007
    }, {
      "title" : "Approximation to bayes risk in repeated play",
      "author" : [ "James Hannan" ],
      "venue" : null,
      "citeRegEx" : "Hannan.,? \\Q1957\\E",
      "shortCiteRegEx" : "Hannan.",
      "year" : 1957
    }, {
      "title" : "Differentially private online learning",
      "author" : [ "Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Jain et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jain et al\\.",
      "year" : 2012
    }, {
      "title" : "Differentially private learning with kernels",
      "author" : [ "Prateek Jain", "Abhradeep Thakurta" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Jain and Thakurta.,? \\Q2013\\E",
      "shortCiteRegEx" : "Jain and Thakurta.",
      "year" : 2013
    }, {
      "title" : "What can we learn privately",
      "author" : [ "Shiva Prasad Kasiviswanathan", "Homin K. Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Kasiviswanathan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kasiviswanathan et al\\.",
      "year" : 2008
    }, {
      "title" : "A rigorous and customizable framework for privacy",
      "author" : [ "Daniel Kifer", "Ashwin Machanavajjhala" ],
      "venue" : "In PODS,",
      "citeRegEx" : "Kifer and Machanavajjhala.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kifer and Machanavajjhala.",
      "year" : 2012
    }, {
      "title" : "A note on differential privacy: Defining resistance to arbitrary side information",
      "author" : [ "Shiva Prasad Kasiviswanathan", "Adam Smith" ],
      "venue" : "[cs.CR],",
      "citeRegEx" : "Kasiviswanathan and Smith.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kasiviswanathan and Smith.",
      "year" : 2008
    }, {
      "title" : "Private convex empirical risk minimization and high-dimensional regression",
      "author" : [ "Daniel Kifer", "Adam Smith", "Abhradeep Thakurta" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Kifer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kifer et al\\.",
      "year" : 2012
    }, {
      "title" : "Privacy-preserving statistical estimators with optimal convergence rates",
      "author" : [ "Adam Smith" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Smith.,? \\Q2011\\E",
      "shortCiteRegEx" : "Smith.",
      "year" : 2011
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "Martin Zinkevich" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zinkevich.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich.",
      "year" : 2003
    } ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader. The technique leads to the first nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T , of the optimal nonprivate regret bounds up to logarithmic factors in T . Our algorithms require logarithmic space and update time.",
    "creator" : "pdftk 1.44 - www.pdftk.com"
  }
}