{
  "name" : "d64a340bcb633f536d56e51874281454.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements",
    "authors" : [ "Divyanshu Vats", "Richard Baraniuk" ],
    "emails" : [ "dvats@rice.edu", "richb@rice.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "An important problem that arises in many applications is that of recovering a high-dimensional sparse (or approximately sparse) vector given a small number of linear measurements. Depending on the problem of interest, the unknown sparse vector can encode relationships between genes [1], power line failures in massive power grid networks [2], sparse representations of signals [3, 4], or edges in a graphical model [5,6], to name just a few applications. The simplest, but still very useful, setting is when the observations can be approximated as a sparse linear combination of the columns in a measurement matrix X weighted by the non-zero entries of the unknown sparse vector. In this paper, we study the problem of recovering the location of the non-zero entries, say S∗, in the unknown vector, which is equivalent to recovering the columns of X that y depends on. In the literature, this problem is often to referred to as the sparse recovery or the support recovery problem.\nAlthough several tractable sparse recovery algorithms have been proposed in the literature, statistical guarantees for accurately estimating S∗ can only be provided under conditions that limit how correlated the columns of X can be. For example, if there exists a column, say Xi, that is nearly linearly dependent on the columns indexed by S∗, some sparse recovery algorithms may falsely select Xi. In certain applications, where X can be specified a priori, correlations can easily be avoided by appropriately choosing X . However, in many applications, X cannot be specified by a practitioner, and correlated measurement matrices are inevitable. For example, when the columns in X correspond to gene expression values, it has been observed that genes in the same pathway produce correlated values [1]. Additionally, it has been observed that regions in the brain that are in close proximity produce correlated signals as measured using an MRI [7].\nIn this paper, we develop new sparse recovery algorithms that can accurately recover S∗ for measurement matrices that exhibit strong correlations. We propose a greedy algorithm, called SWAP, that iteratively swaps variables starting from an initial estimate of S∗ until a desired loss function cannot be decreased any further. We prove that SWAP can accurately identify the true signal support\nunder relatively mild conditions on the restricted eigenvalues of the matrix XTX and under certain conditions on the correlations between the columns of X . A novel aspect of our theory is that the conditions we derive are only needed when conventional sparse recovery algorithms fail to recover S∗. This motivates the use of SWAP as a wrapper around sparse recovery algorithms for improved performance. Finally, using numerical simulations, we show that SWAP consistently outperforms many state of the art algorithms on both synthetic and real data corresponding to gene expression values.\nAs alluded to earlier, several algorithms now exist in the literature for accurately estimating S∗. The theoretical properties of such algorithms either depend on the irrepresentability condition [5, 8–10] or various forms of the restricted eigenvalue conditions [11,12]. See [13] for a comprehensive review of such algorithms and the related conditions. SWAP is a greedy algorithm with novel guarantees for sparse recovery and we make appropriate comparisons in the text. Another line of research when dealing with correlated measurements is to estimate a superset of S∗; see [14–18] for examples.\nThe rest of the paper is organized as follows. Section 2 formally defines the sparse recovery problem. Section 3 introduces SWAP. Section 4 presents theoretical results on the conditions needed for provably correct sparse recovery. Section 5 discusses numerical simulations. Section 6 summarizes the paper and discusses future work."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "Throughout this paper, we assume that y ∈ Rn and X ∈ Rn×p are known and related to each other by the linear model y = Xβ∗ + w , (1) where β∗ ∈ Rp is the unknown sparse vector that we seek to estimate. We assume that the columns of X are normalized, i.e., ‖Xi‖22/n = 1 for all i ∈ [p], where we use the notation [p] = {1, 2, . . . , p} throughout the paper. In practice, normalization can easily be done by scaling X and β∗ accordingly. We assume that the entries of w are i.i.d. zero-mean sub-Gaussian random variables with parameter σ so that E[exp(twi)] ≤ exp(t2σ2/2). The sub-Gaussian condition on w is common in the literature and allows for a wide class of noise models, including Gaussian, symmetric Bernoulli, and bounded random variables. We let k be the number of non-zero entries in β∗, and let S∗ denote the location of the non-zero entries. It is common to refer to S∗ as the support of β∗ and we adopt this notation throughout the paper.\nOnce S∗ has been estimated, it is relatively straightforward to estimate β∗. Thus, we mainly focus on the sparse recovery problem of estimating S∗. A classical strategy for sparse recovery is to search for a support of size k that minimizes a suitable loss function. For a support S, we assume the least-squares loss, which is defined as follows:\nL(S; y,X) := min α∈R|S|\n‖y −XSα‖22 = ∥∥Π⊥[S]y∥∥2 2 , (2)\nwhere XS refers to an n × |S| matrix that only includes the columns indexed by S and Π⊥[S] = I−XS(XTSXS)−1XTS is the orthogonal projection onto the null space of the linear operator XS . In this paper, we design a sparse recovery algorithm that provably, and efficiently, finds the true support for a broad class of measurement matrices that includes matrices with high correlations.\n3 Overview of SWAP\nWe now describe our proposed greedy algorithm SWAP. Recall that our main goal is to find a support Ŝ that minimizes the loss defined in (2). Suppose that we are given an estimate, say S(1), of the true support and let L(1) be the corresponding least-squares loss (see (2)). We want to transition to another estimate S(2) that is closer (in terms of the number of true variables), or equal, to S∗. Our main idea to transition from S(1) to an appropriate S(2) is to swap variables as follows:\nSwap every i ∈ S(1) with i′ ∈ (S(1))c and compute the resulting loss L(1)i,i′ = L({S(1)\\i}∪i′; y,X).\nIf mini,i′ L (1) i,i′ < L (1), there exists a support that has a lower loss than the original one. Subsequently, we find {̂i, î′} = argmini,i′ L(1)i,i′ and let S(2) = {S(1)\\̂i} ∪ {̂i′}. We repeat the\nAlgorithm 1: SWAP(y,X, S)\nInputs: Measurements y, design matrix X , and initial support S. 1 Let r = 1, S(1) = S, and L(1) = L(S(1); y,X) 2 Swap i ∈ S(r) with i′ ∈ (S(r))c and compute the loss L(r)i,i′ = L({S(r)\\i} ∪ i′; y,X). 3 if mini,i′ L(r)i,i′ < L(r) then 4 {̂i, î′} = argmini,i′ L(r)i,i′ (In case of a tie, choose a pair arbitrarily) 5 Let S(r+1) = {S(r)\\̂i} ∪ î′ and L(r+1) be the corresponding loss. 6 Let r = r + 1 and repeat steps 2-4.\nelse 7 Return Ŝ = S(r).\nabove steps to find a sequence of supports S(1), S(2), . . . , S(r), where S(r) has the property that mini,i′ L (r) i,i′ ≥ L(r). In other words, we stop SWAP when perturbing S(r) by one variable increases or does not change the resulting loss. These steps are summarized in Algorithm 1.\nFigure 1 illustrates the performance of SWAP for a matrix X that corresponds to 83 samples of 2308 gene expression values for patients with small round blue cell tumors [19]. Since there is no ground truth available, we simulate the observations y using Gaussian w with σ = 0.5 and randomly chosen sparse vectors with non-zero entries between 1 and 2. Figure 1(a) shows the histogram of the eigenvalues of 10,000 randomly chosen matrices XTAXA/n, where |A| = 10. We clearly see that these eigenvalues are very small. This means that the columns of X are highly correlated with each other. Figure 1(c) shows the mean fraction of variables estimated to be in the true support over 100 different trials. Figure 1(d) shows the mean number of iterations required for SWAP to converge.\nRemark 3.1. The main input to SWAP is the initial supportS. This parameter implicitly specifies the desired sparsity level. Although SWAP can be used with a random initialization S, we recommend using SWAP in combination with another sparse recovery algorithm. For example, in Figure 1(c), we run SWAP using four different types of initializations. The dashed lines represent standard sparse recovery algorithms, while the solid lines with markers represent SWAP algorithms. We clearly see that all SWAP based algorithms outperform standard algorithms. Intuitively, since many sparse recovery algorithms can perform partial support recovery, using such an initialization results in a smaller search space when searching for the true support.\nRemark 3.2. Since each iteration of SWAP necessarily produces a unique loss, the supports S(1), . . . , S(r) are all unique. Thus, SWAP clearly converges in a finite number of iterations. The exact convergence rate depends on the correlations in the matrix X . Although we do not theoretically quantify the convergence rate, in all numerical simulations, and over a broad range of design matrices, we observed that SWAP converged in roughly O(k) iterations. See Figure 1(d) for an example.\nRemark 3.3. Using the properties of orthogonal projections, we can write Line 2 of SWAP as a difference of two rank one projection matrices. The main computational complexity is in computing\nthis quantity k(p − k) times for all i ∈ S(r) and i′ ∈ (S(r)c. If the computational complexity of computing a rank k orthogonal projection is Ik, then Line 2 can be implemented in time O(k(Ik + p− k). When k p is small, then Ik = O(k3). When k is large, then several computational tricks can be used to significantly reduce the computational time.\nRemark 3.4. SWAP differs significantly from other greedy algorithms in the literature. When k is known, the main distinctive feature of SWAP is that it always maintains a k-sparse estimate of the support. Note that the same is true for the computationally intractable exhaustive search algorithm [10]. Other competitive algorithms, such as forward-backwards (FoBa) [20] or CoSaMP [21], usually estimate a signal with higher sparsity level and iteratively remove variables until k variables are selected. The same is true for multi-stage algorithms [22–25]. Intuitively, as we shall see in Section 4, by maintaining a support of size k, the performance of SWAP only depends on correlations among the columns of the matrix XA, where A is of size at most 2k and it includes the true support. In contrast, for other sparse recovery algorithms, |A| ≥ 2k. In Figure 1, we compare SWAP to several state of the art algorithms (see Section 5 for a description of the algorithms). In all cases, SWAP results in superior performance.\n4 Theoretical Analysis of SWAP"
    }, {
      "heading" : "4.1 Some Important Parameters",
      "text" : "In this Section, we collect some important parameters that determine the performance of SWAP. First, we define the restricted eigenvalue as\nρk+ := inf {‖Xθ‖22 n‖θ‖22 : ‖θ‖0 ≤ k + , |S∗ ∩ supp(θ)| = k } . (3)\nThe parameter ρk+ is the minimum eigenvalue of certain blocks of the matrix XTX/n of size 2k that includes the blocks XTS∗XS∗/n. Smaller values of ρk+ correspond to correlated columns in the matrix X . Next, we define the minimum absolute value of the non-zero entries in β∗ as\nβmin := min i∈S∗ |β∗i | . (4) A smaller βmin will evidently require more number of observations for exact recovery of the support. Finally, we define a parameter that characterizes the correlations between the columns of the matrix XS∗ and the columns of the matrix X(S∗)c , where recall that S∗ is the true support of the unknown sparse vector β∗. For a set Ωk,d that contains all supports of size k with atleast k−d active variables from S∗, define γd as\nγ2d := max S∈Ωk,d\\S∗ min i∈(S∗)c∩S\n∥∥∥∥ΣS\\ii,S̄ (ΣS\\iS̄,S̄)−1 ∥∥∥∥ 2\n1\nΣ S\\i i,i\n, S̄ = S∗\\S , (5)\nwhere ΣB = XTΠ⊥[B]X/n. Popular sparse regression algorithms, such as the Lasso and the OMP, can perform accurate support recovery when ζ2 = maxi∈(S∗)c ‖Σi,S∗Σ−1S∗,S∗‖21 < 1. We will show in Section 3.2 that SWAP can perform accurate support recovery when γd < 1. Although the form of γd is similar to ζ, there are several key differences, which we highlight as follows:\n• Since Ωk,d contains all supports such that |S∗\\S| ≤ d, it is clear that γd is the 1 norm of a d× 1 vector, where d ≤ k. In contrast, ζ is the 1 norm of a k× 1 vector. If indeed ζ < 1, i.e., accurate support recovery is possible using the Lasso, then SWAP can be initialized by the output of the Lasso. In this case, γ(Ω) = 0 and SWAP also outputs the true support as long as S∗ minimizes the loss function. We make this statement precise in Theorem 4.1. Thus, it is only when ζ ≥ 1 that the parameter γd plays a role in the performance of SWAP. • The parameter ζ directly computes correlations between the columns of X . In contrast, γd computes correlations between the columns of X when projected onto the null space of a matrix XB , where |B| = d− 1. • Notice that γd is computed by taking a maximum over supports in the set Ωd\\S∗ and a minimum over inactive variables in each support. The reason that the minimum appears in γd is because we choose to swap variables that result in the smallest loss. In contrast, ζ is computed by taking a maximum over all inactive variables."
    }, {
      "heading" : "4.2 Statement of Main Results",
      "text" : "In this Section, we state the main results that characterize the performance of SWAP. Throughout this Section, we assume the following:\n(A1) The observations y and the measurement matrix X follow the linear model in (1), where the noise is sub-Gaussian with parameter σ, and the columns of X have been normalized.\n(A2) SWAP is initialized with a support S(1) of size k and Ŝ is the output of SWAP. Since k is typically unknown, a suitable value can be selected using standard model selection algorithms such as cross-validation or stability selection [26].\nOur first result for SWAP is as follows.\nTheorem 4.1. Suppose (A1)-(A2) holds and |S∗\\S(1)| ≤ 1. If n > 4+log(k2(p−k)) c2β2minρ2k/2 , where 0 < c2 ≤ 1/(18σ2), then P(Ŝ = S∗) → 1 as (n, p, k) → ∞. The proof of Theorem 4.1 can be found in the extended version of our paper [27]. Informally, Theorem 4.1 states that if the input to SWAP falsely detects at most one variable, then SWAP is high-dimensional consistent when given a sufficient number of observations n. The condition on n is mainly enforced to guarantee that the true support S∗ minimizes the loss function. This condition is weaker than the sufficient conditions required for other computationally tractable sparse recovery algorithms. For example, the method FoBa is known to be superior to other methods such as the Lasso and the OMP. As shown in [20], FoBa requires that n = Ω(log(p)/(ρ3k+ β 2 min)) for high-dimensional consistent support recovery, where the choice of , which is greater than k, depends on the correlations in the matrix X . In contrast, the condition in (4.1), which reduces to n = Ω(log(p − k)/(ρ2kβ2min)), is weaker since 1/ρ3k+ < 1/ρ2k for > k and p − k < p. This shows that if a sparse recovery algorithm can accurately estimate the true support, then SWAP does not introduce any false positives and also outputs the true support. Furthermore, if a sparse regression algorithm falsely detects one variable, then SWAP can potentially recover the correct support. Thus, using SWAP with other algorithms does not harm the sparse recovery performance of other algorithms.\nWe now consider the more interesting case when SWAP is initialized by a support S(1) that falsely detects more than one variable. In this case, SWAP will clearly needs more than one iteration to recover the true support. Furthermore, to ensure that the true support can be recovered, we need to impose some additional assumptions on the measurement matrix X . The particular assumption we enforce will depend on the parameter γk defined in (5). As mentioned in Section 4.1, γk captures the correlations between the columns of XS∗ and the columns of X(S∗)c . To simplify the statement in the next Theorem, define let g(δ, ρ, c) = g(δ, ρ, c) = (δ − 1) + 2c(√δ + 1/√ρ) + 2c2 . Theorem 4.2. Suppose (A1)-(A2) holds and |S∗\\S(1)| > 1. If for a constant c such that 0 < c2 < 1/(18σ2), g(γk, ρk,1, cσ) < 0, log ( p k ) > 4 + log(k2(p − k)), and n > 2 log ( p k)\nc2β2minρ 2 2k\n, then\nP(Ŝ = S∗) → 1 as (n, p, k) → ∞. Theorem 4.2 says that if SWAP is initialized with any support of size k, and γk satisfies the condition stated in the theorem, then SWAP will output the true support when given a sufficient number of observations. In the noiseless case, i.e., when σ = 0, the condition required for accurate support recovery reduces to γk < 1. The proof of Theorem 4.2, outlined in [27], relies on imposing conditions on each support of size k such that that there exists a swap so that the loss can be necessarily decreased. Clearly, if such a property holds for each support, except S∗, then SWAP will output the true support since (i) there are only a finite number of possible supports, and (ii) each iteration of SWAP results in a different support. The dependence on ( p k ) in the expression for the number of observations n arises from applying the union bound over all supports of size k.\nThe condition in Theorem 4.2 is independent of the initialization S(1). This is why the sample complexity, i.e., the number of observations n required for consistent support recovery, scales as log ( p k ) . To reduce the sample complexity, we can impose additional conditions on the support S(1) that is used to initialize SWAP. Under such assumptions, assuming that |S∗\\S(1)| > d, the\nperformance of SWAP will depend on γd, which is less than γk, and n will scale as log ( p d ) . We refer to [27] for more details."
    }, {
      "heading" : "5 Numerical Simulations",
      "text" : "In this section, we show how SWAP compares to other sparse recovery algorithms. Section 5.1 presents results for synthetic data and Section 5.2 presents results for real data."
    }, {
      "heading" : "5.1 Synthetic Data",
      "text" : "To illustrate the advantages of SWAP, we use the following examples:\n(A1) We sample the rows of X from a Gaussian distribution with mean zero and covariance Σ. The covariance Σ is block-diagonal with blocks of size 10. The entries in each block Σ̄ are specified as follows: Σ̄ii = 1 for i ∈ [10] and Σ̄ij = a for i = j. This construction of the design matrix is motivated from [18]. The true support is chosen so that each variable in the support is assigned to a different block. The non-zero entries in β∗ are chosen uniformly between 1 and 2. We let σ = 1, p = 500, n = 100, 200, k = 20, and a = 0.5, 0.55, . . . , 0.9, 0.95.\n(A2) We sample X from the same distribution as described in (A1). The only difference is that the true support is chosen so that five different blocks contain active variables and each chosen block contains four active variables. The rest of the parameters are also the same.\nIn both (A1) and (A2), as a increases, the strength of correlations between the columns increases. Further, the restricted eigenvalue parameter for (A1) is greater than the restricted eigenvalue parameter of (A2).\nWe use the following sparse recovery algorithms to initialize SWAP: (i) Lasso, (ii) Thresholded Lasso (TLasso) [25], (iii) Forward-Backward (FoBa) [20], (iv) CoSaMP [21], (v) Marginal Regression (MaR), and (vi) Random. TLasso first applies Lasso to select a superset of the support and then selects the largest k as the estimated support. In our implementation, we used Lasso to select 2k variables and then selected the largest k variables after least-squares. This algorithm is known to have better performance that the Lasso. FoBa uses a combination of a forward and a backwards algorithm. CoSaMP is an iterative greedy algorithm. MaR selects the support by choosing the largest k variables in |XTy|. Finally, Random selects a random subset of size k. We use the notation STLasso to refer to the algorithm that uses TLasso as an initialization for SWAP. A similar notation follows for other algorithms.\nOur results are shown in Figure 2. We use two metrics to assess the performance of SWAP. The first metric is the true positive rate (TPR), i.e., the number of active variables in the estimate divided by the total number of active variables. The second metric is the the number of iterations needed for SWAP to converge. Since all the results are over supports of size k, the false postive rate (FPR) is simply 1 − TPR. All results for SWAP based algorithms have markers, while all results for non SWAP based algorithms are represented in dashed lines.\nFrom the TPR performance, we clearly see the advantages of using SWAP in practice. For different choices the algorithm Alg, when n = 100, the performance of S-Alg is always better than the performance of Alg. When the number of observations increase to n = 200, we observe that all SWAP based algorithms perform better than standard sparse recovery algorithms. For (A1), we have exact support recovery for SWAP when a ≤ 0.9. For (A2), we have exact support recovery when a < 0.8. The reason for this difference is because of the differences in the placement of the non-zero entries.\nFigures 2(a) and 2(b) shows the mean number of iterations required by SWAP based algorithms as the correlations in the matrix X increase. We clearly see that the number of iterations increase with the degree of correlations. For algorithms that estimate a large fraction of the true support (TLasso, FoBa, and CoSaMP), the number of iterations is generally very small. For MaR and Random, the number of iterations is larger, but still comparable to the sparsity level of k = 20."
    }, {
      "heading" : "5.2 Gene Expression Data",
      "text" : "We now present results on two gene expression cancer datasets. The first dataset1 contains expression values from patients with two different types cancers related to leukemia. The second dataset2 contains expression levels from patients with and without prostate cancer. The matrix X contains the gene expression values and the vector y is an indictor of the type of cancer a patient has. Although this is a classification problem, we treat it as a recovery problem. For the leukemia data, p = 5147 and n = 72. For the prostate cancer data, p = 12533 and n = 102. This is clearly a high-dimensional dataset, and the goal is to identify a small set of genes that are predictive of the cancer type.\nFigure 3 shows the performance of standard algorithms vs. SWAP. We use leave-one-out crossvalidation and apply the sparse recovery algorithms described in Section 5.1 using multiple different choices of the sparsity level. For each level of sparsity, we choose the sparse recovery algorithm (labeled as standard) and the SWAP based algorithm that results in the minimum least-squares loss over the training data. This allows us to compare the performance of using SWAP vs. not using SWAP. For both datasets, we clearly see that the training and testing error is lower for SWAP based algorithms. This means that SWAP is able to choose a subset of genes that has better predictive performance than that of standard algorithms for each level of sparsity.\n1see http://www.biolab.si/supp/bi-cancer/projections/info/leukemia.htm 2see http://www.biolab.si/supp/bi-cancer/projections/info/prostata.htm"
    }, {
      "heading" : "6 Summary and Future Work",
      "text" : "We studied the sparse recovery problem of estimating the support of a high-dimensional sparse vector when given a measurement matrix that contains correlated columns. We presented a simple algorithm, called SWAP, that iteratively swaps variables starting from an initial estimate of the support until an appropriate loss function can no longer be decreased further. We showed that SWAP is surprising effective in situations where the measurement matrix contains correlated columns. We theoretically quantified the conditions on the measurement matrix that guarantee accurate support recovery. Our theoretical results show that if SWAP is initialized with a support that contains some active variables, then SWAP can tolerate even higher correlations in the measurement matrix. Using numerical simulations on synthetic and real data, we showed how SWAP outperformed several sparse recovery algorithms.\nOur work in this paper sets up a platform to study the following interesting extensions of SWAP. The first is a generalization of SWAP so that a group of variables can be swapped in a sequential manner. The second is a detailed analysis of SWAP when used with other sparse recovery algorithms. The third is an extension of SWAP to high-dimensional vectors that admit structured sparse representations."
    }, {
      "heading" : "Acknowledgement",
      "text" : "The authors would like to thank Aswin Sankaranarayanan and Christoph Studer for feedback and discussions. The work of D. Vats was partly supported by an Institute for Mathematics and Applications (IMA) Postdoctoral Fellowship."
    } ],
    "references" : [ {
      "title" : "Regression approaches for microarray data analysis",
      "author" : [ "M. Segal", "K. Dahlquist", "B. Conklin" ],
      "venue" : "Journal of Computational Biology, vol. 10, no. 6, pp. 961–980, 2003.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Sparse overcomplete representations for efficient identification of power line outages",
      "author" : [ "H. Zhu", "G. Giannakis" ],
      "venue" : "IEEE Transactions on Power Systems, vol. 27, no. 4, pp. 2215 –2224, nov. 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "E.J. Candès", "J. Romberg", "T. Tao" ],
      "venue" : "IEEE Trans. Information Theory, vol. 52, no. 2, pp. 489–509, 2006.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Single-pixel imaging via compressive sampling",
      "author" : [ "M.F. Duarte", "M.A. Davenport", "D. Takhar", "J.N. Laska", "T. Sun", "K.F. Kelly", "R.G. Baraniuk" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 83–91, Mar. 2008.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "High-dimensional graphs and variable selection with the Lasso",
      "author" : [ "N. Meinshausen", "P. Bühlmann" ],
      "venue" : "Annals of Statistics, vol. 34, no. 3, pp. 1436, 2006.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "High-dimensional Ising model selection using 1-egularized logistic regression",
      "author" : [ "P. Ravikumar", "M. Wainwright", "J. Lafferty" ],
      "venue" : "Annals of Statistics, vol. 38, no. 3, pp. 1287–1319, 2010. 8",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Small-sample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering",
      "author" : [ "G. Varoquaux", "A. Gramfort", "B. Thirion" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012, pp. 1375–1382.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On model selection consistency of Lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research, vol. 7, pp. 2541–2563, 2006.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Signal recovery from random measurements via orthogonal matching pursuit",
      "author" : [ "J.A. Tropp", "A.C. Gilbert" ],
      "venue" : "IEEE Transactions Information Theory, vol. 53, no. 12, pp. 4655–4666, 2007.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Sharp thresholds for noisy and high-dimensional recovery of sparsity using 1-constrained quadratic programming (Lasso)",
      "author" : [ "M.J. Wainwright" ],
      "venue" : "IEEE Transactions Information Theory, vol. 55, no. 5, May 2009.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Lasso-type recovery of sparse representations for highdimensional data",
      "author" : [ "N. Meinshausen", "B. Yu" ],
      "venue" : "Annals of Statistics, vol. 37, no. 1, pp. 246–270, 2009.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "P.J. Bickel", "Y. Ritov", "A.B. Tsybakov" ],
      "venue" : "Annals of Statistics, vol. 37, no. 4, pp. 1705–1732, 2009.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Statistics for High-Dimensional Data: Methods, Theory and Applications, Springer-Verlag",
      "author" : [ "P. Bühlmann", "S. Van De Geer" ],
      "venue" : "New York Inc,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "H. Zou", "T. Hastie" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Sparse regression with exact clustering",
      "author" : [ "Y. She" ],
      "venue" : "Electronic Journal Statistics, vol. 4, pp. 1055–1096, 2010.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Trace Lasso: A trace norm regularization for correlated designs",
      "author" : [ "E. Grave", "G.R. Obozinski", "F.R. Bach" ],
      "venue" : "Advances in Neural Information Processing Systems 24, J. Shawetaylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, Eds., 2011, pp. 2187–2195.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The sparse laplacian shrinkage estimator for highdimensional regression",
      "author" : [ "J. Huang", "S. Ma", "H. Li", "C. Zhang" ],
      "venue" : "Annals of Statistics, vol. 39, no. 4, pp. 2021, 2011.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2021
    }, {
      "title" : "Correlated variables in regression: clustering and sparse estimation",
      "author" : [ "P. Bühlmann", "P. Rütimann", "S. van de Geer", "C.-H. Zhang" ],
      "venue" : "Journal of Statistical Planning and Inference, vol. 143, pp. 1835–1858, Nov. 2013.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1835
    }, {
      "title" : "Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks",
      "author" : [ "J. Khan", "J.S. Wei", "M. Ringner", "L.H. Saal", "M. Ladanyi", "F. Westermann", "F. Berthold", "M. Schwab", "C.R. Antonescu", "C. Peterson" ],
      "venue" : "Nature medicine, vol. 7, no. 6, pp. 673–679, 2001.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Adaptive forward-backward greedy algorithm for learning sparse representations",
      "author" : [ "T. Zhang" ],
      "venue" : "IEEE Transactions Information Theory, vol. 57, no. 7, pp. 4689–4708, 2011.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples",
      "author" : [ "D. Needell", "J.A. Tropp" ],
      "venue" : "Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301–321, 2009.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Some sharp performance bounds for least squares regression with l1 regularization",
      "author" : [ "T. Zhang" ],
      "venue" : "The Annals of Statistics, vol. 37, no. 5A, pp. 2109–2144, 2009.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "High dimensional variable selection",
      "author" : [ "L. Wasserman", "K. Roeder" ],
      "venue" : "Annals of statistics, vol. 37, no. 5A, pp. 2178, 2009.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Analysis of multi-stage convex relaxation for sparse regularization",
      "author" : [ "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, pp. 1081–1107, Mar. 2010.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The adaptive and the thresholded lasso for potentially misspecified models (and a lower bound for the lasso)",
      "author" : [ "S. van de Geer", "P. Bühlmann", "S. Zhou" ],
      "venue" : "Electronic Journal of Statistics, vol. 5, pp. 688–749, 2011.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Stability selection",
      "author" : [ "N. Meinshausen", "P. Bühlmann" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 72, no. 4, pp. 417–473, 2010.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Swapping variables for high-dimensional sparse regression with correlated measurements",
      "author" : [ "D. Vats", "R.G. Baraniuk" ],
      "venue" : "arXiv:1312.1706, 2013. 9",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Depending on the problem of interest, the unknown sparse vector can encode relationships between genes [1], power line failures in massive power grid networks [2], sparse representations of signals [3, 4], or edges in a graphical model [5,6], to name just a few applications.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "Depending on the problem of interest, the unknown sparse vector can encode relationships between genes [1], power line failures in massive power grid networks [2], sparse representations of signals [3, 4], or edges in a graphical model [5,6], to name just a few applications.",
      "startOffset" : 159,
      "endOffset" : 162
    }, {
      "referenceID" : 2,
      "context" : "Depending on the problem of interest, the unknown sparse vector can encode relationships between genes [1], power line failures in massive power grid networks [2], sparse representations of signals [3, 4], or edges in a graphical model [5,6], to name just a few applications.",
      "startOffset" : 198,
      "endOffset" : 204
    }, {
      "referenceID" : 3,
      "context" : "Depending on the problem of interest, the unknown sparse vector can encode relationships between genes [1], power line failures in massive power grid networks [2], sparse representations of signals [3, 4], or edges in a graphical model [5,6], to name just a few applications.",
      "startOffset" : 198,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "Depending on the problem of interest, the unknown sparse vector can encode relationships between genes [1], power line failures in massive power grid networks [2], sparse representations of signals [3, 4], or edges in a graphical model [5,6], to name just a few applications.",
      "startOffset" : 236,
      "endOffset" : 241
    }, {
      "referenceID" : 5,
      "context" : "Depending on the problem of interest, the unknown sparse vector can encode relationships between genes [1], power line failures in massive power grid networks [2], sparse representations of signals [3, 4], or edges in a graphical model [5,6], to name just a few applications.",
      "startOffset" : 236,
      "endOffset" : 241
    }, {
      "referenceID" : 0,
      "context" : "For example, when the columns in X correspond to gene expression values, it has been observed that genes in the same pathway produce correlated values [1].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "Additionally, it has been observed that regions in the brain that are in close proximity produce correlated signals as measured using an MRI [7].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "The theoretical properties of such algorithms either depend on the irrepresentability condition [5, 8–10] or various forms of the restricted eigenvalue conditions [11,12].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 11,
      "context" : "The theoretical properties of such algorithms either depend on the irrepresentability condition [5, 8–10] or various forms of the restricted eigenvalue conditions [11,12].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "See [13] for a comprehensive review of such algorithms and the related conditions.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 18,
      "context" : "Figure 1 illustrates the performance of SWAP for a matrix X that corresponds to 83 samples of 2308 gene expression values for patients with small round blue cell tumors [19].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "Note that the same is true for the computationally intractable exhaustive search algorithm [10].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "Other competitive algorithms, such as forward-backwards (FoBa) [20] or CoSaMP [21], usually estimate a signal with higher sparsity level and iteratively remove variables until k variables are selected.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "Other competitive algorithms, such as forward-backwards (FoBa) [20] or CoSaMP [21], usually estimate a signal with higher sparsity level and iteratively remove variables until k variables are selected.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "Since k is typically unknown, a suitable value can be selected using standard model selection algorithms such as cross-validation or stability selection [26].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "1 can be found in the extended version of our paper [27].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "As shown in [20], FoBa requires that n = Ω(log(p)/(ρ(3)k+ β 2 min)) for high-dimensional consistent support recovery, where the choice of , which is greater than k, depends on the correlations in the matrix X .",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 26,
      "context" : "2, outlined in [27], relies on imposing conditions on each support of size k such that that there exists a swap so that the loss can be necessarily decreased.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "The entries in each block Σ̄ are specified as follows: Σ̄ii = 1 for i ∈ [10] and Σ̄ij = a for i = j.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "This construction of the design matrix is motivated from [18].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 24,
      "context" : "We use the following sparse recovery algorithms to initialize SWAP: (i) Lasso, (ii) Thresholded Lasso (TLasso) [25], (iii) Forward-Backward (FoBa) [20], (iv) CoSaMP [21], (v) Marginal Regression (MaR), and (vi) Random.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "We use the following sparse recovery algorithms to initialize SWAP: (i) Lasso, (ii) Thresholded Lasso (TLasso) [25], (iii) Forward-Backward (FoBa) [20], (iv) CoSaMP [21], (v) Marginal Regression (MaR), and (vi) Random.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "We use the following sparse recovery algorithms to initialize SWAP: (i) Lasso, (ii) Thresholded Lasso (TLasso) [25], (iii) Forward-Backward (FoBa) [20], (iv) CoSaMP [21], (v) Marginal Regression (MaR), and (vi) Random.",
      "startOffset" : 165,
      "endOffset" : 169
    } ],
    "year" : 2013,
    "abstractText" : "We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. SWAP is surprisingly effective in handling measurement matrices with high correlations. We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.",
    "creator" : null
  }
}