{
  "name" : "8ce6790cc6a94e65f17f908f462fae85.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables",
    "authors" : [ "Jing Xiang", "Seyoung Kim" ],
    "emails" : [ "jingx@cs.cmu.edu", "sssykim@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Bayesian networks have been popular tools for representing the probability distribution over a large number of variables. However, learning a Bayesian network structure from data has been known to be an NP-hard problem [1] because of the constraint that the network structure has to be a directed acyclic graph (DAG). Many of the exact methods that have been developed for recovering the optimal structure are computationally expensive and require exponential computation time [15, 7]. Approximate methods based on heuristic search are more computationally efficient, but they recover a suboptimal structure. In this paper, we address the problem of learning a Bayesian network structure for continuous variables in a high-dimensional space and propose an algorithm that recovers the exact solution with less computation time than the previous exact algorithms, and with the flexibility of further reducing computation time without a significant decrease in accuracy.\nMany of the existing algorithms are based on scoring each candidate graph and finding a graph with the best score, where the score decomposes for each variable given its parents in a DAG. Although methods may differ in the scoring method that they use (e.g., MDL [9], BIC [14], and BDe [4]), most of these algorithms, whether exact methods or heuristic search techniques, have a two-stage learning process. In Stage 1, candidate parent sets for each node are identified while ignoring the DAG constraint. Then, Stage 2 employs various algorithms to search for the best-scoring network structure that satisfies the DAG constraint by limiting the search space to the candidate parent sets from Stage 1. For Stage 1, methods such as sparse candidate [2], max-min parents children [17], and\ntotal conditioning [11] algorithms have been previously proposed. For Stage 2, exact methods based on dynamic programming [7, 15] and A* search algorithm [19] as well as inexact methods such as heuristic search technique [17] and linear programming formulation [6] have been developed. These approaches have been developed primarily for discrete variables, and regardless of whether exact or inexact methods are used in Stage 2, Stage 1 involved exponential computation time and space.\nFor continuous variables, L1-regularized Markov blanket (L1MB) [13] was proposed as a two-stage method that uses lasso to select candidate parents for each variable in Stage 1 and performs heuristic search for DAG structure and variable ordering in Stage 2. Although a two-stage approach can reduce the search space by pruning candidate parent sets in Stage 1, Huang et al. [5] observed that applying lasso in Stage 1 as in L1MB is likely to miss the true parents in a high-dimensional setting, thereby limiting the quality of the solution in Stage 2. They proposed the sparse Bayesian network (SBN) algorithm that formulates the problem of Bayesian network structure learning as a singlestage optimization problem and transforms it into a lasso-type optimization to obtain an approximate solution. Then, they applied a heuristic search to refine the solution as a post-processing step.\nIn this paper, we propose a new algorithm, called A* lasso, for learning a sparse Bayesian network structure with continuous variables in high-dimensional space. Our method is a single-stage algorithm that finds the optimal network structure with a sparse set of parents while ensuring the DAG constraint is satisfied. We first show that a lasso-based scoring method can be incorporated within dynamic programming (DP). While previous approaches based on DP required identifying the exponential number of candidate parent sets and their scores for each variable in Stage 1 before applying DP in Stage 2 [7, 15], our approach effectively combines the score computation in Stage 1 within Stage 2 via lasso optimization. Then, we present A* lasso which significantly prunes the search space of DP by incorporating the A* search algorithm [12], while guaranteeing the optimality of the solution. Since in practice, A* search can still be expensive compared to heuristic methods, we explore heuristic schemes that further limit the search space of A* lasso. We demonstrate in our experiments that this heuristic approach can substantially improve the computation time without significantly compromising the quality of the solution, especially on large Bayesian networks."
    }, {
      "heading" : "2 Background on Bayesian Network Structure Learning",
      "text" : "A Bayesian network is a probabilistic graphical model defined over a DAG G with a set of p = |V | nodes V = {v1, . . . , vp}, where each node vj is associated with a random variable Xj [8]. The probability model associated with G in a Bayesian network factorizes as p(X1, . . . , Xp) =∏p\nj=1 p(Xj |Pa(Xj)), where p(Xj |Pa(Xj)) is the conditional probability distribution for Xj given its parents Pa(Xj) with directed edges from each node in Pa(Xj) toXj inG. We assume continuous random variables and use a linear regression model for the conditional probability distribution of each node Xj = Pa(Xj)′βj + , where βj = {βjk’s for Xk ∈ Pa(Xj)} is the vector of unknown parameters to be estimated from data and is the noise distributed as N(0, 1).\nGiven a dataset X = [x1, . . . ,xp], where xj is a vector of n observations for random variable Xj , our goal is to estimate the graph structure G and the parameters βj’s jointly. We formulate this problem as that of obtaining a sparse estimate of βj’s, under the constraint that the overall graph structure G should not contain directed cycles. Then, the nonzero elements of βj’s indicate the presence of edges in G. We obtain an estimate of Bayesian network structure and parameters by minimizing the negative log likelihood of data with sparsity enforcing L1 penalty as follows:\nmin β1,...,βp p∑ j=1 ‖ xj − x−j ′βj ‖22 +λ p∑ j=1 ‖ βj ‖1 s.t. G ∈ DAG, (1)\nwhere x−j represents all columns of X excluding xj , assuming all other variables are candidate parents of node vj . Given the estimate of βj’s, the set of parents for node vj can be found as the support of βj , S(βj) = {vi|βji 6= 0}. The λ is the regularization parameter that determines the amount of sparsity in βj’s and can be determined by cross-validation. We notice that if the acyclicity constraint is ignored, Equation (1) decomposes into individual lasso estimations for each node:\nLassoScore(vj |V \\vj) = min βj ‖ xj − x−j ′βj ‖22 +λ ‖ βj ‖1,\nwhere V \\vj represents the set of all nodes in V excluding vj . The above lasso optimization problem can be solved efficiently with the shooting algorithm [3]. However, the main challenge in optimizing Equation (1) arises from ensuring that the βj’s satisfy the DAG constraint."
    }, {
      "heading" : "3 A* Lasso for Bayesian Network Structure Learning",
      "text" : "3.1 Dynamic Programming with Lasso\nThe problem of learning a Bayesian network structure that satisfies the constraint of no directed cycles can be cast as that of learning an optimal ordering of variables [8]. Once the optimal variable ordering is given, the constraint of no directed cycles can be trivially enforced by constraining the parents of each variable in the local conditional probability distribution to be a subset of the nodes that precede the given node in the ordering. We let ΠV = [πV1 , . . . , π V |V |] denote an ordering of the nodes in V , where πVj indicates the node v ∈ V in the jth position of the ordering, and ΠV≺vj denote the set of nodes in V that precede node vj in ordering ΠV .\nAlgorithms based on DP have been developed to learn the optimal variable ordering for Bayesian networks [16]. These approaches are based on the observation that the score of the optimal ordering of the\nfull set of nodes V can be decomposed into (a) the optimal score for the first node in the ordering, given a choice of the first node and (b) the score of the optimal ordering of the nodes excluding the first node. The optimal variable ordering can be constructed by recursively applying this decomposition to select the first node in the ordering and to find the optimal ordering of the set of remaining nodes U ⊂ V . This recursion is given as follows, with an initial call of the recursion with U = V :\nOptScore(U) = min vj∈U OptScore(U\\vj) + BestScore(vj |V \\U) (2)\nπU1 = argmin vj∈U OptScore(U\\vj) + BestScore(vj |V \\U), (3)\nwhere BestScore(vj |V \\U) is the optimal score of vj under the optimal choice of parents from V \\U . In order to obtain BestScore(vj |V \\U) in Equations (2) and (3), for the case of discrete variables, many previous approaches enumerated all possible subsets of V as candidate sets of parents for node vj to precompute BestScore(vj |V \\U) in Stage 1 before applying DP in Stage 2 [7, 15]. While this approach may perform well in a low-dimensional setting, in a high-dimensional setting, a two-stage method is likely to miss the true parent sets in Stage 1, which in turn affects the performance of Stage 2 [5]. In this paper, we consider the high-dimensional setting and present a single-stage method that applies lasso to obtain BestScore(vj |V \\U) within DP as follows:\nBestScore(vj |V \\U) = LassoScore(vj |V \\U) = min\nβj ,S(βj)⊆V \\U ‖ xj − x−j ′βj ‖22 +λ ‖ βj ‖1 .\nThe constraint S(βj) ⊆ V \\U in the above lasso optimization can be trivially maintained by setting the βjk for vk ∈ U to 0 and optimizing only for the other βjk’s. When applying the recursion in Equations (2) and (3), DP takes advantage of the overlapping subproblems to prune the search space of orderings, since the problem of computing OptScore(U ) for U ⊆ V can appear as a subproblem of scoring orderings of any larger subsets of V that contain U .\nThe problem of finding the optimal variable ordering can be viewed as that of finding the shortest path from the start state to the goal state in a search space given as a subset lattice. The search space consists of a set of states, each of which is associated with one of the 2|V | possible subsets of nodes in V . The start state is the empty set {} and the goal state is the set of all variables V . A valid move in this search space is defined from a state for subset Qs to another state for subset Qs′ , only if Qs′ contains one additional node to Qs. Each move to the next state corresponds to adding a node at the end of the ordering of the nodes in the previous state. The cost of such a move is given by BestScore(v|Qs), where v = Qs′\\Qs. Each path from the start state to the goal state gives one\npossible ordering of nodes. Figure 1 illustrates the search space, where each state is associated with a Qs. DP finds the shortest path from the start state to the goal state that corresponds to the optimal variable ordering by considering all possible paths in this search space and visiting all 2|V | states."
    }, {
      "heading" : "3.2 A* Lasso for Pruning Search Space",
      "text" : "As discussed in the previous section, DP considers all 2|V | states in the subset lattice to find the optimal variable ordering. Thus, it is not sufficiently efficient to be practical for problems with more than 20 nodes. On the other hand, a greedy algorithm is computationally efficient because it explores a single variable ordering by greedily selecting the most promising next state based on BestScore(v|Qs), but it returns a suboptimal solution. In this paper, we propose A* lasso that incorporates the A* search algorithm [12] to construct the optimal variable ordering in the search space of the subset lattice. We show that this strategy can significantly prune the search space compared to DP, while maintaining the optimality of the solution.\nWhen selecting the next move in the process of constructing a path in the search space, instead of greedily selecting the move, A* search also accounts for the estimate of the future cost given by a heuristic function h(Qs) that will be incurred to reach the goal state from the candidate next state. Although the exact future cost is not known until A* search constructs the full path by reaching the goal state, a reasonable estimate of the future cost can be obtained by ignoring the directed acyclicity constraint. It is well-known that A* search is guaranteed to find the shortest path if the heuristic function h(Qs) is admissible [12], meaning that h(Qs) is always an underestimate of the true cost of reaching the goal state. Below, we describe an admissible heuristic for A* lasso.\nWhile exploring the search space, A* search algorithm assigns a score f(Qs) to each state and its corresponding subset Qs of variables for which the ordering has been determined. A* search algorithm computes this score f(Qs) as the sum of the cost g(Qs) that has been incurred so far to reach the current state from the start state and an estimate of the cost h(Qs) that will be incurred to reach the goal state from the current state:\nf(Qs) = g(Qs) + h(Qs). (4) More specifically, given the ordering ΠQs of variables in Qs that has been constructed along the path from the start state to the state for Qs, the cost that has been incurred so far is defined as\ng(Qs) = ∑\nvj∈Qs\nLassoScore(vj |ΠQs≺vj ) (5)\nand the heuristic function for the estimate of the future cost to reach the goal state is defined as: h(Qs) = ∑\nvj∈V \\Qs\nLassoScore(vj |V \\vj) (6)\nNote that the heuristic function is admissible, or an underestimate of the true cost, since the constraint of no directed cycles is ignored and each variable in V \\Qs is free to choose any variables in V as its parents, which lowers the lasso objective value.\nWhen the search space is a graph where multiple paths can reach the same state, we can further improve efficiency if the heuristic function has the property of consistency in addition to admissibility. A consistent heuristic always satisfies h(Qs) ≤ h(Qs′) + LassoScore(vk|Qs), where LassoScore(vk|Qs) is the cost of moving from state Qs to state Qs′ with {vk} = Qs′\\Qs. Consistency ensures that the first path found by A* search to reach the given state is always the shortest path to that state [12]. This allows us to prune the search when we reach the same state via a different path later in the search. The following proposition states that our heuristic function is consistent.\nProposition 1 The heuristic in Equation (6) is consistent.\nProof For any successor state Qs′ of Qs, let vk = Qs′\\Qs. h(Qs) = ∑ vj∈V \\Qs LassoScore(vj |V \\vj)\n= ∑\nvj∈V \\Qs,vj 6=vk\nLassoScore(vj |V \\vj) + LassoScore(vk|V \\vk)\n≤ h(Qs′) + LassoScore(vk|Qs),\nInput : X, V , λ Output: Optimal variable ordering ΠV Initialize OPEN to an empty queue; Initialize CLOSED to an empty set; Compute LassoScore(vj |V \\vj) for all vj ∈ V ; OPEN.insert((Qs = {}, f(Qs) = h({}), g(Qs) = 0,ΠQs = [ ])); while true do\n(Qs, f(Qs), g(Qs),Π Qs)← OPEN.pop(); if h(Qs) = 0 then Return ΠV ← ΠQs ; end foreach v ∈ V \\Qs do\nQs′ ← Qs ∪ {v}; if Qs′ /∈ CLOSED then\nCompute LassoScore(v|Qs) with lasso shooting algorithm; g(Qs′)← g(Qs) + LassoScore(v|Qs); h(Qs′)← h(Qs)− LassoScore(v|V \\v); f(Qs′)← g(Qs′) + h(Qs′); ΠQs′ ← [ΠQs , v]; OPEN.insert(L = (Qs′ , f(Qs′), g(Qs′),ΠQs′ )); CLOSED← CLOSED ∪{Qs′};\nend end\nend Algorithm 1: A* lasso for learning Bayesian network structure\nwhere LassoScore(vk|Qs) is the true cost of moving from state Qs to Qs′ . The inequality above holds because vk has fewer parents to choose from in LassoScore(vk|Qs) than in LassoScore(vk|V \\vk). Thus, our heuristic in Equation (6) is consistent. Given a consistent heuristic, many paths that go through the same state can be pruned by maintaining an OPEN list and a CLOSED list during A* search. In practice, the OPEN list can be implemented with a priority queue and the CLOSED list can be implemented with a hash table. The OPEN list is a priority queue that maintains all the intermediate results (Qs, f(Qs), g(Qs),ΠQs)’s for a partial construction of the variable ordering up to Qs at the frontier of the search, sorted according to the score f(Qs). During search, A* lasso pops from the OPEN list the partial construction of ordering with the lowest score f(Qs), visits the successor states by adding another node to the ordering ΠQs , and queues the results onto the OPEN list. Any state that has been popped by A* lasso is placed in the CLOSED list. The states that have been placed in the CLOSED list are not considered again, even if A* search reaches these states through different paths later in the search.\nThe full algorithm for A* lasso is given in Algorithm 1. As in DP with lasso, A* lasso is a singlestage algorithm that solves lasso within A* search. Every time A* lasso moves from state Qs to the next state Qs′ in the search space, LassoScore(vj |ΠQs≺vj ) for {vj} = Qs′\\Qs is computed with the shooting algorithm and added to g(Qs) to obtain g(Qs′). The heuristic score h(Qs′) can be precomputed as LassoScore(vj |V \\vj) for all vj ∈ V for a simple look-up during A* search."
    }, {
      "heading" : "3.3 Heuristic Schemes for A* Lasso to Improve Scalability",
      "text" : "Although A* lasso substantially prunes the search space compared to DP, it is not sufficiently efficient for large graphs, because it still considers a large number of states in the exponentially large search space. One simple strategy for further pruning the search space would be to limit the size of the priority queue in the OPEN list, forcing A* lasso to discard less promising intermediate results first. In this case, limiting the queue size to one is equivalent to a greedy algorithm with a scoring function in Equation (4). In our experiments, we found that such a naive strategy substantially reduced the quality of solutions because the best-scoring intermediate results tend to be the results at the early stage of the exploration. They are at the shallow part of the search space near the start state because the admissible heuristic underestimates the true cost.\nInstead, given a limited queue size, we propose to distribute the intermediate results to be discarded across different depths/layers of the search space. For example, given the depth of the search space\n|V |, if we need to discard k intermediate results, we discard k/|V | intermediate results at each depth. In our experiments, we found that this heuristic scheme substantially improves the computation time of A* lasso with a small reduction in the quality of the solution. We also considered other strategies such as inflating heuristics [10] and pruning edges in preprocessing with lasso, but such strategies substantially reduced the quality of solutions."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Simulation Study",
      "text" : "We perform simulation studies in order to evaluate the accuracy of the estimated structures and measure the computation time of our method. We created several small networks under 20 nodes and obtained the structure of several benchmark networks between 27 and 56 nodes from the Bayesian Network Repository (the left-most column in Table 1). In addition, we used the tiling technique [18] to generate two networks of approximately 300 nodes so that we could evaluate our method on larger graphs. Given the Bayesian network structures, we set the parameters βj for each conditional probability distribution of node vj such that βjk ∼ ±Uniform[l, u] for predetermined values for u and l if node vk is a parent of node vj and βjk = 0 otherwise. We then generated data from each Bayesian network by forward sampling with noise ∼ N(0, 1) in the regression model, given the true variable ordering. All data were mean-centered.\nWe compare our method to several other methods including DP with lasso for an exact method, L1MB for heuristic search, and SBN for an optimization-based approximate method. We downloaded the software implementations of L1MB and SBN from the authors’ website. For L1MB, we increased the authors’ recommended number of evaluations 2500 to 10 000 in Stage 2 heuristic search for all networks except the two larger networks of around 300 nodes (Alarm 2 and Hailfinder 2), where we used two different settings of 50 000 and 100 000 evaluations. We also evaluated A* lasso with the heuristic scheme with the queue sizes of 5, 100, 200, and 1000.\nDP, A* lasso, and A* lasso with a limited queue size require a selection of the regularization parameter λ with cross-validation. In order to determine the optimal value for λ, for different values of λ, we trained a model on a training set, performed an ordinary least squares re-estimation of the non-zero elements of βj to remove the bias introduced by the L1 penalty, and computed prediction errors on the validation set. Then, we selected the value of λ that gives the smallest prediction error as the optimal λ. We used a training set of 200 samples for relatively small networks with under\n60 nodes and a training set of 500 samples for the two large networks with around 300 nodes. We used a validation set of 500 samples. For L1MB and SBN, we used a similar strategy to select the regularization parameters, while mainly following the strategy suggested by the authors and in their software implementation.\nWe present the computation time for the different methods in Table 1. For DP, A* lasso, and A* lasso with limited queue sizes, we also record the number of states visited in the search space in parentheses in Table 1. All methods were implemented in Matlab and were run on computers with 2.4 GHz processors. We used a dataset generated from a true model with βjk ∼ ±Uniform[1.2, 1.5]. It can be seen from Table 1 that DP considers all possible states 2|V | in the search space that grows exponentially with the number of nodes. It is clear that A* lasso visits significantly fewer states than DP, visiting about 10% of the number of states in DP for the funnel and galaxy networks. We were unable to obtain the computation time for A* lasso and DP for some of the larger graphs in Table 1 as they required significantly more time. Limiting the size of the queue in A* lasso reduces both the computation time and the number of states visited. For smaller graphs, we do not report the computation time for A* lasso with limited queue size, since it is identical to the full A* lasso. We notice that the computation time for A* lasso with a small queue of 5 or 100 is comparable to that of L1MB and SBN.\nIn general, we found that the extent of pruning of the search space by A* lasso compared to DP depends on the strengths of edges (βj values) in the true model. We applied DP and A* lasso to datasets of 200 samples generated from each of the networks under each of the three settings for the true edge strengths, ±Uniform[1.2, 1.5], ±Uniform[1, 1.2], and ±Uniform[0.8, 1]. As can be seen from the computation time and the number of states visited by DP and A* lasso in Table 2, as the strengths of edges increase, the number of states visited by A* lasso and the computation time tend to decrease. The results in Table 2 indicate that the efficiency of A* lasso is affected by the signal-to-noise ratio.\nIn order to evaluate the accuracy of the Bayesian network structures recovered by each method, we make use of the fact that two Bayesian network structures are indistinguishable if they belong to the same equivalence class, where an equivalence class is defined as the set of networks with the same skeleton and v-structures. The skeleton of a Bayesian network is defined as the edge connectivities ignoring edge directions and a v-structure is defined as the local graph structure over three variables, with two variables pointing to the other variables (i.e., A → B ← C). We evaluated the performance of the different methods by comparing the estimated network structure with the true network structure in terms of skeleton and v-structures and computing the precision and recall.\nThe precision/recall curves for the skeleton and v-structures of the models estimated by the different methods are shown in Figures 2 and 3, respectively. Each curve was obtained as an average over the results from 30 different datasets for the two large graphs (Alarm 2 and Hailfinder 2) and from 50 different datasets for all the other Bayesian networks. All data were simulated under the setting βjk ∼ ±Uniform[0.4, 0.7]. For the benchmark Bayesian\nnetworks, we used A* lasso with different queue sizes, including 100, 200, and 1000, whereas for the two large networks (Alarm 2 and Hailfinder 2) that require more computation time, we used A* lasso with queue size of 5 and 100. As can be seen in Figures 2 and 3, all methods perform relatively well on identifying the true skeletons, but find it significantly more challenging to recover the true v-structures. We find that although increasing the size of queues in A* lasso generally improves the performance, even with smaller queue sizes, A* lasso outperforms L1MB and SBN in most of the networks. While A* lasso with a limited queue size preforms consistently well on smaller networks, it significantly outperforms the other methods on the larger graphs such as Alarm 2 and Hailfinder 2, even with a queue size of 5 and even when the number of evaluations for L1MB has been increased to 50 000 and 100 000. This demonstrates that while limiting the queue size in A* lasso will not guarantee the optimality of the solution, it still reduces the computation time of A* lasso dramatically without substantially compromising the quality of the solution. In addition, we compare the performance of the different methods in terms of prediction errors on independent test datasets in Figure 4. We find that the prediction errors of A* lasso are consistently lower even with a limited queue size.\n4.2 Analysis of S&P Stock Data\nWe applied the methods on the daily stock price data of the S&P 500 companies to learn a Bayesian network that models the dependencies in prices among different stocks. We obtained the stock prices of 125 companies over 1500 time points between Jan 3, 2007 and Dec 17, 2012. We estimated a Bayesian network using the first 1000 time points with the different methods, and then computed prediction errors on the last 500 time points. For L1MB, we used two settings for the number of evaluations, 50 000 and 100 000. We applied A* lasso with different queue limits of 5, 100, and 200. The prediction accuracies for the various methods are shown in Figure 5. Our method obtains lower prediction errors than the other methods, even with the smaller queue sizes."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we considered the problem of learning a Bayesian network structure and proposed A* lasso that guarantees the optimality of the solution while reducing the computational time of the well-known exact methods based on DP. We proposed a simple heuristic scheme that further improves the computation time but does not significantly reduce the quality of the solution."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This material is based upon work supported by an NSF CAREER Award No. MCB-1149885, Sloan Research Fellowship, and Okawa Foundation Research Grant to SK and by a NSERC PGS-D to JX."
    } ],
    "references" : [ {
      "title" : "Learning Bayesian networks is NP-complete",
      "author" : [ "David Maxwell Chickering" ],
      "venue" : "In Learning from data,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Learning Bayesian network structure from massive datasets: the “Sparse Candidate",
      "author" : [ "Nir Friedman", "Iftach Nachman", "Dana Peér" ],
      "venue" : "Proceedings of the Fifteenth conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1999
    }, {
      "title" : "Penalized regressions: the bridge versus the lasso",
      "author" : [ "Wenjiang J Fu" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1998
    }, {
      "title" : "Learning Bayesian networks: The combination of knowledge and statistical data",
      "author" : [ "David Heckerman", "Dan Geiger", "David M Chickering" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "A sparse structure learning algorithm for Gaussian Bayesian network identification from high-dimensional data",
      "author" : [ "Shuai Huang", "Jing Li", "Jieping Ye", "Adam Fleisher", "Kewei Chen", "Teresa Wu", "Eric Reiman" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Learning Bayesian network structure using LP relaxations",
      "author" : [ "Tommi Jaakkola", "David Sontag", "Amir Globerson", "Marina Meila" ],
      "venue" : "In Proceedings of the Thirteenth International Conference on Artificial intelligence and Statistics (AISTATS),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Exact Bayesian structure discovery in Bayesian networks",
      "author" : [ "Mikko Koivisto", "Kismat Sood" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "Probabilistic graphical models: principles and techniques",
      "author" : [ "Daphne Koller", "Nir Friedman" ],
      "venue" : "MIT press,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Learning Bayesian belief networks: An approach based on the MDL principle",
      "author" : [ "Wai Lam", "Fahiem Bacchus" ],
      "venue" : "Computational intelligence,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1994
    }, {
      "title" : "ARA*: Anytime A* with provable bounds on sub-optimality",
      "author" : [ "Maxim Likhachev", "Geoff Gordon", "Sebastian Thrun" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Using Markov blankets for causal structure learning",
      "author" : [ "Jean-Philippe Pellet", "André Elisseeff" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Artificial intelligence: a modern approach, volume 74",
      "author" : [ "Stuart Jonathan Russell", "Peter Norvig", "John F Canny", "Jitendra M Malik", "Douglas D Edwards" ],
      "venue" : "Prentice hall Englewood Cliffs,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1995
    }, {
      "title" : "Learning graphical model structure using L1-regularization paths",
      "author" : [ "Mark Schmidt", "Alexandru Niculescu-Mizil", "Kevin Murphy" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Estimating the dimension of a model",
      "author" : [ "Gideon Schwarz" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1978
    }, {
      "title" : "Finding optimal Bayesian networks by dynamic programming",
      "author" : [ "Ajit Singh", "Andrew Moore" ],
      "venue" : "Technical Report 05-106,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2005
    }, {
      "title" : "Ordering-based search: A simple and effective algorithm for learning Bayesian networks",
      "author" : [ "Marc Teyssier", "Daphne Koller" ],
      "venue" : "In Proceedings of the Twentieth conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2005
    }, {
      "title" : "The max-min hill-climbing Bayesian network structure learning algorithm",
      "author" : [ "Ioannis Tsamardinos", "Laura E Brown", "Constantin F Aliferis" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "Generating realistic large Bayesian networks by tiling",
      "author" : [ "Ioannis Tsamardinos", "Alexander Statnikov", "Laura E Brown", "Constantin F Aliferis" ],
      "venue" : "In the Nineteenth International FLAIRS conference,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Learning optimal Bayesian networks using A* search",
      "author" : [ "Changhe Yuan", "Brandon Malone", "Xiaojian Wu" ],
      "venue" : "In Proceedings of the Twenty-Second international joint conference on Artificial Intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "However, learning a Bayesian network structure from data has been known to be an NP-hard problem [1] because of the constraint that the network structure has to be a directed acyclic graph (DAG).",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "Many of the exact methods that have been developed for recovering the optimal structure are computationally expensive and require exponential computation time [15, 7].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 6,
      "context" : "Many of the exact methods that have been developed for recovering the optimal structure are computationally expensive and require exponential computation time [15, 7].",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 8,
      "context" : ", MDL [9], BIC [14], and BDe [4]), most of these algorithms, whether exact methods or heuristic search techniques, have a two-stage learning process.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : ", MDL [9], BIC [14], and BDe [4]), most of these algorithms, whether exact methods or heuristic search techniques, have a two-stage learning process.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : ", MDL [9], BIC [14], and BDe [4]), most of these algorithms, whether exact methods or heuristic search techniques, have a two-stage learning process.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 1,
      "context" : "For Stage 1, methods such as sparse candidate [2], max-min parents children [17], and",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "For Stage 1, methods such as sparse candidate [2], max-min parents children [17], and",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 10,
      "context" : "total conditioning [11] algorithms have been previously proposed.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "For Stage 2, exact methods based on dynamic programming [7, 15] and A* search algorithm [19] as well as inexact methods such as heuristic search technique [17] and linear programming formulation [6] have been developed.",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "For Stage 2, exact methods based on dynamic programming [7, 15] and A* search algorithm [19] as well as inexact methods such as heuristic search technique [17] and linear programming formulation [6] have been developed.",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "For Stage 2, exact methods based on dynamic programming [7, 15] and A* search algorithm [19] as well as inexact methods such as heuristic search technique [17] and linear programming formulation [6] have been developed.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "For Stage 2, exact methods based on dynamic programming [7, 15] and A* search algorithm [19] as well as inexact methods such as heuristic search technique [17] and linear programming formulation [6] have been developed.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "For Stage 2, exact methods based on dynamic programming [7, 15] and A* search algorithm [19] as well as inexact methods such as heuristic search technique [17] and linear programming formulation [6] have been developed.",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "For continuous variables, L1-regularized Markov blanket (L1MB) [13] was proposed as a two-stage method that uses lasso to select candidate parents for each variable in Stage 1 and performs heuristic search for DAG structure and variable ordering in Stage 2.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "[5] observed that applying lasso in Stage 1 as in L1MB is likely to miss the true parents in a high-dimensional setting, thereby limiting the quality of the solution in Stage 2.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "While previous approaches based on DP required identifying the exponential number of candidate parent sets and their scores for each variable in Stage 1 before applying DP in Stage 2 [7, 15], our approach effectively combines the score computation in Stage 1 within Stage 2 via lasso optimization.",
      "startOffset" : 183,
      "endOffset" : 190
    }, {
      "referenceID" : 14,
      "context" : "While previous approaches based on DP required identifying the exponential number of candidate parent sets and their scores for each variable in Stage 1 before applying DP in Stage 2 [7, 15], our approach effectively combines the score computation in Stage 1 within Stage 2 via lasso optimization.",
      "startOffset" : 183,
      "endOffset" : 190
    }, {
      "referenceID" : 11,
      "context" : "Then, we present A* lasso which significantly prunes the search space of DP by incorporating the A* search algorithm [12], while guaranteeing the optimality of the solution.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : ", vp}, where each node vj is associated with a random variable Xj [8].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "The above lasso optimization problem can be solved efficiently with the shooting algorithm [3].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "The problem of learning a Bayesian network structure that satisfies the constraint of no directed cycles can be cast as that of learning an optimal ordering of variables [8].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 15,
      "context" : "Algorithms based on DP have been developed to learn the optimal variable ordering for Bayesian networks [16].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "In order to obtain BestScore(vj |V \\U) in Equations (2) and (3), for the case of discrete variables, many previous approaches enumerated all possible subsets of V as candidate sets of parents for node vj to precompute BestScore(vj |V \\U) in Stage 1 before applying DP in Stage 2 [7, 15].",
      "startOffset" : 279,
      "endOffset" : 286
    }, {
      "referenceID" : 14,
      "context" : "In order to obtain BestScore(vj |V \\U) in Equations (2) and (3), for the case of discrete variables, many previous approaches enumerated all possible subsets of V as candidate sets of parents for node vj to precompute BestScore(vj |V \\U) in Stage 1 before applying DP in Stage 2 [7, 15].",
      "startOffset" : 279,
      "endOffset" : 286
    }, {
      "referenceID" : 4,
      "context" : "While this approach may perform well in a low-dimensional setting, in a high-dimensional setting, a two-stage method is likely to miss the true parent sets in Stage 1, which in turn affects the performance of Stage 2 [5].",
      "startOffset" : 217,
      "endOffset" : 220
    }, {
      "referenceID" : 11,
      "context" : "In this paper, we propose A* lasso that incorporates the A* search algorithm [12] to construct the optimal variable ordering in the search space of the subset lattice.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "It is well-known that A* search is guaranteed to find the shortest path if the heuristic function h(Qs) is admissible [12], meaning that h(Qs) is always an underestimate of the true cost of reaching the goal state.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "Consistency ensures that the first path found by A* search to reach the given state is always the shortest path to that state [12].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "We also considered other strategies such as inflating heuristics [10] and pruning edges in preprocessing with lasso, but such strategies substantially reduced the quality of solutions.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "In addition, we used the tiling technique [18] to generate two networks of approximately 300 nodes so that we could evaluate our method on larger graphs.",
      "startOffset" : 42,
      "endOffset" : 46
    } ],
    "year" : 2013,
    "abstractText" : "We address the problem of learning a sparse Bayesian network structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the first stage and then searches for a network structure satisfying the DAG constraint in the second stage. Although this approach is effective in a lowdimensional setting, it is difficult to ensure that the correct network structure is not pruned in the first stage in a high-dimensional setting. In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efficiency of the well-known exact methods based on dynamic programming. We also present a heuristic scheme that further improves the efficiency of A* lasso without significantly compromising the quality of solutions. We demonstrate our approach on data simulated from benchmark Bayesian networks and real data.",
    "creator" : null
  }
}