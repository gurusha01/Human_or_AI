{
  "name" : "71f6278d140af599e06ad9bf1ba03cb0.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Understanding Dropout",
    "authors" : [ "Pierre Baldi", "Peter Sadowski" ],
    "emails" : [ "pfbaldi@uci.edu", "pjsadows@ics.uci.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Dropout is an algorithm for training neural networks that was described at NIPS 2012 [7]. In its most simple form, during training, at each example presentation, feature detectors are deleted with probability q = 1− p = 0.5 and the remaining weights are trained by backpropagation. All weights are shared across all example presentations. During prediction, the weights are divided by two. The main motivation behind the algorithm is to prevent the co-adaptation of feature detectors, or overfitting, by forcing neurons to be robust and rely on population behavior, rather than on the activity of other specific units. In [7], dropout is reported to achieve state-of-the-art performance on several benchmark datasets. It is also noted that for a single logistic unit dropout performs a kind of “geometric averaging” over the ensemble of possible subnetworks, and conjectured that something similar may occur also in multilayer networks leading to the view that dropout may be an economical approximation to training and using a very large ensemble of networks.\nIn spite of the impressive results that have been reported, little is known about dropout from a theoretical standpoint, in particular about its averaging, regularization, and convergence properties. Likewise little is known about the importance of using q = 0.5, whether different values of q can be used including different values for different layers or different units, and whether dropout can be applied to the connections rather than the units. Here we address these questions."
    }, {
      "heading" : "2 Dropout in Linear Networks",
      "text" : "It is instructive to first look at some of the properties of dropout in linear networks, since these can be studied exactly in the most general setting of a multilayer feedforward network described by an underlying acyclic graph. The activity in unit i of layer h can be expressed as:\nShi (I) = ∑ l<h ∑ j whlij S l j with S 0 j = Ij (1)\nwhere the variables w denote the weights and I the input vector. Dropout applied to the units can be expressed in the form\nShi = ∑ l<h ∑ j whlij δ l jS l j with S 0 j = Ij (2)\nwhere δlj is a gating 0-1 Bernoulli variable, with P (δ l j = 1) = p l j . Throughout this paper we assume that the variables δlj are independent of each other, independent of the weights, and independent of the activity of the units. Similarly, dropout applied to the connections leads to the random variables\nShi = ∑ l<h ∑ j δhlijw hl ij S l j with S 0 j = Ij (3)\nFor brevity in the rest of this paper, we focus exclusively on dropout applied to the units, but all the results remain true for the case of dropout applied to the connections with minor adjustments.\nFor a fixed input vector, the expectation of the activity of all the units, taken over all possible realizations of the gating variables hence all possible subnetworks, is given by:\nE(Shi ) = ∑ l<h ∑ j whlij p l jE(S l j) for h > 0 (4)\nwith E(S0j ) = Ij in the input layer. In short, the ensemble average can easily be computed by feedforward propagation in the original network, simply replacing the weights whlij by w hl ij p l j ."
    }, {
      "heading" : "3 Dropout in Neural Networks",
      "text" : ""
    }, {
      "heading" : "3.1 Dropout in Shallow Neural Networks",
      "text" : "Consider first a single logistic unit with n inputs O = σ(S) = 1/(1 + ce−λS) and S = ∑n\n1 wjIj . To achieve the greatest level of generality, we assume that the unit produces different outputs O1, . . . , Om, corresponding to different sums S1 . . . , Sm with different probabilities P1, . . . , Pm ( ∑ Pm = 1). In the most relevant case, these outputs and these sums are associated with the m = 2n possible subnetworks of the unit. The probabilities P1, . . . , Pm could be generated, for instance, by using Bernoulli gating variables, although this is not necessary for this derivation. It is useful to define the following four quantities: the mean E = ∑ PiOi; the mean of the complements\nE′ = ∑ Pi(1 − Oi) = 1 − E; the weighted geometric mean (WGM ) G = ∏ iO Pi i ; and the\nweighted geometric mean of the complements G′ = ∏ i(1− Oi)Pi . We also define the normalized weighted geometric mean NWGM = G/(G+G′). We can now prove the key averaging theorem for logistic functions:\nNWGM(O1, . . . , Om) = 1\n1 + ce−λE(S) = σ(E(S)) (5)\nTo prove this result, we write\nNWGM(O1, . . . , Om) = 1\n1 + ∏ (1−Oi)Pi∏ O\nPi i\n= 1\n1 + ∏ (1−σ(Si))Pi∏ σ(Si)Pi\n(6)\nThe logistic function satisfies the identity [1− σ(x)]/σ(x) = ce−λx and thus\nNWGM(O1, . . . , Om) = 1 1 + ∏ [ce−λSi ]Pi = 1 1 + ce−λ ∑ PiSi = σ(E(S)) (7)\nThus in the case of Bernoulli gating variables, we can compute the NWGM over all possible dropout configurations by simple forward propagation by: NWGM = σ( ∑n 1 wjpjIj). A similar result is true also for normalized exponential transfer functions. Finally, one can also show that the only class of functions f that satisfy NWGM(f) = f(E) are the constant functions and the logistic functions [1]."
    }, {
      "heading" : "3.2 Dropout in Deep Neural Networks",
      "text" : "We can now deal with the most interesting case of deep feedforward networks of sigmoidal units 1, described by a set of equations of the form\nOhi = σ(S h i ) = σ( ∑ l<h ∑ j whlijO l j) with O 0 j = Ij (8)\nwhere Ohi is the output of unit i in layer h. Dropout on the units can be described by\nOhi = σ(S h i ) = σ( ∑ l<h ∑ j whlij δ l jO l j) with O 0 j = Ij (9)\nusing the Bernoulli selector variables δlj . For each sigmoidal unit\nNWGM(Ohi ) =\n∏ N (O h i ) P (N )∏\nN (O h i ) P (N ) + ∏ N (1−Ohi )P (N )\n(10)\nwhere N ranges over all possible subnetworks. Assume for now that the NWGM provides a good approximation to the expectation (this point will be analyzed in the next section). Then the averaging properties of dropout are described by the following three recursive equations. First the approximation of means by NWGMs:\nE(Ohi ) ≈ NWGM(Ohi ) (11)\nSecond, using the result of the previous section, the propagation of expectation symbols:\nNWGM(Ohi ) = σ h i [ E(Shi ) ] (12)\nAnd third, using the linearity of the expectation with respect to sums, and to products of independent random variables:\nE(Shi ) = ∑ l<h ∑ j whlij p l jE(O l j) (13)\nEquations 11, 12, and 13 are the fundamental equations explaining the averaging properties of the dropout procedure. The only approximation is of course Equation 11 which is analyzed in the next section. If the network contains linear units, then Equation 11 is not necessary for those units and their average can be computed exactly. In the case of regression with linear units in the top layers, this allows one to shave off one layer of approximations. The same is true in binary classification by requiring the output layer to compute directly the NWGM of the ensemble rather than the expectation. It can be shown that for any error function that is convex up (∪), the error of the mean, weighted geometric mean, and normalized weighted geometric mean of an ensemble is always less than the expected error of the models [1].\nEquation 11 is exact if and only if the numbers Ohi are identical over all possible subnetworks N . Thus it is useful to measure the consistency C(Ohi , I) of neuron i in layer h for input I by using the variance V ar [ Ohi (I) ] taken over all subnetworks N and their distribution when the input I is fixed. The larger the variance is, the less consistent the neuron is, and the worse we can expect the approximation in Equation 11 to be. Note that for a random variable O in [0,1] the variance cannot exceed 1/4 anyway. This is because V ar(O) = E(O2) − (E(O))2 ≤ E(O) − (E(O))2 = E(O)(1− E(O)) ≤ 1/4. This measure can also be averaged over a training set or a test set.\n1Given the results of the previous sections, the network can also include linear units or normalized exponential units."
    }, {
      "heading" : "4 The Dropout Approximation",
      "text" : "Given a set of numbersO1, . . . , Om between 0 and 1, with probabilities P1, . . . , PM (corresponding to the outputs of a sigmoidal neuron for a fixed input and different subnetworks), we are primarily interested in the approximation of E by NWGM . The NWGM provides a good approximation because we show below that to a first order of approximation: E ≈ NWGM and E ≈ G. Furthermore, there are formulae in the literature for bounding the error E − G in terms of the consistency (e.g. the Cartwright and Field inequality [6]). However, one can suspect that the NWGM provides even a better approximation to E than the geometric mean. For instance, if the numbers Oi satisfy 0 < Oi ≤ 0.5 (consistently low), then\nG G′ ≤ E E′ and therefore G ≤ G G+G′ ≤ E (14)\nThis is proven by applying Jensen’s inequality to the function lnx− ln(1− x) for x ∈ (0, 0.5]. It is also known as the Ky Fan inequality [2, 8, 9].\nTo get even better results, one must consider a second order approximation. For this, we write Oi = 0.5 + i with 0 ≤ | i| ≤ 0.5. Thus we have E(O) = 0.5 + E( ) and V ar(O) = V ar( ). Using a Taylor expansion:\nG = 1\n2 ∏ i ∞∑ n=0 ( pi n ) (2 i) n = 1 2 1 +∑ i pi2 i + ∑ i pi(pi − 1) 2 (2 i) 2 + ∑ i<j 4pipj i j +R3( i)  (15)\nwhere R3( i) is the remainder and\nR3( i) = ( pi 3 ) (2 i) 3 (1 + ui)3−pi (16)\nwhere |ui| ≤ 2| i|. Expanding the product gives\nG = 1 2 + ∑ i pi i+( ∑ i i) 2− ∑ pi 2 i+R3( ) = 1 2 +E( )−V ar( )+R3( ) = E(O)−V ar(O)+R3( ) (17) By symmetry, we have\nG′ = ∏ i (1−Oi)pi = 1− E(O)− V ar(O) +R3( ) (18)\nwhere R3( ) is the higher order remainder. Neglecting the remainder and writing E = E(O) and V = V ar(O) we have\nG G+G′ ≈ E − V 1− 2V and G′ G+G′ ≈ 1− E − V 1− 2V (19)\nThus, to a second order, the differences between the mean and the geometric mean and the normalized geometric means satisfy\nE −G ≈ V and E − G G+G′ ≈ V (1− 2E) 1− 2V\n(20)\nand\n1− E −G′ ≈ V and (1− E)− G ′ G+G′ ≈ V (1− 2E) 1− 2V (21)\nFinally it is easy to check that the factor (1− 2E)/(1− 2V ) is always less or equal to 1. In addition we always have V ≤ E(1− E), with equality achieved only for 0-1 Bernoulli variables. Thus\n|E − G G+G′ | ≈ V |1− 2E| 1− 2V ≤ E(1− E)|1− 2E| 1− 2V ≤ 2E(1− E)|1− 2E| (22)\nThe first inequality is optimal in the sense that it is attained in the case of a Bernoulli variable with expectation E and, intuitively, the second inequality shows that the approximation error is always small, regardless of whether E is close to 0, 0.5, or 1. In short, the NWGM provides a very good approximation to E, better than the geometric mean G. The property is always true to a second order of approximation and it is exact when the activities are consistently low, or when NWGM ≤ E, since the latter implies G ≤ NWGM ≤ E. Several additional properties of the dropout approximation, including the extension to rectified linear units and other transfer functions, are studied in [1]."
    }, {
      "heading" : "5 Dropout Dynamics",
      "text" : "Dropout performs gradient descent on-line with respect to both the training examples and the ensemble of all possible subnetworks. As such, and with the appropriately decreasing learning rates, it is almost surely convergent like other forms of stochastic gradient descent [11, 4, 5]. To further understand the properties of dropout, it is again instructive to look at the properties of the gradient in the linear case."
    }, {
      "heading" : "5.1 Single Linear Unit",
      "text" : "In the case of a single linear unit, consider the two error functions EENS and ED associated with the ensemble of all possible subnetworks and the network with dropout. For a single input I , these are defined by:\nEENS = 1\n2 (t−OENS)2 =\n1 2 (t− n∑ i=1 piwiIi) 2 (23)\nED = 1\n2 (t−OD)2 =\n1 2 (t− n∑ i=1 δiwiIi) 2 (24)\nWe use a single training input I for notational simplicity, otherwise the errors of each training example can be combined additively. The learning gradient is given by\n∂EENS ∂wi = −(t−OENS) ∂OENS ∂wi = −(t−OENS)piIi (25)\n∂ED ∂wi = −(t−OD) ∂OD ∂wi = −(t−OD)δiIi = −tλiIi + wiδ2i I2i + ∑ j 6=i wjδiδjIiIj (26)\nThe dropout gradient is a random variable and we can take its expectation. A short calculation yields\nE ( ∂ED ∂wi ) = ∂EENS ∂wi + wipi(1− pi)I2i ∂EENS ∂wi + wiI 2 i V ar(δi) (27)\nThus, remarkably, in this case the expectation of the gradient with dropout is the gradient of the regularized ensemble error\nE = EENS + 1\n2 n∑ i=1 w2i I 2 i V ar(δi) (28)\nThe regularization term is the usual weight decay or Gaussian prior term based on the square of the weights to prevent overfitting. Dropout provides immediately the magnitude of the regularization term which is adaptively scaled by the inputs and by the variance of the dropout variables. Note that pi = 0.5 is the value that provides the highest level of regularization."
    }, {
      "heading" : "5.2 Single Sigmoidal Unit",
      "text" : "The previous result generalizes to a sigmoidal unit O = σ(S) = 1/(1+ ce−λS) trained to minimize the relative entropy error E = −(t logO + (1− t) log(1−O)). In this case,\n∂ED ∂wi = −λ(t−O) ∂S ∂wi = −λ(t−O)δiIi (29)\nThe terms O and Ii are not independent but using a Taylor expansion with the NWGM approximation gives\nE ( ∂ED ∂wi ) ≈ ∂EENS ∂wi + λσ′(SENS)wiI 2 i V ar(δi) (30)\nwith SENS = ∑ j wjpjIj . Thus, as in the linear case, the expectation of the dropout gradient is approximately the gradient of the ensemble network regularized by weight decay terms with the proper adaptive coefficients. A similar analysis, can be carried also for a set of normalized exponential units and for deeper networks [1]."
    }, {
      "heading" : "5.3 Learning Phases and Sparse Coding",
      "text" : "During dropout learning, we can expect three learning phases: (1) At the beginning of learning, when the weights are typically small and random, the total input to each unit is close to 0 for all the units and the consistency is high: the output of the units remains roughly constant across subnetworks (and equal to 0.5 with c = 1). (2) As learning progresses, activities tend to move towards 0 or 1 and the consistency decreases, i.e. for a given input the variance of the units across subnetworks increases. (3) As the stochastic gradient learning procedure converges, the consistency of the units converges to a stable value.\nFinally, for simplicity, assume that dropout is applied only in layer h where the units have an output of the formOhi = σ(S h i ) and S h i = ∑ l<h w hl ij δ l jO l j . For a fixed input,O l j is a constant since dropout is not applied to layer l. Thus\nV ar(Shi ) = ∑ l<h (whlij ) 2(Olj) 2plj(1− plj) (31)\nunder the usual assumption that the selector variables δlj are independent of each other. Thus V ar(Shi ) depends on three factors. Everything else being equal, it is reduced by: (1) Small weights which goes together with the regularizing effect of dropout; (2) Small activities, which shows that dropout is not symmetric with respect to small or large activities. Overall, dropout tends to favor small activities and thus sparse coding; and (3) Small (close to 0) or large (close to 1) values of the dropout probabilities plj . Thus values p l j = 0.5 maximize the regularization effect but may also lead to slower convergence to the consistent state. Additional results and simulations are given in [1]."
    }, {
      "heading" : "6 Simulation Results",
      "text" : "We use Monte Carlo simulation to partially investigate the approximation framework embodied by the three fundamental dropout equations 11, 12, and 13, the accuracy of the second-order approximation and bounds in Equations 20 and 22, and the dynamics of dropout learning. We experiment with an MNIST classifier of four hidden layers (784-1200-1200-1200-1200-10) that replicates the results in [7] using the Pylearn2 and Theano software libraries[12, 3]. The network is trained with a dropout probability of 0.8 in the input, and 0.5 in the four hidden layers. For fixed weights and a fixed input, 10,000 Monte Carlo simulations are used to estimate the distribution of activity O in each neuron. Let O∗ be the activation under the deterministic setting with the weights scaled appropriately.\nThe left column of Figure 1 confirms empirically that the second-order approximation in Equation 20 and the bound in Equation 22 are accurate. The right column of Figure 1 shows the difference between the true ensemble average E(O) and the prediction-time neuron activity O∗. This difference grows very slowly in the higher layers, and only for active neurons.\nNext, we examine the neuron consistency during dropout training. Figure 2a shows the three phases of learning for a typical neuron. In Figure 2b, we observe that the consistency does not decline in higher layers of the network.\nOne clue into how this happens is the distribution of neuron activity. As noted in [10] and section 5 above, dropout training results in sparse activity in the hidden layers (Figure 3). This increases the consistency of neurons in the next layer."
    } ],
    "references" : [ {
      "title" : "The Dropout Learning Algorithm",
      "author" : [ "P. Baldi", "P. Sadowski" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Inequalities",
      "author" : [ "E.F. Beckenbach", "R. Bellman" ],
      "venue" : "Springer-Verlag Berlin",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio" ],
      "venue" : "In Proceedings of the Python for Scientific Computing Conference (SciPy),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Online algorithms and stochastic approximations",
      "author" : [ "L. Bottou" ],
      "venue" : "D. Saad, editor, Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Stochastic learning",
      "author" : [ "L. Bottou" ],
      "venue" : "O. Bousquet and U. von Luxburg, editors, Advanced Lectures on Machine Learning, Lecture Notes in Artificial Intelligence, LNAI 3176, pages 146–168. Springer Verlag, Berlin",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A refinement of the arithmetic mean-geometric mean inequality",
      "author" : [ "D. Cartwright", "M. Field" ],
      "venue" : "Proceedings of the American Mathematical Society, pages 36–38",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "G. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "http://arxiv.org/abs/1207.0580",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the Ky Fan inequality and related inequalities i",
      "author" : [ "E. Neuman", "J. Sándor" ],
      "venue" : "MATHEMATI- CAL INEQUALITIES AND APPLICATIONS, 5:49–56",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "On the Ky Fan inequality and related inequalities ii",
      "author" : [ "E. Neuman", "J. Sandor" ],
      "venue" : "Bulletin of the Australian Mathematical Society, 72(1):87–108",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Improving Neural Networks with Dropout",
      "author" : [ "S. Nitish" ],
      "venue" : "PhD thesis, University of Toronto, Toronto, Canada",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A convergence theorem for non negative almost supermartingales and some applications",
      "author" : [ "H. Robbins", "D. Siegmund" ],
      "venue" : "Optimizing methods in statistics, pages 233–257",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1971
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "1 Introduction Dropout is an algorithm for training neural networks that was described at NIPS 2012 [7].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "In [7], dropout is reported to achieve state-of-the-art performance on several benchmark datasets.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "Finally, one can also show that the only class of functions f that satisfy NWGM(f) = f(E) are the constant functions and the logistic functions [1].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "It can be shown that for any error function that is convex up (∪), the error of the mean, weighted geometric mean, and normalized weighted geometric mean of an ensemble is always less than the expected error of the models [1].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 5,
      "context" : "the Cartwright and Field inequality [6]).",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "It is also known as the Ky Fan inequality [2, 8, 9].",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "It is also known as the Ky Fan inequality [2, 8, 9].",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "It is also known as the Ky Fan inequality [2, 8, 9].",
      "startOffset" : 42,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "Several additional properties of the dropout approximation, including the extension to rectified linear units and other transfer functions, are studied in [1].",
      "startOffset" : 155,
      "endOffset" : 158
    }, {
      "referenceID" : 10,
      "context" : "As such, and with the appropriately decreasing learning rates, it is almost surely convergent like other forms of stochastic gradient descent [11, 4, 5].",
      "startOffset" : 142,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "As such, and with the appropriately decreasing learning rates, it is almost surely convergent like other forms of stochastic gradient descent [11, 4, 5].",
      "startOffset" : 142,
      "endOffset" : 152
    }, {
      "referenceID" : 4,
      "context" : "As such, and with the appropriately decreasing learning rates, it is almost surely convergent like other forms of stochastic gradient descent [11, 4, 5].",
      "startOffset" : 142,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "A similar analysis, can be carried also for a set of normalized exponential units and for deeper networks [1].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "Additional results and simulations are given in [1].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "We experiment with an MNIST classifier of four hidden layers (784-1200-1200-1200-1200-10) that replicates the results in [7] using the Pylearn2 and Theano software libraries[12, 3].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "We experiment with an MNIST classifier of four hidden layers (784-1200-1200-1200-1200-10) that replicates the results in [7] using the Pylearn2 and Theano software libraries[12, 3].",
      "startOffset" : 173,
      "endOffset" : 180
    }, {
      "referenceID" : 9,
      "context" : "As noted in [10] and section 5 above, dropout training results in sparse activity in the hidden layers (Figure 3).",
      "startOffset" : 12,
      "endOffset" : 16
    } ],
    "year" : 2013,
    "abstractText" : "Dropout is a relatively new algorithm for training neural networks which relies on stochastically “dropping out” neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function.",
    "creator" : null
  }
}