{
  "name" : "f64eac11f2cd8f0efa196f8ad173178e.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Step–Size for Policy Gradient Methods",
    "authors" : [ "Matteo Pirotta", "Marcello Restelli", "Luca Bascetta" ],
    "emails" : [ "matteo.pirotta@polimi.it", "marcello.restelli@polimi.it", "luca.bascetta@polimi.it" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Policy gradient methods have established as the most effective reinforcement–learning techniques in robotic applications. Such methods perform a policy search to maximize the expected return of a policy in a parameterized policy class. The reasons for their success are many. Compared to several traditional reinforcement–learning approaches, policy gradients scale well to high–dimensional continuous state and action problems, and no changes to the algorithms are needed to face uncertainty in the state due to limited and noisy sensors. Furthermore, policy representation can be properly designed for the given task, thus allowing to incorporate domain knowledge into the algorithm useful to speed up the learning process and to prevent the unexpected execution of dangerous policies that may harm the system. Finally, they are guaranteed to converge to locally optimal policies.\nThanks to these advantages, from the 1990s policy gradient methods have been widely used to learn complex control tasks [1]. The research in these years has focused on obtaining good model–free estimators of the policy gradient using data generated during the task execution. The oldest policy gradient approaches are finite–difference methods [2], that estimate gradient direction by resolving a regression problem based on the performance evaluation of policies associated to different small perturbations of the current parameterization. Finite–difference methods have some advantages: they are easy to implement, do not need assumptions on the differentiability of the policy w.r.t. the policy parameters, and are efficient in deterministic settings. On the other hand, when used on real systems, the choice of parameter perturbations may be difficult and critical for system safeness. Furthermore, the presence of uncertainties may significantly slow down the convergence rate. Such drawbacks have been overcome by likelihood ratio methods [3, 4, 5], since they do not need to generate policy parameters variations and quickly converge even in highly stochastic systems. Several\nstudies have addressed the problem to find minimum variance estimators by the computation of optimal baselines [6]. To further improve the efficiency of policy gradient methods, natural gradient approaches (where the steepest ascent is computed w.r.t. the Fisher information metric) have been considered [7, 8]. Natural gradients still converge to locally optimal policies, are independent from the policy parameterization, need less data to attain good gradient estimate, and are less affected by plateaus.\nOnce an accurate estimate of the gradient direction is obtained, policy parameters are updated by: θt+1 = θt+αt∇θJ θ=θt , where αt ∈ R+ is the step size in the direction of the gradient. Although, given an unbiased gradient estimate, convergence to a local optimum can be guaranteed under mild conditions over the learning–rate values [9], their choice may significantly affect the convergence speed or the behavior during the transient. Updating the policy with large step sizes may lead to policy oscillations or even divergence [10], while trying to avoid such phenomena by using small learning rates determines a growth in the number of iterations that is unbearable in most real–world applications. In general unconstrained programming, the optimal step size for gradient ascent methods is determined through line–search algorithms [11], that require to try different values for the learning rate and evaluate the function value in the corresponding updated points. Such an approach is unfeasible for policy gradient methods, since it would require to perform a large number of policy evaluations. Despite these difficulties, up to now, little attention has been paid to the study of step– size computation for policy gradient algorithms. Nonetheless, some policy search methods based on expectation–maximization have been recently proposed; such methods have properties similar to the ones of policy gradients, but the policy update does not require to tune the step size [12, 13].\nIn this paper, we propose a new approach to compute the step size in policy gradient methods that guarantees an improvement at each step, thus avoiding oscillation and divergence issues. Starting from a lower bound to the difference of performance between two policies, in Section 3 we derive a lower bound in the case where the new policy is obtained from the old one by changing its parameters along the gradient direction. Such a new bound is a (polynomial) function of the step size, that, for positive values of the step size, presents a single, positive maximum ( i.e., it guarantees improvement) which can be computed in closed form. In Section 4, we show how the bound simplifies to a quadratic function of the step size when Gaussian policies are considered, and Section 5 studies how the bound needs to be changed in approximated settings (e.g., model–free case) where the policy gradient needs to be estimated directly from experience."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "A discrete–time continuous Markov decision process (MDP) is defined as a 6-tuple 〈S,A,P,R, γ,D〉, where S is the continuous state space, A is the continuous action space, P is a Markovian transition model where P(s′|s, a) defines the transition density between state s and s′ under action a, R : S × A → [0, R] is the reward function, such that R(s, a) is the expected immediate reward for the state-action pair (s, a) and R is the maximum reward value, γ ∈ [0, 1) is the discount factor for future rewards, and D is the initial state distribution. The policy of an agent is characterized by a density distribution π(·|s) that specifies for each state s the density distribution over the action space A. To measure the distance between two policies we will use this norm:\n‖π′ − π‖∞ = sup s∈S ∫ A |π′(a|s)− π(a|s)|da,\nthat is the superior value over the state space of the total variation between the distributions over the action space of policy π′ and π.\nWe consider infinite horizon problems where the future rewards are exponentially discounted with γ. For each state s, we define the utility of following a stationary policy π as:\nV π(s) = E at ∼ π st ∼ P [ ∞∑ t=0 γtR(st, at)|s0 = s ] .\nIt is known that V π solves the following recursive (Bellman) equation:\nV π(s) = ∫ A π(a|s)R(s, a) + γ ∫ S P (s′|s, a)V π(s′)ds′da.\nPolicies can be ranked by their expected discounted reward starting from the state distribution D:\nJπD = ∫ S D(s)V π(s)ds) = ∫ S dπD(s) ∫ A π(a|s)R(s, a)dads,\nwhere dπD(s) = (1− γ) ∑∞ t=0 γ\ntPr(st = s|π,D) is the γ–discounted future state distribution for a starting state distribution D [5]. Solving an MDP means to find a policy π∗ that maximizes the expected long-term reward: π∗ ∈ argmaxπ∈Π JπD. For any MDP there exists at least one deterministic optimal policy that simultaneously maximizes V π(s), ∀s ∈ S. For control purposes, it is better to consider action values Qπ(s, a), i.e., the value of taking action a in state s and following a policy π thereafter:\nQπ(s, a) = R(s, a) + γ ∫ S P(s′|s, a) ∫ A π(a′|s′)Qπ(s′, a′)da′ds′.\nFurthermore, we define the advantage function:\nAπ(s, a) = Qπ(s, a)− V π(s), that quantifies the advantage (or disadvantage) of taking action a in state s instead of following policy π. In particular, for each state s, we define the advantage of a policy π′ over policy π as Aπ ′ π (s) = ∫ A π ′(a|s)Aπ(s, a)da and, following [14], we define its expected value w.r.t. an initial\nstate distribution µ as Aπ′π,µ = ∫ S d π µ(s)A π′ π (s)ds.\nWe consider the problem of finding a policy that maximizes the expected discounted reward over a class of parameterized policies Πθ = {πθ : θ ∈ Rm}, where πθ is a compact representation of π(a|s,θ). The exact gradient of the expected discounted reward w.r.t. the policy parameters [5] is:\n∇θJµ(θ) = 1\n1− γ ∫ S dπθµ (s) ∫ A ∇θπ(a|s,θ)Qπθ (s, a)dads.\nThe policy parameters can be updated by following the direction of the gradient of the expected discounted reward: θ′ = θ + α∇θJµ(θ). In the following, we will denote with ‖∇θJµ(θ)‖1 and ‖∇θJµ(θ)‖2 the L1– and L2–norm of the policy gradient vector, respectively."
    }, {
      "heading" : "3 Policy Gradient Formulation",
      "text" : "In this section we provide a lower bound to the improvement obtained by updating the policy parameters along the gradient direction as a function of the step size. The idea is to start from the general lower bound on the performance difference between any pair of policies introduced in [15] and specialize it to the policy gradient framework. Lemma 3.1 (Continuous MDP version of Corollary 3.6 in [15]). For any pair of stationary policies corresponding to parameters θ and θ′ and for any starting state distribution µ, the difference between the performance of policy πθ′ and policy πθ can be bounded as follows\nJµ(θ ′)− Jµ(θ) ≥\n1\n1− γ ∫ S dπθµ (s)A πθ′ πθ (s)ds− γ 2(1− γ)2 ‖πθ′ − πθ‖2∞ ‖Q πθ‖∞ , (1)\nwhere ‖Qπθ‖∞ is the supremum norm of the Q–function: ‖Q πθ‖∞ = sup s∈S,a∈A Qπθ (s, a)\nAs we can notice from the above bound, to maximize the performance improvement, we need to find a new policy πθ′ that is associated to large average advantage A πθ′ πθ,µ, but, at the same time, is not too different from the current policy πθ. Policy gradient approaches provide search directions characterized by increasing advantage values and, through the step size value, allow to control the difference between the new policy and the target one. Exploiting a lower bound to the first order Taylor’s expansion, we can bound the difference between the current policy and the new policy, whose parameters are adjusted along the gradient direction, as a function of the step size α. Lemma 3.2. Let the update of the policy parameters be θ′ = θ + α∇θJµ(θ). Then\nπ(a|s,θ′)− π(a|s,θ) ≥α∇θπ(a|s,θ)T∇θJµ(θ) + α2 inf c∈(0,1)\n( m∑\ni,j=1\n∂2π(a|s,θ) ∂θi∂θj ∣∣∣∣ θ+c∆θ ∆θi ∆θj 1 + I(i = j) ) ,\nwhere ∆θ = α∇θJµ(θ).\nBy combining the two previous lemmas, it is possible to derive the policy performance improvement obtained following the gradient direction.\nTheorem 3.3. Let the update of the parameters be θ′ = θ + α∇θJµ(θ). Then for any stationary policy π(a|s,θ) and any starting state distribution µ, the difference in performance between πθ and πθ′ is lower bounded by:\nJµ(θ ′)− Jµ(θ) ≥ α ‖∇θJµ(θ)‖22\n+ α2\n1− γ ∫ S dπθµ (s) ∫ A inf c∈(0,1) ( m∑\ni,j=1\n∂2π(a|s,θ) ∂θi∂θj ∣∣∣∣ θ+c∆θ ∆θi ∆θj 1 + I(i = j) ) Qπθ (s, a)dads\n− γ ‖Qπθ‖∞ 2(1− γ)2 ( α sup s∈S ∫ A ∣∣∇θπ(a|s,θ)T∇θJµ(θ)∣∣ da +α2 sup\ns∈S ∫ A ∣∣∣∣∣ supc∈(0,1) ( m∑ i,j=1 ∂2π(a|s,θ) ∂θi∂θj ∣∣∣∣ θ+c∆θ ∆θi ∆θj 1 + I(i = j) )∣∣∣∣∣ da )2 .\nThe above bound is a forth–order polynomial of the step size, whose stationary points, being the roots of a third–order polynomial ax3 +bx2 +cx+d, can be expressed in closed form. It is worth to notice that, for positive values of α, the bound presents a single stationary point that corresponds to a local maximum. In fact, since a, b ≤ 0 and d ≥ 0, the Descartes’ rule of signs gives the existence and uniqueness of the real positive root.\nIn the following section, we will show, in the case of Gaussian policies, how the bound in Theorem 3.3 can be reduced to a second–order polynomial in α, thus obtaining a simpler closed-form solution for optimal (w.r.t. the bound) step size."
    }, {
      "heading" : "4 The Gaussian Policy Model",
      "text" : "In this section we consider the Gaussian policy model with fixed standard deviation σ and the mean is a linear combination of the state feature vector φ(·) using a parameter vector θ of size m:\nπ(a|s,θ) = 1√ 2πσ2 exp\n( −1\n2\n( a− θTφ(s)\nσ\n)2) .\nIn the case of Gaussian policies, each second–order derivative of policy πθ can be easily bounded. Lemma 4.1. For any Gaussian policy π(a|s,θ) ∼ N (θTφ(s), σ2), the second order derivative of the policy can be bounded as follows:∣∣∣∣∂2π(a|s,θ)∂θi∂θj\n∣∣∣∣ ≤ |φi(s)φj(s)|√2πσ3 , ∀θ ∈ Rm,∀a ∈ A. This result allows to restate Lemma 3.2 in the case of Gaussian policies:\nπ(a|s,θ′)− π(a|s,θ) ≥ α∇θπ(a|s,θ)T∇θJµ(θ)− α2√ 2πσ3\n( |∇θJµ(θ)|T|φ(s)| )2 .\nIn the following we will assume that features φ are uniformly bounded: Assumption 4.1. All the basis functions are uniformly bounded by Mφ: |φi(s)|< Mφ, ∀s ∈ S,∀i = 1, . . . ,m.\nExploiting Pinsker’s inequality [16] (which upper bounds the total variation between two distributions with their Kullback–Liebler divergence), it is possible to provide the following upper bound to the supremum norm between two Gaussian policies. Lemma 4.2. For any pair of stationary policies πθ and πθ′ , so that θ′ = θ+α∇θJµ(θ), supremum norm of their difference can be upper bounded as follows:\n‖πθ′ − πθ‖∞ ≤ αMφ σ ‖∇θJµ(θ)‖1 .\nBy plugging the results of Lemmas 4.1 and 4.2 into Equation (1) we can obtain a lower bound to the performance difference between a Gaussian policy πθ and another policy along the gradient direction that is quadratic in the step size α. Theorem 4.3. For any starting state distribution µ, and any pair of stationary Gaussian policies πθ ∼ N (θTφ(s), σ2) and πθ′ ∼ N (θ′ T φ(s), σ2), so that θ′ = θ +α∇θJµ(θ) and under Assumption 4.1, the difference between the performance of πθ′ and the one of πθ can be lower bounded as follows:\nJµ(θ ′)− Jµ(θ) ≥ α ‖∇θJµ(θ)‖22 − α2 ( 1\n(1− γ) √ 2πσ3 ∫ S dπθµ (s) ( |∇θJµ(θ)|T |φ(s)| )2 ∫ A Qπθ (s, a)dads\n+ γM2φ\n2(1− γ)2σ2 ‖∇θJµ(θ)‖21 ‖Q πθ‖∞\n) .\nSince the linear coefficient is positive and the quadratic one is negative, the bound in Theorem 4.3 has a single maximum attained for some positive value of α. Corollary 4.4. The performance lower bound provided in Theorem 4.3 is maximized by choosing the following step size:\nα∗ = (1− γ)2\n√ 2πσ3 ‖∇θJµ(θ)‖22\nγ √\n2πσM2φ ‖∇θJµ(θ)‖ 2 1 ‖Qπθ‖∞ + 2(1− γ) ∫ S d πθ µ (s) ( |∇θJµ(θ)|T |φ(s)| )2 ∫ AQ πθ (s, a)dads ,\nthat guarantees the following policy performance improvement\nJµ(θ ′)− Jµ(θ) ≥\n1 2 α∗ ‖∇θJµ(θ)‖22 ."
    }, {
      "heading" : "5 Approximate Framework",
      "text" : "The solution for the tuning of the step size presented in the previous section depends on some constants (e.g., discount factor and the variance of the Gaussian policy) and requires to be able to compute some quantities (e.g., the policy gradient and the supremum value of the Q–function). In many real–world applications such quantities cannot be computed (e.g., when the state–transition model is unknown or too large for exact methods) and need to be estimated from experience samples. In this section, we study how the step size can be chosen when the gradient is estimated through sample trajectories to guarantee a performance improvement in high probability.\nFor sake of easiness, we consider a simplified version of the bound in Theorem 4.3, in order to obtain a bound where the only element that needs to be estimated is the policy gradient∇θJµ(θ). Corollary 5.1. For any starting state distribution µ, and any pair of stationary Gaussian policies πθ ∼ N (θTφ(s), σ2) and πθ′ ∼ N (θ′ T φ(s), σ2), so that θ′ = θ +α∇θJµ(θ) and under Assumption 4.1, the difference between the performance of πθ′ and πθ is lower bounded by:\nJµ(θ ′)− Jµ(θ) ≥ α ‖∇θJµ(θ)‖22 − α\n2 RM2φ ‖∇θJµ(θ)‖ 2 1\n(1− γ)2 σ2\n( |A|√ 2πσ + γ 2(1− γ) ) ,\nthat is maximized by the following step size value:\nα̃∗ = (1− γ)3 √ 2πσ3 ‖∇θJµ(θ)‖22(\nγ √ 2πσ + 2(1− γ)|A| ) RM2φ ‖∇θJµ(θ)‖ 2 1 .\nSince we are assuming that the policy gradient ∇θJµ(θ) is estimated through trajectory samples, the lower bound in Corollary 5.1 must take into consideration the associated approximation error. Given a set of trajectories obtained following policy πθ, we can produce an estimate ∇̂θJµ(θ) of the policy gradient and we assume to be able to produce a vector = [ 1, . . . , m]\nT, so that the i–th component of the approximation error is bounded at least with probability 1− δ:\nP (∣∣∣∇θiJµ(θ)− ∇̂θiJµ(θ)∣∣∣ ≥ i) ≤ δ.\nGiven the approximation error vector , we can adjust the bound in Corollary 5.1 to produce a new bound that holds at least with probability (1− δ)m. In particular, to preserve the inequality sign, the estimated approximation error must be used to decrease the L2–norm of the policy gradient in the first term (the one that provides the positive contribution to the performance improvement) and to increase the L1–norm in the penalization term. To lower bound the L2–norm, we introduce the vector ∇̂θJµ(θ) whose components are a lower bound to the absolute value of the policy gradient built on the basis of the approximation error :\n∇̂θJµ(θ) = max(|∇̂θJµ(θ)| − ,0),\nwhere 0 denotes the m–size vector with all zeros, and max denotes the component–wise maximum. Similarly, to upper bound the L1–norm of the policy gradient, we introduce the vector ∇̂θJµ(θ):\n∇̂θJµ(θ) = |∇̂θJµ(θ)|+ .\nTheorem 5.2. Under the same assumptions of Corollary 5.1, and provided that it is available a policy gradient estimate ∇̂θJµ(θ), so that P (∣∣∣∇θiJµ(θ)− ∇̂θiJµ(θ)∣∣∣ ≥ i) ≤ δ, the difference between the performance of πθ′ and πθ can be lower bounded at least with probability (1− δ)m:\nJµ(θ ′)− Jµ(θ) ≥ α ∥∥∥∇̂θJµ(θ)∥∥∥2 2 − α2 RM2φ\n∥∥∥∇̂θJµ(θ)∥∥∥2 1\n(1− γ)2 σ2\n( |A|√ 2πσ + γ 2(1− γ) ) ,\nthat is maximized by the following step size value:\nα̂∗ = (1− γ)3\n√ 2πσ3 ∥∥∥∇̂θJµ(θ)∥∥∥2 2(\nγ √ 2πσ + 2(1− γ)|A| ) RM2φ ∥∥∥∇̂θJµ(θ)∥∥∥2 1 .\nIn the following, we will discuss how the approximation error of the policy gradient can be bounded. Among the several methods that have been proposed over the years, we focus on two well– understood policy–gradient estimation approaches: REINFORCE [3] and G(PO)MDP [4]/policy gradient theorem (PGT) [5]."
    }, {
      "heading" : "5.1 Approximation with REINFORCE gradient estimator",
      "text" : "The REINFORCE approach [3] is the main exponent of the likelihood–ratio family. The episodic REINFORCE gradient estimator is given by:\n∇̂θJRFµ (θ) = 1\nN N∑ n=1 ( H∑ k=1 ∇θ log π (ank ; snk ,θ) ( H∑ l=1 γl−1rnl − b )) ,\nwhere N is the number of H–step trajectories generated from a system by roll–outs and b ∈ R is a baseline that can be chosen arbitrary, but usually with the goal of minimizing the variance of the gradient estimator. The main drawback of REINFORCE is its variance, that is strongly affected by the length of the trajectory horizon H .\nThe goal is to determine the number of trajectories N in order to obtain the desired accuracy of the gradient estimate. To achieve this, we exploit the upper bound to the variance of the episodic REINFORCE gradient estimator introduced in [17] for Gaussian policies. Lemma 5.3 (Adapted from Theorem 2 in [17]). Given a Gaussian policy π(a|s,θ) ∼ N ( θTφ(s), σ2 ) , under the assumption of uniformly bounded rewards and basis functions (Assumption 4.1), we have the following upper bound to the variance of the i–th component of the episodic REINFORCE gradient estimate ∇̂θiJRFµ (θ):\nV ar ( ∇̂θiJRFµ (θ) ) ≤ R2M2φH ( 1− γH )2 Nσ2 (1− γ)2 .\nThe result in the previous Lemma combined with the Chebyshev’s inequality allows to provide a high–probability upper bound to the gradient approximation error using the episodic REINFORCE gradient estimator.\nTheorem 5.4. Given a Gaussian policy π(a|s,θ) ∼ N ( θTφ(s), σ2 ) , under the assumption of uniformly bounded rewards and basis functions (Assumption 4.1), using the following number of H–step trajectories:\nN = R2M2φH\n( 1− γH )2 δ 2iσ 2 (1− γ)2 ,\nthe gradient estimate ∇̂θiJRFµ (θ) generated by REINFORCE is such that with probability 1− δ:∣∣∣∇̂θiJRFµ (θ)−∇θiJµ(θ)∣∣∣ ≤ i."
    }, {
      "heading" : "5.2 Approximation with G(PO)MDP/PGT gradient estimator",
      "text" : "Although the REINFORCE method is guaranteed to converge at the true gradient at the fastest possible pace, its large variance can be problematic in practice. Advances in the likelihood ratio gradient estimators have produced new approaches that significantly reduce the variance of the estimate. Focusing on the class of “vanilla” gradient estimator, two main approaches have been proposed: policy gradient theorem (PGT) [5] and G(PO)MDP [4]. In [6], the authors show that, while the algorithms look different, their gradient estimate are equal, i.e., ∇̂θJPGTµ (θ) = ∇̂θJG(PO)MDPµ (θ). For this reason, we can limit our attention to the PGT formulation:\n∇̂θJPGTµ (θ) = 1\nN H∑ n=1 ( H∑ k=1 ∇θ log π (ank ; snk ,θ) ( H∑ l=k γl−1rnl − bnl )) ,\nwhere bnl ∈ R have the objective to reduce the variance of the gradient estimate. Following the procedure used to bound the approximation error of REINFORCE, we need an upper bound to the variance of the gradient estimate of PGT that is provided by the following lemma (whose proof is similar to the one used in [17] for the REINFORCE case).\nLemma 5.5. Given a Gaussian policy π(a|s,θ) ∼ N ( θTφ(s), σ2 ) , under the assumption of uniformly bounded rewards and basis functions (Assumption 4.1), we have the following upper bound to the variance of the i–th component of the PGT gradient estimate ∇̂θiJPGTµ (θ):\nV ar ( ∇̂θiJPGTµ (θ) ) ≤\nR2M2φ\nN (1− γ)2 σ2\n[ 1− γ2H\n1− γ2 +Hγ2H − 2γH 1− γ H 1− γ\n] .\nAs expected, since the variance of the gradient estimate obtained with PGT is smaller than the one with REINFORCE, also the upper bound of the PGT variance is smaller than REINFORCE one. In particular, while the variance with REINFORCE grows linearly with the time horizon, using PGT the dependence on the time horizon is significantly smaller. Finally, we can derive the upper bound for the approximation error of the gradient estimated of PGT.\nTheorem 5.6. Given a Gaussian policy π(a|s,θ) ∼ N ( θTφ(s), σ2 ) , under the assumption of uniformly bounded rewards and basis functions (Assumption 4.1), using the following number of H–step trajectories:\nN = R2M2φ\nδ 2iσ 2 (1− γ)2\n[ 1− γ2H\n1− γ2 +Hγ2H − 2γH 1− γ H 1− γ ] the gradient estimate ∇̂θiJPGTµ (θ) generated by PGT is such that with probability 1− δ:∣∣∣∇̂θiJPGTµ (θ)−∇θiJµ(θ)∣∣∣ ≤ i."
    }, {
      "heading" : "6 Numerical Simulations and Discussion",
      "text" : "In this section we show results related to some numerical simulations of policy gradient in the linear–quadratic Gaussian regulation (LQG) problem as formulated in [6]. The LQG problem is characterized by a transition model st+1 ∼ N ( st + at, σ 2 ) , Gaussian policy at ∼ N ( θ · s, σ2\n) and quadratic reward rt = −0.5(s2t + a2t ). The range of state and action spaces is bounded to the interval [−2, 2] and the initial state is drawn uniformly at random. This scenario is particularly instructive since it allows to exactly compute all terms involved in the bounds. We first present results in the exact scenario and then we move toward the approximated one.\nTable 1 shows how the number of iterations required to learn a near–optimal value of the policy parameter changes according to the standard deviation of the Gaussian policy and the step–size value. As expected, very small values of the step size allow to avoid divergence, but the learning process needs many iterations to reach a good performance (this can be observed both when the step size is kept constant and when it decreases). On the other hand, larger step–size values may lead to divergence. In this example, the higher the policy variance, the lower is the step size value that allows to avoid divergence, since, in LQG, higher policy variance implies larger policy gradient values. Using the step size α∗ from Corollary 4.4 the policy gradient algorithm avoids divergence (since it guarantees an improvement at each iteration), and the speed of convergence is strongly affected by the variance of the Gaussian policy. In general, when the policy are nearly deterministic (small variance in the Gaussian case), small changes in the parameters lead to large distances between the policies, thus negatively affecting the lower bound in Equation 1. As we can notice from the expression ofα∗ in Corollary 4.4, considering policies with high variance (that might be a problem in real–world applications) allows to safely take larger step size, thus speeding up the learning process. Nonetheless, increasing the variance over some threshold (making policies nearly random) produces very bad policies, so that changing the policy parameter has a small impact on the performance, and as a result slows down the learning process. How to identify an optimal variance value is an interesting future research direction. Table 2 provides numerical results in the approximated settings, showing the effect of varying the number of trajectories used to estimate the gradient by REINFORCE and PGT. Increasing the number of trajectories reduces the uncertainty on the gradient estimates, thus allowing to use larger step sizes and reaching better performances. Furthermore, the smaller variance of PGT w.r.t. REINFORCE allows the former to achieve better performances. However, even with a large number of trajectories, the approximated errors are still quite large preventing to reach very high performance. For this reason, future studies will try to derive tighter bounds. Further developments include extending these results to other policy models (e.g., Gibbs policies) and to other policy gradient approaches (e.g., natural gradient)."
    } ],
    "references" : [ {
      "title" : "Policy gradient methods for robotics",
      "author" : [ "Jan Peters", "Stefan Schaal" ],
      "venue" : "In Intelligent Robots and Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation",
      "author" : [ "James C Spall" ],
      "venue" : "Automatic Control, IEEE Transactions on,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1992
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J. Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1992
    }, {
      "title" : "Infinite-horizon policy-gradient estimation",
      "author" : [ "Jonathan Baxter", "Peter L. Bartlett" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David McAllester", "Satinder Singh", "Yishay Mansour" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "Reinforcement learning of motor skills with policy gradients",
      "author" : [ "Jan Peters", "Stefan Schaal" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "A natural policy gradient",
      "author" : [ "Sham Kakade" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Herbert Robbins", "Sutton Monro" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1951
    }, {
      "title" : "A reinterpretation of the policy oscillation phenomenon in approximate policy iteration",
      "author" : [ "P. Wagner" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Line search algorithms with guaranteed sufficient decrease",
      "author" : [ "Jorge J Moré", "David J Thuente" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1994
    }, {
      "title" : "Policy search for motor primitives in robotics",
      "author" : [ "J. Kober", "J. Peters" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Learning model-free robot control by a monte carlo em algorithm",
      "author" : [ "Nikos Vlassis", "Marc Toussaint", "Georgios Kontes", "Savas Piperidis" ],
      "venue" : "Autonomous Robots,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "On the sample complexity of reinforcement learning",
      "author" : [ "S.M. Kakade" ],
      "venue" : "PhD thesis, PhD thesis,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2003
    }, {
      "title" : "Safe policy iteration",
      "author" : [ "Matteo Pirotta", "Marcello Restelli", "Alessio Pecorino", "Daniele Calandriello" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML-13),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Information and Information Stability of Random Variable and Processes. Holden- Day Series in Time Series Analysis",
      "author" : [ "S. Pinsker" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1964
    }, {
      "title" : "Analysis and improvement of policy gradient estimation",
      "author" : [ "Tingting Zhao", "Hirotaka Hachiya", "Gang Niu", "Masashi Sugiyama" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Thanks to these advantages, from the 1990s policy gradient methods have been widely used to learn complex control tasks [1].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 1,
      "context" : "The oldest policy gradient approaches are finite–difference methods [2], that estimate gradient direction by resolving a regression problem based on the performance evaluation of policies associated to different small perturbations of the current parameterization.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "Such drawbacks have been overcome by likelihood ratio methods [3, 4, 5], since they do not need to generate policy parameters variations and quickly converge even in highly stochastic systems.",
      "startOffset" : 62,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "Such drawbacks have been overcome by likelihood ratio methods [3, 4, 5], since they do not need to generate policy parameters variations and quickly converge even in highly stochastic systems.",
      "startOffset" : 62,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "Such drawbacks have been overcome by likelihood ratio methods [3, 4, 5], since they do not need to generate policy parameters variations and quickly converge even in highly stochastic systems.",
      "startOffset" : 62,
      "endOffset" : 71
    }, {
      "referenceID" : 5,
      "context" : "studies have addressed the problem to find minimum variance estimators by the computation of optimal baselines [6].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "the Fisher information metric) have been considered [7, 8].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : "Although, given an unbiased gradient estimate, convergence to a local optimum can be guaranteed under mild conditions over the learning–rate values [9], their choice may significantly affect the convergence speed or the behavior during the transient.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Updating the policy with large step sizes may lead to policy oscillations or even divergence [10], while trying to avoid such phenomena by using small learning rates determines a growth in the number of iterations that is unbearable in most real–world applications.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "In general unconstrained programming, the optimal step size for gradient ascent methods is determined through line–search algorithms [11], that require to try different values for the learning rate and evaluate the function value in the corresponding updated points.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 10,
      "context" : "Nonetheless, some policy search methods based on expectation–maximization have been recently proposed; such methods have properties similar to the ones of policy gradients, but the policy update does not require to tune the step size [12, 13].",
      "startOffset" : 234,
      "endOffset" : 242
    }, {
      "referenceID" : 11,
      "context" : "Nonetheless, some policy search methods based on expectation–maximization have been recently proposed; such methods have properties similar to the ones of policy gradients, but the policy update does not require to tune the step size [12, 13].",
      "startOffset" : 234,
      "endOffset" : 242
    }, {
      "referenceID" : 4,
      "context" : "where dD(s) = (1− γ) ∑∞ t=0 γ Pr(st = s|π,D) is the γ–discounted future state distribution for a starting state distribution D [5].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 12,
      "context" : "In particular, for each state s, we define the advantage of a policy π′ over policy π as A ′ π (s) = ∫ A π ′(a|s)Aπ(s, a)da and, following [14], we define its expected value w.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "The idea is to start from the general lower bound on the performance difference between any pair of policies introduced in [15] and specialize it to the policy gradient framework.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "Exploiting Pinsker’s inequality [16] (which upper bounds the total variation between two distributions with their Kullback–Liebler divergence), it is possible to provide the following upper bound to the supremum norm between two Gaussian policies.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "Among the several methods that have been proposed over the years, we focus on two well– understood policy–gradient estimation approaches: REINFORCE [3] and G(PO)MDP [4]/policy gradient theorem (PGT) [5].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "Among the several methods that have been proposed over the years, we focus on two well– understood policy–gradient estimation approaches: REINFORCE [3] and G(PO)MDP [4]/policy gradient theorem (PGT) [5].",
      "startOffset" : 165,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "Among the several methods that have been proposed over the years, we focus on two well– understood policy–gradient estimation approaches: REINFORCE [3] and G(PO)MDP [4]/policy gradient theorem (PGT) [5].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 2,
      "context" : "The REINFORCE approach [3] is the main exponent of the likelihood–ratio family.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "To achieve this, we exploit the upper bound to the variance of the episodic REINFORCE gradient estimator introduced in [17] for Gaussian policies.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Focusing on the class of “vanilla” gradient estimator, two main approaches have been proposed: policy gradient theorem (PGT) [5] and G(PO)MDP [4].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "Focusing on the class of “vanilla” gradient estimator, two main approaches have been proposed: policy gradient theorem (PGT) [5] and G(PO)MDP [4].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "In [6], the authors show that, while the algorithms look different, their gradient estimate are equal, i.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 15,
      "context" : "Following the procedure used to bound the approximation error of REINFORCE, we need an upper bound to the variance of the gradient estimate of PGT that is provided by the following lemma (whose proof is similar to the one used in [17] for the REINFORCE case).",
      "startOffset" : 230,
      "endOffset" : 234
    }, {
      "referenceID" : 5,
      "context" : "6 Numerical Simulations and Discussion In this section we show results related to some numerical simulations of policy gradient in the linear–quadratic Gaussian regulation (LQG) problem as formulated in [6].",
      "startOffset" : 203,
      "endOffset" : 206
    } ],
    "year" : 2013,
    "abstractText" : "In the last decade, policy gradient methods have significantly grown in popularity in the reinforcement–learning field. In particular, they have been largely employed in motor control and robotic applications, thanks to their ability to cope with continuous state and action domains and partial observable problems. Policy gradient researches have been mainly focused on the identification of effective gradient directions and the proposal of efficient estimation algorithms. Nonetheless, the performance of policy gradient methods is determined not only by the gradient direction, since convergence properties are strongly influenced by the choice of the step size: small values imply slow convergence rate, while large values may lead to oscillations or even divergence of the policy parameters. Step–size value is usually chosen by hand tuning and still little attention has been paid to its automatic selection. In this paper, we propose to determine the learning rate by maximizing a lower bound to the expected performance gain. Focusing on Gaussian policies, we derive a lower bound that is second–order polynomial of the step size, and we show how a simplified version of such lower bound can be maximized when the gradient is estimated from trajectory samples. The properties of the proposed approach are empirically evaluated in a linear–quadratic regulator problem.",
    "creator" : null
  }
}