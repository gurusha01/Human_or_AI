{
  "name" : "e58cc5ca94270acaceed13bc82dfedf7.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Speedup Matrix Completion with Side Information: Application to Multi-Label Learning",
    "authors" : [ "Miao Xu", "Rong Jin", "Zhi-Hua Zhou" ],
    "emails" : [ "zhouzh}@lamda.nju.edu.cn", "rongjin@cse.msu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Matrix completion concerns the problem of recovering a low-rank matrix from a limited number of observed entries. It has broad applications including collaborative filtering [35], dimensionality reduction [41], multi-class learning [4, 31], clustering [15, 42], etc. Recent studies show that, with a high probability, we can efficiently recover a matrix M ∈ Rn×m of rank r from O(r(n+m) ln2(n+ m)) observed entries when the observed entries are uniformly sampled from M [11, 12, 34].\nAlthough the sample complexity for matrix completion, i.e., the number of observed entries required for perfectly recovering a low rank matrix, is already near optimal (up to a logarithmic factor), its linear dependence on n and m requires a large number of observations for recovering large matrices, significantly limiting its application to real-world problems. Moreover, current techniques for matrix completion require solving an optimization problem that can be computationally prohibitive when the size of the matrix is very large. In particular, although a number of algorithms have been developed for matrix completion [10, 22, 23, 25, 27, 28, 39], most of them require updating the full matrix M at each iteration of optimization, leading to a high computational cost and a large storage requirement when both n and m are large. Several recent efforts [5, 19] try to address this issue, at a price of losing performance guarantee in recovering the target matrix.\nOn the other hand, in several applications of matrix completion, besides the observed entries, side information is often available that can potentially benefit the process of matrix completion. Below we list a few examples:\n• Collaborative filtering aims to predict ratings of individual users based on the ratings from other users [35]. Besides the ratings provided by users, side information, such as the textual description of items and the demographical information of users, is often available and can be used to facilitate the prediction of missing ratings.\n• Link prediction aiming to predict missing links between users in a social network based on the existing ones can be viewed as a matrix completion problem [20], where side information, such as attributes of users (e.g., browse patterns and interaction among users), can be used to assist completing the user-user-link matrix.\nAlthough several studies exploit side information for matrix recovery [1, 2, 3, 16, 29, 32, 33], most of them focus on matrix factorization techniques, which usually result in non-convex optimization problems without guarantee of perfectly recovering the target matrix. In contrast, matrix completion deals with convex optimization problems and perfect recovery is guaranteed under appropriate conditions.\nIn this work, we focus on exploiting side information to improve the sample complexity and scalability of matrix completion. We assume that besides the observed entries in the matrix M , there exist two side information matrices A ∈ Rn×ra and B ∈ Rm×rb , where r ≤ ra ≤ n and r ≤ rb ≤ m. We further assume the target matrix and the side information matrices share the same latent information; that is, the column and row vectors in M lie in the subspaces spanned by the column vectors in A and B, respectively. Unlike the standard theory of matrix completion that needs to find the optimal matrix M of size n×m, our optimization problem is reduced to searching for an optimal matrix of size ra × rb, making the recovery significantly more efficient both computationally and storage wise provided ra ≪ n and/or rb ≪ m. We show that, with the assistance of side information matrices, with a high probability, we can perfectly recover M with O(r(ra + rb) ln(ra + rb) ln(n+m)) observed entries, a sample complexity that is sublinear in n and m.\nWe demonstrate the effectiveness of matrix completion with side information in transductive incomplete multi-label learning [17], which aims to assign multiple labels to individual instances in a transductive learning setting. We formulate transductive incomplete multi-label learning as a matrix completion problem, i.e., completing the instance-label matrix based on the observed entries that correspond to the given label assignments. Both the feature vectors of instances and the class correlation matrix can be used as side information. Our empirical study shows that the proposed approach is particularly effective when the number of given label assignments is small, verifying our theoretical result, i.e., side information can be used to reduce the sample complexity.\nThe rest of the paper is organized as follows: Section 2 briefly reviews some related work. Section 3 presents our main contribution. Section 4 presents our empirical study. Finally Section 5 concludes with future issues."
    }, {
      "heading" : "2 Related work",
      "text" : "Matrix Completion The objective of matrix completion is to fill out the missing entries of a matrix based on the observed ones. Early work on matrix completion, also referred to as maximum margin matrix factorization [37], was developed for collaborative filtering. Theoretical studies show that, it is sufficient to perfectly recover a matrix M ∈ Rn×m of rank r when the number of observed entries is O(r(n + m) ln2(n + m)) [11, 12, 34]. A more general matrix recovery problem, referred to as matrix regression, was examined in [30, 36]. Unlike these studies, our proposed approach reduces the sample complexity with the help of side information matrices.\nSeveral computational algorithms [10, 22, 23, 25, 27, 28, 39] have been developed to efficiently solve the optimization problem of matrix completion. The main problem with these algorithms lies in the fact that they have to explicitly update the full matrix of size n×m, which is expensive both computationally and storage wise for large matrices. This issue has been addressed in several recent studies [5, 19], where the key idea is to store and update the low rank factorization of the target matrix. A preliminary convergence analysis is given in [19], however, none of these approaches guarantees perfect recovery of the target matrix, even with significantly large number of observed entries. In contrast, our proposed approach reduces the computational cost by explicitly exploring the side information matrices and still delivers the promise of perfect recovery.\nSeveral recent studies involve matrix recovery with side information. [2, 3, 29, 33] are based on graphical models by assuming special distribution of latent factors; these algorithms, as well as [16] and [32], consider side information in matrix factorization. The main limitation lies in the fact that they have to solve non-convex optimization problems, and do not have theoretical guarantees on matrix recovery. Matrix completion with infinite dimensional side information was exploited in [1],\nyet lacking guarantee of perfect recovery. In contrast, our work is based on matrix completion theory that deals with a general convex optimization problem and is guaranteed to make a perfect recovery of the target matrix.\nMulti-label Learning Multi-label learning allows each instance to be assigned to multiple classes simultaneously, making it more challenging than multi-class learning. The simplest approach for multi-label learning is to train one binary model for each label, which is also referred to as BR (Binary Relevance) [7]. Many advanced algorithms have been developed to explicitly explore the dependence among labels ( [44] and references therein).\nIn this work, we will evaluate our proposed approach by transductive incomplete multi-label learning [17]. Let X = (x1, . . . ,xn)⊤ ∈ Rn×d be the feature matrix with xi ∈ Rd, where n is the number of instances and d is the dimension. Let C1, . . . , Cm denote the m labels, and let T ∈ {−1,+1}n×m be the instance-label matrix, where Ti,j = +1 when xi is associated with the label Cj , and Ti,j = −1 when xi is not associated with the label Cj . Let Ω denote the subset of the observed entries in T that corresponds to the given label assignments of instances. The objective of transductive incomplete multi-label learning is to predict the missing entries in T based on the feature matrix X and the given label assignments in Ω. The main challenge lies in the fact that only a partial label assignment is given for each training instance. This is in contrast to many studies on common semi-supervised or transductive multi-label learning [18, 24, 26, 43] where each labeled instance receives a complete set of label assignments. This is also different from multi-label learning with weak labels [8, 38] which assumes that only the positive labels can be observed. Here we assume the observed labels can be either positive or negative.\nIn [17], a matrix completion based approach was proposed for transductive incomplete multi-label learning. To effectively exploit the information in the feature matrix X , the authors proposed to complete the matrix T ′ = [X,T ] that combines the input features with label assignments into a single matrix. Two algorithms MC-b and MC-1 were presented there, differing only in the treatment of bias term, whereas the convergence of MC-1 was examined in [9]. The main limitation of both algorithms lies in their high computational cost when both the number of instances and features are large. Unlike MC-1 and MC-b, our proposed approach does not need to deal with the big matrix T ′, and is computationally more efficient. Besides the computational advantage, we show that our proposed approach significantly improves the sample complexity of matrix completion by exploiting side information matrices."
    }, {
      "heading" : "3 Speedup Matrix Completion with Side Information",
      "text" : "We first describe the framework of matrix completion with side information, and then present its theoretical guarantee and application to multi-label learning"
    }, {
      "heading" : "3.1 Matrix Completion using Side Information",
      "text" : "Let M ∈ Rn×m be the target matrix of rank r to be recovered. Without loss of generality, we assume n ≥ m. Let λk, k ∈ {1, . . . , r} be the kth largest singular value of M , and let uk ∈ Rn and vk ∈ Rm be the corresponding left and right singular vectors, i.e., M = UΣV ⊤, where Σ = diag(λ1, . . . , λr), U = (u1, . . . ,ur) and V = (v1, . . . ,vr).\nLet Ω ⊆ {1, . . . , n} × {1, . . . ,m} be the subset of indices of observed entries sampled uniformly from all entries in M . Given Ω, we define a linear operator RΩ(M) : Rn×m 7→ Rn×m as\n[RΩ(M)]i,j = {\nMi,j (i, j) ∈ Ω 0 (i, j) /∈ Ω\nUsing RΩ(·), the standard matrix completion problem is: min\nM̃∈Rn×m ∥M̃∥tr s. t. RΩ(M̃) = RΩ(M), (1)\nwhere ∥ · ∥tr is the trace norm. Let A = (a1, . . . ,ara) ∈ Rn×ra and B = (b1, . . . ,brb) ∈ Rm×rb be the side information matrices, where r ≤ ra ≤ n and r ≤ rb ≤ m. Without loss of generality, we assume that ra ≥ rb and that\nboth A and B are orthonormal matrices, i.e., a⊤i aj = δi,j and b ⊤ i bj = δi,j for any i and j, where δi,j is the Kronecker delta function that outputs 1 if i = j and 0, otherwise. In case when the side information is not available, A and B will be set to identity matrix.\nThe objective is to complete a matrix M of rank r with the side information matrices A and B. We make the following assumption in order to fully exploit the side information:\nAssumption A: the column vectors in M lie in the subspace spanned by the column vectors in A, and the row vectors in M lie in the subspace spanned by the column vectors in B.\nTo understand the implication of this assumption, let us consider the problem of transductive incomplete multi-label learning [17], where the objective is to complete the instance-label matrix based on the observed entries corresponding to the given label assignments, and the side information matrices A and B are given by the feature vectors of instances and the label correlation matrix, respectively. Assumption A essentially implies that all the label assignments can be accurately predicted by a linear combination of feature vectors of instances.\nUsing Assumption A, we can write M as M = AZ0B⊤ and therefore, our goal is to learn Z0 ∈ Rra×rb . Following the standard theory for matrix completion [11, 12, 34], we can cast the matrix completion task into the following optimization problem:\nmin Z∈Rra×rb\n∥Z∥tr s. t. RΩ(AZB⊤) = RΩ(M). (2)\nUnlike the standard algorithm for matrix completion that requires solving an optimization problem involved matrix of n × m, the optimization problem given in (2) only deals with a matrix Z of ra × rb, and therefore can be solved significantly more efficiently if ra ≪ n and rb ≪ m."
    }, {
      "heading" : "3.2 Theoretical Result",
      "text" : "We define µ0 and µ1, the coherence measurements for matrix M as\nµ0 = max\n( n\nr max 1≤i≤n\n∥PUei∥2, m\nr max 1≤j≤m ∥PV ej∥2\n) ,\nµ1 = max i,j\nmn\nr ([UV ⊤]i,j) 2,\nwhere ei is the vector with the ith entry equal to 1 and all others equal to 0, and PU and PV project a vector onto the subspace spanned by the column vectors of U and V , respectively. We also define the coherence measure for matrices A and B as\nµAB = max ( max 1≤i≤n n∥Ai,∗∥2 ra , max 1≤j≤m m∥Bj,∗∥2 rb ) ,\nwhere Ai,∗ and Bi,∗ stand for the ith row of A and B, respectively.\nTheorem 1. Let µ = max(µ0, µAB). Define q0 = 12 (1 + log2 ra − log2 r), Ω0 = 128β 3 µmax(µ1, µ)r(ra + rb) lnn and Ω1 = 8β 3 µ 2(rarb + r 2) lnn. Assume Ω1 ≥ q0Ω0. With a probability at least 1− 4(q0 + 1)n−β+1 − 2q0n−β+2, Z0 is the unique optimizer to the problem in (2) provided\n|Ω| ≥ 64β 3 µmax(µ1, µ) (1 + log2 ra − log2 r) r(ra + rb) lnn.\nCompared to the standard matrix completion theory [34], the side information matrices reduce sample complexity from O(r(n+m) ln2(n+m)) to O(r(ra+ rb) ln(ra+ rb) lnn). When ra ≪ n and rb ≪ m, the side information allows us significantly reduce the number of observed entries required for perfectly recovering matrix M . We defer the technical proof of Theorem 1 to the supplementary material due to page limit. Note that although we follow the framework of [34] for analysis, namely first proving the result under deterministic conditions, and then showing that the deterministic conditions hold with a high probability, our technical proof is quite different due to the involvement of side information matrices A and B."
    }, {
      "heading" : "3.3 Application to Multi-Label Learning",
      "text" : "Similar to the Singular Vector Thresholding (SVT) method [10], we approximate the problem in ( 2) by an unconstrained optimization problem, i.e.,\nmin Z∈Rra×rb\nL(Z) = λ∥Z∥tr + 1\n2 ∥∥RΩ(AZB⊤ −M)∥∥2F , (3) where λ > 0 is introduced to weight the trace norm regularization term against the regression error. We develop an algorithm that exploits the smoothness of the loss function and therefore achieves O(1/T 2) convergence, where T is the number of iterations. Details of the algorithm can be found in the supplementary material. We refer to the proposed algorithm as Maxide.\nFor transductive incomplete multi-label learning, we abuse our notation by defining n as the number of instances, m as the number of labels, and d as the dimensionality of input patterns. Our goal is to complete the instance-label matrix M ∈ Rn×m by using (i) the feature matrix X ∈ Rn×d and (ii) the observed entries Ω in M (i.e., the given label assignments). We thus set the side information matrix A to include the top left singular vectors of X , and B = I to indicate that no side information is available for the dependence among labels. We note that the low rank assumption of instance-label matrix M implies a linear dependence among the label prediction functions. This assumption has been explored extensively in the previous studies of multi-label learning [17, 21, 38]."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate the proposed algorithm for matrix completion with side information on both synthetic and real data sets. Our implementation is in Matlab except that the operation RΩ(L × R) is implemented in C. All the results were obtained on a Linux server with CPU 2.53GHz and 48GB memory."
    }, {
      "heading" : "4.1 Experiments on Synthetic Data",
      "text" : "To create the side information matrices A and B, we first generate a random matrix F ∈ Rn×m, with each entry Fi,j drawn independently from N (0, 1). Side information matrix A includes the first ra left singular vectors of F , and B includes the first rb right singular vectors. To create Z0, we generate two Gaussian random matrices ZA ∈ Rra×r and ZB ∈ Rrb×r, where each entry is sampled independently from N (0, 1). The singular value decomposition of AZA and BZB is given by AZA = UΣ1V T1 and BZB = V Σ2V T 2 , respectively. We create a diagonal matrix Σ ∈ Rr×r, whose diagonal entries are drawn independently from N (0, 104). Z0 is then given by Z0 = (ZAΣ † 1(V T 1 ) †)Σ(ZBΣ † 2(V T 2 )\n†)T where † denotes the pseudo inverse of a matrix. Finally, the target matrix M is given by M = AZ0B⊤.\nSettings and Baselines Our goal is to show that the proposed algorithm is able to accurately recover the target matrix with significantly smaller number of entries and less computational time. In this study, we only consider square matrices (i.e., m = n), with n = 1, 000; 5, 000; 10, 000; 20, 000; 30, 000 and rank r = 10; 50; 100. Both ra and rb of side information matrices are set to be 2r, and |Ω|, the number of observed entries, is set to be r(2n− r), which is significantly smaller than the number of observed entries used in previous studies [10, 25, 27]. We repeat each experiment 10 times, and report the result averaged over 10 runs. We compare the proposed Maxide algorithm with three state-of-the-art matrix completion algorithms: Singular Vector Thresholding (SVT) [10], Fixed Point Bregman Iterative Method (FPCA) [27] and Augmented Lagrangian Method (ALM) [25]. In addition to these matrix completion methods, we also compare with a trace norm minimizing method (TraceMin) [6]. For all the baseline, we use the codes provided by their original authors with their default parameter settings.\nResults We measure the performance of matrix completion by the relative error ∥AZB⊤ − M∥F /∥M∥F and report the results of both relative error and running time in Table 1. For TraceMin, we observe that for n = 1, 000 and r = 10, it gives the result of 1.75 × 10−7 within 2.94 × 104 seconds, which is really slow compared to our proposal. For n = 1, 000 and r = 50, it gives no result within one week. In Table 1, we first observed that for all the cases, the relative error achieved\nby the baseline methods is Ω(1), implying that none of them is able to make accurate recovery of the target matrix given the small number of observed entries. In contrast, our proposed algorithm is able to recover the target matrix with small relative error. In addition, our proposed algorithm is computationally more efficient than the baseline methods. The improvement in computational efficiency becomes more significant for large matrices."
    }, {
      "heading" : "4.2 Application to Transductive Incomplete Multi-Label Learning",
      "text" : "We evaluate the proposed algorithm for transductive incomplete multi-label learning on thirteen benchmark data sets, including eleven data sets for web page classification from “yahoo.com” [40], and two image classification data sets NUS-WIDE [14] and Flickr [45]. For the eleven “yahoo.com” data sets, the number of instances is n = 5, 000 and the number of dimensions varies from 438 to 1,047, with the number of labels varies from 21 to 40. Detailed information of these eleven data sets can be found in [40]. For NUS-WIDE data set, we have n = 209, 347 images each represented by a bag-of-words model with d = 500 visual words, and 81 labels. For the Flickr data set, we only keep the first 1, 000 most popular keywords for labels, leaving us with n = 565, 444 images, each represented by a d = 297-dimension vector.\nSettings and Baselines For each data set, we randomly sample 10% instances for testing (unlabeled data) and use the remaining 90% data for training. No label assignment is provided for any test instance. To create partial label assignments for training data, for each label Cj , we expose the label assignment of Cj for ω% randomly sampled positive and negative training instances and keep the label assignment of Cj unknown for the rest of the training instances. To examine the performance of the proposed algorithm, we vary the ω% in the range {10%, 20%, 40%}. We repeat each experiment 10 times, and report the result averaged over 10 trials. The regularization parameter λ is selected from 2{−10,−9,...,9,10} by cross validation on training data for smaller data sets and set as 1 for larger ones. Parameters γ and ϵ are set to be 2 and 10−5, respectively, for the proposed algorithm, and the maximum number of iterations is set to be 100. The Average Precision [44], which measures the average number of relevant labels ranked before a particular relevant label, is computed over the test data (the metric on all the data is provided in the supplementary material) and used as our evaluation metric.\nWe compare the proposed Maxide method with MC-1 and MC-b, the state-of-the-art methods for transductive incomplete multi-label learning developed in [17]. In addition, we also compare with two reference methods for multi-label learning that train one binary classifier for each label; that is, the Binary Relevance method [7] based on Linear kernel (BR-L) and the method based on RBF kernel (BR-R), where the kernel width is set to 1. For the eleven data sets from “yahoo.com”,\nLIBSVM [13] is used by BR-L and BR-R to learn both a linear and nonlinear SVM classifier. For the two image data sets, due to their large size, only BR-L method is included in comparison and LIBLINEAR is used for the implementation of BR-L due to its high efficiency for large data sets. A similar strategy is used to determine the optimal λ as our proposal.\nResults Table 2 summarizes the results on transductive incomplete multi-label learning. We observe that the proposed Maxide algorithm outperforms the baseline methods, for most setting on several data sets (e.g., Business, Education, and Recreation), and the improvements are significant. More impressively, for most data sets, the proposed algorithm is three order faster than MC-1 and MC-b. For the NUS-WIDE data set, none of MC-1 and MC-b, the two existing matrix completion based algorithms for transductive incomplete multi-label learning, is able to finish within one week. For the Flickr data set, MC-1 and MC-b are not runnable due to the out of memory problem. For the NUS-WIDE and Flickr data sets, our proposed Maxide method gets an average of more than 50% improvement against BR-L, the only runnable baseline, on the Average Precision."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we develop the theory of matrix completion with side information. We show theoretically that, with side information matrices A ∈ Rn×ra and B ∈ Rm×rb , we can perfectly recover an n×m rank-r matrix with only O(r(ra + rb) ln(ra + rb) ln(n+m)) observed entries, a significant improvement compared to the sample complexity O(r(n+m) ln2(n+m)) for the standard theory for matrix completion. We present the Maxide algorithm that can efficiently solve the optimization problem for matrix completion with side information. Empirical studies with synthesized data sets and transductive incomplete multi-label learning show the promising performance of the proposed algorithm.\nAcknowledgement This research was partially supported by 973 Program (2010CB327903), NSFC (61073097, 61273301), and ONR Award (N000141210431)."
    } ],
    "references" : [ {
      "title" : "A new approach to collaborative filtering: Operator estimation with spectral regularization",
      "author" : [ "J. Abernethy", "F. Bach", "T. Evgeniou", "J.-P. Vert" ],
      "venue" : "JMLR, 10:803–826,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Incorporating side information in probabilistic matrix factorization with gaussian processes",
      "author" : [ "R. Adams", "G. Dahl", "I. Murray" ],
      "venue" : "UAI,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Regression-based latent factor models",
      "author" : [ "D. Agarwal", "B.-C. Chen" ],
      "venue" : "KDD,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "MLJ, 73(3):243–272,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Efficient and practical stochastic subgradient descent for nuclear norm regularization",
      "author" : [ "H. Avron", "S. Kale", "S. Kasiviswanathan", "V. Sindhwani" ],
      "venue" : "ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Consistency of trace norm minimization",
      "author" : [ "F. Bach" ],
      "venue" : "JMLR, 9:1019–1048,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning multi-label scene classification",
      "author" : [ "M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown" ],
      "venue" : "Pattern Recognition, 37(9):1757–1771,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Multi-label learning with incomplete class assignments",
      "author" : [ "S. Bucak", "R. Jin", "A. Jain" ],
      "venue" : "CVPR,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Matrix completion for multi-label image classification",
      "author" : [ "R. Cabral", "F. Torre", "J. Costeira", "A. Bernardino" ],
      "venue" : "NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "J.-F. Cai", "E. Candès", "Z. Shen" ],
      "venue" : "SIAM Journal on Optimization, 20(4):1956–1982,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E. Candès", "B. Recht" ],
      "venue" : "CACM, 55(6):111–119,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The power of convex relaxation: near-optimal matrix completion",
      "author" : [ "E. Candès", "T. Tao" ],
      "venue" : "IEEE TIT, 56(5):2053–2080,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Libsvm: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM TIST, 2(3):27,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Nus-wide: A real-world web image database from national university of singapore",
      "author" : [ "T.-S. Chua", "J. Tang", "R. Hong", "H. Li", "Z. Luo", "Y.-T. Zheng" ],
      "venue" : "CIVR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "High-rank matrix completion and subspace clustering with missing data",
      "author" : [ "B. Eriksson", "L. Balzano", "R. Nowak" ],
      "venue" : "CoRR,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Matrix co-factorization for recommendation with rich side information and implicit feedback",
      "author" : [ "Y. Fang", "L. Si" ],
      "venue" : "Proceedings of the 2nd International Workshop on Information Heterogeneity and Fusion in 8  Recommender Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Transduction with matrix completion: Three birds with one stone",
      "author" : [ "A. Goldberg", "X. Zhu", "B. Recht", "J.-M. Xu", "R. Nowak" ],
      "venue" : "NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Semi-supervised multi-label classification - a simultaneous large-margin, subspace learning approach",
      "author" : [ "Y. Guo", "D. Schuurmans" ],
      "venue" : "ECML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Provable matrix sensing using alternating minimization",
      "author" : [ "P. Jain", "P. Netrapalli", "S. Sanghavi" ],
      "venue" : "NIPS Workshop on Optimization for Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Clustering partially observed graphs via convex optimization",
      "author" : [ "A. Jalali", "Y. Chen", "S. Sanghavi", "H. Xu" ],
      "venue" : "ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Extracting shared subspace for multi-label classification",
      "author" : [ "S. Ji", "L. Tang", "S. Yu", "J. Ye" ],
      "venue" : "KDD,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "An accelerated gradient method for trace norm minimization",
      "author" : [ "S. Ji", "J. Ye" ],
      "venue" : "ICML,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Matrix completion from a few entries",
      "author" : [ "R. Keshavan", "A. Montanari", "S. Oh" ],
      "venue" : "IEEE TIT, 56(6):2980– 2998,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Transductive multi-label learning via label set propagation",
      "author" : [ "X. Kong", "M. Ng", "Z.-H. Zhou" ],
      "venue" : "IEEE TKDE, 25(3):704–719,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices",
      "author" : [ "Z. Lin", "M. Chen", "L. Wu", "Y. Ma" ],
      "venue" : "Technical report, UIUC,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Semi-supervised multi-label learning by constrained non-negative matrix factorization",
      "author" : [ "Y. Liu", "R. Jin", "L. Yang" ],
      "venue" : "AAAI,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Fixed point and bregman iterative methods for matrix rank minimization",
      "author" : [ "S. Ma", "D. Goldfarb", "L. Chen" ],
      "venue" : "Mathematical Programming, 128(1-2):321–353,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Spectral regularization algorithms for learning large incomplete matrices",
      "author" : [ "R. Mazumder", "T. Hastie", "R. Tibshirani" ],
      "venue" : "JMLR, 11:2287–2322,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Response prediction using collaborative filtering with hierarchies and side-information",
      "author" : [ "A. Menon", "K. Chitrapura", "S. Garg", "D. Agarwal", "N. Kota" ],
      "venue" : "KDD,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Estimation of (near) low-rank matrices with noise and high dimensional scaling",
      "author" : [ "S. Negahban", "M. Wainwright" ],
      "venue" : "Annual of Statistics, 39(2):1069–1097,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Joint covariate selection and joint subspace selection for multiple classification problems",
      "author" : [ "G. Obozinski", "B. Taskar", "M. Jordan" ],
      "venue" : "Statistics and Computing, 20(2):231–252,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Transfer learning in collaborative filtering for sparsity reduction",
      "author" : [ "W. Pan", "E. Xiang", "N. Liu", "Q. Yang" ],
      "venue" : "AAAI,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Bayesian matrix factorization with side information and dirichlet process mixtures",
      "author" : [ "I. Porteous", "A. Asuncion", "M. Welling" ],
      "venue" : "AAAI,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "B. Recht" ],
      "venue" : "JMLR, 12:3413–3430,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Fast maximum margin matrix factorization for collaborative prediction",
      "author" : [ "J. Rennie", "N. Srebro" ],
      "venue" : "ICML,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Estimation of high dimensional low rank matrices",
      "author" : [ "A. Rhode", "A. Tsybakov" ],
      "venue" : "Annual of Statistics, 39(2):887–930,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "N. Srebro", "Jason D. Rennie", "T. Jaakkola" ],
      "venue" : "In NIPS",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2005
    }, {
      "title" : "Multi-label learning with weak label",
      "author" : [ "Y.-Y. Sun", "Y. Zhang", "Z.-H. Zhou" ],
      "venue" : "AAAI,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems",
      "author" : [ "K.-C. Toh", "Y. Sangwoon" ],
      "venue" : "Pacific Journal of Optimization,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Parametric mixture models for multi-labeled text",
      "author" : [ "N. Ueda", "K. Saito" ],
      "venue" : "NIPS,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Unsupervised learning of image manifolds by semidefinite programming",
      "author" : [ "K. Weinberger", "L. Saul" ],
      "venue" : "IJCV, 70(1):77–90,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Robust ensemble clustering by matrix completion",
      "author" : [ "J. Yi", "T. Yang", "R. Jin", "A. Jain", "M. Mahdavi" ],
      "venue" : "ICDM,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Transductive multi-label ensemble classification for protein function prediction",
      "author" : [ "G. Yu", "C. Domeniconi", "H. Rangwala", "G. Zhang", "Z. Yu" ],
      "venue" : "KDD,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A two-view learning approach for image tag ranking",
      "author" : [ "J. Zhuang", "S. Hoi" ],
      "venue" : "WSDM,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "It has broad applications including collaborative filtering [35], dimensionality reduction [41], multi-class learning [4, 31], clustering [15, 42], etc.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 40,
      "context" : "It has broad applications including collaborative filtering [35], dimensionality reduction [41], multi-class learning [4, 31], clustering [15, 42], etc.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "It has broad applications including collaborative filtering [35], dimensionality reduction [41], multi-class learning [4, 31], clustering [15, 42], etc.",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : "It has broad applications including collaborative filtering [35], dimensionality reduction [41], multi-class learning [4, 31], clustering [15, 42], etc.",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "It has broad applications including collaborative filtering [35], dimensionality reduction [41], multi-class learning [4, 31], clustering [15, 42], etc.",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 41,
      "context" : "It has broad applications including collaborative filtering [35], dimensionality reduction [41], multi-class learning [4, 31], clustering [15, 42], etc.",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 10,
      "context" : "Recent studies show that, with a high probability, we can efficiently recover a matrix M ∈ Rn×m of rank r from O(r(n+m) ln(2)(n+ m)) observed entries when the observed entries are uniformly sampled from M [11, 12, 34].",
      "startOffset" : 205,
      "endOffset" : 217
    }, {
      "referenceID" : 11,
      "context" : "Recent studies show that, with a high probability, we can efficiently recover a matrix M ∈ Rn×m of rank r from O(r(n+m) ln(2)(n+ m)) observed entries when the observed entries are uniformly sampled from M [11, 12, 34].",
      "startOffset" : 205,
      "endOffset" : 217
    }, {
      "referenceID" : 33,
      "context" : "Recent studies show that, with a high probability, we can efficiently recover a matrix M ∈ Rn×m of rank r from O(r(n+m) ln(2)(n+ m)) observed entries when the observed entries are uniformly sampled from M [11, 12, 34].",
      "startOffset" : 205,
      "endOffset" : 217
    }, {
      "referenceID" : 9,
      "context" : "In particular, although a number of algorithms have been developed for matrix completion [10, 22, 23, 25, 27, 28, 39], most of them require updating the full matrix M at each iteration of optimization, leading to a high computational cost and a large storage requirement when both n and m are large.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "In particular, although a number of algorithms have been developed for matrix completion [10, 22, 23, 25, 27, 28, 39], most of them require updating the full matrix M at each iteration of optimization, leading to a high computational cost and a large storage requirement when both n and m are large.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 22,
      "context" : "In particular, although a number of algorithms have been developed for matrix completion [10, 22, 23, 25, 27, 28, 39], most of them require updating the full matrix M at each iteration of optimization, leading to a high computational cost and a large storage requirement when both n and m are large.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 24,
      "context" : "In particular, although a number of algorithms have been developed for matrix completion [10, 22, 23, 25, 27, 28, 39], most of them require updating the full matrix M at each iteration of optimization, leading to a high computational cost and a large storage requirement when both n and m are large.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 26,
      "context" : "In particular, although a number of algorithms have been developed for matrix completion [10, 22, 23, 25, 27, 28, 39], most of them require updating the full matrix M at each iteration of optimization, leading to a high computational cost and a large storage requirement when both n and m are large.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 27,
      "context" : "In particular, although a number of algorithms have been developed for matrix completion [10, 22, 23, 25, 27, 28, 39], most of them require updating the full matrix M at each iteration of optimization, leading to a high computational cost and a large storage requirement when both n and m are large.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 38,
      "context" : "In particular, although a number of algorithms have been developed for matrix completion [10, 22, 23, 25, 27, 28, 39], most of them require updating the full matrix M at each iteration of optimization, leading to a high computational cost and a large storage requirement when both n and m are large.",
      "startOffset" : 89,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Several recent efforts [5, 19] try to address this issue, at a price of losing performance guarantee in recovering the target matrix.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 18,
      "context" : "Several recent efforts [5, 19] try to address this issue, at a price of losing performance guarantee in recovering the target matrix.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 34,
      "context" : "• Collaborative filtering aims to predict ratings of individual users based on the ratings from other users [35].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 19,
      "context" : "• Link prediction aiming to predict missing links between users in a social network based on the existing ones can be viewed as a matrix completion problem [20], where side information, such as attributes of users (e.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 0,
      "context" : "Although several studies exploit side information for matrix recovery [1, 2, 3, 16, 29, 32, 33], most of them focus on matrix factorization techniques, which usually result in non-convex optimization problems without guarantee of perfectly recovering the target matrix.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "Although several studies exploit side information for matrix recovery [1, 2, 3, 16, 29, 32, 33], most of them focus on matrix factorization techniques, which usually result in non-convex optimization problems without guarantee of perfectly recovering the target matrix.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 2,
      "context" : "Although several studies exploit side information for matrix recovery [1, 2, 3, 16, 29, 32, 33], most of them focus on matrix factorization techniques, which usually result in non-convex optimization problems without guarantee of perfectly recovering the target matrix.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "Although several studies exploit side information for matrix recovery [1, 2, 3, 16, 29, 32, 33], most of them focus on matrix factorization techniques, which usually result in non-convex optimization problems without guarantee of perfectly recovering the target matrix.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : "Although several studies exploit side information for matrix recovery [1, 2, 3, 16, 29, 32, 33], most of them focus on matrix factorization techniques, which usually result in non-convex optimization problems without guarantee of perfectly recovering the target matrix.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 31,
      "context" : "Although several studies exploit side information for matrix recovery [1, 2, 3, 16, 29, 32, 33], most of them focus on matrix factorization techniques, which usually result in non-convex optimization problems without guarantee of perfectly recovering the target matrix.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 32,
      "context" : "Although several studies exploit side information for matrix recovery [1, 2, 3, 16, 29, 32, 33], most of them focus on matrix factorization techniques, which usually result in non-convex optimization problems without guarantee of perfectly recovering the target matrix.",
      "startOffset" : 70,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "We demonstrate the effectiveness of matrix completion with side information in transductive incomplete multi-label learning [17], which aims to assign multiple labels to individual instances in a transductive learning setting.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 36,
      "context" : "Early work on matrix completion, also referred to as maximum margin matrix factorization [37], was developed for collaborative filtering.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 10,
      "context" : "Theoretical studies show that, it is sufficient to perfectly recover a matrix M ∈ Rn×m of rank r when the number of observed entries is O(r(n + m) ln(2)(n + m)) [11, 12, 34].",
      "startOffset" : 161,
      "endOffset" : 173
    }, {
      "referenceID" : 11,
      "context" : "Theoretical studies show that, it is sufficient to perfectly recover a matrix M ∈ Rn×m of rank r when the number of observed entries is O(r(n + m) ln(2)(n + m)) [11, 12, 34].",
      "startOffset" : 161,
      "endOffset" : 173
    }, {
      "referenceID" : 33,
      "context" : "Theoretical studies show that, it is sufficient to perfectly recover a matrix M ∈ Rn×m of rank r when the number of observed entries is O(r(n + m) ln(2)(n + m)) [11, 12, 34].",
      "startOffset" : 161,
      "endOffset" : 173
    }, {
      "referenceID" : 29,
      "context" : "A more general matrix recovery problem, referred to as matrix regression, was examined in [30, 36].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 35,
      "context" : "A more general matrix recovery problem, referred to as matrix regression, was examined in [30, 36].",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "Several computational algorithms [10, 22, 23, 25, 27, 28, 39] have been developed to efficiently solve the optimization problem of matrix completion.",
      "startOffset" : 33,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "Several computational algorithms [10, 22, 23, 25, 27, 28, 39] have been developed to efficiently solve the optimization problem of matrix completion.",
      "startOffset" : 33,
      "endOffset" : 61
    }, {
      "referenceID" : 22,
      "context" : "Several computational algorithms [10, 22, 23, 25, 27, 28, 39] have been developed to efficiently solve the optimization problem of matrix completion.",
      "startOffset" : 33,
      "endOffset" : 61
    }, {
      "referenceID" : 24,
      "context" : "Several computational algorithms [10, 22, 23, 25, 27, 28, 39] have been developed to efficiently solve the optimization problem of matrix completion.",
      "startOffset" : 33,
      "endOffset" : 61
    }, {
      "referenceID" : 26,
      "context" : "Several computational algorithms [10, 22, 23, 25, 27, 28, 39] have been developed to efficiently solve the optimization problem of matrix completion.",
      "startOffset" : 33,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : "Several computational algorithms [10, 22, 23, 25, 27, 28, 39] have been developed to efficiently solve the optimization problem of matrix completion.",
      "startOffset" : 33,
      "endOffset" : 61
    }, {
      "referenceID" : 38,
      "context" : "Several computational algorithms [10, 22, 23, 25, 27, 28, 39] have been developed to efficiently solve the optimization problem of matrix completion.",
      "startOffset" : 33,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "This issue has been addressed in several recent studies [5, 19], where the key idea is to store and update the low rank factorization of the target matrix.",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "This issue has been addressed in several recent studies [5, 19], where the key idea is to store and update the low rank factorization of the target matrix.",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "A preliminary convergence analysis is given in [19], however, none of these approaches guarantees perfect recovery of the target matrix, even with significantly large number of observed entries.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "[2, 3, 29, 33] are based on graphical models by assuming special distribution of latent factors; these algorithms, as well as [16] and [32], consider side information in matrix factorization.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 2,
      "context" : "[2, 3, 29, 33] are based on graphical models by assuming special distribution of latent factors; these algorithms, as well as [16] and [32], consider side information in matrix factorization.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 28,
      "context" : "[2, 3, 29, 33] are based on graphical models by assuming special distribution of latent factors; these algorithms, as well as [16] and [32], consider side information in matrix factorization.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 32,
      "context" : "[2, 3, 29, 33] are based on graphical models by assuming special distribution of latent factors; these algorithms, as well as [16] and [32], consider side information in matrix factorization.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 15,
      "context" : "[2, 3, 29, 33] are based on graphical models by assuming special distribution of latent factors; these algorithms, as well as [16] and [32], consider side information in matrix factorization.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 31,
      "context" : "[2, 3, 29, 33] are based on graphical models by assuming special distribution of latent factors; these algorithms, as well as [16] and [32], consider side information in matrix factorization.",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "Matrix completion with infinite dimensional side information was exploited in [1],",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "The simplest approach for multi-label learning is to train one binary model for each label, which is also referred to as BR (Binary Relevance) [7].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 16,
      "context" : "In this work, we will evaluate our proposed approach by transductive incomplete multi-label learning [17].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "This is in contrast to many studies on common semi-supervised or transductive multi-label learning [18, 24, 26, 43] where each labeled instance receives a complete set of label assignments.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "This is in contrast to many studies on common semi-supervised or transductive multi-label learning [18, 24, 26, 43] where each labeled instance receives a complete set of label assignments.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : "This is in contrast to many studies on common semi-supervised or transductive multi-label learning [18, 24, 26, 43] where each labeled instance receives a complete set of label assignments.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 42,
      "context" : "This is in contrast to many studies on common semi-supervised or transductive multi-label learning [18, 24, 26, 43] where each labeled instance receives a complete set of label assignments.",
      "startOffset" : 99,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "This is also different from multi-label learning with weak labels [8, 38] which assumes that only the positive labels can be observed.",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 37,
      "context" : "This is also different from multi-label learning with weak labels [8, 38] which assumes that only the positive labels can be observed.",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "In [17], a matrix completion based approach was proposed for transductive incomplete multi-label learning.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "Two algorithms MC-b and MC-1 were presented there, differing only in the treatment of bias term, whereas the convergence of MC-1 was examined in [9].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 16,
      "context" : "To understand the implication of this assumption, let us consider the problem of transductive incomplete multi-label learning [17], where the objective is to complete the instance-label matrix based on the observed entries corresponding to the given label assignments, and the side information matrices A and B are given by the feature vectors of instances and the label correlation matrix, respectively.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 10,
      "context" : "Following the standard theory for matrix completion [11, 12, 34], we can cast the matrix completion task into the following optimization problem:",
      "startOffset" : 52,
      "endOffset" : 64
    }, {
      "referenceID" : 11,
      "context" : "Following the standard theory for matrix completion [11, 12, 34], we can cast the matrix completion task into the following optimization problem:",
      "startOffset" : 52,
      "endOffset" : 64
    }, {
      "referenceID" : 33,
      "context" : "Following the standard theory for matrix completion [11, 12, 34], we can cast the matrix completion task into the following optimization problem:",
      "startOffset" : 52,
      "endOffset" : 64
    }, {
      "referenceID" : 33,
      "context" : "Compared to the standard matrix completion theory [34], the side information matrices reduce sample complexity from O(r(n+m) ln(2)(n+m)) to O(r(ra+ rb) ln(ra+ rb) lnn).",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 33,
      "context" : "Note that although we follow the framework of [34] for analysis, namely first proving the result under deterministic conditions, and then showing that the deterministic conditions hold with a high probability, our technical proof is quite different due to the involvement of side information matrices A and B.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "Similar to the Singular Vector Thresholding (SVT) method [10], we approximate the problem in ( 2) by an unconstrained optimization problem, i.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "This assumption has been explored extensively in the previous studies of multi-label learning [17, 21, 38].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 20,
      "context" : "This assumption has been explored extensively in the previous studies of multi-label learning [17, 21, 38].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 37,
      "context" : "This assumption has been explored extensively in the previous studies of multi-label learning [17, 21, 38].",
      "startOffset" : 94,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "Both ra and rb of side information matrices are set to be 2r, and |Ω|, the number of observed entries, is set to be r(2n− r), which is significantly smaller than the number of observed entries used in previous studies [10, 25, 27].",
      "startOffset" : 218,
      "endOffset" : 230
    }, {
      "referenceID" : 24,
      "context" : "Both ra and rb of side information matrices are set to be 2r, and |Ω|, the number of observed entries, is set to be r(2n− r), which is significantly smaller than the number of observed entries used in previous studies [10, 25, 27].",
      "startOffset" : 218,
      "endOffset" : 230
    }, {
      "referenceID" : 26,
      "context" : "Both ra and rb of side information matrices are set to be 2r, and |Ω|, the number of observed entries, is set to be r(2n− r), which is significantly smaller than the number of observed entries used in previous studies [10, 25, 27].",
      "startOffset" : 218,
      "endOffset" : 230
    }, {
      "referenceID" : 9,
      "context" : "We compare the proposed Maxide algorithm with three state-of-the-art matrix completion algorithms: Singular Vector Thresholding (SVT) [10], Fixed Point Bregman Iterative Method (FPCA) [27] and Augmented Lagrangian Method (ALM) [25].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 26,
      "context" : "We compare the proposed Maxide algorithm with three state-of-the-art matrix completion algorithms: Singular Vector Thresholding (SVT) [10], Fixed Point Bregman Iterative Method (FPCA) [27] and Augmented Lagrangian Method (ALM) [25].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 24,
      "context" : "We compare the proposed Maxide algorithm with three state-of-the-art matrix completion algorithms: Singular Vector Thresholding (SVT) [10], Fixed Point Bregman Iterative Method (FPCA) [27] and Augmented Lagrangian Method (ALM) [25].",
      "startOffset" : 227,
      "endOffset" : 231
    }, {
      "referenceID" : 5,
      "context" : "In addition to these matrix completion methods, we also compare with a trace norm minimizing method (TraceMin) [6].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 39,
      "context" : "com” [40], and two image classification data sets NUS-WIDE [14] and Flickr [45].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : "com” [40], and two image classification data sets NUS-WIDE [14] and Flickr [45].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 43,
      "context" : "com” [40], and two image classification data sets NUS-WIDE [14] and Flickr [45].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 39,
      "context" : "Detailed information of these eleven data sets can be found in [40].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "We compare the proposed Maxide method with MC-1 and MC-b, the state-of-the-art methods for transductive incomplete multi-label learning developed in [17].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "In addition, we also compare with two reference methods for multi-label learning that train one binary classifier for each label; that is, the Binary Relevance method [7] based on Linear kernel (BR-L) and the method based on RBF kernel (BR-R), where the kernel width is set to 1.",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "LIBSVM [13] is used by BR-L and BR-R to learn both a linear and nonlinear SVM classifier.",
      "startOffset" : 7,
      "endOffset" : 11
    } ],
    "year" : 2013,
    "abstractText" : "In standard matrix completion theory, it is required to have at least O(n ln n) observed entries to perfectly recover a low-rank matrix M of size n × n, leading to a large number of observations when n is large. In many real tasks, side information in addition to the observed entries is often available. In this work, we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries. We show that, under appropriate conditions, with the assistance of side information matrices, the number of observed entries needed for a perfect recovery of matrix M can be dramatically reduced to O(lnn). We demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning.",
    "creator" : null
  }
}