{
  "name" : "6e0721b2c6977135b916ef286bcb49ec.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Near-Optimal Entrywise Sampling for Data Matrices",
    "authors" : [ "Dimitris Achlioptas", "Zohar Karnin" ],
    "emails" : [ "optas@cs.ucsc.edu", "zkarnin@ymail.com", "edo.liberty@ymail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Given an m n matrix A, it is often desirable to find a sparser matrix B that is a good proxy for A. Besides being a natural mathematical question, such sparsification has become a ubiquitous preprocessing step in a number of data analysis operations including approximate eigenvector computations [AM01, AHK06, AM07], semi-definite programming [AHK05, d’A08], and matrix completion problems [CR09, CT10].\nA fruitful measure for the approximation of A by B is the spectral norm of A B, where for any matrix C its spectral norm is defined as C 2 max\nx 2 1 Cx 2. Randomization has been central in the context of matrix approximations and the overall problem is typically cast as follows: given a matrix A and a budget s, devise a distribution over matrices B such that the (expected) number of non-zero entries in B is at most s and A B 2 is as small as possible.\nOur work is motivated by big data matrices that are generated by measurement processes. Each of the n matrix columns correspond to an observation of m attributes. Thus, we expect n m. Also we expect the total number of non-zero entries in A to exceed available memory. We assume that the original data matrix A is accessed in the streaming model where we know only very basic features of A a priori and the actual non-zero entries are presented to us one at a time in an arbitrary order. The streaming model is especially important for tasks like recommendation engines where user-item preferences become available one by one in an arbitrary order. But, it is also important in cases when A exists in durable storage and random access of its entries is prohibitively expensive.\nWe establish that for such matrices the following approach gives provably near-optimal sparsification. Assign to each element A\nij of the matrix a weight that depends only on the elements in its row q\nij\nA ij A i 1. Take ⇢ to be an (appropriate) distribution over the rows. Sample s i.i.d. locations from A using the distribution p\nij ⇢ i q ij . Return B which is the mean of s matrices, each containing a single non zero entry A\nij p ij in the corresponding selected location i, j .\nAs we will see, this simple form of the probabilities p ij falls out naturally from generic optimization considerations. The fact that each entry is kept with probability proportional to its magnitude, be-\nsides being interesting on its own right, has a remarkably practical implication. Every non-zero in the i-th row of B will take the form k\nij A i 1 s⇢i where kij is the number of times location i, j of A was selected. Note that since we sample with replacement k\nij may be more than 1 but, typically, k ij\n0, 1 . The result is a matrix B which is representable in O m log n s log n s bits. This is because there is no reason to store floating point matrix entry values. We use O m log n bits to store1 all values A\ni 1 s⇢i and O s log n s bits to store the non zero index offsets. Note that k\nij s and that some of the offsets may be zero. In a simple experiment we measured the average number of bits per sample resulting from this approach (total size of the sketch divided by the number of samples s). The results were between 5 and 22 bits per sample depending on the matrix and s. It is important to note that the number of bits per sample was usually less than even log2 n log2 m , the minimal number of bits required to represent a pair i, j . Our experiments show a reduction of disc space by a factor of between 2 and 5 relative to the compressed size of the file representing the sample matrix B in the standard row-column-value list format.\nAnother insight of our work is that the distributions we propose are combinations of two L1-based distributions and and which distribution is more dominant depends on the sampling budget. When the number of samples s is small, ⇢\ni is nearly linear in A i 1 resulting in pij Aij . However, as the number of samples grows, ⇢\ni tends towards A i 2 1 resulting in pij Aij A i 1, a distribution\nwe refer to as Row-L1 sampling. The dependence of the preferred distribution on the sample budget is also borne out in experiments, with sampling based on appropriately mixed distributions being consistently best. This highlights that the need to adapt the sampling distribution to the sample budget is a genuine phenomenon."
    }, {
      "heading" : "2 Measure of Error and Related Work",
      "text" : "We measure the difference between A and B with respect to the L2 (spectral) norm as it is highly revealing in the context of data analysis. Let us define a linear trend in the data of A as any tendency of the rows to align with a particular unit vector x. To examine the presence of such a trend, we need only multiply A with x: the ith coordinate of Ax is the projection of the ith row of A onto x. Thus, Ax 2 measures the strength of linear trend x in A, and A 2 measures the strongest linear trend in A. Thus, minimizing A B 2 minimizes the strength of the strongest linear trend of A not captured by B. In contrast, measuring the difference using an entry-wise norm, e.g., the Frobenius norm, can be completely uninformative. This is because the best strategy would be to always pick the largest s matrix entries from A, a strategy that can easily be “fooled”. As a stark example, when the matrix entries are A\nij 0, 1 , the quality of approximation of A by B is completely independent of which elements of A we keep. This is clearly bad; as long as A contains even a modicum of structure certain approximations will be far better than others.\nBy using the spectral norm to measure error we get a natural and sophisticated target: to minimize A B 2 is to make E A B a near-rotation, having only small variations in the amount by which it stretches different vectors. This idea that the error matrix E should be isotropic, thus packing as much Frobenius norm as possible for its L2 norm, motivated the first work on element-wise matrix sampling by Achlioptas and McSherry [AM07]. Concretely, to minimize E 2 it is natural to aim for a matrix E that is both zero-mean, i.e., an unbiased estimator of A, and whose entries are formed by sampling the entries of A (and, thus, of E) independently. In the work of [AM07], E is a matrix of i.i.d. zero-mean random variables. The study of the spectral characteristics of such matrices goes back all the way to Wigner’s famous semi-circle law [Wig58]. Specifically, to bound E 2 in [AM07] a bound due to Alon Krivelevich and Vu [AKV02] was used, a refinement of a bound by Juhász [Juh81] and Füredi and Komlós [FK81]. The most salient feature of that bound is that it depends on the maximum entry-wise variance 2 of A B, and therefore the distribution optimizing the bound is the one in which the variance of all entries in E is the same. In turn, this means keeping each entry of A independently with probability p\nij A2 ij (up to a small wrinkle discussed below).\nSeveral papers have since analyzed L2-sampling and variants [NDT09, NDT10, DZ11, GT09, AM07]. An inherent difficulty of L2-sampling based strategies is the need for special handling of small entries. This is because when each item A\nij is kept with probability p ij A2 ij , the result-\n1It is harmless to assume any value in the matrix is kept using O log n bits of precision. Otherwise, truncating the trailing bits can be shown to be negligible.\ning entry B ij in the sample matrix has magnitude A ij p ij A ij 1. Thus, if an extremely small element A\nij is accidentally picked, the largest entry of the sample matrix “blows up”. In [AM07] this was addressed by sampling small entries with probability proportional to A\nij rather than A2 ij . In the work of Gittens and Tropp [GT09], small entries are not handled separately and the bound derived depends on the ratio between the largest and the smallest non-zero magnitude.\nRandom matrix theory has witnessed dramatic progress in the last few years and [AW02, RV07, Tro12a, Rec11] provide a good overview of the results. This progress motivated Drineas and Zouzias in [DZ11] to revisit L2-sampling using concentration results for sums of random matrices [Rec11], as we do here. This is somewhat different from the original setting of [AM07] since now B is not a random matrix with independent entries, but a sum of many single-element independent matrices, each such matrix resulting by choosing a location of A with replacement. Their work improved upon all previous L2-based sampling results and also upon the L1-sampling result of Arora, Hazan and Kale [AHK06], discussed below, while admitting a remarkably compact proof. The issue of small entries was handled in [DZ11] by deterministically discarding all sufficiently small entries, a strategy that gives a strong mathematical guarantee (but see the discussion regarding deterministic truncation in the experimental section).\nA completely different tack at the problem, avoiding random matrix theory altogether, was taken by Arora et al. [AHK06]. Their approximation keeps the largest entries in A deterministically (specifically all A\nij \" n where the threshold \" needs be known a priori) and randomly rounds the remaining smaller entries to sign A\nij \" n or 0. They exploit the simple fact A B sup\nx 1, y 1 x T A B y by noting that, as a scalar quantity, its concentration around its expectation can be established by standard Bernstein-Bennet type inequalities. A union bound then allows them to prove that with high probability, xT A B y \" for every x and y. The result of [AHK06] admits a relatively simple proof. However, it also requires a truncation that depends on the desired approximation \". Rather interestingly, this time the truncation amounts to keeping every entry larger than some threshold."
    }, {
      "heading" : "3 Our Approach",
      "text" : "Following the discussion in Section 2 and in line with previous works, we: (i) measure the quality of B by A B 2, (ii) sample the entries of A independently, and (iii) require B to be an unbiased estimator of A. We are therefore left with the task of determining a good probability distribution p\nij\nfrom which to sample the entries of A in order to get B. As discussed in Section 2 prior art makes heavy use of beautiful results in the theory of random matrices. Specifically, each work proposes a specific sampling distribution and then uses results from random matrix theory to demonstrate that it has good properties. In this work we reverse the approach, aiming for its logical conclusion. We start from a cornerstone result in random matrix theory and work backwards to reverse-engineer nearoptimal distributions with respect to the notion of probabilistic deviations captured by the inequality. The inequality we use is the Matrix-Bernstein inequality for sums of independent random matrices (see e.g., [Tro12b], Theorem 1.6). In the following, we often denote A 2 as A to lighten notation.\nTheorem 3.1 (Matrix Bernstein inequality). Consider a finite sequence X i of i.i.d. random m n matrices, where E X1 0 and X1 R. Let 2 max E X1XT1 , E XT1 X1 . For some fixed s 1, let X X1 Xs s. For all \" 0,\nPr X \" m n exp s\"2\n2 R\" 3 .\nTo get a feeling for our approach, fix any probability distribution p over the non-zero elements of A. Let B be a random m n matrix with exactly one non-zero element, formed by sampling an element A\nij of A according to p and letting B ij A ij p ij . Observe that for every i, j , regardless of the choice of p, we have E B\nij\nA ij , and thus B is always an unbiased estimator of A. Clearly, the same is true if we repeat this s times, taking i.i.d. samples B1, . . . , Bs, and let our matrix B be their average. With this approach in mind, the goal is now to find a distribution p minimizing E A B1 Bs s . Writing sE A B1 A Bs we see that sE is the operator norm of a sum of i.i.d. zero-mean random matrices X i A B i , i.e., exactly the setting\nof Theorem 3.1. The relevant parameters are\n2 max E A B1 A B1 T , E A B1 T A B1 (1) R max A B1 over all possible realizations of B1 . (2)\nEquations (1) and (2) mark the starting point of our work. Our goal is to find probability distributions over the elements of A that optimize (1) and (2) simultaneously with respect to their functional form in Theorem 3.1, thus yielding the strongest possible bound on A B . A conceptual contribution of our work is the discovery that good distributions depend on the sample budget s, a fact also borne out in experiments. The fact that minimizing the deviation metric of Theorem 3.1, i.e., 2 R✏ 3, suffices to bring out this dependence can be viewed as testament to the theorem’s sharpness.\nTheorem 3.1 is stated as a bound on the probability that the norm of the error matrix is greater than some target error \" given the number of samples s. In practice, the target error \" is typically not known in advance, but rather is the quantity to minimize, given the matrix A, the number of samples s, and the target confidence . Specifically, for any given distribution p on the elements of A, define\n\"1 p inf \" : m n exp s\"2\np 2 R p \" 3 . (3)\nOur goal in the rest of the paper is to seek the distribution p minimizing \"1. Our result is an easily computable distribution p which comes within a factor of 3 of \"1 p and, as a result, within a factor of 9 in terms of sample complexity (in practice we expect this to be even smaller, as the factor of 3 comes from consolidating bounds for a number of different worst-case matrices). To put this in perspective note that the definition of p does not place any restriction either on the access model for A while computing p , or on the amount of time needed to compute p . In other words, we are competing against an oracle which in order to determine p has all of A in its purview at once and can spend an unbounded amount of computation to determine it.\nIn contrast, the only global information regarding A we require are the ratios between the L1 norms of the rows of the matrix. Trivially, the exact L1 norms of the rows (and therefore their ratios) can be computed in a single pass over the matrix, yielding a 2-pass algorithm. Slightly less trivially, standard concentration arguments imply that these ratios can be estimated very well by sampling only a small number of columns. In the setting of data analysis, though, it is in fact reasonable to expect that good estimates of these ratios are available a priori. This is because different rows correspond to different attributes and the ratios between the row norms reflect the ratios between the average absolute values of the features. For example, if the matrix corresponds to text documents, knowing the ratios amounts to knowing global word frequencies. Moreover these ratios do not need to be known exactly to apply the algorithm, as even rough estimates of them give highly competitive results. Indeed, even disregarding this issue completely and simply assuming that all ratios equal 1, yields an algorithm that appears quite competitive in practice, as demonstrated by our experiments."
    }, {
      "heading" : "4 Data Matrices and Statement of Results",
      "text" : "Throughout A i and A j will denote the i-th row and j-th column of A, respectively. Also, we use the notation A 1\ni,j\nA ij and A 2 F\ni,j\nA2 ij . Before we formally state our result we introduce a definition that expresses the class of matrices for which our results hold. Definition 4.1. An m n matrix A is a Data matrix if:\n1. min i A i 1 maxj A j 1. 2. A 21 A 2 2 30m. 3. m 30.\nRegarding Condition 1, recall that we think of A as being generated by a measurement process of a fixed number of attributes (rows), each column corresponding to an observation. As a result, columns have bounded L1 norm, i.e., A j 1 constant. While this constant may depend on the type of object and its dimensionality, it is independent of the number of objects. On the other hand, A\ni 1 grows linearly with the number of columns (objects). As a result, we can expect Definition 4.1 to hold for all large enough data sets. Regarding Condition 2, it is easy to verify that\nunless the values of the entries of A exhibit unbounded variance as n grows, the ratio A 21 A 22 grows as ⌦ n and Condition 2 follows from n m. Condition 3 is trivial. All in all, out of the three conditions the essential one is Condition 1. The other two are merely technical and hold in all non-trivial cases where Condition 1 applies.\nOne last point is that to apply Theorem 3.1, the entries of A must be sampled with replacement. A simple way to achieve this in the streaming model was presented in [DKM06] that uses O s operations per matrix element and O s active memory. In Section D (see supplementary material) we discuss how to implement sampling with replacement far more efficiently, using O log s active memory, ˜O s space, and O 1 operations per element. To simplify the exposition of our algorithm, below, we describe it in the non-streaming setting. That is, we assume we know m and n and that we can compute numbers z\ni A i 1 as well as repeatedly sample entries from the matrix. We stress, however, that these conditions are not required and that the algorithm can be implemented efficiently in the streaming model as discussed in Section D.\nAlgorithm 1 Construct a sketch B of a data matrix A 1: Input: Data matrix A Rm n, sampling budget s, acceptable failure probability 2: Set ⇢ COMPUTEROWDISTRIBUTION(A, s, ) 3: Sample s elements of A with replacement, each A\nij having probability p ij ⇢ i A ij A i 1\n4: For each sample i, j, A ij ` , let B ` be the matrix with B ` i, j A ij p ij and zero elsewhere. 5: Output: B 1\ns\ns ` 1 B`.\n6: function COMPUTEROWDISTRIBUTION(A, s, ) 7: Obtain z such that z\ni A i 1 for i m 8: Set ↵ log m n s and log m n 3s\n9: Define ⇢ i ⇣ ↵z i 2⇣ ↵z i 2⇣ 2 z i ⇣ 2\n10: Find ⇣1 such that m i 1 ⇢i ⇣1 1 11: return ⇢ such that ⇢\ni ⇢ i ⇣1 for i m\nSteps 6–11 compute a distribution ⇢ over the rows. Assuming step 7 can be implemented efficiently (or skipped altogether in the case z are known a priori), we see that the running time of ComputeRowDistribution is independent of n. Specifically, finding ⇣1 in step 10 can be done efficiently by binary search because the function\ni\n⇢ i ⇣ is strictly decreasing in ⇣. Conceptually, we see that the probability assigned to each element A\nij in Step 3 is simply the probability ⇢ i of its row times its intra-row weight A\nij A i 1.\nWe are now able to state our main lemma. We defer its proof to Section 5 and subsequent details to appendices (see supplementary material).\nTheorem 4.2. If A is a Data matrix per Definition 4.1 and p is the probability distribution defined in Algorithm 1, then \"1 p 3 \"1 p , where p is the minimizer of \"1.\nTo compare our result with previous ones we first define several matrix metrics. We then state the bound implied by Theorem 4.2 on the minimal number of samples s0 needed by our algorithm to achieve an approximation B to the matrix A such that A B \" A with constant probability.\nStable rank: Denoted as sr and defined as A 2 F A 22. This is a smooth analog for the algebraic rank, always bounded by it from above, and resilient to small perturbations of the matrix. For data matrices we expect it is small, even constant, and that it captures the “inherent dimensionality” of the data.\nNumeric density: Denoted as nd and defined as A 21 A 2 F , this is a smooth analog of the number of non-zero entries nnz A . For 0-1 matrices it equals nnz A , but when there is variance in the magnitude of the entries it is smaller.\nNumeric row density: Denoted as nrd and defined as i A i 2 1 A 2 F n. In practice, it is often close to the average numeric density of a single row, a quantity typically much smaller than n.\nTheorem 4.3. Let A be a Data Matrix per Definition 4.1 and let B be the matrix returned by Algorithm 1 for 1 10, \" 0 and any\ns s0 ⇥ nrd sr \" 2 log n sr nd \"2 log n 1 2 .\nWith probability at least 9 10, A B \" A .\nThe proof of Theorem 4.3 is given in Appendix C (see supplementary material).\nThe third column of the table below shows the number of samples needed to guarantee that A B \" A occurs with constant probability, in terms of the matrix metrics defined above. The fourth column presents the ratio of the samples needed by previous results divided by the samples needed by our method. (To simplify the expressions, we present the ratio between our bound and [AHK06] only when the result of [AHK06] gives superior bounds to [DZ11], i.e., we always compare our bound to the stronger of the two bounds implied by these works). Holding \" and the stable rank constant we readily see that our method requires roughly 1 n the samples needed by [AHK06]. In the comparison with [DZ11] we see that the key parameter is the ratio nrd n, a quantity typically much smaller than 1 for data matrices. As a point of reference for the assumptions, in the experimental Section 6 we provide the values of all relevant matrix metrics for all the real data matrices we worked with, wherein the ratio nrd n is typically around 10 2. By this discussion, one would expect that L2-sampling should fare better than L1-sampling in experiments. As we will see, quite the opposite is true. A potential explanation for this phenomenon is the relative looseness of the bound of [AHK06] for the performance of L1-sampling.\nCitation Method Number of samples needed Improvement ratio of Theorem 4.3\n[AM07] L1, L2 sr n \"2 n polylog n\n[DZ11] L2 sr n \"2 log n nrd n nd n \" sr log n\n[AHK06] L1 nd n \"2 1 2 sr log n n\nThis paper Bernstein nrd sr \" 2 log n sr nd \"2 log n 1 2"
    }, {
      "heading" : "5 Proof outline",
      "text" : "We start by iteratively replacing the objective functions (1) and (2) with simpler and simpler functions. Each replacement will incur a (small) loss in accuracy but will bring us closer to a function for which we can give a closed form solution. Recalling the definitions of ↵, from Algorithm 1 and rewriting the requirement in (3) as a quadratic form in \" gives \"2 \" R ↵ 2 0. Our first step is to observe that for any c, d 0, the equation \"2 \" c d 0 has one negative and one positive solution and that the latter is at least c d 2 and at most c d. Therefore, if we define2 \"2 : ↵ R we see that 1 2 \"1 \"2 1.\nOur next simplification encompasses Conditions 2, 3 of Definition 4.1. Let \"3 : ↵̃ ˜R where\ñ2 : max max i\nj\nA2 ij p ij\n, max j\ni\nA2 ij p ij\nand ˜R : max ij A ij p ij .\nLemma 5.1. For every matrix A satisfying Conditions 2 and 3 of Definition 4.1, for every probability distribution on the elements of A, \"2 \"3 1 1 30.\nLemma 5.1 is proved in section A (see supplementary material) by showing that ̃ and ˜R R. This allows us to optimize p with respect to \"3 instead of \"2. In minimizing \"3 we see that there is freedom to use different rows to optimize ̃ and ˜R. At a cost of a factor of 2, we will couple the two\n2Here and in the following, to lighten notation, we will omit all arguments, i.e., p, p ,R p , from the objective functions \"i we seeks to optimize, as they are readily understood from context.\nminimizations by minimizing \"4 max \"5, \"6 where\n\"5 : max i ↵ j\nA2 ij\np ij\nmax j\nA ij\np ij\n, \"6 : max j ↵ i\nA2 ij\np ij\nmax i\nA ij\np ij\n. (4)\nNote that the maximization of ˜R in \"5 (and \"6) is coupled with that of the ̃-related term by constraining the optimization to consider only one row (column) at a time. Clearly, 1 \"3 \"4 2.\nNext we focus on \"5, the first term in the maximization of \"4. The following key lemma establishes that for all data matrices satisfying Condition 1 of Definition 4.1, by minimizing \"5 we also minimize \"4 max \"5, \"6 . Lemma 5.2. For every matrix satisfying Condition 1 of Definition 4.1, argmin\np \"5 argmin p \"4.\nAt this point we can derive in closed form the probability distribution p minimizing \"5. Lemma 5.3. The function \"5 is minimized by pij ⇢iqij where qij Aij A\ni\n1. To define ⇢i\nlet z i A i 1 and define ⇢i ⇣ ↵zi 2⇣ ↵zi 2⇣ 2 z i ⇣ 2 . Let ⇣1 0 be the unique\nsolution to3 i ⇢ i ⇣1 1. Let ⇢i : ⇢i ⇣1 .\nTo prove Theorem 4.2 we see that Lemmas 5.2 and 5.3 combined imply that there is an efficient algorithm for minimizing \"4 for every matrix A satisfying Condition 1 of Definition 4.1. If A also satisfies Conditions 2 and 3 of Definition 4.1, then it is possible to lower and upper bound the ratios \"1 \"2, \"2 \"3 and \"3 \"4. Combined, these bounds guarantee a lower and upper bound for \"1 \"4. In general, if c \"4 \"1 C we can conclude that \"1 argmin \"4 C c min \"1 . Thus, calculating the constants shows \"1 argmin \"4 3min \"1 , yielding Theorem 4.3."
    }, {
      "heading" : "6 Experiments",
      "text" : "We experimented with 4 matrices with different characteristics, summarized in the table below. See Section 4 for the definition of the different characteristics.\nMeasure m n nnz A A 1 A F A 2 sr nd nrd Synthetic 1.0e+2 1.0e+4 5.0e+5 1.8e+7 3.2e+4 8.7e+3 1.3e+1 3.1e+5 3.2e+3\nEnron 1.3e+4 1.8e+5 7.2e+5 4.0e+9 5.8e+6 1.0e+6 3.2e+1 4.9e+5 1.5e+3 Images 5.1e+3 4.9e+5 2.5e+8 6.5e+9 2.0e+6 1.8e+6 1.3e+0 1.1e+7 2.3e+3 Wikipedia 4.4e+5 3.4e+6 5.3e+8 5.3e+9 7.5e+5 1.6e+5 2.1e+1 5.0e+7 1.9e+4\nEnron: Subject lines of emails in the Enron email corpus [Sty11]. Columns correspond to subject lines, rows to words, and entries to tf-idf values. This matrix is extremely sparse to begin with. Wikipedia: Term-document matrix of a fragment of Wikipedia in English. Entries are tf-idf values. Images: A collection of images of buildings from Oxford [PCI 07]. Each column represents the wavelet transform of a single 128 128 pixel grayscale image. Synthetic: This synthetic matrix simulates a collaborative filtering matrix. Each row corresponds to an item and each column to a user. Each user and each item was first assigned a random latent vector (i.i.d. Gaussian). Each value in the matrix is the dot product of the corresponding latent vectors plus additional Gaussian noise. We simulated the fact that some items are more popular than others by retaining each entry of each item i with probability 1 i m where i 0, . . . ,m 1."
    }, {
      "heading" : "6.1 Sampling techniques and quality measure",
      "text" : "The experiments report the accuracy of sampling according to four different distributions. In Figure 6.1, Bernstein denotes the distribution of this paper, defined in Lemma 5.3. The Row-L1 distribution is a simplified version of the Bernstein distribution, where p\nij A ij\nA i 1. L1 and L2 refer to p\nij A ij and p ij A ij 2, respectively, as defined earlier in the paper. The case of L2 3Notice that the function ⇢i ⇣ is monotonically decreasing for ⇣ 0 hence the solution is indeed unique.\nsampling was split into three sampling methods corresponding to different trimming thresholds. In the method referred to as L2 no trimming is made and p\nij A ij 2. In the case referred to as L2 trim 0.1, p\nij A ij 2 for any entry where A ij 2 0.1 E ij A ij 2 and p ij 0 otherwise. The sampling technique referred to as L2 trim 0.01 is analogous with threshold 0.01 E\nij A ij\n2 .\nAlthough to derive our sampling probability distributions we targeted minimizing A B 2, in experiments it is more informative to consider a more sensitive measure of quality of approximation. The reason is that for a number of values of s, the scaling of entries required for B to be an unbiased estimator of A, results in A B A which would suggest that the all zeros matrix is a better sketch for A than the sampled matrix. We will see that this is far from being the case. As a trivial example, consider the possibility B 10A. Clearly, B is very informative of A although A B 9 A . To avoid this pitfall, we measure PB\nk\nA F A k F , where PB k is the projection on the top k left singular vectors of B. Thus, A\nk\nPA k A is the optimal rank k approximation of A. Intuitively, this measures how well the top k left singular vectors of B capture A, compared to A’s own (optimal) top-k left singular vectors. We also compute AQB\nk\nF A k F where QB k is the projection on the top k right singular vectors of A. Note that, for a given k, approximating the row-space is harder than approximating the column-space since it is of dimension n which is significantly larger than m, a fact also borne out in the experiments. In the experiments we made sure to choose a sufficiently wide range of sample sizes so that at least the best method for each matrix goes from poor to near-perfect both in approximating the row and the column space. In all cases we report on k 20 which is close to the upper end of what could be efficiently computed on a single machine for matrices of this size. The results for all smaller values of k are qualitatively indistinguishable.\nP k B A F A k , while the bottom plots show the row-space approximation ratio AQk B F A k\n. The number of samples s is on the x-axis in log scale x log10 s ."
    }, {
      "heading" : "6.2 Insights",
      "text" : "The experiments demonstrate three main insights. First and most important, Bernstein-sampling is never worse than any of the other techniques and is often strictly better. A dramatic example of this is the Wikipedia matrix for which it is far superior to all other methods. The second insight is that L1-sampling, i.e., simply taking p\nij\nA ij A 1, performs rather well in many cases. Hence, if it is impossible to perform more than one pass over the matrix and one can not even obtain an estimate of the ratios of the L1-weights of the rows, L1-sampling seems to be a highly viable option. The third insight is that for L2-sampling, discarding small entries may drastically improve the performance. However, it is not clear which threshold should be chosen in advance. In any case, in all of the example matrices, both L1-sampling and Bernstein-sampling proved to outperform or perform equally to L2-sampling, even with the correct trimming threshold."
    } ],
    "references" : [ {
      "title" : "Fast algorithms for approximate semidefinite programming using the multiplicative weights update method",
      "author" : [ "Sanjeev Arora", "Elad Hazan", "Satyen Kale" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "Arora et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2005
    }, {
      "title" : "A fast random sampling algorithm for sparsifying matrices. In Proceedings of the 9th international conference on Approximation Algorithms for Combinatorial Optimization Problems, and 10th international conference on Randomization and Computation, APPROX’06/RANDOM’06",
      "author" : [ "Sanjeev Arora", "Elad Hazan", "Satyen Kale" ],
      "venue" : null,
      "citeRegEx" : "Arora et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2006
    }, {
      "title" : "On the concentration of eigenvalues of random symmetric matrices. Israel",
      "author" : [ "Noga Alon", "Michael Krivelevich", "VanH. Vu" ],
      "venue" : "Journal of Mathematics,",
      "citeRegEx" : "Alon et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2002
    }, {
      "title" : "Fast computation of low rank matrix approximations",
      "author" : [ "Dimitris Achlioptas", "Frank McSherry" ],
      "venue" : "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Achlioptas and McSherry.,? \\Q2001\\E",
      "shortCiteRegEx" : "Achlioptas and McSherry.",
      "year" : 2001
    }, {
      "title" : "Fast computation of low-rank matrix approximations",
      "author" : [ "Dimitris Achlioptas", "Frank Mcsherry" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Achlioptas and Mcsherry.,? \\Q2007\\E",
      "shortCiteRegEx" : "Achlioptas and Mcsherry.",
      "year" : 2007
    }, {
      "title" : "Strong converse for identification via quantum channels",
      "author" : [ "Rudolf Ahlswede", "Andreas Winter" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Ahlswede and Winter.,? \\Q2002\\E",
      "shortCiteRegEx" : "Ahlswede and Winter.",
      "year" : 2002
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "The power of convex relaxation: Near-optimal matrix completion",
      "author" : [ "Emmanuel J Candès", "Terence Tao" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "Candès and Tao.,? \\Q2010\\E",
      "shortCiteRegEx" : "Candès and Tao.",
      "year" : 2010
    }, {
      "title" : "Fast monte carlo algorithms for matrices; approximating matrix multiplication",
      "author" : [ "Petros Drineas", "Ravi Kannan", "Michael W. Mahoney" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Drineas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Drineas et al\\.",
      "year" : 2006
    }, {
      "title" : "A note on element-wise matrix sparsification via a matrixvalued bernstein inequality",
      "author" : [ "Petros Drineas", "Anastasios Zouzias" ],
      "venue" : "Inf. Process. Lett.,",
      "citeRegEx" : "Drineas and Zouzias.,? \\Q2011\\E",
      "shortCiteRegEx" : "Drineas and Zouzias.",
      "year" : 2011
    }, {
      "title" : "The eigenvalues of random symmetric matrices",
      "author" : [ "Z. Füredi", "J. Komlós" ],
      "venue" : null,
      "citeRegEx" : "Füredi and Komlós.,? \\Q1981\\E",
      "shortCiteRegEx" : "Füredi and Komlós.",
      "year" : 1981
    }, {
      "title" : "Error bounds for random matrix approximation schemes",
      "author" : [ "Alex Gittens", "Joel A Tropp" ],
      "venue" : "arXiv preprint arXiv:0911.4108,",
      "citeRegEx" : "Gittens and Tropp.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gittens and Tropp.",
      "year" : 2009
    }, {
      "title" : "On the spectrum of a random graph. In Algebraic methods in graph theory, Vol. I, II (Szeged",
      "author" : [ "F. Juhász" ],
      "venue" : "Colloq. Math. Soc. János Bolyai,",
      "citeRegEx" : "Juhász.,? \\Q1981\\E",
      "shortCiteRegEx" : "Juhász.",
      "year" : 1981
    }, {
      "title" : "Matrix sparsification via the khintchine inequality",
      "author" : [ "NH Nguyen", "Petros Drineas", "TD Tran" ],
      "venue" : null,
      "citeRegEx" : "Nguyen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2009
    }, {
      "title" : "Tensor sparsification via a bound on the spectral norm of random tensors",
      "author" : [ "Nam H Nguyen", "Petros Drineas", "Trac D Tran" ],
      "venue" : "arXiv preprint arXiv:1005.4732,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2010
    }, {
      "title" : "Object retrieval with large vocabularies and fast spatial matching",
      "author" : [ "J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Philbin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Philbin et al\\.",
      "year" : 2007
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "Benjamin Recht" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Recht.,? \\Q2011\\E",
      "shortCiteRegEx" : "Recht.",
      "year" : 2011
    }, {
      "title" : "Sampling from large matrices: An approach through geometric functional analysis",
      "author" : [ "Mark Rudelson", "Roman Vershynin" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Rudelson and Vershynin.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rudelson and Vershynin.",
      "year" : 2007
    }, {
      "title" : "The enronsent corpus",
      "author" : [ "Will Styler" ],
      "venue" : "Technical Report 01-2011,",
      "citeRegEx" : "Styler.,? \\Q2011\\E",
      "shortCiteRegEx" : "Styler.",
      "year" : 2011
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A. Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Tropp.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tropp.",
      "year" : 2012
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "Joel A Tropp" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "Tropp.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tropp.",
      "year" : 2012
    }, {
      "title" : "On the distribution of the roots of certain symmetric matrices",
      "author" : [ "Eugene P. Wigner" ],
      "venue" : "Annals of Mathematics,",
      "citeRegEx" : "Wigner.,? \\Q1958\\E",
      "shortCiteRegEx" : "Wigner.",
      "year" : 1958
    } ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "We consider the problem of selecting non-zero entries of a matrix A in order to produce a sparse sketch of it, B, that minimizes A B 2. For large m n matrices, such that n m (for example, representing n observations over m attributes) we give sampling distributions that exhibit four important properties. First, they have closed forms computable from minimal information regarding A. Second, they allow sketching of matrices whose non-zeros are presented to the algorithm in arbitrary order as a stream, with O 1 computation per non-zero. Third, the resulting sketch matrices are not only sparse, but their non-zero entries are highly compressible. Lastly, and most importantly, under mild assumptions, our distributions are provably competitive with the optimal offline distribution. Note that the probabilities in the optimal offline distribution may be complex functions of all the entries in the matrix. Therefore, regardless of computational complexity, the optimal distribution might be impossible to compute in the streaming model.",
    "creator" : null
  }
}