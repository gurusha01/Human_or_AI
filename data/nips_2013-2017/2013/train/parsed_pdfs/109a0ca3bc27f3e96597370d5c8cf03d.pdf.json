{
  "name" : "109a0ca3bc27f3e96597370d5c8cf03d.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Convex Tensor Decomposition via Structured Schatten Norm Regularization",
    "authors" : [ "Ryota Tomioka", "Taiji Suzuki" ],
    "emails" : [ "tomioka@ttic.edu", "s-taiji@is.titech.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We study a new class of structured Schatten norms for tensors that includes two recently proposed norms (“overlapped” and “latent”) for convex-optimizationbased tensor decomposition. We analyze the performance of “latent” approach for tensor decomposition, which was empirically found to perform better than the “overlapped” approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific unknown mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structured Schatten norms, which is also interesting in the general context of structured sparsity. We confirm through numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error."
    }, {
      "heading" : "1 Introduction",
      "text" : "Decomposition of tensors [10, 14] (or multi-way arrays) into low-rank components arises naturally in many real world data analysis problems. For example, in neuroimaging, spatio-temporal patterns of neural activities that are related to certain experimental conditions or subjects can be found by computing the tensor decomposition of the data tensor, which can be of size channels × timepoints × subjects × conditions [18]. More generally, any multivariate spatio-temporal data (e.g., environmental monitoring) can be regarded as a tensor. If some of the observations are missing, lowrank modeling enables the imputation of missing values. Tensor modelling may also be valuable for collaborative filtering with temporal or contextual dimension.\nConventionally, tensor decomposition has been tackled through non-convex optimization problems, using alternate least squares or higher-order orthogonal iteration [6]. Compared to its empirical success, little has been theoretically understood about the performance of tensor decomposition algorithms. De Lathauwer et al. [5] showed an approximation bound for a truncated higher-order SVD (also known as the Tucker decomposition). Nevertheless the generalization performance of these approaches has been widely open. Moreover, the model selection problem can be highly challenging, especially for the Tucker model [5, 27], because we need to specify the rank rk for each mode (here a mode refers to one dimensionality of a tensor); that is, we have K hyper-parameters to choose for a K-way tensor, which is challenging even for K = 3.\nRecently a convex-optimization-based approach for tensor decomposition has been proposed by several authors [9, 15, 23, 25], and its performance has been analyzed in [26].\nThe basic idea behind their convex approach, which we call overlapped approach, is to unfold1 a tensor into matrices along different modes and penalize the unfolded matrices to be simultaneously low-rank based on the Schatten 1-norm, which is also known as the trace norm and nuclear norm [7, 22, 24]. This approach does not require the rank of the decomposition to be specified beforehand, and due to the low-rank inducing property of the Schatten 1-norm, the rank of the decomposition is automatically determined.\nHowever, it has been noticed that the above overlapped approach has a limitation that it performs poorly for a tensor that is only low-rank in a certain mode. The authors of [25] proposed an alternative approach, which we call latent approach, that decomposes a given tensor into a mixture of tensors that each are low-rank in a specific mode. Figure 1 demonstrates that the latent approach is preferable to the overlapped approach when the underlying tensor is almost full rank in all but one mode. However, so far no theoretical analysis has been presented to support such an empirical success.\nIn this paper, we rigorously study the performance of the latent approach and show that the mean squared error of the latent approach scales no greater than the minimum mode-k rank of the underlying true tensor, which clearly explains why the latent approach performs better than the overlapped approach in Figure 1.\nAlong the way, we show a novel duality between the two types of norms employed in the above two approaches, namely the overlapped Schatten norm and the latent Schatten norm. This result is closely related and generalize the results in structured sparsity literature [2, 13, 17, 21]. In fact, the (plain) overlapped group lasso constrains the weights to be simultaneously group sparse over overlapping groups. The latent group lasso predicts with a mixture of group sparse weights [see also 1, 3, 12]. These approaches clearly correspond to the two variations of tensor decomposition algorithms we discussed above.\nFinally we empirically compare the overlapped approach and latent approach and show that even when the unknown tensor is simultaneously low-rank, which is a favorable situation for the overlapped approach, the latent approach performs better in many cases. Thus we provide both theoretical and empirical evidence that for noisy tensor decomposition, the latent approach is preferable to the overlapped approach. Our result is complementary to the previous study [25, 26], which mainly focused on the noise-less tensor completion setting.\n1For a K-way tensor, there are K ways to unfold a tensor into a matrix. See Section 2.\nThis paper is structured as follows. In Section 2, we provide basic definitions of the two variations of structured Schatten norms, namely the overlapped/latent Schatten norms, and discuss their properties, especially the duality between them. Section 3 presents our main theoretical contributions; we establish the consistency of the latent approach, and we analyze the denoising performance of the latent approach. In Section 4, we empirically confirm the scaling predicted by our theory. Finally, Section 5 concludes the paper. Most of the proofs are presented in the supplementary material."
    }, {
      "heading" : "2 Structured Schatten norms for tensors",
      "text" : "In this section, we define the overlapped Schatten norm and the latent Schatten norm and discuss their basic properties.\nFirst we need some basic definitions. Let W ∈ Rn1×···nK be a K-way tensor. We denote the total number of entries in W by N =∏K k=1 nk. The dot product between two tensors W and X is defined as ⟨W,X⟩ = vec(W)⊤vec(X );\ni.e., the dot product as vectors in RN . The Frobenius norm of a tensor is defined as ∣∣∣∣∣∣W∣∣∣∣∣∣\nF =√\n⟨W,W⟩. Each dimensionality of a tensor is called a mode. The mode k unfolding W (k) ∈ Rnk×N/nk is a matrix that is obtained by concatenating the mode-k fibers along columns; here a mode-k fiber is an nk dimensional vector obtained by fixing all the indices but the kth index of W . The mode-k rank rk of W is the rank of the mode-k unfolding W (k). We say that a tensor W has multilinear rank (r1, . . . , rK) if the mode-k rank is rk for k = 1, . . . ,K [14]. The mode k folding is the inverse of the unfolding operation."
    }, {
      "heading" : "2.1 Overlapped Schatten norms",
      "text" : "The low-rank inducing norm studied in [9, 15, 23, 25], which we call overlapped Schatten 1-norm, can be written as follows: ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 = ∑K k=1 ∥W (k)∥S1 . (1)\nIn this paper, we consider the following more general overlapped Sp/q-norm, which includes the Schatten 1-norm as the special case (p, q) = (1, 1). The overlapped Sp/q-norm is written as follows:∣∣∣∣∣∣W∣∣∣∣∣∣ Sp/q = (∑K k=1 ∥W (k)∥qSp )1/q , (2)\nwhere 1 ≤ p, q ≤ ∞; here\n∥W ∥Sp = (∑r\nj=1 σpj (W ) )1/p is the Schatten p-norm for matrices, where σj(W ) is the jth largest singular value of W .\nWhen used as a regularizer, the overlapped Schatten 1-norm penalizes all modes of W to be jointly low-rank. It is related to the overlapped group regularization [see 13, 16] in a sense that the same object W appears repeatedly in the norm. The following inequality relates the overlapped Schatten 1-norm with the Frobenius norm, which was a key step in the analysis of [26]:\n∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 ≤ K∑\nk=1\n√ rk ∣∣∣∣∣∣W∣∣∣∣∣∣ F , (3)\nwhere rk is the mode-k rank of W . Now we are interested in the dual norm of the overlapped Sp/q-norm, because deriving the dual norm is a key step in solving the minimization problem that involves the norm (2) [see 16], as well as computing various complexity measures, such as, Rademacher complexity [8] and Gaussian width [4]. It turns out that the dual norm of the overlapped Sp/q-norm is the latent Sp∗/q∗-norm as shown in the following lemma (proof is presented in Appendix A).\nLemma 1. The dual norm of the overlapped Sp/q-norm is the latent Sp∗/q∗-norm, where 1/p + 1/p∗ = 1 and 1/q + 1/q∗ = 1, which is defined as follows:∣∣∣∣∣∣X ∣∣∣∣∣∣\nSp∗/q∗ = inf\n(X (1)+···+X (K))=X\n(∑K k=1 ∥X(k)(k)∥ q∗ Sp∗ )1/q∗ . (4)\nHere the infimum is taken over the K-tuple of tensors X (1), . . . ,X (K) that sums to X .\nIn the supplementary material, we show a slightly more general version of the above lemma that naturally generalizes the duality between overlapped/latent group sparsity norms [1, 12, 17, 21]; see Section A. Note that when the groups have no overlap, the overlapped/latent group sparsity norms become identical, and the duality is the ordinary duality between the group Sp/q-norms and the group Sp∗/q∗-norms."
    }, {
      "heading" : "2.2 Latent Schatten norms",
      "text" : "The latent approach for tensor decomposition [25] solves the following minimization problem\nminimize W(1),...,W(K)\nL(W(1) + · · ·+W(K)) + λ K∑\nk=1\n∥W (k)(k)∥S1 , (5)\nwhere L is a loss function, λ is a regularization constant, and W (k)(k) is the mode-k unfolding of W(k). Intuitively speaking, the latent approach for tensor decomposition predicts with a mixture of K tensors that each are regularized to be low-rank in a specific mode.\nNow, since the loss term in the minimization problem (5) only depends on the sum of the tensors W(1), . . . ,W(K), minimization problem (5) is equivalent to the following minimization problem\nminimize W\nL(W) + λ ∣∣∣∣∣∣W∣∣∣∣∣∣\nS1/1 .\nIn other words, we have identified the structured Schatten norm employed in the latent approach as the latent S1/1-norm (or latent Schatten 1-norm for short), which can be written as follows:∣∣∣∣∣∣W∣∣∣∣∣∣\nS1/1 = inf\n(W(1)+···+W(K))=W K∑ k=1 ∥W (k)(k)∥S1 . (6)\nAccording to Lemma 1, the dual norm of the latent S1/1-norm is the overlapped S∞/∞-norm∣∣∣∣∣∣X ∣∣∣∣∣∣ S∞/∞ = max k ∥X(k)∥S∞ , (7) where ∥ · ∥S∞ is the spectral norm. The following lemma is similar to inequality (3) and is a key in our analysis (proof is presented in Appendix B). Lemma 2. ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 ≤ ( min k √ rk ) ∣∣∣∣∣∣W∣∣∣∣∣∣ F ,\nwhere rk is the mode-k rank of W .\nCompared to inequality (3), the latent Schatten 1-norm is bounded by the minimal square root of the ranks instead of the sum. This is the fundamental reason why the latent approach performs betters than the overlapped approach as in Figure 1."
    }, {
      "heading" : "3 Main theoretical results",
      "text" : "In this section, combining the duality we presented in the previous section with the techniques from Agarwal et al. [1], we study the generalization performance of the latent approach for tensor decomposition in the context of recovering an unknown tensor W∗ from noisy measurements. This is the setting of the experiment in Figure 1. We first prove a generic consistency statement that does not take the low-rank-ness of the truth into account. Next we show that a tighter bound that takes the low-rank-ness into account can be obtained with some incoherence assumption. Finally, we discuss the difference between overlapped approach and latent approach and provide an explanation for the empirically observed superior performance of the latent approach in Figure 1."
    }, {
      "heading" : "3.1 Consistency",
      "text" : "Let W∗ be the underlying true tensor and the noisy version Y is obtained as follows:\nY = W∗ + E ,\nwhere E ∈ Rn1×···×nK is the noise tensor. A consistency statement can be obtained as follows (proof is presented in Appendix C): Theorem 1. Assume that the regularization constant λ satisfies λ ≥ ∣∣∣∣∣∣E∣∣∣∣∣∣ S∞/∞ (overlapped S∞/∞\nnorm of the noise), then the estimator defined by Ŵ = argminW ( 1 2 ∣∣∣∣∣∣Y −W∣∣∣∣∣∣2 F + λ ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 ) , satisfies the inequality ∣∣∣∣∣∣Ŵ −W∗∣∣∣∣∣∣ F ≤ 2λ √ min k nk. (8)\nIn particular when the noise goes to zero E → 0, the right hand side of inequality (8) shrinks to zero."
    }, {
      "heading" : "3.2 Deterministic bound",
      "text" : "The consistency statement in the previous section only deals with the sum Ŵ = ∑K\nk=1 Ŵ(k) and the statement does not take into account the low-rank-ness of the truth. In this section, we establish a tighter statement that bounds the errors of individual terms Ŵ(k). To this end, we need some additional assumptions. First, we assume that the unknown tensor W∗ is a mixture of K tensors that each are low-rank in a certain mode and we have a noisy observation Y as follows:\nY = W∗ + E = ∑K\nk=1 W∗(k) + E , (9)\nwhere r̄k = rank(W (k) (k)) is the mode-k rank of the kth component W ∗(k); note that this does not equal the mode-k rank rk of W∗ in general. Second, we assume that the spectral norm of the mode-k unfolding of the lth component is bounded by a constant α for all k ̸= l as follows:\n∥W ∗(l)(k) ∥S∞ ≤ α (∀l ̸= k, k, l = 1, . . . ,K). (10)\nNote that such an additional incoherence assumption has also been used in [1, 3, 11].\nWe employ the following optimization problem to recover the unknown tensor W∗:"
    }, {
      "heading" : "Ŵ = argmin",
      "text" : "W\n( 1\n2\n∣∣∣∣∣∣Y −W∣∣∣∣∣∣2 F + λ ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 s.t. W = K∑\nk=1\nW(k), ∥W (l)(k)∥S∞ ≤ α, ∀l ̸= k\n) ,\n(11)\nwhere λ > 0 is a regularization constant. Notice that we have introduced additional spectral norm constraints to control the correlation between the components; see also [1].\nOur deterministic performance bound can be stated as follows (proof is presented in Appendix D):\nTheorem 2. Let Ŵ(k) be an optimal decomposition of Ŵ induced by the latent Schatten 1-norm (6). Assume that the regularization constant λ satisfies λ ≥ 2 ∣∣∣∣∣∣E∣∣∣∣∣∣ S∞/∞ + α(K − 1). Then there is\na universal constant c such that, any solution Ŵ of the minimization problem (11) satisfies the following deterministic bound:∑K\nk=1\n∣∣∣∣∣∣Ŵ(k) −W∗(k)∣∣∣∣∣∣2 F ≤ cλ2 ∑K k=1 rk. (12)\nMoreover, the overall error can be bounded in terms of the multilinear rank of W∗ as follows:∣∣∣∣∣∣Ŵ −W∗∣∣∣∣∣∣2 F ≤ cλ2 min\nk rk. (13)\nNote that in order to get inequality (13), we exploit the arbitrariness of the decomposition W∗ =∑K k=1 W∗(k) to replace the sum over the ranks with the minimal mode-k rank. This is possible because a singleton decomposition, i.e., W∗(k) = W∗ and W∗(k′) = 0 for k′ ̸= k, is allowed for any k.\nComparing two inequalities (8) and (13), we see that there are two regimes. When the noise is small, (8) is tighter. On the other hand, when the noise is larger and/or mink rk ≪ mink nk, (13) is tighter."
    }, {
      "heading" : "3.3 Gaussian noise",
      "text" : "When the elements of the noise tensor E are Gaussian, we obtain the following theorem. Theorem 3. Assume that the elements of the noise tensor E are independent zero-mean Gaussian random variables with variance σ2. In addition, assume without loss of generality that the dimensionalities of W∗ are sorted in the descending order, i.e., n1 ≥ · · · ≥ nK . Then there is a universal constant c such that, with probability at least 1 − δ, any solution of the minimization problem (11) with regularization constant λ = 2σ( √ N/nK + √ n1 + √ 2 log(K/δ)) + α(K − 1) satisfies\n1\nN K∑ k=1 ∣∣∣∣∣∣Ŵ(k) −W∗(k)∣∣∣∣∣∣2 F ≤ cFσ2 ∑K k=1 r̄k nK , (14)\nwhere F = (( 1 + √\nn1nK N\n) + (√\n2 log(K/δ) + α(K−1)2σ )√ nK N )2 is a factor that mildly depends\non the dimensionalities and the constant α in (10).\nNote that the theoretically optimal choice of regularization constant λ is independent of the ranks of the truth W∗ or its factors in (9), which are unknown in practice. Again we can obtain a bound corresponding to the minimum rank singleton decomposition as in inequality (13) as follows:\n1\nN\n∣∣∣∣∣∣Ŵ −W∗∣∣∣∣∣∣2 F ≤ cFσ2mink rk\nnK , (15)\nwhere F is the same factor as in Theorem 3."
    }, {
      "heading" : "3.4 Comparison with the overlapped approach",
      "text" : "Inequality (15) explains the superior performance of the latent approach for tensor decomposition in Figure 1. The inequality obtained in [26] for the overlapped approach that uses overlapped Schatten 1-norm (1) can be stated as follows:\n1\nN\n∣∣∣∣∣∣Ŵ −W∗∣∣∣∣∣∣2 F ≤ c′σ2\n( 1\nK K∑ k=1 √ 1 nk )2( 1 K K∑ k=1 √ rk )2 . (16)\nComparing inequalities (15) and (16), we notice that the complexity of the overlapped approach depends on the average (square root) of the mode-k ranks r1, . . . , rK , whereas that of the latent approach only grows linearly against the minimum mode-k rank. Interestingly, the latent approach performs as if it knows the mode with the minimum rank, although such information is not given.\nRecently, Mu et al. [19] proved a lower bound of the number of measurements for solving linear inverse problem via the overlapped approach. Although the setting is different, the lower bound depends on the minimum mode-k rank, which agrees with the complexity of the latent approach."
    }, {
      "heading" : "4 Numerical results",
      "text" : "In this section, we numerically confirm the theoretically obtained scaling behavior.\nThe goal of this experiment is to recover the true low rank tensor W∗ from a noisy observation Y . We randomly generated the true low rank tensors W∗ of size 50 × 50 × 20 or 80 × 80 × 40 with various mode-k ranks (r1, r2, r3). A low-rank tensor is generated by first randomly drawing the\nr1 × r2 × r3 core tensor from the standard normal distribution and multiplying an orthogonal factor matrix drawn uniformly to its each mode. The observation tensor Y is obtained by adding Gaussian noise with standard deviation σ = 0.1. There is no missing entries in this experiment.\nFor each observation Y , we computed tensor decompositions using the overlapped approach and the latent approach (11). For the optimization, we used the algorithms2 based on alternating direction method of multipliers described in Tomioka et al. [25]. We computed the solutions for 20 candidate regularization constants ranging from 0.1 to 100 and report the results for three representative values for each method.\nWe measured the quality of the solutions obtained by the two approaches by the mean squared error (MSE) ∣∣∣∣∣∣Ŵ − W∗∣∣∣∣∣∣2 F /N . In order to make our theoretical predictions more concrete, we define the quantities in the right hand side of the bounds (16) and (14) as Tucker rank (TR) complexity and Latent rank (LR) complexity, respectively, as follows:\nTR complexity = (\n1 K ∑K k=1 √ 1 nk )2 ( 1 K ∑K k=1 √ rk )2 , (17)\nLR complexity = ∑K\nk=1 r̄k nK , (18)\nwhere without loss of generality we assume n1 ≥ · · · ≥ nK . We have ignored terms like √ nk/N because they are negligible for nk ≈ 50 and N ≈ 50, 000. The TR complexity is equivalent to the normalized rank in [26]. Note that the TR complexity (17) is defined in terms of the multilinear rank (r1, . . . , rK) of the truth W∗, whereas the LR complexity (18) is defined in terms of the ranks of the latent factors (r1, . . . , rK) in (9). In order to find a decomposition that minimizes the right hand side of (18), we ran the latent approach to the true tensor W∗ without noise, and took the minimum of the sum of ranks found by the run and mink rk, i.e., the minimal mode-k rank (because a singleton solution is also allowed). The whole procedure is repeated 10 times and averaged.\nFigure 2 shows the results of the experiment. The left panel shows the MSE of the overlapped approach against the TR complexity (17). The middle panel shows the MSE of the latent approach against the LR complexity (18). The right panel shows the improvement (i.e., MSE of the overlap approach over that of the latent approach) against the ratio of the respective complexity measures.\nFirst, from the left panel, we can confirm that as predicted by [26], the MSE of the overlapped approach scales linearly against the TR complexity (17) for each value of the regularization constant.\nFrom the central panel, we can clearly see that the MSE of the latent approach scales linearly against the LR complexity (18) as predicted by Theorem 3. The series with △ (λ = 3.79 for 50× 50× 20,\n2The solver is available online: https://github.com/ryotat/tensor.\nλ = 5.46 for 80× 80× 40) is mostly below other series, which means that the optimal choice of the regularization constant is independent of the rank of the true tensor and only depends on the size; this agrees with the condition on λ in Theorem 3. Since the blue series and red series with the same markers lie on top of each other (especially the series with △ for which the optimal regularization constant is chosen), we can see that our theory predicts not only the scaling against the latent ranks but also that against the size of the tensor correctly. Note that the regularization constants are scaled by roughly 1.6 to account for the difference in the dimensionality.\nThe right panel reveals that in many cases the latent approach performs better than the overlapped approach, i.e., MSE (overlap)/ MSE (latent) greater than one. Moreover, we can see that the success of the latent approach relative to the overlapped approach is correlated with high TR complexity to LR complexity ratio. Indeed, we found that an optimal decomposition of the true tensor W∗ was typically a singleton decomposition corresponding to the smallest tucker rank (see Section 3.2). Note that the two approaches perform almost identically when they are under-regularized (crosses).\nThe improvements here are milder than that in Figure 1. This is because most of the randomly generated low-rank tensors were simultaneously low-rank to some degree. It is encouraging that the latent approach perform at least as well as the overlapped approach in such situations as well."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we have presented a framework for structured Schatten norms. The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21]. Moreover, we have shown a duality that holds between the two types of norms.\nFurthermore, we have rigorously studied the performance of the latent approach for tensor decomposition. We have shown the consistency of the latent Schatten 1-norm minimization. Next, we have analyzed the denoising performance of the latent approach and shown that the error of the latent approach is upper bounded by the minimal mode-k rank, which contrasts sharply against the average (square root) dependency of the overlapped approach analyzed in [26]. This explains the empirically observed superior performance of the latent approach compared to the overlapped approach. The most difficult case for the overlapped approach is when the unknown tensor is only low-rank in one mode as in Figure 1.\nWe have also confirmed through numerical simulations that our analysis precisely predicts the scaling of the mean squared error as a function of the dimensionalities and the sum of ranks of the factors of the unknown tensor, which is dominated by the minimal mode-k rank. Unlike mode-k ranks, the ranks of the factors are not easy to compute. However, note that the theoretically optimal choice of the regularization constant does not depend on these quantities.\nThus, we have theoretically and empirically shown that for noisy tensor decomposition, the latent approach is more likely to perform better than the overlapped approach. Analyzing the performance of the latent approach for tensor completion would be an important future work.\nThe structured Schatten norms proposed in this paper include norms for tensors that are not employed in practice yet. Therefore, it would be interesting to explore various extensions, such as, using the overlapped S1/∞-norm instead of the S1/1-norm or a non-sparse tensor decomposition. Acknowledgment: This work was carried out while both authors were at The University of Tokyo. This work was partially supported by JSPS KAKENHI 25870192 and 25730013, and the Aihara Project, the FIRST program from JSPS, initiated by CSTP."
    } ],
    "references" : [ {
      "title" : "Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions",
      "author" : [ "A. Agarwal", "S. Negahban", "M.J. Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Convex optimization with sparsity-inducing norms. In Optimization for Machine Learning",
      "author" : [ "F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "E.J. Candes", "X. Li", "Y. Ma", "J. Wright" ],
      "venue" : "Technical report,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "The convex geometry of linear inverse problems, prepint",
      "author" : [ "V. Chandrasekaran", "B. Recht", "P. Parrilo", "A. Willsky" ],
      "venue" : "Technical report,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "A multilinear singular value decomposition",
      "author" : [ "L. De Lathauwer", "B. De Moor", "J. Vandewalle" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "On the best rank-1 and rank-(R1",
      "author" : [ "L. De Lathauwer", "B. De Moor", "J. Vandewalle" ],
      "venue" : "RN ) approximation of higher-order tensors. SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "A Rank Minimization Heuristic with Application to Minimum Order System Approximation",
      "author" : [ "M. Fazel", "H. Hindi", "S.P. Boyd" ],
      "venue" : "In Proc. of the American Control Conference,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2001
    }, {
      "title" : "Concentration-based guarantees for low-rank matrix reconstruction",
      "author" : [ "R. Foygel", "N. Srebro" ],
      "venue" : "Technical report,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Tensor completion and low-n-rank tensor recovery via convex optimization",
      "author" : [ "S. Gandy", "B. Recht", "I. Yamada" ],
      "venue" : "Inverse Problems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "The expression of a tensor or a polyadic as a sum of products",
      "author" : [ "F.L. Hitchcock" ],
      "venue" : "J. Math. Phys.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1927
    }, {
      "title" : "Robust matrix decomposition with sparse corruptions",
      "author" : [ "D. Hsu", "S.M. Kakade", "T. Zhang" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "A dirty model for multi-task learning",
      "author" : [ "A. Jalali", "P. Ravikumar", "S. Sanghavi", "C. Ruan" ],
      "venue" : "In Advances in NIPS",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Structured variable selection with sparsity-inducing norms",
      "author" : [ "R. Jenatton", "J. Audibert", "F. Bach" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T.G. Kolda", "B.W. Bader" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Tensor completion for estimating missing values in visual data",
      "author" : [ "J. Liu", "P. Musialski", "P. Wonka", "J. Ye" ],
      "venue" : "In Prof. ICCV,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Convex and network flow optimization for structured sparsity",
      "author" : [ "J. Mairal", "R. Jenatton", "G. Obozinski", "F. Bach" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Structured sparsity and generalization",
      "author" : [ "A. Maurer", "M. Pontil" ],
      "venue" : "Technical report,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Applications of tensor (multiway array) factorizations and decompositions in data mining",
      "author" : [ "M. Mørup" ],
      "venue" : "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Square deal: Lower bounds and improved relaxations for tensor recovery",
      "author" : [ "C. Mu", "B. Huang", "J. Wright", "D. Goldfarb" ],
      "venue" : "arXiv preprint arXiv:1307.5870,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "S. Negahban", "P. Ravikumar", "M. Wainwright", "B. Yu" ],
      "venue" : "In Advances in NIPS",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Group lasso with overlaps: the latent group lasso approach",
      "author" : [ "G. Obozinski", "L. Jacob", "J.-P. Vert" ],
      "venue" : "Technical report,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "B. Recht", "M. Fazel", "P. Parrilo" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Nuclear norms for tensors and their use for convex multilinear estimation",
      "author" : [ "M. Signoretto", "L. De Lathauwer", "J. Suykens" ],
      "venue" : "Technical Report 10-186,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Rank, trace-norm and max-norm",
      "author" : [ "N. Srebro", "A. Shraibman" ],
      "venue" : "In Proc. of the 18th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2005
    }, {
      "title" : "Estimation of low-rank tensors via convex optimization",
      "author" : [ "R. Tomioka", "K. Hayashi", "H. Kashima" ],
      "venue" : "Technical report,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Statistical performance of convex tensor decomposition",
      "author" : [ "R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima" ],
      "venue" : "In Advances in NIPS",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Some mathematical notes on three-mode factor analysis",
      "author" : [ "L.R. Tucker" ],
      "venue" : "Psychometrika, 31(3):279–311,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1966
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "Technical report,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Decomposition of tensors [10, 14] (or multi-way arrays) into low-rank components arises naturally in many real world data analysis problems.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "Decomposition of tensors [10, 14] (or multi-way arrays) into low-rank components arises naturally in many real world data analysis problems.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "For example, in neuroimaging, spatio-temporal patterns of neural activities that are related to certain experimental conditions or subjects can be found by computing the tensor decomposition of the data tensor, which can be of size channels × timepoints × subjects × conditions [18].",
      "startOffset" : 278,
      "endOffset" : 282
    }, {
      "referenceID" : 5,
      "context" : "Conventionally, tensor decomposition has been tackled through non-convex optimization problems, using alternate least squares or higher-order orthogonal iteration [6].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "[5] showed an approximation bound for a truncated higher-order SVD (also known as the Tucker decomposition).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Moreover, the model selection problem can be highly challenging, especially for the Tucker model [5, 27], because we need to specify the rank rk for each mode (here a mode refers to one dimensionality of a tensor); that is, we have K hyper-parameters to choose for a K-way tensor, which is challenging even for K = 3.",
      "startOffset" : 97,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : "Moreover, the model selection problem can be highly challenging, especially for the Tucker model [5, 27], because we need to specify the rank rk for each mode (here a mode refers to one dimensionality of a tensor); that is, we have K hyper-parameters to choose for a K-way tensor, which is challenging even for K = 3.",
      "startOffset" : 97,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "Recently a convex-optimization-based approach for tensor decomposition has been proposed by several authors [9, 15, 23, 25], and its performance has been analyzed in [26].",
      "startOffset" : 108,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "Recently a convex-optimization-based approach for tensor decomposition has been proposed by several authors [9, 15, 23, 25], and its performance has been analyzed in [26].",
      "startOffset" : 108,
      "endOffset" : 123
    }, {
      "referenceID" : 22,
      "context" : "Recently a convex-optimization-based approach for tensor decomposition has been proposed by several authors [9, 15, 23, 25], and its performance has been analyzed in [26].",
      "startOffset" : 108,
      "endOffset" : 123
    }, {
      "referenceID" : 24,
      "context" : "Recently a convex-optimization-based approach for tensor decomposition has been proposed by several authors [9, 15, 23, 25], and its performance has been analyzed in [26].",
      "startOffset" : 108,
      "endOffset" : 123
    }, {
      "referenceID" : 25,
      "context" : "Recently a convex-optimization-based approach for tensor decomposition has been proposed by several authors [9, 15, 23, 25], and its performance has been analyzed in [26].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "The basic idea behind their convex approach, which we call overlapped approach, is to unfold1 a tensor into matrices along different modes and penalize the unfolded matrices to be simultaneously low-rank based on the Schatten 1-norm, which is also known as the trace norm and nuclear norm [7, 22, 24].",
      "startOffset" : 289,
      "endOffset" : 300
    }, {
      "referenceID" : 21,
      "context" : "The basic idea behind their convex approach, which we call overlapped approach, is to unfold1 a tensor into matrices along different modes and penalize the unfolded matrices to be simultaneously low-rank based on the Schatten 1-norm, which is also known as the trace norm and nuclear norm [7, 22, 24].",
      "startOffset" : 289,
      "endOffset" : 300
    }, {
      "referenceID" : 23,
      "context" : "The basic idea behind their convex approach, which we call overlapped approach, is to unfold1 a tensor into matrices along different modes and penalize the unfolded matrices to be simultaneously low-rank based on the Schatten 1-norm, which is also known as the trace norm and nuclear norm [7, 22, 24].",
      "startOffset" : 289,
      "endOffset" : 300
    }, {
      "referenceID" : 24,
      "context" : "The authors of [25] proposed an alternative approach, which we call latent approach, that decomposes a given tensor into a mixture of tensors that each are low-rank in a specific mode.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "This result is closely related and generalize the results in structured sparsity literature [2, 13, 17, 21].",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "This result is closely related and generalize the results in structured sparsity literature [2, 13, 17, 21].",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "This result is closely related and generalize the results in structured sparsity literature [2, 13, 17, 21].",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "This result is closely related and generalize the results in structured sparsity literature [2, 13, 17, 21].",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 24,
      "context" : "Our result is complementary to the previous study [25, 26], which mainly focused on the noise-less tensor completion setting.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 25,
      "context" : "Our result is complementary to the previous study [25, 26], which mainly focused on the noise-less tensor completion setting.",
      "startOffset" : 50,
      "endOffset" : 58
    }, {
      "referenceID" : 8,
      "context" : "The low-rank inducing norm studied in [9, 15, 23, 25], which we call overlapped Schatten 1-norm, can be written as follows: ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 = ∑K",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "The low-rank inducing norm studied in [9, 15, 23, 25], which we call overlapped Schatten 1-norm, can be written as follows: ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 = ∑K",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "The low-rank inducing norm studied in [9, 15, 23, 25], which we call overlapped Schatten 1-norm, can be written as follows: ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 = ∑K",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "The low-rank inducing norm studied in [9, 15, 23, 25], which we call overlapped Schatten 1-norm, can be written as follows: ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 = ∑K",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "The following inequality relates the overlapped Schatten 1-norm with the Frobenius norm, which was a key step in the analysis of [26]: ∣∣∣∣∣∣W∣∣∣∣∣∣ S1/1 ≤ K ∑",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "Now we are interested in the dual norm of the overlapped Sp/q-norm, because deriving the dual norm is a key step in solving the minimization problem that involves the norm (2) [see 16], as well as computing various complexity measures, such as, Rademacher complexity [8] and Gaussian width [4].",
      "startOffset" : 267,
      "endOffset" : 270
    }, {
      "referenceID" : 3,
      "context" : "Now we are interested in the dual norm of the overlapped Sp/q-norm, because deriving the dual norm is a key step in solving the minimization problem that involves the norm (2) [see 16], as well as computing various complexity measures, such as, Rademacher complexity [8] and Gaussian width [4].",
      "startOffset" : 290,
      "endOffset" : 293
    }, {
      "referenceID" : 0,
      "context" : "In the supplementary material, we show a slightly more general version of the above lemma that naturally generalizes the duality between overlapped/latent group sparsity norms [1, 12, 17, 21]; see Section A.",
      "startOffset" : 176,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "In the supplementary material, we show a slightly more general version of the above lemma that naturally generalizes the duality between overlapped/latent group sparsity norms [1, 12, 17, 21]; see Section A.",
      "startOffset" : 176,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "In the supplementary material, we show a slightly more general version of the above lemma that naturally generalizes the duality between overlapped/latent group sparsity norms [1, 12, 17, 21]; see Section A.",
      "startOffset" : 176,
      "endOffset" : 191
    }, {
      "referenceID" : 20,
      "context" : "In the supplementary material, we show a slightly more general version of the above lemma that naturally generalizes the duality between overlapped/latent group sparsity norms [1, 12, 17, 21]; see Section A.",
      "startOffset" : 176,
      "endOffset" : 191
    }, {
      "referenceID" : 24,
      "context" : "The latent approach for tensor decomposition [25] solves the following minimization problem",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "[1], we study the generalization performance of the latent approach for tensor decomposition in the context of recovering an unknown tensor W∗ from noisy measurements.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Note that such an additional incoherence assumption has also been used in [1, 3, 11].",
      "startOffset" : 74,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "Note that such an additional incoherence assumption has also been used in [1, 3, 11].",
      "startOffset" : 74,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "Note that such an additional incoherence assumption has also been used in [1, 3, 11].",
      "startOffset" : 74,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Notice that we have introduced additional spectral norm constraints to control the correlation between the components; see also [1].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "The inequality obtained in [26] for the overlapped approach that uses overlapped Schatten 1-norm (1) can be stated as follows:",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "[19] proved a lower bound of the number of measurements for solving linear inverse problem via the overlapped approach.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "The TR complexity is equivalent to the normalized rank in [26].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "First, from the left panel, we can confirm that as predicted by [26], the MSE of the overlapped approach scales linearly against the TR complexity (17) for each value of the regularization constant.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21].",
      "startOffset" : 177,
      "endOffset" : 192
    }, {
      "referenceID" : 14,
      "context" : "The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21].",
      "startOffset" : 177,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : "The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21].",
      "startOffset" : 177,
      "endOffset" : 192
    }, {
      "referenceID" : 24,
      "context" : "The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21].",
      "startOffset" : 177,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : "The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21].",
      "startOffset" : 267,
      "endOffset" : 282
    }, {
      "referenceID" : 12,
      "context" : "The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21].",
      "startOffset" : 267,
      "endOffset" : 282
    }, {
      "referenceID" : 16,
      "context" : "The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21].",
      "startOffset" : 267,
      "endOffset" : 282
    }, {
      "referenceID" : 20,
      "context" : "The current framework includes both the overlapped Schatten 1-norm and latent Schatten 1-norm recently proposed in the context of convex-optimization-based tensor decomposition [9, 15, 23, 25], and connects these studies to the broader studies on structured sparsity [2, 13, 17, 21].",
      "startOffset" : 267,
      "endOffset" : 282
    }, {
      "referenceID" : 25,
      "context" : "Next, we have analyzed the denoising performance of the latent approach and shown that the error of the latent approach is upper bounded by the minimal mode-k rank, which contrasts sharply against the average (square root) dependency of the overlapped approach analyzed in [26].",
      "startOffset" : 273,
      "endOffset" : 277
    } ],
    "year" : 2013,
    "abstractText" : "We study a new class of structured Schatten norms for tensors that includes two recently proposed norms (“overlapped” and “latent”) for convex-optimizationbased tensor decomposition. We analyze the performance of “latent” approach for tensor decomposition, which was empirically found to perform better than the “overlapped” approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a specific unknown mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structured Schatten norms, which is also interesting in the general context of structured sparsity. We confirm through numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error.",
    "creator" : null
  }
}