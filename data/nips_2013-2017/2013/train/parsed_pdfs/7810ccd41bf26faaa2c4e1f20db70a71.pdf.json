{
  "name" : "7810ccd41bf26faaa2c4e1f20db70a71.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Σ-Optimality for Active Learning on Gaussian Random Fields",
    "authors" : [ "Yifei Ma", "Roman Garnett", "Jeff Schneider" ],
    "emails" : [ "yifeim@cs.cmu.edu", "rgarnett@uni-bonn.de", "schneide@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Real-world data are often presented as a graph where the nodes in the graph bear labels that vary smoothly along edges. For example, for scientific publications, the content of one paper is highly correlated with the content of papers that it references or is referenced by, the field of interest of a scholar is highly correlated with other scholars s/he coauthors with, etc. Many of these networks can be described using an undirected graph with nonnegative edge weights set to be the strengths of the connections between nodes.\nThe model for label prediction in this paper is the harmonic function on the Gaussian random field (GRF) by Zhu et al. (2003). It can generalize two popular and intuitive algorithms: label propagation (Zhu & Ghahramani, 2002), and random walk with absorptions (Wu et al., 2012). GRFs can be seen as a Gaussian process (GP) (Rasmussen & Williams, 2006) with its (maybe improper) prior covariance matrix whose (pseudo)inverse is set to be the graph Laplacian.\nLike other learning problems, labels may be insufficient and expensive to gather, especially if one wants to discover a new phenomenon on a graph. Active learning addresses these issues by making automated decisions on which nodes to query for labels from experts or the crowd. Some popular criteria are empirical risk minimization (Settles, 2010; Zhu et al., 2003), mutual information gain (Krause et al., 2008), and V-optimality (Ji & Han, 2012). Here we consider an alternative criterion, Σ-optimality, and establish several related theoretical results. Namely, we show that greedy reduction of Σ-optimality provides a (1− 1/e) approximation bound to the global optimum. We also show\nthat Gaussian random fields satisfy the suppressor-free condition, described below. Finally, we show that Σ-optimality outperforms other approaches for active learning with GRFs for classification."
    }, {
      "heading" : "1.1 V-optimality on Gaussian Random Fields",
      "text" : "Ji & Han (2012) proposed greedy variance minimization as a cheap and high profile surrogate active classification criterion. To decide which node to query next, the active learning algorithm finds the unlabeled node which leads to the smallest average predictive variance on all other unlabeled nodes. It corresponds to standard V-optimality in optimal experiment design.\nWe will discuss several aspects of V-optimality on GRFs below: 1. The motivation behind Voptimality can be paraphrased as the expected risk minimization with the L2-surrogate loss (Section 2.1). 2. The greedy solution to the set optimization problem in V-optimality is comparable to the global solution up to a constant (Theorem 1). 3. The greedy application of V-optimality can also be interpreted as a heuristic which selects nodes that have high correlation to nodes with high variances (Observation 4).\nSome previous work is related to point 2 above. Nemhauser et al. (1978) shows that any submodular, monotone and normalized set function yields a (1 − 1/e) global optimality guarantee for greedy solutions. Our proof techniques coincides with Friedland & Gaubert (2011) in principle, but we are not restricted to spectral functions. Krause et al. (2008) showed a counter example where the V-optimality objective function with GP models does not satisfy submodularity."
    }, {
      "heading" : "1.2 Σ-optimality on Gaussian Random Fields",
      "text" : "We define Σ-optimality on GRFs to be another variance minimization criterion that minimizes the sum of all entries in the predictive covariance matrix. As we will show in Lemma 7, the predictive covariance matrix is nonnegative entry-wise and thus the definition is proper. Σ-optimality was originally proposed by Garnett et al. (2012) in the context of active surveying, which is to determine the proportion of nodes belonging to one class. However, we focus on its performance as a criterion in active classification heuristics. The survey-risk of Σ-optimality replaces the L2-risk of V-optimality as an alternative surrogate risk for the 0/1-risk.\nWe also prove that the greedy application of Σ-optimality has a similar theoretical bound as Voptimality. We will show that greedily minimizing Σ-optimality empirically outperforms greedily minimizing V-optimality on classification problems. The exact reason explaining the superiority of Σ-optimality as a surrogate loss in the GRF model is still an open question, but we observe that Σ-optimality tends to select cluster centers whereas V-optimality goes after outliers (Section 3.1). Finally, greedy application of both Σ-optimality and V-optimality needO(N) time per query candidate evaluation after one-time inverse of a N ×N matrix."
    }, {
      "heading" : "1.3 GRFs Are Suppressor Free",
      "text" : "In linear regression, an explanatory variable is called a suppressor if adding it as a new variable enhances correlations between the old variables and the dependent variable (Walker, 2003; Das & Kempe, 2008). Suppressors are persistent in real-world data. We show GRFs to be suppressorfree. Intuitively, this means that with more labels acquired, the conditional correlation between unlabeled nodes decreases even when their Markov blanket has not formed. That GRFs present natural examples for the otherwise obscure suppressor-free condition is interesting."
    }, {
      "heading" : "2 Learning Model & Active Learning Objectives",
      "text" : "We use Gaussian random field/label propagation (GRF/LP) as our learning model. Suppose the dataset can be represented in the form of a connected undirected graph G = (V,E) where each node has an (either known or unknown) label and each edge eij has a fixed nonnegative weight wij(= wji) that reflects the proximity, similarity, etc. between nodes vi and vj . Define the graph Laplacian of G to be L = diag (W1) −W , i.e., lii = ∑ j wij and lij = −wij when i 6= j. Let Lδ = L+ δI be the generalized Laplacian obtained by adding self-loops. In the following, we will write L to also encompass βLδ for the set of hyper-parameters β > 0 and δ ≥ 0.\nThe binary GRF is a Bayesian model to generate yi ∈ {0,+1} for every node vi according to, p(y) ∝ exp { − β\n2 (∑ i,j wij(yi − yj)2 + δ ∑ i y2i )} = exp ( −1 2 yTLy ) . (2.1)\nSuppose nodes ` = {v`1 , . . . , v`|`|} are labeled as y` = (y`1 , . . . , y`|`|)T ; A GRF infers the output distribution on unlabeled nodes, yu = (yu1 , . . . , yu|u|) T by the conditional distribution given y`, as\nPr(yu|y`) ∝ N (ŷu, L−1u ) = N (ŷu, L−1(v−`)), (2.2)\nwhere ŷu = (−L−1u Lu`y`) is the vector of predictive means on unlabeled nodes and Lu is the principal submatrix consisting of the unlabeled row and column indices in L, that is, the lower-right\nblock of L = (\nL` L`u Lu` Lu ) . By convention, L−1(v−`) means the inverse of the principal submatrix.\nWe use L(v−`) and Lu interchangeably because ` and u partition the set of all nodes v.\nFinally, GRF, or GRF/LP, is a relaxation of the binary GRF to continuous outputs, because the latter is computationally intractable even for a-priori generations. LP stands for label propagation, because the predictive mean on a node is the probability of a random walk leaving that node hitting a positive label before hitting a zero label. For multi-class problems, Zhu et al. (2003) proposed the harmonic predictor which looks at predictive means in one-versus-all comparisons.\nRemark: An alternative approximation to the binary GRF is the GRF-sigmoid model, which draws the binary outputs from Bernoulli distributions with means set to be the sigmoid function of the GRF (latent) variables. However, this alternative is very slow to compute and may not be compatible with the theoretical results in this paper."
    }, {
      "heading" : "2.1 Active Learning Objective 1: L2 Risk Minimization (V-Optimality)",
      "text" : "Since in GRFs, regression responses are taken directly as probability predictions, it is computationally and analytically more convenient to apply the regression loss directly in the GRF as in Ji & Han (2012). Assume the L2 loss to be our classification loss. The risk function, whose input variable is the labeled subset `, is:\nRV (`) = Ey`yu ∑ ui∈u (yui − ŷui)2 = E\n[ E [∑ ui∈u (yui − ŷui)2 ∣∣∣∣∣y` ]] = tr(L−1u ). (2.3)\nThis risk is written with a subscript V because minimizing (2.3) is also the V-optimality criterion, which minimizes mean prediction variance in active learning.\nIn active learning, we strive to select a subset ` of nodes to query for labels, constrained by a given budget C, such that the risk is minimized. Formally,\narg min `: |`|≤C\nR(`) = RV (`) = tr(L −1 (v−`)). (2.4)"
    }, {
      "heading" : "2.2 Active Learning Objective 2: Survey Risk Minimization (Σ-Optimality)",
      "text" : "Another objective building on the GRF model (2.2) is to determine the proportion of nodes belonging to class 1, as would happen when performing a survey. For active surveying, the risk would be:\nRΣ(`) = Ey`yu ( ∑ ui∈u yui − ∑ ui∈u ŷui )2 = E [ E [( 1Tyu − 1T ŷu )2|y`]] = 1TL−1u 1, (2.5) which could substitute the risk R(`) in (2.4) and yield another heuristic for selecting nodes in batch active learning. We will refer to this modified optimization objective as the Σ-optimality heuristic:\narg min `: |`|≤C\nR(`) = RΣ(`) = 1 TL−1(v−`)1. (2.6)\nFurther, we will also consider the application of Σ-optimality in active classification because (2.6) is another metric of the predictive variance. Surprisingly, although both (2.3) and (2.5) are approximations of the real objective (the 0/1 risk), greedy reduction of the Σ-optimality criterion outperforms greedy reduction of the V-optimality criterion in active classification (Section 3.1 and 5.1), as well as several other methods including expected error reduction."
    }, {
      "heading" : "2.3 Greedy Sequential Application of V/Σ-Optimality",
      "text" : "Both (2.4) and (2.6) are subset optimization problems. Calculating the global optimum may be intractable. As will be shown later in the theoretical results, the reduction of both risks are submodular set functions and the greedy sequential update algorithm yields a solution that has a guaranteed approximation ratio to the optimum (Theorem 1).\nAt the k-th query decision, denote the covariance matrix conditioned on the previous (k−1) queries as C = (L(v−`(k−1)))−1. By Shur’s Lemma (or the GP-regression update rule), the one-step lookahead covariance matrix conditioned on `(k−1) ∪ {v}, denoted as C′ = (L(v−(`(k−1)∪{v})))−1, has the following update formula: (\nC′ 0 0 0\n) = C− 1\nCvv · C:vCv:, (2.7)\nwhere without loss of generality v was positioned as the last node. Further denoting Cij = ρijσiσj , we can put (2.7) inside RΣ(·) and RV (·) to get the following equivalent criteria:\nV-optimality : v(k)∗ = arg max v∈u\n∑ t∈u(Cvt) 2 Cvv = ∑ t∈u ρ2vtσ 2 t , (2.8)\nΣ-optimality : v(k)∗ = arg max v∈u\n( ∑ t∈u Cvt) 2 Cvv = ( ∑ t∈u ρvtσt) 2. (2.9)"
    }, {
      "heading" : "3 Theoretical Results & Insights",
      "text" : "For the general GP model, greedy optimization of the L2 risk has no guarantee that the solution can be comparable to the brute-force global optimum (taking exponential time to compute), because the objective function, the trace of the predictive covariance matrix, fails to satisfy submodularity in all cases (Krause et al., 2008). However, in the special case of GPs with kernel matrix equal to the inverse of a graph Laplacian (with ` 6= ∅ or δ > 0), the GRF does provide such theoretical guarantees, both for V-optimality and Σ-optimality. The latter is a novel result.\nThe following theoretical results concern greedy maximization of the risk reduction function (which is shown to be submodular): R∆(`) = R(∅)−R(`) for either R(·) = RV (·) or RΣ(·). Theorem 1 (Near-optimal guarantee for greedy applications of V/Σ-optimality). In risk reduction,\nR∆(`g) ≥ (1− 1/e) ·R∆(`∗), (3.1) where R∆(`) = R(∅) − R(`) for either R(·) = RV (·) or RΣ(·), e is Euler’s number, `g is the greedy optimizer, and `∗ is the true global optimizer under the constraint |`∗| ≤ |`g|.\nAccording to Nemhauser et al. (1978), it suffices to show the following properties of R∆(`): Lemma 2 (Normalization, Monotonicity, and Submodularity). ∀`1 ⊂ `2 ⊂ v, v ∈ v,\nR∆(∅) = 0, (3.2) R∆(`2) ≥ R∆(`1), (3.3)\nR∆ ( `1 ∪ {v} ) −R∆(`1) ≥ R∆ ( `2 ∪ {v} ) −R∆(`2). (3.4)\nAnother sufficient condition for Theorem 1, which is itself an interesting observation, is the suppressor-free condition. Walker (2003) describes a suppressor as a variable, knowing which will suddenly create a strong correlation between the predictors. An example is yi + yj = yk. Knowing any one of these will create correlations between the others. Walker further states that suppressors are common in regression problems. Das & Kempe (2008) extend the suppressor-free condition to sets and showed that this condition is sufficient to prove (2.3). Formally, the condition is:∣∣corr(yi, yj | `1 ∪ `2)∣∣ ≤ ∣∣corr(yi, yj | `1)∣∣\n∀vi, vj ∈ v,∀`1, `2 ⊂ v. (3.5) It may be easier to understand (3.5) as a decreasing correlation property. It is well known for Markov random fields that the labels of two nodes on a graph become independent given labels of their Markov blanket. Here we establish that GRF boasts more than that: the correlation between any two nodes decreases as more nodes get labeled, even before a Markov blanket is formed. Formally:\nTheorem 3 (Suppressor-Free Condition). (3.5) holds for pairs of nodes in the GRF model. Note that since the conditional covariance of the GRF model is L−1(v−`), we can properly define the corresponding conditional correlation to be\ncorr(yu|`) = D− 1 2L−1(v−`)D − 12 , with D = diag ( L−1(v−`) ) . (3.6)"
    }, {
      "heading" : "3.1 Insights From Comparing the Greedy Applications of the Σ/V-Optimality Criteria",
      "text" : "Both the V/Σ-optimality are approximations to the 0/1 risk minimization objective. Unfortunately, we cannot theoretically reason why greedy Σ-optimality outperforms V-optimality in the experiments. However, we made two observations during our investigation that provide some insights. An illustrative toy example is also provided in Section 5.1.\nObservation 4. Eq. (2.8) and (2.9) suggest that both the greedy Σ/V-optimality selects nodes that (1) have high variance and (2) are highly correlated to high-variance nodes, conditioned on the labeled nodes. Notice Lemma 7 proves that predictive correlations are always nonnegative.\nIn order to contrast Σ/V-optimality, rewrite (2.9) as:\n(Σ-optimality) : arg max v∈u\n( ∑ t∈u ρvtσt) 2 = ∑ t∈u ρ 2 vtσ 2 t + ∑ t1 6=t2∈u ρvt1ρvt2σt1σt2 . (3.7)\nObservation 5. Σ-optimality has one more term that involves cross products of (ρvt1σt1) and (ρvt2σt2) (which are nonnegative according to Lemma 9). By the Cauchy–Schwartz Inequality, the sum of these cross products are maximized when they are equal. So, the Σ-optimality additionally favors nodes that (3) have consistent global influence, i.e., that are more likely to be in cluster centers."
    }, {
      "heading" : "4 Proof Sketches",
      "text" : "Our results predicate on and extend to GPs whose inverse covariance matrix meets Proposition 6.\nProposition 6. L satisfies the following. 1\n# Textual description Mathematical expression\np6.1 L has proper signs. lij ≥ 0 if i = j and lij ≤ 0 if i 6= j. p6.2 L is undirected and connected. lij = lji∀i, j and ∑ j 6=i(−lij) > 0.\np6.3 Node degree no less than number of edges. lii ≥ ∑ j 6=i(−lij) = ∑ j 6=i(−lji) > 0,∀i.\np6.4 L is nonsingular and positive-definite. ∃i : lii > ∑ j 6=i(−lij) = ∑ j 6=i(−lji) > 0.\nAlthough the properties of V-optimality fall into the more general class of spectral functions (Friedland & Gaubert, 2011), we have seen no proof of either the suppressor-free condition or the submodularity of Σ-optimality on GRFs. We write the ideas behind the proofs. Details are in the appendix.2\nLemma 7. For any L satisfying (p6.1-4), L−1 ≥ 0 entry-wise.3\nProof. Sketch: Suppose L = D −W = D(I −D−1W ), with D = diag (L). Then we can show the convergence of the Taylor expansion (Appendix A.1):\nL−1 = [I + ∑∞ r=1(D −1W )r]D−1. (4.1)\nIt suffices to observe that every term on the right hand side (RHS) is nonnegative.\nCorollary 8. The GRF prediction operator L−1u Lul maps y` ∈ [0, 1]|`| to ŷu = −L−1u Luly` ∈ [0, 1]|u|. When L is singular, the mapping is onto.\n1Property p6.4 holds after the first query is done or when the regularizor δ > 0 in (2.1). 2Available at http://www.autonlab.org/autonweb/21763.html 3In the following, for any vector or matrix A, A ≥ 0 always stands for A being (entry-wise) nonnegative.\nProof. For y` = 1, (Lu, Lul) · 1 ≥ 0 and L−1u ≥ 0 imply ( I, L−1u Lul ) · 1 ≥ 0, i.e. 1 ≥ −L−1u Lul1 = ŷu. As both Lu ≥ 0 and −Lul ≥ 0, we have y` ≥ 0⇒ ŷu ≥ 0 and y` ≥ y′` ⇒ ŷu ≥ ŷ′u.\nLemma 9. Suppose L = ( L11 L12 L21 L22 ) . Then L−1 − ( L−111 0 0 0 ) ≥ 0 and is positive-semidefinite.\nProof. As L−1 ≥ 0 and is PSD, the RHS below is term-wise nonnegative and the middle term PSD (Appendix A.2): L−1− ( L−111 0\n0 0\n) = ( L−111 (−L12)\nI\n) (L22−L21L−111 L12)−1 ( (−L21)L−111 , I ) As a corollary, the monotonicity in (3.3) for both R(·) = RV (·) or RΣ(·) can be shown. Both proofs for submodularity in (3.4) and Theorem 3 result from more careful execution of matrix inversions similar to Lemma 9 (detailed in Appendix A.4). We sketch Theorem 3 for example.\nProof. Without loss of generality, let u = v − ` = {1, . . . , k}. By Shur’s Lemma (Appendix A.3):\nL(v−`) := ( Au bu bTu cu ) ⇒ Cov(yi, yk|`) Var(yk|`) = (L−1(v−`))ik (L−1(v−`))kk = (A−1u (−bu))i,∀i 6= k (4.2)\nwhere the LHS is a reparamatrization with cu being a scaler. Lemma 9 shows that u1 ⊃ u2 ⇒ A−1u1 ≥ A −1 u2 at corresponding entries. Also notice that −bu1 ≥ −bu2 at corresponding entries and so the RHS of (4.2) is larger with u1. It suffices to draw a similar inequality in the other direction, Cov(yk, yi|`)/ Var(yi|`)."
    }, {
      "heading" : "5 A Toy Example and Some Simulations",
      "text" : ""
    }, {
      "heading" : "5.1 Comparing V-Optimality and Σ-Optimality: Active Node Classification on a Graph",
      "text" : "To visualize the intuitions described in Section 3.1, Figure 1 shows the first few nodes selected by different optimality criteria. This graph is constructed by a breadth-first search from a random node in a larger DBLP coauthorship network graph that we will introduce in the next section. On this toy graph, both criteria pick the same center node to query first. However, for the second and third queries, Voptimality weighs the uncertainty of the candidate node more, choosing outliers, whereas Σ-optimality favors nodes with universal influence over the graph and goes to cluster centers."
    }, {
      "heading" : "5.2 Simulating Labels on a Graph",
      "text" : "To further investigate the behavior of Σ- and V - optimality, we conducted experiments on syn-\nthetic labels generated on real-world network graphs. The node labels were first simulated using the model in order to compare the active learning criteria directly without raising questions of model fit. We carry out tests on the same graphs with real data in the next section.\nWe simulated the binary labels with the GRF-sigmoid model and performed active learning with the GRF/LP model for predictions. The parameters in the generation phase were β = 0.01 and δ = 0.05, which maximizes the average classification accuracy increases from 50 random training nodes to 200 random training nodes using the GRF/LP model for predictions. Figure 2 shows the binary classification accuracy versus the number of queries on both the DBLP coauthorship graph\nand the CORA citation graph that we will describe below. The best possible classification results are indicated by the leave-one-out (LOO) accuracies given under each plot.\nFigure 2 can be a surprise due to the reasoning behind the L2 surrogate loss, especially when the predictive means are trapped between [−1, 1], but we see here that our reasoning in Sections (3.1 and 5.1) can lead to the greedy survey loss actually making a better active learning objective.\nWe have also performed experiments with different values of β and δ. Despite the fact that larger β and δ increase label independence on the graph structure and undermine the effectiveness of both V/Σ-optimality heuristics, we have seen that whenever the V-optimality establishes a superiority over random selections, Σ-optimality yields better performance."
    }, {
      "heading" : "6 Real-World Experiments",
      "text" : "The active learning heuristics to be compared are:4 1. The new Σ-optimality with greedy sequential updates: minv′ ( 1>(Luk\\{v′}) −11 ) .\n2. Greedy V-optimality (Ji & Han, 2012): minv′ tr ( (Luk\\{v′}) −1) . 3. Mutual information gain (MIG) (Krause et al., 2008): maxv′ ( L−1 uk ) v′,v′ / ( (L`k∪{v′}) −1) v′,v′ 4. Uncertainty sampling (US) picking the largest prediction margin: maxv′ ŷ (1) v′ − ŷ (2) v′ . 5. Expected error reduction (EER) (Settles, 2010; Zhu et al., 2003). Selected nodes maximize the average prediction confidence in expectation: maxv′ Eyv′ [(∑ ui∈uŷ (1) ui\n∣∣∣yv′)∣∣∣y`k] . 6. Random selection with 12 repetitions.\nComparisons are made on three real-world network graphs.\n1. DBLP coauthorship network.5 The nodes represent scholars and the weighted edges are the number of papers bearing both scholars’ names. The largest connected component has 1711 nodes and 2898 edges. The node labels were hand assigned in Ji & Han (2012) to one of the four expertise areas of the scholars: machine learning, data mining, information retrieval, and databases. Each class has around 400 nodes. 2. Cora citation network.6 This is a citation graph of 2708 publications, each of which is classified into one of seven classes: case based, genetic algorithms, neural networks, probabilistic methods, reinforcement learning, rule learning, and theory. The network has 5429 links. We took its largest connected component, with 2485 nodes and 5069 undirected and unweighted edges.\n4Code available at http://www.autonlab.org/autonweb/21763 5 http://www.informatik.uni-trier.de/˜ley/db/ 6 http://www.cs.umd.edu/projects/linqs/projects/lbc/index.html\n3. CiteSeer citation network.6 This is another citation graph of 3312 publications, each of which is classified into one of six classes: agents, artificial intelligence, databases, information retrieval, machine learning, human computer interaction. The network has 4732 links. We took its largest connected component, with 2109 nodes and 3665 undirected and unweighted edges.\nOn all three datasets, Σ-optimality outperforms other methods by a large margin especially during the first five to ten queries. The runner-up, EER, catches up to Σ-optimality in some cases, but EER does not have theoretical guarantees.\nThe win of Σ-optimality over V-optimality has been intuitively explained in Section 5.1 as Σoptimality having better exploration ability and robustness against outliers. The node choices by both criteria were also visually inspected after embedding the graph to the 2-dimensional space using OpenOrd method developed by Martin et al. (2011). The analysis there was similar to Figure 1.\nWe also performed real-world experiments on the root-mean-square-error of the class proportion estimations, which is the survey risk that the Σ-optimality minimizes. Σ-optimality beats V-optimality. Details were omitted for space concerns."
    }, {
      "heading" : "7 Conclusion",
      "text" : "For active learning on GRFs, it is common to use variance minimization criteria with greedy onestep lookahead heuristics. V-optimality and Σ-optimality are two criteria based on statistics of the predictive covariance matrix. They both are also risk minimization criteria: V-optimality minimizes the L2 risk (2.3), whereas Σ-optimality minimizes the survey risk (2.5).\nActive learning with both criteria can be seen as subset optimization problems (2.4), (2.6). Both objective functions are supermodular set functions. Therefore, risk reduction is submodular and the greedy one-step lookahead heuristics can achieve a (1− 1/e) global optimality ratio. Moreover, we have shown that GRFs serve as a tangible example of the suppressor-free condition.\nWhile the V-optimality on GRFs inherits from label propagation (and random walk with absorptions) and have good empirical performance, it is not directly minimizing the 0/1 classification risk. We found that the Σ-optimality performs even better. The intuition is described in Section 5.1.\nFuture work include deeper understanding of the direct motivations behind Σ-optimality on the GRF classification model and extending the GRF to continuous spaces."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is funded in part by NSF grant IIS0911032 and DARPA grant FA87501220324."
    } ],
    "references" : [ {
      "title" : "Algorithms for subset selection in linear regression",
      "author" : [ "Das", "Abhimanyu", "Kempe", "David" ],
      "venue" : "In Proceedings of the 40th annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Das et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2008
    }, {
      "title" : "Submodular spectral functions of principal submatrices of a hermitian matrix, extensions and applications",
      "author" : [ "S Friedland", "S. Gaubert" ],
      "venue" : "Linear Algebra and its Applications,",
      "citeRegEx" : "Friedland and Gaubert,? \\Q2011\\E",
      "shortCiteRegEx" : "Friedland and Gaubert",
      "year" : 2011
    }, {
      "title" : "Bayesian optimal active search and surveying",
      "author" : [ "Garnett", "Roman", "Krishnamurthy", "Yamuna", "Xiong", "Xuehan", "Schneider", "Jeff", "Mann", "Richard" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Garnett et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Garnett et al\\.",
      "year" : 2012
    }, {
      "title" : "A variance minimization criterion to active learning on graphs",
      "author" : [ "Ji", "Ming", "Han", "Jiawei" ],
      "venue" : "In AISTAT,",
      "citeRegEx" : "Ji et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2012
    }, {
      "title" : "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies",
      "author" : [ "Krause", "Andreas", "Singh", "Ajit", "Guestrin", "Carlos" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "Krause et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2008
    }, {
      "title" : "Openord: an opensource toolbox for large graph layout. In IS&T/SPIE Electronic Imaging, pp. 786806–786806",
      "author" : [ "Martin", "Shawn", "Brown", "W Michael", "Klavans", "Richard", "Boyack", "Kevin W" ],
      "venue" : "International Society for Optics and Photonics,",
      "citeRegEx" : "Martin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2011
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functionsi",
      "author" : [ "Nemhauser", "George L", "Wolsey", "Laurence A", "Fisher", "Marshall L" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Nemhauser et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Nemhauser et al\\.",
      "year" : 1978
    }, {
      "title" : "Gaussian processes for machine learning, volume 1. MIT press",
      "author" : [ "Rasmussen", "Carl Edward", "Williams", "Christopher KI" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen et al\\.",
      "year" : 2006
    }, {
      "title" : "Active learning literature survey",
      "author" : [ "Settles", "Burr" ],
      "venue" : "University of Wisconsin,",
      "citeRegEx" : "Settles and Burr.,? \\Q2010\\E",
      "shortCiteRegEx" : "Settles and Burr.",
      "year" : 2010
    }, {
      "title" : "Suppressor variable (s) importance within a regression model: an example of salary compression from career services",
      "author" : [ "Walker", "David A" ],
      "venue" : "Journal of College Student Development,",
      "citeRegEx" : "Walker and A.,? \\Q2003\\E",
      "shortCiteRegEx" : "Walker and A.",
      "year" : 2003
    }, {
      "title" : "Learning with partially absorbing random walks",
      "author" : [ "Wu", "Xiao-Ming", "Li", "Zhenguo", "So", "Anthony Man-Cho", "Wright", "John", "Chang", "Shih-Fu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Wu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning from labeled and unlabeled data with label propagation",
      "author" : [ "Zhu", "Xiaojin", "Ghahramani", "Zoubin" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2002
    }, {
      "title" : "Combining active learning and semisupervised learning using gaussian fields and harmonic functions",
      "author" : [ "Zhu", "Xiaojin", "Lafferty", "John", "Ghahramani", "Zoubin" ],
      "venue" : "In ICML 2003 workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "It can generalize two popular and intuitive algorithms: label propagation (Zhu & Ghahramani, 2002), and random walk with absorptions (Wu et al., 2012).",
      "startOffset" : 133,
      "endOffset" : 150
    }, {
      "referenceID" : 12,
      "context" : "Some popular criteria are empirical risk minimization (Settles, 2010; Zhu et al., 2003), mutual information gain (Krause et al.",
      "startOffset" : 54,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : ", 2003), mutual information gain (Krause et al., 2008), and V-optimality (Ji & Han, 2012).",
      "startOffset" : 33,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "The model for label prediction in this paper is the harmonic function on the Gaussian random field (GRF) by Zhu et al. (2003). It can generalize two popular and intuitive algorithms: label propagation (Zhu & Ghahramani, 2002), and random walk with absorptions (Wu et al.",
      "startOffset" : 108,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "Nemhauser et al. (1978) shows that any submodular, monotone and normalized set function yields a (1 − 1/e) global optimality guarantee for greedy solutions.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Nemhauser et al. (1978) shows that any submodular, monotone and normalized set function yields a (1 − 1/e) global optimality guarantee for greedy solutions. Our proof techniques coincides with Friedland & Gaubert (2011) in principle, but we are not restricted to spectral functions.",
      "startOffset" : 0,
      "endOffset" : 220
    }, {
      "referenceID" : 4,
      "context" : "Krause et al. (2008) showed a counter example where the V-optimality objective function with GP models does not satisfy submodularity.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Σ-optimality was originally proposed by Garnett et al. (2012) in the context of active surveying, which is to determine the proportion of nodes belonging to one class.",
      "startOffset" : 40,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "For multi-class problems, Zhu et al. (2003) proposed the harmonic predictor which looks at predictive means in one-versus-all comparisons.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "For the general GP model, greedy optimization of the L(2) risk has no guarantee that the solution can be comparable to the brute-force global optimum (taking exponential time to compute), because the objective function, the trace of the predictive covariance matrix, fails to satisfy submodularity in all cases (Krause et al., 2008).",
      "startOffset" : 311,
      "endOffset" : 332
    }, {
      "referenceID" : 4,
      "context" : "For the general GP model, greedy optimization of the L(2) risk has no guarantee that the solution can be comparable to the brute-force global optimum (taking exponential time to compute), because the objective function, the trace of the predictive covariance matrix, fails to satisfy submodularity in all cases (Krause et al., 2008). However, in the special case of GPs with kernel matrix equal to the inverse of a graph Laplacian (with ` 6= ∅ or δ > 0), the GRF does provide such theoretical guarantees, both for V-optimality and Σ-optimality. The latter is a novel result. The following theoretical results concern greedy maximization of the risk reduction function (which is shown to be submodular): R∆(`) = R(∅)−R(`) for either R(·) = RV (·) or RΣ(·). Theorem 1 (Near-optimal guarantee for greedy applications of V/Σ-optimality). In risk reduction, R∆(`g) ≥ (1− 1/e) ·R∆(`∗), (3.1) where R∆(`) = R(∅) − R(`) for either R(·) = RV (·) or RΣ(·), e is Euler’s number, `g is the greedy optimizer, and `∗ is the true global optimizer under the constraint |`∗| ≤ |`g|. According to Nemhauser et al. (1978), it suffices to show the following properties of R∆(`): Lemma 2 (Normalization, Monotonicity, and Submodularity).",
      "startOffset" : 312,
      "endOffset" : 1103
    }, {
      "referenceID" : 4,
      "context" : "Mutual information gain (MIG) (Krause et al., 2008): maxv′ ( L−1 uk ) v′,v′ / ( (L`k∪{v′}) −1) v′,v′ 4.",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "The node choices by both criteria were also visually inspected after embedding the graph to the 2-dimensional space using OpenOrd method developed by Martin et al. (2011). The analysis there was similar to Figure 1.",
      "startOffset" : 150,
      "endOffset" : 171
    } ],
    "year" : 2013,
    "abstractText" : "A common classifier for unlabeled nodes on undirected graphs uses label propagation from the labeled nodes, equivalent to the harmonic predictor on Gaussian random fields (GRFs). For active learning on GRFs, the commonly used V-optimality criterion queries nodes that reduce the L (regression) loss. V-optimality satisfies a submodularity property showing that greedy reduction produces a (1− 1/e) globally optimal solution. However, L loss may not characterise the true nature of 0/1 loss in classification problems and thus may not be the best choice for active learning. We consider a new criterion we call Σ-optimality, which queries the node that minimizes the sum of the elements in the predictive covariance. Σ-optimality directly optimizes the risk of the surveying problem, which is to determine the proportion of nodes belonging to one class. In this paper we extend submodularity guarantees from V-optimality to Σ-optimality using properties specific to GRFs. We further show that GRFs satisfy the suppressor-free condition in addition to the conditional independence inherited from Markov random fields. We test Σoptimality on real-world graphs with both synthetic and real data and show that it outperforms V-optimality and other related methods on classification.",
    "creator" : null
  }
}