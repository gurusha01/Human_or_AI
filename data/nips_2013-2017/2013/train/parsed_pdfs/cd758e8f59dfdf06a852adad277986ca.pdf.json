{
  "name" : "cd758e8f59dfdf06a852adad277986ca.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cluster Trees on Manifolds",
    "authors" : [ "Sivaraman Balakrishnan", "Srivatsan Narayanan", "Alessandro Rinaldo", "Aarti Singh", "Larry Wasserman" ],
    "emails" : [ "sbalakri@cs.cmu.edu", "srivatsa@cs.cmu.edu", "arinaldo@stat.cmu.edu", "aarti@cs.cmu.edu", "larry@stat.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Cluster Trees on Manifolds\nSivaraman Balakrishnan† sbalakri@cs.cmu.edu Srivatsan Narayanan† srivatsa@cs.cmu.edu Alessandro Rinaldo‡ arinaldo@stat.cmu.edu\nAarti Singh† aarti@cs.cmu.edu Larry Wasserman‡ larry@stat.cmu.edu\nSchool of Computer Science† and Department of Statistics‡\nCarnegie Mellon University\nIn this paper we investigate the problem of estimating the cluster tree for a density f supported on or near a smooth d-dimensional manifold M isometrically embedded in RD. We analyze a modified version of a k-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta (2010). The main results of this paper show that under mild assumptions on f and M , we obtain rates of convergence that depend on d only but not on the ambient dimension D. Finally, we sketch a construction of a sample complexity lower bound instance for a natural class of manifold oblivious clustering algorithms."
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper, we study the problem of estimating the cluster tree of a density when the density is supported on or near a manifold. Let X := {X1, . . . , Xn} be a sample drawn i.i.d. from a distribution P with density f . The connected components Cf (λ) of the upper level set {x : f(x) ≥ λ} are called density clusters. The collection C = {Cf (λ) : λ ≥ 0} of all such clusters is called the cluster tree and estimating this cluster tree is referred to as density clustering.\nThe density clustering paradigm is attractive for various reasons. One of the main difficulties of clustering is that often the true goals of clustering are not clear and this makes clusters, and clustering as a task seem poorly defined. Density clustering however is estimating a well defined population quantity, making its goal, consistent recovery of the population density clusters, clear. Typically only mild assumptions are made on the density f and this allows extremely general shapes and numbers of clusters at each level. Finally, the cluster tree is an inherently hierarchical object and thus density clustering algorithms typically do not require specification of the “right” level, rather they capture a summary of the density across all levels.\nThe search for a simple, statistically consistent estimator of the cluster tree has a long history. Hartigan (1981) showed that the popular single-linkage algorithm is not consistent for a sample from RD, with D > 1. Recently, Chaudhuri and Dasgupta (2010) analyzed an algorithm which is both simple and consistent. The algorithm finds the connected components of a sequence of carefully constructed neighborhood graphs. They showed that, as long as the parameters of the algorithm are chosen appropriately, the resulting collection of connected components correctly estimates the cluster tree with high probability.\nIn this paper, we are concerned with the problem of estimating the cluster tree when the density f is supported on or near a low dimensional manifold. The motivation for this work stems from the problem of devising and analyzing clustering algorithms with provable performance that can be used in high dimensional applications. When data live in high dimensions, clustering (as well as other statistical tasks) generally become prohibitively difficult due to the curse of dimensionality,\nwhich demands a very large sample size. In many high dimensional applications however data is not spread uniformly but rather concentrates around a low dimensional set. This so-called manifold hypothesis motivates the study of data generated on or near low dimensional manifolds and the study of procedures that can adapt effectively to the intrinsic dimensionality of this data.\nHere is a brief summary of the main contributions of our paper: (1) We show that the simple algorithm studied in the paper Chaudhuri and Dasgupta (2010) is consistent and has fast rates of convergence for data on or near a low dimensional manifold M . The algorithm does not require the user to first estimate M (which is a difficult problem). In other words, the algorithm adapts to the (unknown) manifold. (2) We show that the sample complexity for identifying salient clusters is independent of the ambient dimension. (3) We sketch a construction of a sample complexity lower bound instance for a natural class of clustering algorithms that we study in this paper. (4) We introduce a framework for studying consistency of clustering when the distribution is not supported on a manifold but rather, is concentrated near a manifold. The generative model in this case is that the data are first sampled from a distribution on a manifold and then noise is added. The original data are latent (unobserved). We show that for certain noise models we can still efficiently recover the cluster tree on the latent samples."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "The idea of using probability density functions for clustering dates back to Wishart Wishart (1969). Hartigan (1981) expanded on this idea and formalized the notions of high-density clustering, of the cluster tree and of consistency and fractional consistency of clustering algorithms. In particular, Hartigan (1981) showed that single linkage clustering is consistent when D = 1 but is only fractionally consistent when D > 1. Stuetzle and R. (2010) and Stuetzle (2003) have also proposed procedures for recovering the cluster tree. None of these procedures however, come with the theoretical guarantees given by Chaudhuri and Dasgupta (2010), which demonstrated that a generalization of Wishart’s algorithm allows one to estimate parts of the cluster tree for distributions with fulldimensional support near-optimally under rather mild assumptions. This paper forms the starting point for our work and is reviewed in more detail in the next section.\nIn the last two decades, much of the research effort involving the use of nonparametric density estimators for clustering has focused on the more specialized problems of optimal estimation of the support of the distribution or of a fixed level set. However, consistency of estimators of a fixed level set does not imply cluster tree consistency, and extending the techniques and analyses mentioned above to hold simultaneously over a variety of density levels is non-trivial. See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al. (2009); Rinaldo and Wasserman (2010); Rinaldo et al. (2012), and references therein. Estimating the cluster tree has more recently been considered by Kpotufe and von Luxburg (2011) who also give a simple pruning procedure for removing spurious clusters. Steinwart (2011) and Sriperumbudur and Steinwart (2012) propose procedures for determining recursively the lowest split in the cluster tree and give conditions for asymptotic consistency with minimal assumptions on the density."
    }, {
      "heading" : "2 Background and Assumptions",
      "text" : "Let P be a distribution supported on an unknown d-dimensional manifold M . We assume that the manifold M is a d-dimensional Riemannian manifold without boundary embedded in a compact set X ⊂ RD with d < D. We further assume that the volume of the manifold is bounded from above by a constant, i.e., vold(M) ≤ C. The main regularity condition we impose on M is that its condition number be not too large. The condition number of M is 1/τ , where τ is the largest number such that the open normal bundle about M of radius r is imbedded in RD for every r < τ . The condition number is discussed in more detail in the paper Niyogi et al. (2008).\nThe Euclidean norm is denoted by ‖ · ‖ and vd denotes the volume of the d-dimensional unit ball in R d. B(x, r) denotes the full-dimensional ball of radius r centered at x and BM (x, r) ..= B(x, r) ∩\nM . For Z ⊂ Rd and σ > 0, define Zσ = Z +B(0, σ) and ZM,σ = (Z +B(0, σ)) ∩M . Note that Zσ is full dimensional, while if Z ⊆ M then ZM,σ is d-dimensional.\nLet f be the density of P with respect to the uniform measure on M . For λ ≥ 0, let Cf (λ) be the collection of connected components of the level set {x ∈ X : f(x) ≥ λ} and define the cluster tree of f to be the hierarchy C = {Cf (λ) : λ ≥ 0}. For a fixed λ, any member of Cf (λ) is a cluster. For a cluster C its restriction to the sample X is defined to be C[X] = C ∩ X. The restriction of the cluster tree C to X is defined to be C[X] = {C ∩ X : C ∈ C}. Informally, this restriction is a dendrogram-like hierarchical partition of X.\nTo give finite sample results, following Chaudhuri and Dasgupta (2010), we define the notion of salient clusters. Our definitions are slight modifications of those in Chaudhuri and Dasgupta (2010) to take into account the manifold assumption.\nDefinition 1 Clusters A and A′ are (σ, ǫ) separated if there exists a nonempty S ⊂ M such that:\n1. Any path along M from A to A′ intersects S. 2. supx∈SM,σ f(x) < (1− ǫ) infx∈AM,σ∪A′M,σ f(x).\nChaudhuri and Dasgupta (2010) analyze a robust single linkage (RSL) algorithm (in Figure 1). An RSL algorithm estimates the connected components at a level λ in two stages. In the first stage, the sample is cleaned by thresholding the k-nearest neighbor distance of the sample points at a radius r and then, in the second stage, the cleaned sample is connected at a connection radius R. The connected components of the resulting graph give an estimate of the restriction Cf (λ)[X]. In Section 4 we prove a sample complexity lower bound for the class of RSL algorithms which we now define.\nDefinition 2 The class of RSL algorithms refers to any algorithm that is of the form described in the algorithm in Figure 1 and relying on Euclidean balls, with any choice of k, r and R.\nWe define two notions of consistency for an estimator Ĉ of the cluster tree:\nDefinition 3 (Hartigan consistency) For any sets A, A′ ⊂ X , let An (resp., A′n) denote the smallest cluster of Ĉ containing A ∩X (resp, A′ ∩X). We say Ĉ is consistent if, whenever A and A′ are different connected components of {x : f(x) ≥ λ} (for some λ > 0), the probability that An is disconnected from A′n approaches 1 as n → ∞.\nDefinition 4 ((σ, ǫ) consistency) For any sets A, A′ ⊂ X such that A and A′ are (σ, ǫ) separated, let An (resp., A′n) denote the smallest cluster of Ĉ containing A ∩ X (resp, A′ ∩ X). We say Ĉ is consistent if, whenever A and A′ are different connected components of {x : f(x) ≥ λ} (for some λ > 0), the probability that An is disconnected from A′n approaches 1 as n → ∞.\nThe notion of (σ, ǫ) consistency is similar that of Hartigan consistency except restricted to (σ, ǫ) separated clusters A and A′.\nChaudhuri and Dasgupta (2010) prove a theorem, establishing finite sample bounds for a particular RSL algorithm. In their result there is no manifold and f is a density with respect to the Lebesgue measure on RD. Their result in essence says that if\nn ≥ O (\nD\nλǫ2vD(σ/2)D log\nD\nλǫ2vD(σ/2)D\n)\nthen an RSL algorithm with appropriately chosen parameters can resolve any pair of (σ, ǫ) clusters at level at least λ. It is important to note that this theorem does not apply to the setting when distributions are supported on a lower dimensional set for at least two reasons: (1) the density f is singular with respect to the Lebesgue measure on X and so the cluster tree is trivial, and (2) the definitions of saliency with respect to X are typically not satisfied when f has a lower dimensional support."
    }, {
      "heading" : "3 Clustering on Manifolds",
      "text" : "In this section we show that the RSL algorithm can be adapted to recover the cluster tree of a distribution supported on a manifold of dimension d < D with the rates depending only on d. In place of the cluster salience parameter σ, our rates involve a new parameter ρ\nρ := min\n( 3σ\n16 , ǫτ 72d , τ 16\n) .\nThe precise reason for this definition of ρ will be clear from the proofs (particularly of Lemma 7) but for now notice that in addition to σ it is dependent on the condition number 1/τ and deteriorates as the condition number increases. Finally, to succinctly present our results we use µ := log n + d log(1/ρ).\nTheorem 5 There are universal constants C1 and C2 such that the following holds. For any δ > 0, 0 < ǫ < 1/2, run the algorithm in Figure 1 on a sample X drawn from f , where the parameters are set according to the equations\nR = 4ρ and k = C1 log 2(1/δ)(µ/ǫ2).\nThen with probability at least 1−δ, Ĉ is (σ, ǫ) consistent. In particular, the clusters containing A[X] and A′[X], where A and A′ are (σ, ǫ) separated, are internally connected and mutually disconnected in C(r) for r defined by\nvdr dλ =\n1\n1− ǫ/6\n( k\nn +\nC2 log(1/δ)\nn\n√ kµ\n)\nprovided λ ≥ 2 vdρd k n .\nBefore we prove this theorem a few remarks are in order:\n1. To obtain an explicit sample complexity we plug in the value of k and solve for n from the inequality restricting λ. The sample complexity of the RSL algorithm for recovering (σ, ǫ) clusters at level at least λ on a manifold M with condition number at most 1/τ is\nn = O\n( d\nλǫ2vdρd log\nd\nλǫ2vdρd\n)\nwhere ρ = Cmin (σ, ǫτ/d, τ). Ignoring constants that depend on d the main difference between this result and the result of Chaudhuri and Dasgupta (2010) is that our results only depend on the manifold dimension d and not the ambient dimension D (typically D ≫ d). There is also a dependence of our result on 1/(ǫτ)d, for ǫτ ≪ σ. In Section 4 we sketch the construction of an instance that suggests that this dependence is not an artifact of our analysis and that the sample complexity of the class of RSL algorithms is at least n ≥ 1/(ǫτ)Ω(d).\n2. Another aspect is that our choice of the connection radius R depends on the (typically) unknown ρ, while for comparison, the connection radius in Chaudhuri and Dasgupta (2010) is chosen to be\n√ 2r. Under the mild assumption that λ ≤ nO(1) (which is satisfied for instance, if the density on M is bounded from above), we show in Appendix A.8 that an identical theorem holds for R = 4r. k is the only real tuning parameter of this algorithm whose choice depends on ǫ and an unknown leading constant.\n3. It is easy to see that this theorem also establishes consistency for recovering the entire cluster tree by selecting an appropriate schedule on σn, ǫn and kn that ensures that all clusters are distinguished for n large enough (see Chaudhuri and Dasgupta (2010) for a formal proof).\nOur proofs structurally mirror those in Chaudhuri and Dasgupta (2010). We begin with a few technical results in 3.1. In Section 3.2 we establish (σ, ǫ) consistency by showing that the clusters are mutually disjoint and internally connected. The main technical challenge is that the curvature of the manifold, modulated by its condition number 1/τ , limits our ability to resolve the density level sets from a finite sample, by limiting the maximum cleaning and connection radii the algorithm can use. In what follows, we carefully analyze this effect and show that somewhat surprisingly, despite this curvature, essentially the same algorithm is able to adapt to the unknown manifold and produce a consistent estimate of the entire cluster tree. Similar manifold adaptivity results have been shown in classification Dasgupta and Freund (2008) and in non-parametric regression Kpotufe and Dasgupta (2012); Bickel and Li (2006)."
    }, {
      "heading" : "3.1 Technical results",
      "text" : "In our proof, we use the uniform convergence of the empirical mass of Euclidean balls to their true mass. In the full dimensional setting of Chaudhuri and Dasgupta (2010), this follows from standard VC inequalities. To the best of our knowledge however sharp (ambient dimension independent) inequalities for manifolds are unknown. We get around this obstacle by using the insight that, in order to analyze the RSL algorithms, uniform convergence for Euclidean balls around the sample points and around a fixed minimum s-net N of M (for an appropriately chosen s) suffice to analyze the RSL algorithm.\nRecall, an s-net N ⊆ M is such that every point of M is at a distance at most s from some point in N . Let Bn,N := { B(z, s) : z ∈ N ∪ X, s ≥ 0 } be the collection of balls whose centers are\nsample or net points. We now state our uniform convergence lemma. The proof is in Appendix A.3.\nLemma 6 (Uniform Convergence) Assume k ≥ µ. Then there exists a constant C0 such that the following holds. For every δ > 0, with probability > 1− δ, for all B ∈ Bn,N , we have:\nP (B) ≥ Cδµ n =⇒ Pn(B) > 0,\nP (B) ≥ k n + Cδ n\n√ kµ =⇒ Pn(B) ≥ k\nn ,\nP (B) ≤ k n − Cδ n\n√ kµ =⇒ Pn(B) < k\nn ,\nwhere Cδ := 2C0 log(2/δ), and µ := 1 + log n + log |N | = Cd + log n + d log(1/s). Here Pn(B) = |X∩B|/n denotes the empirical probability measure of B, and C is a universal constant.\nNext we provide a tight estimate of the volume of a small ball intersected with M . This bounds the distortion of the apparent density due to the curvature of the manifold and is central to many of our arguments. Intuitively, the claim states that the volume is approximately that of a d-dimensional Euclidean ball, provided that its radius is small enough compared to τ . The lower bound is based on Lemma 5.3 of Niyogi et al. (2008) while the upper bound is based on a modification of the main result of Chazal (2013).\nLemma 7 (Ball volumes) Assume r < τ/2. Define S := B(x, r) ∩M for a point x ∈ M . Then ( 1− r 2\n4τ2\n)d/2 vdr d ≤ vold(S) ≤ vd ( τ\nτ − 2r1\n)d rd1 ,\nwhere r1 = τ − τ √\n1− 2r/τ . In particular, if r ≤ ǫτ/72d for 0 ≤ ǫ < 1, then vdr d(1− ǫ/6) ≤ vold(S) ≤ vdrd(1 + ǫ/6)."
    }, {
      "heading" : "3.2 Separation and Connectedness",
      "text" : "Lemma 8 (Separation) Assume that we pick k, r and R to satisfy the conditions:\nr ≤ ρ, R = 4ρ,\nvdr d(1− ǫ/6)λ ≥ k\nn + Cδ n\n√ kµ, vdr\nd(1 + ǫ/6)λ(1− ǫ) ≤ k n − Cδ n\n√ kµ.\nThen with probability 1 − δ, we have: (1) All points in Aσ−r and A′σ−r are kept, and all points in Sσ−r are removed. (2) The two point sets A ∩X and A′ ∩X are disconnected in Gr,R.\nProof. The proof is analogous to the separation proof of Chaudhuri and Dasgupta (2010) with several modifications. Most importantly, we need to ensure that despite the curvature of the manifold we can still resolve the density well enough to guarantee that we can identify and eliminate points in the region of separation.\nThroughout the proof, we will assume that the good event in Lemma 6 (uniform convergence for Bn,N ) occurs. Since r ≤ ǫτ/72d, by Lemma 7 vol(BM (x, r)) is between vdrd(1 − ǫ/6) and vdr\nd(1+ǫ/6), for any x ∈ M . So if Xi ∈ A∪A′, then BM (Xi, r) has mass at least vdrd(1−ǫ/6)·λ. Since this is ≥ kn + Cδn √ kµ by assumption, this ball contains at least k sample points, and hence Xi is kept. On the other hand, if Xi ∈ Sσ−r, then the set BM (Xi, r) contains mass at most vdr d(1+ ǫ/6) ·λ(1− ǫ). This is ≤ kn − Cδn √ kµ. Thus by Lemma 6 BM (Xi, r) contains fewer than k sample points, and hence Xi is removed.\nTo prove the graph is disconnected, we first need a bound on the geodesic distance between two points that are at most R apart in Euclidean distance. Such an estimate follows from Proposition 6.3 in Niyogi et al. (2008) who show that if ‖p − q‖ = R ≤ τ/2, then the geodesic distance dM (p, q) ≤ τ − τ √ 1− 2Rτ . In particular, if R ≤ τ/4, then dM (p, q) < R ( 1 + 4Rτ ) ≤ 2R. Now, notice that if the graph is connected there must be an edge that connects two points that are at a geodesic distance of at least 2(σ − r). Any path between a point in A and a point in A′ along M must pass through Sσ−r and must have a geodesic length of at least 2(σ − r). This is impossible if the connection radius satisfies 2R < 2(σ − r), which follows by the assumptions on r and R.\nAll the conditions in Lemma 8 can be simultaneously satisfied by setting k := 16C2δ (µ/ǫ 2), and\nvdr d(1− ǫ/6) · λ = k\nn + Cδ n\n√ kµ. (1)\nThe condition on r is satisfied since λ ≥ 2 vdρd k n and the condition on R is satisfied by its definition.\nLemma 9 (Connectedness) Assume that the parameters k, r and R satisfy the separation conditions (in Lemma 8). Then, with probability at least 1− δ, A[X] is connected in Gr,R.\nProof. Let us show that any two points in A ∩X are connected in Gr,R. Consider y, y′ ∈ A ∩X. Since A is connected, there is a path P between y, y′ lying entirely inside A, i.e., a continuous map P : [0, 1] → A such that P (0) = y and P (1) = y′. We can find a sequence of points y0, . . . , yt ∈ P such that y0 = y, yt = y′, and the geodesic distance on M (and hence the Euclidean distance) between yi−1 and yi is at most η, for an arbitrarily small constant η.\nLet N be minimal R/4-net of M . There exist zi ∈ N such that ‖yi− zi‖ ≤ R/4. Since yi ∈ A, we have zi ∈ AM,R/4, and hence the ball BM (zi, R/4) lies completely inside AM,R/2 ⊆ AM,σ−r. In particular, the density inside the ball is at least λ everywhere, and hence the mass inside it is at least\nvd(R/4) d(1− ǫ/6)λ ≥ Cδµ\nn .\nObserve that R ≥ 4r and so this condition is satisfied as a consequence of satisfying Equation 1. Thus Lemma 6 guarantees that the ball BM (zi, R/4) contains at least one sample point, say xi. (Without loss of generality, we may assume x0 = y and xt = y′.) Since the ball lies completely in AM,σ−r, the sample point xi is not removed in the cleaning step (Lemma 8).\nFinally, we bound d(xi−1, xi) by considering the sequence of points (xi−1, zi−1, yi−1, yi, zi, xi). The pair (yi−1, yi) are at most s apart and the other successive pairs at most R/4 apart, hence d(xi−1, xi) ≤ 4(R/4) + η = R+ η. The claim follows by letting η → 0."
    }, {
      "heading" : "4 A lower bound instance for the class of RSL algorithms",
      "text" : "Recall that the sample complexity in Theorem 5 scales as n = O (\nd λǫ2vdρd log d λǫ2vdρd\n) where\nρ = Cmin (σ, ǫτ/d, τ). For full dimensional densities, Chaudhuri and Dasgupta (2010) showed the information theoretic lower bound n = Ω (\n1 λǫ2vDσD log 1 λǫ2vDσD\n) . Their construction can be\nstraightforwardly modified to a d-dimensional instance on a smooth manifold. Ignoring constants that depend on d, these upper and lower bounds can still differ by a factor of 1/(ǫτ)d, for ǫτ ≪ σ. In this section we provide an informal sketch of a hard instance for the class of RSL algorithms (see Definition 2) that suggests a sample complexity lower bound of n ≥ 1/(ǫτ)Ω(d).\nWe first describe our lower bound instance. The manifold M consists of two disjoint components, C and C ′ (whose sole function is to ensure f integrates to 1). The component C in turn contains three parts, which we call ‘top’, ‘middle’, and ‘bottom’ respectively. The middle part, denoted M2, is the portion of the standard d-dimensional unit sphere Sd(0, 1) between the planes x1 = + √ 1− 4τ2\nand x1 = − √ 1− 4τ2. The top part, denoted M1, is the upper hemisphere of radius 2τ centered\nat (+ √ 1− 4τ2, 0, 0, . . . , 0). The bottom part, denoted M3, is a symmetric hemisphere centered at\n(− √ 1− 4τ2, 0, 0, . . . , 0). Thus C is obtained by gluing a portion of the unit sphere with two (small) hemispherical caps. C as described does not have a condition number at most 1/τ because of the “corners” at the intersection of M2 and M1 ∪M3. This can be fixed without affecting the essence of the construction by smoothing this intersection by rolling a ball of radius τ around it (a similar construction is made rigorous in Theorem 6 of Genovese et al. (2012)). Let P be the distribution on M whose density over C is λ if |x1| > 1/2, and λ(1 − ǫ) if |x1| ≤ 1/2, where λ is chosen small enough such that λ vold(C) ≤ 1. The density over C ′ is chosen such that the total mass of the manifold is 1. Now M1 and M3 are (σ, ǫ) separated at level λ for σ = Ω(1). The separator set S is the equator of M2 in the plane x1 = 0.\nWe now provide some intuition for why RSL algorithms will require n ≥ 1/(ǫτ)Ω(d) to succeed on this instance. We focus our discussion on RSL algorithms with k > 2, i.e. on algorithms that do in fact use a cleaning step, ignoring the single linkage algorithm which is known to be inconsistent for full dimensional densities. Intuitively, because of the curvature of the described instance, the mass of a sufficiently large Euclidean ball in the separator set is larger than the mass of a corresponding ball in the true clusters. This means that any algorithm that uses large balls cannot reliably clean the sample and this restricts the size of the balls that can be used. Now if points in the regions of high density are to survive then there must be k sample points in the small ball around any point in the true clusters and this gives us a lower bound on the necessary sample size.\nThe RSL algorithms work by counting the number of sample points inside the balls B(x, r) centered at the sample points x, for some radius r. In order for the algorithm to reliably resolve (σ, ǫ) clusters, it should distinguish points in the separator set S ⊂ M2 from those in the level λ clusters M1∪M3. A necessary condition for this is that the mass of a ball B(x, r) for x ∈ Sσ−r should be strictly smaller than the mass inside B(y, r) for y ∈ M1 ∪ M3. In Appendix A.4, we show that this condition restricts the radius r to be at most O(τ √ ǫ/d). Now, consider any sample point x0 in M1 ∪ M3 (such an x exists with high probability). Since x0 should not be removed during the cleaning step, the ball B(x0, r) must contain some other sample point (indeed, it must contain at least k − 1 more sample points). By a union bound, this happens with probability at most (n − 1)vdrdλ ≤\nO(d−d/2nτdǫd/2λ). If we want the algorithm to succeed with probability at least 1/2 (say) then n ≥ Ω ( dd/2\nτdλǫd/2\n) ."
    }, {
      "heading" : "5 Cluster tree recovery in the presence of noise",
      "text" : "So far we have considered the problem of recovering the cluster tree given samples from a density supported on a lower dimensional manifold. In this section we extend these results to the more general situation when we have noisy samples concentrated near a lower dimensional manifold. Indeed it can be argued that the manifold + noise model is a natural and general model for highdimensional data. In the noisy setting, it is clear that we can infer the cluster tree of the noisy density in a straightforward way. A stronger requirement would be consistency with respect to the underlying latent sample. Following the literature on manifold estimation (Balakrishnan et al. (2012); Genovese et al. (2012)) we consider two main noise models. For both of them, we specify a distribution Q for the noisy sample.\n1. Clutter Noise: We observe data Y1, . . . , Yn from the mixture Q := (1 − π)U + πP where 0 < π ≤ 1 and U is a uniform distribution on X . Denote the samples drawn from P in this mixture X = {X1, . . . , Xm}. The points drawn from U are called background clutter. In this case, we can show:\nTheorem 10 There are universal constants C1 and C2 such that the following holds. For any δ > 0, 0 < ǫ < 1/2, run the algorithm in Figure 1 on a sample {Y1, . . . , Yn}, with parameters\nR := 4ρ k := C1 log 2(1/δ)(µ/ǫ2).\nThen with probability at least 1 − δ, Ĉ is (σ, ǫ) consistent. In particular, the clusters containing A[X] and A′[X] are internally connected and mutually disconnected in C(r) for r defined by\nπvdr dλ =\n1\n1− ǫ/6\n( k\nn +\nC2 log(1/δ)\nn\n√ kµ\n)\nprovided λ ≥ max {\n2 vdρd k n ,\n2v d/D D (1−π) d/D\nvdǫd/Dπ\n( k n\n)1−d/D }\nwhere ρ is now slightly modified (in con-\nstants), i.e., ρ := min ( σ 7 , ǫτ 72d , τ 24 ) .\n2. Additive Noise: The data are of the form Yi = Xi+ηi where X1, . . . , Xn ∼ P ,and η1, . . . , ηn are a sample from any bounded noise distribution Φ, with ηi ∈ B(0, θ). Note that Q is the convolution of P and Φ, Q = P ⋆ Φ.\nTheorem 11 There are universal constants C1 and C2 such that the following holds. For any δ > 0, 0 < ǫ < 1/2, run the algorithm in Figure 1 on the sample {Y1, . . . , Yn} with parameters\nR := 5ρ k := C1 log 2(1/δ)(µ/ǫ2).\nThen with probability at least 1− δ, Ĉ is (σ, ǫ) consistent for θ ≤ ρǫ/24d. In particular, the clusters containing {Yi : Xi ∈ A} and {Yi : Xi ∈ A′} are internally connected and mutually disconnected in C(r) for r defined by\nvdr d(1− ǫ/12)(1− ǫ/6)λ = k\nn + Cδ n\n√ kµ\nif λ ≥ 2 vdρd k n and θ ≤ ρǫ/24d, where ρ := min ( σ 7 , τ 24 , ǫτ 144d ) .\nThe proofs for both Theorems 10 and 11 appear in Appendix A.5. Notice that in each case we receive samples from a full D-dimensional distribution but are still able to achieve rates independent of D because these distributions are concentrated around the lower dimensional M . For the clutter noise case we produce a tree that is consistent for samples drawn from P (which are exactly on M ), while in the additive noise case we produce a tree on the observed Yis which is (σ, ǫ) consistent for the latent Xis (for θ small enough). It is worth noting that in the case of clutter noise we can still consistently recover the entire cluster tree. Intuitively, this is because the k-NN distances for points on M are much smaller than for clutter points that are far away from M . As a result the clutter noise only affects a vanishingly low level set of the cluster tree."
    } ],
    "references" : [ {
      "title" : "Minimax rates for homology inference",
      "author" : [ "S. Balakrishnan", "A. Rinaldo", "D. Sheehy", "A. Singh", "L. Wasserman" ],
      "venue" : null,
      "citeRegEx" : "Balakrishnan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Balakrishnan et al\\.",
      "year" : 2012
    }, {
      "title" : "Local polynomial regression on unknown manifolds",
      "author" : [ "P. Bickel", "B. Li" ],
      "venue" : "In Technical report, Department of Statistics, UC Berkeley",
      "citeRegEx" : "Bickel and Li.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bickel and Li.",
      "year" : 2006
    }, {
      "title" : "Rates of convergence for the cluster tree",
      "author" : [ "K. Chaudhuri", "S. Dasgupta" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Chaudhuri and Dasgupta.,? \\Q2010\\E",
      "shortCiteRegEx" : "Chaudhuri and Dasgupta.",
      "year" : 2010
    }, {
      "title" : "An upper bound for the volume of geodesic balls in submanifolds of euclidean spaces",
      "author" : [ "F. Chazal" ],
      "venue" : "Personal Communication, available at http://geometrica.saclay.inria.fr/team/Fred.Chazal/BallVolumeJan2013.pdf,",
      "citeRegEx" : "Chazal.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chazal.",
      "year" : 2013
    }, {
      "title" : "A plug-in approach to support estimation",
      "author" : [ "A. Cuevas", "R. Fraiman" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Cuevas and Fraiman.,? \\Q1997\\E",
      "shortCiteRegEx" : "Cuevas and Fraiman.",
      "year" : 1997
    }, {
      "title" : "Rodríguez-Casal. Plug-in estimation of general level",
      "author" : [ "A. Cuevas", "W. González-Manteiga" ],
      "venue" : "sets. Aust. N. Z. J. Stat.,",
      "citeRegEx" : "Cuevas et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cuevas et al\\.",
      "year" : 2006
    }, {
      "title" : "Random projection trees and low dimensional manifolds",
      "author" : [ "S. Dasgupta", "Y. Freund" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Dasgupta and Freund.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dasgupta and Freund.",
      "year" : 2008
    }, {
      "title" : "Minimax manifold estimation",
      "author" : [ "C.R. Genovese", "M. Perone-Pacifico", "I. Verdinelli", "L. Wasserman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Genovese et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Genovese et al\\.",
      "year" : 2012
    }, {
      "title" : "Consistency of single linkage for high-density clusters",
      "author" : [ "J.A. Hartigan" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hartigan.,? \\Q1981\\E",
      "shortCiteRegEx" : "Hartigan.",
      "year" : 1981
    }, {
      "title" : "A tree-based regressor that adapts to intrinsic dimension",
      "author" : [ "S. Kpotufe", "S. Dasgupta" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Kpotufe and Dasgupta.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kpotufe and Dasgupta.",
      "year" : 2012
    }, {
      "title" : "Pruning nearest neighbor cluster trees",
      "author" : [ "S. Kpotufe", "U. von Luxburg" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11),",
      "citeRegEx" : "Kpotufe and Luxburg.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kpotufe and Luxburg.",
      "year" : 2011
    }, {
      "title" : "Optimal construction of k-nearest-neighbor graphs for identifying noisy clusters",
      "author" : [ "M. Maier", "M. Hein", "U. von Luxburg" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "Maier et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Maier et al\\.",
      "year" : 2009
    }, {
      "title" : "Finding the homology of submanifolds with high confidence from random samples",
      "author" : [ "P. Niyogi", "S. Smale", "S. Weinberger" ],
      "venue" : "Discrete & Computational Geometry,",
      "citeRegEx" : "Niyogi et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Niyogi et al\\.",
      "year" : 2008
    }, {
      "title" : "Measuring mass concentrations and estimating density contour clusters: an excess mass approach",
      "author" : [ "W. Polonik" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Polonik.,? \\Q1995\\E",
      "shortCiteRegEx" : "Polonik.",
      "year" : 1995
    }, {
      "title" : "Fast rates for plug-in estimators of density level",
      "author" : [ "P. Rigollet", "R. Vert" ],
      "venue" : "sets. Bernoulli,",
      "citeRegEx" : "Rigollet and Vert.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rigollet and Vert.",
      "year" : 2009
    }, {
      "title" : "Stability of density-based clustering",
      "author" : [ "A. Rinaldo", "A. Singh", "R. Nugent", "L. Wasserman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Rinaldo et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rinaldo et al\\.",
      "year" : 2012
    }, {
      "title" : "Generalized density clustering",
      "author" : [ "A. Rinaldo", "L. Wasserman" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Rinaldo and Wasserman.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rinaldo and Wasserman.",
      "year" : 2010
    }, {
      "title" : "Adaptive {H}ausdorff estimation of density level",
      "author" : [ "A. Singh", "C. Scott", "R. Nowak" ],
      "venue" : "sets. Ann. Statist.,",
      "citeRegEx" : "Singh et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2009
    }, {
      "title" : "Consistency and rates for clustering with dbscan",
      "author" : [ "B.K. Sriperumbudur", "I. Steinwart" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Sriperumbudur and Steinwart.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sriperumbudur and Steinwart.",
      "year" : 2012
    }, {
      "title" : "Adaptive density level set clustering",
      "author" : [ "I. Steinwart" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Steinwart.,? \\Q2011\\E",
      "shortCiteRegEx" : "Steinwart.",
      "year" : 2011
    }, {
      "title" : "Estimating the cluster tree of a density by analyzing the minimal spanning tree of a sample",
      "author" : [ "W. Stuetzle" ],
      "venue" : "J. Classification,",
      "citeRegEx" : "Stuetzle.,? \\Q2003\\E",
      "shortCiteRegEx" : "Stuetzle.",
      "year" : 2003
    }, {
      "title" : "A generalized single linkage method for estimating the cluster tree of a density",
      "author" : [ "N.R.W. Stuetzle" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Stuetzle,? \\Q2010\\E",
      "shortCiteRegEx" : "Stuetzle",
      "year" : 2010
    }, {
      "title" : "On nonparametric estimation of density level",
      "author" : [ "A.B. Tsybakov" ],
      "venue" : "sets. Ann. Statist.,",
      "citeRegEx" : "Tsybakov.,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsybakov.",
      "year" : 1997
    }, {
      "title" : "Granulometric smoothing",
      "author" : [ "G. Walther" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Walther.,? \\Q1997\\E",
      "shortCiteRegEx" : "Walther.",
      "year" : 1997
    }, {
      "title" : "Mode analysis: a generalization of nearest neighbor which reduces chaining",
      "author" : [ "D. Wishart" ],
      "venue" : "In Proceedings of the Colloquium on Numerical Taxonomy held in the University of St. Andrews,",
      "citeRegEx" : "Wishart.,? \\Q1969\\E",
      "shortCiteRegEx" : "Wishart.",
      "year" : 1969
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "We analyze a modified version of a k-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta (2010). The main results of this paper show that under mild assumptions on f and M , we obtain rates of convergence that depend on d only but not on the ambient dimension D.",
      "startOffset" : 91,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "Hartigan (1981) showed that the popular single-linkage algorithm is not consistent for a sample from R, with D > 1.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 2,
      "context" : "Recently, Chaudhuri and Dasgupta (2010) analyzed an algorithm which is both simple and consistent.",
      "startOffset" : 10,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "Here is a brief summary of the main contributions of our paper: (1) We show that the simple algorithm studied in the paper Chaudhuri and Dasgupta (2010) is consistent and has fast rates of convergence for data on or near a low dimensional manifold M .",
      "startOffset" : 123,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "The idea of using probability density functions for clustering dates back to Wishart Wishart (1969). Hartigan (1981) expanded on this idea and formalized the notions of high-density clustering, of the cluster tree and of consistency and fractional consistency of clustering algorithms.",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "Hartigan (1981) expanded on this idea and formalized the notions of high-density clustering, of the cluster tree and of consistency and fractional consistency of clustering algorithms.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "Hartigan (1981) expanded on this idea and formalized the notions of high-density clustering, of the cluster tree and of consistency and fractional consistency of clustering algorithms. In particular, Hartigan (1981) showed that single linkage clustering is consistent when D = 1 but is only fractionally consistent when D > 1.",
      "startOffset" : 0,
      "endOffset" : 216
    }, {
      "referenceID" : 7,
      "context" : "Hartigan (1981) expanded on this idea and formalized the notions of high-density clustering, of the cluster tree and of consistency and fractional consistency of clustering algorithms. In particular, Hartigan (1981) showed that single linkage clustering is consistent when D = 1 but is only fractionally consistent when D > 1. Stuetzle and R. (2010) and Stuetzle (2003) have also proposed procedures for recovering the cluster tree.",
      "startOffset" : 0,
      "endOffset" : 350
    }, {
      "referenceID" : 7,
      "context" : "Hartigan (1981) expanded on this idea and formalized the notions of high-density clustering, of the cluster tree and of consistency and fractional consistency of clustering algorithms. In particular, Hartigan (1981) showed that single linkage clustering is consistent when D = 1 but is only fractionally consistent when D > 1. Stuetzle and R. (2010) and Stuetzle (2003) have also proposed procedures for recovering the cluster tree.",
      "startOffset" : 0,
      "endOffset" : 370
    }, {
      "referenceID" : 2,
      "context" : "None of these procedures however, come with the theoretical guarantees given by Chaudhuri and Dasgupta (2010), which demonstrated that a generalization of Wishart’s algorithm allows one to estimate parts of the cluster tree for distributions with fulldimensional support near-optimally under rather mild assumptions.",
      "startOffset" : 80,
      "endOffset" : 110
    }, {
      "referenceID" : 10,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al.",
      "startOffset" : 27,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al.",
      "startOffset" : 27,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al.",
      "startOffset" : 27,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al.",
      "startOffset" : 76,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al.",
      "startOffset" : 76,
      "endOffset" : 124
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al.",
      "startOffset" : 76,
      "endOffset" : 150
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al.",
      "startOffset" : 76,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al. (2009); Rinaldo and Wasserman (2010); Rinaldo et al.",
      "startOffset" : 76,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al. (2009); Rinaldo and Wasserman (2010); Rinaldo et al.",
      "startOffset" : 76,
      "endOffset" : 222
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al. (2009); Rinaldo and Wasserman (2010); Rinaldo et al. (2012), and references therein.",
      "startOffset" : 76,
      "endOffset" : 245
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al. (2009); Rinaldo and Wasserman (2010); Rinaldo et al. (2012), and references therein. Estimating the cluster tree has more recently been considered by Kpotufe and von Luxburg (2011) who also give a simple pruning procedure for removing spurious clusters.",
      "startOffset" : 76,
      "endOffset" : 366
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al. (2009); Rinaldo and Wasserman (2010); Rinaldo et al. (2012), and references therein. Estimating the cluster tree has more recently been considered by Kpotufe and von Luxburg (2011) who also give a simple pruning procedure for removing spurious clusters. Steinwart (2011) and Sriperumbudur and Steinwart (2012) propose procedures for determining recursively the lowest split in the cluster tree and give conditions for asymptotic consistency with minimal assumptions on the density.",
      "startOffset" : 76,
      "endOffset" : 456
    }, {
      "referenceID" : 4,
      "context" : "See for example the papers Polonik (1995); Tsybakov (1997); Walther (1997); Cuevas and Fraiman (1997); Cuevas et al. (2006); Rigollet and Vert (2009); Maier et al. (2009); Singh et al. (2009); Rinaldo and Wasserman (2010); Rinaldo et al. (2012), and references therein. Estimating the cluster tree has more recently been considered by Kpotufe and von Luxburg (2011) who also give a simple pruning procedure for removing spurious clusters. Steinwart (2011) and Sriperumbudur and Steinwart (2012) propose procedures for determining recursively the lowest split in the cluster tree and give conditions for asymptotic consistency with minimal assumptions on the density.",
      "startOffset" : 76,
      "endOffset" : 495
    }, {
      "referenceID" : 12,
      "context" : "The condition number is discussed in more detail in the paper Niyogi et al. (2008).",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "To give finite sample results, following Chaudhuri and Dasgupta (2010), we define the notion of salient clusters.",
      "startOffset" : 41,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "To give finite sample results, following Chaudhuri and Dasgupta (2010), we define the notion of salient clusters. Our definitions are slight modifications of those in Chaudhuri and Dasgupta (2010) to take into account the manifold assumption.",
      "startOffset" : 41,
      "endOffset" : 197
    }, {
      "referenceID" : 2,
      "context" : "Ignoring constants that depend on d the main difference between this result and the result of Chaudhuri and Dasgupta (2010) is that our results only depend on the manifold dimension d and not the ambient dimension D (typically D ≫ d).",
      "startOffset" : 94,
      "endOffset" : 124
    }, {
      "referenceID" : 2,
      "context" : "Ignoring constants that depend on d the main difference between this result and the result of Chaudhuri and Dasgupta (2010) is that our results only depend on the manifold dimension d and not the ambient dimension D (typically D ≫ d). There is also a dependence of our result on 1/(ǫτ), for ǫτ ≪ σ. In Section 4 we sketch the construction of an instance that suggests that this dependence is not an artifact of our analysis and that the sample complexity of the class of RSL algorithms is at least n ≥ 1/(ǫτ). 2. Another aspect is that our choice of the connection radius R depends on the (typically) unknown ρ, while for comparison, the connection radius in Chaudhuri and Dasgupta (2010) is chosen to be",
      "startOffset" : 94,
      "endOffset" : 689
    }, {
      "referenceID" : 2,
      "context" : "It is easy to see that this theorem also establishes consistency for recovering the entire cluster tree by selecting an appropriate schedule on σn, ǫn and kn that ensures that all clusters are distinguished for n large enough (see Chaudhuri and Dasgupta (2010) for a formal proof).",
      "startOffset" : 231,
      "endOffset" : 261
    }, {
      "referenceID" : 1,
      "context" : "Our proofs structurally mirror those in Chaudhuri and Dasgupta (2010). We begin with a few technical results in 3.",
      "startOffset" : 40,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Our proofs structurally mirror those in Chaudhuri and Dasgupta (2010). We begin with a few technical results in 3.1. In Section 3.2 we establish (σ, ǫ) consistency by showing that the clusters are mutually disjoint and internally connected. The main technical challenge is that the curvature of the manifold, modulated by its condition number 1/τ , limits our ability to resolve the density level sets from a finite sample, by limiting the maximum cleaning and connection radii the algorithm can use. In what follows, we carefully analyze this effect and show that somewhat surprisingly, despite this curvature, essentially the same algorithm is able to adapt to the unknown manifold and produce a consistent estimate of the entire cluster tree. Similar manifold adaptivity results have been shown in classification Dasgupta and Freund (2008) and in non-parametric regression Kpotufe and Dasgupta (2012); Bickel and Li (2006).",
      "startOffset" : 40,
      "endOffset" : 843
    }, {
      "referenceID" : 1,
      "context" : "Our proofs structurally mirror those in Chaudhuri and Dasgupta (2010). We begin with a few technical results in 3.1. In Section 3.2 we establish (σ, ǫ) consistency by showing that the clusters are mutually disjoint and internally connected. The main technical challenge is that the curvature of the manifold, modulated by its condition number 1/τ , limits our ability to resolve the density level sets from a finite sample, by limiting the maximum cleaning and connection radii the algorithm can use. In what follows, we carefully analyze this effect and show that somewhat surprisingly, despite this curvature, essentially the same algorithm is able to adapt to the unknown manifold and produce a consistent estimate of the entire cluster tree. Similar manifold adaptivity results have been shown in classification Dasgupta and Freund (2008) and in non-parametric regression Kpotufe and Dasgupta (2012); Bickel and Li (2006).",
      "startOffset" : 40,
      "endOffset" : 904
    }, {
      "referenceID" : 1,
      "context" : "Similar manifold adaptivity results have been shown in classification Dasgupta and Freund (2008) and in non-parametric regression Kpotufe and Dasgupta (2012); Bickel and Li (2006).",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 2,
      "context" : "In the full dimensional setting of Chaudhuri and Dasgupta (2010), this follows from standard VC inequalities.",
      "startOffset" : 35,
      "endOffset" : 65
    }, {
      "referenceID" : 11,
      "context" : "3 of Niyogi et al. (2008) while the upper bound is based on a modification of the main result of Chazal (2013).",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "(2008) while the upper bound is based on a modification of the main result of Chazal (2013).",
      "startOffset" : 78,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "The proof is analogous to the separation proof of Chaudhuri and Dasgupta (2010) with several modifications.",
      "startOffset" : 50,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "3 in Niyogi et al. (2008) who show that if ‖p − q‖ = R ≤ τ/2, then the geodesic distance dM (p, q) ≤ τ − τ √ 1− 2R τ .",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "For full dimensional densities, Chaudhuri and Dasgupta (2010) showed the information theoretic lower bound n = Ω ( 1 λǫ(2)vDσ log 1 λǫ(2)vDσ ) .",
      "startOffset" : 32,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "This can be fixed without affecting the essence of the construction by smoothing this intersection by rolling a ball of radius τ around it (a similar construction is made rigorous in Theorem 6 of Genovese et al. (2012)).",
      "startOffset" : 196,
      "endOffset" : 219
    }, {
      "referenceID" : 0,
      "context" : "Following the literature on manifold estimation (Balakrishnan et al. (2012); Genovese et al.",
      "startOffset" : 49,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Following the literature on manifold estimation (Balakrishnan et al. (2012); Genovese et al. (2012)) we consider two main noise models.",
      "startOffset" : 49,
      "endOffset" : 100
    } ],
    "year" : 2013,
    "abstractText" : "In this paper we investigate the problem of estimating the cluster tree for a density f supported on or near a smooth d-dimensional manifold M isometrically embedded in R. We analyze a modified version of a k-nearest neighbor based algorithm recently proposed by Chaudhuri and Dasgupta (2010). The main results of this paper show that under mild assumptions on f and M , we obtain rates of convergence that depend on d only but not on the ambient dimension D. Finally, we sketch a construction of a sample complexity lower bound instance for a natural class of manifold oblivious clustering algorithms.",
    "creator" : null
  }
}