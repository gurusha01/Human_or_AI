{
  "name" : "f4be00279ee2e0a53eafdaa94a151e2c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Approximate inference in latent Gaussian-Markov models from continuous time observations",
    "authors" : [ "Botond Cseke", "Manfred Opper", "Guido Sanguinetti" ],
    "emails" : [ "bcseke@inf.ed.ac.uk", "gsanguin@inf.ed.ac.uk", "manfred.opper@tu-berlin.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose an approximate inference algorithm for continuous time Gaussian Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of (1) expectation propagation updates for discrete time terms and (2) variational updates for the continuous time term. We introduce postinference corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application."
    }, {
      "heading" : "1 Introduction",
      "text" : "Continuous time stochastic processes provide a flexible and popular framework for data modelling in a broad spectrum of scientific and engineering disciplines. Their intrinsically non-parametric, infinitedimensional nature also makes them a challenging field for the development of efficient inference algorithms. Recent years have seen several such algorithms being proposed for a variety of models [Opper and Sanguinetti, 2008, Opper et al., 2010, Rao and Teh, 2012]. Most inference work has focused on the scenario when observations are available at a finite set of time points, however, modern technologies are making effectively continuous time observations increasingly common: for example, high speed imaging technologies now enable the acquisition of biological data at around 100Hz for extended periods of time. Other scenarios give intrinsically continuous time observations: for example, sensors monitoring the transit of a particle through a barrier provide continuous time data on the particle’s position. To the best of our knowledge, this problem has not been addressed in the statistical machine learning community.\nIn this paper, we propose an expectation-propagation (EP)-type algorithm [Opper and Winther, 2000, Minka, 2001] for latent diffusion processes observed in either discrete or continuous time. We derive fixed-point update equations by considering a continuous time limit of the parallel EP algorithm [e.g. Opper and Winther, 2005, Cseke and Heskes, 2011b]: these fixed point updates naturally become differential equations in the continuous time limit. Remarkably, we show that, in the presence of continuous time observations, the update equations for the EP algorithm reduce to updates for a variational Gaussian approximation [Archambeau et al., 2007]. We also generalise to the continuous-time limit the EP correction scheme of [Cseke and Heskes, 2011b], which enable us to capture some of the non-Gaussian behaviour of the time marginals."
    }, {
      "heading" : "2 Models and methods",
      "text" : "We consider dynamical systems described by multivariate stochastic differential equations (SDEs) of Ornstein-Uhlenbeck (OU) type over the [0, 1] time interval\ndxt = (Atxt + ct)dt+B 1/2 t dWt, (1)\nwhere {Wt}t is the standard Wiener process [Gardiner, 2002] and At,Bt and ct are time dependent matrix and vector valued functions respectively with Bt being positive definite for all t ∈ [0, 1]. Even though the process does not posses a formulation through density functions (with respect to the Lebesgue measure), in order to be able to symbolically represent and manipulate the variables of the process in the Bayesian formalism, we will use the proxy p0({xt}) to denote their distribution. The process can be observed (noisily) both at discrete time points, and for continuous time intervals; we will partition the observations in ydti , ti ∈ Td and y c t , t ∈ [0, 1] accordingly. We assume that the likelihood function admits the general formulation\np({ydti}i, {y c t}| {xt}) ∝\n\nti∈Td\np(ydti |xti)× exp  −  1\n0\ndtV (t,yct ,xt)  . (2)\nWe refer to p(ydti |xti) and V (t,y c t ,xt) as discrete time likelihood term and continuous time loss function, respectively. We notice that, using Girsanov’s theorem and Ito’s lemma, non-linear diffusion equations with constant (diagonal) diffusion matrix can be re-written in the form (1)-(2), provided the drift can be obtained as the gradient of a potential function [e.g. Øksendal, 2010].\nOur aim is to propose approximate inference methods to compute the marginals p(xt|{ydti}i, {y c t}) of the posterior distribution\np({xt}t|{ydti}i, {y c t}) ∝ p({ydti}i, {y c t}| {xt})× p0({xt})."
    }, {
      "heading" : "2.1 Exact inference in Gaussian models",
      "text" : "We start form the exact case of Gaussian observations and quadratic loss function. The linearity of equation (1) implies that the marginal distributions of the process at every time point are Gaussian (assuming Gaussian initial conditions). The time evolution of the marginal mean mt and covariance Vt is governed by the pair of differential equations [Gardiner, 2002]\nd dt mt = Atmt + ct and d dt Vt = AtVt + VtA T t +Bt. (3)\nIn the case of Gaussian observations and a quadratic loss function V (t,yct ,xt) = const. − xTt hct + 1 2x T t Q c txt, these equations, together with their backward analogues, enable an exact recursive inference algorithm, known as the Kalman-Bucy smoother [e.g. Särkkä, 2006]. This algorithm arises because we can recast the loss function as an auxiliary (observation) process\ndyct = xtdt+R 1/2 t dWt, (4)\nwhere R−1t = Qct and R −1 t dy c t/dt = h c t . This follows by the Gaussianity of the observation process and the fundamental property of Ito’s calculus dW 2t = Idt.\nThe Kalman-Bucy algorithm computes the posterior marginal means and covariances by solving the differential equations in a forward-backward fashion. These can be combined with classical Kalman filtering to account for discrete-time observations. The exact form of the equations as well as the variational derivation of the Kalman-Bucy problem are given in Section B of the Supplementary Material."
    }, {
      "heading" : "2.2 Approximate inference",
      "text" : "In this section we use an Euler discretisation of the prior and the continuous time likelihood to turn our model into a multivariate latent Gaussian model. We review the EP algorithm for such models and then we show that when taking the limit ∆t → 0 the updates of the EP algorithm exist. The resulting approximate posterior process is again an OU process and we compute its parameters. Finally, we show how corrections to the marginals proposed [Cseke and Heskes, 2011b] can be extended to the continuous time case."
    }, {
      "heading" : "2.2.1 Euler discretisation",
      "text" : "Let T = {t1 = 0, t2, . . . , tK−1, tK = 1} be a discretisation of the [0, 1] interval and let the matrix x = [xt1 , . . . ,xtK ] represent the process {xt}t using the discretisation given by T . Without loss of generality we can assume that Td ⊂ T . We assume the Euler-Maruyama approach and approximate p({xt}) by1\np0(x) = N(x0;m0,V0) \nk\nN(xtk+1 ;xtk + (Atkxtk + ctk )∆tk,∆tkBtk )\nand in a similar fashion we approximate the continuous time likelihood by\np(yc|x) ∝ exp  − \nk\n∆tkV (tk,y c tk ,xtk )\n ,\nwhere yc is the matrix yc = [yct1 , . . . ,y c tK ]. Consequently we approximate our model by the latent Gaussian model\np({ydti}i,y c,x) = p0(x)×\n\ni\np(ydti |xti) \nk\nexp  −∆tkV (tk,yctk ,xtk ) \nwhere we remark that the prior p0 has a block-diagonal precision structure. To simplify notation, in the following we use the aliases φdi (xti) = p(ydti |xti) and φ c k(xtk ;∆tk) = exp  −∆tkV (tk,yctk ,xtk)  ."
    }, {
      "heading" : "2.2.2 Inference using expectation propagation",
      "text" : "Expectation propagation [Opper and Winther, 2000, Minka, 2001] is a well known algorithm that provides good approximations of the posterior marginals in latent Gaussian models. We use here the parallel EP approach [e.g. Cseke and Heskes, 2011b]; similar continuous time limiting arguments can be made for the original (sequential) EP approach. The algorithm approximates the posterior p(x|{ydti}i,y c) by a Gaussian\nq0(x) ∝ p0(x) \ni\nφ̃di (xti) \nk\nφ̃ck(xtk ;∆tk),\nwhere φ̃di and φ̃ck are Gaussian functions. When applied to our model the algorithm proceeds by performing the fixed point iteration\n[φ̃di (xti)] new ∝ Collapse(φ d i (xti)φ̃ d i (xti) −1q0(xti);N ) q0(xti) × φ̃di (xti) for all ti ∈ Td, (5)\n[φ̃ck(xtk ;∆tk)] new ∝ Collapse(φ c k(xtk ;∆tk)φ̃ c k(xtk ;∆tk) −1q0(xtk );N ) q0(xtk ) × φ̃ck(xtk ;∆tk) for all tk ∈ T, (6) where Collapse(p(z);N ) = argminq∈ND[p(z)||q(z)] denotes the projection of the density p(z) into the Gaussian family denoted by N . In other words, Collapse(p(z);N ) is the Gaussian density that matches the first and second moments of p(z). Readers familiar with the classical formulation of EP [Minka, 2001] will recognise in equation (5) the so-called term updates, where φ̃di (xti)−1q0(xti) is the cavity distribution and φdi (xti)φ̃di (xti)−1q0(xti) the tilted distribution. Equations (5-6) imply that at any fixed point of the iterations we have q(xti) = Collapse(φdi (xti)φ̃di (xti)−1q0(xti);N ) and q(xtk) = Collapse(φck(xtk ;∆tk)φ̃ c k(xtk ;∆tk)\n−1q0(xtk);N ). The algorithm can also be derived and justified as a constrained optimisation problem of a Gibbs free energy formulation [Heskes et al., 2005]; this alternative approach can also be shown to extend to the continuous time limit (see Section A.2 of the Supplementary Material) and provides a useful tool for approximate evidence calculations.\nEquation (5) does not depend on the time discretisation, and hence provides a valid update equation also working directly with the continuous time process. On the other hand, the quantities in equation (6) depend explicitly on ∆tk, and it is necessary to ensure that they remain well defined (and computable) in the continuous time limit. In order to derive the limiting behaviour of (6) we introduce the the following notation: (i) we use f(z) = (z,−zzT /2) to denote the sufficient statistic of a multivariate Gaussian (ii), we use λdti = (h d ti ,Q d ti) as the canonical parameters corresponding to the Gaussian function φ̃di (xti) ∝ exp{λdti · f(xti)} 2, (iii) we use λctk = (h c tk ,Q c tk) as the canonical parameters corresponding to the Gaussian function φ̃ck(xtk) ∝ exp{∆tkλctk · f(xtk)}, and finally, (iv) we use Collapse(p(z);f) as 1We remark that one could also integrate the OU process between time steps, yielding an exact finite dimensional marginal of the prior. In the limit however both procedures are equivalent. 2We use “·” as scalar product for general (concatenated) vector objects, for example, x·y = xTy when x,y ∈ Rn.\nthe canonical parameters corresponding to the density Collapse(p(z);N ). By using this notation we can rewrite (6) as\n[λctk ] new = λctk + 1 ∆tk  Collapse(qc(xtk );f)− Collapse(q0(xtk );f)  (7)\nwith\nqc(xtk ) ∝ exp(−∆tk[V (tk,xtk ) + λ c tk · f(xtk )])q0(xtk ). (8)\nThe approximating density can then be written as\nq0(x) ∝ p0(x)× exp \ni\nλ d ti · f(xti) +\n\nk\n∆tkλ c tk · f(xtk )\n . (9)\nBy direct Taylor expansion of Collapse(qc(xtk);f) one can show that the update equation (7) remains finite when we take the limit ∆tk → 0. A slightly more general perspective however affords greater insight into the algorithm, as shown below."
    }, {
      "heading" : "2.2.3 Continuous time limit of the update equations",
      "text" : "Let µtk = Collapse(q0(xtk);f) and denote by Z(∆tk,µtk) and Z(µtk) the normalisation constant of qc(xtk) and q0(xtk) respectively. The notation emphasises that qc(xtk) differs from q0(xtk) by a term dependent on the granularity of the discretisation ∆tk. We exploit the well known fact that the derivatives with respect to the canonical parameters of the log normalisation constant of a distribution within the exponential family give the moment parameters of the distribution. From the definition of qc(xtk) in equation (8) we then have that its first two moments can be computed as ∂µtk logZ(∆tk,µtk). The Collapse operation in (7) can then be rewritten as\nCollapse(qc(xtk );f) = Ψ(∂µtk logZ(∆tk,µtk )), (10)\nwhere Ψ is the function transforming the moment parameters of a Gaussian into its (canonical) parameters. We now assume ∆tk to be small and expand Z(∆tk,µtk) to first order in ∆tk. By using the property that limα→0+ g(z)α 1/α p(z) = exp(log g(x)p) for any distribution p(z) and g(z) > 0, one can write\nlim ∆tk→0 1 ∆tk [logZ(∆tk,µtk )− logZ(µtk )] = log lim ∆tk→0  exp{−∆tk[V (tk,xtk ) + λ c tk · f(xtk )]} 1/∆tk q0(xtk )\n= −  [V (tk,xtk ) + λ c tk · f(xtk )]  q0(xtk ) = −V (tk,xtk )q0(xtk ) −Ψ −1(µtk )λ c tk , (11)\nwhere we exploited the fact that f(xtk)q0(xtk ) are the moments of the q0(xtk) distribution. We can now exploit the fact that ∆tk is small and linearise the nonlinear map Ψ about the moments of q0(xtk) to obtain a first order approximation to equation (10) as\nCollapse(qc(xtk );f)  µtk −∆tkλ c tk −∆tkJΨ(µtk )∂µtk V (tk,xtk )q0(xtk ) (12)\nwhere JΨ(µtk) denotes the Jacobian matrix of the map Ψ evaluated at µtk . The second term on the r.h.s. of equation (12) follows from the obvious identity ∂µtkΨ(Ψ −1(µtk)) = I.\nBy substituting (12) into (7), we take the limit ∆tk → 0 and obtain the update equations [λct ]\nnew = −JΨ(µt)∂µt V (t,xt)q0(xt) for all t ∈ [0, 1]. (13)\nNotice that the updating of λct is somewhat hidden in equation (13); the “old” parameters are in fact contained in the parameters µtk . Since λct corresponds to the canonical parameters of a multivariate Gaussian, we can use the representation λct = (hct ,Qct) and after some algebra on the moment-canonical transformation of Gaussians we write the fixed point iteration as\n[hct ] new = −∂mt V (t,xt)q0(xt) + 2∂Vt V (t,xt)q0(xt) mt and [Q c t ] new = ∂Vt V (t,xt)q0(xt) , (14)\nwhere mt and Vt are the marginal means and covariances of q0 at the ∆tk → 0. Algorithmically, computing the marginal moments and covariances of the discretised Gaussian q0(x) in (9) can be done by solving a sparse linear system and doing partial matrix inversion using the Cholesky factorisation and the Takahashi equations as in Cseke and Heskes [2011b]. This corresponds to a junction tree algorithm on a (block) chain graph [Davis, 2006] which, in the continuous time limit, can be reduced to a set of differential equations\ndue to the chain structure of the graph. Alternatively, one can notice that, in the continuous time limit, the structure of q0(x) in equation (9) defines a posterior process for an OU process p0({xt}) observed at discrete times with Gaussian noise (corresponding to the terms φ̃di (xti) with canonical parameters λdti ) and with a quadratic continuous time loss, which is computed using equation (14). The moments therefore be computed using the Kalman-Bucy algorithm; details of the algorithm are given in Section B.1 of the Supplementary Material. The derivation above illustrates another interesting characteristic of working with continuous-time likelihoods. Readers familiar with the fractional free energies and the power EP algorithm may notice that the time lag ∆tk plays a similar role as the fractional or power parameter α. It is well known property that in the α → 0 limit the algorithm and the free energy collapses to variational [e.g. Wiegerinck and Heskes, 2003, Cseke and Heskes, 2011a] and thus, intuitively, the collapse and the existence of the limit is related to this property.\nOverall, we arrive to a hybrid algorithm in which: (i) the canonical parameters (hdti ,Q d ti) corresponding to the discrete time terms are updated by the usual EP updates in (5), (ii) the canonical parameters (hct ,Qct) corresponding to the continuous loss function V (t,xt) are updated by the variational updates in (14) (iii), the marginal moment parameters of q0(xt) are computed by the forward-backward differential equations referred to in Section 2.1. We can use either parallel or a forward-backward type scheduling. A more detailed description of the inference algorithm is given in Section C of the Supplementary Material. The algorithm performs well in the comfort zone of EP, that is, log-concave discrete likelihood terms and convex loss. Non-convergence can occur in case of multimodal likelihoods and loss functions and alternative options to optimise the free energy have to be explored [e.g. Heskes et al., 2005, Archambeau et al., 2007]."
    }, {
      "heading" : "2.2.4 Parameters of the approximating OU process",
      "text" : "The fixed point iteration scheme computes only the marginal means and covariances of q0({xt}) and it does not provide a parametric OU process as an approximation. However, this can be computed by finding the parameters of an OU process that matches q0 in the moment matching Kullback-Leibler divergence. That is, if q∗({xt}) minimises D[q0({xt})||q∗({xt})], then the parameters of q∗ are given by\nA ∗ t = At −Bt[V bwt ]−1, c∗t = ct +Bt[V bwt ]−1mbwt and B∗t = Bt, (15)\nwhere mbwt and V bwt are computed by the backward Kalman-Bucy filtering equations. The computations are somewhat lengthy; a full derivation can be found in Section B.3 of the Supplementary Material."
    }, {
      "heading" : "2.2.5 Corrections to the marginals",
      "text" : "In this section we extend the factorised correction method for multivariate latent Gaussian models introduced in Cseke and Heskes [2011b] to continuous time observations. Other correction schemes [e.g. Opper et al., 2009] can in principle also be applied. We start again from the discretised representation and then take the ∆tk → 0. To begin with, we focus on the corrections from the continuous time observation process. By removing the Gaussian terms (with canonical parameters λctk ) from the approximate posterior and replacing them with the exact likelihood, we can rewrite the exact discretised posterior as\np(x) ∝ q0(x)× exp  − \nk\n∆tk[V (tk,xtk ) + λ c tk · f(xt)]\n .\nThe exact posterior marginal at time tj is thus given by\np(xtj ) ∝ q0(xtj )× exp  −∆tj [V (tj ,xtj + λ c tj · f(xtj ))]  × cT (xtj )\nwith\ncT (xtj ) =  dx\\tj q0(x\\tj |xtj )× exp  − \nk =j\n∆tk[V (tk,xtk ) + λ c tk · f .(xtk )]\n ,\nwhere the subscript \\j indicates the whole vector with the j-th entry removed. By approximating the joint conditional q0(x\\tj |xtj ) with a product of its marginals and taking the ∆tk → 0 limit, we obtain\nc(xt)  exp  −\n 1\n0\nds V (s,xs) + λcs · f(xs)q0(xs|xt)  .\nWhen combining the continuous part and the factorised discrete time corrections—by adding the discrete time terms to the formalism above—we arrive to the corrected approximate marginal\np̃(xt) ∝ q0(xt) exp  −\n 1\n0\nds V (s,xs) + λcs · f(xs)q0(xs|xt)  × \ni\n p(ydti |xti)\nexp{λdti · f(xti)}\n\nq0(xti |xt)\n.\nFor any fixed t one can compute the correlations in linear time by using the parametric form of the approximation in 15. The evaluations for a fixed xt are also linear in time."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Inference in a (soft) box",
      "text" : "The first example we consider is a mixed discrete-continuous time inference under box and soft box likelihood observations respectively. We consider a diffusing particle on the line under an OU prior process of the form\ndxt = (−axt + ct)dt+ √ bdWt\nwith a = −1, ct = 4π cos(4πt) and b = 4. The likelihood model is given by the loss function V (t, xt) = (2xt)8 for all t ∈ [1/2, 2/3] and 0 otherwise, effectively confining the process to a narrow strip near zero (soft box). This likelihood is therefore an approximation to physically realistic situations where particles can perform diffusion in a confined environment. The box has hard gates: two discrete time likelihoods given by the indicator functions I[−0.25,0.25](xt1) and I[−0.25,0.25](xt2) placed at the ends of the interval, that is, Td = {1/3, 2/3}. The left panel in Figure 1 shows the prior and approximate posterior processes (mean ± one standard deviation) in pink and cyan respectively: the confinement of the process to the box is in clear evidence, as well as the narrowing of the confidence intervals corresponding to the two discrete time observations. The right panel in Figure 1 shows the marginal approximations at a time point shortly after the “gate” to the box, these are: (i) sampling (grey) (ii) the Gaussian EP approximation (blue line), and (iii) its corrected version (red line). The time point was chosen as we expect the strongest non-Gaussian effects to be felt near the discrete likelihoods; the corrected distribution does indeed show strong skewness. To benchmark the method, we compare it to MCMC sampling obtained by using slice sampling [Murray et al., 2010] on the discretised model with ∆t = 10−3. We emphasise that this is an approximation to the model, hence the benchmark is not a true gold standard; however, we are not aware of sampling schemes that would be able to perform inference under the exact continuous time likelihood. The histogram in Figure 1 was generated from a sample size of 105 following a burn in of 104. The Gaussian EP approach gives a very good reconstruction of the first two moments of the distribution. The corrected EP approximation is very close to the MCMC results."
    }, {
      "heading" : "3.2 Log Gaussian Cox processes",
      "text" : "Another family of models where one encounters continuous time likelihoods is point processes; these processes find wide application in a number of disciplines, from neuroscience Smith and Brown [2003] to conflict modelling Zammit-Mangion et al. [2012]. We assume that we have a multivariate log Gaussian Cox process model [Kingman, 1992]: this is defined by a d-variate Ornstein-Uhlenbeck process {xt}t\non the [0, 1] interval. Conditioned on {xt}t we have d Poisson point processes with intensities given by λit = e µi+x i t for all i = 1, . . . , d and t ∈ [0, 1]. The likelihood of this point process model is formed by both discrete time (point probabilities) and continuous time (void probability) terms and can be written as\nlog \ni\np(Yi|{xit}t) . =\n\ni\n − eµi\n 1\n0\ndtex i t + |Yi|µi +\n\ntk∈Yi\nxit  ,\nwhere Yi denotes the set of observed event times corresponding to {xit}t. Clearly, the discrete time observations in this model are (degenerate) Gaussians, therefore, one may opt for starting with an OU process with a translated drift, however, for consistency reasons, we treat them as discrete time observations.\nIn this example we chose d = 4 and A = [−2, 1, 0, 1; 1,−2, 1, 0; 0, 1,−2, 1; 1, 0, 1,−2], thus coupling the various processes. We chose cit = 4iπ cos(2iπt), B = 4I and µi = 0. We generate a sample path {x̃t}t, draw observations Yi based on {x̃it}t and perform inference. The results are shown in Figure 2, with four colours distinguishing the four processes. The left panel shows prior processes (mean ± standard deviation), sample paths and (bottom row) the sampled points (i.e. the data). The right panel shows the corresponding posterior processes approximations. The results reflect the general pattern characteristic of fitting point process data: in regions with a substantial number of events the sampled path can be inferred with great accuracy (accurate mean, low standard deviation) whereas in regions with no or only a few events the fit reverts to a skewed/shifted prior path, as the void probability dominates."
    }, {
      "heading" : "3.3 Point process modelling of neural spikes trains",
      "text" : "In a third example we consider continuous time point process inference for spike time recordings from a population of neurons. This type of data is frequently modelled using (discrete time) state-space models with point process observations (SSPP) [Smith and Brown, 2003, Zammit Mangion et al., 2011, Macke et al., 2011]; parameter estimation from such models can reveal biologically relevant facts about the neuron’s electrophysiology which are not apparent from the spike trains themselves. We consider a dataset from Di Lorenzo and Victor [2003], available at www.neurodatabase.org, consisting of recordings of spiking patterns of taste response cells in Sprague-Dawley rats during presentation of different taste stimuli. The recordings are 10s each at a resolution of 10−3s, and four different taste stimuli: (i) NaCL, (ii) Quinine HCl, (iii) Quinine HCl, and (iv) Sucrose are presented to the subjects for the duration of the first 5s of the 10s recording window. We modelled the spike train recordings by univariate log Gaussian Cox process models (see Section 3.2) with homogeneous OU priors, that is, At, ct and Bt were considered constant. We use the variational EM algorithm (discrete time likelihoods are Gaussian) to learn the prior\nparameters A, c and µ and initial conditions for each individual recording. We scaled the 10s window into the unit interval [0, 1] and used a 10−4 resolution.\nFig 3 shows example results of this procedure. The right panel shows an emergent pattern of stimulus based clustering of µ and c as in Zammit Mangion et al. [2011]. We observe that discrete-time approaches such as [Smith and Brown, 2003, Zammit Mangion et al., 2011] are usually forced to take very fine time discretisation by the requirement that at most one spike happens during one time step. This leads to significant computational resources being invested in regions with few spikes. Our continuous time approach, on the other hand, handles uneven observations naturally."
    }, {
      "heading" : "4 Conclusion",
      "text" : "Inference methodologies for continuous time stochastic processes are a subject of intense research, both for fundamental and applied research. This paper contributes a novel approach which allows inference from both discrete time and continuous time observations. Our results show that the method is effective in accurately reconstructing marginal posterior distributions, and can be deployed effectively on real world problems. Furthermore, it has recently been shown [Kappen et al., 2012] that optimal control problems can be recast in inference terms: in many cases, the relevant inference problem is of the same type as the one considered here, hence this methodology could in principle also be used in control problems. The method is based on the parallel EP formulation of Cseke and Heskes [2011b]: interestingly, we show that the EP updates from continuous time observations collapse to variational updates [Archambeau et al., 2007]. Algorithmically, our approach results in efficient forward-backward updates, compared to the gradient ascent algorithm of Archambeau et al. [2007]. Furthermore, the EP perspective allows us to compute corrections to the Gaussian marginals; in our experiments, these turned out to be highly accurate.\nOur modelling framework assumes a latent linear diffusion process; however, as mentioned before, some non-linear diffusion processes are equivalent to posterior processes for OU processes observed in continuous time [Øksendal, 2010]. Our approach, hence, can also be viewed as a method for accurate marginal computations in (a class of) nonlinear diffusion processes observed with noise. In general, all non-linear diffusion processes can be recast in a form similar to the one considered here; the important difference though is that the continuous time likelihood is in general an Ito integral, not a regular integral. In the future, it would be interesting to explore the extension of this approach to general non-linear diffusion processes, as well as discrete and hybrid stochastic processes [Rao and Teh, 2012, Ocone et al., 2013]."
    }, {
      "heading" : "Acknowledgements",
      "text" : "B.Cs. is funded by BBSRC under grant BB/I004777/1. M.O. would like to thank for the support by EU grant FP7-ICT-270327 (Complacs). G.S. acknowledges support from the ERC under grant MLCS-306999."
    } ],
    "references" : [ {
      "title" : "Gaussian process approximations of stochastic differential equations",
      "author" : [ "C. Archambeau", "D. Cornford", "M. Opper", "J. Shawe-Taylor" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Archambeau et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Archambeau et al\\.",
      "year" : 2007
    }, {
      "title" : "Properties of Bethe free energies and message passing in Gaussian models",
      "author" : [ "B. Cseke", "T. Heskes" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Cseke and Heskes.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cseke and Heskes.",
      "year" : 2011
    }, {
      "title" : "Approximate marginals in latent Gaussian models",
      "author" : [ "B. Cseke", "T. Heskes" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Cseke and Heskes.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cseke and Heskes.",
      "year" : 2011
    }, {
      "title" : "Direct Methods for Sparse Linear Systems (Fundamentals of Algorithms 2)",
      "author" : [ "T.A. Davis" ],
      "venue" : "Society for Industrial and Applied Mathematics, Philadelphia,",
      "citeRegEx" : "Davis.,? \\Q2006\\E",
      "shortCiteRegEx" : "Davis.",
      "year" : 2006
    }, {
      "title" : "Taste response variability and temporal coding in the nucleus of the solitary tract of the rat",
      "author" : [ "P.M. Di Lorenzo", "J.D. Victor" ],
      "venue" : "Journal of Neurophysiology,",
      "citeRegEx" : "Lorenzo and Victor.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lorenzo and Victor.",
      "year" : 2003
    }, {
      "title" : "Handbook of stochastic methods: for physics, chemistry and the natural sciences",
      "author" : [ "C.W. Gardiner" ],
      "venue" : "Springer series in synergetics,",
      "citeRegEx" : "Gardiner.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gardiner.",
      "year" : 2002
    }, {
      "title" : "Approximate inference techniques with expectation constraints",
      "author" : [ "T. Heskes", "M. Opper", "W. Wiegerinck", "O. Winther", "O. Zoeter" ],
      "venue" : "Journal of Statistical Mechanics: Theory and Experiment,",
      "citeRegEx" : "Heskes et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Heskes et al\\.",
      "year" : 2005
    }, {
      "title" : "Optimal control as a graphical model inference problem",
      "author" : [ "H.J. Kappen", "V. Gómez", "M. Opper" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kappen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kappen et al\\.",
      "year" : 2012
    }, {
      "title" : "Kingman. Poisson Processes",
      "author" : [ "C.J. F" ],
      "venue" : null,
      "citeRegEx" : "F.,? \\Q1992\\E",
      "shortCiteRegEx" : "F.",
      "year" : 1992
    }, {
      "title" : "Graphical Models",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : null,
      "citeRegEx" : "Lauritzen.,? \\Q1996\\E",
      "shortCiteRegEx" : "Lauritzen.",
      "year" : 1996
    }, {
      "title" : "Empirical models of spiking in neural populations",
      "author" : [ "J.H. Macke", "L. Buesing", "J.P. Cunningham", "B.M. Yu", "K.V. Shenoy", "M. Sahani" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Macke et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Macke et al\\.",
      "year" : 2011
    }, {
      "title" : "A family of algorithms for approximate Bayesian inference",
      "author" : [ "T.P. Minka" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Minka.,? \\Q2001\\E",
      "shortCiteRegEx" : "Minka.",
      "year" : 2001
    }, {
      "title" : "Elliptical slice sampling",
      "author" : [ "I. Murray", "R.P. Adams", "D.J.C. MacKay" ],
      "venue" : "In Proceedings of the 13th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Murray et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2010
    }, {
      "title" : "Hybrid regulatory models: a statistically tractable approach to model regulatory network",
      "author" : [ "A. Ocone", "A.J. Millar", "G. Sanguinetti" ],
      "venue" : "dynamics. Bioinformatics,",
      "citeRegEx" : "Ocone et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ocone et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic differential equations",
      "author" : [ "B. Øksendal" ],
      "venue" : "Universitext. Springer,",
      "citeRegEx" : "Øksendal.,? \\Q2010\\E",
      "shortCiteRegEx" : "Øksendal.",
      "year" : 2010
    }, {
      "title" : "Variational inference for Markov jump processes",
      "author" : [ "M. Opper", "G. Sanguinetti" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Opper and Sanguinetti.,? \\Q2008\\E",
      "shortCiteRegEx" : "Opper and Sanguinetti.",
      "year" : 2008
    }, {
      "title" : "Gaussian processes for classification: Mean-field algorithms",
      "author" : [ "M. Opper", "O. Winther" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Opper and Winther.,? \\Q2000\\E",
      "shortCiteRegEx" : "Opper and Winther.",
      "year" : 2000
    }, {
      "title" : "Expectation consistent approximate inference",
      "author" : [ "M. Opper", "O. Winther" ],
      "venue" : "Journal of Machine Learing Research,",
      "citeRegEx" : "Opper and Winther.,? \\Q2005\\E",
      "shortCiteRegEx" : "Opper and Winther.",
      "year" : 2005
    }, {
      "title" : "Improving on Expectation Propagation",
      "author" : [ "M. Opper", "U. Paquet", "O. Winther" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Opper et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Opper et al\\.",
      "year" : 2009
    }, {
      "title" : "Approximate inference in continuous time Gaussian-Jump processes",
      "author" : [ "M. Opper", "A. Ruttor", "G. Sanguinetti" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Opper et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Opper et al\\.",
      "year" : 2010
    }, {
      "title" : "MCMC for continuous-time discrete-state systems",
      "author" : [ "V. Rao", "Y-W Teh" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Rao and Teh.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rao and Teh.",
      "year" : 2012
    }, {
      "title" : "Recursive Bayesian Inference on Stochastic Differential Equations",
      "author" : [ "S. Särkkä" ],
      "venue" : "PhD thesis, Helsinki University of Technology,",
      "citeRegEx" : "Särkkä.,? \\Q2006\\E",
      "shortCiteRegEx" : "Särkkä.",
      "year" : 2006
    }, {
      "title" : "Estimating a state-space model from point process observations",
      "author" : [ "A.C. Smith", "E.N. Brown" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Smith and Brown.,? \\Q2003\\E",
      "shortCiteRegEx" : "Smith and Brown.",
      "year" : 2003
    }, {
      "title" : "Fractional Belief Propagation",
      "author" : [ "W. Wiegerinck", "T. Heskes" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Wiegerinck and Heskes.,? \\Q2003\\E",
      "shortCiteRegEx" : "Wiegerinck and Heskes.",
      "year" : 2003
    }, {
      "title" : "Generalized belief propagation",
      "author" : [ "J.S. Yedidia", "W.T. Freeman", "Y. Weiss" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Yedidia et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Yedidia et al\\.",
      "year" : 2000
    }, {
      "title" : "Online variational inference for state-space models with point-process observations",
      "author" : [ "A. Zammit Mangion", "K. Yuan", "V. Kadirkamanathan", "M. Niranjan", "G. Sanguinetti" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Mangion et al\\.,? \\Q1967\\E",
      "shortCiteRegEx" : "Mangion et al\\.",
      "year" : 1967
    }, {
      "title" : "Point process modelling of the Afghan war diary",
      "author" : [ "A. Zammit-Mangion", "M.G. Dewar", "A. Kadirkamanathan V", "G. Sanguinetti" ],
      "venue" : "Proceeding of the National Academy of Sciences,",
      "citeRegEx" : "Zammit.Mangion et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zammit.Mangion et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Remarkably, we show that, in the presence of continuous time observations, the update equations for the EP algorithm reduce to updates for a variational Gaussian approximation [Archambeau et al., 2007].",
      "startOffset" : 176,
      "endOffset" : 201
    }, {
      "referenceID" : 5,
      "context" : "where {Wt}t is the standard Wiener process [Gardiner, 2002] and At,Bt and ct are time dependent matrix and vector valued functions respectively with Bt being positive definite for all t ∈ [0, 1].",
      "startOffset" : 43,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "The time evolution of the marginal mean mt and covariance Vt is governed by the pair of differential equations [Gardiner, 2002]",
      "startOffset" : 111,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "Readers familiar with the classical formulation of EP [Minka, 2001] will recognise in equation (5) the so-called term updates, where φ̃i (xti)q0(xti) is the cavity distribution and φi (xti)φ̃i (xti)q0(xti) the tilted distribution.",
      "startOffset" : 54,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "The algorithm can also be derived and justified as a constrained optimisation problem of a Gibbs free energy formulation [Heskes et al., 2005]; this alternative approach can also be shown to extend to the continuous time limit (see Section A.",
      "startOffset" : 121,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "This corresponds to a junction tree algorithm on a (block) chain graph [Davis, 2006] which, in the continuous time limit, can be reduced to a set of differential equations",
      "startOffset" : 71,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "To benchmark the method, we compare it to MCMC sampling obtained by using slice sampling [Murray et al., 2010] on the discretised model with ∆t = 10−3.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, it has recently been shown [Kappen et al., 2012] that optimal control problems can be recast in inference terms: in many cases, the relevant inference problem is of the same type as the one considered here, hence this methodology could in principle also be used in control problems.",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "The method is based on the parallel EP formulation of Cseke and Heskes [2011b]: interestingly, we show that the EP updates from continuous time observations collapse to variational updates [Archambeau et al., 2007].",
      "startOffset" : 189,
      "endOffset" : 214
    }, {
      "referenceID" : 14,
      "context" : "Our modelling framework assumes a latent linear diffusion process; however, as mentioned before, some non-linear diffusion processes are equivalent to posterior processes for OU processes observed in continuous time [Øksendal, 2010].",
      "startOffset" : 216,
      "endOffset" : 232
    } ],
    "year" : 2013,
    "abstractText" : "We propose an approximate inference algorithm for continuous time Gaussian Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid fixed point iteration consisting of (1) expectation propagation updates for discrete time terms and (2) variational updates for the continuous time term. We introduce postinference corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and significant computational savings compared to discrete-time approaches in a neural application.",
    "creator" : null
  }
}