{
  "name" : "a9be4c2a4041cadbf9d61ae16dd1389e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Convex Two-Layer Modeling",
    "authors" : [ "Özlem Aslan", "Hao Cheng", "Dale Schuurmans", "Xinhua Zhang" ],
    "emails" : [ "ozlem@cs.ualberta.ca", "hcheng2@cs.ualberta.ca", "dale@cs.ualberta.ca", "xinhua.zhang@anu.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep learning has recently been enjoying a resurgence [1, 2] due to the discovery that stage-wise pre-training can significantly improve the results of classical training methods [3–5]. The advantage of latent variable models is that they allow abstract “semantic” features of observed data to be represented, which can enhance the ability to capture predictive relationships between observed variables. In this way, latent variable models can greatly simplify the description of otherwise complex relationships between observed variates. For example, in unsupervised (i.e., “generative”) settings, latent variable models have been used to express feature discovery problems such as dimensionality reduction [6], clustering [7], sparse coding [8], and independent components analysis [9]. More recently, such latent variable models have been used to discover abstract features of visual data invariant to low level transformations [1, 2, 4]. These learned representations not only facilitate understanding, they can enhance subsequent learning.\nOur primary focus in this paper, however, is on conditional modeling. In a supervised (i.e. “conditional”) setting, latent variable models are used to discover intervening feature representations that allow more accurate reconstruction of outputs from inputs. One advantage in the supervised case is that output information can be used to better identify relevant features to be inferred. However, latent variables also cause difficulty in this case because they impose nested nonlinearities between the input and output variables. Some important examples of conditional latent learning approaches include those that seek an intervening lower dimensional representation [10] latent clustering [11], sparse feature representation [8] or invariant latent representation [1, 3, 4, 12] between inputs and outputs. Despite their growing success, the difficulty of training a latent variable model remains clear: since the model parameters have to be trained concurrently with inference over latent variables, the convexity of the training problem is usually destroyed. Only highly restricted models can be trained to optimality, and current deep learning strategies provide no guarantees about solution quality. This remains true even when restricting attention to a single stage of stage-wise pre-training: simple models such as the two-layer auto-encoder or restricted Boltzmann machine (RBM) still pose intractable training problems, even within a single stage (in fact, simply computing the gradient of the RBM objective is currently believed to be intractable [13]).\nMeanwhile, a growing body of research has investigated reformulations of latent variable learning that are able to yield tractable global training methods in special cases. Even though global training formulations are not a universally accepted goal of deep learning research [14], there are several useful methodologies that have been been applied successfully to other latent variable models: boosting strategies [15–17], semidefinite relaxations [18–20], matrix factorization [21–23], and moment based estimators (i.e. “spectral methods”) [24, 25]. Unfortunately, none of these approaches has yet been able to accommodate a non-trivial hidden layer between an input and output layer while retaining the representational capacity of an auto-encoder or RBM (e.g. boosting strategies embed an intractable subproblem in these cases [15–17]). Some recent work has been able to capture restricted forms of latent structure in a conditional model—namely, a single latent cluster variable [18–20]—but this remains a rather limited approach.\nIn this paper we demonstrate that more general latent variable structures can be accommodated within a tractable convex framework. In particular, we show how two-layer latent conditional models with a single latent layer can be expressed equivalently in terms of a latent feature kernel. This reformulation allows a rich set of latent feature representations to be captured, while allowing useful convex relaxations in terms of a semidefinite optimization. Unlike [26], the latent kernel in this model is explicitly learned (nonparametrically). To cope with scaling issues we further develop an efficient algorithmic approach for the proposed relaxation. Importantly, the resulting method preserves sufficient problem structure to recover prediction models that cannot be represented by any one-layer architecture over the same input features, while improving the quality of local training."
    }, {
      "heading" : "2 Two-Layer Conditional Modeling",
      "text" : "We address the problem of training a two-layer latent conditional model in the form of Figure 1; i.e., where there is a single layer of h latent variables, , between a layer of n input variables, x, and m output variables, y. The goal is to predict an output vector y given an input vector x. Here, a prediction model consists of the composition of two nonlinear conditional models, f\n1 (Wx) ; and f 2 (V ) ; ŷ, parameterized by the matrices W 2 Rh⇥n and V 2 Rm⇥h. Once the parameters W and V have been specified, this architecture defines a point predictor that can determine ˆy from x by first computing an intermediate representation .\nTo learn the model parameters, we assume we are given t training pairs {(xj ,yj)}tj=1, stacked in two matrices X = (x\n1 , ...,xt) 2 Rn⇥t and Y = (y1, ...,yt) 2 Rm⇥t, but the corresponding set of latent variable values = (\n1 , ..., t) 2 Rh⇥t remains unobserved.\nf\n1 (Wx) ; , f 2 (V ) ; ŷ, where j is a latent variable, xj is an observed input vector, yj is an observed output vector, W are first layer parameters, and V are second layer parameters.\nTo formulate the training problem, we will consider two losses, L 1 and L 2 , that relate the input to the latent layer, and the latent to the output layer respectively. For example, one can think of losses as negative log-likelihoods in a conditional model that generates each successive layer given its predecessor; i.e., L\n1 (Wx, ) = log pW ( |x) and L2(V ,y) = log pV (y| ). (However, a loss based formulation is more flexible, since every negative log-likelihood is a loss but not vice versa.) Similarly to RBMs and probabilistic networks (PFNs) [27] (but unlike auto-encoders and classical feed-forward networks), we will not assume is a deterministic output of the first layer; instead we will consider to be a variable whose value is the subject of inference during training.\nGiven such a set-up many training principles become possible. For simplicity, we consider a Viterbi based training principle where the parameters W and V are optimized with respect to an optimal imputation of the latent values . To do so, define the first and second layer training objectives as\nF 1 (W, ) = L 1 (WX, ) + ↵ 2 kWk2F , and F2( , V ) = L2(V , Y ) + 2 kV k2F , (1) where we assume the losses are convex in their first arguments. Here it is typical to assume that the losses decompose columnwise; that is, L\n1\n(\nˆ , ) = Pt\nj=1 L1( ˆ j , j) and L2(Z, Y ) =Pt\nj=1 L2(ˆzj ,yj), where ˆ j is the jth column of ˆ and ˆzj is the jth column of ˆZ respectively. This\nfollows for example if the training pairs (xj ,yj) are assumed I.I.D., but such a restriction is not necessary. Note that we have also introduced Euclidean regularization over the parameters (i.e. negative log-priors under a Gaussian), which will provide a useful representer theorem [28] we exploit later. These two objectives can be combined to obtain the following joint training problem:\nmin W,V min\nF 1 (W, ) + F 2 ( , V ), (2)\nwhere > 0 is a trade off parameter that balances the first versus second layer discrepancy. Unfortunately (2) is not jointly convex in the unknowns W , V and .\nA key modeling question concerns the structure of the latent representation . As noted, the extensive literature on latent variable modeling has proposed a variety of forms for latent structure. Here, we follow work on deep learning and sparse coding and assume that the latent variables are boolean, 2 {0, 1}h⇥1; an assumption that is also often made in auto-encoders [13], PFNs [27], and RBMs [5]. A boolean representation can capture structures that range from a single latent clustering [11, 19, 20], by imposing the assumption that 01 = 1, to a general sparse code, by imposing the assumption that 01 = k for some small k [1, 4, 13].1 Observe that, in the latter case, one can control the complexity of the latent representation by imposing a constraint on the number of “active” variables k rather than directly controlling the latent dimensionality h."
    }, {
      "heading" : "2.1 Multi-Layer Perceptrons and Large-Margin Losses",
      "text" : "To complete a specification of the two-layer model in Figure 1 and the associated training problem (2), we need to commit to specific forms for the transfer functions f\n1 and f 2 and the losses in (1). For simplicity, we will adopt a large-margin approach over two-layer perceptrons. Although it has been traditional in deep learning research to focus on exponential family conditional models (e.g. as in auto-encoders, PFNs and RBMs), these are not the only possibility; a large-margin approach offers additional sparsity and algorithmic simplifications that will clarify the development below. Despite its simplicity, such an approach will still be sufficient to prove our main point.\nFirst, consider the second layer model. We will conduct our primary evaluations on multiclass classification problems, where output vectors y encode target classes by indicator vectors y 2 {0, 1}m⇥1 such that y01 = 1. Although it is common to adopt a softmax transfer for f\n2 in such a case, it is also useful to consider a perceptron model defined by f\n2\n(\nˆ z) = indmax( ˆ z) such that indmax( ˆ\nz) = 1i (vector of all 0s except a 1 in the ith position) where ẑi ẑl for all l. Therefore, for multi-class classification, we will simply adopt the standard large-margin multi-class loss [29]:\nL 2 ( ˆ z,y) = max(1 y + ˆz 1y0ˆz). (3) Intuitively, if yc = 1 is the correct label, this loss encourages the response ẑc = y0ˆz on the correct label to be a margin greater than the response ẑi on any other label i 6= c. Second, consider the first layer model. Although the loss (3) has proved to be highly successful for multi-class classification problems, it is not suitable for the first layer because it assumes there is only a single target component active in any latent vector ; i.e. 01 = 1. Although some work has considered learning a latent clustering in a two-layer architecture [11, 18–20], such an approach is not able to capture the latent sparse code of a classical PFN or RBM in a reasonable way: using clustering to simulate a multi-dimensional sparse code causes exponential blow-up in the number of latent classes required. Therefore, we instead adopt a multi-label perceptron model for the first layer, defined by the transfer function f\n1\n( ˆ ) = step( ˆ ) applied componentwise to the response vector ˆ ; i.e. step( ˆ i) = 1 if ˆ i > 0, 0 otherwise. Here again, instead of using a traditional negative loglikelihood loss, we will adopt a simple large-margin loss for multi-label classification that naturally accommodates multiple binary latent classifications in parallel. Although several loss formulations exist for multi-label classification [30, 31], we adopt the following: L 1 (\nˆ , ) = max(1 + ˆ 01 1 0 ˆ ) ⌘ max (1 )/( 01) + ˆ 1 0 ˆ /( 01) . (4) Intuitively, this loss encourages the average response on the active labels, 0 ˆ /( 01), to exceed the response ˆ i on any inactive label i, i = 0, by some margin, while also encouraging the response on any active label to match the average of the active responses. Despite their simplicity, large-margin multi-label losses have proved to be highly successful in practice [30, 31]. Therefore, the overall architecture we investigate embeds two nonlinear conditionals around a non-trivial latent layer.\n1 Throughout this paper we let 1 denote the vector of all 1s with length determined by context."
    }, {
      "heading" : "3 Equivalent Reformulation",
      "text" : "The main contribution of this paper is to show that the training problem (2) has a convex relaxation that preserves sufficient structure to transcend one-layer models. To demonstrate this relaxation, we first need to establish the key observation that problem (2) can be re-expressed in terms of a kernel matrix between latent representation vectors. Importantly, this reformulation allows the problem to be re-expressed in terms of an optimization objective that is jointly convex in all participating variables. We establish this key intermediate result in this section in three steps: first, by re-expressing the latent representation in terms of a latent kernel; second, by reformulating the second layer objective; and third, by reformulating the first layer objective by exploiting large-margin formulation outlined in Section 2.1. Below let K = X 0X denote the kernel matrix over the input data, let Im(N) denote the row space of N , and let and † denote Moore-Penrose pseudo-inverse.\nFirst, simply define N = 0 . Next, re-express the second layer objective F 2\nin (1) by the following. Lemma 1. For any fixed , letting N = 0 , it follows that\nmin V F 2 ( , V ) = min B2Im(N) L 2 (B, Y ) + 2\ntr(BN†B0). (5)\nProof. The result follows from the following sequence of equivalence preserving transformations:\nmin V L 2 (V , Y ) + 2 kV k2F = min A L 2 (AN, Y ) + 2 tr(ANA0) (6)\n= min B2Im(N) L 2 (B, Y ) + 2\ntr(BN†B0), (7)\nwhere, starting with the definition of F 2 in (1), the first equality in (6) follows from the representer theorem applied to kV k2F , which implies that the optimal V must be in the form of V = A 0 for some A 2 Rm⇥t [28]; and finally, (7) follows by the change of variable B = AN . Note that Lemma 1 holds for any loss L\n2 . In fact, the result follows solely from the structure of the regularizer. However, we require L\n2 to be convex in its first argument to ensure a convex problem below. Convexity is indeed satisfied by the choice (3). Moreover, the term tr(BN†B0) is jointly convex in N and B since it is a perspective function [32], hence the objective in (5) is jointly convex.\nNext, we reformulate the first layer objective F 1 in (1). Since this transformation exploits specific structure in the first layer loss, we present the result in two parts: first, by showing how the desired outcome follows from a general assumption on L\n1 , then demonstrating that this assumption is satisfied by the specific large-margin multi-label loss defined in (4). To establish this result we will exploit the following augmented forms for the data and variables: let ˜ = [ , kI], ˜N = ˜ 0 ˜ , ˜\n= [\nˆ , 0], ˜X = [X, 0], ˜K = ˜X 0 ˜X , and ˜t = t+ h.\nLemma 2. For any L 1 if there exists a function ˜L 1 such that L 1 ( ˆ , ) = ˜L 1 ( ˜\n0 ˜ , ˜ 0 ˜ ) for all ˆ\n2 Rh⇥t and 2 {0, 1}h⇥t, such that 01 = 1k, it then follows that min\nW F 1 (W, ) = min D2Im( ˜N) ˜L 1\n(D ˜K, ˜N) + ↵ 2 tr(D0 ˜N†D ˜K). (8)\nProof. Similar to above, consider the sequence of equivalence preserving transformations:\nmin W L 1\n(WX, ) + ↵ 2 kWk2F = min W ˜L 1 ( ˜ 0W ˜X, ˜ 0 ˜ ) + ↵ 2 kWk2F (9) = min\nC ˜L 1 ( ˜\n0 ˜ C ˜X 0 ˜X, ˜ 0 ˜ ) + 2 tr( ˜XC 0 ˜ 0 ˜ C ˜X 0) (10)\n= min D2Im( ˜N) ˜L 1\n(D ˜K, ˜N) + ↵ 2 tr(D0 ˜N†D ˜K), (11)\nwhere, starting with the definition of F 1 in (1), the first equality (9) simply follows from the assumption. The second equality (10) follows from the representer theorem applied to kWk2F , which implies that the optimal W must be in the form of W = ˜ C ˜X 0 for some C 2 R˜t⇥˜t (using the fact that ˜ has full rank h) [28]. Finally, (11) follows by the change of variable D = ˜NC.\nObserve that the term tr(D0 ˜N†D ˜K) is again jointly convex in ˜N and D (also a perspective function), while it is easy to verify that ˜L\n1 (D ˜K, ˜N) as defined in Lemma 3 below is also jointly convex in ˜N and D [32]; therefore the objective in (8) is jointly convex.\nNext, we show that the assumption of Lemma 2 is satisfied by the specific large-margin multi-label formulation in Section 2.1; that is, assume L\n1\nis given by the large-margin multi-label loss (4):\nL 1 ( ˆ\n, ) = P\nj max 1 j + ˆ j 0j1 1 0j ˆ j\n= ⌧ 11 0 + ˆ diag( 01) 1 diag( 0 ˆ )0 , such that ⌧(⇥) := Pj max(✓j), (12) where we use ˆ j , j and ✓j to denote the jth columns of ˆ , and ⇥ respectively.\nLemma 3. For the multi-label loss L 1 defined in (4), and for any fixed 2 {0, 1}h⇥t where\n0 1 = 1k, the definition ˜L\n1\n(\n˜ 0 ˜ , ˜ 0 ˜ ) := ⌧(˜ 0 ˜ ˜ 0 ˜ /k)+ t tr(˜ 0 ˜ ) using the augmentation above satisfies the property that L\n1\n(\nˆ , ) = ˜L 1 ( ˜\n0 ˜ , ˜ 0 ˜ ) for any ˆ 2 Rh⇥t. Proof. Since 01 = 1k we obtain a simplification of L\n1\n:\nL 1 ( ˆ , ) = ⌧ 11 0 + k ˆ 1 diag( 0 ˆ )0 = ⌧(k ˆ ) + t tr(˜ 0 ˜ ). (13) It only remains is to establish that ⌧(k ˆ ) = ⌧(˜ 0 ˜ ˜ 0 ˜ /k). To do so, consider the sequence of equivalence preserving transformations:\n⌧(k ˆ ) = max ⇤2Rh⇥t̃+ :⇤01=1 tr ⇤\n0 (k ˜ ˜ ) (14)\n= max ⌦2Rt̃⇥t̃+ :⌦01=1 1 k tr ⌦\n0 ˜ 0 (k ˜ ˜ ) = ⌧(˜ 0 ˜ ˜ 0 ˜ /k), (15)\nwhere the equalities in (14) and (15) follow from the definition of ⌧ and the fact that linear maximizations over the simplex obtain their solutions at the vertices. To establish the equality between (14) and (15), since ˜ embeds the submatrix kI , for any⇤ 2 Rh⇥˜t\n+ there must exist an⌦ 2 R˜t⇥˜t +\nsatisfying ⇤ = ˜ ⌦/k. Furthermore, these matrices satisfy ⇤01 = 1 iff ⌦0 ˜ 01/k = 1 iff ⌦01 = 1.\nTherefore, the result (8) holds for the first layer loss (4), using ˜L 1 defined in Lemma 3. (The same result can be established for other loss functions, such as the multi-class large-margin loss.) Combining these lemmas yields the desired result of this section. Theorem 1. For any second layer loss and any first layer loss that satisfies the assumption of Lemma 2 (for example the large-margin multi-label loss (4)), the following equivalence holds:\n(2) = min { ˜N :9 2{0,1}t⇥hs.t. 1=1k, ˜N= ˜ 0 ˜ } min B2Im( ˜N) min D2Im( ˜N) ˜L 1 (D ˜K, ˜N) + ↵ 2 tr(D0 ˜N†D ˜K)\n+ L 2 (B, Y ) + 2\ntr(B ˜N†B0). (16)\n(Theorem 1 follows immediately from Lemmas 1 and 2.) Note that no relaxation has occurred thus far: the objective value of (16) matches that of (2). Not only has this reformulation resulted in (2) being entirely expressed in terms of the latent kernel matrix ˜N , the objective in (16) is jointly convex in all participating unknowns, ˜N , B and D. Unfortunately, the constraints in (16) are not convex."
    }, {
      "heading" : "4 Convex Relaxation",
      "text" : "We first relax the problem by dropping the augmentation 7! ˜ and working with the t⇥ t variable N = 0 . Without the augmentation, Lemma 3 becomes a lower bound (i.e. (14) (15)), hence a relaxation. To then achieve a convex form we further relax the constraints in (16). To do so, consider\nN 0 =\nN : 9 2 {0, 1}t⇥h such that 1 = 1k and N = 0 (17)\nN 1 =\nN : N 2 {0, ..., k}t⇥t, N ⌫ 0, diag(N) = 1k, rank(N)  h (18)\nN 2 = {N : N 0, N ⌫ 0, diag(N) = 1k} , (19) where it is clear from the definitions that N\n0 ✓ N 1 ✓ N 2 . (Here we use N ⌫ 0 to also encode N 0 = N .) Note that the set N\n0\ncorresponds to the original set of constraints from (16). The set\nAlgorithm 1: ADMM to optimize F(N) for N 2 N 2\n. 1 Initialize: M\n0 = I , 0 = 0. 2 while T = 1, 2, . . . do 3 NT argminN⌫0 L(N,MT 1, T 1), by using the boosting Algorithm 2. 4 MT argminM 0,Mii=k L(NT ,M, T 1), which has an efficient closed form solution. 5 T T 1 + 1µ (MT NT ); i.e. update the multipliers. 6 return NT .\nAlgorithm 2: Boosting algorithm to optimize G(N) for N ⌫ 0. 1 Initialize: N\n0\n0, H 0 [ ] (empty set). 2 while T = 1, 2, . . . do 3 Find the smallest arithmetic eigenvalue ofrG(NT 1), and its eigenvector hT . 4 Conic search by LBFGS: (aT , bT ) mina 0,b 0 G(aNT 1 + bhTh0T ).\nLocal search by LBFGS: HT local minHG(HH 0) initialized by H=(paHT 1, p bhT ).\n5 Set NT HTH 0T ; break if stopping criterion met. 6 return NT .\nN 1 simplifies the characterization of this constraint set on the resulting kernel matrices N = 0 . However, neither N\n0 nor N 1 are convex. Therefore, we need to adopt the further relaxed set N 2\n, which is convex. (Note that Nij  k has been implied by N ⌫ 0 and Nii = k in N2.) Since dropping the rank constraint eliminates the constraints B 2 Im(N) and D 2 Im(N) in (16) when N 0 [32], we obtain the following relaxed problem, which is jointly convex in N , B and D:\nmin N2N2 min B2Rt⇥t min D2Rt⇥t ˜L 1\n(DK,N) + ↵ 2 tr(D0N†DK) + L 2 (B, Y ) + 2 tr(BN†B0). (20)"
    }, {
      "heading" : "5 Efficient Training Approach",
      "text" : "Unfortunately, nonlinear semidefinite optimization problems in the form (20) are generally thought to be too expensive in practice despite their polynomial theoretical complexity [33, 34]. Therefore, we develop an effective training algorithm that exploits problem structure to bypass the main computational bottlenecks. The key challenge is that N\n2 contains both semidefinite and affine constraints, and the pseudo-inverse N† makes optimization over N difficult even for fixed B and D.\nTo mitigate these difficulties we first treat (20) as the reduced problem, minN2N2 F(N), where F is an implicit objective achieved by minimizing out B and D. Note that F is still convex in N by the joint convexity of (20). To cope with the constraints on N we adopt the alternating direction method of multipliers (ADMM) [35] as the main outer optimization procedure; see Algorithm 1. This approach allows one to divide N\n2 into two groups, N ⌫ 0 and {Nij 0, Nii = k}, yielding the augmented Lagrangian\nL(N,M, ) = F(N) + (N⌫0) + (Mij 0,Mii=k) h , N Mi+ 1 2µ kN Mk2F , (21)\nwhere µ > 0 is a small constant, and denotes an indicator such that (·) = 0 if · is true, and1 otherwise. In this procedure, Steps 4 and 5 cost O(t2) time; whereas the main bottleneck is Step 3, which involves minimizing GT (N) := L(N,MT 1, T 1) over N ⌫ 0 for fixed MT 1 and T 1. Boosting for Optimizing over the Positive Semidefinite Cone. To solve the problem in Step 3 we develop an efficient boosting procedure based on [36] that retains low rank iterates NT while avoiding the need to determine N† when computing G(N) and rG(N); see Algorithm 2. The key idea is to use a simple change of variable. For example, consider the first layer objective and let G 1 (N) = minD ˜L1(DK,N) + ↵ 2 tr(D0N†DK). By defining D = NC, we obtain G 1 (N) = minC ˜L 1 (NCK,N) + ↵ 2\ntr(C 0NCK), which no longer involves N† but remains convex in C; this problem can be solved efficiently after a slight smoothing of the objective [37] (e.g. by LBFGS). Moreover, the gradient rG\n1\n(N) can be readily computed given C⇤. Applying the same technique\nto the second layer yields an efficient procedure for evaluating G(N) and rG(N). Finally note that many of the matrix-vector multiplications in this procedure can be further accelerated by exploiting the low rank factorization of N maintained by the boosting algorithm; see the Appendix for details.\nAdditional Relaxation. One can further reduce computation cost by adopting additional relaxations to (20). For example, by dropping N 0 and relaxing diag(N) = 1k to diag(N)  1k, the objective can be written as min{N⌫0,maxi Niik} F(N). Since maxi Nii is convex in N , it is well known that there must exist a constant c\n1 > 0 such that the optimal N is also an optimal solution to minN⌫0 F(N) + c1 (maxi Nii)2. While maxi Nii is not smooth, one can further smooth it with a softmax, to instead solve minN⌫0 F(N) + c1 (log P i exp(c2Nii)) 2 for some large c 2\n. This formulation avoids the need for ADMM entirely and can be directly solved by Algorithm 2."
    }, {
      "heading" : "6 Experimental Evaluation",
      "text" : "To investigate the effectiveness of the proposed relaxation scheme for training a two-layer conditional model, we conducted a number of experiments to compare learning quality against baseline methods. Note that, given an optimal solution N , B and D to (20), an approximate solution to the original problem (2) can be recovered heuristically by first rounding N to obtain , then recovering W and V , as shown in Lemmas 1 and 2. However, since our primary objective is to determine whether any convex relaxation of a two-layer model can even compete with one-layer or locally trained two-layer models (rather than evaluate heuristic rounding schemes), we consider a transductive evaluation that does not require any further modification of N , B and D. In such a set-up, training data is divided into a labeled and unlabeled portion, where the method receives X = [X`, Xu] and Y`, and at test time the resulting predictions ˆYu are evaluated against the held-out labels Yu.\nMethods. We compared the proposed convex relaxation scheme (CVX2) against the following methods: simple alternating minimization of the same two-layer model (2) (LOC2), a one-layer linear SVM trained on the labeled data (SVM1), the transductive one-layer SVM methods of [38] (TSJ1) and [39] (TSS1), and the transductive latent clustering method of [18, 19] (TJB2), which is also a two-layer model. Linear input kernels were used for all methods (standard in most deep learning models) to control the comparison between one and two-layer models. Our experiments were conducted with the following common protocol: First, the data was split into a separate training and test set. Then the parameters of each procedure were optimized by a three-fold cross validation on the training set. Once the optimal parameters were selected, they were fixed and used on the test set. For transductive procedures, the same three training sets from the first phase were used, but then combined with ten new test sets drawn from the disjoint test data (hence 30 overall) for the final evaluation. At no point were test examples used to select any parameters for any of the methods. We considered different proportions between labeled/unlabeled data; namely, 100/100 and 200/200.\nSynthetic Experiments. We initially ran a proof of concept experiment on three binary labeled artificial data sets depicted in Figure 2 (showing data set sizes n⇥ t) with 100/100 labeled/unlabeled training points. Here the goal was simply to determine whether the relaxed two-layer training method could preserve sufficient structure to overcome the limits of a one-layer architecture. Clearly, none of the data sets in Figure 2 are adequately modeled by a one-layer architecture (that does not cheat and use a nonlinear kernel). The results are shown in the Figure 2(d) table.\nAs expected, the one-layer models SVM1 and TSS1 were unable to capture any useful classification structure in these problems. (TSJ1 behaves similarly to TSS1.) The results obtained by CVX2, on the other hand, are encouraging. In these data sets, CVX2 is easily able to capture latent nonlinearities while outperforming the locally trained LOC2. Although LOC2 is effective in the first two cases, it exhibits weaker test accuracy while failing on the third data set. The two-layer method TJB2 exhibited convergence difficulties on these problems that prevented reasonable results.\nExperiments on “Real” Data Sets. Next, we conducted experiments on real data sets to determine whether the advantages in controlled synthetic settings could translate into useful results in a more realistic scenario. For these experiments we used a collection of binary labeled data sets: USPS, COIL and G241N from [40], Letter from [41], MNIST, and CIFAR-100 from [42]. (See Appendix B in the supplement for further details.) The results are shown in Tables 1 and 2 for the labeled/unlabeled proportions 100/100 and 200/200 respectively.\nThe relaxed two-layer method CVX2 again demonstrates effective results, although some data sets caused difficulty for all methods. The data sets can be divided into two groups, (MNIST, USPS, COIL) versus (Letter, CIFAR, G241N). In the first group, two-layer modeling demonstrates a clear advantage: CVX2 outperforms SVM1 by a significant margin. Note that this advantage must be due to two-layer versus one-layer modeling, since the transductive SVM methods TSS1 and TSJ1 demonstrate no advantage over SVM1. For the second group, the effectiveness of SVM1 demonstrates that only minor gains can be possible via transductive or two-layer extensions, although some gains are realized. The locally trained two-layer model LOC2 performed quite poorly in all cases. Unfortunately, the convex latent clustering method TJB2 was also not competitive on any of these data sets. Overall, CVX2 appears to demonstrate useful promise as a two-layer modeling approach."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have introduced a new convex approach to two-layer conditional modeling by reformulating the problem in terms of a latent kernel over intermediate feature representations. The proposed model can accommodate latent feature representations that go well beyond a latent clustering, extending current convex approaches. A semidefinite relaxation of the latent kernel allows a reasonable implementation that is able to demonstrate advantages over single-layer models and local training methods. From a deep learning perspective, this work demonstrates that trainable latent layers can be expressed in terms of reproducing kernel Hilbert spaces, while large margin methods can be usefully applied to multi-layer prediction architectures. Important directions for future work include replacing the step and indmax transfers with more traditional sigmoid and softmax transfers, while also replacing the margin losses with more traditional Bregman divergences; refining the relaxation to allow more control over the structure of the latent representations; and investigating the utility of convex methods for stage-wise training within multi-layer architectures."
    } ],
    "references" : [ {
      "title" : "Building high-level features using large scale unsupervised learning",
      "author" : [ "Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "G. Corrado", "K. Chen", "J. Dean", "A. Ng" ],
      "venue" : "Proceedings ICML,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multimodal learning with deep Boltzmann machines",
      "author" : [ "N. Srivastava", "R. Salakhutdinov" ],
      "venue" : "NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Y. Bengio" ],
      "venue" : "Foundat. and Trends in Machine Learning, 2:1–127,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning multiple layers of representations",
      "author" : [ "G. Hinton" ],
      "venue" : "Trends in Cognitive Sciences, 11:428–434,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A fast algorithm for deep belief nets",
      "author" : [ "G. Hinton", "S. Osindero", "Y. Teh" ],
      "venue" : "Neur. Comp., 18(7),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Probabilistic non-linear principal component analysis",
      "author" : [ "N. Lawrence" ],
      "venue" : "JMLR, 6:1783–1816,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Clustering with Bregman divergences",
      "author" : [ "A. Banerjee", "S. Merugu", "I. Dhillon", "J. Ghosh" ],
      "venue" : "J. Mach. Learn. Res., 6:1705–1749,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Image denoising via sparse and redundant representations over learned dictionaries",
      "author" : [ "M. Elad", "M. Aharon" ],
      "venue" : "IEEE Trans. on Image Processing, 15:3736–3745,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Independent component analysis, a new concept",
      "author" : [ "P. Comon" ],
      "venue" : "Signal Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1994
    }, {
      "title" : "dimensionality reduction by unsupervised regression",
      "author" : [ "M. Carreira-Perpiñán", "Z. Lu" ],
      "venue" : "CVPR,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The information bottleneck method",
      "author" : [ "N. Tishby", "F. Pereira", "W. Bialek" ],
      "venue" : "Allerton Conf.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Contractive auto-encoders: Explicit invariance during feature extraction",
      "author" : [ "S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio" ],
      "venue" : "ICML,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On autoencoders and score matching for energy based models",
      "author" : [ "K. Swersky", "M. Ranzato", "D. Buchman", "B. Marlin", "N. de Freitas" ],
      "venue" : "In Proceedings ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Who is afraid of non-convex loss functions? http://videolectures.net/eml07 lecun wia",
      "author" : [ "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Convex neural networks",
      "author" : [ "Y. Bengio", "N. Le Roux", "P. Vincent", "O. Delalleau" ],
      "venue" : "NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A decoupled approach to exemplar-based unsupervised learning",
      "author" : [ "S. Nowozin", "G. Bakir" ],
      "venue" : "Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Convex coding",
      "author" : [ "D. Bradley", "J. Bagnell" ],
      "venue" : "UAI,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A convex relaxation for weakly supervised classifiers",
      "author" : [ "A. Joulin", "F. Bach" ],
      "venue" : "Proc. ICML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficient optimization for discrimin",
      "author" : [ "A. Joulin", "F. Bach", "J. Ponce" ],
      "venue" : "latent class models. In NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convex relaxations of latent variable training",
      "author" : [ "Y. Guo", "D. Schuurmans" ],
      "venue" : "Proc. NIPS 20,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Transduction with matrix completion: Three birds with one stone",
      "author" : [ "A. Goldberg", "X. Zhu", "B. Recht", "J. Xu", "R. Nowak" ],
      "venue" : "NIPS 23,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "E. Candes", "X. Li", "Y. Ma", "J. Wright" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "Accelerated training for matrix-norm regularization: A boosting approach",
      "author" : [ "X. Zhang", "Y. Yu", "D. Schuurmans" ],
      "venue" : "Advances in Neural Information Processing Systems 25,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A method of moments for mixture models and hidden Markov models",
      "author" : [ "A. Anandkumar", "D. Hsu", "S. Kakade" ],
      "venue" : "Proc. Conference on Learning Theory,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions",
      "author" : [ "D. Hsu", "S. Kakade" ],
      "venue" : "Innovations in Theoretical Computer Science (ITCS),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Large margin classification in infinite neural networks",
      "author" : [ "Y. Cho", "L. Saul" ],
      "venue" : "Neural Comput., 22,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Connectionist learning of belief networks",
      "author" : [ "R. Neal" ],
      "venue" : "Artificial Intelligence, 56(1):71–113,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Some results on Tchebycheffian spline functions",
      "author" : [ "G. Kimeldorf", "G. Wahba" ],
      "venue" : "JMAA, 33:82–95,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "JMLR, pages 265–292,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Multilabel classification via calibrated label ranking",
      "author" : [ "J. Fuernkranz", "E. Huellermeier", "E. Mencia", "K. Brinker" ],
      "venue" : "Machine Learning, 73(2):133–153,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Adaptive large margin training for multilabel classification",
      "author" : [ "Y. Guo", "D. Schuurmans" ],
      "venue" : "AAAI,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Convex multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Mach. Learn., 73(3),",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Nimirovskii. Interior-Point Polynomial Algorithms in Convex Programming",
      "author" : [ "A.Y. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1994
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : "Cambridge U. Press,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundat. Trends in Mach. Learn., 3(1):1–123,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A hybrid algorithm for convex semidefinite optimization",
      "author" : [ "S. Laue" ],
      "venue" : "Proc. ICML,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Training a support vector machine in the primal",
      "author" : [ "O. Chapelle" ],
      "venue" : "Neural Comput., 19(5):1155–1178,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Transductive inference for text classification using support vector machines",
      "author" : [ "T. Joachims" ],
      "venue" : "ICML,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Large scale semi-supervised linear SVMs",
      "author" : [ "V. Sindhwani", "S. Keerthi" ],
      "venue" : "SIGIR,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction Deep learning has recently been enjoying a resurgence [1, 2] due to the discovery that stage-wise pre-training can significantly improve the results of classical training methods [3–5].",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction Deep learning has recently been enjoying a resurgence [1, 2] due to the discovery that stage-wise pre-training can significantly improve the results of classical training methods [3–5].",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : ", “generative”) settings, latent variable models have been used to express feature discovery problems such as dimensionality reduction [6], clustering [7], sparse coding [8], and independent components analysis [9].",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : ", “generative”) settings, latent variable models have been used to express feature discovery problems such as dimensionality reduction [6], clustering [7], sparse coding [8], and independent components analysis [9].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : ", “generative”) settings, latent variable models have been used to express feature discovery problems such as dimensionality reduction [6], clustering [7], sparse coding [8], and independent components analysis [9].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : ", “generative”) settings, latent variable models have been used to express feature discovery problems such as dimensionality reduction [6], clustering [7], sparse coding [8], and independent components analysis [9].",
      "startOffset" : 211,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "More recently, such latent variable models have been used to discover abstract features of visual data invariant to low level transformations [1, 2, 4].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "More recently, such latent variable models have been used to discover abstract features of visual data invariant to low level transformations [1, 2, 4].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 3,
      "context" : "More recently, such latent variable models have been used to discover abstract features of visual data invariant to low level transformations [1, 2, 4].",
      "startOffset" : 142,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "Some important examples of conditional latent learning approaches include those that seek an intervening lower dimensional representation [10] latent clustering [11], sparse feature representation [8] or invariant latent representation [1, 3, 4, 12] between inputs and outputs.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : "Some important examples of conditional latent learning approaches include those that seek an intervening lower dimensional representation [10] latent clustering [11], sparse feature representation [8] or invariant latent representation [1, 3, 4, 12] between inputs and outputs.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "Some important examples of conditional latent learning approaches include those that seek an intervening lower dimensional representation [10] latent clustering [11], sparse feature representation [8] or invariant latent representation [1, 3, 4, 12] between inputs and outputs.",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 0,
      "context" : "Some important examples of conditional latent learning approaches include those that seek an intervening lower dimensional representation [10] latent clustering [11], sparse feature representation [8] or invariant latent representation [1, 3, 4, 12] between inputs and outputs.",
      "startOffset" : 236,
      "endOffset" : 249
    }, {
      "referenceID" : 2,
      "context" : "Some important examples of conditional latent learning approaches include those that seek an intervening lower dimensional representation [10] latent clustering [11], sparse feature representation [8] or invariant latent representation [1, 3, 4, 12] between inputs and outputs.",
      "startOffset" : 236,
      "endOffset" : 249
    }, {
      "referenceID" : 3,
      "context" : "Some important examples of conditional latent learning approaches include those that seek an intervening lower dimensional representation [10] latent clustering [11], sparse feature representation [8] or invariant latent representation [1, 3, 4, 12] between inputs and outputs.",
      "startOffset" : 236,
      "endOffset" : 249
    }, {
      "referenceID" : 11,
      "context" : "Some important examples of conditional latent learning approaches include those that seek an intervening lower dimensional representation [10] latent clustering [11], sparse feature representation [8] or invariant latent representation [1, 3, 4, 12] between inputs and outputs.",
      "startOffset" : 236,
      "endOffset" : 249
    }, {
      "referenceID" : 12,
      "context" : "This remains true even when restricting attention to a single stage of stage-wise pre-training: simple models such as the two-layer auto-encoder or restricted Boltzmann machine (RBM) still pose intractable training problems, even within a single stage (in fact, simply computing the gradient of the RBM objective is currently believed to be intractable [13]).",
      "startOffset" : 353,
      "endOffset" : 357
    }, {
      "referenceID" : 13,
      "context" : "Even though global training formulations are not a universally accepted goal of deep learning research [14], there are several useful methodologies that have been been applied successfully to other latent variable models: boosting strategies [15–17], semidefinite relaxations [18–20], matrix factorization [21–23], and moment based estimators (i.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "Unlike [26], the latent kernel in this model is explicitly learned (nonparametrically).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 26,
      "context" : ") Similarly to RBMs and probabilistic networks (PFNs) [27] (but unlike auto-encoders and classical feed-forward networks), we will not assume is a deterministic output of the first layer; instead we will consider to be a variable whose value is the subject of inference during training.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 27,
      "context" : "negative log-priors under a Gaussian), which will provide a useful representer theorem [28] we exploit later.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "Here, we follow work on deep learning and sparse coding and assume that the latent variables are boolean, 2 {0, 1}h⇥1; an assumption that is also often made in auto-encoders [13], PFNs [27], and RBMs [5].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 26,
      "context" : "Here, we follow work on deep learning and sparse coding and assume that the latent variables are boolean, 2 {0, 1}h⇥1; an assumption that is also often made in auto-encoders [13], PFNs [27], and RBMs [5].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 4,
      "context" : "Here, we follow work on deep learning and sparse coding and assume that the latent variables are boolean, 2 {0, 1}h⇥1; an assumption that is also often made in auto-encoders [13], PFNs [27], and RBMs [5].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 10,
      "context" : "A boolean representation can capture structures that range from a single latent clustering [11, 19, 20], by imposing the assumption that 01 = 1, to a general sparse code, by imposing the assumption that 01 = k for some small k [1, 4, 13].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 18,
      "context" : "A boolean representation can capture structures that range from a single latent clustering [11, 19, 20], by imposing the assumption that 01 = 1, to a general sparse code, by imposing the assumption that 01 = k for some small k [1, 4, 13].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "A boolean representation can capture structures that range from a single latent clustering [11, 19, 20], by imposing the assumption that 01 = 1, to a general sparse code, by imposing the assumption that 01 = k for some small k [1, 4, 13].",
      "startOffset" : 91,
      "endOffset" : 103
    }, {
      "referenceID" : 0,
      "context" : "A boolean representation can capture structures that range from a single latent clustering [11, 19, 20], by imposing the assumption that 01 = 1, to a general sparse code, by imposing the assumption that 01 = k for some small k [1, 4, 13].",
      "startOffset" : 227,
      "endOffset" : 237
    }, {
      "referenceID" : 3,
      "context" : "A boolean representation can capture structures that range from a single latent clustering [11, 19, 20], by imposing the assumption that 01 = 1, to a general sparse code, by imposing the assumption that 01 = k for some small k [1, 4, 13].",
      "startOffset" : 227,
      "endOffset" : 237
    }, {
      "referenceID" : 12,
      "context" : "A boolean representation can capture structures that range from a single latent clustering [11, 19, 20], by imposing the assumption that 01 = 1, to a general sparse code, by imposing the assumption that 01 = k for some small k [1, 4, 13].",
      "startOffset" : 227,
      "endOffset" : 237
    }, {
      "referenceID" : 28,
      "context" : "Therefore, for multi-class classification, we will simply adopt the standard large-margin multi-class loss [29]: L",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 29,
      "context" : "Although several loss formulations exist for multi-label classification [30, 31], we adopt the following: L",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 30,
      "context" : "Although several loss formulations exist for multi-label classification [30, 31], we adopt the following: L",
      "startOffset" : 72,
      "endOffset" : 80
    }, {
      "referenceID" : 29,
      "context" : "Despite their simplicity, large-margin multi-label losses have proved to be highly successful in practice [30, 31].",
      "startOffset" : 106,
      "endOffset" : 114
    }, {
      "referenceID" : 30,
      "context" : "Despite their simplicity, large-margin multi-label losses have proved to be highly successful in practice [30, 31].",
      "startOffset" : 106,
      "endOffset" : 114
    }, {
      "referenceID" : 27,
      "context" : "2 in (1), the first equality in (6) follows from the representer theorem applied to kV k(2)F , which implies that the optimal V must be in the form of V = A 0 for some A 2 Rm⇥t [28]; and finally, (7) follows by the change of variable B = AN .",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 31,
      "context" : "Moreover, the term tr(BN†B0) is jointly convex in N and B since it is a perspective function [32], hence the objective in (5) is jointly convex.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "The second equality (10) follows from the representer theorem applied to kWk(2)F , which implies that the optimal W must be in the form of W = ̃ C  ̃ X 0 for some C 2 R ̃ t⇥ ̃ t (using the fact that  ̃ has full rank h) [28].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 31,
      "context" : "1 (D  ̃ K,  ̃ N) as defined in Lemma 3 below is also jointly convex in  ̃ N and D [32]; therefore the objective in (8) is jointly convex.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 31,
      "context" : ") Since dropping the rank constraint eliminates the constraints B 2 Im(N) and D 2 Im(N) in (16) when N 0 [32], we obtain the following relaxed problem, which is jointly convex in N , B and D:",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 32,
      "context" : "(20) 5 Efficient Training Approach Unfortunately, nonlinear semidefinite optimization problems in the form (20) are generally thought to be too expensive in practice despite their polynomial theoretical complexity [33, 34].",
      "startOffset" : 214,
      "endOffset" : 222
    }, {
      "referenceID" : 33,
      "context" : "(20) 5 Efficient Training Approach Unfortunately, nonlinear semidefinite optimization problems in the form (20) are generally thought to be too expensive in practice despite their polynomial theoretical complexity [33, 34].",
      "startOffset" : 214,
      "endOffset" : 222
    }, {
      "referenceID" : 34,
      "context" : "To cope with the constraints on N we adopt the alternating direction method of multipliers (ADMM) [35] as the main outer optimization procedure; see Algorithm 1.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 35,
      "context" : "To solve the problem in Step 3 we develop an efficient boosting procedure based on [36] that retains low rank iterates NT while avoiding the need to determine N† when computing G(N) and rG(N); see Algorithm 2.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 36,
      "context" : "2 tr(C 0NCK), which no longer involves N† but remains convex in C; this problem can be solved efficiently after a slight smoothing of the objective [37] (e.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 37,
      "context" : "We compared the proposed convex relaxation scheme (CVX2) against the following methods: simple alternating minimization of the same two-layer model (2) (LOC2), a one-layer linear SVM trained on the labeled data (SVM1), the transductive one-layer SVM methods of [38] (TSJ1) and [39] (TSS1), and the transductive latent clustering method of [18, 19] (TJB2), which is also a two-layer model.",
      "startOffset" : 261,
      "endOffset" : 265
    }, {
      "referenceID" : 38,
      "context" : "We compared the proposed convex relaxation scheme (CVX2) against the following methods: simple alternating minimization of the same two-layer model (2) (LOC2), a one-layer linear SVM trained on the labeled data (SVM1), the transductive one-layer SVM methods of [38] (TSJ1) and [39] (TSS1), and the transductive latent clustering method of [18, 19] (TJB2), which is also a two-layer model.",
      "startOffset" : 277,
      "endOffset" : 281
    }, {
      "referenceID" : 17,
      "context" : "We compared the proposed convex relaxation scheme (CVX2) against the following methods: simple alternating minimization of the same two-layer model (2) (LOC2), a one-layer linear SVM trained on the labeled data (SVM1), the transductive one-layer SVM methods of [38] (TSJ1) and [39] (TSS1), and the transductive latent clustering method of [18, 19] (TJB2), which is also a two-layer model.",
      "startOffset" : 339,
      "endOffset" : 347
    }, {
      "referenceID" : 18,
      "context" : "We compared the proposed convex relaxation scheme (CVX2) against the following methods: simple alternating minimization of the same two-layer model (2) (LOC2), a one-layer linear SVM trained on the labeled data (SVM1), the transductive one-layer SVM methods of [38] (TSJ1) and [39] (TSS1), and the transductive latent clustering method of [18, 19] (TJB2), which is also a two-layer model.",
      "startOffset" : 339,
      "endOffset" : 347
    } ],
    "year" : 2013,
    "abstractText" : "Latent variable prediction models, such as multi-layer networks, impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction. Unfortunately, such models are difficult to train because inference over latent variables must be performed concurrently with parameter optimization—creating a highly non-convex problem. Instead of proposing another local training method, we develop a convex relaxation of hidden-layer conditional models that admits global training. Our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer. The resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features, while improving training quality over local heuristics.",
    "creator" : null
  }
}