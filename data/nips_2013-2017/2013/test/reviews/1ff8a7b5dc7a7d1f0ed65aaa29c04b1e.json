{"title": "Training and Analysing Deep Recurrent Neural Networks", "abstract": "Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.", "id": "1ff8a7b5dc7a7d1f0ed65aaa29c04b1e", "authors": ["Michiel Hermans", "Benjamin Schrauwen"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The authors propose a new deep architecture, which combines the hierarchy of deep learning with time-series modelling known from HMMs or recurrent neural networks. The proposed training algorithm builds the network layer-by-layer using supervised (pre-)training a next-letter prediction objective. The experiments demonstrate that after training very large networks for about 10 days, the network performance on a Wikipedia dataset published by Hinton et al. improves over previous work. The authors then proceed to analyze and discuss details of how the network approaches its task. For example, long-term dependencies are modelled in higher layers, correspondence between opening and closing parenthesis are modelled as a \u201cpseudo-stable attractor-like state\u201d. \n\nThe paper is of good quality. It is well-written, experiments are set up well and the figures support the findings well. My main issue with the paper is the experiments. The comparison is done on one dataset only, which does not seem to have much work on it. Direct comparison is only with other RNN-related models, not with maybe more common hierarchical HMMs etc., which are able to solve similar tasks (Tab. 1 lists other approaches, but on a different corpus, which makes them incomparable). \n\nThe contribution of the paper is more in the analysis of /how/ the network operates (which is analyzed quite well) than in what it achieves, but this is not what the paper sets out for.  A new deep architecture is proposed, combining hierarchical features of deep learning with time-series modelling. The paper is well-written, the model is analyzed well, but results are somewhat inconclusive.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "REPLY TO REBUTTAL: \n- I mostly agree with the one task comment. \n- It would still be worth trying truncated backprop with e.g., 30 steps. It is less likely to learn to balance parentheses (but it would be good to verify that), but it may still achieve competitive entropies at a fraction of the cost. \n\n\nThe usefulness of deep recurrent neural networks is investigated. \nThe paper reaches interesting conclusions: when holding the number \nof parameter constant, deeper recurrent neural networks outperform \nstandard RNNs. Other interesting findings include an analysis \nthat shows the deeper networks to contain more \"long term information\", \nand the result that plain BPTT can train character-level RNNs \nto balance parentheses, where previously it was thought to be possible \nonly with HF. \n\nBut it claims to introduce the deep RNN, but those been used before; \nfor example, in http://www.cs.toronto.edu/~graves/icassp_2013.pdf, and likely \nmuch earlier than that. So the paper should not claim to have introduced this \narchitecture. It is OK to not introduce new architectures and to focus \non an analysis. \n\nThe analysis, while meaningful, would be even more interesting if it \nwere shown on several problems, perhaps something on speech. Otherwise \nthe findings may be an artifact of the problem. \n\nFinally, the HF experiments were likely too expensive, since the \ntruncated backprop approach (that was introduced in the 90) was \nsuccessfully used by Mikolov (see his PhD thesis) to train RNNs \nto be excellent language models. Thus it is likely that truncated \nBPTT would do good here too, and it would be nice to obtain a \nconfirmation of this fact.  This work presents an interesting analysis of deep RNNs. The two main results are: 1) deep RNNs outperform standard RNNs when the number of parameters is fixed on character-level language modelling; 2) the deeper layers exhibit more long-range structure; and 3) truncated BPTT can train character-level RNNs to balance parentheses.The results are interesting, but given that it is only an analysis, the paper would be a lot stronger if it had a similar analysis on some speech task (for example, to show that deeper RNNs do better, and to show that their deeper layers have more long range information)>", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I like the point about the arbitrariness of older data benefiting from potentially many more layer of processing compared to newer data. And building a deep RNN seems like a reasonable way to do better on shorter term stuff, in addition to possible other benefits. Another obvious thing to do would be to put extra layers of nonlinearities between each timestep. Have you tried this? \n\nHow important was the gradient normalizations and why does this \"avoid of the problem of bifurcations\"? There is a recent paper in ICML 2013 by researchers from U of Montreal that looks at gradient truncation and optimization in RNNs that that may be relevant here. Also, will normalizing the gradient in this way potentially mess up the \"averaging\" behavior of SGD? \n\nIn terms of previous results on these kinds of Wikipedia compression tasks, there is also some work by Mikolov et al. that you may want to compare to. \n\nOf the various experiments designed to examine the different roles played by each layer in terms of time-scale and perhaps \"abstraction\", the one I find most persuasive is the text generation one (as shown in Table 2). However, as pointed out by the authors themselves in the paragraph on line 241, it may be problematic to interpret the effect of this kind of \"brain surgery\" on RNNs due to the complex interdependencies that may have developed between the outputs of the various layers. \n\n\nFor me, the biggest missing piece of the empirical puzzle in this paper is the question of whether the higher layers are actually *better* at processing more abstract and long-term properties of the text than if the units were moved to the first layer. i.e. are they benefiting from the extra levels of processing that proceed them in a nontrivial way? That they happen to take on these seemingly more abstract and longer term roles after training is good but incomplete evidence that this is the case. \n\nI notice that the regular RNN seems to have less parameters in these experiments since 2*767^2 * 5 > 2119^2 (I'm counting both recurrent and inter-layer weight matrices, hence the multiplication by 2), so the comparison might be a bit unfair. A more convincing would be if the deeper RNNs did better than a standard RNN with the same number of parameters, or even better, the same number of *units*. Say a 2 layer DRNN versus such an RNN with the same number of units, where the different in the number of parameters wouldn't favor the RNN so much that the comparison would be rendered unfair. This would be strengthen the paper's claims a lot in my opinion. \n\nAlso, instead of Figure 2, it would be better to have seen how well various depths of DRNN did on the benchmarks when trained from scratch, possibly with wider layers than their deeper counterparts to make up for the difference the numbers of parameters and/or units. \n\n\n\nMinor: \n- You should define DRNN-AO and IO in the text somewhere and not just in Figure 1. This paper looks at a hybrid of deep and recurrent neural networks called DRNNs, which are like deep networks but with recurrent connections at each layer that operate through time. The authors show how such an architecture can work very well for text prediction/compression compared to existing approaches.A large bulk of this paper is devoted to a series of experiments designed argue that the higher level layers are processing/representing more abstract and long-term structures in the data. These experiments are pretty convincing but I have a few reservations, as elaborated on below in my full review, and would like to see a couple more experiments.I think that it is worthwhile to look at these kinds of deep temporal networks and to gain insight into how they function after training. The paper is also easy to read, and seems quite intellectually honest and thorough about its own potential problems and shortcomings, which is something I especially appreciate.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
