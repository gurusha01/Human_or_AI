{"title": "Unsupervised Structure Learning of Stochastic And-Or Grammars", "abstract": "Stochastic And-Or grammars compactly represent both compositionality and reconfigurability and have been used to model different types of data such as images and events. We present a unified formalization of stochastic And-Or grammars that is agnostic to the type of the data being modeled, and propose an unsupervised approach to learning the structures as well as the parameters of such grammars. Starting from a trivial initial grammar, our approach iteratively induces compositions and reconfigurations in a unified manner and optimizes the posterior probability of the grammar. In our empirical evaluation, we applied our approach to learning event grammars and image grammars and achieved comparable or better performance than previous approaches.", "id": "24681928425f5a9133504de568f5f6df", "authors": ["Kewei Tu", "Maria Pavlovskaia", "Song-Chun Zhu"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper presents a method for learning the structure of stochastic And-Or grammars. The paper suggests that this \"generalizes\" previous work on structure learning, but it's actually a special case, which makes the problem tractable. This is a reasonable point on its own, and the paper makes a nice contribution, so I wouldn't try to argue that the problem is more general than the structure learning problem faced in NLP. The basic algorithm is sensible and successful when compared against other methods for inducing grammars. \n\nThe experimental evaluation could be improved by adding more comparisons to other methods. In particular, the method of Stolcke and Omohondro (reference 14) seems like it could be applied to both tasks and would be a valuable comparison point. This is a solid paper presenting a method for learning the structure of a particular class of grammars.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a method for learning the structure as well as the parameters of stochastic AND-OR grammars. Such grammars contain AND rules and OR rules and can be used to represent several recursive phenomena, including natural language and even grammars. The authors present a nice method for unsupervised structure learning of these grammars by introducing new AND-OR fragments at consecutive steps, and measuring the likelihood and prior gains of their model. \n\nThe authors present experiments on two tasks: learning event grammars and learning image grammars. In both they achieve results that are competitive with prior art. \n\nI liked the overall paper as it seems to be a tractable way of learning stochastic grammars that can be modeled using AND and OR rules. My criticism of the paper stems from the following observations: \n\n1) The authors do not mention how tractable the learning algorithm is. Will it scale to thousands of datapoints? \n\n2) I would have liked to seen experiments on natural language sentences as natural language is the most obvious application of such grammars. Will it be even possible to learn using the presented methods on the Penn Treebank dataset for example, on which previous work has focused on (say, Klein and Manning)? This paper presents a way of estimating the structure and parameters of stochastic AND-OR grammars and presents nice results on two tasks; I would have liked to see more experiments, especially on natural language data.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a new approach to unsupervised learning of the \nstructure of AND-OR grammars. In contrast to previous approaches, the \npresent one induces unified AND-OR fragments, rather than searching \nseparately for AND and OR operations. The value of a proposed new \nAND-OR fragment can be computed efficiently using a set of sufficient \nstatistics. The approach is evaluated on several tasks, parsing \nevents and visual objects. \n\nThis is a generally good paper. The problem is important. The \napproach is novel and well-motivated. The algorithm appears \ntechnically sound, but I was not able to check the supplemental \nderivations. A few things weren't clear to me, which could just \nreflect my own limited time for reading, but I would urge the authors \nto mark these points more clearly: First, it wasn't clear whether the \n\"surrogate measure\" of the likelihood gain was an approximate or an \nexact notion. Second, it wasn't clear whether the approach could \nextend to learning fragments with -arity more than 2, or whether it \ncould only tractably learn grammars in \"Chomsky normal form\". \n\nMy main concerns with the paper focus on the experiments. I had \ntrouble understanding most aspects of them: exactly what was done, how \nthe examples were represented, and what was learned. I got the \nimpression that the paper's ideal audience is a reader who has \nfollowed, studied intimately and preferrably re-implemented all of the \nrecent visual-grammar papers coming out of S. C. Zhu's group. To such \na reader, the experimental section would probably have been more \nunderstandable and the results more useful. But for outsiders, it was \nless clear. I understand that the short NIPS format has its \nlimitations. But fortunately NIPS permits supplementary materials, \nand if the paper is accepted, I would urge the authors to include in \ntheir supplement many more details and concrete illustrations of their \nexperiments. \n\nI would especially like to have seen (or to see) examples of any \ninteresting structure discovered in the learned grammars. The paper \nhas a great set up about the importance of grammars and structure \nlearning, but then it is a bit of a letdown for these expectations to \nsee that the results are presented purely quantitatively. I agree \nwith the authors that those quantitative results are satisfactory on \ntheir own terms. But they are not very exciting or illuminating. I \nwould have found the paper more compelling if the authors could show \nthat interesting structure is learned, and that more interesting \nstructure is learned by this approach relative to competing \napproaches. This is an interesting paper on unsupervised learning of AND-OR grammars. While I liked it, I had trouble following the experiments and interpreting the results, not being familiar with a lot of prior work (mostly from Zhu's UCLA group) that it seemed to build on heavily. If accepted, the authors should include supplementary material with more details and illustrations of how the experiments worked and what interesting structure was learned.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
