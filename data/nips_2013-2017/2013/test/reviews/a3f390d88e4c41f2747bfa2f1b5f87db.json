{"title": "Bayesian optimization explains human active search", "abstract": "Many real-world problems have complicated objective functions. To optimize such functions, humans utilize sophisticated sequential decision-making strategies. Many optimization algorithms have also been developed for this same purpose, but how do they compare to humans in terms of both performance and behavior? We try to unravel the general underlying algorithm people may be using while searching for the maximum of an invisible 1D function. Subjects click on a blank screen and are shown the ordinate of the function at each clicked abscissa location. Their task is to find the function\u2019s maximum in as few clicks as possible. Subjects win if they get close enough to the maximum location. Analysis over 23 non-maths undergraduates, optimizing 25 functions from different families, shows that humans outperform 24 well-known optimization algorithms. Bayesian Optimization based on Gaussian Processes, which exploit all the x values tried and all the f(x) values obtained so far to pick the next x, predicts human performance and searched locations better. In 6 follow-up controlled experiments over 76 subjects, covering interpolation, extrapolation, and optimization tasks, we further confirm that Gaussian Processes provide a general and unified theoretical account to explain passive and active function learning and search in humans.", "id": "a3f390d88e4c41f2747bfa2f1b5f87db", "authors": ["Ali Borji", "Laurent Itti"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "In this well-written and dense paper the authors compare human active learning, optimization, and inter/extrapolation of 1D functions. Their two conclusions are that (a) people outperform modern optimization algorithms, and (b) their behavior is largely consistent with bayesian inference/search/prediction based on gaussian process assumptions about function smoothness. \n\nI have some concerns about point (a) (arguably less important than point b). I'm not convinced that the authors eliminated all the additional information people have. Critically, the range of the y axis remains an issue. The authors say that they jittered the y offset to alleviate the problem, but I think numbers here are important: what is the range of the function, relative to the displayed range (the remainder being the range of the jitter). Are these relative ranges constant across trials? In general, if a point appears at the bottom of the screen, subjects know that it must be far from the maximum, if it appears close to the top, it must be close to the maximum y value. None of the optimization algorithms know this, and thus can't use it to help search. \n\nWhile this paper has a lot of interesting measures, which each speak to different aspects of the (mis)match between algorithm and human behavior, I find this array of measures confusing, and largely unnecessary. How about a single measure as follows: \ngive the algorithm the first K points a subject observed. Have the algorithm pick point K+1. Compare the K+1th point for the algorithm and for the person. \nMaybe something else would work better. However, looking at figure 4, I am overwhelmed by the noisiness and apparent lack of diagnosticity of the assorted measures. I also find it hard to assess why agreement on the distribution of search step sizes is important. \n\nHuman consistency. Instead of the convoluted array of measures, I'd like to see some space dedicated to assessing across-subject agreement. The low standard deviations of crude performance across people are suggestive of consistency, but not enough. How high are the split-half correlations of performance over the different trials? How well does the distribution of clicks from one half of the subjects predict the distribution of clicks from the other half? This later measure I find particularly important to be able to compare the across-subject agreement to the agreement with various algorithms (this could be done for most of the marginal measures the authors use). \n\nI'm not sure why in Expt 6 the authors thought the first click should be to the maximum variance point. I think a complete decision theoretic analysis of the problem is important. Given that both clicks are \"equally important\" (ok, admittedly I don't know what the authors meant here), it's not so clear that the correct strategy is to gain maximal information on the first click. if the maximum is unambiguous, wouldn't that be the correct place to click? \n\nMinor points: I was a bit confused about the claim that subjects use the 'gradient': of course, they don't have access to the gradient, so they use something like the 'estimated smoothness' -- which is another further point to argue for some gaussian process like algorithm that does this sort of estimation. \n The paper has a rich set of (somewhat redundant) experiments on human optimization assessed via a large model bake-off. It would be improved by using fewer, more thoughtful measures by which to evaluate model fit and by more careful estimates of across-subject consistency.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Paper 68 \u2013 Bayesian optimization explains human active search \n\nThe authors explore different optimization strategies for 1-D continuous functions and their relationship to how people optimize the functions. They used a wide variety of continuous functions (with one exception): polynomial, exponential, trigonometric, and the Dirac function. They also explore how people interpolate and extrapolate noisy samples from a latent function (which has a long tradition in psychology under the name of function learning) and how people select an additional sample to observe under the task of interpolating or extrapolating. Over all, they found that Gaussian processes do a better job at describing human performance than any of the approx. 20 other tested optimization methods. \n\nBroadly, I really enjoyed this paper and I believe that it is a strong NIPS submission. They tackled interesting problems for machine learning-oriented cognitive scientists (although the stimuli may be a bit too abstract to be of interest for many cognitive psychologists): optimizing a function, and active interpolation and extrapolation. It is well written and besides for some minor quibbles, the presentation is very good. \n\nBesides for some minor points that I enumerate below, I have two main criticisms of the paper. \n\n1. In contrast to most function learning experiments, the experimental stimuli and procedure were maximally abstract (dots from a function and \u201cfind the maximum value of a function\u201d). This is not ecologically valid and thus, it is difficult to interpret what the mean for actual human function learning. The functions used in the experiments are probably not those functions that people expect in their everyday lives. Additionally, there typically is a strong effect of context on this sort of human learning and it is not clear how their results can be extended to account for context (which in my opinion would be a very interesting direction for them to go into for future research.). It is not enough to merely state \u201cnote that many real-world problems can be translated into our synthetic tasks here.\u201d There needs to be examples and a justification for why the synthetic tasks can be generalized to real-world examples. However, they got compelling results, where participants seem to be doing the task. \n\n2. I would have liked to see the authors integrate their experiments and results better with previous human function learning work. For example, I do not see why they did not choose to do their experiment using the standard cover story and stimuli of a function learning experiment. I understand why the authors did not just use a positive or negative linear function (although given f20 of experiment 1 is unbounded, it seems reasonable to include these too), but it seems including a piecewise linear function would have been a smart choice. As f16 is nearly a negative linear function and has been previously shown to be easy to learn, it is sensible that it would be one of the easiest function to maximize (and this is the sort of integration with previous work that I would have liked to have seen). Integrating with previous work would have made the paper stronger and the results more interpretable. \n\nMore minor concerns: \n\nPersonally, I would have preferred to see the previous psychological work to be earlier in the paper (e.g., in the introduction) rather than thrown in at the very end. \n\nFigure 1 is unreadable when printed out in B&W. \n\nThe stimuli generation in Experiment 1 seemed odd. Why not directly generate the functions via generating random coefficients rather than sample random points and fit a polynomial? If there was a good reason for this, explaining it in the paper would be helpful. \n\n\u201cModel-free results\u201d -> \u201cHuman results\u201d \n\nAt times, the framing gets into hairy theoretical territory as to the level of analysis of the paper, but at other times, it is fine. Examples of troubling language include \u201chumans might be following a GP,\u201d and \u201chumans may use GP.\u201d This ventures dangerously into the process level. The authors have not shown any evidence that the cognitive mechanism is \u201cGP-like\u201d whatever that might mean. An example of language at the appropriate level of analysis is \u201cResults of optimization tasks suggest that human clicks during search for a maximum of a 1D function can be predicted by a Gaussian processes [sic] model.\u201d (Perhaps \u201ca Gaussian processes model\u201d should be \u201ca Gaussian process model\u201d instead?). \n\nI think the authors should report the rank-order correlation of the DVs for people and each optimization method (where the correlation is over the different function types). \n\nI don\u2019t understand why different tolerance levels were used for different optimization algorithms, although I do not see how it would prejudice their results in favor of their conclusion. \n\nFigure 3 is extremely difficult to read when printed out in B&W. \n\nText is too small on Figure 4. \n\nResults of Experiment 2 & 3: \u201cInterestingly, on Deg5 functions, GP is closer to human clicks than the actual function (signed-rank test, p = 0.053) implying that GP captures clicks well in this case.\u201d Would this hold up after correcting for multiple comparisons? (you tested Deg2 and Deg3 as well). \n\nExperiments 2 & 4 are essentially function learning experiments. \n\nFigure 5 is pretty difficult to read when printed out in B&W. \n\nExperiment 5: Why were subjects always asked to guess the value of f(200) rather than a few different values? Also for the active extrapolation in this case, why not ask to find the value of the point right next to f(200) and guess basically that value? \nAnother control method for the interpolation and extrapolation methods that would have been nice to see how people and GPs relate to is estimating the parameters of a polynomial of degree up to m (and vary m). \n\nSome recent work on optimal foraging presented at last year\u2019s NIPS was Abbott, Austerweil, & Griffiths (2011) and a GP account might relate to the model used by Hills et al. (2011). \n A strong paper, although I would have liked to see more integration with previous work and more ecologically valid stimuli/cover story.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper compared human behavior on several active search tasks to a set of established active search algorithms, and found that the Bayesian optimization algorithms combined with GP prior are better in capturing human data. \n\nQuality: \n\nIn general, the paper is technically sound. Nevertheless, one major problem I have regarding their method is that the forms of the algorithms are not very proper (they are too simple) to solve the more complicated tasks as given to the humans. For example, the algorithms do not incorporate the step cost of querying a location whereas the humans do. The humans are essentially solving a composite sampling/stopping problem, whereas the algorithms separate sampling and stopping, and use seemingly arbitrary stopping rules that are not sensitive to the objective (i.e. higher hit with smaller function calls). This might not affect the general conclusion of this paper (that BO algorithms with GP can capture human search well), but it needs to be addressed to make the comparison between human and algorithms really fair. As I see it, Figure 4e actually implies that if the algorithms use a more sensible, joint policy for search and stopping, they could have achieved the same performance as people. I see that the authors partially addressed this issue in their discussion, but I think this issue needs more elaboration. \n\nAnother problem I have is that most of the BO algorithms use different methods than the non-Bayesian algorithms to decide where to query the next location. How do the authors separate the contribution of GP and the sampling policy? For example, GP-UCB looks pretty good. Is it because of GP learning or the UCB sampling policy? \n\nClarity: \n\nOverall, the text is pretty well-written, but the figures lack clear descriptions (see list of specific questions). \n\nOriginality: \n\nThis paper is original. \n\nSignificance: \n\nI have problems with the method as stated above, but I think the paper poses an important and interesting question that opens many future directions, along with unique, interesting experimental data. It also considered a pretty complete set of the most well-known search algorithms. \n\nMinor comments: \n\n- Page 3 1st paragraph: shouldn't it be 25 functions and 23 subjects? \n- Figure 3: I don't see MPI intersect the box (as claimed in the text).. \n- Page 4: Which second-order measure captures the order of the query sequence? I guess I'm not clear how \"mean shortest distance\" is calculated. \n- What are the histograms in several plots in Figure 6? \n\n\n A good paper. It is original and addresses important, interesting question. Some problems with the current form of their approach/method.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
