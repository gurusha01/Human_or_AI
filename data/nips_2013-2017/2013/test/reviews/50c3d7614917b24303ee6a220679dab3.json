{"title": "Approximate Inference in Continuous Determinantal Processes", "abstract": "Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient algorithm for sampling based on the eigendecomposition of the defining kernel matrix. Recently, there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required cannot be directly extended except in a few restricted cases. In this paper, we present efficient approximate DPP sampling schemes based on Nystrom and random Fourier feature approximations that apply to a wide range of kernel functions. We demonstrate the utility of continuous DPPs in repulsive mixture modeling applications and synthesizing human poses spanning activity spaces.", "id": "50c3d7614917b24303ee6a220679dab3", "authors": ["Raja Hafiz Affandi", "Emily Fox", "Ben Taskar"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "This paper extends determinantal point process (DPP) sampling schemes \nfrom discrete to continuous spaces by exploiting low-rank kernels. \nStandard kernels are approximated using the Nystroem or \nrandom Fourier features methods. The method is demonstrated on \nrepulsive mixture models and separated sampling. \n\nThe paper exploits a low rank kernel in order to derive an efficient \n\"dual sampler\", as described in Alg 1. The authors show in which cases \nof quality function and similarity kernel the necessary computations \ncan be carried out for the RFF and Nystroem approximations (Supp tab \n1). This is a non-trivial piece of work. \n\nIn sec 5 the continuous DPP is used as a prior on the means \nin a mixture model. However, note that the approximation \nderived before is not actually used; as the authors say \n\"since we are sampling just one item from the DPP there is a closed \nform and we do not rely on approximations.\" The results show that \nDPPs do (as expected) have a repulsive effect, as evinced by \nthe mixture weight entropies. \n\nAside: surely to qualify as a repulsive mixture, the closeness of the \nmeans should really be relative to their variances, but in supp mat p \n4 the variances are independent of the means ... \n\nIn sec 6 a DPP is used to sample a diverse set of poses from a given \ndensity. While the resulting poses are credibly diverse, I suspect \nthat visually similar results could have been obtained e.g. using hard \ncore processes (see p 1), or some scheme like farthest point \nclustering (Gonzales, 1985). \n\nQuality: As far as I can judge the material is technically correct. \n\nClarity: Generally clear. \n\nAs to Fig 1, I wanted more detail as to the defn of (15) -- for a \ngiven set X we can evaluate this, but how is the sum defined and \nevaluated? \n\nOriginality: as the authors make clear, this work extends earlier work \non low-rank approximations in discrete spaces. And as they mention the \nrepulsive prior construction has been used before [20,32]. \n\nSignificance: it is not very clear about the applicability of the main \nresult. Sec 5 does not actually use it, and sec 6 is just a \nvisualization which does not require a precise sampler. Of course it \nis possible that the result could be exploited in other interesting \nways. \n\n\nOther: \n\n* l 102. [11] are not the first to show that a matrix with the form B^T B \n(D << N) has a fast eigendecomposition, while the form of the citation \nimplies it is. For example this result was used in \nL. Sirovich and M. Kirby (1987). \"Low-dimensional procedure for the \ncharacterization of human faces\". Journal of the Optical Society of \nAmerica A 4 (3): 519\u2013524, but is surely \nin any good advanced linear algebra text, e.g. Wilkinson? \n\n* in eq 2 define the \\bar{ \\phi_n} notation (presumably as complex \nconjugate). \n\n* l 241-242 why not sigma^2 I instead of diag(sigma^2, \\ldots, sigma^2), \nand the same with rho^2? \n\n* l 304 -- why use only 100 test observations? Reduce variance \nby using many more! \n\n This paper extends determinantal point process (DPP) sampling schemesfrom discrete to continuous spaces by exploiting low-rank kernels.Standard kernels are approximated using the Nystroem orrandom Fourier features methods. This is a non-trivial piece of work.The method is demonstrated on repulsive mixture models and separated sampling.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The goal of this paper is to sample from a continuous \ndeterminantal point process, which until now has not been done \nfor general continuous kernels. The authors provide a general \n`meta'-algorithm that works in principal for general kernels --- \nthough ease-of-derivation may vary from kernel to kernel. \n\nPrevious work dealt with translation invariant kernels in compact domains. \nThe random Fourier features method proposed here extends the \napplicability of random Fourier features to general Euclidean spaces \n(where the kernel is translation invariant). And the proposed Nystrom \nmethod extends this even further to general kernels but relies \non good landmark selection (and generally performs better in low \ndimensional problems). \n\nWhile the RFF and Nystrom methods had been used in the discrete \nversion of the DPP, the main novelty of the current work is to \nfigure out how to do parts of the approximation `analytically' in some \nspecial cases. In particular, explicit sampling algorithms are worked \nout for Gaussian/Cauchy/Laplace kernel settings. \n\nOverall, I quite enjoyed reading the paper as it was well motivated \nand accessible. The NIPS margins have been violated, however, which made \nthe paper quite long. Otherwise, it makes for a nice contribution \nto the DPP thread in machine learning and I recommend acceptance. \n\nMinor comments/suggestions: \n- I would have appreciated more detail in the derivation of the Gibbs \nsampling step for the repulsive mixture. The authors seem to have \njust written down a posterior which takes the form of a DPP. It \nwould be easier to follow if the prior and likelihood for the \n`interesting' sampling step were written out explicitly. \n- In Algorithm 1 in the appendix, the authors use Y and script-Y, \nwhich are *not* the same, but somewhat confusing on a first read. \n- It would be interesting to see a derivation of the Nystrom method \non a non-translation invariant kernel (to show off what it can \nhandle that the Fourier based features cannot). \n\n This paper forms a nice extension to the exciting DPP (determinantal point processes) line of work in machine learning. It was also a pleasure to read. I hope it is accepted.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper describes a method for approximately sampling from certain \nparameteric forms of a continuous-state determinantal point process. It \nemploys an eigen-decomposition (of the dual kernel matrix instead of the \nkernel function) and depends on the kernel function having particular forms \nand decompositions. \n\nIn the experimental results, the authors demonstrate that when coupled with \na mixture density model, the resulting models give better test performance \nby promoting more diverse clusters. \n\nThe paper is presented well and the results are nice. The topic is of \ninterest to the ML community. \n\nA few minor comments: \n\nThe notation for n (above equation 16) is unclear. Although I think I \nunderstand the paper well, I am uncertain what n_j means. \n\nThe figure labels in the text do not align (for instance in Section 5, \nFigure 5 is referred to). \n The paper shows an interesting way of sampling from a continuous DPP. The results demonstrate clear utility. A good paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
