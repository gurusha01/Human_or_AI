{"title": "Visual Concept Learning: Combining Machine Vision and Bayesian Generalization on Concept Hierarchies", "abstract": "Learning a visual concept from a small number of positive examples is a significant challenge for machine learning algorithms. Current methods typically fail to find the appropriate level of generalization in a concept hierarchy for a given set of visual examples. Recent work in cognitive science on Bayesian models of generalization addresses this challenge, but prior results assumed that objects were perfectly  recognized. We present an algorithm for learning visual concepts directly from images, using  probabilistic predictions generated by visual classifiers as the input to a Bayesian generalization model. As no existing challenge data tests this paradigm, we collect and make available a new, large-scale dataset for visual concept learning using the ImageNet hierarchy as the source of possible concepts, with human annotators to provide ground truth labels as to whether a new image is an instance of each concept using a paradigm similar to that used in experiments studying word learning in children.  We compare the performance of our system to several baseline algorithms, and show a significant advantage results from combining visual classifiers with the ability to identify an appropriate level of abstraction using Bayesian generalization.", "id": "d2ed45a52bc0edfa11c2064e9edee8bf", "authors": ["Yangqing Jia", "Joshua T. Abbott", "Joseph L. Austerweil", "Tom Griffiths", "Trevor Darrell"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The authors present in this paper an algorithm for visual concept learning - given a few images of a concept (different concepts can be from different levels of abstraction) the algorithm learns to recognize other images from this concept. The algorithm combines object classification tools from machine vision and Bayesian generalization from cognitive science. The authors compare the generalization performances to other baseline methods and show there is a significant improvement when combining visual classifiers with the Bayesian cognitive model (as their algorithm does). \nThe authors also provide large-scale dataset for visual concept learning which is based on ImageNet images and human annotators. \n\nI find the problem of visual concept learning interesting and relevant to the NIPS community. \nThe paper's quality is high: it is well written and very interesting to read. \n\nThere are few points that I think the authors should address: \n- In section 3.2 the authors claim that their criteria (eq. 1) for choosing levels of nested concepts for generating their dataset result in sub-category, basic. super-basic and super-category levels in the taxonomy - I would like to see this claim validated. \nI also think that the authors should cite Rosch et al. 1976 when they talk about basic-level categories, it would be interesting to connect between ideas from Rosch work to this work. \n\n- Could there be some mix up in the equations of section 4.1? \nShould equation 3 be P(h|X) or as it written P(X|h)? \nI didn't understand why in equation 4 you have |h|^-N rather than |h|^-1 (maybe it should be P(X|h) and not P(x_i|h)) and also, it seems p(x_new|h) from line 226 doesn't fit equation 4 which is a bit confusing (I guess it should be p(x_new\\inC|h). \n\n- the description of the extension of [5] (HB) is unclear in my opinion. Shouldn't the subtree maintain the accuracy over the 5 example images rather than the query images (line 345)? Well written paper about and interesting subject (concept learning) that is addressed using both tools from machine vision and cognitive science. I think it should be accepted.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper studies an interesting problem of visual concept learning, particularly paying attention to determine the degree to which a concept should be generalized. For this purpose, the work first focuses on how to generate a large-scale dataset of concepts at various levels of abstraction from ILSVRC and collect human judgements on AMT, and then presents Bayesian concept learning algorithm to handle perceptual uncertainty. The problem of learning concepts of various levels of abstraction is interesting. However, this reviewer finds that the algorithm presentation is unclear and lacks intuitive explanations. In addition, the experimental evaluation is somewhat weak because it doesn\u2019t compare with strong baselines. \n\nMore detailed comments are listed as follows: \n\n1. In Eq. (4) and (5), is I(.) an indicator function? Please clarity. \n\n2. In Eq. (4), I(x_i \\subseteq h) implies that $h$ is a set and $x_i$ is also a set. This is confusing to this reviewer. Why is $x_i$ a set? In contrast, in Eq. (5), I(\\hat{y}_i \\in h) shows that $\\hat{y}_i$ is an item. Please clarify. \n\n3. In Eq. (5), it seems that the summation is only effective for the first term, i.e. A_{j\\hat{y}_i}. The remaining part is irrelevant to index $j$. Therefore the summation of A_{j\\hat{y}_i} becomes one column vector. Is this what you want to get? I cannot understand the motivation of this equation. \n\n4. It seems there is a typo in the last line of page 5. \u201c\u2026 the true leaf node is $j$ given the classifier output being $j$\u201d. Please check if you do want to put two $j$. \n\n5. In line 243, page 5, the sentence \u201cfor example, as the number of examples that are all Dalmatians increases, it becomes increasingly likely that the concept is just Dalmatians and not dogs in general even though both are logically possible, \u2026\u201d is true. But what does this mean to the hypothesis size? The hypothesis is quite confusing in the paper. It seems that the authors define the hypothesis size as the number of samples belonging to a hypothesis. How can this be defined in real world, given that the number of potential images belonging to any concept can be infinitely large? Is the number of samples in ImageNet a reasonable way to define the hypothesis size? \n\n6. Given so many unclear places in the algorithm presentation, it is quite hard to understand the proposed algorithm and know why it can really work, although the algorithm presentation is just of high level. \n\n7. The baseline methods are not sufficiently strong and convincing. For example: \na) For the na\u00efve vision approach, only GIST feature is used, whereas the proposed algorithm uses a overcomplete feature set of 160K dimensions. This is unfair to compare the two algorithms. \nb) The paper mentions the latest work of deep neural network [10, 9] in Section 4.3. Why not choose DNN or its variant as a baseline method? Simply because it requires more training data? If DNN could output the proposed algorithm given that the training data can be easily collected from ImageNet, I would doubt the value of this work. \n The paper studies an interesting problem of learning concepts at various levels of abstraction and the authors did put great efforts in constructing a large-scale concept learning dataset with human judgements collected on AMT. However, the algorithm presentation is unclear and lacks intuitive explanations. Also the experimental evaluation is unconvincing as some baseline method is implemented in a too simple way and some stronger baselines are not compared.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a method to embed classification confidence into concept learning, and provides a dataset to evaluate it. \n\n1. The proposed dataset is well constructed, and will make concrete contributions to category classification. \n\n2. The idea of adding classification confidence into the system makes perfect sense, and provides a reasonable and practical way to approximate non-perceptual word learning method. \n\n3. The paper devises a effective way to compute confusion matrix with limited data, facilitating the proposed algorithm. \n\n4. The evaluation is well conducted, where the proposed algorithm is compared to nearest neighbor approaches and ideal non-perceptual word learning, showing a good performance. \n\n----- \n\n1. There might be a few typos in the paper: \nIn equation 4, the right hand side should be |h|^(-1) instead of |h|^(-N). \nIn equation 5, should the last term be I(j \\in h) instead of I(y_i \\in h)? Because otherwise the 1/|h|*I term can be move outside summation, and then the summation of confusion matrix does not make sense. \nIn line 269, Aj,i should be the confusion of i and j, not j and j. \nThese typos can be quite misleading to readers, so the paper should be check again for the final version. \n\n2. The way classification confidence is added to the system is through confusion matrix in this paper. What if just use the confidence itself? For example, can the equation 5 be change to \\sum_j confidence(j | x_i) 1/|h| I(j \\in h)? How would this formulation work compared to the proposed method? \n Overall, this paper proposes a effective way of concept learning, and a dataset to test on. It would be great if the typos are fixed and justification are given on why the proposed method is the best way to utilize classification confidence.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
