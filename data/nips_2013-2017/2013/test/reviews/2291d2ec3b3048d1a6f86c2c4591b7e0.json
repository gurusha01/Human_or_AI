{"title": "Reshaping Visual Datasets for Domain Adaptation", "abstract": "In visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential. However, image data is difficult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.) We propose an approach to automatically discover latent domains in image or video datasets. Our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability. By maximum distinctiveness, we require the underlying distributions of the identified domains to be different from each other; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain. We devise a nonparametric representation and efficient optimization procedure for distinctiveness, which, when coupled with our learnability constraint, can successfully discover domains among both training and test data. We extensively evaluate our approach on object recognition and human activity recognition tasks.", "id": "2291d2ec3b3048d1a6f86c2c4591b7e0", "authors": ["Boqing Gong", "Kristen Grauman", "Fei Sha"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The goal of this work is to automatically discover latent domains in a training set, which is subsequently used in a domain adaptation framework to yield improved classification performance on a test set. The paper defines a function that measures the difference between two feature vectors over a specified kernel. The goal is to partition the data points into domains such that the function is maximized over the set of points across each pair of domains. The problem is formulated as an integer programming problem with two constraints: each point is assigned to exactly one domain and the distribution over class labels in each domain must match the input distribution over the entire point set. The problem is relaxed to a continuous optimization over a quadratic cost with linear constraints. Finally, the number of domains is found via cross validation. \n\nThe approach is evaluated over two datasets: static images of [2] and the IXMAS multi-view action dataset of [15]. A highlight is that improved performance is shown over the domain adaptation approach of [19]. \n\nPositives: The paper is well-written and as far as I'm aware the approach is novel (although I'm not an expert on domain adaptation). The performance gains over [19] is also appreciated. \n\nNegatives: At this point I slightly lean towards reject. I have two main concerns that I would like to see addressed in the rebuttal that may convince me to change my score: \n\n(i) The motivation for this paper is not clear to me. On line 77 the paper argues that \"simply clustering images by their appearance is prone to reshaping datasets into per-category domains\". First, what is the evidence for this claim? Second, how does the model formulation in Section 2 overcome this issue, i.e. how is not reshaping into per-category domains enforced? \n\n(ii) Somewhat related, on line 154, why is the second constraint (\"label prior constraint\") needed? I'm curious what would the performance be without this constraint. In fact, a baseline where the data is partitioned using k-means clustering or unsupervised object discovery (e.g. Sivic et al ICCV '05) over the appearance vectors should be shown. Also, what is the performance when the dataset is randomly partitioned into equal sets? \n\nSome additional comments: \n\n+ Line 82, \"maximally different in distribution from each other\": This is mentioned throughout the paper. It would be good to clarify what this means. Distribution over what? \n\n+ Line 166: Doesn't \\beta_{mk} as formulated already live on a simplex? \n\n+ Line 182: These seem to be different constraints than formulated before (starting on line 153), no? \n\n+ Line 215: What is the justification/proof for this bound? \n\n+ Line 289: Please provide more details on the use of the geodesic flow kernel [4]. Is this a reimplementation or was publicly available source code used? \n\n+ Lines 313/340: Please provide some insights into the differences in performance. I want to better understand *why* the proposed approach is performing better. It would be good to show systematic failures of the baselines that the proposed approach overcomes. \n\n+ Eq (1): M'_k => M_k' \n\n+ This citation may be good to include as well: \n\nUnbiased Look at Dataset Bias A. Torralba, A. Efros. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2011. The rebuttal addressed my concerns regarding the paper motivation and the label prior constraint. I lean slightly towards accept.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a new technique for domain adaptation for computer vision datasets, which typically present multiple aspects, e.g. viewpoint, illumination, level of clutter, compression, etc. Instead of simply equating domains with datasets, which ends up mixing those aspects together, the proposed technique automatically partitions the set of all images over all datasets into domains. The partitioner is driven by two principles: making separate domains in feature space, and making them so that a good discriminative classifier can be trained on each of them (to classify the original classes, not to separate the domains). This technique is more likely to partition according to the underlying aspects rather than datasets. \n\n\nOriginality and significance: \nThere is only little work on automatically defining domains, and this paper proposes a good idea towards automatically discovering useful domains, that will support training better classifiers for the original problem. \n\nOn the negative side, I am not fully convinced of the proposed optimization method (section 2), as it's not clear how closely it solves the original problem (2)+(3). Moreover, the two proposed driving criteria are not well integrated yet: the maximal learnability criterion is only used as a 'wrapper around' the maximal separability criterion, in order to determine the number of domains. Essentially, it acts as a post-hoc validation score deciding how good is the domain partitioning learned for a given number of domains, but the partitioning itself is made based on the separability criterion alone. An integrated process would instead produce the partitioning that maximises some goal function including both criteria directly. \n\nDespite these shortcomings, which might be due to the fact that this type of work is still at quite exploratory stages, I feel that the paper is a step in the right direction for the community and should be accepted. \n\n\nQuality and clarity: \nThe paper is well written, but lacks figures to illustrate the concepts presented. \n\n\nExperiments: \nOn the positive side, the experiments show a significant advantage in using the domains produced by the proposed method, over just using datasets as domains, and over the very recent domain discovery technique [19] (sec. 4.2). \n\nThe idea of including the test set in the 'reshaping' process is interesting, but not clearly presented (sec. 4.3). Also, this corresponds to an imputation setting (i.e. all test data is available at the same time), but this is not stated clearly. \n\nOn the negative side, the image descriptor used is very simple and outdated: just one bag-of-words of sparse SURF features for the entire image, and with just 800 codebook entries. This is really weak nowadays. I recommend the authors to use a spatial pyramid of bag-of-words, computed on _dense_ SURF features. Also, it is not clear what similarity measure is used to compare image descriptors, hopefully X^2 or an intersection kernel? The Euclidean distance is not suitable for comparing histograms. As a next step, a Fisher Vector representation could help further and finally place the image representation in this paper at the level of modern systems. The following papers might help the authors: Zhang IJCV 2006; Jurie CVPR 2005; Lazebnik CVPR 2006; Perronin ECCV 2010. \n Overall, the paper presents interesting novel ideas on an important problem and achieve good results. The paper can be improved in several way, especially in terms of the image representation used.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a convex framework for splitting the dataset(s) into K subsets such that each subset has similar semantic class distributions and as distinct from each other as possible. Distinction, named as maximum distinctiveness, is achieved by maximizing the pairwise mean differences of subsets in the RKHS induced by a selected kernel. Each of these subsets are named as latent domains. Identification of K (number of latent domains) is achieved by maximizing average learnability (how well a classifier can be learned) within each latent domain. \n\nEven though forcing the distribution of the classes within each domain to be as similar as possible prevents the clusters to be dominated by a single class each, it may also be a limiting assumption for latent domain discovery for certain tasks. For instance if we consider the poses as the latent domains for an animal classification task(e.g. horses and cows), then latent domains distribution within each class might not be similar. For instance, we observe both horses and cows in left-right standing pose, however horses are not pictured in a sitting pose often whereas the cows are. \n\nAlthough the discovery of latent domains is not new, the idea of controlling class label distributions for better identification of latent domains within a (relaxed) convex framework is new. Identifying the number of latent domains through checking the quality of classification within each latent domain is also a notable practice which makes sure that the latent domain has enough number of samples for each class in order to better generalize and learn discrimination between classes. \n\nIn several places the concepts of dataset, domain and latent domain is not clear and can easily be confused. These concepts should be clearly defined, and preferably with some supporting examples. Particularly the experiments section 4.2. needs clarification. As far as I understand, the words dataset and domain is used interchangeably since S_i is both named as datasets and source domains. Nevertheless the experimental setting and the concept definition should be clarified in section 4.2. Additionally, max_k r(U_k,B) is not defined but used in eq.7. \n\nThe experimental validation appears to be adequate. The results have a reasonable improvement above the baselines. However, why the current selection of source and target datasets to report on is preferred is not clear. For instance leave one dataset out adaptation might be a more reasonable evaluation. The qualitative results are helpful to see what the algorithm visually achieves. \n\non line 074, the problem(learning latent domains) being stated as an unsupervised learning problem might be misleading since the methods use semantic class labels. The discovery of latent domains via encouraging similar class distribution in each latent domain formulated in a (relaxed) convex framework is a notable technical contribution. However the some concepts in the paper and the experimental validation need to be clarified.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
