{"title": "Distributed $k$-means and $k$-median Clustering on General Topologies", "abstract": "", "id": "7f975a56c761db6506eca0b37ce6ec87", "authors": ["Maria-Florina F. Balcan", "Steven Ehrlich", "Yingyu Liang"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "The paper provides provably efficient algorithms for performing k-means and k-median clustering in the distributed setting. \n\nThe main focus of the paper is minimizing communication cost in the distributed network. Although, i am not very much aware of the literature, the paper seems to provide a very novel idea of distributed coresets that leads to clustering algorithms which provably improves the state of the art communication complexity significantly. \n\nExisting approaches only use the idea of approximating coresets by taking the union of local coresets. The key idea in this papers is a construction of distributed coresets which is different from taking the union and this can be of independent interest in itself. The theoretical ideas in are based on the notion of dimension of the function space and the sampling lemma. \n\nThe paper also provides a rigorous experimental evaluation and shows that the proposed algorithms outperforms existing state of art methods in terms of communication complexity supporting the theory. \n\nOne concern is that in experiments the accuracy of the clustering algorithms is not compared (say for the same communication budget).  A good paper about minimizing communication cost of distributed clustering, with solid theoretical & experimental support.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents new distributed algoritms for clustering methods that use centers as cluster representation. The new algorithms are shown to reduce the communication overhead. At the same communication cost, the new methods improve clustering objectives for both synthetic and three UCI datasets. \n\nThe paper is very hard-core. More explanation about the derivation clue and the values of the theorems would be helpful. \n\nI am a little bit skeptical on the approximation quality of coreset when the dimensionality increases. Datasets in the current experiments are most low-dimensional. \n\nThe title is too large. The work is actually on some particular clustering methods that uses centers and coresets. The word \"graphs\" is also vague, which does not reveal any highlights or distinguishable points. Improved methods for center-based distributed clustering are presented. The paper is very hard-core.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes new distributed algorithms for k-means and k-median methods. Its main idea is to construct a global coreset in a distributed fashion using cost information from local approximations. This paper is well written with solid content. It proposes distributed protocol for k-means/median clustering based on coreset construction, provides theoretical guarantee on the approximation and upper bounds on the communication cost, performs a couple of experiments to show the proposed approach outperforms the existing ones. \n\nHaving said, the paper also comes with several caveats. First of all, it is not very clear what is the main technical contribution of the paper, especially given the existing works in [5], [10] and [19]. Using coreset idea for approximating clustering algorithms is well studied (e.g., in [5],[10]), and a distributed clustering algorithm based on coreset is proposed in [19]. I clearly see that this paper contributes a distributed method for constructing coreset independent of the network topology, and develops a better algorithm than that in [19], by reducing the communication cost by a factor of sqrt(n). However, the distributed coreset construction in Algorithm 1 and the related bounds seems to be a straightforward application of results in [5]. \n\nIt is nice to see the theoretical analysis on the algorithm and the upper bounds on coreset size and communication cost. However, the upper bounds seem to be too loose to use in practical applications. Given a dataset and its location over a distributed network, how could you determine the size of coreset? The bound for achieving this goal for k-means is along the line of O(kd/(e^4)), which could be huge even for moderate large k, d and relative small e. For example, for k=10, d = 100, e = 0.1, the size of coreset predicted by your bound is 10 million, which could be way more larger than the actual data size. \n\nIn the experiments, the number of distributed sites is small (e.g., 10 for Pendigits, 25 for ColorHistogram). Have you tried experiments with larger number of distributed sites? In addition, the three datasets used are relative-low dimensional. Would this kind of approach work for high-dimensional sparse data? This paper is well-written with solid content, but also comes with several caveats, including unclear technical contribution and loose upper bounds.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
