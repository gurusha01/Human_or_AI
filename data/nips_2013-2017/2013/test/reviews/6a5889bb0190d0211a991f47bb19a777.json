{"title": "(More) Efficient Reinforcement Learning via Posterior Sampling", "abstract": "", "id": "6a5889bb0190d0211a991f47bb19a777", "authors": ["Ian Osband", "Daniel Russo", "Benjamin Van Roy"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "Title: (More) Efficient Reinforcement Learning via Posterior Sampling \n\nSummary: The paper proposes a Thompson sampling-type approach for episodic finite-horizon reinforcement learning, called posterior sampling for reinforcement learning. A regret analysis showing state of the art performance is provided, as well as simulations showing better empirical performance on some toy problems than other algorithms with similar regret bounds. \n\nComments: \n\nThe paper is clear and very well written. The contribution is also clearly identified: showing that is possible to use a Thompson sampling-type approach in a RL setting, and also theoretically analyse that is can be efficient. \n\nRegarding the regret bound, the authors say that the regret bound of PSRL is close to state of the art. It would be nice to recall in this paper what are such state-of-the art values. \n\nThis is a rather frequent question in the field of multi-armed bandit algorithms, but how large is the constant in practice (in the O())?; \n\nComparisons could be made with the algorithm KL-UCRL (Optimism in reinforcement learning and Kullback-Leibler divergence - S. Filippi, O. Capp\u00e9, A. Garivier), an algorithm based on the optimism principle, which was proven to have better empirical performance than UCRL2 on the riverSwim environment, with a regret in $\\tilde O(|S|\\sqrt{|A&T})$ as well; \n\nMinor comments: \n- l.99: it seems that the $s_1$ should be replaced by a $s_i$; \n- l.192: \"we how we can\" -> how we can; \n- l.434: \"markov\" -> Markov; \n- references should be uniformed (Firstname Name, F. Name, etc) This is a technically strong, well-written and interesting paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: this paper gives a regret analysis for an episodic reinforcement learning motivated by posterior sampling (as known as Thompson sampling). The idea is to sample, from the posterior, an MDP model at the beginning of each episode, follow this MDP's optimal policy in that episode, update the model posterior, and then repeat. In the past, despite good empirical performance, this algorithm (called PSRL) remains a heuristic approach without strong theoretical guarantees. This paper provides the first regret analysis, with a bound of O(\\tau * S * \\sqrt{A * T}), where H is the length of the episode and T is the total number of stpes. Numerical experiments on very simple MDPs show PSRL significantly outperform the other algorithm UCRL2 that enjoys a similar regret bound. \n\n* Quality: this paper makes an interesting theoretical contribution to RL in finite-state, finite-action, episodic MDPs. Building on recent advances in the multi-armed bandit literature, it is a successful step towards understanding finite-sample performance of Bayesian-style RL algorithms. \n\n* Clarity: the paper is written very clearly. Despite the heavy technical details, the basic ideas are explained well and intuitively. \n\n* Originality: The algorithm PSRL is not new; instead, the novelty in the paper is in its (first) regret analysis. The key in the analysis is Lemma 2, which equates the *expected* regret with the regret in the sampled MDP (assuming the MDP is sampled from the posterior). A similar observation has been used to proved expected regret bounds of posterior sampling in multi-armed bandits [15]. While the rest of the analysis seems pretty standard in reinforcement learning, the application of Lemma 2 in this kind of analysis, and especially for expected regret, appears sufficiently novel. \n\n* Significance: Posterior sampling has been very successful in multi-armed bandits that raised quite a lot of interests in its theoretical study recently. This paper is the first that extends analytic ideas to MDPs and shows strong regret bounds of posterior sampling. Not only is the result the first of its kind, it may generate further interests in the RL community to investigate a new class of algorithms based on posterior sampling. \n\nDetailed comments: \n\n* While the analysis and bounds are very interesting, the paper should make it clear that the nature of the bounds here is fundamentally different from that in the literature. In particular, existing bounds are worst-case bounds, while the bounds here are average-case (averaged by the prior/posterior). It is the averaging that makes the key Lemma 2 possible. For the same reason, it seems the paper's *expected* bounds may be weaker than worst-case bounds. I hope the authors can comment on these points in the paper. \n\n* In the paper, UCRL2 is the only algorithm to compare against PSRL. Justifications are needed for this choice. PSRL, in the current description in the paper, applies to episodic tasks and aims to optimize undiscounted, finite-horizon total reward. In contrast, UCRL2 applies to continuing tasks and aims to optimize average reward. As a consequence, their regret bounds are not directly comparable: UCRL2 depends on a diameter parameter, while PSRL has the horizon length in the bound. There may be a way to relate them, but it is not obvious how. \n\n* Instead of UCRL2, another piece of work may be more relevant: \nClaude-Nicolas Fiechter: Efficient Reinforcement Learning. COLT 1994: 88-97 \nAlthough the Fiechter paper considers the PAC framework, it considers finite-horizon problems, so seems relevant to the present work. \n\n* The simple experiments show PSRL is empirically much better than UCRL2. It does not strike me as a surprise, given the very conservative nature of UCRL2. And, because of the gap the kinds of problems the two algorithms are designed for (see a related comment above), the empirical comparison carries even less information. \n\n* Line 255, 'depends only the' --> 'depends only on the' \n\n* Line 278, please be specific what the 'high probability' is. \n\n* Line 411, is it too restrictive to assume that the optimal average reward is uncorrelated with episodic length? \n An interesting analysis for an important algorithm, although the regret bounds are not directly comparable to previous results. The paper is almost purely theoretical, with limited experiments in simple MDPs.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper provides the first regret bounds for a reinforcement learning algorithm that 1) maintains a posterior over the MDP, 2) samples one MDP from the prior, and 3) follows the policy that is optimal for that one MDP. This kind of \"Posterior-Sampling Reinforcement Learning\" (PSRL) algorithm has been proposed before, but without regret bounds. All algorithms with regret bounds are based on the principle of optimism in the face of uncertainty. This is the first regret bound analysis of an algorithm not based on this principle. This algorithm's bound is not quite as tight as those based on this principle, but in practice in typical cases it is substantially more exploitive, and appears to be much more efficient overall. \n\nIf it is correct that this is the first regret result for such an algorithm (not based on optimism), and if this result is correct, then the paper should be accepted. \n\nThe paper is written with admirable clarity. Nevertheless, I did not understand the proof in detail, so I may have missed errors. \n\nFor simplicity, the work assumes an episodic formulation with fixed-length episodes. This is unrealistic, but is ok for a result that breaks new ground such as this one. This and other aspects of the algorithm that are undesirable in practice can (probably) be removed in future work if the main result shown here holds up. One related idea is discussed in a later section of the paper. This is good, but it could be omitted if it made room to include what is asked for in the paragraph below. \n\nThe paper claims that the regret bound for its algorithm is close to the state of the art, but does not state what the state of the art is. It should do this, both as part of making its claim clear and allowing the reader to judge that the new algorithm's bound is in fact close. The paper should discuss the sense in which the new algorithm's bound is close. \n\nPlease don't include citations as parts of sentences. \n The first regret bound, and promising empirical results, for a reinforcement learning algorithm based on posterior sampling rather than optimism in the face of uncertainty.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
