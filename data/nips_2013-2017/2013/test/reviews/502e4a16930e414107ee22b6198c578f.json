{"title": "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration", "abstract": "", "id": "502e4a16930e414107ee22b6198c578f", "authors": ["Bruno Scherrer"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "Summary: \nThe paper considers the complexity of policy iteration (PI) for finite MDPs \nwith n states and m actions per state in a number of settings. \nThe following results are derived: \nFor Howard's PI, a previous bound is improved by removing a log(n) factor. \nFor Simplex PI, a previous bound is improved by halving its multiplicative constant. \nWhen the MDP is deterministic, a previous bound for Simplex PI \nis improved by decreasing the power of \ndependence on n by one. \nThis result is generalized to stochastic MDPs under additional restrictions on the MDP. \n\nOriginality and significance: \nThe complexity of PI is of major interest to the RL community. Even though the finite \nsetting is not practical, it is important to have a good understanding of this \nsimple case before moving to more complicated settings. This paper essentially picks \nup the previous line of research that started with a paper by Yinyu Ye in 2011 and \ncontinued by Post and Ye, as well as by Hansen, Miltersen and Zwick. \nSome of the improvements are marginal and the core ideas of the proofs go back to the \n2011 work of Ye. However, the proofs are much more streamlined and are way more \naccessible to RL researchers than those of Ye (who prefers to use the language \nof linear programming). The extension of strong polynomial results to stochastic MDPs \nunder structural assumptions are the most important novel parts of the paper. \n\nClarity: \nThe paper is relatively well-written, though the authors should check it carefully \nas it contains a few typos (lines 285, 292). Also, at times fluency could be improved \n(e.g., \"However, on the other hand\" on line 303 is awkward). \n\n\nSoundness: \nI believe that the results are sound (I did not have time to check all the details). \n\nMinor issues: \n- line 367: specify the lemma used \n The mildly improves previous results on the time complexity of PI,while making the proofs more accessible to RL researchers.Perhaps more importantly, the paper shows that the class of MDPsthat simplex PI is stronglypolynomial for is much larger than what was known beforehand.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This is a nice paper that tightens previous bounds on policy iteration, a problem on which recent progress has been made after decades of stagnation. The results are nice, though not shockingly impressive in themselves. Importantly, however, the paper provides short proofs of some key results and good, short summaries with real insight in cases where the proofs aren't short. The paper is extremely well written. \n\nMinor comments: \n\nLine 97.5: at least on state -> at least one state \n\nline 106: all iteration k -> all iterations k \n\nline 114.5: convergen often occur -> convergence often occurs \n\nline 203: on path -> on paths \n\n Modest improvement on previous policy iteration results. Very nicely written paper, a pleasure to read.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper provides a theoretical analysis of two variants of policy iteration. \nThe analysis improves the known worst-case iteration complexity by O(log(n)) in \none case, and by the constant factor 2 in the other. They generalize their \nanalysis for one variant run on deterministic MDPs, improving its iteration \ncomplexity by O(n). \n\nI cannot recommend this paper be accepted in its current form. The exposition \nis extremely rushed, and the improved bounds in themselves are not impressive. \nIt is possible that the proof techniques provide some scientific value (as the \nauthors claim), but this is incredibly difficult to distill from the paper. \n\nThe proof techniques that improve the first two bounds follow prior work \nclosely. The proofs in the deterministic MDP setting appear more novel, but \n(a) key terms, like transient and recurrent, are not defined and (b) the \nauthors state that the assumptions required for their proofs are overly strong. \nThat is, their correctness and applicability cannot be evaluated. \n\nThe authors need to better motivate this work and more clearly \ndescribe/highlight their contributions. \n\nMinor comments: \n- please spell check \n- line 85, \\pi' is not used inside the max \n- 115, convergen \n- 125, observtation \n- 134, mentionned \n- footnote 6, not immediate put in appendix \n- 177, Sherman-Morrisson \n- 180, this statement is not immediate, though I believe it \n- 182, explaination for conjectures? \n- 233, transient and recurrent need to be defined \n- 277, appdx \n- 292, mentionned and developped \n\nAfter viewing author feedback and discussions with other reviewers, it is apparent the initial quality score was overly harsh. I suggest drawing more attention to contribution contained in the proofs themselves in the next revision to aid less astute readers like myself. I recommend this paper be rejected due as it does not appear to provide significantly better bounds, or any obviously insightful proof techniques.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents bounds on the complexity of policy iteration for two versions of the algorithm: one which changes the policy at all states where there is an action better than the current policy (which the author call Howard PI), and one which only changes policy at states with a maximal advantage (which corresponds to running Simplex in the linear programming version of the algorithm). The paper re-derives some existing bounds in a simpler way and also improves some existing bounds. \n\nThe paper presents an interesting contribution and the proofs seem correct. The novelty is not very high, as the paper builds on existing results, but nonetheless, there are improvements on these results. The writing in the paper is clear. \n\nSmall comments: \n- line o43: When at state -> At state \n- The definition of advantage (line 085) is missing a \\prime, otherwise it's trivially - line 097: on state -> one state \n- line 112: and that -> that \n- line 195: the the \n- line 217: Material and \n- line 367: \"this lemma\" - which lemma do you mean? \n- In the proof of Lemma 3, line 72, you have a \\max_{s,\\bar{\\pi}}. Here, the order in which you take the max matters, so you should write \\max_s \\max_{\\bar{\\pi}} to clarify (I am assuming this is what is done). \n- It would be very useful to have some examples of types of MDPs in which Corollary 2 holds. The paper presents improvements on the current bounds of policy iteration, making progress on existing bounds. It is a nice contribution, though it does not depart in a major way from existing proof techniques.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
