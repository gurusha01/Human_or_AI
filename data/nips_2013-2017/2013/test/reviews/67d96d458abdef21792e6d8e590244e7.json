{"title": "Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty", "abstract": "Typical blur from camera shake often deviates from the standard uniform convolutional assumption, in part because of problematic rotations which create greater blurring away from some unknown center point.  Consequently, successful blind deconvolution for removing shake artifacts requires the estimation of a spatially-varying or non-uniform blur operator.  Using ideas from Bayesian inference and convex analysis, this paper derives a non-uniform blind deblurring algorithm with several desirable, yet previously-unexplored attributes.  The underlying objective function includes a spatially-adaptive penalty that couples the latent sharp image, non-uniform blur operator, and noise level together.  This coupling allows the penalty to automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted.  Remaining regions with modest blur and revealing edges therefore dominate the overall estimation process without explicitly incorporating structure-selection heuristics.  The algorithm can be implemented using an optimization strategy  that is virtually parameter free and simpler than existing methods.  Detailed theoretical analysis and empirical validation on real images serve to validate the proposed method.", "id": "67d96d458abdef21792e6d8e590244e7", "authors": ["Haichao Zhang", "David Wipf"], "conference": "NIPS2013", "accepted": true, "reviews": [{"comments": "In this paper, the authors develop a novel blind deconvolution algorithm that provides very good results for both uniform and non-uniform blind deconvolution problems. They provide a compelling analysis of the reasons why their method works well. A number of experimental results are provided against the state-of-the art algorithms for both uniform and non-uniform cases. The paper is clear and elegantly written. I believe this is a significant contribution to the literature on blind deconvolution. \n\nIt would be interesting if the authors discussed how their new technique relates to the variational deblurring algorithm of Levin et. al. [15], which also uses marginalization over x to determine the blur kernel. In fact, the authors' new formulation starts with an even more simplified prior than the heavy tailed one used by Levin et. al. However, instead of fixed variances as used in Levin et. al., the authors make these variances spatially varying hyper-parameters...is this the crucial difference? A novel and interesting new algorithm for blind deconvolution is presented.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper describes a method for non-uniform (space-varying) blind deconvolution. \nThe method is strongly based on ref [26]. Indeed the authors propose to solve \neq. 9, which is essentially the same as in eq. 26 and 27 in ref [26], by using the \nupper bound derived in eq 7, which is one of the novel bits. \nThe analysis of the effects of such prior, however, is carried out on the original cost. \nA summary of the analysis is that the proposed prior adaptively regulates \nthe concavity/convexity of the image-blur prior depending on the \nmagnitude of the local image gradients and the L2 norm of the blur. \nIf the blur is given at any pixel, the theory is that a high L2 norm (up to 1) can only be achieved \nwith a Dirac delta and hence the image is sharp to start with. Here a very concave prior which \nstrongly induces gradient sparsity is welcome. Vice versa, where the blur tends to be an averaging \nkernel, the prior tends to be much less concave to allow for less sensitivity to fine details and more \nsensitivity to coarse details. \n\nQuality \n-- \nThe algorithm and the underlining theory are interesting and compelling. \nAlthough a good portion of the paper is devoted to explaining the effect of the \nproposed prior, there are several points left unexamined. \n1 - The analysis is carried out on the original cost 9 but \nthe effects of the bound 7, which is the novelty in the algorithm, are not discussed. \n2- It would be useful to see how the analysis changes under the uniform case (that would \ncomplement well with the experiments in Fig 2). \n3- In light of the results in ref [16] it would be useful to see \na discussion of how this prior addresses the limitations of the classic \npriors; specifically, it would be interesting to see that the blurry image & no blur \nsolution is no longer a global minimum, or, even better, no longer a local minimum. \n\n\nClarity \n-- \nOverall the authors do a good job with explaining the approach. \nHowever, there is somehow a jump between the paragraphs in sec 3. \nFor example, the connection between eq 7 (the approx) and cost 9 \nis not made clear. Indeed eq. 7 is never used later on. One can \nthen find it only in the supplementary material. \nI recommend to revise/rewrite this section. \nThe use of w for the weights and \\hat w for the blur kernels is \nquite confusing. Despite the relation in eq 8, their meaning is \nvery different. Please consider changing one of the two (e.g. h_i for \\hat w_i \nwould be much more meaningful). \nEq 6 might have some typos: check that T at the exponent is not -1 and that \nyou are not missing a product over i under the integral. \n\n\n\nOriginality \n-- \nThe originality is limited by all the body of work by ref [26]. \nIt probably would have been very useful to discuss the differences \nwith respect to [26]. In my opinion the originality is limited to a bound (eq7) \nand an explanation of how this prior operates (via Theorem 1). \nThe explanation however, is quite approximate due to the \ncomplexity of the prior. \n\n\nSignificance \n-- \nThe study and development of novel priors for blind deconvolution is quite important \nand this paper further develops the new direction introduced by ref [26]. \nMoreover, given the experimental performance of this algorithm, this approach \ndeserves attention. \n\n\n\n Overall this paper introduces some novel elements: a practical bound for a cost function that simplifies the implementation, andanalysis that explains the general behavior of the adaptive prior. The performance is quite good.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper addresses the problem of single image blind deconvolution with non-uniform blur caused by camera shake. The authors propose a two step procedure: first to estimate the motion blur and second the recovery of the sought-after latent image through non-blind deconvolution. This procedure is common practice in blind deconvolution. The main contribution made here is the derivation of a Bayesian inference strategy for motion blur estimation. In a number of experiments the authors demonstrate the validity of their approach and compare its performance against other state-of-the-art methods. The mathematical derivation appears sound and the proposed scheme is claimed to exhibit a number of favourable properties. Although these details are discussed in some detail in the second half of Section 4, no empirical evidence is given, which would strengthen the argumentation and verify the claims made. The authors also miss to make connections to other recent approaches that advocate a similar line of reasoning, in particular [15] and the missing reference: \n\nS.D.Babacan, et al., \"Bayesian Blind Deconvolution with General Sparse Image Priors\", ECCV 2012 \n\nIn addition, the authors miss another relevant reference, namely \n\nR.Kohler, et al., \"Recording and playback of camera shake: Benchmarking blind deconvolution with a real-world database.\" ECCV 2012. \n\nthat would allow them to benchmark their approach against other state-of-the-art single image blind deblurring methods for both uniform and non-uniform blur. Besides, a number of questions remain unanswered (see below) in the presented exposition, that lower the overall quality of the paper. \n\nThe paper is clearly structured and well written. Section 4 would benefit from a division in smaller subsections as it feels a bit lengthy for my tasting. Unfortunately, some important details are missing (e.g. which algorithm is used for the final non-blind deconvolution, see also comments below), which impair the overall clarity of the paper. \n\nThe theoretical foundation of the proposed approach build on previous work [19,26], however its application to the problem of single image blind deconvolution seems novel. \n\nAlthough the presented algorithm seems to deliver comparable results to state-of-the-art algorithms, the provided insights are very limited. It remains unclear in which situations/cases the proposed method works better and why. It also fails to make connections to other recent relevant work (see above) and place its contribution into context, which lowers its significance considerably. \n\nFurther comments: \n* What method is employed for the final non-blind deconvolution? \n* It is stated, that the approach is \"parameter-free\". Is this also true for the kernel size? \n* It is mentioned in Section 4, that initialization with a large $\\lambda$ and its subsequent evolution render the deconvolution process more stable. It would be interesting to see a plot that shows the evolution of $\\lambda$ in the course of the motion blur estimation process. \n* What basis (i.e. $B$ in Eq.(8)) is chosen for the experiments with non-uniform blur? Is $w^\\bar_i$ evaluated for every pixel? If not, how are the evaluation locations chosen? \n* From the main paper it is not clear that a coarse-to-fine scheme is employed. Great if this could be made clearer. \n* What are typical run-times for the proposed method? \n* Large blur is challenging even for state-of-the-art methods. How does performance scale with kernels size? \n* The ringing artifacts in Whyte's result in Fig.4 are stemming primarily from the final non-blind deconvolution (see his talk slides). It would be more fair to use the one published on the accompanying project webpage (http://www.di.ens.fr/willow/research/deblurring/). \n* The correct reference for the efficient filter flow framework mentioned in Section 2 is \nM.Hirsch, et al., \"Efficient filter flow for space-variant multiframe blind deconvolution.\" CVPR 2010. \n* What are the limitations of the proposed approach? \n The paper presents an interesting Bayesian approach to motion blur estimation. Unfortunately, it fails to make connections to relevant previous work and lacks a quantitative evaluation for the case of camera shake removal with non-uniform blur, which lower its significance and value to the community considerably.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
