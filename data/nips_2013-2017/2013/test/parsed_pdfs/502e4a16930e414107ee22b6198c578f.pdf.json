{
  "name" : "502e4a16930e414107ee22b6198c578f.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Improved and Generalized Upper Bounds on the Complexity of Policy Iteration",
    "authors" : [ "Bruno Scherrer" ],
    "emails" : [ "bruno.scherrer@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Ï 1 1≠“ log 1 1 1≠“ 2Ì = O 1 nm 1≠“ log 1 1 1≠“ 22 iterations,\nimproving by a factor O(log n) a result by [3], while Simplex-PI terminates after at most n2(m ≠ 1) 1 1 + 2 1≠“ log 1 1 1≠“ 22 = O 1 n2m 1≠“ log 1 1 1≠“ 22\niterations, improving by a factor O(log n) a result by [11]. Under some structural assumptions of the MDP, we then consider bounds that are independent of the discount factor “: given a measure of the maximal transient time ·t and the maximal time ·r to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most n2(m≠ 1) (Á·r log(n·r)Ë + Á·r log(n·t)Ë) # (m ≠ 1)Án·t log(n·t)Ë + Án·t log(n2·t)Ë $ = Õ ! n3m2·t·r \" iterations. This generalizes a recent result for deterministic MDPs by [8], in which ·t Æ n and ·r Æ n. We explain why similar results seem hard to derive for Howard’s PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively states that are transient and recurrent for all policies, we show that Howard’s PI terminates after at most n(m ≠ 1) (Á·t log n·tË + Á·r log n·rË) = Õ(nm(·t + ·r)) iterations while Simplex-PI terminates after n(m ≠ 1) (Án·t log n·tË + Á·r log n·rË) = Õ(n2m(·t + ·r)) iterations."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider a discrete-time dynamic system whose state transition depends on a control. We assume that there is a state space X of finite size n. At state i œ {1, .., n}, the control is chosen from a control space A of finite size1 m. The control a œ A specifies the transition probability pij(a) = P(it+1 = j|it = i, at = a) to the next state j. At each transition, the system is given a reward r(i, a, j) where r is the instantaneous reward function. In this context, we look for a stationary deterministic policy (a function fi : X æ A that maps\n1In the works of [11, 8, 3] that we reference, the integer “m” denotes the total number of actions, that is nm with our notation. When we restate their result, we do it with our own notation, that is we replace their ÕÕmÕÕ by ÕÕnmÕÕ.\nstates into controls2) that maximizes the expected discounted sum of rewards from any state i, called the value of policy fi at state i:\nvfi(i) := E C Œÿ\nk=0\n“kr(ik, ak, ik+1) ----- i0 = i, ’k Ø 0, ak = fi(ik), ik+1 ≥ P(·|ik, ak) D\nwhere “ œ (0, 1) is a discount factor. The tuple ÈX, A, p, r, “Í is called a Markov Decision Process (MDP) [9, 1], and the associated problem is known as optimal control. The optimal value starting from state i is defined as\nvú(i) := max fi vfi(i).\nFor any policy fi, we write Pfi for the n ◊ n stochastic matrix whose elements are pij(fi(i)) and rfi the vector whose components are q j pij(fi(i))r(i, fi(i), j). The value functions vfi and vú can be seen as vectors on X. It is well known that vfi is the solution of the following Bellman equation: vfi = rfi + “Pfivfi, that is vfi is a fixed point of the a ne operator Tfi : v ‘æ rfi + “Pfiv. It is also well known that vú satisfies the following Bellman equation:\nvú = max fi (rfi + “Pfivú) = max fi Tfivú\nwhere the max operator is componentwise. In other words, vú is a fixed point of the nonlinear operator T : v ‘æ maxfi Tfiv. For any value vector v, we say that a policy fi is greedy with respect to the value v if it satisfies:\nfi œ arg max fiÕ TfiÕv\nor equivalently Tfiv = Tv. With some slight abuse of notation, we write G(v) for any policy that is greedy with respect to v. The notions of optimal value function and greedy policies are fundamental to optimal control because of the following property: any policy fiú that is greedy with respect to the optimal value vú is an optimal policy and its value vfiú is equal to vú. Let fi be some policy. We call advantage with respect to fi the following quantity:\nafi = max fiÕ TfiÕvfi ≠ vfi = Tvfi ≠ vfi.\nWe call the set of switchable states of fi the following set Sfi = {i, afi(i) > 0}.\nAssume now that fi is non-optimal (this implies that Sfi is a non-empty set). For any non-empty subset Y of Sfi, we denote switch(fi, Y ) a policy satisfying:\n’i, switch(fi, Y )(i) = ;\nG(vfi)(i) if i œ Y fi(i) if i ”œ Y.\nThe following result is well known (see for instance [9]). Lemma 1. Let fi be some non-optimal policy. If fiÕ = switch(fi, Y ) for some non-empty subset Y of Sfi, then vfiÕ Ø vfi and there exists at least one state i such that vfiÕ(i) > vfi(i).\nThis lemma is the foundation of the well-known iterative procedure, called Policy Iteration (PI), that generates a sequence of policies (fik) as follows.\nfik+1 Ω switch(fik, Yk) for some set Yk such that ÿ ( Yk ™ Sfik . The choice for the subsets Yk leads to di erent variations of PI. In this paper we will focus on two specific variations:\n2Restricting our attention to stationary deterministic policies is not a limitation. Indeed, for the optimality criterion to be defined soon, it can be shown that there exists at least one stationary deterministic policy that is optimal [9].\n• When for all iterations k, Yk = Sfik , that is one switches the actions in all states with positive advantage with respect to fik, the above algorithm is known as Howard’s PI; it can be seen then that fik+1 œ G(vfik ).\n• When for all k, Yk is a singleton containing a state ik œ arg maxi afik (i), that is if we only switch one action in the state with maximal advantage with respect to fik, we will call it Simplex-PI3.\nSince it generates a sequence of policies with increasing values, any variation of PI converges to the optimal policy in a number of iterations that is smaller than the total number of policies mn. In practice, PI converges in very few iterations. On random MDP instances, convergence often occurs in time sub-linear in n. The aim of this paper is to discuss existing and provide new upper bounds on the number of iterations required by Howard’s PI and Simplex-PI that are much sharper than mn. In the next sections, we describe some known results—see [11] for a recent and comprehensive review—about the number of iterations required by Howard’s PI and Simplex-PI, along with some of our original improvements and extensions.4"
    }, {
      "heading" : "2 Bounds with respect to a Fixed Discount Factor “ < 1",
      "text" : "A key observation for both algorithms, that will be central to the results we are about to discuss, is that the sequence they generate satisfies some contraction property5. For any vector u œ Rn, let ÎuÎŒ = max1ÆiÆn|u(i)| be the max-norm of u. Let 1 be the vector of which all components are equal to 1. Lemma 2 (Proof in Section A). The sequence (Îvú ≠ vfik ÎŒ)kØ0 built by Howard’s PI is contracting with coe cient “. Lemma 3 (Proof in Section B). The sequence (1T (vú ≠ vfik ))kØ0 built by Simplex-PI is contracting with coe cient 1 ≠ 1≠“n .\nThough this observation is widely known for Howard’s PI, it was to our knowledge never mentionned explicitly in the literature for Simplex-PI. These contraction properties have the following immediate consequence6. Corollary 1. Let V\nmax = maxfi ÎrfiÎŒ 1≠“ be an upper bound on ÎvfiÎŒ for all policies fi. In\norder to get an ‘-optimal policy, that is a policy fik satisfying Îvú ≠ vfik ÎŒ Æ ‘, Howard’s PI requires at most Ï log V max\n‘ 1≠“\nÌ iterations, while Simplex-PI requires at most Ï n log nVmax‘\n1≠“\nÌ\niterations.\nThese bounds depend on the precision term ‘, which means that Howard’s PI and SimplexPI are weakly polynomial for a fixed discount factor “. An important breakthrough was recently achieved by [11] who proved that one can remove the dependency with respect to ‘, and thus show that Howard’s PI and Simplex-PI are strongly polynomial for a fixed discount factor “. Theorem 1 ([11]). Simplex-PI and Howard’s PI both terminate after at most n(m ≠ 1) Ï n 1≠“ log 1 n2 1≠“ 2Ì iterations.\n3In this case, PI is equivalent to running the simplex algorithm with the highest-pivot rule on a linear program version of the MDP problem [11].\n4For clarity, all proofs are deferred to the Appendix. The first proofs about bounds for the case “ < 1 are given in the Appendix of the paper. The other proofs, that are more involved, are provided in the Supplementary Material.\n5A sequence of non-negative numbers (xk)kØ0 is contracting with coe cient – if and only if for all k Ø 0, xk+1 Æ –xk.\n6For Howard’s PI, we have: Îvú≠vfik ÎŒ Æ “kÎvú≠vfi0 ÎŒ Æ “kVmax. Thus, a su cient condition for Îvú ≠vfik ÎŒ < ‘ is “kVmax < ‘, which is implied by k Ø log V max ‘ 1≠“ > log V max ‘\nlog\n1 “ . For Simplex-PI, we\nhave Îvú ≠vfik ÎŒ Æ Îvú ≠vfik Î1 Æ ! 1 ≠ 1≠“n \"k Îvú ≠vfi 0 Î 1 Æ ! 1 ≠ 1≠“n \"k nV max\n, and the conclusion is similar to that for Howard’s PI.\nThe proof is based on the fact that PI corresponds to the simplex algorithm in a linear programming formulation of the MDP problem. Using a more direct proof, [3] recently improved the result by a factor O(n) for Howard’s PI. Theorem 2 ([3]). Howard’s PI terminates after at most (nm + 1) Ï 1 1≠“ log 1 n 1≠“ 2Ì iterations.\nOur first two results, that are consequences of the contraction properties (Lemmas 2 and 3), are stated in the following theorems. Theorem 3 (Proof in Section C). Howard’s PI terminates after at most n(m ≠ 1) Ï 1 1≠“ log 1 1 1≠“ 2Ì iterations.\nTheorem 4 (Proof in Section D). Simplex-PI terminates after at most n(m ≠ 1) Ï n 1≠“ log 1 n 1≠“ 2Ì iterations.\nOur result for Howard’s PI is a factor O(log n) better than the previous best result of [3]. Our result for Simplex-PI is only very slightly better (by a factor 2) than that of [11], and uses a proof that is more direct. Using more refined argument, we managed to also improve the bound for Simplex-PI by a factor O(log n). Theorem 5 (Proof in Section E). Simplex-PI terminates after at most n2(m ≠ 1) 1 1 + 2\n1≠“ log 1 1≠“\n2 iterations.\nCompared to Howard’s PI, our bound for Simplex-PI is a factor O(n) larger. However, since one changes only one action per iteration, each iteration may have a complexity lower by a factor n: the update of the value can be done in time O(n2) through the Sherman-Morrisson formula, though in general each iteration of Howard’s PI, which amounts to compute the value of some policy that may be arbitrarily di erent from the previous policy, may require O(n3) time. Overall, both algorithms seem to have a similar complexity. It is easy to see that the linear dependency of the bound for Howard’s PI with respect to n is optimal. We conjecture that the linear dependency of both bounds with respect to m is also optimal. The dependency with respect to the term 1\n1≠“ may be improved, but removing it is impossible for Howard’s PI and very unlikely for Simplex-PI. [2] describes an MDP for which Howard’s PI requires an exponential (in n) number of iterations for “ = 1 and [5] argued that this holds also when “ is in the vicinity of 1. Though a similar result does not seem to exist for Simplex-PI in the literature, [7] consider four variations of PI that all switch one action per iteration, and show through specifically designed MDPs that they may require an exponential (in n) number of iterations when “ = 1."
    }, {
      "heading" : "3 Bounds for Simplex-PI that are independent of “",
      "text" : "In this section, we will describe some bounds that do not depend on “ but that will be based on some structural assumptions of the MDPs. On this topic, [8] recently showed the following result for deterministic MDPs. Theorem 6 ([8]). If the MDP is deterministic, then Simplex-PI terminates after at most O(n5m2 log2 n) iterations.\nGiven a policy fi of a deterministic MDP, states are either on cycles or on paths induced by fi. The core of the proof relies on the following lemmas that altogether show that cycles are created regularly and that significant progress is made every time a new cycle appears; in other words, significant progress is made regularly. Lemma 4. If the MDP is deterministic, after at most nmÁ2(n ≠ 1) log nË iterations, either Simplex-PI finishes or a new cycle appears. Lemma 5. If the MDP is deterministic, when Simplex-PI moves from fi to fiÕ where fiÕ involves a new cycle, we have\n1T (vfiú ≠ vfiÕ) Æ 3\n1 ≠ 1 n\n4 1T (vfiú ≠ vfi).\nIndeed, these observations su ce to prove7 that Simplex-PI terminates after O(n4m2 log n\n1≠“ ) = Õ(n 4m2). Removing completely the dependency with respect to the\ndiscount factor “—the term in O(log 1 1≠“ )—requires a careful extra work described in [8], which incurs an extra term of order O(n log(n)). At a more technical level, the proof of [8] critically relies on some properties of the vector xfi = (I ≠ “P Tfi )≠11 that provides a discounted measure of state visitations along the trajectories induced by a policy fi starting from a uniform distribution:\n’i œ X, xfi(i) = n Œÿ\nt=0\n“tP(it = i | i0 ≥ U, at = fi(it)),\nwhere U denotes the uniform distribution on the state space X. For any policy fi and state i, we trivially have xfi(i) œ 1 1, n\n1≠“\n2 . The proof exploits the fact that xfi(i) belongs to the\nset (1, n) when i is on a path of fi, while xfi(i) belongs to the set ( 1 1≠“ , n 1≠“ ) when i is on a cycle of fi. As we are going to show, it is possible to extend the proof of [8] to stochastic MDPs. Given a policy fi of a stochastic MDP, states are either in recurrent classes or transient classes (these two categories respectively generalize those of cycles and paths). We will consider the following structural assumption. Assumption 1. Let ·t Ø 1 and ·r Ø 1 be the smallest constants such that for all policies fi and all states i,\n(1 Æ )xfi(i) Æ ·t if i is transient for fi, and n\n(1 ≠ “)·r Æ xfi(i) 3 Æ n1 ≠ “ 4 if i is recurrent for fi.\nThe constant ·t (resp. ·r) can be seen as a measure of the time needed to leave transient states (resp. the time needed to revisit states in recurrent classes). In particular, when “ tends to 1, it can be seen that ·t is an upper bound of the expected time L needed to “Leave the set of transient states”, since for any policy fi,\nlim “æ1 ·t Ø 1 n lim “æ1\nÿ\ni transient for fi xfi(i) =\nŒÿ\nt=0\nP(it transient for fi | i0 ≥ U, at = fi(it))\n= E [ L | i 0 ≥ U, at = fi(it)] .\nSimilarly, when “ is in the vicinity of 1, 1·r is the minimal asymptotic frequency 8 in recurrent states given that one starts from a random uniform state, since for any policy fi and recurrent state i:\nlim “æ1 1 ≠ “ n xfi(i) = lim “æ1\n(1 ≠ “) Œÿ\nt=0\n“tP(it = i | i0 ≥ U, at = fi(it))\n= lim T æŒ 1 T\nT ≠1ÿ\nt=0\nP(it = i | i0 ≥ U, at = fi(it)).\nWith Assumption 1 in hand, we can generalize Lemmas 4-5 as follows. Lemma 6. If the MDP satisfies Assumption 1, after at most n # (m ≠ 1)Án·t log(n·t)Ë + Án·t log(n2·t)Ë $ iterations either Simplex-PI finishes or a new recurrent class appears. 7This can be done by using arguments similar to the proof of Theorem 4 in Section D. 8If the MDP is aperiodic and irreducible, and thus admits a stationary distribution ‹fi for any policy fi, one can see that 1 ·r = min fi, i recurrent for fi ‹fi(i).\nLemma 7. If the MDP satisfies Assumption 1, when Simplex-PI moves from fi to fiÕ where fiÕ involves a new recurrent class, we have\n1T (vfiú ≠ vfiÕ) Æ 3\n1 ≠ 1 ·r\n4 1T (vfiú ≠ vfi).\nFrom these generalized observations, we can deduce the following original result. Theorem 7 (Proof in Appendix F of the Supp. Material). If the MDP satisfies Assumption 1, then Simplex-PI terminates after at most\nn2(m ≠ 1) (Á·r log(n·r)Ë + Á·r log(n·t)Ë) # (m ≠ 1)Án·t log(n·t)Ë + Án·t log(n2·t)Ë $\niterations. Remark 1. This new result is a strict generalization of the result for deterministic MDPs. Indeed, in the deterministic case, we have ·t Æ n and ·r Æ n, and it is is easy to see that Lemmas 6, 7 and Theorem 7 respectively imply Lemmas 4, 5 and Theorem 6.\nAn immediate consequence of the above result is that Simplex-PI is strongly polynomial for sets of MDPs that are much larger than the deterministic MDPs mentionned in Theorem 6. Corollary 2. For any family of MDPs indexed by n and m such that ·t and ·r are polynomial functions of n and m, Simplex-PI terminates after a number of steps that is polynomial in n and m."
    }, {
      "heading" : "4 Similar results for Howard’s PI?",
      "text" : "One may then wonder whether similar results can be derived for Howard’s PI. Unfortunately, and as quickly mentionned by [8], the line of analysis developped for Simplex-PI does not seem to adapt easily to Howard’s PI, because simultaneously switching several actions can interfere in a way that the policy improvement turns out to be small. We can be more precise on what actually breaks in the approach we have described so far. On the one hand, it is possible to write counterparts of Lemmas 4 and 6 for Howard’s PI (see Appendix G of the Supp. Material). Lemma 8. If the MDP is deterministic, after at most n iterations, either Howard’s PI finishes or a new cycle appears. Lemma 9. If the MDP satisfies Assumption 1, after at most nmÁ·t log n·tË iterations, either Howard’s PI finishes or a new recurrent class appears.\nHowever, on the other hand, we did not manage to adapt Lemma 5 nor Lemma 7. In fact, it is unlikely that a result similar to that of Lemma 5 will be shown to hold for Howard’s PI. In a recent deterministic example due to [4] to show that Howard’s PI may require at most O(n2) iterations, new cycles are created every single iteration but the sequence of values satisfies9 for all iterations k < n2\n4 + n 4 and states i,\nvú(i) ≠ vfik+1(i) Ø C 1 ≠ 3\n2 n\n4kD (vú(i) ≠ vfik (i)).\nContrary to Lemma 5, as k grows, the amount of contraction gets (exponentially) smaller and smaller. With respect to Simplex-PI, this suggests that Howard’s PI may su er from subtle specific pathologies. In fact, the problem of determining the number of iterations required by Howard’s PI has been challenging for almost 30 years. It was originally identified as an open problem by [10]. In the simplest—deterministic—case, the question is still open: the currently best known lower bound is the O(n2) bound by [4] we have just mentionned, while the best known upper bound is O( mnn ) (valid for all MDPs) due to [6].\n9This MDP has an even number of states n = 2p. The goal is to minimize the long term expected cost. The optimal value function satisfies vú(i) = ≠pN for all i, with N = p2 + p. The policies generated by Howard’s PI have values vfik (i) œ (pN≠k≠1, pN≠k). We deduce that for all iterations k and states i,\nvú(i)≠vfik+1 (i) vú(i)≠vfik (i) Ø 1+p ≠k≠2 1+p≠k = 1 ≠ p ≠k≠p≠k≠2 1+p≠k Ø 1 ≠ p≠k(1 ≠ p≠2) Ø 1 ≠ p≠k.\nOn the positive side, an adaptation of the line of proof we have considered so far can be carried out under the following assumption. Assumption 2. The state space X can be partitioned in two sets T and R such that for all policies fi, the states of T are transient and those of R are recurrent.\nIndeed, under this assumption, we can prove for Howard’s PI a variation of Lemma 7 introduced for Simplex-PI. Lemma 10. For an MDP satisfying Assumptions 1-2, suppose Howard’s PI moves from fi to fiÕ and that fiÕ involves a new recurrent class. Then\n1T (vfiú ≠ vfiÕ) Æ 3\n1 ≠ 1 ·r\n4 1T (vfiú ≠ vfi).\nAnd we can deduce the following original bound (that also applies to Simplex-PI). Theorem 8 (Proof in Appendix H of the Supp. Material). If the MDP satisfies Assumptions 1-2, then Howard’s PI terminates after at most n(m ≠ 1) (Á·t log n·tË + Á·r log n·rË) iterations, while Simplex-PI terminates after at most n(m ≠ 1) (Án·t log n·tË + Á·r log n·rË) iterations.\nIt should however be noted that Assumption 2 is rather restrictive. It implies that the algorithms converge on the recurrent states independently of the transient states, and thus the analysis can be decomposed in two phases: 1) the convergence on recurrent states and then 2) the convergence on transient states (given that recurrent states do not change anymore). The analysis of the first phase (convergence on recurrent states) is greatly facilitated by the fact that in this case, a new recurrent class appears every single iteration (this is in contrast with Lemmas 4, 6, 8 and 9 that were designed to show under which conditions cycles and recurrent classes are created). Furthermore, the analysis of the second phase (convergence on transient states) is similar to that of the discounted case of Theorems 3 and 4. In other words, if this last result sheds some light on the practical e ciency of Howard’s PI and Simplex-PI, a general analysis of Howard’s PI is still largely open, and constitutes our main future work."
    }, {
      "heading" : "A Contraction property for Howard’s PI (Proof of Lemma 2)",
      "text" : "For any k, using the facts that {’fi, Tfivfi = vfi}, {Tfiúvfik≠1 Æ Tfik vfik≠1} and {Lemma 1 and Pfik is positive definite}, we have\nvfiú ≠ vfik = Tfiúvfiú ≠ Tfiúvfik≠1 + Tfiúvfik≠1 ≠ Tfik vfik≠1 + Tfik vfik≠1 ≠ Tfik vfik Æ “Pfiú(vfiú ≠ vfik≠1) + “Pfik (vfik≠1 ≠ vfik ) Æ “Pfiú(vfiú ≠ vfik≠1).\nSince vfiú ≠ vfik is non negative, we can take the max norm and get: Îvfiú ≠ vfik ÎŒ Æ “Îvfiú ≠ vfik≠1ÎŒ."
    }, {
      "heading" : "B Contraction property for Simplex-PI (Proof of Lemma 3)",
      "text" : "By using the fact that {vfi = Tfivfi ∆ vfi = (I ≠ “Pfi)≠1rfi}, we have that for all pairs of policies fi and fiÕ.\nvfiÕ ≠ vfi = (I ≠ “PfiÕ)≠1rfiÕ ≠ vfi = (I ≠ “PfiÕ)≠1(rfiÕ + “PfiÕvfi ≠ vfi) = (I ≠ “PfiÕ)≠1(TfiÕvfi ≠ vfi). (1)\nOn the one hand, by using this lemma and the fact that {Tfik+1vfik ≠ vfik Ø 0}, we have for any k: vfik+1 ≠ vfik = (I ≠ “Pk+1)≠1(Tfik+1vfik ≠ vfik ) Ø Tfik+1vfik ≠ vfik , which implies that\n1T (vfik+1 ≠ vfik ) Ø 1T (Tfik+1vfik ≠ vfik ). (2)\nOn the other hand, using Equation (1) and the facts that {Î(I ≠ “Pfiú)≠1ÎŒ = 1\n1≠“ and (I ≠ “Pfiú) ≠1 is positive definite}, {maxs Tfik+1vfik (s) = maxs,fĩ Tfĩvfik (s)} and\n{’x Ø 0, maxs x(s) Æ 1T x}, we have:\nvfiú ≠ vfik = (I ≠ “Pfiú)≠1(Tfiúvfik ≠ vfik ) Æ 1\n1 ≠ “ maxs Tfiúvfik (s) ≠ vfik (s)\nÆ 11 ≠ “ maxs Tfik+1vfik (s) ≠ vfik (s) Æ 1 1 ≠ “1 T (Tfik+1vfik ≠ vfik ),\nwhich implies (using {’x, 1T x Æ nÎxÎŒ}) that\n1T (Tfik+1vfik ≠ vfik ) Ø (1 ≠ “)Îvfiú ≠ vfik ÎŒ Ø 1 ≠ “\nn 1T (vfiú ≠ vfik ). (3)\nCombining Equations (2) and (3), we get: 1T (vfiú ≠ vfik+1) = 1T (vfiú ≠ vfik ) ≠ 1T (vfik+1 ≠ vfik )\nÆ 1T (vfiú ≠ vfik ) ≠ 1 ≠ “\nn 1T (vfiú ≠ vfik ) =\n3 1 ≠ 1 ≠ “\nn\n4 1T (vfiú ≠ vfik )."
    }, {
      "heading" : "C A bound for Howard’s PI when “ < 1 (Proof of Theorem 3)",
      "text" : "For any k, by using Equation (1) and the fact {vú ≠ vfik Ø 0 and Pfik positive definite}, we have:\nvú ≠ Tfik vú = (I ≠ “Pfik )(vú ≠ vfik ) Æ vú ≠ vfik . Since vú≠Tfik vú is non negative, we can take the max norm and, using Lemma 2, Equation (1) and the fact that {Î(I ≠ “Pfi\n0 )≠1ÎŒ = 1 1≠“ }, we get:\nÎvú ≠ Tfik vúÎŒ Æ Îvú ≠ vfik ÎŒ Æ “kÎvfiú ≠ vfi0ÎŒ\n= “kÎ(I ≠ “Pfi 0 )≠1(vú ≠ Tfi 0 vú)ÎŒ Æ “k\n1 ≠ “ Îvú ≠ Tfi0vúÎŒ. (4)\nBy definition of the max-norm, there exists a state s 0 such that vú(s0) ≠ [Tfi 0 vú](s0) = Îvú ≠ Tfi\n0\nvúÎŒ. From Equation (4), we deduce that for all k,\nvú(s0) ≠ [Tfik vú](s0) Æ Îvú ≠ Tfik vúÎŒ Æ “k 1 ≠ “ Îvú ≠ Tfi0vúÎŒ = “k 1 ≠ “ (vú(s0) ≠ [Tfi0vú](s0)).\nAs a consequence, the action fik(s0) must be di erent from fi0(s0) when “ k 1≠“ < 1, that is for all values of k satisfying k Ø kú = Ï log 1\n1≠“ 1≠“\nÌ > Ï log 1\n1≠“ log 1\n“\nÌ . In other words, if some policy fi\nis not optimal, then one of its non-optimal actions will be eliminated for good after at most kú iterations. By repeating this argument, one can eliminate all non-optimal actions (they are at most n(m ≠ 1)), and the result follows."
    }, {
      "heading" : "D A bound for Simplex-PI when “ < 1 (Proof of Theorem 4)",
      "text" : "Using {’x Ø 0, ÎxÎŒ Æ 1T x}, Lemma 3, {’x, 1T x Æ nÎxÎŒ}, Equation (1) and {Î(I ≠ “Pfi\n0 )≠1ÎŒ = 1 1≠“ }, we have for all k,\nÎvfiú ≠ Tfik vfiúÎŒ Æ Îvfiú ≠ vfik ÎŒ Æ 1T (vfiú ≠ vfik )\nÆ 3\n1 ≠ 1 ≠ “ n\n4k 1T (vfiú ≠ vfi0) Æ n 3 1 ≠ 1 ≠ “\nn\n4k Îvfiú ≠ vfi0ÎŒ\n= n 3\n1 ≠ 1 ≠ “ n\n4k Î(I ≠ “Pfi\n0 )≠1(vú ≠ Tfi 0 vú)ÎŒ Æ n\n1 ≠ “\n3 1 ≠ 1 ≠ “\nn\n4k Îvfiú ≠ Tfi0vfiúÎŒ\nSimilarly to the proof for Howard’s PI, we deduce that a non-optimal action is eliminated after at most kú = Ï n\n1≠“ log n 1≠“\nÌ Ø\n9 log\nn 1≠“\nlog(1≠ 1≠“n )\n: , and the overall number of iterations is\nobtained by noting that there are at most n(m ≠ 1) non optimal actions to eliminate."
    } ],
    "references" : [ {
      "title" : "Neurodynamic Programming",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Exponential lower bounds for policy iteration",
      "author" : [ "J. Fearnley" ],
      "venue" : "Proceedings of the 37th international colloquium conference on Automata, languages and programming: Part II, ICALP’10, pages 551–562, Berlin, Heidelberg",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor",
      "author" : [ "T.D. Hansen", "P.B. Miltersen", "U. Zwick" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Lower bounds for howard’s algorithm for finding minimum mean-cost cycles",
      "author" : [ "T.D. Hansen", "U. Zwick" ],
      "venue" : "ISAAC (1), pages 415–426",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The complexity of policy iteration is exponential for discounted markov decision processes",
      "author" : [ "R. Hollanders", "J.C. Delvenne", "R. Jungers" ],
      "venue" : "51st IEEE conference on Decision and control (CDC’12)",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the complexity of policy iteration",
      "author" : [ "Y. Mansour", "S.P. Singh" ],
      "venue" : "UAI, pages 401–408",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "On the complexity of the policy improvement algorithm for markov decision processes",
      "author" : [ "M. Melekopoglou", "A. Condon" ],
      "venue" : "INFORMS Journal on Computing, 6(2):188–192",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "The simplex method is strongly polynomial for deterministic markov decision processes",
      "author" : [ "I. Post", "Y. Ye" ],
      "venue" : "Technical report, arXiv:1208.5083v2",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Markov Decision Processes",
      "author" : [ "M. Puterman" ],
      "venue" : "Wiley, New York",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "How good is howard’s policy improvement algorithm? Zeitschrift für Operations Research",
      "author" : [ "N. Schmitz" ],
      "venue" : "29(7):315–316",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1985
    }, {
      "title" : "The simplex and policy-iteration methods are strongly polynomial for the markov decision problem with a fixed discount rate",
      "author" : [ "Y. Ye" ],
      "venue" : "Math. Oper. Res., 36(4):593–603",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "1≠“ 22 iterations, improving by a factor O(log n) a result by [3], while Simplex-PI terminates after at most n(2)(m ≠ 1) 1 1 + 2 1≠“ log 1",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "iterations, improving by a factor O(log n) a result by [11].",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "This generalizes a recent result for deterministic MDPs by [8], in which ·t Æ n and ·r Æ n.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 10,
      "context" : "In the works of [11, 8, 3] that we reference, the integer “m” denotes the total number of actions, that is nm with our notation.",
      "startOffset" : 16,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "In the works of [11, 8, 3] that we reference, the integer “m” denotes the total number of actions, that is nm with our notation.",
      "startOffset" : 16,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "In the works of [11, 8, 3] that we reference, the integer “m” denotes the total number of actions, that is nm with our notation.",
      "startOffset" : 16,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "The tuple ÈX, A, p, r, “Í is called a Markov Decision Process (MDP) [9, 1], and the associated problem is known as optimal control.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "The tuple ÈX, A, p, r, “Í is called a Markov Decision Process (MDP) [9, 1], and the associated problem is known as optimal control.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "The following result is well known (see for instance [9]).",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "Indeed, for the optimality criterion to be defined soon, it can be shown that there exists at least one stationary deterministic policy that is optimal [9].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "In the next sections, we describe some known results—see [11] for a recent and comprehensive review—about the number of iterations required by Howard’s PI and Simplex-PI, along with some of our original improvements and extensions.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "An important breakthrough was recently achieved by [11] who proved that one can remove the dependency with respect to ‘, and thus show that Howard’s PI and Simplex-PI are strongly polynomial for a fixed discount factor “.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "In this case, PI is equivalent to running the simplex algorithm with the highest-pivot rule on a linear program version of the MDP problem [11].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "Using a more direct proof, [3] recently improved the result by a factor O(n) for Howard’s PI.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "Our result for Howard’s PI is a factor O(log n) better than the previous best result of [3].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "Our result for Simplex-PI is only very slightly better (by a factor 2) than that of [11], and uses a proof that is more direct.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "[2] describes an MDP for which Howard’s PI requires an exponential (in n) number of iterations for “ = 1 and [5] argued that this holds also when “ is in the vicinity of 1.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[2] describes an MDP for which Howard’s PI requires an exponential (in n) number of iterations for “ = 1 and [5] argued that this holds also when “ is in the vicinity of 1.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "Though a similar result does not seem to exist for Simplex-PI in the literature, [7] consider four variations of PI that all switch one action per iteration, and show through specifically designed MDPs that they may require an exponential (in n) number of iterations when “ = 1.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "On this topic, [8] recently showed the following result for deterministic MDPs.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "Removing completely the dependency with respect to the discount factor “—the term in O(log 1 1≠“ )—requires a careful extra work described in [8], which incurs an extra term of order O(n log(n)).",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : "At a more technical level, the proof of [8] critically relies on some properties of the vector xfi = (I ≠ “P T fi )≠11 that provides a discounted measure of state visitations along the trajectories induced by a policy fi starting from a uniform distribution:",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "As we are going to show, it is possible to extend the proof of [8] to stochastic MDPs.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "Unfortunately, and as quickly mentionned by [8], the line of analysis developped for Simplex-PI does not seem to adapt easily to Howard’s PI, because simultaneously switching several actions can interfere in a way that the policy improvement turns out to be small.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "In a recent deterministic example due to [4] to show that Howard’s PI may require at most O(n(2)) iterations, new cycles are created every single iteration but the sequence of values satisfies(9) for all iterations k < n2",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "It was originally identified as an open problem by [10].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "In the simplest—deterministic—case, the question is still open: the currently best known lower bound is the O(n(2)) bound by [4] we have just mentionned, while the best known upper bound is O( m n ) (valid for all MDPs) due to [6].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "In the simplest—deterministic—case, the question is still open: the currently best known lower bound is the O(n(2)) bound by [4] we have just mentionned, while the best known upper bound is O( m n ) (valid for all MDPs) due to [6].",
      "startOffset" : 227,
      "endOffset" : 230
    } ],
    "year" : 2013,
    "abstractText" : "Given a Markov Decision Process (MDP) with n states and m actions per state, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal “-discounted optimal policy. We consider two variations of PI: Howard’s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard’s PI terminates after at most n(m ≠ 1) Ï",
    "creator" : null
  }
}