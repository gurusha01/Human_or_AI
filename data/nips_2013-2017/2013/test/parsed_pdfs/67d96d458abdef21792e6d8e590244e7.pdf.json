{
  "name" : "67d96d458abdef21792e6d8e590244e7.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty",
    "authors" : [ "Haichao Zhang", "David Wipf" ],
    "emails" : [ "hczhang1@gmail.com", "davidwipf@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Image blur is an undesirable degradation that often accompanies the image formation process and may arise, for example, because of camera shake during acquisition. Blind image deblurring strategies aim to recover a sharp image from only a blurry, compromised observation. Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29]. Unfortunately, many real-world photographs contain blur effects that vary across the image plane, such as when unknown rotations are introduced by camera shake [17].\nMore recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12]. Note that the original uniform blur model can be achieved equivalently when H is forced to adopt certain structure (e.g., block-toeplitz structure with toeplitz-blocks). In general, nonuniform blur may arise under several different contexts. This paper will focus on the blind removal of non-uniform blur caused by general camera shake (as opposed to blur from object motion) using only a single image, with no additional hardware assistance.\nWhile existing algorithms for addressing non-uniform camera shake have displayed a measure of success, several important limitations remain. First, some methods require either additional spe-\ncialized hardware such as high-speed video capture [23] or inertial measurement sensors [13] for estimating motion, or else multiple images of the same scene [4]. Secondly, even the algorithms that operate given only data from a single image typically rely on carefully engineered initializations, heuristics, and trade-off parameters for selecting salient image structure or edges, in part to avoid undesirable degenerate, no-blur solutions [7, 8, 9, 11]. Consequently, enhancements and rigorous analysis may be problematic. To address these shortcomings, we present an alternative blind deblurring algorithm built upon a simple, closed-form cost function that automatically discounts regions of the image that contain little information about the blur operator without introducing any additional salient structure selection steps. This transparency leads to a nearly tuning-parameter free algorithm based upon a sparsity penalty whose shape adapts to the estimated degree of local blur, and provides theoretical arguments regarding how to robustly handle non-uniform degradations.\nThe rest of the paper is structured as follows. Section 2 briefly describes relevant existing work on non-uniform blind deblurring operators and implementation techniques. Section 3 then introduces the proposed non-uniform blind deblurring model, while further theoretical justification and analyses are provided in Section 4. Experimental comparisons with state-of-the-art methods are carried out in Section 5 followed by conclusions in Section 6."
    }, {
      "heading" : "2 Non-Uniform Deblurring Operators",
      "text" : "Perhaps the most direct way of handling non-uniform blur is to simply partition the image into different regions and then learn a separate, uniform blur kernel for each region, possibly with an additional weighting function for smoothing the boundaries between two adjacent kernels. The resulting algorithm has been adopted extensively [18, 8, 22, 12] and admits an efficient implementation called efficient filter flow (EFF) [10]. The downside with this type of model is that geometric relationships between the blur kernels of different regions derived from the the physical motion path of the camera are ignored.\nIn contrast, to explicitly account for camera motion, the projective motion path (PMP) model [23] treats a blurry image as the weighted summation of projectively transformed sharp images, leading to the revised observation model\ny = ∑\nj\nwjPjx + n, (1)\nwhere Pj is the j-th projection or homography operator (a combination of rotations and translations) andwj is the corresponding combination weight representing the proportion of time spent at that particular camera pose during exposure. The uniform convolutional model can be obtained by restricting the general projection operators {Pj} to be translations. In this regard, (1) represents a more general model that has been used in many recent non-uniform deblurring efforts [23, 25, 7, 11, 4]. PMP also retains the bilinear property of uniform convolution, meaning that\ny = Hx + n = Dw + n, (2)\nwhere H = ∑\nj wjPj and D = [P1x,P2x, · · · ,Pjx, · · · ] is a matrix of transformed sharp images. The disadvantage of PMP is that it typically leads to inefficient algorithms because the evaluation of the matrix-vector product Hx = Dw requires generating many expensive intermediate transformed images. However, EFF can be combined with the PMP model by introducing a set of basis images efficiently generated by transforming a grid of delta peak images [9]. The computational cost can be further reduced by using an active set for pruning out the projection operators with small responses [11]."
    }, {
      "heading" : "3 A New Non-Uniform Blind Deblurring Model",
      "text" : "Following previous work [6, 16], we will work in the derivative domain of images for ease of modeling and better performance, meaning that x ∈ Rm and y ∈ Rn will denote the lexicographically ordered sharp and blurry image derivatives respectively. 1\n1The derivative filters used in this work are {[−1, 1], [−1, 1]T }. Other choices are also possible.\nThe observation model (1) is equivalent to the likelihood function p(y|x,w) ∝ exp [ − 1\n2λ ‖y − Hx‖22\n] , (3)\nwhere λ denotes the noise variance. Maximum likelihood estimation of x and w using (3) is clearly ill-posed and so further regularization is required to constrain the solution space. For this purpose we adopt the Gaussian prior p(x) ∼ N (x;0,Γ), where Γ diag[γ] with γ = [γ1, . . . , γm]T a vector ofm hyperparameter variances, one for each element of x = [x 1, . . . , xm]T . While presently γ is unknown, if we first marginalize over the unknown x, we can estimate it jointly along with the blur parameters w and the unknown noise variance λ. This type II maximum likelihood procedure has been advocated in the context of sparse estimation, where the goal is to learn vectors with mostly zero-valued coefficients [24, 26]. The final sharp image can then be recovered using the estimated kernel and noise level along with standard non-blind deblurring algorithms (e.g., [15]).\nMathematically, the proposed estimation scheme requires that we solve\nmax γ,w,λ≥0\n∫ p(y|x,w)p(x)dx ≡ min\nγ,w,λ≥0 yT\n( HΓHT + λI )−1 y + log ∣∣HΓHT + λI∣∣ , (4) where a − log transformation has been included for convenience. Clearly (4) does not resemble the traditional blind non-uniform deblurring script, where estimation proceeds using the more transparent penalized regression model [4, 7, 9]\nmin x;w≥0\n‖y − Hx‖22 + α ∑\ni\ng(xi) + β ∑\nj\nh(wj) (5)\nand α and β are user-defined trade-off parameters, g is an image penalty which typically favors sparsity, and h is usually assumed to be quadratic. Despite the differing appearances however, (4) has some advantageous properties with respect to deconvolution problems. In particular, it is devoid of tuning parameters and it possesses more favorable minimization conditions. For example, consider the simplified non-uniform deblurring situation where the true x has a single non-zero element and H is defined such that each column indexed by i is independently parameterized with finite support symmetric around pixel i. Moreover, assume this support matches the true support of the unknown blur operator. Then we have the following:\nLemma 1 Given the idealized non-uniform deblurring problem described above, the cost function (4) will be characterized by a unique minimizing solution that correctly locates the nonzero element in x and the corresponding true blur kernel at this location. No possible problem in the form of (5), with g(x) = |x|p, h(w) = wq , and {p, q} arbitrary non-negative scalars, can achieve a similar result (there will always exist either multiple different minimizing solutions or an global minima that does not produce the correct solution).\nThis result, which can be generalized with additional effort, can be shown by expanding on some of the derivations in [26]. Although obviously the conditions upon which Lemma 1 is based are extremely idealized, it is nonetheless emblematic of the potential of the underlying cost function to avoid local minima, etc., and [26] contains complementary results in the case where H is fixed.\nWhile optimizing (4) is possible using various general techniques such as the EM algorithm, it is computationally expensive in part because of the high-dimensional determinants involved with realistic-sized images. Consequently we are presently considering various specially-tailored optimization schemes for future work. But for the present purposes, we instead minimize a convenient upper bound allowing us to circumvent such computational issues. Specifically, using Hadamard’s inequality we have\nlog ∣∣HΓHT + λI∣∣ = n logλ+ log |Γ| + log ∣∣λ−1HTH + Γ−1∣∣\n≤ n logλ+ log |Γ| + log ∣∣λ−1diag [HTH] + Γ−1∣∣ =\n∑ i log ( λ+ γi‖w̄i‖22 ) + (n−m) logλ, (6)\nwhere w̄i denotes the i-th column of H. Note that Hadamard’s inequality is applied by using λ−1HTH + Γ−1 = VT V for some matrix V = [v1, . . . ,vm]. We then have log |λ−1HTH + Γ−1| = 2 log |V| ≤ 2 log (∏i ‖vi‖2) = log ∣∣diag [λ−1HTH + Γ−1]∣∣, leading to the stated result.\nAlso, the quantity ‖w̄i‖2 which appears in (6) can be viewed as a measure of the degree of local blur at location i. Given the feasible region w ≥ 0 and without loss of generality the constraint∑\niwi = 1 for normalization purposes, it can easily be shown that 1/L ≤ ‖w̄i‖22 ≤ 1, where L is the maximum number of elements in any local blur kernel w̄i or column of H. The upper bound is achieved when the local kernel is a delta solution, meaning only one nonzero element and therefore minimal blur. In contrast, the lower bound on ‖ w̄i‖22 occurs when every element of w̄i has an equal value, constituting the maximal possible blur. This metric, which will influence our analysis in the next section, can be computing using ‖ w̄i‖22 = wT (BTi Bi)w, where Bi [P1ei,P2ei, · · · ,Pjei, · · · ] and ei denotes an all-zero image with a one at site i. In the uniform deblurring case, BTi Bi = I ignoring edge effects, and therefore ‖w̄i‖2 = ‖w‖2 for all i. While optimizing (4) using the upper bound from (6) can be justified in part using Bayesian-inspired arguments and the lack of trade-off parameters, the augmented cost function unfortunately no longer satisfies Lemma 1. However, it is still well-equipped for estimating sparse image gradients and avoiding degenerate no-blur solutions. For example, consider the case of an asymptotically large image with iid distributed sparse image gradients, with some constant fraction exactly equal to zero and the remaining nonzero elements drawn from any continuous distribution. Now suppose that this image is corrupted with a non-uniform blur operator of the form H = ∑ j wjPj , where the cardinality of the summation is finite and H satisfies minimal regularity conditions. Then it can be shown that any global minimum of (4), with or without the bound from (6), will produce the true blur operator. Related intuition applies when noise is present or when the image gradients are not exactly sparse (we will defer more detailed analysis to a future publication).\nRegardless, the simplified γ-dependent cost function is still far less intuitive than the penalized regression models dependent on x such as (5) that are typically employed for non-uniform blind deblurring. However, using the framework from [26], it can be shown that the kernel estimate obtained by this process is formally equivalent to the one obtained via\nmin x;w≥0,λ≥0 1 λ ‖y − Hx‖22 + ∑ i ψ(|xi|‖w̄i‖2, λ) + (n−m) logλ, with (7)\nψ(u, λ) 2u u+ √ 4λ+ u2\n+ log ( 2λ+ u2 + u √ 4λ+ u2 ) u ≥ 0.\nThe optimization from (7) closely resembles a standard penalized regression (or equivalently MAP) problem used for blind deblurring. The primary distinction is the penalty term ψ, which jointly regularizes x, w, and λ as discussed Section 4. The supplementary file derives a simple majorizationminimization algorithm for solving (7) along with additional implementational details. The underlying procedure is related to variational Bayesian (VB) models from [1, 16, 20]; however, these models are based on a completely different mean-field approximation and a uniform blur assumption, and they do not learn the noise parameter. Additionally, the analysis provided with these VB models is limited by relatively less transparent underlying cost functions."
    }, {
      "heading" : "4 Model Properties",
      "text" : "The proposed blind deblurring strategy involves simply minimizing (7); no additional steps for tradeoff parameter selection or structure/salient-edge detection are required unlike other state-of-the-art approaches. This section will examine theoretical properties of (7) that ultimately allow such a simple algorithm to succeed. First, we will demonstrate a form of intrinsic column normalization that facilitates the balanced sparse estimation of the unknown latent image and implicitly de-emphasizes regions with large blur and few dominate edges. Later we describe an appealing form of noisedependent shape adaptation that helps in avoiding local minima. While there are multiple, complementary perspectives for interpreting the behavior of this algorithm, more detailed analyses, as well as extensions to other types of underdetermined inverse problems such as dictionary learning, will be deferred to a later publication."
    }, {
      "heading" : "4.1 Column-Normalized Sparse Estimation",
      "text" : "Using the simple reparameterization zi xi‖w̄i‖2 it follows that (7) is exactly equivalent to solving min\nz;w≥0,λ≥0 1 λ ‖y − H̃z‖22 + ∑ i ψ(|zi|, λ) + (n−m) logλ, (8)\nwhere z = [z1, . . . , zm]T and H̃ is simply the 2-column-normalized version of H. Moreover, it can be shown that this ψ is a concave, non-decreasing function of |z|, and hence represents a canonical sparsity-promoting penalty function with respect to z [26]. Consequently, noise and kernel dependencies notwithstanding, this reparameterization places the proposed cost function in a form exactly consistent with nearly all prototypical sparse regression problems, where 2 column normalization is ubiquitous, at least in part, to avoid favoring one column over another during the estimation process (which can potentially bias the solution). To understand the latter point, note that ‖y − H̃z‖22 ≡ zT H̃T H̃z − 2yT H̃z. Among other things, because of the normalization, the quadratic factor H̃T H̃ now has a unit diagonal, and likewise the inner products yT H̃ are scaled by the consistent induced 2 norms, which collectively avoids the premature favoring of any one element of z over another. Moreover, no additional heuristic kernel penalty terms such as in (5) are required since H̃ is in some sense self-regularized by the normalization. Additional ancillary benefits of (8) will be described in Section 4.2.\nOf course we can always apply the same reparameterization to existing algorithms in the form of (5). While this will indeed result in normalized columns and a properly balanced data-fit term, these raw norms will now appear in the penalty function g, giving the equivalent objective\nmin z;w≥0\n‖y − H̃z‖22 + α ∑\ni\ng ( zi‖w̄i‖−12 ) + β ∑ j h(wj). (9)\nHowever, the presence of these norms now embedded in g may have undesirable consequences. Simply put, the problem (9) will favor solutions where the ratio z i/‖w̄i‖2 is sparse or nearly so, which can be achieved by either making many z i zero or many ‖w̄i‖2 big. If some zi is estimated to be zero (and many zi will provably be exactly zero at any local minima if g(x) is a concave, non-decreasing function of |x|), then the corresponding ‖ w̄i‖2 will be unconstrained. In contrast, if a given zi is non-zero, there will be a stronger push for the associated ‖ w̄i‖2 to be large, i.e., more like the delta kernel which maximizes the 2 norm. Thus, the relative penalization of the kernel norms will depend on the estimated local image gradients, and no-blur delta solutions may be arbitrarily favored in parts of the image plane dominated by edges, the very place where blur estimation information is paramount.\nIn reality, the local kernel norms ‖w̄i‖2, which quantify the degree of local blur as mentioned previously, should be completely independent of the sparsity of the image gradients in the same location. This is of course because the different blurring effects from camera shake are independent of the locations of strong edges in a given scene, since the blur operator is only a function of camera motion (at least to first order approximation). One way to compensate for this independence would be to simply optimize (9) with ‖w̄i‖2 removed from g. While this is possible in principle, enforcing the non-convex, and coupled constraints required to maintain normalized columns is extremely difficult. Another option would be to carefully choose β and h to somehow compensate. In contrast, our algorithm handles these complications seamlessly without any additional penalty terms."
    }, {
      "heading" : "4.2 Noise-Dependent, Parameter-Free Homotopy Continuation",
      "text" : "Column normalization can be viewed as a principled first step towards solving challenging sparse estimation problems. However, when non-convex sparse regularizers are used for the image penalty, e.g., p norms with p < 1, then local minima can be a significant problem. The rationalization for using such potentially problematic non-convexity is as follows; more details can be found in [17, 27]. When applied to a sharp image, any blur operator will necessarily contribute two opposing effects: (i) It reduces a measure of the image sparsity, which normally increases the penalty ∑ i |yi|p, and\n(ii) It broadly reduces the overall image variance, which actually reduces ∑\ni |yi|p. Additionally, the greater the degree of blur, the more effect (ii) will begin to overshadow (i). Note that we can always apply greater and greater blur to any sharp image x such that the variance of the resulting blurry y is arbitrarily small. This then produces an arbitrarily small p norm, which implies that∑\ni |yi|p < ∑\ni |xi|p, meaning that the penalty actually favors the blurry image over the sharp one. In a practical sense though, the amount of blur that can be tolerated before this undesirable preference for y over x occurs is much larger as p approaches zero. This is because the more concave the image penalty becomes (as a function of coefficient magnitudes), the less sensitive it is to image variance and the more sensitive it is to image sparsity. In fact the scale-invariant special case where\np→ 0 depends only on sparsity, or the number of elements that are exactly equal to zero. 2 We may therefore expect such a highly concave, sparsity promoting penalty to favor the sharp image over the blurry one in a broader range of blur conditions. Even with other families of penalty functions the same basic notion holds: greater concavity means greater sparsity preference and less sensitivity to variance changes that favor no-blur degenerate solutions.\nFrom an implementational standpoint, homotopy continuation methods provide one attractive means of dealing with difficult non-convex penalty functions and the associated constellation of local minima [3]. The basic idea is to use a parameterized family of sparsity-promoting functions g(x; θ), where different values of θ determine the relative degree of concavity allowing a transition from something convex such as the 1 norm (with θ large) to something concave such as the 0 norm (with θ small). Moreover, to ensure cost function descent (see below), we also require that g(x; θ2) ≥ g(x; θ1) whenever θ2 ≥ θ1, noting that this rules out simply setting θ = p and using the family of p norms. We then begin optimization with a large θ value; later as the estimation progresses and hopefully we are near a reasonably good basin of attraction, θ is reduced introducing greater concavity, a process which is repeated until convergence, all the while guaranteeing cost function descent. While potentially effective in practice, homotopy continuation methods require both a trade-off parameter for g(x; θ) and a pre-defined schedule or heuristic for adjusting θ, both of which could potentially be image dependent.\nThe proposed deblurring algorithm automatically implements a form of noise-dependent, parameterfree homotopy continuation with several attractive auxiliary properties [26]. To make this claim precise and facilitate subsequent analysis, we first introduce the definition of relative concavity [19]:\nDefinition 1 Let u be a strictly increasing function on [a, b]. The function ν is concave relative to u on the interval [a, b] if and only if ν(y) ≤ ν(x) + ν′(x)u′(x) [u(y) − u(x)] holds ∀x, y ∈ [a, b].\nWe will use ν ≺ u to denote that ν is concave relative to u on [0,∞). This can be understood as a natural generalization of the traditional notion of a concavity, in that a concave function is equivalently concave relative to a linear function per Definition 1. In general, if ν ≺ u, then when ν and u are set to have the same functional value and the same slope at any given point (i.e., by an affine transformation of u), then ν lies completely under u. In the context of homotopy continuation, an ideal candidate penalty would be one for which g(x; θ1) ≺ g(x; θ2) whenever θ1 ≤ θ2. This would ensure that greater sparsity-inducing concavity is introduced as θ is reduced. We now demonstrate that ψ(|z|, λ) is such a function, with λ occupying the role of θ. This dependency on the noise parameter is unlike other continuation methods and ultimately leads to several attractive attributes.\nTheorem 1 If λ1 < λ2, then ψ(u, λ1) ≺ ψ(u, λ2) for u ≥ 0. Additionally, in the limit as λ → 0, then ∑ i ψ(|zi|, λ) converges to the 0 norm (up to an inconsequential scaling and translation).\nConversely, as λ becomes large, ∑ i ψ(|zi|, λ) converges to 2‖z‖1/ √ λ.\nThe proof has been deferred to the supplementary file. The relevance of this result can be understood as follows. First, at the beginning of the optimization process λ will be large both because of initialization and because we have not yet found a relatively sparse z and associated w such that y can be well-approximated; hence the estimated λ should not be small. Based on Theorem 1, in this regime (8) approaches\nmin z\n‖y − H̃z‖22 + 2 √ λ‖z‖1 (10)\nassuming w and λ are fixed. Note incidentally that this square-root dependency on λ, which arises naturally from our model, is frequently advocated when performing regular 1-norm penalized sparse regression given that the true noise variance is λ [2]. Additionally, because λ must be relatively large to arrive at this 1 approximation, the estimation need only focus on reproducing the largest elements in z since the sparse penalty will dominate the data fit term. Furthermore, these larger elements are on average more likely to be in regions of relatively lower blurring or high ‖w̄i‖2 value by virtue of the reparameterization z i = xi‖w̄i‖2. Consequently, the less concave initial estimation can proceed successfully by de-emphasizing regions with high blur or low ‖ w̄i‖2, and focusing on coarsely approximating regions with relatively less blur.\n2Note that even if the true sharp image is not exactly sparse, as long as it can be reasonably wellapproximated by some exactly sparse image in an 2 norm sense, then the analysis here still holds [27].\nLater as the estimation proceeds and w and z are refined, λwill be reduced which in turn necessarily increases the relative concavity of the penalty ψ per Theorem 1. However, the added concavity will now be welcome for resolving increasingly fine details uncovered by a lower noise variance and the concomitant boosted importance of the data fidelity term, especially since many of these uncovered details may reside near increasingly blurry regions of the image and we need to avoid unwanted noblur solutions. Eventually the penalty can even approach the 0 norm (although images are generally not exactly sparse, and other noise factors and unmodeled artifacts are usually present such that λ will never go all the way to zero). Importantly, all of this implicit, spatially-adaptive penalization occurs without the need for trade-off parameters or additional structure selection measures, meaning carefully engineered heuristics designed to locate prominent edges such that good global solutions can be found without strongly concave image penalties [21, 5, 28, 8, 9]. Figure 1 displays results of this procedure both with and without the spatially-varying column normalizations and the implicit adaptive penalization that help compensate for locally varying image blur."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "This section compares the proposed method with several state-of-the-art algorithms for non-uniform blind deblurring using real-world images from previously published papers (note that source code is not available for conducting more widespread evaluations with most algorithms). The supplementary file contains a number of additional comparisons, including assessments with a benchmark uniform blind deblurring dataset where ground truth is available. Overall, our algorithm consistently performs comparably or better on all of these respective images. Experimental specifics of our implementation (e.g., regarding the non-blind deblurring step, projection operators, etc.) are also contained in the supplementary file for space considerations.\nComparison with Harmeling et al. [8] and Hirsch et al. [9]: Results are based on three test images provided in [8]. Figure 2 displays deblurring comparisons based on the Butchershop and Vintage-car images. In both cases, the proposed algorithm reveals more fine details than the other methods, despite its simplicity and lack of salient structure selection heuristics or trade-off parameters. Note that with these images, ground truth blur kernels were independently estimated using a special capturing process [8]. As shown in the supplementary file, the estimated blur kernel patterns obtained from our algorithm better resemble the ground truth relative to the other methods, a performance result that compensates for any differences in the non-blind step.\nComparison with Whyte et al. [25]: Results on the Pantheon test image from [25] are shown in Figure 3 (top row), where we observe that the deblurred image from Whyte et al. has noticeable ringing artifacts. In contrast, our result is considerably cleaner.\nComparison with Gupta et al. [7]: We next experiment using the test image Building from [7], which contains large rotational blurring that can be challenging for blind deblurring algorithms. Figure 3 (middle row) reveals that our algorithm contains less ringing and more fine details relative to Gupta et al.\nComparison with Joshi et al. [13]: Joshi et al. presents a deblurring algorithm that relies upon additional hardware for estimating camera motion [13]. However, even without this additional in-\nformation, our algorithm produces a better sharp estimate of the Sculpture image from [13], with fewer ringing artifacts and higher resolution details. See Figure 3 (bottom row)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper presents a strikingly simple yet effective method for non-uniform camera shake removal based upon a principled, transparent cost function that is open to analysis and further extensions/refinements. For example, it can be combined with the model from [29] to perform joint multi-image alignment, denoising, and deblurring. Both theoretical and empirical evidence are provided demonstrating the efficacy of the blur-dependent, spatially-adaptive sparse regularization which emerges from our model. The framework also suggests exploring other related cost functions that, while deviating from the original probabilistic script, nonetheless share similar properties. One such simple example is a penalty of the form ∑ i log( √ λ+ |xi|‖w̄i‖2); many others are possible."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by National Natural Science Foundation of China (61231016)."
    } ],
    "references" : [ {
      "title" : "Bayesian blind deconvolution with general sparse image priors",
      "author" : [ "S.D. Babacan", "R. Molina", "M.N. Do", "A.K. Katsaggelos" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Iteratively reweighted algorithms for compressive sensing",
      "author" : [ "R. Chartrand", "W. Yin" ],
      "venue" : "In ICASSP,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Registration based non-uniform motion deblurring",
      "author" : [ "S. Cho", "H. Cho", "Y.-W. Tai", "S. Lee" ],
      "venue" : "Comput. Graph. Forum,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Fast motion deblurring",
      "author" : [ "S. Cho", "S. Lee" ],
      "venue" : "In SIGGRAPH ASIA,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Removing camera shake from a single photograph",
      "author" : [ "R. Fergus", "B. Singh", "A. Hertzmann", "S.T. Roweis", "W.T. Freeman" ],
      "venue" : "In SIGGRAPH,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Single image deblurring using motion density functions",
      "author" : [ "A. Gupta", "N. Joshi", "C.L. Zitnick", "M. Cohen", "B. Curless" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Space-variant single-image blind deconvolution for removing camera shake",
      "author" : [ "S. Harmeling", "M. Hirsch", "B. Schölkopf" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Fast removal of non-uniform camera shake",
      "author" : [ "M. Hirsch", "C.J. Schuler", "S. Harmeling", "B. Schölkopf" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Efficient filter flow for space-variant multiframe blind deconvolution",
      "author" : [ "M. Hirsch", "S. Sra", "B. Scholkopf", "S. Harmeling" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Fast non-uniform deblurring using constrained camera pose subspace",
      "author" : [ "Z. Hu", "M.-H. Yang" ],
      "venue" : "In BMVC,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "A two-stage approach to blind spatially-varying motion deblurring",
      "author" : [ "H. Ji", "K. Wang" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Image deblurring using inertial measurement sensors",
      "author" : [ "N. Joshi", "S.B. Kang", "C.L. Zitnick", "R. Szeliski" ],
      "venue" : "In ACM SIGGRAPH,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Blind deconvolution using a normalized sparsity measure",
      "author" : [ "D. Krishnan", "T. Tay", "R. Fergus" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Deconvolution using natural image priors",
      "author" : [ "A. Levin", "R. Fergus", "F. Durand", "W.T. Freeman" ],
      "venue" : "Technical report,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Efficient marginal likelihood optimization in blind deconvolution",
      "author" : [ "A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "Understanding blind deconvolution algorithms",
      "author" : [ "A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Restoring images degraded by spatially variant blur",
      "author" : [ "J.G. Nagy", "D.P. O’Leary" ],
      "venue" : "SIAM J. Sci. Comput.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Relatve convexity",
      "author" : [ "J.A. Palmer" ],
      "venue" : "Technical report, UCSD,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2003
    }, {
      "title" : "Variational EM algorithms for non-Gaussian latent variable models",
      "author" : [ "J.A. Palmer", "D.P. Wipf", "K. Kreutz-Delgado", "B.D. Rao" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "High-quality motion deblurring from a single image",
      "author" : [ "Q. Shan", "J. Jia", "A. Agarwala" ],
      "venue" : "In SIGGRAPH,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Image Restoration: Fundamentals and Advances",
      "author" : [ "M. Sorel", "F. Sroubek" ],
      "venue" : "CRC Press,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Richardson-Lucy deblurring for scenes under a projective motion path",
      "author" : [ "Y.-W. Tai", "P. Tan", "M.S. Brown" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell.,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Sparse bayesian learning and the relevance vector machine",
      "author" : [ "M.E. Tipping" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2001
    }, {
      "title" : "Non-uniform deblurring for shaken images",
      "author" : [ "O. Whyte", "J. Sivic", "A. Zisserman", "J. Ponce" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Latent variable Bayesian models for promoting sparsity",
      "author" : [ "D.P. Wipf", "B.D. Rao", "S.S. Nagarajan" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    }, {
      "title" : "Revisiting Bayesian blind deconvolution",
      "author" : [ "D.P. Wipf", "H. Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Two-phase kernel estimation for robust motion deblurring",
      "author" : [ "L. Xu", "J. Jia" ],
      "venue" : "In ECCV,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "Multi-image blind deblurring using a coupled adaptive sparse prior",
      "author" : [ "H. Zhang", "D.P. Wipf", "Y. Zhang" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 19,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 15,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 3,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 26,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 12,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 0,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 25,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 27,
      "context" : "Extensive efforts have been devoted to the uniform blur (shift-invariant) case, which can be described with the convolutional model y = k ∗ x + n, where x is the unknown sharp image, y is the observed blurry image, k is the unknown blur kernel (or point spread function), and n is a zero-mean Gaussian noise term [6, 21, 17, 5, 28, 14, 1, 27, 29].",
      "startOffset" : 313,
      "endOffset" : 346
    }, {
      "referenceID" : 15,
      "context" : "Unfortunately, many real-world photographs contain blur effects that vary across the image plane, such as when unknown rotations are introduced by camera shake [17].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "More recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12].",
      "startOffset" : 278,
      "endOffset" : 306
    }, {
      "referenceID" : 5,
      "context" : "More recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12].",
      "startOffset" : 278,
      "endOffset" : 306
    }, {
      "referenceID" : 6,
      "context" : "More recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12].",
      "startOffset" : 278,
      "endOffset" : 306
    }, {
      "referenceID" : 7,
      "context" : "More recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12].",
      "startOffset" : 278,
      "endOffset" : 306
    }, {
      "referenceID" : 9,
      "context" : "More recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12].",
      "startOffset" : 278,
      "endOffset" : 306
    }, {
      "referenceID" : 2,
      "context" : "More recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12].",
      "startOffset" : 278,
      "endOffset" : 306
    }, {
      "referenceID" : 20,
      "context" : "More recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12].",
      "startOffset" : 278,
      "endOffset" : 306
    }, {
      "referenceID" : 10,
      "context" : "More recently, algorithms have been generalized to explicitly handle some degree of non-uniform blur using the more general observation model y = Hx+n, where each column of the blur operator H contains the spatially-varying effective blur kernel at the corresponding pixel site [25, 7, 8, 9, 11, 4, 22, 12].",
      "startOffset" : 278,
      "endOffset" : 306
    }, {
      "referenceID" : 21,
      "context" : "cialized hardware such as high-speed video capture [23] or inertial measurement sensors [13] for estimating motion, or else multiple images of the same scene [4].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "cialized hardware such as high-speed video capture [23] or inertial measurement sensors [13] for estimating motion, or else multiple images of the same scene [4].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "cialized hardware such as high-speed video capture [23] or inertial measurement sensors [13] for estimating motion, or else multiple images of the same scene [4].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : "Secondly, even the algorithms that operate given only data from a single image typically rely on carefully engineered initializations, heuristics, and trade-off parameters for selecting salient image structure or edges, in part to avoid undesirable degenerate, no-blur solutions [7, 8, 9, 11].",
      "startOffset" : 279,
      "endOffset" : 292
    }, {
      "referenceID" : 6,
      "context" : "Secondly, even the algorithms that operate given only data from a single image typically rely on carefully engineered initializations, heuristics, and trade-off parameters for selecting salient image structure or edges, in part to avoid undesirable degenerate, no-blur solutions [7, 8, 9, 11].",
      "startOffset" : 279,
      "endOffset" : 292
    }, {
      "referenceID" : 7,
      "context" : "Secondly, even the algorithms that operate given only data from a single image typically rely on carefully engineered initializations, heuristics, and trade-off parameters for selecting salient image structure or edges, in part to avoid undesirable degenerate, no-blur solutions [7, 8, 9, 11].",
      "startOffset" : 279,
      "endOffset" : 292
    }, {
      "referenceID" : 9,
      "context" : "Secondly, even the algorithms that operate given only data from a single image typically rely on carefully engineered initializations, heuristics, and trade-off parameters for selecting salient image structure or edges, in part to avoid undesirable degenerate, no-blur solutions [7, 8, 9, 11].",
      "startOffset" : 279,
      "endOffset" : 292
    }, {
      "referenceID" : 16,
      "context" : "The resulting algorithm has been adopted extensively [18, 8, 22, 12] and admits an efficient implementation called efficient filter flow (EFF) [10].",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "The resulting algorithm has been adopted extensively [18, 8, 22, 12] and admits an efficient implementation called efficient filter flow (EFF) [10].",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 20,
      "context" : "The resulting algorithm has been adopted extensively [18, 8, 22, 12] and admits an efficient implementation called efficient filter flow (EFF) [10].",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "The resulting algorithm has been adopted extensively [18, 8, 22, 12] and admits an efficient implementation called efficient filter flow (EFF) [10].",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "The resulting algorithm has been adopted extensively [18, 8, 22, 12] and admits an efficient implementation called efficient filter flow (EFF) [10].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "In contrast, to explicitly account for camera motion, the projective motion path (PMP) model [23] treats a blurry image as the weighted summation of projectively transformed sharp images, leading to the revised observation model",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "In this regard, (1) represents a more general model that has been used in many recent non-uniform deblurring efforts [23, 25, 7, 11, 4].",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 23,
      "context" : "In this regard, (1) represents a more general model that has been used in many recent non-uniform deblurring efforts [23, 25, 7, 11, 4].",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "In this regard, (1) represents a more general model that has been used in many recent non-uniform deblurring efforts [23, 25, 7, 11, 4].",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "In this regard, (1) represents a more general model that has been used in many recent non-uniform deblurring efforts [23, 25, 7, 11, 4].",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "In this regard, (1) represents a more general model that has been used in many recent non-uniform deblurring efforts [23, 25, 7, 11, 4].",
      "startOffset" : 117,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "However, EFF can be combined with the PMP model by introducing a set of basis images efficiently generated by transforming a grid of delta peak images [9].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "The computational cost can be further reduced by using an active set for pruning out the projection operators with small responses [11].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "Following previous work [6, 16], we will work in the derivative domain of images for ease of modeling and better performance, meaning that x ∈ R and y ∈ R will denote the lexicographically ordered sharp and blurry image derivatives respectively.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 14,
      "context" : "Following previous work [6, 16], we will work in the derivative domain of images for ease of modeling and better performance, meaning that x ∈ R and y ∈ R will denote the lexicographically ordered sharp and blurry image derivatives respectively.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 22,
      "context" : "This type II maximum likelihood procedure has been advocated in the context of sparse estimation, where the goal is to learn vectors with mostly zero-valued coefficients [24, 26].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 24,
      "context" : "This type II maximum likelihood procedure has been advocated in the context of sparse estimation, where the goal is to learn vectors with mostly zero-valued coefficients [24, 26].",
      "startOffset" : 170,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "Clearly (4) does not resemble the traditional blind non-uniform deblurring script, where estimation proceeds using the more transparent penalized regression model [4, 7, 9]",
      "startOffset" : 163,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "Clearly (4) does not resemble the traditional blind non-uniform deblurring script, where estimation proceeds using the more transparent penalized regression model [4, 7, 9]",
      "startOffset" : 163,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "Clearly (4) does not resemble the traditional blind non-uniform deblurring script, where estimation proceeds using the more transparent penalized regression model [4, 7, 9]",
      "startOffset" : 163,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "This result, which can be generalized with additional effort, can be shown by expanding on some of the derivations in [26].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 24,
      "context" : ", and [26] contains complementary results in the case where H is fixed.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 24,
      "context" : "However, using the framework from [26], it can be shown that the kernel estimate obtained by this process is formally equivalent to the one obtained via",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "The underlying procedure is related to variational Bayesian (VB) models from [1, 16, 20]; however, these models are based on a completely different mean-field approximation and a uniform blur assumption, and they do not learn the noise parameter.",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 14,
      "context" : "The underlying procedure is related to variational Bayesian (VB) models from [1, 16, 20]; however, these models are based on a completely different mean-field approximation and a uniform blur assumption, and they do not learn the noise parameter.",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 18,
      "context" : "The underlying procedure is related to variational Bayesian (VB) models from [1, 16, 20]; however, these models are based on a completely different mean-field approximation and a uniform blur assumption, and they do not learn the noise parameter.",
      "startOffset" : 77,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "Moreover, it can be shown that this ψ is a concave, non-decreasing function of |z|, and hence represents a canonical sparsity-promoting penalty function with respect to z [26].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "The rationalization for using such potentially problematic non-convexity is as follows; more details can be found in [17, 27].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "The rationalization for using such potentially problematic non-convexity is as follows; more details can be found in [17, 27].",
      "startOffset" : 117,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "From an implementational standpoint, homotopy continuation methods provide one attractive means of dealing with difficult non-convex penalty functions and the associated constellation of local minima [3].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 24,
      "context" : "The proposed deblurring algorithm automatically implements a form of noise-dependent, parameterfree homotopy continuation with several attractive auxiliary properties [26].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 17,
      "context" : "To make this claim precise and facilitate subsequent analysis, we first introduce the definition of relative concavity [19]:",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 25,
      "context" : "Note that even if the true sharp image is not exactly sparse, as long as it can be reasonably wellapproximated by some exactly sparse image in an 2 norm sense, then the analysis here still holds [27].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 19,
      "context" : "Importantly, all of this implicit, spatially-adaptive penalization occurs without the need for trade-off parameters or additional structure selection measures, meaning carefully engineered heuristics designed to locate prominent edges such that good global solutions can be found without strongly concave image penalties [21, 5, 28, 8, 9].",
      "startOffset" : 321,
      "endOffset" : 338
    }, {
      "referenceID" : 3,
      "context" : "Importantly, all of this implicit, spatially-adaptive penalization occurs without the need for trade-off parameters or additional structure selection measures, meaning carefully engineered heuristics designed to locate prominent edges such that good global solutions can be found without strongly concave image penalties [21, 5, 28, 8, 9].",
      "startOffset" : 321,
      "endOffset" : 338
    }, {
      "referenceID" : 26,
      "context" : "Importantly, all of this implicit, spatially-adaptive penalization occurs without the need for trade-off parameters or additional structure selection measures, meaning carefully engineered heuristics designed to locate prominent edges such that good global solutions can be found without strongly concave image penalties [21, 5, 28, 8, 9].",
      "startOffset" : 321,
      "endOffset" : 338
    }, {
      "referenceID" : 6,
      "context" : "Importantly, all of this implicit, spatially-adaptive penalization occurs without the need for trade-off parameters or additional structure selection measures, meaning carefully engineered heuristics designed to locate prominent edges such that good global solutions can be found without strongly concave image penalties [21, 5, 28, 8, 9].",
      "startOffset" : 321,
      "endOffset" : 338
    }, {
      "referenceID" : 7,
      "context" : "Importantly, all of this implicit, spatially-adaptive penalization occurs without the need for trade-off parameters or additional structure selection measures, meaning carefully engineered heuristics designed to locate prominent edges such that good global solutions can be found without strongly concave image penalties [21, 5, 28, 8, 9].",
      "startOffset" : 321,
      "endOffset" : 338
    }, {
      "referenceID" : 7,
      "context" : "[9]: Results are based on three test images provided in [8].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[9]: Results are based on three test images provided in [8].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "Note that with these images, ground truth blur kernels were independently estimated using a special capturing process [8].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 23,
      "context" : "[25]: Results on the Pantheon test image from [25] are shown in Figure 3 (top row), where we observe that the deblurred image from Whyte et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25]: Results on the Pantheon test image from [25] are shown in Figure 3 (top row), where we observe that the deblurred image from Whyte et al.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "[7]: We next experiment using the test image Building from [7], which contains large rotational blurring that can be challenging for blind deblurring algorithms.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7]: We next experiment using the test image Building from [7], which contains large rotational blurring that can be challenging for blind deblurring algorithms.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "presents a deblurring algorithm that relies upon additional hardware for estimating camera motion [13].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "Comparison with Harmeling [8] and Hirsch [9] on real-world images.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "Comparison with Harmeling [8] and Hirsch [9] on real-world images.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 23,
      "context" : "Comparison with Whyte [25], Gupta [7], and Joshi [13] on real-world images.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "Comparison with Whyte [25], Gupta [7], and Joshi [13] on real-world images.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : "Comparison with Whyte [25], Gupta [7], and Joshi [13] on real-world images.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : "formation, our algorithm produces a better sharp estimate of the Sculpture image from [13], with fewer ringing artifacts and higher resolution details.",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 27,
      "context" : "For example, it can be combined with the model from [29] to perform joint multi-image alignment, denoising, and deblurring.",
      "startOffset" : 52,
      "endOffset" : 56
    } ],
    "year" : 2013,
    "abstractText" : "Typical blur from camera shake often deviates from the standard uniform convolutional assumption, in part because of problematic rotations which create greater blurring away from some unknown center point. Consequently, successful blind deconvolution for removing shake artifacts requires the estimation of a spatiallyvarying or non-uniform blur operator. Using ideas from Bayesian inference and convex analysis, this paper derives a simple non-uniform blind deblurring algorithm with a spatially-adaptive image penalty. Through an implicit normalization process, this penalty automatically adjust its shape based on the estimated degree of local blur and image structure such that regions with large blur or few prominent edges are discounted. Remaining regions with modest blur and revealing edges therefore dominate on average without explicitly incorporating structureselection heuristics. The algorithm can be implemented using an optimization strategy that is virtually tuning-parameter free and simpler than existing methods, and likely can be applied in other settings such as dictionary learning. Detailed theoretical analysis and empirical comparisons on real images serve as validation.",
    "creator" : null
  }
}