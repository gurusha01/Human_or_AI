{
  "name" : "0ff39bbbf981ac0151d340c9aa40e63e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Novel Two-Step Method for Cross Language Representation Learning",
    "authors" : [ "Min Xiao", "Yuhong Guo" ],
    "emails" : [ "minxiao@temple.edu", "yuhong@temple.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Cross language text classification is an important natural language processing task that exploits a large amount of labeled documents in an auxiliary source language to train a classification model for classifying documents in a target language where labeled data is scarce. An effective cross language learning system can greatly reduce the manual annotation effort in the target language for learning good classification models. Previous work in the literature has demonstrated successful performance of cross language learning systems on various cross language text classification problems, including multilingual document categorization [2], cross language fine-grained genre classification [14], and cross-lingual sentiment classification [18, 16].\nThe challenge of cross language text classification lies in the language barrier. That is documents in different languages are expressed with different word vocabularies and thus have disjoint feature spaces. A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].\nIn this paper, we propose a two-step learning method to induce cross-lingual feature representations for cross language text classification by exploiting a set of unlabeled parallel bilingual documents. First we construct a concatenated bilingual document-term matrix where each document is represented in the concatenated vocabulary of two languages. In such a matrix, a pair of parallel\ndocuments are represented as a row vector filled with observed word features from both the source language domain and the target language domain, while a non-parallel document in a single language is represented as a row vector filled with observed word features only from its own language and has missing values for the word features from the other language. We then learn the unobserved feature entries of this sparse matrix by formulating a matrix completion problem and solving it using a projected gradient descent optimization algorithm. By doing so, we expect to automatically capture important and robust low-rank information based on the word co-occurrence patterns expressed both within each language and across languages. Next we perform latent semantic indexing over the recovered document-term matrix and induce a low-dimensional dense cross-lingual representation of the documents, on which standard monolingual classifiers can be applied. To evaluate the effectiveness of the proposed learning method, we conduct a set of experiments with cross language sentiment classification tasks on multilingual Amazon product reviews. The empirical results show that the proposed method significantly outperforms a number of cross language learning methods. Moreover, the proposed method produces good performance even with a very small number of unlabeled parallel bilingual documents."
    }, {
      "heading" : "2 Related Work",
      "text" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12]. For example, [17] proposed an expectation-maximization based self-training method, which first initializes a monolingual classifier in the target language with the translated labeled documents from the source language and then retrains the model by adding unlabeled documents from the target language with automatically predicted labels. [21] proposed an instance and feature bi-weighting method by first translating documents from one language domain to the other one and then simultaneously re-weighting instances and features to address the distribution difference across domains. [22] proposed to use the co-training method for cross language sentiment classification on parallel corpora. [2] proposed a multi-view majority voting method to categorize documents in multiple views produced from machine translation tools. [1] proposed a multi-view co-classification method for multilingual document categorization, which minimizes both the training loss for each view and the prediction disagreement between different language views. Our proposed approach in this paper shares similarity with these approaches in exploiting parallel data produced by machine translation tools. But our approach only requires a small set of unlabeled parallel documents, while these approaches require at least translating all the training documents in one language domain.\nAnother important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14]. [16] proposed a cross-language structural correspondence learning method to induce language-independent features by using pivot word pairs produced by word translation oracles. [10] proposed a cross-language latent semantic indexing (CL-LSI) method to induce cross-lingual representations by performing LSI over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text. [20] proposed a cross-lingual kernel canonical correlation analysis (CL-KCCA) method. It first learns two projections (one for each language) by conducting kernel canonical correlation analysis over a paired bilingual corpus and then uses them to project documents from language-specific feature spaces to the shared multilingual semantic feature space. [15] employed cross-lingual oriented principal component analysis (CL-OPCA) over concatenated parallel documents to learn a multilingual projection by simultaneously minimizing the projected distance between parallel documents and maximizing the projected covariance of documents across languages. Some other work uses multilingual topic models such as the coupled probabilistic latent semantic analysis and the bilingual latent Dirichlet allocation to extract latent cross-lingual topics as interlingual representations [19]. [14] proposed to use language-specific part-of-speech (POS) taggers to tag each word and then map those language-specific POS tags to twelve universal POS tags as interlingual features for cross language fine-grained genre classification. Similar to the multilingual semantic representation learning approaches such as CL-LSI, CL-KCCA and CL-OPCA, our two-step learning method exploits parallel documents. But different from these methods which apply operations such as LSI, KCCA, and OPCA directly on the original concatenated document-\nterm matrix, our method first fills the missing entries of the document-term matrix using matrix completion, and then performs LSI over the recovered low-rank matrix."
    }, {
      "heading" : "3 Approach",
      "text" : "In this section, we present the proposed two-step learning method for learning cross-lingual document representations. We assume a subset of unlabeled parallel documents from the two languages are given, which can be used to capture the co-occurrence of terms across languages and build connections between the vocabulary sets of the two languages. We first construct a unified documentterm matrix for all documents from the auxiliary source language domain and the target language domain, whose columns correspond to the word features from the unified vocabulary set of the two languages. In this matrix, each pair of parallel documents is represented as a fully observed row vector, and each non-parallel document is represented as a partially observed row vector where only entries corresponding to words in its own language vocabulary are observed. Instead of learning a low-dimensional cross-lingual document representation from this matrix directly, we perform a twostep learning procedure: First we learn a low-rank document-term matrix by automatically filling the missing entries via matrix completion. Next we produce cross-lingual representations by applying the latent semantic indexing method over the learned matrix.\nLet M0 ∈ Rt×d be the unified document-term matrix, which is partially filled with observed nonnegative feature values, where t is the number of documents and d is the size of the unified vocabulary. We use Ω to denote the index set of the observed features in M0, such that (i, j) ∈ Ω if only if M0ij is observed; and use Ω̂ to denote the index set of the missing features in M0, such that (i, j) ∈ Ω̂ if only if M0ij is unobserved. For the i-th document in the data set from one language, if the document does not have a parallel translation in the other language, then all the features in row M0i: corresponding to the words in the vocabulary of the other language are viewed as missing features."
    }, {
      "heading" : "3.1 Matrix Completion",
      "text" : "Note that the document-term matrix M0 has a large fraction of missing features and the only bridge between the vocabulary sets of the two languages is the small set of parallel bilingual documents. Learning from this partially observed matrix directly by treating missing features as zeros certainly will lose a lot of information. On the other hand, a fully observed document-term matrix is naturally low-rank and sparse, as the vocabulary set is typically very large and each document only contains a small fraction of the words in the vocabulary. Thus we propose to automatically fill the missing entries of M0 based on the feature co-occurrence information expressed in the observed data, by conducting matrix completion to recover a low-rank and sparse matrix. Specifically, we formulate the matrix completion as the following optimization problem\nmin M\nrank(M) + µ‖M‖1 subject to Mij = M 0 ij , ∀(i, j) ∈ Ω; Mij ≥ 0, ∀(i, j) ∈ Ω̂ (1)\nwhere ‖ · ‖1 denotes a ℓ1 norm and is used to enforce sparsity. The rank function however is nonconvex and difficult to optimize. We can relax it to its convex envelope, a convex trace norm ‖M‖∗. Moreover, instead of using the equality constraints in (1), we propose to minimize a regularization loss function, c(Mij ,M0ij), to cope with observation noise for all the observed feature entries.\nMeanwhile, we also add regularization terms over the missing features, c(Mij , 0), ∀(i, j) ∈ Ω̂, to avoid overfitting. In particular, we use the least squared loss function c(x, y) = 12‖x − y‖\n2. Hence we obtain the following relaxed convex optimization problem for matrix completion\nmin M\nγ‖M‖∗ + µ‖M‖1 + ∑\n(i,j)∈Ω\nc(Mij ,M 0 ij) + ρ\n∑\n(i,j)∈Ω̂\nc(Mij , 0) subject to M ≥ 0 (2)\nWith nonnegativity constraints M ≥ 0, the non-smooth ℓ1 norm regularizer in the objective function of (2) is equivalent to a smooth linear function ‖M‖1 = ∑ ij Mij . Nevertheless, with the nonsmooth trace norm ‖M‖∗, the optimization problem (2) remains to be convex but non-smooth. Moreover, the matrix M in cross-language learning tasks is typically very large, and thus a scalable optimization algorithm needs to be developed to conduct efficient optimization. In next section, we will present a scalable projected gradient descent algorithm to solve this minimization problem.\nAlgorithm 1 Projected Gradient Descent Algorithm\nInput: M0, γ, ρ ≤ 1, 0 < τ < min(2, 2 ρ ), µ. Initialize M as the nonnegative projection of the rank-1 approximation of M0. while not converged do\n1. gradient descent: M = M − τ∇g(M). 2. shrink: M = Sτγ(M). 3. project onto feasible set: M = max(M, 0).\nend while"
    }, {
      "heading" : "3.2 Latent Semantic Indexing",
      "text" : "After solving (2) for an optimal low-rank solution M∗, we can use each row of the sparse matrix M∗ as a vector representation for each document in the concatenated vocabulary space of the two languages. However exploiting such a matrix representation directly for cross language text classification lacks sufficient capacity of handling feature noise and sparseness, as each document is represented using a very small set of words in the vocabulary set. We thus propose to apply a latent semantic indexing (LSI) method on M∗ to produce a low-dimensional semantic representation of the data. LSI uses singular value decomposition to discover the important associative relationships of word features [10], and create a reduced-dimension feature space. Specifically, we first perform singular value decomposition over M∗, M∗ = USV ⊤, and then obtain a low dimensional representation matrix Z via a projection Z = M∗Vk, where Vk contains the top k right singular vectors of M∗. Cross-language text classification can then be conducted over Z using monolingual classifiers."
    }, {
      "heading" : "4 Optimization Algorithm",
      "text" : ""
    }, {
      "heading" : "4.1 Projected Gradient Descent Algorithm",
      "text" : "A number of algorithms have been developed to solve matrix completion problems in the literature [4, 11]. We use a projected gradient descent algorithm to solve the non-smooth convex optimization problem in (2). This algorithm takes the objective function f(M) in (2) as a composition of a non-smooth term and a convex smooth term such as f(M) = γ‖M‖∗ + g(M) where\ng(M) = µ‖M‖1 + ∑\n(i,j)∈Ω\nc(Mij ,M 0 ij) + ρ\n∑\n(i,j)∈Ω̂\nc(Mij , 0). (3)\nIt first initializes M as the nonnegative projection of the rank-1 approximation of M0, and then iteratively updates M using a projected gradient descent procedure. In each iteration, we perform three steps to update M . First, we take a gradient descent step M = M − τ∇g(M) with stepsize τ and gradient function\n∇g(M) = µE + (M −M0) ◦ Y + ρM ◦ Ŷ (4)\nwhere E is a t × d matrix with all 1s; Y and Ŷ are t × d indicator matrices such that Yij = 1 if and only if (i, j) ∈ Ω and Ŷ = E − Y ; and “◦” denotes the Hadamard product. Next we perform a shrinkage operation M = Sν(M) over the resulting matrix from the first step to minimize its rank. The shrinkage operator is based on singular value decomposition\nSν(M) = UΣ(ν)V ⊤, M = UΣV ⊤, Σ(ν) = max(Σ− ν, 0), (5)\nwhere ν = τγ. Finally we project the resulting matrix into the nonnegative feasible set by M = max(M, 0). This update procedure provably converges to an optimal solution. The overall algorithm is given in Algorithm 1."
    }, {
      "heading" : "4.2 Convergence Analysis",
      "text" : "Let h(·) = I(·) − τ∇g(·) be the gradient descent operator used in the gradient descent step, and let PC(·) = max(·, 0) be the projection operator, while Sν(·) is the shrinkage operator. Below we prove the convergence of the projected gradient descent algorithm.\nLemma 1. Let E be a t×d matrix with all 1s, and Q = E−τ(Y +ρŶ ). For τ ∈ (0,min(2, 2 ρ )), the operator h(·) is non-expansive, i.e., for any M and M ′ ∈ Rt×d, ‖h(M)−h(M ′)‖F ≤ ‖M−M ′‖F . Moreover, ‖h(M)− h(M ′)‖F = ‖M −M ′‖F if and only if h(M)− h(M ′) = M −M ′.\nProof. Note that for τ ∈ (0,min(2, 2 ρ )), we have −1 < Qij < 1, ∀(i, j). Then following the gradient definition in (4), we have\n‖h(M)− h(M ′)‖F = ∥∥(M −M ′) ◦Q‖F = ( ∑\nij\n(Mij −M ′ ij) 2Q2ij) 1 2 ≤ ‖M −M ′‖F\nThe inequalities become equalities if only if h(M)− h(M ′) = M −M ′.\nLemma 2. [11, Lemma 1] The shrinkage operator Sν(·) is non-expansive, i.e., for any M and M ′ ∈ Rt×d, ‖Sν(M)−Sν(M ′)‖F ≤ ‖M−M ′‖F . Moreover, ‖Sν(M)−Sν(M ′)‖F = ‖M−M ′‖F if and only if Sν(M)− Sν(M ′) = M −M ′.\nLemma 3. The projection operator PC(·) is non-expansive, i.e., ‖PC(M) − PC(M ′)‖F ≤ ‖M − M ′‖F . Moreover, ‖PC(M)−PC(M ′)‖F = ‖M−M ′‖F if and only if PC(M)−PC(M ′) = M−M ′.\nProof. For any given entry index (i, j), there are four cases:\n• Case 1: Mij ≥ 0,M ′ij ≥ 0. We have (PC(Mij)− PC(M ′ ij)) 2 = (Mij −M ′ ij) 2.\n• Case 2: Mij ≥ 0,M ′ij < 0. We have (PC(Mij)− PC(M ′ ij)) 2 = M2ij < (Mij −M ′ ij) 2.\n• Case 3: Mij < 0,M ′ij ≥ 0. We have (PC(Mij)− PC(M ′ ij)) 2 = M ′ 2 ij < (Mij −M ′ ij) 2.\n• Case 4: Mij < 0,M ′ij < 0. We have (PC(Mij)− PC(M ′ ij)) 2 = 0 ≤ (Mij −M ′ ij) 2.\nTherefore,\n‖PC(M)− PC(M ′)‖F =\n(∑\nij\n(PC(Mij)− PC(M ′ ij))\n2 ) 1 2 ≤ (∑\nij\n(Mij −M ′ ij)\n2 ) 1\n2 = ‖M −M ′‖F\nand ‖PC(M)− PC(M ′)‖F = ‖M −M ′‖F if only if PC(M)− PC(M ′) = M −M ′.\nTheorem 1. The sequence {Mk} generated by the projected gradient descent iterations in Algorithm 1 with 0 < τ < min(2, 2\nρ ) converges to M∗, which is an optimal solution of (2).\nProof. Since h(·), Sν(·) and PC(·) are all non-expansive, the composite operator PC(Sν(h(·))) is non-expansive as well. This theorem can then be proved following [11, Theorem 4]."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we evaluate the proposed two-step learning method by conducting extensive cross language sentiment classification experiments on multilingual Amazon product reviews."
    }, {
      "heading" : "5.1 Experimental Setting",
      "text" : "Dataset We used the multilingual Amazon product reviews dataset [16], which contains three categories (Books (B), DVD (D), Music (M)) of product reviews in four different languages (English (E), French (F), German (G), Japanese (J)). For each category of the product reviews, there are 2000 positive and 2000 negative English reviews, and 1000 positive and 1000 negative reviews for each of the other three languages. In addition, there are another 2000 unlabeled parallel reviews between English and each of the other three languages. Each review is preprocessed into a unigram bag-ofword feature vector with TF-IDF values. We focused on cross-lingual learning between English and the other three languages and constructed 18 cross language sentiment classification tasks (EFB, FEB, EFD, FED, EFM, FEM, EGB, GEB, EGD, GED, EGM, GEM, EJB, JEB, EJD, JED, EJM, JEM), each for one combination of selected source language, target language and category. For example, the task EFB uses English Books reviews as the source language data and uses French Books reviews as the target language data.\nApproaches We compared the proposed two-step learning (TSL) method with the following four methods: TBOW, CL-LSI, CL-OPCA and CL-KCCA. The Target Bag-Of-Word (TBOW) baseline method trains a supervised monolingual classifier in the original bag-of-word feature space with the labeled training data from the target language domain. The Cross-Lingual Latent Semantic Indexing (CL-LSI) method [10] and the Cross-Lingual Oriented Principal Component Analysis (CL-OPCA) method [15] first learn cross-lingual representations with all data from both language domains by performing LSI or OPCA and then train a monolingual classifier with labeled data from both language domains in the induced low-dimensional feature space. The Cross-Lingual Kernel Canonical Component Analysis (CL-KCCA) method [20] first induces two language projections by using unlabeled parallel data and then trains a monolingual classifier on labeled data from both language domains in the projected low-dimensional space. For all experiments, we used linear support vector machine (SVM) as the monolingual classification model. For implementation, we used the libsvm package [5] with default parameter setting."
    }, {
      "heading" : "5.2 Classification Accuracy",
      "text" : "For each of the 18 cross language sentiment classification tasks, we used all documents from the two languages and the additional 2000 unlabeled parallel documents for representation learning. Then we used all documents in the auxiliary source language and randomly chose 100 documents from the target language as labeled data for classification model training, and used the remaining data in the target language as test data. For the proposed method, TSL, we set µ = 10−6 and τ = 1, chose γ value from {0.01, 0.1, 1, 10}, chose ρ value from {10−5, 10−4, 10−3, 10−2, 10−1, 1}, and chose the dimension k value from {20, 50, 100, 200, 500}. We used the first task EFB to perform model parameter selection by running the algorithm 3 times based on random selections of 100 labeled target training data. This gave us the following parameter setting: γ = 0.1, ρ = 10−4, k = 50. We used the same procedure to select the dimensionality of the learned semantic representations for the other three approaches, CL-LSI, CL-OPCA and CL-KCCA, which produced k = 50 for CL-LSI and CL-OPCA, and k = 100 for CL-KCCA. We then used the selected model parameters for all the 18 tasks and ran each experiment for 10 times based on random selections of 100 labeled target documents. The average classification accuracies and standard deviations are reported in Table 1.\nWe can see that the proposed two-step learning method, TSL, outperforms all other four comparison methods in general. The target baseline TBOW performs poorly on all the 18 tasks, which implies that 100 labeled target training documents are far from enough to obtain a robust sentiment classifier\nin the target language domain. All the other three cross-lingual representation learning methods, CL-LSI, CL-KCCA and CL-OPCA, consistently outperform this baseline method across all the 18 tasks, which demonstrates that the labeled training data from the source language domain is useful for classifying the target language data under a unified data representation. Nevertheless, the improvements achieved by these three methods over the baseline are much smaller than the proposed TSL method. Across all the 18 tasks, TSL increases the average test accuracy over the baseline TBOW method by at least 8.59 (%) on the EJM task and up to 14.61 (%) on the EFB task. Moreover, TSL also outperforms both CL-KCCA and CL-OPCA across all the 18 tasks, outperforms CL-LSI on 17 out of the 18 tasks and achieves comparable performance with CL-LSI on the remaining one task (EJB). All these results demonstrate the efficacy and robustness of the proposed two-step representation learning method for cross language text classification."
    }, {
      "heading" : "5.3 Impact of the Size of Unlabeled Parallel Data",
      "text" : "All the four cross-lingual adaptation learning methods, CL-LSI, CL-KCCA, CL-OPCA and TSL, exploit unlabeled parallel reviews for learning cross-lingual representations. Next we investigated the performance of these methods with respect to different numbers of unlabeled parallel reviews. We tested a set of different numbers, np ∈ {200, 500, 1000, 2000}. For each number np in the set, we randomly chose np parallel documents from all the 2000 unlabeled parallel reviews to conduct experiments using the same setting from the previous experiments. Each experiment was repeated 10 times based on random selections of labeled target training data. The average test classification accuracies and standard deviations are plotted in Figure 1 and Figure 2. Figure 1 presents the results for the 9 cross-lingual classification tasks that adapt classification systems from English to French, German and Japanese, while Figure 2 presents the results for the other 9 cross-lingual classification tasks that adapt classification systems from French, German and Japanese to English.\nFrom these results, we can see that the performance of all four methods in general improves with the increase of the unlabeled parallel data. The proposed method, TSL, nevertheless outperforms the other three cross-lingual adaptation learning methods across the range of different np values for 16 out of the 18 cross language sentiment classification tasks. For the remaining two tasks, EFM and EGM, it has similar performance with the CL-KCCA method while significantly outperforming the other two methods. Moreover, for the 9 tasks that make adaptation from English to the other three languages, the TSL method achieves great performance with only 200 unlabeled parallel documents, while the performance of the other three methods decreases significantly with the decrease of the number of unlabeled parallel documents. These results demonstrate the robustness and efficacy of the proposed method, comparing to other methods."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we developed a novel two-step method to learn cross-lingual semantic data representations for cross language text classification by exploiting unlabeled parallel bilingual documents. We first formulated a matrix completion problem to infer unobserved feature values of the concatenated document-term matrix in the space of unified vocabulary set from the source and target languages. Then we performed latent semantic indexing over the completed low-rank document-term matrix to produce a low-dimensional cross-lingual representation of the documents. Monolingual classifiers were then used to conduct cross language text classification based on the learned document representation. To investigate the effectiveness of the proposed learning method, we conducted extensive experiments with tasks of cross language sentiment classification on Amazon product reviews. Our experimental results demonstrated that the proposed two-step learning method significantly outperforms the other four comparison methods. Moreover, the proposed approach needs much less parallel documents to produce a good cross language text classification system."
    } ],
    "references" : [ {
      "title" : "A co-classification approach to learning from multilingual corpora",
      "author" : [ "M. Amini", "C. Goutte" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Learning from multiple partially observed views - an application to multilingual text categorization",
      "author" : [ "M. Amini", "N. Usunier", "C. Goutte" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Cross-lingual sentiment analysis for indian languages using linked wordnets",
      "author" : [ "B.A.R", "A. Joshi", "P. Bhattacharyya" ],
      "venue" : "In Proc. of COLING,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E. Candés", "B. Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C. Chang", "C. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Translated learning: Transfer learning across different feature spaces",
      "author" : [ "W. Dai", "Y. Chen", "G. Xue", "Q. Yang", "Y. Yu" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization",
      "author" : [ "A. Gliozzo" ],
      "venue" : "In Proc. of ICCL-ACL,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Improving bilingual projections via sparse covariance matrices",
      "author" : [ "J. Jagarlamudi", "R. Udupa", "H. Daumé III", "A. Bhole" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Can chinese web pages be classified with English data source",
      "author" : [ "X. Ling", "G. Xue", "W. Dai", "Y. Jiang", "Q. Yang", "Y. Yu" ],
      "venue" : "In Proc. of WWW,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Automatic cross-language information retrieval using latent semantic indexing",
      "author" : [ "M. Littman", "S. Dumais", "T. Landauer" ],
      "venue" : "In Cross-Language Information Retrieval,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "Fixed point and bregman iterative methods for matrix rank minimization",
      "author" : [ "S. Ma", "D. Goldfarb", "L. Chen" ],
      "venue" : "Mathematical Programming: Series A and B archive,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Cross-lingual mixture model for sentiment classification",
      "author" : [ "X. Meng", "F. Wei", "X. Liu", "M. Zhou", "G. Xu", "H. Wang" ],
      "venue" : "In Proc. of ACL,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Cross-lingual sentiment classification via bi-view nonnegative matrix tri-factorization",
      "author" : [ "J. Pan", "G. Xue", "Y. Yu", "Y. Wang" ],
      "venue" : "In Proc. of PAKDD,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Label propagation for fine-grained cross-lingual genre classification",
      "author" : [ "P. Petrenz", "B. Webber" ],
      "venue" : "In Proc. of the NIPS xLiTe workshop,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "Translingual document representations from discriminative projections",
      "author" : [ "J. Platt", "K. Toutanova", "W. Yih" ],
      "venue" : "In Proc. of EMNLP,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Cross-language text classification using structural correspondence learning",
      "author" : [ "P. Prettenhofer", "B. Stein" ],
      "venue" : "In Proc. of ACL,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "An EM based training algorithm for cross-language text categorization",
      "author" : [ "L. Rigutini", "M. Maggini" ],
      "venue" : "In Proc. of the Web Intelligence Conference,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2005
    }, {
      "title" : "Mining multilingual opinions through classification and translation",
      "author" : [ "J. Shanahan", "G. Grefenstette", "Y. Qu", "D. Evans" ],
      "venue" : "In AAAI Spring Symp. on Explor. Attit. and Affect in Text,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Knowledge transfer across multilingual corpora via latent topics",
      "author" : [ "W. Smet", "J. Tang", "M. Moens" ],
      "venue" : "In Proc. of PAKDD,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Inferring a semantic representation of text via cross-language correlation analysis",
      "author" : [ "A. Vinokourov", "J. Shawe-taylor", "N. Cristianini" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Bi-weighting domain adaptation for cross-language text classification",
      "author" : [ "C. Wan", "R. Pan", "J. Li" ],
      "venue" : "In Proc. of IJCAI,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Co-training for cross-lingual sentiment classification",
      "author" : [ "X. Wan" ],
      "venue" : "In Proc. of ACL-IJCNLP,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "Cross language text categorization using a bilingual lexicon",
      "author" : [ "K. Wu", "X. Wang", "B. Lu" ],
      "venue" : "In Proc. of IJCNLP,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Previous work in the literature has demonstrated successful performance of cross language learning systems on various cross language text classification problems, including multilingual document categorization [2], cross language fine-grained genre classification [14], and cross-lingual sentiment classification [18, 16].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 13,
      "context" : "Previous work in the literature has demonstrated successful performance of cross language learning systems on various cross language text classification problems, including multilingual document categorization [2], cross language fine-grained genre classification [14], and cross-lingual sentiment classification [18, 16].",
      "startOffset" : 264,
      "endOffset" : 268
    }, {
      "referenceID" : 17,
      "context" : "Previous work in the literature has demonstrated successful performance of cross language learning systems on various cross language text classification problems, including multilingual document categorization [2], cross language fine-grained genre classification [14], and cross-lingual sentiment classification [18, 16].",
      "startOffset" : 313,
      "endOffset" : 321
    }, {
      "referenceID" : 15,
      "context" : "Previous work in the literature has demonstrated successful performance of cross language learning systems on various cross language text classification problems, including multilingual document categorization [2], cross language fine-grained genre classification [14], and cross-lingual sentiment classification [18, 16].",
      "startOffset" : 313,
      "endOffset" : 321
    }, {
      "referenceID" : 17,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 302,
      "endOffset" : 313
    }, {
      "referenceID" : 5,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 302,
      "endOffset" : 313
    }, {
      "referenceID" : 22,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 302,
      "endOffset" : 313
    }, {
      "referenceID" : 15,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 438,
      "endOffset" : 442
    }, {
      "referenceID" : 9,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 463,
      "endOffset" : 475
    }, {
      "referenceID" : 19,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 463,
      "endOffset" : 475
    }, {
      "referenceID" : 14,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 463,
      "endOffset" : 475
    }, {
      "referenceID" : 2,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 510,
      "endOffset" : 517
    }, {
      "referenceID" : 13,
      "context" : "A variety of methods have been proposed in the literature to address cross language text classification by bridging the cross language gap, including transforming the training or test data from one language domain into the other language domain by using machine translation tools or bilingual lexicons [18, 6, 23], and constructing cross-lingual representations by using readily available auxiliary resources such as bilingual word pairs [16], comparable corpora [10, 20, 15], and other multilingual resources [3, 14].",
      "startOffset" : 510,
      "endOffset" : 517
    }, {
      "referenceID" : 17,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 264,
      "endOffset" : 272
    }, {
      "referenceID" : 22,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 264,
      "endOffset" : 272
    }, {
      "referenceID" : 16,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 303,
      "endOffset" : 314
    }, {
      "referenceID" : 8,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 303,
      "endOffset" : 314
    }, {
      "referenceID" : 20,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 303,
      "endOffset" : 314
    }, {
      "referenceID" : 21,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 347,
      "endOffset" : 365
    }, {
      "referenceID" : 1,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 347,
      "endOffset" : 365
    }, {
      "referenceID" : 0,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 347,
      "endOffset" : 365
    }, {
      "referenceID" : 12,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 347,
      "endOffset" : 365
    }, {
      "referenceID" : 11,
      "context" : "Many works in the literature address cross language text classification by first translating documents from one language domain into the other one via machine translation tools or bilingual lexicons and then applying standard monolingual classification algorithms [18, 23], domain adaptation techniques [17, 9, 21], or multi-view learning methods [22, 2, 1, 13, 12].",
      "startOffset" : 347,
      "endOffset" : 365
    }, {
      "referenceID" : 16,
      "context" : "For example, [17] proposed an expectation-maximization based self-training method, which first initializes a monolingual classifier in the target language with the translated labeled documents from the source language and then retrains the model by adding unlabeled documents from the target language with automatically predicted labels.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : "[21] proposed an instance and feature bi-weighting method by first translating documents from one language domain to the other one and then simultaneously re-weighting instances and features to address the distribution difference across domains.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] proposed to use the co-training method for cross language sentiment classification on parallel corpora.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[2] proposed a multi-view majority voting method to categorize documents in multiple views produced from machine translation tools.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] proposed a multi-view co-classification method for multilingual document categorization, which minimizes both the training loss for each view and the prediction disagreement between different language views.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 19,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 14,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 18,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 7,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 2,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 230,
      "endOffset" : 237
    }, {
      "referenceID" : 13,
      "context" : "Another important group of cross language text classification methods in the literature construct cross-lingual representations by exploiting bilingual word pairs [16, 7], parallel corpora [10, 20, 15, 19, 8], and other resources [3, 14].",
      "startOffset" : 230,
      "endOffset" : 237
    }, {
      "referenceID" : 15,
      "context" : "[16] proposed a cross-language structural correspondence learning method to induce language-independent features by using pivot word pairs produced by word translation oracles.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[10] proposed a cross-language latent semantic indexing (CL-LSI) method to induce cross-lingual representations by performing LSI over a dual-language document-term matrix, where each dual-language document contains its original words and the corresponding translation text.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] proposed a cross-lingual kernel canonical correlation analysis (CL-KCCA) method.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] employed cross-lingual oriented principal component analysis (CL-OPCA) over concatenated parallel documents to learn a multilingual projection by simultaneously minimizing the projected distance between parallel documents and maximizing the projected covariance of documents across languages.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Some other work uses multilingual topic models such as the coupled probabilistic latent semantic analysis and the bilingual latent Dirichlet allocation to extract latent cross-lingual topics as interlingual representations [19].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 13,
      "context" : "[14] proposed to use language-specific part-of-speech (POS) taggers to tag each word and then map those language-specific POS tags to twelve universal POS tags as interlingual features for cross language fine-grained genre classification.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "LSI uses singular value decomposition to discover the important associative relationships of word features [10], and create a reduced-dimension feature space.",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "A number of algorithms have been developed to solve matrix completion problems in the literature [4, 11].",
      "startOffset" : 97,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "A number of algorithms have been developed to solve matrix completion problems in the literature [4, 11].",
      "startOffset" : 97,
      "endOffset" : 104
    }, {
      "referenceID" : 15,
      "context" : "Dataset We used the multilingual Amazon product reviews dataset [16], which contains three categories (Books (B), DVD (D), Music (M)) of product reviews in four different languages (English (E), French (F), German (G), Japanese (J)).",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "The Cross-Lingual Latent Semantic Indexing (CL-LSI) method [10] and the Cross-Lingual Oriented Principal Component Analysis (CL-OPCA) method [15] first learn cross-lingual representations with all data from both language domains by performing LSI or OPCA and then train a monolingual classifier with labeled data from both language domains in the induced low-dimensional feature space.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "The Cross-Lingual Latent Semantic Indexing (CL-LSI) method [10] and the Cross-Lingual Oriented Principal Component Analysis (CL-OPCA) method [15] first learn cross-lingual representations with all data from both language domains by performing LSI or OPCA and then train a monolingual classifier with labeled data from both language domains in the induced low-dimensional feature space.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 19,
      "context" : "The Cross-Lingual Kernel Canonical Component Analysis (CL-KCCA) method [20] first induces two language projections by using unlabeled parallel data and then trains a monolingual classifier on labeled data from both language domains in the projected low-dimensional space.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "For implementation, we used the libsvm package [5] with default parameter setting.",
      "startOffset" : 47,
      "endOffset" : 50
    } ],
    "year" : 2013,
    "abstractText" : "Cross language text classification is an important learning task in natural language processing. A critical challenge of cross language learning arises from the fact that words of different languages are in disjoint feature spaces. In this paper, we propose a two-step representation learning method to bridge the feature spaces of different languages by exploiting a set of parallel bilingual documents. Specifically, we first formulate a matrix completion problem to produce a complete parallel document-term matrix for all documents in two languages, and then induce a low dimensional cross-lingual document representation by applying latent semantic indexing on the obtained matrix. We use a projected gradient descent algorithm to solve the formulated matrix completion problem with convergence guarantees. The proposed method is evaluated by conducting a set of experiments with cross language sentiment classification tasks on Amazon product reviews. The experimental results demonstrate that the proposed learning method outperforms a number of other cross language representation learning methods, especially when the number of parallel bilingual documents is small.",
    "creator" : null
  }
}