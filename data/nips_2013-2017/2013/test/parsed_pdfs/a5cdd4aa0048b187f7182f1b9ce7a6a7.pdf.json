{
  "name" : "a5cdd4aa0048b187f7182f1b9ce7a6a7.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Convex Calibrated Surrogates for Low-Rank Loss Matrices with Applications to Subset Ranking Losses",
    "authors" : [ "Harish G. Ramaswamy", "Shivani Agarwal" ],
    "emails" : [ "gurup@csa.iisc.ernet.in", "shivani@csa.iisc.ernet.in", "tewaria@umich.edu", "precision@q,", "Precision@q" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "There has been much interest in recent years in understanding consistency properties of learning algorithms – particularly algorithms that minimize a surrogate loss – for a variety of finite-output learning problems, including binary classification, multiclass classification, multi-label classification, subset ranking, and others [1–17]. For algorithms minimizing a surrogate loss, the question of consistency reduces to the question of calibration of the surrogate loss with respect to the target loss of interest [5–7, 16]; in general, one is interested in convex surrogates that can be minimized efficiently. In particular, the existence (and lack thereof) of convex calibrated surrogates for various subset ranking problems, with target losses including for example the discounted cumulative gain (DCG), mean average precision (MAP), mean reciprocal rank (MRR), and pairwise disagreement (PD), has received significant attention recently [9, 11–13, 15–17].\nIn this paper, we develop a general result which allows us to give an explicit convex, calibrated surrogate defined on a low-dimensional surrogate space for any finite-output learning problem for which the loss matrix has low rank. Recently, Ramaswamy and Agarwal [16] showed the existence of such surrogates, but their result involved an unwieldy surrogate space, and moreover did not give an explicit, usable construction for the mapping needed to transform predictions in the surrogate space back to the original prediction space. Working in the same general setting as theirs, we give an explicit construction that leads to a simple least-squares type surrogate. We then apply this result to obtain several new results related to subset ranking. Specifically, we first obtain calibrated, score-based surrogates for the Precision@q loss, which includes the winner-take-all (WTA) loss as a special case, and the expected rank utility (ERU) loss; to the best of our knowledge, consistency with respect to these losses has not been studied previously in the literature. When there are r documents to be ranked for each query, the score-based surrogates operate on an r-dimensional surrogate space. We then turn to the MAP and PD losses, which are both widely used in practice, and for which it has been shown that no convex score-based surrogate can be calibrated for all probability distributions [11,15,16]. For the PD loss, Duchi et al. [11] gave certain low-noise conditions on the probability distribution under which a convex, calibrated score-based surrogate could be designed;\nwe are unaware of such a result for the MAP loss. A straightforward application of our low-rank result to these losses yields convex calibrated surrogates defined on O(r2)-dimensional surrogate spaces, but in both cases, the mapping needed to transform back to predictions in the original space involves solving a computationally hard problem. Inspired by these surrogates, we then give a convex score-based surrogate with an efficient mapping that is calibrated with respect to MAP under certain conditions on the probability distribution; this is the first such result for the MAP loss that we are aware of. We also give a family of convex score-based surrogates calibrated with the PD loss under certain noise conditions, generalizing the surrogate and conditions of Duchi et al. [11]. Finally, we give an efficient mapping for the O(r2)-dimensional surrogate for the PD loss, and show that this leads to a convex surrogate calibrated with the PD loss under a more general condition, i.e. over a larger set of probability distributions, than those associated with the score-based surrogates.\nPaper outline. We start with some preliminaries and background in Section 2. Section 3 gives our primary result, namely an explicit convex surrogate calibrated for low-rank loss matrices, defined on a surrogate space of dimension at most the rank of the matrix. Sections 4–7 then give applications of this result to the Precision@q, ERU, MAP, and PD losses, respectively. All proofs not included in the main text can be found in the appendix."
    }, {
      "heading" : "2 Preliminaries and Background",
      "text" : "Setup. We work in the same general setting as that of Ramaswamy and Agarwal [16]. There is an instance space X , a finite set of class labels Y = [n] = {1, . . . , n}, and a finite set of target labels (possible predictions) T = [k] = {1, . . . , k}. Given training examples (X1, Y1), . . . , (Xm, Ym) drawn i.i.d. from a distribution D on X×Y , the goal is to learn a prediction model h : X→T . Often, T = Y , but this is not always the case (for example, in the subset ranking problems we consider, the labels in Y are typically relevance vectors or preference graphs over a set of r documents, while the target labels in T are permutations over the r documents). The performance of a prediction model h : X→T is measured via a loss function � : Y ×T →R+ (where R+ = [0,∞)); here �(y, t) denotes the loss incurred on predicting t ∈ T when the label is y ∈ Y . Specifically, the goal is to learn a model h with low expected loss or �-error er�D[h] = E(X,Y )∼D[�(Y, h(X))]; ideally, one wants the �-error of the learned model to be close to the optimal �-error er�,∗D = infh:X→T er � D[h]. An algorithm which when given a random training sample as above produces a (random) model hm : X→T is said to be consistent w.r.t. � if the �-error of the learned model hm converges in probability to the optimal: er�D[hm] P−→ er�,∗D .1\nTypically, minimizing the discrete �-error directly is computationally difficult; therefore one uses instead a surrogate loss function ψ : Y ×Rd→R̄+ (where R̄+ = [0,∞]), defined on the continuous surrogate target space Rd for some d ∈ Z+ instead of the discrete target space T , and learns a model f : X→Rd by minimizing (approximately, based on the training sample) the ψ-error erψD[f ] = E(X,Y )∼D[ψ(Y, f(X))]. Predictions on new instances x ∈ X are then made by applying the learned model f and mapping back to predictions in the target space T via some mapping pred : Rd→T , giving h(x) = pred(f(x)). Under suitable conditions, algorithms that approximately minimize the ψ-error based on a training sample are known to be consistent with respect to ψ, i.e. to converge in probability to the optimal ψ-error erψ,∗D = inff :X→Rd er ψ D[f ]. A desirable property of ψ is that it be calibrated w.r.t. �, in which case consistency w.r.t. ψ also guarantees consistency w.r.t. �; we give a formal definition of calibration and statement of this result below. In what follows, we will denote by Δn the probability simplex in Rn: Δn = {p ∈ Rn+ : �\ni pi = 1}. For z ∈ R, let (z)+ = max(z, 0). We will find it convenient to view the loss function � : Y×T →R+ as an n × k matrix with elements �yt = �(y, t) for y ∈ [n], t ∈ [k], and column vectors �t = (�1t, . . . , �nt)\n� ∈ Rn+ for t ∈ [k]. We will also represent the surrogate loss ψ : Y × Rd→R̄+ as a vector function ψ : Rd→R̄n+ with ψy(u) = ψ(y,u) for y ∈ [n],u ∈ Rd, and ψ(u) = (ψ1(u), . . . ,ψn(u))\n� ∈ R̄n+ for u ∈ Rd. Definition 1 (Calibration). Let � : Y ×T →R+ and let P ⊆ Δn. A surrogate loss ψ : Y ×Rd→R̄+ is said to be calibrated w.r.t. � over P if there exists a function pred : Rd→T such that\n∀p ∈ P : inf u∈Rd:pred(u)/∈argmintp��t p�ψ(u) > inf u∈Rd p�ψ(u) .\n1Here P−→ denotes convergence in probability: Xm P−→ a if ∀� > 0, P(|Xm − a| ≥ �)→ 0 as m→∞.\nIn this case we also say (ψ, pred) is (�,P)-calibrated, or if P = Δn, simply �-calibrated. Theorem 2 ( [6, 7, 16]). Let � : Y × T →R+ and ψ : Y × Rd→R̄+. Then ψ is calibrated w.r.t. � over Δn iff ∃ a function pred : Rd→T such that for all distributions D on X ×Y and all sequences of random (vector) functions fm : X→Rd (depending on (X1, Y1), . . . , (Xm, Ym)),\nerψD[fm] P−→ erψ,∗D implies er�D[pred ◦ fm] P−→ er�,∗D .\nFor any instance x ∈ X , let p(x) ∈ Δn denote the conditional label probability vector at x, given by p(x) = (p1(x), . . . , pn(x))\n� where py(x) = P(Y = y |X = x). Then one can extend the above result to show that for P ⊂ Δn, ψ is calibrated w.r.t. � over P iff ∃ a function pred : Rd→T such that the above implication holds for all distributions D on X × Y for which p(x) ∈ P ∀x ∈ X . Subset ranking. Subset ranking problems arise frequently in information retrieval applications. In a subset ranking problem, each instance in X consists of a query together with a set of say r documents to be ranked. The label space Y varies from problem to problem: in some cases, labels consist of binary or multi-level relevance judgements for the r documents, in which case Y = {0, 1}r or Y = {0, 1, . . . , s}r for some appropriate s ∈ Z+; in other cases, labels consist of pairwise preference graphs over the r documents, represented as (possibly weighted) directed acyclic graphs (DAGs) over r nodes. Given examples of such instance-label pairs, the goal is to learn a model to rank documents for new queries/instances; in most cases, the desired ranking takes the form of a permutation over the r documents, so that T = Sr (where Sr denotes the group of permutations on r objects). As noted earlier, various loss functions are used in practice, and there has been much interest in understanding questions of consistency and calibration for these losses in recent years [9–15, 17]. The focus so far has mostly been on designing r-dimensional surrogates, which operate on a surrogate target space of dimension d = r; these are also termed ‘score-based’ surrogates since the resulting algorithms can be viewed as learning one real-valued score function for each of the r documents, and in this case the pred mapping usually consists of simply sorting the documents according to these scores. Below we will apply our result on calibrated surrogates for low-rank loss matrices to obtain new calibrated surrogates – both r-dimensional, score-based surrogates and, in some cases, higher-dimensional surrogates – for several subset ranking losses."
    }, {
      "heading" : "3 Calibrated Surrogates for Low Rank Loss Matrices",
      "text" : "The following is the primary result of our paper. The result gives an explicit construction for a convex, calibrated, least-squares type surrogate loss defined on a low-dimensional surrogate space for any target loss matrix that has a low-rank structure.\nTheorem 3. Let � : Y × T →R+ be a loss function such that there exist d ∈ Z+, vectors α1, . . . ,αn ∈ Rd, β1, . . . ,βk ∈ Rd and c ∈ R such that\n�(y, t) =\nd�\ni=1\nαyiβti + c .\nLet ψ∗� : Y × Rd→R̄+ be defined as\nψ∗� (y,u) = d�\ni=1\n(ui − αyi)2\nand let pred∗� : Rd→T be defined as pred∗� (u) ∈ argmint∈[k]u�βt . Then � ψ∗� , pred ∗ � � is �-calibrated.\nProof. Let p ∈ Δn. Define up ∈ Rd as upi = �n y=1 pyαyi ∀i ∈ [d]. Now for any u ∈ Rd, we have\np�ψ∗� (u) = d�\ni=1\nn�\ny=1\npy(ui − αyi)2 .\nMinimizing this over u ∈ Rd yields that up is the unique minimizer of p�ψ∗� (u). Also, for any t ∈ [k], we have\np��t = n�\ny=1\npy\n� d�\ni=1\nαyiβti + c � = (up)�βt + c .\nNow, for each t ∈ [k], define regret�p(t) � = p��t − min\nt�∈[k] p��t� = (u p)�βt − min t�∈[k] (up)�βt� .\nClearly, by definition of pred∗� , we have regret � p(pred ∗ � (u\np)) = 0. Also, if regret�p(t) = 0 for all t ∈ [k], then trivially pred∗� (u) ∈ argmintp��t ∀u ∈ Rd (and there is nothing to prove in this case). Therefore assume ∃t ∈ [k] : regret�p(t) > 0, and let\n� = min t∈[k]:regret�p(t)>0\nregret�p(t) .\nThen we have inf\nu∈Rd:pred∗� (u)/∈argmintp��t p�ψ∗� (u) = inf u∈Rd:regret�p(pred∗� (u))≥� p�ψ∗� (u)\n= inf u∈Rd:regret�p(pred∗� (u))≥regret�p(pred∗� (up))+�\np�ψ∗� (u) .\nNow, we claim that the mapping u �→ regret�p(pred∗� (u)) is continuous at u = up. To see this, suppose the sequence {um} converges to up. Then we have\nregret�p(pred ∗ � (um)) = (u p)�βpred∗� (um) − mint�∈[k](u p)�βt�\n= (up − um)�βpred∗� (um) + u � mβpred∗� (um) − mint�∈[k](u p)�βt�\n= (up − um)�βpred∗� (um) + mint�∈[k]u � mβt� − min t�∈[k] (up)�βt�\nThe last equality holds by definition of pred∗� . It is easy to see the term on the right goes to zero as um converges to up. Thus regret�p(pred ∗ � (um)) converges to regret � p(pred ∗ � (u\np)) = 0, yielding continuity at up. In particular, this implies ∃δ > 0 such that\n�u− up� < δ =⇒ regret�p(pred∗� (u))− regret�p(pred∗� (up)) < � . This gives\ninf u∈Rd:regret�p(pred∗� (u))≥regret�p(pred∗� (up))+� p�ψ∗� (u) ≥ inf u∈Rd:�u−up�≥δ p�ψ∗� (u)\n> inf u∈Rd\np�ψ∗� (u) ,\nwhere the last inequality holds since p�ψ∗� (u) is a strictly convex function of u and u p is its unique minimizer. The above sequence of inequalities give us that\ninf u∈Rd:pred∗� (u)/∈argmintp��t p�ψ∗� (u) > inf u∈Rd p�ψ∗� (u) .\nSince this holds for all p ∈ Δn, we have that (ψ∗� , pred∗� ) is �-calibrated.\nWe note that Ramaswamy and Agarwal [16] showed a similar least-squares type surrogate calibrated for any loss � : Y × T →R+; indeed our proof technique above draws inspiration from the proof technique there. However, the surrogate they gave was defined on a surrogate space of dimension n−1, where n is the number of class labels in Y . For many practical problems, this is an intractably large number. For example, as noted above, in the subset ranking problems we consider, the number of class labels is typically exponential in r, the number of documents associated with each query. On the other hand, as we will see below, many subset ranking losses have a low-rank structure, with rank linear or quadratic in r, allowing us to use the above result to design convex calibrated surrogates on an O(r) or O(r2)-dimensional space. Ramaswamy and Agarwal also gave another result in which they showed that any loss matrix of rank d has a d-dimensional convex calibrated surrogate; however the surrogate there was defined such that it took values < ∞ on an awkward space in Rd (not the full space Rd) that would be difficult to construct in practice, and moreover, their result did not yield an explicit construction for the pred mapping required to use a calibrated surrogate in practice. Our result above combines the benefits of both these previous results, allowing explicit construction of low-dimensional least-squares type surrogates for any low-rank loss matrix. The following sections will illustrate several applications of this result.\n4 Calibrated Surrogates for Precision@q The Precision@q is a popular performance measure for subset ranking problems in information retrieval. As noted above, in a subset ranking problem, each instance in X consists of a query together with a set of r documents to be ranked. Consider a setting with binary relevance judgement labels, so that Y = {0, 1}r with n = 2r. The prediction space is T = Sr (group of permutations on r objects) with k = r!. For y ∈ {0, 1}r and σ ∈ Sr, where σ(i) denotes the position of document i under σ, the Precision@q loss for any integer q ∈ [r] can be written as follows:\n�P@q(y,σ) = 1− 1\nq\nq�\ni=1\nyσ−1(i)\n= 1− 1 q\nr�\ni=1\nyi · 1(σ(i) ≤ q) .\nTherefore, by Theorem 3, for the r-dimensional surrogate ψ∗P@q : {0, 1}r ×Rr→R̄+ and pred∗P@q : Rr→Sr defined as\nψ∗P@q(y,u) = r�\ni=1\n(ui − yi)2\npred∗P@q(u) ∈ argmaxσ∈Sr r�\ni=1\nui · 1(σ(i) ≤ q) ,\nwe have that (ψ∗P@q, pred ∗ P@q) is �P@q-calibrated. It can easily be seen that for any u ∈ Rr, any permutation σ which places the top q documents sorted in decreasing order of scores ui in the top q positions achieves the maximum in pred∗P@q(u); thus pred ∗ P@q(u) can be implemented efficiently using a standard sorting or selection algorithm. Note that the popular winner-take-all (WTA) loss, which assigns a loss of 0 if the top-ranked item is relevant (i.e. if yσ−1(1) = 1) and 1 otherwise, is simply a special case of the above loss with q = 1; therefore the above construction also yields a calibrated surrogate for the WTA loss. To our knowledge, this is the first example of convex, calibrated surrogates for the Precision@q and WTA losses."
    }, {
      "heading" : "5 Calibrated Surrogates for Expected Rank Utility",
      "text" : "The expected rank utility (ERU) is a popular subset ranking performance measure used in recommender systems displaying short ranked lists [18]. In this case the labels consist of multi-level relevance judgements (such as 0 to 5 stars), so that Y = {0, 1, . . . , s}r for some appropriate s ∈ Z+ with n = (s + 1)r. The prediction space again is T = Sr with k = r!. For y ∈ {0, 1, . . . , s}r and σ ∈ Sr, where σ(i) denotes the position of document i under σ, the ERU loss is defined as\n�ERU(y,σ) = z − r�\ni=1\nmax(yi − v, 0) · 2 1−σ(i) w−1 ,\nwhere z is a constant to ensure the positivity of the loss, v ∈ [s] is a constant that indicates a neutral score, and w ∈ R is a constant indicating the viewing half-life. Thus, by Theorem 3, for the r-dimensional surrogate ψ∗ERU : {0, 1, . . . , s}r × Rr→R̄+ and pred∗ERU : Rr→Sr defined as\nψ∗ERU(y,u) = r�\ni=1\n(ui −max(yi − v, 0))2\npred∗ERU(u) ∈ argmaxσ∈Sr r�\ni=1\nui · 2 1−σ(i) w−1 ,\nwe have that (ψ∗ERU, pred ∗ ERU) is �ERU-calibrated. It can easily be seen that for any u ∈ Rr, any permutation σ satisfying the condition\nui > uj =⇒ σ(i) < σ(j) achieves the maximum in pred∗ERU(u), and therefore pred ∗ ERU(u) can be implemented efficiently by simply sorting the r documents in decreasing order of scores ui. As for Precision@q, to our knowledge, this is the first example of a convex, calibrated surrogate for the ERU loss."
    }, {
      "heading" : "6 Calibrated Surrogates for Mean Average Precision",
      "text" : "The mean average precision (MAP) is a widely used ranking performance measure in information retrieval and related applications [15, 19]. As with the Precision@q loss, Y = {0, 1}r and T = Sr. For y ∈ {0, 1}r and σ ∈ Sr, where σ(i) denotes the position of document i under σ, the MAP loss is defined as follows:\n�MAP(y,σ) = 1− 1 |{γ : yγ = 1}| �\ni:yi=1\n1\nσ(i)\nσ(i)�\nj=1\nyσ−1(j) .\nIt was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for the MAP loss [15]. We now re-write the MAP loss above in a manner that allows us to show the existence of an O(r2)-dimensional convex, calibrated surrogate. In particular, we can write\n�MAP(y,σ) = 1− 1�r\nγ=1 yγ\nr�\ni=1\ni�\nj=1\nyσ−1(i)yσ−1(j) i . = 1− 1�r\nγ=1 yγ\nr�\ni=1\ni�\nj=1\nyiyj max(σ(i),σ(j))\nThus, by Theorem 3, for the r(r+1)2 -dimensional surrogate ψ ∗ MAP : {0, 1}r × Rr(r+1)/2→R̄+ and pred∗MAP : Rr(r+1)/2→Sr defined as\nψ∗MAP(y,u) = r�\ni=1\ni�\nj=1\n� uij −\nyiyj�r γ=1 yγ\n�2\npred∗MAP(u) ∈ argmaxσ∈Sr r�\ni=1\ni�\nj=1\nuij · 1\nmax(σ(i),σ(j)) ,\nwe have that (ψ∗MAP, pred ∗ MAP) is �MAP-calibrated. Note however that the optimization problem associated with computing pred∗MAP(u) above can be written as a quadratic assignment problem (QAP), and most QAPs are known to be NP-hard. We conjecture that the QAP associated with the mapping pred∗MAP above is also NP-hard. Therefore, while the surrogate loss ψ∗MAP is calibrated for �MAP and can be minimized efficiently over a training sample to learn a model f : X→Rr(r+1)/2, for large r, evaluating the mapping required to transform predictions in Rr(r+1)/2 back to predictions in Sr is likely to be computationally infeasible. Below we describe an alternate mapping in place of pred∗MAP which can be computed efficiently, and show that under certain conditions on the probability distribution, the surrogate ψ∗MAP together with this mapping is still calibrated for �MAP.\nSpecifically, define predMAP : Rr(r+1)/2→Sr as follows: predMAP(u) ∈ � σ ∈ Sr : uii > ujj =⇒ σ(i) < σ(j) � .\nClearly, predMAP(u) can be implemented efficiently by simply sorting the ‘diagonal’ elements uii for i ∈ [r]. Also, let ΔY denote the probability simplex over Y , and for each p ∈ ΔY , define up ∈ Rr(r+1)/2 as follows:\nupij = EY∼p � YiYj�r γ=1 Yγ � = � y∈Y py � yiyj�r γ=1 yγ � ∀i, j ∈ [r] : i ≥ j .\nNow define Preinforce ⊂ ΔY as follows: Preinforce = � p ∈ ΔY : upii ≥ upjj =⇒ upii ≥ upjj + �\nγ∈[r]\\{i,j} (upjγ − upiγ)+\n� ,\nwhere we set upij = u p ji for i < j. Then we have the following result:\nTheorem 4. (ψ∗MAP, predMAP) is (�MAP,Preinforce)-calibrated.\nThe ideal predictor pred∗MAP uses the entire u matrix, but the predictor predMAP, uses only the diagonal elements. The noise conditions Preinforce can be viewed as basically enforcing that the diagonal elements dominate and enforce a clear ordering themselves.\nIn fact, since the mapping predMAP depends on only the diagonal elements of u, we can equivalently define an r-dimensional surrogate that is calibrated w.r.t. �MAP over Preinforce. Specifically, we have the following immediate corollary:\nCorollary 5. Let �ψMAP : {0, 1}r × Rr→R̄+ and �predMAP : Rr→Sr be defined as �ψMAP(y, �u) = r�\ni=1\n� �ui −\nyi�r γ=1 yγ\n�2\n�predMAP(�u) ∈ � σ ∈ Sr : �ui > �uj =⇒ σ(i) < σ(j) � .\nThen ( �ψMAP, �predMAP) is (�MAP,Preinforce)-calibrated.\nLooking at the form of �ψMAP and �predMAP, we can see that the function s : Y→Rr defined as si(y) = yi/( �r γ=1 yr) is a ‘standardization function’ for the MAP loss over Preinforce, and therefore it follows that any ‘order-preserving surrogate’ with this standardization function is also calibrated with the MAP loss over Preinforce [13]. To our knowledge, this is the first example of conditions on the probability distribution under which a convex calibrated (and moreover, score-based) surrogate can be designed for the MAP loss."
    }, {
      "heading" : "7 Calibrated Surrogates for Pairwise Disagreement",
      "text" : "The pairwise disagreement (PD) loss is a natural and widely used loss in subset ranking [11, 17]. The label space Y here consists of a finite number of (possibly weighted) directed acyclic graphs (DAGs) over r nodes; we can represent each such label as a vector y ∈ Rr(r−1)+ where at least one of yij or yji is 0 for each i �= j, with yij > 0 indicating a preference for document i over document j and yij denoting the weight of the preference. The prediction space as usual is T = Sr with k = r!. For y ∈ Y and σ ∈ Sr, where σ(i) denotes the position of document i under σ, the PD loss is defined as follows:\n�PD(y,σ) =\nr�\ni=1\n� j �=i yij 1 � σ(i) > σ(j) � .\nIt was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for the PD loss [15, 16]. By Theorem 3, for the r(r − 1)-dimensional surrogate ψ∗PD : Y × Rr(r−1)→R̄+ and pred∗PD : Rr(r−1)→Sr defined as\nψ∗PD(y,u) = r�\ni=1\n� j �=i (uij − yij)2 (1)\npred∗PD(u) ∈ argminσ∈Sr r�\ni=1\n� j �=i uij · 1 � σ(i) > σ(j) �\nwe immediately have that (ψ∗PD, pred ∗ PD) is �PD-calibrated (in fact the loss matrix �PD has rank at most r(r−1) 2 , allowing for an r(r−1) 2 -dimensional surrogate; we use r(r−1) dimensions for convenience). In this case, the optimization problem associated with computing pred∗PD(u) above is a minimum weighted feedback arc set (MWFAS) problem, which is known to be NP-hard. Therefore, as with the MAP loss, while the surrogate loss ψ∗PD is calibrated for �PD and can be minimized efficiently over a training sample to learn a model f : X→Rr(r−1), for large r, evaluating the mapping required to transform predictions in Rr(r−1) back to predictions in Sr is likely to be computationally infeasible.\nBelow we give two sets of results. In Section 7.1, we give a family of score-based (r-dimensional) surrogates that are calibrated with the PD loss under different conditions on the probability distribution; these surrogates and conditions generalize those of Duchi et al. [11]. In Section 7.2, we give a different condition on the probability distribution under which we can actually avoid ‘difficult’ graphs being passed to pred∗PD. This condition is more general (i.e. encompasses a larger set of probability distributions) than those associated with the score-based surrogates; this gives a new (non-score-based, r(r−1)-dimensional) surrogate with an efficiently computable pred mapping that is calibrated with the PD loss over a larger set of probability distributions than previous surrogates for this loss.\n7.1 Family of r-Dimensional Surrogates Calibrated with �PD Under Noise Conditions\nThe following gives a family of score-based surrogates, parameterized by functions f : Y→Rr, that are calibrated with the PD loss under different conditions on the probability distribution:\nTheorem 6. Let f : Y→Rr be any function that maps DAGs y ∈ Y to score vectors f(y) ∈ Rr. Let ψf : Y × Rr→R̄+, pred : Rr→Sr and Pf ⊂ ΔY be defined as\nψf (y,u) =\nr�\ni=1\n� ui − fi(y) �2\npred(u) ∈ � σ ∈ Sr : ui > uj =⇒ σ(i) < σ(j) �\nPf = � p ∈ ΔY : EY∼p[Yij ] > EY∼p[Yji] =⇒ EY∼p[fi(Y )] > EY∼p[fj(Y )] � .\nThen (ψf , pred) is (�PD,Pf )-calibrated. The noise conditions Pf state that the expected value of function f must decide the ‘right’ ordering. We note that the surrogate given by Duchi et al. [11] can be written in our notation as\nψDMJ(y,u) =\nr�\ni=1\n� j �=i yij(uj − ui) + ν r� i=1 λ(ui) ,\nwhere λ is a strictly convex and 1-coercive function and ν > 0. Taking λ(z) = z2 and ν = 12 gives a special case of the family of score-based surrogates in Theorem 6 above obtained by taking f as\nfi(y) = �\nj �=i (yij − yji) .\nIndeed, the set of noise conditions under which the surrogate ψDMJ is shown to be calibrated with the PD loss in Duchi et al. [11] is exactly the set Pf above with this choice of f . We also note that f can be viewed as a ‘standardization function’ [13] for the PD loss over Pf .\n7.2 An O(r2)-dimensional Surrogate Calibrated with �PD Under More General Conditions\nConsider now the r(r − 1)-dimensional surrogate ψ∗PD : Y × Rr(r−1) defined in Eq. (1). We noted the corresponding mapping pred∗PD involved an NP-hard optimization problem. Here we give an alternate mapping predPD : Rr(r−1)→Sr that can be computed efficiently, and show that under certain conditions on the probability distribution , the surrogate ψ∗PD together with this mapping predPD is calibrated for �PD. The mapping predPD is described by Algorithm 1 below:\nAlgorithm 1 predPD (Input: u ∈ Rr(r−1); Output: Permutation σ ∈ Sr) Construct a directed graph over [r] with edge (i, j) having weight (uij − uji)+. If this graph is acyclic, return any topological sorted order. If the graph has cycles, sort the edges in ascending order by weight and delete them one by one (smallest weight first) until the graph becomes acyclic; return any topological sorted order of the resulting acyclic graph.\nFor each p ∈ ΔY , define Ep = {(i, j) ∈ [r]× [r] : EY∼p[Yij ] > EY∼p[Yji]}, and define PDAG = � p ∈ ΔY : � [r], Ep � is a DAG � .\nThen we have the following result: Theorem 7. (ψ∗PD, predPD) is (�PD,PDAG)-calibrated. It is easy to see that PDAG � Pf ∀f (where Pf is as defined in Theorem 6), so that the above result yields a low-dimensional, convex surrogate with an efficiently computable pred mapping that is calibrated for the PD loss under a broader set of conditions than the previous surrogates."
    }, {
      "heading" : "8 Conclusion",
      "text" : "Calibration of surrogate losses is an important property in designing consistent learning algorithms. We have given an explicit method for constructing calibrated surrogates for any learning problem with a low-rank loss structure, and have used this to obtain several new results for subset ranking, including new calibrated surrogates for the Precision@q, ERU, MAP and PD losses."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank the anonymous reviewers, Aadirupa Saha and Shiv Ganesh for their comments. HGR acknowledges a Tata Consultancy Services (TCS) PhD fellowship and the Indo-US Virtual Institute for Mathematical and Statistical Sciences (VIMSS). SA thanks the Department of Science & Technology (DST) and Indo-US Science & Technology Forum (IUSSTF) for their support. AT gratefully acknowledges the support of NSF under grant IIS-1319810."
    } ],
    "references" : [ {
      "title" : "On the Bayes-risk consistency of regularized boosting methods",
      "author" : [ "Gábor Lugosi", "Nicolas Vayatis" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "Process consistency for AdaBoost",
      "author" : [ "Wenxin Jiang" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Statistical behavior and consistency of classification methods based on convex risk minimization",
      "author" : [ "Tong Zhang" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Consistency of support vector machines and other regularized kernel classifiers",
      "author" : [ "Ingo Steinwart" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2005
    }, {
      "title" : "Convexity, classification and risk bounds",
      "author" : [ "Peter L. Bartlett", "Michael Jordan", "Jon McAuliffe" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Statistical analysis of some multi-category large margin classification methods",
      "author" : [ "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "On the consistency of multiclass classification methods",
      "author" : [ "Ambuj Tewari", "Peter L. Bartlett" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "How to compare different loss functions and their risks",
      "author" : [ "Ingo Steinwart" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Statistical analysis of bayes optimal subset ranking",
      "author" : [ "David Cossock", "Tong Zhang" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Listwise approach to learning to rank: Theory and algorithm",
      "author" : [ "Fen Xia", "Tie-Yan Liu", "Jue Wang", "Wensheng Zhang", "Hang Li" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "On the consistency of ranking algorithms",
      "author" : [ "John Duchi", "Lester Mackey", "Michael Jordan" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "On NDCG consistency of listwise ranking methods",
      "author" : [ "Pradeep Ravikumar", "Ambuj Tewari", "Eunho Yang" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Learning scoring functions with order-preserving losses and standardized supervision",
      "author" : [ "David Buffoni", "Clément Calauzènes", "Patrick Gallinari", "Nicolas Usunier" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "On the consistency of multi-label learning",
      "author" : [ "Wei Gao", "Zhi-Hua Zhou" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "On the (non-)existence of convex, calibrated surrogate losses for ranking",
      "author" : [ "Clément Calauzènes", "Nicolas Usunier", "Patrick Gallinari" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Classification calibration dimension for general multiclass losses",
      "author" : [ "Harish G. Ramaswamy", "Shivani Agarwal" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Statistical consistency of ranking methods in a rank-differentiable probability space",
      "author" : [ "Yanyan Lan", "Jiafeng Guo", "Xueqi Cheng", "Tie-Yan Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Direct optimization of ranking measures, arXiv:0704.3359",
      "author" : [ "Quoc V. Le", "Alex Smola" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "A support vector method for optimizing average precision",
      "author" : [ "Yisong Yue", "Thomas Finley", "Filip Radlinski", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 30th ACM SIGIR International Conference on Research and Development in Information Retrieval,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Recently, Ramaswamy and Agarwal [16] showed the existence of such surrogates, but their result involved an unwieldy surrogate space, and moreover did not give an explicit, usable construction for the mapping needed to transform predictions in the surrogate space back to the original prediction space.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "We then turn to the MAP and PD losses, which are both widely used in practice, and for which it has been shown that no convex score-based surrogate can be calibrated for all probability distributions [11,15,16].",
      "startOffset" : 200,
      "endOffset" : 210
    }, {
      "referenceID" : 14,
      "context" : "We then turn to the MAP and PD losses, which are both widely used in practice, and for which it has been shown that no convex score-based surrogate can be calibrated for all probability distributions [11,15,16].",
      "startOffset" : 200,
      "endOffset" : 210
    }, {
      "referenceID" : 15,
      "context" : "We then turn to the MAP and PD losses, which are both widely used in practice, and for which it has been shown that no convex score-based surrogate can be calibrated for all probability distributions [11,15,16].",
      "startOffset" : 200,
      "endOffset" : 210
    }, {
      "referenceID" : 10,
      "context" : "[11] gave certain low-noise conditions on the probability distribution under which a convex, calibrated score-based surrogate could be designed;",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "We work in the same general setting as that of Ramaswamy and Agarwal [16].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 15,
      "context" : "We note that Ramaswamy and Agarwal [16] showed a similar least-squares type surrogate calibrated for any loss � : Y × T →R+; indeed our proof technique above draws inspiration from the proof technique there.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "The expected rank utility (ERU) is a popular subset ranking performance measure used in recommender systems displaying short ranked lists [18].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 14,
      "context" : "The mean average precision (MAP) is a widely used ranking performance measure in information retrieval and related applications [15, 19].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "The mean average precision (MAP) is a widely used ranking performance measure in information retrieval and related applications [15, 19].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "It was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for the MAP loss [15].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Looking at the form of � ψMAP and � predMAP, we can see that the function s : Y→Rr defined as si(y) = yi/( �r γ=1 yr) is a ‘standardization function’ for the MAP loss over Preinforce, and therefore it follows that any ‘order-preserving surrogate’ with this standardization function is also calibrated with the MAP loss over Preinforce [13].",
      "startOffset" : 335,
      "endOffset" : 339
    }, {
      "referenceID" : 10,
      "context" : "The pairwise disagreement (PD) loss is a natural and widely used loss in subset ranking [11, 17].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "The pairwise disagreement (PD) loss is a natural and widely used loss in subset ranking [11, 17].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "It was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for the PD loss [15, 16].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : "It was recently shown that there cannot exist any r-dimensional convex, calibrated surrogates for the PD loss [15, 16].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "[11] is exactly the set Pf above with this choice of f .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "We also note that f can be viewed as a ‘standardization function’ [13] for the PD loss over Pf .",
      "startOffset" : 66,
      "endOffset" : 70
    } ],
    "year" : 2013,
    "abstractText" : "The design of convex, calibrated surrogate losses, whose minimization entails consistency with respect to a desired target loss, is an important concept to have emerged in the theory of machine learning in recent years. We give an explicit construction of a convex least-squares type surrogate loss that can be designed to be calibrated for any multiclass learning problem for which the target loss matrix has a low-rank structure; the surrogate loss operates on a surrogate target space of dimension at most the rank of the target loss. We use this result to design convex calibrated surrogates for a variety of subset ranking problems, with target losses including the precision@q, expected rank utility, mean average precision, and pairwise disagreement.",
    "creator" : null
  }
}