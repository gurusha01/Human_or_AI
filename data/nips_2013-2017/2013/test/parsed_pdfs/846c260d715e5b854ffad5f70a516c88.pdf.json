{
  "name" : "846c260d715e5b854ffad5f70a516c88.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bayesian Mixture Modeling and Inference based Thompson Sampling in Monte-Carlo Tree Search",
    "authors" : [ "Aijun Bai", "Feng Wu", "Xiaoping Chen" ],
    "emails" : [ "baj@mail.ustc.edu.cn", "fw6e11@ecs.soton.ac.uk", "xpchen@ustc.edu.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Markov decision processes (MDPs) provide a general framework for planning and learning under uncertainty. We consider the problem of online planning in MDPs without prior knowledge on the underlying transition probabilities. Monte-Carlo tree search (MCTS) can find near-optimal policies in our domains by combining tree search methods with sampling techniques. The key idea is to iteratively evaluate each state in a best-first search tree by the mean outcome of simulation samples. It is model-free and requires only a black-box simulator (generative model) of the underlying problems. To date, great success has been achieved by MCTS in variety of domains, such as game play [1, 2], planning under uncertainty [3, 4, 5], and Bayesian reinforcement learning [6, 7].\nWhen applying MCTS, one of the fundamental challenges is the so-called exploration versus exploitation dilemma: an agent must not only exploit by selecting the best action based on the current information, but should also keep exploring other actions for possible higher future payoffs. Thompson sampling is one of the earliest heuristics to address this dilemma in multi-armed bandit problems (MABs) according to the principle of randomized probability matching [8]. The basic idea is to select actions stochastically, based on the probabilities of being optimal. It has recently been shown to perform very well in MABs both empirically [9] and theoretically [10]. It has been proved that Thompson sampling algorithm achieves logarithmic expected regret which is asymptotically optimal for MABs. Comparing to the UCB1 heuristic [3], the main advantage of Thompson sampling is that it allows more robust convergence under a wide range of problem settings.\nIn this paper, we borrow the idea of Thompson sampling and propose the Dirichlet-NormalGamma MCTS (DNG-MCTS) algorithm — a novel Bayesian mixture modeling and inference based Thompson sampling approach for online planning in MDPs. In this algorithm, we use a mixture of Normal distributions to model the unknown distribution of the accumulated reward of performing a particular action in the MCTS search tree. In the present of online planning for MDPs, a conjugate prior\nexists in the form of a combination of Dirichlet and NormalGamma distributions. By choosing the conjugate prior, it is then relatively simple to compute the posterior distribution after each accumulated reward is observed by simulation in the search tree. Thompson sampling is then used to select the action to be performed by simulation at each decision node. We have tested our DNG-MCTS algorithm and compared it with the popular UCT algorithm in several benchmark problems. Experimental results show that our proposed algorithm has outperformed the state-of-the-art for online planning in general MDPs. Furthermore, we show the convergence of our algorithm, confirming its technical soundness.\nThe reminder of this paper is organized as follows. In Section 2, we briefly introduce the necessary background. Section 3 presents our main results — the DNG-MCTS algorithm. We show experimental results on several benchmark problems in Section 4. Finally in Section 5 the paper is concluded with a summary of our contributions and future work."
    }, {
      "heading" : "2 Background",
      "text" : "In this section, we briefly review the MDP model, the MAB problem, the MCTS framework, and the UCT algorithm as the basis of our algorithm. Some related work is also presented."
    }, {
      "heading" : "2.1 MDPs and MABs",
      "text" : "Formally, an MDP is defined as a tuple 〈S,A, T,R〉, where S is the state space, A is the action space, T (s′|s, a) is the probability of reaching state s′ if action a is applied in state s, and R(s, a) is the reward received by the agent. A policy is a decision rule mapping from states to actions and specifying which action should be taken in each state. The aim of solving an MDP is to find the optimal policy π that maximizes the expected reward defined as Vπ(s) = E[ ∑H t=0 γ\ntR(st, π(st))], where H is the planing horizon, γ ∈ (0, 1] is the discount factor, st is the state in time step t and π(st) is the action selected by policy π in state st.\nIntuitively, an MAB can be seen as an MDP with only one state s and a stochastic reward function R(s, a) := Xa, where Xa is a random variable following an unknown distribution fXa(x). At each time step t, one action at must be chosen and executed. A stochastic reward Xat is then received accordingly. The goal is to find a sequence of actions that minimizes the cumulative regret defined as RT = E[ ∑T t=1(Xa∗ −Xat)], where a∗ is the true best action."
    }, {
      "heading" : "2.2 MCTS and UCT",
      "text" : "To solve MDPs, MCTS iteratively evaluates a state by: (1) selecting an action based on a given action selection strategy; (2) performing the selected action by Monte-Carlo simulation; (3) recursively evaluating the resulted state if it is already in the search tree, or inserting it into the search tree and running a rollout policy by simulations. This process is applied to descend through the search tree until some terminate conditions are reached. The simulation result is then back-propagated through the selected nodes to update their statistics.\nThe UCT algorithm is a popular approach based on MCTS for planning under uncertainty [3]. It treats each state of the search tree as an MAB, and selects the action that maximizes the UCB1 heuristic Q̄(s, a) + c √ logN(s)/N(s, a), where Q̄(s, a) is the mean return of action a in state s from all previous simulations, N(s, a) is the visitation count of action a in state s, N(s) is the overall count N(s) = ∑ a∈AN(s, a), and c is the exploration constant that determines the relative ratio of exploration to exploitation. It is proved that with an appropriate choice of c the probability of selecting the optimal action converges to 1 as the number of samples grows to infinity."
    }, {
      "heading" : "2.3 Related Work",
      "text" : "The fundamental assumption of our algorithm is modeling unknown distribution of the accumulated reward for each state-action pair in the search tree as a mixture of Normal distributions. A similar assumption has been made in [11], where they assumed a Normal distribution over the rewards. Comparing to their approach, as we will show in Section 3, our assumption on Normal mixture is more realistic for our problems. Tesauro et al.[12] developed a Bayesian UCT approach to MCTS\nusing Gaussian approximation. Specifically, their method propagates probability distributions of rewards from leaf nodes up to the root node by applying MAX (or MIN) extremum distribution operator for the interior nodes. Then, it uses modified UCB1 heuristics to select actions on the basis of the interior distributions. However, extremum distribution operation on decision nodes is very time-consuming because it must consider over all the child nodes. In contrast, we treat each decision node in the search tree as an MAB, maintain a posterior distribution over the accumulated reward for each applicable actions separately, and then select the best action using Thompson sampling."
    }, {
      "heading" : "3 The DNG-MCTS Algorithm",
      "text" : "This section presents our main results — a Bayesian mixture modeling and inference based Thompson sampling approach for MCTS (DNG-MCTS)."
    }, {
      "heading" : "3.1 The Assumptions",
      "text" : "For a given MDP policy π, let Xs,π be a random variable that denotes the accumulated reward of following policy π starting from state s, and Xs,a,π denotes the accumulated reward of first performing action a in state s and then following policy π thereafter. Our assumptions are: (1) Xs,π is sampled from a Normal distribution, and (2) Xs,a,π can be modeled as a mixture of Normal distributions. These are realistic approximations for our problems with the following reasons.\nGiven policy π, an MDP reduces to a Markov chain {st} with finite state space S and the transition function T (s′|s, π(s)). Suppose that the resulting chain {st} is ergodic. That is, it is possible to go from every state to every other state (not necessarily in one move). Let w denote the stationary distribution of {st}. According to the central limit theorem on Markov chains [13, 14], for any bounded function f on the state space S, we have:\n1√ n ( n∑ t=0 f(st)− nµ)→ N(0, σ2) as n→∞, (1)\nwhere µ = Ew[f ] and σ is a constant depending only on f and w. This indicates that the sum of f(st) follows N(nµ, nσ2) as n grows to infinity. It is then natural to approximate the distribution of ∑n t=0 f(st) as a Normal distribution if n is sufficiently large.\nConsidering finite-horizon MDPs with horizon H , if γ = 1, Xs0,π = ∑H t=0R(st, π(st)) is a sum of f(st) = R(st, π(st)). Thus, Xs0,π is approximately normally distributed for each s0 ∈ S if H is sufficiently large. On the other hand, if γ 6= 1, Xs0,π = ∑H t=0 γ\ntR(st, π(st)) can be rewritten as a linear combination of ∑n t=0 f(st) for n = 0 to H as follow:\nXs0,π = (1− γ) H−1∑ n=0 γn n∑ t=0 f(st) + γ H H∑ t=0 f(st) (2)\nNotice that a linear combination of independent or correlated normally distributed random variables is still normally distributed. If H is sufficiently large and γ is close to 1, it is reasonable to approximateXs0,π as a Normal distribution. Therefore, we assume thatXs,π is normally distributed in both cases.\nIf the policy π is not fixed and may change over time (e.g., the derived policy of an online algorithm before it converges), the real distribution of Xs,π is actually unknown and could be very complex. However, if the algorithm is guaranteed to converge in the limit (as explained in Section 3.5, this holds for our DNG-MCTS algorithm), it is convenient and reasonable to approximate Xs,π as a Normal distribution.\nNow consider the accumulated reward of first performing action a in s and following policy π thereafter. By definition, Xs,a,π = R(s, a)+γXs′,π , where s′ is the next state distributed according to T (s′|s, a). Let Ys,a,π be a random variable defined as Ys,a,π = (Xs,a,π−R(s, a))/γ. We can see that the pdf of Ys,a,π is a convex combination of the pdfs of Xs′,π for each s′ ∈ S. Specifically, we have fYs,a,π (y) = ∑ s′∈S T (s\n′|s, a)fXs′,π (y). Hence it is straightforward to model the distribution of Ys,a,π as a mixture of Normal distributions ifXs′,π is assumed to be normally distributed for each s′ ∈ S. Since Xs,a,π is a linear function of Ys,a,π , Xs,a,π is also a mixture of Normal distributions under our assumptions."
    }, {
      "heading" : "3.2 The Modeling and Inference Methods",
      "text" : "In Bayesian settings, the unknown distribution of a random variable X can be modeled as a parametric likelihood function L(x|θ) depending on the parameters θ. Given a prior distribution P (θ), and a set of past observations Z = {x1, x2, . . . }, the posterior distribution of θ can then be obtained using Bayes’ rules: P (θ|Z) ∝ ∏ i L(xi|θ)P (θ).\nAssumption (1) implies that it suffices to model the distribution of Xs,π as a Normal likelihood N(µs, 1/τs) with unknown mean µs and precision τs. The precision is defined as the reciprocal of the variance, τ = 1/σ2. This is chosen for mathematical convenience of introducing the NomralGamma distribution as a conjugate prior. A NormalGamma distribution is defined by the hyper-parameters 〈µ0, λ, α, β〉 with λ > 0, α ≥ 1 and β ≥ 0. It is said that (µ, τ) follows a NormalGamma distribution NormalGamma(µ0, λ, α, β) if the pdf of (µ, τ) has the form\nf(µ, τ |µ0, λ, α, β) = βα √ λ\nΓ(α) √ 2π τα− 1 2 e−βτ e−\nλτ(µ−µ0) 2\n2 . (3)\nBy definition, the marginal distribution over τ is a Gamma distribution, τ ∼ Gamma(α, β), and the conditional distribution over µ given τ is a Normal distribution, µ ∼ N(µ0, 1/(λτ)). Let us briefly recall the posterior of (µ, τ). Suppose X is normally distributed with unknown mean µ and precision τ , x ∼ N(µ, 1/τ), and that the prior distribution of (µ, τ) has a NormalGamma distribution, (µ, τ) ∼ NormalGamma(µ0, λ0, α0, β0). After observing n independent samples of X , denoted {x1, x2, . . . , xn}, according to the Bayes’ theorem, the posterior distribution of (µ, τ) is also a NormalGamma distribution, (µ, τ) ∼ NormalGamma(µn, λn, αn, βn), where µn = (λ0µ0+nx̄)/(λ0+n), λn = λ0+n, αn = α0+n/2 and βn = β0+(ns+λ0n(x̄−µ0)2/(λ0+n))/2, where x̄ = ∑n i=1 xi/n is the sample mean and s = ∑n i=1(xi − x̄)2/n is the sample variance.\nBased on Assumption 2, the distribution of Ys,a,π can be modeled as a mixture of Normal distributions Ys,a,π = (Xs,a,π − R(s, a))/γ ∼ ∑ s′∈S ws,a,s′N(µs′ , 1/τs′), where ws,a,s′ = T (s\n′|s, a) are the mixture weights such that ws,a,s′ ≥ 0 and ∑ s′∈S ws,a,s′ = 1, which are previously unknown in Monte-Carlo settings. A natural representation on these unknown weights is via Dirichlet distributions, since Dirichlet distribution is the conjugate prior of a general discrete probability distribution. For state s and action a, a Dirichlet distribution, denoted Dir(ρs,a) where ρs,a = (ρs,a,s1 , ρs,a,s2 , · · · ), gives the posterior distribution of T (s′|s, a) for each s′ ∈ S if the transition to s′ has been observed ρs,a,s′ − 1 times. After observing a transition (s, a) → s′, the posterior distribution is also Dirichlet and can simply be updated as ρs,a,s′ ← ρs,a,s′ + 1. Therefore, to model the distribution of Xs,π and Xs,a,π we only need to maintain a set of hyperparameters 〈µs,0, λs, αs, βs〉 and ρs,a for each state s and action a encountered in the MCTS search tree and update them by using Bayes’ rules.\nNow we turn to the question of how to choose the priors by initializing hyper-parameters. While the impact of the prior tends to be negligible in the limit, its choice is important especially when only a small amount of data has been observed. In general, priors should reflect available knowledge of the hidden model.\nIn the absence of any knowledge, uninformative priors may be preferred. According to the principle of indifference, uninformative priors assign equal probabilities to all possibilities. For NormalGamma priors, we hope that the sampled distribution of µ given τ , i.e., N(µ0, 1/(λτ)), is as flat as possible. This implies an infinite variance 1/(λτ) → ∞, so that λτ → 0. Recall that τ follows a Gamma distribution Gamma(α, β) with expectation E[τ ] = α/β, so we have in expectation λα/β → 0. Considering the parameter space (λ > 0, α ≥ 1, β ≥ 0), we can choose λ small enough, α = 1 and β sufficiently large to approximate this condition. Second, we hope the sampled distribution is in the middle of axis, so µ0 = 0 seems to be a good selection. It is worth noting that intuitively β should not be set too large, or the convergence process may be very slow. For Dirichlet priors, it is common to set ρs,a,s′ = δ where δ is a small enough positive for each s ∈ S, a ∈ A and s′ ∈ S encountered in the search tree to have uninformative priors. On the other hand, if some prior knowledge is available, informative priors may be preferred. By exploiting domain knowledge, a state node can be initialized with informative priors indicating its priority over other states. In DNG-MCTS, this is done by setting the hyper-parameters based\non subjective estimation for states. According to the interpretation of hyper-parameters of NormalGamma distribution in terms of pseudo-observations, if one has a prior mean of µ0 from λ samples and a prior precision of α/β from 2α samples, the prior distribution over µ and τ is NormalGamma(µ0, λ, α, β), providing a straightforward way to initialize the hyper-parameters if some prior knowledge (such as historical data of past observations) is available. Specifying detailed priors based on prior knowledge for particular domains is beyond the scope of this paper. The ability to include prior information provides important flexibility and can be considered an advantage of the approach."
    }, {
      "heading" : "3.3 The Action Selection Strategy",
      "text" : "In DNG-MCTS, action selection strategy is derived using Thompson sampling. Specifically, in general Bayesian settings, action a is chosen with probability:\nP (a) = ∫ 1 [ a = argmax\na′ E [Xa′ |θa′ ] ]∏ a′ Pa′(θa′ |Z) dθ (4)\nwhere 1 is the indicator function, θa is the hidden parameter prescribing the underlying distribution of reward by applying a, E[Xa|θa] = ∫ xLa(x|θa) dx is the expectation of Xa given θa, and θ = (θa1 , θa2 , . . . ) is the vector of parameters for all actions. Fortunately, this can efficiently be approached by sampling method. To this end, a set of parameters θa is sampled according to the posterior distributions Pa(θa|Z) for each a ∈ A, and the action a∗ = argmaxa E[Xa|θa] with highest expectation is selected.\nIn our implementation, at each decision node s of the search tree, we sample the mean µs′ and mixture weights ws,a,s′ according to NormalGamma(µs′,0, λs′ , αs′ , βs′) and Dir(ρs,a) respectively for each possible next state s′ ∈ S. The expectation of Xs,a,π is then computed as R(s, a) + γ ∑ s′∈S ws,a,s′µs′ . The action with highest expectation is then selected to be performed in simulation."
    }, {
      "heading" : "3.4 The Main Algorithm",
      "text" : "The main process of DNG-MCTS is outlined in Figure 1. It is worth noting that the function ThompsonSampling has a boolean parameter sampling. If sampling is true, Thompson sampling method is used to select the best action as explained in Section 3.3, otherwise a greedy action is returned with respect to the current expected transition probabilities and accumulated rewards of next states, which are E[ws,a,s′ ] = ρs,a,s′/ ∑ x∈S ρs,a,x and E[Xs,π] = µs,0 respectively.\nAt each iteration, the function DNG-MCTS uses Thompson sampling to recursively select actions to be executed by simulation from the root node to leaf nodes through the existing search tree T . It inserts each newly visited node into the tree, plays a default rollout policy from the new node, and propagates the simulated outcome to update the hyper-parameters for visited states and actions. Noting that the rollout policy is only played once for each new node at each iteration, the set of past observations Z in the algorithm has size n = 1.\nThe function OnlinePlanning is the overall procedure interacting with the real environment. It is called with current state s, search tree T initially empty and the maximal horizon H . It repeatedly calls the function DNG-MCTS until some resource budgets are reached (e.g., the computation is timeout or the maximal number of iterations is reached), by when a greedy action to be performed in the environment is returned to the agent."
    }, {
      "heading" : "3.5 The Convergency Property",
      "text" : "For Thompson sampling in stationary MABs (i.e., the underlying reward function will not change), it is proved that: (1) the probability of selecting any suboptimal action a at the current step is bounded by a linear function of the probability of selecting the optimal action; (2) the coefficient in this linear function decreases exponentially fast with the increase in the number of selection of optimal action [15]. Thus, the probability of selecting the optimal action in an MAB is guaranteed to converge to 1 in the limit using Thompson sampling.\nThe distribution ofXs,π is determined by the transition function and theQ values given the policy π. When the Q values converge, the distribution of Xs,π becomes stationary with the optimal policy. For the leaf nodes (level H) of the search tree, Thompson sampling will converge to the optimal actions with probability 1 in the limit since the MABs are stationary. When all the leaf nodes converge, the distributions of return values from them will not change. So the MABs of the nodes in level H − 1 become stationary as well. Thus, Thompson sampling will also converge to the optimal actions for nodes in level H − 1. Recursively, this holds for all the upper-level nodes. Therefore, we conclude that DNG-MCTS can find the optimal policy for the root node if unbounded computational resources are given."
    }, {
      "heading" : "4 Experiments",
      "text" : "We have tested our DNG-MCTS algorithm and compared the results with UCT in three common MDP benchmark domains, namely Canadian traveler problem, racetrack and sailing. These problems are modeled as cost-based MDPs. That is, a cost function c(s, a) is used instead of the reward function R(s, a), and the min operator is used in the Bellman equation instead of the max operator. Similarly, the objective of solving a cost-based MDPs is to find an optimal policy that minimizes the expected accumulated cost for each state. Notice that algorithms developed for reward-based MDPs can be straightforwardly transformed and applied to cost-based MDPs by simply using the min operator instead of max in the Bellman update routines. Accordingly, the min operator is used in the function ThompsonSampling of our transformed DNG-MCTS algorithm. We implemented our codes and conducted the experiments on the basis of MDP-engine, which is an open source software package with a collection of problem instances and base algorithms for MDPs.1\n1MDP-engine can be publicly accessed via https://code.google.com/p/mdp-engine/\nIn each benchmark problem, we (1) ran the transformed algorithms for a number of iterations from the current state, (2) applied the best action based on the resulted action-values, (3) repeated the loop until terminating conditions (e.g., a goal state is satisfied or the maximal number of running steps is reached), and (4) reported the total discounted cost. The performance of algorithms is evaluated by the average value of total discounted costs over 1,000 independent runs. In all experiments, (µs,0, λs, αs, βs) is initialized to (0, 0.01, 1, 100), and ρs,a,s′ is initialized to 0.01 for all s ∈ S, a ∈ A and s′ ∈ S. For fair comparison, we also use the same settings as in [16]: for each decision node, (1) only applicable actions are selected, (2) applicable actions are forced to be selected once before any of them are selected twice or more, and 3) the exploration constant for the UCT algorithm is set to be the current mean action-values Q(s, a, d).\nThe Canadian traveler problem (CTP) is a path finding problem with imperfect information over a graph whose edges may be blocked with given prior probabilities [17]. A CTP can be modeled as a deterministic POMDP, i.e., the only source of uncertainty is the initial belief. When transformed to an MDP, the size of the belief space is n × 3m, where n is the number of nodes and m is the number of edges. This problem has a discount factor γ = 1. The aim is to navigate to the goal state as quickly as possible. It has recently been addressed by an anytime variation of AO*, named AOT [16], and two domain-specific implementations of UCT which take advantage of the specific MDP structure of the CTP and use a more informed base policy, named UCTB and UCTO [18]. In this experiment, we used the same 10 problem instances with 20 nodes as done in their papers.\nWhen running DNG-MCTS and UCT in those CTP instances, the number of iterations for each decision-making was set to be 10,000, which is identical to [16]. Two types of default rollout policy were tested: the random policy that selects actions with equal probabilities and the optimistic policy that assumes traversability for unknown edges and selects actions according to estimated cost. The results are shown in Table 1. Similar to [16], we included the results of UCTB and UCTO as a reference. From the table, we can see that DNG-MCTS outperformed the domain-independent version of UCT with random rollout policy in several instances, and particularly performed much better than UCT with optimistic rollout policy. Although DNG-MCTS is not as good as domainspecific UCTO, it is competitive comparing to the general UCT algorithm in this domain.\nThe racetrack problem simulates a car race [19], where a car starts in a set of initial states and moves towards the goal. At each time step, the car can choose to accelerate to one of the eight directions. When moving, the car has a possibility of 0.9 to succeed and 0.1 to fail on its acceleration. We tested DNG-MCTS and UCT with random rollout policy and horizon H = 100 in the instance of barto-big, which has a state space with size |S| = 22534. The discount factor is γ = 0.95 and the optimal cost produced is known to be 21.38. We reported the curve of the average cost as a function of the number of iterations in Figure 2a. Each data point in the figure was averaged over 1,000\nruns, each of which was allowed for running at most 100 steps. It can be seen from the figure that DNG-MCTS converged faster than UCT in terms of sample complexity in this domain.\nThe sailing domain is adopted from [3]. In this domain, a sailboat navigates to a destination on an 8-connected grid. The direction of the wind changes over time according to prior transition probabilities. The goal is to reach the destination as quickly as possible, by choosing at each grid location a neighbour location to move to. The discount factor in this domain is γ = 0.95 and the maximum horizon is set to be H = 100. We ran DNG-MCTS and UCT with random rollout policy in a 100× 100 instance of this domain. This instance has 80000 states and the optimal cost is 26.08. The performance curve is shown in Figure 2b. A trend similar to the racetrack problem can be observed in the graph: DNG-MCTS converged faster than UCT in terms of sample complexity.\nRegarding computational complexity, although the total computation time of our algorithm is linear with the total sample size, which is at most width × depth (width is the number of iterations and depth is the maximal horizon), our approach does require more computation than simple UCT methods. Specifically, we observed that most of the computation time of DNG-MCTS is due to the sampling from distributions in Thompson sampling. Thus, DNG-MCTS usually consumes more time than UCT in a single iteration. Based on our experimental results on the benchmark problems, DNG-MCTS typically needs about 2 to 4 times (depending on problems and the iterating stage of the algorithms) of computational time more than UCT algorithm for a single iteration. However, if the simulations are expensive (e.g., computational physics in 3D environment where the cost of executing the simulation steps greatly exceeds the time needed by action-selection steps in MCTS), DNG-MCTS can obtain much better performance than UCT in terms of computational complexity because DNG-MCTS is expected to have lower sample complexity."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed our DNG-MCTS algorithm — a novel Bayesian modeling and inference based Thompson sampling approach using MCTS for MDP online planning. The basic assumption of DNG-MCTS is modeling the uncertainty of the accumulated reward for each state-action pair as a mixture of Normal distributions. We presented the overall Bayesian framework for representing, updating, decision-making and propagating of probability distributions over rewards in the MCTS search tree. Our experimental results confirmed that, comparing to the general UCT algorithm, DNG-MCTS produced competitive results in the CTP domain, and converged faster in the domains of racetrack and sailing with respect to sample complexity. In the future, we plan to extend our basic assumption to using more complex distributions and test our algorithm on real-world applications."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported in part by the National Hi-Tech Project of China under grant 2008AA01Z150 and the Natural Science Foundation of China under grant 60745002 and 61175057. Feng Wu is supported in part by the ORCHID project (http://www.orchid.ac.uk). We are grateful to the anonymous reviewers for their constructive comments and suggestions."
    } ],
    "references" : [ {
      "title" : "Monte-carlo tree search and rapid action value estimation in computer go",
      "author" : [ "S. Gelly", "D. Silver" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Monte carlo tree search in lines of action",
      "author" : [ "Mark HM Winands", "Yngvi Bjornsson", "J Saito" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Bandit based monte-carlo planning",
      "author" : [ "L. Kocsis", "C. Szepesvári" ],
      "venue" : "In European Conference on Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Monte-carlo planning in large pomdps",
      "author" : [ "D. Silver", "J. Veness" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Online planning for ad hoc autonomous agent teams",
      "author" : [ "Feng Wu", "Shlomo Zilberstein", "Xiaoping Chen" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Efficient bayes-adaptive reinforcement learning using sample-based search",
      "author" : [ "Arthur Guez", "David Silver", "Peter Dayan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Learning is planning: near bayes-optimal reinforcement learning via monte-carlo tree search",
      "author" : [ "John Asmuth", "Michael L. Littman" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "William R. Thompson" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1933
    }, {
      "title" : "An empirical evaluation of thompson sampling",
      "author" : [ "Olivier Chapelle", "Lihong Li" ],
      "venue" : "In Advances Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Thompson sampling: An optimal finite time analysis",
      "author" : [ "Emilie Kaufmann", "Nathaniel Korda", "Rémi Munos" ],
      "venue" : "In Algorithmic Learning Theory, pages 199–213,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Bayesian q-learning",
      "author" : [ "Richard Dearden", "Nir Friedman", "Stuart Russell" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Bayesian inference in monte-carlo tree search",
      "author" : [ "Gerald Tesauro", "V.T. Rajan", "Richard Segal" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "On the markov chain central limit theorem",
      "author" : [ "Galin L Jones" ],
      "venue" : "Probability surveys,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "Asymptotic theory of statistics and probability",
      "author" : [ "Anirban DasGupta" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Further optimal regret bounds for thompson sampling",
      "author" : [ "Shipra Agrawal", "Navin Goyal" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Action selection for mdps: Anytime ao* vs. uct",
      "author" : [ "Blai Bonet", "Hector Geffner" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Shortest paths without a map",
      "author" : [ "Christos H Papadimitriou", "Mihalis Yannakakis" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1991
    }, {
      "title" : "High-quality policies for the canadian traveler’s problem",
      "author" : [ "Patrick Eyerich", "Thomas Keller", "Malte Helmert" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Learning to act using real-time dynamic programming",
      "author" : [ "A.G. Barto", "S.J. Bradtke", "S.P. Singh" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1995
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "To date, great success has been achieved by MCTS in variety of domains, such as game play [1, 2], planning under uncertainty [3, 4, 5], and Bayesian reinforcement learning [6, 7].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "To date, great success has been achieved by MCTS in variety of domains, such as game play [1, 2], planning under uncertainty [3, 4, 5], and Bayesian reinforcement learning [6, 7].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "To date, great success has been achieved by MCTS in variety of domains, such as game play [1, 2], planning under uncertainty [3, 4, 5], and Bayesian reinforcement learning [6, 7].",
      "startOffset" : 125,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "To date, great success has been achieved by MCTS in variety of domains, such as game play [1, 2], planning under uncertainty [3, 4, 5], and Bayesian reinforcement learning [6, 7].",
      "startOffset" : 125,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : "To date, great success has been achieved by MCTS in variety of domains, such as game play [1, 2], planning under uncertainty [3, 4, 5], and Bayesian reinforcement learning [6, 7].",
      "startOffset" : 125,
      "endOffset" : 134
    }, {
      "referenceID" : 5,
      "context" : "To date, great success has been achieved by MCTS in variety of domains, such as game play [1, 2], planning under uncertainty [3, 4, 5], and Bayesian reinforcement learning [6, 7].",
      "startOffset" : 172,
      "endOffset" : 178
    }, {
      "referenceID" : 6,
      "context" : "To date, great success has been achieved by MCTS in variety of domains, such as game play [1, 2], planning under uncertainty [3, 4, 5], and Bayesian reinforcement learning [6, 7].",
      "startOffset" : 172,
      "endOffset" : 178
    }, {
      "referenceID" : 7,
      "context" : "Thompson sampling is one of the earliest heuristics to address this dilemma in multi-armed bandit problems (MABs) according to the principle of randomized probability matching [8].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 8,
      "context" : "It has recently been shown to perform very well in MABs both empirically [9] and theoretically [10].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "It has recently been shown to perform very well in MABs both empirically [9] and theoretically [10].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 2,
      "context" : "Comparing to the UCB1 heuristic [3], the main advantage of Thompson sampling is that it allows more robust convergence under a wide range of problem settings.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "The UCT algorithm is a popular approach based on MCTS for planning under uncertainty [3].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "A similar assumption has been made in [11], where they assumed a Normal distribution over the rewards.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "Tesauro et al.[12] developed a Bayesian UCT approach to MCTS",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "According to the central limit theorem on Markov chains [13, 14], for any bounded function f on the state space S, we have:",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "According to the central limit theorem on Markov chains [13, 14], for any bounded function f on the state space S, we have:",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : ", the underlying reward function will not change), it is proved that: (1) the probability of selecting any suboptimal action a at the current step is bounded by a linear function of the probability of selecting the optimal action; (2) the coefficient in this linear function decreases exponentially fast with the increase in the number of selection of optimal action [15].",
      "startOffset" : 367,
      "endOffset" : 371
    }, {
      "referenceID" : 17,
      "context" : "UCTB and UCTO are the two domain-specific UCT implementations [18].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "The data of UCTB, UCTO and UCT are taken form [16].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 15,
      "context" : "For fair comparison, we also use the same settings as in [16]: for each decision node, (1) only applicable actions are selected, (2) applicable actions are forced to be selected once before any of them are selected twice or more, and 3) the exploration constant for the UCT algorithm is set to be the current mean action-values Q(s, a, d).",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "The Canadian traveler problem (CTP) is a path finding problem with imperfect information over a graph whose edges may be blocked with given prior probabilities [17].",
      "startOffset" : 160,
      "endOffset" : 164
    }, {
      "referenceID" : 15,
      "context" : "It has recently been addressed by an anytime variation of AO*, named AOT [16], and two domain-specific implementations of UCT which take advantage of the specific MDP structure of the CTP and use a more informed base policy, named UCTB and UCTO [18].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "It has recently been addressed by an anytime variation of AO*, named AOT [16], and two domain-specific implementations of UCT which take advantage of the specific MDP structure of the CTP and use a more informed base policy, named UCTB and UCTO [18].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 15,
      "context" : "When running DNG-MCTS and UCT in those CTP instances, the number of iterations for each decision-making was set to be 10,000, which is identical to [16].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "Similar to [16], we included the results of UCTB and UCTO as a reference.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 18,
      "context" : "The racetrack problem simulates a car race [19], where a car starts in a set of initial states and moves towards the goal.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "The sailing domain is adopted from [3].",
      "startOffset" : 35,
      "endOffset" : 38
    } ],
    "year" : 2013,
    "abstractText" : "Monte-Carlo tree search (MCTS) has been drawing great interest in recent years for planning and learning under uncertainty. One of the key challenges is the trade-off between exploration and exploitation. To address this, we present a novel approach for MCTS using Bayesian mixture modeling and inference based Thompson sampling and apply it to the problem of online planning in MDPs. Our algorithm, named Dirichlet-NormalGamma MCTS (DNG-MCTS), models the uncertainty of the accumulated reward for actions in the search tree as a mixture of Normal distributions. We perform inferences on the mixture in Bayesian settings by choosing conjugate priors in the form of combinations of Dirichlet and NormalGamma distributions and select the best action at each decision node using Thompson sampling. Experimental results confirm that our algorithm advances the state-of-the-art UCT approach with better values on several benchmark problems.",
    "creator" : null
  }
}