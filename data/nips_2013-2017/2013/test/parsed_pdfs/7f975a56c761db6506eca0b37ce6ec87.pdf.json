{
  "name" : "7f975a56c761db6506eca0b37ce6ec87.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distributed k-Means and k-Median Clustering on General Topologies",
    "authors" : [ "Maria Florina Balcan", "Steven Ehrlich", "Yingyu Liang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12]. In many of these applications the data is inherently distributed because, as in sensor networks, it is collected at different sites. As a consequence it has become crucial to develop clustering algorithms which are effective in the distributed setting.\nSeveral algorithms for distributed clustering have been proposed and empirically tested. Some of these algorithms [10, 22, 7] are direct adaptations of centralized algorithms which rely on statistics that are easy to compute in a distributed manner. Other algorithms [14, 17] generate summaries of local data and transmit them to a central coordinator which then performs the clustering algorithm. No theoretical guarantees are provided for the clustering quality in these algorithms, and they do not try to minimize the communication cost. Additionally, most of these algorithms assume that the distributed nodes can communicate with all other sites or that there is a central coordinator that communicates with all other sites.\nIn this paper, we study the problem of distributed clustering where the data is distributed across nodes whose communication is restricted to the edges of an arbitrary graph. We provide algorithms with small communication cost and provable guarantees on the clustering quality. Our technique for reducing communication in general graphs is based on the construction of a small set of points which act as a proxy for the entire data set.\nAn -coreset is a weighted set of points whose cost on any set of centers is approximately the cost of the original data on those same centers up to accuracy . Thus an approximate solution for the coreset is also an approximate solution for the original data. Coresets have previously been studied in the centralized setting ([13, 8]) but have also recently been used for distributed clustering as in [23] and as implied by [9]. In this work, we propose a distributed algorithm for k-means and k-\nmedian, by which each node constructs a local portion of a global coreset. Communicating the approximate cost of a global solution to each node is enough for the local construction, leading to low communication cost overall. The nodes then share the local portions of the coreset, which can be done efficiently in general graphs using a message passing approach.\nMore precisely, in Section 3, we propose a distributed coreset construction algorithm based on local approximate solutions. Each node computes an approximate solution for its local data, and then constructs the local portion of a coreset using only its local data and the total cost of each node’s approximation. For constant, this builds a coreset of size Õ(kd+nk) for k-median and k-means when the data lies in d dimensions and is distributed over n sites. If there is a central coordinator among the n sites, then clustering can be performed on the coordinator by collecting the local portions of the coreset with a communication cost equal to the coreset size Õ(kd + nk). For distributed clustering over general connected topologies, we propose an algorithm based on the distributed coreset construction and a message-passing approach, whose communication cost improves over previous coreset-based algorithms. We provide a detailed comparison below.\nExperimental results on large scale data sets show that our algorithm performs well in practice. For a fixed amount of communication, our algorithm outperforms other coreset construction algorithms.\nComparison to Other Coreset Algorithms: Since coresets summarize local information they are a natural tool to use when trying to reduce communication complexity. If each node constructs an - coreset on its local data, then the union of these coresets is clearly an -coreset for the entire data set. Unfortunately the size of the coreset in this approach increases greatly with the number of nodes.\nAnother approach is the one presented in [23]. Its main idea is to approximate the union of local coresets with another coreset. They assume nodes communicate over a rooted tree, with each node passing its coreset to its parent. Because the approximation factor of the constructed coreset depends on the quality of its component coresets, the accuracy a coreset needs (and thus the overall communication complexity) scales with the height of this tree. Although it is possible to find a spanning tree in any communication network, when the graph has large diameter every tree has large height. In particular many natural networks such as grid networks have a large diameter (Ω( √ n) for grids) which greatly increases the size of the local coresets. We show that it is possible to construct a global coreset with low communication overhead. This is done by distributing the coreset construction procedure rather than combining local coresets. The communication needed to construct this coreset is negligible – just a single value from each data set representing the approximate cost of their local optimal clustering. Since the sampled global -coreset is the same size as any local -coreset, this leads to an improvement of the communication cost over the other approaches. See Figure 1 for an illustration. The constructed coreset is smaller by a factor of n in general graphs, and is independent of the communication topology. This method excels in sparse networks with large diameters, where the previous approach in [23] requires coresets that are quadratic in the size of the diameter for k-median and quartic for k-means; see Section 4 for details. [9] also merge coresets using coreset construction, but they do so in a model of parallel computation and ignore communication costs.\nBalcan et al. [3] and Daume et al. [6] consider communication complexity questions arising when doing classification in distributed settings. In concurrent and independent work, Kannan and Vem-\npala [15] study several optimization problems in distributed settings, including k-means clustering under an interesting separability assumption."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Let d(p, q) denote the Euclidean distance between any two points p, q ∈ Rd. The goal of k-means clustering is to find a set of k centers x = {x1, x2, . . . , xk}which minimize the k-means cost of data set P ⊆ Rd. Here the k-means cost is defined as cost(P,x) = ∑p∈P d(p,x)2 where d(p,x) = minx∈x d(p, x). If P is a weighted data set with a weighting function w, then the k-means cost is defined as ∑ p∈P w(p)d(p,x) 2. Similarly, the k-median cost is defined as ∑ p∈P d(p,x). Both k-means and k-median cost functions are known to be NP-hard to minimize (see for example [2]). For both objectives, there exist several readily available polynomial-time algorithms that achieve constant approximation solutions (see for example [16, 18]).\nIn distributed clustering, we consider a set of n nodes V = {vi, 1 ≤ i ≤ n} which communicate on an undirected connected graph G = (V,E) with m = |E| edges. More precisely, an edge (vi, vj) ∈ E indicates that vi and vj can communicate with each other. Here we measure the communication cost in number of points transmitted, and assume for simplicity that there is no latency in the communication. On each node vi, there is a local data set Pi, and the global data set is P = ⋃n i=1 Pi. The goal is to find a set of k centers x which optimize cost(P,x) while keeping the computation efficient and the communication cost as low as possible. Our focus is to reduce the communication cost while preserving theoretical guarantees for approximating clustering cost.\nCoresets: For the distributed clustering task, a natural approach to avoid broadcasting raw data is to generate a local summary of the relevant information. If each site computes a summary for their own data set and then communicates this to a central coordinator, a solution can be computed from a much smaller amount of data, drastically reducing the communication.\nIn the centralized setting, the idea of summarization with respect to the clustering task is captured by the concept of coresets [13, 8]. A coreset is a set of weighted points whose cost approximates the cost of the original data for any set of k centers. The formal definition of coresets is: Definition 1 (coreset). An -coreset for a set of points P with respect to a center-based cost function is a set of points S and a set of weights w : S → R such that for any set of centers x, we have (1− )cost(P,x) ≤∑p∈S w(p)cost(p,x) ≤ (1 + )cost(P,x). In the centralized setting, many coreset construction algorithms have been proposed for k-median, k-means and some other cost functions. For example, for points in Rd, algorithms in [8] construct coresets of size Õ(kd/ 4) for k-means and coresets of size Õ(kd/ 2) for k-median. In the distributed setting, it is natural to ask whether there exists an algorithm that constructs a small coreset for the entire point set but still has low communication cost. Note that the union of coresets for multiple data sets is a coreset for the union of the data sets. The immediate construction of combining the local coresets from each node would produce a global coreset whose size was larger by a factor of n, greatly increasing the communication complexity. We present a distributed algorithm which constructs a global coreset the same size as the centralized construction and only needs a single value1 communicated to each node. This serves as the basis for our distributed clustering algorithm."
    }, {
      "heading" : "3 Distributed Coreset Construction",
      "text" : "Here we design a distributed coreset construction algorithm for k-means and k-median. The underlying technique can be extended to other additive clustering objectives such as k-line median.\nTo gain some intuition on the distributed coreset construction algorithm, we briefly review the construction algorithm in [8] in the centralized setting. The coreset is constructed by computing a constant approximation solution for the entire data set, and then sampling points proportional to their contributions to the cost of this solution. Intuitively, the points close to the nearest centers can be approximately represented by the centers while points far away cannot be well represented. Thus, points should be sampled with probability proportional to their contributions to the cost. Directly adapting the algorithm to the distributed setting would require computing a constant approximation\n1The value that is communicated is the sum of the costs of approximations to the local optimal clustering. This is guaranteed to be no more than a constant factor times larger than the optimal cost.\nAlgorithm 1 Communication aware distributed coreset construction Input: Local datasets {Pi, 1 ≤ i ≤ n}, parameter t (number of points to be sampled).\nRound 1: on each node vi ∈ V • Compute a constant approximation Bi for Pi.\nCommunicate cost(Pi, Bi) to all other nodes. Round 2: on each node vi ∈ V • Set ti = t cost(Pi,Bi)∑n\nj=1 cost(Pj ,Bj) and mp = cost(p,Bi),∀p ∈ Pi.\n• Pick a non-uniform random sample Si of ti points from Pi, where for every q ∈ Si and p ∈ Pi, we have q = p with probability mp/ ∑ z∈Pi mz .\nLet wq = ∑ i ∑ z∈Pi mz\ntmq for each q ∈ Si. • For ∀b ∈ Bi, let Pb = {p ∈ Pi : d(p, b) = d(p,Bi)}, wb = |Pb| − ∑ q∈Pb∩S wq.\nOutput: Distributed coreset: points Si ∪Bi with weights {wq : q ∈ Si ∪Bi}, 1 ≤ i ≤ n.\nsolution for the entire data set. We show that a global coreset can be constructed in a distributed fashion by estimating the weight of the entire data set with the sum of local approximations. With this approach, it suffices for nodes to communicate the total costs of their local solutions. Theorem 1. For distributed k-means and k-median clustering on a graph, there exists an algorithm such that with probability at least 1 − δ, the union of its output on all nodes is an -coreset for P = ⋃n i=1 Pi. The size of the coreset isO( 1 4 (kd+log 1 δ )+nk log nk δ ) for k-means, andO( 1 2 (kd+ log 1δ ) + nk) for k-median. The total communication cost is O(mn).\nAs described below, the distributed coreset construction can be achieved by using Algorithm 1 with appropriate t, namely O( 1 4 (kd + log 1 δ ) + nk log nk δ ) for k-means and O( 1 2 (kd + log 1 δ )) for kmedian. Due to space limitation, we describe a proof sketch highlighting the intuition and provide the details in the supplementary material.\nProof Sketch of Theorem 1: The analysis relies on the definition of the pseudo-dimension of a function space and a sampling lemma. Definition 2 ([19, 8]). Let F be a finite set of functions from a set P to R≥0. For f ∈ F , let B(f, r) = {p : f(p) ≤ r}. The dimension of the function space dim(F, P ) is the smallest integer d such that for any G ⊆ P ,\n∣∣{G ∩B(f, r) : f ∈ F, r ≥ 0}∣∣ ≤ |G|d. Suppose we draw a sample S according to {mp : p ∈ P}, namely for each q ∈ S and p ∈ P , q = p with probability mp∑\nz∈P mz . Set the weights of the points as wp = ∑ z∈P mz mp|S| for p ∈ P . Then for\nany f ∈ F , the expectation of the weighted cost of S equals the cost of the original data P , since E [∑ q∈S wqf(q) ] = ∑ q∈S E[wqf(q)] = ∑ q∈S ∑ p∈P Pr[q = p]wpf(p) = ∑ p∈P f(p).\nIf the sample size is large enough, then we also have concentration for any f ∈ F . The lemma is implicit in [8] and we include the proof in the supplementary material. Lemma 1. Fix a set F of functions f : P → R≥0. Let S be a sample drawn i.i.d. from P according to {mp ∈ R≥0 : p ∈ P}: for each q ∈ S and p ∈ P , q = p with probability mp∑\nz∈P mz . Let wp =∑\nz∈P mz mp|S| for p ∈ P . For a sufficiently large c, if |S| ≥ c 2 ( dim(F, P ) + log 1δ ) , then with probabil-\nity at least 1− δ, ∀f ∈ F : ∣∣∣∑p∈P f(p)−∑q∈S wqf(q)∣∣∣ ≤ (∑p∈P mp)(maxp∈P f(p)mp ) .\nTo get a small bound on the difference between ∑ p∈P f(p) and ∑ q∈S wqf(q), we need to choose\nmp such that ∑ p∈P mp is small and maxp∈P f(p) mp\nis bounded. More precisely, if we choose mp = maxf∈F f(p), then the difference is bounded by ∑ p∈P mp.\nWe first consider the centralized setting and review how [8] applied the lemma to construct a coreset for k-median as in Definition 1. A natural approach is to apply this lemma directly to the cost fx(p) := cost(p,x). The problem is that a suitable upper bound mp is not available for cost(p,x). However, we can still apply the lemma to a different set of functions defined as follows. Let bp denote the closest center to p in the approximation solution. Aiming to approximate\nthe error ∑ p[cost(p,x)− cost(bp,x)] rather than to approximate ∑ p cost(p,x) directly, we define fx(p) := cost(p,x)−cost(bp,x)+cost(p, bp), where cost(p, bp) is added so that fx(p) ≥ 0. Since 0 ≤ fx(p) ≤ 2cost(p, bp), we can apply the lemma with mp = 2cost(p, bp). It bounds the difference |∑p∈P fx(p)−∑q∈S wqfx(q)| by 2 ∑p∈P cost(p, bp), so we have an O( )-approximation. Note that ∑ p∈P fx(p) − ∑ q∈S wqfx(q) does not equal ∑ p∈P cost(p,x) − ∑ q∈S wqcost(q,x).\nHowever, it equals the difference between ∑ p∈P cost(p,x) and a weighted cost of the sampled points and the centers in the approximation solution. To get a coreset as in Definition 1, we need to add the centers of the approximation solution with specific weights to the coreset. Then when the sample is sufficiently large, the union of the sampled points and the centers is an -coreset.\nOur key contribution in this paper is to show that in the distributed setting, it suffices to choose bp from the local approximation solution for the local dataset containing p, rather than from an approximation solution for the global dataset. Furthermore, the sampling and the weighting of the coreset points can be done in a local manner. In the following, we provide a formal verification of our discussion above. We have the following lemma for k-median with F = {fx : fx(p) = d(p,x)− d(bp,x) + d(p, bp),x ∈ (Rd)k}. Lemma 2. For k-median, the output of Algorithm 1 is an -coreset with probability at least 1 − δ, if t ≥ c 2 ( dim(F, P ) + log 1δ ) for a sufficiently large constant c.\nProof Sketch of Lemma 2: We want to show that for any set of centers x the true cost for using these centers is well approximated by the cost on the weighted coreset. Note that our coreset has two types of points: sampled points q ∈ S = ∪ni=1Si with weight wq := ∑ z∈P mz mq|S| and local solution\ncenters b ∈ B = ∪ni=1Bi with weight wb := |Pb|− ∑ q∈S∩Pb wq . We use bp to represent the nearest center to p in the local approximation solution. We use Pb to represent the set of points which have b as their closest center in the local approximation solution.\nAs mentioned above, we construct fx(p) to be the difference between the cost of p and the cost of bp so that Lemma 1 can be applied. Note that the centers are weighted such that ∑ b∈B wbd(b,x) = ∑ b∈B |Pb|d(b,x) − ∑ b∈B ∑ q∈S∩Pb wqd(b,x) = ∑ p∈P d(bp,x) −∑\nq∈S wqd(bq,x). Taken together with the fact that ∑ p∈P mp = ∑ q∈S wqmq , we can show\nthat ∣∣∣∑p∈P d(p,x)−∑q∈S∪B wqd(q,x)∣∣∣ = ∣∣∣∑p∈P fx(p)−∑q∈S wqfx(q)∣∣∣. Note that 0 ≤ fx(p) ≤ 2d(p, bp) by triangle inequality, and S is sufficiently large and chosen according to weights mp = d(p, bp), so the conditions of Lemma 1 are met. Thus we can conclude that∣∣∣∑p∈P d(p,x)−∑q∈S∪B wqd(q,x)∣∣∣ ≤ O( )∑p∈P d(p,x), as desired. In [8] it is shown that dim(F, P ) = O(kd). Therefore, by Lemma 2, when |S| ≥ O ( 1 2 (kd+ log 1 δ ) ) , the weighted cost of S ∪ B approximates the k-median cost of P for any set of centers, then (S ∪ B,w) becomes an -coreset for P . The total communication cost is bounded by O(mn), since even in the most general case that every node only knows its neighbors, we can broadcast the local costs with O(mn) communication (see Algorithm 3).\nProof Sketch for k-means: Similar methods prove that for k-means when t = O( 1 4 (kd+ log 1 δ ) + nk log nkδ )), the algorithm constructs an -coreset with probability at least 1− δ. The key difference is that triangle inequality does not apply directly to the k-means cost, and so the error |cost(p,x)− cost(bp,x)| and thus fx(p) are not bounded. The main change to the analysis is that we divide the points into two categories: good points whose costs approximately satisfy the triangle inequality (up to a factor of 1/ ) and bad points. The good points for a fixed set of centers x are defined as G(x) = {p ∈ P : |cost(p,x)− cost(bp,x)| ≤ ∆p} where the upper bound is ∆p = cost(p,bp) , and the analysis follows as in Lemma 2. For bad points we can show that the difference in cost must still be small, namely O( min{cost(p,x), cost(bp,x)}). More formally, let fx(p) = cost(p,x)− cost(bp,x) + ∆p, and let gx(p) be fx(p) if p ∈ G(x) and 0 otherwise. Then ∑ p∈P cost(p,x)−\n∑ q∈S∪B wqcost(q,x) is decomposed into three terms:∑ p∈P gx(p)− ∑ q∈S\nwqgx(q)︸ ︷︷ ︸ (A)\n+ ∑\np∈P\\G(x) fx(p)︸ ︷︷ ︸ (B)\n− ∑\nq∈S\\G(x) wqfx(q)︸ ︷︷ ︸ (C)\nAlgorithm 2 Distributed clustering on a graph Input: {Pi, 1 ≤ i ≤ n}: local datasets; {Ni, 1 ≤ i ≤ n}: the neighbors of vi; Aα: an αapproximation algorithm for weighted clustering instances.\nRound 1: on each node vi • Construct its local portion Di of an /2-coreset by Algorithm 1,\nusing Message-Passing for communicating the local costs. Round 2: on each node vi • Call Message-Passing(Di, Ni). Compute x = Aα( ⋃ j Dj).\nOutput: x\nAlgorithm 3 Message-Passing(Ii, Ni) Input: Ii is the message, Ni are the neighbors.\n• Let Ri denote the information received. Initialize Ri = {Ii}, and send Ii to Ni. •While Ri 6= {Ij , 1 ≤ j ≤ n}:\nIf receive message Ij 6∈ Ri, then let Ri = Ri ∪ {Ij} and send Ij to Ni.\nLemma 1 bounds (A) by O( )cost(P,x), but we need an accuracy of 2 to compensate for the 1/ factor in the upper bound of fx(p). This leads to an O(1/ 4) factor in the sample complexity.\nFor (B) and (C), |cost(p,x) − cost(bp,x)| > ∆p since p 6∈ G(x). This can be used to show that p and bp are close to each other and far away from x, and thus |cost(p,x) − cost(bp,x)| is O( ) smaller than cost(p,x) and cost(bp,x). This fact bounds ((B)) by O( )cost(P,x). It also bounds (C), noting that E[ ∑ q∈Pb∩S wq] = |Pb|, and thus ∑ q∈Pb∩S wq ≤ 2|Pb| when t ≥ O(nk log nk δ ). The proof is completed by bounding the function space dimension by O(kd) as in [8]."
    }, {
      "heading" : "4 Effect of Network Topology on Communication Cost",
      "text" : "If there is a central coordinator in the communication graph, then we can run distributed coreset construction algorithm and send the local portions of the coreset to the coordinator, which can perform the clustering task. The total communication cost is just the size of the coreset.\nIn this section, we consider distributed clustering over arbitrary connected topologies. We propose to use a message passing approach for collecting information for coreset construction and sharing the local portions of the coreset. The details are presented in Algorithm 2 and 3. Since each piece of the coreset is shared at most twice across any particular edge in message passing, we have Theorem 2. Given an α-approximation algorithm for weighted k-means (k-median respectively) as a subroutine, there exists an algorithm that with probability at least 1 − δ outputs a (1 + )αapproximation solution for distributed k-means (k-median respectively). The communication cost is O(m( 1 4 (kd+ log 1 δ ) + nk log nk δ )) for k-means, and O(m( 1 2 (kd+ log 1 δ ) + nk)) for k-median.\nIn contrast, an approach where each node constructs an -coreset for k-means and sends it to the other nodes incurs communication cost of Õ(mnkd 4 ). Our algorithm significantly reduces this.\nOur algorithm can also be applied on a rooted tree: we can send the coreset portions to the root which then applies an approximation algorithm. Since each portion are transmitted at most h times, Theorem 3. Given an α-approximation algorithm for weighted k-means (k-median respectively) as a subroutine, there exists an algorithm that with probability at least 1 − δ outputs a (1 + )αapproximation solution for distributed k-means (k-median respectively) clustering on a rooted tree of height h. The total communication cost is O(h( 1 4 (kd + log 1 δ ) + nk log nk δ )) for k-means, and O(h( 1 2 (kd+ log 1 δ ) + nk)) for k-median.\nOur approach improves the cost of Õ(nh 4kd 4 ) for k-means and the cost of Õ( nh2kd 2 ) for k-median in [23] 2. The algorithm in [23] builds on each node a coreset for the union of coresets from its 2 Their algorithm used coreset construction as a subroutine. The construction algorithm they used builds coreset of size Õ(nkh d\nlog |P |). Throughout this paper, when we compare to [23] we assume they use the coreset construction technique of [8] to reduce their coreset size and communication cost.\nchildren, and thus needs O( /h) accuracy to prevent the accumulation of errors. Since the coreset construction subroutine has quadratic dependence on 1/ for k-median (quartic for k-means), the algorithm then has quadratic dependence on h (quartic for k-means). Our algorithm does not build coreset on top of coresets, resulting in a better dependence on the height of the tree h.\nIn a general graph, any rooted tree will have its height h at least as large as half the diameter. For sensors in a grid network, this implies h = Ω( √ n). In this case, our algorithm gains a significant improvement over existing algorithms."
    }, {
      "heading" : "5 Experiments",
      "text" : "Here we evaluate the effectiveness of our algorithm and compare it to other distributed coreset algorithms. We present the k-means cost of the solution by our algorithm with varying communication cost, and compare to those of other algorithms when they use the same amount of communication.\nData sets: We present results on YearPredictionMSD (515345 points in R90, k = 50). Similar results are observed on five other datasets, which are presented in the supplementary material.\nExperimental Methodology: We first generate a communication graph connecting local sites, and then partition the data into local data sets. The algorithms were evaluated on Erdös-Renyi random graphs with p = 0.3, grid graphs, and graphs generated by the preferential attachment mechanism [1]. We used 100 sites for YearPredictionMSD.\nThe data is then distributed over the local sites. There are four partition methods: uniform, similarity-based, weighted, and degree-based. In all methods, each example is distributed to the local sites with probability proportional to the site’s weight. In uniform partition, the sites have equal weights; in similarity-based partition, each site has an associated data point randomly selected from the global data and the weight is the similarity to the associated point; in weighted partition, the weights are chosen from |N(0, 1)|; in degree-based, the weights are the sites’ degrees. To measure the quality of the coreset generated, we run Lloyd’s algorithm on the coreset and the global data respectively to get two solutions, and compute the ratio between the costs of the two solutions over the global data. The average ratio over 30 runs is then reported. We compare our algorithm with COMBINE, the method of combining coresets from local data sets, and with the algorithm of [23] (Zhang et al.). When running the algorithm of Zhang et al., we restrict the network to a spanning tree by picking a root uniformly at random and performing a breadth first search.\nResults: Figure 2 shows the results over different network topologies and partition methods. We observe that the algorithms perform well with much smaller coreset sizes than predicted by the theoretical bounds. For example, to get 1.1 cost ratio, the coreset size and thus the communication needed is only 0.1%− 1% of the theoretical bound. In the uniform partition, our algorithm performs nearly the same as COMBINE. This is not surprising since our algorithm reduces to the COMBINE algorithm when each local site has the same cost and the two algorithms use the same amount of communication. In this case, since in our algorithm the sizes of the local samples are proportional to the costs of the local solutions, it samples the same number of points from each local data set. This is equivalent to the COMBINE algorithm with the same amount of communication. In the similarity-based partition, similar results are observed as it also leads to balanced local costs. However, when the local sites have significantly different costs (as in the weighted and degree-based partitions), our algorithm outperforms COMBINE. As observed in Figure 2, the costs of our solutions consistently improve over those of COMBINE by 2% − 5%. Our algorithm then saves 10%− 20% communication cost to achieve the same approximation ratio. Figure 3 shows the results over the spanning trees of the graphs. Our algorithm performs much better than the algorithm of Zhang et al., achieving about 20% improvement in cost. This is due to the fact that their algorithm needs larger coresets to prevent the accumulation of errors when constructing coresets from component coresets, and thus needs higher communication cost to achieve the same approximation ratio.\nAcknowledgements This work was supported by ONR grant N00014-09-1-0751, AFOSR grant FA9550-09-1-0538, and by a Google Research Award. We thank Le Song for generously allowing us to use his computer cluster."
    } ],
    "references" : [ {
      "title" : "Statistical mechanics of complex networks",
      "author" : [ "R. Albert", "A.-L. Barabási" ],
      "venue" : "Reviews of Modern Physics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Center based clustering: A foundational perspective",
      "author" : [ "P. Awasthi", "M. Balcan" ],
      "venue" : "Survey Chapter in Handbook of Cluster Analysis (Manuscript),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "M.-F. Balcan", "A. Blum", "S. Fine", "Y. Mansour" ],
      "venue" : "In Proceedings of the Conference on Learning Thoery,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Approximate aggregation techniques for sensor databases",
      "author" : [ "J. Considine", "F. Li", "G. Kollios", "J. Byers" ],
      "venue" : "In Proceedings of the International Conference on Data Engineering,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2004
    }, {
      "title" : "Spanner: Googles globally-distributed database",
      "author" : [ "J.C. Corbett", "J. Dean", "M. Epstein", "A. Fikes", "C. Frost", "J. Furman", "S. Ghemawat", "A. Gubarev", "C. Heiser", "P. Hochschild" ],
      "venue" : "In Proceedings of the USENIX Symposium on Operating Systems Design and Implementation,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Efficient protocols for distributed classification and optimization",
      "author" : [ "H. Daumé III", "J.M. Phillips", "A. Saha", "S. Venkatasubramanian" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "K-means clustering over peer-to-peer networks",
      "author" : [ "S. Dutta", "C. Gianella", "H. Kargupta" ],
      "venue" : "In Proceedings of the International Workshop on High Performance and Distributed Mining,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "A unified framework for approximating and clustering data",
      "author" : [ "D. Feldman", "M. Langberg" ],
      "venue" : "In Proceedings of the Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "An effective coreset compression algorithm for large scale sensor networks",
      "author" : [ "D. Feldman", "A. Sugaya", "D. Rus" ],
      "venue" : "In Proceedings of the International Conference on Information Processing in Sensor Networks,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Distributed data clustering can be efficient and exact",
      "author" : [ "G. Forman", "B. Zhang" ],
      "venue" : "ACM SIGKDD Explorations Newsletter,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2000
    }, {
      "title" : "Distributed query processing for mobile surveillance",
      "author" : [ "S. Greenhill", "S. Venkatesh" ],
      "venue" : "In Proceedings of the International Conference on Multimedia,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Power-conserving computation of order-statistics over sensor networks",
      "author" : [ "M. Greenwald", "S. Khanna" ],
      "venue" : "In Proceedings of the ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "On coresets for k-means and k-median clustering",
      "author" : [ "S. Har-Peled", "S. Mazumdar" ],
      "venue" : "In Proceedings of the Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "Towards effective and efficient distributed clustering",
      "author" : [ "E. Januzaj", "H. Kriegel", "M. Pfeifle" ],
      "venue" : "In Workshop on Clustering Large Data Sets in the IEEE International Conference on Data Mining,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2003
    }, {
      "title" : "Nimble algorithms for cloud computing",
      "author" : [ "R. Kannan", "S. Vempala" ],
      "venue" : "arXiv preprint arXiv:1304.3162,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "A local search approximation algorithm for k-means clustering",
      "author" : [ "T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu" ],
      "venue" : "In Proceedings of the Annual Symposium on Computational Geometry,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2002
    }, {
      "title" : "Distributed clustering using collective principal component analysis",
      "author" : [ "H. Kargupta", "W. Huang", "K. Sivakumar", "E. Johnson" ],
      "venue" : "Knowledge and Information Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2001
    }, {
      "title" : "Approximating k-median via pseudo-approximation",
      "author" : [ "S. Li", "O. Svensson" ],
      "venue" : "In Proceedings of the Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Improved bounds on the sample complexity of learning",
      "author" : [ "Y. Li", "P.M. Long", "A. Srinivasan" ],
      "venue" : "In Proceedings of the eleventh annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2000
    }, {
      "title" : "Characterizing webbased video sharing workloads",
      "author" : [ "S. Mitra", "M. Agrawal", "A. Yadav", "N. Carlsson", "D. Eager", "A. Mahanti" ],
      "venue" : "ACM Transactions on the Web,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Adaptive filters for continuous queries over distributed data streams",
      "author" : [ "C. Olston", "J. Jiang", "J. Widom" ],
      "venue" : "In Proceedings of the ACM SIGMOD International Conference on Management of Data,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2003
    }, {
      "title" : "Unsupervised distributed clustering",
      "author" : [ "D. Tasoulis", "M. Vrahatis" ],
      "venue" : "In Proceedings of the International Conference on Parallel and Distributed Computing and Networks,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2004
    }, {
      "title" : "Approximate clustering on distributed data streams",
      "author" : [ "Q. Zhang", "J. Liu", "W. Wang" ],
      "venue" : "In Proceedings of the IEEE International Conference on Data Engineering,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Following a classic approach in clustering by [13], we reduce the problem of finding a clustering with low cost to the problem of finding a coreset of small size.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 20,
      "context" : "Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12].",
      "startOffset" : 181,
      "endOffset" : 188
    }, {
      "referenceID" : 4,
      "context" : "Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12].",
      "startOffset" : 181,
      "endOffset" : 188
    }, {
      "referenceID" : 19,
      "context" : "Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 10,
      "context" : "Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12].",
      "startOffset" : 241,
      "endOffset" : 245
    }, {
      "referenceID" : 3,
      "context" : "Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12].",
      "startOffset" : 266,
      "endOffset" : 273
    }, {
      "referenceID" : 11,
      "context" : "Most classic clustering algorithms are designed for the centralized setting, but in recent years data has become distributed over different locations, such as distributed databases [21, 5], images and videos over networks [20], surveillance [11] and sensor networks [4, 12].",
      "startOffset" : 266,
      "endOffset" : 273
    }, {
      "referenceID" : 9,
      "context" : "Some of these algorithms [10, 22, 7] are direct adaptations of centralized algorithms which rely on statistics that are easy to compute in a distributed manner.",
      "startOffset" : 25,
      "endOffset" : 36
    }, {
      "referenceID" : 21,
      "context" : "Some of these algorithms [10, 22, 7] are direct adaptations of centralized algorithms which rely on statistics that are easy to compute in a distributed manner.",
      "startOffset" : 25,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Some of these algorithms [10, 22, 7] are direct adaptations of centralized algorithms which rely on statistics that are easy to compute in a distributed manner.",
      "startOffset" : 25,
      "endOffset" : 36
    }, {
      "referenceID" : 13,
      "context" : "Other algorithms [14, 17] generate summaries of local data and transmit them to a central coordinator which then performs the clustering algorithm.",
      "startOffset" : 17,
      "endOffset" : 25
    }, {
      "referenceID" : 16,
      "context" : "Other algorithms [14, 17] generate summaries of local data and transmit them to a central coordinator which then performs the clustering algorithm.",
      "startOffset" : 17,
      "endOffset" : 25
    }, {
      "referenceID" : 12,
      "context" : "Coresets have previously been studied in the centralized setting ([13, 8]) but have also recently been used for distributed clustering as in [23] and as implied by [9].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Coresets have previously been studied in the centralized setting ([13, 8]) but have also recently been used for distributed clustering as in [23] and as implied by [9].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "Coresets have previously been studied in the centralized setting ([13, 8]) but have also recently been used for distributed clustering as in [23] and as implied by [9].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "Coresets have previously been studied in the centralized setting ([13, 8]) but have also recently been used for distributed clustering as in [23] and as implied by [9].",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "Another approach is the one presented in [23].",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "This method excels in sparse networks with large diameters, where the previous approach in [23] requires coresets that are quadratic in the size of the diameter for k-median and quartic for k-means; see Section 4 for details.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "[9] also merge coresets using coreset construction, but they do so in a model of parallel computation and ignore communication costs.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] consider communication complexity questions arising when doing classification in distributed settings.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 14,
      "context" : "pala [15] study several optimization problems in distributed settings, including k-means clustering under an interesting separability assumption.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "Both k-means and k-median cost functions are known to be NP-hard to minimize (see for example [2]).",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "For both objectives, there exist several readily available polynomial-time algorithms that achieve constant approximation solutions (see for example [16, 18]).",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : "For both objectives, there exist several readily available polynomial-time algorithms that achieve constant approximation solutions (see for example [16, 18]).",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 12,
      "context" : "In the centralized setting, the idea of summarization with respect to the clustering task is captured by the concept of coresets [13, 8].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "In the centralized setting, the idea of summarization with respect to the clustering task is captured by the concept of coresets [13, 8].",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "For example, for points in R, algorithms in [8] construct coresets of size Õ(kd/ (4)) for k-means and coresets of size Õ(kd/ (2)) for k-median.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "To gain some intuition on the distributed coreset construction algorithm, we briefly review the construction algorithm in [8] in the centralized setting.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "The lemma is implicit in [8] and we include the proof in the supplementary material.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "We first consider the centralized setting and review how [8] applied the lemma to construct a coreset for k-median as in Definition 1.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "In [8] it is shown that dim(F, P ) = O(kd).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "The proof is completed by bounding the function space dimension by O(kd) as in [8].",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 22,
      "context" : "Our approach improves the cost of Õ( (4)kd 4 ) for k-means and the cost of Õ( nh(2)kd 2 ) for k-median in [23] 2.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 22,
      "context" : "The algorithm in [23] builds on each node a coreset for the union of coresets from its 2 Their algorithm used coreset construction as a subroutine.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 22,
      "context" : "Throughout this paper, when we compare to [23] we assume they use the coreset construction technique of [8] to reduce their coreset size and communication cost.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 7,
      "context" : "Throughout this paper, when we compare to [23] we assume they use the coreset construction technique of [8] to reduce their coreset size and communication cost.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "3, grid graphs, and graphs generated by the preferential attachment mechanism [1].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "We compare our algorithm with COMBINE, the method of combining coresets from local data sets, and with the algorithm of [23] (Zhang et al.",
      "startOffset" : 120,
      "endOffset" : 124
    } ],
    "year" : 2013,
    "abstractText" : "This paper provides new algorithms for distributed clustering for two popular center-based objectives, k-median and k-means. These algorithms have provable guarantees and improve communication complexity over existing approaches. Following a classic approach in clustering by [13], we reduce the problem of finding a clustering with low cost to the problem of finding a coreset of small size. We provide a distributed method for constructing a global coreset which improves over the previous methods by reducing the communication complexity, and which works over general communication topologies. Experimental results on large scale data sets show that this approach outperforms other coreset-based distributed clustering algorithms.",
    "creator" : null
  }
}