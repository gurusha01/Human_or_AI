{
  "name" : "39027dfad5138c9ca0c474d71db915c3.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "The Self-Normalized Estimator for Counterfactual Learning",
    "authors" : [ "Adith Swaminathan" ],
    "emails" : [ "adith@cs.cornell.edu", "tj@cs.cornell.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Most interactive systems (e.g. search engines, recommender systems, ad platforms) record large quantities of log data which contain valuable information about the system’s performance and user experience. For example, the logs of an ad-placement system record which ad was presented in a given context and whether the user clicked on it. While these logs contain information that should inform the design of future systems, the log entries do not provide supervised training data in the conventional sense. This prevents us from directly employing supervised learning algorithms to improve these systems. In particular, each entry only provides bandit feedback since the loss/reward is only observed for the particular action chosen by the system (e.g. the presented ad) but not for all the other actions the system could have taken. Moreover, the log entries are biased since actions that are systematically favored by the system will by over-represented in the logs.\nLearning from historical logs data can be formalized as batch learning from logged bandit feedback (BLBF) [2, 1]. Unlike the well-studied problem of online learning from bandit feedback [3], this setting does not require the learner to have interactive control over the system. Learning in such a setting is closely related to the problem of off-policy evaluation in reinforcement learning [4] – we would like to know how well a new system (policy) would perform if it had been used in the past. This motivates the use of counterfactual estimators [5]. Following an approach analogous to Empirical Risk Minimization (ERM), it was shown that such estimators can be used to design learning algorithms for batch learning from logged bandit feedback [6, 5, 1].\nHowever the conventional counterfactual risk estimator used in prior works on BLBF exhibits severe anomalies that can lead to degeneracies when used in ERM. In particular, the estimator exhibits a new form of Propensity Overfitting that causes severely biased risk estimates for the ERM minimizer. By introducing multiplicative control variates, we propose to replace this risk estimator with a Self-Normalized Risk Estimator that provably avoids these degeneracies. An extensive empirical evaluation confirms that the desirable theoretical properties of the Self-Normalized Risk Estimator translate into improved generalization performance and robustness."
    }, {
      "heading" : "2 Related work",
      "text" : "Batch learning from logged bandit feedback is an instance of causal inference. Classic inference techniques like propensity score matching [7] are, hence, immediately relevant. BLBF is closely related to the problem of learning under covariate shift (also called domain adaptation or sample bias correction) [8] as well as off-policy evaluation in reinforcement learning [4]. Lower bounds for domain adaptation [8] and impossibility results for off-policy evaluation [9], hence, also apply to propensity score matching [7], costing [10] and other importance sampling approaches to BLBF.\nSeveral counterfactual estimators have been developed for off-policy evaluation [11, 6, 5]. All these estimators are instances of importance sampling for Monte Carlo approximation and can be traced back to What-If simulations [12]. Learning (upper) bounds have been developed recently [13, 1, 14] that show that these estimators can work for BLBF. We additionally show that importance sampling can overfit in hitherto unforeseen ways with the capacity of the hypothesis space during learning. We call this new kind of overfitting Propensity Overfitting.\nClassic variance reduction techniques for importance sampling are also useful for counterfactual evaluation and learning. For instance, importance weights can be “clipped” [15] to trade-off bias against variance in the estimators [5]. Additive control variates give rise to regression estimators [16] and doubly robust estimators [6]. Our proposal uses multiplicative control variates. These are widely used in financial applications (see [17] and references therein) and policy iteration for reinforcement learning (e.g. [18]). In particular, we study the self-normalized estimator [12] which is superior to the vanilla estimator when fluctuations in the weights dominate the variance [19]. We additionally show that the self-normalized estimator neatly addresses propensity overfitting."
    }, {
      "heading" : "3 Batch learning from logged bandit feedback",
      "text" : "Following [1], we focus on the stochastic, cardinal, contextual bandit setting and recap the essence of the CRM principle. The inputs of a structured prediction problem x∈X are drawn i.i.d. from a fixed but unknown distribution Pr(X ). The outputs are denoted by y∈Y . The hypothesis space H contains stochastic hypotheses h(Y | x) that define a probability distribution over Y . A hypothesis h∈H makes predictions by sampling from the conditional distribution y∼h(Y |x). This definition of H also captures deterministic hypotheses. For notational convenience, we denote the probability distribution h(Y |x) by h(x), and the probability assigned by h(x) to y as h(y |x). We use (x, y)∼h to refer to samples of x∼Pr(X ), y∼ h(x), and when clear from the context, we will drop (x, y). Bandit feedback means we only observe the feedback δ(x, y) for the specific y that was predicted, but not for any of the other possible predictions Y \\ {y}. The feedback is just a number, called the loss δ : X ×Y 7→ R. Smaller numbers are desirable. In general, the loss is the (noisy) realization of a stochastic random variable. The following exposition can be readily extended to the general case by setting δ(x, y) = E [δ | x, y]. The expected loss – called risk – of a hypothesis R(h) is\nR(h) = Ex∼Pr(X )Ey∼h(x) [δ(x, y)] = Eh [δ(x, y)] . (1) The aim of learning is to find a hypothesis h ∈ H that has minimum risk.\nCounterfactual estimators. We wish to use the logs of a historical system to perform learning. To ensure that learning will not be impossible [9], we assume the historical algorithm whose predictions we record in our logged data is a stationary policy h0(x) with full support over Y . For a new hypothesis h 6= h0, we cannot use the empirical risk estimator used in supervised learning [20] to directly approximate R(h), because the data contains samples drawn from h0 while the risk from Equation (1) requires samples from h.\nImportance sampling fixes this distribution mismatch, R(h) = Eh [δ(x, y)] = Eh0 [ δ(x, y)\nh(y |x) h0(y |x)\n] .\nSo, with data collected from the historical system D = {(x1, y1, δ1, p1), . . . , (xn, yn, δn, pn)},\nwhere (xi, yi) ∼ h0, δi ≡ δ(xi, yi) and pi ≡ h0(yi | xi), we can derive an unbiased estimate of R(h) via Monte Carlo approximation,\nR̂(h) = 1\nn n∑ i=1 δi h(yi |xi) pi . (2)\nThis classic inverse propensity estimator [7] has unbounded variance: pi ' 0 in D can cause R̂(h) to be arbitrarily far away from the true risk R(h). To remedy this problem, several thresholding schemes have been proposed and studied in the literature [15, 8, 5, 11]. The straightforward option is to cap the propensity weights [15, 1], i.e. pick M > 1 and set\nR̂M (h) = 1\nn n∑ i=1 δi min { M, h(yi |xi) pi } .\nSmaller values of M reduce the variance of R̂M (h) but induce a larger bias.\nCounterfactual Risk Minimization. Importance sampling also introduces variance in R̂M (h) through the variability of h(yi|xi)pi . This variance can be drastically different for different h ∈ H. The CRM principle is derived from a generalization error bound that reasons about this variance using an empirical Bernstein argument [1, 13]. Let δ(·, ·) ∈ [−1, 0] and consider the random variable uh = δ(x, y) min { M, h(y|x)h0(y|x) } . Note that D contains n i.i.d. observations uhi.\nTheorem 1. Denote the empirical variance of uh by ˆV ar(uh). With probability at least 1−γ in the random vector (xi, yi) ∼ h0, for a stochastic hypothesis spaceH with capacity C(H) and n ≥ 16,\n∀h ∈ H : R(h) ≤ R̂M (h) +\n√ 18 ˆV ar(uh) log( 10C(H) γ )\nn +M\n15 log( 10C(H)γ )\nn− 1 .\nProof. Refer Theorem 1 of [1] and the proof of Theorem 6 of [13].\nFollowing Structural Risk Minimization [20], this bound motivates the CRM principle for designing algorithms for BLBF. A learning algorithm should jointly optimize the estimate R̂M (h) as well as its empirical standard deviation, where the latter serves as a data-dependent regularizer.\nĥCRM = argmin h∈H R̂M (h) + λ √ ˆV ar(uh) n  . (3) M > 1 and λ ≥ 0 are regularization hyper-parameters."
    }, {
      "heading" : "4 The Propensity Overfitting problem",
      "text" : "The CRM objective in Equation (3) penalizes those h ∈ H that are “far” from the logging policy h0 (as measured by their empirical variance ˆV ar(uh)). This can be intuitively understood as a safeguard against overfitting. However, overfitting in BLBF is more nuanced than in conventional supervised learning. In particular, the unbiased risk estimator of Equation (2) has two anomalies. Even if δ(·, ·) ∈ [5,4], the value of R̂(h) estimated on a finite sample need not lie in that range. Furthermore, if δ(·, ·) is translated by a constant δ(·, ·) +C, R(h) becomes R(h) +C by linearity of expectation – but the unbiased estimator on a finite sample need not equal R̂(h) + C. In short, this risk estimator is not equivariant [19]. The various thresholding schemes for importance sampling only exacerbate this effect. These anomalies leave us vulnerable to a peculiar kind of overfitting, as we see in the following example.\nExample 1. For the input space of integers X = {1..k} and the output space Y = {1..k}, define\nδ(x, y) = { −2 if y = x −1 otherwise.\nThe hypothesis spaceH is the set of all deterministic functions f : X 7→ Y . hf (y|x) = {\n1 if f(x) = y 0 otherwise.\nData is drawn uniformly, x ∼ U(X ) and h0(Y|x) = U(Y) for all x. The hypothesis h∗ with minimum true risk is h∗f with f ∗(x) = x, which has risk R(h∗) = −2.\nWhen drawing a training sample D = ((x1, y1, δ1, p1), ..., (xn, yn, δn, pn)), let us first consider the special case where all xi in the sample are distinct. This is quite likely if n is small relative to k. In this case H contains a hypothesis hoverfit, which assigns f(xi) = yi for all i. This hypothesis has the following empirical risk as estimated by Equation (2):\nR̂(hoverfit) = 1\nn n∑ i=1 δi hoverfit(yi | xi) pi = 1 n n∑ i=1 δi 1 1/k ≤ 1 n n∑ i=1 −1 1 1/k = −k.\nClearly this risk estimate shows severe overfitting, since it can be arbitrarily lower than the true risk R(h∗) = −2 of the best hypothesis h∗ with appropriately chosen k (or, more generally, the choice of h0). This is in stark contrast to overfitting in full-information supervised learning, where at least the overfitted risk is bounded by the lower range of the loss function. Note that the empirical risk R̂(h∗) of h∗ concentrates around −2. ERM will, hence, almost always select hoverfit over h∗. Even if we are not in the special case of having a sample with all distinct xi, this type of overfitting still exists. In particular, if there are only l distinct xi in D, then there still exists a hoverfit with R̂(hoverfit) ≤ −k ln . Finally, note that this type of overfitting behavior is not an artifact of this example. Section 7 shows that this is ubiquitous in all the datasets we explored.\nMaybe this problem could be avoided by transforming the loss? For example, let’s translate the loss by adding 2 to δ so that now all loss values become non-negative. This results in the new loss function δ′(x, y) taking values 0 and 1. In conventional supervised learning an additive translation of the loss does not change the empirical risk minimizer. Suppose we draw a sample D in which not all possible values y for xi are observed for all xi in the sample (again, such a sample is likely for sufficiently large k). Now there are many hypotheses hoverfit′ that predict one of the unobserved y for each xi, basically avoiding the training data.\nR̂(hoverfit′) = 1\nn n∑ i=1 δi hoverfit′(yi | xi) pi = 1 n n∑ i=1 δi 0 1/k = 0.\nAgain we are faced with overfitting, since many overfit hypotheses are indistinguishable from the true risk minimizer h∗ with true risk R(h∗) = 0 and empirical risk R̂(h∗) = 0.\nThese examples indicate that this overfitting occurs regardless of how the loss is transformed. Intuitively, this type of overfitting occurs since the risk estimate according to Equation (2) can be minimized not only by putting large probability mass h(y | x) on the examples with low loss δ(x, y), but by maximizing (for negative losses) or minimizing (for positive losses) the sum of the weights\nŜ(h) = 1\nn n∑ i=1 h(yi | xi) pi . (4)\nFor this reason, we call this type of overfitting Propensity Overfitting. This is in stark contrast to overfitting in supervised learning, which we call Loss Overfitting. Intuitively, Loss Overfitting occurs because the capacity of H fits spurious patterns of low δ(x, y) in the data. In Propensity Overfitting, the capacity inH allows overfitting of the propensity weights pi – for positive δ, hypotheses that avoid D are selected; for negative δ, hypotheses that overrepresent D are selected. The variance regularization of CRM combats both Loss Overfitting and Propensity Overfitting by optimizing a more informed generalization error bound. However the empirical variance estimate is also affected by Propensity Overfitting – especially for positive losses. Can we avoid Propensity Overfitting more directly?"
    }, {
      "heading" : "5 Control variates and the Self-Normalized estimator",
      "text" : "To avoid Propensity Overfitting, we must first detect when and where it is occurring. For this, we draw on diagnostic tools used in importance sampling. Note that for any h ∈ H, the sum of propensity weights Ŝ(h) from Equation (4) always has expected value 1 under the conditions required for the unbiased estimator of Equation (2).\nE [ Ŝ(h) ] = 1\nn n∑ i=1 ∫ h(yi | xi) h0(yi | xi) h0(yi | xi) Pr(xi)dyidxi = 1 n n∑ i=1 ∫ 1 Pr(xi)dxi = 1. (5)\nThis means that we can identify hypotheses that suffer from Propensity Overfitting based on how far Ŝ(h) deviates from its expected value of 1. Since h(y|x)h0(y|x) is likely correlated with δ(x, y) h(y|x) h0(y|x) , a large deviation in Ŝ(h) suggests a large deviation in R̂(h) and consequently a bad risk estimate.\nHow can we use the knowledge that ∀h ∈ H : E [ Ŝ(h) ] = 1 to avoid degenerate risk estimates in\na principled way? While one could use concentration inequalities to explicitly detect and eliminate overfit hypotheses based on Ŝ(h), we use control variates to derive an improved risk estimator that directly incorporates this knowledge.\nControl variates. Control variates – random variables whose expectation is known – are a classic tool used to reduce the variance of Monte Carlo approximations [21]. Let V (X) be a control variate with known expectation EX [V (X)] = v 6= 0, and let EX [W (X)] be an expectation that we would like to estimate based on independent samples of X . Employing V (X) as a multiplicative control variate, we can write EX [W (X)] = E[W (X)]E[V (X)] v. This motivates the ratio estimator\nŴSN = ∑n i=1W (Xi)∑n i=1 V (Xi) v, (6)\nwhich is called the Self-Normalized estimator in the importance sampling literature [12, 22, 23]. This estimator has substantially lower variance if W (X) and V (X) are correlated.\nSelf-Normalized risk estimator. Let us use S(h) as a control variate for R(h), yielding\nR̂SN (h) =\n∑n i=1 δi\nh(yi|xi) pi∑n\ni=1 h(yi|xi) pi\n. (7)\nHesterberg reports that this estimator tends be more accurate than the unbiased estimator of Equation (2) when fluctuations in the sampling weights dominate the fluctuations in δ(x, y) [19].\nObserve that the estimate is just a convex combination of the δi observed in the sample. If δ(·, ·) is now translated by a constant δ(·, ·) + C, both the true risk R(h) and the finite sample estimate R̂SN (h) get shifted by C. Hence R̂SN (h) is equivariant, unlike R̂(h) [19]. Moreover, R̂SN (h) is always bounded within the range of δ. So, the overfitted risk due to ERM will now be bounded by the lower range of the loss, analogous to full-information supervised learning.\nFinally, while the self-normalized risk estimator is not unbiased (E [ R̂(h)\nŜ(h)\n] 6= R(h)\nE[Ŝ(h)] in general), it\nis strongly consistent and approaches the desired expectation when n is large.\nTheorem 2. Let D be drawn (xi, yi) i.i.d.∼ h0, from a h0 that has full support over Y . Then,\n∀h ∈ H : Pr( lim n→∞ R̂SN (h) = R(h)) = 1.\nProof. The numerator of R̂SN (h) in (7) are i.i.d. observations with mean R(h). Strong law of large numbers gives Pr(limn→∞ 1n ∑n i=1 δi h(yi|xi) pi\n= R(h)) = 1. Similarly, the denominator has i.i.d. observations with mean 1. So, the strong law of large numbers implies Pr(limn→∞ 1 n ∑n i=1 h(yi|xi) pi = 1) = 1. Hence, Pr(limn→∞ R̂SN (h) = R(h)) = 1.\nIn summary, the self-normalized risk estimator R̂SN (h) in Equation (7) resolves all the problems of the unbiased estimator R̂(h) from Equation (2) identified in Section 4."
    }, {
      "heading" : "6 Learning method: Norm-POEM",
      "text" : "We now derive a learning algorithm, called Norm-POEM, for structured output prediction. The algorithm is analogous to POEM [1] in its choice of hypothesis space and its application of the CRM principle, but it replaces the conventional estimator (2) with the self-normalized estimator (7).\nHypothesis space. Following [1, 24], Norm-POEM learns stochastic linear rules hw ∈ Hlin parametrized by w that operate on a d−dimensional joint feature map φ(x, y).\nhw(y | x) = exp(w · φ(x, y))/Z(x). Z(x) = ∑ y′∈Y exp(w · φ(x, y′)) is the partition function.\nVariance estimator. In order to instantiate the CRM objective from Equation (3), we need an empirical variance estimate ˆV ar(R̂SN (h)) for the self-normalized risk estimator. Following [23, Section 4.3], we use an approximate variance estimate for the ratio estimator of Equation (6). Using the Normal approximation argument [21, Equation 9.9],\nˆV ar(R̂SN (h)) =\n∑n i=1(δi − R̂SN (h))2( h(yi|xi) pi )2\n( ∑n i=1 h(yi|xi) pi )2 . (8)\nUsing the delta method to approximate the variance [22] yields the same formula. To invoke asymptotic normality of the estimator (and indeed, for reliable importance sampling estimates) we require the true variance of the self-normalized estimator V ar(R̂SN (h)) to exist. We can guarantee this by thresholding the importance weights, analogous to R̂M (h).\nThe benefits of the self-normalized estimator come at a computational cost. The risk estimator of POEM had a simpler variance estimate which could be approximated by Taylor expansion and optimized using stochastic gradient descent. The variance of Equation (8) does not admit stochastic optimization. Surprisingly, in our experiments in Section 7 we find that the improved robustness of Norm-POEM permits fast convergence during training even without stochastic optimization.\nTraining objective of Norm-POEM. The objective is now derived by substituting the selfnormalized risk estimator of Equation (7) and its sample variance estimate from Equation (8) into the CRM objective (3) for the hypothesis space Hlin. By design, hw lies in the exponential family of distributions. So, the gradient of the resulting objective can be tractably computed whenever the partition functions Z(xi) are tractable. Doing so yields a non-convex objective in the parameters w which we optimize using L-BFGS. The choice of L-BFGS for non-convex and non-smooth optimization is well supported [25, 26]. Analogous to POEM, the hyper-parameters M (clipping to prevent unbounded variance) and λ (strength of variance regularization) can be calibrated via counterfactual evaluation on a held out validation set. In summary, the per-iteration cost of optimizing the Norm-POEM objective has the same complexity as the per-iteration cost of POEM with L-BFGS. It requires the same set of hyper-parameters. And it can be done tractably whenever the corresponding supervised CRF can be learnt efficiently. Software implementing Norm-POEM is available at http://www.cs.cornell.edu/∼adith/POEM."
    }, {
      "heading" : "7 Experiments",
      "text" : "We will now empirically verify if the self-normalized estimator as used in Norm-POEM can indeed guard against propensity overfitting and attain robust generalization performance. We follow the Supervised 7→ Bandit methodology [2, 1] to test the limits of counterfactual learning in a wellcontrolled environment. As in prior work [1], the experiment setup uses supervised datasets for multi-label classification from the LibSVM repository. In these datasets, the inputs x ∈ Rp. The predictions y ∈ {0, 1}q are bitvectors indicating the labels assigned to x. The datasets have a range of features p, labels q and instances n:\nName p(# features) q(# labels) ntrain ntest Scene 294 6 1211 1196 Yeast 103 14 1500 917 TMC 30438 22 21519 7077 LYRL 47236 4 23149 781265\nPOEM uses the CRM principle instantiated with the unbiased estimator while Norm-POEM uses the self-normalized estimator. Both use a hypothesis space isomorphic to a Conditional Random Field (CRF) [24]. We therefore report the performance of a full-information CRF (essentially, logistic regression for each of the q labels independently) as a “skyline” for what we can possibly hope to reach by partial-information batch learning from logged bandit feedback. The joint feature map φ(x, y) = x ⊗ y for all approaches. To simulate a bandit feedback dataset D, we use a CRF with default hyper-parameters trained on 5% of the supervised dataset as h0, and replay the training data 4 times and collect sampled labels from h0. This is inspired by the observation that supervised labels are typically hard to collect relative to bandit feedback. The BLBF algorithms only have access to the Hamming loss ∆(y∗, y) between the supervised label y∗ and the sampled label y for input x. Generalization performance R is measured by the expected Hamming loss on the held-out supervised test set. Lower is better. Hyper-parameters λ,M were calibrated as recommended and validated on a 25% hold-out of D – in summary, our experimental setup is identical to POEM [1]. We report performance of BLBF approaches without l2−regularization here; we observed NormPOEM dominated POEM even after l2−regularization. Since the choice of optimization method could be a confounder, we use L-BFGS for all methods and experiments.\nWhat is the generalization performance of Norm-POEM ? The key question is whether the appealing theoretical properties of the self-normalized estimator actually lead to better generalization performance. In Table 1, we report the test set loss for Norm-POEM and POEM averaged over 10 runs. On each run, h0 has varying performance (trained on random 5% subsets) but Norm-POEM consistently beats POEM.\nThe plot below (Figure 1) shows how generalization performance improves with more training data for a single run of the experiment on the Yeast dataset. We achieve this by varying the number of times we replay the training set to collect samples from h0 (ReplayCount). Norm-POEM consistently outperforms POEM for all training sample sizes.\nDoes Norm-POEM avoid Propensity Overfitting? While the previous results indicate that Norm-POEM achieves better performance, it remains to be verified that this improved performance is indeed due to improved control over Propensity Overfitting. Table 2 (left) shows the average Ŝ(ĥ) for the hypothesis ĥ selected by each approach. Indeed, Ŝ(ĥ) is close to its known expectation of 1 for Norm-POEM, while it is severely biased for POEM. Furthermore, the value of Ŝ(ĥ) depends heavily on how the losses δ are translated for POEM, as predicted by theory. As anticipated by our earlier observation that the self-normalized estimator is equivariant, Norm-POEM is unaffected by translations of δ. Table 2 (right) shows that the same is true for the prediction error on the test\nset. Norm-POEM is consistenly good while POEM fails catastrophically (for instance, on the TMC dataset, POEM is worse than random guessing).\nIs CRM variance regularization still necessary? It may be possible that the improved selfnormalized estimator no longer requires variance regularization. The loss of the unregularized estimator is reported (Norm-IPS) in Table 3. We see that variance regularization still helps.\nHow computationally efficient is Norm-POEM ? The runtime of Norm-POEM is surprisingly faster than POEM. Even though normalization increases the per-iteration computation cost, optimization tends to converge in fewer iterations than for POEM. We find that POEM picks a hypothesis with large ‖w‖, attempting to assign a probability of 1 to all training points with negative losses. However, Norm-POEM converges to a much shorter ‖w‖. The loss of an instance relative to others in a sample D governs how Norm-POEM tries to fit to it. This is another nice consequence of the fact that the overfitted risk of R̂SN (h) is bounded and small. Overall, the runtime of Norm-POEM is on the same order of magnitude as those of a full-information CRF, and is competitive with the runtimes reported for POEM with stochastic optimization and early stopping [1], while providing substantially better generalization performance.\nWe observe the same trends for Norm-POEM when different properties of h0 are varied (e.g. stochasticity and quality), as reported for POEM [1]."
    }, {
      "heading" : "8 Conclusions",
      "text" : "We identify the problem of propensity overfitting when using the conventional unbiased risk estimator for ERM in batch learning from bandit feedback. To remedy this problem, we propose the use of a multiplicative control variate that leads to the self-normalized risk estimator. This provably avoids the anomalies of the conventional estimator. Deriving a new learning algorithm called Norm-POEM based on the CRM principle using the new estimator, we show that the improved estimator leads to significantly improved generalization performance."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This research was funded in part through NSF Awards IIS-1247637, IIS-1217686, IIS-1513692, the JTCII Cornell-Technion Research Fund, and a gift from Bloomberg."
    } ],
    "references" : [ {
      "title" : "Counterfactual risk minimization: Learning from logged bandit feedback",
      "author" : [ "Adith Swaminathan", "Thorsten Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "The offset tree for learning with partial labels",
      "author" : [ "Alina Beygelzimer", "John Langford" ],
      "venue" : "In KDD,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gabor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1998
    }, {
      "title" : "Counterfactual reasoning and learning systems: the example of computational advertising",
      "author" : [ "Léon Bottou", "Jonas Peters", "Joaquin Q. Candela", "Denis X. Charles", "Max Chickering", "Elon Portugaly", "Dipankar Ray", "Patrice Y. Simard", "Ed Snelson" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Doubly robust policy evaluation and learning",
      "author" : [ "Miroslav Dudı́k", "John Langford", "Lihong Li" ],
      "venue" : "In ICML,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "The central role of the propensity score in observational studies for causal effects",
      "author" : [ "P. Rosenbaum", "D. Rubin" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1983
    }, {
      "title" : "Learning bounds for importance weighting",
      "author" : [ "C. Cortes", "Y. Mansour", "M. Mohri" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Exploration scavenging",
      "author" : [ "John Langford", "Alexander Strehl", "Jennifer Wortman" ],
      "venue" : "In ICML,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Cost-sensitive learning by cost-proportionate example weighting",
      "author" : [ "Bianca Zadrozny", "John Langford", "Naoki Abe" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Learning from logged implicit exploration data",
      "author" : [ "Alexander L. Strehl", "John Langford", "Lihong Li", "Sham Kakade" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Conditional monte carlo for normal samples",
      "author" : [ "H.F. Trotter", "J.W. Tukey" ],
      "venue" : "In Symposium on Monte Carlo Methods,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1956
    }, {
      "title" : "Empirical bernstein bounds and sample-variance penalization",
      "author" : [ "Andreas Maurer", "Massimiliano Pontil" ],
      "venue" : "In COLT,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "High-confidence off-policy evaluation",
      "author" : [ "Philip S. Thomas", "Georgios Theocharous", "Mohammad Ghavamzadeh" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "Truncated importance sampling",
      "author" : [ "Edward L. Ionides" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Toward minimax off-policy value estimation",
      "author" : [ "Lihong Li", "R. Munos", "C. Szepesvari" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Monte carlo methods for security pricing",
      "author" : [ "Phelim Boyle", "Mark Broadie", "Paul Glasserman" ],
      "venue" : "Journal of Economic Dynamics and Control,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1997
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "John Schulman", "Sergey Levine", "Pieter Abbeel", "Michael I. Jordan", "Philipp Moritz" ],
      "venue" : "In ICML,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Weighted average importance sampling and defensive mixture",
      "author" : [ "Tim Hesterberg" ],
      "venue" : "distributions. Technometrics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1995
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "Monte Carlo theory, methods and examples",
      "author" : [ "Art B. Owen" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "A note on importance sampling using standardized weights",
      "author" : [ "Augustine Kong" ],
      "venue" : "Technical Report 348,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1992
    }, {
      "title" : "Simulation and the Monte Carlo",
      "author" : [ "R. Rubinstein", "D. Kroese" ],
      "venue" : "Method. Wiley,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira" ],
      "venue" : "In ICML,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2001
    }, {
      "title" : "Nonsmooth optimization via quasi-newton methods",
      "author" : [ "Adrian S. Lewis", "Michael L. Overton" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "A quasi-Newton approach to nonsmooth convex optimization problems in machine learning",
      "author" : [ "Jin Yu", "S.V.N. Vishwanathan", "S. Günter", "N. Schraudolph" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The Counterfactual Risk Minimization (CRM) principle [1] offers a general recipe for designing BLBF algorithms.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Learning from historical logs data can be formalized as batch learning from logged bandit feedback (BLBF) [2, 1].",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "Learning from historical logs data can be formalized as batch learning from logged bandit feedback (BLBF) [2, 1].",
      "startOffset" : 106,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "Unlike the well-studied problem of online learning from bandit feedback [3], this setting does not require the learner to have interactive control over the system.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "Learning in such a setting is closely related to the problem of off-policy evaluation in reinforcement learning [4] – we would like to know how well a new system (policy) would perform if it had been used in the past.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "This motivates the use of counterfactual estimators [5].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 5,
      "context" : "Following an approach analogous to Empirical Risk Minimization (ERM), it was shown that such estimators can be used to design learning algorithms for batch learning from logged bandit feedback [6, 5, 1].",
      "startOffset" : 193,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "Following an approach analogous to Empirical Risk Minimization (ERM), it was shown that such estimators can be used to design learning algorithms for batch learning from logged bandit feedback [6, 5, 1].",
      "startOffset" : 193,
      "endOffset" : 202
    }, {
      "referenceID" : 0,
      "context" : "Following an approach analogous to Empirical Risk Minimization (ERM), it was shown that such estimators can be used to design learning algorithms for batch learning from logged bandit feedback [6, 5, 1].",
      "startOffset" : 193,
      "endOffset" : 202
    }, {
      "referenceID" : 6,
      "context" : "Classic inference techniques like propensity score matching [7] are, hence, immediately relevant.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "BLBF is closely related to the problem of learning under covariate shift (also called domain adaptation or sample bias correction) [8] as well as off-policy evaluation in reinforcement learning [4].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "BLBF is closely related to the problem of learning under covariate shift (also called domain adaptation or sample bias correction) [8] as well as off-policy evaluation in reinforcement learning [4].",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : "Lower bounds for domain adaptation [8] and impossibility results for off-policy evaluation [9], hence, also apply to propensity score matching [7], costing [10] and other importance sampling approaches to BLBF.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "Lower bounds for domain adaptation [8] and impossibility results for off-policy evaluation [9], hence, also apply to propensity score matching [7], costing [10] and other importance sampling approaches to BLBF.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "Lower bounds for domain adaptation [8] and impossibility results for off-policy evaluation [9], hence, also apply to propensity score matching [7], costing [10] and other importance sampling approaches to BLBF.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "Lower bounds for domain adaptation [8] and impossibility results for off-policy evaluation [9], hence, also apply to propensity score matching [7], costing [10] and other importance sampling approaches to BLBF.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "Several counterfactual estimators have been developed for off-policy evaluation [11, 6, 5].",
      "startOffset" : 80,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : "Several counterfactual estimators have been developed for off-policy evaluation [11, 6, 5].",
      "startOffset" : 80,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "Several counterfactual estimators have been developed for off-policy evaluation [11, 6, 5].",
      "startOffset" : 80,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "All these estimators are instances of importance sampling for Monte Carlo approximation and can be traced back to What-If simulations [12].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : "Learning (upper) bounds have been developed recently [13, 1, 14] that show that these estimators can work for BLBF.",
      "startOffset" : 53,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Learning (upper) bounds have been developed recently [13, 1, 14] that show that these estimators can work for BLBF.",
      "startOffset" : 53,
      "endOffset" : 64
    }, {
      "referenceID" : 13,
      "context" : "Learning (upper) bounds have been developed recently [13, 1, 14] that show that these estimators can work for BLBF.",
      "startOffset" : 53,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "For instance, importance weights can be “clipped” [15] to trade-off bias against variance in the estimators [5].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "For instance, importance weights can be “clipped” [15] to trade-off bias against variance in the estimators [5].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 15,
      "context" : "Additive control variates give rise to regression estimators [16] and doubly robust estimators [6].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "Additive control variates give rise to regression estimators [16] and doubly robust estimators [6].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "These are widely used in financial applications (see [17] and references therein) and policy iteration for reinforcement learning (e.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 11,
      "context" : "In particular, we study the self-normalized estimator [12] which is superior to the vanilla estimator when fluctuations in the weights dominate the variance [19].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "In particular, we study the self-normalized estimator [12] which is superior to the vanilla estimator when fluctuations in the weights dominate the variance [19].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "Following [1], we focus on the stochastic, cardinal, contextual bandit setting and recap the essence of the CRM principle.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "To ensure that learning will not be impossible [9], we assume the historical algorithm whose predictions we record in our logged data is a stationary policy h0(x) with full support over Y .",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "For a new hypothesis h 6= h0, we cannot use the empirical risk estimator used in supervised learning [20] to directly approximate R(h), because the data contains samples drawn from h0 while the risk from Equation (1) requires samples from h.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "This classic inverse propensity estimator [7] has unbounded variance: pi ' 0 in D can cause R̂(h) to be arbitrarily far away from the true risk R(h).",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "To remedy this problem, several thresholding schemes have been proposed and studied in the literature [15, 8, 5, 11].",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : "To remedy this problem, several thresholding schemes have been proposed and studied in the literature [15, 8, 5, 11].",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "To remedy this problem, several thresholding schemes have been proposed and studied in the literature [15, 8, 5, 11].",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "To remedy this problem, several thresholding schemes have been proposed and studied in the literature [15, 8, 5, 11].",
      "startOffset" : 102,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "The straightforward option is to cap the propensity weights [15, 1], i.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "The straightforward option is to cap the propensity weights [15, 1], i.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "The CRM principle is derived from a generalization error bound that reasons about this variance using an empirical Bernstein argument [1, 13].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "The CRM principle is derived from a generalization error bound that reasons about this variance using an empirical Bernstein argument [1, 13].",
      "startOffset" : 134,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "Refer Theorem 1 of [1] and the proof of Theorem 6 of [13].",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "Refer Theorem 1 of [1] and the proof of Theorem 6 of [13].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Following Structural Risk Minimization [20], this bound motivates the CRM principle for designing algorithms for BLBF.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "Even if δ(·, ·) ∈ [5,4], the value of R̂(h) estimated on a finite sample need not lie in that range.",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "Even if δ(·, ·) ∈ [5,4], the value of R̂(h) estimated on a finite sample need not lie in that range.",
      "startOffset" : 18,
      "endOffset" : 23
    }, {
      "referenceID" : 18,
      "context" : "In short, this risk estimator is not equivariant [19].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "Control variates – random variables whose expectation is known – are a classic tool used to reduce the variance of Monte Carlo approximations [21].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : "which is called the Self-Normalized estimator in the importance sampling literature [12, 22, 23].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 21,
      "context" : "which is called the Self-Normalized estimator in the importance sampling literature [12, 22, 23].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : "which is called the Self-Normalized estimator in the importance sampling literature [12, 22, 23].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "Hesterberg reports that this estimator tends be more accurate than the unbiased estimator of Equation (2) when fluctuations in the sampling weights dominate the fluctuations in δ(x, y) [19].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 18,
      "context" : "Hence R̂ (h) is equivariant, unlike R̂(h) [19].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "The algorithm is analogous to POEM [1] in its choice of hypothesis space and its application of the CRM principle, but it replaces the conventional estimator (2) with the self-normalized estimator (7).",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "Following [1, 24], Norm-POEM learns stochastic linear rules hw ∈ Hlin parametrized by w that operate on a d−dimensional joint feature map φ(x, y).",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 23,
      "context" : "Following [1, 24], Norm-POEM learns stochastic linear rules hw ∈ Hlin parametrized by w that operate on a d−dimensional joint feature map φ(x, y).",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 21,
      "context" : "Using the delta method to approximate the variance [22] yields the same formula.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : "The choice of L-BFGS for non-convex and non-smooth optimization is well supported [25, 26].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 25,
      "context" : "The choice of L-BFGS for non-convex and non-smooth optimization is well supported [25, 26].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "We follow the Supervised 7→ Bandit methodology [2, 1] to test the limits of counterfactual learning in a wellcontrolled environment.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "We follow the Supervised 7→ Bandit methodology [2, 1] to test the limits of counterfactual learning in a wellcontrolled environment.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "As in prior work [1], the experiment setup uses supervised datasets for multi-label classification from the LibSVM repository.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 23,
      "context" : "Both use a hypothesis space isomorphic to a Conditional Random Field (CRF) [24].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "Hyper-parameters λ,M were calibrated as recommended and validated on a 25% hold-out of D – in summary, our experimental setup is identical to POEM [1].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "Overall, the runtime of Norm-POEM is on the same order of magnitude as those of a full-information CRF, and is competitive with the runtimes reported for POEM with stochastic optimization and early stopping [1], while providing substantially better generalization performance.",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 26,
      "context" : "CRF is implemented by scikit-learn [27].",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "stochasticity and quality), as reported for POEM [1].",
      "startOffset" : 49,
      "endOffset" : 52
    } ],
    "year" : 2015,
    "abstractText" : "This paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem. In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy. This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs. The Counterfactual Risk Minimization (CRM) principle [1] offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator. We show that this conventional estimator suffers from a propensity overfitting problem when used for learning over complex hypothesis spaces. We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem. This naturally gives rise to a new learning algorithm – Normalized Policy Optimizer for Exponential Models (Norm-POEM) – for structured output prediction using linear rules. We evaluate the empirical effectiveness of NormPOEM on several multi-label classification problems, finding that it consistently outperforms the conventional estimator.",
    "creator" : null
  }
}