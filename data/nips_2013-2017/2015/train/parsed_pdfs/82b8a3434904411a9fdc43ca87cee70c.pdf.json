{
  "name" : "82b8a3434904411a9fdc43ca87cee70c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Regularization Path of Cross-Validation Error Lower Bounds",
    "authors" : [ "Atsushi Shibagaki", "Yoshiki Suzuki", "Masayuki Karasuyama", "Ichiro Takeuchi" ],
    "emails" : [ "shibagaki.a.mllab.nit@gmail.com", "suzuki.mllab.nit@gmail.com", "karasuyama@nitech.ac.jp", "takeuchi.ichiro@nitech.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances. Nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error. In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter, which we call regularization path of CV error lower bounds. The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters. Our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many machine learning tasks involve careful tuning of a regularization parameter that controls the balance between an empirical loss term and a regularization term. A regularization parameter is usually selected by comparing the cross-validation (CV) errors at several different regularization parameters. Although its choice has a significant impact on the generalization performances, the current practice is still more of an art than a science. For example, in commonly used grid-search, it is hard to tell how many grid points we should search over for obtaining sufficiently small CV error.\nIn this paper we introduce a novel framework for a class of regularized binary classification problems that can compute a regularization path of CV error lower bounds. For an ε ∈ [0, 1], we define εapproximate regularization parameters to be a set of regularization parameters such that the CV error of the solution at the regularization parameter is guaranteed to be no greater by ε than the best possible CV error in the entire range of regularization parameters. Given a set of solutions obtained, for example, by grid-search, the proposed framework allows us to provide a theoretical guarantee of the current best solution by explicitly quantifying its approximation level ε in the above sense. Furthermore, when a desired approximation level ε is specified, the proposed framework can be used for efficiently finding one of the ε-approximate regularization parameters.\nThe proposed framework is built on a novel CV error lower bound represented as a function of the regularization parameter, and this is why we call it as a regularization path of CV error lower bounds. Our CV error lower bound can be computed by only using a finite number of solutions obtained by arbitrary algorithms. It is thus easy to apply our framework to common regularization parameter tuning strategies such as grid-search or Bayesian optimization. Furthermore, the proposed framework can be used not only with exact optimal solutions but also with sufficiently good approximate solu-\ntions, which is computationally advantageous because completely solving an optimization problem is often much more costly than obtaining a reasonably good approximate solution.\nOur main contribution in this paper is to show that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs. To the best of our knowledge, there is no other existing methods for providing such a theoretical guarantee on CV error that can be used as generally as ours. Figure 1 illustrates the behavior of the algorithm for obtaining ε = 0.1 approximate regularization parameter (see §5 for the setup). Related works Optimal regularization parameter can be found if its exact regularization path can be computed. Exact regularization path has been intensively studied [1, 2], but they are known to be numerically unstable and do not scale well. Furthermore, exact regularization path can be computed only for a limited class of problems whose solutions are written as piecewise-linear functions of the regularization parameter [3]. Our framework is much more efficient and can be applied to wider classes of problems whose exact regularization path cannot be computed. This work was motivated by recent studies on approximate regularization path [4, 5, 6, 7]. These approximate regularization paths have a property that the objective function value at each regularization parameter value is no greater by ε than the optimal objective function value in the entire range of regularization parameters. Although these algorithms are much more stable and efficient than exact ones, for the task of tuning a regularization parameter, our interest is not in objective function values but in CV errors. Our approach is more suitable for regularization parameter tuning tasks in the sense that the approximation quality is guaranteed in terms of CV error.\nAs illustrated in Figure 1, we only compute a finite number of solutions, but still provide approximation guarantee in the whole interval of the regularization parameter. To ensure such a property, we need a novel CV error lower bound that is sufficiently tight and represented as a monotonic function of the regularization parameter. Although several CV error bounds (mostly for leave-one-out CV) of SVM and other similar learning frameworks exist (e.g., [8, 9, 10, 11]), none of them satisfy the above required properties. The idea of our CV error bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16] (see Appendix A for the detail). Furthermore, we emphasize that our contribution is not in presenting a new generalization error bound, but in introducing a practical framework for providing a theoretical guarantee on the choice of a regularization parameter. Although generalization error bounds such as structural risk minimization [17] might be used for a rough tuning of a regularization parameter, they are known to be too loose to use as an alternative to CV (see, e.g., §11 in [18]). We also note that our contribution is not in presenting new method for regularization parameter tuning such as Bayesian optimization [19], random search [20] and gradient-based search [21]. As we demonstrate in experiments, our approach can provide a theoretical approximation guarantee of the regularization parameter selected by these existing methods."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We consider linear binary classification problems. Let {(xi, yi) ∈ Rd×{−1, 1}}i∈[n] be the training set where n is the size of the training set, d is the input dimension, and [n] := {1, . . . , n}. An independent held-out validation set with size n′ is denoted similarly as {(x′i, y′i) ∈ Rd × {−1, 1}}i∈[n′]. A linear decision function is written as f(x) = w⊤x, where w ∈ Rd is a vector of coefficients, and ⊤ represents the transpose. We assume the availability of a held-out validation set only for simplifying the exposition. All the proposed methods presented in this paper can be straightforwardly\nadapted to a cross-validation setup. Furthermore, the proposed methods can be kernelized if the loss function satisfies a certain condition. In this paper we focus on the following class of regularized convex loss minimization problems:\nw∗C := arg min w∈Rd\n1 2 ∥w∥2 + C\n∑\ni∈[n]\nℓ(yi, w ⊤xi), (1)\nwhere C > 0 is the regularization parameter, and ∥ · ∥ is the Euclidean norm. The loss function is denoted as ℓ : {−1, 1}× R → R. We assume that ℓ(·, ·) is convex and subdifferentiable in the 2nd argument. Examples of such loss functions include logistic loss, hinge loss, Huber-hinge loss, etc. For notational convenience, we denote the individual loss as ℓi(w) := ℓ(yi, w⊤xi) for all i ∈ [n]. The optimal solution for the regularization parameter C is explicitly denoted as w∗C . We assume that the regularization parameter is defined in a finite interval [Cℓ, Cu], e.g., Cℓ = 10−3 and Cu = 103 as we did in the experiments.\nFor a solution w ∈ Rd, the validation error1 is defined as\nEv(w) := 1\nn′\n∑\ni∈[n′]\nI(y′iw ⊤x′i < 0), (2)\nwhere I(·) is the indicator function. In this paper, we consider two problem setups. The first problem setup is, given a set of (either optimal or approximate) solutions w∗C1 , . . . , w ∗ CT\nat T different regularization parameters C1, . . . , CT ∈ [Cℓ, Cu], to compute the approximation level ε such that\nmin Ct∈{C1,...,CT }\nEv(w ∗ Ct)− E ∗ v ≤ ε, where E∗v := min\nC∈[Cl,Cu] Ev(w\n∗ C), (3)\nby which we can find how accurate our search (grid-search, typically) is in a sense of the deviation of the achieved validation error from the true minimum in the range, i.e., E∗v . The second problem setup is, given the approximation level ε, to find an ε-approximate regularization parameter within an interval C ∈ [Cl, Cu], which is defined as an element of the following set\nC(ε) := { C ∈ [Cl, Cu] ∣∣∣ Ev(w∗C)− E∗v ≤ ε } .\nOur goal in this second setup is to derive an efficient exploration procedure which achieves the specified validation approximation level ε. These two problem setups are both common scenarios in practical data analysis, and can be solved by using our proposed framework for computing a path of validation error lower bounds."
    }, {
      "heading" : "3 Validation error lower bounds as a function of regularization parameter",
      "text" : "In this section, we derive a validation error lower bound which is represented as a function of the regularization parameter C. Our basic idea is to compute a lower and an upper bound of the inner product score w∗⊤C x ′ i for each validation input x′i, i ∈ [n′], as a function of the regularization parameter C. For computing the bounds of w∗⊤C x ′ i, we use a solution (either optimal or approximate) for a different regularization parameter C̃ ̸= C."
    }, {
      "heading" : "3.1 Score bounds",
      "text" : "We first describe how to obtain a lower and an upper bound of inner product score w∗⊤C x ′ i based on an approximate solution ŵC̃ at a different regularization parameter C̃ ̸= C. Lemma 1. Let ŵC̃ be an approximate solution of the problem (1) for a regularization parameter value C̃ and ξi(ŵC) be a subgradient of ℓi at w = ŵC such that a subgradient of the objective function is\ng(ŵC̃) := ŵC + C̃ ∑\ni∈[n]\nξi(ŵC). (4)\n1 For simplicity, we regard a validation instance whose score is exactly zero, i.e., w⊤x′i = 0, is correctly classified in (2). Hereafter, we assume that there are no validation instances whose input vector is completely 0, i.e., x′i = 0, because those instances are always correctly classified according to the definition in (2).\nThen, for any C > 0, the score w∗⊤C x ′ i, i ∈ [n′], satisfies\nw∗⊤C x ′ i≥LB(w∗⊤C x′i|ŵC̃) :=\n{ α(ŵC̃ , x ′ i)− 1C̃ (β(ŵC̃ , x ′ i) + γ(g(ŵC̃), x ′ i))C, if C > C̃,\n−β(ŵC̃ , x′i) + 1 C̃ (α(ŵC̃ , x ′ i) + δ(g(ŵC̃), x ′ i))C, if C < C̃,\n(5a)\nw∗⊤C x ′ i≤UB(w∗⊤C x′i|ŵC̃) :=\n{ −β(ŵC̃ , x′i) + 1 C̃ (α(ŵC̃ , x ′ i) + δ(g(ŵC̃), x ′ i))C, if C > C̃,\nα(ŵC̃ , x ′ i)− 1C̃ (β(ŵC̃ , x ′ i) + γ(g(ŵC̃), x ′ i))C, if C < C̃,\n(5b)\nwhere\nα(w∗ C̃ , x′i) :=\n1 2 (∥w∗ C̃ ∥∥x′i∥+ w∗⊤C̃ x ′ i) ≥ 0, γ(g(ŵC̃), x ′ i) := 1 2 (∥g(ŵC̃)∥∥x ′ i∥+ g(ŵC̃) ⊤x′i) ≥ 0,\nβ(w∗ C̃ , x′i) :=\n1 2 (∥w∗ C̃ ∥∥x′i∥ − w∗⊤C̃ x ′ i) ≥ 0, δ(g(ŵC̃), x ′ i) := 1 2 (∥g(ŵC̃)∥∥x ′ i∥ − g(wC̃) ⊤x′i) ≥ 0.\nThe proof is presented in Appendix A. Lemma 1 tells that we have a lower and an upper bound of the score w∗⊤C x ′ i for each validation instance that linearly change with the regularization parameter C. When ŵC̃ is optimal, it can be shown that (see Proposition B.24 in [22]) there exists a subgradient such that g(ŵC̃) = 0, meaning that the bounds are tight because γ(g(ŵC̃), x ′ i) = δ(g(ŵC̃), x ′ i) = 0. Corollary 2. When C = C̃, the score w∗⊤ C̃\nx′i, i ∈ [n′], for the regularization parameter value C̃ itself satisfies w∗⊤\nC̃ x′i≥LB(w∗⊤C̃ x ′ i|ŵC̃)= ŵ ⊤ C̃ x′i−γ(g(ŵC̃), x ′ i), w ∗⊤ C̃ x′i≤UB(w∗⊤C̃ x ′ i|ŵC̃)= ŵ ⊤ C̃ x′i+δ(g(ŵC̃), x ′ i).\nThe results in Corollary 2 are obtained by simply substituting C = C̃ into (5a) and (5b)."
    }, {
      "heading" : "3.2 Validation Error Bounds",
      "text" : "Given a lower and an upper bound of the score of each validation instance, a lower bound of the validation error can be computed by simply using the following facts:\ny′i = +1 and UB(w ∗⊤ C x ′ i|ŵC̃) < 0⇒ mis-classified, (6a)\ny′i = −1 and LB(w∗⊤C x′i|ŵC̃) > 0⇒ mis-classified. (6b) Furthermore, since the bounds in Lemma 1 linearly change with the regularization parameter C, we can identify the interval of C within which the validation instance is guaranteed to be mis-classified. Lemma 3. For a validation instance with y′i = +1, if\nC̃ < C < β(ŵC̃ , x\n′ i)\nα(ŵC̃ , x ′ i) + δ(g(ŵC̃), x ′ i) C̃ or\nα(ŵC̃ , x ′ i)\nβ(ŵC̃ , x ′ i) + γ(g(ŵC̃), x ′ i) C̃ < C < C̃,\nthen the validation instance (x′i, y′i) is mis-classified. Similarly, for a validation instance with y′i = −1, if\nC̃ < C < α(ŵC̃ , x\n′ i)\nβ(ŵC̃ , x ′ i) + γ(g(ŵC̃), x ′ i) C̃ or\nβ(ŵC̃ , x ′ i)\nα(ŵC̃ , x ′ i) + δ(g(ŵC̃), x ′ i) C̃ < C < C̃,\nthen the validation instance (x′i, y′i) is mis-classified.\nThis lemma can be easily shown by applying (5) to (6).\nAs a direct consequence of Lemma 3, the lower bound of the validation error is represented as a function of the regularization parameter C in the following form. Theorem 4. Using an approximate solution ŵC̃ for a regularization parameter C̃, the validation error Ev(w∗C) for any C > 0 satisfies Ev(w ∗ C) ≥ LB(Ev(w∗C)|ŵC̃) := (7)\n1\nn′\n( ∑\ny′i=+1\nI ( C̃<C<\nβ(ŵC̃ , x ′ i)\nα(ŵC̃ , x ′ i)+δ(g(ŵC̃), x ′ i) C̃\n) + ∑\ny′i=+1\nI\n( α(ŵC̃ , x ′ i)\nβ(ŵC̃ , x ′ i)+γ(g(ŵC̃), x ′ i) C̃<C<C̃\n)\n+ ∑\ny′i=−1\nI ( C̃<C<\nα(ŵC̃ , x ′ i)\nβ(ŵC̃ , x ′ i)+γ(g(ŵC̃), x ′ i) C̃\n) + ∑\ny′i=−1\nI\n( β(ŵC̃ , x ′ i)\nα(ŵC̃ , x ′ i)+δ(g(ŵC̃), x ′ i) C̃<C<C̃\n)) .\nAlgorithm 1: Computing the approximation level ε from the given set of solutions Input: {(xi, yi)}i∈[n], {(x′i, y′i)}i∈[n′], Cl, Cu, W := {wC̃1 , . . . , wC̃T }\n1: Ebestv ← minC̃t∈{C̃1,...,C̃T } UB(Ev(w ∗ C̃t )|wC̃t)\n2: LB(E∗v )← minc∈[Cl,Cu] { maxC̃t∈{C̃1,...,C̃T } LB(Ev(w ∗ c )|wC̃t) }\nOutput: ε = Ebestv − LB(E∗v )\nThe lower bound (7) is a staircase function of the regularization parameter C. Remark 5. We note that our validation error lower bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16], which identifies sparsity of the optimal solutions before solving the optimization problem. A key technique used in those studies is to bound Lagrange multipliers at the optimal, and we utilize this technique to prove Lemma 1, which is a core of our framework.\nBy setting C = C̃, we can obtain a lower and an upper bound of the validation error for the regularization parameter C̃ itself, which are used in the algorithm as a stopping criteria for obtaining an approximate solution ŵC̃ . Corollary 6. Given an approximate solution ŵC̃ , the validation error Ev(w∗C̃) satisfies\nEv(w ∗ C̃ ) ≥ LB(Ev(w∗C̃)|ŵC̃)\n= 1\nn′\n( ∑\ny′i=+1\nI ( ŵ⊤\nC̃ x′i + δ(g(ŵC̃), x ′ i) < 0\n) + ∑\ny′i=−1\nI ( ŵ⊤\nC̃ x′i − γ(g(ŵC̃), x ′ i) > 0\n) ) , (8a)\nEv(w ∗ C̃ ) ≤ UB(Ev(w∗C̃)|ŵC̃)\n= 1− 1 n′\n( ∑\ny′i=+1\nI ( ŵ⊤\nC̃ x′i − γ(g(ŵC̃), x ′ i) ≥ 0\n) + ∑\ny′i=−1\nI ( ŵ⊤\nC̃ x′i + δ(g(ŵC̃), x ′ i) ≤ 0\n) ) . (8b)"
    }, {
      "heading" : "4 Algorithm",
      "text" : "In this section we present two algorithms for each of the two problems discussed in §2. Due to the space limitation, we roughly describe the most fundamental forms of these algorithms. Details and several extensions of the algorithms are presented in supplementary appendices B and C."
    }, {
      "heading" : "4.1 Problem setup 1: Computing the approximation level ε from a given set of solutions",
      "text" : "Given a set of (either optimal or approximate) solutions ŵC̃1 , . . . , ŵC̃T , obtained e.g., by ordinary grid-search, our first problem is to provide a theoretical approximation level ε in the sense of (3)2. This problem can be solved easily by using the validation error lower bounds developed in §3.2. The algorithm is presented in Algorithm 1, where we compute the current best validation error Ebestv in line 1, and a lower bound of the best possible validation error E∗v := minC∈[Cℓ,Cu] Ev(w ∗ C) in line 2. Then, the approximation level ε can be simply obtained by subtracting the latter from the former. We note that LB(E∗v ), the lower bound of E∗v , can be easily computed by using T evaluation error lower bounds LB(Ev(w∗C)|wC̃t), t = 1, . . . , T , because they are staircase functions of C."
    }, {
      "heading" : "4.2 Problem setup 2: Finding an ε-approximate regularization parameter",
      "text" : "Given a desired approximation level ε such as ε = 0.01, our second problem is to find an εapproximate regularization parameter. To this end we develop an algorithm that produces a set of optimal or approximate solutions ŵC̃1 , . . . , ŵC̃T such that, if we apply Algorithm 1 to this sequence, then approximation level would be smaller than or equal to ε. Algorithm 2 is the pseudo-code of this algorithm. It computes approximate solutions for an increasing sequence of regularization parameters in the main loop (lines 2-11).\n2 When we only have approximate solutions ŵC̃1 , . . . , ŵC̃T , Eq. (3) is slightly incorrect. The first term of the l.h.s. of (3) should be minC̃t∈{C̃1,...,C̃T } UB(Ev(ŵC̃t)|ŵC̃t).\nAlgorithm 2: Finding an ε approximate regularization parameter with approximate solutions Input: {(xi, yi)}i∈[n], {(x′i, y′i)}i∈[n′], Cl, Cu, ε\n1: t← 1, C̃t ← Cl, Cbest ← Cl, Ebestv ← 1 2: while C̃t ≤ Cu do 3: ŵC̃t←solve (1) approximately for C=C̃t 4: Compute UB(Ev(w∗C̃t)|ŵC̃t) by (8b). 5: if UB(Ev(w∗C̃t)|ŵC̃t) < E best v then 6: Ebestv ← UB(Ev(w∗C̃t)|ŵC̃t) 7: Cbest ← C̃t 8: end if 9: Set C̃t+1 by (10)\n10: t← t+ 1 11: end while Output: Cbest ∈ C(ε).\nLet us now consider tth iteration in the main loop, where we have already computed t−1 approximate solutions ŵC̃1 , . . . , ŵC̃t−1 for C̃1 < . . . < C̃t−1. At this point,\nCbest := arg min C̃τ∈{C̃1,...,C̃t−1} UB(Ev(w ∗ C̃τ )|ŵC̃τ ),\nis the best (in worst-case) regularization parameter obtained so far and it is guaranteed to be an ε-approximate regularization parameter in the interval [Cl,C̃t] in the sense that the validation error,\nEbestv := min C̃τ∈{C̃1,...,C̃t−1} UB(Ev(w ∗ C̃τ )|ŵC̃τ ),\nis shown to be at most greater by ε than the smallest possible validation error in the interval [Cl, C̃t]. However, we are not sure whether Cbest can still keep ε-approximation property for C > C̃t. Thus, in line 3, we approx-\nimately solve the optimization problem (1) at C = C̃t and obtain an approximate solution ŵC̃t . Note that the approximate solution ŵC̃t must be sufficiently good enough in the sense that UB(Ev(w∗C̃τ )|ŵC̃τ )− LB(Ev(w ∗ C̃τ\n)|ŵC̃τ ) is sufficiently smaller than ε (typically 0.1ε). If the upper bound of the validation error UB(Ev(w∗C̃τ )|ŵC̃τ ) is smaller than E best v , we update Ebestv and Cbest (lines 5-8).\nOur next task is to find C̃t+1 in such a way that Cbest is an ε-approximate regularization parameter in the interval [Cl, C̃t+1]. Using the validation error lower bound in Theorem 4, the task is to find the smallest C̃t+1 > C̃t that violates\nEbestv − LB(Ev(w∗C)|ŵC̃t) ≤ ε, ∀C ∈ [C̃t, Cu], (9)\nIn order to formulate such a C̃t+1, let us define\nP := {i ∈ [n′]|y′i = +1, UB(w∗⊤C̃t x ′ i|ŵC̃t) < 0},N := {i ∈ [n ′]|y′i = −1, LB(w∗⊤C̃t x ′ i|ŵC̃t) > 0}.\nFurthermore, let\nΓ := { β(ŵC̃t , x ′ i)\nα(ŵC̃t , x ′ i) + δ(g(ŵC̃t), x ′ i) C̃t } i∈P ∪ { α(ŵC̃t , x ′ i) β(ŵC̃t , x ′ i) + γ(g(ŵC̃t), x ′ i) C̃t } i∈N ,\nand denote the kth-smallest element of Γ as kth(Γ) for any natural number k. Then, the smallest C̃t+1 > C̃t that violates (9) is given as\nC̃t+1←(⌊n′(LB(Ev(w∗C̃t)|ŵC̃t)−E best v +ε)⌋+1)th(Γ). (10)"
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section we present experiments for illustrating the proposed methods. Table 2 summarizes the datasets used in the experiments. They are taken from libsvm dataset repository [23]. All the input features except D9 and D10 were standardized to [−1, 1]3. For illustrative results, the instances were randomly divided into a training and a validation sets in roughly equal sizes. For quantitative results, we used 10-fold CV. We used Huber hinge loss (e.g., [24]) which is convex and subdifferentiable with respect to the second argument. The proposed methods are free from the choice of optimization solvers. In the experiments, we used an optimization solver described in [25], which is also implemented in well-known liblinear software [26]. Our slightly modified code\n3 We use D9 and D10 as they are for exploiting sparsity.\n(for adaptation to Huber hinge loss) is provided as a supplementary material, and is also available on https://github.com/takeuchi-lab/RPCVELB. Whenever possible, we used warmstart approach, i.e., when we trained a new solution, we used the closest solutions trained so far (either approximate or optimal ones) as the initial starting point of the optimizer. All the computations were conducted by using a single core of an HP workstation Z800 (Xeon(R) CPU X5675 (3.07GHz), 48GB MEM). In all the experiments, we set Cℓ = 10−3 and Cu = 103.\nResults on problem 1 We applied Algorithm 1 in §4 to a set of solutions obtained by 1) gridsearch, 2) Bayesian optimization (BO) with expected improvement acquisition function, and 3) adaptive search with our framework which sequentially computes a solution whose validation lower bound is smallest based on the information obtained so far. Figure 2 illustrates the results on three datasets, where we see how the approximation level ε in the vertical axis changes as the number of solutions (T in our notation) increases. In grid-search, as we increase the grid points, the approximation level ε tends to be improved. Since BO tends to focus on a small region of the regularization parameter, it was difficult to tightly bound the approximation level. We see that the adaptive search, using our framework straightforwardly, seems to offer slight improvement from grid-search.\nResults on problem 2 We applied Algorithm 2 to benchmark datasets for demonstrating theoretically guaranteed choice of a regularization parameter is possible with reasonable computational costs. Besides the algorithm presented in §4, we also tested a variant described in supplementary Appendix B. Specifically, we have three algorithm options. In the first option (op1), we used optimal solutions {w∗\nC̃t }t∈[T ] for computing CV error lower bounds. In the second option (op2),\nwe instead used approximate solutions {ŵC̃t}t∈[T ]. In the last option (op3), we additionally used speed-up tricks described in supplementary Appendix B. We considered four different choices of ε ∈ {0.1, 0.05, 0.01, 0}. Note that ε = 0 indicates the task of finding the exactly optimal regular-\nization parameter. In some datasets, the smallest validation errors are less than 0.1 or 0.05, in which cases we do not report the results (indicated as “Ev < 0.05” etc.). In trick1, we initially computed solutions at four different regularization parameter values evenly allocated in [10−3, 103] in the logarithmic scale. In trick2, the next regularization parameter C̃t+1 was set by replacing ε in (10) with 1.5ε (see supplementary Appendix B). For the purpose of illustration, we plot examples of validation error curves in several setups. Figure 3 shows the validation error curves of ionosphere (D3) dataset for several options and ε.\nTable 1 shows the number of optimization problems solved in the algorithm (denoted as T ), and the total computation time in CV setups. The computational costs mostly depend on T , which gets smaller as ε increases. Two tricks in supplementary Appendix B was effective in most cases for reducing T . In addition, we see the advantage of using approximate solutions by comparing the computation times of op1 and op2 (though this strategy is only for ε ̸= 0). Overall, the results suggest that the proposed algorithm allows us to find theoretically guaranteed approximate regularization parameters with reasonable costs except for ε = 0 cases. For example, the algorithm found an ε = 0.01 approximate regularization parameter within a minute in 10-fold CV for a dataset with more than 50000 instances (see the results on D10 for ε = 0.01 with op2 and op3 in Table 1)."
    }, {
      "heading" : "6 Conclusions and future works",
      "text" : "We presented a novel algorithmic framework for computing CV error lower bounds as a function of the regularization parameter. The proposed framework can be used for a theoretically guaranteed choice of a regularization parameter. Additional advantage of this framework is that we only need to compute a set of sufficiently good approximate solutions for obtaining such a theoretical guarantee, which is computationally advantageous. As demonstrated in the experiments, our algorithm is practical in the sense that the computational cost is reasonable as long as the approximation quality ε is not too close to 0. An important future work is to extend the approach to multiple hyper-parameters tuning setups."
    } ],
    "references" : [ {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. TIbshirani" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2004
    }, {
      "title" : "The entire regularization path for the support vector machine",
      "author" : [ "T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Piecewise linear regularized solution paths",
      "author" : [ "S. Rosset", "J. Zhu" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Approximating Concavely Parameterized Optimization Problems",
      "author" : [ "J. Giesen", "J. Mueller", "S. Laue", "S. Swiercy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Approximating Parameterized Convex Optimization Problems",
      "author" : [ "J. Giesen", "M. Jaggi", "S. Laue" ],
      "venue" : "ACM Transactions on Algorithms,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Robust and Efficient Kernel Hyperparameter Paths with Guarantees",
      "author" : [ "J. Giesen", "S. Laue", "Wieschollek P" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Complexity analysis of the Lasso reguralization path",
      "author" : [ "J. Mairal", "B. Yu" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Bounds on Error Expectation for Support Vector Machines",
      "author" : [ "V. Vapnik", "O. Chapelle" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Estimating the generalization performance of a SVM efficiently",
      "author" : [ "T. Joachims" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "Radius margin bounds for support vector machines with the RBF kernel",
      "author" : [ "K. Chung", "W. Kao", "C. Sun", "L. Wang", "C. Lin" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "An efficient method for computing leave-one-out error in support vector machines with Gaussian kernels",
      "author" : [ "M. Lee", "S. Keerthi", "C. Ong", "D. DeCoste" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Safe feature elimination in sparse supervised learning",
      "author" : [ "L. El Ghaoui", "V. Viallon", "T. Rabbani" ],
      "venue" : "Pacific Journal of Optimization,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Learning sparse representations of high dimensional data on large scale dictionaries",
      "author" : [ "Z. Xiang", "H. Xu", "P. Ramadge" ],
      "venue" : "In Advances in Neural Information Processing Sysrtems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Safe screening of non-support vectors in pathwise SVM computation",
      "author" : [ "K. Ogawa", "Y. Suzuki", "I. Takeuchi" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Safe Screening with Variational Inequalities and Its Application to Lasso",
      "author" : [ "J. Liu", "Z. Zhao", "J. Wang", "J. Ye" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "A Safe Screening Rule for Sparse Logistic Regression",
      "author" : [ "J. Wang", "J. Zhou", "J. Liu", "P. Wonka", "J. Ye" ],
      "venue" : "In Advances in Neural Information Processing Sysrtems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "The Nature of Statistical Learning",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1996
    }, {
      "title" : "Understanding machine learning",
      "author" : [ "S. Shalev-Shwartz", "S. Ben-David" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Practical Bayesian Optimization of Machine Learning Algorithms",
      "author" : [ "J. Snoek", "H. Larochelle", "R. Adams" ],
      "venue" : "In Advances in Neural Information Processing Sysrtems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Random Search for Hyper-Parameter Optimization",
      "author" : [ "J. Bergstra", "Y. Bengio" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Choosing multiple parameters for support vector machines",
      "author" : [ "O. Chapelle", "V. Vapnik", "O. Bousquet", "S. Mukherjee" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "Nonlinear Programming",
      "author" : [ "P D. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1999
    }, {
      "title" : "LIBSVM : A Library for Support Vector Machines",
      "author" : [ "C. Chang", "C. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Training a support vector machine in the primal",
      "author" : [ "O. Chapelle" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Trust Region Newton Method for Large-Scale Logistic Regression",
      "author" : [ "C. Lin", "R. Weng", "S. Keerthi" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2008
    }, {
      "title" : "LIBLINEAR: A library for large linear classification",
      "author" : [ "R. Fan", "K. Chang", "C. Hsieh" ],
      "venue" : "The Journal of Machine Learning,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Exact regularization path has been intensively studied [1, 2], but they are known to be numerically unstable and do not scale well.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "Exact regularization path has been intensively studied [1, 2], but they are known to be numerically unstable and do not scale well.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, exact regularization path can be computed only for a limited class of problems whose solutions are written as piecewise-linear functions of the regularization parameter [3].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 3,
      "context" : "This work was motivated by recent studies on approximate regularization path [4, 5, 6, 7].",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "This work was motivated by recent studies on approximate regularization path [4, 5, 6, 7].",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "This work was motivated by recent studies on approximate regularization path [4, 5, 6, 7].",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "This work was motivated by recent studies on approximate regularization path [4, 5, 6, 7].",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : ", [8, 9, 10, 11]), none of them satisfy the above required properties.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : ", [8, 9, 10, 11]), none of them satisfy the above required properties.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : ", [8, 9, 10, 11]), none of them satisfy the above required properties.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : ", [8, 9, 10, 11]), none of them satisfy the above required properties.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 11,
      "context" : "The idea of our CV error bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16] (see Appendix A for the detail).",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "The idea of our CV error bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16] (see Appendix A for the detail).",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "The idea of our CV error bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16] (see Appendix A for the detail).",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 14,
      "context" : "The idea of our CV error bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16] (see Appendix A for the detail).",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "The idea of our CV error bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16] (see Appendix A for the detail).",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "Although generalization error bounds such as structural risk minimization [17] might be used for a rough tuning of a regularization parameter, they are known to be too loose to use as an alternative to CV (see, e.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "We also note that our contribution is not in presenting new method for regularization parameter tuning such as Bayesian optimization [19], random search [20] and gradient-based search [21].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 19,
      "context" : "We also note that our contribution is not in presenting new method for regularization parameter tuning such as Bayesian optimization [19], random search [20] and gradient-based search [21].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 20,
      "context" : "We also note that our contribution is not in presenting new method for regularization parameter tuning such as Bayesian optimization [19], random search [20] and gradient-based search [21].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "24 in [22]) there exists a subgradient such that g(ŵC̃) = 0, meaning that the bounds are tight because γ(g(ŵC̃), x ′ i) = δ(g(ŵC̃), x ′ i) = 0.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 11,
      "context" : "We note that our validation error lower bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16], which identifies sparsity of the optimal solutions before solving the optimization problem.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "We note that our validation error lower bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16], which identifies sparsity of the optimal solutions before solving the optimization problem.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 13,
      "context" : "We note that our validation error lower bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16], which identifies sparsity of the optimal solutions before solving the optimization problem.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "We note that our validation error lower bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16], which identifies sparsity of the optimal solutions before solving the optimization problem.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "We note that our validation error lower bound is inspired from recent studies on safe screening [12, 13, 14, 15, 16], which identifies sparsity of the optimal solutions before solving the optimization problem.",
      "startOffset" : 96,
      "endOffset" : 116
    }, {
      "referenceID" : 22,
      "context" : "They are taken from libsvm dataset repository [23].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 23,
      "context" : ", [24]) which is convex and subdifferentiable with respect to the second argument.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 24,
      "context" : "In the experiments, we used an optimization solver described in [25], which is also implemented in well-known liblinear software [26].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 25,
      "context" : "In the experiments, we used an optimization solver described in [25], which is also implemented in well-known liblinear software [26].",
      "startOffset" : 129,
      "endOffset" : 133
    } ],
    "year" : 2015,
    "abstractText" : "Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances. Nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error. In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter, which we call regularization path of CV error lower bounds. The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters. Our numerical experiments demonstrate that a theoretically guaranteed choice of a regularization parameter in the above sense is possible with reasonable computational costs.",
    "creator" : null
  }
}