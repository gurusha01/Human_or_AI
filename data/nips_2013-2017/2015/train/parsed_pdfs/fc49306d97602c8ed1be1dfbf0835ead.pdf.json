{
  "name" : "fc49306d97602c8ed1be1dfbf0835ead.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cross-Domain Matching for Bag-of-Words Data via Kernel Embeddings of Latent Distributions",
    "authors" : [ "Yuya Yoshikawa", "Tomoharu Iwata", "Takeshi Yamada" ],
    "emails" : [ "yoshikawa.yuya.yl9@is.naist.jp", "iwata.tomoharu@lab.ntt.co.jp", "sawada.hiroshi@lab.ntt.co.jp", "yamada.tak@lab.ntt.co.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features, e.g., a bag-ofwords representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags."
    }, {
      "heading" : "1 Introduction",
      "text" : "The discovery of matched instances in different domains is an important task, which appears in natural language processing, information retrieval and data mining tasks such as finding the alignment of cross-lingual sentences [1], attaching tags to images [2] or text documents [3], and matching user identifications in different databases [4].\nWhen given an instance in a source domain, our goal is to find the instance in a target domain that is the most closely related to the given instance. In this paper, we focus on a supervised setting, where correspondence information between some instances in different domains is given. To find matching in a single domain, e.g., find documents relevant to an input document, a similarity (or distance) measure between instances can be used. On the other hand, when trying to find matching between instances in different domains, we cannot directly measure the distances since they consist of different types of features. For example, when matching documents in different languages, since the documents have different vocabularies we cannot directly measure the similarities between documents across different languages without dictionaries.\n∗The author moved to Software Technology and Artificial Intelligence Research Laboratory (STAIR Lab) at Chiba Institute of Technology, Japan.\nOne solution is to map instances in both the source and target domains into a shared latent space. One such method is canonical correspondence analysis (CCA) [5], which maps instances into a latent space by linear projection to maximize the correlation between paired instances in the latent space. However, in practice, CCA cannot solve non-linear relationship problems due to its linearity. To find non-linear correspondence, kernel CCA [6] can be used. It has been reported that kernel CCA performs well as regards document/sentence alignment between different languages [7, 8], when searching for images from text queries [9] and when matching 2D-3D face images [10]. Note that the performance of kernel CCA depends on how appropriately we define the kernel function for measuring the similarity between instances within a domain. Many kernels, such as linear, polynomial and Gaussian kernels, cannot consider the occurrence of different but semantically similar words in two instances because these kernels use the inner-product between the feature vectors representing the instances. For example, words, ‘PC’ and ‘Computer’, are different but indicate the same meaning. Nevertheless, the kernel value between instances consisting only of ‘PC’ and consisting only of ‘Computer’ is equal to zero with linear and polynomial kernels. Even if a Gaussian kernel is used, the kernel value is determined only by the vector length of the instances.\nIn this paper, we propose a kernel-based cross-domain matching method that can overcome the problem of kernel CCA. Figure 1 shows an example of the proposed method. The proposed method assumes that each feature in source and target domains is associated with a latent vector in a shared latent space. Since all the features are mapped into the latent space, the proposed method can measure the similarity between features in different domains. Then, each instance is represented as a distribution of the latent vectors of features that are contained in the instance. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions, which measures the difference between distributions in a reproducing kernel Hilbert space (RKHS) without the need to define parametric distributions. The latent vectors are estimated by minimizing the differences between the distributions of paired instances while keeping unpaired instances apart. The proposed method can discover unseen matching in test data by using the distributions of the estimated latent vectors. We will explain matching between two domains below, however, the proposed method can be straightforwardly extended to matching between three and more domains by regarding one of the domains as a pivot domain.\nIn our experiments, we demonstrate the effectiveness of our proposed method in tasks that involve finding the correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags, by comparison with existing linear and non-linear matching methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "As described above, canonical correlation analysis (CCA) and kernel CCA have been successfully used for finding various types of cross-domain matching. When we want to match cross-domain instances represented by bag-of-words such as documents, bilingual topic models [1, 11] can also be used. The difference between the proposed method and these methods is that since the proposed method represents each instance as a set of latent vectors of its own features, the proposed method can learn a more complex representation of the instance than these existing methods that represent\neach instance as a single latent vector. Another difference is that the proposed method employs a discriminative approach, while kernel CCA and bilingual topic models employ generative ones.\nTo model cross-domain data, deep learning and neural network approaches have been recently proposed [12, 13]. Unlike such approaches, the proposed method performs non-linear matching without deciding the number of layers of the networks, which largely affects their performances.\nA key technique of the proposed method is the kernel embeddings of distributions [14], which can represent a distribution as an element in an RKHS, while preserving the moment information of the distribution such as the mean, covariance and higher-order moments without density estimation. The kernel embeddings of distributions have been successfully used for a statistical test of the independence of two sample sets [15], discriminative learning on distribution data [16], anomaly detection for group data [17], density estimation [18] and a three variable interaction test [19]. Most previous studies about the kernel embeddings of distributions consider cases where the distributions are unobserved but the samples generated from the distributions are observed. Additionally, each of the samples is represented as a dense vector. With the proposed method, the kernel embedding technique cannot be used to represent the observed multisets of features such as bag-of-words for documents, since each of the features is represented as a one-hot vector whose dimensions are zero except for the dimension indicating that the feature has one. In this study, we benefit from the kernel embeddings of distributions by representing each feature as a dense vector in a shared latent space. The proposed method is inspired by the use of the kernel embeddings of distributions in bag-ofwords data classification [20] and regression [21]. Their methods can be applied to single domain data, and the latent vectors of features are used to measure the similarity between the features in a domain. Unlike these methods, the proposed method is used for the cross-domain matching of two different types of domain data, and the latent vectors are used for measuring the similarity between the features in different domains."
    }, {
      "heading" : "3 Kernel Embeddings of Distributions",
      "text" : "In this section, we introduce the framework of the kernel embeddings of distributions. The kernel embeddings of distributions are used to embed any probability distribution P on space X into a reproducing kernel Hilbert space (RKHS) Hk specified by kernel k, and the distribution is represented as element m∗(P) in the RKHS. More precisely, when given distribution P, the kernel embedding of the distribution m∗(P) is defined as follows:\nm∗(P) := Ex∼P[k(·,x)] = ∫ X k(·,x)dP ∈ Hk, (1)\nwhere kernel k is referred to as embedding kernel. It is known that kernel embedding m∗(P) preserves the properties of probability distribution P such as the mean, covariance and higher-order moments by using characteristic kernels (e.g., Gaussian RBF kernel) [22].\nWhen a set of samples X = {xl}nl=1 is drawn from the distribution, by interpreting sample set X as empirical distribution P̂ = 1n ∑n l=1 δxl(·), where δx(·) is the Dirac delta function at point x ∈ X , empirical kernel embedding m(X) is given by\nm(X) = 1\nn n∑ l=1 k(·,xl), (2)\nwhich can be approximated with an error rate of ||m(X)−m∗(P)||Hk = Op(n− 1 2 ) [14]. Unlike kernel density estimation, the error rate of the kernel embeddings is independent of the dimensionality of the given distribution."
    }, {
      "heading" : "3.1 Measuring Difference between Distributions",
      "text" : "By using the kernel embedding representation Eq. (2), we can measure the difference between two distributions. Given two sets of samples X = {xl}nl=1 and Y = {yl′}n ′\nl′=1 where xl and yl′ belong to the same space, we can obtain their kernel embedding representations m(X) and m(Y). Then, the difference between m(X) and m(Y) is given by\nD(X,Y) = ||m(X)−m(Y)||2Hk . (3) Intuitively, it reflects the difference in the moment information of the distributions. The difference is equivalent to the square of maximum mean discrepancy (MMD), which is used for a statistical test\nof independence of two distributions [15]. The difference can be calculated by expanding Eq. (3) as follows:\n||m(X)−m(Y)||2Hk = ⟨m(X),m(X)⟩Hk + ⟨m(Y),m(Y)⟩Hk − 2⟨m(X),m(Y)⟩Hk , (4) where, ⟨·, ·⟩Hk is an inner-product in the RKHS. In particular, ⟨m(X),m(Y)⟩Hk is given by\n⟨m(X),m(Y)⟩Hk =\n⟨ 1\nn n∑ l=1 k(·,xl), 1 n′ n′∑ l′=1 k(·,yl′) ⟩ Hk = 1 nn′ n∑ l=1 n′∑ l′=1 k(xl,yl′). (5)\n⟨m(X),m(X)⟩Hk and ⟨m(Y),m(Y)⟩Hk can also be calculated by Eq. (5)."
    }, {
      "heading" : "4 Proposed Method",
      "text" : "Suppose that we are given a training set consisting of N instance pairs O = {(dsi , dti)}Ni=1, where dsi is the ith instance in a source domain and dti is the ith instance in a target domain. These instances dsi and d t i are represented as multisets of features included in source feature set Fs and target feature set F t, respectively. This means that these instances are represented as bag-of-words (BoW). The goal of our task is to determine the unseen relationship between instances across source and target domains in test data. The number of instances in the source domain may be different to that in the target domain."
    }, {
      "heading" : "4.1 Kernel Embeddings of Distributions in a Shared Latent Space",
      "text" : "As described in Section 1, the difficulty as regards finding cross-domain instance matching is that the similarity between instances across source and target domains cannot be directly measured. We have also stated that although we can find a latent space that can measure the similarity by using kernel CCA, standard kernel functions, e.g., a Gaussian kernel, cannot reflect the co-occurrence of different but related features in a kernel calculation between instances. To overcome them, we propose a new data representation for finding cross-domain instance matching. The proposed method assumes that each feature in a source feature set, f ∈ Fs, has a q-dimensional latent vector xf ∈ Rq in a shared space. Likewise, each feature in target feature set, g ∈ F t, has a q-dimensional latent vector yg ∈ Rq in the shared space. Since all the features in the source and target domains are mapped into a common shared space, the proposed method can capture the relationship between features both in each domain and across different domains. We define the sets of latent vectors in the source and target domains as X = {xf}f∈Fs and Y = {yg}g∈Ft , respectively. The proposed method assumes that each instance is represented by a distribution (or multiset) of the latent vectors of the features that are contained in the instance. The ith instance in the source domain dsi is represented by a set of latent vectors Xi = {xf}f∈dsi and the jth instance in the target domain dtj is represented by a set of latent vectors Yj = {yg}g∈dtj . Note that Xi and Yj lie in the same latent space.\nIn Section 3, we introduced the kernel embedding representation of a distribution and described how to measure the difference between two distributions when samples generated from the distribution are observed. In the proposed method, we employ the kernel embeddings of distributions to represent the distributions of the latent vectors for the instances. The kernel embedding representations for the ith source and the jth target domain instances are given by\nm(Xi) = 1 |dsi | ∑ f∈dsi k(·,xf ), m(Yj) = 1 |dtj | ∑ g∈dtj k(·,yg). (6)\nThen, the difference between the distributions of the latent vectors are measured by using Eq. (3), that is, the difference between the ith source and the jth target domain instances is given by\nD(Xi,Yj) = ||m(Xi)−m(Yj)||2Hk . (7)"
    }, {
      "heading" : "4.2 Model",
      "text" : "The proposed method assumes that paired instances have similar distributions of latent vectors and unpaired instances have different distributions. In accordance with the assumption, we define the likelihood of the relationship between the ith source domain instance and the jth target domain instance as follows:\np(dtj |dsi ,X,Y, θ) = exp (−D(Xi,Yj))∑N\nj′=1 exp (−D(Xi,Yj′)) , (8)\nwhere, θ is a set of hyper-parameters for the embedding kernel used in Eq. (6). Eq. (8) is in fact the conditional probability with which the jth target domain instance is chosen given the ith source domain instance. This formulation is more efficient than we consider a bidirectional matching. Intuitively, when distribution Xi is more similar to Yj than other distributions {Yj′ | j′ ̸= j}Nj′=1, the probability has a higher value.\nWe define the posterior distribution of latent vectors X and Y. By placing Gaussian priors with precision parameter ρ > 0 for X and Y, that is, p(X|ρ) ∝ ∏ x∈X exp ( −ρ2 ||x|| 2 2 ) , p(Y|ρ) ∝∏\ny∈Y exp ( −ρ2 ||y|| 2 2 ) , the posterior distribution is given by\np(X,Y|O,Θ) = 1 Z p(X|ρ)p(Y|ρ) N∏ i=1 p(dti|dsi ,X,Y, θ), (9)\nwhere, O = {(dsi , dti)}Ni=1 is a training set of N instance pairs, Θ = {θ, ρ} is a set of hyperparameters and Z = ∫ ∫ p(X,Y,O,Θ)dXdY is a marginal probability, which is constant with respect to X and Y."
    }, {
      "heading" : "4.3 Learning",
      "text" : "We estimate latent vectors X and Y by maximizing the posterior probability of the latent vectors given by Eq. (9). Instead of Eq. (9), we consider the following negative logarithm of the posterior probability,\nL(X,Y) = N∑ i=1 D(Xi,Yi) + log N∑ j=1 exp (−D(Xi,Yj)) + ρ2 ∑ x∈X ||x||22 + ∑ y∈Y ||y||22  , (10) and minimize it with respect to the latent vectors. Here, maximizing Eq. (9) is equivalent to minimizing Eq. (10). To minimize Eq. (10) with respect to X and Y, we perform a gradient-based optimization. The gradient of Eq. (10) with respect to each xf ∈ X is given by\n∂L(X,Y) ∂xf\n= ∑\ni:f∈dsi\n∂D(Xi,Yi)∂xf − 1ci N∑ j=1 eij ∂D(Xi,Yj) ∂xf + ρxf (11) where,\neij = exp (−D(Xi,Yj)) , ci = N∑ j=1 exp (−D(Xi,Yj)) , (12)\nand the gradient of the difference between distributions Xi and Yj with respect to xf is given by ∂D(Xi,Yj)\n∂xf =\n1 |dsi |2 ∑ l∈dsi ∑ l′∈dsi ∂k(xl,xl′) ∂xf − 2 |dsi ||dtj | ∑ l∈dsi ∑ g∈dti ∂k(xl,yg) ∂xf . (13)\nWhen the distribution Xi does not include the latent vector xf , the gradient consistently becomes a zero vector. ∂k(xl,xl′ )∂xf is the gradient of an embedding kernel. This depends on the choice of kernel. When the embedding kernel is a Gaussian kernel, the gradient is calculated as with Eq. (15) in [21]. Similarly, The gradient of Eq. (10) with respect to each yg ∈ Y is given by\n∂L(X,Y) ∂yg = N∑ i=1 ∂D(Xi,Yi)∂yg − 1ci ∑ j:g∈dtj eij ∂D(Xi,Yj) ∂yg + ρyg, (14) where, the gradient of the difference between distributions Xi and Yj with respect to yg can be calculated as with Eq. (13)\nLearning is performed by alternately updating X using Eq. (11) and updating Y using Eq. (14) until the improvement in the negative log likelihood Eq. (10) converges."
    }, {
      "heading" : "4.4 Matching",
      "text" : "After the estimation of the latent vectors X and Y, the proposed method can reveal the matching between test instances. The matching is found by first measuring the difference between a given source domain instance and target domain instances using Eq. (7), and then searching for the instance pair with the smallest difference."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we report our experimental results for three different types of cross-domain datasets: multi-lingual Wikipedia, document-tag and image-tag datasets.\nSetup of proposed method. Throughout these experiments, we used a Gaussian kernel with parameter γ ≥ 0: k(xf ,yg) = exp ( −γ2 ||xf − yg|| 2 2 ) as an embedding kernel. The hyper-parameters of the proposed method are the dimensionality of a shared latent space q, a regularizer parameter for latent vectors ρ and a Gaussian embedding kernel parameter γ. After we train the proposed method with various hyper-parameters q ∈ {8, 10, 12}, ρ ∈ {0, 10−2, 10−1} and γ ∈ {10−1, 100, · · · , 103}, we chose the optimal hyper-parameters by using validation data. When training the proposed method, we initialized latent vectors X and Y by applying principle component analysis (PCA) to a matrix concatenating two feature-frequency matrices in the source and target domains. Then, we employed the L-BFGS method [23] with gradients given by Eqs. (11) (14) to learn the latent vectors.\nComparison methods. We compared the proposed method with the k-nearest neighbor method (KNN), canonical correspondence analysis (CCA), kernel CCA (KCCA), bilingual latent Dirichlet allocation (BLDA), and kernel CCA with the kernel embeddings of distributions (KED-KCCA). For a test instance in the source domain, our KNN searches for the nearest neighbor source instances in the training data, and outputs a target instance in the test data, which is located close to the target instances that are paired with the searched for source instances. CCA and KCCA first learn the projection of instances into a shared latent space using training data, and then they find matching between instances by projecting the test instances into the shared latent space. KCCA used a Gaussian kernel for measuring the similarity between instances and chose the optimal Gaussian kernel parameter and regularizer parameter by using validation data. With BLDA, we first learned the same model as [1, 11] and found matching between instances in the test data by obtaining the topic distributions of these instances from the learned model. KED-KCCA uses the kernel embeddings of distributions described in Section 3 for obtaining the kernel values between the instances. The vector representations of features were obtained by applying singular value decomposition (SVD) for instance-feature frequency matrices. Here, we set the dimensionality of the vector representations to 100. Then, KED-KCCA learns kernel CCA with the kernel values as with the above KCCA. With CCA, KCCA, BLDA and KED-KCCA, we chose the optimal latent dimensionality (or number of topics) within {10, 20, · · · , 100} by using validation data. Evaluation method. Throughout the experiments, we quantitatively evaluated the matching performance by using the precision with which the true target instance is included in a set of R candidate instances, S(R), found by each method. More formally, the precision is given by\nPrecision@R = 1\nNte Nte∑ i=1 δ (ti ∈ Si(R)) , (15)\nwhere, Nte is the number of test instances in the target domain, ti is the ith true target instance, Si(R) is R candidate instances of the ith source instance and δ(·) is the binary function that returns 1 if the argument is true, and 0 otherwise."
    }, {
      "heading" : "5.1 Matching between Bilingual Documents",
      "text" : "With a multi-lingual Wikipedia document dataset, we examine whether the proposed method can find the correct matching between documents written in different languages. The dataset includes 34,024 Wikipedia documents for each of six languages: German (de), English (en), Finnish (fi), French (fr), Italian (it) and Japanese (ja), and documents with the same content are aligned across the languages. From the dataset, we create 6C2 = 15 bilingual document pairs. We regard the first component of the pair as a source domain and the other as a target domain. For each of the bilingual document pairs, we randomly create 10 evaluation sets that consist of 1,000 document pairs as training data, 100 document pairs as validation data and 100 document pairs as test data. Here, each document is represented as a bag-of-words without stopwords and low frequency words.\nFigure 2 shows the matching precision for each of the bilingual pairs of the Wikipedia dataset. With all the bilingual pairs, the proposed method achieves significantly higher precision than the other methods with a wide range of R. Table 1 shows examples of predicted matching with the Japanese-English Wikipedia dataset. Compared with KCCA, which is the second best method, the\nproposed method can find both the correct document and many related documents. For example, in Table 1(a), the correct document title is “SD card”. The proposed method outputs the SD card’s document and documents related to computer technology such as “Intel” and “MPlayer”. This is because the proposed method can capture the relationship between words and reflect the difference between documents across different domains by learning the latent vectors of the words."
    }, {
      "heading" : "5.2 Matching between Documents and Tags, and between Images and Tags",
      "text" : "We performed experiments matching documents and tailgates, and matching images and tailgates with the datasets used in [3]. When matching documents and tailgates, we use datasets obtained from two social bookmarking sites, delicious1 and hatena2, and patent dataset. The delicious and the hatena datasets include pairs consisting of a web page and a tag list labeled by users, and the patent dataset includes pairs consisting of a patent description and a tag list representing the category of the patent. Each web page and each patent description are represented\n1https://delicious.com/ 2http://b.hatena.ne.jp/\nas a bag-of-words as with the experiments using the Wikipedia dataset, and the tag list is represented as a set of tags. With the matching of images and tag lists, we use the flickr dataset, which consists of pairs of images and tag lists. Each image is represented as a bag-of-visual-words, which is obtained by first extracting features using SIFT, and then applying K-means clustering with 200 components to the SIFT features. For all the datasets, the numbers of training, test and validation pairs are 1,000, 100 and 100, respectively.\nFigure 3 shows the precision of the matching prediction of the proposed and comparison methods for the delicious, hatena, patent and flickr datasets. The precision of the comparison methods with these datasets was much the same as the precision of random prediction. Nevertheless, the proposed method achieved very high precision particularly for the delicious, hatena and patent datasets. Figure 4 shows examples of input tag lists and the top five images matched by the proposed method with the flickr dataset. In the examples, the proposed method found the correct images and similar related images from given tag lists."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have proposed a novel kernel-based method for addressing cross-domain instance matching tasks with bag-of-words data. The proposed method represents each feature in all the domains as a latent vector in a shared latent space to capture the relationship between features. Each instance is represented by a distribution of the latent vectors of features associated with the instance, which can be regarded as samples from the unknown distribution corresponding to the instance. To calculate difference between the distributions efficiently and nonparametrically, we employ the framework of kernel embeddings of distributions, and we learn the latent vectors so as to minimize the difference between the distributions of paired instances in a reproducing kernel Hilbert space. Experiments on various types of cross-domain datasets confirmed that the proposed method significantly outperforms the existing methods for cross-domain matching.\nAcknowledgments. This work was supported by JSPS Grant-in-Aid for JSPS Fellows (259867)."
    } ],
    "references" : [ {
      "title" : "Cross Lingual Entity Linking with Bilingual Topic Model",
      "author" : [ "T Zhang", "K Liu", "J Zhao" ],
      "venue" : "Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics",
      "author" : [ "Yunchao Gong", "Qifa Ke", "Michael Isard", "Svetlana Lazebnik" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Modeling Social Annotation Data with Content Relevance using a Topic Model",
      "author" : [ "Tomoharu Iwata", "T. Yamada", "N. Ueda" ],
      "venue" : "In Advances in Neural Information Processing Systems. Citeseer,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Transfer Learning for Collaborative Filtering via a Rating- Matrix Generative Model",
      "author" : [ "Bin Li", "Qiang Yang", "Xiangyang Xue" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Relations Between Two Sets of Variants",
      "author" : [ "H. Hotelling" ],
      "venue" : "Biometrika, 28:321–377",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1936
    }, {
      "title" : "A Kernel Method for Canonical Correlation Analysis",
      "author" : [ "S Akaho" ],
      "venue" : "Proceedings of International Meeting on Psychometric Society, number 4",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis",
      "author" : [ "Alexei Vinokourov", "John Shawe-Taylor", "Nello Cristianini" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Using KCCA for Japanese-English Cross-Language Information Retrieval and Document Classification",
      "author" : [ "Yaoyong Li", "John Shawe-Taylor" ],
      "venue" : "Journal of Intelligent Information Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "A New Approach to Cross-Modal Multimedia Retrieval",
      "author" : [ "Nikhil Rasiwasia", "Jose Costa Pereira", "Emanuele Coviello", "Gabriel Doyle", "Gert R.G. Lanckriet", "Roger Levy", "Nuno Vasconcelos" ],
      "venue" : "In Proceedings of the International Conference on Multimedia,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Zachariasov. 2D-3D Face Recognition Method Based on a Modified CCA-PCA Algorithm",
      "author" : [ "Patrik Kamencay", "Robert Hudec", "Miroslav Benco", "Martina" ],
      "venue" : "International Journal of Advanced Robotic Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Fashion Coordinates Recommender System Using Photographs from Fashion Magazines",
      "author" : [ "Tomoharu Iwata", "Shinji Watanabe", "Hiroshi Sawada" ],
      "venue" : "In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence. AAAI Press,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Multimodal Deep Learning",
      "author" : [ "Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng" ],
      "venue" : "In Proceedings of The 28th International Conference on Machine Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Deep Canonical Correlation Analysis",
      "author" : [ "Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu" ],
      "venue" : "In Proceedings of The 30th International Conference on Machine Learning,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "A Hilbert Space Embedding for Distributions",
      "author" : [ "Alex Smola", "Arthur Gretton", "Le Song", "Bernhard Schölkopf" ],
      "venue" : "In Algorithmic Learning Theory",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "A Kernel Statistical Test of Independence",
      "author" : [ "A. Gretton", "K. Fukumizu", "C.H. Teo", "L. Song", "B. Schölkopf", "A.J. Smola" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Learning from Distributions via Support Measure Machines",
      "author" : [ "Krikamol Muandet", "Kenji Fukumizu", "Francesco Dinuzzo", "Bernhard Schölkopf" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "One-Class Support Measure Machines for Group Anomaly Detection",
      "author" : [ "Krikamol Muandet", "Bernhard Schölkopf" ],
      "venue" : "In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling",
      "author" : [ "M Dudik", "S J Phillips", "R E Schapire" ],
      "venue" : "Journal of Machine Learning Research, 8:1217–1260",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A Kernel Test for Three-Variable Interactions",
      "author" : [ "Dino Sejdinovic", "Arthur Gretton", "Wicher Bergsma" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Latent Support Measure Machines for Bag-of- Words Data Classification",
      "author" : [ "Yuya Yoshikawa", "Tomoharu Iwata", "Hiroshi Sawada" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Non-linear Regression for Bag-of-Words Data via Gaussian Process Latent Variable Set Model",
      "author" : [ "Yuya Yoshikawa", "Tomoharu Iwata", "Hiroshi Sawada" ],
      "venue" : "In Proceedings of the 29th AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Hilbert Space Embeddings and Metrics on Probability Measures",
      "author" : [ "Bharath K. Sriperumbudur", "Arthur Gretton", "Kenji Fukumizu", "Bernhard Schölkopf", "Gert R.G. Lanckriet" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "On the Limited Memory BFGS Method for Large Scale Optimization",
      "author" : [ "Dong C. Liu", "Jorge Nocedal" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The discovery of matched instances in different domains is an important task, which appears in natural language processing, information retrieval and data mining tasks such as finding the alignment of cross-lingual sentences [1], attaching tags to images [2] or text documents [3], and matching user identifications in different databases [4].",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "The discovery of matched instances in different domains is an important task, which appears in natural language processing, information retrieval and data mining tasks such as finding the alignment of cross-lingual sentences [1], attaching tags to images [2] or text documents [3], and matching user identifications in different databases [4].",
      "startOffset" : 255,
      "endOffset" : 258
    }, {
      "referenceID" : 2,
      "context" : "The discovery of matched instances in different domains is an important task, which appears in natural language processing, information retrieval and data mining tasks such as finding the alignment of cross-lingual sentences [1], attaching tags to images [2] or text documents [3], and matching user identifications in different databases [4].",
      "startOffset" : 277,
      "endOffset" : 280
    }, {
      "referenceID" : 3,
      "context" : "The discovery of matched instances in different domains is an important task, which appears in natural language processing, information retrieval and data mining tasks such as finding the alignment of cross-lingual sentences [1], attaching tags to images [2] or text documents [3], and matching user identifications in different databases [4].",
      "startOffset" : 339,
      "endOffset" : 342
    }, {
      "referenceID" : 4,
      "context" : "One such method is canonical correspondence analysis (CCA) [5], which maps instances into a latent space by linear projection to maximize the correlation between paired instances in the latent space.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "To find non-linear correspondence, kernel CCA [6] can be used.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "It has been reported that kernel CCA performs well as regards document/sentence alignment between different languages [7, 8], when searching for images from text queries [9] and when matching 2D-3D face images [10].",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "It has been reported that kernel CCA performs well as regards document/sentence alignment between different languages [7, 8], when searching for images from text queries [9] and when matching 2D-3D face images [10].",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "It has been reported that kernel CCA performs well as regards document/sentence alignment between different languages [7, 8], when searching for images from text queries [9] and when matching 2D-3D face images [10].",
      "startOffset" : 170,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "It has been reported that kernel CCA performs well as regards document/sentence alignment between different languages [7, 8], when searching for images from text queries [9] and when matching 2D-3D face images [10].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "When we want to match cross-domain instances represented by bag-of-words such as documents, bilingual topic models [1, 11] can also be used.",
      "startOffset" : 115,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "When we want to match cross-domain instances represented by bag-of-words such as documents, bilingual topic models [1, 11] can also be used.",
      "startOffset" : 115,
      "endOffset" : 122
    }, {
      "referenceID" : 11,
      "context" : "To model cross-domain data, deep learning and neural network approaches have been recently proposed [12, 13].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : "To model cross-domain data, deep learning and neural network approaches have been recently proposed [12, 13].",
      "startOffset" : 100,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "A key technique of the proposed method is the kernel embeddings of distributions [14], which can represent a distribution as an element in an RKHS, while preserving the moment information of the distribution such as the mean, covariance and higher-order moments without density estimation.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "The kernel embeddings of distributions have been successfully used for a statistical test of the independence of two sample sets [15], discriminative learning on distribution data [16], anomaly detection for group data [17], density estimation [18] and a three variable interaction test [19].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "The kernel embeddings of distributions have been successfully used for a statistical test of the independence of two sample sets [15], discriminative learning on distribution data [16], anomaly detection for group data [17], density estimation [18] and a three variable interaction test [19].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 16,
      "context" : "The kernel embeddings of distributions have been successfully used for a statistical test of the independence of two sample sets [15], discriminative learning on distribution data [16], anomaly detection for group data [17], density estimation [18] and a three variable interaction test [19].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 17,
      "context" : "The kernel embeddings of distributions have been successfully used for a statistical test of the independence of two sample sets [15], discriminative learning on distribution data [16], anomaly detection for group data [17], density estimation [18] and a three variable interaction test [19].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 18,
      "context" : "The kernel embeddings of distributions have been successfully used for a statistical test of the independence of two sample sets [15], discriminative learning on distribution data [16], anomaly detection for group data [17], density estimation [18] and a three variable interaction test [19].",
      "startOffset" : 287,
      "endOffset" : 291
    }, {
      "referenceID" : 19,
      "context" : "The proposed method is inspired by the use of the kernel embeddings of distributions in bag-ofwords data classification [20] and regression [21].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 20,
      "context" : "The proposed method is inspired by the use of the kernel embeddings of distributions in bag-ofwords data classification [20] and regression [21].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "which can be approximated with an error rate of ||m(X)−m(P)||Hk = Op(n 1 2 ) [14].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "of independence of two distributions [15].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 22,
      "context" : "Then, we employed the L-BFGS method [23] with gradients given by Eqs.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "With BLDA, we first learned the same model as [1, 11] and found matching between instances in the test data by obtaining the topic distributions of these instances from the learned model.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "With BLDA, we first learned the same model as [1, 11] and found matching between instances in the test data by obtaining the topic distributions of these instances from the learned model.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "2 Matching between Documents and Tags, and between Images and Tags We performed experiments matching documents and tailgates, and matching images and tailgates with the datasets used in [3].",
      "startOffset" : 186,
      "endOffset" : 189
    } ],
    "year" : 2015,
    "abstractText" : "We propose a kernel-based method for finding matching between instances across different domains, such as multilingual documents and images with annotations. Each instance is assumed to be represented as a multiset of features, e.g., a bag-ofwords representation for documents. The major difficulty in finding cross-domain relationships is that the similarity between instances in different domains cannot be directly measured. To overcome this difficulty, the proposed method embeds all the features of different domains in a shared latent space, and regards each instance as a distribution of its own features in the shared latent space. To represent the distributions efficiently and nonparametrically, we employ the framework of the kernel embeddings of distributions. The embedding is estimated so as to minimize the difference between distributions of paired instances while keeping unpaired instances apart. In our experiments, we show that the proposed method can achieve high performance on finding correspondence between multi-lingual Wikipedia articles, between documents and tags, and between images and tags.",
    "creator" : null
  }
}