{
  "name" : "f2fc990265c712c49d51a18a32b39f0c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "HONOR: Hybrid Optimization for NOn-convex Regularized problems",
    "authors" : [ "Pinghua Gong", "Jieping Ye" ],
    "emails" : [ "gongp@umich.edu", "jpye@umich.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20]. However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11]. Popular non-convex sparsity-inducing penalties include Smoothly Clipped Absolute Deviation (SCAD) [10], Log-Sum Penalty (LSP) [6] and Minimax Concave Penalty (MCP) [23]. Although non-convex sparse learning reveals its advantage over the convex one, it remains a challenge to develop an efficient algorithm to solve the non-convex optimization problem especially for large-scale data.\nDC programming [21] is a popular approach to solve non-convex problems whose objective functions can be expressed as the difference of two convex functions. However, a potentially non-trivial convex subproblem is required to solve at each iteration, which is not practical for large-scale problems. SparseNet [16] can solve a least squares problem with a non-convex penalty. At each step, SparseNet solves a univariate subproblem with a non-convex penalty which admits a closed-form solution. However, to establish the convergence analysis, the parameter of the non-convex penalty is required to be restricted to some interval such that the univariate subproblem (with a non-convex penalty) is convex. Moreover, it is quite challenging to extend SparseNet to non-convex problems with a non-least-squares loss, as the univariate subproblem generally does not admit a closed-form solution. The GIST algorithm [14] can solve a class of non-convex regularized problems by iteratively solving a possibly non-convex proximal operator problem, which in turn admits a closed-form solution. However, GIST does not well exploit the second-order information. The DC-PN algorithm\n[18] can incorporate the second-order information to solve non-convex regularized problems but it requires to solve a non-trivial regularized quadratic subproblem at each iteration.\nIn this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR), which incorporates the second-order information to speed up the convergence. HONOR adopts a hybrid optimization scheme which chooses either a Quasi-Newton (QN) step or a Gradient Descent (GD) step per iteration mainly depending on whether an iterate has very small components. If an iterate does not have any small component, the QN-step is adopted, which uses L-BFGS to exploit the second-order information. The key advantage of the QN-step is that it does not need to solve a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. If an iterate has small components, we switch to a GD-step. Our detailed theoretical analysis sheds light on the effect of such a hybrid scheme on the convergence of the algorithm. Specifically, we provide a rigorous convergence analysis for HONOR, which shows that every limit point of the sequence generated by HONOR is a Clarke critical point. It is worth noting that the convergence analysis for a non-convex problem is typically much more challenging than the convex one, because many important properties for a convex problem may not hold for non-convex problems. Empirical studies are also conducted on large-scale data sets which include up to millions of samples and features; results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms."
    }, {
      "heading" : "2 Non-convex Sparse Learning",
      "text" : "We focus on the following non-convex regularized optimization problem:\nmin x∈Rn {f(x) = l(x) + r(x)} , (1)\nwhere we make the following assumptions throughout the paper:\n(A1) l(x) is coercive, continuously differentiable and ∇l(x) is Lipschitz continuous with constant L. Moreover, l(x) > −∞ for all x ∈ Rn. (A2) r(x) = ∑n\ni=1 ρ(|xi|), where ρ(t) is non-decreasing, continuously differentiable and concave with respect to t in [0,∞); ρ(0) = 0 and ρ′(0) 6= 0 with ρ′(t) = ∂ρ(t)/∂t denoting the derivative of ρ(t) at the point t.\nRemark 1 Assumption (A1) allows l(x) to be non-convex. Assumption (A2) implies that ρ(|xi|) is generally non-convex with respect to xi and the only convex case is ρ(|xi|) = λ|xi| with λ > 0. Moreover, ρ(|xi|) is continuously differentiable with respect to xi in (−∞, 0) ∪ (0,∞) and nondifferentiable at xi = 0. In particular, ∂ρ(|xi|)/∂xi = σ(xi)ρ\n′(|xi|) for any xi 6= 0, where σ(xi) = 1, if xi > 0; σ(xi) = −1, if xi < 0 and σ(xi) = 0, otherwise. In addition, ρ′(0) > 0 must hold (Otherwise ρ′(0) < 0 implies ρ(t) ≤ ρ(0) + ρ′(0)t < 0 for any t > 0, contradicting the fact that ρ(t) is non-decreasing). It is also easy to show that, under the assumptions above, both l(x) and r(x) are locally Lipschitz continuous. Thus, the Clarke subdifferential [7] is well-defined.\nThe commonly used least squares loss and the logistic regression loss satisfy the assumption (A1); we can add a small term δ‖x‖2 to make them coercive. The following popular non-convex regularizers satisfy the assumption (A2), where λ > 0 and θ > 0 except that θ > 2 for SCAD.\n• LSP: ρ(|xi|) = λ log(1 + |xi|/θ).\n• SCAD: ρ(|xi|) =\n\n\n\nλ|xi|, if |xi| ≤ λ, −x2\ni +2θλ|xi|−λ\n2\n2(θ−1) , if λ < |xi| ≤ θλ,\n(θ + 1)λ2/2, if |xi| > θλ.\n• MCP: ρ(|xi|) =\n{\nλ|xi| − x2i /(2θ), if |xi| ≤ θλ, θλ2/2, if |xi| > θλ.\nDue to the non-convexity and non-differentiability of problem (1), the traditional subdifferential concept for the convex optimization is not applicable here. Thus, we use the Clarke subdifferential [7] to characterize the optimality of problem (1). We say x̄ is a Clarke critical point of problem (1), if 0 ∈ ∂of(x̄), where ∂of(x̄) is the Clarke subdifferential of f(x) at x = x̄. To be self-contained,\nwe briefly review the Clarke subdifferential: for a locally Lipschitz continuous function f(x), the Clarke generalized directional derivative of f(x) at x = x̄ along the direction d is defined as\nfo(x̄;d) = lim sup x→x̄,α↓0\nf(x+ αd)− f(x)\nα .\nThen, the Clarke subdifferential of f(x) at x = x̄ is defined as\n∂of(x̄) = {δ ∈ Rn : fo(x̄;d) ≥ dTδ, ∀d ∈ Rn}.\nInterested readers may refer to Proposition 4 in the Supplement A for more properties about the Clarke Subdifferential. We want to emphasize that some basic properties of the subdifferential of a convex function may not hold for the Clarke Subdifferential of a non-convex function."
    }, {
      "heading" : "3 Proposed Optimization Algorithm: HONOR",
      "text" : "Since each decomposable component function of the regularizer is only non-differentiable at the origin, the objective function is differentiable, if the segment between any two consecutive iterates do not cross any axis. This motivates us to design an algorithm which can keep the current iterate in the same orthant of the previous iterate. Before we present the detailed HONOR algorithm, we introduce two functions as follows:\nDefine a function π : Rn 7→ Rn with the i-th entry being:\nπi(xi; yi) =\n{\nxi, if σ(xi) = σ(yi), 0, otherwise,\nwhere y ∈ Rn (yi is the i-th entry of y) is the parameter of the function π; σ(·) is the sign function defined as follows: σ(xi) = 1, if xi > 0; σ(xi) = −1, if xi < 0 and σ(xi) = 0, otherwise.\nDefine the pseudo-gradient ⋄f(x) whose i-th entry is given by:\n⋄if(x) =\n\n   \n   \n∇il(x) + ρ′(|xi|), if xi > 0, ∇il(x)− ρ′(|xi|), if xi < 0, ∇il(x) + ρ ′(0), if xi = 0, ∇il(x) + ρ ′(0) < 0, ∇il(x)− ρ′(0), if xi = 0, ∇il(x)− ρ′(0) > 0, 0, otherwise,\nwhere ρ′(t) is the derivative of ρ(t) at the point t.\nRemark 2 If r(x) is convex, ⋄f(x) is the minimum-norm sub-gradient of f(x) at x. Thus,−⋄f(x) is a descent direction. However, ⋄f(x) is not even a sub-gradient of f(x) if r(x) is non-convex. This indicates that some obvious concepts and properties for a convex problem may not hold in the non-convex case. Thus, it is significantly more challenging to develop and analyze algorithms for a non-convex problem.\nInterestingly, we can still show that vk = − ⋄ f(xk) is a descent direction at the point xk (refer to Supplement D and replace pk = π(dk;vk) with vk). To utilize the second-order information, we may perform the optimization along the direction dk = Hkvk , where Hk is a positive definite matrix containing the second-order information. However, dk is not necessarily a descent direction. To address this issue, we use the following slightly modified direction pk:\npk = π(dk;vk).\nWe can show that pk is a descent direction (proof is provided in Supplement D). Thus, we can perform the optimization along the direction pk. Recall that we need to keep the current iterate in the same orthant of the previous iterate. So the following iterative scheme is proposed:\nxk(α) = π(xk + αpk; ξk), (2)\nwhere\nξki =\n{\nσ(xki ), if x k i 6= 0, σ(vki ), if x k i = 0,\n(3)\nand α is a step size chosen by the following line search procedure: for constants α0 > 0, β, γ ∈ (0, 1) and m = 0, 1, · · · , find the smallest integer m with α = α0β\nm such that the following inequality holds:\nf(xk(α)) ≤ f(xk)− γα(vk)Tdk. (4)\nHowever, only using the above iterative scheme may not guarantee the convergence. The main challenge is: if there exists a subsequence K such that {xki }K converges to zero, it is possible that for sufficiently large k ∈ K, |xki | is arbitrarily small but never equal to zero (refer to the proof of Theorem 1 for more details). To address this issue, we propose a hybrid optimization scheme. Specifically, for a small constant ǫ > 0, if Ik = {i ∈ {1, · · · , n} : 0 < |xki | ≤ min(‖v k‖, ǫ), xki v k i < 0} is not empty, we switch the iteration to the following gradient descent step (GD-step):\nxk(α) = argmin x\n{\n∇l(xk)T (x− xk) + 1\n2α ‖x− xk‖2 + r(x)\n}\n,\nwhere α is a step size chosen by the following line search procedure: for constants α0 > 0, β, γ ∈ (0, 1) and m = 0, 1, · · · , find the smallest integer m with α = α0βm such that the following inequality holds:\nf(xk(α)) ≤ f(xk)− γ\n2α ‖xk(α)− xk‖2. (5)\nThe detailed steps of the algorithm are presented in Algorithm 1.\nRemark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1, 3, 4, 17, 13]. However, HONOR is significantly different from them: (1) The OWL-QN-type algorithms can only handle ℓ1regularized convex problems while HONOR is applicable to a class of non-convex problems beyond ℓ1-regularized ones. (2) The convergence analyses of the OWL-QN-type algorithms heavily rely on the convexity of the ℓ1-regularized problem. In contrast, the convergence analysis for HONOR is applicable to non-convex cases beyond the convex ones, which is a non-trivial extension.\nAlgorithm 1: HONOR: Hybrid Optimization for NOn-convex Regularized problems\n1 Initialize x0, H0 and choose β, γ ∈ (0, 1), ǫ > 0, α0 > 0; 2 for k = 0 to maxiter do 3 Compute vk ← − ⋄ f(xk) and Ik = {i ∈ {1, · · · , n} : 0 < |xki | ≤ ǫ k, xki v k i < 0}, where ǫk = min(‖vk‖, ǫ); 4 Initialize α← α0; 5 if Ik = ∅ then 6 (QN-step) 7 Compute dk ← Hkvk with a positive definite matrix Hk using L-BFGS; 8 Alignment: pk ← π(dk;vk); 9 while Eq. (4) is not satisfied do\n10 α← αβ; xk(α)← π(xk + αpk; ξk); 11 end\n12 else 13 (GD-step) 14 while Eq. (5) is not satisfied do 15 α← αβ; 16 xk(α)← argmin x { ∇l(xk)T (x− xk) + 12α‖x− x k‖2 + r(x) } ; 17 end\n18 end 19 xk+1 ← xk(α); 20 if some stopping criterion is satisfied then 21 stop and return xk+1; 22 end\n23 end"
    }, {
      "heading" : "4 Convergence Analysis",
      "text" : "We first present a few basic propositions and then provide the convergence theorem based on the propositions; all proofs of the presented propositions are carefully handled due to the lack of convexity. First of all, an optimality condition is presented (proof is provided in Supplement B), which will be directly used in the proof of Theorem 1.\nProposition 1 Let x̄ = limk∈K,k→∞ x k, vk = − ⋄ f(xk) and v̄ = − ⋄ f(x̄), where K is a subsequence of {1, 2, · · · , k, k + 1, · · · }. If lim infk∈K,k→∞ |vki | = 0 for all i ∈ {1, · · · , n}, then v̄ = 0 and x̄ is a Clarke critical point of problem (1).\nWe subsequently show that we have a Lipschitz-continuous-like inequality in the following proposition (proof is provided in Supplement C), which is crucial to prove the final convergence theorem.\nProposition 2 Let vk = −⋄f(xk), xk(α) = π(xk+αpk; ξk) and qkα = 1 α (π(xk+αpk; ξk)−xk) with α > 0. Then under assumptions (A1) and (A2), we have\n(i) ∇l(xk)T (xk(α) − xk) + r(xk(α)) − r(xk) ≤ −(vk)T (xk(α)− xk), (6)\n(ii) f(xk(α)) ≤ f(xk)− α(vk)Tqkα + α2L\n2 ‖qkα‖ 2. (7)\nWe next show that both line search criteria in the QN-step [Eq. (4)] and the GD-step [Eq. (5)] at any iteration k is satisfied in a finite number of trials (proof is provided in Supplement D).\nProposition 3 At any iteration k of the HONOR algorithm, if xk is not a Clarke critical point of problem (1), then (a) for the QN-step, there exists an α ∈ [ᾱk, α0] with 0 < ᾱk ≤ α0 such that the line search criterion in Eq. (4) is satisfied; (b) for the GD-step, the line search criterion in Eq. (5) is satisfied whenever α ≥ βmin(α0, (1 − γ)/L). That is, both line search criteria at any iteration k are satisfied in a finite number of trials.\nWe are now ready to provide the convergence proof for the HONOR algorithm:\nTheorem 1 The sequence {xk} generated by the HONOR algorithm has at least a limit point and every limit point of {xk} is a Clarke critical point of problem (1).\nProof It follows from Proposition 3 that both line search criteria in the QN-step [Eq. (4)] and the GD-step [Eq. (5)] at each iteration can be satisfied in a finite number of trials. Let αk be the accepted step size at iteration k. Then we have\nf(xk)− f(xk+1) ≥ γαk(vk)Tdk = γαk(vk)THkvk (QN-step), (8)\nor f(xk)− f(xk+1) ≥ γ\n2αk ‖xk+1 − xk‖2 ≥\nγ\n2α0 ‖xk+1 − xk‖2 (GD-step). (9)\nRecall that Hk is positive definite and γ > 0, αk > 0, which together with Eqs.(8), (9) imply that {f(xk)} is monotonically decreasing. Thus, {f(xk)} converges to a finite value f̄ , since f is bounded from below (note that l(x) > −∞ and r(x) ≥ 0 for all x ∈ Rn). Due to the boundedness of {xk} (see Proposition 7 in Supplement F), the sequence {xk} generated by the HONOR algorithm has at least a limit point x̄. Since f is continuous, there exists a subsequence K of {1, 2 · · · , k, k + 1, · · · } such that\nlim k∈K,k→∞\nxk = x̄, (10)\nlim k→∞ f(xk) = lim k∈K,k→∞ f(xk) = f̄ = f(x̄). (11)\nIn the following, we prove the theorem by contradiction. Assume that x̄ is not a Clarke critical point of problem (1). Then by Proposition 1, there exists at least one i ∈ {1, · · · , n} such that\nlim inf k∈K,k→∞\n|vki | > 0. (12)\nWe next consider the following two cases:\n(a) There exist a subsequence K̃ of K and an integer k̃ > 0 such that for all k ∈ K̃, k ≥ k̃, the GD-step is adopted. Then for all k ∈ K̃, k ≥ k̃, we have\nxk+1 =argmin x\n{\n∇l(xk)T (x− xk) + 1\n2αk ‖x− xk‖2 + r(x)\n}\n.\nThus, by the optimality condition of the above problem and properties of the Clarke subdifferential (Proposition 4 in Supplement A), we have\n0 ∈ ∇l(xk) + 1\nαk (xk+1 − xk) + ∂or(xk+1). (13)\nTaking limits with k ∈ K̃ for Eq. (9) and considering Eqs. (10), (11), we have\nlim k∈K̃,k→∞ ‖xk+1 − xk‖2 ≤ 0⇒ lim k∈K̃,k→∞ xk = lim k∈K̃,k→∞ xk+1 = x̄. (14)\nTaking limits with k ∈ K̃ for Eq. (13) and considering Eq. (14), αk ≥ βmin(α0, (1 − γ)/L) [Proposition 3] and ∂or(·) is upper-semicontinuous (upper-hemicontinuous) [8] (see Proposition 4 in the Supplement A), we have\n0 ∈ ∇l(x̄) + ∂or(x̄) = ∂of(x̄),\nwhich contradicts the assumption that x̄ is not a Clarke critical point of problem (1).\n(b) There exists an integer k̂ > 0 such that for all k ∈ K, k ≥ k̂, the QN-step is adopted. According to Remark 7 (in Supplement F), we know that the smallest eigenvalue of Hk is uniformly bounded from below by a positive constant, which together with Eq. (12) implies\nlim inf k∈K,k→∞\n(vk)THkvk > 0. (15)\nTaking limits with k ∈ K for Eq. (8), we have\nlim k∈K,k→∞\nγαk(vk)THkvk ≤ 0,\nwhich together with γ ∈ (0, 1), αk ∈ (0, α0] and Eq. (15) implies that\nlim k∈K,k→∞\nαk = 0. (16)\nEq. (12) implies that there exist an integer ǩ > 0 and a constant ǭ > 0 such that ǫk = min(‖vk‖, ǫ) ≥ ǭ for all k ∈ K, k ≥ ǩ. Notice that for all k ∈ K, k ≥ k̂, the QN-step is adopted. Thus, we obtain that Ik = {i ∈ {1, · · · , n} : 0 < |xki | ≤ ǫ k, xki v k i < 0} = ∅ for all k ∈ K, k ≥ k̂. We also notice that, if |xki | ≥ ǭ, then there exists a constant ᾱi > 0 such that xki (α) = πi(x k i + αp k i ; ξ k i ) = x k i + αp k i for all α ∈ (0, ᾱi], as {p k i } is bounded (Proposition 8 in Supplement F). Therefore, we conclude that, for all k ∈ K, k ≥ k̄ = max(ǩ, k̂) and for all i ∈ {1, · · · , n}, at least one of the following three cases must happen:\nxki = 0⇒ x k i (α) = πi(x k i + αp k i ; ξ k i ) = x k i + αp k i , ∀α > 0,\nor |xki | > ǫ k ≥ ǭ⇒ xki (α) = πi(x k i + αp k i ; ξ k i ) = x k i + αp k i , ∀α ∈ (0, ᾱi], or xki v k i ≥ 0⇒ x k i p k i ≥ 0⇒ x k i (α) = πi(x k i + αp k i ; ξ k i ) = x k i + αp k i , ∀α > 0.\nIt follows that there exists a constant ᾱ > 0 such that\nqkα = 1\nα (xk(α) − xk) = pk, ∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ]. (17)\nThus, considering |pki | = |πi(d k i ; v k i )| ≤ |d k i | and v k i p k i ≥ v k i d k i for all i ∈ {1, · · · , n}, we have\n‖qkα‖ 2 = ‖pk‖2 ≤ ‖dk‖2 = (vk)T (Hk)2vk, ∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ], (18) (vk)Tqkα = (v k)Tpk ≥ (vk)Tdk = (vk)THkvk, ∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ]. (19)\nAccording to Proposition 8 (in Supplement F), we know that the largest eigenvalue of Hk is uniformly bounded from above by some positive constant M . Thus, we have\n(vk)T (Hk)2vk ≤ 2\nαL (vk)THkvk −\n(\n2\nαL −M\n)\n(vk)THkvk, ∀k,\nwhich together with Eqs. (18), (19) and dk = Hkvk implies\n‖qkα‖ 2 ≤\n2\nαL (vk)Tqkα −\n(\n2\nαL −M\n)\n(vk)Tdk, ∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ]. (20)\nConsidering Eqs. (7), (20), we have\nf(xk(α)) ≤ f(xk)− α\n(\n1− αLM\n2\n)\n(vk)Tdk, ∀k ∈ K, k ≥ k̄, α ∈ (0, ᾱ],\nwhich together with (vk)Tdk = (vk)THkvk ≥ 0 implies that the line search criterion in the QN-step [Eq. (4)] is satisfied if\n1− αLM\n2 ≥ γ , 0 < α ≤ α0 and 0 < α ≤ ᾱ, ∀k ∈ K, k ≥ k̄.\nConsidering the backtracking form of the line search in QN-step [Eq. (4)], we conclude that the line search criterion in the QN-step [Eq. (4)] is satisfied whenever\nαk ≥ βmin(min(ᾱ, α0), 2(1− γ)/(LM)) > 0, ∀k ∈ K, k ≥ k̄.\nThis leads to a contradiction with Eq. (16).\nBy (a) and (b), we conclude that x̄ = limk∈K,k→∞ x k is a Clarke critical point of problem (1)."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we evaluate the efficiency of HONOR on solving the non-convex regularized logistic regression problem1 by setting l(x) = 1/N ∑N\ni=1 log(1 + exp(−yia T i x)), where ai ∈\nR n is the i-th sample associated with the label yi ∈ {1,−1}. Three non-convex regularizers (LSP, MCP and SCAD) are included in experiments, where the parameters are set as λ = 1/N and θ = 10−2λ (θ is set as 2 + 10−2λ for SCAD as it requires θ > 2). We compare HONOR with the non-convex solver2 GIST [14] on three large-scale, high-dimensional and sparse data sets which are summarized in Table 1. All data sets can be downloaded from http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/.\nAll algorithms are implemented in Matlab 2015a under a Linux operating system and executed on an Intel Core i7-4790 CPU (@3.6GHz) with 32GB memory. We choose the starting points\nx0 for the compared algorithms using the same random vector whose entries are i.i.d. sampled from the standard Gaussian distribution. We terminate the compared algorithms if the relative change of two consecutive objective function values is less than 10−5 or the number of iterations exceeds 1000 (HONOR) or 10000 (GIST). For HONOR, we set γ = 10−5, β = 0.5, α0 = 1 and the number of unrolling steps in L-BFGS as m = 10. For GIST, we use the non-monotone line search in experiments as it usually performs better than its monotone counterpart. To show how the convergence behavior of HONOR varies over the parameter ǫ, we use three values: ǫ = 10−10, 10−6, 10−2.\nWe report the objective function value (in log-scale) vs. CPU time (in seconds) plots in Figure 1. We can observe from Figure 1 that: (1) If ǫ is set to a small value, the QN-step is adopted at almost all steps in HONOR and HONOR converges significantly faster than GIST for all three non-convex\n1We do not include the term δ‖x‖2 in the objective and find that the proposed algorithm still works well. 2We do not involve SparseNet, DC programming and DC-PN in comparison, because (1) adapting SparseNet to the logistic regression problem is challenging; (2) DC programming is shown to be much inferior to GIST; (3) The objective function value of DC-PN is larger than GIST in most cases [18].\nregularizers on all three data sets. This shows that using the second-order information greatly speeds up the convergence. (2) When ǫ increases, the ratio of the GD-step adopted in HONOR increases. Meanwhile, the convergence performance of HONOR generally degrades. In some cases, setting a slightly larger ǫ and adopting a small number of GD steps even sligtly boosts the convergence performance of HONOR (the green curves in the first row). But setting ǫ to a very small value is always safe to guarantee the fast convergence of HONOR. (3) When ǫ is large enough, the GD steps dominate all iterations of HONOR and HONOR converge much slower. In this case, HONOR converges even slower than GIST. The reason is that, at each iteration of HONOR, extra computational cost is required in addition to the basic computation in the GD-step. Moreover, the non-monotone line search is used in GIST while the monotone line search is adopted in the GD-step. (4) In some cases (the first row), GIST is trapped in a local solution which has a much larger objective function value than HONOR with a small ǫ. This implies that HONOR may have a potential of escaping from high error plateau which often exists in high dimensional non-convex problems. These results show the great potential of HONOR for solving large-scale non-convex sparse learning problems."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we propose an efficient optimization algorithm called HONOR for solving non-convex regularized sparse learning problems. HONOR incorporates the second-order information to speed up the convergence in practice and uses a carefully designed hybrid optimization scheme to guarantee the convergence in theory. Experiments are conducted on large-scale data sets and results show that HONOR converges significantly faster than state-of-the-art algorithms. In our future work, we plan to develop parallel/distributed variants of HONOR to tackle much larger data sets."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported in part by research grants from NIH (R01 LM010730, U54 EB020403) and NSF (IIS- 0953662, III-1539991, III-1539722)."
    } ],
    "references" : [ {
      "title" : "Scalable training of  l1-regularized log-linear models",
      "author" : [ "G. Andrew", "J. Gao" ],
      "venue" : "ICML, pages 33–40",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A new TwIST: two-step iterative shrinkage/thresholding algorithms for image restoration",
      "author" : [ "J. Bioucas-Dias", "M. Figueiredo" ],
      "venue" : "IEEE Transactions on Image Processing, 16(12):2992–3004",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A family of second-order methods for convex l1-regularized optimization",
      "author" : [ "R.H. Byrd", "G.M. Chin", "J. Nocedal", "F. Oztoprak" ],
      "venue" : "Technical report, Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sample size selection in optimization methods for machine learning",
      "author" : [ "R.H. Byrd", "G.M. Chin", "J. Nocedal", "Y. Wu" ],
      "venue" : "Mathematical Programming, 134(1):127–155",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A limited memory algorithm for bound constrained optimization",
      "author" : [ "R.H. Byrd", "P. Lu", "J. Nocedal", "C. Zhu" ],
      "venue" : "SIAM Journal on Scientific Computing, 16(5):1190–1208",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Enhancing sparsity by reweighted  l1 minimization",
      "author" : [ "E. Candes", "M. Wakin", "S. Boyd" ],
      "venue" : "Journal of Fourier Analysis and Applications, 14(5):877–905",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Optimization and Nonsmooth Analysis",
      "author" : [ "F. Clarke" ],
      "venue" : "John Wiley&Sons, New York",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Generalized derivatives and nonsmooth optimization",
      "author" : [ "J. Dutta" ],
      "venue" : "a finite dimensional tour. Top, 13(2):185– 279",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Sparse machine learning methods for understanding large text corpora",
      "author" : [ "L. El Ghaoui", "G. Li", "V. Duong", "V. Pham", "A. Srivastava", "K. Bhaduri" ],
      "venue" : "CIDU, pages 159–173",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Variable selection via nonconcave penalized likelihood and its oracle properties",
      "author" : [ "J. Fan", "R. Li" ],
      "venue" : "Journal of the American Statistical Association, 96(456):1348–1360",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Strong oracle optimality of folded concave penalized estimation",
      "author" : [ "J. Fan", "L. Xue", "H. Zou" ],
      "venue" : "Annals of Statistics, 42(3):819",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Recovering sparse signals with a certain family of nonconvex penalties and dc programming",
      "author" : [ "G. Gasso", "A. Rakotomamonjy", "S. Canu" ],
      "venue" : "IEEE Transactions on Signal Processing, 57(12):4686–4698",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A modified orthant-wise limited memory quasi-newton method with convergence analysis",
      "author" : [ "P. Gong", "J. Ye" ],
      "venue" : "ICML",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems",
      "author" : [ "P. Gong", "C. Zhang", "Z. Lu", "J. Huang", "J. Ye" ],
      "venue" : "ICML, volume 28, pages 37–45",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Numerical Optimization",
      "author" : [ "N. Jorge", "J. Stephen" ],
      "venue" : "Springer",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Sparsenet: Coordinate descent with nonconvex penalties",
      "author" : [ "R. Mazumder", "J. Friedman", "T. Hastie" ],
      "venue" : "Journal of the American Statistical Association, 106(495)",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Newton-like methods for sparse inverse covariance estimation",
      "author" : [ "P. Olsen", "F. Oztoprak", "J. Nocedal", "S. Rennie" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 764–772",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Dc proximal newton for non-convex optimization problems",
      "author" : [ "A. Rakotomamonjy", "R. Flamary", "G. Gasso" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "A simple and efficient algorithm for gene selection using sparse logistic regression",
      "author" : [ "S. Shevade", "S. Keerthi" ],
      "venue" : "Bioinformatics, 19(17):2246",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Sparse learning via iterative minimization with application to mimo radar imaging",
      "author" : [ "X. Tan", "W. Roberts", "J. Li", "P. Stoica" ],
      "venue" : "IEEE Transactions on Signal Processing, 59(3):1088–1101",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The dc (difference of convex functions) programming and dca revisited with dc models of real world nonconvex optimization problems",
      "author" : [ "P. Tao", "L. An" ],
      "venue" : "Annals of Operations Research, 133(1-4):23–46",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Robust face recognition via sparse representation",
      "author" : [ "J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210–227",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Nearly unbiased variable selection under minimax concave penalty",
      "author" : [ "C. Zhang" ],
      "venue" : "The Annals of Statistics, 38(2):894–942",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A general theory of concave regularization for high-dimensional sparse estimation problems",
      "author" : [ "C. Zhang", "T. Zhang" ],
      "venue" : "Statistical Science, 27(4):576–593",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Analysis of multi-stage convex relaxation for sparse regularization",
      "author" : [ "T. Zhang" ],
      "venue" : "JMLR, 11:1081–1107",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multi-stage convex relaxation for feature selection",
      "author" : [ "T. Zhang" ],
      "venue" : "Bernoulli",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "One-step sparse estimates in nonconcave penalized likelihood models",
      "author" : [ "H. Zou", "R. Li" ],
      "venue" : "Annals of Statistics, 36(4):1509",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 21,
      "context" : "Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20].",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 1,
      "context" : "Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20].",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 8,
      "context" : "Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 19,
      "context" : "Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 26,
      "context" : "However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11].",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11].",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 22,
      "context" : "However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11].",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 24,
      "context" : "However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11].",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 15,
      "context" : "However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11].",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 25,
      "context" : "However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11].",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 23,
      "context" : "However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11].",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 10,
      "context" : "However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11].",
      "startOffset" : 156,
      "endOffset" : 188
    }, {
      "referenceID" : 9,
      "context" : "Popular non-convex sparsity-inducing penalties include Smoothly Clipped Absolute Deviation (SCAD) [10], Log-Sum Penalty (LSP) [6] and Minimax Concave Penalty (MCP) [23].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "Popular non-convex sparsity-inducing penalties include Smoothly Clipped Absolute Deviation (SCAD) [10], Log-Sum Penalty (LSP) [6] and Minimax Concave Penalty (MCP) [23].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 22,
      "context" : "Popular non-convex sparsity-inducing penalties include Smoothly Clipped Absolute Deviation (SCAD) [10], Log-Sum Penalty (LSP) [6] and Minimax Concave Penalty (MCP) [23].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : "DC programming [21] is a popular approach to solve non-convex problems whose objective functions can be expressed as the difference of two convex functions.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 15,
      "context" : "SparseNet [16] can solve a least squares problem with a non-convex penalty.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 13,
      "context" : "The GIST algorithm [14] can solve a class of non-convex regularized problems by iteratively solving a possibly non-convex proximal operator problem, which in turn admits a closed-form solution.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "[18] can incorporate the second-order information to solve non-convex regularized problems but it requires to solve a non-trivial regularized quadratic subproblem at each iteration.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "Thus, the Clarke subdifferential [7] is well-defined.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Thus, we use the Clarke subdifferential [7] to characterize the optimality of problem (1).",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Remark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1, 3, 4, 17, 13].",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Remark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1, 3, 4, 17, 13].",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : "Remark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1, 3, 4, 17, 13].",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "Remark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1, 3, 4, 17, 13].",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "Remark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1, 3, 4, 17, 13].",
      "startOffset" : 61,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "(14), α ≥ βmin(α0, (1 − γ)/L) [Proposition 3] and ∂r(·) is upper-semicontinuous (upper-hemicontinuous) [8] (see Proposition 4 in the Supplement A), we have 0 ∈ ∇l(x̄) + ∂r(x̄) = ∂f(x̄), which contradicts the assumption that x̄ is not a Clarke critical point of problem (1).",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "We compare HONOR with the non-convex solver(2) GIST [14] on three large-scale, high-dimensional and sparse data sets which are summarized in Table 1.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "(2)We do not involve SparseNet, DC programming and DC-PN in comparison, because (1) adapting SparseNet to the logistic regression problem is challenging; (2) DC programming is shown to be much inferior to GIST; (3) The objective function value of DC-PN is larger than GIST in most cases [18].",
      "startOffset" : 287,
      "endOffset" : 291
    } ],
    "year" : 2015,
    "abstractText" : "Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.",
    "creator" : null
  }
}