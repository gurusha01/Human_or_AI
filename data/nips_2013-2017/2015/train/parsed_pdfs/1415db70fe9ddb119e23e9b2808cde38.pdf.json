{
  "name" : "1415db70fe9ddb119e23e9b2808cde38.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality",
    "authors" : [ "Zhaoran Wang", "Quanquan Gu", "Yang Ning" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The expectation-maximization (EM) algorithm [12] is the most popular approach for calculating the maximum likelihood estimator of latent variable models. Nevertheless, due to the nonconcavity of the likelihood function of latent variable models, the EM algorithm generally only converges to a local maximum rather than the global one [30]. On the other hand, existing statistical guarantees for latent variable models are only established for global optima [3]. Therefore, there exists a gap between computation and statistics. Significant progress has been made toward closing the gap between the local maximum attained by the EM algorithm and the maximum likelihood estimator [2, 18, 25, 30]. In particular, [30] first establish general sufficient conditions for the convergence of the EM algorithm. [25] further improve this result by viewing the EM algorithm as a proximal point method applied to the Kullback-Leibler divergence. See [18] for a detailed survey. More recently, [2] establish the first result that characterizes explicit statistical and computational rates of convergence for the EM algorithm. They prove that, given a suitable initialization, the EM algorithm converges at a geometric rate to a local maximum close to the maximum likelihood estimator. All these results are established in the low dimensional regime where the dimension d is much smaller than the sample size n. In high dimensional regimes where the dimension d is much larger than the sample size n, there exists no theoretical guarantee for the EM algorithm. In fact, when d n, the maximum likelihood estimator is in general not well defined, unless the models are carefully regularized by sparsity-type assumptions. Furthermore, even if a regularized maximum likelihood estimator can be obtained in a computationally tractable manner, establishing the corresponding statistical properties, especially asymptotic normality, can still be challenging because of the existence of high dimensional nuisance parameters. To address such a challenge, we develop a general inferential theory of the EM algorithm for parameter estimation and uncertainty assessment of high dimensional latent variable models. In particular, we make two contributions in this paper: • For high dimensional parameter estimation, we propose a novel high dimensional EM algorithm by\nattaching a truncation step to the expectation step (E-step) and maximization step (M-step). Such a ⇤Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910, NSF IIS1546482-BIGDATA, NSF DMS1454377-CAREER, NIH R01GM083084, NIH R01HG06841, NIH R01MH102339, and FDA HHSF223201000072C.\ntruncation step effectively enforces the sparsity of the attained estimator and allows us to establish significantly improved statistical rate of convergence. • Based upon the estimator attained by the high dimensional EM algorithm, we propose a decorrelated score statistic for testing hypotheses related to low dimensional components of the high dimensional parameter.\nUnder a unified analytic framework, we establish simultaneous statistical and computational guarantees for the proposed high dimensional EM algorithm and the respective uncertainty assessment procedure. Let ⇤ 2 Rd be the true parameter, s⇤ be its sparsity level and (t) T\nt=0 be the iterative\nsolution sequence of the high dimensional EM algorithm with T being the total number of iterations. In particular, we prove that: • Given an appropriate initialization init with relative error upper bounded by a constant  2 (0, 1),\ni.e.,\ninit ⇤\n2\n/k ⇤k 2  , the iterative solution sequence\n(t) T\nt=0 satisfies\n(t) ⇤\n2\n\n1 · ⇢t/2 | {z }\nOptimization Error\n+\n2\n·\np\ns⇤ · log d/n | {z }\nStatistical Error: Optimal Rate\n(1.1)\nwith high probability. Here ⇢ 2 (0, 1), and 1 , 2 are quantities that possibly depend on ⇢,  and ⇤. As the optimization error term in (1.1) decreases to zero at a geometric rate with respect to t, the overall estimation error achieves the p\ns⇤ · log d/n statistical rate of convergence (up to an extra factor of log n), which is (near-)minimax-optimal. See Theorem 3.4 for details. • The proposed decorrelated score statistic is asymptotically normal. Moreover, its limiting variance is optimal in the sense that it attains the semiparametric information bound for the low dimensional components of interest in the presence of high dimensional nuisance parameters. See Theorem 4.6 for details.\nOur framework allows two implementations of the M-step: the exact maximization versus approximate maximization. The former one calculates the maximizer exactly, while the latter one conducts an approximate maximization through a gradient ascent step. Our framework is quite general. We illustrate its effectiveness by applying it to two high dimensional latent variable models, that is, Gaussian mixture model and mixture of regression model. Comparison with Related Work: A closely related work is by [2], which considers the low dimensional regime where d is much smaller than n. Under certain initialization conditions, they prove that the EM algorithm converges at a geometric rate to some local optimum that attains the p\nd/n statistical rate of convergence. They cover both maximization and gradient ascent implementations of the M-step, and establish the consequences for the two latent variable models considered in our paper under low dimensional settings. Our framework adopts their view of treating the EM algorithm as a perturbed version of gradient methods. However, to handle the challenge of high dimensionality, the key ingredient of our framework is the truncation step that enforces the sparsity structure along the solution path. Such a truncation operation poses significant challenges for both computational and statistical analysis. In detail, for computational analysis we need to carefully characterize the evolution of each intermediate solution’s support and its effects on the evolution of the entire iterative solution sequence. For statistical analysis, we need to establish a fine-grained characterization of the entrywise statistical error, which is technically more challenging than just establishing the `\n2 -norm error employed by [2]. In high dimensional regimes, we need to establish the p\ns⇤ · log d/n statistical rate of convergence, which is much sharper than their p\nd/n rate when d n. In addition to point estimation, we further construct hypothesis tests for latent variable models in the high dimensional regime, which have not been established before. High dimensionality poses significant challenges for assessing the uncertainty (e.g., testing hypotheses) of the constructed estimators. For example, [15] show that the limiting distribution of the Lasso estimator is not Gaussian even in the low dimensional regime. A variety of approaches have been proposed to correct the Lasso estimator to attain asymptotic normality, including the debiasing method [13], the desparsification methods [26, 32] as well as instrumental variable-based methods [4]. Meanwhile, [16, 17, 24] propose the post-selection procedures for exact inference. In addition, several authors propose methods based on data splitting [20, 29], stability selection [19] and `\n2 -confidence sets [22]. However, these approaches mainly focus on generalized linear models rather than latent variable models. In addition, their results heavily rely on the fact that the estimator is a global optimum of a convex program. In comparison, our approach applies to a much broader family of statistical models with latent structures. For these latent variable models, it is computationally infeasible to\nobtain the global maximum of the penalized likelihood due to the nonconcavity of the likelihood function. Unlike existing approaches, our inferential theory is developed for the estimator attained by the proposed high dimensional EM algorithm, which is not necessarily a global optimum to any optimization formulation. Another line of research for the estimation of latent variable models is the tensor method, which exploits the structures of third or higher order moments. See [1] and the references therein. However, existing tensor methods primarily focus on the low dimensional regime where d ⌧ n. In addition, since the high order sample moments generally have a slow statistical rate of convergence, the estimators obtained by the tensor methods usually have a suboptimal statistical rate even for d ⌧ n. For example, [9] establish the p\nd6/n statistical rate of convergence for mixture of regression model, which is suboptimal compared with the p\nd/n minimax lower bound. Similarly, in high dimensional settings, the statistical rates of convergence attained by tensor methods are significantly slower than the statistical rate obtained in this paper. The latent variable models considered in this paper have been well studied. Nevertheless, only a few works establish theoretical guarantees for the EM algorithm. In particular, for Gaussian mixture model, [10, 11] establish parameter estimation guarantees for the EM algorithm and its extensions. For mixture of regression model, [31] establish exact parameter recovery guarantees for the EM algorithm under a noiseless setting. For high dimensional mixture of regression model, [23] analyze the gradient EM algorithm for the `\n1 -penalized log-likelihood. They establish support recovery guarantees for the attained local optimum but have no parameter estimation guarantees. In comparison with existing works, this paper establishes a general inferential framework for simultaneous parameter estimation and uncertainty assessment based on a novel high dimensional EM algorithm. Our analysis provides the first theoretical guarantee of parameter estimation and asymptotic inference in high dimensional regimes for the EM algorithm and its applications to a broad family of latent variable models. Notation: The matrix (p, q)-norm, i.e., k · kp,q, is obtained by taking the `p-norm of each row and then taking the `q-norm of the obtained row norms. We use C,C 0, . . . to denote generic constants. Their values may vary from line to line. We will introduce more notations in §2.2."
    }, {
      "heading" : "2 Methodology",
      "text" : "We first introduce the high dimensional EM Algorithm and then the respective inferential procedure. As examples, we consider their applications to Gaussian mixture model and mixture of regression model. For compactness, we defer the details to §A of the appendix. More models are included in the longer version of this paper. Algorithm 1 High Dimensional EM Algorithm 1: Parameter: Sparsity Parameter bs, Maximum Number of Iterations T 2: Initialization: bS init supp init, bs , (0) trunc init, bS init\nsupp(·, ·) and trunc(·, ·) are defined in (2.2) and (2.3)\n3: For t = 0 to T 1 4: E-step: Evaluate Qn ; (t) 5: M-step: (t+0.5) Mn (t)\nMn(·) is implemented as in Algorithm 2 or 3\n6: T-step: bS(t+0.5) supp (t+0.5), bs , (t+1) trunc (t+0.5), bS(t+0.5) 7: End For 8: Output: b (T )\nAlgorithm 2 Maximization Implementation of the M-step 1: Input: (t), Qn ; (t) Output: Mn (t) argmax Qn ; (t)\nAlgorithm 3 Gradient Ascent Implementation of the M-step 1: Input: (t), Qn ; (t) Parameter: Stepsize ⌘ > 0\n2: Output: Mn (t)\n(t) + ⌘ ·rQn (t); (t)"
    }, {
      "heading" : "2.1 High Dimensional EM Algorithm",
      "text" : "Before we introduce the proposed high dimensional EM Algorithm (Algorithm 1), we briefly review the classical EM algorithm. Let h (y) be the probability density function of Y 2 Y , where 2 Rd is the model parameter. For latent variable models, we assume that h (y) is obtained by marginalizing over an unobserved latent variable Z 2 Z , i.e., h (y) = R\nZ f (y, z) dz. Let k (z | y) be the density\nof Z conditioning on the observed variable Y = y, i.e., k (z | y) = f (y, z)/h (y). We define\nQn( ; 0 ) = 1\nn\nn X\ni=1\nZ\nZ k 0(z | yi) · log f (yi, z) dz. (2.1)\nSee §B of the appendix for a detailed derivation. At the t-th iteration of the classical EM algorithm, we evaluate Qn ; (t) at the E-step and then perform max Qn ; (t)\nat the M-step. The proposed high dimensional EM algorithm (Algorithm 1) is built upon the E-step and M-step (lines 4 and 5) of the classical EM algorithm. In addition to the exact maximization implementation of the M-step (Algorithm 2), we allow the gradient ascent implementation of the M-step (Algorithm 3), which performs an approximate maximization via a gradient ascent step. To handle the challenge of high dimensionality, in line 6 of Algorithm 1 we perform a truncation step (T-step) to enforce the sparsity structure. In detail, we define\nsupp( , s): The set of index j’s corresponding to the top s largest | j |’s. (2.2) Also, for an index set S ✓ {1, . . . , d}, we define the trunc(·, ·) function in line 6 as\n⇥ trunc( ,S) ⇤\nj = j · 1{j 2 S}. (2.3)\nNote that (t+0.5) is the output of the M-step (line 5) at the t-th iteration of the high dimensional EM algorithm. To obtain (t+1), the T-step (line 6) preserves the entries of (t+0.5) with the top bs large magnitudes and sets the rest to zero. Here bs is a tuning parameter that controls the sparsity level (line 1). By iteratively performing the E-step, M-step and T-step, the high dimensional EM algorithm attains an bs-sparse estimator b = (T ) (line 8). Here T is the total number of iterations."
    }, {
      "heading" : "2.2 Asymptotic Inference",
      "text" : "Notation: Let r\n1 Q( ; 0) be the gradient with respect to and r 2 Q( ; 0) be the gradient with respect to 0. If there is no confusion, we simply denote rQ( ; 0) = r\n1 Q( ; 0) as in the previous sections. We define the higher order derivatives in the same manner, e.g., r2\n1,2Q( ; 0 ) is calculated\nby first taking derivative with respect to and then with respect to 0. For = > 1 , > 2\n> 2 Rd with\n1 2 Rd1 , 2 2 Rd2 and d 1 + d 2 = d, we use notations such as v 1 2 Rd1 and A 1, 2 2 Rd1⇥d2 to denote the corresponding subvector of v 2 Rd and the submatrix of A 2 Rd⇥d. We aim to conduct asymptotic inference for low dimensional components of the high dimensional parameter ⇤. Without loss of generality, we consider a single entry of ⇤. In particular, we assume ⇤ = ⇥ ↵⇤, ( ⇤)> ⇤>, where ↵⇤ 2 R is the entry of interest, while ⇤ 2 Rd 1 is treated as the nuisance parameter. In the following, we construct a high dimensional score test named decorrelated score test. It is worth noting that, our method and theory can be easily generalized to perform statistical inference for an arbitrary low dimensional subvector of ⇤. Decorrelated Score Test: For score test, we are primarily interested in testing H\n0 : ↵⇤ = 0, since this null hypothesis characterizes the uncertainty in variable selection. Our method easily generalizes to H\n0 : ↵⇤ = ↵ 0 with ↵ 0\n6= 0. For notational simplicity, we define the following key quantity Tn( ) = r 2 1,1Qn( ; ) +r 2 1,2Qn( ; ) 2 Rd⇥d. (2.4)\nLet = ↵, > >. We define the decorrelated score function Sn(·, ·) 2 R as\nSn( , ) = ⇥ r 1 Qn( ; ) ⇤ ↵ w( , )> · ⇥ r 1 Qn( ; ) ⇤ . (2.5)\nHere w( , ) 2 Rd 1 is obtained using the following Dantzig selector [8] w( , ) = argmin\nw2Rd 1 kwk 1\n, subject to\n⇥ Tn( ) ⇤\n,↵\n⇥ Tn( ) ⇤\n, ·w 1  , (2.6)\nwhere > 0 is a tuning parameter. Let b = b↵, b > >, where b is the estimator attained by the high dimensional EM algorithm (Algorithm 1). We define the decorrelated score statistic as p\nn · Sn b 0 ,\n⇥\nTn b 0\n⇤\n↵|\n1/2 , (2.7)\nwhere b 0 = 0, b > > , and ⇥ Tn b 0 ⇤\n↵| = ⇥ 1, w b 0\n, >⇤\n· Tn b 0 ·\n⇥\n1, w\nb 0\n, >⇤> .\nHere we use b 0 instead of b since we are interested in the null hypothesis H 0 : ↵⇤ = 0. We can also replace b\n0 with b and the theoretical results will remain the same. In §4 we will prove the proposed decorrelated score statistic in (2.7) is asymptotically N(0, 1). Consequently, the decorrelated score\ntest with significance level 2 (0, 1) takes the form\nS( ) = 1\np\nn · Sn b 0 ,\n⇥\nTn b 0\n⇤\n↵|\n1/2 /2 ⇥\n1 (1 /2), 1(1 /2)\n⇤\n,\nwhere 1(·) is the inverse function of the Gaussian cumulative distribution function. If S( ) = 1, we reject the null hypothesis H\n0 : ↵⇤ = 0. The intuition of this decorrelated score test is explained in §D of the appendix. The key theoretical observation is Theorem 2.1, which connects r\n1 Qn(·; ·) in (2.5) and Tn(·) in (2.7) with the score function and Fisher information in the presence of latent structures. Let `n( ) be the log-likelihood. Its score function is r`n( ) and the Fisher information is I( ⇤) = E ⇤ ⇥ r 2`n( ⇤) ⇤\nn, where E ⇤(·) is the expectation under the model with parameter ⇤. Theorem 2.1. For the true parameter ⇤ and any 2 Rd, it holds that\nr\n1 Qn( ; ) = r`n( )/n, and E ⇤ ⇥ Tn( ⇤ ) ⇤ = I( ⇤) = E ⇤ ⇥ r 2`n( ⇤ ) ⇤ n. (2.8)\nProof. See §I.1 of the appendix for a detailed proof.\nBased on the decorrelated score test, it is easy to establish the decorrelated Wald test, which allows us to construct confidence intervals. For compactness we defer it to the longer version of this paper."
    }, {
      "heading" : "3 Theory of Computation and Estimation",
      "text" : "Before we present the main results, we introduce three technical conditions, which will significantly ease our presentation. They will be verified for specific latent variable models in §E of the appendix. The first two conditions, proposed by [2], characterize the properties of the population version lower bound function Q(·; ·), i.e., the expectation of Qn(·; ·) defined in (2.1). We define the respective population version M-step as follows. For the M-step in Algorithm 2, we define\nM( ) = argmax 0\nQ( 0; ). (3.1)\nFor the M-step in Algorithm 3, we define M( ) = + ⌘ ·r\n1 Q( ; ), (3.2) where ⌘ > 0 is the stepsize in Algorithm 3. We use B to denote the basin of attraction, i.e., the local region where the high dimensional EM algorithm enjoys desired guarantees. Condition 3.1. We define two versions of this condition. • Lipschitz-Gradient-1(\n1\n,B). For the true parameter ⇤ and any 2 B, we have\nr\n1\nQ ⇥ M( ); ⇤ ⇤ r\n1\nQ ⇥ M( ); ⇤\n2\n 1 · k ⇤k 2\n, (3.3) where M(·) is the population version M-step (maximization implementation) defined in (3.1).\n• Lipschitz-Gradient-2( 2 ,B). For the true parameter ⇤ and any 2 B, we have\nr\n1 Q( ; ⇤) r 1 Q( ; )\n2\n 2 · k ⇤k 2 . (3.4)\nCondition 3.1 defines a variant of Lipschitz continuity for r 1 Q(·; ·). In the sequel, we will use (3.3) and (3.4) in the analysis of the two implementations of the M-step respectively. Condition 3.2 Concavity-Smoothness(µ, ⌫,B). For any\n1 , 2 2 B, Q(·; ⇤) is µ-smooth, i.e., Q(\n1 ; ⇤) Q( 2 ; ⇤) + ( 1\n2\n) > ·r\n1 Q( 2 ; ⇤) µ/2 · k 2\n1\nk\n2\n2 , (3.5) and ⌫-strongly concave, i.e.,\nQ( 1 ; ⇤)  Q( 2 ; ⇤) + ( 1\n2\n) > ·r\n1 Q( 2 ; ⇤) ⌫/2 · k 2\n1\nk\n2\n2 . (3.6) This condition indicates that, when the second variable of Q(·; ·) is fixed to be ⇤, the function is ‘sandwiched’ between two quadratic functions. The third condition characterizes the statistical error between the sample version and population version M-steps, i.e., Mn(·) defined in Algorithms 2 and 3, and M(·) in (3.1) and (3.2). Recall k · k\n0 denotes the total number of nonzero entries in a vector. Condition 3.3 Statistical-Error(✏, , s, n,B). For any fixed 2 B with k k\n0\n s, we have that\nM( ) Mn( ) 1  ✏ (3.7) holds with probability at least 1 . Here ✏ > 0 possibly depends on , sparsity level s, sample size n, dimension d, as well as the basin of attraction B. In (3.7) the statistical error ✏ quantifies the `1-norm of the difference between the population version and sample version M-steps. Particularly, we constrain the input of M(·) and Mn(·) to be s-sparse. Such a condition is different from the one used by [2]. In detail, they quantify the statistical error\nwith the ` 2 -norm and do not constrain the input of M(·) and Mn(·) to be sparse. Consequently, our subsequent statistical analysis is different from theirs. The reason we use the `1-norm is that, it characterizes the more refined entrywise statistical error, which converges at a fast rate of p\nlog d/n (possibly with extra factors depending on specific models). In comparison, the `\n2 -norm statistical error converges at a slow rate of p\nd/n, which does not decrease to zero as n increases with d n. Furthermore, the fine-grained entrywise statistical error is crucial to our key proof for quantifying the effects of the truncation step (line 6 of Algorithm 1) on the iterative solution sequence."
    }, {
      "heading" : "3.1 Main Results",
      "text" : "To simplify the technical analysis of the high dimensional EM algorithm, we focus on its resampling version, which is illustrated in Algorithm 4 in §C of the appendix. Theorem 3.4. We define B = : k ⇤k 2  R , where R =  · k ⇤k 2\nfor some  2 (0, 1). We assume Condition Concavity-Smoothness(µ, ⌫,B) holds and init ⇤\n2  R/2. • For the maximization implementation of the M-step (Algorithm 2), we suppose that Condition\nLipschitz-Gradient-1( 1 ,B) holds with ⇢ 1 := 1 /⌫ 2 (0, 1) and bs = ⌃ C ·max\n16/(1/⇢ 1 1) 2, 4 · (1 + )2/(1 )2\n· s⇤ ⌥ , (3.8)\np\nbs+ C 0/ p 1  · p s⇤ · ✏  min (1 p\n⇢ 1 ) 2 ·R, (1 )2/[2 · (1 + )] · k ⇤k 2\n. (3.9) Here C 1 and C 0 > 0 are constants. Under Condition Statistical-Error(✏, /T, bs, n/T,B) we have that, for t = 1, . . . , T ,\n(t) ⇤\n2\n ⇢t/2 1\n·R | {z }\nOptimization Error\n+\np\nbs+ C 0/ p 1  · p s⇤ /(1 p\n⇢ 1 ) · ✏ | {z }\nStatistical Error\n(3.10)\nholds with probability at least 1 , where C 0 is the same constant as in (3.9). • For the gradient ascent implementation of the M-step (Algorithm 3), we suppose that Condition\nLipschitz-Gradient-2( 2 ,B) holds with ⇢ 2 := 1 2 · (⌫ 2 )/(⌫+µ) 2 (0, 1) and the stepsize in Algorithm 3 is set to ⌘ = 2/(⌫ + µ). Meanwhile, we assume (3.8) and (3.9) hold with ⇢\n1 replaced by ⇢\n2 . Under Condition Statistical-Error(✏, /T, bs, n/T,B) we have that, for t = 1, . . . , T , (3.10) holds with probability at least 1 , in which ⇢\n1 is replaced with ⇢ 2 .\nProof. See §G.1 of the appendix for a detailed proof.\nThe assumption in (3.8) states that the sparsity parameter bs is chosen to be sufficiently large and also of the same order as the true sparsity level s⇤. This assumption ensures that the error incurred by the truncation step can be upper bounded. In addition, as is shown for specific latent variable models in §E of the appendix, the error term ✏ in Condition Statistical-Error(✏, /T, bs, n/T,B) decreases as sample size n increases. By the assumption in (3.8), p bs+ C 0/ p 1  · p s⇤\nis of the same order as p\ns⇤. Therefore, the assumption in (3.9) suggests the sample size n is sufficiently large such that p\ns⇤ · ✏ is sufficiently small. These assumptions guarantee that the entire iterative solution sequence remains within the basin of attraction B in the presence of statistical error. Theorem 3.4 illustrates that, the upper bound of the overall estimation error can be decomposed into two terms. The first term is the upper bound of optimization error, which decreases to zero at a geometric rate of convergence, because we have ⇢\n1 , ⇢ 2 < 1. Meanwhile, the second term is the upper bound of statistical error, which does not depend on t. Since p bs+C 0/ p 1  · p s⇤\nis of the same order as p s⇤, this term is proportional to p\ns⇤ · ✏, where ✏ is the entrywise statistical error between M(·) and Mn(·). In §E of the appendix we prove that, for each specific latent variable model, ✏ is roughly of the order p\nlog d/n. (There may be extra factors attached to ✏ depending on each specific model.) Therefore, the statistical error term is roughly of the order p\ns⇤ · log d/n. Consequently, for a sufficiently large t = T such that the optimization and statistical error terms in (3.10) are of the same order, the final estimator b = (T ) attains a (near-)optimal p\ns⇤ · log d/n (possibly with extra factors) statistical rate. For compactness, we give the following example and defer the details to §E. Implications for Gaussian Mixture Model: We assume y\n1 , . . . ,yn are the n i.i.d. realizations of Y = Z · ⇤ + V . Here Z is a Rademacher random variable, i.e., P(Z = +1) = P(Z = 1) = 1/2, and V ⇠ N(0, 2 · Id) is independent of Z, where is the standard deviation. Suppose that we have k ⇤k\n2 / r, where r > 0 is a sufficiently large constant that denotes the minimum signal-to-noise ratio. In §E of the appendix we prove that there exists some constant C > 0 such that Conditions\nLipschitz-Gradient-1( 1 ,B) and Concavity-Smoothness(µ, ⌫,B) hold with 1 = exp C · r2 , µ = ⌫ = 1, B = : k ⇤k 2  R with R =  · k ⇤k 2 ,  = 1/4.\nFor a sufficiently large n, we have that Condition Statistical-Error(✏, , s, n,B) holds with\n✏ = C · k ⇤k1 +\n·\nq\n⇥ log d+ log(2/ ) ⇤ n.\nThen the first part of Theorem 3.4 implies\nb ⇤\n2\n C · p s⇤ · log d · log n/n for a sufficiently large T , which is near-optimal with respect to the minimax lower bound p s⇤ log d/n."
    }, {
      "heading" : "4 Theory of Inference",
      "text" : "To simplify the presentation of the unified framework, we lay out several technical conditions, which will be verified for each model. Let ⇣EM, ⇣G, ⇣T and ⇣L be four quantities that scale with s⇤, d and n. These conditions will be verified for specific latent variable models in §F of the appendix. Condition 4.1 Parameter-Estimation ⇣EM . We have b ⇤\n1\n= OP ⇣EM .\nCondition 4.2 Gradient-Statistical-Error ⇣G . We have\nr\n1 Qn( ⇤ ; ⇤) r 1 Q( ⇤; ⇤)\n1 = OP ⇣G .\nCondition 4.3 Tn(·)-Concentration ⇣T . We have Tn( ⇤) E ⇤ ⇥ Tn( ⇤) ⇤\n1,1 = OP ⇣T .\nCondition 4.4 Tn(·)-Lipschitz ⇣L . For any , we have\nTn( ) Tn( ⇤ )\n1,1 = OP ⇣L\n· k ⇤k 1 .\nIn the sequel, we lay out an assumption on several population quantities and the sample size n. Recall that ⇤ = [↵⇤, ( ⇤)>]>, where ↵⇤ 2 R is the entry of interest, while ⇤ 2 Rd 1 is the nuisance parameter. By the notations in §2.2, ⇥ I( ⇤) ⇤\n, 2 R(d 1)⇥(d 1) and ⇥ I( ⇤) ⇤ ,↵ 2 R(d 1)⇥1 denote\nthe submatrices of the Fisher information matrix I( ⇤) 2 Rd⇥d. We define w⇤, s⇤w and S⇤w as w ⇤ = ⇥\nI( ⇤) ⇤ 1 , · ⇥ I( ⇤) ⇤ ,↵ 2 Rd 1, s⇤w = kw⇤k0, and S⇤w = supp(w⇤). (4.1)\nWe define 1\n⇥ I( ⇤) ⇤ and d ⇥ I( ⇤) ⇤ as the largest and smallest eigenvalues of I( ⇤), and ⇥\nI( ⇤) ⇤ ↵| = ⇥ I( ⇤) ⇤ ↵,↵ ⇥ I( ⇤) ⇤> ,↵ · ⇥ I( ⇤) ⇤ 1 , · ⇥ I( ⇤) ⇤ ,↵ 2 R. (4.2)\nAccording to (4.1) and (4.2), we can easily verify that ⇥\nI( ⇤) ⇤ ↵| = ⇥ 1, (w⇤)> ⇤ · I( ⇤) · ⇥ 1, (w⇤)> ⇤> . (4.3)\nThe following assumption ensures that d ⇥ I( ⇤) ⇤ > 0. Hence, ⇥ I( ⇤) ⇤\n, in (4.1) is invertible.\nAlso, according to (4.3) and the fact that d ⇥ I( ⇤) ⇤ > 0, we have ⇥ I( ⇤) ⇤\n↵| > 0.\nAssumption 4.5 . We impose the following assumptions. • For positive constants ⇢\nmax and ⇢ min , we assume\n⇢ max\n1\n⇥ I( ⇤) ⇤\nd ⇥ I( ⇤) ⇤ ⇢ min , ⇥ I( ⇤) ⇤ ↵| = O(1), ⇥ I( ⇤) ⇤ 1 ↵| = O(1). (4.4)\n• The tuning parameter of the Dantzig selector in (2.6) is set to = C · ⇣T + ⇣L · ⇣EM · 1 + kw ⇤ k\n1 , (4.5) where C 1 is a sufficiently large constant. The sample size n is sufficiently large such that\nmax kw ⇤ k\n1\n, 1 · s⇤w · = o(1), ⇣ EM = o(1), s⇤w · · ⇣ G = o(1/ p n), (4.6)\n· ⇣EM = o(1/ p n), max 1, kw⇤k 1 · ⇣L · ⇣EM 2 = o(1/ p n).\nThe assumption on d ⇥ I( ⇤) ⇤\nguarantees that the Fisher information matrix is positive definite. The other assumptions in (4.4) guarantee the existence of the asymptotic variance of p\nn · Sn b 0 , in the score statistic defined in (2.7). Similar assumptions are standard in existing asymptotic inference results. For example, for mixture of regression model, [14] impose variants of these assumptions. For specific models, we will show that ⇣EM, ⇣G, ⇣T and all decrease with n, while ⇣L increases with n at a slow rate. Therefore, the assumptions in (4.6) ensure that the sample size n is sufficiently large. We will make these assumptions more explicit after we specify ⇣EM, ⇣G, ⇣T and ⇣L for each\nmodel. Note the assumptions in (4.6) imply that s⇤w = kw⇤k0 needs to be small. For instance, for specified in (4.5), max kw ⇤ k\n1\n, 1 · s⇤w · = o(1) in (4.6) implies s⇤w · ⇣T = o(1). In the following, we will prove that ⇣T is of the order p\nlog d/n. Hence, we require that s⇤w = o\np\nn/ log d ⌧ d 1, i.e., w⇤ 2 Rd 1 is sparse. Such a sparsity assumption can be understood as follows. According to the definition of w⇤ in (4.1), we have ⇥ I( ⇤) ⇤\n, · w\n⇤ = ⇥ I( ⇤) ⇤\n,↵ . Therefore, such a sparsity\nassumption suggests ⇥ I( ⇤) ⇤\n,↵ lies within the span of a few columns of\n⇥ I( ⇤) ⇤\n, . Such a sparsity\nassumption on w⇤ is necessary, because otherwise it is difficult to accurately estimate w⇤ in high dimensional regimes. In the context of high dimensional generalized linear models, [26, 32] impose similar sparsity assumptions."
    }, {
      "heading" : "4.1 Main Results",
      "text" : "Decorrelated Score Test: The next theorem establishes the asymptotic normality of the decorrelated score statistic defined in (2.7).\nTheorem 4.6. We consider ⇤ = ⇥ ↵⇤, ( ⇤)> ⇤> with ↵⇤ = 0. Under Assumption 4.5 and Conditions 4.1-4.4, we have that for n ! 1, p\nn · Sn b 0 ,\n⇥\nTn b 0\n⇤\n↵|\n1/2 D ! N(0, 1), (4.7)\nwhere b 0 and ⇥ Tn b 0 ⇤ ↵| 2 R are defined in (2.7). The limiting variance of the decorrelated score function p\nn · Sn b 0 ,\nis ⇥ I( ⇤) ⇤\n↵| , which is defined in (4.2).\nProof. See §G.2 of the appendix for a detailed proof. Optimality: [27] prove that for inferring ↵⇤ in the presence of nuisance parameter ⇤, ⇥ I( ⇤) ⇤\n↵| is the semiparametric efficient information, i.e., the minimum limiting variance of the (rescaled) score function. Our proposed decorrelated score function achieves such a semiparametric information lower bound and is therefore in this sense optimal. In the following, we use Gaussian mixture model to illustrate the effectiveness of Theorem 4.6. We defer the details and the implications for mixture of regression to §F of the appendix. Implications for Gaussian Mixture Model: Under the same model considered in §3.1, if we assume all quantities except s⇤w, s⇤, d and n are constant, then we have that Conditions 4.1-4.4 hold with ⇣EM = s⇤ p log d · log n/n, ⇣G = p log d/n, ⇣T = p log d/n and ⇣L = log d+ log n\n3/2. Thus, under Assumption 4.5, (4.7) holds when n ! 1. Also, we can verify that (4.6) in Assumption 4.5 holds if max\ns⇤w, s ⇤ 2 · (s⇤)2 · (log d)5 = o ⇥ n/(log n)2 ⇤ ."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure. Our theory shows that, with a suitable initialization, the proposed algorithm converges at a geometric rate and achieves an estimator with the (near-)optimal statistical rate of convergence. Beyond point estimation, we further propose the decorrelated score and Wald statistics for testing hypotheses and constructing confidence intervals for low dimensional components of high dimensional parameters. We apply the proposed algorithmic framework to a broad family of high dimensional latent variable models. For these models, our framework establishes the first computationally feasible approach for optimal parameter estimation and asymptotic inference under high dimensional settings."
    } ],
    "references" : [ {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "A N A N D K U M A R", "G E A", "H S U R" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
      "author" : [ "N B A L A K R I S H N A", ". S", "T WA I N W R I G H", ". Y U M . J" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Latent variable models and factor analysis: A unified",
      "author" : [ "W B A R T H O L O M E", ". D . J", "T K N O T", ". M", "I I M O U S TA K" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Sparse models and methods for optimal instruments with an application to eminent",
      "author" : [ "I B E L L O N", ". A", "N C H E", ". D", "V V. C H E R N O Z H U K O", "N C H A N S E" ],
      "venue" : "domain. Econometrica",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "L B I C K E", ". P. J", "V Y. R I T O", "V T S Y B A K O", "B A" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Concentration inequalities: A nonasymptotic theory of independence",
      "author" : [ "N B O U C H E R O", ". S", "I L U G O S", ". G", "T P. M A S S A R" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "A constrained `1 minimization approach to sparse precision matrix estimation",
      "author" : [ "I T. C A", "U W. L I", "O X L U" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "The Dantzig selector: Statistical estimation when p is much larger than n",
      "author" : [ "S C A N D È", ". E", "O T. TA" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Spectral experts for estimating mixtures of linear regressions",
      "author" : [ "Y C H A G A N T", "T. A", "G P. L I A N" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "A probabilistic analysis of EM for mixtures of separated, spherical Gaussians",
      "author" : [ "TA D A S G U P", ". S", "N L S C H U L M A" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "R D E M P S T E", "P. A", "D L A I R", ". N . M", "N R U B I", "B D" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Statistical Methodology)",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1977
    }, {
      "title" : "Variables selection in finite mixture of regression models",
      "author" : [ "I K H A L I L", ". A", "N J C H E" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Asymptotics for Lasso-type estimators",
      "author" : [ "T K N I G H", ". F U K" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2000
    }, {
      "title" : "Exact inference after model selection via the Lasso",
      "author" : [ "E L E", ". S U N J . D", ". S U N D . L", "R TAY L O", "E J" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "A significance test for the Lasso",
      "author" : [ "RT L O C K H A", ". R", "R J TAY L O" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Confidence sets in sparse regression",
      "author" : [ "L N I C K", ". R", "R S VA N D E G E E" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "`1-penalization for mixture regression models. TEST",
      "author" : [ "R S T Ä D L E", ". N", "N P. B Ü H L M A N", "R S VA N D E G E E" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Post-selection adaptive inference for least angle regression and the Lasso",
      "author" : [ "R TAY L O", ". J", "T R L O C K H A R" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "An analysis of the EM algorithm and entropy-like proximal point methods",
      "author" : [ "G P. T S E N" ],
      "venue" : "Mathematics of Operations Research",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2004
    }, {
      "title" : "On asymptotically optimal confidence regions and tests for high-dimensional models",
      "author" : [ "R VA N D E G E E", ". S", "N P. B Ü H L M A N", "V Y. R I T O", "E R D E Z E U R" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "N R V E R S H Y N I" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2010
    }, {
      "title" : "High-dimensional variable selection",
      "author" : [ "N WA S S E R M A", ". L", "R K R O E D E" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "On the convergence properties of the EM algorithm",
      "author" : [ "U W", "J C . F" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1983
    }, {
      "title" : "Alternating minimization for mixed linear regression",
      "author" : [ "I Y", ". X", "S C A R A M A N I", ". C", "I S S A N G H AV" ],
      "venue" : null,
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2013
    }, {
      "title" : "Confidence intervals for low dimensional parameters in high dimensional linear models",
      "author" : [ "G Z H A N", ". C . - H", "S S" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology)",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "1 Introduction The expectation-maximization (EM) algorithm [12] is the most popular approach for calculating the maximum likelihood estimator of latent variable models.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 22,
      "context" : "Nevertheless, due to the nonconcavity of the likelihood function of latent variable models, the EM algorithm generally only converges to a local maximum rather than the global one [30].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, existing statistical guarantees for latent variable models are only established for global optima [3].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "Significant progress has been made toward closing the gap between the local maximum attained by the EM algorithm and the maximum likelihood estimator [2, 18, 25, 30].",
      "startOffset" : 150,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "Significant progress has been made toward closing the gap between the local maximum attained by the EM algorithm and the maximum likelihood estimator [2, 18, 25, 30].",
      "startOffset" : 150,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : "Significant progress has been made toward closing the gap between the local maximum attained by the EM algorithm and the maximum likelihood estimator [2, 18, 25, 30].",
      "startOffset" : 150,
      "endOffset" : 165
    }, {
      "referenceID" : 22,
      "context" : "In particular, [30] first establish general sufficient conditions for the convergence of the EM algorithm.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 18,
      "context" : "[25] further improve this result by viewing the EM algorithm as a proximal point method applied to the Kullback-Leibler divergence.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "More recently, [2] establish the first result that characterizes explicit statistical and computational rates of convergence for the EM algorithm.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "Comparison with Related Work: A closely related work is by [2], which considers the low dimensional regime where d is much smaller than n.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "For statistical analysis, we need to establish a fine-grained characterization of the entrywise statistical error, which is technically more challenging than just establishing the ` 2 -norm error employed by [2].",
      "startOffset" : 208,
      "endOffset" : 211
    }, {
      "referenceID" : 12,
      "context" : "For example, [15] show that the limiting distribution of the Lasso estimator is not Gaussian even in the low dimensional regime.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "A variety of approaches have been proposed to correct the Lasso estimator to attain asymptotic normality, including the debiasing method [13], the desparsification methods [26, 32] as well as instrumental variable-based methods [4].",
      "startOffset" : 172,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "A variety of approaches have been proposed to correct the Lasso estimator to attain asymptotic normality, including the debiasing method [13], the desparsification methods [26, 32] as well as instrumental variable-based methods [4].",
      "startOffset" : 172,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "A variety of approaches have been proposed to correct the Lasso estimator to attain asymptotic normality, including the debiasing method [13], the desparsification methods [26, 32] as well as instrumental variable-based methods [4].",
      "startOffset" : 228,
      "endOffset" : 231
    }, {
      "referenceID" : 13,
      "context" : "Meanwhile, [16, 17, 24] propose the post-selection procedures for exact inference.",
      "startOffset" : 11,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "Meanwhile, [16, 17, 24] propose the post-selection procedures for exact inference.",
      "startOffset" : 11,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "Meanwhile, [16, 17, 24] propose the post-selection procedures for exact inference.",
      "startOffset" : 11,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "In addition, several authors propose methods based on data splitting [20, 29], stability selection [19] and ` 2 -confidence sets [22].",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "In addition, several authors propose methods based on data splitting [20, 29], stability selection [19] and ` 2 -confidence sets [22].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "In particular, for Gaussian mixture model, [10, 11] establish parameter estimation guarantees for the EM algorithm and its extensions.",
      "startOffset" : 43,
      "endOffset" : 51
    }, {
      "referenceID" : 23,
      "context" : "For mixture of regression model, [31] establish exact parameter recovery guarantees for the EM algorithm under a noiseless setting.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "For high dimensional mixture of regression model, [23] analyze the gradient EM algorithm for the ` 1 -penalized log-likelihood.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "5) Here w( , ) 2 Rd 1 is obtained using the following Dantzig selector [8] w( , ) = argmin w2Rd 1 kwk 1 , subject to ⇥",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "The first two conditions, proposed by [2], characterize the properties of the population version lower bound function Q(·; ·), i.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "Such a condition is different from the one used by [2].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "For example, for mixture of regression model, [14] impose variants of these assumptions.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "In the context of high dimensional generalized linear models, [26, 32] impose similar sparsity assumptions.",
      "startOffset" : 62,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "In the context of high dimensional generalized linear models, [26, 32] impose similar sparsity assumptions.",
      "startOffset" : 62,
      "endOffset" : 70
    } ],
    "year" : 2015,
    "abstractText" : "We provide a general theory of the expectation-maximization (EM) algorithm for inferring high dimensional latent variable models. In particular, we make two contributions: (i) For parameter estimation, we propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure into parameter estimation. With an appropriate initialization, this algorithm converges at a geometric rate and attains an estimator with the (near-)optimal statistical rate of convergence. (ii) Based on the obtained estimator, we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters. For a broad family of statistical models, our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions.",
    "creator" : null
  }
}