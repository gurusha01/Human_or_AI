{
  "name" : "a50abba8132a77191791390c3eb19fe7.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Stop Wasting My Gradients: Practical SVRG",
    "authors" : [ "Reza Babanezhad", "Mohamed Osama Ahmed", "Alim Virani", "Mark Schmidt", "Scott Sallinen" ],
    "emails" : [ "kubo.konecny@gmail.com", "scotts@ece.ubc.ca" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of optimizing the average of a finite but large sum of smooth functions,\nmin x∈Rd\nf(x) = 1\nn n∑ i=1 fi(x). (1)\nA huge proportion of the model-fitting procedures in machine learning can be mapped to this problem. This includes classic models like least squares and logistic regression but also includes more advanced methods like conditional random fields and deep neural network models. In the highdimensional setting (large d), the traditional approaches for solving (1) are: full gradient (FG) methods which have linear convergence rates but need to evaluate the gradient fi for all n examples on every iteration, and stochastic gradient (SG) methods which make rapid initial progress as they only use a single gradient on each iteration but ultimately have slower sublinear convergence rates.\nLe Roux et al. [1] proposed the first general method, stochastic average gradient (SAG), that only considers one training example on each iteration but still achieves a linear convergence rate. Other methods have subsequently been shown to have this property [2, 3, 4], but these all require storing a previous evaluation of the gradient f ′i or the dual variables for each i. For many objectives this only requires O(n) space, but for general problems this requires O(np) space making them impractical.\nRecently, several methods have been proposed with similar convergence rates to SAG but without the memory requirements [5, 6, 7, 8]. They are known as mixed gradient, stochastic variance-reduced gradient (SVRG), and semi-stochastic gradient methods (we will use SVRG). We give a canonical SVRG algorithm in the next section, but the salient features of these methods are that they evaluate two gradients on each iteration and occasionally must compute the gradient on all examples. SVRG\nmethods often dramatically outperform classic FG and SG methods, but these extra evaluations mean that SVRG is slower than SG methods in the important early iterations. They also mean that SVRG methods are typically slower than memory-based methods like SAG.\nIn this work we first show that SVRG is robust to inexact calculation of the full gradients it requires (§3), provided the accuracy increases over time. We use this to explore growing-batch strategies that require fewer gradient evaluations when far from the solution, and we propose a mixed SG/SVRG method that may improve performance in the early iterations (§4). We next explore using support vectors to reduce the number of gradients required when close to the solution (§5), give a justification for the regularized SVRG update that is commonly used in practice (§6), consider alternative minibatch strategies (§7), and finally consider the generalization error of the method (§8)."
    }, {
      "heading" : "2 Notation and SVRG Algorithm",
      "text" : "SVRG assumes f is µ-strongly convex, each fi is convex, and each gradient f ′i is Lipschitzcontinuous with constant L. The method begins with an initial estimate x0, sets x0 = x0 and then generates a sequence of iterates xt using\nxt = xt−1 − η(f ′it(xt−1)− f ′ it(x s) + µs), (2)\nwhere η is the positive step size, we set µs = f ′(xs), and it is chosen uniformly from {1, 2, . . . , n}. After every m steps, we set xs+1 = xt for a random t ∈ {1, . . . ,m}, and we reset t = 0 with x0 = x s+1.\nTo analyze the convergence rate of SVRG, we will find it convenient to define the function\nρ(a, b) = 1\n1− 2ηa\n( 1\nmµη + 2bη\n) .\nas it appears repeatedly in our results. We will use ρ(a) to indicate the value of ρ(a, b) when a = b, and we will simply use ρ for the special case when a = b = L. Johnson & Zhang [6] show that if η and m are chosen such that 0 < ρ < 1, the algorithm achieves a linear convergence rate of the form\nE[f(xs+1)− f(x∗)] ≤ ρE[f(xs)− f(x∗)], where x∗ is the optimal solution. This convergence rate is very fast for appropriate η and m. While this result relies on constants we may not know in general, practical choices with good empirical performance include setting m = n, η = 1/L, and using xs+1 = xm rather than a random iterate.\nUnfortunately, the SVRG algorithm requires 2m + n gradient evaluations for every m iterations of (2), since updating xt requires two gradient evaluations and computing µs require n gradient evaluations. We can reduce this to m+ n if we store the gradients f ′i(x\ns), but this is not practical in most applications. Thus, SVRG requires many more gradient evaluations than classic SG iterations of memory-based methods like SAG."
    }, {
      "heading" : "3 SVRG with Error",
      "text" : "We first give a result for the SVRG method where we assume that µs is equal to f ′(xs) up to some error es. This is in the spirit of the analysis of [9], who analyze FG methods under similar assumptions. We assume that ‖xt−x∗‖ ≤ Z for all t, which has been used in related work [10] and is reasonable because of the coercity implied by strong-convexity. Proposition 1. If µs = f ′(xs) + es and we set η and m so that ρ < 1, then the SVRG algorithm (2) with xs+1 chosen randomly from {x1, x2, . . . , xm} satisfies\nE[f(xs+1)− f(x∗)] ≤ ρE[f(xs)− f(x∗)] + ZE‖e s‖+ ηE‖es‖2\n1− 2ηL .\nWe give the proof in Appendix A. This result implies that SVRG does not need a very accurate approximation of f ′(xs) in the crucial early iterations since the first term in the bound will dominate. Further, this result implies that we can maintain the exact convergence rate of SVRG as long as the errors es decrease at an appropriate rate. For example, we obtain the same convergence rate provided that max{E‖es‖,E‖es‖2} ≤ γρ̃s for any γ ≥ 0 and some ρ̃ < ρ. Further, we still obtain a linear convergence rate as long as ‖es‖ converges to zero with a linear convergence rate.\nAlgorithm 1 Batching SVRG Input: initial vector x0, update frequency m, learning rate η. for s = 0, 1, 2, . . . do\nChoose batch size |Bs| Bs = |Bs| elements sampled without replacement from {1, 2, . . . , n}. µs = 1|Bs| ∑ i∈Bs f ′ i (x s) x0=xs for t = 1, 2, . . . ,m do\nRandomly pick it ∈ 1, . . . , n xt = xt−1 − η(f ′\nit (xt−1)− f\n′ it (xs) + µs) (∗)\nend for option I: set xs+1 = xm option II: set xs+1 = xt for random t ∈ {1, . . . ,m}\nend for"
    }, {
      "heading" : "3.1 Non-Uniform Sampling",
      "text" : "Xiao & Zhang [11] show that non-uniform sampling (NUS) improves the performance of SVRG. They assume each f ′i is Li-Lipschitz continuous, and sample it = i with probability Li/nL̄ where L̄ = 1n ∑n i=1 Li. The iteration is then changed to\nxt = xt−1 − η ( L̄\nLit [f\n′ it(xt−1)− f ′ it(x̃)] + µ s\n) ,\nwhich maintains that the search direction is unbiased. In Appendix A, we show that if µs is computed with error for this algorithm and if we set η andm so that 0 < ρ(L̄) < 1, then we have a convergence rate of\nE[f(xs+1)− f(x∗)] ≤ ρ(L̄)E[f(xs)− f(x∗)] + ZE‖e s‖+ ηE‖es‖2\n1− 2ηL̄ ,\nwhich can be faster since the average L̄ may be much smaller than the maximum value L."
    }, {
      "heading" : "3.2 SVRG with Batching",
      "text" : "There are many ways we could allow an error in the calculation of µs to speed up the algorithm. For example, if evaluating each f ′i involves solving an optimization problem, then we could solve this optimization problem inexactly. For example, if we are fitting a graphical model with an iterative approximate inference method, we can terminate the iterations early to save time.\nWhen the fi are simple but n is large, a natural way to approximate µs is with a subset (or ‘batch’) of training examples Bs (chosen without replacement),\nµs = 1 |Bs| ∑ i∈Bs f ′i(x s).\nThe batch size |Bs| controls the error in the approximation, and we can drive the error to zero by increasing it to n. Existing SVRG methods correspond to the special case where |Bs| = n for all s. Algorithm 1 gives pseudo-code for an SVRG implementation that uses this sub-sampling strategy. If we assume that the sample variance of the norms of the gradients is bounded by S2 for all xs,\n1\nn− 1 n∑ i=1 [ ‖f ′i(xs)‖2 − ‖f ′(xs)‖2 ] ≤ S2,\nthen we have that [12, Chapter 2]\nE‖es‖2 ≤ n− |B s|\nn|Bs| S2.\nSo if we want E‖es‖2 ≤ γρ̃2s, where γ ≥ 0 is a constant for some ρ̃ < 1, we need\n|Bs| ≥ nS 2\nS2 + nγρ̃2s . (3)\nAlgorithm 2 Mixed SVRG and SG Method Replace (*) in Algorithm 1 with the following lines: if fit ∈ Bs then xt = xt−1 − η(f ′\nit (xt−1)− f\n′ it (xs) + µs)\nelse xt = xt−1 − ηf ′\nit (xt−1)\nend if\nIf the batch size satisfies the above condition then\nZE‖es−1‖+ ηE‖es−1‖2 ≤ Z√γρ̃s + ηγρ̃2s\n≤ 2 max{Z√γ, ηγρ̃}ρ̃s,\nand the convergence rate of SVRG is unchanged compared to using the full batch on all iterations.\nThe condition (3) guarantees a linear convergence rate under any exponentially-increasing sequence of batch sizes, the strategy suggested by [13] for classic SG methods. However, a tedious calculation shows that (3) has an inflection point at s = log(S2/γn)/2 log(1/ρ̃), corresponding to |Bs| = n 2 . This was previously observed empirically [14, Figure 3], and occurs because we are sampling without replacement. This transition means we don’t need to increase the batch size exponentially."
    }, {
      "heading" : "4 Mixed SG and SVRG Method",
      "text" : "An approximate µs can drastically reduce the computational cost of the SVRG algorithm, but does not affect the 2 in the 2m+n gradients required form SVRG iterations. This factor of 2 is significant in the early iterations, since this is when stochastic methods make the most progress and when we typically see the largest reduction in the test error.\nTo reduce this factor, we can consider a mixed strategy: if it is in the batch Bs then perform an SVRG iteration, but if it is not in the current batch then use a classic SG iteration. We illustrate this modification in Algorithm 2. This modification allows the algorithm to take advantage of the rapid initial progress of SG, since it predominantly uses SG iterations when far from the solution. Below we give a convergence rate for this mixed strategy.\nProposition 2. Let µs = f ′(xs)+es and we set η andm so that 0 < ρ(L,αL) < 1 withα = |Bs|/n. If we assume E‖f ′i(x)‖2 ≤ σ2 then Algorithm 2 has\nE[f(xs+1)− f(x∗)] ≤ ρ(L,αL)E[f(xs)− f(x∗)] + ZE‖es‖+ ηE‖es‖2 + ησ\n2\n2 (1− α) 1− 2ηL\nWe give the proof in Appendix B. The extra term depending on the variance σ2 is typically the bottleneck for SG methods. Classic SG methods require the step-size η to converge to zero because of this term. However, the mixed SG/SVRG method can keep the fast progress from using a constant η since the term depending on σ2 converges to zero as α converges to one. Since α < 1 implies that ρ(L,αL) < ρ, this result implies that when [f(xs)− f(x∗)] is large compared to es and σ2 that the mixed SG/SVRG method actually converges faster.\nSharing a single step size η between the SG and SVRG iterations in Proposition 2 is sub-optimal. For example, if x is close to x∗ and |Bs| ≈ n, then the SG iteration might actually take us far away from the minimizer. Thus, we may want to use a decreasing sequence of step sizes for the SG iterations. In Appendix B, we show that using η = O∗( √ (n− |B|)/n|B|) for the SG iterations can improve the dependence on the error es and variance σ2."
    }, {
      "heading" : "5 Using Support Vectors",
      "text" : "Using a batch Bs decreases the number of gradient evaluations required when SVRG is far from the solution, but its benefit diminishes over time. However, for certain objectives we can further\nAlgorithm 3 Heuristic for skipping evaluations of fi at x if ski = 0 then\ncompute f ′i(x). if f ′i(x) = 0 then psi = psi + 1. {Update the number of consecutive times f ′i(x) was zero.} ski = 2\nmax{0,psi−2}. {Skip exponential number of future evaluations if it remains zero.} else psi = 0. {This could be a support vector, do not skip it next time.} end if return f ′i(x).\nelse ski = ski − 1. {In this case, we skip the evaluation.} return 0. end if\nreduce the number of gradient evaluations by identifying support vectors. For example, consider minimizing the Huberized hinge loss (HSVM) with threshold [15],\nmin x∈Rd\n1\nn n∑ i=1 f(bia T i x), f(τ) =  0 if τ > 1 + , 1− τ if τ < 1− , (1+ −τ)2\n4 if |1− τ | ≤ ,\nIn terms of (1), we have fi(x) = f(biaTi x). The performance of this loss function is similar to logistic regression and the hinge loss, but it has the appealing properties of both: it is differentiable like logistic regression meaning we can apply methods like SVRG, but it has support vectors like the hinge loss meaning that many examples will have fi(x∗) = 0 and f ′i(x\n∗) = 0. We can also construct Huberized variants of many non-smooth losses for regression and multi-class classification.\nIf we knew the support vectors where fi(x∗) > 0, we could solve the problem faster by ignoring the non-support vectors. For example, if there are 100000 training examples but only 100 support vectors in the optimal solution, we could solve the problem 1000 times faster. While we typically don’t know the support vectors, in this section we outline a heuristic that gives large practical improvements by trying to identify them as the algorithm runs.\nOur heuristic has two components. The first component is maintaining the list of non-support vectors at xs. Specifically, we maintain a list of examples i where f ′i(x\ns) = 0. When SVRG picks an example it that is part of this list, we know that f ′it(x\ns) = 0 and thus the iteration only needs one gradient evaluation. This modification is not a heuristic, in that it still applies the exact SVRG algorithm. However, at best it can only cut the runtime in half.\nThe heuristic part of our strategy is to skip f ′i(x s) or f ′i(xt) if our evaluation of f ′ i has been zero more than two consecutive times (and skipping it an exponentially larger number of times each time it remains zero). Specifically, for each example i we maintain two variables, ski (for ‘skip’) and psi (for ‘pass’). Whenever we need to evaluate f ′i for some x\ns or xt, we run Algorithm 3 which may skip the evaluation. This strategy can lead to huge computational savings in later iterations if there are few support vectors, since many iterations will require no gradient evaluations.\nIdentifying support vectors to speed up computation has long been an important part of SVM solvers, and is related to the classic shrinking heuristic [16]. While it has previously been explored in the context of dual coordinate ascent methods [17], this is the first work exploring it for linearly-convergent stochastic gradient methods."
    }, {
      "heading" : "6 Regularized SVRG",
      "text" : "We are often interested in the special case where problem (1) has the decomposition\nmin x∈Rd f(x) ≡ h(x) + 1 n n∑ i=1 gi(x). (4)\nA common choice of h is a scaled 1-norm of the parameter vector, h(x) = λ‖x‖1. This non-smooth regularizer encourages sparsity in the parameter vector, and can be addressed with the proximalSVRG method of Xiao & Zhang [11]. Alternately, if we want an explicit Z we could set h to the indicator function for a 2-norm ball containing x∗. In Appendix C, we give a variant of Proposition 1 that allows errors in the proximal-SVRG method for non-smooth/constrained settings like this.\nAnother common choice is the `2-regularizer, h(x) = λ2 ‖x‖ 2. With this regularizer, the SVRG updates can be equivalently written in the form xt+1 = xt − η ( h′(xt) + g ′ it(xt)− g ′ it(x s) + µs ) , (5)\nwhere µs = 1n ∑n i=1 gi(x\ns). That is, they take an exact gradient step with respect to the regularizer and an SVRG step with respect to the gi functions. When the g′i are sparse, this form of the update allows us to implement the iteration without needing full-vector operations. A related update is used by Le Roux et al. to avoid full-vector operations in the SAG algorithm [1, §4]. In Appendix C, we prove the below convergence rate for this update. Proposition 3. Consider instances of problem (1) that can be written in the form (4) where h′ is Lh-Lipschitz continuous and each g′i is Lg-Lipschitz continuous, and assume that we set η and m so that 0 < ρ(Lm) < 1 with Lm = max{Lg, Lh}. Then the regularized SVRG iteration (5) has\nE[f(xs+1)− f(x∗)] ≤ ρ(Lm)E[f(xs)− f(x∗)],\nSince Lm ≤ L, and strictly so in the case of `2-regularization, this result shows that for `2- regularized problems SVRG actually converges faster than the standard analysis would indicate (a similar result appears in Konečný et al. [18]). Further, this result gives a theoretical justification for using the update (5) for other h functions where it is not equivalent to the original SVRG method."
    }, {
      "heading" : "7 Mini-Batching Strategies",
      "text" : "Konečný et al. [18] have also recently considered using batches of data within SVRG. They consider using ‘mini-batches’ in the inner iteration (the update of xt) to decrease the variance of the method, but still use full passes through the data to compute µs. This prior work is thus complimentary to the current work (in practice, both strategies can be used to improve performance). In Appendix D we show that sampling the inner mini-batch proportional to Li achieves a convergence rate of\nE [ f(xs+1)− f(x∗) ] ≤ ρME [f(xs)− f(x∗)] ,\nwhere M is the size of the mini-batch while\nρM = 1\nM − 2ηL̄\n( M\nmµη + 2L̄η\n) ,\nand we assume 0 < ρM < 1. This generalizes the standard rate of SVRG and improves on the result of Konečný et al. [18] in the smooth case. This rate can be faster than the rate of the standard SVRG method at the cost of a more expensive iteration, and may be clearly advantageous in settings where parallel computation allows us to compute several gradients simultaneously.\nThe regularized SVRG form (5) suggests an alternate mini-batch strategy for problem (1): consider a mini-batch that contains a ‘fixed’ set Bf and a ‘random’ set Bt. Without loss of generality, assume that we sort the fi based on their Li values so that L1 ≥ L2 ≥ · · · ≥ Ln. For the fixed Bf we will always choose the Mf values with the largest Li, Bf = {f1, f2, . . . , fMf }. In contrast, we choose the members of the random set Bt by sampling from Br = {fMf+1, . . . , fn} proportional to their Lipschitz constants, pi = Li(Mr)L̄r with L̄r = (1/Mr) ∑n i=Mf+1\nLi. In Appendix D, we show the following convergence rate for this strategy: Proposition 4. Let g(x) = (1/n) ∑ i/∈[Bf ] fi(x) and h(x) = (1/n) ∑ i∈[Bf ] fi(x). If we replace the SVRG update with\nxt+1 = xt − η ( h′(xt) + (1/Mr)\n∑ i∈Bt L̄r Li (f ′i(xt)− f ′i(xs)) + g′(xs)\n) ,\nthen the convergence rate is E[f(xs+1)− f(x∗)] ≤ ρ(κ, ζ)E[F (xs)− f(x∗)].\nwhere ζ = (n−Mf )L̄r(M−Mf )n and κ = max{ L1 n , ζ}.\nIf L1 ≤ nL̄/M and Mf < (α−1)nMαn−M with α = L̄ L̄r\n, then we get a faster convergence rate than SVRG with a mini-batch of size M . The scenario where this rate is slower than the existing minibatch SVRG strategy is when L1 ≤ nL̄/M . But we could relax this assumption by dividing each element of the fixed set Bf into two functions: βfi and (1− β)fi, where β = 1/M , then replacing each function fi in Bf with βfi and adding (1 − β)fi to the random set Br. This result may be relevant if we have access to a field-programmable gate array (FPGA) or graphical processing unit (GPU) that can compute the gradient for a fixed subset of the examples very efficiently. However, our experiments (Appendix F) indicate this strategy only gives marginal gains.\nIn Appendix F, we also consider constructing mini-batches by sampling proportional to fi(xs) or ‖f ′i(xs)‖. These seemed to work as well as Lipschitz sampling on all but one of the datasets in our experiments, and this strategy is appealing because we have access to these values while we may not know the Li values. However, these strategies diverged on one of the datasets."
    }, {
      "heading" : "8 Learning efficiency",
      "text" : "In this section we compare the performance of SVRG as a large-scale learning algorithm compared to FG and SG methods. Following Bottou & Bousquet [19], we can formulate the generalization error E of a learning algorithm as the sum of three terms\nE = Eapp + Eest + Eopt where the approximation error Eapp measures the effect of using a limited class of models, the estimation error Eest measures the effect of using a finite training set, and the optimization error Eopt measures the effect of inexactly solving problem (1). Bottou & Bousquet [19] study asymptotic performance of various algorithms for a fixed approximation error and under certain conditions on the distribution of the data depending on parameters α or ν. In Appendix E, we discuss how SVRG can be analyzed in their framework. The table below includes SVRG among their results.\nAlgorithm Time to reach Eopt ≤ Time to reach E = O(Eapp + ) Previous with κ ∼ n FG O ( nκd log ( 1 )) O ( d2κ 1/α log2 ( 1 )) O ( d3 2/α log3 ( 1 )) SG O ( dνκ2 ) O ( dνκ2 ) O ( d3ν log 2 ( 1\n)) SVRG O ( (n+ κ)d log ( 1 )) O ( d2 1/α log2 ( 1 ) + κd log ( 1 )) O ( d2 1/α log2 ( 1 ))\nIn this table, the condition number is κ = L/µ. In this setting, linearly-convergent stochastic gradient methods can obtain better bounds for ill-conditioned problems, with a better dependence on the dimension and without depending on the noise variance ν."
    }, {
      "heading" : "9 Experimental Results",
      "text" : "In this section, we present experimental results that evaluate our proposed variations on the SVRG method. We focus on logistic regression classification: given a set of training data (a1, b1) . . . (an, bn) where ai ∈ Rd and bi ∈ {−1,+1}, the goal is to find the x ∈ Rd solving\nargmin x∈Rd\nλ 2 ‖x‖2 + 1 n n∑ i=1 log(1 + exp(−biaTi x)),\nWe consider the datasets used by [1], whose properties are listed in the supplementary material. As in their work we add a bias variable, normalize dense features, and set the regularization parameter λ to 1/n. We used a step-size of α = 1/L and we usedm = |Bs|which gave good performance across methods and datasets. In our first experiment, we compared three variants of SVRG: the original strategy that uses all n examples to form µs (Full), a growing batch strategy that sets |Bs| = 2s (Grow), and the mixed SG/SVRG described by Algorithm 2 under this same choice (Mixed). While a variety of practical batching methods have been proposed in the literature [13, 20, 21], we did not find that any of these strategies consistently outperformed the doubling used by the simple Grow\nstrategy. Our second experiment focused on the `2-regularized HSVM on the same datasets, and we compared the original SVRG algorithm with variants that try to identify the support vectors (SV).\nWe plot the experimental results for one run of the algorithms on one dataset in Figure 1, while Appendix F reports results on the other 8 datasets over 10 different runs. In our results, the growing batch strategy (Grow) always had better test error performance than using the full batch, while for large datasets it also performed substantially better in terms of the training objective. In contrast, the Mixed strategy sometimes helped performance and sometimes hurt performance. Utilizing support vectors often improved the training objective, often by large margins, but its effect on the test objective was smaller."
    }, {
      "heading" : "10 Discussion",
      "text" : "As SVRG is the only memory-free method among the new stochastic linearly-convergent methods, it represents the natural method to use for a huge variety of machine learning problems. In this work we show that the convergence rate of the SVRG algorithm can be preserved even under an inexact approximation to the full gradient. We also showed that using mini-batches to approximate µs gives a natural way to do this, explored the use of support vectors to further reduce the number of gradient evaluations, gave an analysis of the regularized SVRG update, and considered several new mini-batch strategies. Our theoretical and experimental results indicate that many of these simple modifications should be considered in any practical implementation of SVRG."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank the reviewers for their helpful comments. This research was supported by the Natural Sciences and Engineering Research Council of Canada (RGPIN 312176-2010, RGPIN 311661-08, RGPIN-06068-2015). Jakub Konečný is supported by a Google European Doctoral Fellowship."
    } ],
    "references" : [ {
      "title" : "A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets",
      "author" : [ "N. Le Roux", "M. Schmidt", "F. Bach" ],
      "venue" : "Advances in neural information processing systems (NIPS), 2012.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "S. Shalev-Schwartz", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Research, vol. 14, pp. 567–599, 2013.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Optimization with first-order surrogate functions",
      "author" : [ "J. Mairal" ],
      "venue" : "International Conference on Machine Learning (ICML), 2013.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "Advances in neural information processing systems (NIPS), 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Mixed optimization for smooth functions",
      "author" : [ "M. Mahdavi", "L. Zhang", "R. Jin" ],
      "venue" : "Advances in neural information processing systems (NIPS), 2013.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "Advances in neural information processing systems (NIPS), 2013.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Linear convergence with condition number independent access of full gradients",
      "author" : [ "L. Zhang", "M. Mahdavi", "R. Jin" ],
      "venue" : "Advances in neural information processing systems (NIPS), 2013.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Semi-stochastic gradient descent methods",
      "author" : [ "J. Konečný", "P. Richtárik" ],
      "venue" : "arXiv preprint, 2013.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convergence rates of inexact proximal-gradient methods for convex optimization",
      "author" : [ "M. Schmidt", "N. Le Roux", "F. Bach" ],
      "venue" : "Advances in neural information processing systems (NIPS), 2011.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Accelerated gradient methods for stochastic optimization and online learning",
      "author" : [ "C. Hu", "J. Kwok", "W. Pan" ],
      "venue" : "Advances in neural information processing systems (NIPS), 2009.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "L. Xiao", "T. Zhang" ],
      "venue" : "SIAM Journal on Optimization, vol. 24, no. 2, pp. 2057–2075, 2014.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sampling: design and analysis",
      "author" : [ "S. Lohr" ],
      "venue" : "Cengage Learning,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Hybrid deterministic-stochastic methods for data fitting",
      "author" : [ "M.P. Friedlander", "M. Schmidt" ],
      "venue" : "SIAM Journal of Scientific Computing, vol. 34, no. 3, pp. A1351–A1379, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Robust inversion, dimensionality reduction, and randomized sampling",
      "author" : [ "A. Aravkin", "M.P. Friedlander", "F.J. Herrmann", "T. Van Leeuwen" ],
      "venue" : "Mathematical Programming, vol. 134, no. 1, pp. 101–125, 2012.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Piecewise linear regularized solution paths",
      "author" : [ "S. Rosset", "J. Zhu" ],
      "venue" : "The Annals of Statistics, vol. 35, no. 3, pp. 1012–1030, 2007.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Making large-scale SVM learning practical",
      "author" : [ "T. Joachims" ],
      "venue" : "Advances in Kernel Methods - Support Vector Learning (B. Schölkopf, C. Burges, and A. Smola, eds.), ch. 11, pp. 169–184, Cambridge, MA: MIT Press, 1999.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Guarantees for approximate incremental svms",
      "author" : [ "N. Usunier", "A. Bordes", "L. Bottou" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "ms2gd: Mini-batch semi-stochastic gradient descent in the proximal setting",
      "author" : [ "J. Konečný", "J. Liu", "P. Richtárik", "M. Takáč" ],
      "venue" : "arXiv preprint, 2014.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "L. Bottou", "O. Bousquet" ],
      "venue" : "Advances in neural information processing systems (NIPS), 2007.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Sample size selection in optimization methods for machine learning",
      "author" : [ "R.H. Byrd", "G.M. Chin", "J. Nocedal", "Y. Wu" ],
      "venue" : "Mathematical programming, vol. 134, no. 1, pp. 127–155, 2012.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive and stochastic algorithms for EIT and DC resistivity problems with piecewise constant solutions and many measurements",
      "author" : [ "K. van den Doel", "U. Ascher" ],
      "venue" : "SIAM J. Scient. Comput, vol. 34, 2012. 9",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1] proposed the first general method, stochastic average gradient (SAG), that only considers one training example on each iteration but still achieves a linear convergence rate.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Other methods have subsequently been shown to have this property [2, 3, 4], but these all require storing a previous evaluation of the gradient f ′ i or the dual variables for each i.",
      "startOffset" : 65,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "Other methods have subsequently been shown to have this property [2, 3, 4], but these all require storing a previous evaluation of the gradient f ′ i or the dual variables for each i.",
      "startOffset" : 65,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Other methods have subsequently been shown to have this property [2, 3, 4], but these all require storing a previous evaluation of the gradient f ′ i or the dual variables for each i.",
      "startOffset" : 65,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Recently, several methods have been proposed with similar convergence rates to SAG but without the memory requirements [5, 6, 7, 8].",
      "startOffset" : 119,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "Recently, several methods have been proposed with similar convergence rates to SAG but without the memory requirements [5, 6, 7, 8].",
      "startOffset" : 119,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "Recently, several methods have been proposed with similar convergence rates to SAG but without the memory requirements [5, 6, 7, 8].",
      "startOffset" : 119,
      "endOffset" : 131
    }, {
      "referenceID" : 7,
      "context" : "Recently, several methods have been proposed with similar convergence rates to SAG but without the memory requirements [5, 6, 7, 8].",
      "startOffset" : 119,
      "endOffset" : 131
    }, {
      "referenceID" : 5,
      "context" : "Johnson & Zhang [6] show that if η and m are chosen such that 0 < ρ < 1, the algorithm achieves a linear convergence rate of the form E[f(x)− f(x∗)] ≤ ρE[f(x)− f(x∗)], where x∗ is the optimal solution.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "This is in the spirit of the analysis of [9], who analyze FG methods under similar assumptions.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "We assume that ‖xt−x‖ ≤ Z for all t, which has been used in related work [10] and is reasonable because of the coercity implied by strong-convexity.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "Xiao & Zhang [11] show that non-uniform sampling (NUS) improves the performance of SVRG.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 12,
      "context" : "The condition (3) guarantees a linear convergence rate under any exponentially-increasing sequence of batch sizes, the strategy suggested by [13] for classic SG methods.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 14,
      "context" : "For example, consider minimizing the Huberized hinge loss (HSVM) with threshold [15],",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "Identifying support vectors to speed up computation has long been an important part of SVM solvers, and is related to the classic shrinking heuristic [16].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 16,
      "context" : "While it has previously been explored in the context of dual coordinate ascent methods [17], this is the first work exploring it for linearly-convergent stochastic gradient methods.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "This non-smooth regularizer encourages sparsity in the parameter vector, and can be addressed with the proximalSVRG method of Xiao & Zhang [11].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "[18] have also recently considered using batches of data within SVRG.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Following Bottou & Bousquet [19], we can formulate the generalization error E of a learning algorithm as the sum of three terms E = Eapp + Eest + Eopt",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "Bottou & Bousquet [19] study asymptotic performance of various algorithms for a fixed approximation error and under certain conditions on the distribution of the data depending on parameters α or ν.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "We consider the datasets used by [1], whose properties are listed in the supplementary material.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "While a variety of practical batching methods have been proposed in the literature [13, 20, 21], we did not find that any of these strategies consistently outperformed the doubling used by the simple Grow",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "While a variety of practical batching methods have been proposed in the literature [13, 20, 21], we did not find that any of these strategies consistently outperformed the doubling used by the simple Grow",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 20,
      "context" : "While a variety of practical batching methods have been proposed in the literature [13, 20, 21], we did not find that any of these strategies consistently outperformed the doubling used by the simple Grow",
      "startOffset" : 83,
      "endOffset" : 95
    } ],
    "year" : 2015,
    "abstractText" : "We present and analyze several strategies for improving the performance of stochastic variance-reduced gradient (SVRG) methods. We first show that the convergence rate of these methods can be preserved under a decreasing sequence of errors in the control variate, and use this to derive variants of SVRG that use growing-batch strategies to reduce the number of gradient calculations required in the early iterations. We further (i) show how to exploit support vectors to reduce the number of gradient computations in the later iterations, (ii) prove that the commonly–used regularized SVRG iteration is justified and improves the convergence rate, (iii) consider alternate mini-batch selection strategies, and (iv) consider the generalization error of the method.",
    "creator" : null
  }
}