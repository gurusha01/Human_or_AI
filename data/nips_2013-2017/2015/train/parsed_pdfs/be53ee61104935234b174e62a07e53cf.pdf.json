{
  "name" : "be53ee61104935234b174e62a07e53cf.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Fast, Provable Algorithms for Isotonic Regression in all L_p-norms",
    "authors" : [ "Anup Rao", "Sushant Sachdeva" ],
    "emails" : [ "rasmus.kyng@yale.edu", "arao89@gatech.edu", "sachdeva@cs.yale.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A directed acyclic graph (DAG) G(V,E) defines a partial order on V where u precedes v if there is a directed path from u to v. We say that a vector x ∈ RV is isotonic (with respect to G) if it is a weakly order-preserving mapping of V into R. Let IG denote the set of all x that are isotonic with respect to G. It is immediate that IG can be equivalently defined as follows:\nIG = {x ∈ RV | xu ≤ xv for all (u, v) ∈ E}. (1) Given a DAG G, and a norm ‖·‖ on RV , the Isotonic Regression of observations y ∈ RV , is given by x ∈ IG that minimizes ‖x− y‖ . Such monotonic relationships are fairly common in data. They allow one to impose only weak assumptions on the data, e.g. the typical height of a young girl child is an increasing function of her age, and the heights of her parents, rather than a more constrained parametric model.\nIsotonic Regression is an important shape-constrained nonparametric regression method that has been studied since the 1950’s [1, 2, 3]. It has applications in diverse fields such as Operations Research [4, 5] and Signal Processing [6]. In Statistics, it has several applications (e.g. [7, 8]), and the statistical properties of Isotonic Regression under the `2-norm have been well studied, particularly over linear orderings (see [9] and references therein). More recently, Isotonic regression has found several applications in Learning [10, 11, 12, 13, 14]. It was used by Kalai and Sastry [10] to provably learn Generalized Linear Models and Single Index Models; and by Zadrozny and Elkan [13], and Narasimhan and Agarwal [14] towards constructing binary Class Probability Estimation models.\nThe most common norms of interest are weighted `p-norms, defined as\n‖z‖w,p =\n{(∑ v∈V w p v · |zv|p )1/p , p ∈ [1,∞),\nmaxv∈V wv · |zv|, p =∞, where wv > 0 is the weight of a vertex v ∈ V. In this paper, we focus on algorithms for Isotonic Regression under weighted `p-norms. Such algorithms have been applied to large data-sets from Microarrays [15], and from the web [16, 17]. ∗Code from this work is available at https://github.com/sachdevasushant/Isotonic †Part of this work was done when this author was a graduate student at Yale University.\nGiven a DAGG, and observations y ∈ RV , our regression problem can be expressed as the following convex program:\nmin ‖x− y‖w,p such that xu ≤ xv for all (u, v) ∈ E. (2)"
    }, {
      "heading" : "1.1 Our Results",
      "text" : "Let |V | = n, and |E| = m. We’ll assume that G is connected, and hence m ≥ n− 1. `p-norms, p < ∞. We give a unified, optimization-based framework for algorithms that provably solve the Isotonic Regression problem for p ∈ [1,∞). The following is an informal statement of our main theorem (Theorem 3.1) in this regard (assuming wv are bounded by poly(n)). Theorem 1.1 (Informal). There is an algorithm that, given a DAG G, observations y, and δ > 0, runs in time O(m1.5 log2 n log n/δ), and computes an isotonic xALG ∈ IG such that\n‖xALG − y‖pw,p ≤ minx∈IG ‖x− y‖pw,p + δ.\nThe previous best time bounds were O(nm log n 2\nm ) for p ∈ (1,∞) [18] and O(nm + n 2 log n) for\np = 1 [19].\n`∞-norms. For `∞-norms, unlike `p-norms for p ∈ (1,∞), the Isotonic Regression problem need not have a unique solution. There are several specific solutions that have been studied in the literature (see [20] for a detailed discussion). In this paper, we show that some of them (MAX, MIN, and AVG to be precise) can be computed in time linear in the size of G. Theorem 1.2. There is an algorithm that, given a DAG G(V,E), a set of observations y ∈ RV , and weights w, runs in expected time O(m), and computes an isotonic xINF ∈ IG such that\n‖xINF − y‖w,∞ = minx∈IG ‖x− y‖w,∞ .\nOur algorithm achieves the best possible running time. This was not known even for linear or tree orders. The previous best running time was O(m log n) [20].\nStrict Isotonic Regression. We also give improved algorithms for Strict Isotonic Regression. Given observations y, and weights w, its Strict Isotonic Regression xSTRICT is defined to be the limit of x̂p as p goes to ∞, where x̂p is the Isotonic Regression for y under the norm ‖·‖w,p . It is immediate that xStrict is an `∞ Isotonic Regression for y. In addition, it is unique and satisfies several desirable properties (see [21]). Theorem 1.3. There is an algorithm that, given a DAG G(V,E), a set of observation y ∈ RV , and weights w, runs in expected time O(mn), and computes xSTRICT, the strict Isotonic Regression of y.\nThe previous best running time was O(min(mn, nω) + n2 log n) [21]."
    }, {
      "heading" : "1.2 Detailed Comparison to Previous Results",
      "text" : "`p-norms, p <∞. There has been a lot of work for fast algorithms for special graph families, mostly for p = 1, 2 (see [22] for references). For some cases where G is very simple, e.g. a directed path (corresponding to linear orders), or a rooted, directed tree (corresponding to tree orders), several works give algorithms with running times of O(n) or O(n log n) (see [22] for references).\nTheorem 1.1 not only improves on the previously best known algorithms for general DAGs, but also on several algorithms for special graph families (see Table 1). One such setting is where V is a point set in d-dimensions, and (u, v) ∈ E whenever ui ≤ vi for all i ∈ [d]. This setting has applications to data analysis, as in the example given earlier, and has been studied extensively (see [23] for references). For this case, it was proved by Stout (see Prop. 2, [23]) that these partial orders can be embedded in a DAG with O(n logd−1 n) vertices and edges, and that this DAG can be computed in time linear in its size. The bounds then follow by combining this result with our theorem above.\nWe obtain improved running times for all `p norms for DAGs with m = o(n2/ log6 n), and for d-dim point sets for d ≥ 3. For d = 2, Stout [19] gives an O(n log2 n) time algorithm.\n`∞-norms. For weighted `∞-norms on arbitrary DAGs, the previous best result was O(m log n + n log2 n) due to Kaufman and Tamir [24]. A manuscript by Stout [20] improves it to O(m log n). These algorithms are based on parametric search, and are impractical. Our algorithm is simple, achieves the best possible running time, and only requires random sampling and topological sort.\nIn a parallel independent work, Stout [25] gives O(n)-time algorithms for linear order, trees, and d-grids, and an O(n logd−1 n) algorithm for point sets in d-dimensions. Theorem 1.2 implies the linear-time algorithms immediately. The result for d-dimensional point sets follows after embedding the point sets into DAGs of size O(n logd−1 n), as for `p-norms.\nStrict Isotonic Regression. Strict Isotonic regression was introduced and studied in [21]. It also gave the only previous algorithm for computing it, that runs in time O(min(mn, nω) + n2 log n). Theorem 1.3 is an improvement when m = o(n log n)."
    }, {
      "heading" : "1.3 Overview of the Techniques and Contribution",
      "text" : "`p-norms, p < ∞. It is immediate that Isotonic Regression, as formulated in Equation (2), is a convex programming problem. For weighted `p-norms with p < ∞, applying generic convexprogramming algorithms such as Interior Point methods to this formulation leads to algorithms that are quite slow.\nWe obtain faster algorithms for Isotonic Regression by replacing the computationally intensive component of Interior Point methods, solving systems of linear equations, with approximate solves. This approach has been used to design fast algorithms for generalized flow problems [26, 27, 28].\nWe present a complete proof of an Interior Point method for a large class of convex programs that only requires approximate solves. Daitch and Spielman [26] had proved such a result for linear programs. We extend this to `p-objectives, and provide an improved analysis that only requires linear solvers with a constant factor relative error bound, whereas the method from Daitch and Spielman required polynomially small error bounds.\nThe linear systems in [27, 28] are Symmetric Diagonally Dominant (SDD) matrices. The seminal work of Spielman and Teng [29] gives near-linear time approximate solvers for such systems, and later research has improved these solvers further [30, 31]. Daitch and Spielman [26] extended these solvers to M-matrices (generalizations of SDD). The systems we need to solve are neither SDD, nor M-matrices. We develop fast solvers for this new class of matrices using fast SDD solvers. We stress that standard techniques for approximate inverse computation, e.g. Conjugate Gradient, are not sufficient for approximately solving our systems in near-linear time. These methods have at least a square root dependence on the condition number, which inevitably becomes huge in IPMs.\n`∞-norms and Strict Isotonic Regression. Algorithms for `∞-norms and Strict Isotonic Regression are based on techniques presented in a recent paper of Kyng et al. [32]. We reduce `∞-norm Isotonic Regression to the following problem, referred to as Lipschitz learning on directed graphs in [32] (see Section 4 for details) : We have a directed graph H, with edge lengths given by len. Given x ∈ RV (H), for every (u, v) ∈ E(H), define grad+G[x](u, v) = max { x(u)−x(v) len(u,v) , 0 } . Now,\ngiven y that assigns real values to a subset of V (H), the goal is to determine x ∈ RV (H) that agrees with y and minimizes max(u,v)∈E(H) grad + G[x](u, v).\nThe above problem is solved in O(m + n log n) time for general directed graphs in [32]. We give a simple linear-time reduction to the above problem with the additional property that H is a DAG. For DAGs, their algorithm can be implemented to run in O(m+ n) time.\nIt is proved in [21] that computing the Strict Isotonic Regression is equivalent to computing the isotonic vector that minimizes the error under the lexicographic ordering (see Section 4). Under the same reduction as in the `∞-case, we show that this is equivalent to minimizing grad+ under the lexicographic ordering. It is proved in [32] that the lex-minimizer can be computed with basically n calls to `∞-minimization, immediately implying our result."
    }, {
      "heading" : "1.4 Further Applications",
      "text" : "The IPM framework that we introduce to design our algorithm for Isotonic Regression (IR), and the associated results, are very general, and can be applied as-is to other problems. As a concrete application, the algorithm of Kakade et al. [12] for provably learning Generalized Linear Models and Single Index Models learns 1-Lipschitz monotone functions on linear orders in O(n2) time (procedure LPAV). The structure of the associated convex program resembles IR. Our IPM results and solvers immediately imply an n1.5 time algorithm (up to log factors).\nImproved algorithms for IR (or for learning Lipschitz functions) on d-dimensional point sets could be applied towards learning d-dim multi-index models where the link-function is nondecreasing w.r.t. the natural ordering on d-variables, extending [10, 12]. They could also be applied towards constructing Class Probability Estimation (CPE) models from multiple classifiers, by finding a mapping from multiple classifier scores to a probabilistic estimate, extending [13, 14].\nOrganization. We report experimental results in Section 2. An outline of the algorithms and analysis for `p-norms, p <∞, are presented in Section 3. In Section 4, we define the Lipschitz regression problem on DAGs, and give the reduction from `∞-norm Isotonic Regression. We defer a detailed description of the algorithms, and most proofs to the accompanying supplementary material."
    }, {
      "heading" : "2 Experiments",
      "text" : "An important advantage of our algorithms is that they can be implemented quite efficiently. Our algorithms are based on what is known as a short-step method (see Chapter 11, [33]), that leads to an O( √ m) bound on the number of iterations. Each iteration corresponds to one linear solve in the Hessian matrix. A variant, known as the long-step method (see [33]) typically require much fewer iterations, about logm, even though the only provable bound known is O(m).\n0 2 4 6 8 10\nx 10 4\n0\n10\n20\n30\n40\n50\n60\n70\n80\nNumber of Vertices\nT im\ne i n\nS e c s\nRunning Time in Practice\nGrid−1 Grid−10 RandReg−1 RandReg−10 For the important special case of `2-Isotonic Regression, we have implemented our algorithm in Matlab, with long step barrier method, combined with our approximate solver for the linear systems involved. A number of heuristics recommended in [33] that greatly improve the running time in practice have also been incorporated. Despite the changes, our implementation is theoretically correct and also outputs an upper bound on the error by giving a feasible point to the dual program. Our implementation is available at https://github.com/ sachdevasushant/Isotonic.\nIn the figure, we plot average running times (with error bars denoting standard deviation) for `2-Isotonic Regression on DAGs, where the underlying graphs are 2-d grid graphs and random regular graphs (of constant degree). The edges for 2-d grid graphs are all oriented towards one of the corners. For random regular graphs, the edges are oriented according to a random permutation. The vector of initial observations y is chosen to be a random permutation of 1 to n obeying the partial order, perturbed by adding i.i.d. Gaussian noise to each coordinate. For each graph size, and two different noise levels (standard deviation for the noise on each coordinate being 1 or 10), the experiment is repeated multiple time. The relative error in the objective was ascertained to be less than 1%.\n3 Algorithms for `p-norms, p <∞\nWithout loss of generality, we assume y ∈ [0, 1]n. Given p ∈ [1,∞), let p-ISO denote the following `p-norm Isotonic Regression problem, and OPTp-ISO denote its optimum:\nmin x∈IG\n‖x− y‖pw,p . (3)\nLet wp denote the entry-wise pth power of w. We assume the minimum entry of wp is 1, and the maximum entry is wpmax ≤ exp(n). We also assume the additive error parameter δ is lower bounded by exp(−n), and that p ≤ exp(n). We use the Õ notation to hide poly log log n factors. Theorem 3.1. Given a DAG G(V,E), a set of observations y ∈ [0, 1]V , weights w, and an error parameter δ > 0, the algorithm ISOTONICIPM runs in time Õ ( m1.5 log2 n log (npw p max/δ) ) , and with probability at least 1− 1/n, outputs a vector xALG ∈ IG with\n‖xALG − y‖pw,p ≤ OPTp-ISO + δ.\nThe algorithm ISOTONICIPM is obtained by an appropriate instantiation of a general Interior Point Method (IPM) framework which we call APPROXIPM.\nTo state the general IPM result, we need to introduce two important concepts. These concepts are defined formally in Supplementary Material Section A.1. The first concept is self-concordant barrier functions; we denote the class of these functions by SCB. A self-concordant barrier function f is a special convex function defined on some convex domain set S. The function approaches infinity at the boundary of S. We associate with each f a complexity parameter θ(f) which measures how well-behaved f is. The second important concept is the symmetry of a point z w.r.t. S: A nonnegative scalar quantity sym(z, S). A large symmetry value guarantees that a point is not too close to the boundary of the set. For our algorithms to work, we need a starting point whose symmetry is not too small. We later show that such a starting point can be constructed for the p-ISO problem.\nAPPROXIPM is a primal path following IPM: Given a vector c, a domain D and a barrier function f ∈ SCB for D, we seek to compute minx∈D 〈c, x〉 . To find a minimizer, we consider a function fc,γ(x) = f(x) + γ 〈c, x〉, and attempt to minimize fc,γ for changing values of γ by alternately updating x and γ. As x approaches the boundary of D the f(x) term grows to infinity and with some care, we can use this to ensure we never move to a point x outside the feasible domain D. As we increase γ, the objective term 〈c, x〉 contributes more to fc,γ . Eventually, for large enough γ, the objective value 〈c, x〉 of the current point x will be close to the optimum of the program. To stay near the optimum x for each new value of γ, we use a second-order method (Newton steps) to update x when γ is changed. This means that we minimize a local quadratic approximation to our objective. This requires solving a linear systemHz = g, where g andH are the gradient and Hessian of f at x respectively. Solving this system to find z is the most computationally intensive aspect of the algorithm. Crucially we ensure that crude approximate solutions to the linear system suffices, allowing the algorithm to use fast approximate solvers for this step. APPROXIPM is described in detail in Supplementary Material Section A.5, and in this section we prove the following theorem.\nTheorem 3.2. Given a convex bounded domain D ⊆ IRn and vector c ∈ IRn, consider the program\nmin x∈D 〈c, x〉 . (4)\nLet OPT denote the optimum of the program. Let f ∈ SCB be a self-concordant barrier function for D. Given a initial point x0 ∈ D, a value upper bound K ≥ sup{〈c, x〉 : x ∈ D}, a symmetry lower bound s ≤ sym(x0, D), and an error parameter 0 < < 1, the algorithm APPROXIPM runs for\nTapx = O (√ θ(f) log (θ(f)/ ·s) )\niterations and returns a point xapx, which satisfies 〈c,xapx〉−OPT K−OPT ≤ .\nThe algorithm requires O(Tapx) multiplications of vectors by a matrix M(x) satisfying 9/10 · H(x)−1 M(x) 11/10 · H(x)−1, where H(x) is the Hessian of f at various points x ∈ D specified by the algorithm.\nWe now reformulate the p-ISO program to state a version which can be solved using the APPROXIPM framework. Consider points (x, t) ∈ IRn × IRn, and define a set\nDG = {(x, t) : for all v ∈ V . |x(v)− y(v)|p − t(v) ≤ 0} . To ensure boundedness, as required by APPROXIPM, we add the constraint 〈wp , t〉 ≤ K. Definition 3.3. We define the domain DK = (IG × IRn) ∩DG ∩ {(x, t) : 〈wp , t〉 ≤ K} .\nThe domain DK is convex, and allows us to reformulate program (3) with a linear objective: min x,t 〈wp , t〉 such that (x, t) ∈ DK . (5)\nOur next lemma determines a choice of K which suffices to ensure that programs (3) and (5) have the same optimum. The lemma is proven in Supplementary Material Section A.4. Lemma 3.4. For all K ≥ 3nwpmax, DK is non-empty and bounded, and the optimum of program (5) is OPTp-ISO.\nThe following result shows that for program (5) we can compute a good starting point for the path following IPM efficiently. The algorithm GOODSTART computes a starting point in linear time by running a topological sort on the vertices of the DAG G and assigning values to x according to the vertex order of the sort. Combined with an appropriate choice of t, this suffices to give a starting point with good symmetry. The algorithm GOODSTART is specified in more detail in Supplementary Material Section A.4, together with a proof of the following lemma. Lemma 3.5. The algorithm GOODSTART runs in time O(m) and returns an initial point (x0, t0) that is feasible, and for K = 3nwpmax, satisfies sym((x0, t0),DK) ≥ 118n2pwpmax .\nCombining standard results on self-concordant barrier functions with a barrier for p-norms developed by Hertog et al. [34], we can show the following properties of a function FK whose exact definition is given in Supplementary Material Section A.2. Corollary 3.6. The function FK is a self-concordant barrier for DK and it has complexity parameter θ(FK) = O(m). Its gradient gFK is computable in O(m) time, and an implicit representation of the Hessian HFK can be computed in O(m) time as well.\nThe key reason we can use APPROXIPM to give a fast algorithm for Isotonic Regression is that we develop an efficient solver for linear equations in the Hessian of FK . The algorithm HESSIANSOLVE solves linear systems in Hessian matrices of the barrier function FK . The Hessian is composed of a structured main component plus a rank one matrix. We develop a solver for the main component by doing a change of variables to simplify its structure, and then factoring the matrix by a blockwise LDL>-decomposition. We can solve straightforwardly in the L and L>, and we show that the D factor consists of blocks that are either diagonal or SDD, so we can solve in this factor approximately using a nearly-linear time SDD solver. The algorithm HESSIANSOLVE is given in full in Supplementary Material Section A.3, along with a proof of the following result. Theorem 3.7. For any instance of program (5) given by some (G, y), at any point z ∈ DK , for any vector a, HESSIANSOLVE((G, y), z, µ, a) returns a vector b =Ma for a symmetric linear operator M satisfying 9/10 ·HFK (z)−1 M 11/10 ·HFK (z)−1. The algorithm fails with probability < µ. HESSIANSOLVE runs in time Õ(m log n log(1/µ)).\nThese are the ingredients we need to prove our main result on solving p-ISO. The algorithm ISOTONICIPM is simply APPROXIPM instantiated to solve program (5), with an appropriate choice of parameters. We state ISOTONICIPM informally as Algorithm 1 below. ISOTONICIPM is given in full as Algorithm 6 in Supplementary Material Section A.5.\nProof of Theorem 3.1: ISOTONICIPM uses the symmetry lower bound s = 1 18n2pwpmax , the value upper bound K = 3nwpmax, and the error parameter = δK when calling APPROXIPM. By Corollary 3.6, the barrier function FK used by ISOTONICIPM has complexity parameter θ(FK) ≤ O(m). By Lemma 3.5 the starting point (x0, t0) computed by GOODSTART and used by ISOTONICIPM is feasible and has symmetry sym(x0,DK) ≥ 118n2pwpmax .\nBy Theorem 3.2 the point (xapx, tapx) output by ISOTONICIPM satisfies 〈wp ,tapx〉−OPT\nK−OPT ≤ , where OPT is the optimum of program (5), and K = 3nwpmax is the value used by ISOTONICIPM for the\nconstraint 〈wp , t〉 ≤ K, which is an upper bound on the supremum of objective values of feasible points of program (5). By Lemma 3.4, OPT = OPTp-ISO. Hence, ‖y − xapx‖pp ≤ 〈w\np , tapx〉 ≤ OPT+ K = OPTp-ISO + δ.\nAgain, by Theorem 3.2, the number of calls to HESSIANSOLVE by ISOTONICIPM is bounded by O(T ) ≤ O (√ θ(FK) log (θ(FK)/ ·s) ) ≤ O (√ m log (npw p max/δ) ) .\nEach call to HESSIANSOLVE fails with probability < n−3. Thus, by a union bound, the probability that some call to HESSIANSOLVE fails is upper bounded by O( √ m log(npwpmax/δ))/n3 = O(1/n). The algorithm usesO ( √ m log (npw p max/δ)) calls to HESSIANSOLVE that each take time Õ(m log2 n),\nas µ = n3. Thus the total running time is Õ ( m1.5 log2 n log (npw p max/δ) ) .\nAlgorithm 1: Sketch of Algorithm ISOTONICIPM\n1. Pick a starting point (x, t) using the GOODSTART algorithm 2. for r = 1, 2 3. if r = 1 then γ ← −1; ρ← 1; c = − gradient of f at (x, t) 4. else γ ← 1; ρ← 1/poly(n); c = (0,wp) 5. for i← 1, . . . , C1m0.5 logm : 6. ρ← ρ · (1 + γC2m−0.5) 7. Let H, g be the Hessian and gradient of fc,ρ at x 8. Call HESSIANSOLVE to compute z ≈ H−1g 9. Update x← x− z\n10. Return x."
    }, {
      "heading" : "4 Algorithms for `∞ and Strict Isotonic Regression",
      "text" : "We now reduce `∞ Isotonic Regression and Strict Isotonic Regression to the Lipschitz Learning problem, as defined in [32]. Let G = (V,E, len) be any DAG with non-negative edge lengths len : E → R≥0, and y : V → R∪ {∗} a partial labeling. We think of a partial labeling as a function that assigns real values to a subset of the vertex set V . We call such a pair (G, y) a partially-labeled DAG. For a complete labeling x : V → R, define the gradient on an edge (u, v) ∈ E due to x to be grad+G[x](u, v) = max { x(u)−x(v) len(u,v) , 0 } . If len(u, v) = 0, then grad+G[x](u, v) = 0 unless x(u) > x(v), in which case it is defined as +∞. Given a partially-labelled DAG (G, y), we say that a complete assignment x is an inf-minimizer if it extends y, and for all other complete assignments x′ that extends y we have\nmax (u,v)∈E grad+G[x](u, v) ≤ max (u,v)∈E grad+G[x ′](u, v).\nNote that when len = 0, then max(u,v)∈E grad + G[x](u, v) <∞ if and only if x is isotonic on G.\nSuppose we are interested in Isotonic Regression on a DAG G(V,E) under ‖·‖w,∞. To reduce this problem to that of finding an inf-minimizer, we add some auxiliary nodes and edges toG. Let VL, VR be two copies of V . That is, for every vertex u ∈ V , add a vertex uL to VL and a vertex uR to VR. Let EL = {(uL, u)}u∈V and ER = {(u, uR)}u∈V . We then let len′(uL, u) = 1/w(u) and len′(u, uR) = 1/w(u). All other edge lengths are set to 0. Finally, let G′ = (V ∪ VL ∪ VR, E ∪ EL ∪ ER, len′). The partial assignment y′ takes real values only on the the vertices in VL ∪ VR. For all u ∈ V , y′(uL) := y(u), y′(uR) := y(u) and y′(u) := ∗. (G′, y′) is our partially-labeled DAG. Observe that G′ has n′ = 3n vertices and m′ = m+ 2n edges. Lemma 4.1. Given a DAG G(V,E), a set of observations y ∈ RV , and weights w, construct G′ and y′ as above. Let x be an inf-minimizer for the partially-labeled DAG (G′, y′). Then, x |V is the Isotonic Regression of y with respect to G under the norm ‖·‖w,∞ .\nProof. We note that since the vertices corresponding to V in (G′, y′) are connected to each other by zero length edges, max(u,v)∈E grad + G[x](u, v) < ∞ iff x is isotonic on those edges. Since G is a DAG, we know that there are isotonic labelings on G. When x is isotonic on vertices corresponding to V , gradient is zero on all the edges going in between vertices in V . Also, note that every vertex\nx corresponding to V in G′ is attached to two auxiliary nodes xL ∈ VL, xR ∈ VR. We also have y′(xL) = y\n′(xR) = y(x). Thus, for any x that extends y and is Isotonic on G′, the only non-zero entries in grad+ correspond to edges in ER and EL, and thus\nmax (u,v)∈E′ grad+G′ [x](u, v) = max u∈V wu · |y(u)− x(u)| = ‖x− y‖w,∞ .\nAlgorithm COMPINFMIN from [32] is proved to compute the inf-minimizer, and is claimed to work for directed graphs (Section 5, [32]). We exploit the fact that Dijkstra’s algorithm in COMPINFMIN can be implemented in O(m) time on DAGs using a topological sorting of the vertices, giving a linear time algorithm for computing the inf-minimizer. Combining it with the reduction given by the lemma above, and observing that the size ofG′ isO(m+n),we obtain Theorem 1.2. A complete description of the modified COMPINFMIN is given in Section B.2. We remark that the solution to the `∞-Isotonic Regression that we obtain has been referred to as AVG `∞ Isotonic Regression in the literature [20]. It is easy to modify the algorithm to compute the MAX, MIN `∞ Isotonic Regressions. Details are given in Section B.\nFor Strict Isotonic Regression, we define the lexicographic ordering. Given r ∈ Rm, let πr denote a permutation that sorts r in non-increasing order by absolute value, i.e., ∀i ∈ [m− 1], |r(πr(i))| ≥ |r(πr(i+ 1))|. Given two vectors r, s ∈ Rm, we write r lex s to indicate that r is smaller than s in the lexicographic ordering on sorted absolute values, i.e.\n∃j ∈ [m], |r(πr(j))| < |s(πs(j))| and ∀i ∈ [j − 1], |r(πr(i))| = |s(πs(i))| or ∀i ∈ [m], |r(πr(i))| = |s(πs(i))| .\nNote that it is possible that r lex s and s lex r while r 6= s. It is a total relation: for every r and s at least one of r lex s or s lex r is true. Given a partially-labelled DAG (G, y), we say that a complete assignment x is a lex-minimizer if it extends y and for all other complete assignments x′ that extend y we have grad+G[x] lex grad + G[x ′]. Stout [21] proves that computing the Strict Isotonic Regression is equivalent to finding an Isotonic x that minimizes zu = wu · (xu − yu) in the lexicographic ordering. With the same reduction as above, it is immediate that this is equivalent to minimizing grad+G′ in the lex-ordering. Lemma 4.2. Given a DAG G(V,E), a set of observations y ∈ RV , and weights w, construct G′ and y′ as above. Let x be the lex-minimizer for the partially-labeled DAG (G′, y′). Then, x |V is the Strict Isotonic Regression of y with respect to G with weights w.\nAs for inf-minimization, we give a modification of the algorithm COMPLEXMIN from [32] that computes the lex-minimizer in O(mn) time. The algorithm is described in Section B.2. Combining this algorithm with the reduction from Lemma 4.2, we can compute the Strict Isotonic Regression in O(m′n′) = O(mn) time, thus proving Theorem 1.3.\nAcknowledgements. We thank Sabyasachi Chatterjee for introducing the problem to us, and Daniel Spielman for his advice and comments. We would also like to thank Quentin Stout and anonymous reviewers for their suggestions. This research was partially supported by AFOSR Award FA955012-1-0175, NSF grant CCF-1111257, and a Simons Investigator Award to Daniel Spielman."
    } ],
    "references" : [ {
      "title" : "An empirical distribution function for sampling with incomplete information",
      "author" : [ "M. Ayer", "H.D. Brunk", "G.M. Ewing", "W.T. Reid", "E. Silverman" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1955
    }, {
      "title" : "Statistical inference under order restrictions: the theory and application of Isotonic Regression",
      "author" : [ "D.J. Barlow", "R. E", "Bartholomew", "J.M. Bremner", "H.D. Brunk" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1972
    }, {
      "title" : "An algorithm for monotone Regression with one or more independent variables",
      "author" : [ "F. Gebhardt" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1970
    }, {
      "title" : "Establishing consistent and realistic reorder intervals in productiondistribution systems",
      "author" : [ "W.L. Maxwell", "J.A. Muckstadt" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1985
    }, {
      "title" : "A 98%-effective lot-sizing rule for a multi-product, multi-stage production / inventory system",
      "author" : [ "R. Roundy" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1986
    }, {
      "title" : "Nonlinear image estimation using piecewise and local image models",
      "author" : [ "S.T. Acton", "A.C. Bovik" ],
      "venue" : "Image Processing, IEEE Transactions on,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "The Min-Max algorithm and Isotonic Regression",
      "author" : [ "C.I.C. Lee" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1983
    }, {
      "title" : "An algorithm for Isotonic Regression for two or more independent variables",
      "author" : [ "R.L. Dykstra", "T. Robertson" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1982
    }, {
      "title" : "The isotron algorithm: High-dimensional Isotonic Regression",
      "author" : [ "A.T. Kalai", "R. Sastry" ],
      "venue" : "In COLT,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Intervalrank: Isotonic Regression with listwise and pairwise constraints",
      "author" : [ "T. Moon", "A. Smola", "Y. Chang", "Z. Zheng" ],
      "venue" : "In WSDM,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Efficient learning of generalized linear and single index models with Isotonic Regression",
      "author" : [ "S. M Kakade", "V. Kanade", "O. Shamir", "A. Kalai" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Transforming classifier scores into accurate multiclass probability estimates",
      "author" : [ "B. Zadrozny", "C. Elkan" ],
      "venue" : "KDD, pages 694–699,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "On the relationship between binary classification, bipartite ranking, and binary class probability estimation",
      "author" : [ "H. Narasimhan", "S. Agarwal" ],
      "venue" : "In NIPS",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Weighted Isotonic Regression under the l1 norm",
      "author" : [ "S. Angelov", "B. Harb", "S. Kannan", "L. Wang" ],
      "venue" : "In SODA,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Enhanced hierarchical classification via Isotonic smoothing",
      "author" : [ "K. Punera", "J. Ghosh" ],
      "venue" : "In WWW,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2008
    }, {
      "title" : "Query-level learning to rank using Isotonic Regression",
      "author" : [ "Z. Zheng", "H. Zha", "G. Sun" ],
      "venue" : "In Communication,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Minimizing a convex cost closure set",
      "author" : [ "D.S. Hochbaum", "M. Queyranne" ],
      "venue" : "SIAM Journal on Discrete Mathematics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2003
    }, {
      "title" : "Isotonic Regression via partitioning",
      "author" : [ "Q.F. Stout" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Weighted l∞",
      "author" : [ "Q.F. Stout" ],
      "venue" : "Isotonic Regression. Manuscript,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Strict l∞ Isotonic Regression",
      "author" : [ "Q.F. Stout" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Isotonic Regression for multiple independent variables",
      "author" : [ "Q.F. Stout" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Locating service centers with precedence constraints",
      "author" : [ "Y. Kaufman", "A. Tamir" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1993
    }, {
      "title" : "L infinity Isotonic Regression for linear, multidimensional, and tree orders",
      "author" : [ "Q.F. Stout" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2015
    }, {
      "title" : "Faster approximate lossy generalized flow via interior point algorithms",
      "author" : [ "S.I. Daitch", "D.A. Spielman" ],
      "venue" : "STOC ’08,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    }, {
      "title" : "Navigating central path with electrical flows",
      "author" : [ "A. Madry" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Path finding methods for linear programming: Solving linear programs in Õ(  √ rank) iterations and faster algorithms for maximum flow",
      "author" : [ "Y.T. Lee", "A. Sidford" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems",
      "author" : [ "D.A. Spielman", "S. Teng" ],
      "venue" : "STOC ’04,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2004
    }, {
      "title" : "A nearly-m logn time solver for SDD linear systems",
      "author" : [ "I. Koutis", "G.L. Miller", "R. Peng" ],
      "venue" : "FOCS ’11,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2011
    }, {
      "title" : "Solving SDD linear systems in nearly m log n time",
      "author" : [ "M.B. Cohen", "R. Kyng", "G.L. Miller", "J.W. Pachocki", "R. Peng", "A.B. Rao", "S.C. Xu" ],
      "venue" : "STOC ’14,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2014
    }, {
      "title" : "Algorithms for Lipschitz learning on graphs",
      "author" : [ "R. Kyng", "A. Rao", "S. Sachdeva", "D.A. Spielman" ],
      "venue" : "In Proceedings of COLT",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2004
    }, {
      "title" : "A sufficient condition for self-concordance",
      "author" : [ "D. den Hertog", "F. Jarre", "C. Roos", "T. Terlaky" ],
      "venue" : "Math. Program.,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 1995
    }, {
      "title" : "A mathematical view of interior-point methods in convex optimization",
      "author" : [ "J. Renegar" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2001
    }, {
      "title" : "Lecure notes: Interior point polynomial time methods in convex programming",
      "author" : [ "A. Nemirovski" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2004
    }, {
      "title" : "Extension of range of functions",
      "author" : [ "E.J. McShane" ],
      "venue" : "Bull. Amer. Math. Soc., 40(12):837–842,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 1934
    }, {
      "title" : "Analytic extensions of differentiable functions defined in closed sets",
      "author" : [ "H. Whitney" ],
      "venue" : "Transactions of the American Mathematical Society,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 1934
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Isotonic Regression is an important shape-constrained nonparametric regression method that has been studied since the 1950’s [1, 2, 3].",
      "startOffset" : 125,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "Isotonic Regression is an important shape-constrained nonparametric regression method that has been studied since the 1950’s [1, 2, 3].",
      "startOffset" : 125,
      "endOffset" : 134
    }, {
      "referenceID" : 2,
      "context" : "Isotonic Regression is an important shape-constrained nonparametric regression method that has been studied since the 1950’s [1, 2, 3].",
      "startOffset" : 125,
      "endOffset" : 134
    }, {
      "referenceID" : 3,
      "context" : "It has applications in diverse fields such as Operations Research [4, 5] and Signal Processing [6].",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "It has applications in diverse fields such as Operations Research [4, 5] and Signal Processing [6].",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "It has applications in diverse fields such as Operations Research [4, 5] and Signal Processing [6].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "[7, 8]), and the statistical properties of Isotonic Regression under the `2-norm have been well studied, particularly over linear orderings (see [9] and references therein).",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "[7, 8]), and the statistical properties of Isotonic Regression under the `2-norm have been well studied, particularly over linear orderings (see [9] and references therein).",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "More recently, Isotonic regression has found several applications in Learning [10, 11, 12, 13, 14].",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "More recently, Isotonic regression has found several applications in Learning [10, 11, 12, 13, 14].",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "More recently, Isotonic regression has found several applications in Learning [10, 11, 12, 13, 14].",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "More recently, Isotonic regression has found several applications in Learning [10, 11, 12, 13, 14].",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "More recently, Isotonic regression has found several applications in Learning [10, 11, 12, 13, 14].",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "It was used by Kalai and Sastry [10] to provably learn Generalized Linear Models and Single Index Models; and by Zadrozny and Elkan [13], and Narasimhan and Agarwal [14] towards constructing binary Class Probability Estimation models.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "It was used by Kalai and Sastry [10] to provably learn Generalized Linear Models and Single Index Models; and by Zadrozny and Elkan [13], and Narasimhan and Agarwal [14] towards constructing binary Class Probability Estimation models.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "It was used by Kalai and Sastry [10] to provably learn Generalized Linear Models and Single Index Models; and by Zadrozny and Elkan [13], and Narasimhan and Agarwal [14] towards constructing binary Class Probability Estimation models.",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 13,
      "context" : "Such algorithms have been applied to large data-sets from Microarrays [15], and from the web [16, 17].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "Such algorithms have been applied to large data-sets from Microarrays [15], and from the web [16, 17].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 15,
      "context" : "Such algorithms have been applied to large data-sets from Microarrays [15], and from the web [16, 17].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "The previous best time bounds were O(nm log n 2 m ) for p ∈ (1,∞) [18] and O(nm + n 2 log n) for p = 1 [19].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 17,
      "context" : "The previous best time bounds were O(nm log n 2 m ) for p ∈ (1,∞) [18] and O(nm + n 2 log n) for p = 1 [19].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 18,
      "context" : "There are several specific solutions that have been studied in the literature (see [20] for a detailed discussion).",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "The previous best running time was O(m log n) [20].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : "In addition, it is unique and satisfies several desirable properties (see [21]).",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 19,
      "context" : "The previous best running time was O(min(mn, n) + n(2) log n) [21].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 20,
      "context" : "This setting has applications to data analysis, as in the example given earlier, and has been studied extensively (see [23] for references).",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "2, [23]) that these partial orders can be embedded in a DAG with O(n logd−1 n) vertices and edges, and that this DAG can be computed in time linear in its size.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 17,
      "context" : "For d = 2, Stout [19] gives an O(n log(2) n) time algorithm.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 17,
      "context" : "Previous best This paper `1 `p, 1 < p <∞ `p, p <∞ d-dim vertex set, d ≥ 3 n(2) log n [19] n(2) log n [19] n log n arbitrary DAG nm+ n(2) log n [15] nm log n 2 m [18] m 1.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "Previous best This paper `1 `p, 1 < p <∞ `p, p <∞ d-dim vertex set, d ≥ 3 n(2) log n [19] n(2) log n [19] n log n arbitrary DAG nm+ n(2) log n [15] nm log n 2 m [18] m 1.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : "Previous best This paper `1 `p, 1 < p <∞ `p, p <∞ d-dim vertex set, d ≥ 3 n(2) log n [19] n(2) log n [19] n log n arbitrary DAG nm+ n(2) log n [15] nm log n 2 m [18] m 1.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "Previous best This paper `1 `p, 1 < p <∞ `p, p <∞ d-dim vertex set, d ≥ 3 n(2) log n [19] n(2) log n [19] n log n arbitrary DAG nm+ n(2) log n [15] nm log n 2 m [18] m 1.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 21,
      "context" : "For weighted `∞-norms on arbitrary DAGs, the previous best result was O(m log n + n log(2) n) due to Kaufman and Tamir [24].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "A manuscript by Stout [20] improves it to O(m log n).",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : "In a parallel independent work, Stout [25] gives O(n)-time algorithms for linear order, trees, and d-grids, and an O(n logd−1 n) algorithm for point sets in d-dimensions.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 19,
      "context" : "Strict Isotonic regression was introduced and studied in [21].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 23,
      "context" : "This approach has been used to design fast algorithms for generalized flow problems [26, 27, 28].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 24,
      "context" : "This approach has been used to design fast algorithms for generalized flow problems [26, 27, 28].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 25,
      "context" : "This approach has been used to design fast algorithms for generalized flow problems [26, 27, 28].",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "Daitch and Spielman [26] had proved such a result for linear programs.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 24,
      "context" : "The linear systems in [27, 28] are Symmetric Diagonally Dominant (SDD) matrices.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "The linear systems in [27, 28] are Symmetric Diagonally Dominant (SDD) matrices.",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 26,
      "context" : "The seminal work of Spielman and Teng [29] gives near-linear time approximate solvers for such systems, and later research has improved these solvers further [30, 31].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 27,
      "context" : "The seminal work of Spielman and Teng [29] gives near-linear time approximate solvers for such systems, and later research has improved these solvers further [30, 31].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 28,
      "context" : "The seminal work of Spielman and Teng [29] gives near-linear time approximate solvers for such systems, and later research has improved these solvers further [30, 31].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 23,
      "context" : "Daitch and Spielman [26] extended these solvers to M-matrices (generalizations of SDD).",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 29,
      "context" : "We reduce `∞-norm Isotonic Regression to the following problem, referred to as Lipschitz learning on directed graphs in [32] (see Section 4 for details) : We have a directed graph H, with edge lengths given by len.",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 29,
      "context" : "The above problem is solved in O(m + n log n) time for general directed graphs in [32].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 19,
      "context" : "It is proved in [21] that computing the Strict Isotonic Regression is equivalent to computing the isotonic vector that minimizes the error under the lexicographic ordering (see Section 4).",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 29,
      "context" : "It is proved in [32] that the lex-minimizer can be computed with basically n calls to `∞-minimization, immediately implying our result.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "[12] for provably learning Generalized Linear Models and Single Index Models learns 1-Lipschitz monotone functions on linear orders in O(n(2)) time (procedure LPAV).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "the natural ordering on d-variables, extending [10, 12].",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 10,
      "context" : "the natural ordering on d-variables, extending [10, 12].",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "They could also be applied towards constructing Class Probability Estimation (CPE) models from multiple classifiers, by finding a mapping from multiple classifier scores to a probabilistic estimate, extending [13, 14].",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 12,
      "context" : "They could also be applied towards constructing Class Probability Estimation (CPE) models from multiple classifiers, by finding a mapping from multiple classifier scores to a probabilistic estimate, extending [13, 14].",
      "startOffset" : 209,
      "endOffset" : 217
    }, {
      "referenceID" : 30,
      "context" : "Our algorithms are based on what is known as a short-step method (see Chapter 11, [33]), that leads to an O( √ m) bound on the number of iterations.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 30,
      "context" : "A variant, known as the long-step method (see [33]) typically require much fewer iterations, about logm, even though the only provable bound known is O(m).",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 30,
      "context" : "A number of heuristics recommended in [33] that greatly improve the running time in practice have also been incorporated.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 31,
      "context" : "[34], we can show the following properties of a function FK whose exact definition is given in Supplementary Material Section A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "We now reduce `∞ Isotonic Regression and Strict Isotonic Regression to the Lipschitz Learning problem, as defined in [32].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 29,
      "context" : "Algorithm COMPINFMIN from [32] is proved to compute the inf-minimizer, and is claimed to work for directed graphs (Section 5, [32]).",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 29,
      "context" : "Algorithm COMPINFMIN from [32] is proved to compute the inf-minimizer, and is claimed to work for directed graphs (Section 5, [32]).",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 18,
      "context" : "We remark that the solution to the `∞-Isotonic Regression that we obtain has been referred to as AVG `∞ Isotonic Regression in the literature [20].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "Stout [21] proves that computing the Strict Isotonic Regression is equivalent to finding an Isotonic x that minimizes zu = wu · (xu − yu) in the lexicographic ordering.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 29,
      "context" : "As for inf-minimization, we give a modification of the algorithm COMPLEXMIN from [32] that computes the lex-minimizer in O(mn) time.",
      "startOffset" : 81,
      "endOffset" : 85
    } ],
    "year" : 2015,
    "abstractText" : "Given a directed acyclic graphG, and a set of values y on the vertices, the Isotonic Regression of y is a vector x that respects the partial order described by G, and minimizes ‖x− y‖ , for a specified norm. This paper gives improved algorithms for computing the Isotonic Regression for all weighted `p-norms with rigorous performance guarantees. Our algorithms are quite practical, and variants of them can be implemented to run fast in practice.",
    "creator" : null
  }
}