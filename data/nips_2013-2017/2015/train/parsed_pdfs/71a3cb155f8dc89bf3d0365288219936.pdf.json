{
  "name" : "71a3cb155f8dc89bf3d0365288219936.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Non-convex Statistical Optimization for Sparse Tensor Graphical Model",
    "authors" : [ "Wei Sun", "Zhaoran Wang", "Guang Cheng" ],
    "emails" : [ "sunweisurrey@yahoo-inc.com", "zhaoran@princeton.edu", "hanliu@princeton.edu", "chengg@stat.purdue.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "High-dimensional tensor-valued data are prevalent in many fields such as personalized recommendation systems and brain imaging research [1, 2]. Traditional recommendation systems are mainly based on the user-item matrix, whose entry denotes each user’s preference for a particular item. To incorporate additional information into the analysis, such as the temporal behavior of users, we need to consider a user-item-time tensor. For another example, functional magnetic resonance imaging (fMRI) data can be viewed as a three way (third-order) tensor since it contains the brain measurements taken on different locations over time for various experimental conditions. Also, in the example of microarray study for aging [3], thousands of gene expression measurements are recorded on 16 tissue types on 40 mice with varying ages, which forms a four way gene-tissue-mouse-age tensor.\nIn this paper, we study the estimation of conditional independence structure within tensor data. For example, in the microarray study for aging we are interested in the dependency structure across different genes, tissues, ages and even mice. Assuming data are drawn from a tensor normal distribution, a straightforward way to estimate this structure is to vectorize the tensor and estimate the underlying Gaussian graphical model associated with the vector. Such an approach ignores the tensor structure\nand requires estimating a rather high dimensional precision matrix with insufficient sample size. For instance, in the aforementioned fMRI application the sample size is one if we aim to estimate the dependency structure across different locations, time and experimental conditions. To address such a problem, a popular approach is to assume the covariance matrix of the tensor normal distribution is separable in the sense that it is the Kronecker product of small covariance matrices, each of which corresponds to one way of the tensor. Under this assumption, our goal is to estimate the precision matrix corresponding to each way of the tensor. See §1.1 for a detailed survey of previous work.\nDespite the fact that the assumption of the Kronecker product structure of covariance makes the statistical model much more parsimonious, it poses significant challenges. In particular, the penalized negative log-likelihood function is non-convex with respect to the unknown sparse precision matrices. Consequently, there exists a gap between computational and statistical theory. More specifically, as we will show in §1.1, existing literature mostly focuses on establishing the existence of a local optimum that has desired statistical guarantees, rather than offering efficient algorithmic procedures that provably achieve the desired local optima. In contrast, we analyze an alternating minimization algorithm which iteratively minimizes the non-convex objective function with respect to each individual precision matrix while fixing the others. The established theoretical guarantees of the proposed algorithm are as follows. Suppose that we have n observations from a K-th order tensor normal distribution. We denote by mk, sk, dk (k = 1, . . . ,K) the dimension, sparsity, and max number of non-zero entries in each row of the precision matrix corresponding to the k-th way of the tensor. Besides, we define m =\nQK k=1 mk. The k-th precision matrix estimator from our alternating\nminimization algorithm achieves a p\nmk(mk + sk) logmk/(nm) statistical rate of convergence in Frobenius norm, which is minimax-optimal since this is the best rate one can obtain even when the rest K 1 true precision matrices are known [4]. Furthermore, under an extra irrepresentability condition, we establish a p\nmk logmk/(nm) rate of convergence in max norm, which is also optimal, and a dk p\nmk logmk/(nm) rate of convergence in spectral norm. These estimation consistency results and a sufficiently large signal strength condition further imply the model selection consistency of recovering all the edges. A notable implication of these results is that, when K 3, our alternating minimization algorithm can achieve estimation consistency in Frobenius norm even if we only have access to one tensor sample, which is often the case in practice. This phenomenon is unobserved in previous work. Finally, we conduct extensive experiments to evaluate the numerical performance of the proposed alternating minimization method. Under the guidance of theory, we propose a way to significantly accelerate the algorithm without sacrificing the statistical accuracy."
    }, {
      "heading" : "1.1 Related work and our contribution",
      "text" : "A special case of our sparse tensor graphical model when K = 2 is the sparse matrix graphical model, which is studied by [5–8]. In particular, [5] and [6] only establish the existence of a local optima with desired statistical guarantees. Meanwhile, [7] considers an algorithm that is similar to ours. However, the statistical rates of convergence obtained by [6, 7] are much slower than ours when K = 2. See Remark 3.6 in §3.1 for a detailed comparison. For K = 2, our statistical rate of convergence in Frobenius norm recovers the result of [5]. In other words, our theory confirms that the desired local optimum studied by [5] not only exists, but is also attainable by an efficient algorithm. In addition, for matrix graphical model, [8] establishes the statistical rates of convergence in spectral and Frobenius norms for the estimator attained by a similar algorithm. Their results achieve estimation consistency in spectral norm with only one matrix observation. However, their rate is slower than ours with K = 2. See Remark 3.11 in §3.2 for a detailed discussion. Furthermore, we allow K to increase and establish estimation consistency even in Frobenius norm for n = 1. Most importantly, all these results focus on matrix graphical model and can not handle the aforementioned motivating applications such as the gene-tissue-mouse-age tensor dataset.\nIn the context of sparse tensor graphical model with a general K, [9] shows the existence of a local optimum with desired rates, but does not prove whether there exists an efficient algorithm that provably attains such a local optimum. In contrast, we prove that our alternating minimization algorithm achieves an estimator with desired statistical rates. To achieve it, we apply a novel theoretical framework to separately consider the population and sample optimizers, and then establish the onestep convergence for the population optimizer (Theorem 3.1) and the optimal rate of convergence for the sample optimizer (Theorem 3.4). A new concentration result (Lemma B.1) is developed for this purpose, which is also of independent interest. Moreover, we establish additional theoretical\nguarantees including the optimal rate of convergence in max norm, the estimation consistency in spectral norm, and the graph recovery consistency of the proposed sparse precision matrix estimator.\nIn addition to the literature on graphical models, our work is also closely related to a recent line of research on alternating minimization for non-convex optimization problems [10–13]. These existing results mostly focus on problems such as dictionary learning, phase retrieval and matrix decomposition. Hence, our statistical model and analysis are completely different from theirs. Also, our paper is related to a recent line of work on tensor decomposition. See, e.g., [14–17] and the references therein. Compared with them, our work focuses on the graphical model structure within tensor-valued data.\nNotation: For a matrix A = (Ai,j) 2 Rd⇥d, we denote kAk1, kAk2, kAkF as its max, spectral, and Frobenius norm, respectively. We define kAk 1,off := P\ni 6=j |Ai,j | as its off-diagonal `1 norm and |||A|||1 := maxi P\nj |Ai,j | as the maximum absolute row sum. Denote vec(A) as the vectorization of A which stacks the columns of A. Let tr(A) be the trace of A. For an index set S = {(i, j), i, j 2 {1, . . . , d}}, we define [A]S as the matrix whose entry indexed by (i, j) 2 S is equal to Ai,j , and zero otherwise. We denote 1d as the identity matrix with dimension d⇥ d. Throughout this paper, we use C,C\n1 , C 2 , . . . to denote generic absolute constants, whose values may vary from line to line."
    }, {
      "heading" : "2 Sparse tensor graphical model",
      "text" : ""
    }, {
      "heading" : "2.1 Preliminary",
      "text" : "We employ the tensor notations used by [18]. Throughout this paper, higher order tensors are denoted by boldface Euler script letters, e.g. T . We consider a K-th order tensor T 2 Rm1⇥m2⇥···⇥mK . When K = 1 it reduces to a vector and when K = 2 it reduces to a matrix. The (i\n1 , . . . , iK)-th element of the tensor T is denoted to be Ti1,...,iK . Meanwhile, we define the vectorization of T as vec(T ) := (T\n1,1,...,1, . . . , Tm1,1,...,1, . . . , T1,m2,...,mK , Tm1,m2,...,mK ) > 2 Rm with m = Q k mk. In addition, we define the Frobenius norm of a tensor T as kT kF := P\ni1,...,iK T\n2\ni1,...,iK\n1/2 .\nFor tensors, a fiber refers to the higher order analogue of the row and column of matrices. A fiber is obtained by fixing all but one of the indices of the tensor, e.g., the mode-k fiber of T\n(k) is given by Ti1,...,,ik 1,:,ik+1,...,iK . Matricization, also known as unfolding, is the process to transform a tensor into a matrix. We denote T\n(k) as the mode-k matricization of a tensor T , which arranges the mode-k fibers to be the columns of the resulting matrix. Another useful operation in tensors is the k-mode product. The k-mode product of a tensor T 2 Rm1⇥m2⇥···⇥mK with a matrix A 2 RJ⇥mk is denoted as T ⇥kA and is of the size m1⇥ · · ·⇥mk 1⇥J⇥mk+1⇥ · · ·⇥mK . Its entry is defined as (T ⇥k A)i1,...,ik 1,j,ik+1,...,iK := Pmk ik=1\nTi1,...,iKAj,ik . In addition, for a list of matrices {A1, . . . ,AK} with Ak 2 Rmk⇥mk , k = 1, . . . ,K, we define T ⇥ {A1, . . . ,AK} := T ⇥1 A1 ⇥2 · · ·⇥K AK ."
    }, {
      "heading" : "2.2 Model",
      "text" : "A tensor T 2 Rm1⇥m2⇥···⇥mK follows the tensor normal distribution with zero mean and covariance matrices ⌃\n1 , . . . ,⌃K , denoted as T ⇠ TN(0;⌃1, . . . ,⌃K), if its probability density function is\np(T |⌃ 1 , . . . ,⌃K) = (2⇡) m/2\n⇢ K Y\nk=1\n|⌃k| m/(2mk) exp\nkT ⇥⌃ 1/2k2F /2 , (2.1)\nwhere m = QK k=1 mk and ⌃ 1/2 := {⌃ 1/2 1 , . . . ,⌃ 1/2K }. When K = 1, this tensor normal distribution reduces to the vector normal distribution with zero mean and covariance ⌃\n1 . According to [9, 18], it can be shown that T ⇠ TN(0;⌃\n1 , . . . ,⌃K) if and only if vec(T ) ⇠ N(vec(0);⌃K ⌦ · · ·⌦⌃\n1 ), where vec(0) 2 Rm and ⌦ is the matrix Kronecker product. We consider the parameter estimation for the tensor normal model. Assume that we observe independently and identically distributed tensor samples T\n1 , . . . , Tn from TN(0;⌃⇤ 1 , . . . ,⌃⇤K). We aim to estimate the true covariance matrices (⌃⇤\n1 , . . . ,⌃⇤K) and their corresponding true precision matrices (⌦⇤\n1 , . . . ,⌦⇤K) where ⌦ ⇤ k = ⌃ ⇤ 1 k (k = 1, . . . ,K). To address the identifiability issue in\nthe parameterization of the tensor normal distribution, we assume that k⌦⇤kkF = 1 for k = 1, . . . ,K. This renormalization assumption does not change the graph structure of the original precision matrix.\nA standard approach to estimate ⌦⇤k, k = 1, . . . ,K, is to use the maximum likelihood method via (2.1). Up to a constant, the negative log-likelihood function of the tensor normal distribution is tr[S(⌦K ⌦ · · · ⌦ ⌦1)] PK k=1(m/mk) log |⌦k|, where S := 1 n Pn i=1 vec(Ti)vec(Ti)\n>. To encourage the sparsity of each precision matrix in the high-dimensional scenario, we consider a penalized log-likelihood estimator, which is obtained by minimizing\nqn(⌦1, . . . ,⌦K) := 1 m tr[S(⌦K ⌦ · · ·⌦⌦1)]\nK X\nk=1\n1\nmk log |⌦k|+\nK X\nk=1\nP k(⌦k), (2.2)\nwhere P k(·) is a penalty function indexed by the tuning parameter k. In this paper, we focus on the lasso penalty [19], i.e., P k(⌦k) = kk⌦kk1,off. This estimation procedure applies similarly to a broad family of other penalty functions.\nWe name the penalized model from (2.2) as the sparse tensor graphical model. It reduces to the sparse vector graphical model [20, 21] when K = 1, and the sparse matrix graphical model [5–8] when K = 2. Our framework generalizes them to fulfill the demand of capturing the graphical structure of higher order tensor-valued data."
    }, {
      "heading" : "2.3 Estimation",
      "text" : "This section introduces the estimation procedure for the sparse tensor graphical model. A computationally efficient algorithm is provided to estimate the precision matrix for each way of the tensor.\nRecall that in (2.2), qn(⌦1, . . . ,⌦K) is jointly non-convex with respect to ⌦1, . . . ,⌦K . Nevertheless, qn(⌦1, . . . ,⌦K) is a bi-convex problem since qn(⌦1, . . . ,⌦K) is convex in ⌦k when the rest K 1 precision matrices are fixed. The bi-convex property plays a critical role in our algorithm construction and its theoretical analysis in §3.\nAccording to its bi-convex property, we propose to solve this non-convex problem by alternatively update one precision matrix with other matrices fixed. Note that, for any k = 1, . . . ,K, minimizing (2.2) with respect to ⌦k while fixing the rest K 1 precision matrices is equivalent to minimizing\nL(⌦k) := 1 mk tr(Sk⌦k)\n1\nmk log |⌦k|+ kk⌦kk1,off. (2.3)\nHere Sk := mknm Pn i=1 V k i V k> i , where Vki := ⇥ Ti ⇥ ⌦1/2 1 , . . . ,⌦1/2k 1,1mk ,⌦ 1/2 k+1, . . . ,⌦ 1/2 K ⇤ (k) with ⇥ the tensor product operation and [·] (k) the mode-k matricization operation defined in §2.1. The\nresult in (2.3) can be shown by noting that Vki = [Ti](k) ⌦1/2K ⌦ · · ·⌦⌦ 1/2 k+1⌦⌦ 1/2 k 1⌦ · · ·⌦⌦ 1/2 1\n>\naccording to the properties of mode-k matricization shown by [18]. Hereafter, we drop the superscript k of Vki if there is no confusion. Note that minimizing (2.3) corresponds to estimating vector-valued Gaussian graphical model and can be solved efficiently via the glasso algorithm [21].\nAlgorithm 1 Solve sparse tensor graphical model via Tensor lasso (Tlasso) 1: Input: Tensor samples T\n1 . . . , Tn, tuning parameters 1, . . . , K , max number of iterations T . 2: Initialize ⌦(0)\n1 , . . . ,⌦(0)K randomly as symmetric and positive definite matrices and set t = 0. 3: Repeat: 4: t = t+ 1. 5: For k = 1, . . . ,K: 6: Given ⌦(t)\n1 , . . . ,⌦(t)k 1,⌦ (t 1) k+1 , . . . ,⌦ (t 1) K , solve (2.3) for ⌦ (t) k via glasso [21].\n7: Normalize ⌦(t)k such that k⌦ (t) k kF = 1. 8: End For 9: Until t = T .\n10: Output: b⌦k = ⌦(T )k (k = 1, . . . ,K).\nThe details of our Tensor lasso (Tlasso) algorithm are shown in Algorithm 1. It starts with a random initialization and then alternatively updates each precision matrix until it converges. In §3, we will illustrate that the statistical properties of the obtained estimator are insensitive to the choice of the initialization (see the discussion following Theorem 3.5)."
    }, {
      "heading" : "3 Theory of statistical optimization",
      "text" : "We first prove the estimation errors in Frobenius norm, max norm, and spectral norm, and then provide the model selection consistency of our Tlasso estimator. We defer all the proofs to the appendix."
    }, {
      "heading" : "3.1 Estimation error in Frobenius norm",
      "text" : "Based on the penalized log-likelihood in (2.2), we define the population log-likelihood function as\nq(⌦ 1 , . . . ,⌦K) := 1 m E tr ⇥ vec(T )vec(T )>(⌦K ⌦ · · ·⌦⌦1) ⇤\nK X\nk=1\n1\nmk log |⌦k|. (3.1)\nBy minimizing q(⌦ 1 , . . . ,⌦K) with respect to ⌦k, k = 1, . . . ,K, we obtain the population minimization function with the parameter ⌦\n[K] k := {⌦1, . . . ,⌦k 1,⌦k+1, . . . ,⌦K}, i.e., Mk(⌦\n[K] k) := argmin ⌦k\nq(⌦ 1 , . . . ,⌦K). (3.2)\nTheorem 3.1. For any k = 1, . . . ,K, if ⌦j (j 6= k) satisfies tr(⌃⇤j⌦j) 6= 0, then the population minimization function in (3.2) satisfies Mk(⌦ [K] k) = m ⇥ mk Q j 6=k tr(⌃ ⇤ j⌦j) ⇤ 1 ⌦⇤k.\nTheorem 3.1 shows a surprising phenomenon that the population minimization function recovers the true precision matrix up to a constant in only one iteration. If ⌦j = ⌦⇤j , j 6= k, then Mk(⌦[K] k) = ⌦⇤k. Otherwise, after a normalization such that kMk(⌦[K] k)kF = 1, the normalized population minimization function still fully recovers ⌦⇤k. This observation suggests that setting T = 1 in Algorithm 1 is sufficient. Such a suggestion will be further supported by our numeric results.\nIn practice, when (3.1) is unknown, we can approximate it via its sample version qn(⌦1, . . . ,⌦K) defined in (2.2), which gives rise to the statistical error in the estimation procedure. Analogously to (3.2), we define the sample-based minimization function with parameter ⌦\n[K] k as cMk(⌦\n[K] k) := argmin ⌦k\nqn(⌦1, . . . ,⌦K). (3.3)\nIn order to prove the estimation error, it remains to quantify the statistical error induced from finite samples. The following two regularity conditions are assumed for this purpose. Condition 3.2 (Bounded Eigenvalues). For any k = 1, . . . ,K, there is a constant C\n1 > 0 such that, 0 < C\n1  min (⌃⇤k)  max(⌃ ⇤ k)  1/C1 < 1,\nwhere min (⌃⇤k) and max(⌃ ⇤ k) refer to the minimal and maximal eigenvalue of ⌃ ⇤ k, respectively.\nCondition 3.2 requires the uniform boundedness of the eigenvalues of true covariance matrices ⌃⇤k. It has been commonly assumed in the graphical model literature [22]. Condition 3.3 (Tuning). For any k = 1, . . . ,K and some constant C\n2 > 0, the tuning parameter k satisfies 1/C\n2\np logmk/(nmmk)  k  C2 p logmk/(nmmk).\nCondition 3.3 specifies the choice of the tuning parameters. In practice, a data-driven tuning procedure [23] can be performed to approximate the optimal choice of the tuning parameters.\nBefore characterizing the statistical error, we define a sparsity parameter for ⌦⇤k, k = 1, . . . ,K. Let Sk := {(i, j) : [⌦⇤k]i,j 6= 0}. Denote the sparsity parameter sk := |Sk| mk, which is the number of nonzero entries in the off-diagonal component of ⌦⇤k. For each k = 1, . . . ,K, we define B(⌦⇤k) as the set containing ⌦⇤k and its neighborhood for some sufficiently large constant radius ↵ > 0, i.e., B(⌦⇤k) := {⌦ 2 Rmk⇥mk : ⌦ = ⌦>;⌦ 0; k⌦ ⌦⇤kkF  ↵}. (3.4) Theorem 3.4. Assume Conditions 3.2 and 3.3 hold. For any k = 1, . . . ,K, the statistical error of the sample-based minimization function defined in (3.3) satisfies that, for any fixed ⌦j 2 B(⌦⇤j ) (j 6= k),\ncMk(⌦ [K] k) Mk(⌦[K] k) F = OP\nr\nmk(mk + sk) logmk nm\n!\n, (3.5)\nwhere Mk(⌦ [K] k) and cMk(⌦[K] k) are defined in (3.2) and (3.3), and m = QK k=1 mk.\nTheorem 3.4 establishes the statistical error associated with cMk(⌦ [K] k) for arbitrary ⌦j 2 B(⌦⇤j ) with j 6= k. In comparison, previous work on the existence of a local solution with desired statistical property only establishes theorems similar to Theorem 3.4 for ⌦j = ⌦⇤j with j 6= k. The extension to an arbitrary ⌦j 2 B(⌦⇤j ) involves non-trivial technical barriers. Particularly, we first establish the rate of convergence of the difference between a sample-based quadratic form with its expectation (Lemma B.1) via concentration of Lipschitz functions of Gaussian random variables [24]. This result is also of independent interest. We then carefully characterize the rate of convergence of Sk defined in (2.3) (Lemma B.2). Finally, we develop (3.5) using the results for vector-valued graphical models developed by [25].\nAccording to Theorem 3.1 and Theorem 3.4, we obtain the rate of convergence of the Tlasso estimator in terms of Frobenius norm, which is our main result.\nTheorem 3.5. Assume that Conditions 3.2 and 3.3 hold. For any k = 1, . . . ,K, if the initialization satisfies ⌦(0)j 2 B(⌦⇤j ) for any j 6= k, then the estimator b⌦k from Algorithm 1 with T = 1 satisfies,\nb⌦k ⌦ ⇤ k F = OP\nr\nmk(mk + sk) logmk nm\n!\n, (3.6)\nwhere m = QK k=1 mk and B(⌦⇤j ) is defined in (3.4).\nTheorem 3.5 suggests that as long as the initialization is within a constant distance to the truth, our Tlasso algorithm attains a consistent estimator after only one iteration. This initialization condition ⌦(0)j 2 B(⌦⇤j ) trivially holds since for any ⌦ (0)\nj that is positive definite and has unit Frobenius norm, we have k⌦(0)j ⌦ ⇤ kkF  2 by noting that k⌦ ⇤ kkF = 1 (k = 1, . . . ,K) for the identifiability of the tensor normal distribution. In literature, [9] shows that there exists a local minimizer of (2.2) whose convergence rate can achieve (3.6). However, it is unknown if their algorithm can find such minimizer since there could be many other local minimizers.\nA notable implication of Theorem 3.5 is that, when K 3, the estimator from our Tlasso algorithm can achieve estimation consistency even if we only have access to one observation, i.e., n = 1, which is often the case in practice. To see it, suppose that K = 3 and n = 1. When the dimensions m\n1 ,m 2 , and m\n3 are of the same order of magnitude and sk = O(mk) for k = 1, 2, 3, all the three error rates corresponding to k = 1, 2, 3 in (3.6) converge to zero.\nThis result indicates that the estimation of the k-th precision matrix takes advantage of the information from the j-th way (j 6= k) of the tensor data. Consider a simple case that K = 2 and one precision matrix ⌦⇤\n1 = 1m1 is known. In this scenario the rows of the matrix data are independent and hence the effective sample size for estimating ⌦⇤\n2\nis in fact nm 1 . The optimality result for the vector-valued graphical model [4] implies that the optimal rate for estimating ⌦⇤\n2\nis p\n(m 2 + s 2 ) logm 2 /(nm 1 ), which matches our result in (3.6). Therefore, the rate in (3.6) obtained by our Tlasso estimator is minimax-optimal since it is the best rate one can obtain even when ⌦⇤j (j 6= k) are known. As far as we know, this phenomenon has not been discovered by any previous work in tensor graphical model.\nRemark 3.6. For K = 2, our tensor graphical model reduces to matrix graphical model with Kronecker product covariance structure [5–8]. In this case, the rate of convergence of b⌦\n1 in (3.6) reduces to p\n(m 1 + s 1 ) logm 1 /(nm 2\n), which is much faster than p\nm 2 (m 1 + s 1 )(logm 1 + logm 2 )/n established by [6] and p\n(m 1 +m 2 ) log[max(m 1 ,m 2 , n)]/(nm 2 ) established by [7]. In literature, [5] shows that there exists a local minimizer of the objective function whose estimation errors match ours. However, it is unknown if their estimator can achieve such convergence rate. On the other hand, our theorem confirms that our algorithm is able to find such estimator with optimal rate of convergence."
    }, {
      "heading" : "3.2 Estimation error in max norm and spectral norm",
      "text" : "We next show the estimation error in max norm and spectral norm. Trivially, these estimation errors are bounded by that in Frobenius norm shown in Theorem 3.5. To develop improved rates of convergence in max and spectral norms, we need to impose stronger conditions on true parameters.\nWe first introduce some important notations. Denote dk as the maximum number of non-zeros in any row of the true precision matrices ⌦⇤k, that is,\ndk := max i2{1,...,mk}\n{j 2 {1, . . . ,mk} : [⌦ ⇤ k]i,j 6= 0} , (3.7)\nwith | · | the cardinality of the inside set. For each covariance matrix ⌃⇤k, we define ⌃⇤k := |||⌃ ⇤ k|||1. Denote the Hessian matrix ⇤k := ⌦ ⇤ 1 k ⌦⌦ ⇤ 1 k 2 Rm 2 k⇥m 2 k , whose entry [ ⇤k](i,j),(s,t) corresponds to the second order partial derivative of the objective function with respect to [⌦k]i,j and [⌦k]s,t. We define its sub-matrix indexed by the index set Sk as [ ⇤k]Sk,Sk = [⌦ ⇤ 1 k ⌦⌦ ⇤ 1 k ]Sk,Sk , which is the |Sk|⇥ |Sk| matrix with rows and columns of ⇤k indexed by Sk and Sk, respectively. Moreover, we define \n⇤ k := ([ ⇤k]Sk,Sk) 1 1. In order to establish the rate of convergence in max norm, we need to impose an irrepresentability condition on the Hessian matrix. Condition 3.7 (Irrepresentability). For each k = 1, . . . ,K, there exists some ↵k 2 (0, 1] such that\nmax e2Sck\n[ ⇤k]e,Sk [ ⇤k]Sk,Sk 1\n1\n 1 ↵k.\nCondition 3.7 controls the influence of the non-connected terms in Sck on the connected edges in Sk. This condition has been widely applied in lasso penalized models [26, 27]. Condition 3.8 (Bounded Complexity). For each k = 1, . . . ,K, the parameters \n⌃ ⇤ k , ⇤ k are bounded and the parameter dk in (3.7) satisfies dk = o p nm/(mk logmk)\n. Theorem 3.9. Suppose Conditions 3.2, 3.3, 3.7 and 3.8 hold. Assume sk = O(mk) for k = 1, . . . ,K and assume m0ks are in the same order, i.e., m1 ⇣ m2 ⇣ · · · ⇣ mK . For each k, if the initialization satisfies ⌦(0)j 2 B(⌦⇤j ) for any j 6= k, then the estimator b⌦k from Algorithm 1 with T = 2 satisfies,\nb⌦k ⌦ ⇤ k 1 = OP\nr\nmk logmk nm\n!\n. (3.8)\nIn addition, the edge set of b⌦k is a subset of the true edge set of ⌦⇤k, that is, supp(b⌦k) ✓ supp(⌦ ⇤ k).\nTheorem 3.9 shows that our Tlasso estimator achieves the optimal rate of convergence in max norm [4]. Here we consider the estimator obtained after two iterations since we require a new concentration inequality (Lemma B.3) for the sample covariance matrix, which is built upon the estimator in Theorem 3.5. A direct consequence from Theorem 3.9 is the estimation error in spectral norm. Corollary 3.10. Suppose the conditions of Theorem 3.9 hold, for any k = 1, . . . ,K, we have\nb⌦k ⌦ ⇤ k\n2\n= OP dk\nr\nmk logmk nm\n!\n. (3.9)\nRemark 3.11. Now we compare our obtained rate of convergence in spectral norm for K = 2 with that established in the sparse matrix graphical model literature. In particular, [8] establishes the rate of OP p mk(sk _ 1) log(m1 _m2)/(nmk)\nfor k = 1, 2. Therefore, when d2k  (sk _ 1), which holds for example in the bounded degree graphs, our obtained rate is faster. However, our faster rate comes at the price of assuming the irrepresentability condition. Using recent advance in nonconvex regularization [28], we can eliminate the irrepresentability condition. We leave this to future work."
    }, {
      "heading" : "3.3 Model selection consistency",
      "text" : "Theorem 3.9 ensures that the estimated precision matrix correctly excludes all non-informative edges and includes all the true edges (i, j) with |[⌦⇤k]i,j | > C p\nmk logmk/(nm) for some constant C > 0. Therefore, in order to achieve the model selection consistency, a sufficient condition is to assume that, for each k = 1, . . . ,K, the minimal signal ✓k := min\n(i,j)2supp(⌦⇤k)[⌦ ⇤ k]i,j is not too small.\nTheorem 3.12. Under the conditions of Theorem 3.9, if ✓k C p\nmk logmk/(nm) for some constant C > 0, then for any k = 1, . . . ,K, sign\nb⌦k = sign(⌦⇤k), with high probability.\nTheorem 3.12 indicates that our Tlasso estimator is able to correctly recover the graphical structure of each way of the high-dimensional tensor data. To the best of our knowledge, these is the first model selection consistency result in high dimensional tensor graphical model."
    }, {
      "heading" : "4 Simulations",
      "text" : "We compare the proposed Tlasso estimator with two alternatives. The first one is the direct graphical lasso (Glasso) approach [21] which applies the glasso to the vectorized tensor data to estimate ⌦⇤\n1 ⌦ · · · ⌦ ⌦⇤K directly. The second alternative method is the iterative penalized maximum likelihood method (P-MLE) proposed by [9], whose termination condition is set to be PK\nk=1 b⌦(t)k b⌦(t 1)k\nF\nK  0.001.\nFor simplicity, in our Tlasso algorithm we set the initialization of k-th precision matrix as 1mk for each k = 1, . . . ,K and the total iteration T = 1. The tuning parameter k is set as 20 p\nlogmk/(nmmk). For a fair comparison, the same tuning parameter is applied in the P-MLE method. In the direct Glasso approach, its tuning parameter is chosen by cross-validation via huge package [29].\nWe consider two simulations with a third order tensor, i.e., K = 3. In Simulation 1, we construct a triangle graph, while in Simulation 2, we construct a four nearest neighbor graph for each precision matrix. An illustration of the generated graphs are shown in Figure 1. In each simulation, we consider three scenarios, i.e., s1: n = 10 and (m\n1 ,m 2 ,m 3 ) = (10, 10, 10); s2: n = 50 and (m 1 ,m 2 ,m 3 ) =\n(10, 10, 10); s3: n = 10 and (m 1 ,m 2 ,m 3 ) = (100, 5, 5). We repeat each example 100 times and compute the averaged computational time, the averaged estimation error of the Kronecker product of precision matrices (m\n1 m 2 m 3 ) 1 b⌦ 1 ⌦ · · ·⌦ b⌦K ⌦⇤ 1 ⌦ · · ·⌦⌦⇤K F , the true positive\nrate (TPR), and the true negative rate (TNR). More specifically, we denote a⇤i,j be the (i, j)-th entry of ⌦⇤\n1 ⌦ · · · ⌦ ⌦⇤K , and define TPR := P i,j 1(bai,j 6= 0, a ⇤ i,j 6= 0)/ P i,j 1(a ⇤ i,j 6= 0) and\nTNR := P i,j 1(bai,j = 0, a ⇤ i,j = 0)/ P i 1(a ⇤ i,j = 0).\nAs shown in Figure 1, our Tlasso is dramatically faster than both alternative methods. In Scenario s3, Tlasso takes about five seconds for each replicate, the P-MLE takes about 500 seconds while the direct Glasso method takes more than one hour and is omitted in the plot. Tlasso algorithm is not only computationally efficient but also enjoys superior estimation accuracy. In all examples, the direct Glasso method has significantly larger errors than Tlasso due to ignoring the tensor graphical structure. Tlasso outperforms P-MLE in Scenarios s1 and s2 and is comparable to it in Scenario s3.\nTable 1 shows the variable selection performance. Our Tlasso identifies almost all edges in these six examples, while the Glasso and P-MLE method miss several true edges. On the other hand, Tlasso tends to include more non-connected edges than other methods."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We would like to thank the anonymous reviewers for their helpful comments. Han Liu is grateful for the support of NSF CAREER Award DMS1454377, NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841. Guang Cheng’s research is sponsored by NSF CAREER Award DMS1151692, NSF DMS1418042, Simons Fellowship in Mathematics, ONR N00014-15-1-2331 and a grant from Indiana Clinical and Translational Sciences Institute."
    } ],
    "references" : [ {
      "title" : "Pairwise interaction tensor factorization for personalized tag recommendation",
      "author" : [ "S. Rendle", "L. Schmidt-Thieme" ],
      "venue" : "In International Conference on Web Search and Data Mining,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Sparse higher-order principal components analysis",
      "author" : [ "G.I. Allen" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "AGEMAP: A gene expression database for aging in mice",
      "author" : [ "J. Zahn", "S. Poosala", "A. Owen", "D. Ingram" ],
      "venue" : "PLOS Genetics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Estimating sparse precision matrix: Optimal rates of convergence and adaptive estimation",
      "author" : [ "T. Cai", "W. Liu", "H.H. Zhou" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Sparse matrix graphical models",
      "author" : [ "C. Leng", "C.Y. Tang" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Model selection and estimation in the matrix normal graphical model",
      "author" : [ "J. Yin", "H. Li" ],
      "venue" : "Journal of Multivariate Analysis,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "On convergence of Kronecker graphical Lasso algorithms",
      "author" : [ "T. Tsiligkaridis", "A.O. Hero", "S. Zhou" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Gemini: Graph estimation with matrix variate normal instances",
      "author" : [ "S. Zhou" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Graphical model selection and estimation for high dimensional tensor data",
      "author" : [ "S. He", "J. Yin", "H. Li", "X. Wang" ],
      "venue" : "Journal of Multivariate Analysis,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "P. Jain", "P. Netrapalli", "S. Sanghavi" ],
      "venue" : "In Symposium on Theory of Computing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Phase retrieval using alternating minimization",
      "author" : [ "P. Netrapalli", "P. Jain", "S. Sanghavi" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Complete dictionary recovery over the sphere",
      "author" : [ "J. Sun", "Q. Qu", "J. Wright" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Simple, efficient, and neural algorithms for sparse coding",
      "author" : [ "S. Arora", "R. Ge", "T. Ma", "A. Moitra" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "A. Anandkumar", "R. Ge", "D. Hsu", "S. Kakade", "M. Telgarsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Provable sparse tensor decomposition",
      "author" : [ "W. Sun", "J. Lu", "H. Liu", "G. Cheng" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Scalable nonparametric multiway data analysis",
      "author" : [ "S. Zhe", "Z. Xu", "X. Chu", "Y. Qi", "Y. Park" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Sparse bayesian multiview learning for simultaneous association discovery and diagnosis of alzheimer’s disease",
      "author" : [ "S. Zhe", "Z. Xu", "Y. Qi", "P. Yu" ],
      "venue" : "In Twenty-Ninth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T. Kolda", "B. Bader" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Regression shrinkage and selection via the Lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1996
    }, {
      "title" : "Model selection and estimation in the gaussian graphical model",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical",
      "author" : [ "J. Friedman", "H. Hastie", "R. Tibshirani" ],
      "venue" : "Lasso. Biostatistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Sparse permutation invariant covariance estimation",
      "author" : [ "A.J. Rothman", "P.J. Bickel", "E. Levina", "J. Zhu" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Consistent selection of tuning parameters via variable selection stability",
      "author" : [ "W. Sun", "J. Wang", "Y. Fang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Probability in Banach Spaces: Isoperimetry and Processes",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Network exploration via the adaptive Lasso and scad penalties",
      "author" : [ "J. Fan", "Y. Feng", "Y. Wu" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "On model selection consistency of Lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "High-dimensional covariance estimation by minimizing `1-penalized log-determinant divergence",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Optimal computational and statistical rates of convergence for sparse nonconvex learning problems",
      "author" : [ "Z. Wang", "H. Liu", "T. Zhang" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2014
    }, {
      "title" : "The huge package for high-dimensional undirected graph estimation in R",
      "author" : [ "T. Zhao", "H. Liu", "K. Roeder", "J. Lafferty", "L. Wasserman" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "Matrix variate distributions",
      "author" : [ "A. Gupta", "D. Nagar" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2000
    }, {
      "title" : "Separable covariance arrays via the Tucker product, with applications to multivariate relational data",
      "author" : [ "P. Hoff" ],
      "venue" : "Bayesian Analysis,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Some matrix-variate distribution theory: Notational considerations and a bayesian application",
      "author" : [ "A.P. Dawid" ],
      "venue" : "Biometrika, 68:265–274,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1981
    }, {
      "title" : "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
      "author" : [ "S. Negahban", "M.J. Wainwright" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction High-dimensional tensor-valued data are prevalent in many fields such as personalized recommendation systems and brain imaging research [1, 2].",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction High-dimensional tensor-valued data are prevalent in many fields such as personalized recommendation systems and brain imaging research [1, 2].",
      "startOffset" : 151,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "Also, in the example of microarray study for aging [3], thousands of gene expression measurements are recorded on 16 tissue types on 40 mice with varying ages, which forms a four way gene-tissue-mouse-age tensor.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "mk(mk + sk) logmk/(nm) statistical rate of convergence in Frobenius norm, which is minimax-optimal since this is the best rate one can obtain even when the rest K 1 true precision matrices are known [4].",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 4,
      "context" : "In particular, [5] and [6] only establish the existence of a local optima with desired statistical guarantees.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "In particular, [5] and [6] only establish the existence of a local optima with desired statistical guarantees.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "Meanwhile, [7] considers an algorithm that is similar to ours.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "However, the statistical rates of convergence obtained by [6, 7] are much slower than ours when K = 2.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "However, the statistical rates of convergence obtained by [6, 7] are much slower than ours when K = 2.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "For K = 2, our statistical rate of convergence in Frobenius norm recovers the result of [5].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "In other words, our theory confirms that the desired local optimum studied by [5] not only exists, but is also attainable by an efficient algorithm.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "In addition, for matrix graphical model, [8] establishes the statistical rates of convergence in spectral and Frobenius norms for the estimator attained by a similar algorithm.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "In the context of sparse tensor graphical model with a general K, [9] shows the existence of a local optimum with desired rates, but does not prove whether there exists an efficient algorithm that provably attains such a local optimum.",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "1 Preliminary We employ the tensor notations used by [18].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 8,
      "context" : "According to [9, 18], it can be shown that T ⇠ TN(0;⌃",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 17,
      "context" : "According to [9, 18], it can be shown that T ⇠ TN(0;⌃",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 18,
      "context" : "In this paper, we focus on the lasso penalty [19], i.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "It reduces to the sparse vector graphical model [20, 21] when K = 1, and the sparse matrix graphical model [5–8] when K = 2.",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 20,
      "context" : "It reduces to the sparse vector graphical model [20, 21] when K = 1, and the sparse matrix graphical model [5–8] when K = 2.",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "3) can be shown by noting that Vk i = [Ti](k) ⌦ K ⌦ · · ·⌦⌦ 1/2 k+1⌦⌦ 1/2 k 1⌦ · · ·⌦⌦ 1/2 1 > according to the properties of mode-k matricization shown by [18].",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "3) corresponds to estimating vector-valued Gaussian graphical model and can be solved efficiently via the glasso algorithm [21].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "It has been commonly assumed in the graphical model literature [22].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 22,
      "context" : "In practice, a data-driven tuning procedure [23] can be performed to approximate the optimal choice of the tuning parameters.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 23,
      "context" : "1) via concentration of Lipschitz functions of Gaussian random variables [24].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 24,
      "context" : "5) using the results for vector-valued graphical models developed by [25].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "In literature, [9] shows that there exists a local minimizer of (2.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 3,
      "context" : "The optimality result for the vector-valued graphical model [4] implies that the optimal rate for estimating ⌦⇤",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "In literature, [5] shows that there exists a local minimizer of the objective function whose estimation errors match ours.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 25,
      "context" : "This condition has been widely applied in lasso penalized models [26, 27].",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 26,
      "context" : "This condition has been widely applied in lasso penalized models [26, 27].",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "9 shows that our Tlasso estimator achieves the optimal rate of convergence in max norm [4].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "In particular, [8] establishes the rate of OP p",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 27,
      "context" : "Using recent advance in nonconvex regularization [28], we can eliminate the irrepresentability condition.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "The first one is the direct graphical lasso (Glasso) approach [21] which applies the glasso to the vectorized tensor data to estimate ⌦⇤",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "The second alternative method is the iterative penalized maximum likelihood method (P-MLE) proposed by [9], whose termination condition is set to be K k=1 b ⌦ k b ⌦ 1) k F K  0.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "In the direct Glasso approach, its tuning parameter is chosen by cross-validation via huge package [29].",
      "startOffset" : 99,
      "endOffset" : 103
    } ],
    "year" : 2015,
    "abstractText" : "We consider the estimation of sparse graphical models that characterize the dependency structure of high-dimensional tensor-valued data. To facilitate the estimation of the precision matrix corresponding to each way of the tensor, we assume the data follow a tensor normal distribution whose covariance has a Kronecker product structure. The penalized maximum likelihood estimation of this model involves minimizing a non-convex objective function. In spite of the non-convexity of this estimation problem, we prove that an alternating minimization algorithm, which iteratively estimates each sparse precision matrix while fixing the others, attains an estimator with the optimal statistical rate of convergence as well as consistent graph recovery. Notably, such an estimator achieves estimation consistency with only one tensor sample, which is unobserved in previous work. Our theoretical results are backed by thorough numerical studies.",
    "creator" : null
  }
}