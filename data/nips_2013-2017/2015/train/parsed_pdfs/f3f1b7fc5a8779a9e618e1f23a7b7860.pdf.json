{
  "name" : "f3f1b7fc5a8779a9e618e1f23a7b7860.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Beyond Sub-Gaussian Measurements: High-Dimensional Structured Estimation with Sub-Exponential Designs",
    "authors" : [ "Vidyashankar Sivakumar", "Arindam Banerjee", "Pradeep Ravikumar" ],
    "emails" : [ "sivakuma@cs.umn.edu", "banerjee@cs.umn.edu", "pradeepr@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ log p times the Gaussian width\nof the set, yielding Gaussian width based results even for the sub-exponential case. Further, for certain popular estimators, viz Lasso and Group Lasso, using a VCdimension based analysis, we show that the sample complexity will in fact be the same order as Gaussian designs. Our general analysis and results are the first in the sub-exponential setting, and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the following problem of high dimensional linear regression:\ny = Xθ∗ + ω , (1)\nwhere y ∈ Rn is the response vector, X ∈ Rn×p has independent isotropic sub-exponential random rows, ω ∈ Rn has i.i.d sub-exponential entries and the number of covariates p is much larger compared to the number of samples n. Given y,X and assuming that θ∗ is ‘structured’, usually characterized as having a small value according to some norm R(·), the problem is to recover θ̂ close to θ∗. Considerable progress has been made over the past decade on high-dimensional structured estimation using suitable M-estimators or norm-regularized regression [16, 2] of the form:\nθ̂λn = argmin θ∈Rp\n1\n2n ‖y −Xθ‖22 + λnR(θ) , (2)\nwhere R(θ) is a suitable norm, and λn > 0 is the regularization parameter. Early work focused on high-dimensional estimation of sparse vectors using the Lasso and related estimators, where R(θ) = ‖θ‖1 [13, 22, 23]. Sample complexity of such estimators have been rigorously established based on the RIP(restricted isometry property) [4, 5] and the more general RE(restricted eigenvalue) conditions [3, 16, 2]. Several subsequent advances have considered structures beyond `1, using more general norms such as (overlapping) group sparse norms, k-support norm, nuclear norm, and so on [16, 8, 7]. In recent years, much of the literature has been unified and nonasymptotic estimation error bound analysis techniques have been developed for regularized estimation with any norm [2].\nIn spite of such advances, most of the existing literature relies on the assumption that entries in the design matrix X ∈ Rn×p are sub-Gaussian. In particular, recent unified treatments based on decomposable norms, atomic norms, or general norms all rely on concentration properties of subGaussian distributions [16, 7, 2]. Certain estimators, such as the Dantzig selector and variants, consider a constrained problem rather than a regularized problem as in (2) but the analysis again relies on entries of X being sub-Gaussian [6, 8]. For the setting of constrained estimation, building on prior work by [10], [20] outlines a possible strategy for such analysis which can work for any distribution, but works out details only for the sub-Gaussian case. In recent work [9] considered sub-Gaussian design matrices but with heavy-tailed noise, and suggested modifying the estimator in (1) via a median-of-means type estimator based on multiple estimates of θ̂ from sub-samples.\nIn this paper, we establish results for the norm-regularized estimation problem as in (2) for any norm R(θ) under the assumption that elements Xij of the design matrix X ∈ Rn×p follow a subexponential distribution, whose tails are dominated by scaled versions of the (symmetric) exponential distribution, i.e., P (|Xij | > t) ≤ c1 exp(−t/c2) for all t ≥ 0 and for suitable constants c1, c2 [12, 21]. To understand the motivation of our work, note that in most of machine learning and statistics, unlike in compressed sensing, the design matrix cannot be chosen but gets determined by the problem. In many application domains like finance, climate science, ecology, social network analysis, etc., variables with heavier tails than sub-Gaussians are frequently encountered. For example in climate science, to understand the relationship between extreme value phenomena like heavy precipitation variables from the extreme-value distributions are used. While high dimensional statistical techniques have been used in practice for such applications, currently lacking is the theoretical guarantees on their performance. Note that the class of sub-exponential distributions have heavier tails compared to sub-Gaussians but have all moments. To the best of our knowledge, this is the first paper to analyze regularized high-dimensional estimation problems of the form (2) with sub-exponential design matrices and noise.\nIn our main result, we obtain bounds on the estimation error ‖∆̂n‖2 = ‖θ̂λn − θ∗‖2, where θ∗ is the optimal structured parameter. The sample complexity bounds are log p worse compared to the sub-Gaussian case. For example for the `1 norm, we obtain n = O(s log2 p) sample complexity bound instead of O(s log p) for the sub-Gaussian case. The analysis depends on two key ingredients which have been discussed in previous work [16, 2]: 1. The satisfaction of the RE condition on a set A which is the error set associated with the norm, and 2. The design matrix-noise interaction manifested in the form of lower bounds on the regularization parameter. Specifically, the RE condition depends on the properties of the design matrix. We outline two different approaches for obtaining the sample complexity, to satisfy the RE condition: one based on the ‘exponential width’ of A and another based on the VC-dimension of linear predictors drawn from A [10, 20, 11]. For two widely used cases, Lasso and group-lasso, we show that the VC-dimension based analysis leads to a sharp bound on the sample complexity, which is exactly the same order as that for sub-Gaussian design matrices! In particular, for Lasso with s-sparsity, O(s log p) samples are sufficient to satisfy the RE condition for sub-exponential designs. Further, we show that the bound on the regularization parameter depends on the ‘exponential width’ we(ΩR) of the unit norm ball ΩR = {u ∈ Rp|R(u) ≤ 1}. Through a careful argument based on generic chaining [19], we show that for any set T ⊂ Rp, the exponential width we(T ) ≤ cwg(T ) √ log p, where wg(T ) is the Gaussian width of the set T and c is an absolute constant. Recent advances on computing or bounding wg(T ) for various structured sets can then be used to bound we(T ). Again, for the case of Lasso, we(ΩR) ≤ c log p. The rest of the paper is organized as follows. In Section 2 we describe various aspects of the problem and highlight our contributions. In Section 3 we establish a key result on the relationship between Gaussian and exponential widths of sets which will be used for our subsequent analysis. In Section 4 we establish results on the regularization parameter λn, RE constant κ and the non-asymptotic estimation error ‖∆̂n‖2. We show some experimental results before concluding in Section 6."
    }, {
      "heading" : "2 Background and Preliminaries",
      "text" : "In this section, we describe various aspects of the problem, introducing notations along the way, and highlight our contributions. Throughout the paper values of constants change from line to line."
    }, {
      "heading" : "2.1 Problem setup",
      "text" : "We consider the problem defined in (2). The goal of this paper is to establish conditions for consistent estimation and derive bounds on ‖∆̂n‖2 = ‖θ̂ − θ∗‖2.\nError set: Under the assumption λn ≥ βR∗( 1nX T (y−Xθ∗)), β > 1, the error vector ∆̂n = θ̂−θ∗ lies in a cone A ⊆ Sp−1 [3, 16, 2]. Regularization parameter: For β > 1, λn ≥ βR∗( 1nX\nT (y −Xθ)) following analysis in [16, 2]. Restricted Eigenvalue (RE) conditions: For consistent estimation, the design matrix X should satisfy the following RE condition infu∈A 1√n‖Xu‖2 ≥ κ on the error set A for some constant κ > 0 [3, 16, 2, 20, 18]. The RE sample complexity is the number of samples n required to satisfy the RE condition and has been shown to be related to the Gaussian width of the error set. [7, 2, 20].\nDeterministic recovery bounds: If X satisfies the RE condition on the error set A and λn satisfies the assumptions stated earlier, [2] show the error bound ‖∆̂n‖2 ≤ cΨ(A)λnκ with high probability (w.h.p), for some constant c, where Ψ(A) = supu∈A R(u) ‖u‖2 is the norm compatibility constant.\n`1 norm regularization: One example for R(·) we will consider throughout the paper is the `1 norm regularization. In particular we will always consider ‖θ∗‖0 = s. Group-sparse norms: Another popular example we consider is the group-sparse norm. Let G = {G1,G2, . . . ,GNG} denote a collection of groups, which are blocks of any vector θ ∈ Rp. For any vector θ ∈ Rp, let θNG denote a vector with coordinates θNGi = θi if i ∈ GNG , else θ NG i = 0. Let m = maxi∈[1,··· ,NG ] |Gi| be the maximum size of any group. In the group sparse setting for any subset SG ⊆ {1, 2, . . . , NG} with cardinality |SG | = sG , we assume that the parameter vector θ∗ ∈ Rp satisfies θ∗NG = ~0, ∀NG 6∈ SG . Such a vector is called SG-group sparse. We will focus on the case when R(θ) = ∑NG i=1 ‖θi‖2."
    }, {
      "heading" : "2.2 Contributions",
      "text" : "One of our major results is the relationship between the Gaussian and exponential width of sets using arguments from generic chaining [19]. Existing analysis frameworks for our problem for sub-Gaussian X and ω obtain results in terms of Gaussian widths of suitable sets associated with the norm [2, 20]. For sub-exponential X and ω this dependency, in some cases, is replaced by the exponential width of the set. By establishing a precise relationship between the two quantities, we leverage existing results on the computation of Gaussian widths for our scenario. Another contribution is obtaining the same order of the RE sample complexity bound as for the sub-Gaussian case for `1 and group-sparse norms. While this strong result has already been explored in [11] for `1, we adapt it for our analysis framework and also extend it to the group-sparse setting. As for the application of our work, the results are applicable to all log-concave distributions which by definition are distributions admitting a log-concave density f i.e. a density of the form f = eΨ with Ψ any concave function. This covers many practically used distributions including extreme value distributions."
    }, {
      "heading" : "3 Relationship between Gaussian and Exponential Widths",
      "text" : "In this section we introduce a complexity parameter of a set we(·), which we call the exponential width of the set, and establish a sharp upper bound for it in terms of the Gaussian width of the set wg(·). In particular, we prove the inequality: we(A) ≤ c · wg(A) √ log p for some fixed constant c. To see the connection with the rest of the paper, remember that our subsequent results for λn and κ are expressed in terms of the Gaussian width and exponential width of specific sets associated with the norm. With this result, we establish precise sample complexity bounds by leveraging a body of literature on the computation of Gaussian widths for various structured sets [7, 20]. We note that while the exponential width has been defined and used earlier, see for e.g. [19, 15], to the best of our knowledge this is the first result establishing the relation between the Gaussian and exponential widths of sets. Our result relies on generic chaining [19]."
    }, {
      "heading" : "3.1 Generic Chaining, Gaussian Width and Exponential Widths",
      "text" : "Consider a process {Xt}t∈T = 〈h, t〉 indexed by a set T ⊆ Rp, where each element hi has mean 0. It follows from the definition that the process is centered, i.e., E(Xt) = 0,∀t ∈ T . We will also assume for convenience w.l.o.g that set T is finite. Also, for any s, t ∈ T , consider a canonical distance metric d(s, t). We are interested in computing the quantity E supt∈T Xt. Now, for reasons detailed in the supplement, consider that we split T into a sequence of subsets T0 ⊆ T1 ⊆ . . . ⊆ T , with T0 = {t0}, |Tn| ≤ 22 n\nfor n ≥ 1 and Tm = T for some large m. Let function πn : T → Tn, defined as πn(t) = {s : d(s, t) ≤ d(s1, t),∀s, s1 ∈ Tn}, maps each point t ∈ T to some point s ∈ Tn closest according to d. The set Tn and the associated function πn define a partition An of the set T . Each element of the partition An has some element s ∈ Tn and all t ∈ T closest to it according to the map πn. Also the size of the partition |An| ≤ 22 n\n. An are called admissible sequences in generic chaining. Note that there are multiple admissible sequences corresponding to multiple ways of defining the sets T0, T1, . . . , Tm. We will denote by ∆(An(t)) the diameter of the element An(t) w.r.t distance metric d defined as ∆(An(t)) = sups,t∈An(t) d(s, t). Definition 1 γ-functionals: [19] Given α > 0, and a metric space (T, d) we define\nγα(T, d) = inf sup t ∑ n≥0 2n/α∆(An(t)) , (3)\nwhere the inf is taken over all possible admissible sequences of the set T . Gaussian width: Let {Xt}t∈T = 〈g, t〉 where each element gi is i.i.d N(0, 1). The quantity wg(T ) = E supt∈T Xt is called the Gaussian width of the set T . Define the distance metric d2(s, t) = ‖s − t‖2. The relation between Gaussian width and the γ-functionals is seen from the following result from [Theorem 2.1.1] of [19] stated below:\n1 L γ2(T, d2) ≤ wg(T ) ≤ Lγ2(T, d2) . (4)\nNote that, following [Theorem 2.1.5] in [19] any process which satisfies the concentration bound P (|Xs −Xt| ≥ u) ≤ 2 exp ( − u 2\nd2(s,t)2\n) satisfies the upper bound in (4).\nExponential width: Let {Xt}t∈T = 〈e, t〉 where each element ei is is a centered i.i.d exponential random variable satisfying P (|ei| ≥ u) = exp(−u). Define the distance metrics d2(s, t) = ‖s−t‖2 and d∞(s, t) = ‖s − t‖∞. The quantity we(T ) = E supt∈T Xt is called the exponential width of the set T . By [Theorem 1.2.7] and [Theorem 5.2.7] in [19], for some universal constant L, we(T ) satisfies:\n1 L (γ2(T, d2) + γ1(T, d∞)) ≤ we(T ) ≤ L(γ2(T, d2) + γ1(T, d∞)) (5)\nNote that any process which satisfies the sub-exponential concentration bound P (|Xs−Xt| ≥ u) ≤ 2 exp ( −K min ( u2\nd2(s,t)2 , ud∞(s,t)\n)) satisfies the upper bound in the above inequality [15, 19]."
    }, {
      "heading" : "3.2 An Upper Bound for the Exponential Width",
      "text" : "In this section we prove the following relationship between the exponential and Gaussian widths:\nTheorem 1 For any set T ⊂ Rp, for some constant c the following holds: we(T ) ≤ c · wg(T ) √ log p . (6)\nProof: The result depends on geometric results [Lemma 2.6.1] and [Theorem 2.6.2] in [19].\nTheorem 2 [19] Consider a countable set T ⊂ Rp, and a number u > 0. Assume that the Gaussian width is bounded i.e. S = γ2(T, d2) ≤ ∞. Then there is a decomposition T ⊂ T1 + T2 where T1 + T2 = {t1 + t2 : t1 ∈ T1, t2 ∈ T2}, such that\nγ2(T1, d2) ≤ LS , γ1(T1, d∞) ≤ LSu (7)\nγ2(T2, d2) ≤ LS , T2 ⊂ LS\nu B1 , (8)\nwhere L is some universal constant and B1 is the unit `1 norm ball in Rp.\nWe first examine the exponential widths of the sets T1 and T2. For the set T1:\nwe(T1) ≤ L[γ2(T1, d2) + γ1(T1, d∞)] ≤ L[S + Su] = L(wg(T ) + wg(T )u) , (9)\nwhere the first inequality follows from (5) and the second inequality follows from (7). We will need the following result on bounding the exponential width of an unit `1-norm ball in p dimensions to compute the exponential width of T2. The proof, given in the supplement, is based on the fact supt∈B1〈e, t〉 = ‖e‖∞ and then using a simple union bound argument to bound ‖e‖∞.\nLemma 1 Consider the set B1 = {t ∈ Rp : ‖t‖1 ≤ 1}. Then for some universal constant L:\nwe(B1) = E [ sup t∈B1 〈e, t〉 ] ≤ L log p . (10)\nThe exponential width of T2 is:\nwe(T2) = we((LS/u)B1) = (LS/u)we(B1) = (L/u)wg(T )we(B1) ≤ (L/u)wg(T ) log p . (11)\nThe first equality follows from (8) as T2 is a subset of a (LS/u)-scaled `1 norm ball, the second inequality follows from elementary properties of widths of sets and the last inequality follows from Lemma 1. Now as stated in Theorem 2, u in (9) and (11) is any number greater than 0. We choose u = √ log p and noting that (1 + √ log p) ≤ L √ log p for some constant L yields:\nwe(T1) ≤ Lwg(T ) √ log p, we(T2) ≤ Lwg(T ) √ log p (12)\nThe final step, following arguments as [Theorem 2.1.6] [19], is to bound exponential width of set T .\nwe(T ) = E[sup t∈T 〈h, t〉] ≤ E[ sup t1∈T1 〈h, t1〉] + E[ sup t2∈T2 〈h, t2〉] ≤ we(T1) + we(T2) ≤ Lwg(T )\n√ log p .\nThis proves Theorem 1."
    }, {
      "heading" : "4 Recovery Bounds",
      "text" : "We obtain bounds on the error vector ∆̂n = θ̂ − θ∗. If the regularization parameter λn ≥ βR∗( 1nX\nT (y − Xθ∗)), β > 1 and the RE condition is satisfied on the error set A with RE constant κ, then [2, 16] obtain the following error bound w.h.p for some constant c:\n‖∆̂n‖2 ≤ c · λn κ Ψ(A) , (13)\nwhere Ψ(A) is the norm compatibility constant given by supu∈A(R(u)/‖u‖2)."
    }, {
      "heading" : "4.1 Regularization Parameter",
      "text" : "As discussed earlier, for our analysis the regularization parameter should satisfy λn ≥ βR∗( 1nX\nT (y − Xθ∗)), β > 1. Observe that for the linear model (1), ω = y − Xθ∗ is the noise, implying that λn ≥ βR∗( 1nX\nTω). With e denoting a sub-exponential random vector with i.i.d entries,\nE [ R∗ ( 1\nn XTω\n)] = E [ sup u∈ΩR ‖ω‖2 〈 1 n XT ω ‖ω‖2 , u 〉] = 1 n E[‖ω‖2]E [ sup u∈ΩR 〈e, u〉 ] . (14)\nThe first equality follows from the definition of dual norm. The second inequality follows from the fact that X and ω are independent of each other. Also by elementary arguments [21], e = XT (ω/|ω‖2) has i.i.d sub-exponential entries with sub-exponential norm bounded by supω∈Rn ‖〈XTi , ω/‖ω‖2〉‖ψ1 . The above argument was first proposed for the sub-Gaussian case in [2]. For sub-exponential design and noise, the difference compared to the sub-Gaussian case is the dependence on the exponential width instead of the Gaussian width of the unit norm ball. Using known results on the Gaussian widths of unit `1 and group-sparse norms, corollaries below are derived using the relationship between Gaussian and exponential widths derived in Section 3:\nCorollary 1 If R(·) is the `1 norm, for sub-exponential design matrix X and noise ω,\nE [ R∗ ( 1\nn XT (y −Xθ∗)\n)] ≤ η0√\nn log p . (15)\nCorollary 2 If R(·) is the group-sparse norm, for sub-exponential design matrix X and noise ω,\nE [ R∗ ( 1\nn XT (y −Xθ∗)\n)] ≤ η0√\nn\n√ (m+ logNG) log p . (16)"
    }, {
      "heading" : "4.2 The RE condition",
      "text" : "For Gaussian and sub-Gaussian X , previous work has established RIP bounds of the form κ1 ≤ inf u∈A ( 1√ n )‖Xu‖2 ≤ sup u∈A ( 1√ n )‖Xu‖2 ≤ κ2. In particular, RIP is satisfied w.h.p if the number of samples is of the order of square of the Gaussian width of the error set ,i.e., O(w2g(A)), which we will call the sub-Gaussian RE sample complexity bound. As we move to heavier tails, establishing such two-sided bounds requires assumptions on the boundedness of the Euclidean norm of the rows ofX [15, 17, 10]. On the other hand, analysis of only the lower bound requires very few assumptions onX . In particular, ‖Xu‖2 being the sum of random non-negative quantities the lower bound should be satisfied even with very weak moment assumptions on X . Making these observations, [10, 17] develop arguments obtaining sub-Gaussian RE sample complexity bounds when set A is the unit sphere Sp−1 even for design matrices having only bounded fourth moments. Note that with such weak moment assumptions, a non-trivial non-asymptotic upper bound cannot be established. Our analysis for the RE condition essentially follow this premise and arguments from [10]."
    }, {
      "heading" : "4.2.1 A Bound Based on Exponential Width",
      "text" : "We obtain a sample complexity bound which depends on the exponential width of the error set A. The result we state below follows along similar arguments made in [20], which in turn are based on arguments from [10, 14].\nTheorem 3 Let X ∈ Rn×p have independent isotropic sub-exponential rows. Let A ⊆ Sp−1, 0 < ξ < 1, and c is a constant that depends on the sub-exponential normK = supu∈A ‖|〈X,u〉|‖ψ1 . Let we(A) denote the exponential width of the set. Then for some τ > 0 with probability atleast (1− exp(−τ2/2)),\ninf u∈A ‖Xu‖2 ≥ cξ(1− ξ2)2\n√ n− 4we(A)− ξτ . (17)\nContrasting the result (17) with previous results for the sub-Gaussian case [2, 20] the dependence on wg(A) on the r.h.s is replaced by we(A), thus leading to a log p worse sample complexity bound. The corollary below applies the result for the `1 norm. Note that results from [1] for `1 norm show RIP bounds w.h.p for the same number of samples.\nCorollary 3 For an s-sparse θ∗ and `1 norm regularization, if n ≥ c · s log2 p then with probability atleast (1− exp(−τ2/2)) and constants c, κ depending on ξ and τ ,\ninf u∈A ‖Xu‖2 ≥ κ . (18)"
    }, {
      "heading" : "4.2.2 A Bound Based on VC-Dimensions",
      "text" : "In this section, we show a stronger sub-Gaussian RE sample complexity result for sub-exponential X and `1, group-sparse regularization. The arguments follow along similar lines to [11, 10].\nTheorem 4 Let X ∈ Rn×p be a random matrix with isotropic random sub-exponential rows Xi ∈ Rp. Let A ⊆ Sp−1, 0 < ξ < 1, c is a constant that depends on the sub-exponential norm K = supu∈A ‖|〈X,u〉|‖ψ1 and define β = c(1− ξ2)2. Let we(A) denote the exponential width of the set"
    }, {
      "heading" : "A. Let Cξ = {I[|〈Xi, u〉| > ξ], u ∈ A} be a VC-class with VC-dimension V C(Cξ) ≤ d. For some",
      "text" : "suitable constant c1, if n ≥ c1(d/β2), then with probability atleast 1− exp(−η0β2n):\ninf u∈A 1√ n ‖Xu‖2 ≥\ncξ(1− ξ2)2\n2 . (19)\nConsider the case of `1 norm. A consequence of the above result is that the RE condition is satisfied on the set B = {u|‖u‖0 = s1} ∩ Sp−1 for some s1 ≥ c · s where c is a constant that will depend on the RE constant κ when n is O(s1 log p). The argument follows from the fact that B ∩ Sp−1 is a union of ( p s1 ) spheres. Thus the result is obtained by applying Theorem 4 to each sphere and using a union bound argument. The final step involves showing that the RE condition is satisfied on the error set A if it is satisfied on B using Maurey’s empirical approximation argument [17, 18, 11].\nCorollary 4 For set A ⊆ Sp−1, which is the error set for the `1 norm, if n ≥ c2s log(ep/s)/β2 for some suitable constant c2, then with probability atleast 1 − exp(−η0nβ2) − 1wη1pη1−1 , where η0, η1, w > 1 are constants, the following result holds for κ depending on the constant ξ:\ninf u∈A 1√ n ‖Xu‖2 ≥ κ . (20)\nEssentially the same arguments for the group-sparse norm lead to the following result:\nCorollary 5 For set A ⊆ Sp−1, which is the error set for the group-sparse norm, if n ≥ (c(msG + sG log(eNG/sG)))/β\n2, then with probability atleast 1 − exp(−η0nβ2) − 1 wη1N\nη1−1 G m\nη1−1 where\nη0, η1, w > 1 are constants and κ depending on constant ξ,\ninf u∈A 1√ n ‖Xu‖2 ≥ κ . (21)"
    }, {
      "heading" : "4.3 Recovery Bounds for `1 and Group-Sparse Norms",
      "text" : "We combine result (13) with results obtained for λn and κ previously for `1 and group-sparse norms.\nCorollary 6 For the `1 norm, when n ≥ cs log p for some constant c, with high probability:\n‖∆̂n‖2 ≤ O( √ s log p/ √ n) . (22)\nCorollary 7 For the group-sparse norm, when n ≥ c(msG + sG log(NG)), for some constant c, with high probability:\n‖∆̂n‖2 ≤ O\n(√ sG log p(m+ logNG)\nn\n) . (23)\nBoth bounds are √\nlog p worse compared to corresponding bounds for the sub-Gaussian case. In terms of sample complexity, n should scale as O(s log2 p), instead of O(s log p) for sub-Gaussian, for `1 norm and O(sG log p(m + logNG)), instead of O(sG(m + logNG)) for the sub-Gaussian case, for group-sparse lasso to get upto a constant order error bound."
    }, {
      "heading" : "5 Experiments",
      "text" : "We perform experiments on synthetic data to compare estimation errors for Gaussian and subexponential design matrices and noise for both `1 and group sparse norms. For `1 we run experiments with dimensionality p = 300 and sparsity level s = 10. For group sparse norms we run experiments with dimensionality p = 300, max. group size m = 6, number of groups NG = 50 groups each of size 6 and 4 non-zero groups. For the design matrix X , for the Gaussian case we sample rows randomly from an isotropic Gaussian distribution, while for sub-exponential design\n0 20 40 60 80 100 120 140 160 180 200 0\n0.2\n0.4\n0.6\n0.8\n1\nNumber of samples\nP ro\nb a\nb il it y o\nf s u\nc c e\ns s\nBasis pursuit with Gaussian design\nBasis pursuit with sub−exponential design\nGroup sparse with Gaussian design\nGroup sparse with sub−exponential design\nFigure 1: Probability of recovery in noiseless case with increasing sample size. There is a sharp phase transition and the curves overlap for Gaussian and subexponential designs.\nmatrices we sample each row of X randomly from an isotropic extreme-value distribution. The number of samples n in X is incremented in steps of 10 with an initial starting value of 5. For the noise ω, it is sampled i.i.d from the Gaussian and extreme-value distributions with variance 1 for the Gaussian and sub-exponential cases respectively. For each sample size n, we repeat the procedure above 100 times and all results reported in the plots are average values over the 100 runs. We report two sets of results. Figure 1 shows percentage of success vs sample size for the noiseless case when y = Xθ∗. A success in the noiseless case denotes exact recovery which is possible when the RE condition is satisfied. Hence we expect the sample complexity for recovery to be order of square of Gaussian width for Gaussian and extreme-value distributions as validated by the plots in Figure 1. Figure 2 shows average estimation error vs number of samples for the noisy case when y = Xθ∗+ω. The noise is added only for runs in which exact recovery was possible in the noiseless case. For example when n = 5 we do not have any results in Figure 2 as even noiseless recovery is not possible. For each n, the estimation errors are average values over 100 runs. As seen in Figure 2, the error decay is slower for extreme-value distributions compared to the Gaussian case."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This paper presents a unified framework for analysis of non-asymptotic error and structured recovery in norm regularized regression problems when the design matrix and noise are sub-exponential, essentially generalizing the corresponding analysis and results for the sub-Gaussian case. The main observation is that the dependence on Gaussian width is replaced by the exponential width of suitable sets associated with the norm. Together with the result on the relationship between exponential and Gaussian widths, previous analysis techniques essentially carry over to the sub-exponential case. We also show that a stronger result exists for the RE condition for the Lasso and group-lasso problems. As future work we will consider extending the stronger result for the RE condition for all norms.\nAcknowledgements: This work was supported by NSF grants IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A."
    } ],
    "references" : [ {
      "title" : "Restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling",
      "author" : [ "R. Adamczak", "A.E. Litvak", "A. Pajor", "N. Tomczak-Jaegermann" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Estimation with Norm Regularization",
      "author" : [ "A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Simultaneous analysis of Lasso and Dantzig selector",
      "author" : [ "P.J. Bickel", "Y. Ritov", "A.B. Tsybakov" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Robust Uncertainty Principles : Exact Signal Reconstruction from Highly Incomplete Frequency Information",
      "author" : [ "E.J. Candes", "J. Romberg", "T. Tao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Decoding by Linear Programming",
      "author" : [ "E.J. Candes", "T. Tao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "The Dantzig selector : statistical estimation when p is much larger than n",
      "author" : [ "E.J. Candes", "T. Tao" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "The Convex Geometry of Linear Inverse Problems",
      "author" : [ "V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Generalized Dantzig Selector: Application to the k-support norm",
      "author" : [ "S. Chatterjee", "S. Chen", "A. Banerjee" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Heavy-tailed regression with a generalized median-of-means",
      "author" : [ "D. Hsu", "S. Sabato" ],
      "venue" : "In ICML,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Bounding the smallest singular value of a random matrix without concentration",
      "author" : [ "V. Koltchinskii", "S. Mendelson" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Sparse recovery under weak moment assumptions",
      "author" : [ "G. Lecué", "S. Mendelson" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Probability in Banach Spaces: Isoperimetry and Processes",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1991
    }, {
      "title" : "Lasso-type recovery of sparse representations for high-dimensional data",
      "author" : [ "N. Meinshausen", "B. Yu" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Learning without concentration",
      "author" : [ "S. Mendelson" ],
      "venue" : "Journal of the ACM, To appear,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "On generic chaining and the smallest singular value of random matrices with heavy tails",
      "author" : [ "S. Mendelson", "G. Paouris" ],
      "venue" : "Journal of Functional Analysis,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "A Unified Framework for High- Dimensional Analysis of $M$-Estimators with Decomposable Regularizers",
      "author" : [ "S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "The lower tail of random quadratic forms, with applications to ordinary least squares and restricted eigenvalue properties",
      "author" : [ "R.I. Oliveira" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Reconstruction from anisotropic random measurements",
      "author" : [ "M. Rudelson", "S. Zhou" ],
      "venue" : "IEEE Transaction on Information Theory,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "The Generic Chaining",
      "author" : [ "M. Talagrand" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "Convex recovery of a structured signal from independent random linear measurements. In Sampling Theory - a Renaissance",
      "author" : [ "J.A. Tropp" ],
      "venue" : "To appear,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Sharp thresholds for high-dimensional and noisy sparsity recovery using L1 -constrained quadratic programmming",
      "author" : [ "M. J Wainwright" ],
      "venue" : "IEEE Transaction on Information Theory,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "On Model Selection Consistency of Lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Considerable progress has been made over the past decade on high-dimensional structured estimation using suitable M-estimators or norm-regularized regression [16, 2] of the form:",
      "startOffset" : 158,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "Considerable progress has been made over the past decade on high-dimensional structured estimation using suitable M-estimators or norm-regularized regression [16, 2] of the form:",
      "startOffset" : 158,
      "endOffset" : 165
    }, {
      "referenceID" : 12,
      "context" : "Early work focused on high-dimensional estimation of sparse vectors using the Lasso and related estimators, where R(θ) = ‖θ‖1 [13, 22, 23].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 21,
      "context" : "Early work focused on high-dimensional estimation of sparse vectors using the Lasso and related estimators, where R(θ) = ‖θ‖1 [13, 22, 23].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 22,
      "context" : "Early work focused on high-dimensional estimation of sparse vectors using the Lasso and related estimators, where R(θ) = ‖θ‖1 [13, 22, 23].",
      "startOffset" : 126,
      "endOffset" : 138
    }, {
      "referenceID" : 3,
      "context" : "Sample complexity of such estimators have been rigorously established based on the RIP(restricted isometry property) [4, 5] and the more general RE(restricted eigenvalue) conditions [3, 16, 2].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Sample complexity of such estimators have been rigorously established based on the RIP(restricted isometry property) [4, 5] and the more general RE(restricted eigenvalue) conditions [3, 16, 2].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "Sample complexity of such estimators have been rigorously established based on the RIP(restricted isometry property) [4, 5] and the more general RE(restricted eigenvalue) conditions [3, 16, 2].",
      "startOffset" : 182,
      "endOffset" : 192
    }, {
      "referenceID" : 15,
      "context" : "Sample complexity of such estimators have been rigorously established based on the RIP(restricted isometry property) [4, 5] and the more general RE(restricted eigenvalue) conditions [3, 16, 2].",
      "startOffset" : 182,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : "Sample complexity of such estimators have been rigorously established based on the RIP(restricted isometry property) [4, 5] and the more general RE(restricted eigenvalue) conditions [3, 16, 2].",
      "startOffset" : 182,
      "endOffset" : 192
    }, {
      "referenceID" : 15,
      "context" : "Several subsequent advances have considered structures beyond `1, using more general norms such as (overlapping) group sparse norms, k-support norm, nuclear norm, and so on [16, 8, 7].",
      "startOffset" : 173,
      "endOffset" : 183
    }, {
      "referenceID" : 7,
      "context" : "Several subsequent advances have considered structures beyond `1, using more general norms such as (overlapping) group sparse norms, k-support norm, nuclear norm, and so on [16, 8, 7].",
      "startOffset" : 173,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "Several subsequent advances have considered structures beyond `1, using more general norms such as (overlapping) group sparse norms, k-support norm, nuclear norm, and so on [16, 8, 7].",
      "startOffset" : 173,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "In recent years, much of the literature has been unified and nonasymptotic estimation error bound analysis techniques have been developed for regularized estimation with any norm [2].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 15,
      "context" : "In particular, recent unified treatments based on decomposable norms, atomic norms, or general norms all rely on concentration properties of subGaussian distributions [16, 7, 2].",
      "startOffset" : 167,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : "In particular, recent unified treatments based on decomposable norms, atomic norms, or general norms all rely on concentration properties of subGaussian distributions [16, 7, 2].",
      "startOffset" : 167,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "In particular, recent unified treatments based on decomposable norms, atomic norms, or general norms all rely on concentration properties of subGaussian distributions [16, 7, 2].",
      "startOffset" : 167,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "Certain estimators, such as the Dantzig selector and variants, consider a constrained problem rather than a regularized problem as in (2) but the analysis again relies on entries of X being sub-Gaussian [6, 8].",
      "startOffset" : 203,
      "endOffset" : 209
    }, {
      "referenceID" : 7,
      "context" : "Certain estimators, such as the Dantzig selector and variants, consider a constrained problem rather than a regularized problem as in (2) but the analysis again relies on entries of X being sub-Gaussian [6, 8].",
      "startOffset" : 203,
      "endOffset" : 209
    }, {
      "referenceID" : 9,
      "context" : "For the setting of constrained estimation, building on prior work by [10], [20] outlines a possible strategy for such analysis which can work for any distribution, but works out details only for the sub-Gaussian case.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "For the setting of constrained estimation, building on prior work by [10], [20] outlines a possible strategy for such analysis which can work for any distribution, but works out details only for the sub-Gaussian case.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "In recent work [9] considered sub-Gaussian design matrices but with heavy-tailed noise, and suggested modifying the estimator in (1) via a median-of-means type estimator based on multiple estimates of θ̂ from sub-samples.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 11,
      "context" : ", P (|Xij | > t) ≤ c1 exp(−t/c2) for all t ≥ 0 and for suitable constants c1, c2 [12, 21].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 20,
      "context" : ", P (|Xij | > t) ≤ c1 exp(−t/c2) for all t ≥ 0 and for suitable constants c1, c2 [12, 21].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "The analysis depends on two key ingredients which have been discussed in previous work [16, 2]: 1.",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "The analysis depends on two key ingredients which have been discussed in previous work [16, 2]: 1.",
      "startOffset" : 87,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "We outline two different approaches for obtaining the sample complexity, to satisfy the RE condition: one based on the ‘exponential width’ of A and another based on the VC-dimension of linear predictors drawn from A [10, 20, 11].",
      "startOffset" : 216,
      "endOffset" : 228
    }, {
      "referenceID" : 19,
      "context" : "We outline two different approaches for obtaining the sample complexity, to satisfy the RE condition: one based on the ‘exponential width’ of A and another based on the VC-dimension of linear predictors drawn from A [10, 20, 11].",
      "startOffset" : 216,
      "endOffset" : 228
    }, {
      "referenceID" : 10,
      "context" : "We outline two different approaches for obtaining the sample complexity, to satisfy the RE condition: one based on the ‘exponential width’ of A and another based on the VC-dimension of linear predictors drawn from A [10, 20, 11].",
      "startOffset" : 216,
      "endOffset" : 228
    }, {
      "referenceID" : 18,
      "context" : "Through a careful argument based on generic chaining [19], we show that for any set T ⊂ R, the exponential width we(T ) ≤ cwg(T ) √ log p, where wg(T ) is the Gaussian width of the set T and c is an absolute constant.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "Error set: Under the assumption λn ≥ βR∗( 1 nX T (y−Xθ∗)), β > 1, the error vector ∆̂n = θ̂−θ∗ lies in a cone A ⊆ Sp−1 [3, 16, 2].",
      "startOffset" : 119,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Error set: Under the assumption λn ≥ βR∗( 1 nX T (y−Xθ∗)), β > 1, the error vector ∆̂n = θ̂−θ∗ lies in a cone A ⊆ Sp−1 [3, 16, 2].",
      "startOffset" : 119,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "Error set: Under the assumption λn ≥ βR∗( 1 nX T (y−Xθ∗)), β > 1, the error vector ∆̂n = θ̂−θ∗ lies in a cone A ⊆ Sp−1 [3, 16, 2].",
      "startOffset" : 119,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Regularization parameter: For β > 1, λn ≥ βR∗( 1 nX T (y −Xθ)) following analysis in [16, 2].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Regularization parameter: For β > 1, λn ≥ βR∗( 1 nX T (y −Xθ)) following analysis in [16, 2].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "Restricted Eigenvalue (RE) conditions: For consistent estimation, the design matrix X should satisfy the following RE condition infu∈A 1 √n‖Xu‖2 ≥ κ on the error set A for some constant κ > 0 [3, 16, 2, 20, 18].",
      "startOffset" : 192,
      "endOffset" : 210
    }, {
      "referenceID" : 15,
      "context" : "Restricted Eigenvalue (RE) conditions: For consistent estimation, the design matrix X should satisfy the following RE condition infu∈A 1 √n‖Xu‖2 ≥ κ on the error set A for some constant κ > 0 [3, 16, 2, 20, 18].",
      "startOffset" : 192,
      "endOffset" : 210
    }, {
      "referenceID" : 1,
      "context" : "Restricted Eigenvalue (RE) conditions: For consistent estimation, the design matrix X should satisfy the following RE condition infu∈A 1 √n‖Xu‖2 ≥ κ on the error set A for some constant κ > 0 [3, 16, 2, 20, 18].",
      "startOffset" : 192,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "Restricted Eigenvalue (RE) conditions: For consistent estimation, the design matrix X should satisfy the following RE condition infu∈A 1 √n‖Xu‖2 ≥ κ on the error set A for some constant κ > 0 [3, 16, 2, 20, 18].",
      "startOffset" : 192,
      "endOffset" : 210
    }, {
      "referenceID" : 17,
      "context" : "Restricted Eigenvalue (RE) conditions: For consistent estimation, the design matrix X should satisfy the following RE condition infu∈A 1 √n‖Xu‖2 ≥ κ on the error set A for some constant κ > 0 [3, 16, 2, 20, 18].",
      "startOffset" : 192,
      "endOffset" : 210
    }, {
      "referenceID" : 1,
      "context" : "Deterministic recovery bounds: If X satisfies the RE condition on the error set A and λn satisfies the assumptions stated earlier, [2] show the error bound ‖∆̂n‖2 ≤ cΨ(A)n κ with high probability (w.",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : "One of our major results is the relationship between the Gaussian and exponential width of sets using arguments from generic chaining [19].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 1,
      "context" : "Existing analysis frameworks for our problem for sub-Gaussian X and ω obtain results in terms of Gaussian widths of suitable sets associated with the norm [2, 20].",
      "startOffset" : 155,
      "endOffset" : 162
    }, {
      "referenceID" : 19,
      "context" : "Existing analysis frameworks for our problem for sub-Gaussian X and ω obtain results in terms of Gaussian widths of suitable sets associated with the norm [2, 20].",
      "startOffset" : 155,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "While this strong result has already been explored in [11] for `1, we adapt it for our analysis framework and also extend it to the group-sparse setting.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "With this result, we establish precise sample complexity bounds by leveraging a body of literature on the computation of Gaussian widths for various structured sets [7, 20].",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "With this result, we establish precise sample complexity bounds by leveraging a body of literature on the computation of Gaussian widths for various structured sets [7, 20].",
      "startOffset" : 165,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "[19, 15], to the best of our knowledge this is the first result establishing the relation between the Gaussian and exponential widths of sets.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 14,
      "context" : "[19, 15], to the best of our knowledge this is the first result establishing the relation between the Gaussian and exponential widths of sets.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 18,
      "context" : "Our result relies on generic chaining [19].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "Definition 1 γ-functionals: [19] Given α > 0, and a metric space (T, d) we define",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "1] of [19] stated below: 1 L γ2(T, d2) ≤ wg(T ) ≤ Lγ2(T, d2) .",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 18,
      "context" : "5] in [19] any process which satisfies the concentration bound",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 18,
      "context" : "7] in [19], for some universal constant L, we(T ) satisfies: 1 L (γ2(T, d2) + γ1(T, d∞)) ≤ we(T ) ≤ L(γ2(T, d2) + γ1(T, d∞)) (5)",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 14,
      "context" : "Note that any process which satisfies the sub-exponential concentration bound P (|Xs−Xt| ≥ u) ≤ 2 exp ( −K min ( u(2) d2(s,t)(2) , u d∞(s,t) )) satisfies the upper bound in the above inequality [15, 19].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "Note that any process which satisfies the sub-exponential concentration bound P (|Xs−Xt| ≥ u) ≤ 2 exp ( −K min ( u(2) d2(s,t)(2) , u d∞(s,t) )) satisfies the upper bound in the above inequality [15, 19].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "Theorem 2 [19] Consider a countable set T ⊂ R, and a number u > 0.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 18,
      "context" : "6] [19], is to bound exponential width of set T .",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "If the regularization parameter λn ≥ βR∗( 1 nX T (y − Xθ∗)), β > 1 and the RE condition is satisfied on the error set A with RE constant κ, then [2, 16] obtain the following error bound w.",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 15,
      "context" : "If the regularization parameter λn ≥ βR∗( 1 nX T (y − Xθ∗)), β > 1 and the RE condition is satisfied on the error set A with RE constant κ, then [2, 16] obtain the following error bound w.",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 20,
      "context" : "Also by elementary arguments [21], e = X (ω/|ω‖2) has i.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "The above argument was first proposed for the sub-Gaussian case in [2].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 14,
      "context" : "As we move to heavier tails, establishing such two-sided bounds requires assumptions on the boundedness of the Euclidean norm of the rows ofX [15, 17, 10].",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 16,
      "context" : "As we move to heavier tails, establishing such two-sided bounds requires assumptions on the boundedness of the Euclidean norm of the rows ofX [15, 17, 10].",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "As we move to heavier tails, establishing such two-sided bounds requires assumptions on the boundedness of the Euclidean norm of the rows ofX [15, 17, 10].",
      "startOffset" : 142,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "Making these observations, [10, 17] develop arguments obtaining sub-Gaussian RE sample complexity bounds when set A is the unit sphere Sp−1 even for design matrices having only bounded fourth moments.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 16,
      "context" : "Making these observations, [10, 17] develop arguments obtaining sub-Gaussian RE sample complexity bounds when set A is the unit sphere Sp−1 even for design matrices having only bounded fourth moments.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 9,
      "context" : "Our analysis for the RE condition essentially follow this premise and arguments from [10].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 19,
      "context" : "The result we state below follows along similar arguments made in [20], which in turn are based on arguments from [10, 14].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "The result we state below follows along similar arguments made in [20], which in turn are based on arguments from [10, 14].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "The result we state below follows along similar arguments made in [20], which in turn are based on arguments from [10, 14].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Contrasting the result (17) with previous results for the sub-Gaussian case [2, 20] the dependence on wg(A) on the r.",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 19,
      "context" : "Contrasting the result (17) with previous results for the sub-Gaussian case [2, 20] the dependence on wg(A) on the r.",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "Note that results from [1] for `1 norm show RIP bounds w.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "The arguments follow along similar lines to [11, 10].",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "The arguments follow along similar lines to [11, 10].",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "The final step involves showing that the RE condition is satisfied on the error set A if it is satisfied on B using Maurey’s empirical approximation argument [17, 18, 11].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 17,
      "context" : "The final step involves showing that the RE condition is satisfied on the error set A if it is satisfied on B using Maurey’s empirical approximation argument [17, 18, 11].",
      "startOffset" : 158,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "The final step involves showing that the RE condition is satisfied on the error set A if it is satisfied on B using Maurey’s empirical approximation argument [17, 18, 11].",
      "startOffset" : 158,
      "endOffset" : 170
    } ],
    "year" : 2015,
    "abstractText" : "We consider the problem of high-dimensional structured estimation with normregularized estimators, such as Lasso, when the design matrix and noise are drawn from sub-exponential distributions. Existing results only consider sub-Gaussian designs and noise, and both the sample complexity and non-asymptotic estimation error have been shown to depend on the Gaussian width of suitable sets. In contrast, for the sub-exponential setting, we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets, and the analysis holds for any norm. Further, using generic chaining, we show that the exponential width for any set will be at most √ log p times the Gaussian width of the set, yielding Gaussian width based results even for the sub-exponential case. Further, for certain popular estimators, viz Lasso and Group Lasso, using a VCdimension based analysis, we show that the sample complexity will in fact be the same order as Gaussian designs. Our general analysis and results are the first in the sub-exponential setting, and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions.",
    "creator" : null
  }
}