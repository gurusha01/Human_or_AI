{
  "name" : "c45147dee729311ef5b5c3003946c48f.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution",
    "authors" : [ "Yan Huang", "Wei Wang", "Liang Wang" ],
    "emails" : [ "wangliang}@nlpr.ia.ac.cn" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Since large numbers of high-definition displays have sprung up, generating high-resolution videos from previous low-resolution contents, namely video super-resolution (SR), is under great demand. Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.\nExisting multi-frame SR methods generally model the temporal dependency by extracting subpixel motions of video frames, e.g., estimating optical flow based on sparse prior integration or variation regularity [2, 14, 13]. But such accurate motion estimation can only be effective for video sequences which contain small motions. In addition, the high computational cost of these methods limits the real-world applications. Several solutions have been explored to overcome these issues by avoiding the explicit motion estimation [21, 16]. Unfortunately, they still have to perform implicit motion estimation to reduce temporal aliasing and achieve resolution enhancement when large motions are encountered.\nGiven the fact that recurrent neural networks (RNNs) can well model long-term contextual information for video sequence, we propose a bidirectional recurrent convolutional network (BRCN)\nto efficiently learn the temporal dependency for multi-frame SR. The proposed network exploits three convolutions. 1) Feedforward convolution models visual spatial dependency between a lowresolution frame and its high-resolution result. 2) Recurrent convolution connects the hidden layers of successive frames to learn temporal dependency. Different from the commonly-used full recurrent connection in vanilla RNNs, it is a weight-sharing convolutional connection here. 3) Conditional convolution connects input layers at the previous timestep to the current hidden layer, to further enhance visual-temporal dependency modelling. To simultaneously consider the temporal dependency from both previous and future frames, we exploit a forward recurrent network and a backward recurrent network, respectively, and then combine them together for the final prediction. We apply the proposed model to super resolve videos with complex motions. The experimental results demonstrate that the model can achieve state-of-the-art performance, as well as orders of magnitude faster speed than other multi-frame SR methods.\nOur main contributions can be summarized as follows. We propose a bidirectional recurrent convolutional network for multi-frame SR, where the temporal dependency can be efficiently modelled by bidirectional recurrent and conditional convolutions. It is an end-to-end framework which does not need pre-/post-processing. We achieve better performance and faster speed than existing multiframe SR methods."
    }, {
      "heading" : "2 Related Work",
      "text" : "We will review the related work from the following prospectives.\nSingle-Image SR. Irani and Peleg [10] propose the primary work for this problem, followed by Freeman et al. [8] studying this problem in a learning-based way. To alleviate high computational complexity, Bevilacqua et al. [4] and Chang et al. [5] introduce manifold learning techniques which can reduce the required number of image patch exemplars. For further acceleration, Timofte et al. [23] propose the anchored neighborhood regression method. Yang et al. [25] and Zeyde et al. [26] exploit compressive sensing to encode image patches with a compact dictionary and obtain sparse representations. Dong et al. [6] learn a convolutional neural network for single-image SR which achieves the current state-of-the-art result. In this work, we focus on multi-frame SR by modelling temporal dependency in video sequences.\nMulti-Frame SR. Baker and Kanade [2] extract optical flow to model the temporal dependency in video sequences for video SR. Then, various improvements [14, 13] around this work are explored to better handle visual motions. However, these methods suffer from the high computational cost due to the motion estimation. To deal with this problem, Protter et al. [16] and Takeda et al. [21] avoid motion estimation by employing nonlocal mean and 3D steering kernel regression. In this work, we propose bidirectional recurrent and conditional convolutions as an alternative to model temporal dependency and achieve faster speed."
    }, {
      "heading" : "3 Bidirectional Recurrent Convolutional Network",
      "text" : ""
    }, {
      "heading" : "3.1 Formulation",
      "text" : "Given a low-resolution, noisy and blurry video, our goal is to obtain a high-resolution, noise-free and blur-free version. In this paper, we propose a bidirectional recurrent convolutional network (BRCN) to map the low-resolution frames to high-resolution ones. As shown in Figure 1, the proposed network contains a forward recurrent convolutional sub-network and a backward recurrent convolutional sub-network to model the temporal dependency from both previous and future frames. Note that similar bidirectional scheme has been proposed previously in [18]. The two sub-networks of BRCN are denoted by two black blocks with dash borders, respectively. In each sub-network, there are four layers including the input layer, the first hidden layer, the second hidden layer and the output layer, which are connected by three convolutional operations:\n1. Feedforward Convolution. The multi-layer convolutions denoted by black lines learn visual spatial dependency between a low-resolution frame and its high-resolution result. Similar configurations have also been explored previously in [11, 7, 6].\n2. Recurrent Convolution. The convolutions denoted by blue lines aim to model long-term temporal dependency across video frames by connecting adjacent hidden layers of successive frames, where the current hidden layer is conditioned on the hidden layer at the previous timestep. We use the recurrent convolution in both forward and backward subnetworks. Such bidirectional recurrent scheme can make full use of the forward and backward temporal dynamics.\n3. Conditional Convolution. The convolutions denoted by red lines connect input layer at the previous timestep to the current hidden layer, and use previous inputs to provide longterm contextual information. They enhance visual-temporal dependency modelling with this kind of conditional connection.\nWe denote the frame sets of a low-resolution video1 X as {Xi}i=1,2,...,T , and infer the other three layers as follows.\nFirst Hidden Layer. When inferring the first hidden layer Hf1 (Xi) (or Hb1(Xi)) at the ith timestep in the forward (or backward) sub-network, three inputs are considered: 1) the current input layer Xi connected by a feedforward convolution, 2) the hidden layer H f 1 (Xi−1) (or H b 1(Xi+1)) at the i−1th (or i+1th) timestep connected by a recurrent convolution, and 3) the input layer Xi−1 (or Xi+1) at the i−1th (or i+1th) timestep connected by a conditional convolution.\nHf1 (Xi) = λ(W f v1 ∗Xi + W f r1 ∗H f 1 (Xi−1) + W f t1 ∗Xi−1 + B f 1 ) Hb1(Xi) = λ(W b v1 ∗Xi + W b r1 ∗H b 1(Xi+1) + W b t1 ∗Xi+1 + B b 1)\n(1)\nwhere Wfv1 (or W b v1 ) and W f t1 (or W b t1 ) represent the filters of feedforward and conditional convolutions in the forward (or backward) sub-network, respectively. Both of them have the size of c×fv1×fv1×n1, where c is the number of input channels, fv1 is the filter size and n1 is the number of filters. Wfr1 (or W b r1 ) represents the filters of recurrent convolutions. Their filter size fr1 is set to 1 to avoid border effects. Bf1 (or B b 1) represents biases. The activation function is the rectified linear unit (ReLu): λ(x)=max(0, x) [15]. Note that in Equation 1, the filter responses of recurrent and\n1Note that we upscale each low-resolution frame in the sequence to the desired size with bicubic interpolation in advance.\nconditional convolutions can be regarded as dynamic changing biases, which focus on modelling the temporal changes across frames, while the filter responses of feedforward convolution focus on learning visual content.\nSecond Hidden Layer. This phase projects the obtained feature maps Hf1 (Xi) (or Hb1(Xi)) from n1 to n2 dimensions, which aims to capture the nonlinear structure in sequence data. In addition to intra-frame mapping by feedforward convolution, we also consider two inter-frame mappings using recurrent and conditional convolutions, respectively. The projected n2-dimensional feature maps in the second hidden layer Hf2 (Xi) (or (H b 2(Xi)) in the forward (or backward) sub-network can be obtained as follows:\nHf2 (Xi) = λ(W f v2 ∗H f 1 (Xi) + W f r2 ∗H f 2 (Xi−1) + W f t2 ∗H f 1 (Xi−1) + B f 2 ) Hb2(Xi) = λ(W b v2 ∗H b 1(Xi) + W b r2 ∗H b 2(Xi+1) + W b t2 ∗H b 1(Xi+1) + B b 2)\n(2)\nwhere Wfv2 (or W b v2 ) and W f t2 (or W b t2 ) represent the filters of feedforward and conditional convolutions, respectively, both of which have the size of n1×1×1×n2. Wfr2 (or W b r2 ) represents the filters of recurrent convolution, whose size is n2×1×1×n2. Note that the inference of the two hidden layers can be regarded as a representation learning phase, where we could stack more hidden layers to increase the representability of our network to better capture the complex data structure.\nOutput Layer. In this phase, we combine the projected n2-dimensional feature maps in both forward and backward sub-networks to jointly predict the desired high-resolution frame:\nO(Xi) =W f v3 ∗H f 2 (Xi) + W f t3 ∗H f 2 (Xi−1) + B f 3 + W b v3 ∗H b 2(Xi) + W b t3 ∗H b 2(Xi+1) + B b 3 (3) where Wfv3 (or W b v3 ) and W f t3 (or W b t3 ) represent the filters of feedforward and conditional convolutions, respectively. Their sizes are both n2×fv3×fv3×c. We do not use any recurrent convolution for output layer."
    }, {
      "heading" : "3.2 Connection with Temporal Restricted Boltzmann Machine",
      "text" : "In this section, we discuss the connection between the proposed BRCN and temporal restricted boltzmann machine (TRBM) [20] which is a widely used model in sequence modelling.\nAs shown in Figure 2, TRBM and BRCN contain similar recurrent connections (blue lines) between hidden layers, and conditional connections (red lines) between input layer and hidden layer. They share the common flexibility to model and propagate temporal dependency along the time. However, TRBM is a generative model while BRCN is a discriminative model, and TRBM contains an additional connection (green line) between input layers for sample generation.\nIn fact, BRCN can be regarded as a deterministic, bidirectional and patch-based implementation of TRBM. Specifically, when inferring the hidden layer in BRCN, as illustrated in Figure 2 (b), feedforward and conditional convolutions extract overlapped patches from the input, each of which is\nfully connected to a n1-dimensional vector in the feature maps H f 1 (Xi). For recurrent convolutions, since each filter size is 1 and all the filters contain n1×n1 weights, a n1-dimensional vector in Hf1 (Xi) is fully connected to the corresponding n1-dimensional vector in H f 1 (Xi−1) at the previous time step. Therefore, the patch connections of BRCN are actually those of a “discriminative” TRBM. In other words, by setting the filter sizes of feedforward and conditional convolutions as the size of the whole frame, BRCN is equivalent to TRBM.\nCompared with TRBM, BRCN has the following advantages for handling the task of video superresolution. 1) BRCN restricts the receptive field of original full connection to a patch rather than the whole frame, which can capture the temporal change of visual details. 2) BRCN replaces all the full connections with weight-sharing convolutional ones, which largely reduces the computational cost. 3) BRCN is more flexible to handle videos of different sizes, once it is trained on a fixed-size video dataset. Similar to TRBM, the proposed model can be generalized to other sequence modelling applications, e.g., video motion modelling [22]."
    }, {
      "heading" : "3.3 Network Learning",
      "text" : "Through combining Equations 1, 2 and 3, we can obtain the desired prediction O(X ; Θ) from the low-resolution video X , where Θ denotes the network parameters. Network learning proceeds by minimizing the Mean Square Error (MSE) between the predicted high-resolution video O(X ; Θ) and the groundtruth Y:\nL = ‖O(X ; Θ)− Y‖2 (4)\nvia stochastic gradient descent. Actually, stochastic gradient descent is enough to achieve satisfying results, although we could exploit other optimization algorithms with more computational cost, e.g., L-BFGS. During optimization, all the filter weights of recurrent and conditional convolutions are initialized by randomly sampling from a Gaussian distribution with mean 0 and standard deviation 0.001, whereas the filter weights of feedforward convolution are pre-trained on static images [6]. Note that the pretraining step only aims to speed up training by providing a better parameter initialization, due to the limited size of training set. This step can be avoided by alternatively using a larger scale dataset. We experimentally find that using a smaller learning rate (e.g., 1e−4) for the weights in the output layer is crucial to obtain good performance."
    }, {
      "heading" : "4 Experimental Results",
      "text" : "To verify the effectiveness, we apply the proposed model to the task of video SR, and present both quantitative and qualitative results as follows."
    }, {
      "heading" : "4.1 Datasets and Implementation Details",
      "text" : "We use 25 YUV format video sequences2 as our training set, which have been widely used in many video SR methods [13, 16, 21]. To enlarge the training set, model training is performed in a volumebased way, i.e., cropping multiple overlapped volumes from training videos and then regarding each volume as a training sample. During cropping, each volume has a spatial size of 32×32 and a temporal step of 10. The spatial and temporal strides are 14 and 8, respectively. As a result, we can generate roughly 41,000 volumes from the original dataset. We test our model on a variety of challenging videos, including Dancing, Flag, Fan, Treadmill and Turbine [19], which contain complex motions with severe motion blur and aliasing. Note that we do not have to extract volumes during testing, since the convolutional operation can scale to videos of any spatial size and temporal step. We generate the testing dataset with the following steps: 1) using Gaussian filter with standard deviation 2 to smooth each original frame, and 2) downsampling the frame by a factor of 4 with bicubic method3.\n2http://www.codersvoice.com/a/webbase/video/08/152014/130.html. 3Here we focus on the factor of 4, which is usually considered as the most difficult case in super-resolution.\nSome important parameters of our network are illustrated as follows: fv1=9, fv3=5, n1=64, n2=32 and c=14. Note that varying the number and size of filters does not have a significant impact on the performance, because some filters with certain sizes are already in a regime where they can almost reconstruct the high-resolution videos [24, 6]."
    }, {
      "heading" : "4.2 Quantitative and Qualitative Comparison",
      "text" : "We compare our BRCN with two multi-frame SR methods including 3DSKR [21] and a commercial software namely Enhancer [1], and seven single-image SR methods including Bicubic, SC [25], KSVD [26], NE+NNLS [4], ANR [23], NE+LLE [5] and SR-CNN [6].\nThe results of all the methods are compared in Table 1, where evaluation measures include both peak signal-to-noise ratio (PSNR) and running time (Time). Specifically, compared with the state-of-theart single-image SR methods (e.g., SR-CNN, ANR and K-SVD), our multi-frame-based method can surpass them by 0.28∼0.54 dB, which is mainly attributed to the beneficial mechanism of temporal dependency modelling. BRCN also performs much better than the two representative multi-frame SR methods (3DSKR and Enhancer) by 1.51 dB and 1.63 dB, respectively. In fact, most existing multi-frame-based methods tend to fail catastrophically when dealing with very complex motions, because it is difficult for them to estimate the motions with pinpoint accuracy.\nFor the proposed BRCN, we also investigate the impact of model architecture on the performance. We take a simplified network containing only feedforward convolution as a benchmark, and then study its several variants by successively adding other operations including bidirectional scheme, recurrent and conditional convolutions. The results by all the variants of BRCN are shown in Table 2, where the elements in the brace represent the included operations. As we can see, due to the ben-\n4Similar to [23], we only deal with luminance channel in the YCrCb color space. Note that our model can be generalized to handle all the three channels by setting c=3. Here we simply upscale the other two channels with bicubic method for well illustration.\nefit of learning temporal dependency, exploiting either recurrent convolution {v, r} or conditional convolution {v, t} can greatly improve the performance. When combining these two convolutions together {v, r, t}, they obtain much better results. The performance can still be further promoted when adding the bidirectional scheme {v, r, t, b}, which results from the fact that each video frame is related to not only its previous frame but also the future one.\nIn addition to the quantitative evaluation, we also present some qualitative results in terms of singleframe (in Figure 3) and multi-frame (in Figure 5). Please enlarge and view these figures on the screen for better comparison. From these figures, we can observe that our method is able to recover more image details than others under various motion conditions.\n4.3 Running Time\nWe present the comparison of running time in both Table 1 and Figure 4, where all the methods are implemented on the same machine (Intel CPU 3.10 GHz and 32 GB memory). The publicly available codes of compared methods are all in MATLAB while SR-CNN and ours are in Python. From the table and figure, we can see that our BRCN takes 1.36 sec per frame on average, which is orders of magnitude faster than the fast multi-frame SR method 3DSKR. It should be noted that the speed gap is not caused by the different MATLAB/Python implementations. As stated in [13, 21], the computational bottleneck for existing multi-frame SR methods is the accurate motion estimation, while our model explores an alternative based on efficient spatial-temporal con-\nvolutions which has lower computational complexity. Note that the speed of our method is worse than the fastest single-image SR method ANR. It is likely that our method involves the additional phase of temporal dependency modelling but we achieve better performance (28.15 vs. 27.59 dB)."
    }, {
      "heading" : "4.4 Filter Visualization",
      "text" : "We visualize the learned filters of feedforward and conditional convolutions in Figure 6. The filters of Wfv1 and W f t1 exhibit some strip-like patterns, which can be viewed as edge detectors. The filters of Wfv3 and W f t3 show some centrally-averaging patterns, which indicate that the predicted highresolution frame is obtained by averaging over the feature maps in the second hidden layer. This averaging operation is also in consistent with the corresponding reconstruction phase in patch-based SR methods (e.g., [25]), but the difference is that our filters are automatically learned rather than pre-defined. When comparing the learned filters between feedforward and conditional convolutions, we can also observe that the patterns in the filters of feedforward convolution are much more regular and clear."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we have proposed the bidirectional recurrent convolutional network (BRCN) for multiframe SR. Our main contribution is the novel use of bidirectional scheme, recurrent and conditional convolutions for temporal dependency modelling. We have applied our model to super resolve videos containing complex motions, and achieved better performance and faster speed. In the future, we will perform comparisons with other multi-frame SR methods."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is jointly supported by National Natural Science Foundation of China (61420106015, 61175003, 61202328, 61572504) and National Basic Research Program of China (2012CB316300)."
    } ],
    "references" : [ {
      "title" : "Super-resolution optical flow",
      "author" : [ "S. Baker", "T. Kanade" ],
      "venue" : "Technical report,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1999
    }, {
      "title" : "Motion deblurring and super-resolution from an image sequence",
      "author" : [ "B. Bascle", "A. Blake", "A. Zisserman" ],
      "venue" : "European Conference on Computer Vision,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1996
    }, {
      "title" : "Low-complexity single-image superresolution based on nonnegative neighbor embedding",
      "author" : [ "M. Bevilacqua", "A. Roumy", "C. Guillemot", "M.-L.A. Morel" ],
      "venue" : "British Machine Vision Conference,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Super-resolution through neighbor embedding",
      "author" : [ "H. Chang", "D.-Y. Yeung", "Y. Xiong" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Learning a deep convolutional network for image superresolution",
      "author" : [ "C. Dong", "C.C. Loy", "K. He", "X. Tang" ],
      "venue" : "European Conference on Computer Vision,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Restoring an image taken through a window covered with dirt or rain",
      "author" : [ "D. Eigen", "D. Krishnan", "R. Fergus" ],
      "venue" : "IEEE International Conference on Computer Vision,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Learning low-level vision",
      "author" : [ "W.T. Freeman", "E.C. Pasztor", "O.T. Carmichael" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Super-resolution from a single image",
      "author" : [ "D. Glasner", "S. Bagon", "M. Irani" ],
      "venue" : "IEEE International Conference on Computer Vision,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Improving resolution by image registration",
      "author" : [ "M. Irani", "S. Peleg" ],
      "venue" : "CVGIP: Graphical Models and Image Processing,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1991
    }, {
      "title" : "Natural image denoising with convolutional networks",
      "author" : [ "V. Jain", "S. Seung" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Image transformation based on learning dictionaries across image spaces",
      "author" : [ "K. Jia", "X. Wang", "X. Tang" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "On bayesian adaptive video super resolution",
      "author" : [ "C. Liu", "D. Sun" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Video super resolution using duality based tv-l 1 optical flow",
      "author" : [ "D. Mitzel", "T. Pock", "T. Schoenemann", "D. Cremers" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "V. Nair", "G.E. Hinton" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Generalizing the nonlocal-means to super-resolution reconstruction",
      "author" : [ "M. Protter", "M. Elad", "H. Takeda", "P. Milanfar" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Extraction of high-resolution frames from video sequences",
      "author" : [ "R.R. Schultz", "R.L. Stevenson" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1996
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "M. Schusterand", "K.K. Paliwal" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1997
    }, {
      "title" : "Space-time super-resolution from a single video",
      "author" : [ "O. Shahar", "A. Faktor", "M. Irani" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Learning multilevel distributed representations for high-dimensional sequences",
      "author" : [ "I. Sutskever", "G.E. Hinton" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Super-resolution without explicit subpixel motion estimation",
      "author" : [ "H. Takeda", "P. Milanfar", "M. Protter", "M. Elad" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1958
    }, {
      "title" : "Modeling human motion using binary latent variables",
      "author" : [ "G. Taylor", "G. Hinton", "S. Roweis" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2006
    }, {
      "title" : "Anchored neighborhood regression for fast example-based superresolution",
      "author" : [ "R. Timofte", "V. De", "L.V. Gool" ],
      "venue" : "IEEE International Conference on Computer",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1920
    }, {
      "title" : "Deep convolutional neural network for image deconvolution",
      "author" : [ "L. Xu", "J.S. Ren", "C. Liu", "J. Jia" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Image super-resolution via sparse representation",
      "author" : [ "J. Yang", "J. Wright", "T.S. Huang", "Y. Ma" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "On single image scale-up using sparse-representations",
      "author" : [ "R. Zeyde", "M. Elad", "M. Protte" ],
      "venue" : "Curves and Surfaces,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 3,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 7,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 6,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 10,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 23,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 21,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 236,
      "endOffset" : 258
    }, {
      "referenceID" : 15,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 236,
      "endOffset" : 258
    }, {
      "referenceID" : 1,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 236,
      "endOffset" : 258
    }, {
      "referenceID" : 0,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 236,
      "endOffset" : 258
    }, {
      "referenceID" : 12,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 236,
      "endOffset" : 258
    }, {
      "referenceID" : 11,
      "context" : "Recently, various methods have been proposed to handle this problem, which can be classified into two categories: 1) single-image SR [10, 5, 9, 8, 12, 25, 23] super resolves each of the video frames independently, and 2) multi-frame SR [13, 17, 3, 2, 14, 13] models and exploits temporal dependency among video frames, which is usually considered as an essential component of video SR.",
      "startOffset" : 236,
      "endOffset" : 258
    }, {
      "referenceID" : 0,
      "context" : ", estimating optical flow based on sparse prior integration or variation regularity [2, 14, 13].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : ", estimating optical flow based on sparse prior integration or variation regularity [2, 14, 13].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : ", estimating optical flow based on sparse prior integration or variation regularity [2, 14, 13].",
      "startOffset" : 84,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "Several solutions have been explored to overcome these issues by avoiding the explicit motion estimation [21, 16].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 14,
      "context" : "Several solutions have been explored to overcome these issues by avoiding the explicit motion estimation [21, 16].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "Irani and Peleg [10] propose the primary work for this problem, followed by Freeman et al.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "[8] studying this problem in a learning-based way.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] introduce manifold learning techniques which can reduce the required number of image patch exemplars.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 21,
      "context" : "[23] propose the anchored neighborhood regression method.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] exploit compressive sensing to encode image patches with a compact dictionary and obtain sparse representations.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[6] learn a convolutional neural network for single-image SR which achieves the current state-of-the-art result.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "Baker and Kanade [2] extract optical flow to model the temporal dependency in video sequences for video SR.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 12,
      "context" : "Then, various improvements [14, 13] around this work are explored to better handle visual motions.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "Then, various improvements [14, 13] around this work are explored to better handle visual motions.",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 19,
      "context" : "[21] avoid motion estimation by employing nonlocal mean and 3D steering kernel regression.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Note that similar bidirectional scheme has been proposed previously in [18].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "Similar configurations have also been explored previously in [11, 7, 6].",
      "startOffset" : 61,
      "endOffset" : 71
    }, {
      "referenceID" : 5,
      "context" : "Similar configurations have also been explored previously in [11, 7, 6].",
      "startOffset" : 61,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "Similar configurations have also been explored previously in [11, 7, 6].",
      "startOffset" : 61,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "The activation function is the rectified linear unit (ReLu): λ(x)=max(0, x) [15].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : "In this section, we discuss the connection between the proposed BRCN and temporal restricted boltzmann machine (TRBM) [20] which is a widely used model in sequence modelling.",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 4,
      "context" : "001, whereas the filter weights of feedforward convolution are pre-trained on static images [6].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "We use 25 YUV format video sequences2 as our training set, which have been widely used in many video SR methods [13, 16, 21].",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 14,
      "context" : "We use 25 YUV format video sequences2 as our training set, which have been widely used in many video SR methods [13, 16, 21].",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 19,
      "context" : "We use 25 YUV format video sequences2 as our training set, which have been widely used in many video SR methods [13, 16, 21].",
      "startOffset" : 112,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "We test our model on a variety of challenging videos, including Dancing, Flag, Fan, Treadmill and Turbine [19], which contain complex motions with severe motion blur and aliasing.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "Video Bicubic SC [25] K-SVD [26] NE+NNLS [4] ANR [23] PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time Dancing 26.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 24,
      "context" : "Video Bicubic SC [25] K-SVD [26] NE+NNLS [4] ANR [23] PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time Dancing 26.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "Video Bicubic SC [25] K-SVD [26] NE+NNLS [4] ANR [23] PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time Dancing 26.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 21,
      "context" : "Video Bicubic SC [25] K-SVD [26] NE+NNLS [4] ANR [23] PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time Dancing 26.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Video NE+LLE [5] SR-CNN [6] 3DSKR [21] Enhancer [1] BRCN PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time Dancing 27.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 4,
      "context" : "Video NE+LLE [5] SR-CNN [6] 3DSKR [21] Enhancer [1] BRCN PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time Dancing 27.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 19,
      "context" : "Video NE+LLE [5] SR-CNN [6] 3DSKR [21] Enhancer [1] BRCN PSNR Time PSNR Time PSNR Time PSNR Time PSNR Time Dancing 27.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "Note that varying the number and size of filters does not have a significant impact on the performance, because some filters with certain sizes are already in a regime where they can almost reconstruct the high-resolution videos [24, 6].",
      "startOffset" : 229,
      "endOffset" : 236
    }, {
      "referenceID" : 4,
      "context" : "Note that varying the number and size of filters does not have a significant impact on the performance, because some filters with certain sizes are already in a regime where they can almost reconstruct the high-resolution videos [24, 6].",
      "startOffset" : 229,
      "endOffset" : 236
    }, {
      "referenceID" : 19,
      "context" : "We compare our BRCN with two multi-frame SR methods including 3DSKR [21] and a commercial software namely Enhancer [1], and seven single-image SR methods including Bicubic, SC [25], KSVD [26], NE+NNLS [4], ANR [23], NE+LLE [5] and SR-CNN [6].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 23,
      "context" : "We compare our BRCN with two multi-frame SR methods including 3DSKR [21] and a commercial software namely Enhancer [1], and seven single-image SR methods including Bicubic, SC [25], KSVD [26], NE+NNLS [4], ANR [23], NE+LLE [5] and SR-CNN [6].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "We compare our BRCN with two multi-frame SR methods including 3DSKR [21] and a commercial software namely Enhancer [1], and seven single-image SR methods including Bicubic, SC [25], KSVD [26], NE+NNLS [4], ANR [23], NE+LLE [5] and SR-CNN [6].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "We compare our BRCN with two multi-frame SR methods including 3DSKR [21] and a commercial software namely Enhancer [1], and seven single-image SR methods including Bicubic, SC [25], KSVD [26], NE+NNLS [4], ANR [23], NE+LLE [5] and SR-CNN [6].",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 21,
      "context" : "We compare our BRCN with two multi-frame SR methods including 3DSKR [21] and a commercial software namely Enhancer [1], and seven single-image SR methods including Bicubic, SC [25], KSVD [26], NE+NNLS [4], ANR [23], NE+LLE [5] and SR-CNN [6].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 3,
      "context" : "We compare our BRCN with two multi-frame SR methods including 3DSKR [21] and a commercial software namely Enhancer [1], and seven single-image SR methods including Bicubic, SC [25], KSVD [26], NE+NNLS [4], ANR [23], NE+LLE [5] and SR-CNN [6].",
      "startOffset" : 223,
      "endOffset" : 226
    }, {
      "referenceID" : 4,
      "context" : "We compare our BRCN with two multi-frame SR methods including 3DSKR [21] and a commercial software namely Enhancer [1], and seven single-image SR methods including Bicubic, SC [25], KSVD [26], NE+NNLS [4], ANR [23], NE+LLE [5] and SR-CNN [6].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 21,
      "context" : "Similar to [23], we only deal with luminance channel in the YCrCb color space.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 21,
      "context" : "(a) Original (b) Bicubic (c) ANR [23] (d) SR-CNN [6] (e) BRCN",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "(a) Original (b) Bicubic (c) ANR [23] (d) SR-CNN [6] (e) BRCN",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "As stated in [13, 21], the computational bottleneck for existing multi-frame SR methods is the accurate motion estimation, while our model explores an alternative based on efficient spatial-temporal convolutions which has lower computational complexity.",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 19,
      "context" : "As stated in [13, 21], the computational bottleneck for existing multi-frame SR methods is the accurate motion estimation, while our model explores an alternative based on efficient spatial-temporal convolutions which has lower computational complexity.",
      "startOffset" : 13,
      "endOffset" : 21
    }, {
      "referenceID" : 21,
      "context" : "(a) Original (b) Bicubic (c) ANR [23] (d) SR-CNN [6] (e) BRCN",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "(a) Original (b) Bicubic (c) ANR [23] (d) SR-CNN [6] (e) BRCN",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 23,
      "context" : ", [25]), but the difference is that our filters are automatically learned rather than pre-defined.",
      "startOffset" : 2,
      "endOffset" : 6
    } ],
    "year" : 2015,
    "abstractText" : "Super resolving a low-resolution video is usually handled by either single-image super-resolution (SR) or multi-frame SR. Single-Image SR deals with each video frame independently, and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution. Multi-Frame SR generally extracts motion information, e.g., optical flow, to model the temporal dependency, which often shows high computational cost. Considering that recurrent neural networks (RNNs) can model long-term contextual information of temporal sequences well, we propose a bidirectional recurrent convolutional network for efficient multi-frame SR. Different from vanilla RNNs, 1) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2) conditional convolutional connections from previous input layers to the current hidden layer are added for enhancing visual-temporal dependency modelling. With the powerful temporal dependency modelling, our model can super resolve videos with complex motions and achieve state-of-the-art performance. Due to the cheap convolution operations, our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods.",
    "creator" : null
  }
}