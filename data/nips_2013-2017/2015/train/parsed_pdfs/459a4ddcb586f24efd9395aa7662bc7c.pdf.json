{
  "name" : "459a4ddcb586f24efd9395aa7662bc7c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A hybrid sampler for Poisson-Kingman mixture models",
    "authors" : [ "Stefano Favaro" ],
    "emails" : [ "mlomeli@gatsby.ucl.ac.uk", "stefano.favaro@unito.it", "y.w.teh@stats.ox.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "According to Ghahramani [9], models that have a nonparametric component give us more flexiblity that could lead to better predictive performance. This is because their capacity to learn does not saturate hence their predictions should continue to improve as we get more and more data. Furthermore, we are able to fully consider our uncertainty about predictions thanks to the Bayesian paradigm. However, a major impediment to the widespread use of Bayesian nonparametric models is the problem of inference. Over the years, many MCMC methods have been proposed to perform inference which usually rely on a tailored representation of the underlying process [5, 4, 18, 20, 28, 6]. This is an active research area since dealing with this infinite dimensional component forbids the direct use of standard simulation-based methods for posterior inference. These methods usually require a finite-dimensional representation. There are two main sampling approaches to facilitate simulation in the case of Bayesian nonparametric models: random truncation and marginalization. These two schemes are known in the literature as conditional and marginal samplers.\nIn conditional samplers, the infinite-dimensional prior is replaced by a finite-dimensional representation chosen according to a truncation level. In marginal samplers, the need to represent the infinite-dimensional component can be bypassed by marginalising it out. Marginal samplers have less storage requirements than conditional samplers but could potentially have worst mixing properties. However, not integrating out the infinite dimensional compnent leads to a more comprehensive representation of the random probability measure, useful to compute expectations of interest with respect to the posterior.\nIn this paper, we propose a novel MCMC sampler for Poisson-Kingman mixture models, a very large class of Bayesian nonparametric mixture models that encompass all previously explored ones in the literature. Our approach is based on a hybrid scheme that combines the main strengths of\nboth conditional and marginal samplers. In the flavour of probabilistic programming, we view our contribution as a step towards wider usage of flexible Bayesian nonparametric models, as it allows automated inference in probabilistic programs built out of a wide variety of Bayesian nonparametric building blocks."
    }, {
      "heading" : "2 Poisson-Kingman processes",
      "text" : "Poisson-Kingman random probability measures (RPMs) have been introduced in Pitman [23] as a generalization of homogeneous Normalized Random Measures (NRMs) [25, 13]. Let X be a complete and separable metric space endowed with the Borel σ-field BpXq, let µ „ CRMpρ,H0q be a homogeneous Completely Random Measure (CRM) with Lévy measure ρ and base distribution H0, see Kingman [15] for a good overview about CRMs and references therein. Then, the corresponding total mass of µ is T “ µpXq and let it be finite, positive almost surely, and absolutely continuous with respect to Lebesgue measure. For any t P R`, let us consider the conditional distribution of µ{t given that the total mass T P dt. This distribution is denoted by PKpρ, δt, H0q, it is the distribution of a RPM, where δt denotes the usual Dirac delta function. Poisson-Kingman RPMs form a class of RPMs whose distributions are obtained by mixing PKpρ, δt, H0q, over t, with respect to some distribution γ on the positive real line. Specifically, a Poisson-Kingman RPM has following the hierarchical representation\nT „ γ P |T “ t „ PKpρ, δt, H0q. (1)\nThe RPM P is referred to as the Poisson-Kingman RPM with Lévy measure ρ, base distribution H0 and mixing distribution γ. Throughout the paper we denote by PKpρ, γ,H0q the distribution of P and, without loss of generality, we will assume that γpdtq9hptqfρptqdt where fρ is the density of the total mass T under the CRM and h is a non-negative function. Note that, when γpdtq “ fρptqdt then the distribution PKpρ, fρ, H0q coincides with NRMpρ,H0q. The resulting P “ ř\nkě1 pkδφk is almost surely discrete and since µ is homogeneous, the atoms pφkqkě1 of P are independent of their masses ppkqkě1 and form a sequence of independent random variables identically distributed according to H0. Finally, the masses of P have distribution governed by the Lévy measure ρ and the distribution γ.\nOne nice property is that P is almost surely discrete: if we obtain a sample tYiuni“1 from it, there is a positive probability of Yi “ Yj for each pair of indexes i ‰ j. Hence, it induces a random partition Π on N, where i and j are in the same block in Π if and only if Yi “ Yj . Kingman [16] showed that Π is exchangeable, this property will be one of the main tools for the derivation of our hybrid sampler."
    }, {
      "heading" : "2.1 Size-biased sampling Poisson-Kingman processes",
      "text" : "A second object induced by a Poisson-Kingman RPM is a size-biased permutation of its atoms. Specifically, order the blocks in Π by increasing order of the least element in each block, and for each k P N let Zk be the least element of the kth block. Zk is the index among pYiqiě1 of the first appearance of the kth unique value in the sequence. Let J̃k “ µptYZkuq be the mass of the corresponding atom in µ. Then pJ̃kqkě1 is a size-biased permutation of the masses of atoms in µ, with larger masses tending to appear earlier in the sequence. It is easy to see that ř\nkě1 J̃k “ T , and that the sequence can be understood as a stick-breaking construction: starting with a stick of length T0 “ T ; break off the first piece of length J̃1; the surplus length of stick is T1 “ T0 ´ J̃1; then the second piece with length J̃2 is broken off, etc.\nTheorem 2.1 of Perman et al. [21] states that the sequence of surplus masses pTkqkě0 forms a Markov chain and gives the corresponding initial distribution and transition kernels. The corresponding generative process for the sequence pYiqiě1 is as follows:\ni) Start with drawing the total mass from its distribution Pρ,h,H0pT P dtq9hptqfρptqdt. ii) The first draw Y1 from P is a size-biased pick from the masses of µ. The actual value of Y1\nis simply Y ˚1 „ H0, while the mass of the corresponding atom in µ is J̃1, with conditional\ndistribution\nPρ,h,H0pJ̃1 P ds1|T P dtq “ s1 t ρpds1q fρpt´ s1q fρptq , with surplus mass T1 “ T ´ J̃1.\niii) For subsequent draws i ě 2: – Let K be the current number of distinct values among Y1, . . . , Yi´1, and Y ˚1 , . . . , Y ˚K\nthe unique values, i.e., atoms in µ. The masses of these first K atoms are denoted by J̃1, . . . , J̃K and the surplus mass is TK “ T ´ řK k“1 J̃k. – For each k ď K, with probability J̃k{T , we set Yi “ Y ˚k . – With probability TK{T , Yi takes on the value of an atom in µ besides the first K\natoms. The actual value Y ˚K`1 is drawn from H0, while its mass is drawn from\nPρ,h,H0pJ̃K`1 P dsK`1|TK P dtKq “ sK`1 tK ρpdsK`1q fρptK ´ sK`1q fρptKq , TK`1 “ TK´J̃K`1.\nBy multiplying the above infinitesimal probabilities, one obtains the joint distribution of the random elements T , Π, pJ̃iqiě1 and pY ˚i qiě1\nPρ,h,H0pΠn “ pckqkPrKs, Y ˚k P dy˚k , J̃k P dsk for k P rKs, T P dtq (2)\n“ t´nfρpt´ řK k“1 skqhptqdt\nK ź k“1 s |ck| k ρpdskqH0pdy ˚ k q,\nwhere pckqkPrKs denotes a particular partition of rns with K blocks, c1, . . . , cK , ordered by increasing least element and |ck| is the cardinality of block ck. The distribution (2) is invariant to the size-biased order. Such a joint distribution was first obtained in Pitman [23] , see also Pitman [24] for further details."
    }, {
      "heading" : "2.2 Relationship to the usual Stick-breaking construction",
      "text" : "In the generative process above, we mentioned that it is reminiscent of the well known stick breaking construction from Ishwaran & James [12], where you break a stick of length one but it is not the same. However, we can effectively reparameterize the model, starting with Equation (2), due to two useful identities in distribution: Pj\nd“ J̃j T´ ř `ăj J̃` and Vj d“ Pj1´ř`ăj P` for j “ 1, . . . ,K. Indeed, using this reparameterization, we obtain the corresponding joint in terms of K p0, 1q-valued stick-breaking weights tVjuKj“1 which correspond to a stick-breaking representation. Note that this joint distribution is for a general Lévy measure ρ, density fρ and it is conditioned on the valued of the random variable T . We can recover the well known Stick breaking representations for the Dirichlet and Pitman-Yor processes, for a specific choice of ρ and if we integrate out T , see the supplementary material for further details about the latter. However, in general, these stick-breaking random variables form a sequence of dependent random variables with a complicated distribution, except for the two previously mentioned processes, see Pitman [22] for details."
    }, {
      "heading" : "2.3 Poisson-Kingman mixture model",
      "text" : "We are mainly interested in using Poisson-Kingman RPMs as a building block for an infinite mixture model. Indeed, we can use Equation (1) as the top level of the following hierarchical specification\nT „ γ P |T „ PKpρσ, δT , H0q\nYi | P iid„ P\nXi | Yi ind„ F p¨ | Yiq (3)\nwhere F p¨ | Y q is the likelihood term for each mixture component, and our dataset consists of n observations pxiqiPrns of the corresponding variables pXiqiPrns. We will assume that F p¨ | Y q is smooth. After specifying the model we would like to carry out inference for clustering and/or density estimation tasks. We can do it exactly and more efficiently than with known MCMC samplers with our novel approach. In the next section, we present our main contribution and in the following one we show how it outperforms other samplers."
    }, {
      "heading" : "3 Hybrid Sampler",
      "text" : "Equation’s (2) joint distribution is written in terms of the first K size-biased weights. In order to obtain a complete representation of the RPM, we need to size-bias sample from it a countably infinite number of times. Succesively, devise some way of representing this object exactly in a computer with finite memory and storage is needed.\nWe introduce the following novel strategy: starting from equation (2), we exploit the generative process of section 2.1 when reassigning observations to clusters. In addition to this, we reparameterize the model in terms of a surplus mass random variable V “ T ´\nřK k“1 J̃k and end up with the\nfollowing joint distribution\nPρ,h,H0pΠn “ pckqkPrKs, Y ˚k P dy˚k , J̃k P dsk for k P rKs, T ´ K ÿ\nk“1 J̃k P dv,Xi P dxi for i P rnsq\n(4)\n“ pv ` K ÿ\nk“1 skq´nh\n˜\nv ` K ÿ\nk“1 sk\n¸\nfρpvq K ź\nk“1 s |ck| k ρpdskqH0pdy ˚ k q ź iPck F pdxi|y˚k q.\nFor this reason, while having a complete representation of the infinite dimensional part of the model we only need to explicitly represent those size-biased weights associated to occupied clusters plus a surplus mass term which is associated to the rest of the empty clusters, as Figure 1 shows. The cluster reassignment step can be seen as a lazy sampling scheme: we explicitly represent and update the weights associated to occupied clusters and create a size-biased weight only when a new cluster appears. To make this possible we use the induced partition and we call Equation (4) the varying table size Chinese restaurant representation because the size-biased weights can be thought as the sizes of the tables in our restaurant. In the next subsection, we compute the complete conditionals of each random variable of interest to implement an overall Gibbs sampling MCMC scheme."
    }, {
      "heading" : "3.1 Complete conditionals",
      "text" : "Starting from equation (4), we obtain the following complete conditionals for the Gibbs sampler\nP pV P dv | Restq9 ˜ v ` K ÿ\nk“1 sk\n¸´n\nfρpvqh ˜ v ` K ÿ\nk“1 sk\n¸\ndv (5)\nP ´ J̃i P dsi | Rest ¯ 9 ˜ v ` si ` ÿ\nk‰i sk\n¸´n\nh\n˜\nv ` si ` ÿ\nk‰i sk\n¸\ns |ci| i ρpdsiqIp0,Surpmassiqpsiqdsi\nwhere Surpmassi “ V ` řk j“1 J̃j ´ ř jăi J̃j .\nPpci “ c | c´i,Restq9 # scF pdxi | tXjujPc Y ˚ c q if i is assigned to existing cluster c\nv M F pdxi | Y ˚ c q if i is assigned to a new cluster c\nAccording to the rule above, the ith observation will be either reassigned to an existing cluster or to one of the M new clusters in the ReUse algorithm as in Favaro & Teh [6]. If it is assigned to a new cluster, then we need to sample a new size-biased weight from the following\nP ´ J̃k`1 P dsk`1 | Rest ¯ 9fρpv ´ sk`1qρpsk`1qsk`1Ip0,vqpsk`1qdsk`1. (6)\nEvery time a new cluster is created we need to obtain its corresponding size-biased weight which could happen 1 ď R ď n times per iteration hence, it has a significant contribution to the overall computational cost. For this reason, an independent and identically distributed (i.i.d.) draw from its corresponding complete conditional (6) is highly desirable. In the next subsection we present a way to achieve this. Finally, for updating cluster parameters tY ˚k ukPrKs, in the case where H0 is non-conjugate to the likelihood, we use an extension of Favaro & Teh [6]’s ReUse algorithm, see Algorithm 3 in the supplementary material for details.\nThe complete conditionals in Equation (5) do not have a standard form but a generic MCMC method can be applied to sample from each within the Gibbs sampler. We use slice sampling from Neal [19] to update the size-biased weights and the surplus mass. However, there is a class of priors where the total mass’s density is intractable so an additional step needs to be introduced to sample the surplus mass. In the next subsection we present two alternative ways to overcome this issue."
    }, {
      "heading" : "3.2 Example of classes of Poisson-Kingman priors",
      "text" : "a) σ-Stable Poisson-Kingman processes [23]. For any σ P p0, 1q, let fσptq “ 1 π ř8 j“0 p´1qj`1 j! sinpπσjq Γpσj`1q tσj`1 be the density function of a positive σ-Stable random variable and ρpdxq “ ρσpdxq :“ σΓp1´σqx ´σ´1dx. This class of RPMs is denoted by PKpρσ, hT , H0q where h is a function that indexes each member of the class. For example, in the experimental section, we picked 3 choices of the h function that index the following processes: Pitman-Yor, Normalized Stable and Normalized Generalized Gamma processes. This class includes all Gibbs type priors with parameter σ P p0, 1q, so other choices of h are possible, see Gnedin & Pitman [10] and De Blasi et al. [1] for a noteworthy account of this class of Bayesian nonparametric priors. In this case, the total mass’s density is intractable and we propose two ways of dealing with this. Firstly, we used Kanter [14]’s integral representation for the σ-Stable density as in Lomeli et al. [17], introduce an auxiliary variable Z and slice sample each variable\nP pV P dv | Restq9 ˜ v ` k ÿ\ni“1 si\n¸´n\nv´ σ 1´σ exp ” ´v ´σ 1´σApzq ı h\n˜\nv ` k ÿ\ni“1 si\n¸\ndv\nP pZ P dz | Restq9Apzq exp ” ´vp´ σ 1´σ qApzq ı dz,\nsee Algorithm 1 in the supplementary material for details. Alternatively, we can completely bypass the evaluation of the total mass’s density by updating the surplus mass with a Metropolis-Hastings step with an independent proposal from a Stable or from an Exponentially Tilted Stable(λ). It is straight forward to obtain i.i.d draws from these proposals, see Devroye [3] and Hofert [11] for an improved rejection sampling method for the Exponentially tilted case. This leads to the following acceptance ratio\nP pV 1 P dv1 | Restq fσpvq exp p´λvq P pV P dv | Restq fσpv1q exp p´λv1q “\n´\nv1 ` řk i“1 si\n¯´n h ´\nv1 ` řk i“1 si\n¯\ndv1 exp p´vq ´\nv ` řk i“1 si\n¯´n h ´\nv ` řk i“1 si\n¯ dv exp p´v1q ,\nsee Algorithm 2 in the supplementary material for details. Finally, to sample a new size-biased weight\nP ´ J̃k`1 P dsk`1 | Rest ¯ 9fσpv ´ sk`1qs´σk`1Ip0,vqpsk`1qdsk`1.\nFortunately, we can get an i.i.d. draw from the above due to an identity in distribution given by Favaro et al. [8] for the usual stick breaking weights for any prior in this class such that σ “ uv where u ă v are coprime integers. Then we just reparameterize it back to obtain the new size-biased weight, see Algorithm 4 in the supplementary material for details.\nb) ´ logBeta-Poisson-Kingman processes [25, 27]. Let fρptq “ Γpa`bq\nΓpaqΓpbq exp p´atq p1´ expp´tqq b´1 be the density of a positive random variable X d“ ´ log Y , where Y „ Betapa, bq and ρpxq “ expp´axqp1´expp´bxqqxp1´expp´xqq . This class of RPMs generalises the Gamma process but has similar properties. Indeed, if we take b “ 1 and the density function for T is γptq “ fρptq we recover the Lévy measure and total mass’s density function of a Gamma process. Finally, to sample a new size-biased weight\nP ´ J̃k`1 P dsk`1 | Rest ¯ 9p1´ exppsk`1 ´ vqq b´1 p1´ expp´bsk`1qq\n1´ expp´sk`1q dsk`1Ip0,vqpsk`1q.\nIf b ą 1, this complete conditional is a monotone decreasing unnormalised density with maximum at b. We can easily get an i.i.d. draw with a simple rejection sampler [2] where the rejection constant is bv and the proposal is Up0, vq. There is no other known sampler for this process."
    }, {
      "heading" : "3.3 Relationship to marginal and conditional MCMC samplers",
      "text" : "Starting from equation (2), another strategy would be to reparameterize the model in terms of the usual stick breaking weights. Next, we could choose a random truncation level and represent finitely many sticks as in Favaro & Walker [7]. Alternatively, we could integrate out the random probability measure and sample only the partition induced by it as in Lomeli et al. [17]. Conditional samplers have large memory requirements as often, the number of sticks needed can be very large. Furthermore, the conditional distributions of the stick lengths are quite involved so they tend to have slow running times. Marginal samplers have less storage requirements than conditional samplers but could potentially have worst mixing properties. For example, Lomeli et al. [17] had to introduce a number of auxiliary variables which worsen the mixing.\nOur novel hybrid sampler exploits marginal and conditional samplers advantages. It has less memory requirements since it just represents the size-biased weights of occupied as opposed to conditional samplers which represent both empty and occupied clusters. Also, it does not integrate out the size-biased weights thus, we obtain a more comprehensive representation of the RPM."
    }, {
      "heading" : "4 Performance assesssment",
      "text" : "We illustrate the performance of our hybrid sampler on a range of Bayesian nonparametric mixture models, obtained by different specifications of ρ and γ, as in Equation (3). At the top level of this hierarchical specification, different Bayesian nonparametric priors were chosen from both classes presented in the examples section. We chose the base distribution H0 and the likelihood term F for the kth cluster to be\nH0pdµkq “ N ` dµk | µ0, σ20 ˘\nand F pdx1, . . . , dxnk | µk, τ1q “ śnk i“1 N ` xi | µk, σ21 ˘ ,\nwhere tXjunkj“1 are the nk observations assigned to the kth cluster at some iteration. N denotes a Normal distribution with mean µk and variance σ21 , a common parameter among all clusters. The mean’s prior distribution is Normal, centered at µ0 and with variance σ20 . Although the base distribution is conjugate to the likelihood we treated it as non-conjugate case and sampled the parameters at each iteration rather than integrating them out.\nWe used the dataset from Roeder [26] to test the algorithmic performance in terms of running time and effective sample size (ESS), as Table 1 shows. The dataset consists of measurements of velocities in km/sec of n “ 82 galaxies from a survey of the Corona Borealis region. For the σ-Stable Poisson-Kingman class, we compared it against our implementation of Favaro & Walker [7]’s conditional sampler and against the marginal sampler of Lomeli et al. [17]. We chose to compare our hybrid sampler against these existing approaches which follow the same general purpose paradigm.\nTable 1 shows that different choices of σ result in differences in the algorithm’s running times and ESS. The reason for this is that in the σ “ 0.5 case there are readily available random number generators which do not increase the computational cost. In contrast, in the σ “ 0.3 case, a rejection sampler method is needed every time a new size-biased weight is sampled which increases the computational cost, see Favaro et al. [8] for details. Even so, in most cases, we outperform both marginal and conditional MCMC schemes in terms of running times and in all cases, in terms of ESS. In the Hybrid-MH case, even thought the ESS and running times are competitive, we found that the acceptance rate is not optimal, we are currently exploring other choices of proposals. Finally, in Example b), our approach is the only one available and it has good running times and ESS. This qualitative comparison confirms our previous statements about our novel approach."
    }, {
      "heading" : "5 Discussion",
      "text" : "Our main contribution is our Hybrid MCMC sampler as a general purpose tool for inference with a very large class of infinite mixture models. We argue in favour of an approach in which a generic algorithm can be applied to a very large class of models, so that the modeller has a lot of flexibility in choosing specific models suitable for his/her problem of interest. Our method is a hybrid approach since it combines the perks of the conditional and marginal schemes. Indeed, our experiments confirm that our hybrid sampler is more efficient since it outperforms both marginal and conditional samplers in running times in most cases and in ESS in all cases.\nWe introduced a new compact way of representing the infinite dimensional component such that it is feasible to perform inference and how to deal with the corresponding intractabilities. However, there are still various challenges that remain when dealing with these type of models. For instance, there are some values for σ which we are unable to perform inference with our novel sampler. Secondly, when a Metropolis-Hastings step is used, there could be other ways to improve the mixing in terms of better proposals. Finally, all BNP MCMC methods can be affected by the dimensionality and size of the dataset when dealing with an infinite mixture model. Indeed, all methods rely on the same way of dealing with the likelihood term. When adding a new cluster, all methods sample its\ncorresponding parameter from the prior distribution. In a high dimensional scenario, it could be very difficult to sample parameter values close to the existing data points. We consider these points to be an interesting avenue of future research."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Konstantina Palla for her insightful comments. Marı́a Lomelı́ is funded by the Gatsby Charitable Foundation, Stefano Favaro is supported by the European Research Council through StG N-BNP 306406 and Yee Whye Teh is supported by the European Research Council under the European Unions Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071."
    } ],
    "references" : [ {
      "title" : "Are Gibbs-type priors the most natural generalization of the Dirichlet process? Pages 212–229",
      "author" : [ "P. De Blasi", "S. Favaro", "A. Lijoi", "R.H. Mena", "I. Prüenster", "M. Ruggiero" ],
      "venue" : "of: IEEE Transactions on Pattern Analysis & Machine Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Non-Uniform Random Variate Generation",
      "author" : [ "L. Devroye" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1986
    }, {
      "title" : "Random variate generation for exponentially and polynomially tilted Stable distributions",
      "author" : [ "L. Devroye" ],
      "venue" : "ACM Transactions on Modelling and Computer Simulation,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Estimating normal means with a Dirichlet process prior",
      "author" : [ "M.D. Escobar" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1994
    }, {
      "title" : "Bayesian density estimation and inference using mixtures",
      "author" : [ "M.D. Escobar", "M. West" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1995
    }, {
      "title" : "MCMC for Normalized Random Measure Mixture Models",
      "author" : [ "S. Favaro", "Y.W. Teh" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Slice sampling σ-Stable Poisson-Kingman mixture models",
      "author" : [ "S. Favaro", "S.G. Walker" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "On the Stick-Breaking representation of σ-Stable Poisson-Kingman models",
      "author" : [ "S. Favaro", "M. Lomeli", "B. Nipoti", "Y.W. Teh" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Probabilistic Machine Learning and Artificial Inteligence",
      "author" : [ "Z. Ghahramani" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Exchangeable Gibbs partitions and Stirling triangles",
      "author" : [ "A. Gnedin", "J. Pitman" ],
      "venue" : "Journal of Mathematical Sciences,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Efficiently sampling nested Archimedean copulas",
      "author" : [ "M. Hofert" ],
      "venue" : "Comput. Statist. Data Anal.,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Gibbs Sampling Methods for Stick-Breaking Priors",
      "author" : [ "H. Ishwaran", "L.F. James" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2001
    }, {
      "title" : "Poisson process partition calculus with applications to exchangeable models and Bayesian nonparametrics",
      "author" : [ "L.F. James" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2002
    }, {
      "title" : "Stable densities under change of scale and total variation inequalities",
      "author" : [ "M. Kanter" ],
      "venue" : "Annals of Probability,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1975
    }, {
      "title" : "Completely Random Measures",
      "author" : [ "J.F.C. Kingman" ],
      "venue" : "Pacific Journal of Mathematics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1967
    }, {
      "title" : "The representation of partition structures",
      "author" : [ "J.F.C. Kingman" ],
      "venue" : "Journal of the London Mathematical Society,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1978
    }, {
      "title" : "A marginal sampler for σ-stable Poisson-Kingman mixture models",
      "author" : [ "M. Lomeli", "S. Favaro", "Y.W. Teh" ],
      "venue" : "Journal of Computational and Graphical Statistics (To appear)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Markov Chain Sampling Methods for Dirichlet Process",
      "author" : [ "R.M. Neal" ],
      "venue" : "Mixture Models. Tech. rept. 9815",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1998
    }, {
      "title" : "Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models",
      "author" : [ "O. Papaspiliopoulos", "G.O. Roberts" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Size-biased sampling of Poisson point processes and excursions",
      "author" : [ "M. Perman", "J. Pitman", "M. Yor" ],
      "venue" : "Probability Theory and Related Fields,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1992
    }, {
      "title" : "Random discrete distributions invariant under size-biased permutation",
      "author" : [ "J. Pitman" ],
      "venue" : "Advances in Applied Probability,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1996
    }, {
      "title" : "Poisson-Kingman Partitions. Pages 1–34",
      "author" : [ "J. Pitman" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "Combinatorial Stochastic Processes",
      "author" : [ "J. Pitman" ],
      "venue" : "Lecture Notes in Mathematics. Springer-Verlag,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2006
    }, {
      "title" : "Distributional results for means of normalized random measures with independent increments",
      "author" : [ "E. Regazzini", "A. Lijoi", "I. Prüenster" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2003
    }, {
      "title" : "Density estimation with confidence sets exemplified by super-clusters and voids in the galaxies",
      "author" : [ "K. Roeder" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1990
    }, {
      "title" : "Quasi-invariance properties of a class of subordinators",
      "author" : [ "M. von Renesse", "M. Yor", "L. Zambotti" ],
      "venue" : "Stochastic Processes and their Applications,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "Sampling the Dirichlet Mixture Model with Slices",
      "author" : [ "Walker", "Stephen G" ],
      "venue" : "Communications in Statistics - Simulation and Computation,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "1 Introduction According to Ghahramani [9], models that have a nonparametric component give us more flexiblity that could lead to better predictive performance.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "Over the years, many MCMC methods have been proposed to perform inference which usually rely on a tailored representation of the underlying process [5, 4, 18, 20, 28, 6].",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 3,
      "context" : "Over the years, many MCMC methods have been proposed to perform inference which usually rely on a tailored representation of the underlying process [5, 4, 18, 20, 28, 6].",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 17,
      "context" : "Over the years, many MCMC methods have been proposed to perform inference which usually rely on a tailored representation of the underlying process [5, 4, 18, 20, 28, 6].",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 18,
      "context" : "Over the years, many MCMC methods have been proposed to perform inference which usually rely on a tailored representation of the underlying process [5, 4, 18, 20, 28, 6].",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 26,
      "context" : "Over the years, many MCMC methods have been proposed to perform inference which usually rely on a tailored representation of the underlying process [5, 4, 18, 20, 28, 6].",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 5,
      "context" : "Over the years, many MCMC methods have been proposed to perform inference which usually rely on a tailored representation of the underlying process [5, 4, 18, 20, 28, 6].",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 21,
      "context" : "2 Poisson-Kingman processes Poisson-Kingman random probability measures (RPMs) have been introduced in Pitman [23] as a generalization of homogeneous Normalized Random Measures (NRMs) [25, 13].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "2 Poisson-Kingman processes Poisson-Kingman random probability measures (RPMs) have been introduced in Pitman [23] as a generalization of homogeneous Normalized Random Measures (NRMs) [25, 13].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : "2 Poisson-Kingman processes Poisson-Kingman random probability measures (RPMs) have been introduced in Pitman [23] as a generalization of homogeneous Normalized Random Measures (NRMs) [25, 13].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 14,
      "context" : "Let X be a complete and separable metric space endowed with the Borel σ-field BpXq, let μ „ CRMpρ,H0q be a homogeneous Completely Random Measure (CRM) with Lévy measure ρ and base distribution H0, see Kingman [15] for a good overview about CRMs and references therein.",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 15,
      "context" : "Kingman [16] showed that Π is exchangeable, this property will be one of the main tools for the derivation of our hybrid sampler.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 19,
      "context" : "[21] states that the sequence of surplus masses pTkqkě0 forms a Markov chain and gives the corresponding initial distribution and transition kernels.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "Such a joint distribution was first obtained in Pitman [23] , see also Pitman [24] for further details.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "Such a joint distribution was first obtained in Pitman [23] , see also Pitman [24] for further details.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "2 Relationship to the usual Stick-breaking construction In the generative process above, we mentioned that it is reminiscent of the well known stick breaking construction from Ishwaran & James [12], where you break a stick of length one but it is not the same.",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 20,
      "context" : "However, in general, these stick-breaking random variables form a sequence of dependent random variables with a complicated distribution, except for the two previously mentioned processes, see Pitman [22] for details.",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 5,
      "context" : "scF pdxi | tXjujPc Y  ̊ c q if i is assigned to existing cluster c v M F pdxi | Y  ̊ c q if i is assigned to a new cluster c According to the rule above, the ith observation will be either reassigned to an existing cluster or to one of the M new clusters in the ReUse algorithm as in Favaro & Teh [6].",
      "startOffset" : 297,
      "endOffset" : 300
    }, {
      "referenceID" : 5,
      "context" : "Finally, for updating cluster parameters tY  ̊ k ukPrKs, in the case where H0 is non-conjugate to the likelihood, we use an extension of Favaro & Teh [6]’s ReUse algorithm, see Algorithm 3 in the supplementary material for details.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 21,
      "context" : "2 Example of classes of Poisson-Kingman priors a) σ-Stable Poisson-Kingman processes [23].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 9,
      "context" : "This class includes all Gibbs type priors with parameter σ P p0, 1q, so other choices of h are possible, see Gnedin & Pitman [10] and De Blasi et al.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "[1] for a noteworthy account of this class of Bayesian nonparametric priors.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "Firstly, we used Kanter [14]’s integral representation for the σ-Stable density as in Lomeli et al.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "[17], introduce an auxiliary variable Z and slice sample each variable",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "d draws from these proposals, see Devroye [3] and Hofert [11] for an improved rejection sampling method for the Exponentially tilted case.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "d draws from these proposals, see Devroye [3] and Hofert [11] for an improved rejection sampling method for the Exponentially tilted case.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "[8] for the usual stick breaking weights for any prior in this class such that σ “ uv where u ă v are coprime integers.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 23,
      "context" : "b)  ́ logBeta-Poisson-Kingman processes [25, 27].",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "b)  ́ logBeta-Poisson-Kingman processes [25, 27].",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "draw with a simple rejection sampler [2] where the rejection constant is bv and the proposal is Up0, vq.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "Next, we could choose a random truncation level and represent finitely many sticks as in Favaro & Walker [7].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "[17] had to introduce a number of auxiliary variables which worsen the mixing.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "We used the dataset from Roeder [26] to test the algorithmic performance in terms of running time and effective sample size (ESS), as Table 1 shows.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "For the σ-Stable Poisson-Kingman class, we compared it against our implementation of Favaro & Walker [7]’s conditional sampler and against the marginal sampler of Lomeli et al.",
      "startOffset" : 101,
      "endOffset" : 104
    } ],
    "year" : 2015,
    "abstractText" : "This paper concerns the introduction of a new Markov Chain Monte Carlo scheme for posterior sampling in Bayesian nonparametric mixture models with priors that belong to the general Poisson-Kingman class. We present a novel compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous MCMC schemes. We describe comparative simulation results demonstrating the efficacy of the proposed MCMC algorithm against existing marginal and conditional MCMC samplers.",
    "creator" : null
  }
}