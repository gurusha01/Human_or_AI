{
  "name" : "dabd8d2ce74e782c65a973ef76fd540b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Collaboratively Learning Preferences from Ordinal Data",
    "authors" : [ "Sewoong Oh", "Kiran K. Thekumparampil", "Jiaming Xu" ],
    "emails" : [ "swoh@illinois.edu", "thekump2@illinois.edu", "jiamingx@wharton.upenn.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recommendation systems and revenue management, it is important to predict preferences on items that have not been seen by a user or predict outcomes of comparisons among those that have never been compared. Predicting such hidden preferences would be hopeless without further assumptions on the structure of the preference. Motivated by the success of matrix factorization models on collaborative filtering applications, we model hidden preferences with low-rank matrices to collaboratively learn preference matrices from ordinal data. This paper considers the following scenarios:\n• Collaborative ranking. Consider an online market that collects each user’s preference as a ranking over a subset of items that are ‘seen’ by the user. Such data can be obtained by directly asking to compare some items, or by indirectly tracking online activities on which items are viewed, how much time is spent on the page, or how the user rated the items. In order to make personalized recommendations, we want a model which (a) captures how users who prefer similar items are also likely to have similar preferences on unseen items, (b) predicts which items a user might prefer, by learning from such ordinal data. • Bundled choice modeling. Discrete choice models describe how a user makes decisions on\nwhat to purchase. Typical choice models assume the willingness to buy an item is independent of what else the user bought. In many cases, however, we make ‘bundled’ purchases: we buy particular ingredients together for one recipe or we buy two connecting flights. One choice (the first flight) has a significant impact on the other (the connecting flight). In order to optimize the assortment (which flight schedules to offer) for maximum expected revenue, it is crucial to accurately predict the willingness of the consumers to purchase, based on past history. We consider a case where there are two types of products (e.g. jeans and shirts), and want (a) a model that captures such interacting preferences for pairs of items, one from each category; and (b) to predict the consumer’s choice probabilities on pairs of items, by learning such models from past purchase history.\nWe use a discrete choice model known as MultiNomial Logit (MNL) model [1] (described in Section 2.1) to represent the preferences. In collaborative ranking context, MNL uses a low-rank matrix to represent the hidden preferences of the users. Each row corresponds to a user’s preference over all the items, and when presented with a subset of items the user provides a ranking over those items, which is a noisy version of the hidden true preference. The low-rank assumption naturally captures the similarities among users and items, by representing each on a low-dimensional space. In bundled choice modeling context, the low-rank matrix now represents how pairs of items are matched. Each row corresponds to an item from the first category and each column corresponds to an item from the second category. An entry in the matrix represents how much the pair is preferred by a randomly chosen user from a pool of users. Notice that in this case we do not model individual preferences, but the preference of the whole population. The purchase history of the population is the record of which pair was chosen among a subsets of items that were presented, which is again a noisy version of the hidden true preference. The low-rank assumption captures the similarities and dis-similarities among the items in the same category and the interactions across categories.\nContribution. A natural approach to learn such a low-rank model, from noisy observations, is to solve a convex relaxation of nuclear norm minimization (described in Section 2.2), since nuclear norm is the tightest convex surrogate for the rank function. We present such an approach for learning the MNL model from ordinal data, in two contexts: collaborative ranking and bundled choice modeling. In both cases, we analyze the sample complexity of the algorithm, and provide an upper bound on the resulting error with finite samples. We prove minimax-optimality of our approach by providing a matching information-theoretic lower bound (up to a poly-logarithmic factor). Technically, we utilize the Random Utility Model (RUM) [2, 3, 4] interpretation (outlined in Section 2.1) of the MNL model to prove both the upper bound and the fundamental limit, which could be of interest to analyzing more general class of RUMs.\nRelated work. In the context of collaborative ranking, MNL models have been proposed to model partial rankings from a pool of users. Recently, there has been new algorithms and analyses of those algorithms to learn MNL models from samples, in the case when each user provides pair-wise comparisons [5, 6]. [6] proposes solving a convex relaxation of maximizing the likelihood over matrices with bounded nuclear norm. It is shown that this approach achieves statistically optimal generalization error rate, instead of Frobenius norm error that we analyze. Our analysis techniques are inspired by [5], which proposed the convex relaxation for learning MNL, but when the users provide only pair-wise comparisons. In this paper, we generalize the results of [5] by analyzing more general sampling models beyond pairwise comparisons.\nThe remainder of the paper is organized as follows. In Section 2, we present the MNL model and propose a convex relaxation for learning the model, in the context of collaborative ranking. We provide theoretical guarantees for collaborative ranking in Section 3. In Section 4, we present the problem statement for bundled choice modeling, and analyze a similar convex relaxation approach.\nNotations. We use |||A|||F and |||A|||∞to denote the Frobenius norm and the `∞ norm, |||A|||nuc =∑ i σi(A) to denote the nuclear norm where σi(A) denote the i-th singular value, and |||A|||2 =\nσ1(A) for the spectral norm. We use 〈〈u, v〉〉 = ∑ i uivi and ‖u‖ to denote the inner product and the Euclidean norm. All ones vector is denoted by 1 and I(A) is the indicator function of the event A. The set of the fist N integers are denoted by [N ] = {1, . . . , N}."
    }, {
      "heading" : "2 Model and Algorithm",
      "text" : "In this section, we present a discrete choice modeling for collaborative ranking, and propose an inference algorithm for learning the model from ordinal data."
    }, {
      "heading" : "2.1 MultiNomial Logit (MNL) model for comparative judgment",
      "text" : "In collaborative ranking, we want to model how people who have similar preferences on a subset of items are likely to have similar tastes on other items as well. When users provide ratings, as in collaborative filtering applications, matrix factorization models are widely used since the low-rank structure captures the similarities between users. When users provide ordered preferences, we use a discrete choice model known as MultiNomial Logit (MNL) [1] model that has a similar low-rank structure that captures the similarities between users and items.\nLet Θ∗ be the d1 × d2 dimensional matrix capturing the preference of d1users on d2 items, where the rows and columns correspond to users and items, respectively. Typically, Θ∗ is assumed to be low-rank, having a rank r that is much smaller than the dimensions. However, in the following we allow a more general setting where Θ∗ might be only approximately low rank. When a user i is presented with a set of alternatives Si ⊆ [d2], she reveals her preferences as a ranked list over those items. To simplify the notations we assume all users compare the same number k of items, but the analysis naturally generalizes to the case when the size might differ from a user to a user. Let vi,` ∈ Si denote the (random) `-th best choice of user i. Each user gives a ranking, independent of other users’ rankings, from\nP {vi,1, . . . , vi,k} = k∏\n`=1\ne Θ∗i,vi,` ∑ j∈Si,` e Θ∗i,j , (1)\nwhere with Si,` ≡ Si \\ {vi,1, . . . , vi,`−1} and Si,1 ≡ Si. For a user i, the i-th row of Θ∗ represents the underlying preference vector of the user, and the more preferred items are more likely to be ranked higher. The probabilistic nature of the model captures the noise in the revealed preferences.\nThe random utility model (RUM), pioneered by [2, 3, 4], describes the choices of users as manifestations of the underlying utilities. The MNL models is a special case of RUM where each decision maker and each alternative are represented by a r-dimensional feature vectors ui and vj respectively, such that Θ∗ij = 〈〈ui, vj〉〉, resulting in a low-rank matrix. When presented with a set of alternatives Si, the decision maker i ranks the alternatives according to their random utility drawn from\nUij = 〈〈ui, vj〉〉+ ξij , (2) for item j, where ξij follow the standard Gumbel distribution. Intuitively, this provides a justification for the MNL model as modeling the decision makers as rational being, seeking to maximize utility. Technically, this RUM interpretation plays a crucial role in our analysis, in proving restricted strong convexity in Appendix A.5 and also in proving fundamental limit in Appendix C.\nThere are a few cases where the Maximum Likelihood (ML) estimation for RUM is tractable. One notable example is the Plackett-Luce (PL) model, which is a special case of the MNL model where Θ∗ is rank-one and all users have the same features. PL model has been widely applied in econometrics [1], analyzing elections [7], and machine learning [8]. Efficient inference algorithms has been proposed [9, 10, 11], and the sample complexity has been analyzed for the MLE [12] and for the Rank Centrality [13]. Although PL is quite restrictive, in the sense that it assumes all users share the same features, little is known about inference in RUMs beyond PL. Recently, to overcome such a restriction, mixed PL models have been studied, where Θ∗ is rank-r but there are only r classes of users and all users in the same class have the same features. Efficient inference algorithms with provable guarantees have been proposed by applying recent advances in tensor decomposition methods [14, 15], directly clustering the users [16, 17], or using sampling methods [18]. However, this mixture PL is still restrictive, and both clustering and tensor based approaches rely heavily on the fact that the distribution is a “mixture” and require additional incoherence assumptions on Θ∗. For more general models, efficient inference algorithms have been proposed [19] but no performance guarantee is known for finite samples. Although the MLE for the general MNL model in (1) is intractable, we provide a polynomial-time inference algorithm with provable guarantees."
    }, {
      "heading" : "2.2 Nuclear norm minimization",
      "text" : "Assuming Θ∗ is well approximated by a low-rank matrix, we estimate Θ∗ by solving the following convex relaxation given the observed preference in the form of ranked lists {(vi,1, . . . , vi,k)}i∈[d1].\nΘ̂ ∈ arg min Θ∈Ω L(Θ) + λ|||Θ|||nuc, (3)\nwhere the (negative) log likelihood function according to (1) is\nL(Θ) = − 1 k d1\nd1∑\ni=1\nk∑\n`=1\n 〈〈Θ, eieTvi,`〉〉 − log   ∑\nj∈Si,` exp\n( 〈〈Θ, eieTj 〉〉 )     , (4)\nwith Si = {vi,1, . . . , vi,k} and Si,` ≡ Si\\{vi,1, . . . , vi,`−1}, and appropriately chosen set Ω defined in (7). Since nuclear norm is a tight convex surrogate for the rank, the above optimization searches\nfor a low-rank solution that maximizes the likelihood. Nuclear norm minimization has been widely used in rank minimization problems [20], but provable guarantees typically exists only for quadratic loss function L(Θ) [21, 22]. Our analysis extends such analysis techniques to identify the conditions under which restricted strong convexity is satisfied for a convex loss function that is not quadratic.\n3 Collaborative ranking from k-wise comparisons\nWe first provide background on the MNL model, and then present main results on the performance guarantees. Notice that the distribution (1) is independent of shifting each row of Θ∗ by a constant. Hence, there is an equivalent class of Θ∗ that gives the same distributions for the ranked lists:\n[Θ∗] = {A ∈ Rd1×d2 | A = Θ∗ + u1T for some u ∈ Rd1} . (5) Since we can only estimate Θ∗ up to this equivalent class, we search for the one whose rows sum to zero, i.e.\n∑ j∈[d2] Θ ∗ i,j = 0 for all i ∈ [d1]. Let α ≡ maxi,j1,j2 |Θ∗ij1 − Θ∗ij2 | denote the dynamic\nrange of the underlying Θ∗, such that when k items are compared, we always have 1\nk e−α ≤ 1 1 + (k − 1)eα ≤ P {vi,1 = j} ≤ 1 1 + (k − 1)e−α ≤ 1 k eα , (6)\nfor all j ∈ Si, all Si ⊆ [d2] satisfying |Si| = k and all i ∈ [d1]. We do not make any assumptions on α other than that α = O(1) with respect to d1 and d2. The purpose of defining the dynamic range in this way is that we seek to characterize how the error scales with α. Given this definition, we solve the optimization in (3) over\nΩα = { A ∈ Rd1×d2 ∣∣ |||A|||∞ ≤ α, and ∀i ∈ [d1] we have ∑\nj∈[d2] Aij = 0\n} . (7)\nWhile in practice we do not require the `∞ norm constraint, we need it for the analysis. For a related problem of matrix completion, where the loss L(θ) is quadratic, either a similar condition on `∞ norm is required or a different condition on incoherence is required."
    }, {
      "heading" : "3.1 Performance guarantee",
      "text" : "We provide an upper bound on the resulting error of our convex relaxation, when a multi-set of items Si presented to user i is drawn uniformly at random with replacement. Precisely, for a given k, Si = {ji,1, . . . , ji,k} where ji,`’s are independently drawn uniformly at random over the d2 items. Further, if an item is sampled more than once, i.e. if there exists ji,`1 = ji,`2 for some i and `1 6= `2, then we assume that the user treats these two items as if they are two distinct items with the same MNL weights Θ∗i,ji,`1 = Θ ∗ i,ji,`2\n.The resulting preference is therefore always over k items (with possibly multiple copies of the same item), and distributed according to (1). For example, if k = 3, it is possible to have Si = {ji,1 = 1, ji,2 = 1, ji,3 = 2}, in which case the resulting ranking can be (vi,1 = ji,1, vi,2 = ji,3, vi,3 = ji,2) with probability (eΘ ∗ i,1)/(2 eΘ ∗ i,1 + eΘ ∗ i,2) × (eΘ∗i,2)/(eΘ∗i,1 + eΘ∗i,2). Such sampling with replacement is necessary for the analysis, where we require independence in the choice of the items in Si in order to apply the symmetrization technique (e.g. [23]) to bound the expectation of the deviation (cf. Appendix A.5). Similar sampling assumptions have been made in existing analyses on learning low-rank models from noisy observations, e.g. [22]. Let d ≡ (d1 + d2)/2, and let σj(Θ∗) denote the j-th singular value of the matrix Θ∗. Define\nλ0 ≡ e2α √ d1 log d+ d2 (log d)2(log 2d)4\nk d21 d2 .\nTheorem 1. Under the described sampling model, assume 24 ≤ k ≤ min{d21 log d, (d21 + d22)/(2d1) log d, (1/e) d2(4 log d2+2 log d1)}, and λ ∈ [480λ0, c0λ0] with any constant c0 = O(1) larger than 480. Then, solving the optimization (3) achieves\n1\nd1d2\n∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ 2\nF ≤ 288\n√ 2 e4αc0λ0 √ r ∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ F + 288e4αc0λ0 min{d1,d2}∑\nj=r+1\nσj(Θ ∗) , (8)\nfor any r ∈ {1, . . . ,min{d1, d2}} with probability at least 1− 2d−3− d−32 where d = (d1 + d2)/2.\nA proof is provided in Appendix A. The above bound shows a natural splitting of the error into two terms, one corresponding to the estimation error for the rank-r component and the second one corresponding to the approximation error for how well one can approximate Θ∗ with a rank-r matrix. This bound holds for all values of r and one could potentially optimize over r. We show such results in the following corollaries.\nCorollary 3.1 (Exact low-rank matrices). Suppose Θ∗ has rank at most r. Under the hypotheses of Theorem 1, solving the optimization (3) with the choice of the regularization parameter λ ∈ [480λ0, c0λ0] achieves with probability at least 1− 2d−3 − d−32 ,\n1√ d1d2\n∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ F ≤ 288 √ 2e6αc0\n√ r(d1 log d+ d2 (log d)2(log 2d)4)\nk d1 . (9)\nThe number of entries is d1d2 and we rescale the Frobenius norm error appropriately by 1/ √ d1d2. When Θ∗ is a rank-r matrix, then the degrees of freedom in representing Θ∗ is r(d1 + d2) − r2 = O(r(d1 +d2)). The above theorem shows that the total number of samples, which is (k d1), needs to scale as O(rd1(log d) + rd2 (log d)2(log 2d)4 in order to achieve an arbitrarily small error. This is only poly-logarithmic factor larger than the degrees of freedom. In Section 3.2, we provide a lower bound on the error directly, that matches the upper bound up to a logarithmic factor.\nThe dependence on the dynamic range α, however, is sub-optimal. It is expected that the error increases with α, since the Θ∗ scales as α, but the exponential dependence in the bound seems to be a weakness of the analysis, as seen from numerical experiments in the right panel of Figure 1. Although the error increase with α, numerical experiments suggests that it only increases at most linearly. However, tightening the scaling with respect to α is a challenging problem, and such suboptimal dependence is also present in existing literature for learning even simpler models, such as the Bradley-Terry model [13] or the Plackett-Luce model [12], which are special cases of the MNL model studied in this paper. A practical issue in achieving the above rate is the choice of λ, since the dynamic range α is not known in advance. Figure 1 illustrates that the error is not sensitive to the choice of λ for a wide range.\nAnother issue is that the underlying matrix might not be exactly low rank. It is more realistic to assume that it is approximately low rank. Following [22] we formalize this notion with “`q-ball” of matrices defined as\nBq(ρq) ≡ {Θ ∈ Rd1×d2 | ∑\nj∈[min{d1,d2}] |σj(Θ∗)|q ≤ ρq} . (10)\nWhen q = 0, this is a set of rank-ρ0 matrices. For q ∈ (0, 1], this is set of matrices whose singular values decay relatively fast. Optimizing the choice of r in Theorem 1, we get the following result.\nCorollary 3.2 (Approximately low-rank matrices). Suppose Θ∗ ∈ Bq(ρq) for some q ∈ (0, 1] and ρq > 0. Under the hypotheses of Theorem 1, solving the optimization (3) with the choice of the regularization parameter λ ∈ [480λ0, c0λ0] achieves with probability at least 1− 2d−3,\n1√ d1d2\n∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ F ≤ 2 √ ρq√\nd1d2\n 288 √ 2c0e 6α √ d1d2(d1 log d+ d2 (log d)2(log 2d)2)\nk d1\n  2−q 2\n. (11)\nThis is a strict generalization of Corollary 3.1. For q = 0 and ρ0 = r, this recovers the exact low-rank estimation bound up to a factor of two. For approximate low-rank matrices in an `q-ball, we lose in the error exponent, which reduces from one to (2 − q)/2. A proof of this Corollary is provided in Appendix B.\nThe left panel of Figure 1 confirms the scaling of the error rate as predicted by Corollary 3.1. The lines merge to a single line when the sample size is rescaled appropriately. We make a choice of λ = (1/2) √ (log d)/(kd2), This choice is independent of α and is smaller than proposed in Theorem 1. We generate random rank-r matrices of dimension d × d, where Θ∗ = UV T with U ∈ Rd×r and V ∈ Rd×r entries generated i.i.d from uniform distribution over [0, 1]. Then the\nrow-mean is subtracted form each row, and then the whole matrix is scaled such that the largest entry is α = 5. Note that this operation does not increase the rank of the matrix Θ. This is because this de-meaning can be written as Θ − Θ11T /d2 and both terms in the operation are of the same column space as Θ which is of rank r. The root mean squared error (RMSE) is plotted where RMSE = (1/d)|||Θ∗ − Θ̂|||F. We implement and solve the convex optimization (3) using proximal gradient descent method as analyzed in [24]. The right panel in Figure 1 illustrates that the actual error is insensitive to the choice of λ for a broad range of λ ∈ [ √ (log d)/(kd2), 28 √ (log d)/(kd2)], after which it increases with λ."
    }, {
      "heading" : "3.2 Information-theoretic lower bound for low-rank matrices",
      "text" : "For a polynomial-time algorithm of convex relaxation, we gave in the previous section a bound on the achievable error. We next compare this to the fundamental limit of this problem, by giving a lower bound on the achievable error by any algorithm (efficient or not). A simple parameter counting argument indicates that it requires the number of samples to scale as the degrees of freedom i.e., kd1 ∝ r(d1 + d2), to estimate a d1 × d2 dimensional matrix of rank r. We construct an appropriate packing over the set of low-rank matrices with bounded entries in Ωα defined as (7), and show that no algorithm can accurately estimate the true matrix with high probability using the generalized Fano’s inequality. This provides a constructive argument to lower bound the minimax error rate, which in turn establishes that the bounds in Theorem 1 is sharp up to a logarithmic factor, and proves no other algorithm can significantly improve over the nuclear norm minimization.\nTheorem 2. Suppose Θ∗ has rank r. Under the described sampling model, for large enough d1 and d2 ≥ d1, there is a universal numerical constant c > 0 such that\ninf Θ̂ sup Θ∗∈Ωα\nE [ 1√\nd1d2\n∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ F ] ≥ c min\n{ αe−α √ r d2 k d1 , αd2√ d1d2 log d } , (12)\nwhere the infimum is taken over all measurable functions over the observed ranked lists {(vi,1, . . . , vi,k)}i∈[d1].\nA proof of this theorem is provided in Appendix C. The term of primary interest in this bound is the first one, which shows the scaling of the (rescaled) minimax rate as √ r(d1 + d2)/(kd1) (when d2 ≥ d1), and matches the upper bound in (8). It is the dominant term in the bound whenever the number of samples is larger than the degrees of freedom by a logarithmic factor, i.e., kd1 > r(d1+d2) log d, ignoring the dependence on α. This is a typical regime of interest, where the sample size is comparable to the latent dimension of the problem. In this regime, Theorem 2 establishes that the upper bound in Theorem 1 is minimax-optimal up to a logarithmic factor in the dimension d."
    }, {
      "heading" : "4 Choice modeling for bundled purchase history",
      "text" : "In this section, we use the MNL model to study another scenario of practical interest: choice modeling from bundled purchase history. In this setting, we assume that we have bundled purchase history data from n users. Precisely, there are two categories of interest with d1 and d2 alternatives in each category respectively. For example, there are d1 tooth pastes to choose from and d2 tooth brushes to choose from. For the i-th user, a subset Si ⊆ [d1] of alternatives from the first category is presented along with a subset Ti ⊆ [d2] of alternatives from the second category. We use k1 and k2 to denote the number of alternatives presented to a single user, i.e. k1 = |Si| and k2 = |Ti|, and we assume that the number of alternatives presented to each user is fixed, to simplify notations. Given these sets of alternatives, each user makes a ‘bundled’ purchase and we use (ui, vi) to denote the bundled pair of alternatives (e.g. a tooth brush and a tooth paste) purchased by the i-th user. Each user makes a choice of the best alternative, independent of other users’s choices, according to the MNL model as\nP {(ui, vi) = (j1, j2)} = eΘ ∗ j1,j2\n∑ j′1∈Si,j′2∈Ti e Θ∗ j′1,j ′ 2\n, (13)\nfor all j1 ∈ Si and j2 ∈ Ti. The distribution (13) is independent of shifting all the values of Θ∗ by a constant. Hence, there is an equivalent class of Θ∗ that gives the same distribution for the choices: [Θ∗] ≡ {A ∈ Rd1×d2 |A = Θ∗ + c11T for some c ∈ R} . Since we can only estimate Θ∗ up to this equivalent class, we search for the one that sum to zero, i.e.\n∑ j1∈[d1],j2∈[d2] Θ ∗ j1,j2\n= 0. Let α = maxj1,j′1∈[d1],j2,j′2∈[d2] |Θ∗j1,j2 −Θ∗j′1,j′2 |, denote the dynamic range of the underlying Θ\n∗, such that when k1 × k2 alternatives are presented, we always have\n1\nk1k2 e−α ≤ P {(ui, vi) = (j1, j2)} ≤\n1\nk1k2 eα , (14)\nfor all (j1, j2) ∈ Si × Ti and for all Si ⊆ [d1] and Ti ⊆ [d2] such that |Si| = k1 and |Ti| = k2. We do not make any assumptions on α other than that α = O(1) with respect to d1 and d2. Assuming Θ∗ is well approximate by a low-rank matrix, we solve the following convex relaxation, given the observed bundled purchase history {(ui, vi, Si, Ti)}i∈[n]:\nΘ̂ ∈ arg min Θ∈Ω′α L(Θ) + λ|||Θ|||nuc , (15)\nwhere the (negative) log likelihood function according to (13) is\nL(Θ) = − 1 n\nn∑\ni=1\n 〈〈Θ, euieTvi〉〉 − log   ∑\nj1∈Si,j2∈Ti exp\n( 〈〈Θ, ej1eTj2〉〉 )     , and (16)\nΩ′α ≡ { A ∈ Rd1×d2 ∣∣ |||A|||∞ ≤ α, and ∑\nj1∈[d1],j2∈[d2] Aj1,j2 = 0\n} . (17)\nCompared to collaborative ranking, (a) rows and columns of Θ∗ correspond to an alternative from the first and second category, respectively; (b) each sample corresponds to the purchase choice of a user which follow the MNL model with Θ∗; (c) each person is presented subsets Si and Ti of items from each category; (d) each sampled data represents the most preferred bundled pair of alternatives."
    }, {
      "heading" : "4.1 Performance guarantee",
      "text" : "We provide an upper bound on the error achieved by our convex relaxation, when the multi-set of alternatives Si from the first category and Ti from the second category are drawn uniformly at random with replacement from [d1] and [d2] respectively. Precisely, for given k1 and k2, we let Si = {j(i)1,1, . . . , j (i) 1,k1 } and Ti = {j(i)2,1, . . . , j (i) 2,k2 }, where j(i)1,`’s and j (i) 2,`’s are independently drawn uniformly at random over the d1 and d2 alternatives, respectively. Similar to the previous section, this sampling with replacement is necessary for the analysis. Define\nλ1 =\n√ e2α max{d1, d2} log d\nn d1 d2 . (18)\nTheorem 3. Under the described sampling model, assume 16e2α min{d1, d2} log d ≤ n ≤ min{d5, k1k2 max{d21, d22}} log d, and λ ∈ [8λ1, c1λ1] with any constant c1 = O(1) larger than max{8, 128/ √ min{k1, k2}}. Then, solving the optimization (15) achieves\n1\nd1d2\n∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ 2\nF ≤ 48\n√ 2 e2αc1λ1 √ r ∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ F + 48e2αc1λ1 min{d1,d2}∑\nj=r+1\nσj(Θ ∗) , (19)\nfor any r ∈ {1, . . . ,min{d1, d2}} with probability at least 1− 2d−3 where d = (d1 + d2)/2.\nA proof is provided in Appendix D. Optimizing over r gives the following corollaries. Corollary 4.1 (Exact low-rank matrices). Suppose Θ∗ has rank at most r. Under the hypotheses of Theorem 3, solving the optimization (15) with the choice of the regularization parameter λ ∈ [8λ1, c1λ1] achieves with probability at least 1− 2d−3,\n1√ d1d2\n∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ F ≤ 48 √ 2e3αc1\n√ r(d1 + d2) log d\nn . (20)\nThis corollary shows that the number of samples n needs to scale as O(r(d1 + d2) log d) in order to achieve an arbitrarily small error. This is only a logarithmic factor larger than the degrees of freedom. We provide a fundamental lower bound on the error, that matches the upper bound up to a logarithmic factor. For approximately low-rank matrices in an `1-ball as defined in (10), we show an upper bound on the error, whose error exponent reduces from one to (2− q)/2. Corollary 4.2 (Approximately low-rank matrices). Suppose Θ∗ ∈ Bq(ρq) for some q ∈ (0, 1] and ρq > 0. Under the hypotheses of Theorem 3, solving the optimization (15) with the choice of the regularization parameter λ ∈ [8λ1, c1λ1] achieves with probability at least 1− 2d−3,\n1√ d1d2\n∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ F ≤ 2 √ ρq√\nd1d2\n( 48 √ 2c1e 3α √ d1d2(d1 + d2) log d\nn\n) 2−q 2\n. (21)\nSince the proof is almost identical to the proof of Corollary 3.2 in Appendix B, we omit it. Theorem 4. Suppose Θ∗ has rank r. Under the described sampling model, there is a universal constant c > 0 such that that the minimax rate where the infimum is taken over all measurable functions over the observed purchase history {(ui, vi, Si, Ti)}i∈[n] is lower bounded by\ninf Θ̂ sup Θ∗∈Ωα\nE [ 1√\nd1d2\n∣∣∣ ∣∣∣ ∣∣∣Θ̂−Θ∗ ∣∣∣ ∣∣∣ ∣∣∣ F ] ≥ c min\n{√ e−5α r (d1 + d2)\nn , α(d1 + d2)√ d1d2 log d\n} . (22)\nSee Appendix E.1 for the proof. The first term is dominant, and when the sample size is comparable to the latent dimension of the problem, Theorem 3 is minimax optimal up to a logarithmic factor."
    }, {
      "heading" : "5 Discussion",
      "text" : "We presented a convex program to learn MNL parameters from ordinal data, motivated by two scenarios: recommendation systems and bundled purchases. We take the first principle approach of identifying the fundamental limits and also developing efficient algorithms matching those fundamental trade offs. There are several remaining challenges. (a) Nuclear norm minimization, while polynomial-time, is still slow. We want first-order methods that are efficient with provable guarantees. The main challenge is providing a good initialization to start such non-convex approaches. (b) For simpler models, such as the PL model, more general sampling over a graph has been studied. We want analytical results for more general sampling. (c) The practical use of the model and the algorithm needs to be tested on real datasets on purchase history and recommendations."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported in part by NSF CMMI award MES-1450848 and NSF SaTC award CNS1527754."
    } ],
    "references" : [ {
      "title" : "Conditional logit analysis of qualitative choice behavior",
      "author" : [ "Daniel McFadden" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1973
    }, {
      "title" : "Thurstone. A law of comparative judgment",
      "author" : [ "L Louis" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1927
    }, {
      "title" : "Binary-choice constraints and random utility indicators",
      "author" : [ "Jacob Marschak" ],
      "venue" : "In Proceedings of a symposium on mathematical methods in the social sciences,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1960
    }, {
      "title" : "Individual Choice Behavior",
      "author" : [ "D.R. Luce" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1959
    }, {
      "title" : "Individualized rank aggregation using nuclear norm regularization",
      "author" : [ "Yu Lu", "Sahand N Negahban" ],
      "venue" : "arXiv preprint arXiv:1410.0860,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Preference completion: Large-scale collaborative ranking from pairwise comparisons",
      "author" : [ "Dohyung Park", "Joe Neeman", "Jin Zhang", "Sujay Sanghavi", "Inderjit S Dhillon" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "A grade of membership model for rank data",
      "author" : [ "Isobel Claire Gormley", "Thomas Brendan Murphy" ],
      "venue" : "Bayesian Analysis,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "Tie-Yan Liu" ],
      "venue" : "Foundations and Trends in Information Retrieval,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Mm algorithms for generalized bradley-terry models",
      "author" : [ "D.R. Hunter" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2004
    }, {
      "title" : "Bayesian inference for plackett-luce ranking models",
      "author" : [ "John Guiver", "Edward Snelson" ],
      "venue" : "In proceedings of the 26th annual international conference on machine learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Efficient bayesian inference for generalized bradley–terry models",
      "author" : [ "Francois Caron", "Arnaud Doucet" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Minimax-optimal inference from partial rankings",
      "author" : [ "B. Hajek", "S. Oh", "J. Xu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Iterative ranking from pair-wise comparisons",
      "author" : [ "S. Negahban", "S. Oh", "D. Shah" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Learning mixed multinomial logit model from ordinal data",
      "author" : [ "S. Oh", "D. Shah" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "A topic modeling approach to rank aggregation",
      "author" : [ "W. Ding", "P. Ishwar", "V. Saligrama" ],
      "venue" : "Boston University Center for Info. and Systems Engg. Technical Report http://www.bu.edu/systems/publications,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "What’s your choice? learning the mixed multi-nomial logit model",
      "author" : [ "A. Ammar", "S. Oh", "D. Shah", "L. Voloch" ],
      "venue" : "In Proceedings of the ACM SIGMETRICS/international conference on Measurement and modeling of computer systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Clustering and inference from pairwise comparisons",
      "author" : [ "Rui Wu", "Jiaming Xu", "R Srikant", "Laurent Massoulié", "Marc Lelarge", "Bruce Hajek" ],
      "venue" : "arXiv preprint arXiv:1502.04631,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Generalized random utility models with multiple types",
      "author" : [ "H. Azari Soufiani", "H. Diao", "Z. Lai", "D.C. Parkes" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Random utility theory for social choice",
      "author" : [ "H.A. Soufiani", "D.C. Parkes", "L. Xia" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "B. Recht", "M. Fazel", "P. Parrilo" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E.J. Candès", "B. Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Restricted strong convexity and (weighted) matrix completion: Optimal bounds with noise",
      "author" : [ "S. Negahban", "M.J. Wainwright" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Concentration inequalities: A nonasymptotic theory of independence",
      "author" : [ "Stéphane Boucheron", "Gábor Lugosi", "Pascal Massart" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Fast global convergence rates of gradient methods for high-dimensional statistical recovery",
      "author" : [ "A. Agarwal", "S. Negahban", "M. Wainwright" ],
      "venue" : "In In NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2010
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "J. Tropp" ],
      "venue" : "Foundations of Comput. Math.,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2011
    }, {
      "title" : "Empirical Processes in M-estimation, volume 6",
      "author" : [ "S. Van De Geer" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2000
    }, {
      "title" : "The concentration of measure phenomenon",
      "author" : [ "M. Ledoux" ],
      "venue" : "Number 89. American Mathematical Soc.,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We use a discrete choice model known as MultiNomial Logit (MNL) model [1] (described in Section 2.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "Technically, we utilize the Random Utility Model (RUM) [2, 3, 4] interpretation (outlined in Section 2.",
      "startOffset" : 55,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "Technically, we utilize the Random Utility Model (RUM) [2, 3, 4] interpretation (outlined in Section 2.",
      "startOffset" : 55,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "Technically, we utilize the Random Utility Model (RUM) [2, 3, 4] interpretation (outlined in Section 2.",
      "startOffset" : 55,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "Recently, there has been new algorithms and analyses of those algorithms to learn MNL models from samples, in the case when each user provides pair-wise comparisons [5, 6].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "Recently, there has been new algorithms and analyses of those algorithms to learn MNL models from samples, in the case when each user provides pair-wise comparisons [5, 6].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "[6] proposes solving a convex relaxation of maximizing the likelihood over matrices with bounded nuclear norm.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Our analysis techniques are inspired by [5], which proposed the convex relaxation for learning MNL, but when the users provide only pair-wise comparisons.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "In this paper, we generalize the results of [5] by analyzing more general sampling models beyond pairwise comparisons.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "When users provide ordered preferences, we use a discrete choice model known as MultiNomial Logit (MNL) [1] model that has a similar low-rank structure that captures the similarities between users and items.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "The random utility model (RUM), pioneered by [2, 3, 4], describes the choices of users as manifestations of the underlying utilities.",
      "startOffset" : 45,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "The random utility model (RUM), pioneered by [2, 3, 4], describes the choices of users as manifestations of the underlying utilities.",
      "startOffset" : 45,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "The random utility model (RUM), pioneered by [2, 3, 4], describes the choices of users as manifestations of the underlying utilities.",
      "startOffset" : 45,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "PL model has been widely applied in econometrics [1], analyzing elections [7], and machine learning [8].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "PL model has been widely applied in econometrics [1], analyzing elections [7], and machine learning [8].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "PL model has been widely applied in econometrics [1], analyzing elections [7], and machine learning [8].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "Efficient inference algorithms has been proposed [9, 10, 11], and the sample complexity has been analyzed for the MLE [12] and for the Rank Centrality [13].",
      "startOffset" : 49,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "Efficient inference algorithms has been proposed [9, 10, 11], and the sample complexity has been analyzed for the MLE [12] and for the Rank Centrality [13].",
      "startOffset" : 49,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : "Efficient inference algorithms has been proposed [9, 10, 11], and the sample complexity has been analyzed for the MLE [12] and for the Rank Centrality [13].",
      "startOffset" : 49,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "Efficient inference algorithms has been proposed [9, 10, 11], and the sample complexity has been analyzed for the MLE [12] and for the Rank Centrality [13].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 12,
      "context" : "Efficient inference algorithms has been proposed [9, 10, 11], and the sample complexity has been analyzed for the MLE [12] and for the Rank Centrality [13].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 13,
      "context" : "Efficient inference algorithms with provable guarantees have been proposed by applying recent advances in tensor decomposition methods [14, 15], directly clustering the users [16, 17], or using sampling methods [18].",
      "startOffset" : 135,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "Efficient inference algorithms with provable guarantees have been proposed by applying recent advances in tensor decomposition methods [14, 15], directly clustering the users [16, 17], or using sampling methods [18].",
      "startOffset" : 135,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "Efficient inference algorithms with provable guarantees have been proposed by applying recent advances in tensor decomposition methods [14, 15], directly clustering the users [16, 17], or using sampling methods [18].",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 16,
      "context" : "Efficient inference algorithms with provable guarantees have been proposed by applying recent advances in tensor decomposition methods [14, 15], directly clustering the users [16, 17], or using sampling methods [18].",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 17,
      "context" : "Efficient inference algorithms with provable guarantees have been proposed by applying recent advances in tensor decomposition methods [14, 15], directly clustering the users [16, 17], or using sampling methods [18].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 18,
      "context" : "For more general models, efficient inference algorithms have been proposed [19] but no performance guarantee is known for finite samples.",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 19,
      "context" : "Nuclear norm minimization has been widely used in rank minimization problems [20], but provable guarantees typically exists only for quadratic loss function L(Θ) [21, 22].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 20,
      "context" : "Nuclear norm minimization has been widely used in rank minimization problems [20], but provable guarantees typically exists only for quadratic loss function L(Θ) [21, 22].",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 21,
      "context" : "Nuclear norm minimization has been widely used in rank minimization problems [20], but provable guarantees typically exists only for quadratic loss function L(Θ) [21, 22].",
      "startOffset" : 162,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : "[23]) to bound the expectation of the deviation (cf.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "However, tightening the scaling with respect to α is a challenging problem, and such suboptimal dependence is also present in existing literature for learning even simpler models, such as the Bradley-Terry model [13] or the Plackett-Luce model [12], which are special cases of the MNL model studied in this paper.",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 11,
      "context" : "However, tightening the scaling with respect to α is a challenging problem, and such suboptimal dependence is also present in existing literature for learning even simpler models, such as the Bradley-Terry model [13] or the Plackett-Luce model [12], which are special cases of the MNL model studied in this paper.",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 21,
      "context" : "Following [22] we formalize this notion with “`q-ball” of matrices defined as",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 23,
      "context" : "We implement and solve the convex optimization (3) using proximal gradient descent method as analyzed in [24].",
      "startOffset" : 105,
      "endOffset" : 109
    } ],
    "year" : 2015,
    "abstractText" : "In personalized recommendation systems, it is important to predict preferences of a user on items that have not been seen by that user yet. Similarly, in revenue management, it is important to predict outcomes of comparisons among those items that have never been compared so far. The MultiNomial Logit model, a popular discrete choice model, captures the structure of the hidden preferences with a low-rank matrix. In order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases, we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples, and provide a matching information-theoretic lower bound.",
    "creator" : null
  }
}