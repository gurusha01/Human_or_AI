{"title": "On the Global Linear Convergence of Frank-Wolfe Optimization Variants", "abstract": "The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take <code>away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that has been successfully applied in practice: FW with away steps, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence under a weaker condition than strong convexity. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of the</code>condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.", "id": "c058f544c737782deacefa532d9add4c", "authors": ["Simon Lacoste-Julien", "Martin Jaggi"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Summary of paper:\n\nThis paper proves linear convergence of (four) variants of the Frank-Wolfe algorithm, under weaker conditions than in the existing literature.\n\nQuality:\n\nThe paper is well written and the results are interesting. I think overall it would make a decent addition to NIPS, as it is a meaningful advancement of our understanding of the Frank-Wolfe procedure, which is increasingly important eg. for sub modular problems.\n\n Clarity:\n\nThe paper is clearly written overall. I feel the reader of an 8-page nips version would be better served with a \"in words\" description of the different variants, with the exact algorithmic description boxes being in the appendix. As currently written it is hard to parse what precisely the variants are doing differently, one any way needs to rely on the text.\n\nThe intuition section is useful.\n\nOriginality:\n\nAs per this reviewer, the main original contribution of this paper is to present a more unified view of the various FW variants already proposed, and also a more unified proof of their convergence.\n\n Significance:\n\nOverall, this paper will provide a good reference for variants of the FW algorithm. FW algorithms, like other first-order methods that leverage problem structure, have seen renewed interest in the ML community, so this could be interesting to the NIPS audience. This paper proves linear convergence of (four) variants of the Frank-Wolfe algorithm, under weaker conditions than in the existing literature. It gives both a unifying view of existing results, and establishes new convergence guarantees.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper has some writing issues. There are a couple of typos, missing punctuation signs (like in page 2, \"Section 4As A is finite\". The reading is pleasant in general until page 4. The description of the Pairwise Frank-Wolfe method is problematic. Repeated words, undefined concepts (weight), weird sentences... There are some other writing problems along page 4, and in the rest of the document. For instance, there are undefined acronyms (like QP).\n\nAs mentioned, the theoretical results are interesting, as well as the interpretations.\n\nThe experimental section however, is quite weak. Firstly, it's very hard to see anything in Fig 2, and the lines are indistinguishable in printed version.\n\nSecondly, the (experimental) advantages of the variants of the FW algorithms are known. I expected to see some comments on the corroboration of the theoretical results presented in the paper: convergence rate, constants, etc. Finally, I know that the presence of the sparse coding problem is didactic, as a toy example, but it would be correct to add some state-of-the-art methods for the Lasso problem.\n\nMinor comments:\n\n- be consistent in the bibliography: for instance: ArXiv, arXiv, arXiv.org, for three different papers.\n\n- a section with the conclusions would be appreciated\n\n- the content of Section 4 could be placed somewhere through the text, there's no need for a whole section  In this paper, the authors address the convergence rate for the Frank-Wolfe algorithm and some of its variants.The theory is well presented (although the general writing of the paper should be significantly improved), and the results seem interesting and powerful. The last two sections have some problems", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Although I acknowledge the main thrust of the paper is theoretical, the experimental part is rather weak. In addition to illustrating the improved performance achieved by the several variants of Frank-Wolfe considered, it would also be very useful to give an idea about when these variants become competitive (in practice) with other methods, specially in problems for which many other methods can be used, such as l1-regularized regression in its unconstrained formulation.\n\n  This is a nice paper, providing theoretical support to a number of improvements to the family of Frank-Wolfe algorithms, some of which had been shown in practice to considerably speed up convergence, but still lacked theoretical guarantees. The paper seems solid and well written, apart from a few minor mistakes that the authors can surely correct by carefully proofreading it.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Report for \"On the Global Linear Convergence of Frank-Wolfe Optimization Variants\".\n\n ## Summary\n\nThe authors consider the problem of minimizing a strongly convex or partially strongly convex function over a polyhedron. They describe and analyse different variants of the Frank-Wolfe algorithm. The authors show that the different variants of the algorithm allow to bypass the known fact that the vanilla Frank-Wolfe algorithm is not able to take advantage of strong convexity properties of the objective function. This leads to proof of linear convergence for all the variants presented. The analysis is based on the introduction of a combinatorial positive constant that relates to a notion of condition number for polyhedra. Numerical results on both synthetic and real examples are presented.\n\n## Main comments\n\nThe paper present various versions of the original algorithm which were introduced previously in the literature. Linear convergence is a new finding for some of the variants and constitutes an interesting contribution. Furthermore, the pyramidal width introduced in the paper may have interesting applications beyond linear oracle based method. The proof arguments are reasonable up to my understanding and I do not see any major issue related to the content of this work. I detail a few comments that should be taken into account in the next paragraph, some of them may need to be reflected in the text. Additional minor comments are given in the following section.\n\n ## Detailed comments\n\n- It may be worth mentioning what happens for these algorithms when the strong convexity assumption is not verified.\n\n- There is a lot of emphasis on the notion of affine invariance. I understand the idea and it is important. But I think it could be clarified. For example, I think that the fact that the linear map is surjective is important. Also I do not think that the statement that A is arbitrary in [17] (line 710) is true as the paragraph dedicated to affine invariance in [17] mentions explicitly the surjectivity.\n\n - In the proof of Theorem 3 in the appendix, I think that the conclusion of line 640 is correct but the arguments are not. First, it could be explicitly mentioned what is meant by KKT. Second KKT only expresses stationarity at the first order and I am not sure that this is enough to conclude that the solution of minimizing a linear function on the intersection of a cone and the unit (euclidean) sphere is on the relative boundary of the cone. I do not know what the authors expicitly meant by \"KKT\", but for example for the cone $\\RR_+ \\times \\RR$, $d= (-1,0)$ does not belong to the cone and $y = (1,0)$ is in the interior of the cone and satisfies some stationarity condition\n\nwhich I guess correspond more or less to the KKT condition the authors have in mind.\n\n- The MNP variant actually requires to have a subroutine that minimizes the objective over an affine hull. Since in the model, $f$ is only defined on script M, the setting in which this make sense is slightly different. Furthermore, this has very important practical implications in terms of implementation. This could be emphasized. I guess the line search in step 7 involves the constraints. In this case, this is another significant requirement compared to other variants for which this is explicitly avoided (see line 163).\n\n- The authors claim that everything works the same for the partially strongly convex model just by replacing a constant by another. It is a bit fast since the constants in (21) and (34) look quite different, in particular, there are multiplicative 2 factor that do not occur at the same place. Some hints are welcome.\n\n- The away step variant under the partial strongly convex model was treated in [4]. Is there a difference in the estimated rates? It would be nice to have a discussion, for example based on the cases for which the pyramidal width is known.\n\n- For the simplex, the fact that the pyramidal width is the same as the width deserves more justification. Also the presentation would benefit from a more precise citation of [2] (which result the authors refere to in this paper).\n\n- Up to my understanding, the convergence analysis of [12] does not treat the case \"strongly convex objective + strongly convex set\". This paper actually proposes the linear convergence as an open problem contrary to what is stated in the introduction.\n\n ## Minor comments\n\n- The abstract mentions \"weaker condition than strong convexity\". I would add \"of the objective\".\n\n- May be add some reference about the convergence rates of projected gradient method\n\n- Line 99: typo \"While the tracking the active set\"\n\n- Regarding footnote 3, the lower bound actually holds for specific sequences that never reach the face of interest. It does not hold for every sequence, it depends on the initialization and the problem at hand.\n\n- Line (151) mentions that the active set is small, while after a large number of iterations it is potentially large, unless there is some specific step performed to optimize the vertex representation.\n\n- Line 178: \"is to in each step only move\", I would add commas\n\n- Line 188: typo \"due our\"\n\n- Line 210: typo \"more computed more efficiently\"\n\n- Line 212: \"overarching\"?\n\n- (7) could actually be summarized as $h_t \\geq \\min{g_t/2, g_t^2/(LM^2)}\n\n- The authors conjecture a nonincreasingness property for the constant they introduced. Do they have an intuition why this should be the case beyond the fact that this constant is greater for smaller for the cube than the simplex?\n\n- Figure 2 is hardly readable\n\n- On page 12 (appendix), the notation $r$ is used in the main text and in footnote 7 to denote two different thing. Similar comment holds for y.\n\n- The argument following (14) go really fast and non trivial details are missing. It should be written explicitly that (14) is equivalent to the problem mentioned on line 640. Here also, $y$ and $y^*$ are used to denote the decision variable and solution of two different problems.\n\n- On line 689, (14) should be (15).\n\n- Line 700 makes a self reference to \"the appendix\": it is itself part of the appendix.\n\n- Line 950, I guess \"drop step\" should be \"swap step\".\n\n- In (33), $B$ should be $B_1$.  Linear convergence is a new finding for some of the variants presented and constitutes an interesting contribution. I do not see any major issue related to the content of this work.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
