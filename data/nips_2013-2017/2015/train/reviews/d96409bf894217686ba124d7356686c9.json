{"title": "Deeply Learning the Messages in Message Passing Inference", "abstract": "Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to directly estimate the messages in message passing inference for structured prediction with Conditional Random Fields CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension of message estimators is the same as the number of classes, rather than exponentially growing in the order of the potentials. Hence it is more scalable for cases that a large number of classes are involved. We apply our method to semantic image segmentation and achieve impressive performance, which demonstrates the effectiveness and usefulness of our CNN message learning method.", "id": "d96409bf894217686ba124d7356686c9", "authors": ["Guosheng Lin", "Chunhua Shen", "Ian Reid", "Anton van den Hengel"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Line 91: joint -> jointly Line 161: massage -> message Line 266: convlutional -> convolutional Line 269: correspond -> corresponds Line 301: reminding -> remaining Line 408: We has -> We have Line 416: perform -> performs The authors present a solid idea and demonstrate that it obtains a modest improvement over state-of-the-art on PASCAL 2012 segmentation. The idea is interesting however it is not particularly well presented in the text. I would recommend the use of figures in the exposition.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a CNN-CRF approach for structured prediction by explicitly learning CNN messages for inference. I have several concerns as described below:\n\n(1) A schematic figure might help explain the overall method. So far the exact roles of the proposed components are not clear. The equations should be made more intuitive.\n\n(2) The scope of the proposed method seems to be quite limited as only very limited number of neighborhood connects can be considered and there are many approximations to the method. It lacks sufficient elegance and cleanness in the algorithm.\n\n(3) The experimental results are only reported on the VOC dataset. Since this is a NIPS submission, justification on other popular datasets for structured labeling, e.g. OCR and POS, is needed.\n\nOverall, the method is quite convoluted and it is hard to see what exactly the benefit of the proposed method is. The general direction of tackling the structured prediction problem by learning message passing in convolutional neural networks based CRF model is good. Improved results over the state-of-the-art on the PASCAL VOC image labeling dataset are reported. However, the scope of the proposed method is till somewhat limited and the solution is kind of intermediate.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes to directly learn the messages in message-passing inference (as in inference machines[8]) using CNNs. The paper shows state-of-the-art results on PASCAL VOC semantic segmentation.\n\nFirst, note that [12] also learn messages in one-step message-passing inference. The math is not described well, and the details are different (that paper tackles pose estimation) but the insight that learning messages is easier than learning potentials, and that CNNs can be used to learn messages, is similar. This paper is missing an acknowledgement of this, and a discussion of what this paper adds.\n\nSecond, let's take a closer look at what the approach is actually doing: the final score vector for a pixel is the sum of the scores output by each factor->pixel message, and each factor->pixel message is computed by a *separate deep network*. This means that the method is eerily close to a simple ensemble of four models trained with different initializations.\n\nAll the other methods compared to (except ContextDCRF, which suffers from a similar problem) have only one network making the prediction. This suggests that an ensemble model is a natural baseline. I would encourage the authors to try such a model.\n\nGiven these two points I am somewhat skeptical of the originality and significance. In terms of clarity, the paper is well written, and the math is very clearly described. I would still recommend the paper for acceptance because this idea of learning messages using CNNs is important, and because the results are state-of-the-art.\n\n The notion that one can train a CNN to predict the messages is quite interesting and is described well. However, [12] also directly learn the messages (for a one-step message passign inference) using CNNs, albeit for a different task. The ensemble effect coming from multiple networks is also not disentangled (see below)", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Given CRF potentials, one can write down the marginal predictions after only passing the initial messages from factors to variables. This paper incorporates that parametric form into a neural network, and fits it directly to minimize training error. This architecture appears to be empirically successful on a meaningful benchmark. I found the framing of the work partially misleading: As far as I can tell, the cost function reported doesn't care about structured prediction just pixel-wise errors, and no CRF model is actually fitted.\n\nTo me \"structured CRF prediction\" means that there is a joint distribution over the labels given an input. Given such a model, it would be possible to fantasize joint plausible configurations of the labels, or report the jointly most probable labeling.\n\nI don't think the \"Intersection over Union\" score cares if the vector of labels is jointly plausible. The way to maximize this score is to predict the most probable label for each pixel independently. Learning a CRF model would learn a joint distribution, but here predictive performance, which doesn't care about the joint distribution, is maximized, not a CRF model. It isn't even clear to me if the \"messages\" in this paper will always correspond to realizable factors(?), let alone likely ones.\n\nThe work is certainly interesting. The idea is a neat one, and seems to be an excellent heuristic for setting the parametric form of the output layer of a neural network to get good performance. I simply found some of the framing misleading and genuinely confusing on first reading.\n\n line 332: \"One advantage of message learning is that we are able to explicitly incorporate the expected number of inference iteration into the learning procedure.\" -- However, only one iteration is ever tried, and I presume implemented. I'm not convinced the details have been worked out or investigated enough to make this claim.\n\n Typo: line 54: massage -> message\n\nThere's quite a few incorrect word endings, usually singular/plural disagreement. I lost motivation to note them down after noticing a spell checker hadn't been run: line 66: potenials line 120: sematic line 265: convlutional A neat way to set the output layer of a neural network for labelling pixels. Inspired by the parametric form of CRF messages, but not actually fitting a structured prediction model (as advertised) as far as I can tell.Nothing in the rebuttal changes my view: this paper isn't doing structured prediction as it claims it does. It does appear to be accurate at pixel-wise labelling.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
