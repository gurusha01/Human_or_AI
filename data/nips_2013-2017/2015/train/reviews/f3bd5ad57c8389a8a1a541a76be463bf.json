{"title": "Stochastic Expectation Propagation", "abstract": "Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local-approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale datasets settings. However, EP has a crucial limitation in this context: the number approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP).  Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of N. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.", "id": "f3bd5ad57c8389a8a1a541a76be463bf", "authors": ["Yingzhen Li", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Richard E. Turner"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "QUALITY This is an interesting idea. However, I think the paper requires two major changes to be interesting to a NIPS reader.\n\nFirst, the paper should be up front about its experimental nature and follow with an in-depth study. Specifically, the experiments should clearly highlight the strengths (big datasets) and weaknesses (clear, thorough comparisons and failure cases).\n\nSecond, the paper requires some editorial work. The important sections (see below) could use more explanation. The experimental study should clearly motivate the setup and carefully discuss the results with technical detail. This is what a NIPS reader would want to get from such a paper.\n\nSome details:\n\nBayesian Probit Regression: I do not understand why this approximate KL is a good metric to report. Why not just compare means and variances of parameter estimates as compared to Stan/NUTS? Isn't that much more straightforward and natural?\n\nFigure 3(c): I am confused as to why SEP with K=1 is compared to DSEP with K=10. Am I missing something here?\n\nMixture of Gaussians: why did you now switch to this F-norm metric? Also wouldn't it be better to simulate some data where EP fails to recover the truth? I would rather want to see whether SEP \"fails in the same way\" as EP, or whether the single f(\\theta) factor provides a different behavior.\n\nHaving reached Section 5.3, I am a bit dissatisfied by what these first two experimental sections present. I have not learned much about the nature of SEP: specifically, I don't understand how it fails. The investigation of minibatch sizes is limited, as the experiments do not paint a clear picture. Also, given that the main motivation of this work is to scale EP, I don't understand why the majority of the experiments focus on tiny datasets.\n\nSection 5.3 requires significant overhaul. The experimental setup is brushed aside with a citation: this is unacceptable. The results are discussed in a completely conversational manner, with little technical detail or specificity, using words like \"often\", \"interestingly\", \"possibly\", and \"likely\".\n\nCLARITY Figure 3(b): DAEP is never discussed, either in the caption or in the text.\n\nI am not sure what Figure 2 adds to the paper. I would rather the authors expand on Sections 3 and 4.1: both of these sections are important for the paper, and yet they would benefit from a clearer exposition. Removing Figure 2 would afford this space to clarify these important ideas of the paper.\n\nORIGINALITY The idea is nice. It is also original: i have not seen any work that addresses EP's memory constraints. The proposal is straightforward; thus a NIPS reader will immediately ask: \"that's a good idea, I wonder why it works.\"\n\nSPECIFICS line 48: ... having A myriad OF local ...\n\nline 69: truely\n\nline 149: summaried\n\nline 199: arguable\n\nline 214: this paragraph is vague. I cannot tell what you are referring to without looking at the supplementary material; thus I have little motivation to look it up in the supplementary material.\n\nline 257: why is \"mixture\" capitalized?\n\nfigure 2 should appear on page 5, not page 6.\n\nline 290: i am thoroughly confused by this comparison metric.\n\nline 292: there is no citation numbered [25]. should it be [24]? This makes me worried about all of the citations in the rest of the paper... This paper proposes a solution to EP's memory constraint. It does to by considering a single approximating factor instead of one per likelihood term. The paper contains little theory (and the authors admit that) but presents a slightly dissatisfactory empirical story. As such I am on the fence of accepting this paper to NIPS in its current form. Depending on the other reviews, I would encourage the authors to refresh the empirical section, clarify the narrative, and resubmit. I have no doubt that it will eventually get accepted.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a new version of the EP algorithm where one iteratively refines a global approximation of the target instead of refining individual approximating factors. This reduces memory requirements and performs experimentally well compared to standard EP.\n\nThis is a useful and neat idea.\n\n Quality: good, nice idea.\n\nClarity: good, however a few typos to correct.\n\nOriginality: original to the best of my knowledge.\n\nSignificance: a useful algorithm which could find quite a few applications.\n\n Minor comments:\n\n - The convergence of EP is not guaranteed, have the authors observed similar convergence problems for SEP?\n\n- In the mixture of Gaussian clustering example, the posterior distribution has 4! modes. Is your SEP approximation enjoying this property? (I am not sure I fully understand Figure 4) The paper presents a new variation of the EP algorithm where iterative refinement of a global approximation of the target is performed. Simple and interesting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes a variant of EP (SEP) which assumes the local factor (site) approximations to be identical with the result that the memory requirements are reduced by a factor of N. This is helpful in large data applications since only the global posterior approximation needs to be held in memory during the EP iterations. By experiments with probit regression, Gaussian mixture models and neural networks, the paper demonstrates that the SEP approximation can be quite close to EP in terms of accuracy.\n\nQuality:\n\nThe proposed methods are reasonably well justified with references to existing literature and the experiments are well made. My biggest concern about the SEP approximation is the regime where the SEP/AEP assumption fails? Based on 5.1 DSEP with a manageable number of different local approximations seems to be the method of choice? Also it would be nice to illustrate possible problematic cases in terms of approximation quality with very simple examples.\n\n Clarity:\n\nThe paper is well written and clearly structured. The analogy with SVI/VI could be explained more clearly in the main text. The remarks on the VI limit of PEP on lines 138-140 could be explained more clearly. How is this related to the discussion in [23]? Also in A.2 the approximate family and the definition of the natural parameters could be written down to clarify the text.\n\n Lines 152-154: In many usual settings with Gaussian approximations the complexity of the site approximations is much less than D^2, and doesn't this also often depend on the number of model factors that are being approximated with each site approximation?\n\n Originality:\n\nThe average local factor assumption is similar to AEP, which has already been published. For this reason it would be nice to summarize e.g. the DSEP/DAEP comparison from figure 2 of the supplements more clearly in the main text. Also distributed EP settings have been proposed earlier but overall I like the paper because it puts together many different variants of EP.\n\n Significance:\n\nOverall the paper makes incremental contributions to various directions that extend the EP framework but I like the paper because it puts together many different variants of EP.  A well written paper that makes incremental extensions to EP but puts together many different variants of EP. The models and other settings where the SEP/AEP assumption possibly fails could be illustrated more thoroughly.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents an elegant modification of EP that reduces memory requirements at cost of reduced accuracy (and sometimes more computation).\n\nThe paper explores connections with related algorithms.\n\nThe extensive set of experiments give a convincing argument for the practicality of the algorithm.\n\nThe only weakness of the paper is some sloppiness in the presentation.\n\nThe abstract and introduction use the word \"computation\" when they actually mean \"memory\".\n\nMemory does not equal computation.\n\nSEP is actually adding computation in some cases, in exchange for less memory.\n\n The description of SEP is unclear about the number of iterations.\n\nIn Table 1, was SEP run to convergence (if such a concept exists), or was it manually stopped?\n\n In section 4.1, it is not clear what is meant by \"converges in expectation\".\n\nExepctation of what?\n\nWith what being random?\n\nTheorem 1 has a similar issue.\n\nFurthermore, this claim is not proven.\n\nThe \"proof\" of theorem 1 in appendix A is not a proof---it is simply re-asserting the theorem.\n\nIf the authors cannot make a precise statement with a real proof then this claim should not appear in the paper.\n\nThe claim in section 4.4, that messages to latent variables associated with a single data point need not be stored, requires a special assumption.\n\nThe assumption is that the term p(x_n | h_n, theta) can be updated in one step.\n\nThis is true for the model in section 5.2, but it is not true in general.\n\nFor example, LDA has a latent variable for each document (the topic proportions) but it cannot be updated in one step.\n\nSo these messages will need to be stored, unless you want to significantly increase the amount of computation.\n\nAppendix B.2 should say \"This procedure still reduces memory by a factor of N/K\".\n\nIn section 5.1, ADF should be collapsing to a delta function at the true posterior mode, not mean.  An elegant modification of EP, a nice discussion of connections to other algorithms, and extensive experimental results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: The paper provides a stochastic generalization of expectation propagation. In EP local contributions are removed and updated for each data point - the down side being that sufficient statistics must be saved for each data point. For models with many parameters this becomes expensive. Instead of removing the sufficient statistics for each data point, instead the authors remove the global sufficient statistic, weighted by the sample size being updated. The method is generalized to other EP variants such as parallel EP and distributed EP for when there is heterogeneity in the data set, and to models with latent variables. For the experiments the method is applied to probit regression, gaussian mixture models, and Bayesian inference in neural networks. Empirically they show that their method provides computational savings without sacrificing performance.\n\nQuality: The method is sound, well motivated and experimentally validated in three different, but canonical, applied domains.\n\n Clarity: Easily readable and clear. Excellent writing style.\n\nOriginality: The core idea behind the paper is simple but clever, elegantly solving the memory overhead in EP. Novel extensions of the stochastic EP method to parallel, mini-batches for better approximations, and latent variable models are also developed and explored. To my knowledge this approach and its extensions are all completely original.\n\nSignificance: The contributions are significant as they provide scalable alternatives for EP. I expect the paper to be as important to EP as SVI was to VB. The paper is well motivated, extremely clear, and the research contributions are important for scaling EP inference for complex models with many parameters. Extensions and connections with existing EP and VB techniques are explored for an overall very thorough and mature paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
