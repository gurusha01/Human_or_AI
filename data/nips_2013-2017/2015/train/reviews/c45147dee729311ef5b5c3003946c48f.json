{"title": "Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution", "abstract": "Super resolving a low-resolution video is usually handled by either single-image super-resolution (SR) or multi-frame SR. Single-Image SR deals with each video frame independently, and ignores intrinsic temporal dependency of video frames which actually plays a very important role in video super-resolution. Multi-Frame SR generally extracts motion information, e.g. optical flow, to model the temporal dependency, which often shows high computational cost. Considering that recurrent neural network (RNN) can model long-term contextual information of temporal sequences well, we propose a bidirectional recurrent convolutional network for efficient multi-frame SR.Different from vanilla RNN, 1) the commonly-used recurrent full connections are replaced with weight-sharing convolutional connections and 2) conditional convolutional connections from previous input layers to current hidden layer are added for enhancing visual-temporal dependency modelling. With the powerful temporal dependency modelling, our model can super resolve videos with complex motions and achieve state-of-the-art performance. Due to the cheap convolution operations, our model has a low computational complexity and runs orders of magnitude faster than other multi-frame methods.", "id": "c45147dee729311ef5b5c3003946c48f", "authors": ["Yan Huang", "Wei Wang", "Liang Wang"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper uses a bidirectional, convolutional RNN to do multi-frame super resolution. They compare their method to a number of single frame and multi-frame methods.\n\n This appears to be an extension of the model proposed by Dong et al 2014 to the temporal domain. The convolutional network is extended into the temporal domain by having featuremaps which are dependent in time alongside the featuremaps found in a standard feedforward convolutional network. The recurrent convolution is novel. The model is fairly simple and elegant to implement.\n\nThe bidirectional aspect is not novel and goes back to at least as early as 1997 (Schuster and Paliwal) [http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf]. Bidirectional networks are also extremely common in the NLP and speech recognition literature.\n\nThe authors should provide more detail if pretraining the feedforward weights is absolutely necessary for the model to work as intended. If it is necessary, the authors should provide more detail to the pretraining process (whether it was trained on static images or video images frame by frame).\n\nThe recurrent/conditional convolutions provide the most substantial gains is a nice result. But the marginal improvement provided by both connections combined (v,r,t) seems to imply one becomes redundant in the presence of the other.\n\nThe method seems to work better than other previous methods, although it is hard to evaluate by how much.\n\nOther comments: -- No citations for bidirectional RNNs (see above) -- The filter visualization section is not very illuminating -- Could offer better motivation for the architecture, why would one expect this to work better than existing methods? -- Not clear how they deal with edge effects. Eg. using a convolution reduces the size of the image. Going forward in time, they have filter sizes of size 1 so the size doesn't change, but when going deeper into the network, the filter size is 9, so the resulting feature map is smaller than the original image. Not sure how they deal with this.\n\n-- figure 3, the image with the flag and power lines, the region with the power lines has some ringing artifacts, comments?\n\n Overall, I like the paper. the recurrent convolution is a very good/obvious idea.\n\n The use of a convolutional RNN is a good idea and a novel contribution to this problem.However the paper could do more to shed insight on why the method actually works well compared to existing methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Paper proposes a recurrent CNN architecture for multi-frame super resolution. It employs three kinds of convolutional filters: (i) Feed-forward; (ii) Recurrent; (iii) conditional convolutions. Overall the architecture is novel but seems straightforward combination of existing deep learning modules.\n\n Technical quality of paper is borderline. It is a straightforward combination of existing deep learning modules. However, at the same time novelty lies in the fact that it has not been done before for multi-frame SR.\n\n In the rebuttal authors should address the following questions: (i) How many parameters does the model have? (ii) Did they compare with pre-trained SR-CNN or whether they re-trained SR-CNN on the new data set? (iii) It is not clear why SR-CNN run-time slower than BRCN? (iv) In Fig 3 & 5 how does the images for SR-CNN looks like?\n\n Originality of the paper is incremental. In table 2 authors show how individual parts of the architecture contribute. Though simply BRCN {v,r} outperforms the existing methods. It is well acceptable by now that making architecture more complex and increasing the number of parameters helps improve the accuracy. This is evident from table 2 that BRCN {v,t} {v,r,t} & {v,r,t,b} gives incremental improvement in performance.\n\n Significance of the paper for NIPS audience is open for discussion. The architecture introduced in the paper is novel but it is incremental to the field of deep learning. Paper proposes a novel architecture for multi-frame SR. It is easy to follow and builds upon previous work on single-frame CNN for SR [6]. In experiments some details are missing which can be fixed. Overall the paper is incremental but thorough. Its suitability to NIPS audience is open for discussion.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a bidirectional recurrent convolutional network for multi-frame super resolution. In a sense the paper formulates the problem in a way that I think most people familiar with RNNs would consider as a straightforward and sensible way to formulate the problem. The approach appears novel. The Dong et al. ECCV 14 work on learning deep convolutional networks for image super resolution appears to indeed be the most relevant recent work, but it was applied to static frames. This work seems like a very natural extension in the general trajectory of deep convents for super resolution. This combined with the fact that this work provides an evaluation of reasonable quality makes the paper acceptable for NIPS in my view.\n\nThe most interesting aspect of this work is perhaps the exploration in Table 2 where the effect of feedforward convolutions, recurrent convolutions and conditional convolutions are evaluated.\n\n The statements about MATLAB vs Python when considering different explanations for the differences in run times seems like a side issue compared to the more important question of GPU acceleration. Modern convolutional neural networks are almost always implemented using GPUs in state of the art systems. This paper states on line 362 that the implementation of the approach presented here is in Python, but there is no discussion either way concerning the use of GPU acceleration.\n\nPlease clarify this issue. If a GPU was not used, this method could be dramatically faster. If a GPU was used, then the comparison with prior work really needs to be cast in that light. Many prior methods are likely amenable to GPU acceleration as well.\n\nAccurate motion estimation in particular is given as the traditional bottleneck for non-RNN based methods. Such techniques could potentially be quite effectively accelerated with GPU methods.\n\nLanguage Issues:\n\nAbstract:\n\n* Considering that recurrent neural network[s] (RNNs) can... * Different from vanilla RNN[s]... * conditional convolutional connections from previous input layers to [the]current hidden layer are added for enhancing visual-temporal dependency mod[el].\n\nPlease proof read the body text for other language issues.\n\nConclusions: In the future, we will [perform] comparison[s] with [other] multi-frame SR methods\n\n This paper presents both some strong quantitative results as well as clear visual results illustrating the effectiveness of a bidirectional recurrent convolutional network approach for multi-frame super resolution. The paper has a few language issues (which need to be resolved by the authors for the paper to be acceptable), but the model explored is very sensible and the results have a good mix of quantitative performance increases, increases to visual quality and compelling computation times relative to prior art.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "[this is a light review] I would encourage the authors to discuss in more detail how their approach compares to other approaches w.r.t. quantitative and qualitative evaluation of between frame quality (i.e. consistency/flickering etc.).\n\n== post rebuttal == According to the rebuttal, a positive point is that the the authors plan to release their model. Additionally it would be helpful if they release their code to allow repeatability.  The paper presents a novel and efficient (w.r.t. runtime) approach, which is evaluated against a large body of related work as well as ablations. Given that the task is video, it would be good to extend the qualitative evaluation of multi-frame in Fig 5. with a discussion and a quantitative comparison (e.g. consistency measure).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "81 : Maybe explain the current motion based methods better for readers who are not familiar. -Is it the first work that use convolution for recurrent transition for any kind of problem? If not please mention it. -The name conditional convolution is misleading and not accurate as there is no conditioning , it's just a convolution from previous time step input. 180: Explaining second hidden layer in detail is redundant\n\n3.2 Why out of no where suddenly it's compared to TRBM in a whole section? If TRBM an important related work it should have been mentioned before.  The paper propose a new sequence based video super resolution model. The model replace the fully connected recurrent transition with convolutional recurrent transition and add extra convolution transition from the last time step inputs. They achieve better results with lower computational cost. It's an interesting computer vision application paper, with good results and I enjoyed reading it. But I am not sure about its impact to the nips community and it might be better received at computer vision conference.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
