{"title": "Collaboratively Learning Preferences from Ordinal Data", "abstract": "In personalized recommendation systems, it is important to predict preferences of a user on items that have not been seen by that user yet. Similarly, in revenue management, it is important to predict outcomes of comparisons among those items that have never been compared so far. The MultiNomial Logit model, a popular discrete choice model, captures  the structure of the hidden preferences  with a low-rank matrix. In order to predict the preferences, we want to learn the underlying model from noisy observations of the low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural approach to learn such a model is to solve a convex relaxation of nuclear norm minimization. We present the convex relaxation approach in two contexts of interest: collaborative ranking and bundled choice modeling. In both cases, we show that the convex relaxation is minimax optimal. We prove an upper bound on the resulting error with finite samples, and  provide a matching information-theoretic lower bound.", "id": "dabd8d2ce74e782c65a973ef76fd540b", "authors": ["Sewoong Oh", "Kiran K. Thekumparampil", "Jiaming Xu"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The authors analyze techniques for statistical preference learning given ordered data, proving recovery guarantees when the underlying parameter matrix is low rank. The manuscript is well written and satisfactorily addresses important technical questions, but lacks sufficient empirical evaluation. The presented results depend on notions of restricted strong convexity, under fairly standard assumptions. Further, the authors provide both upper and lower bounds for their analysis, suggesting that the proposed sample complexity is close to optimal under the presented assumptions.\n\nThe authors combine models for collaborative ranking and bundle choice modeling under the rubric of collaborative preference learning. While it is clear that the statistical results on bundled choice modeling are closely related to the results on collaborative ranking, I think the transition between the two models is clunky, and the authors may have been better served be focusing on one or the other problem class.\n\nI suggest that the authors consider additional experimental evaluation. For instance, it would have been useful to empirically compare the proposed approach to other approaches proposed in the literature to evaluate real work performance and/or evaluate the robustness of the proposed approaches to model mis-specification. Further, no experimental evaluation is presented for the bundled choice modeling problem (further emphasizing my suggestion of removing this section). I understand that the goal of the paper is mainly to present theoretical results, but the area of study is well served by empirical results for new techniques.\n\n Minor comments: Line 124: If l is indexed starting from 1 what is S_{i,1} (since v_{i,0} does not exist)? The authors should clarify if the null definition is implicit.\n\nThe claim (6) is not obvious to me, can the authors provide some more detail?\n\nLine 288: Note that deleting the row mean increases the rank of Theta by 1 in general. Please fix and double check the results of Fig 1, or show that the rank of the de-meaned parameter does not change. The authors analyze techniques for statistical preference learning given ordered data, proving recovery guarantees when the underlying parameter matrix is low rank. The manuscript is well written and satisfactorily addresses important technical questions, but lacks sufficient empirical evaluation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "summary of paper: This paper proposes a new approach to inference for a low-rank matrix factorization model with ordinal data and another application.\n\nIt is not clear if MultiNomialLogit (MNL) models have already been applied to these particular problems and the contribution is just the inference approach (nuclear norm minimization), or if the models are contributions as well.\n\nRegardless, the authors give performance guarantees for their approach, but do not demonstrate it on real or even synthetic data.\n\nquality: good - The theory appears to be thorough.\n\nThe contributions obviously required substantial work. clarity: poor - Needs significant editing to address both minor issues and also major flow of ideas within sections.\n\nShould have more discussion of implications of contributions. originality: mediocre - Low rank matrix factorization has already been addressed from many, many angles; there, the contribution appears incremental.\n\nI am less familiar with the other application. significance: poor - The authors do not compare to any other methods (there are many well-established ones for matrix factorization), either empirically or theoretically.\n\nEven a small improvement can have a large impact, but the reader is left to assume that this paper would have no practical and only minor theoretical impact on the problems described.\n\nsome detailed points - line 20 (abstract), Awkward/unclear/editing mistake?: \"A popular discrete choice model of multinomial logit model captures...\"\n\n- line 18, we're obviously talking about personalized user preferences, but that could be more clear-the user isn't mentioned until after the problem of preference prediction, which seems backward; then, we're talking about comparisons without talking about what we're comparing.\n\nThis sequence of ideas is very convoluted. - abstract starts off with motivating applications, but then the contribution bears to only be theoretical (presenting an approach and proving a bound), with no justification as to why the theory is important, aside from it being a \"natural approach\" to for inference on \"a popular model.\"\n\nNo hint of how the theory impacts application, nor mention of experiments on real or even synthetic data. - line 33: should be comma after applications: \"applications, such as...\" - 1st line of intro and abstract are identical; while not strictly prohibited, it disengages the reader - line 44, 53: elements in a list (i.e., (a) something, (b) something) should have some kind of parallel structure.\n\nHere, (b) is the goal and (a) is the means.\n\nAgain, the sequences of ideas seem convoluted. - line 57: MultiNomialLogit model not capitalized consistently with abstract; needs citation - line 61: \"The\" should be capitalized at start of sentence - line 62: it might have been better to only use one example in the intro (the bundling is more novel, so I'd pick that); you can always add the other in the experiments/results - line 70: it's great to have your contribution highlighted - line 70: why is it natural to take this approach? - contribution: I think you're saying that MNL is already applied to ordinal low-rank matrix factorization, but that you're doing a new kind of inference for the model and applying that to two cases.\n\nIs that right?\n\nOr has the MNL only been applied to non-ordinal MF problems and you are applying the MNL to ordinal data for the first time? - line 76: RUM should also have a citation here - line 94: spacing issues - Sections 3 and 4, generally: the practical implications of the theorems and their corollaries are unclear.\n\nPerhaps I need to study them in greater detail, but the authors could highlight in words, or empirically, the take home message.\n\nThe single figure does not immediately contribute to greater understanding. - line 289: RMSE comes out of nowhere.\n\nFurther, while RMSE is commonly used, it's not actually a good metric for rank-based item recommendation. - Discussion shouldn't just be a to-do list for the future.\n\nWhat should the reader take home? The contributions are mostly clear, but they are not well motivated or justified.This paper needs to be thoroughly edited for clarity and the contributions should be compared to other methods on real or synthetic data.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors give a multinomial logit (MNL) model to describe user preferences, which (a) under a collaborative ranking context uses a low-rank matrix to capture the underlying preferences and (b) under the bundled choice modeling the low rank structure captures how pairs of items are matched. In order to prove bounds, the authors use the random utility model.\n\n One contribution of this paper is that it provides a polynomial-time inference algorithm, though the Maximum Likelihood estimation for the general MNL model is intractable. Though there room for improvement, this paper could serve as the intermediate step to more general approaches which are more efficient.\n\nThe fact that the authors illuminate the weak spots of the model (e.g above equation 10) can actually help advance the research in this field.  This paper shows that the convex relaxation of nuclear norm minimization approach is minimax optimal. The authors give an upper bound on the resulting error with finite samples, and provide a matching lower bound.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
