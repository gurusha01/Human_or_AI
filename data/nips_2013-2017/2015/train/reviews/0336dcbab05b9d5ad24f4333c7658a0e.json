{"title": "Top-k Multiclass SVM", "abstract": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.", "id": "0336dcbab05b9d5ad24f4333c7658a0e", "authors": ["Maksim Lapin", "Matthias Hein", "Bernt Schiele"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper proposes a top-k hinge loss for top-k classification. It generalizes the multiclass SVM and uses the sum of k largest components as a convex approximation to the desired top-k loss. An efficient optimization algorithm is proposed and relationships with previous work are discussed. Paper is clearly written. Experiment results show the effectiveness of the proposed method compared to baselines.\n\n1. The authors did not report comparisons with [25, 8] due to availability of the implementations. However, I am really interested in seeing the comparisons.\n\n2. Can the authors make the code public upon acceptance?\n\n  A neat formulation of top-k multiclass SVM that works well.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In many multiclass classification problems with a large number of classes, semantic similarity between classes can typically be observed. This paper proposed an algorithm called top-k SVM to explicitly deal with the learning scenario where you only get a cost when the ground truth is outside the top k proposed classes (e.g. imagenet challenge). After extending the loss function to top-k loss, an efficient optimization procedure is formulated. Experiments on large scale datasets showed the scalability of the algorithm and clear improvement could be observed over compared algorithms.\n\nThe paper is clearly motivated and easy to follow. And the problem (similar classes in multiclass learning) they work on is an important one that happens a lot in many practical applications. This paper formulated the top-k SVM algorithm to handle the case in multiclass classification when some classes might be very similar to each other. The algorithm scales to very large dataset and showed improvements over compared methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The material in this paper is non-trivial and the paper is well-written. The experimental results are also good.\n\nThe only suggestion I have is to motivate the choice of the loss function further.\n\n Also, I spotted a typo in page 3 - \"globally optimal leads to\" (remove optimal).  This is a nice paper that describes an interesting problem. The idea to to learn with respect to the top-k loss, where k predictions are made and the loss is zero if the true class is included in the k predictions.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
