{"title": "Convolutional Neural Networks with Intra-Layer Recurrent Connections for Scene Labeling", "abstract": "Scene labeling is a challenging computer vision task. It requires the use of both local discriminative features and global context information. We adopt a deep recurrent convolutional neural network (RCNN) for this task, which is originally proposed for object recognition. Different from traditional convolutional neural networks (CNN), this model has intra-layer recurrent connections in the convolutional layers. Therefore each convolutional layer becomes a two-dimensional recurrent neural network. The units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods. While recurrent iterations proceed, the region of context captured by each unit expands. In this way, feature extraction and context modulation are seamlessly integrated, which is different from typical methods that entail separate modules for the two steps. To further utilize the context, a multi-scale RCNN is proposed. Over two benchmark datasets, Standford Background and Sift Flow, the model outperforms many state-of-the-art models in accuracy and efficiency.", "id": "9cf81d8026a9018052c429cc4e56739b", "authors": ["Ming Liang", "Xiaolin Hu", "Bo Zhang"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper presents a recurrent convolutional neural network for semantic image segmentation to encode and take advantage of contextual relationships. The method is basically a combination of [13] where a very similar RCNN is used for object recognition, and [4] from which the multi-scale pipeline is inspired. Thus, technically, the paper is not very novel (Sec. 3.1. is much the same as in previous works - maybe that should be stated more clearly; the combination can be seen as a novelty of course). However, the paper is well executed, very easy and clear to read, largely well written and provides a seemingly fair evaluation to state-of-the-art. Some recent works or interesting evaluations could be added, see below. Results are shown on the Sift Flow and the Stanford Background dataset where the proposed technique outperforms state-of-the-art using a limited number of training data for all approaches (limited by these datasets). Also, parameter ablation studies are conducted to some extend. The method is very efficient.\n\nDetailed comments:\n\n- The related work section could be written more sharply. It is not always crystal clear what the differences are with respect to the closest related works.\n\n- Results on PASCAL VOC could be added as most recent segmentation works evaluate on those datasets.\n\n- l.276 is very vague and should be made more clear.\n\n- Table 1: Why are only \\gamma \\in {0,1} considered? The paper could provide experiments/plots with varying gammas.\n\n- page 7: The ordering of text/plots/tables should be changed to have less interleaving text/plots/tables.\n\n- The model mentioned in l.387 should be added to the table, maybe with a footnote that it uses different training data or a line break.\n\n- Some further qualitative analysis would be nice if it fits.\n\n- This recent work seems to be missing (the first one provides even stronger results on Stanford Background):\n\n Feedforward semantic segmentation with zoom-out features Mohammadreza Mostajabi, Payman Yadollahpour and Gregory Shakhnarovich Toyota Technological Institute at Chicago\n\n@inproceedings{crfasrnn_arXiv2015,\n\nauthor = {Shuai Zheng and Sadeep Jayasumana and Bernardino Romera-Paredes and Vibhav Vineet and Zhizhong Su and Dalong Du and Chang Huang and Philip Torr},\n\ntitle = {Conditional Random Fields as Recurrent Neural Networks},\n\nbooktitle = {arXiv:1502.03240}, year = {2015}\n\n} The paper is well executed and provides a principled combination of two existing techniques. The results seem convincing, up to some additional studies which would benefit the paper (see below).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a pixelwise classification system using recurrent convolutional connections.\n\nThe method employs a multiscale convolutional network similar to Farabet et al., but introduces shared weights and additive skip-connections between layers in the networks applied at each scale.\n\nThe model is evaluated on Stanford Background and SIFT Flow datasets.\n\nThis approach appears to achieve good performance for being trained from scratch, but I think the several aspects could be better explored and evaluated.\n\n* First, the effect of sharing weights between recurrent connections could be compared to the same network with different convolution kernels in each iteration; while this introduces more parameters, it might also stand to increase performance for about the same computational cost (i.e., same number of connections).\n\nThis was almost performed, but the CNN2 model has fewer total layers and a smaller RF.\n\nIt is also possible the findings for varying gamma (enabling/disabling skip connections) might change under these conditions.\n\n* Second, the effects of N and T could be further expanded upon.\n\nFrom what I can tell, all the RCNN networks use T=3, except RCNN-large, which uses T=4.\n\nBut RCNN-large is evaluated only for N=3.\n\nHow does it perform at N=4 and N=5?\n\n* In addition, Table 2 RCNN has a reported performance of 83.5/35.8 PA/CA.\n\nBut in Table 1, RCNN-large has what looks like better performance at 83.4/38.9 PA/CA (last line).\n\nIs there a reason the latter wasn't used in Table 2?\n\n* The related work section states that [4] and similar networks rely on postprocessing, e.g. superpixels, to ensure consistency of neighboring pixels labels.\n\nThis seems to imply the proposed model does not suffer from this problem, but this is not evaluated in the experiments or with qualitative examples.\n\nNo example predictions are shown, so it isn't really clear what the outputs look like qualitatively.\n\n* There are also some other recent works in this area that I think could be discussed or compared with, particularly Chen et al. \"Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs\", and Eigen & Fergus \"Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture\" This approach appears to achieve good performance for being trained from scratch, however I think some aspects could be better explored and evaluated.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors claim that the effective receptive field of each unit expands with larger T (Line 174 in Page 4). I didn't get this point. As you input an still image to the network, how can the receptive field of each unit get expanded with larger T? Can you clarify on this? The paper proposes a recurrent convolutional layer for CNN to improve the performance of scene image parsing. The algorithm looks technically sound and the result looks good.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "[this is a light review] Specifically similar approache: - Shuai Zheng; Sadeep Jayasumana; Bernardino Romera-Paredes; Chang Huang; Philip Torr, http://arxiv.org/abs/1502.03240. - [15] should be included in the results table, maybe with a star to denote different/more training data.\n\n The paper adapts the recurrent neural network approach for object detection [13] to scene labeling/semantic segmentation. The approach outperforms baselines but not state-of-the-art. The authors should relate their work conceptually, quantitatively and w.r.t. the difference in approach more clearly from the semantic segmentation task & datasets and corresponding approaches (see e.g. http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6 for a list of winning approaches).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a method for scene labelling based on Recurrent Convolutional Neural Networks, where the output of a convolutional layer is used as an additional input of the same layer (this is implemented by duplicating the layer several times). The input to the network is the input image plus several downscaled versions of of the input image to exploit contextual information better.\n\nThe approach is tested evaluated on two datasets, and compared to previous methods. The accuracy improvement is good, and the computation time impressive as the algorithm can run entirely on the GPU.\n\nI found the paper interesting as it discusses very trendy issues but I have two main concerns: - the text is difficult to follow. For example, the use of the passive form in the first sentence of the last paragraph of the introduction makes it ambiguous. It took me some time to understand (or guess) that the authors meant \"we adopt a multi-scale version of RCNN\". - while the results are interesting, the contribution is limited, as it is only an application of RCNN (which were already applied to computer vision problems before) to image labeling.\n\nMore minor comments: - Section 3: state explicitly that RCL stands for recurrent convolutional layer. Same problem with LRN - just before Eq (3): it should be g(Z_ijkz), not sigma(z_ijk) - is it really worth discussing gamma?\n\nThe results with gamma = 0 are not as good as with gamma = 1, which is the \"standard\" way. This should not be surprising, because if a small gamma was interesting, the network could learn to use large values for w_k^rec  This paper presents a method for scene labelling based on Recurrent Convolutional Neural Networks. Results are interesting, but the text is difficult to follow and the contribution seems limited.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
