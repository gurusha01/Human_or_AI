{"title": "Variational Consensus Monte Carlo", "abstract": "Practitioners of Bayesian statistics have long depended on Markov chain Monte Carlo (MCMC) to obtain samples from intractable posterior distributions. Unfortunately, MCMC algorithms are typically serial, and do not scale to the large datasets typical of modern machine learning. The recently proposed consensus Monte Carlo algorithm removes this limitation by partitioning the data and drawing samples conditional on each partition in parallel (Scott et al, 2013). A fixed aggregation function then combines these samples, yielding approximate posterior samples. We introduce variational consensus Monte Carlo (VCMC), a variational Bayes algorithm that optimizes over aggregation functions to obtain samples from a distribution that better approximates the target. The resulting objective contains an intractable entropy term; we therefore derive a relaxation of the objective and show that the relaxed problem is blockwise concave under mild conditions. We illustrate the advantages of our algorithm on three inference tasks from the literature, demonstrating both the superior quality of the posterior approximation and the moderate overhead of the optimization step. Our algorithm achieves a relative error reduction (measured against serial MCMC) of up to 39% compared to consensus Monte Carlo on the task of estimating 300-dimensional probit regression parameter expectations; similarly, it achieves an error reduction of 92% on the task of estimating cluster comembership probabilities in a Gaussian mixture model with 8 components in 8 dimensions. Furthermore, these gains come at moderate cost compared to the runtime of serial MCMC, achieving near-ideal speedup in some instances.", "id": "e94550c93cd70fe748e6982b3439ad3b", "authors": ["Maxim Rabinovich", "Elaine Angelino", "Michael I. Jordan"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Consensus Monte Carlo (CMC) is a method for parallelizing MCMC for posterior inference over large datasets. It works by factorizing the posterior distribution into sub-posteriors each of which depend on only a subset of datapoints, sampling from each of these sub-posteriors in parallel, and then transforming samples from the sub-posteriors using an aggregation function to samples from the real posterior. Existing works use very naive methods of aggregation which result in high bias, or are computationally very expensive, which make it difficult to use Consensus Monte Carlo in practice. This paper proposes a more principled way of combining samples by optimizing over aggregation functions using variational inference.\n\nClarity: The paper is well written and easy to follow.\n\nSignificance: Bayesian inference for big datasets is a very important problem. This paper is significant because it presents a way of making parallel MCMC using CMC more practical.\n\nOriginality: The novelty in the paper is the use of variational inference to optimize over CMC aggregation functions instead of using a fixed one. This is rather straightforward, except that the entropy term is difficult to estimate and the authors propose minimizing a lower bound of it instead. Nevertheless, this is an important difference and allows the proposed method to achieve a large reduction in error in estimating posterior expectations compared to very simple baselines.\n\nQuality: The method itself seems sound, but I am not quite satisfied with the experiments. All experiments are on toy problems and datasets. Although it is nice to have these toy examples as they illustrate important aspects of the algorithm like the ability to aggregate structured samples, more realistic experiments are also necessary. Parallel MCMC is useful mainly for large datasets, but the authors compare on very small datasets, the largest of which only had 50K datapoints. I also wish the authors had experiments on more interesting models, e.g. LDA rather than on a Gaussian mixture model or normal-wishart, so that their method is of more direct interest to practitioners (plus there are large, real datasets for LDA) Also, the authors compare only against very simple baselines. How does this compare to Neiswanger et al or the Weierstrass sampler method of Wang and Dunson (for settings where these methods are also applicable)? How does this compare to serial mini-batch algorithms like Stochastic Gradient Langevin Dynamics (SGLD)? Although the proposed method could also use SGLD for sampling from sub-posteriors, does the additional time in aggregating samples and optimizing the aggregating function prevent it from having an advantage over vanilla SGLD? Finally, I really wish these methods were applied to a problem where there is a clear advantage of using Bayesian inference as compared to using a point estimate, e.g. where point estimates overfit, or the uncertainty obtained from Bayesian inference is actually used for something.\n\n This is a good paper and is well written, but the experimental section could be a lot better. I lean towards accepting it, but rejecting the paper to wait for more convincing and realistic experiments would not be a big loss.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I found the methodology and experiments presented in this paper fairly convincing and have only one some minor comments.\n\nIf (as is the case for many applications involving large datasets) there is a focus on predictive inference it may make more sense to aggregate functions of the parameters of relevance for prediction rather than the full set of parameters.\n\nFor example, in the probit regression one could aggregate fitted probabilities and in the mixture of Gaussians example one could do something similar for cluster membership probabilities.\n\nI wonder whether the results are more or less sensitive to the aggregation method in such a case.\n\nHave the authors done any experiments along these lines?\n\nAnother question that occurred to me was about the structured aggregation for the case of positive semidefinite matrices;\n\nthere are certainly different reparametrizations that could be used here (such as a Cholesky factorization) and I wondered why the aggregation is only being done on the D(\\Lambda_k) matrices.\n\nIs there any reason why this would be the best choice?\n\nMinor comment:\n\nthere is a missing integration in equation (3).\n\n This paper refines recently suggested consensus Monte Carlo algorithms suggested in the literature by using variational Bayes methods within the aggregation step of these algorithms.The relaxation of the variational objective function suggested is clever and I found the experiments convincing that the proposed approach produces some improvements over simpler methods.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Paper Title: Variational Consensus Monte Carlo\n\nPaper Summary: This paper presents a new method for aggregating samples in a low-communication MCMC setting. In this setting, a large dataset is partitioned over multiple cores, MCMC is performed on a subset of data on each core (technically, samples are drawn from a subposterior distribution on each core), and the resulting samples are aggregated with some combination function to produce samples from the full posterior distribution. Instead of developing a fixed combination function, as previous methods have done, this paper generalizes the Consensus Monte Carlo [Scott et al., 2013] combination function to define a family of combination functions, and performs variational inference over this family to find the function that yields samples from the best approximation (within the family) to the full posterior distribution. One advantage of this method is that it may allow for easier combination of more sophisticated mathematical objects (instead of simply vectors) due to the generalized form of the aggregation. Experiments are shown on a Bayesian probit regression model, a normal-inverse Wishart model, and a mixture of Gaussians.\n\n Comments:\n\n- I feel that the goal of developing scalable methods for Bayesian inference that maintain a good approximation to the posterior distribution is important, and that this paper takes a good step towards this end. I also feel that this the paper provides a clever way of viewing an existing (low-communication) parallel MCMC strategy in terms of optimizing over a variational objective (another, typically separate, strategy in approximate Bayesian inference).\n\n- The primary goal of these low-communication parallel MCMC methods is to reduce the time of inference (while maintaining a good approximation of the posterior distribution), particularly relative to communication-based parallel MCMC methods. This paper does a good job of presenting a method that provides a good approximation of the posterior distribution (compared with existing methods), though there are relatively few experiments verifying that this method maintains the increased speed benefits of low-communication MCMC (relative to existing low-communication parallel MCMC methods, and communication-based parallel MCMC methods). This paper does indeed show comparison against speedups attained by the CMC method in one model, though a more thorough investigation would be nicer.\n\n- I wonder how the presented VCMC method would compare (in terms of inference speed and error) against serial methods for variational inference. VCMC was presented as a \"variational Bayes algorithm\", though all comparisons were against MCMC methods, and not variational Bayes methods. At its heart, this method is optimizing an approximate posterior (formed via distributed sampling), so it seems natural to compare against variational inference methods. This is particularly important given the recent popularity of scalable variational inference methods (such as stochastic gradient variational inference). Furthermore, there have recently been a few presented methods for low-communication parallel variational inference, which would make good experimental comparisons---see the following two references: (1) Broderick, Tamara, et al. \"Streaming variational bayes.\" Advances in Neural Information Processing Systems. 2013. (2) Campbell, Trevor, and Jonathan P. How. \"Approximate Decentralized Bayesian Inference.\" UAI. 2014.\n\n- One downside of this method is that it has little ability to make any guarantees about the correctness of the final aggregated samples (which some of the current scalable MCMC methods attempt to do); it simply generates samples using the the best combination function chosen out of a pre-specified family of functions. On the other hand, I suppose this isn't much worse than most variational inference methods (which simply choose the best posterior approximation from a family of distributions). Furthermore, this method should, in general, produce better results than CMC method [Scott et al., 2013] (assuming the weighted-average combination function is in the pre-specified family of functions).\n\n- A few theoretical results were shown in this paper (blockwise concavity under certain conditions, and consequences of this). It would be nice to include more discussion regarding the purpose of these results, and why it is beneficial to prove them (perhaps also mentioning what types of concavity results are typically shown in variational inference literature); currently, they are added in the paper without much comment as to why. I feel that this paper makes good steps towards the goal of developing scalable approximate Bayesian inference methods (specifically, low-communication parallel methods) that maintain a good posterior approximation. Additionally, optimizing over the sample aggregation function is a clever idea, and seems to produce good results (relative to CMC). However, a more-thorough empirical exploration into inference times/speedups, and comparisons with other methods (particularly variational methods), would strengthen this paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
