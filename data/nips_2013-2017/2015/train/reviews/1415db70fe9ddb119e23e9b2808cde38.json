{"title": "High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality", "abstract": "We provide a general theory of the expectation-maximization (EM) algorithm for inferring high dimensional latent variable models.   In particular, we make two contributions: (i) For parameter estimation, we propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure into parameter estimation.  With an appropriate initialization, this algorithm converges at a geometric rate and attains an estimator with the (near-)optimal statistical rate of convergence.  (ii) Based on the obtained estimator, we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters.  For a broad family of statistical models,  our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions.", "id": "1415db70fe9ddb119e23e9b2808cde38", "authors": ["Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper studies EM in a high-dimensional setting. Convergence results are established and inference procedure developed.\n\n Authors should discuss under what conditions can we find the initial solution, \\beta^{\\rm init} that satisfies the requirement in line 60 on page 2.\n\nThere are two algorithms proposed. Is there a practical guidance on which one to use?\n\nHow does one choose $\\hat s$ and $\\lamdba$ in practice? How sensitive are results on the choice of these tuning parameters? Is there a way to choose them in a data dependent way? In the existing literature on high-dimensional inference, choice of the tuning parameters is of utmost importance to obtaining good sampling properties.\n\nThe decorrelated score test is the same as in [*]. This fact should be cited and not swept under the rug. Is there anything new that needs to be developed in order to apply the existing inference procedure in the current context?\n\nThe main theoretical results hold under a large number of technical conditions. The authors should discuss whether these condition/assumptions are satisfied for the considered latent variable models.\n\n[*] A General Theory of Hypothesis Tests and Confidence Regions for Sparse High Dimensional Models. Yang Ning, Han Liu The paper extends [2] to a high-dimensional setting. It also applies the work of Ning and Liu [*] to the problem of inference. The paper combines existing results in a clever way. However, it should be made more clear what is exactly novel in this paper and why is application of existing tools nontrivial. The submitted paper is a \"trailer\" for the long supplementary material. This trailer should contain more information on the technical challenges required to establish the main results.[*] A General Theory of Hypothesis Tests and Confidence Regions for Sparse High Dimensional Models. Yang Ning, Han Liu", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this paper, the authors propose a modification of the EM algorithm for the case that the true parameter is sparse and high-dimensional. The authors show some conditions for its convergence to the global solution and derive the rate of convergence. Also, a statistical test framework is constructed, which enables to check the sparsity of the estimator.\n\nThe paper is well organized. The related work is sufficiently summarized. However, the contents are very dense and some parts (mainly, section 3 and 4) are hard to follow. More simplification would be helpful for readers.\n\n- Although in line 066 the authors argue that sqrt(s*log(d/n)) is minimax-optimal, I couldn't find the proof of this from the main manuscript (I didn't read the supplementary material.) Can you prove this clearly?\n\n- In Eq.(3.8), ^s depends on s*, but we don't know s* in real situations. How should ^s be chosen in practice? Also, how can we find an appropriate initial value beta^init satisfying ||beta^init - beta*||_2 being less than R/2 (see line 275) in practice?\n\n- I didn't understand why the truncation step in Algorithm 1 is important. Can you explain what does happen if we skip the truncation step in your EM algorithm?\n\n****** After rebuttal ******\n\nThank you for your response. It was satisfactory for me and I increased my score from 6 to 7. A solid stats paper, but not easy to read for a MLresearcher. Also, the usefulness of the proposed method for realapplications is somewhat questionable.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "A sparse EM procedure is proposed which modifies classical EM by truncating the parameter vector at each step to a desired sparsity level. Also proposed is an asymptotically normal statistic based on a decorrelated score for testing whether a specific entry of the parameter vector is non-zero.\n\n The guarantee provided for the sparse EM states that the estimate converges geometrically to the true value (in the number of iterations), up to a statistical error term which is like the square root of the sparsity level and the infinity norm of the statistical error in the M-step, which is typically only logarithmic in the ambient dimension.\n\nThis follows on the heels of recent similar results for EM in the low-dimensional setting. As in the previous work, it is assumed that the Q function is sufficiently smooth and nearly quadratic in a certain basin of attraction, and that the algorithm is initialized in the same basin. The new condition is the infinity norm control on the error in estimating M. This condition is required only for sparse vectors, but for many models estimating M at all possible vectors is just as easy as at sparse vectors (this holds for the Gaussian mixture example considered, and for the non-gradient version of the mixture of regression model). This raises a question -- is the truncated EM algorithm necessary? Could a similar guarantee be obtained, for instance, by performing ordinary EM under the same conditions, followed by a single truncation step? EDIT: the authors' observation that linear regression is a special case here seems to be a convincing argument against a single truncation step.\n\nThere may be some related work in sparse optimization that perhaps should be discussed (like \"Sparse online learning via truncated gradient\" Langford et al 2009, just as an example).\n\nThe experimental section should state what initialization was used. A sparse EM variant is proposed for parameter estimation in high dimensional settings. The convergence guarantees provided are an important result for high dimensional learning, although the importance of the sparse variant of EM to the obtained results is not certain.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
