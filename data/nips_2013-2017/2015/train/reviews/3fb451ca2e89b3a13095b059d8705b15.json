{"title": "Robust Gaussian Graphical Modeling with the Trimmed Graphical Lasso", "abstract": "Gaussian Graphical Models (GGMs) are popular tools for studying network structures. However, many modern applications such as gene network discovery and social interactions analysis often involve high-dimensional noisy data with outliers or heavier tails than the Gaussian distribution. In this paper, we propose the Trimmed Graphical Lasso for robust estimation of sparse GGMs.  Our method guards against outliers by an implicit trimming mechanism akin to the popular Least Trimmed Squares method used for linear regression. We provide a rigorous statistical analysis of our estimator in the high-dimensional setting. In contrast, existing approaches for robust sparse GGMs estimation lack statistical guarantees. Our theoretical results are complemented by experiments on simulated and real gene expression data which further demonstrate the value of our approach.", "id": "3fb451ca2e89b3a13095b059d8705b15", "authors": ["Eunho Yang", "Aurelie C. Lozano"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper presents a method for robust inference of sparse GGM models. Theoretical guarantees on estimated models are given even when these estimates correspond to merely local optima of the relevant objective function.\n\nThe paper is clearly presented and organised logically: after introducing the problem to be solved the objective function (3) is presented. A reasonable approach to optimisation is presented - Algorithm 1. Since the objective is only bi-convex not convex no attempt is made to find a global minimum. Instead theoretical guarantees are given on *any* local minimum subject to certain conditions. Some work is done in Section 3 to provide the reader with some feeling for when these conditions will be met. Experiments with both synthetic and real data are provided.\n\n(3) is a reasonable way of dealing with outliers/noise and it is clearly good to have the theoretical guarantees. However, I found it hard to be convinced that much had been gained - for practical problems - over existing technqiues. (The authors deserve praise for comparing against a good selection of alternatives.) The results in Fig 1 show that the proposed method is indeed \"competitive\" but not superior by a large margin. On the other hand doing somewhat better than alternatives *and* having some theoretical guarantees is a contribution.\n\nOther points...\n\nPlease mention that \\tau_{1}, \\tau_{2} and \\tau are all defined in the supplementary material - I searched hard for their definitions in the main paper!\n\nI did not understand the sentence at line 236. What does it mean to choose a parameter \"from [a] quantity\"?\n\n51 paid on -> paid to 123 if i-th sample -> if the i-th sample 125 only small number -> only a small number 242 how easy we can satisfy -> how easily we can satisfy 277 Note that -> Note that the 278 Corollary 2 recover -> Corollary 2 reveals an 285 by arbitrary optimization -> by an arbitrary optimization\n\n286 multiple local optimum -> multiple local optima 291 from other -> from another\n\n365 Fix j subscript. Also is -> if 463 gaussian -> Gaussian  A method with useful theoretical guarantees and competitive empirical performance is presented.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a method similar to Least Trimmed Squares for robust linear regression, except for graphical lasso.\n\nThe main idea is to improve upon the original graphical lasso such that it is more robust to outliers.\n\n The most significant contribution of this paper is the theoretical analysis showing that despite being a non-convex method, where the objective is biconvex, consistency results are given for any local minimum.\n\nIn C-4, was tao1 and tao2 ever defined?\n\n The weakness of the this paper is in the experimental results.\n\nIt would be great if the sensitivity vs. 1-specificity figures were included in the supplemental material with the x and y axes both going from 0 to 1.\n\nAs it's currently plotted, it's difficult to assess how much better the trimmed graphical lasso is performing. From examining the plots as is, my guess is that it does not make that much difference.\n\nAlso, for the trimmed graphical lasso,\n\nthe 100h/n ratio's considered were 80,85, and 90, which means there are very few outliers.\n\nA stronger result would be if you could show that as the outliers increase, then trimmed graphical lasso's performance begins to differentiate itself from the other methods, especially the classical graphical lasso.  This paper presents Trimmed Graphical Lasso, which is a method that induces the trimming of particular samples that are less reliable, such that the method is more robust to outliers.The main contribution is providing statistical guarantees on the consistency of the estimator.The experimental results show that the method is competitive with existing methods, but do not demonstrate clear superiority.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper is well-written in general.\n\nThe proposed algorithm presents a slight improvement (more significant in some cases) over the classical glasso and robust techniques, and the theoretical results give consistency guarantees.  In this paper, the authors address the problem of precision matrix estimation using sparsity-promoting methods. Based on the classical glasso, a new algorithm is proposed by adding weights to the data points. This yields a non-convex optimization problem, and the authors propose two different algorithms for solving this problem.Of course, due to the non-convexity, the obtained solutions are local minima. However, a theoretical result is proven, showing that any local minima is guaranteed to have some consistency properties.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
