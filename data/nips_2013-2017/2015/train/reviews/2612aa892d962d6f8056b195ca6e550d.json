{"title": "Interactive Control of Diverse Complex Characters with Neural Networks", "abstract": "We present a method for training recurrent neural networks to act as near-optimal feedback controllers. It is able to generate stable and realistic behaviors for a range of dynamical systems and tasks -- swimming, flying, biped and quadruped walking with different body morphologies. It does not require motion capture or task-specific features or state machines. The controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state. The action generated by the network is defined as velocity. Thus the network is not learning a control policy, but rather the dynamics under an implicit policy. Essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions.", "id": "2612aa892d962d6f8056b195ca6e550d", "authors": ["Igor Mordatch", "Kendall Lowrey", "Galen Andrew", "Zoran Popovic", "Emanuel V. Todorov"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The author begins by proposing a strategy to embed feedback gains in neural network policies that makes them robust to disturbances and regression error. Trajectory optimization and policy optimization are softly coupled by a cost penalty for deviation, allowing the policy to learn reasonable approximations of optimal trajectories that are in fact simpler and more robust than the products of MPC-based trajectory optimization on their own. The tasks are optimized using sophisticated parallelization, though this is not the central purpose of the paper and more of a means to an end. Finally, a diverse set of robots are controlled using the trained feedback controllers.\n\nThis paper is of the highest quality, is incredibly clear, represents a tour de force of technique, and presents quite beautiful results in its applications. Bravo. This is a beautiful body of work that marries graphics, control, neural networks in the pursuit of generation of controllers for a diversity of body morphologies and tasks. It's inspiring.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "\"Interactive control of diverse complex characters with neural networks\" shows\n\n how a small, fast neural network can be used to represent a control policy for interactive control of a character in a physics simulator.\n\nThe approach uses trajectory optimization in a training phase to learn a neural network policy. The contribution is to show that the same architecture works across a range of more-or-less cyclic behaviours, and a range of body types.\n\n General Comments ----------------\n\n My reading of this paper is that it is an application of deep learning to existing (albeit recent) techniques in computer graphics.\n\n The ability of trajectory optimization to find realistic control signals has\n\nbeen established by previous work, some of which is cited here.\n\n The use of neural networks to approximate the step-by-step actions required of a policy in order to trace out trajectories similar to the ones found by trajectory optimization has also been developed in previous work (such as e.g. [9]).\n\n The authors of this paper must therefore work harder to position the work that went into this paper relative to these previously-published approaches.\n\nThe\n\nclosest work to my mind is the recent line of work by Pieter Abbeel and Sergey Levine. How would the authors compare the approach here to that one? Is it meant to learn faster? To support a wider variety of body shapes? Is it meant to require less computation at runtime? To be more robust to certain environmental variabilities?\n\n Specific Comments -----------------\n\n What is meant by \"we found the resulting process can produce very large and ill-conditioned feedback gains\"?\n\n Consider moving the discussion of why you did not use LQG until after you have presented your strategy.\n\n The notation around lines 226-240 is confusing. You introduce s(X) = \\bar s, and then define a cost in terms of s(X) - \\bar s. The bars and squiggles are visually similar.\n\n What is meant by \"our approach produces softer feedback gains according to parameter \\lambda\"? Softer than what? What does it mean for gains to be either soft or not soft? Why is it a good thing to change the initial state as part of your procedure? Sound work, but the contribution relative to other recent related work is not clear enough.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this submission, authors propose an algorithm for real-time control of 3D models, where a neural network (NN) is used to generate animations, after having been trained offline to reproduce the output of Contact-Invariant Optimization (CIO). Importantly, CIO and NN training are not independent: they are performed in an alternating fashion, the output of one being used in the criterion optimized by the other. Another core element of the method is the injection of noise, through data augmentation by small perturbations as well as additive noise in the NN's hidden layers. Experiments show that such a network is able to produce realistic and stable control policies on a range of very different character models.\n\nThis is a very interesting approach, considering that real-time control of arbitrary models is a challenging task. Although the proposed method may initially appear relatively straightforward (\"learn a NN to predict the output of CIO\"), the additional steps to make it work (joint training and noise injection) are significant new contributions, and shown in experiments to help get much better generalization.\n\nThose experimental results would be stronger, however, if comparisons were made with a larger dataset. Since the base training trajectories were generated with only \"between 100 to 200 trials, each with 5 branched segments\", it is not surprising for the \"no noise\" variant to overfit. The \"no joint\" variant might also benefit from more diverse training trajectories -- even if that is less obvious.\n\nOverall the comparative evaluation is the weak point of the paper (acknowledged in the introduction: \"A systematic comparison of these more direct methods with the present trajectory-optimization-based methods remains to be done\"). The only comparison to a competing method is a quick one with MPC at the end of the paper (and there is no reference for this method by the way).\n\nFor the most part, the maths are clearly explained and motivated, although someone willing to replicate these results will definitely need to read up additional references, since some points are only addressed superficially (especially the CIO step -- see Eq. 6 whose notations are not defined). Providing code would certainly be much appreciated by the community. There is one part I was not able to properly understand: section 3.2 about generating optimal actions for noisy inputs. Notations got really confusing for me at this point, and it was not clear to me what was being done (in particular the time index \"t\" was dropped, but is it still implied for some of the quantities?).\n\nIt would have also been useful to get some time measurements for computations during real-time control, since that could be a major bottleneck (and also knowing how these computations scale with the complexity of the 3D model). In an application like a video game, there is not much CPU available to animate a single character on screen.\n\nAdditional minor remarks: - \"biped location\": locomotion? - \"It is our aim to unity these disparate approaches\": unite? unify? - Am I right that the target is only about x,y,z coordinates, and there is no constraint on the angle the character is facing when reaching it? If yes, would it be easy to add an extra target angle? (which seems important in practice) - I would not call \"noise injection\" the step where additional data is generated from noisy inputs, because the target is re-computed to match the modified inputs. To me, this is simply \"data augmentation\", and it has nothing to do with the idea behind denoising autoencoders. - Are you really using the initial (random) network weights theta in the very first iteration, to compute the first X's in Alg. 1? Or are you starting with X's computed without the regularization term? - What is the justification for using the same hyperparameter eta in the two steps of Alg. 1? - \"it is non-trivial to adapt them to asynchronous and stochastic setting we have\": missing \"the\"? - Acronym LQG is used one line before it is defined. - \"While we can use Linear Quadratic Gaussian (LQG) pass\": missing \"a\"? - l. 231 the star in the argmin is in the wrong place, and is \\bar{s} missing in \\tilde{C}? - \"the minimizer of a quadratic around optimal trajectory\": missing a word? - \"sX and aX are subsets of \\phiX\": why is that guaranteed? - \"a single pass over the data to reduce the objective in (4)\": it would rather be the objective from Alg. 1 since only theta is optimized - The asynchronous joint optimization scheme described in Section 6 is very different from the alternating scheme from Alg. 1. It would be worth at least mentioning this earlier. - l. 315, f should be f^{des}? - \"In the current work, the dynamics constraints are enforced softly and thus may include some root forces in simulation.\": I did not understand this sentence - The \"no noise\" variant in experiments could be split between \"no data augmentation\" and \"no hidden noise injection\". - A question I would find interesting to investigate is whether alpha can be decreased all the way to zero during optimization (for CIO only). In other words, is it working mostly because of a \"curriculum learning\" effect (easier task initially), or is it always useful to keep the trajectories learned by CIO close to what the network can learn? - Caption of Table 1a says 4 layers but I believe this is 3 A great-looking application of neural networks to character control, whose practical benefits and applicability still need to be established.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes to combine trajectory optimization with policy learning to enable interactive control of complex characters. The main proposal is to jointly learn the trajectories and the policy, with the additional empirical insight that decoupling the two optimization problems into alternating subproblems i) enables reusing of existing, well documented methods and ii) works sufficiently well in practice. Although I could follow the paper well, I'm not immersed in this domain well enough to comment on the potential impact of this paper, which on the surface certaintly looks like a technical prowess albeit with no novel theoretical insights.\n\n The authors combine an impressive level of technical sophistication at all levels - from the concepts down to their implementation - with equally impressive results. I found the claimed advantages of i) using sensory noise during training and ii) learning trajectories jointly with the policy, to have been clearly demonstrated.\n\n I also found the paper was clearly written, with what I think is the appropriate level of detail given the complexity of the problem and its implementation. Each section treats a separate aspect of the problem and there are very useful cross-references between sections.\n\nIn think the motivations for the extra optimization setp in sec 7 could have been better explained. At this stage, given what had been announced in previous sections, I was somehow expecting the policy to run without the need of any extra optimization.\n\nIs that because there is always a residual trajectory cost (or policy regression error?) such that feeding the action given by the policy into the physical simulator would accumulate errors? I would imagine that the forward evolution of the dynamics takes into account physical constraints so that no physical implausibility can result anyway, but I might be missing something? I appreciate, though, that re-optimizing at every timestep lets you use larger timestep.\n\nl153: similar to recent approaches... can you perhaps give one or two refs? It's counterintuitive to me that keeping updates small in parameter space would be a sensible thing to do in general, though in the end I understood that it's useful in a scenario like yours where you're block-alternating between subproblems.\n\ntypo: l89: to unify  The paper's results look rather impressive, and the proposed optimization scheme + implementation sensible to me. However this research area is slightly off my radar so it's difficult for me to be confident about novelty and significance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "- It would be nice to mention from the beginning that you need full model knowledge (Eq. 6)\n\n- Sect. 4 and 6 are disproportionate.\n\n - Sect. 4 does not mention the recurrent units from the abstract.\n\n- l. 405: How is this a stochastic NN? The training data is stochastic but the network itself seems deterministic.\n\nRebuttal ======== Thank you for the additional information. LIGHT REVIEW The paper shows promising results but should be restructured.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
