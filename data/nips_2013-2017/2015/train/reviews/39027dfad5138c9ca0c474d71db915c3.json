{"title": "The Self-Normalized Estimator for Counterfactual Learning", "abstract": "This paper identifies a severe problem of the counterfactual risk estimator typically used in batch learning from logged bandit feedback (BLBF), and proposes the use of an alternative estimator that avoids this problem.In the BLBF setting, the learner does not receive full-information feedback like in supervised learning, but observes feedback only for the actions taken by a historical policy.This makes BLBF algorithms particularly attractive for training online systems (e.g., ad placement, web search, recommendation) using their historical logs.The Counterfactual Risk Minimization (CRM) principle offers a general recipe for designing BLBF algorithms. It requires a counterfactual risk estimator, and virtually all existing works on BLBF have focused on a particular unbiased estimator.We show that this conventional estimator suffers from apropensity overfitting problem when used for learning over complex hypothesis spaces.We propose to replace the risk estimator with a self-normalized estimator, showing that it neatly avoids this problem.This naturally gives rise to a new learning algorithm -- Normalized Policy Optimizer for Exponential Models (Norm-POEM) --for structured output prediction using linear rules.We evaluate the empirical effectiveness of Norm-POEM on severalmulti-label classification problems, finding that it consistently outperforms the conventional estimator.", "id": "39027dfad5138c9ca0c474d71db915c3", "authors": ["Adith Swaminathan", "Thorsten Joachims"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Since I am currently traveling, I only had very little time to finish this (additional) light review. It's unfortunate since I found this paper quite interesting to read. It might very well be that I have missed or misunderstood some points. If this is the case, I apologize.\n\nObserving data from a given hypothesis, we might be interested in evaluating a different hypothesis. If these hypotheses are non-deterministic (e.g. conditional distributions), we can estimate the loss under the hypothesis under investigation by reweighting the data points, see eq. between (1) and (2).\n\nThe authors seem to address a problem that is related but somewhat different (see below) from the problem of large variance of the weights. The authors call this problem \"propensity overfitting\". The examples indicate that this problem occurs if there is not enough exploration, i.e. for a given x, we see only very few or even not a single y that would correspond to the new hypothesis that is to be evaluated. Since a similar argument explains the large variance of the weights, it would be nice to explain the relation between these two problems in a bit more detail.\n\nI didn't have time to look into the details or check novelty of the proposed solution but its idea seems sensible.\n\n- l. 199: I don't get this, if n is small compared to k, we see only few data points with x_i = y_i, right? Then, \\hat R(h^\u001a) should be even smaller than -2? Am I missing sth.?\n\n- \"unbiased counterfactual risk estimator used in prior works on BLBF [4, 5, 1]\". I am not sure whether this is a fair statement. I don't know all three papers but I doubt that they are not using sth. like clipping which leads to biased estimation with reduced variance.\n\n - I appreciate that code is available.\n\n Summarizing, I could not check whether the paper is technically sound. But it definitely contains interesting ideas that I would like to think about. I therefore suggest acceptance.  -", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The commentary and notations concerning the main equation - (2) - seem wrong or at least misleading:\n\nan expectation is a quantity of the type \\sum f(x) p(x). The expected quantity, f(x), is everything else than p(x), the probability measure.\n\nIn equation (2), the loss is so obviously not the expected quantity! Therefore, the following comment (page 2, second paragraph) that (2) has an \"anomaly\" because it is not invariant to translation of the loss is technically incorrect. Again, the ensuing Example 1 is based on this wrong presumption - that, unsurprisingly, does not hold.\n\nIn practical terms, the estimator in (2) is weak to its denominator, p_i, which can be 0 if the sample has never been observed in the prior. The modified estimator in (7) seems better in principle because the effects of p_i are mollified through the double denominator.\n\n I really hope that the authors will be so gracious to modify their commentary should the paper be accepted.\n\nThe experimental results show that Norm-POEM outperforms POEM on all datasets and is not far from a CRF using full multi-label information. This paper presents an approach for \"counterfactual learning\", a learning scenario that is a specialisation of multi-label learning. In multi-label learning, the ground-truth annotation for a sample provides a 0/1 label for each of the classes; here, instead, only the label for one class (i.e., bandit feedback).However, the above is only a justification preamble. De facto, this paper has a precise aim: to improve an estimator (POEM) recently presented by reference [1] at ICML 2015. I would say that, overall, the paper is convincing and worth publication, but with some remarks that I raise in the Comments to authors field.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "### Summary\n\nThe paper studies the problem of batch learning from logged bandit feedback, a very relevant problem in for example ad ranking. The paper identifies an \"overfitting\" problem in the recently proposed CRM principle and proposes a solution based on multiplicative control variates (this results in a biased estimator).\n\n ### Quality\n\nHigh-quality and well motivated. Again, as it is follow-up work on [1] I would have expected additional experiments and some additional analysis on the optimization of the training objective (more details than in the paragraph on line 415).\n\n ### Clarity\n\nPresentation is clear and paper is well written.\n\n ### Originality\n\nFollow-up work on [1], but modifying the risk estimator to be self-normalizing, which seems to make a big difference in the experiments studied by the authors.\n\n ### Significance\n\nThe problem of learning from logged data is relevant, however it is difficult to say how the substantial experimental improvements translate to more real-world applications.\n\n ### Various Remarks\n\n365: if you already run several repetitions, please also include some measure of variance. The paper identifies a problem in the recently proposed Conterfactual Risk Minimization (CRM) principle and introduces a solution to the problem. Overall the paper is solid, well written and the work is relevant and novel. I would however have appreciated some extended experiments and especially additional applications then the ones in [1], including some actual real-world applications.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "- A question to clarify in the rebuttal: from Section 4.1 in [1], it is said that the CRM approach only makes sense for delta in [-1,0] (and thus they propose to rescale a general loss to [-1,0]). My guess is that it should be the same for NORM-POEM here -- i.e. (8) and (3) both uses the re-scale delta in [-1,0], correct? In this case, I do not understand what it means to use \"delta > 0\" in the experiment in Table 1. Please clarify.\n\n== Other comments ==\n\n- There is something I find missing in the setup for \"batch learning from logged bandit feedback\" of Section 3 (or from [1]): I feel the feedback should also be seen as a random variable, rather than a deterministic function as presented on line 110. Here is my rationale. Suppose that the partial feedback is coming from a classification problem where we just do not know the labels, we only get feedback by trying a prediction and seeing what is the loss. Suppose that there is a true classification loss Delta(y',y), and that the way the feedback is generated is, for a given input x, we make a prediction y using h_0; some god agent labels also the input to y'; and then tells us the loss Delta(y',y) that we incur (so here delta(x,y) is Delta(y',y) for the y' given to this example). On the other hand, often in classification, we suppose that the labeling can be noisy and so there is not necessarily a unique y' assigned to each x. This means that somewhere else in the log, we could have the same x as input, the same y that we played, but with a different delta(x,y) as the god agent just had given a different label for this one... This is why I think that in general delta(x,y) should be seen as a random number (in the classification example above the mean of this random variable for a fix x & played y should be the expectation of Delta(y',y) for y' distributed according to p(y'|x), the true noisy labeling distribution).\n\nThis perhaps does not change anything about the risk estimator (2); but at least the setup in (1) with the true risk should be presented with an additional expectation over the randomness of delta, to be more general.\n\n- Line 170-171: I think it should be h(y|x) / h_0(y|x). Also, it might be worthwhile to mention that the lack of linearity was already pointed out in [1].\n\n- Line 276: you should probably put argmin instead of argmax given that you are minimizing the risk...\n\n- Line 319: replace bars with hats in the notation to be consistent with the previous convention.\n\n === Update after rebuttal ==\n\nThanks for the clarifications. [light reviewer]quality:6(out of 10) clarity: 6 originality: 8 significance: 8I think this is an interesting follow-up work on [1]. I find their analysis of the \"propensity overfitting\" interesting; and like the idea of using a multiplicative control variate to reduce it. The results show a clear improvement over POEM, on a problem (batch learning from logged bandit feedback) for which there is more and more interest.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper propose a method for batch learning from logged bandit feedback that avoids the propensity overfitting problem.\n\nIt builds on Swaminathan and Joachims 2015 and provides significantly better algorithm.\n\nQuality: I found the quality of research discussed in the paper to be above average. The extensions proposed in the paper does result in significant improvements in training of the original POEM.\n\n Clarity: For the most part the paper is clearly written and easy to follow.\n\nOriginality: The paper gives significantly new formulation to the original POEM algorithm for BLBF problem. I believe the contributions significantly original.\n\nSignificance: I think the results in the paper are very interesting and certainly require further research. I am impressed that the proposed algorithm is faster than the original POEM despite having more regularization mechanism built into it. This algorithm may see widespread use in near future.\n\n This is a good paper. It is clear and reads well.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Counterfactual risk minimization is a technique aimed at learning in a bandit setting but from logged data by correcting for the bias of generating logged examples. While consistent, the paper shows that it suffers from an overfitting problem where a hypothesis class can overfit based on the generation probability of an example. The authors propose to fix this using a multiplicative parameter bounds the variance while maintaining consistency. The experimental results show that the multiplicative (or normalized) counter factual technique outperforms the non-normalized one.\n\n The paper presents a technique for learning self-normalized estimator for counter-factual learning in the logged bandit setting. It presents ideas for dealing with a form of overfitting where the hypothesis can overfit by having stronger support for the logged data. This is a good paper and should be accepted although I am not absolutely certain.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
