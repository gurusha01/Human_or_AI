{"title": "On-the-Job Learning with Bayesian Decision Theory", "abstract": "Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an \u201con-the-job\u201d setting, where as inputs arrive, we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets-- named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F1 improvement over having a single human label the whole set, and a 28% F1 improvement over online learning.", "id": "333222170ab9edca4785c39f55221fe7", "authors": ["Keenon Werling", "Arun Tejasvi Chaganty", "Percy S. Liang", "Christopher D. Manning"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The main contribution of this paper is to provide a more theoretically sound formulation of an on the job learner proposed in [22]. The learner starts by queuing the crowd to produce its output, but gradually starts making its own predictions and querying the crowd infrequently. The choice of whether to query the crowd for any given input is modeled as a decision problem solved via a Monte Carlo Tree Search.\n\n The presentation is mostly good, although lacking in important details. The empirical results vis a vis a threshold-based heuristic are somewhat underwhelming. But it certainly seems like a reasonable approach and merits further investigation.\n\n Please address/clarify the following:\n\n1. Equation (2) isn't making much sense. The LHS should also be conditioned on s. And how does the y in p_{R}(r_i | y, q_i) vanish?\n\n2. What is N(s) in Algorithm 1?\n\n3. How did you get the 0.3 and 0.88 in the threshold baseline?\n\n 4. I am not familiar with the exact tasks used here, but are the \"online\" systems the best machine learned systems on this task? If not why not? If so, then any gain on top of it is purely as a result of human annotation. What are some such cases? Some error analysis would be handy.  This paper presents a theoretical formulation of an \"on the job learner\" that starts as a pure crowdsourcing system and gradually transitions to a learned model that only queries the crowd on low confidence inputs. The empirical results are somewhat of a mixed bag, but it will be good to get the discussion going along these lines.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I like that this paper compares classifier accuracy to human performance. It is interesting in Table 2 to see that the LENSE system outperforms the 3-vote baseline, but underperforms the 5-vote. This indicates a significant comparison about the relative efficiencies of de-noising crowd responses via additional consensus vs. the LENSE system approach. The example given in the paragraph starting at Line 154 is rather abstract and difficult to follow. For easy readability, it might be useful to use a specific query example with crowd responses and times similar to what they would be in a real case. In Tables 2 and 3, it is confusing that the threshold baseline is sometimes referred to as entropic and sometimes as threshold. I do not understand why the related work is in Section 5. Overall, a compelling problem but the explanations were somewhat opaque.\n\n Line 086: Figure ?? This paper tackles the problem of creating high-accuracy classifiers beginning with zero labeled examples. A component problem in this paper is handling noisy crowd respondents, and while this topic has be considered from many angles in the literature, the timing and latency optimization factors of this paper are novel and interesting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a hybrid approach for using both\n\n crowdsourced labels and an incrementally (online) trained model\n\n to address prediction problems; the core idea is to lean heavily\n\n on the crowd as the system is ramping up, learn from the labels\n\n thus acquired, and then use the crowd less and less often as the\n\n model becomes more confident. This is done via a sophisticated\n\n framing of the problem as a stochastic game based on a CRF\n\n prediction model in which the system and the crowd are both\n\n players. The system can issue one or more queries q for tokens x\n\n (with true label y) which elicit responses r, where there is a\n\n utility U(q,r) for each outcome; the system thus attempts to\n\n pick the actions that will maximize the expected\n\n utility. Furthermore, the queries are not issued all at once,\n\n but at times s (with response times t); utility is maximized\n\n with respect to a t_deadline by which an answer needs to be\n\n computed (this thus determines how many queries are sent out, at\n\n what rate, etc.)\n\nComputing this expected utility requires using\n\n the simulation dynamics model P(y,r,t|x,q,s) in order to compute\n\n the utilities as in (4). Given the utility values, the optimal\n\n action could be chosen; however, the introduction of continuous\n\n time makes this intractable to optimize and as such an\n\n approximation is used based on Monte Carlo Tree Search and TD\n\n learning (Algorithm 1). Experiments were conducted on a\n\n simplified named entity recognition (NER) task, a sentiment\n\n recognition task, and a face identification task, using four\n\n methods: the majority vote of n human judges (1,3,5), online\n\n learning (using the true labels), the \"threshold\" baseline (the\n\n authors' model but without continuous time, in which m queries\n\n are sent out at each step until the model prediction's\n\n uncertainty is reduced below a threshold), and finally the\n\n authors' full model (LENSE). In terms of precision/recall/F1,\n\n the full model outperforms all but the 5-vote crowdsourced\n\n version, though the \"threshold\" baseline does nearly as well\n\n (and with lower latency for NER; Table 2). The authors also show\n\n how the model requires fewer and fewer crowdsourced labels over\n\n time given a sufficiently strong model to train (see Figure 3),\n\n and that compared to online learning the accuracy is high from\n\n the first example (Figure 4), since the system can leverage\n\n crowd expertise more heavily while the model uncertainty is\n\n still high.\n\n This was a really interesting paper, and one that I expect could\n\n generate both a lot of discussion and further work as well as\n\n adoption in practice. There have been a variety of heuristic\n\n approaches to learning from the crowd while training a system,\n\n but this is the first complete and principled proposal I have\n\n seen. The results against reasonable baselines are\n\n impressive. As such, I feel the work is highly original and\n\n significant, and in my mind deserves to be included in the\n\n conference. That said, it does not fare nearly as well on the\n\n clarity front, which unfortunately could detract from the\n\n paper's potential impact. Many of these issues are correctible\n\n (even for the camera-ready) if the authors are willing to put in\n\n the time and effort and perhaps somewhat adjust the tone with\n\n respect to the \"threshold\" baseline vs. the full continuous time\n\n version.\n\n The primary issue with clarity comes from the introduction of\n\n continuous time, beginning partway through Section 3. The\n\n motivation is reasonable, i.e., it might be necessary to operate\n\n on a time (as well as a cost) budget for each classification,\n\n but experimentally it seems to have very little benefit, while\n\n the cost is that the formulation and discussion (as well as the\n\n optimization procedure) become substantially more complex.\n\nIn\n\n fact, in Table 2, it appears that the \"threshold\" baseline\n\n achieves significantly lower latency on the NER task while still\n\n getting roughly equivalent performance (marginally worse); on\n\n the face ID task it actually performs better (though still\n\n roughly equivalent). The authors argue (l. 314) that the\n\n baseline does better on the second task because it's a single\n\n label prediction task and doesn't have significant interaction\n\n between labels - however, the NER task, which has such\n\n interaction in spades, only seems to benefit marginally from\n\n this.\n\nFor many practical tasks, the \"threshold\" baseline will\n\n be good enough, and is already such a signficant gain (in terms\n\n of being a principled hybrid learning system that outperforms\n\n active and online learning).\n\nThe paper would likely have\n\n greater impact if the authors made this clear in the\n\n development, i.e., the core method could be developed fully\n\n (without the introduction of time, and as such would be easier\n\n to understand and implement), and the addition of continuous\n\n time could be shown as an extension.\n\nIn the same vein, when\n\n discussing the results, the authors could be more clear that the\n\n performance of the \"threshold\" method is quite strong, even in\n\n cases where there is a sequence dependence between tasks. This\n\n is a suggestion and not strictly necessarily, as I feel the\n\n paper is strong enough for inclusion as it is, but I do feel it\n\n would improve the paper and the chances for the technique being\n\n widely understood and adopted.\n\n There are a number of smaller issues with respect to the results\n\n and how they are reported. First, the introduction states that\n\n the model \"beats human performance\" - but this is not true in\n\n the 5-vote case for NER; strangely the 5-vote case is missing\n\n for the Face-ID and Sentiment tasks in multiple tables and\n\n figures (the right half of Tables 2, Table 3 and Figure 4) - it\n\n really should be included. Likewise, in Table 2, the results for\n\n the 5-vote case should be bolded as they represent the best\n\n performance. The latency value for Threshold in NER should also\n\n be bolded for the same reason (and the LENSE value for Face\n\n ID). More importantly, the \"Threshold\" baseline is missing for\n\n the sentiment experiment, and as such doesn't appear in Table 3\n\n or Figure 4 - again, it should really be included as well.\n\n A few minor comments with respect to the method description:\n\n -While there is a discussion of t_deadline in l.183-187,\n\n it is not clear what if any t_deadline was used in the\n\n experiments, and what criteria was used to determine when the\n\n answer was ready to report an answer - was it an entropic\n\n threshold as with the baseline or was it reaching\n\nsome t_deadline?\n\n -In equation (2), it seems the distribution needs to be given\n\n the variable s as well (i.e., p(y,r,t|x,q,s).\n\n -In equation 5, the use of q in F(q) is *very* confusing - I\n\n assumed this referred to a query q, but in fact it just\n\n represents a nominal distribution function - please choose some other\n\n letter, as there are plenty to choose from.\n\n -In the description of the Threshold baseline (l.246-252), I\n\n assume the the expression should be (1 - p_theta(y_i|x))*0.3^m\n\n <= (1-0.88) (note the RHS) as opposed to >= 0.88, as increasing\n\n m will reduce uncertainty, not increase it.\n\n  This paper presents a hybrid approach for using bothcrowdsourced labels and an incrementally (online) trained modelto address prediction problems, elegantly cast as a stochasticgame that models many aspects of the data, true labels, andcrowdworker responses, including the time at which queries aresent out and answers received. Impressive results are shown withrespect to multiple human labelers, online learning with oraclelabels, and a \"threshold\" baseline using the authors' model butremoving the dependence on continuous time that performs onpar. The method is original, novel, and interesting, and as suchthe paper should be accepted, but there are some issues withclarity, particularly with respect to the introduction ofcontinuous time, which greatly complicates the discussion andalgorithmic mechanics yet seems to yield minimal benefits inperformance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors describe a scenario called \"on the job learning\", where requests are made to annotators in between receiving an example and outputting a prediction.\n\n Whilst it is clear how the algorithm works for a structured prediction task,\n\nwhere elements of the sequence are queried, it's not clear at all how this was applied to the face recognition task - was it simply the instance being queried? In the non-structured setting the model wouldn't propagate information between adjacent positions (examples), so it seems like the utility function wouldn't make sense.\n\nThe threshold baseline has two parameters that are seemingly arbitrary. Where did these values come from and what is the sensitivity to these?\n\n A further baseline of uncertainty sampling, akin to that used in active learning, would be interesting.\n\n Finally, the authors are keen to make the distinction between this setting and active online learning, but active learning could be adapted quite simply to work in this domain, where the active set under consideration is now a sequence. For example, there's no reason why the model couldn't be updated in the same fashion on receiving responses, and then output a prediction at the end. Whilst this clearly wouldn't capture the temporal nature of the task, there is much theory to draw upon for choosing the query set. A seemingly novel framework, although some details are not clear, and it seems to be fairly restricted in applicability.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
