{"title": "Finite-Time Analysis of Projected Langevin Monte Carlo", "abstract": "We analyze the projected Langevin Monte Carlo (LMC) algorithm, a close cousin of projected Stochastic Gradient Descent (SGD). We show that LMC allows to sample in polynomial time from a posterior distribution restricted to a convex body and with concave log-likelihood. This gives the first Markov chain to sample from a log-concave distribution with a first-order oracle, as the existing chains with provable guarantees (lattice walk, ball walk and hit-and-run) require a zeroth-order oracle. Our proof uses elementary concepts from stochastic calculus which could be useful more generally to understand SGD and its variants.", "id": "c0f168ce8900fa56e57789e2a2f2c9d0", "authors": ["Sebastien Bubeck", "Ronen Eldan", "Joseph Lehec"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The analysis by the authors is highly technical and impressive.\n\n The issues with the paper are: 1. There are many areas where facts are stated and a citation is expected, but none is provided. For example, \"Indeed several Markov Chain Monte Carlo methods have been analyzed for the case where the posterior distribution is supported on a convex set, and the negative log-likelihood is convex.\" As well as, \"... convexity is a key property to obtain algorithms with provable guarantees for this task.\"\n\n2. It would be interesting if the results section compared to more standard MCMC algorithms on interesting inference problems rather than just \"volume estimation\" benchmarks. The performance gains should also be specified quantitatively rather than just graphically.\n\n3. There is no conclusions section (even a few sentences). That is not a very polished way to end a paper.\n\n4. There are no axis labels on the plots. Error bars would be nice too.\n\nOther comments: 1. It would be nice if the authors could elaborate on what exactly they mean with \"Unfortunately the techniques used in Dalalyan [2014] cannot deal with the singularities in the diffusion process ...\". What singularities?\n\n2. It appears there might be a typo on line 307. Is the C supposed to appear in the inequality? Would that make Lemma 3 contain C^2? The authors do a heavy theoretical analysis to provide guarantees on the accuracy of Langevin Monte Carlo methods. Their main result shows a big-O like result relating the number of samples required, the dimension of the space, and the desired accuracy (as measured in total variation distance between the target distribution and the one actually sampled from).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents a finite-time analysis of new MC algorithm called Projected LMC.\n\nThe optimization counterpart of Projected LMC is stochastic gradient descent (SGD). The main theoretical result states that for an appropriately chosen step-size and after a large number of iterations, the distribution of the samples from Projected LMC is arbitrarily close to the target distribution log-concave density.\n\nThe main contribution here is to show that the step-size of Projected LMC is similar to the step-size in SGD and that the maximum number of iteration of Projected-LMC depend polynomially on the dimension of the space (up to a logarithmic factor).\n\nFurther, the proposed algorithm does not involve evaluation of the density, but its gradient.\n\nThis is extremely useful in cases where the normalization constant is not tractable.\n\nThe paper is clear and well-written. It introduces a new theoretical analysis of Projected LMC algorithm for sampling from distributions with log-concave densities that involves evaluation of the first order oracle.  This work introduces a finite-time analysis of the projected Langevin Monte Carlo (LMC) algorithm.The approach first theoretical analysis of a MC algorithm to sample from a log-concave distribution that only requires values of the gradient of log-density.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors consider the projected Langevin Monte-Carlo (PLMC) algorithm for sampling from a log-concave density with a compact support. They establish a polynomial-time bound for sampling with a prescribed accuracy. The result is new, nontrivial and, in my opinion provides a very interesting contribution to the theory of computational statistics/machine learning.\n\n I went through the proof (without reading the parts which are in the supplementary material) and checked all the details. I have found only a couple of small typos (cf. below). Not only the proof is correct, but also it contains a very elegant argument that makes use of the Skorohod embedding. I am sure any mathematically driven person will appreciate this argument.\n\nHere are some minor remarks: - I recommend to add somewhere in the discussion that the PLMC needs not only a first-order oracle but also a projector to the compact set under consideration.\n\n- Concerning the discussion on H&R in Section 1.2: the authors seem to claim that H&R requires a zeroth-order oracle, but my understanding is that H&M requires to sample from the restriction of the distribution on the segments. Could the authors comment on the relationship between these two tasks? - Beginning of section 2.1: why do you need to introduce the pair (x,phi)? It is my impression that it suffices to introduce x. Then you can remove the equation of line 184 and replace phi by x-w in the equation on line 187.\n\n- line 210: \"Dirac\" instead of \"dirac\" -line 225: \"paths\" instead of \"path\" - line 244 and Eq (3): in these two formulae there is a \\bar W_T that should be replaced by \\bar W_t (small t instead of capital T). - line 298: please provide a precise reference for the version of the Khintchine inequality you use.\n\n- line 321: could you provide a reference where it is clearly stated that the uniform distribution on a compact set is a stationary distribution for the reflected BM ?\n\nN.B. In the confidence evaluation above I have put 4 (and not 5) just because I cannot be absolutely certain of my evaluation of the paper without carefully reading all the details of the proofs which are put in the supplementary material.\n\n  This papers contains a very nice theoretical result about the computational complexity of sampling from a compactly supported distribution with a log-concave density when a first-order oracle is available. This contribution is original and concerns an important topic which deserves to receive more attention.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
