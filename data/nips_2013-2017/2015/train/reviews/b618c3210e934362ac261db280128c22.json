{"title": "A Recurrent Latent Variable Model for Sequential Data", "abstract": "In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN) can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.", "id": "b618c3210e934362ac261db280128c22", "authors": ["Junyoung Chung", "Kyle Kastner", "Laurent Dinh", "Kratarth Goel", "Aaron C. Courville", "Yoshua Bengio"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper investigates the application of the variational autoencoder to a recurrent model. The work is well motivated in the introduction, then the paper becomes less and less clear (for instance equations 1 and 2 have a problem). The real contribution starts in the middle of page 4.\n\nNote that the first two sentences of the paragraph \"Generation\" in this page is the true summary of the paper. These two sentences could appear earlier in the paper, even in a different version.\n\n However, the main issue comes with the experimental part and table 1. The evaluation criterion is the average log-probability. If this really the case, all results in table 1 should be negative, not only the 3 first scores of the first row. Otherwise, maybe it's the minus Log probability, but in this case the lower is the better. At the end, I cannot understand the numbers in this table. Note that these are the only results.\n\n Comments (reading order):\n\nIt is true that summarizing the paper of Kingma and Welling, 2014 in less than one page is difficult. However, the result is not always easy to read. It could be maybe easier to directly describe the VAE in the recurrent context.\n\n Page 2: \"extend the variational autoencoder (VAE) into *an* recurrent\"\n\n\" offers a interesting combination highly \" -> \" offers *an* interesting combination *of* highly ...\" Note that this sentence is very long with too much \"and\".\n\n There is maybe an issue with with these two equations: xt depends on ht in (1) and then in (2) ht depends on xt.\n\nThere must be a \"t-1\" somewhere to avoid this cycle. Is that correct ?\n\n Page 6: \"We use truncated backpropagation through time and initialize hidden state with the final hidden state of previous mini batch,\n\nresetting to a zero-vector every four updates.\" This part sounds tricky. Could you comment if this necessary because for the conventional model or for the one with latente variables, or both ?  The work is well motivated in the introduction. The idea to apply the variational autoencoder at each time step of a recurrent network is really nice. However, there is a problem with the experimental results that I cannot understand.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper empirically shows how adding latent variables to an RNN improves its generative power by showing nice samples and better likelihoods (or lower bound on likelihoods) than previous attempts at generating distributions over continuous variables.\n\nThe paper is well written and clear - except for the figure, which is quite confusing and confused me for a while. As the authors admit, their model is a slight modification to e.g. [1]. The authors did, as far as one can tell, a fair comparison with the model presented in [1], and showed how adding more structure to the prior over latent variables z_t (by means of making the mean / variances of those a function of the previous hidden state) helped generation.\n\nMy main concern about the paper is that is rather incremental. Given that the main novelty involves a better prior over z_t, I would have liked to see more analysis on why that helped (other than through empirical evaluation). Also, unlike previous work, the authors seem to use the same hidden state h_t at generation and for inference. What is the motivation behind this?\n\nRegarding the experiments with speech, could the authors clarify how exactly the windowing was made for the 200 waveform samples (was it overlapping from sample to sample?).\n\nThe authors could reference DRAW as another application / use case of a VRNN-like architecture. Despite the fact that adding latent variables to RNNs has already been explored in previous (recent) work, this paper is well written and presents a slight modification that outperforms all previous attempts at integrating the variational AE framework with RNNs.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper introduces a generative model for sequences. The novelty consists in considering explicit latent stochastic variables that are used to condition the data production. These latent variables may depend on the past.\n\n Pro: provides an extension of recent latent variable models to sequences.\n\nThey show that the model is competitive with other sequence models on a series of datasets.\n\nCons: some claims are not fully supported. The experiments are only proof of concepts. They just used raw signal and no pre-preprocessing\n\nat all, which makes it difficult to compare with state of the art performance on these classical datasets.  aa", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a variational inference approach to learn Recurrent Neural Networks (RNNs) that have stochastic transitions between hidden states. The model is a stochastic nonlinear dynamical system similar to a nonlinear state-space model where the noise between hidden states (i.e. the process noise) is not identically distributed but depends on the hidden state. The parameters of the generative model are learnt simultaneously with a recognition model via maximization of the evidence lower bound.\n\nThis is a solid paper which bridges the gap between RNNs and nonlinear state-space models. The presentation is clear with an emphasis on presenting the general ideas rather than the technical details. The model itself is simple to describe and the variational training procedure is now standard. There is probably an important engineering effort to achieve the reported results which is not obvious when reading the paper. Also there is no mention of computing machinery or training times.\n\nThe notation with subindices \"< t\" sometimes hides the Markovian structure of the model which can be useful when interpreting the factorizations of joint distributions.\n\nThe literature review does not seem to mention nonlinear state-space models (i.e. nonlinear versions of linear state-space models aka \"Kalman filters\"). Those models are popular in engineering, robotics, neuroscience and many other fields. The model presented in this paper could have an impact on fields beyond machine learning. It could be useful to be more explicit in the connection with nonlinear state-space models.\n\n A few questions:\n\nIf VRNN-Gauss was not able to generate well-formed handwritting, are all the plots labelled VRNN actually VRNN-GMM?\n\nIs there any regularization applied to the model? If not, how was overfitting prevented?\n\nEquation (1) gives h_t as a function of x_t while Equation (2) gives a distribution over x_t as a function of h_t. Is this intended? This is a solid paper which bridges the gap between RNNs and nonlinear state-space models. The experiments are convincing and the presentation is clear and well polished.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
