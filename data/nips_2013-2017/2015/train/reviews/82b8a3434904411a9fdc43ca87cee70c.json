{"title": "Regularization Path of Cross-Validation Error Lower Bounds", "abstract": "Careful tuning of a regularization parameter is indispensable in many machine learning tasks because it has a significant impact on generalization performances.Nevertheless, current practice of regularization parameter tuning is more of an art than a science, e.g., it is hard to tell how many grid-points would be needed in cross-validation (CV) for obtaining a solution with sufficiently small CV error.In this paper we propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter, which we call regularization path of CV error lower bounds.The proposed framework can be used for providing a theoretical approximation guarantee on a set of solutions in the sense that how far the CV error of the current best solution could be away from best possible CV error in the entire range of the regularization parameters.We demonstrate through numerical experiments that a theoretically guaranteed a choice of regularization parameter in the above sense is possible with reasonable computational costs.", "id": "82b8a3434904411a9fdc43ca87cee70c", "authors": ["Atsushi Shibagaki", "Yoshiki Suzuki", "Masayuki Karasuyama", "Ichiro Takeuchi"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "For the important goal of test error guarantees, the combination of ideas from screening together with approximate regularization paths as proposed here is completely novel, and nicely surprising. While the result here is only for binary linear classification, it might be a useful first step towards more general ML models. The paper is clearly written, and the experimental results are encouraging.\n\nFor the reader it might be nice to more clearly distinguish explicit definitions of eps-approximate solutions in terms of training error (1) and validation error (2), to avoid confusion. (and also 'eps-approximate regularization parameter'. maybe better call it 'eps-approximate best regularization parameter'?)\n\nThe authors should discuss the theoretical complexity of the proposed approach as a function of eps more in detail. More concretely in this sense, assume the C values would be chosen naively and equally-spaced, what do your results say about the interval size? This would also be a good contact point to discuss the connection to the path methods for training error (vs text as done here) more in detail.\n\nminor: l061: the -> this l264 soluitons\n\n(this is a light review) The paper contributes a novel approach to compute guarantees on the cross-validation error along the entire regularization path. This is a highly relevant first step towards much stronger methods for hyperparameter search, such that those methods could in the future come with guarantees for test error.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a novel framework for computing a lower bound of the CV errors as a function of the regularization parameter. The main motivating question of the manuscript (\"it is hard to tell how many grid-points would be needed in cross-validation\") has been already extensively studied both from theoretical and empirical perspective. For example, \u001asee (Rifkin et al, MIT-CSAIL-TR-2007-025) or (Rasmussen and Williams, Gaussian Processes for Machine Learning, Chapter 5.)\n\n The proposed score and validation error bounds are technically sound. These results are direct application of the ideas from safe screening (bounding Lagrangian multipliers at the optimal solution). While this was clearly stated in the supplemental material a similar remark in the main body (e.g. section 3) of the manuscript could be helpful for the reader.\n\n The experimental setup to evaluate a theoretical approximation of the regularization parameter is not clearly presented. How exactly the grid search is performed: 10 fold cross-validation on the training set to obtain optimal parameters with final evaluation on the validation set? In that case, I assume that E_{v} on the training set should have been reported in Fig(3) instead of validation.\n\n The proposed score and validation error bounds are technically sound. Novelty is incremental and experimental setup/result is dissapointing.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Interesting paper, with a potentially useful result.\n\nIt is not clear that the bound would remain valid if the method were kernelized and the kernel parameters also tuned using cross-validation (as the kernel and regularization parameters tend not to be independent).\n\nThe experimental evaluation is a little weak as there are only a few of small datasets.\n\nThe size of the datasets is especially an issue as the paper discusses the computational practicality of the bound.\n\nIt is not clear that the bound enables better models to be selected, than would be selected simply by optimizing the cross-validation error directly (within some computational budget).\n\nThe method may be useful in avoiding over-tuning the regularization parameter (i.e. over-fitting the cross-validation estimate).  The paper presents a bound on the cross-validation error for linear models and a procedure for finding a value with guaranteed CV error.The paper is interesting, and well written, but the experimental evaluation is a little weak.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "A common task in machine learning is to learn the regularization parameter of a model. Usually this is done using cross-validation. The main result of the paper is that is presents a new way to analytically compute bounds on validation error. First, a lower and an upper bound for wx_i is computed for each validation instance x_i (w is the parameter of the model) as a function of the regularization parameter, given a solution for a different regularization parameter. Then, the scores for validation instances are combined to yield the bounds on the validation error. A central result in the paper is Lemma 1, that shows how compute the bounds for wx_i. The proof is not given in the paper, but the reader is referred to the Appendix. Giving intuition of this core result in the main text would be very important for the reader to be able to understand the idea without going to the Appendix, even if it's entirely reasonably to omit the full proof from the main text. The proof (in the appendix) seems to rely heavily on the convexity of the regularized loss function, limiting the usability (although being more general than presented before). Also, L2 prior on the parameter is used. Furthermore, the paper only deals with tuning a univariate hyper-parameter. The results appear correct, though I did not check the proofs in detail. The text is clearly written (though see the remark above). The experiments show that the regularization parameter can be optimized efficiently by assuming a reasonably tight error bound and, importantly, that the error bound is available in the first place. The topic seems relevant for the NIPS community, and the results interesting, though potentially too specific to raise wide interest.  The paper presents analytical bounds for cross-validation error and demonstrates how these can be used to optimize the parameters efficiently.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
