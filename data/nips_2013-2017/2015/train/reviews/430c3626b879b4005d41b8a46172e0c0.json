{"title": "Equilibrated adaptive learning rates for non-convex optimization", "abstract": "Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of thecritical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments demonstrate that both schemes yield very similar step directions but that ESGD sometimes surpasses RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.", "id": "430c3626b879b4005d41b8a46172e0c0", "authors": ["Yann Dauphin", "Harm de Vries", "Yoshua Bengio"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Paper 921 introduces a new variant of the adaptive SGD method for non-convex optimization called \"equilibrated gradient descent\". The paper states that the equilibration preconditioner is better than Jacobi preconditioner theoretically and empirically.\n\n For the experiments, MNIST and CURVES are two easy tasks and it cannot say too much insight about the proposed algorithm. I think more experiments should be implemented. When we use neural network, what we really cares are the test accuracies rather than \"training error (MSE)\". On the other hand, I think it is not enough to compare the ESGD with SGD when the x-axis is \"epoch\".\n\nThere should be another figure whose x-axis is the training time measured by \"minute\" or \"hour\".\n\n The paper is not easy to understand clearly. (1) In Algorithm 1, should $H$ be re-calculated every iteration? If yes, there should be a subscript for $H$. Also, in line 277 (the update of $\\theta$), should it be $\\sqrt{D / i}$? (2) I think in Section 4, paper 921 tries to state that it is the $D^{E}$ that can greatly reduce the condition number $\\kappa(D^{E-1}H)$ and I think it should be declared clearly. (3) In line 178, what does $D^{-1}=\\Vert A_{I,.}\\Vert_2$ mean? (4) In line 267, $R(H,v)=...$ is a vector while (11) is a scalar, so what do you want to express here? Since there is no theorem that can provide a convergence rate for the proposed ESGD, I think a clearer explanation is needed.\n\n Overall, I expect more convincing experiments and a clearer explanation of the proposed ESGD.  Overall, I expect more convincing experiments and a clearer explanation of the proposed ESGD.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors describe the approach of pre-conditioning, a numerical method that uses a linear change of variables to perform gradient descent in a better conditioned parameter space. The authors propose using a pre-conditioner known as \"equilibration\", which row-normalizes a matrix, to pre-condition the loss Hessian. They then propose a practical numerical technique to estimate the equlibration pre-conditioner, which costs little more than normal gradient-based training. They show experimentally that this approach reduces the condition number of random Hessians, and they show theoretically that it reduces an upper-bound on the condition number of the Hessian, though a more direct proof is lacking. They further show that the resulting algorithm \"Equilibrated Gradient Descent\" (EGD) improves the convergence time of deep neural networks on some data sets. Additionally, they argue that the heuristic algorithm RMSProp is an approximation to equilibrated EGD.\n\nI think this is a clear, insightful paper with a well-rounded analysis. It is original and may be immediately useful for neural network training. This is a well-written paper with clear and precise experimental results and good theory on a topic of reasonable importance -- a justification of the success of one of the most common online training algorithms for neural networks and also the provision of a new, fairly practical online training algorithm.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a new adaptive learning rate scheme for optimizing nonlinear objective functions that arise during the training of deep neural networks. The main argument is based on recent results that indicate that the difficulty of the optimization stems from the presence of saddle points rather than local minima in the optimization path.\n\nThe saddle points slow down training since the objective function tends to be flat in many directions and ill-conditioned in the neighbourhood of the saddle points. The authors propose a new method for reducing the ill-conditioning (the problem of pathological curvature) by \"preconditioning\" the objective function through a linear change of variables, which reduces to left-multiplying the gradient descent update step with a learned preconditioning matrix D. They focus specifically on the case where D is diagonal, and they show how a diagonal D reduces to methods for learning parameter-specific learning rates, such as the well-known Jacobi preconditioner or RMSProp.\n\n This is a nice framework within which to consider different schemes for adaptive learning rates. As a side note, it was unclear to me whether this link has been established before, or whether this was a contribution of this paper?\n\nTheir main *theoretical* contribution is to show that the Jacobi preconditioner has undesirable behaviour in the presence of both positive and negative curvature, and to derive the optimal preconditioner which they call the \"equilibration preconditioner\" D^E as |H|^-1. However, since D^E is a function of the Hession which is O(n^2) in the number of weights of the network, both computing it and storing it becomes computationally intractable. Their main *practical* contribution is an efficient approximate algorithm \"Equilibrated Stochastic Gradient Descent\" (ESGD) which estimates the optimal D^E by using the R-operator and requiring roughly two additional gradient calculations (further amortized by only performing this step every ~20 iterations) and sampling a vector from a Gaussian.\n\nThe theoretical and practical contributions are verified by their empirical results which show that ESGD outperforms both standard SGD, Jacobi preconditioning and RMSProp for training deep auto-encoder networks.\n\nOverall, the paper is fairly clearly written, although I had some issues with ambiguous notation, primarily between distinguishing between element-wise operators and matrix-operators and at some points where it appears that matrices are defined element-wise but equated with full matrices, or vice versa, e.g.:\n\n* Eqn 7 and the definition for D^-1 = 1 / ||A_{i, .}||_2 just above it. Do you mean here the i'th element of D^-1 is defined like that? Otherwise, what does the subscript i refer to in A_{i,.} ? * Eqn 10 (which I'm taking to mean that each element i on the diagonal of D^E is defined as the norm of the i'th row of the Hessian, ||H_{i,.}||, is that correct?).\n\n Overall it would be helpful to add a section at the beginning on your notation, and specifying which operators act element-wise (e.g. |A|) and how you index into matrices, e.g. what do you mean by\n\n * A_{i,.} vs A_i (I'm presuming one indexes a row from a full matrix and the other an element from a diagonal matrix?), or\n\n* q_i q_j (e.g. below Eqn 11) vs q_{j,i} (in the supplementary material). Are these both the same?\n\nIn the supplementary material, it was not clear to me in Proposition 3 how the q on the RHS of the inequality is still squared after applying (q_*^2)^-{1/2} which seems to be q_*^{-1}? This is a well-written paper that presents a new adaptive learning rate scheme for training deep neural networks. It establishes theoretical links between different methods within a preconditioning framework, derives and presents an efficient approximation to the optimal preconditioner and gives empirical results showing that it works better than three alternatives in practice.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
