{"title": "Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis", "abstract": "An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is in particular challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object classes (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long- term dependencies along a sequence of transformations, and we demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability of disentangling latent data factors without using object class labels.", "id": "109a0ca3bc27f3e96597370d5c8cf03d", "authors": ["Jimei Yang", "Scott E. Reed", "Ming-Hsuan Yang", "Honglak Lee"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "In this paper, the authors design a recurrent convolutional encoder-decoder network that can render an object from different pivoting viewpoints given a single 2D image of the object. The authors evaluate the performance of their proposed model via its ability to generate images of faces and chairs from rotated viewpoints. They also perform additional experiments to examine the benefits of curriculum learning, to evaluate the model's performance of disentangled representations through cross-view object recognition, and to explore the model's ability to perform class interpolation with chairs.\n\n Although the proposed model is well presented and detailed, the evaluation of its performance is rather inadequate. The experiments used to demonstrate the effectiveness of the proposed RNN model are largely qualitative (figures with visualizations of results should be scaled to an appropriate size for the reader to see). Introducing a quantitative measure of performance with an appropriate error metric (to compare with state-of-the-art results) should prove to be more insightful than qualitative assessments alone.\n\n In general, the paper is well written and easy to follow. There are a few minor grammatical errors (i.e. line 124 \"rotate\" -> \"rotated\").\n\nThe significance of this paper is predominantly impaired by its lack of ample experimentation and analysis. It would be relevant to include experiments with more datasets other than Multi-PIE and Chairs. Demonstrating the model's ability to render rotated viewpoints for different types of objects would make the network architecture seem less ad hoc. Additionally, it would also be interesting to see how the proposed model could generalize to handle object rotation trajectories that deviate from a static axis (non-repeating rotations). The paper develops a deep learning model for the task of rendering an object from different rotational viewpoints given a single 2D image of that object. Although the approach is novel and interesting, the paper lacks sufficient experimentation and relevant analysis to flesh out the model's significance and potential for results that are comparable to state-of-the-art 'geometry-based approaches' (line 51).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this paper the authors propose a novel recurrent convolutional encoder-decoder network for learning to apply out-of-plane rotations to 3d objects such as human faces and 3d chair models. The proposed network starts from a basic model, where its encoder network disentangles the input image into identity units and pose units, then with the action units applied on pose units to control the rotation direction, its decoder network which consists of convolution and unsampling decode the identity and pose into an image of rotated object and the corresponding object mask. To support longer rotation trajectories, the proposed network is then extended to have the recurrent architecture where the encoded identity unit of input image is fixed and the pose unit is changed by a sequence of action units, and finally both identity and pose units are fed into decoder to generate the result image.\n\n One of main contribution of this paper is learning to disentangle the representations for identity/appearance and pose factors, where the identity units are shown to be a discriminative view-invariant features in the cross-view object recognition task. In addition, this disentangling properties will benefit more and predict better rendering while using the longer rotation trajectories in the curriculum training stages for training the proposed recurrent convolutional encoder-decoder network.\n\n The paper is well-written, easy to follow, and the motivation for different parts of proposed method is all clearly described. Also the qualitative results for predicted rendering of rotated images and quantitative evaluation on cross-view object recognition task provide good support for the method, especially the disentangled representations for pose and identity factors.\n\n Some minor weakness are listed as follows and hopefully the authors can address them in the rebuttal period:\n\n - The proposed network can only support discrete rotation angles, depending on the set of rotation angles shown in the training data. Do the authors have any initial idea how to extend the proposed method to support continuous rotation angles?\n\n - The proposed recurrent convolutional encoder-decoder network is trained with fixed-length, which is actually contradictory to general recurrent neural networks.\n\n  This paper proposes a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The main contribution of generative disentangling the identity and pose factors which emerged from the recurrent rotation prediction objective is well demonstrated by the qualitative and quantitative evaluations.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
