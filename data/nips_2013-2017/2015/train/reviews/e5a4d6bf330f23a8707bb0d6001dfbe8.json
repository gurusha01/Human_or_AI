{"title": "Explore no more: Improved high-probability regret bounds for non-stochastic bandits", "abstract": "", "id": "e5a4d6bf330f23a8707bb0d6001dfbe8", "authors": ["Gergely Neu"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "A new loss estimation procedure for adversarial multi-armed bandit is proposed which allows to bypass explicit exploration to obtain high probability bounds. Technically this leads to improved constants as well as simpler proofs. It's a good paper which deserved to be published. It is unfortunate that the author does not comment on whether this idea can be used for the linear bandit case, where a lot of work has gone into building good exploration distribution. Nice observation that escaped everyone else in the bandit community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "ADDED after author response:\n\nThank you for the clarifications. It would be great if you could include some of the points about implicit vs. explicit exploration and about choice of parameters in the two settings in the final version.\n\n======\n\nThe work is motivated by the observation that the explicit exploration term often used in multi-armed bandit algorithms when selecting the next arm can adversely affect performance, especially in situations where some arms are \"obviously\" suboptimal. The authors instead propose to use the implicit exploration (IX) idea of Kocak et al [NIPS-2014] and show that the corresponding loss function with implicit exploration (a) simplifies the regret bound analysis and (b) improves the constant factor in known regret bounds for many variants of the basic MAB problem.\n\n The paper is interesting, the writing is clear, and the math seems solid (although I didn't check all proofs carefully). Table 1 neatly summarizes the main theoretical results. I enjoyed reading the paper but have only lukewarm feelings of acceptance because:\n\n 1. Novelty and significance: The novel part here is the application of the known IX idea to a number of known algorithms, and their analysis. The results in table 1 are clearly improvements, but \"only\" by small constant factors. I'm not sure how significant is the result in advancing the field or how much practical impact they would have.\n\n 2. Implicit vs. Explicit exploration: The basic tenet here is that explicit exploration is undesirable, which I agree with. There are at least two things that make it undesirable. First is that with explicit exploration comes a tradeoff parameter (how much to explore, how much to exploit) that must often be tuned in practice based on the domain at hand. The proposed algorithm, however, effectively has this parameter as well (the gamma in the denominator) and thus doesn't resolve this issue. Second, as noted in the abstract, explicit exploration makes algorithms sample the losses of every arm at least Omega(sqrt(T)) times over T rounds, even when many arms are \"obviously suboptimal\". The (implicit) claim is that the proposed algorithm is less wasteful on arms that are \"obviously suboptimal\". However, this part of the theory has not been explored in the paper. Table 1 doesn't address this. Reading the abstract, I was expecting to see some result of the form: if some fraction of arms is \"obviously suboptimal\" then the IX algorithm will provably waste less resources on them. Having a result along these lines would have made the paper significantly stronger.\n\n Overall, I like the paper but have some reservations.\n\n  A well-written paper with solid math that applies a previously proposed implicit exploration idea to a number of MAB algorithms and improves the constant factor in several previous bounds. The advantage of implicit vs. explicit exploration isn't fully clear to me.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Overall 9 This paper develops and generalizes the principle of implicit exploration proposed in \"Efficient learning by implicit exploration in bandit problems with side observations\", NIPS 2014. . Implicit exploration consists in using a lower confidence bound to estimate the true loss.\n\nWhile the previous analysis was done with respect to the expected regret, here, the regret bounds are given with high probability.\n\n The main claim of the paper is to demonstrate the advantage of high confidence algorithms based on an implicit exploration. The main result is a general concentration inequality concerning the sum of losses, which is provided in Lemma 1. Then, this result is applied to three variants of the non-stochastic bandit problems: the MAB problem with expert advices, tracking the best sequence of arms, the MAB problem with side-information. For these three problems, the analysis of the regret bounds shows an improvement of around a factor 2 of the pre-factors. Moreover, although that in practice Exp3.P using an explicit exploration is outperformed by standard Exp3, Exp3-IX outperforms Exp3 on the single but interesting experience done: Exp3-IX seems to be less sensible to the value of eta, and provides more robust estimation of the regret.\n\nRemark 1: Exp3 does not work for the switching bandit problem (i. e. on the best sequence of arms). It will be interesting to add a comparison of Exp3.S and Exp3.SIX when the best arm changes several times. Remark 2: there is a typo line 4-5 Algorithm 1: \\hat in place of \\sim as stated in the first lines of proof of Lemma 1.  This paper is simply excellent. It is particularly well written. The analysis is gradual and relevant. The theoretical results involve a large class of MAB problem and can be re-used by the machine learning community.An experiment illustrates the practical interest of the implicit exploration.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
