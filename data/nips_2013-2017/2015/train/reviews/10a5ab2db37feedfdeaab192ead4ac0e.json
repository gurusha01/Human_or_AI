{"title": "Backpropagation for Energy-Efficient Neuromorphic Computing", "abstract": "", "id": "10a5ab2db37feedfdeaab192ead4ac0e", "authors": ["Steve K. Esser", "Rathinakumar Appuswamy", "Paul Merolla", "John V. Arthur", "Dharmendra S. Modha"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "1) The authors should at least mention the algorithm in section 3.2 is deeply similar to the cited references\n\n[14] J. Zhao, J. Shawe-Taylor, and M. van Daalen, \"Learning in stochastic bit stream neural networks,\" Neural Networks, vol. 9, no. 6, pp. 991 - 998, 1996. and [15] Soudry, Daniel, Itay Hubara, and Ron Meir. \"Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights.\" Advances in Neural Information Processing Systems. 2014.\n\nin its final form (which is a hybrid of both algorithms), and was derived using identical approximations.\n\n2) The first paragraph in the discussion gives the impression the \"constrain-then-train\" methodology (learning a weight distribution and then sampling from it to produce a fixed network ensemble at test time) is novel, even though it was already suggested and tested in ref [15].\n\n3) Novelty claim (i) in the last paragraph of the introduction is also misleading. Ref. [15] \"expectation backpropgation\" algorithm is a backpropgation update rule (using expectation propagation), implemented for binary neurons (i.e., with a sign activation function, in which zero input is never reached exactly) with binary weights, and can easily have constrained connectivity.\n\n4) The cited previous state-of-the-art are wrong. To the best of my knowledge, the previous best results for binary weights (and neurons) were achieved by the algorithm in ref. [15], which was tested on the mnist (http://arxiv.org/pdf/1503.03562v3.pdf) without data augmentation, as was done in this paper.\n\n%%% Edited after author's feedback Thank you for agreeing to address these concerns.  This paper has quite strong and novel experimental results, in terms of accuracy and power consumption. However, both the general methodology and the algorithm itself are not as novel as claimed in the paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a way to reconcile the need for high precision of continuous variables in backpropagation algorithms with the desire to use spike-based neural network devices with binary synapses. They introduce the idea of considering the analog variables as probabilities that are sampled to map the neural network used off-line for training to the spiking network chip used for deployment of the application. They show how to adapt the backpropagation algorithm to be compatible with this strategy and validate the method proposed on the MNIST benchmark. They map the trained network onto two different architectures, optimizing for size and accuracy, and present performance figures measured from the chip. The quality of the manuscript is very high, as well as its clarity. The work is quite original and can be useful for other approaches and other hardware platforms. BTW, in reviewing the different HW platforms available, the authors should mention also Qiao et al., Frontiers in Neuroscience 2015, and the paper at http://arxiv.org/abs/1506.05427 The significance of the work is not highlighted in the paper. It is not clear how the method proposed will be useful to deploy the TrueNorth chip in practical applications, and in what specific applications. It is not clear if and to what extent this specific method has advantages over other alternative options, such as those cited in [9] to [12].\n\n The authors present a method for applying backpropagation to spiking neural networks with binary synapses and validate it by successfully mapping the trained network onto the TrueNorth device. The paper is well structured, clear and to the point. The results are convincing, but lack some details (e.g. how are the inputs converted into spikes, what are the frequencies used, the shared weight values, the neuron models, etc.)", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors present an interesting paper about a backpropagation training method using spike probabilities and takes into account the hardware constraints of a particular platform which has spiking neurons and discrete synapses. This is a topic of current interest in mapping deep networks to multi-neuron hardware platforms.\n\nQuality: The proposed training method is useful especially in consideration of new multi-neuron hardware platforms with constraints.\n\n Clarity: The paper is easy to read. The claims made at the end of Section 1 can be reworded because as it stands, if not read carefully, suggests that the paper proposes for the first time a training methodology that employs spiking neurons, synapses with reduced precision. I would not necessarily put the demonstration of running this network on a specific hardware platform, in this case TN, as a novel feature to be included in this case. Running the network on TN is a validation of the training method.\n\nIn Section 2, what does it mean for 0.15 bits per synapse? The network topology is not clearly described in one place. How much longer is the training time because of the probabilistic synaptic connection update? Why is the result from a single ensemble of the 30 core network so low especially when compared to the 5 core network? Why are the results of the different ensembles of the 5 core network about the same? What are the spike rates for the inputs during testing? Is the input a spike train? Ref 12 on line 399 includes constraints (e.g. bias=0) during training so it is not just a train-then-constrain alone approach (typo on this line, \"approach approach\").\n\nOriginality:\n\n Can the authors provide a discussion or comparison with other spiking backprop-type rules like SpikeProp?\n\n Significance: The development of new training methods that consider constraints of hardware platforms is of great interest, The constraints which are considered during training in this paper are based on the TN architecture, these constraints might not all apply to other platforms. If the results are based on a 64 ensemble, that means more hardware resources have to be dedicated to obtain the accuracy needed of the network.  An interesting paper about a training method using backpropagation which takes into account the hardware constraints of a particular platform which has spiking neurons and discrete synapses.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
