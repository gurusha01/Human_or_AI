{"title": "Bayesian Optimization with Exponential Convergence", "abstract": "This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the delta-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence requires access to the delta-cover sampling, which was considered to be impractical. Our approach eliminates both requirements and achieves an exponential convergence rate.", "id": "0ebcc77dc72360d0eb8e9504c78d38bd", "authors": ["Kenji Kawaguchi", "Leslie Pack Kaelbling", "Tom\u00e1s Lozano-P\u00e9rez"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper presents a new algorithm for global optimization that avoids delta-cover sampling and that achieves exponential regret, generalizing this way the work of Freitas el al., 2012 that relies on an impractical sampling procedure.\n\n The paper is well written and is easy to follow and the role of the new algorithm within the current literature of bounded-based search methods is well explained. In this sense, I think that this work deserves some attention from the community.\n\n I vote 5, however, a not a better score because I think that the experimental section is below the standard of what one expect to see in a BO paper. My main critics to this section are:\n\n- The authors do no compare the methods for multiple initial evaluations of *f* as it is the standard in BO methods. Therefore no meaningful statical comparison of the methods can be obtained.\n\n- Although they propose a bounded-based search method, the authors should compare they results with state of the art of BO approaches. Information theoretic approaches, such as, Entropy Search, are not used in the experiments.\n\n - All the experiments are carried out in synthetic functions (whose maximum dimension is 6). No real wetlab of parameter tuning experiment is used to illustrate the performance of the method in real scenarios.\n\n  The paper presents a new global optimization algorithm that avoids delta-cover sampling and that achieves exponential regret. The paper is nice, with some interesting theoretical results but in my opinion the experimental section is below the standards of NIPS paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents a variant of Bayesian Global Optimization where future samples are generated through a space/interval partitioning algorithm.\n\nUsing UCB, this avoids the (internal) global optimization problem of how to choose the next sample.\n\nTheoretical analysis shows that the approach yields theoretical improvements (exponential regret), thereby improving over previous work.\n\nThe algorithm is clearly described and explained/illustrated with an example.\n\nThe paper makes a novel contribution and is overall clearly presented.\n\nI have one reservation with this paper.\n\nThe experimental results seem to be only for 1D test functions (not explicitly stated).\n\nI suspect that the interval partitioning approach does not scale well to higher dimensions (hyperrectangles) because the \"resolution\" would be required to grow exponentially.\n\nThe approach is related to the well-known DIRECT algorithm, which is known to suffer badly when the dimensionality of the problem increases.\n\nI think something at least needs to be said about this in the paper.\n\nIt does not change the theoretical contribution but is clearly significant for any practical purposes.\n\nMinor comments:\n\np.2, UCB is considered \"for brevity\".\n\nDoes this mean you could do something with expected improvement, for example?\n\nI got the feeling it had to be UCB.\n\np.3 \"...we simultaneously conduct global and local searches based on all the candidates of the bounds.\"\n\nI couldn't understand this statement.\n\np.4 \"At n=16, the far right...but no function evaluation occurs.\"\n\nCan you say why for clarity? The paper presents a variant of Bayesian Global Optimization where future samples are generated through a space/interval partitioning algorithm, which yields theoretical improvements (exponential regret).The work appears sound and novel but seems to be only evaluated on 1D test problems.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper describes a method of Bayesian global optimization which does not require d-cover sampling or auxiliary optimization.\n\nThe paper is organized such that the main contributions of the paper are well described in the main section.\n\nThe authors provide a good illustrative example of the algorithm in section 3.2, which gives the reader a nice high level understanding.\n\nSection 3.3 also clearly communicates the workings of the algorithm programmatically for future implementation/replication.\n\nExperimental results as well prove to be quite exciting.\n\nAdditionally, the authors note that the paper brings together ideas from the Bayesian global optimization literature that do and do not use continuity estimation.\n\n The paper describes Infinite-Metric GP Optimization (IMGPO), an algorithm for Bayesian optimization without the need for auxiliary optimization or d-cover sampling.The paper is not only clearly written, but provides a strong contribution to the NIPS community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "(light review) Abstract formulation is redondant l49 the function l72 misplaced parenthesis Being unfamiliar with GP and global optimization, the reviewer's evaluation is an educated guess.The paper is well written and rather didactic, making good use of graphs to explain the procedures. Both theoretical and experimental results seem to conclusively demonstrate the advantage of the authors' approach.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
