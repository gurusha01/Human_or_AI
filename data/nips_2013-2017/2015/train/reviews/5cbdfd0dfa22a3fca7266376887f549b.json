{"title": "Sparse and Low-Rank Tensor Decomposition", "abstract": "Motivated by the problem of robust factorization of a low-rank tensor, we study the question of sparse and low-rank tensor decomposition. We present an efficient computational algorithm that modifies Leurgans' algoirthm for tensor factorization. Our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction. We use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor. We delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm. We validate our algorithm with numerical experiments.", "id": "5cbdfd0dfa22a3fca7266376887f549b", "authors": ["Parikshit Shah", "Nikhil Rao", "Gongguo Tang"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "In this paper, a modified Leurgans' algorithm is proposed to do decompositions for sparse and low rank tensors. Tensor decomposition is very hard and usually computationally expensive. So it is interesting to study efficient algorithms to do tensor decompsitions for sparse and low rank tensors. In the paper, the authors use order 3 tensors as examples, and clearly present the algorithm and theoretically discuss the efficiency. In section 4, the authors discuss the algorithm can be extended to higher order tensor. The paper is well organized. Here are some questions:\n\n(1) In the algorithm, it needs to do several eigenvalue decompositions for matrices, which are computationally expensive. And, it also needs to solve two convex matrix optimization problems (6). Can this method be applied to solve large scale tensor decompositions? How to understand the sparsity in the paper? What is the largest tensor that can be decomposed by this algorithm? In section 2,2, Numerical examples only present results for 50 dimensional order-3 tensor, which is not enough to show the efficiency of the algorithm.\n\n (2) Page 2, line 071, the definition of low-rank is r <= n1. In Numerical implementation part, (Section 2.2) for 50 *50 *50 Tensor, computational results are list for r = 1-4, which is very small. What is the computational result for r =40-49? Is it also very good, or the algorithm only works for very small r?\n\n(3) Section 2.2, how to solve problem (6) in your implementation?\n\n Other suggestions:\n\n- Typo, line 093, the resulting n*n matrix, should be n1*n2 matrix;\n\n- Typo, line 96-98, \\sum_k should be \\sum_i;\n\n - Typo, page 5, line 216, X_a should be X_a^3;\n\n- Typo, page 4, line 198, \"coefficient\" should be coefficient. In this paper, a modified Leurgans' algorithm is proposed to do decompositions for sparse and low rank tensors. Tensor decomposition is very hard and usually computationally expensive. So it is interesting to study efficient algorithms to do tensor decompositions for sparse and low rank tensors. The paper is organized well. However, the numerical implementation part is not good enough. It only shows the result for a very special order-3 tensor with very small r. It might be better to give more examples.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a tensor decomposition algorithm with a sparse noise setting. The proposed method is interesting and useful for general settings of tensor decompositions. And the algorithm could be tractable by reducing the original problem into matrix decompositions of matrices in pairs of modes. However experimental evaluation is limited.\n\nQuality: This result is of decent quality. The proposed approach is general and useful, which can easily extend to high order tensors without increasing much computational cost. However numerical evaluation is limited and the used evaluation measure is unusual. The reason why authors used the accurate probability instead of MSE should be provided. Additional numerical evaluations with several noise settings are necessary.\n\n Clarity: This paper totally reads very well.\n\nHowever no optimization algorithm for (6) is provided. I guess authors used [22] or [23]. At least, the summary of used algorithms should be described. In section 2.2, the model description X=\\sum_{i=1}^r \\lambda ... includes a mistake.\n\nOriginality: The proposed decomposition algorithms and its theoretical analysis are quite novel.\n\nSignificance: The proposed approach seems to be a promising decomposition. This approach can be easily implemented and extended to more general settings of tensor decomposition. To stress these advantages, discussions about related works is necessary.\n\n  The proposed tensor decomposition algorithm is interesting and useful.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: It is firstly assumed that an observable tensor consists of two additive parts: a low-rank tensor that is not sparse; and a sparse corruption tensor that is not low-rank. Then the paper proposes an extension of the Leurgans' Algorithm in terms of solving the convex problem defined by the model assumption. This extension has the advantage of simplicity and less computational complexity. With theoretical proofs provided afterwards it can be verified that the both assumed tensor components can be recovered correctly under given conditions.\n\nQuality: The contents are very well structured and the main idea is clearly introduced and motivated. The provided proofs are also convincing and helpful for understanding. But there are quite a few number of trivial typos. Clarity: Altogether the main concept and motivation are quite comprehensible. The authors have made especially remarkable effort to derive and demonstrate the constraints of the proposed approach.\n\nOriginality: Although the proposed algorithm strongly depends on that of Leurgans, its combination with convex optimization could be a novel solution to the defined assumption.\n\nSignificance: It has been shown that the model is capable of modeling the two additive tensors being low-rank and sparse, respectively. But the proposed algorithm has certain constraints that might compromise the applicability and significance. Especially the final conclusion (Corollary 3.7) is based on uniform dimension without giving further arguments.  The paper proposes an interesting extension of the Leurgans' Algorithm. The concept and main results are very clearly formulated, although it is yet unclear whether the applicability of the model might suffer from the mentioned constraints, since experiments on real-life data are missing in the paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In this submission, the authors propose to solve a tensor factorisation problem of the form Z=X+Y, where X is a low-rank representation of Z and Y represents the sparse corruption. The authors define two subspaces of matrices T_{U,V} and T_{V,W} for a third-order non-symmetric tensor that describe the span of tensor after contraction along either the 1st or 3d mode, respectively. The sparsity after contraction along mode m is characterised by the union of support sets of each matrix (tensor slice) extracted along this mode. Therefore, the authors define incoherence parameters based on L_{inf} norms to control the sparsity, e.g. make spectra of matrices 'diffuse' rather than sparse. The authors employ an extension of the simultaneous diagonalisation [18] by contracting a tensor along modes 1 and 3 by vectors a and b randomly chosen for uniformly distributed unit sphere. This allows obtaining a set of eigenvalues u and v, and v' and w. After pairing eigenvalues u, v, and w, a set of equation arises in which the authors can solve for eigenvectors. For simultaneously imposing low-rank X and sparse Y, the authors apply the simultaneous diagonalisation on contractions of X and Y, respectively, and impose the nuclear norm on all X and \\ell_1 norm on all Y, and solve equation 6. This particular subproblem is well explained in [10], e.g. equation 1.3.\n\nFor evaluations, the authors generate 50^3 dimensional non-symmetric tensors Z composed from low rank X and sparse Y according to distributions described in the paper. The authors present the likelihood of recovery of tensors X and Y from Z w.r.t sparsity and true tensor rank r.\n\n Pros: - an interesting non-symmetric tensor factorisation/decomposition problem - interesting expansion of existing symultaneous positive-definite matrix diagonalisation problem [18]. - interesting way of imposing the low-rank and sparsity related constraints on contractions extending problem [10]\n\nCons: - this submission would benefit from more evaluations, e.g. beyond figure 1 - the last steps of the algorithm in section 2 could be described better - the proofs feel a little bit hard to follow.\n\n Major comments: 1. This is a very interesting paper that extends several ideas verging from simultaneous diagonalisation for a set of matrices to low-rank and sparse matrix recovery by applying/extending these methods to third order non-symmetric tensors. The maths behind these methods seems convincing.\n\n2. The symultaneous diagonalisation in [18] identifies the problem with using merely one projection vector (equvalent of one contraction). The authors of [18] note in section 3 that the quality of estimated eigenvalues depends on the value of the smallest eigengap. Hence, the authors of [18] propose random projections in order to minimise the situations where a single projection could lead to zeroing some eigenvalues. Do authors of this submission perform multiple contractions? It feels so however it is still very unclear form section 2 or algorithm 1. How does this affect the stability of diagonalisaton given non-symmetric tensors require solving for three sets of eigenvectors?\n\n3. The evaluations of the proposed algorithm feel somewhat limited. It'd be very convincing to see an experiment on non-synthetic data. Also, some of the guarantees\n\ngiven in section 3 could be also illustrated in the experimental setting. For instance, what happens in practical simulations when v_3 and v_1 are outside of the guaranteed regime from corollary 3.4?\n\nThis paper should be interesting to the community researching higher-order tensors. It is sufficiently clear and original despite it relies heavily on prior developments.  In this submission, the authors propose to solve a tensor factorisation problem of the form Z=X+Y, where X is a low-rank representation of Z and Y represents the sparse corruption.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
