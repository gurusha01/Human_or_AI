{"title": "Deep learning with Elastic Averaging SGD", "abstract": "We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.", "id": "d18f655c3fce66ca401d5f38b48c89af", "authors": ["Sixin Zhang", "Anna E. Choromanska", "Yann LeCun"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "In this paper, the authors propose a distributed stochastic gradient learning scheme amenable to infrequent synchronization. Their claim is that, due to the non-convexity, existing algorithms are not very robust to distant synchronizations. Further, their resilience to differences between parameter values across different nodes favors exploration, which leads to better solutions.\n\nThe algorithm is simple and seems to work well on the experiments. The authors compared with many existing algorithms and for several values of tau. To nitpick, since the authors talk about deep learning without more details, I would also have appreciated experiments on other architectures than CNNs.\n\nMy main gripe with this paper is that, while they revisit an old method (see section 4.1 of \"Notes on Big-n Problems\" by Mark Schmidt, an excellent review of the literature), their coverage of existing implementations of these methods is scarce. Basically, they are referenced in the introduction but the in-depth analysis is limited to ADMM.\n\nI believe positive results on distributed stochastic optimization in the context of nonconvex losses can definitely be of great significance. However, the authors should do a better job at reviewing the existing literature and not just referencing it. The algorithm seems to outperform existing distributed techniques but is very slim on the comparisons with non-deep learning oriented distribution techniques which have been widely studies in the optimization literature.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper studies general-purpose training algorithms for deep learning and proposes a family of algorithms called elastic averaging SGD. The idea is novel and the paper is of very high quality.\n\nThe paper focuses on training large-scale deep learning models under communication constraints. This problem is difficult since there are many local optima in non-convex problems like in deep learning. The optimization problem is formulated as a global variable consensus problem such that local workers would not fall into different local optima, and then its gradient update rules are reinterpreted using the elastic forces between local and global parameters. This is why the proposed family of algorithms is named elastic averaging SGD (EASGD). There are four algorithms in the family: a synchronous version, an asynchronous version, and their momentum versions. They are demonstrated promising theoretically and experimentally.\n\nIn the theoretical analysis, the authors present a vivid illustrative example in which the popular ADMM can be unstable and its stability condition is unknown, but the stability condition of EASGD is quite simple. The writing style here is very impressive and friendly. All theorems are in the supplementary material and there is only a small part of theoretical analysis in the main paper, but the organization made me feel quite smooth (though I did not carefully go through the supplementary material). I have some minor questions here: What is the most important reason for the superior stability of EASGD? Is it because the maps of ADMM are asymmetric while the maps of EASGD are symmetric? Is it possible to find another illustrative example such that on this specific example EASGD is more unstable than ADMM?\n\nIn their experiments, the proposed algorithms were used to train convolutional neural networks and they outperformed state-of-the-art SGD algorithms on CIFAR and ImageNet. The proposed algorithms achieved the goal such that the larger the communication periods are, the better EASGD would be than other SGD as shown in Figure 2. Figures 3 and 4 also showed that the speedup of EASGD would be better than the baselines when there were more workers. As further experimental results, we have implemented similar gradient update rules adapted to our clusters (according to the arxiv version) and they successfully improved our baselines. The resulted models have been or will be launched online and one fifth of the world's population would benefit from these improved models. So I believe that this is a significant paper. This paper studies general-purpose training algorithms for deep learning and proposes a family of algorithms called elastic averaging SGD. The idea is novel and the paper is of very high quality.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper addresses the important problem of parallel optimization. The proposed algorithm is shown to have better tolerance to staleness and thus carries potentially smaller communication burden than the state-of-the-art. It also obtains better learning results as measured by test error. The paper is overall well-written and a pleasure to read.\n\nIn the current EASGD, the center variable is updated by following a symmetric elastic force (Eq.4). However given the objective in Eq.2, a more natural solution seems to be always taking the exact average of all the local variables (still in an online fashion). I wonder whether the authors have investigated this variant and would like to see some discussion in the paper.\n\nIs there any experimental study on the effect of rho? More specifically, how does the exploration affect the performance?\n\nthe line under Eq.4: the stochastic gradient of \"F\"? The paper proposes a parallel algorithm that encourages simultaneous exploration among computing nodes for more effectively optimizing objectives with many local optima. The proposed idea is well motivated, clearly presented and supported by extensive experimental studies.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper introduces EAMSGD algorithm for fast convergence of distributed SGD. The idea is based off several prior class works in distributed optimization and is first drawn a connection with deep learning models. Results on public datasets CIFAR and ImageNet are solid and shown clearly.\n\n Quality: high quality paper with good ideas and solid results.\n\n Clarity: the paper is well structured, at the same time, clearer and more direct formulations of the algorithm might be more appreciated by the reader. For instance, the different types of distributed sgd at start of section 5 is not clearly documented. Suggest a table format or show the list of available algorithms with clear categories. It will bring out the results even better.\n\n Originality: since the idea is draw from prior classic works, the idea is not entirely new, at the same time the application to recent deep learning models and dataset is a first-time.\n\n Significance: due to the flourishing of distributed deep learning in industry and academia, this paper should be appreciated by many engineers and researchers.\n\n The paper, with the proposed elastic averaging method for distributed SGD, shows good results on empirical image classification experiments. The authors also provided theoretical analysis and mathematical formulations by drawing analogy to ADMM. The research idea is novel and results are solid and show clearly. Other than some details in experiments which could be improved, this is a good paper recommended to be published at NIPS.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
