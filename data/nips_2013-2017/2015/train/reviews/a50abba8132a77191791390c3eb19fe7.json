{"title": "StopWasting My Gradients: Practical SVRG", "abstract": "We present and analyze several strategies for improving the performance ofstochastic variance-reduced gradient (SVRG) methods. We first show that theconvergence rate of these methods can be preserved under a decreasing sequenceof errors in the control variate, and use this to derive variants of SVRG that usegrowing-batch strategies to reduce the number of gradient calculations requiredin the early iterations. We further (i) show how to exploit support vectors to reducethe number of gradient computations in the later iterations, (ii) prove that thecommonly\u2013used regularized SVRG iteration is justified and improves the convergencerate, (iii) consider alternate mini-batch selection strategies, and (iv) considerthe generalization error of the method.", "id": "a50abba8132a77191791390c3eb19fe7", "authors": ["Reza Babanezhad Harikandeh", "Mohamed Osama Ahmed", "Alim Virani", "Mark Schmidt", "Jakub Kone\u010dn\u00fd", "Scott Sallinen"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper extends the stochastic optimization algorithm SVRG proposed in recent years. These modifications mainly includes: the convergence analysis of SVRG with corrupted full gradient; Mix the iteration of SGD and SVRG; the strategy of mini-batch; Using support vectors etc. For each modification, the author makes clear proofs and achieves linear convergence under smooth and strong convex assumptions. However, this paper's novelty is not big enough. The improvement of convergence rate is not obvious and the proof outline is very similar to the original SVRG. The key problem such as the support for non-strongly convex loss is still unsolved. Besides, there exists some small mistakes on the notations in the appendix.\n\n This paper makes some practical improvements based on the stochastic optimization algorithm SVRG proposed in recent years. These improvements are proved to be effective by the theory and experiment results. However, the novelty and improvements of performance are not big enough. The proof idea is very similar to the original SVRG and key problems such as the support for non-strongly convex loss is still unsolved.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Comments:\n\n- Tricks like non-uniform sampling and then correcting by importance sampling have been around for much longer, see e.g. [1] which claims a speed-up of 3x. Those older approaches chose the probability based on error instead of L_i -- could you discuss/compare such an approach, at least in practice? I'd suspect this may be useful in practice where the L_i may not be known.\n\n- Channeling Leon Bottou: please discuss the limitations, and the cases where your method may *not* be applicable.\n\n- I would really appreciate an open-source code release accompanying the paper, which would increase its impact.\n\n- The experimental section and figures are clearly the weakest part of the paper, I'd recommend a serious second pass. For example, all plots should have error bars, state over how many runs they were averaged, labeled more clearly (SVRG -> full batch in top plots), the test-error plot does not have the resolution to be readable (maybe log-scale is better even if it won't go to zero). The same holds for the figures in the appendix. I do not understand why there is no difference at all between uniform sampling and Lipschitz sampling (in experiment 3)? Of course, a more challenging domain than logistic regression would be appreciated, but that can rightfully be shown in another paper.\n\n [1] Geoff Hinton, \"To recognize objects, first learn to generate images\", 2007.\n\nTypos:\n\nL19 a variant L53 increases over L77 expectation missing L96, L132 expectation symbol misformatted L164 B^s L289 can be used L316 superscript instead of subscript L325 sometimes L416 sometimes often? This is a solid and important paper about variance-reduced gradient descent. It has many good, new ideas, explains them clearly, and determines convergence rates for all its proposed variants. The paper's weakness is the experimental section.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors should be explicit on what this new methods adds to existing stochastic optimization (e.g., Hu Kowk and Pan, 2009) and related mini-batch optimization (e.g. Konecny et al, 2013 and 2014), or Smola's work (e.g., M Zinkevich, M Weimer, L Li, AJ Smola, 2010), including using such methods as baseline on the experiments.\n\nMerely comparing the method to variants of itself is insufficient.\n\nThis is a crowded field.\n\nIt behooves the authors to explicitly delineate their novel contributions.\n\n The paper is relatively well written, and the comparative bounds table (sec 8) is useful.\n\n The paper discusses the stochastic variance-reduced gradient method to speed convergence in optimization of a sum of strongly convex Lipschitz continuous functions. The method trades off inexact computation for speed, and it is not clear whether it represents a sufficient improvement over the state of the art in stochastic optimization to merit publication in NIPS.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes numerous methods to speed up svrg. The methods are growing batch size, mixed SGD/svrg, and a new minibatching scheme.\n\nSpecific comments 1. It would be nice to have empirical comparison against a well-tuned SGD implementation. 2. I'm not clear on how the results for \"time to reach E_est + E_opt for FG and SVRG were reached. E_est should only depend on n. For alpha =1, it should be O(1/n). It is unclear how log^2 1/epsilon and d^2 arises.  This is a nice paper that proposes numerous modifications to SVRG.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper introduces several approaches to increase the convergence speed of a well-known stochastic gradient method called stochastic variance-reduced gradient (SVRG).\n\n The paper is well-written and it is very easy to follow the arguments of the paper. The authors start with a short introduction of SVRG method and subsequently explain three approaches for improving the convergence speed of SVRG.\n\nThis paper starts with a key proposition showing that SVRG does not require a very accurate approximation of the total gradient of the objective function needed by SVRG algorithm. The authors use this proposition to derive a batching SVRG algorithm with the same convergence rate as that of original SVRG. Then, the authors propose a mixed stochastic gradient/SVRG approach and give a convergence proof for such a scheme. As a different approach of speeding up, the authors proposed a speed-up technique for Huberized hinge-loss support vector machine.\n\n In addition to speeding-up strategies and their convergence analysis, the authors include similar analysis for the case of regularized SVRG algorithms. The authors provide propositions for the convergence of mini-batching strategies. Through extensive simulations, the authors show the improvements obtained by their methods.\n\nExcept minor comments stated below, I do not have any major comments for further improving the manuscript.\n\n Minor Comments: * On the right hand side of inequality in line 77, there should be an expectation * Question marks in the first line and middle of page 10 should be corrected The topic of the paper is of interest for nips community and the analysis and simulation results were done perfectly.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
