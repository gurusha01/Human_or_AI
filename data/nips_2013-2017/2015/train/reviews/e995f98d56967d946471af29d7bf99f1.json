{"title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks", "abstract": "Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence.  We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead.  Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning bid to the MSCOCO image captioning challenge, 2015.", "id": "e995f98d56967d946471af29d7bf99f1", "authors": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Recent work on neural machine translation and other text generation tasks has trained models directly to minimize perplexity/negative log-likelihood of observed sequences. While this has shown very promising results, the setup ignores the fact that in practice the model is conditioning on generated symbols as opposed to gold symbols, and may therefore be conditioning on contexts that are quite different from the contexts seen in the gold data.\n\n This paper attempts to remedy this problem with by utilizing generated sequences at training time. Instead of conditioning on the gold context it utilizes the generated context. Unfortunately at early rounds of the algorithm this produces junk, so they introduce a \"scheduled sampling\" approach that alternates between the two training methods based on a predefined decay schedule inspired by curriculum learning.\n\nThe strength of this paper is in its simplicity and the comprehensive empirical testing. The important model and inference assumptions are defined clearly, and the details about the internal architecture of the model are appropriately elided. It seems like it would be very straightforward to re-implement this approach on LSTM's or any other non-Markov model.\n\n Empirically, the method seems to work quite well. There is a relatively large gain across several quite different tasks, and the schedule part seems to have a significant effect as always sampled does quite poorly.\n\nThe parsing results still pretty far behind state-of-the-art, but they use a very reduced input representation (no features). The speech results also seem to be using a somewhat unique setup, but the improvement here is quite large.\n\n - I would be to know how performance changes based on the footnote 1. It seems like flipping on a token level is very different than flipping on an example level, since the worst-case distance between gold tokens is much lower.\n\nThe main weakness is the lack of comparison to other methods that attempt a similar goal.\n\nFor one, the authors are too quick to dismiss early-update perceptron (Collins and Roark, 2004) with beam search as being not applicable, \"as the state sequence can not easily be factored\". While the factoring is utilized in parsing, nothing about beam search requires this assumption to work.\n\n(This connection between beam search and DP is also made on l.130. Beam search is rarely used for HMMs, at least in NLP, and when it is, it is often exactly when it is not possible to use dynamic programming.) The continuous nature of the state shouldn't effect the use of this algorithm, and in fact there is a paper at ACL this year \"Structured Training for Neural Network Transition-Based Parsing\" that that using this method on neural-net model that makes similar assumptions.\n\nSecondly, I did not feel like an appropriate distinction was made with SEARN and reinforcement learning type algorithms. The related work talks about these as as \"batch\" approaches. While the SEARN paper itself may have chosen to use a batch multi-class classifier (since they are fast), that does not mean it couldn't be applied in the SGD case. It seems like the key idea of SEARN is to interpolate the current prediction from the model with gold based to produce a sampled trajectory. The major difference is that they may learn the policy versus using a schedule.\n\n  This paper clearly presents a simple method that yields improvements across several sequence modeling tasks. My only concern is that there does not seem to be any serious baseline comparisons, and other past methods are, to my mind, inappropriately dismissed as non-applicable.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The usual The usual approach to training recurrent nets, each prediction is all based on the current hidden state and the previous correct token (from the training set). But for test, we actually expect the trained rnn could generate the whole sequence by make prediction based on the previous self-generated token. The paper suggests during the training we should force the model to gradually generate the whole sequence (the previous token more and more likely generate by the model self).\n\n Quality\n\n Technically sound and the usefulness of scheduled sampling is supported well.\n\nClarity\n\n The paper is well written and organized.\n\nSignificance:\n\nThe main idea is well motivated and interesting. The new training process could have important impacts on the study of recurrent net training.\n\nMinor comments Do you have any intuition of the differences of the three decay schedules? How those different decay schedules represent on the training set? Does the training easily remain stuck in sub-optimal solution? Training recurrent nets could very tricky (there are lots of choices, momentum, gradient clipping, rmsprop and so on). Please provide more details of the training and make the experiments reproducible. Please also report the cost on the training set. Would the scheduled sampling be helpful for optimization?  It is a good paper that proposed a simple and straightforward scheduled sampling strategy for alleviating the discrepancy between training and inference of recurrent nets applied on generating sequences. The trained recurrent nets by this scheduled sampling outperform some fairly strong baselines on image captioning, constituency parsing and speech recognition.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "TL;DR This paper describes a training heuristic, scheduled sampling (SS), for RNNs in which gold truth labels are sometimes replaced with sampled predictions from the model. Different schedules for deciding when to replace the gold labels are suggested, which all amount to different kinds of decay functions (linear, exponential, inverse sigmoid). Improvements over a comparable RNN model without SS are presented for the following tasks: image captioning, constituency parsing, and speech recognition.\n\nThis is a neat experimental result! While noise injection is an old idea, the focus on improving robustness at test time is interesting. But I worry that this paper raises more questions than it answers. Here are some specific concerns:\n\n- If SS is working as a regularizer, it's good to know that it appears to be additive to dropout. However, it would also have been good to include the following baseline: what about *always* randomly sampling a label (according to the proposed schedules) rather than using model predictions?\n\n- If the idea is to mitigate search error, I would have liked to see a comparison to baselines which use different beam widths. Is there still a benefit from SS if the model uses a larger beam width?\n\n - I'm a little worried about the hyper-parameter k. Setting it based on \"expected speed of convergence\" is a little nebulous, as there's no discussion of how sensitive it is, or how it was tuned in the experiments.\n\nAside from these specific concerns, at a high level I think this paper would benefit from a more rigorous probabilistic analysis. It would be great if the paper shed some light on *why* the proposed heuristic appears to work, e.g. by teasing apart the regularization effect. I would have liked to see some experiments showing the benefit of SS as the amount of supervision is varied.\n\nUPDATE AFTER AUTHOR RESPONSE:\n\nThanks for addressing some of my concerns. However, I still worry a little bit about how difficult it is in practice to tune the sampling schedule, and wish there a little more analysis of the method. This paper describes a neat training heuristic for RNNs that improves robustness of predictions at test time. While the reported experiments are encouraging, it's not clear why the proposed method works and there's a severe lack of analysis (both experimental and theoretical).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
