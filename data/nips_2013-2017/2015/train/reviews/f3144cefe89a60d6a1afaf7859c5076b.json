{"title": "The Poisson Gamma Belief Network", "abstract": "To infer a multilayer representation of high-dimensional count vectors, we propose the Poisson gamma belief network (PGBN) that factorizes each of its layers into the product of a connection weight matrix and the nonnegative real hidden units of the next layer. The PGBN's hidden layers are jointly trained with an upward-downward Gibbs sampler, each iteration of which upward samples Dirichlet distributed connection weight vectors starting from the first layer (bottom data layer), and then downward samples gamma distributed hidden units starting from the top hidden layer. The gamma-negative binomial process combined with a layer-wise training strategy allows the PGBN to infer the width of each layer given a fixed budget on the width of the first layer. The PGBN with a single hidden layer reduces to Poisson factor analysis. Example results on text analysis illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the PGBN, whose hidden units are imposed with correlated gamma priors, can add more layers to increase its performance gains over Poisson factor analysis, given the same limit on the width of the first layer.", "id": "f3144cefe89a60d6a1afaf7859c5076b", "authors": ["Mingyuan Zhou", "Yulai Cong", "Bo Chen"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The authors propose a multilayer representation of count vectors using a hierarchical model. The model has hidden layers consisting of gamma variables drawn from distributions with factorized shape parameters.\n\nThe proposed Gibbs sampling based inference is capable of learning the widths of the hidden layers, with the first-layer width acting as limit. Using this, the experimental results show that a deeper network has better classification accuracy and perplexity than a single-layer model (which is equivalent to the model of [12]). The superior performance is attributed to capturing correlations between hidden units and modeling overdispersion.\n\nThe model and algorithm are clearly presented.\n\n The paper draws out the advantage of the multilayer over the single layer model -- equivalent to a nonparametric extension to the PFA presented in Zhou et al. (2015) -- and the experimental results are convincing.\n\n While the model is novel -- using nonnegative hidden units instead of binary, and automatically learning the widths -- the advantage over the leading competing method, that of the over-replicated softmax [21] -- a two layer DBM model -- is not addressed in any depth. The authors note that\n\nthe classification accuracy is worse than the over-replicated softmax [21] when the first-layer budget is 512. This is noted to possibly due to word preprocessing.\n\n Further, on pg 8., the authors draw observations about layer width decay rates, but these come from inference on a single data set. To what extent is this a function of the data vs. the budget of the first-layer? The claim on pg.2 that you reveal this relationship surely needs analysis on more than 1 dataset.\n\n  A novel multilayer model of count vectors with nonnegative hidden units and Gibbs sampling-based inference is proposed. The merits over a (possibly wider) single-layer model has been clearly presented, and the experimental results are convincing. However, the paper does not demonstrate that the model is the state-of-the-art in a domain (e.g., topic modeling); neither does it demonstrate merits over the leading competing method -- the overreplicated softmax model of Srivastava et al. (2013), a deep Bolztman machine with binary hidden units.Further, the authors draw conclusions from results on a single data set.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Acknowledge the author's rebuttal.\n\nI overall maintain my general sentiment about the paper and look forward to more discussion of this work's relationship to traditional neural / deep networks.\n\n---\n\nIn this paper the authors present a deep belief network in which the intermediate hidden layers are represented by nonnegative weights.\n\nThey apply this model to text documents as a \"deep belief topic model\" (my own phrasing).\n\nThey derive an inference / update step, in which dirichlet vectors are propagated up the network and gamma weights are propagated back down it, and perform an empirical evaluation of the model.\n\nOverall this paper was written fairly clearly.\n\nI thought the paper was mostly fine, but I believe that the empirical analysis could be improved (more on that shortly).\n\nI also would be shocked if nobody has tried to implement a neural network with weights outside the range [0, 1] before; it would be good to see some background material discussed even if it wasn't used to motivate this model.\n\nI also had a few other minor comments:\n\n - It's not clear whether a fixed budget is appropriate for fixing the size of the various layers (and, in the absence of more detail about the method, I am skeptical).\n\nIt still seems that these layers could easily be too large or too small despite the fixed budget approach.\n\nI suspect that some held-out subset of data could be used to decide whether to grow or shrink the model.\n\n - It's not clear whether adding any layers beyond the second layer help.\n\nThe performance in the figures indicates that they're marginally better, but it's difficult to be confident given how much variance there is in the plots (with the possible exception of Figure 3).\n\nFurther, the topic descriptions from sample words (line 417) aren't substantially different; it's very possible that the authors' observation that the topics are more specific at the top layer are simply because that layer is largest.\n\n - The authors describe a process to jointly train all layers in contrast to training in a greedy fashion (line 264).\n\nIn this respect it resembles the way a traditional neural network (from 20 years ago) was fit, i.e., forward/back propagation, except that it is with samples.\n\n - It seems that this paper could introduce the idea of the Gamma-Poisson belief network without needing to use an application which uses the CRT (i.e., the application itself is complicated, and it's not clear that this paper necessitates a complicated application to evaluate a neural network architecture.\n\nRegarding the experimental validation, I believe it could be improved.\n\nFirst, it would be useful to see more comparisons with baselines.\n\nFor the classification task, for example, it would be good to see an SVM or ridge regression using word counts as covariates, which I have seen outcompete topic models for classification. (N.B. I don't think the model needs to beat these baselines).\n\nIt would be good to see some comparison with e.g. a typical neural network, in which weights are in [0,1], perhaps with the bottom layer adapted to suit the Poisson/Multinomial observations.\n\nParticularly, claims like that on 145 (\"..clearly shows that the gamma distributed nonnegative hidden units could carry richer information than the binary hidden units and model more complex nonlinear functions\") could an should be backed up with a comparison with a model using binary hidden units.\n\nIt's not clear that a binary network couldn't express similar nonlinear functions with the right architecture.\n\nIs it necessary to remove stopwords?\n\nTopic models like LDA can handle them fine if you're okay with a stopword topic (and it's not clear why that would hurt an evaluation).\n\nNit picks:\n\n - The authors discuss classification in the experiment section (line 349) before the actual task is introduced in 353, which was a bit confusing.\n\n - Line 56: \"budge\" -> \"budget\" This paper offers an interesting variant of a typical deep network.The idea is interesting, and the presentation is mostly clear, but the experimental validation could be improved.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: A poisson gamma hierarchy is presented for modeling counts data.\n\nThe primary contribution lies in utilizing a hierarchy of gamma distributions and using recently proposed clever augmentation tricks to develop a simple, tractable Gibbs sampler in spite of non conjugacy. Coupled with a Poisson likelihood, the authors demonstrate the utility of the hierarchy for modeling counts data.\n\n The paper is technically sound. The problem of extracting unsupervised multilayered representations of data is interesting, and the developed model provides an interesting alternative for counts data.\n\n Detailed comments:\n\n1) While it is true that RBMs have traditionally used binary hidden units, recent work [1] has found that using rectified linear nonlinearities leads to better representations. The non linearities induced by the proposed gamma units appear to subsume the linear regime of rectified linear nonlinearity (with the linear regime being recovered in expectation, when the expected rate parameter is 1). This is cool and it would be nice to discuss this connection more explicitly in the text.\n\n2) Scalability appears to be a big concern for the inference procedure. It isn't completely clear, but it appears that a 1000 Gibbs sampling iterations are run after adding each new layer, sweeping over all the variables in the network. This would be difficult to scale to deeper architectures. How long does it currently take to train the 5 layer networks employed for multi-class classification? A discussion of these computational issues would be useful.\n\n3) The external classification comparisons, to DocNADE and over replicated softmax are sloppy. The classification numbers are not really comparable, considering the competing algorithms are trained on distinct vocabularies. The paragraph on qualitative analysis is not useful at all to the reader and should be reworked. The authors claim that the discovered topics specialize as one traverses the hierarchy downwards, but present very little evidence in support of this claim. It would be interesting for the reader to explore the per layer discovered topics more closely and they could be made available in a supplement. The authors also claim, without supporting\n\nevidence, that they can generate interpretable synthetic documents from the trained network. It would be interesting to see these synthetic documents generated by the network. Again something that could be easily provided in an appendix.\n\n[1] Nair, Vinod, and Geoffrey E. Hinton. \"Rectified linear units improve restricted boltzmann machines.\" Proceedings of the 27th International Conference on Machine Learning (ICML-10). 2010.  Overall, this is an interesting paper. I would have scored the paper higher, if not for sloppy experimental evaluations.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
