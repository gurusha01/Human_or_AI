{"title": "Skip-Thought Vectors", "abstract": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.", "id": "f442d33fa06832082290ad8544a8da27", "authors": ["Ryan Kiros", "Yukun Zhu", "Russ R. Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "To clarify after reading author response: the pre-trained sentence vectors are going to be easy to use and thus will be most certainly useful. What I meant was that to retrain the model would not be as easy as using, say, word2vec.\n\nThe package shared on GitHub does not seem to include the training part yet. The paper extends an interesting application of the sequence to sequence models by borrowing the idea of skip-gram, using the current sentence to predict the sentence before and after. Although the experiments do show the potential of this model, the approach didn't really outperform the state-of-the-art on any of the tested cases. Given that the model takes a lot of time to train, it might be difficult for users to retrain the vectors using their, domain-specific, corpus.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose an encoder-decoder approach to learning vector representations of entire sentences. In much the same way that CBOW exploits the distributional semantics of words, the hope is that in natural discourse, sentences will obey similar distributional properties. If the hypothesis holds then training a sentence to predict its neighboring sentences would produce good learned representations of sentences. This approach is particularly appealing, because like CBOW/SkipGram, the model can be trained in an unsupervised fashion using the data itself to constrain the weights.\n\n They also propose and study a particular embodiment of this idea in which they employ gated RNNs for encoding and decoding functions. As noted by the authors, similar models have been used for machine translation, and you can think of this model as \"translating\" a current sentence into a previous and next sentence. The authors evaluate the model on eight different tasks, showing the learned embeddings (the hidden states of the encoder) are semantically meaningful and useful for training classifiers (competitive or better than many existing systems).\n\nOverall, the paper is well written, but I have a few comments:\n\n*In the experimental results section, it's not always clear if the number reported for a system is from the literature or if it's reproduced from scratch. For example, in the first results table (Table 3), it's clear that the numbers from the challenge are ones reported in the literature, but in Table 6 it's not clear if paragraph vector was re-trained on the same data as the skip-vector model (the book data), or if the reported number is from the literature.\n\n*Another potential weakness of the particular gated RNN implementation of skip-thoughts is that compared to paragraph vectors, it is much more difficult to train (more than a week to train?). While I can see that paragraph vector requires inference to embed new sentences, is this inference really that much more expensive than having the gated RNN consume the sentence one token at a time?\n\n*It might be worth extending Table4 with additional sentence pairs that show the cases in which skip-thoughts improve performance over the baseline. A proxy for this would be to search for sentences with poor lexical similarity, but high semantic similarity (both predicted and true semantic similarity). A well written paper with a novel contribution and thorough experimental evaluation against numerous baselines on eight different tasks.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I think the idea of extending distributional methods from words to sentences is great.\n\nThe experiments look carefully done and they compare on a large variety of different problems.\n\n This paper introduces an unsupervised way to train sentence vector representations. The idea is to take GRU to encode a sentence and then predict words of surrounding sentences.I like this paper a lot. I would strongly encourage to change it to skip-sentence vector though, sentence /= thought ;)", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors present a technique inspired by the Skip-gram model of Tomas Mikolov to encode a sentence representation. The representation is the last layer in the sentence sequence of a GRU RNN. From this representation, the model is trained on a Billion words corpus (extracted from books) to generate the previous and next sentence in a paragraph as a GRU RNN Language Model conditioned on the encoded representation. Even if the model is trained on a relatively small vocabulary (20K), they present an easy way to extend it to 1M using word2vec and a linear mapping. They present an extensive set of experiments across many tasks for NLP, using the raw sentence representation as feature.\n\nI really liked the paper and I think it is a good starting point for upcoming research in learning sentence representation, potentially learning representation at the paragraph level. Potential criticism of the paper would be that there is no new state of the art from the method but the research community knows how hard it is to find a representation that generalizes well across a wide range of NLP tasks.\n\nThe only experiment missing in my opinion would be to fine-tune through the GRU RNN from the sentence representation on a few datasets described in the paper to verify how far the obtained improvement would be from the state of the art. Regarding the bidirectional model, do you have results with a single model that would take the forward and backward pass rather than concatenating the representation?\n\n minor comments: 2nd 3rd paragraph in Introduction - Fig 2 and Table 2 should be replaced by Fig 1 and Table 1 Sec 1 last paragraph 'such an *experimental* is begin' Sec 3.2 2nd paragraph 'learned representation *fair* against heavily'\n\ntraining details: how many words per seconds the model is able to process for training using adam? how many passes thourgh the whole dataset do you perform in the 2 weeks training? I really liked the paper and I think it is a good starting point for upcoming research in learning sentence representation, potentially learning representation at the paragraph level.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "TL; DR This paper presents a natural generalization of neural word embeddings: sentence embeddings! The training objective of the proposed \"skip-thoughts\" model is to predict the previous and next sentence given an encoding of the current sentence. Thus, it may be trained without supervision from raw text, and the resulting embeddings used as \"features\" for downstream tasks. The authors present experiments on 8 tasks, and while the results of the proposed method are not state-of-the-art, it outperforms many previous baselines with less feature engineering.\n\nThe proposed \"skip-thoughts\" model is a fairly ingenious application of the recently popularized sequence-to-sequence RNN framework. It's been known for a long time now (e.g., Ando & Zhang, 2005) that learning representations of observed data X via auxiliary tasks can be helpful in downstream tasks, in which one is interested in making predictions Y. However, doing so at the sentence level is a nice twist, though not an entirely novel one (e.g. the cited paragraph vector work, and earlier work cited therein). I think the technical approach in the paper, being largely derived from existing RNN frameworks, is sensible. I also liked the vocabulary expansion strategy.\n\nI do have some concerns regarding the experimental evaluation. The main question in my mind is if the proposed approach is a better way of learning from a big pile of unlabeled data X than alternatives, in terms of downstream performance on predicting Y. Unfortunately, I don't believe this question is addressed in the experiments; there is no baseline which uses to the same X (the Book11K corpus). Thus, it is not clear if the reported results are simply as a result of (a) using a lot more data, or (b) the model is capturing something interesting beyond superficial word co-occurence statistics.\n\nI would have liked to see a comparison to a strong baseline which uses the same training data. Short of this, it would have at least been good to see a *weak* baseline, e.g. a bag-of-words model. Yet another option would have been to train word embeddings on Book11K, and use these in some of the more engineered baselines so that at least the data conditions are matched.\n\nOne common issue with RNNs in the encoder-decoder framework is the encoder must summarize the entire input in a single vector. One heuristic that has been used to alleviate this problem is to reverse the order of the tokens. More recently, attention models have been used. I would have liked to see a little discussion of this issue. In a similar vein, I would have liked to see more details about:\n\n-how the proposed approach was tuned, and how sensitive it is to hyperparameters\n\n-what alternate architectures were used, if any. For instance, an obvious alternative is to only predict the next sentence given the current sentence (it's not 100% obvious why it makes sense to predict sentences in reverse, even though I'm willing to believe it's helpful as it doubles the amount of data).\n\n-how performance improves with increasing amounts of unlabeled data X (presumably could be derived from model checkpoints from a prior training run).\n\nUPDATE AFTER AUTHOR RESPONSE:\n\nIncluding a baseline that uses word-level embeddings trained on Book11K would improve the paper. However, it seems to me that the natural baseline to compare to is the paragraph vector model [1], with matched train and test conditions. I don't understand why this comparison wasn't made; perhaps this should be clarified in the paper.\n\n[1] http://arxiv.org/pdf/1405.4053v2.pdf\n\n The proposed \"skip-thoughts\" model is a fairly ingenious application of the recently popularized sequence-to-sequence RNN framework. However, I worry that the experimental evaluation focuses on conditions in which there is a data mismatch between the baselines and the proposed approach.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
