{"title": "Expressing an Image Stream with a Sequence of Natural Sentences", "abstract": "We propose an approach for generating a sequence of natural sentences for an image stream. Since general users usually take a series of pictures on their special moments, much online visual information exists in the form of image streams, for which it would better take into consideration of the whole set to generate natural language descriptions. While almost all previous studies have dealt with the relation between a single image and a single natural sentence, our work extends both input and output dimension to a sequence of images and a sequence of sentences. To this end, we design a novel architecture called coherent recurrent convolutional network (CRCN), which consists of convolutional networks, bidirectional recurrent networks, and entity-based local coherence model. Our approach directly learns from vast user-generated resource of blog posts as text-image parallel training data. We demonstrate that our approach outperforms other state-of-the-art candidate methods, using both quantitative measures (e.g. BLEU and top-K recall) and user studies via Amazon Mechanical Turk.", "id": "17e62166fc8586dfa4d1bc0e1742c08b", "authors": ["Cesc C. Park", "Gunhee Kim"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper deals with a problem of retrieving natural sentences that describe a given sequence of images and forming a sequence of sentences. This problem is definitely significant in the research fields of computer vision and pattern recognition, but it is closely related to generating multiple sentences that describe given video content (e.g. [Rohrbach+ 2014]): Single video content can be regarded as a sequence of images, since video content can be separated into multiple video shots and every shot can be well described by a key frame. However in this paper, none in this line of researches have not been cited.\n\nThis paper is well written and easy to follow. The problem is well stated and solved with a set of solid techniques.\n\nThe proposed neural network model named CRCN acquires relationships between a sequence of natural sentences and a sequence of images. The architecture of the proposed model is technically sound and it also describes discourse relationships very well with the help of entity-based coherence models.\n\nThe method for generating a sequence of natural sentences is reasonable but rather primitive: Every natural sentence is directly associated with training images similar to a given image, and all the sequences are simply concatenated in the order of the corresponding images.\n\n In Section 4.1, \"We reuse the blog data of Disneyland from the dataset of [11], and newly collect the data of NYC, using the same crawling method with [11],\" The data set has not been disclosed and the corresponding paper does not describe the details how to crawl the data set. This indicates that the authors of this paper is definitely almost the same as the ones in [11] and thus this situation deliberately lacks the anonymity.\n\nEquation (2) might be incorrect, since it implies that s_t can be derived from only o_t and thus the network is not fully connected. The proposed neural network model is technically sound and it also describes discourse relationships well with the help of entity-based coherence models. Meanwhile, the method for generating a sequence of sentences for a given image stream is rather ad-hoc. I think that this paper can be accepted as a poster paper as is.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper attacks the problem of describing a sequence of images from blog-posts with a sequence of consistent sentences. For this the paper proposes to first retrieve the K=5 most similar images and associated sentences from the training set for each query image. The main contribution of the paper lies in defining a way to select the most relevant sentences for the query image sequence, providing a coherent description. For this sentences are first embedded in a vector and then the sequence of sentences is modeled with a bidirectional LSTM. The output of the bi-directional LSTM is first fed through a relu and fully connected layer and then scored with a compatibility score between image and sentence. Additionally a local coherence \u001amodel [1] is included to enforce the compatibility between sentences.\n\n Strength / positive aspects: - The paper proposes a novel and effective architecture to retrieve coherent sentences for an image sequence. - The paper provides an extensive quantitative, qualitative, and human evaluation, showing the superiority of their approach against several baselines, not using coherence. They also provide an ablation experiment, removing the coherence \u001amodel [1]. - The authors promise to release source code and dataset.\n\n Weaknesses / Questions / Unclarities: 1. Line 94: The paper claims that there is \"no mechanism for the coherence between sentences\" in [5]. Although not the contribution of [5], [5] predicts an intermediate semantic representation of videos, which is coherent across sentences by modeling the topic of the multi-sentence description. 2. Correctness/clarity: Figure 2b does not seem to correspond to the description 3.3/Equation (2). While Figure 2b implies that the fully connected layer are connected to all sentences, this it not the case in Eq2, which implies that the parameters are shared across sentences, but only connected to the vector representing a single sentence. 3. A better metric to automatically evaluate the generated sentences is Meteor (http://www.cs.cmu.edu/~alavie/METEOR/) instead of BLEU, especially if there is only a single reference sentence. 4. Why two linear functions in Eq2 (W_{f2}, W_{f1}) are applied behind each other? Given that two linear functions are again a linear function the benefit is unclear. An ablation study showing the benefit of these functions would be interesting. 5. Why the same parameters of the fully connected layers are used for the BRNN output (o_t) and the local coherence model q (Equation 2)? 6. Is the paragraph vector [16] fine-tuned or kept fixed?\n\n === post rebuttal === After reading the rebuttal I recommend the paper for acceptance. The authors successfully addressed issues with the formulation, evaluation, and related work.\n\nPlease make the promised changes to the final and also clarify the following point in the final. 6. Is the paragraph vector [16] fine-tuned or kept fixed? The paper proposes an interesting new model to retrieve coherent sentences for an image stream, which is convincingly evaluated. However, to be a convincing paper, several clarifications have to be made.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This work studies how to generate sentences from an image stream. It designs a\n\ncoherent recurrent convolutional network (CRCN), which consists of convolutional neural networks, bidirectional recurrent neural networks, and an entity-based local coherence model.\n\nOverall it is a nice work, although it can be improved from the following aspects: * Several related work about video to sentence is missing, e.g.,\n\nJointly modeling deep video and compositional text to bridge vision and language in a uni\u001aed framework\n\n *While the quantitative results of the proposed method looks quite good, the user study in table 2 shows that it performs similar to one baseline, RCN. Significance test is needed to verify whether the improvement is reliable.  Nice algorithm for sentences generation from an image stream.The quantitative results of the algo looks good but the user study only shows weak advantage over baselines.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
