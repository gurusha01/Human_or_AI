{"title": "LASSO with Non-linear Measurements is Equivalent to One With Linear Measurements", "abstract": "", "id": "2d1b2a5ff364606ff041650887723470", "authors": ["CHRISTOS THRAMPOULIDIS", "Ehsan Abbasi", "Babak Hassibi"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Answer to authors' response:\n\nThe reviews of other reviewers and the authors' response have not significantly affected my opinion of the paper. That said, I am not familiar with the single index model/sufficient dimension reduction literature, so I could very well be unaware of previous work that may be relevant. The authors derive asymptotic guarantees for the generalized square-root lasso applied to Gaussian measurements that are (optionally) transformed by a possibly unknown nonlinear function. The results are very interesting, as they provide a precise characterization of the performance of the method, and the paper is very well written (although there are quite a few typos. especially in the proof section).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The article extends the work on precisely characterizing the statistical performance of the l2-lasso to single-index models. The article also relaxes the usual Gaussian assumption on the prior (the distribution of x_0). It is a worthwhile contribution to the literature on precise asymptotic behavior of lasso-type estimators.\n\nThe article is well-written. However,\n\n1. as pointed out by Brillinger (1982), the single index model is equivalent to a linear model with scaled unknown signal and non-standard noise. The connection is desribed rigorously in Appendix A, but it should also be mentioned in the main article to heuristically justify the statement in the blockquote on line 140.\n\n2. the article focuses on the l2-lasso, also called the square root-lasso. The main benefit of the l2-lasso over the popular l2^2-lasso is the ``oracle'' value of the regularization parameter has no dependence on the (usually unknown) noise variance (sigma). A practical question that the results could shed light on is whether this justification for the l2-lasso over the l2^2-lasso is valid.\n\n The article extends the work on precisely characterizing the statistical performance of the l2-lasso to single-index models. It is a worthwhile contribution to the literature on precise asymptotic behavior of lasso-type estimators.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: This paper considers the Generalized Lasso with non-linear measurements and Gaussian design. A known heuristic often used is that non-linear observations may be treated as noisy linear observations. Capitalizing on the work of Plan and Vershynin [17], the authors here extend the results of [17], and by moving to an asymptotic regime, they provide novel precise explicit expressions of the error estimation. The accuracy of their predictions is confirmed with many special cases (including the Lasso, the group Lasso and the nuclear norm). The application to the design of optimal quantizers in the context of q-Bit CS and its relation to the Lloyd-Max quantizer is very insightful.\n\n Quality: This is a nice piece of work.\n\n Clarity: The paper is quite well-written, though the treatment is somewhat unbalanced. The lack of space however prevents including all the necessary details.\n\n Originality: The paper builds upon the work of Plan and Vershynin [17] and expands in several aspects that are novel. For instance, the explicit expressions of the performance estimation error are new.\n\n Comments:\n\n 1) Page 4, line 193: the regularized and penalized versions are equivalent with appropriate correspondence.\n\n 2) Caption of Figure 1: 'n=768' shouldbe '$n$=768'.\n\n 3) Page 5, line 247: a typo at 'When ...'.\n\n 4) Page 5, line 294: what the authors call the proximal function is (very well) known in convex analysis as the Moreau envelope of index \\tau. The notation used there is awkward as prox is generally used for the proximal point/operator but not for the Moreau envelope. I suggest changing it.\n\n 5) Page 5, Assumption 1: I think it would instructive to give this assumption a more formal interpretation using results from variational analysis.\n\n 6) The authors should avoid calling the generalized Lasso an algorithm in the text. It is certainly a (convex) program (or decoder to borrow the terminology widely used in CS), but not an algorithm.\n\n 7) Experiments: the authors show the results when applying the generalized Lasso to the non-linear measurements, and how the theorems predict the estimation error. This paper considers the Generalized Lasso with non-linear measurements and Gaussian design are proves that asymptotically, the estimation performance of the Generalized Lasso with such measurements is the same as if the measurements were linear and noisy (Gaussian). The authors provide novel precise explicit expressions of the estimation error, and confirm their findings with many special cases (including the Lasso, the group Lasso and the nuclear norm). The application to the design of optimal quantizers in the context of q-Bit CS and its relation to the Lloyd-Max quantizer is very insightful.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers about linear estimation with nonlinear link function. Inspired by the recent work of Plan and Vershynin, it studies the asymptotical estimation error using generalized lasso for arbitrary link function with \\mu \\ne 0. It turns out that no matter the link function is linear or nonlinear, the asymptotical error is exactly the same as shown in theorem 2.1.\n\n Theorem 2.1 holds when the linear measurement vector is sampled from normal distribution, which might be strict in some cases. The author might need to explain why this assumption is necessary for the conclusion to hold.\n\nA few questions: 1. How to choose regularizer based on set \\kai in which x_0 lies? While the assumption of convex regularizer is discussed in this paper, it seems the aforementioned problem is not addressed yet. 2. While the result shows nonlinear is equal to linear measurement, but the conclusion involves with \\mu and \\sigma which characterizes the hardness of estimation from nonlinear function g. I would like to know if the authors have any thought on the necessity of these dependences.\n\nThere are some references missing. 1. Single index model and sufficient dimension reduction study linear vector/subspace estimation under unknown link function. However, the paper does not review any paper in this area. 2. A recent paper \"Optimal linear estimation under unknown linear estimation\" by Yi et al. introduces new algorithm for sparse recovery even with link function that has zero \\mu. The paper establishes the equivalence of high dimensional linear estimation with linear and nonlinear link function under proper conditions. The conclusion is supported by careful theoretical proof and empirical results. This is a well written paper with fairly interesting results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
