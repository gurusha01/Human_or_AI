{"title": "Principal Differences Analysis: Interpretable Characterization of Differences between Distributions", "abstract": "We introduce principal differences analysis for analyzing differences between high-dimensional distributions. The method operates by finding the projection that maximizes the Wasserstein divergence between the resulting univariate populations. Relying on the Cramer-Wold device, it requires no assumptions about the form of the underlying distributions, nor the nature of their inter-class differences. A sparse variant of the method is introduced to identify features responsible for the differences. We provide algorithms for both the original minimax formulation as well as its semidefinite relaxation.  In addition to deriving some convergence results, we illustrate how the approach may be applied to identify differences between cell populations in the somatosensory cortex and hippocampus as manifested by single cell RNA-seq. Our broader framework extends beyond the specific choice of Wasserstein divergence.", "id": "83fa5a432ae55c253d0e60dbfa716723", "authors": ["Jonas W. Mueller", "Tommi Jaakkola"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper give a rather original approach to characterizing differences between two distributions (e.g. populations) by linear projections. Experiments on synthetic and real data seem to support the conclusions. Looks like a very solid and original paper.\n\n Unfortunately I have low confidence for this review mainly due to my ignorance of relevant literature. Interesting and algorithmically sophisticated paper on a method trying to find differences between populations, going beyond mere classification.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors proposed a new dimensionality reduction method that finds the most different direction between input X and Y. The novelty of the proposed method is to use the squared Wasserstein distance as discrepancy measure and can be solved by semidefinite programming. Through experiments, authors showed that the proposed method compares favorably with existing methods.\n\nQuality: The method technically sounds.\n\nClarity: The paper is well written and easy to follow.\n\nOriginality: The approach is new. The problem in this paper is similar to the one in Transfer component analysis, which finds a subspace that have small discrepancy between two datasets.\n\nSignificance:\n\nThe formulation with Wasserstein distance + SDP is interesting. Thus, it would have some impact in ML community.\n\nDetailed comments: 1. The problem can be easily formulated by using simple Lasso. For example, if we add positive pseudo labels for X and negative pseudo labels for Y and solve the problem ||Y - Z^t \\beta||_2^2 + \\lamda ||beta||_1, you may be able to obtain similar results. Actually, this approach only useful if X and Y are linearly related, thus it can be a good baseline.\n\n2. Is it possible to extend the algorithm to nonlinear case? 3. For the problem, transfer component analysis can be used to find a most different direction. (Although TCA was originally proposed for finding common subspace, it can be easily applied for your task). http://www.cse.ust.hk/~qyang/Docs/2009/TCA.pdf\n\n The proposed formulation is interesting. If author add can add simple Lasso based baseline, it would be a plus.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The proposed approach is motivated by the \"Cramer-Wold device\", which ensures the existence of a linear projection that differentiates two distributions. The authors apply the Wasserstein metric directly on samples from both distributions, and show favorable theoretical properties of such an approach under reasonable assumptions (such as bounded domain variables). The analysis is extended to distributions that differ only in a few dimensions, by considering sparse projections. The resulting computational problem turns out to be non-convex. The authors propose a semidefinite program relaxation and further pre-processing to estimate a solution. Empirical evaluation is provided on simulated and real datasets, including an application to bioinformatics.\n\nThe paper is well written and clearly lays out the main ideas. The paper dresses a problem of interest to the community using a novel technique and sufficient theoretical and empirical validation.\n\n Minor issues: - I would encourage the authors to include more description and legends in each figure so they are more self contained and simplify interpretation. - the term \"Cramer-Wold device\" seems to be fairly non-standard outside of the reference [10]. Would the authors prefer the more standard \"Cramer-Wold theorem\"?\n\nSuggestions for future work: The presented theoretical results are concerned with the sample complexity as compared to the optimal linear projection. It would also be useful to characterize the relationship between the induced divergences and more standard divergences computed directly between the high dimensional distributions, as this would help clarify the convergence in distribution implied by the Cramer Wold Theorem. The authors propose a technique to differentiate between distributions, based on the maximal univariate Wasserstein divergence between projections of the underlying random variables. The paper dresses a problem of interest to the community using a novel technique and sufficient theoretical and empirical validation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes to find sparse projection bases that maximise the Earth Mover's Distance (EMD) between distributions. To the\n\nbest of my knowledge component analysis using EMD has received limited attention (except [A] for NMF). A non-convex optimisation problem is formulated and solved via a kind of alternating optimization. Some experiments (which importance I cannot fully evaluate) show quite some improvement over the state-of-the-art.\n\n [A] Sandler, Roman, and Michael Lindenbaum. \"Nonnegative matrix factorization with earth mover's distance metric for image analysis.\" Pattern Analysis and Machine Intelligence, IEEE Transactions on 33.8 (2011): 1590-1602.\n\nAfter reading the rebuttal I believe the paper is of NIPS quality and recommend acceptance. The paper proposes to find sparse projection bases that maximise the Earth Mover's Distance (EMD) between distributions. To thebest of my knowledge component analysis using EMD has received limited attention. An alternating optimisation procedure is proposed to find the bases. Some experiments (which importance I cannot fully evaluate) show quite some improvement over the state-of-the-art. I recommend acceptance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
