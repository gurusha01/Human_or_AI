{"title": "Efficient Learning of Continuous-Time Hidden Markov Models for Disease Progression", "abstract": "The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time. However, the lack of an efficient parameter learning algorithm for CT-HMM restricts its use to very small models or requires unrealistic constraints on the state transitions. In this paper, we present the first complete characterization of efficient EM-based learning methods for CT-HMM models. We demonstrate that the learning problem consists of two challenges: the estimation of posterior state probabilities and the computation of end-state conditioned statistics. We solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden Markov model. The second challenge is addressed by adapting three approaches from the continuous time Markov chain literature to the CT-HMM domain. We demonstrate the use of CT-HMMs with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an Alzheimer's disease dataset.", "id": "a591024321c5e2bdbd23ed35f0574dde", "authors": ["Yu-Ying Liu", "Shuang Li", "Fuxin Li", "Le Song", "James M. Rehg"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper presents several EM-style algorithms for learning Continuous-Time Hidden Markov Models (CT-HMMs) and illustrates them on two disease evolution prediction tasks (Glaucoma and Alzheimer's). The paper motivation is nicely laid out and this framework is indeed more realistic than the usual discrete-time HMM in practical applications such as medicine and customer interaction. The framework of CT-HMM is clearly explained and the derivations of the algorithms appear to be correct. From a theoretical point of view, the approach is straightforward but the technical details for getting the algorithms are not trivial, so this makes a nice theoretical contribution. In terms of the empirical evaluation, it is nice that the authors work with real data sets, and this strengthens the paper. However, on the Glaucoma dataset, the comparison methods are very weak (linear and Bayesian linear regression). The more interesting comparison would be against a discrete-time HMM (one which ignores the elapsed time between visits). This comparison should be added to the paper.\n\nThe Alzheimer;'s results are more qualitative but it is nice to have a second task. Sec. 5 needs to be revised from the point of view of the writing, as it has a lot more unclear sentences and grammar errors than the rest of the paper. As a side note, using only exponential distributions for the transitions times, while standard in CTMCs, is not always best in practical terms - mixtures may be needed. This might be interesting for future work. The authors derive EM-style algorithms for learning Continuous-Time Hidden Markov Models (in which state transitions may occur between observations). The paper makes a nice theoretical contribution. The empirical evaluation could be improved, but the use of real data makes the work worthy of publication.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I wonder what trade-off the discretized time interval (Sec 3.1) introduces, is this exact? Analogously, the same discretization in time can be applied to CTMC, what will this do?\n\nThe writing of this paper is quite dense overall.\n\nI feel that too much space is spent explaining prior art (until line 235, 4.5 pages into the paper).\n\n In sec 5.1 results with simulated data - the mean of the observations from each state is 1.0 apart, with standard deviation \\sigma=0.25. With states 4\\sigma apart, the results from this section has hardly any noise. I wonder how this result helps show the efficacy of the proposed scheme, as opposed to CTMC.\n\n Furthermore, the error metric \"2-norm error\" is not defined, assume this is on the state sequence? as said above, where could the errors in states have come from?\n\nIn section 2 background, it is better to clarify the contributions of [8,9,10,12,13] relative to each other, it will help the reader conceptually appreciate this topic more.\n\n One recent paper should have been cited and commented on: V. Rao and Y. W. Teh. Fast MCMC sampling for Markov jump processes and extensions. Journal of Machine Learning Research, 13, 2014. This paper proposes two inference schemes for continuos-time hidden markov models (CT-HMM) by extending recent inference methods for continuous-time markov chains (CTMC). The algorithmic contribution is a little bit novel. Applications relevant and somewhat novel (to the NIPS community, from disease progression modeling).The main concerns are in writing quality and quality of results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper presents new methods to learn parameters and then perform inference (either Viterbi or forward/backward) for continous time HMM models (CT-HMM) with non uniformly distributed time points. They solve the inference problem for the transition rate matrix Q by constructing a discrete time inhomogeneous HMM then applying two recently developed methods to get sufficient statistics for their EM (Expfm, Unif - from Hobolth & Jensen 2011). They follow with a detailed time complexity analysis for the different flavors of the their algorithm (Table 1) before proceeding to experimental evaluation. Experiments include a 5 state synthetic dataset and two disease progression datasets for Glaucoma and AD.\n\nPros: The paper is mostly well written and the problem addressed is interesting in terms of the computational challenge, the modeling aspects, and the biomedical domain. The authors provide mathematical foundation for their approach building on previous work by\n\nHobolth & Jensen 2011 for the specific task at hand. The authors do a good job explaining the biomedical datasets/questions and relating their results to what is known about the disease.\n\nCons: 1. The synthetic data evaluation is not sufficient. Why 5 states? 10^6 samples seems an awful lot of samples to train on an not representative of real life problems. The difference between states emission (line 319-320) seems too easy? That may explain why all their algorithm's variants seem to preform similarly. Harder settings and varying different parameters may flush out differences better and give a bette sense of how the algorithm can perform on real life data. 2. Sec. 5.2 is rather cryptic, especially the description of the competing methods. Consider adding details about those in the appendix to make this section more clear.\n\nMinor comments: 1. Fig2 is not self contained. Referring to it first (line 40) is premature. 2. Line 91 \"the rate the process\" 3. Line 161 \"in only at\" 4. Sec 5.2: the claim about binary search (lines 348-350) is not clear. 5. Line 399 \"to shown\"  The paper develop new methods to efficiently perform parameter learning and subsequent inference for CT-HMM, then apply the method to two disease progression datasets.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper builds upon previous EM method for CTMC, and tackles the conditional statistics borrowing some recently developed tools. It makes a detailed comparison over a few inference methods, in terms of accuracy and complexity.\n\nQuality: a. For experiments, the baselines chosen are too week since they do not consider state transition/trajectory information. It is natural to compare with discrete time HMM approaches where you can set discretize the time horizon by setting a proper bin size.\n\nb. Related to a., considering the time complexity to do inference on CT-HMM, why not simply discretize the time horizon? If the sampling rate is irregular, we can consider a proper bin size: if bin size is small, we have 1 or 0 (missing) observation in each bin; if bin size is large, we have >=1 observations in each bin and take a local average. I expect in most problems, this method works just as fine. Have you considered this approach for your dataset? c. For CT-HMM, other inference methods like sampling also exist, i.e., Fast MCMC sampling for Markov jump processes and extensions, Rao et al., JMLR 2013. Have you also compared with such approaches?\n\nClarity: The paper is clearly written.\n\nOriginality: This is novel in the sense it provides an EM-based approach for CT-HMM, although the tools/building blocks used were proposed in previous work.\n\nSignificance: The paper proposes an EM-based inference method for CT-HMM. If the author can release their software package, it will be a useful toolbox for the community. This work presents an EM method for CT-HMM model, using some of recently developed tools. Though incremental, it provides an inference method for CT-HMMs.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
