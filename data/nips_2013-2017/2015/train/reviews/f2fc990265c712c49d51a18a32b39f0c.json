{"title": "HONOR: Hybrid Optimization for NOn-convex Regularized problems", "abstract": "Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient \\underline{H}ybrid \\underline{O}ptimization algorithm for \\underline{NO}n convex \\underline{R}egularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2)  We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.", "id": "f2fc990265c712c49d51a18a32b39f0c", "authors": ["Pinghua Gong", "Jieping Ye"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper presents an algorithm HONOR applicable for a wide range of non-convex sparse learning formulations.\n\n The HONOR method combines a Quasi Newton step with a standard GD step for a first order approximation of the\n\nobjective without evaluating the actual Hessian for the QN step and using L- BFGS to scale it up for large scale objectives. The authors show that any limit point the algorithm might converge to is a Clarke critical point of the objective and the sequence generated by the objective leads to a limit point.\n\nThe convergence for non-convex problems is typically hard to analyze rigorously.\n\nThis paper succeeds in showing the analysis of convergence to a proved limit point which is guaranteed to be a Clarke critical point. This analysis is pretty deep and the mathematics looks correct. It would be good to see whether the same kind of method can be applied to more generalized non-convex objectives.\n\nThe empirical evaluation looks pretty reasonable with experiments analyzing the decrease in objective value over time for different large scale and high dimensional data sets. The results show that the algorithm converges faster than other comparable methods. It would be good to see some example of a dataset where there might be a local minima (even if synthetic) and see how the algorithm performs in presence of such an objective as well some guidance to how the different parameters particuarly \\gamma can be chosen to ensure faster convergence. This paper looks at a specific kind of regularized non-convex problem which appears in different problems in machine learning. They give a hybrid algorithm combining second order information using Quasi newton steps as well as gradient descent steps depending on a certain condition which is cheap to compute at every iteration. They provide an extensive analysis, pointing out the steps that are complicated due to lack of convexity and prove that their algorithm converges to a Clarke critical point. The analysis is also extendable to other non-convex general scenarios.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers efficient implementations of non-convex sparse learning formulations. Recent studies have shown that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice. However, it is still quite challenging to efficiently solve non-convex sparse optimization problems for large-scale data. This paper presents a novel algorithm HONOR which is applicable for a wide range of non-convex sparse learning formulations.\n\n One of the key ideas in HONOR is to incorporate the second-order information to greatly speed up the convergence, while unlike most existing second-order methods it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. Thus, HONOR has a low computational complexity at each iteration and it is scalable to large-size problems.\n\n The convergence for non-convex problems is typically challenging to analyze and establish. One major contribution of this paper is to establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems. The key to the convergence analysis is the hybrid optimization scheme which chooses either a Quasi-Newton step or a Gradient Descent step per iteration. The presented analysis is nontrivial. It will be good to include a high-level description of the intuition behind the proposed hybrid scheme.\n\n The empirical evaluation presented in this paper is convincing. The authors evaluate the proposed algorithm using large-scale datasets which include up to millions of samples and features. Two of the datasets include over 20 million features. Such scale of datasets is needed to evaluate the behavior of the algorithms. Results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms. Some guidance on the selection of \\epsilon will be helpful.\n\n It is mentioned that HONOR may have the potential of escaping from high error plateau which often exists in high dimensional non-convex problems. It will be interesting to explore theoretical properties of the HONOR solutions.\n\n The proposed algorithm empirically converges very fast. Is there any guarantee on the (local) convergence rate of HONOR?\n\nCan the proposed algorithm be extended to other sparsity-inducing penalties such as group Lasso and fused Lasso? This paper considers efficient implementations of non-convex sparse learning formulations. Recent studies have shown that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice. However, it is still quite challenging to efficiently solve non-convex sparse optimization problems for large-scale data. This paper presents a novel algorithm HONOR which is applicable for a wide range of non-convex sparse learning formulations.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes an efficient optimization algorithm (HONOR) for nonconvex regularized problems by incorporating the second-order information to speed up the convergence. Although the convergence analysis for nonconvex optimization methods is typically more difficult than convex methods, the authors have proved that every limit point of the sequence obtained by HONOR is a Clarke critical point. The proposed method is not only theoretically sound, but also computationally efficient. However, I still have a few concerns about the proposed algorithm.\n\n1: Using the fact that each decomposable component function of the non-convex regularizer is only non-differentiable at the origin, the authors have designed an algorithm which can keep the current iterate in the same orthant of the previous iterate so that the segment between any two consecutive iterates do not cross any axis. The strategy seems to make the algorithm dependent on an initial point x^0.\n\n + Can the authors show how the solution of HONOR is influenced by the choice of the initial solution?\n\n + Which vector did the authors use as the initial solution in the numerical experiments?\n\n + Does the results shown in Figure 1 change by using different the initial solution?\n\n2: One of the advantages of HONOR is to incorporate the second-order information to speed up the convergence but HONOR might sacrifice memory usage. When the given problem is highly nonconvex, the positive definite matrix H^k given by L-BFGS might be completely different from the Hessian matrix of the nonconvex optimization problem. Do the authors have any thoughts about those issues?  The paper is very well-written and an efficient algorithm with theoretical guarantee for nonconvex problems is of great significance. However, I have some concerns about the proposed algorithm.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper considers an algorithm for the optimization problem\n\n\\min l(x) + r(x)\n\nwhere l is a smooth function and r(x) = \\sum(\\rho(|x_i|) for a concave smooth function \\rho. The authors present a hybrid algorithm that combines a quasi newton step and a gradient descent step. The main idea of the algorithm is borrowed from the OWL-QN algorithm. That is, in order to deal with the non-smoothness of the regularizer, the iterates of the algorithm are constrained to remain on the same quadrant. However, the fact that the regularizer is non-convex does not allow the use of subgradient properties. Instead, the proofs are based on the properties\n\nClarke sub-differential.\n\n I believe that the problem being tackled is not trivial. Dealing with non-convexity is an added difficulty to the non-smoothness of the regularizer. There are three problems that I see with this paper.\n\n(1) In spite of the criticism of DC programming for non-convex optimization, this algorithm provides linear convergence guarantees whereas the proposed algorithm does not provide any rates of convergence.\n\n (2) In order to obtain convergence the algorithm requires a gradient descent step. However, it seems that the only way to find the optimal tradeoff between the GD step and the QN step is by actually running the algorithm with different tradeoff parameters.\n\n(3) As a practitioner I have never used a non-convex regularizer like the ones described on this paper nor have I seen them in any publications. Probably this is only because of the type of applications I do but I do want to make sure that this problem is indeed relevant to the machine learning community.\n\n The paper deals with minimizationunder a non-convex regularizer that promotes sparsity. The paper is well written and the math is correct. However I am not entirely sure of the relevance of the results for the learning community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
