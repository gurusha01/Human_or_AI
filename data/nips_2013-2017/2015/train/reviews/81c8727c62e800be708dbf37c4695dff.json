{"title": "Sample Complexity of Learning Mahalanobis Distance Metrics", "abstract": "Metric learning seeks a transformation of the feature space that enhances prediction quality for a given task. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lower- and upper-bounds showing that sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. In addition, by leveraging the structure of the data distribution, we provide rates fine-tuned to a specific notion of the intrinsic complexity of a given dataset, allowing us to relax the dependence on representation dimension. We show both theoretically and empirically that augmenting the metric learning optimization criterion with a simple norm-based regularization is important and can help adapt to a dataset\u2019s intrinsic complexity yielding better generalization, thus partly explaining the empirical success of similar regularizations reported in previous works.", "id": "81c8727c62e800be708dbf37c4695dff", "authors": ["Nakul Verma", "Kristin Branson"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "I have one concern with the fact that in the proof of Theorem\n\n 2, lambda and L are set to depend on d. What are the\n\n implications of this choice? Is it a realistic choice?\n\n For a potentially useful additional citation, the following\n\n paper considers the same loss as the one you have in (4), and\n\n gives an efficient algorithm to globally solve it:\n\n \"Scalable Metric Learning for Co-embedding\", Mirzazadeh et\n\n al., 2015, ECML This paper is clearly written and provides noveltheoretical insights into the sample complexity of metriclearning and improvements from added regularization. Inaddition to carefully exploring the topic for both general andspecific cases theoretically, the empirical results aresimilarly methodical but compact and further reinforcethese properties on real datasets.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This work proposes an optimization criteria that automatically adapts to the intrinsic complexity of the underlying dataset. Thus this method de-emphasize uninformative or weakly informative features and concentrates more on the relevant ones.\n\nThe paper is well written. The technical part is clear. It addresses an important issue in machine learning. This method seems novel to me although it might be even better to find the noise in the dataset and discard those items. Finally the experiments confirm their claim. This work presents a novel method that adapts to the intrinsic complexity of dataset. The author show theoretically and empirically the better generalization of the algorithm.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This is the cleanest piece of work I have seen on sample complexity for metric learning and, as noted by the authors, presents several advantages over related work in this area.\n\nIn particular, I like that the authors present several important cases: both the \"distance-based\" and \"classifier-based\" frameworks, which highlight dependence on the dimensionality, as well as the regime in which there may be noisy or uninformative dimensions.\n\nIt's clearly written and appears to be correct.\n\nGiven that there is a relative dearth of work of this sort in the metric learning sub-community, I think it's worth publishing.\n\nThe experiments are somewhat underwhelming, though since this is more of a theoretical paper, it's not a key drawback.\n\nA couple things I noticed were: there has actually been quite a bit of work on using the type of regularizer advocated in this work (the ||M^T M||_F^2 regularizer); see, e.g., \"Metric Learning: A Survey\", section 2.4.1 for examples).\n\nIt might be good to discuss these a bit.\n\nAlso, both ITML and LMNN can be viewed as already having a regularizer on the M matrix; for instance, ITML is typically viewed as applying a LogDet regularization on M^T M and LMNN is often viewed as applying a weighted trace norm regularization on M^T M.\n\nPerhaps these were not necessarily the best choices of methods to add the proposed regularization?\n\nAlso, it's stated that ITML is initialized with a rank-one metric; given that the LogDet regularizer maintains rank (the rank cannot change from the initialization), this would lead to a low-rank solution, which would probably be undesirable (if I'm understanding the experimental setup correctly).\n\nThese are all fairly minor things, however, and shouldn't detract from the fact that the paper is a clear and effective look at sample complexity for metric learning. This is a nice paper that presents several key results regarding the sample complexity for Mahalanobis metric learning.It is a good addition to the literature on metric learning, and deserves to be published.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary In this article the authors present a theoretical analysis of non-regularized supervised metric learning formulations. They study two frameworks. On the one hand, the distance-based framework considers algorithms that optimize a distance w.r.t. class label information. On the other hand, the classifier-based framework considers algorithms which optimize a metric for a specific prediction or a ranking task.\n\nFor both frameworks, the authors provide PAC style bounds (sample complexity bounds) on the difference between the true risk and the empirical risk of the learned metric. They also analyze the difference between the true risk of the learned metric and the true risk of the best in-class metric and discuss the the necessity of a strong dependency on the dimensionality of the data. In the last part of the paper, the authors consider some regularized formulation using the Frobenius norm as a\n\nregularizer, they propose some refinement of their results in this context and argue that adding a regularization term provides better guarantees. The authors propose finally an empirical evaluation of two well known metric learning approaches (namely ITML and LMNN) showing that adding a regularization term is indeed beneficial.\n\n The paper is relatively clear and well written. The results are new and provide new theoretical understanding for metric learning .\n\n Comments.\n\n-Correct the proof of Lemma 2 to make it independent from the dimension. You should add some comments to make a link between Th1 and Th2. In the current form of the paper, they are presented separately and with formulations that do not allow the reader to directly get the consequences on the dependence on D.\n\n-Mention that the results of Jin et al. (NIPS 2009) can be extended to replace the dependency on d by a term based on the data concentration (term B in author's paper), for any distributions and convex regularized formulations.\n\nThis can be done by using the techniques of Bousquet & Elisseeff, JMLR 2002 - note that the convexity of the formulation is mandatory for applying algorithmic stability. +Bellet et al. Metric Learning. Morgan & Claypool publishers. 2015. (chapter 8)\n\n-One of the claim of the paper is to generalize some previous results to possibly non convex loss. However, in this context there is no guarantee to find the best model (matrix) minimizing the risk which makes the results a bit non informative on what can be done in practise.\n\nDo not forget that even for convex formulation, taking into all the quadratic constraints involved by classical metric learning formulations is heavy and in general some heuristics are used to reduce the number of constraints.\n\n One interesting perspective would be to know if the dependency on D impacts more severely non convex non regularized formulations.\n\n-The authors have focused on the dependence over the representation dimension D, however it could be interesting to consider also the true rank of the matrix M rather than the data dimension. This would allow to make some relationships with metric learning based on Cholesky decomposition of the learned metric and methods making use of some low rank regularization. This point should be at least discussed in the perspectives.\n\n-Considering, the classifier-based approach, the bibliography lacks a reference on methods learning a linear classifier from good similarity functions, even if this paper adresses other settings. +Balcan et al. Improved Guarantees for Learning via Similarity Functions. COLT 2008. +Bellet et al. Similarity Learning for Provably Accurate Sparse Linear Classification. ICML 2012. This aspect is also studied in the paper of Guo and Ying cited by the authors.\n\nOverall, the results are interesting and I think that providing theoretical guarantees on classifiers making use of the result of a known metric is very important and results in that direction must continued.\n\nThe results are based on covering and fat shattering dimensions arguments, is there any perspective to use other frameworks (Rademacher-based for example?)\n\n-About the experiments.\n\nEven though they do not consider noisy data in their analysis, the authors create noise augmented datasets to see the influcence of the regularization. The conclusion of the experiments is that regularization can help to deal with noisy datasets or in other words they point out that the well known problem of overfitting can be partly adressed by choosing a good regularizer which is known fact. The scaling for the x-axis is different for the 3 figures, what is the reason for that? In particular, one can wonder what happens after 120/150 for Iris and Ionosphere? For the comparison with ITML, it could be interesting to have the accuracy obtained by directly using the rank-1 matrix alone in the distance to compare the influence of the regularizer. It seems that the choice of this matrix may do a lot of the \"job\". The experiments lack of datasets - 3 is very small and the number of dimensions is relatively low - and lack of methods for comparison. We can also wonder why no experiment on any classifier-based setting has been made. There also exist other datasets with higher dimensions in the UCI datasets, you can also check here : +Y. Shi et al.:Sparse Compositional Metric Learning. AAAI 2014: 2078-2084. The experiments would have gained in interest, if they authors have presented the evolution of the bounds with the data and with the dimension.\n\n Other comments:\n\n-line 075: the authors consider binary classification, however the loss considered below allows to consider a multiclass setting and some data sets in the experiments are multiclass. -I am wondering why in the definition of the true risk of the classifier-based approach, the margin is chosen at 1/2, while for the empirical risk the margin can be fixed to any value between 0 and 1/2. -line 219: \"do now know\" -> \"do not know\" -in the proof of Lemma 4, the notion of covering is used extensively.\n\n Is it possible to extend the results to other regularization norms?  Pro:-new and original bounds for Metric Learning-analysis wrt to a notion of hypothesis space interesting-analysis provided in the classifier-based setting Cons:-limited experimental evaluation-some discussions about regularization and links with other references lack of precision", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
