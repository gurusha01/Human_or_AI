{
  "name" : "c164bbc9d6c72a52c599bbb43d8db8e1.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Universal Catalyst for First-Order Optimization",
    "authors" : [ "Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui" ],
    "emails" : [ "hongzhou.lin@inria.fr", "julien.mairal@inria.fr", "zaid.harchaoui@nyu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A large number of machine learning and signal processing problems are formulated as the minimization of a composite objective function F : Rp → R:\nmin x∈Rp\n{ F (x) , f(x) + ψ(x) } , (1)\nwhere f is convex and has Lipschitz continuous derivatives with constant L and ψ is convex but may not be differentiable. The variable x represents model parameters and the role of f is to ensure that the estimated parameters fit some observed data. Specifically, f is often a large sum of functions\nf(x) , 1\nn\nn ∑\ni=1\nfi(x), (2)\nand each term fi(x) measures the fit between x and a data point indexed by i. The function ψ in (1) acts as a regularizer; it is typically chosen to be the squared ℓ2-norm, which is smooth, or to be a non-differentiable penalty such as the ℓ1-norm or another sparsity-inducing norm [2]. Composite minimization also encompasses constrained minimization if we consider extended-valued indicator functions ψ that may take the value +∞ outside of a convex set C and 0 inside (see [11]). Our goal is to accelerate gradient-based or first-order methods that are designed to solve (1), with a particular focus on large sums of functions (2). By “accelerating”, we mean generalizing a mechanism invented by Nesterov [17] that improves the convergence rate of the gradient descent algorithm. More precisely, when ψ = 0, gradient descent steps produce iterates (xk)k≥0 such that F (xk)− F ∗ = O(1/k), where F ∗ denotes the minimum value of F . Furthermore, when the objective F is strongly convex with constant µ, the rate of convergence becomes linear inO((1−µ/L)k). These rates were shown by Nesterov [16] to be suboptimal for the class of first-order methods, and instead optimal rates—O(1/k2) for the convex case and O((1 − √\nµ/L)k) for the µ-strongly convex one—could be obtained by taking gradient steps at well-chosen points. Later, this acceleration technique was extended to deal with non-differentiable regularization functions ψ [4, 19].\nFor modern machine learning problems involving a large sum of n functions, a recent effort has been devoted to developing fast incremental algorithms [6, 7, 14, 24, 25, 27] that can exploit the particular\nstructure of (2). Unlike full gradient approaches which require computing and averaging n gradients ∇f(x) = (1/n)∑ni=1 ∇fi(x) at every iteration, incremental techniques have a cost per-iteration that is independent of n. The price to pay is the need to store a moderate amount of information regarding past iterates, but the benefit is significant in terms of computational complexity.\nMain contributions. Our main achievement is a generic acceleration scheme that applies to a large class of optimization methods. By analogy with substances that increase chemical reaction rates, we call our approach a “catalyst”. A method may be accelerated if it has linear convergence rate for strongly convex problems. This is the case for full gradient [4, 19] and block coordinate descent methods [18, 21], which already have well-known accelerated variants. More importantly, it also applies to incremental algorithms such as SAG [24], SAGA [6], Finito/MISO [7, 14], SDCA [25], and SVRG [27]. Whether or not these methods could be accelerated was an important open question. It was only known to be the case for dual coordinate ascent approaches such as SDCA [26] or SDPC [28] for strongly convex objectives. Our work provides a universal positive answer regardless of the strong convexity of the objective, which brings us to our second achievement.\nSome approaches such as Finito/MISO, SDCA, or SVRG are only defined for strongly convex objectives. A classical trick to apply them to general convex functions is to add a small regularization ε‖x‖2 [25]. The drawback of this strategy is that it requires choosing in advance the parameter ε, which is related to the target accuracy. A consequence of our work is to automatically provide a direct support for non-strongly convex objectives, thus removing the need of selecting ε beforehand.\nOther contribution: Proximal MISO. The approach Finito/MISO, which was proposed in [7] and [14], is an incremental technique for solving smooth unconstrained µ-strongly convex problems when n is larger than a constant βL/µ (with β = 2 in [14]). In addition to providing acceleration and support for non-strongly convex objectives, we also make the following specific contributions: • we extend the method and its convergence proof to deal with the composite problem (1); • we fix the method to remove the “big data condition” n ≥ βL/µ. The resulting algorithm can be interpreted as a variant of proximal SDCA [25] with a different step size and a more practical optimality certificate—that is, checking the optimality condition does not require evaluating a dual objective. Our construction is indeed purely primal. Neither our proof of convergence nor the algorithm use duality, while SDCA is originally a dual ascent technique.\nRelated work. The catalyst acceleration can be interpreted as a variant of the proximal point algorithm [3, 9], which is a central concept in convex optimization, underlying augmented Lagrangian approaches, and composite minimization schemes [5, 20]. The proximal point algorithm consists of solving (1) by minimizing a sequence of auxiliary problems involving a quadratic regularization term. In general, these auxiliary problems cannot be solved with perfect accuracy, and several notations of inexactness were proposed, including [9, 10, 22]. The catalyst approach hinges upon (i) an acceleration technique for the proximal point algorithm originally introduced in the pioneer work [9]; (ii) a more practical inexactness criterion than those proposed in the past.1 As a result, we are able to control the rate of convergence for approximately solving the auxiliary problems with an optimization method M. In turn, we are also able to obtain the computational complexity of the global procedure for solving (1), which was not possible with previous analysis [9, 10, 22]. When instantiated in different first-order optimization settings, our analysis yields systematic acceleration.\nBeyond [9], several works have inspired this paper. In particular, accelerated SDCA [26] is an instance of an inexact accelerated proximal point algorithm, even though this was not explicitly stated in [26]. Their proof of convergence relies on different tools than ours. Specifically, we use the concept of estimate sequence from Nesterov [17], whereas the direct proof of [26], in the context of SDCA, does not extend to non-strongly convex objectives. Nevertheless, part of their analysis proves to be helpful to obtain our main results. Another useful methodological contribution was the convergence analysis of inexact proximal gradient methods of [23]. Finally, similar ideas appear in the independent work [8]. Their results overlap in part with ours, but both papers adopt different directions. Our analysis is for instance more general and provides support for non-strongly convex objectives. Another independent work with related results is [13], which introduce an accelerated method for the minimization of finite sums, which is not based on the proximal point algorithm.\n1Note that our inexact criterion was also studied, among others, in [22], but the analysis of [22] led to the conjecture that this criterion was too weak to warrant acceleration. Our analysis refutes this conjecture."
    }, {
      "heading" : "2 The Catalyst Acceleration",
      "text" : "We present here our generic acceleration scheme, which can operate on any first-order or gradientbased optimization algorithm with linear convergence rate for strongly convex objectives.\nLinear convergence and acceleration. Consider the problem (1) with a µ-strongly convex function F , where the strong convexity is defined with respect to the ℓ2-norm. A minimization algorithm M, generating the sequence of iterates (xk)k≥0, has a linear convergence rate if there exists τM,F in (0, 1) and a constant CM,F in R such that\nF (xk)− F ∗ ≤ CM,F (1− τM,F )k, (3) where F ∗ denotes the minimum value of F . The quantity τM,F controls the convergence rate: the larger is τM,F , the faster is convergence to F\n∗. However, for a given algorithm M, the quantity τM,F depends usually on the ratio L/µ, which is often called the condition number of F .\nThe catalyst acceleration is a general approach that allows to wrap algorithm M into an accelerated algorithm A, which enjoys a faster linear convergence rate, with τA,F ≥ τM,F . As we will also see, the catalyst acceleration may also be useful when F is not strongly convex—that is, when µ = 0. In that case, we may even consider a method M that requires strong convexity to operate, and obtain an accelerated algorithm A that can minimize F with near-optimal convergence rate Õ(1/k2).2\nOur approach can accelerate a wide range of first-order optimization algorithms, starting from classical gradient descent. It also applies to randomized algorithms such as SAG, SAGA, SDCA, SVRG and Finito/MISO, whose rates of convergence are given in expectation. Such methods should be contrasted with stochastic gradient methods [15, 12], which minimize a different non-deterministic function. Acceleration of stochastic gradient methods is beyond the scope of this work.\nCatalyst action. We now highlight the mechanics of the catalyst algorithm, which is presented in Algorithm 1. It consists of replacing, at iteration k, the original objective function F by an auxiliary objective Gk, close to F up to a quadratic term:\nGk(x) , F (x) + κ\n2 ‖x− yk−1‖2, (4)\nwhere κ will be specified later and yk is obtained by an extrapolation step described in (6). Then, at iteration k, the accelerated algorithm A minimizes Gk up to accuracy εk. Substituting (4) to (1) has two consequences. On the one hand, minimizing (4) only provides an approximation of the solution of (1), unless κ = 0; on the other hand, the auxiliary objective Gk enjoys a better condition number than the original objective F , which makes it easier to minimize. For instance, when M is the regular gradient descent algorithm with ψ = 0, M has the rate of convergence (3) for minimizing F with τM,F = µ/L. However, owing to the additional quadratic term, Gk can be minimized by M with the rate (3) where τM,Gk = (µ + κ)/(L + κ) > τM,F . In practice, there exists an “optimal” choice for κ, which controls the time required by M for solving the auxiliary problems (4), and the quality of approximation of F by the functions Gk. This choice will be driven by the convergence analysis in Sec. 3.1-3.3; see also Sec. C for special cases.\nAcceleration via extrapolation and inexact minimization. Similar to the classical gradient descent scheme of Nesterov [17], Algorithm 1 involves an extrapolation step (6). As a consequence, the solution of the auxiliary problem (5) at iteration k+1 is driven towards the extrapolated variable yk. As shown in [9], this step is in fact sufficient to reduce the number of iterations of Algorithm 1 to solve (1) when εk = 0—that is, for running the exact accelerated proximal point algorithm.\nNevertheless, to control the total computational complexity of an accelerated algorithm A, it is necessary to take into account the complexity of solving the auxiliary problems (5) using M. This is where our approach differs from the classical proximal point algorithm of [9]. Essentially, both algorithms are the same, but we use the weaker inexactness criterion Gk(xk)−G∗k ≤ εk, where the sequence (εk)k≥0 is fixed beforehand, and only depends on the initial point. This subtle difference has important consequences: (i) in practice, this condition can often be checked by computing duality gaps; (ii) in theory, the methods M we consider have linear convergence rates, which allows us to control the complexity of step (5), and then to provide the computational complexity of A.\n2In this paper, we use the notation O(.) to hide constants. The notation Õ(.) also hides logarithmic factors.\nAlgorithm 1 Catalyst input initial estimate x0 ∈ Rp, parameters κ and α0, sequence (εk)k≥0, optimization method M; 1: Initialize q = µ/(µ+ κ) and y0 = x0; 2: while the desired stopping criterion is not satisfied do 3: Find an approximate solution of the following problem using M\nxk ≈ argmin x∈Rp\n{ Gk(x) , F (x) + κ\n2 ‖x− yk−1‖2\n}\nsuch that Gk(xk)−G∗k ≤ εk. (5)\n4: Compute αk ∈ (0, 1) from equation α2k = (1− αk)α2k−1 + qαk; 5: Compute\nyk = xk + βk(xk − xk−1) with βk = αk−1(1− αk−1) α2k−1 + αk . (6)\n6: end while output xk (final estimate)."
    }, {
      "heading" : "3 Convergence Analysis",
      "text" : "In this section, we present the theoretical properties of Algorithm 1, for optimization methods M with deterministic convergence rates of the form (3). When the rate is given as an expectation, a simple extension of our analysis described in Section 4 is needed. For space limitation reasons, we shall sketch the proof mechanics here, and defer the full proofs to Appendix B."
    }, {
      "heading" : "3.1 Analysis for µ-Strongly Convex Objective Functions",
      "text" : "We first analyze the convergence rate of Algorithm 1 for solving problem 1, regardless of the complexity required to solve the subproblems (5). We start with the µ-strongly convex case.\nTheorem 3.1 (Convergence of Algorithm 1, µ-Strongly Convex Case). Choose α0 = √ q with q = µ/(µ+ κ) and\nεk = 2\n9 (F (x0)− F ∗)(1− ρ)k with ρ <\n√ q.\nThen, Algorithm 1 generates iterates (xk)k≥0 such that\nF (xk)− F ∗ ≤ C(1− ρ)k+1(F (x0)− F ∗) with C = 8\n( √ q − ρ)2 . (7)\nThis theorem characterizes the linear convergence rate of Algorithm 1. It is worth noting that the choice of ρ is left to the discretion of the user, but it can safely be set to ρ = 0.9 √ q in practice. The choice α0 = √ q was made for convenience purposes since it leads to a simplified analysis, but larger values are also acceptable, both from theoretical and practical point of views. Following an advice from Nesterov[17, page 81] originally dedicated to his classical gradient descent algorithm, we may for instance recommend choosing α0 such that α 2 0 + (1− q)α0 − 1 = 0.\nThe choice of the sequence (εk)k≥0 is also subject to discussion since the quantity F (x0) − F ∗ is unknown beforehand. Nevertheless, an upper bound may be used instead, which will only affects the corresponding constant in (7). Such upper bounds can typically be obtained by computing a duality gap at x0, or by using additional knowledge about the objective. For instance, when F is non-negative, we may simply choose εk = (2/9)F (x0)(1− ρ)k. The proof of convergence uses the concept of estimate sequence invented by Nesterov [17], and introduces an extension to deal with the errors (εk)k≥0. To control the accumulation of errors, we borrow the methodology of [23] for inexact proximal gradient algorithms. Our construction yields a convergence result that encompasses both strongly convex and non-strongly convex cases. Note that estimate sequences were also used in [9], but, as noted by [22], the proof of [9] only applies when using an extrapolation step (6) that involves the true minimizer of (5), which is unknown in practice. To obtain a rigorous convergence result like (7), a different approach was needed.\nTheorem 3.1 is important, but it does not provide yet the global computational complexity of the full algorithm, which includes the number of iterations performed by M for approximately solving the auxiliary problems (5). The next proposition characterizes the complexity of this inner-loop.\nProposition 3.2 (Inner-Loop Complexity, µ-Strongly Convex Case). Under the assumptions of Theorem 3.1, let us consider a method M generating iterates (zt)t≥0 for minimizing the function Gk with linear convergence rate of the form Gk(zt)−G∗k ≤ A(1− τM)t(Gk(z0)−G∗k). (8) When z0 = xk−1, the precision εk is reached with a number of iterations TM = Õ(1/τM), where the notation Õ hides some universal constants and some logarithmic dependencies in µ and κ.\nThis proposition is generic since the assumption (8) is relatively standard for gradient-based methods [17]. It may now be used to obtain the global rate of convergence of an accelerated algorithm. By calling Fs the objective function value obtained after performing s = kTM iterations of the method M, the true convergence rate of the accelerated algorithm A is\nFs−F ∗ = F (\nx s TM\n)\n−F ∗ ≤ C(1− ρ) s TM (F (x0)−F ∗) ≤ C ( 1− ρ TM\n)s\n(F (x0)−F ∗). (9)\nAs a result, algorithm A has a global linear rate of convergence with parameter τA,F = ρ/TM = Õ(τM √ µ/ √ µ+ κ), where τM typically depends on κ (the greater, the faster is M). Consequently, κ will be chosen to maximize the ratio τM/ √ µ+ κ. Note that for other algorithms M that do not satisfy (8), additional analysis and possibly a different initialization z0 may be necessary (see Appendix D for example)."
    }, {
      "heading" : "3.2 Convergence Analysis for Convex but Non-Strongly Convex Objective Functions",
      "text" : "We now state the convergence rate when the objective is not strongly convex, that is when µ = 0.\nTheorem 3.3 (Convergence of Algorithm 1, Convex, but Non-Strongly Convex Case). When µ = 0, choose α0 = ( √ 5− 1)/2 and\nεk = 2(F (x0)− F ∗) 9(k + 2)4+η with η > 0. (10)\nThen, Algorithm 1 generates iterates (xk)k≥0 such that\nF (xk)− F ∗ ≤ 8\n(k + 2)2\n(\n(\n1 + 2\nη\n)2\n(F (x0)− F ∗) + κ\n2 ‖x0 − x∗‖2\n)\n. (11)\nThis theorem is the counter-part of Theorem 3.1 when µ = 0. The choice of η is left to the discretion of the user; it empirically seem to have very low influence on the global convergence speed, as long as it is chosen small enough (e.g., we use η = 0.1 in practice). It shows that Algorithm 1 achieves the optimal rate of convergence of first-order methods, but it does not take into account the complexity of solving the subproblems (5). Therefore, we need the following proposition:\nProposition 3.4 (Inner-Loop Complexity, Non-Strongly Convex Case). Assume that F has bounded level sets. Under the assumptions of Theorem 3.3, let us consider a method M generating iterates (zt)t≥0 for minimizing the function Gk with linear convergence rate of the form (8). Then, there exists TM = Õ(1/τM), such that for any k ≥ 1, solving Gk with initial point xk−1 requires at most TM log(k + 2) iterations of M.\nWe can now draw up the global complexity of an accelerated algorithm A when M has a linear convergence rate (8) for κ-strongly convex objectives. To produce xk, M is called at most kTM log(k + 2) times. Using the global iteration counter s = kTM log(k + 2), we get\nFs − F ∗ ≤ 8T 2M log 2(s)\ns2\n(\n(\n1 + 2\nη\n)2\n(F (x0)− F ∗) + κ\n2 ‖x0 − x∗‖2\n)\n. (12)\nIf M is a first-order method, this rate is near-optimal, up to a logarithmic factor, when compared to the optimal rate O(1/s2), which may be the price to pay for using a generic acceleration scheme."
    }, {
      "heading" : "4 Acceleration in Practice",
      "text" : "We show here how to accelerate existing algorithms M and compare the convergence rates obtained before and after catalyst acceleration. For all the algorithms we consider, we study rates of convergence in terms of total number of iterations (in expectation, when necessary) to reach accuracy ε. We first show how to accelerate full gradient and randomized coordinate descent algorithms [21]. Then, we discuss other approaches such as SAG [24], SAGA [6], or SVRG [27]. Finally, we present a new proximal version of the incremental gradient approaches Finito/MISO [7, 14], along with its accelerated version. Table 4.1 summarizes the acceleration obtained for the algorithms considered.\nDeriving the global rate of convergence. The convergence rate of an accelerated algorithm A is driven by the parameter κ. In the strongly convex case, the best choice is the one that maximizes the ratio τM,Gk/ √ µ+ κ. As discussed in Appendix C, this rule also holds when (8) is given in expectation and in many cases where the constant CM,Gk is different thanA(Gk(z0)−G∗k) from (8). When µ = 0, the choice of κ > 0 only affects the complexity by a multiplicative constant. A rule of thumb is to maximize the ratio τM,Gk/ √ L+ κ (see Appendix C for more details). After choosing κ, the global iteration-complexity is given by Comp ≤ kinkout, where kin is an upperbound on the number of iterations performed by M per inner-loop, and kout is the upper-bound on the number of outer-loop iterations, following from Theorems 3.1-3.3. Note that for simplicity, we always consider that L≫ µ such that we may write L− µ simply as “L” in the convergence rates."
    }, {
      "heading" : "4.1 Acceleration of Existing Algorithms",
      "text" : "Composite minimization. Most of the algorithms we consider here, namely the proximal gradient method [4, 19], SAGA [6], (Prox)-SVRG [27], can handle composite objectives with a regularization penalty ψ that admits a proximal operator proxψ , defined for any z as\nFull gradient method. A first illustration is the algorithm obtained when accelerating the regular “full” gradient descent (FG), and how it contrasts with Nesterov’s accelerated variant (AFG). Here, the optimal choice for κ is L − 2µ. In the strongly convex case, we get an accelerated rate of convergence in Õ(n √\nL/µ log(1/ε)), which is the same as AFG up to logarithmic terms. A similar result can also be obtained for randomized coordinate descent methods [21].\nRandomized incremental gradient. We now consider randomized incremental gradient methods, resp. SAG [24] and SAGA [6]. When µ > 0, we focus on the “ill-conditioned” setting n ≤ L/µ, where these methods have the complexityO((L/µ) log(1/ε)). Otherwise, their complexity becomes O(n log(1/ε)), which is independent of the condition number and seems theoretically optimal [1].\nFor these methods, the best choice for κ has the form κ = a(L− µ)/(n+ b) − µ, with (a, b) = (2,−2) for SAG, (a, b) = (1/2, 1/2) for SAGA. A similar formula, with a constant L′ in place of L, holds for SVRG; we omit it here for brevity. SDCA [26] and Finito/MISO [7, 14] are actually related to incremental gradient methods, and the choice for κ has a similar form with (a, b) = (1, 1)."
    }, {
      "heading" : "4.2 Proximal MISO and its Acceleration",
      "text" : "Finito/MISO was proposed in [7] and [14] for solving the problem (1) when ψ = 0 and when f is a sum of n µ-strongly convex functions fi as in (2), which are also differentiable with L-Lipschitz derivatives. The algorithm maintains a list of quadratic lower bounds—say (dki ) n i=1 at iteration k— of the functions fi and randomly updates one of them at each iteration by using strong-convexity\ninequalities. The current iterate xk is then obtained by minimizing the lower-bound of the objective\nxk = argmin x∈Rp\n{\nDk(x) = 1\nn\nn ∑\ni=1\ndki (x)\n}\n. (13)\nInterestingly, since Dk is a lower-bound of F we also have Dk(xk) ≤ F ∗, and thus the quantity F (xk) −Dk(xk) can be used as an optimality certificate that upper-bounds F (xk) − F ∗. Furthermore, this certificate was shown to converge to zero with a rate similar to SAG/SDCA/SVRG/SAGA under the condition n ≥ 2L/µ. In this section, we show how to remove this condition and how to provide support to non-differentiable functions ψ whose proximal operator can be easily computed. We shall briefly sketch the main ideas, and we refer to Appendix D for a thorough presentation.\nThe first idea to deal with a nonsmooth regularizer ψ is to change the definition of Dk:\nDk(x) = 1\nn\nn ∑\ni=1\ndki (x) + ψ(x),\nwhich was also proposed in [7] without a convergence proof. Then, because the dki ’s are quadratic functions, the minimizer xk of Dk can be obtained by computing the proximal operator of ψ at a particular point. The second idea to remove the condition n ≥ 2L/µ is to modify the update of the lower bounds dki . Assume that index ik is selected among {1, . . . , n} at iteration k, then\ndki (x) =\n{\n(1− δ)dk−1i (x)+ δ(fi(xk−1)+〈∇fi(xk−1), x− xk−1〉+ µ2 ‖x− xk−1‖2) if i = ik dk−1i (x) otherwise\nWhereas the original Finito/MISO uses δ = 1, our new variant uses δ = min(1, µn/2(L− µ)). The resulting algorithm turns out to be very close to variant “5” of proximal SDCA [25], which corresponds to using a different value for δ. The main difference between SDCA and MISOProx is that the latter does not use duality. It also provides a different (simpler) optimality certificate F (xk)−Dk(xk), which is guaranteed to converge linearly, as stated in the next theorem. Theorem 4.1 (Convergence of MISO-Prox). Let (xk)k≥0 be obtained by MISO-Prox, then\nE[F (xk)]− F ∗ ≤ 1\nτ (1− τ)k+1 (F (x0)−D0(x0)) with τ ≥ min\n{ µ 4L , 1 2n } . (14)\nFurthermore, we also have fast convergence of the certificate\nE[F (xk)−Dk(xk)] ≤ 1\nτ (1− τ)k (F ∗ −D0(x0)) .\nThe proof of convergence is given in Appendix D. Finally, we conclude this section by noting that MISO-Prox enjoys the catalyst acceleration, leading to the iteration-complexity presented in Table 4.1. Since the convergence rate (14) does not have exactly the same form as (8), Propositions 3.2 and 3.4 cannot be used and additional analysis, given in Appendix D, is needed. Practical forms of the algorithm are also presented there, along with discussions on how to initialize it."
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate the Catalyst acceleration on three methods that have never been accelerated in the past: SAG [24], SAGA [6], and MISO-Prox. We focus on ℓ2-regularized logistic regression, where the regularization parameter µ yields a lower bound on the strong convexity parameter of the problem.\nWe use three datasets used in [14], namely real-sim, rcv1, and ocr, which are relatively large, with up to n = 2500 000 points for ocr and p = 47 152 variables for rcv1. We consider three regimes: µ = 0 (no regularization), µ/L = 0.001/n and µ/L = 0.1/n, which leads significantly larger condition numbers than those used in other studies (µ/L ≈ 1/n in [14, 24]). We compare MISO, SAG, and SAGA with their default parameters, which are recommended by their theoretical analysis (step-sizes 1/L for SAG and 1/3L for SAGA), and study several accelerated variants. The values of κ and ρ and the sequences (εk)k≥0 are those suggested in the previous sections, with η=0.1 in (10). Other implementation details are presented in Appendix E.\nThe restarting strategy for M is key to achieve acceleration in practice. All of the methods we compare store n gradients evaluated at previous iterates of the algorithm. We always use the gradients from the previous run of M to initialize a new one. We detail in Appendix E the initialization for each method. Finally, we evaluated a heuristic that constrain M to always perform at most n iterations (one pass over the data); we call this variant AMISO2 for MISO whereas AMISO1 refers to the regular “vanilla” accelerated variant, and we also use this heuristic to accelerate SAG.\nThe results are reported in Table 1. We always obtain a huge speed-up for MISO, which suffers from numerical stability issues when the condition number is very large (for instance, µ/L = 10−3/n = 4.10−10 for ocr). Here, not only does the catalyst algorithm accelerate MISO, but it also stabilizes it. Whereas MISO is slower than SAG and SAGA in this “small µ” regime, AMISO2 is almost systematically the best performer. We are also able to accelerate SAG and SAGA in general, even though the improvement is less significant than for MISO. In particular, SAGA without acceleration proves to be the best method on ocr. One reason may be its ability to adapt to the unknown strong convexity parameter µ′ ≥ µ of the objective near the solution. When µ′/L ≥ 1/n, we indeed obtain a regime where acceleration does not occur (see Sec. 4). Therefore, this experiment suggests that adaptivity to unknown strong convexity is of high interest for incremental optimization."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported by ANR (MACARON ANR-14-CE23-0003-01), MSR-Inria joint centre, CNRS-Mastodons program (Titan), and NYU Moore-Sloan Data Science Environment."
    } ],
    "references" : [ {
      "title" : "A lower bound for the optimization of finite sums",
      "author" : [ "A. Agarwal", "L. Bottou" ],
      "venue" : "Proc. International Conference on Machine Learning (ICML)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Optimization with sparsity-inducing penalties",
      "author" : [ "F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski" ],
      "venue" : "Foundations and Trends in Machine Learning, 4(1):1–106",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Convex Analysis and Monotone Operator Theory in Hilbert Spaces",
      "author" : [ "H.H. Bauschke", "P.L. Combettes" ],
      "venue" : "Springer",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences, 2(1):183–202",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Convex Optimization Algorithms",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A.J. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "Adv. Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Finito: A faster",
      "author" : [ "A.J. Defazio", "T.S. Caetano", "J. Domke" ],
      "venue" : "permutable incremental gradient method for big data problems. In Proc. International Conference on Machine Learning (ICML)",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Un-regularizing: approximate proximal point algorithms for empirical risk minimization",
      "author" : [ "R. Frostig", "R. Ge", "S.M. Kakade", "A. Sidford" ],
      "venue" : "Proc. International Conference on Machine Learning (ICML)",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "New proximal point algorithms for convex minimization",
      "author" : [ "O. Güler" ],
      "venue" : "SIAM Journal on Optimization, 2(4):649–664",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "An accelerated inexact proximal point algorithm for convex minimization",
      "author" : [ "B. He", "X. Yuan" ],
      "venue" : "Journal of Optimization Theory and Applications, 154(2):536–548",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Convex Analysis and Minimization Algorithms I",
      "author" : [ "J.-B. Hiriart-Urruty", "C. Lemaréchal" ],
      "venue" : "Springer",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "First order methods for nonsmooth convex large-scale optimization",
      "author" : [ "A. Juditsky", "A. Nemirovski" ],
      "venue" : "Optimization for Machine Learning, MIT Press",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "An optimal randomized incremental gradient method",
      "author" : [ "G. Lan" ],
      "venue" : "arXiv:1507.02000",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Incremental majorization-minimization optimization with application to large-scale machine learning",
      "author" : [ "J. Mairal" ],
      "venue" : "SIAM Journal on Optimization, 25(2):829–855",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization, 19(4):1574–1609",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/k2)",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Soviet Mathematics Doklady, 27(2):372–376",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A Basic Course",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Springer",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "SIAM Journal on Optimization, 22(2):341–362",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Gradient methods for minimizing composite functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical Programming, 140(1):125–161",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Proximal algorithms",
      "author" : [ "N. Parikh", "S.P. Boyd" ],
      "venue" : "Foundations and Trends in Optimization, 1(3):123–231",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "P. Richtárik", "M. Takáč" ],
      "venue" : "Mathematical Programming, 144(1-2):1–38",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Inexact and accelerated proximal point algorithms",
      "author" : [ "S. Salzo", "S. Villa" ],
      "venue" : "Journal of Convex Analysis, 19(4):1167–1192",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Convergence rates of inexact proximal-gradient methods for convex optimization",
      "author" : [ "M. Schmidt", "N. Le Roux", "F. Bach" ],
      "venue" : "Adv. Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "M. Schmidt", "N. Le Roux", "F. Bach" ],
      "venue" : "arXiv:1309.2388",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Proximal stochastic dual coordinate ascent",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "arXiv:1211.2717",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Mathematical Programming",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "L. Xiao", "T. Zhang" ],
      "venue" : "SIAM Journal on Optimization, 24(4):2057–2075",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic primal-dual coordinate method for regularized empirical risk minimization",
      "author" : [ "Y. Zhang", "L. Xiao" ],
      "venue" : "Proc. International Conference on Machine Learning (ICML)",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "The function ψ in (1) acts as a regularizer; it is typically chosen to be the squared l2-norm, which is smooth, or to be a non-differentiable penalty such as the l1-norm or another sparsity-inducing norm [2].",
      "startOffset" : 204,
      "endOffset" : 207
    }, {
      "referenceID" : 10,
      "context" : "Composite minimization also encompasses constrained minimization if we consider extended-valued indicator functions ψ that may take the value +∞ outside of a convex set C and 0 inside (see [11]).",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 16,
      "context" : "By “accelerating”, we mean generalizing a mechanism invented by Nesterov [17] that improves the convergence rate of the gradient descent algorithm.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "These rates were shown by Nesterov [16] to be suboptimal for the class of first-order methods, and instead optimal rates—O(1/k(2)) for the convex case and O((1 − √",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "Later, this acceleration technique was extended to deal with non-differentiable regularization functions ψ [4, 19].",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "Later, this acceleration technique was extended to deal with non-differentiable regularization functions ψ [4, 19].",
      "startOffset" : 107,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "For modern machine learning problems involving a large sum of n functions, a recent effort has been devoted to developing fast incremental algorithms [6, 7, 14, 24, 25, 27] that can exploit the particular",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 6,
      "context" : "For modern machine learning problems involving a large sum of n functions, a recent effort has been devoted to developing fast incremental algorithms [6, 7, 14, 24, 25, 27] that can exploit the particular",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "For modern machine learning problems involving a large sum of n functions, a recent effort has been devoted to developing fast incremental algorithms [6, 7, 14, 24, 25, 27] that can exploit the particular",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 23,
      "context" : "For modern machine learning problems involving a large sum of n functions, a recent effort has been devoted to developing fast incremental algorithms [6, 7, 14, 24, 25, 27] that can exploit the particular",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "For modern machine learning problems involving a large sum of n functions, a recent effort has been devoted to developing fast incremental algorithms [6, 7, 14, 24, 25, 27] that can exploit the particular",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 26,
      "context" : "For modern machine learning problems involving a large sum of n functions, a recent effort has been devoted to developing fast incremental algorithms [6, 7, 14, 24, 25, 27] that can exploit the particular",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 3,
      "context" : "This is the case for full gradient [4, 19] and block coordinate descent methods [18, 21], which already have well-known accelerated variants.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "This is the case for full gradient [4, 19] and block coordinate descent methods [18, 21], which already have well-known accelerated variants.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "This is the case for full gradient [4, 19] and block coordinate descent methods [18, 21], which already have well-known accelerated variants.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "This is the case for full gradient [4, 19] and block coordinate descent methods [18, 21], which already have well-known accelerated variants.",
      "startOffset" : 80,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "More importantly, it also applies to incremental algorithms such as SAG [24], SAGA [6], Finito/MISO [7, 14], SDCA [25], and SVRG [27].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "More importantly, it also applies to incremental algorithms such as SAG [24], SAGA [6], Finito/MISO [7, 14], SDCA [25], and SVRG [27].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 6,
      "context" : "More importantly, it also applies to incremental algorithms such as SAG [24], SAGA [6], Finito/MISO [7, 14], SDCA [25], and SVRG [27].",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "More importantly, it also applies to incremental algorithms such as SAG [24], SAGA [6], Finito/MISO [7, 14], SDCA [25], and SVRG [27].",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 24,
      "context" : "More importantly, it also applies to incremental algorithms such as SAG [24], SAGA [6], Finito/MISO [7, 14], SDCA [25], and SVRG [27].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 26,
      "context" : "More importantly, it also applies to incremental algorithms such as SAG [24], SAGA [6], Finito/MISO [7, 14], SDCA [25], and SVRG [27].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 25,
      "context" : "It was only known to be the case for dual coordinate ascent approaches such as SDCA [26] or SDPC [28] for strongly convex objectives.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 27,
      "context" : "It was only known to be the case for dual coordinate ascent approaches such as SDCA [26] or SDPC [28] for strongly convex objectives.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 24,
      "context" : "A classical trick to apply them to general convex functions is to add a small regularization ε‖x‖2 [25].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "The approach Finito/MISO, which was proposed in [7] and [14], is an incremental technique for solving smooth unconstrained μ-strongly convex problems when n is larger than a constant βL/μ (with β = 2 in [14]).",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 13,
      "context" : "The approach Finito/MISO, which was proposed in [7] and [14], is an incremental technique for solving smooth unconstrained μ-strongly convex problems when n is larger than a constant βL/μ (with β = 2 in [14]).",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "The approach Finito/MISO, which was proposed in [7] and [14], is an incremental technique for solving smooth unconstrained μ-strongly convex problems when n is larger than a constant βL/μ (with β = 2 in [14]).",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 24,
      "context" : "The resulting algorithm can be interpreted as a variant of proximal SDCA [25] with a different step size and a more practical optimality certificate—that is, checking the optimality condition does not require evaluating a dual objective.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "The catalyst acceleration can be interpreted as a variant of the proximal point algorithm [3, 9], which is a central concept in convex optimization, underlying augmented Lagrangian approaches, and composite minimization schemes [5, 20].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "The catalyst acceleration can be interpreted as a variant of the proximal point algorithm [3, 9], which is a central concept in convex optimization, underlying augmented Lagrangian approaches, and composite minimization schemes [5, 20].",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "The catalyst acceleration can be interpreted as a variant of the proximal point algorithm [3, 9], which is a central concept in convex optimization, underlying augmented Lagrangian approaches, and composite minimization schemes [5, 20].",
      "startOffset" : 228,
      "endOffset" : 235
    }, {
      "referenceID" : 19,
      "context" : "The catalyst acceleration can be interpreted as a variant of the proximal point algorithm [3, 9], which is a central concept in convex optimization, underlying augmented Lagrangian approaches, and composite minimization schemes [5, 20].",
      "startOffset" : 228,
      "endOffset" : 235
    }, {
      "referenceID" : 8,
      "context" : "In general, these auxiliary problems cannot be solved with perfect accuracy, and several notations of inexactness were proposed, including [9, 10, 22].",
      "startOffset" : 139,
      "endOffset" : 150
    }, {
      "referenceID" : 9,
      "context" : "In general, these auxiliary problems cannot be solved with perfect accuracy, and several notations of inexactness were proposed, including [9, 10, 22].",
      "startOffset" : 139,
      "endOffset" : 150
    }, {
      "referenceID" : 21,
      "context" : "In general, these auxiliary problems cannot be solved with perfect accuracy, and several notations of inexactness were proposed, including [9, 10, 22].",
      "startOffset" : 139,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "The catalyst approach hinges upon (i) an acceleration technique for the proximal point algorithm originally introduced in the pioneer work [9]; (ii) a more practical inexactness criterion than those proposed in the past.",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "In turn, we are also able to obtain the computational complexity of the global procedure for solving (1), which was not possible with previous analysis [9, 10, 22].",
      "startOffset" : 152,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "In turn, we are also able to obtain the computational complexity of the global procedure for solving (1), which was not possible with previous analysis [9, 10, 22].",
      "startOffset" : 152,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : "In turn, we are also able to obtain the computational complexity of the global procedure for solving (1), which was not possible with previous analysis [9, 10, 22].",
      "startOffset" : 152,
      "endOffset" : 163
    }, {
      "referenceID" : 8,
      "context" : "Beyond [9], several works have inspired this paper.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 25,
      "context" : "In particular, accelerated SDCA [26] is an instance of an inexact accelerated proximal point algorithm, even though this was not explicitly stated in [26].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 25,
      "context" : "In particular, accelerated SDCA [26] is an instance of an inexact accelerated proximal point algorithm, even though this was not explicitly stated in [26].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 16,
      "context" : "Specifically, we use the concept of estimate sequence from Nesterov [17], whereas the direct proof of [26], in the context of SDCA, does not extend to non-strongly convex objectives.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 25,
      "context" : "Specifically, we use the concept of estimate sequence from Nesterov [17], whereas the direct proof of [26], in the context of SDCA, does not extend to non-strongly convex objectives.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : "Another useful methodological contribution was the convergence analysis of inexact proximal gradient methods of [23].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 7,
      "context" : "Finally, similar ideas appear in the independent work [8].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "Another independent work with related results is [13], which introduce an accelerated method for the minimization of finite sums, which is not based on the proximal point algorithm.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "(1)Note that our inexact criterion was also studied, among others, in [22], but the analysis of [22] led to the conjecture that this criterion was too weak to warrant acceleration.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "(1)Note that our inexact criterion was also studied, among others, in [22], but the analysis of [22] led to the conjecture that this criterion was too weak to warrant acceleration.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "Such methods should be contrasted with stochastic gradient methods [15, 12], which minimize a different non-deterministic function.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "Such methods should be contrasted with stochastic gradient methods [15, 12], which minimize a different non-deterministic function.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 16,
      "context" : "Similar to the classical gradient descent scheme of Nesterov [17], Algorithm 1 involves an extrapolation step (6).",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "As shown in [9], this step is in fact sufficient to reduce the number of iterations of Algorithm 1 to solve (1) when εk = 0—that is, for running the exact accelerated proximal point algorithm.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : "This is where our approach differs from the classical proximal point algorithm of [9].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "The proof of convergence uses the concept of estimate sequence invented by Nesterov [17], and introduces an extension to deal with the errors (εk)k≥0.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 22,
      "context" : "To control the accumulation of errors, we borrow the methodology of [23] for inexact proximal gradient algorithms.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "Note that estimate sequences were also used in [9], but, as noted by [22], the proof of [9] only applies when using an extrapolation step (6) that involves the true minimizer of (5), which is unknown in practice.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 21,
      "context" : "Note that estimate sequences were also used in [9], but, as noted by [22], the proof of [9] only applies when using an extrapolation step (6) that involves the true minimizer of (5), which is unknown in practice.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 8,
      "context" : "Note that estimate sequences were also used in [9], but, as noted by [22], the proof of [9] only applies when using an extrapolation step (6) that involves the true minimizer of (5), which is unknown in practice.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "This proposition is generic since the assumption (8) is relatively standard for gradient-based methods [17].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "We first show how to accelerate full gradient and randomized coordinate descent algorithms [21].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "Then, we discuss other approaches such as SAG [24], SAGA [6], or SVRG [27].",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Then, we discuss other approaches such as SAG [24], SAGA [6], or SVRG [27].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "Then, we discuss other approaches such as SAG [24], SAGA [6], or SVRG [27].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "Finally, we present a new proximal version of the incremental gradient approaches Finito/MISO [7, 14], along with its accelerated version.",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "Finally, we present a new proximal version of the incremental gradient approaches Finito/MISO [7, 14], along with its accelerated version.",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "Most of the algorithms we consider here, namely the proximal gradient method [4, 19], SAGA [6], (Prox)-SVRG [27], can handle composite objectives with a regularization penalty ψ that admits a proximal operator proxψ , defined for any z as proxψ(z) , argmin y∈Rp {",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "Most of the algorithms we consider here, namely the proximal gradient method [4, 19], SAGA [6], (Prox)-SVRG [27], can handle composite objectives with a regularization penalty ψ that admits a proximal operator proxψ , defined for any z as proxψ(z) , argmin y∈Rp {",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "Most of the algorithms we consider here, namely the proximal gradient method [4, 19], SAGA [6], (Prox)-SVRG [27], can handle composite objectives with a regularization penalty ψ that admits a proximal operator proxψ , defined for any z as proxψ(z) , argmin y∈Rp {",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 26,
      "context" : "Most of the algorithms we consider here, namely the proximal gradient method [4, 19], SAGA [6], (Prox)-SVRG [27], can handle composite objectives with a regularization penalty ψ that admits a proximal operator proxψ , defined for any z as proxψ(z) , argmin y∈Rp {",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 23,
      "context" : "The exception is SAG [24], for which proximal variants are not analyzed.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 20,
      "context" : "A similar result can also be obtained for randomized coordinate descent methods [21].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Otherwise, their complexity becomes O(n log(1/ε)), which is independent of the condition number and seems theoretically optimal [1].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "SDCA [26] and Finito/MISO [7, 14] are actually related to incremental gradient methods, and the choice for κ has a similar form with (a, b) = (1, 1).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "SDCA [26] and Finito/MISO [7, 14] are actually related to incremental gradient methods, and the choice for κ has a similar form with (a, b) = (1, 1).",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "SDCA [26] and Finito/MISO [7, 14] are actually related to incremental gradient methods, and the choice for κ has a similar form with (a, b) = (1, 1).",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "2 Proximal MISO and its Acceleration Finito/MISO was proposed in [7] and [14] for solving the problem (1) when ψ = 0 and when f is a sum of n μ-strongly convex functions fi as in (2), which are also differentiable with L-Lipschitz derivatives.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 13,
      "context" : "2 Proximal MISO and its Acceleration Finito/MISO was proposed in [7] and [14] for solving the problem (1) when ψ = 0 and when f is a sum of n μ-strongly convex functions fi as in (2), which are also differentiable with L-Lipschitz derivatives.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "1 ε ) ) SAGA [6] Finito/MISO-Prox not avail.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 26,
      "context" : "The quantity L for SVRG is the average Lipschitz constant of the functions fi (see [27]).",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "which was also proposed in [7] without a convergence proof.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 24,
      "context" : "The resulting algorithm turns out to be very close to variant “5” of proximal SDCA [25], which corresponds to using a different value for δ.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "We evaluate the Catalyst acceleration on three methods that have never been accelerated in the past: SAG [24], SAGA [6], and MISO-Prox.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 5,
      "context" : "We evaluate the Catalyst acceleration on three methods that have never been accelerated in the past: SAG [24], SAGA [6], and MISO-Prox.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "We use three datasets used in [14], namely real-sim, rcv1, and ocr, which are relatively large, with up to n = 2500 000 points for ocr and p = 47 152 variables for rcv1.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : "1/n, which leads significantly larger condition numbers than those used in other studies (μ/L ≈ 1/n in [14, 24]).",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 23,
      "context" : "1/n, which leads significantly larger condition numbers than those used in other studies (μ/L ≈ 1/n in [14, 24]).",
      "startOffset" : 103,
      "endOffset" : 111
    } ],
    "year" : 2015,
    "abstractText" : "We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants. For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.",
    "creator" : null
  }
}