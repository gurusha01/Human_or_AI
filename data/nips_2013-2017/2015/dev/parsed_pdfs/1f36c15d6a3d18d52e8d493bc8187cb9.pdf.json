{
  "name" : "1f36c15d6a3d18d52e8d493bc8187cb9.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Optimal Testing for Properties of Distributions",
    "authors" : [ "Jayadev Acharya", "Constantinos Daskalakis", "Gautam Kamath" ],
    "emails" : [ "g}@mit.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The quintessential scientific question is whether an unknown object has some property, i.e. whether a model from a specific class fits the object’s observed behavior. If the unknown object is a probability distribution, p, to which we have sample access, we are typically asked to distinguish whether p belongs to some class C or whether it is sufficiently far from it. This question has received tremendous attention in the field of statistics (see, e.g., [1, 2]), where test statistics for important properties such as the ones we consider here have been proposed. Nevertheless, the emphasis has been on asymptotic analysis, characterizing the rates of convergence of test statistics under null hypotheses, as the number of samples tends to infinity. In contrast, we wish to study the following problem in the small sample regime:\n⇧(C, \"): Given a family of distributions C, some \" > 0, and sample access to an unknown distribution p over a discrete support, how many samples are required to distinguish between p 2 C versus dTV(p, C) > \"?1\nThe problem has been studied intensely in the literature on property testing and sublinear algorithms [3, 4, 5], where the emphasis has been on characterizing the optimal tradeoff between p’s support size and the accuracy \" in the number of samples. Several results have been obtained, roughly\n1We want success probability at least 2/3, which can be boosted to 1 by repeating the test O(log(1/ )) times and taking the majority.\nclustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].\nWith respect to (iii), [13] exactly characterizes the number of samples required to test identity to each distribution q, providing a single tester matching this bound simultaneously for all q. Nevertheless, this tester and its precursors are not applicable to the composite identity testing problem that we consider. If our class C were finite, we could test against each element in the class, albeit this would not necessarily be sample optimal. If our class C were a continuum, we would need tolerant identity testers, which tend to be more expensive in terms of sample complexity [12], and result in substantially suboptimal testers for the classes we consider. Or we could use approaches related to generalized likelihood ratio test, but their behavior is not well-understood in our regime, and optimizing likelihood over our classes becomes computationally intense.\nOur Contributions We obtain sample-optimal and computationally efficient testers for ⇧(C, \") for the most fundamental shape restrictions to a distribution. Our contributions are the following:\n1. For a known distribution q over [n], and sample access to p, we show that distinguishing the cases: (a) whether the 2-distance between p and q is at most \"2/2, versus (b) the `1 distance between p and q is at least 2\", requires ⇥( p n/\"2) samples. As a corollary, we obtain an alternate\nargument that shows that identity testing requires ⇥( p n/\"2) samples (previously shown in [13]).\n2. For the class C = Md n of monotone distributions over [n]d we require an optimal ⇥ nd/2/\"2\nnumber of samples, where prior work requires ⌦ p n log n/\"6\nsamples for d = 1 and ˜ ⌦ nd 1/2poly (1/\") for d > 1 [6, 7]. Our results improve the exponent of n with respect to d, shave all logarithmic factors in n, and improve the exponent of \" by at least a factor of 2. (a) A useful building block and interesting byproduct of our analysis is extending Birgé’s obliv-\nious decomposition for single-dimensional monotone distributions [14] to monotone distributions in d 1, and to the stronger notion of 2-distance. See Section C.1.\n(b) Moreover, we show that O(logd n) samples suffice to learn a monotone distribution over [n]d in 2-distance. See Lemma 3 for the precise statement.\n3. For the class C = ⇧ d of product distributions over [n1] ⇥ · · · ⇥ [nd], our algorithm requires O ( Q ` n ` ) 1/2 + P ` n ` /\"2\nsamples. We note that a product distribution is one where all marginals are independent, so this is equivalent to testing if a collection of random variables are all independent. In the case where n\n` ’s are large, then the first term dominates, and the sample complexity is O(( Q ` n ` ) 1/2 /\"2). In particular, when d is a constant and all n `\n’s are equal to n, we achieve the optimal sample complexity of ⇥(nd/2/\"2). To the best of our knowledge, this is the first result for d 3, and when d = 2, this improves the previously known complexity from O n\n\"\n6 polylog(n/\") [8, 15], significantly improving the dependence on \" and shaving all logarithmic factors.\n4. For the classes C = LCD n , C = MHR n and C = U n\nof log-concave, monotone-hazard-rate and unimodal distributions over [n], we require an optimal ⇥ p n/\"2 number of samples. Our testers for LCD n and C = MHR n\nare to our knowledge the first for these classes for the low sample regime we are studying—see [16] and its references for statistics literature on the asymptotic regime. Our tester for U\nn improves the dependence of the sample complexity on \" by at least a factor of 2 in the exponent, and shaves all logarithmic factors in n, compared to testers based on testing monotonicity. (a) A useful building block and important byproduct of our analysis are the first computation-\nally efficient algorithms for properly learning log-concave and monotone-hazard-rate distributions, to within \" in total variation distance, from poly(1/\") samples, independent of the domain size n. See Corollaries 4 and 6. Again, these are the first computationally efficient algorithms to our knowledge in the low sample regime. [17] provide algorithms for density estimation, which are non-proper, i.e. will approximate an unknown distribution from these classes with a distribution that does not belong to these classes. On the other hand, the statistics literature focuses on maximum-likelihood estimation in the asymptotic regime—see e.g. [18] and its references.\n5. For all the above classes we obtain matching lower bounds, showing that the sample complexity of our testers is optimal with respect to n, \" and when applicable d. See Section 8. Our lower bounds are based on extending Paninski’s lower bound for testing uniformity [10].\nOur Techniques At the heart of our tester lies a novel use of the 2 statistic. Naturally, the 2 and its related `2 statistic have been used in several of the afore-cited results. We propose a new use of the 2 statistic enabling our optimal sample complexity. The essence of our approach is to first draw a small number of samples (independent of n for log-concave and monotone-hazard-rate distributions and only logarithmic in n for monotone and unimodal distributions) to approximate the unknown distribution p in 2 distance. If p 2 C, our learner is required to output a distribution q that is O(\")-close to C in total variation and O(\"2)-close to p in 2 distance. Then some analysis reduces our testing problem to distinguishing the following cases:\n• p and q are O(\"2)-close in 2 distance; this case corresponds to p 2 C. • p and q are ⌦(\")-far in total variation distance; this case corresponds to dTV(p, C) > \".\nWe draw a comparison with robust identity testing, in which one must distinguish whether p and q are c1\"-close or c2\"-far in total variation distance, for constants c2 > c1 > 0. In [12], Valiant and Valiant show that ⌦(n/ log n) samples are required for this problem – a nearly-linear sample complexity, which may be prohibitively large in many settings. In comparison, the problem we study tests for 2 closeness rather than total variation closeness: a relaxation of the previous problem. However, our tester demonstrates that this relaxation allows us to achieve a substantially sublinear complexity of O( p n/\"2). On the other hand, this relaxation is still tight enough to be useful, demonstrated by our application in obtaining sample-optimal testers.\nWe note that while the 2 statistic for testing hypothesis is prevalent in statistics providing optimal error exponents in the large-sample regime, to the best of our knowledge, in the small-sample regime, modified-versions of the 2 statistic have only been recently used for closeness-testing in [19, 20, 21] and for testing uniformity of monotone distributions in [22]. In particular, [19] design an unbiased statistic for estimating the 2 distance between two unknown distributions.\nOrganization In Section 4, we show that a version of the 2 statistic, appropriately excluding certain elements of the support, is sufficiently well-concentrated to distinguish between the above cases. Moreover, the sample complexity of our algorithm is optimal for most classes. Our base tester is combined with the afore-mentioned extension of Birgé’s decomposition theorem to test monotone distributions in Section 5 (see Theorem 2 and Corollary 1), and is also used to test independence of distributions in Section 6 (see Theorem 3).\nIn Section 7, we give our results on testing unimodal, log-concave and monotone hazard rate distributions. Naturally, there are several bells and whistles that we need to add to the above skeleton to accommodate all classes of distributions that we are considering. In Remark 1 we mention the additional modifications for these classes.\nRelated Work. For the problems that we study in this paper, we have provided the related works in the previous section along with our contributions. We cannot do justice to the role of shape restrictions of probability distributions in probabilistic modeling and testing. It suffices to say that the classes of distributions that we study are fundamental, motivating extensive literature on their learning and testing [23]. In the recent times, there has been work on shape restricted statistics, pioneered by Jon Wellner, and others. [24, 25] study estimation of monotone and k-monotone densities, and [26, 27] study estimation of log-concave distributions. Due to the sheer volume of literature in statistics in this field, we will restrict ourselves to those already referenced.\nAs we have mentioned, statistics has focused on the asymptotic regime as the number of samples tends to infinity. Instead we are considering the low sample regime and are more stringent about the behavior of our testers, requiring 2-sided guarantees. We want to accept if the unknown distribution is in our class of interest, and also reject if it is far from the class. For this problem, as discussed above, there are few results when C is a whole class of distributions. Closer related to our paper is the line of papers [6, 7, 28] for monotonicity testing, albeit these papers have sub-optimal sample complexity as discussed above. Testing independence of random variables has a long history in statisics [29, 30]. The theoretical computer science community has also considered the problem of\ntesting independence of two random variables [8, 15]. While our results sharpen the case where the variables are over domains of equal size, they demonstrate an interesting asymmetric upper bound when this is not the case. More recently, Acharya and Daskalakis provide optimal testers for the family of Poisson Binomial Distributions [31].\nFinally, contemporaneous work of Canonne et al [32] provides a generic algorithm and lower bounds for the single-dimensional families of distributions considered here. We note that their algorithm has a sample complexity which is suboptimal in both n and \", while our algorithms are optimal. Their algorithm also extends to mixtures of these classes, though some of these extensions are not computationally efficient. They also provide a framework for proving lower bounds, giving the optimal bounds for many classes when \" is sufficiently large with respect to 1/n. In comparison, we provide these lower bounds unconditionally by modifying Paninski’s construction [10] to suit the classes we consider."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We use the following probability distances in our paper.\nThe total variation distance between distributions p and q is dTV(p, q) def = sup\nA |p(A) q(A)| = 1 2kp qk1. The 2-distance between p and q over [n] is defined as 2(p, q) def = P i2[n] (pi qi)2 qi\n. The Kolmogorov distance between two probability measures p and q over an ordered set (e.g., R) with cumulative density functions F\np and F q is dK(p, q) def = sup x2R |Fp(x) Fq(x)|. Our paper is primarily concerned with testing against classes of distributions, defined formally as: Definition 1. Given \" 2 (0, 1] and sample access to a distribution p, an algorithm is said to test a class C if it has the following guarantees:\n• If p 2 C, the algorithm outputs ACCEPT with probability at least 2/3; • If dTV(p, C) \", the algorithm outputs REJECT with probability at least 2/3.\nWe note the following useful relationships between these distances [33]: Proposition 1. dK(p, q)2  dTV(p, q)2  14 2(p, q). Definition 2. An ⌘-effective support of a distribution p is any set S such that p(S) 1 ⌘. The flattening of a function f over a subset S is the function ¯f such that ¯f\ni = p(S)/|S|. Definition 3. Let p be a distribution, and support I1, . . . is a partition of the domain. The flattening of p with respect to I1, . . . is the distribution p̄ which is the flattening of p over the intervals I1, . . ..\nPoisson Sampling Throughout this paper, we use the standard Poissonization approach. Instead of drawing exactly m samples from a distribution p, we first draw m0 ⇠ Poisson(m), and then draw m0 samples from p. As a result, the number of times different elements in the support of p occur in the sample become independent, giving much simpler analyses. In particular, the number of times we will observe domain element i will be distributed as Poisson(mpi), independently for each i. Since Poisson(m) is tightly concentrated around m, this additional flexibility comes only at a sub-constant cost in the sample complexity with an inversely exponential in m, additive increase in the error probability."
    }, {
      "heading" : "3 The Testing Algorithm – An Overview",
      "text" : "Our algorithm for testing a class C can be decomposed into three steps. Near-proper learning in 2-distance. Our first step requires learning with very specific guarantees. Given sample access to p 2 C, we wish to output q such that (i) q is close to C in total variation distance, and (ii) p and q are O(\"2)-close in 2-distance on an \" effective support2 of p. When\n2We also require the algorithm to output a description of an effective support for which this property holds. This requirement can be slightly relaxed, as we show in our results for testing unimodality.\np is not in C, we do not guarantee anything about q. From an information theoretic standpoint, this problem is harder than learning the distribution in total variation, since 2-distance is more restrictive than total variation distance. Nonetheless, for the structured classes we consider, we are able to learn in 2 by modifying the approaches to learn in total variation.\nComputation of distance to class. The next step is to see if the hypothesis q is close to the class C or not. Since we have an explicit description of q, this step requires no further samples from p, i.e. it is purely computational. If we find that q is far from the class C, then it must be that p 62 C, as otherwise the guarantees from the previous step would imply that q is close to C. Thus, if it is not, we can terminate the algorithm at this point.\n2-testing. At this point, the previous two steps guarantee that our distribution q is such that: - If p 2 C, then p and q are close in 2 distance on a (known) effective support of p; - If dTV(p, C) \", then p and q are far in total variation distance. We can distinguish between these two cases using O( p n/\"2) samples with a simple statistical 2- test, that we describe in Section 4.\nUsing the above three-step approach, our tester, as described in the next section, can directly test monotonicity, log-concavity, and monotone hazard rate. With an extra trick, using Kolmogorov’s max inequality, it can also test unimodality.\n4 A Robust 2-`1 Identity Test\nOur main result in this section is Theorem 1. Theorem 1. Given \" 2 (0, 1], a class of probability distributions C, sample access to a distribution p, and an explicit description of a distribution q, both over [n] with the following properties:\nProperty 1. dTV(q, C)  \"2 .\nProperty 2. If p 2 C, then 2(p, q)  \"2500 . Then there exists an algorithm such that: If p 2 C, it outputs ACCEPT with probability at least 2/3; If dTV(p, C) \", it outputs REJECT with probability at least 2/3. The time and sample complexity of this algorithm are O p n/\"2 .\nProof. Algorithm 1 describes a 2 testing procedure that gives the guarantee of the theorem.\nAlgorithm 1 Chi-squared testing algorithm 1: Input: \"; an explicit distribution q; (Poisson) m samples from a distribution p, where N\ni denotes the number of occurrences of the ith domain element.\n2: A {i : q i \"2/50n} 3: Z P\ni2A (Ni mqi)2 Ni\nmqi\n4: if Z  m\"2/10 return close 5: else return far\nIn Section A we compute the mean and variance of the statistic Z (defined in Algorithm 1) as:\nE [Z] = m · X\ni2A\n(p i q i )\n2\nq i\n= m · 2(pA, qA), Var [Z] = X\ni2A\n 2 p2 i\nq2 i\n+ 4m · pi · (pi qi) 2\nq2 i\n(1)\nwhere by pA and qA we denote respectively the vectors p and q restricted to the coordinates in A, and we slightly abuse notation when we write 2(pA, qA), as these do not then correspond to probability distributions.\nLemma 1 demonstrates the separation in the means of the statistic Z in the two cases of interest, i.e., p 2 C versus dTV(p, C) \", and Lemma 2 shows the separation in the variances in the two cases. These two results are proved in Section B.\nLemma 1. If p 2 C, then E [Z]  m\"2/500. If dTV(p, C) \", then E [Z] m\"2/5. Lemma 2. Let m 20000pn/\"2. If p 2 C then Var [Z]  1500000m2\"4. If dTV(p, C) \", then Var [Z]  1100E[Z]2.\nAssuming Lemmas 1 and 2, Theorem 1 is now a simple application of Chebyshev’s inequality. When p 2 C, we have that E [Z] +p3Var [Z]  ⇣ 1/500 + p 3/500000 ⌘ m\"2  m\"2/200. Thus,\nChebyshev’s inequality gives\nPr ⇥ Z m\"2/10⇤  Pr ⇥Z m\"2/200⇤  Pr h Z E [Z] p3Var [Z]1/2 i  1/3.\nWhen dTV(p, C) \", E [Z] p 3Var [Z] ⇣ 1 p3/100 ⌘ E[Z] 3m\"2/20. Therefore,\nPr ⇥ Z  m\"2/10⇤  Pr ⇥Z  3m\"2/20⇤  Pr h Z E [Z]  p3Var [Z]1/2 i  1/3.\nThis proves the correctness of Algorithm 1. For the running time, we divide the summation in Z into the elements for which N\ni > 0 and N i = 0. When N i = 0, the contribution of the term to the summation is mq\ni , and we can sum them up by subtracting the total probability of all elements appearing at least once from 1.\nRemark 1. To apply Theorem 1, we need to learn distribution in C and find a q that is O(\"2)-close in 2-distance to p. For the class of monotone distributions, we are able to efficiently obtain such a q, which immediately implies sample-optimal learning algorithms for this class. However, for some classes, we may not be able to learn a q with such strong guarantees, and we must consider modifications to our base testing algorithm.\nFor example, for log-concave and monotone hazard rate distributions, we can obtain a distribution q and a set S with the following guarantees:\n- If p 2 C, then 2(p S , q S )  O(\"2) and p(S) 1 O(\"); - If dTV(p, C) \", then dTV(p, q) \"/2. In this scenario, the tester will simply pretend that the support of p and q is S, ignoring any samples and support elements in [n] \\ S. Analysis of this tester is extremely similar to Theorem 1. In particular, we can still show that the statistic Z will be separated in the two cases. When p 2 C, excluding [n] \\ S will only reduce Z. On the other hand, when dTV(p, C) \", since p(S) 1 O(\"), p and q must still be far on the remaining support, and we can show that Z is still sufficiently large. Therefore, a small modification allows us to handle this case with the same sample complexity of O( p n/\"2).\nFor unimodal distributions, we are even unable to identify a large enough subset of the support where the 2 approximation is guaranteed to be tight. But we can show that there exists a light enough piece of the support (in terms of probability mass under p) that we can exclude to make the 2 approximation tight. Given that we only use Chebyshev’s inequality to prove the concentration of the test statistic, it would seem that our lack of knowledge of the piece to exclude would involve a union bound and a corresponding increase in the required number of samples. We avoid this through a careful application of Kolmogorov’s max inequality in our setting. See Theorem 7 of Section 7."
    }, {
      "heading" : "5 Testing Monotonicity",
      "text" : "As the first application of our testing framework, we will demonstrate how to test for monotonicity. Let d 1, and i = (i1, . . . , id), j = (j1, . . . , jd) 2 [n]d. We say i < j if il jl for l = 1, . . . , d. A distribution p over [n]d is monotone (decreasing) if for all i < j, pi  pj3. We follow the steps in the overview. The learning result we show is as follows (proved in Section C).\n3This definition describes monotone non-increasing distributions. By symmetry, identical results hold for monotone non-decreasing distributions.\nLemma 3. Let d 1. There is an algorithm that takes m = O((d log(n)/\"2)d/\"2) samples from a distribution p over [n]d, and outputs a distribution q such that if p is monotone, then with probability at least 5/6, 2(p, q)  \"2500 . Furthermore, the distance of q to monotone distributions can be computed in time poly(m).\nThis accomplishes the first two steps in the overview. In particular, if the distance of q from monotone distributions is more than \"/2, we declare that p is not monotone. Therefore, Property 1 in Theorem 1 is satisfied, and the lemma states that Property 2 holds with probability at least 5/6. We then proceed to the 2 `1 test. At this point, we have precisely the guarantees needed to apply Theorem 1 over [n]d, directly implying our main result of this section: Theorem 2. For any d 1, there exists an algorithm for testing monotonicity over [n]d with sample complexity\nO\nnd/2\n\"2 +\n✓ d log n\n\"2\n◆ d\n· 1 \"2\n!\nand time complexity O nd/2/\"2 + poly(log n, 1/\")d .\nIn particular, this implies the following optimal algorithms for monotonicity testing for all d 1: Corollary 1. Fix any d 1, and suppose \" > pd log n/n1/4. Then there exists an algorithm for testing monotonicity over [n]d with sample complexity O nd/2/\"2 .\nWe note that the class of monotone distributions is the simplest of the classes we consider. We now consider testing for log-concavity, monotone hazard rate, and unimodality, all of which are much more challenging to test. In particular, these classes require a more sophisticated structural understanding, more complex proper 2-learning algorithms, and non-trivial modifications to our 2-tester. We have already given some details on the required adaptations to the tester in Remark 1.\nOur algorithms for learning these classes use convex programming. One of the main challenges is to enforce log-concavity of the PDF when learning LCD\nn (respectively, of the CDF when learning MHR\nn ), while simultaneously enforcing closeness in total variation distance. This involves a careful choice of our variables, and we exploit structural properties of the classes to ensure the soundness of particular Taylor approximations. We encourage the reader to refer to the proofs of Theorems 7, 8, and 9 for more details."
    }, {
      "heading" : "6 Testing Independence of Random Variables",
      "text" : "Let X def= [n1] ⇥ . . . ⇥ [nd], and let ⇧d be the class of all product distributions over X . Similar to learning monotone distributions in 2 distance we prove the following result in Section E. Lemma 4. There is an algorithm that takes O ⇣ ( P d `=1 n`)/\" 2 ⌘\nsamples from a distribution p and outputs a q 2 ⇧\nd such that if p 2 ⇧ d , then with probability at least 5/6, 2(p, q)  O(\"2). The distribution q always satisfies Property 1 since it is in ⇧\nd , and by this lemma, with probability at least 5/6 satisfies Property 2 in Theorem 1. Therefore, we obtain the following result. Theorem 3. For any d 1, there exists an algorithm for testing independence of random variables over [n1]⇥ . . . [nd] with sample and time complexity O ⇣⇣ ( Q d `=1 n`) 1/2 + P d `=1 n` ⌘ /\"2 ⌘ .\nWhen d = 2 and n1 = n2 = n this improves the result of [8] for testing independence of two random variables. Corollary 2. Testing if two distributions over [n] are independent has sample complexity ⇥(n/\"2)."
    }, {
      "heading" : "7 Testing Unimodality, Log-Concavity and Monotone Hazard Rate",
      "text" : "Unimodal distributions over [n] (denoted by U n ) are all distributions p for which there exists an i⇤ such that p\ni is non-decreasing for i  i⇤ and non-increasing for i i⇤. Log-concave distributions over [n] (denoted by LCD\nn ), is the sub-class of unimodal distributions for which p i 1pi+1  p2\ni\n.\nMonotone hazard rate (MHR) distributions over [n] (denoted by MHR n\n), are distributions p with CDF F for which i < j implies fi1 Fi  fj 1 Fj .\nThe following theorem bounds the complexity of testing these classes (for moderate \"). Theorem 4. Suppose \" > n 1/5. For each of the classes, unimodal, log-concave, and MHR, there exists an algorithm for testing the class over [n] with sample complexity O( p n/\"2).\nThis result is a corollary of the specific results for each class, which is proved in the appendix. In particular, a more complete statement for unimodality, log-concavity and monotone-hazard rate, with precise dependence on both n and \" is given in Theorems 7, 8 and 9 respectively. We mention some key points about each class, and refer the reader to the respective appendix for further details.\nTesting Unimodality Using a union bound argument, one can use the results on testing monotonicity to give an algorithm with O p n log n/\"2 samples. However, this is unsatisfactory, since our lower bound (and as we will demonstrate, the true complexity of this problem) is p n/\"2. We overcome the logarithmic barrier introduced by the union bound, by employing a non-oblivious decomposition of the domain, and using Kolmogorov’s max-inequality.\nTesting Log-Concavity The key step is to design an algorithm to learn a log-concave distribution in 2 distance. We formulate the problem as a linear program in the logarithms of the distribution and show that using O(1/\"5) samples, it is possible to output a log-concave distribution that has a 2 distance at most O(\"2) from the underlying log-concave distribution.\nTesting Monotone Hazard Rate For learning MHR distributions in 2 distance, we formulate a linear program in the logarithms of the CDF and show that using O(log(n/\")/\"5) samples, it is possible to output a MHR distribution that has a 2 distance at most O(\"2) from the underlying MHR distribution."
    }, {
      "heading" : "8 Lower Bounds",
      "text" : "We now prove sharp lower bounds for the classes of distributions we consider. We show that the example studied by Paninski [10] to prove lower bounds on testing uniformity can be used to prove lower bounds for the classes we consider. They consider a class Q consisting of 2n/2 distributions defined as follows. Without loss of generality assume that n is even. For each of the 2n/2 vectors z0z1 . . . z n/2 1 2 { 1, 1}n/2, define a distribution q 2 Q over [n] as follows.\nq i =\n( (1+z`c\")\nn for i = 2`+ 1 (1 z`c\")\nn\nfor i = 2`. (2)\nEach distribution in Q has a total variation distance c\"/2 from U n\n, the uniform distribution over [n]. By choosing c to be an appropriate constant, Paninski [10] showed that a distribution picked uniformly at random from Q cannot be distinguished from U\nn\nwith fewer than p n/\"2 samples with\nprobability at least 2/3.\nSuppose C is a class of distributions such that (i) The uniform distribution U n is in C, (ii) For appropriately chosen c, dTV(C,Q) \", then testing C is not easier than distinguishing Un from Q. Invoking [10] immediately implies that testing the class C requires ⌦(pn/\"2) samples. The lower bounds for all the one dimensional distributions will follow directly from this construction, and for testing monotonicity in higher dimensions, we extend this construction to d 1, appropriately. These arguments are proved in Section H, leading to the following lower bounds for testing these classes: Theorem 5.\n• For any d 1, any algorithm for testing monotonicity over [n]d requires ⌦(nd/2/\"2) samples. • For d 1, testing independence over [n1]⇥· · ·⇥[nd] requires ⌦ (n1n2 . . . nd)1/2/\"2 samples. • Testing unimodality, log-concavity, or monotone hazard rate over [n] needs ⌦(pn/\"2) samples."
    } ],
    "references" : [ {
      "title" : "Statistical Methods for Research Workers",
      "author" : [ "R.A. Fisher" ],
      "venue" : "Edinburgh: Oliver and Boyd,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1925
    }, {
      "title" : "Testing statistical hypotheses",
      "author" : [ "E. Lehmann", "J. Romano" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "The art of uninformed decisions: A primer to property testing",
      "author" : [ "E. Fischer" ],
      "venue" : "Science, 2001.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Sublinear-time algorithms",
      "author" : [ "R. Rubinfeld" ],
      "venue" : "International Congress of Mathematicians, 2006.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A survey on distribution testing: your data is big, but is it blue",
      "author" : [ "C.L. Canonne" ],
      "venue" : "ECCC, 2015.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Sublinear algorithms for testing monotone and unimodal distributions",
      "author" : [ "T. Batu", "R. Kumar", "R. Rubinfeld" ],
      "venue" : "Proceedings of STOC, 2004.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Testing monotonicity of distributions over general partial orders",
      "author" : [ "A. Bhattacharyya", "E. Fischer", "R. Rubinfeld", "P. Valiant" ],
      "venue" : "ICS, 2011, pp. 239–252.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Testing random variables for independence and identity",
      "author" : [ "T. Batu", "E. Fischer", "L. Fortnow", "R. Kumar", "R. Rubinfeld", "P. White" ],
      "venue" : "Proceedings of FOCS, 2001.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Testing k-wise and almost k-wise independence",
      "author" : [ "N. Alon", "A. Andoni", "T. Kaufman", "K. Matulef", "R. Rubinfeld", "N. Xie" ],
      "venue" : "Proceedings of STOC, 2007.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A coincidence-based test for uniformity given very sparsely sampled discrete data.",
      "author" : [ "L. Paninski" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Generalized error exponents for small sample universal hypothesis testing",
      "author" : [ "D. Huang", "S. Meyn" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 59, no. 12, pp. 8157–8181, Dec 2013.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Estimating the unseen: An n/ log n-sample estimator for entropy and support size, shown optimal via new CLTs",
      "author" : [ "G. Valiant", "P. Valiant" ],
      "venue" : "Proceedings of STOC, 2011.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An automatic inequality prover and instance optimal identity testing",
      "author" : [ "——" ],
      "venue" : "FOCS, 2014.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Estimating a density under order restrictions: Nonasymptotic minimax risk",
      "author" : [ "L. Birgé" ],
      "venue" : "The Annals of Statistics, vol. 15, no. 3, pp. 995–1012, September 1987.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Testing properties of collections of distributions",
      "author" : [ "R. Levi", "D. Ron", "R. Rubinfeld" ],
      "venue" : "Theory of Computing, vol. 9, no. 8, pp. 295–347, 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Testing for monotone increasing hazard rate",
      "author" : [ "P. Hall", "I. Van Keilegom" ],
      "venue" : "Annals of Statistics, pp. 1109–1137, 2005.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning mixtures of structured distributions over discrete domains",
      "author" : [ "S.O. Chan", "I. Diakonikolas", "R.A. Servedio", "X. Sun" ],
      "venue" : "Proceedings of SODA, 2013.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Theoretical properties of the log-concave maximum likelihood estimator of a multidimensional density",
      "author" : [ "M. Cule", "R. Samworth" ],
      "venue" : "Electronic Journal of Statistics, vol. 4, pp. 254–270, 2010.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Competitive classification and closeness testing",
      "author" : [ "J. Acharya", "H. Das", "A. Jafarpour", "A. Orlitsky", "S. Pan", "A.T. Suresh" ],
      "venue" : "COLT, 2012, pp. 22.1–22.18.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Optimal algorithms for testing closeness of discrete distributions",
      "author" : [ "S. Chan", "I. Diakonikolas", "G. Valiant", "P. Valiant" ],
      "venue" : "SODA, 2014, pp. 1193–1203.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Testing closeness with unequal sized samples",
      "author" : [ "B.B. Bhattacharya", "G. Valiant" ],
      "venue" : "NIPS, 2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A competitive test for uniformity of monotone distributions",
      "author" : [ "J. Acharya", "A. Jafarpour", "A. Orlitsky", "A. Theertha Suresh" ],
      "venue" : "Proceedings of AISTATS, 2013, pp. 57–65.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Estimation of a discrete monotone density",
      "author" : [ "H.K. Jankowski", "J.A. Wellner" ],
      "venue" : "Electronic Journal of Statistics, vol. 3, pp. 1567–1605, 2009.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Estimation of a k-monotone density: characterizations, consistency and minimax lower bounds",
      "author" : [ "F. Balabdaoui", "J.A. Wellner" ],
      "venue" : "Statistica Neerlandica, vol. 64, no. 1, pp. 45–70, 2010.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Maximum likelihood estimation and confidence bands for a discrete log-concave distribution",
      "author" : [ "F. Balabdaoui", "H. Jankowski", "K. Rufibach" ],
      "venue" : "2011. [Online]. Available: http://arxiv.org/abs/1107.3904v1",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Log-concavity and strong log-concavity: a review",
      "author" : [ "A. Saumard", "J.A. Wellner" ],
      "venue" : "Statistics Surveys, vol. 8, pp. 45–114, 2014.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Testing monotone continuous distributions on highdimensional real cubes",
      "author" : [ "M. Adamaszek", "A. Czumaj", "C. Sohler" ],
      "venue" : "SODA, 2010, pp. 56–65.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The analysis of categorical data from complex sample surveys",
      "author" : [ "J.N. Rao", "A.J. Scott" ],
      "venue" : "Journal of the American Statistical Association, vol. 76, no. 374, pp. 221–230, 1981.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Testing Poisson Binomial Distributions",
      "author" : [ "J. Acharya", "C. Daskalakis" ],
      "venue" : "SODA, 2015, pp. 1829–1840.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Testing shape restrictions of discrete distributions",
      "author" : [ "C. Canonne", "I. Diakonikolas", "T. Gouleakis", "R. Rubinfeld" ],
      "venue" : "STACS, 2016.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On choosing and bounding probability metrics",
      "author" : [ "A.L. Gibbs", "F.E. Su" ],
      "venue" : "International Statistical Review, vol. 70, no. 3, pp. 419–435, dec 2002.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Efficient compression of monotone and m-modal distributions",
      "author" : [ "J. Acharya", "A. Jafarpour", "A. Orlitsky", "A.T. Suresh" ],
      "venue" : "ISIT, 2014.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On learning distributions from their samples",
      "author" : [ "S. Kamath", "A. Orlitsky", "D. Pichapati", "A.T. Suresh" ],
      "venue" : "COLT, 2015.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality",
      "author" : [ "P. Massart" ],
      "venue" : "The Annals of Probability, vol. 18, no. 3, pp. 1269–1283, 07 1990. 9",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : ", [1, 2]), where test statistics for important properties such as the ones we consider here have been proposed.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : ", [1, 2]), where test statistics for important properties such as the ones we consider here have been proposed.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 2,
      "context" : "The problem has been studied intensely in the literature on property testing and sublinear algorithms [3, 4, 5], where the emphasis has been on characterizing the optimal tradeoff between p’s support size and the accuracy \" in the number of samples.",
      "startOffset" : 102,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "The problem has been studied intensely in the literature on property testing and sublinear algorithms [3, 4, 5], where the emphasis has been on characterizing the optimal tradeoff between p’s support size and the accuracy \" in the number of samples.",
      "startOffset" : 102,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "The problem has been studied intensely in the literature on property testing and sublinear algorithms [3, 4, 5], where the emphasis has been on characterizing the optimal tradeoff between p’s support size and the accuracy \" in the number of samples.",
      "startOffset" : 102,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 220
    }, {
      "referenceID" : 8,
      "context" : "clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].",
      "startOffset" : 214,
      "endOffset" : 220
    }, {
      "referenceID" : 7,
      "context" : "clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].",
      "startOffset" : 345,
      "endOffset" : 360
    }, {
      "referenceID" : 9,
      "context" : "clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].",
      "startOffset" : 345,
      "endOffset" : 360
    }, {
      "referenceID" : 10,
      "context" : "clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].",
      "startOffset" : 345,
      "endOffset" : 360
    }, {
      "referenceID" : 12,
      "context" : "clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [6, 7]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [8, 9]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [8, 10, 11, 13].",
      "startOffset" : 345,
      "endOffset" : 360
    }, {
      "referenceID" : 12,
      "context" : "With respect to (iii), [13] exactly characterizes the number of samples required to test identity to each distribution q, providing a single tester matching this bound simultaneously for all q.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 11,
      "context" : "If our class C were a continuum, we would need tolerant identity testers, which tend to be more expensive in terms of sample complexity [12], and result in substantially suboptimal testers for the classes we consider.",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "As a corollary, we obtain an alternate argument that shows that identity testing requires ⇥( p n/\"2) samples (previously shown in [13]).",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 13,
      "context" : "(a) A useful building block and interesting byproduct of our analysis is extending Birgé’s oblivious decomposition for single-dimensional monotone distributions [14] to monotone distributions in d 1, and to the stronger notion of 2-distance.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 7,
      "context" : "\" 6 polylog(n/\") [8, 15], significantly improving the dependence on \" and shaving all logarithmic factors.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "\" 6 polylog(n/\") [8, 15], significantly improving the dependence on \" and shaving all logarithmic factors.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "n are to our knowledge the first for these classes for the low sample regime we are studying—see [16] and its references for statistics literature on the asymptotic regime.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "[17] provide algorithms for density estimation, which are non-proper, i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "Our lower bounds are based on extending Paninski’s lower bound for testing uniformity [10].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : "In [12], Valiant and Valiant show that ⌦(n/ log n) samples are required for this problem – a nearly-linear sample complexity, which may be prohibitively large in many settings.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "We note that while the 2 statistic for testing hypothesis is prevalent in statistics providing optimal error exponents in the large-sample regime, to the best of our knowledge, in the small-sample regime, modified-versions of the 2 statistic have only been recently used for closeness-testing in [19, 20, 21] and for testing uniformity of monotone distributions in [22].",
      "startOffset" : 296,
      "endOffset" : 308
    }, {
      "referenceID" : 19,
      "context" : "We note that while the 2 statistic for testing hypothesis is prevalent in statistics providing optimal error exponents in the large-sample regime, to the best of our knowledge, in the small-sample regime, modified-versions of the 2 statistic have only been recently used for closeness-testing in [19, 20, 21] and for testing uniformity of monotone distributions in [22].",
      "startOffset" : 296,
      "endOffset" : 308
    }, {
      "referenceID" : 20,
      "context" : "We note that while the 2 statistic for testing hypothesis is prevalent in statistics providing optimal error exponents in the large-sample regime, to the best of our knowledge, in the small-sample regime, modified-versions of the 2 statistic have only been recently used for closeness-testing in [19, 20, 21] and for testing uniformity of monotone distributions in [22].",
      "startOffset" : 296,
      "endOffset" : 308
    }, {
      "referenceID" : 21,
      "context" : "We note that while the 2 statistic for testing hypothesis is prevalent in statistics providing optimal error exponents in the large-sample regime, to the best of our knowledge, in the small-sample regime, modified-versions of the 2 statistic have only been recently used for closeness-testing in [19, 20, 21] and for testing uniformity of monotone distributions in [22].",
      "startOffset" : 365,
      "endOffset" : 369
    }, {
      "referenceID" : 18,
      "context" : "In particular, [19] design an unbiased statistic for estimating the 2 distance between two unknown distributions.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 22,
      "context" : "[24, 25] study estimation of monotone and k-monotone densities, and [26, 27] study estimation of log-concave distributions.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 23,
      "context" : "[24, 25] study estimation of monotone and k-monotone densities, and [26, 27] study estimation of log-concave distributions.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "[24, 25] study estimation of monotone and k-monotone densities, and [26, 27] study estimation of log-concave distributions.",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 25,
      "context" : "[24, 25] study estimation of monotone and k-monotone densities, and [26, 27] study estimation of log-concave distributions.",
      "startOffset" : 68,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "Closer related to our paper is the line of papers [6, 7, 28] for monotonicity testing, albeit these papers have sub-optimal sample complexity as discussed above.",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Closer related to our paper is the line of papers [6, 7, 28] for monotonicity testing, albeit these papers have sub-optimal sample complexity as discussed above.",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "Closer related to our paper is the line of papers [6, 7, 28] for monotonicity testing, albeit these papers have sub-optimal sample complexity as discussed above.",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 27,
      "context" : "Testing independence of random variables has a long history in statisics [29, 30].",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "testing independence of two random variables [8, 15].",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 14,
      "context" : "testing independence of two random variables [8, 15].",
      "startOffset" : 45,
      "endOffset" : 52
    }, {
      "referenceID" : 28,
      "context" : "More recently, Acharya and Daskalakis provide optimal testers for the family of Poisson Binomial Distributions [31].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 29,
      "context" : "Finally, contemporaneous work of Canonne et al [32] provides a generic algorithm and lower bounds for the single-dimensional families of distributions considered here.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "In comparison, we provide these lower bounds unconditionally by modifying Paninski’s construction [10] to suit the classes we consider.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "We note the following useful relationships between these distances [33]: Proposition 1.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "When d = 2 and n1 = n2 = n this improves the result of [8] for testing independence of two random variables.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "We show that the example studied by Paninski [10] to prove lower bounds on testing uniformity can be used to prove lower bounds for the classes we consider.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "By choosing c to be an appropriate constant, Paninski [10] showed that a distribution picked uniformly at random from Q cannot be distinguished from U",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "Invoking [10] immediately implies that testing the class C requires ⌦(pn/\"2) samples.",
      "startOffset" : 9,
      "endOffset" : 13
    } ],
    "year" : 2015,
    "abstractText" : "Given samples from an unknown discrete distribution p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, as well as in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of discrete distributions such as monotonicity, independence, logconcavity, unimodality, and monotone-hazard rate, the optimal sample complexity is unknown. We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in 2-distance, or far in total variation distance? The optimality of our testers is established by providing matching lower bounds, up to constant factors. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave, monotone hazard rate distributions.",
    "creator" : null
  }
}