{
  "name" : "d9731321ef4e063ebbee79298fa36f56.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian Active Model Selection with an Application to Automated Audiometry",
    "authors" : [ "Jacob R. Gardner", "Gustavo Malkomes", "John P. Cunningham" ],
    "emails" : [ "jrg365@cornell.edu", "luizgustavo@wustl.edu", "garnett@wustl.edu", "kqw4@cornell.edu", "dbarbour@wustl.edu", "jpc2181@columbia.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Personalized medicine has long been a critical application area for machine learning [1–3], in which automated decision making and diagnosis are key components. Beyond improving quality of life, machine learning in diagnostic settings is particularly important because collecting additional medical data often incurs significant financial burden, time cost, and patient discomfort. In machine learning one often considers this problem to be one of active feature selection: acquiring each new feature (e.g., a blood test) incurs some cost, but will, with hope, better inform diagnosis, treatment, and prognosis. By careful analysis, we may optimize this trade off.\nHowever, many diagnostic settings in medicine do not involve feature selection, but rather involve querying a sample space to discriminate different models describing patient attributes. A particular, clarifying example that motivates this work is noise-induced hearing loss (NIHL), a prevalent disorder affecting 26 million working-age adults in the United States alone [4] and affecting over half of workers in particular occupations such as mining and construction. Most tragically, NIHL is entirely preventable with simple, low-cost solutions (e.g., earplugs). The critical requirement for prevention is effective early diagnosis.\nTo be tested for NIHL, patients must complete a time-consuming audiometric exam that presents a series of tones at various frequencies and intensities; at each tone the patient indicates whether he/she hears the tone [5–7]. From the responses, the clinician infers the patient’s audible threshold on a set of discrete frequencies (the audiogram); this process requires the delivery of up to hundreds of tones. Audiologists scan the audiogram for a hearing deficit with a characteristic notch shape—a\nnarrow band that can be anywhere in the frequency domain that is indicative of NIHL. Unfortunately, at early stages of the disorder, notches can be small enough that they are undetectable in a standard audiogram, leaving many cases undiagnosed until the condition has become severe. Increasing audiogram resolution would require higher sample counts (more presented tones) and thus only lengthen an already burdensome procedure. We present here a better approach.\nNote that the NIHL diagnostic challenge is not one of feature selection (choosing the next test to run and classifying the result), but rather of model selection: is this patient’s hearing better described by a normal hearing model, or a notched NIHL model? Here we propose a novel active model selection algorithm to make the NIHL diagnosis in as few tones as possible, which directly reflects the time and personnel resources required to make accurate diagnoses in large populations. We note that this is a model-selection problem in the truest sense: a diagnosis corresponds to selecting between two or more sets of indexed probability distributions (models), rather than the more-common misnomer of choosing an index from within a model (i.e., hyperparameter optimization). In the NIHL case this distinction is critical. We are choosing between two models, the set of possible NIHL hearing functions and the set of normal hearing functions. This approach suggests a very different and more direct algorithm than first learning the most likely NIHL function and then accepting or rejecting it as different from normal, the standard approach.\nWe make the following contributions: first, we design a completely general active-model-selection method based on maximizing the mutual information between the response to a tone and the posterior on the model class. Critically, we develop an analytical approximation of this criterion for Gaussian process (GP) models with arbitrary observation likelihoods, enabling active structure learning for GPs. Second, we extend the work of Gardner et al. [8] (which uses active learning to speed up audiogram inference) to the broader question of identifying which model—normal or NIHL—best fits a given patient. Finally, we develop a novel GP prior mean that parameterizes notched hearing loss for NIHL patients. To our knowledge, this is the first publication with an active model-selection approach that does not require updating each model for every candidate point, allowing audiometric diagnosis of NIHL to be performed in real time. Finally, using patient data from a clinical trial, we show empirically that our method typically automatically detects simulated noise-induced hearing loss with fewer than 15 query tones. This is vastly fewer than the number required to infer a conventional audiogram or even an actively learned audiogram [8], highlighting the importance of both the active-learning approach and our focus on model selection."
    }, {
      "heading" : "2 Bayesian model selection",
      "text" : "We consider supervised learning problems defined on an input space X and an output space Y . Suppose we are given a set of observed data D = (X,y), where X represents the design matrix of independent variables xi ∈ X and y the associated vector of dependent variables yi = y(xi) ∈ Y . LetM be a probabilistic model, and let θ be an element of the parameter space indexingM. Given a set of observations D, we wish to compute the probability ofM being the correct model to explain D, compared to other models. The key quantity of interest to model selection is the model evidence:\np(y | X,M) = ∫ p(y | X, θ,M)p(θ | M) dθ, (1)\nwhich represents the probability of having generating the observed data under the model, marginalized over θ to account for all possible members of that model under a prior p(θ | M) [9]. Given a set of M candidate models {Mi}Mi=1, and the computed evidence for each, we can apply Bayes’ rule to compute the posterior probability of each model given the data:\np(M | D) = p(y | X,M)p(M) p(y | X) = p(y | X,M)p(M)∑ i p(y | X,Mi)p(Mi) , (2)\nwhere p(M) represents the prior probability distribution over the models."
    }, {
      "heading" : "2.1 Active Bayesian model selection",
      "text" : "Suppose that we have a mechanism for actively selecting new data—choosing x∗ ∈ X and observing y∗ = y(x∗)—to add to our dataset D = (X,y), in order to better distinguish the candidate models\n{Mi}. After making this observation, we will form an augmented dataset D′ = D ∪ { (x∗, y∗) }\n, from which we can recompute a new model posterior p(M | D′). An approach motivated by information theory is to select the location maximizing the mutual information between the observation value y∗ and the unknown model:\nI(y∗;M | x∗,D) = H[M | D]− Ey∗ [ H[M | D′] ] (3)\n= H[y∗ | x∗,D]− EM [ H[y∗ | x∗,D,M] ] , (4)\nwhere H indicates (differential) entropy. Whereas Equation (3) is computationally problematic (involving costly model retraining), the equivalent expression (4) is typically more tractable, has been applied fruitfully in various active-learning settings [10, 11, 8, 12, 13], and requires only computing the differential entropy of the model-marginal predictive distribution:\np(y∗ | x∗,D) = M∑ i=1 p(y∗ | x∗,D,Mi)p(Mi | D) (5)\nand the model-conditional predictive distributions { p(y∗ | x∗,D,Mi) } with all models trained with the currently available data. In contrast to (3), this does not involve any retraining cost. Although computing the entropy in (5) might be problematic, we note that this is a one-dimensional integral that can easily be resolved with quadrature. Our proposed approach, which we call Bayesian active model selection (BAMS) is then to compute, for each candidate location x∗, the mutual information between y∗ and the unknown model, and query where this is maximized:\narg max x∗\nI(y∗;M | x∗,D). (6)"
    }, {
      "heading" : "2.2 Related work",
      "text" : "Although active learning and model selection have been widely investigated, active model selection has received comparatively less attention. Ali et al. [14] proposed an active learning model selection method that requires leave-two-out cross validation when evaluating each candidate x∗, requiring O ( B2M |X∗| ) model updates per iteration, where B is the total budget. Kulick et al. [15] also considered an information-theoretic approach to active model selection, suggesting maximizing the expected cross entropy between the current model posterior p(M | D) and the updated distribution p(M | D′). This approach also requires extensive model retraining, with O ( M |X∗| ) model updates per iteration, to estimate this expectation for each candidate. These approaches become prohibitively expensive for real-time applications with large number of candidates. In our audiometric experiments, for example, we consider 10 000 candidate points, expending 1–2 seconds per iteration, whereas these mentioned techniques would take several hours to selected the next point to query."
    }, {
      "heading" : "3 Active model selection for Gaussian processes",
      "text" : "In the previous section, we proposed a general framework for performing sequential active Bayesian model selection, without making any assumptions about the forms of the models {Mi}. Here we will discuss specific details of our proposal when these models represent alternative structures for Gaussian process priors on a latent function.\nWe assume that our observations are generated via a latent function f : X → R with a known observation model p(y | f), where fi = f(xi). A standard nonparametric Bayesian approach with such models is to place a Gaussian process (GP) prior distribution on f , p(f) = GP(f ;µ,K), where µ : X → R is a mean function and K : X 2 → R is a positive-definite covariance function or kernel [16]. We condition on the observed data to form a posterior distribution p(f | D), which is typically an updated Gaussian process (making approximations if necessary). We make predictions at a new input x∗ via the predictive distribution p(y∗ |x∗,D) = ∫ p(y∗ | f∗,D)p(f∗ |x∗,D) df∗, where f∗ = f(x∗). The mean and kernel functions are parameterized by hyperparameters that we concatenate into a vector θ, and different choices of these hyperparameters imply that the functions drawn from the GP will have particular frequency, amplitude, and other properties. Together, µ and K define a model parametrized by the hyperparameters θ. Much attention is paid to learning these hyperparameters in a fixed model class, sometimes under the unfortunate term “model selection.”\nNote, however, that the structural (not hyperparameter) choices made in the mean function µ and covariance function K themselves are typically done by selecting (often blindly!) from several off-the-shelf solutions (see, for example, [17, 16]; though also see [18, 19]), and this choice has substantial bearing on the resulting functions f we can model. Indeed, in many settings, choosing the nature of plausible functions is precisely the problem of model selection; for example, to decide whether the function has periodic structure, exhibits nonstationarity, etc. Our goal is to automatically and actively decide these structural choices during GP modeling through intelligent sampling.\nTo connect to our active learning formulation, let {Mi} be a set of Gaussian process models for the latent function f . Each model comprises a mean function µi, covariance function Ki, and associated hyperparameters θi. Our approach outlined in Section 2.1 requires the computation of three quantities that are not typically encountered in GP modeling and inference: the hyperparameter posterior p(θ | D,M), the model evidence p(y | X,M), and the predictive distribution p(y∗ | x∗,D,M), where we have marginalized over θ in the latter two quantities. The most-common approaches to GP inference are maximum likelihood–II (MLE) or maximum a posteriori–II (MAP) estimation, where we maximize the hyperparameter posterior [20, 16]:1\nθ̂ = arg max θ log p(θ | D,M) = arg max θ log p(θ | M) + log(y | X, θ,M). (7)\nTypically, predictive distributions and other desired quantities are then reported at the MLE/MAP hyperparameters, implicitly making the assumption that p(θ | D,M) ≈ δ(θ̂). Although a computationally convenient choice, this does not account for uncertainty in the hyperparameters, which can be nontrivial with small datasets [9]. Furthermore, accounting correctly for model parameter uncertainty is crucial to model selection, where it naturally introduces a model-complexity penalty. We discuss less-drastic approximations to these quantities below."
    }, {
      "heading" : "3.1 Approximating the model evidence and hyperparameter posterior",
      "text" : "The model evidence p(y | X,M) and hyperparameter posterior distribution p(θ | D,M) are in general intractable for GPs, as there is no conjugate prior distribution p(θ | M) available. Instead, we will use a Laplace approximation, where we make a second-order Taylor expansion of log p(θ | D,M) around its mode θ̂ (7). The result is a multivariate Gaussian approximation:\np(θ | D,M) ≈ N (θ; θ̂,Σ); Σ−1 = −∇2 log p(θ | D,M) ∣∣ θ=θ̂ . (8)\nThe Laplace approximation also results in an approximation to the model evidence:\nlog p(y | X,M) ≈ log p(y | X, θ̂,M) + log p(θ̂ | M)− 12 log det Σ−1 + d2 log 2π, (9) where d is the dimension of θ [21, 22]. The Laplace approximation to the model evidence can be interpreted as rewarding explaining the data well while penalizing model complexity. Note that the Bayesian information criterion (BIC), commonly used for model selection, can be seen as an approximation to the Laplace approximation [23, 24]."
    }, {
      "heading" : "3.2 Approximating the predictive distribution",
      "text" : "We next consider the predictive distribution: p(y∗ | x∗,D,M) = ∫ p(y∗ | f∗) ∫ p(f∗ | x∗,D, θ,M)p(θ | D,M) dθ︸ ︷︷ ︸\np(f∗|x∗,D,M)\ndf∗. (10)\nThe posterior p(f∗ | x∗,D, θ,M) in (10) is typically a known Gaussian distribution, derived analytically for Gaussian observation likelihoods or approximately using standard approximate GP inference techniques [25, 26]. However, the integral over θ in (10) is intractible, even with a Gaussian approximation to the hyperparameter posterior as in (8).\nGarnett et al. [11] introduced a mechanism for approximately marginalizing GP hyperparameters (called the MGP), which we will adopt here due to its strong empirical performance. The MGP assumes\n1Using a noninformative prior p(θ | M) ∝ 1 in the case of maximum likelihood.\nthat we have a Gaussian approximation to the hyperparameter posterior, p(θ | D,M) ≈ N (θ; θ̂,Σ).2 We define the posterior predictive mean and variance functions as\nµ∗(θ) = E[f∗ | x∗,D, θ,M]; ν∗(θ) = Var[f∗ | x∗,D, θ,M]. The MGP works by making an expansion of the predictive distribution around the posterior mean hyperparameters θ̂. The nature of this expansion is chosen so as to match various derivatives of the true predictive distribution; see [11] for details. The posterior distribution of f∗ is approximated by\np(f∗ | x∗,D,M) ≈ N ( f∗;µ∗(θ̂), σ2MGP ) , (11)\nwhere σ2MGP = 4 3ν ∗(θ̂) + [ ∇µ∗(θ̂) ]> Σ [ ∇µ∗(θ̂) ] + 1\n3ν∗(θ̂)\n[ ∇ν∗(θ̂) ]> Σ [ ∇ν∗(θ̂) ] . (12)\nThe MGP thus inflates the predictive variance from the the posterior mean hyperparameters θ̂ by a term that is commensurate with the uncertainty in θ, measured by the posterior covariance Σ, and the dependence of the latent predictive mean and variance on θ, measured by the gradients ∇µ∗ and ∇ν∗. With the Gaussian approximation in (11), the integral in (10) now reduces to integrating the observation likelihood against a univariate Gaussian. This integral is often analytic [16] and at worse requires one-dimensional quadrature."
    }, {
      "heading" : "3.3 Implementation",
      "text" : "Given the development above, we may now efficiently compute an approximation to the BAMS criterion for active GP model selection. Given currently observed data D, for each of our candidate modelsMi, we first find the Laplace approximation to the hyperparameter posterior (8) and model evidence (9). Given the approximations to the model evidence, we may compute an approximation to the model posterior (2). Suppose we have a set of candidate points X∗ from which we may select our next point. For each of our models, we compute the MGP approximation (11) to the latent posteriors { p(f∗ | X∗,D,Mi) } , from which we use standard techniques to compute the predictive\ndistributions { p(y∗ | X∗,D,Mi) } . Finally, with the ability to compute the differential entropies of these model-conditional predictive distributions, as well as the marginal predictive distribution (5), we may compute the mutual information of each candidate in parallel. See the Appendix for explicit formulas for common likelihoods and a description of general-purpose, reusable code we will release in conjunction with this manuscript to ease implementation."
    }, {
      "heading" : "4 Audiometric threshold testing",
      "text" : "Standard audiometric tests [5–7] are calibrated such that the average human subject has a 50% chance of hearing a tone at any frequency; this empirical unit of intensity is defined as 0 dB HL. Humans give binary reports (whether or not a tone was heard) in response to stimuli, and these observations are inherently noisy. Typical audiometric tests present tones in a predefined order on a grid, in increments of 5–10 dB HL at each of six octaves. Recently, Gardner et al. [8] demonstrated that Bayesian active learning of a patient’s audiometric function significantly improves the state-of-the-art in terms of accuracy and number of stimuli required.\nHowever, learning a patient’s entire audiometric function may not always be necessary. Audiometric testing is frequently performed on otherwise young and healthy patients to detect noise-induced hearing loss (NIHL). Noise-induced hearing loss occurs when an otherwise healthy individual is habitually subjected to high-intensity sound [27]. This can result in sharp, notch-shaped hearing loss in a narrow (sometimes less than one octave) frequency range. Early detection of NIHL is critical to desirable long-term clinical outcomes, so large-scale screenings of susceptible populations (for example, factory workers), is commonplace [28]. Noise-induced hearing loss is difficult to diagnose with standard audiometry, because a frequency–intensity grid must be very fine to ensure that a notch is detected. The full audiometric test of Gardner et al. [8] may also be inefficient if the only goal of testing is to determine whether a notch is present, as would be the case for large-scale screening.\nWe cast the detection of noise-induced hearing loss as an active model selection problem. We will describe two Gaussian process models of audiometric functions: a baseline model of normal human\n2This is arbitrary and need not be the Laplace approximation in (8), so this is a slight abuse of notation.\nhearing, and a model reflecting NIHL. We then use the BAMS framework introduced above to, as rapidly as possible for a given patient, determine which model best describes his or her hearing.\nNormal-patient model. To model a healthy patient’s audiometric function, we use the model described in [8]. The GP prior proposed in that work combines a constant prior mean µhealthy = c (modeling a frequency-independent natural threshold) with a kernel taken to be the sum of two components: a linear covariance in intensity and a squared-exponential covariance in frequency. Let [i, φ] represent a tone stimulus, with i representing its intensity and φ its frequency. We define:\nK ( [i, φ], [i′, φ′] ) = αii′ + β exp ( − 12`2 |φ− φ′|2 ) , (13)\nwhere α, β > 0 weight each component and ` > 0 is a length scale of frequency-dependent random deviations from a constant hearing threshold. This kernel encodes two fundamental properties of human audiologic response. First, hearing is monotonic in intensity. The linear contribution αii′ ensures that the posterior probability of detecting a fixed frequency will be monotonically increasing after conditioning on a few tones. Second, human hearing ability is locally smooth in frequency, because nearby locations in the cochlea are mechanically coupled. The combination of µhealthy with K specifies our healthy modelMhealthy, with parameters θhealthy = [c, α, β, `]>. Noise-induced hearing loss model. We extend the model above to create a second GP model reflecting a localized, notch-shaped hearing deficit characteristic of NIHL. We create a novel, flexible prior mean function for this purpose, the parameters of which specify the exact nature of the hearing loss. Our proposed notch mean is: µNIHL(i, φ) = c− dN ′(φ; ν, w2), (14) where N ′(φ; ν, w) denotes the unnormalized normal probability density function with mean ν and standard deviation w, which we scale by a depth parameter d > 0 to reflect the prominence of the hearing loss. This contribution results in a localized subtractive notch feature with tunable center, width, and height. We retain a constant offset c to revert to the normal-hearing model outside the vicinity of the localized hearing deficit. Note that we completely model the effect of NIHL on patient responses with this mean notch function; the kernel K above remains appropriate. The combination of µNIHL with K specifies our NIHL modelMNIHL with, in addition to the parameters of our healthy model, the additional parameters θNIHL = [ν, w, d]>."
    }, {
      "heading" : "5 Results",
      "text" : "To test BAMS on our NIHL detection task, we evaluate our algorithm using audiometric data, comparing to several baselines. From the results of a small-scale clinical trial, we have examples of high-fidelity audiometric functions inferred for several human patients using the method of Gardner et al. [8]. We may use these to simulate audiometric examinations of healthy patients using different methods to select tone presentations. We simulate patients with NIHL by adjusting ground truth inferred from nine healthy patients with in-model samples from our notch mean prior. Recall that high-resolution audiogram data is extremely scarce.\nWe first took a thorough pure-tone audiometric test of each of nine patients from our trial with normal hearing using 100 samples selected using the algorithm in [8] on the domain X = [250, 8000] Hz× [−10, 80] dB HL,3 typical ranges for audiometric testing [6]. We inferred the audiometric function over the entire domain from the measured responses, using the healthy-patient GP modelMhealthy with parameters learned via MLE–II inference. The observation model was p(y = 1 | f) = Φ(f), where Φ is the standard normal CDF, and approximate GP inference was performed via a Laplace approximation. We then used the approximate GP posterior p(f | D, θ̂,Mhealthy) for this patient as ground-truth for simulating a healthy patient’s responses. The posterior probability of tone detection learned from one patient is shown in the background of Figure 1(a). We simulated a healthy patient’s response to a given query tone x∗ = [i∗, φ∗] by sampling a conditionally independent Bernoulli random variable with parameter p(y∗ = 1 | x∗,D, θ̂,Mhealthy). We simulated a patient with NIHL by then drawing notch parameters (the parameters of (14)) from an expert-informed prior, adding the corresponding notch to the learned healthy ground-truth latent mean, recomputing the detection probabilities, and proceeding as above. Example NIHL ground-truth detection probabilities generated in this manner are depicted in the background of Figure 1(b).\n3Inference was done in log-frequency domain."
    }, {
      "heading" : "5.1 Diagnosing NIHL",
      "text" : "To test our active model-selection approach to diagnosing NIHL, we simulated a series of audiometric tests, selecting tones using three alternatives: BAMS, the algorithm of [8], and random sampling.4 Each algorithm shared a candidate set of 10 000 quasirandom tones X∗ generated using a scrambled Halton set so as to densely cover the two-dimensional search space. We simulated nine healthy patients and a total of 27 patients exhibiting a range of NIHL presentations, using independent draws from our notch mean prior in the latter case. For each audiometric test simulation, we initialized with five random tones, then allowed each algorithm to actively select a maximum of 25 additional tones, a very small fraction of the hundreds typically used in a regular audiometric test. We repeated this procedure for each of our nine healthy patients using the normal-patient ground-truth model. We further simulated, for each patient, three separate presentations of NIHL as described above. We plot the posterior probability of the correct model after each iteration for each method in Figure 2.\nIn all runs with both ground-truth models, BAMS was able to rapidly achieve greater than 99% confidence in the correct model without expending the entire budget. Although all methods correctly inferred high healthy posterior probability for the healthy patient, BAMS wass more confident. For the NIHL patients, neither baseline inferred the correct model, whereas BAMS rarely required more than 15 actively chosen samples to confidently make the correct diagnosis. Note that, when BAMS was used on NIHL patients, there was often an initial period during which the healthy model was favored, followed by a rapid shift towards the correct model. This is because our method penalizes the increased complexity of the notch model until sufficient evidence for a notch is acquired.\nFigure 1 shows the samples selected by BAMS for typical healthy and NIHL patients. The fundamental strategy employed by BAMS in this application is logical: it samples in a row of relatively highintensity tones. The intuition for this design is that failure to recognize a normally heard, high-intensity sound is strong evidence of a notch deficit. Once the notch has been found (Figure 1(b)), BAMS continues to sample within the notch to confirm its existence and rule out the possibility of the miss (tone not heard) being due to the stochasticity of the process. Once satisfied, the BAMS approach then samples on the periphery of the notch to further solidify its belief.\nThe BAMS algorithm sequentially makes observations where the healthy and NIHL model disagree the most, typically in the top-center of the MAP notch location. The exact intensity at which BAMS samples is determined by the prior over the notch-depth parameter d. When we changed the notch depth prior to support shallower or deeper notches (data not shown), BAMS sampled at lower or\n4We also compared with uncertainty sampling and query by committee (QBC); the performance was comparable to random sampling and is omitted for clarity.\nhigher intensities, respectively, to continue to maximize model disagreement. Similarly, the spacing between samples is controlled by the prior over the notch-width parameter w.\nFinally, it is worth emphasizing the stark difference between the sampling pattern of BAMS and the audiometric tests of [8]; see Figure 1. Indeed, when the goal is learning the patient’s audiometric function, the audiometric testing algorithm proposed in that work typically has a very good estimate after 20 samples. However, when using BAMS, the primary goal is to detect or rule out NIHL. As a result, the samples selected by BAMS reveal little about the nuances of the patient’s audiometric function, while being highly informative about the correct model to explain the data. This is precisely the tradeoff one seeks in a large-scale diagnostic setting, highlighting the critical importance of focusing on the model-selection problem directly."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduced a novel information-theoretic approach for active model selection, Bayesian active model selection, and successfully applied it to rapid screening for noise-induced hearing loss. Our method for active model selection does not require model retraining to evaluate candidate points, making it more feasible than previous approaches. Further, we provided an effective and efficient analytic approximation to our criterion that can be used for automatically learning the model class of Gaussian processes with arbitrary observation likelihoods, a rich and commonly used class of potential models."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This material is based upon work supported by the National Science Foundation (NSF) under award number IIA-1355406. Additionally, JRG and KQW are supported by NSF grants IIS-1525919, IIS-1550179, and EFMA-1137211; GM is supported by CAPES/BR; DB acknowledges NIH grant R01-DC009215 as well as the CIMIT; JPC acknowledges the Sloan Foundation."
    } ],
    "references" : [ {
      "title" : "Machine Learning for Medical Diagnosis: History",
      "author" : [ "I. Kononenko" ],
      "venue" : "State of the Art and Perspective. Artificial Intelligence in Medicine, 23(1):89–109",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Integration of Early Physiological Responses Predicts Later Illness Severity in Preterm Infants",
      "author" : [ "S. Saria", "A.K. Rajani", "J. Gould", "D.L. Koller", "A.A. Penn" ],
      "venue" : "Science Translational Medicine, 2(48):48ra65",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A Trial of a Real-Time Alert for Clinical Deterioration in Patients Hospitalized on General Medical Wards",
      "author" : [ "T.C. Bailey", "Y. Chen", "Y. Mao", "C. Lu", "G. Hackmann", "S.T. Micek", "K.M. Heard", "K.M. Faulkner", "M.H. Kollef" ],
      "venue" : "Journal of Hospital Medicine, 8(5):236–242",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Change in Prevalence of Hearing Loss in US Adolescents",
      "author" : [ "J. Shargorodsky", "S.G. Curhan", "G.C. Curhan", "R. Eavey" ],
      "venue" : "Journal of the American Medical Association, 304(7):772–778",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Preferred Method for Clinical Determination of Pure-Tone Thresholds",
      "author" : [ "R. Carhart", "J. Jerger" ],
      "venue" : "Journal of Speech and Hearing Disorders, 24(4):330–345",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1959
    }, {
      "title" : "Manual for program outline for rehabilitation of aural casualties both military and civilian",
      "author" : [ "W. Hughson", "H. Westlake" ],
      "venue" : "Transactions of the American Academy of Ophthalmology and Otolaryngology, 48 (Supplement):1–15",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1944
    }, {
      "title" : "Reconstruction of the audiogram using brain stem responses and high-pass noise masking",
      "author" : [ "M. Don", "J.J. Eggermont", "D.E. Brackmann" ],
      "venue" : "Annals of Otology, Rhinology and Laryngology., 88(3 Part 2, Supplement 57):1–20",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Psychophysical Detection Testing with Bayesian Active Learning",
      "author" : [ "J.R. Gardner", "X. Song", "K.Q. Weinberger", "D. Barbour", "J.P. Cunningham" ],
      "venue" : "UAI",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Information Theory",
      "author" : [ "D.J. MacKay" ],
      "venue" : "Inference, and Learning Algorithms. Cambridge University Press",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Collaborative Gaussian Processes for Preference Learning",
      "author" : [ "N. Houlsby", "F. Huszar", "Z. Ghahramani", "J.M. Hernández-Lobato" ],
      "venue" : "NIPS, pages 2096–2104",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Active Learning of Linear Embeddings for Gaussian Processes",
      "author" : [ "R. Garnett", "M.A. Osborne", "P. Hennig" ],
      "venue" : "UAI, pages 230–239",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Predictive Entropy Search for Efficient Global Optimization of Black-box Functions",
      "author" : [ "J.M. Hernández-Lobato", "M.W. Hoffman", "Z. Ghahramani" ],
      "venue" : "NIPS, pages 918–926",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Cold-start Active Learning with Robust Ordinal Matrix Factorization",
      "author" : [ "N. Houlsby", "J.M. Hernández-Lobato", "Z. Ghahramani" ],
      "venue" : "ICML, pages 766–774",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Active Learning with Model Selection",
      "author" : [ "A. Ali", "R. Caruana", "A. Kapoor" ],
      "venue" : "AAAI",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Active Learning of Hyperparameters: An Expected Cross Entropy Criterion for Active Model Selection",
      "author" : [ "J. Kulick", "R. Lieck", "M. Toussaint" ],
      "venue" : "CoRR, abs/1409.7552",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : "MIT Press",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Automatic Model Construction with Gaussian Processes",
      "author" : [ "D. Duvenaud" ],
      "venue" : "PhD thesis, Computational and Biological Learning Laboratory, University of Cambridge",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Structure Discovery in Nonparametric Regression through Compositional Kernel Search",
      "author" : [ "D. Duvenaud", "J.R. Lloyd", "R. Grosse", "J.B. Tenenbaum", "Z. Ghahramani" ],
      "venue" : "ICML, pages 1166–1174",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Fast Kernel Learning for Multidimensional Pattern Extrapolation",
      "author" : [ "A.G. Wilson", "E. Gilboa", "A. Nehorai", "J.P. Cunningham" ],
      "venue" : "NIPS, pages 3626–3634",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Gaussian processes for regression",
      "author" : [ "C. Williams", "C. Rasmussen" ],
      "venue" : "NIPS",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Approximate Bayes Factors and Accounting for Model Uncertainty in Generalised Linear Models",
      "author" : [ "A.E. Raftery" ],
      "venue" : "Biometrika, 83(2):251–266",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "AIC and BIC: Comparisons of Assumptions and Performance",
      "author" : [ "J. Kuha" ],
      "venue" : "Sociological Methods and Research, 33(2):188–229",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Estimating the Dimension of a Model",
      "author" : [ "G. Schwarz" ],
      "venue" : "Annals of Statistics, 6(2):461–464",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Machine Learning: A Probabilistic Perspective",
      "author" : [ "K.P. Murphy" ],
      "venue" : "MIT Press",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Assessing Approximate Inference for Binary Gaussian Process Classification",
      "author" : [ "M. Kuss", "C.E. Rasmussen" ],
      "venue" : "Journal of Machine Learning Research, 6:1679–1704",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Expectation Propagation for Approximate Bayesian Inference",
      "author" : [ "T.P. Minka" ],
      "venue" : "UAI, pages 362–369",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Audiometric notch as a sign of noise induced hearing loss",
      "author" : [ "D. McBride", "S. Williams" ],
      "venue" : "Occupational Environmental Medicine, 58(1):46–51",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The global burden of occupational noise-induced hearing loss",
      "author" : [ "D.I. Nelson", "R.Y. Nelson", "M. Concha-Barrientos", "M. Fingerhut" ],
      "venue" : "American Journal of Industrial Medicine, 48(6):446–458",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "A particular, clarifying example that motivates this work is noise-induced hearing loss (NIHL), a prevalent disorder affecting 26 million working-age adults in the United States alone [4] and affecting over half of workers in particular occupations such as mining and construction.",
      "startOffset" : 184,
      "endOffset" : 187
    }, {
      "referenceID" : 7,
      "context" : "[8] (which uses active learning to speed up audiogram inference) to the broader question of identifying which model—normal or NIHL—best fits a given patient.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "This is vastly fewer than the number required to infer a conventional audiogram or even an actively learned audiogram [8], highlighting the importance of both the active-learning approach and our focus on model selection.",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "which represents the probability of having generating the observed data under the model, marginalized over θ to account for all possible members of that model under a prior p(θ | M) [9].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "Whereas Equation (3) is computationally problematic (involving costly model retraining), the equivalent expression (4) is typically more tractable, has been applied fruitfully in various active-learning settings [10, 11, 8, 12, 13], and requires only computing the differential entropy of the model-marginal predictive distribution:",
      "startOffset" : 212,
      "endOffset" : 231
    }, {
      "referenceID" : 10,
      "context" : "Whereas Equation (3) is computationally problematic (involving costly model retraining), the equivalent expression (4) is typically more tractable, has been applied fruitfully in various active-learning settings [10, 11, 8, 12, 13], and requires only computing the differential entropy of the model-marginal predictive distribution:",
      "startOffset" : 212,
      "endOffset" : 231
    }, {
      "referenceID" : 7,
      "context" : "Whereas Equation (3) is computationally problematic (involving costly model retraining), the equivalent expression (4) is typically more tractable, has been applied fruitfully in various active-learning settings [10, 11, 8, 12, 13], and requires only computing the differential entropy of the model-marginal predictive distribution:",
      "startOffset" : 212,
      "endOffset" : 231
    }, {
      "referenceID" : 11,
      "context" : "Whereas Equation (3) is computationally problematic (involving costly model retraining), the equivalent expression (4) is typically more tractable, has been applied fruitfully in various active-learning settings [10, 11, 8, 12, 13], and requires only computing the differential entropy of the model-marginal predictive distribution:",
      "startOffset" : 212,
      "endOffset" : 231
    }, {
      "referenceID" : 12,
      "context" : "Whereas Equation (3) is computationally problematic (involving costly model retraining), the equivalent expression (4) is typically more tractable, has been applied fruitfully in various active-learning settings [10, 11, 8, 12, 13], and requires only computing the differential entropy of the model-marginal predictive distribution:",
      "startOffset" : 212,
      "endOffset" : 231
    }, {
      "referenceID" : 13,
      "context" : "[14] proposed an active learning model selection method that requires leave-two-out cross validation when evaluating each candidate x∗, requiring O ( B(2)M |X∗| ) model updates per iteration, where B is the total budget.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] also considered an information-theoretic approach to active model selection, suggesting maximizing the expected cross entropy between the current model posterior p(M | D) and the updated distribution p(M | D′).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "A standard nonparametric Bayesian approach with such models is to place a Gaussian process (GP) prior distribution on f , p(f) = GP(f ;μ,K), where μ : X → R is a mean function and K : X 2 → R is a positive-definite covariance function or kernel [16].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 16,
      "context" : "Note, however, that the structural (not hyperparameter) choices made in the mean function μ and covariance function K themselves are typically done by selecting (often blindly!) from several off-the-shelf solutions (see, for example, [17, 16]; though also see [18, 19]), and this choice has substantial bearing on the resulting functions f we can model.",
      "startOffset" : 234,
      "endOffset" : 242
    }, {
      "referenceID" : 15,
      "context" : "Note, however, that the structural (not hyperparameter) choices made in the mean function μ and covariance function K themselves are typically done by selecting (often blindly!) from several off-the-shelf solutions (see, for example, [17, 16]; though also see [18, 19]), and this choice has substantial bearing on the resulting functions f we can model.",
      "startOffset" : 234,
      "endOffset" : 242
    }, {
      "referenceID" : 17,
      "context" : "Note, however, that the structural (not hyperparameter) choices made in the mean function μ and covariance function K themselves are typically done by selecting (often blindly!) from several off-the-shelf solutions (see, for example, [17, 16]; though also see [18, 19]), and this choice has substantial bearing on the resulting functions f we can model.",
      "startOffset" : 260,
      "endOffset" : 268
    }, {
      "referenceID" : 18,
      "context" : "Note, however, that the structural (not hyperparameter) choices made in the mean function μ and covariance function K themselves are typically done by selecting (often blindly!) from several off-the-shelf solutions (see, for example, [17, 16]; though also see [18, 19]), and this choice has substantial bearing on the resulting functions f we can model.",
      "startOffset" : 260,
      "endOffset" : 268
    }, {
      "referenceID" : 19,
      "context" : "The most-common approaches to GP inference are maximum likelihood–II (MLE) or maximum a posteriori–II (MAP) estimation, where we maximize the hyperparameter posterior [20, 16]:1",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "The most-common approaches to GP inference are maximum likelihood–II (MLE) or maximum a posteriori–II (MAP) estimation, where we maximize the hyperparameter posterior [20, 16]:1",
      "startOffset" : 167,
      "endOffset" : 175
    }, {
      "referenceID" : 8,
      "context" : "Although a computationally convenient choice, this does not account for uncertainty in the hyperparameters, which can be nontrivial with small datasets [9].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 20,
      "context" : "log p(y | X,M) ≈ log p(y | X, θ̂,M) + log p(θ̂ | M)− 1 2 log det Σ−1 + d2 log 2π, (9) where d is the dimension of θ [21, 22].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 21,
      "context" : "log p(y | X,M) ≈ log p(y | X, θ̂,M) + log p(θ̂ | M)− 1 2 log det Σ−1 + d2 log 2π, (9) where d is the dimension of θ [21, 22].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "Note that the Bayesian information criterion (BIC), commonly used for model selection, can be seen as an approximation to the Laplace approximation [23, 24].",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 23,
      "context" : "Note that the Bayesian information criterion (BIC), commonly used for model selection, can be seen as an approximation to the Laplace approximation [23, 24].",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 24,
      "context" : "The posterior p(f∗ | x∗,D, θ,M) in (10) is typically a known Gaussian distribution, derived analytically for Gaussian observation likelihoods or approximately using standard approximate GP inference techniques [25, 26].",
      "startOffset" : 210,
      "endOffset" : 218
    }, {
      "referenceID" : 25,
      "context" : "The posterior p(f∗ | x∗,D, θ,M) in (10) is typically a known Gaussian distribution, derived analytically for Gaussian observation likelihoods or approximately using standard approximate GP inference techniques [25, 26].",
      "startOffset" : 210,
      "endOffset" : 218
    }, {
      "referenceID" : 10,
      "context" : "[11] introduced a mechanism for approximately marginalizing GP hyperparameters (called the MGP), which we will adopt here due to its strong empirical performance.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "The nature of this expansion is chosen so as to match various derivatives of the true predictive distribution; see [11] for details.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "This integral is often analytic [16] and at worse requires one-dimensional quadrature.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 7,
      "context" : "[8] demonstrated that Bayesian active learning of a patient’s audiometric function significantly improves the state-of-the-art in terms of accuracy and number of stimuli required.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 26,
      "context" : "Noise-induced hearing loss occurs when an otherwise healthy individual is habitually subjected to high-intensity sound [27].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 27,
      "context" : "Early detection of NIHL is critical to desirable long-term clinical outcomes, so large-scale screenings of susceptible populations (for example, factory workers), is commonplace [28].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 7,
      "context" : "[8] may also be inefficient if the only goal of testing is to determine whether a notch is present, as would be the case for large-scale screening.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "To model a healthy patient’s audiometric function, we use the model described in [8].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "We first took a thorough pure-tone audiometric test of each of nine patients from our trial with normal hearing using 100 samples selected using the algorithm in [8] on the domain X = [250, 8000] Hz× [−10, 80] dB HL,3 typical ranges for audiometric testing [6].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "We first took a thorough pure-tone audiometric test of each of nine patients from our trial with normal hearing using 100 samples selected using the algorithm in [8] on the domain X = [250, 8000] Hz× [−10, 80] dB HL,3 typical ranges for audiometric testing [6].",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 7,
      "context" : "[8] (white) when run on (a) the normal-hearing ground truth, and (b), the NIHL model ground truth.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "To test our active model-selection approach to diagnosing NIHL, we simulated a series of audiometric tests, selecting tones using three alternatives: BAMS, the algorithm of [8], and random sampling.",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 7,
      "context" : "Finally, it is worth emphasizing the stark difference between the sampling pattern of BAMS and the audiometric tests of [8]; see Figure 1.",
      "startOffset" : 120,
      "endOffset" : 123
    } ],
    "year" : 2015,
    "abstractText" : "We introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application. Although our method can work with arbitrary models, we focus on actively learning the appropriate structure for Gaussian process (GP) models with arbitrary observation likelihoods. We then apply this framework to rapid screening for noise-induced hearing loss (NIHL), a widespread and preventible disability, if diagnosed early. We construct a GP model for pure-tone audiometric responses of patients with NIHL. Using this and a previously published model for healthy responses, the proposed method is shown to be capable of diagnosing the presence or absence of NIHL with drastically fewer samples than existing approaches. Further, the method is extremely fast and enables the diagnosis to be performed in real time.",
    "creator" : null
  }
}