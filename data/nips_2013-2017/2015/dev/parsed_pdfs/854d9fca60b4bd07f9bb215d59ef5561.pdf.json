{
  "name" : "854d9fca60b4bd07f9bb215d59ef5561.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On the Optimality of Classifier Chain for Multi-label Classification",
    "authors" : [ "Weiwei Liu", "Ivor W. Tsang" ],
    "emails" : [ "liuweiwei863@gmail.com,", "ivor.tsang@uts.edu.au" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multi-label classification, where each instance can belong to multiple labels simultaneously, has significantly attracted the attention of researchers as a result of its various applications, ranging from document classification and gene function prediction, to automatic image annotation. For example, a document can be associated with a range of topics, such as Sports, Finance and Education [1]; a gene belongs to the functions of protein synthesis, metabolism and transcription [2]; an image may have both beach and tree tags [3].\nOne popular strategy for multi-label classification is to reduce the original problem into many binary classification problems. Many works have followed this strategy. For example, binary relevance (BR) [4] is a simple approach for multi-label learning which independently trains a binary classifier for each label. Recently, Dembczynski et al. [5] have shown that methods of multi-label learning which explicitly capture label dependency will usually achieve better prediction performance. Therefore, modeling the label dependency is one of the major challenges in multi-label classification problems. Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency. Amongst them, the classifier chain (CC) model is one of the most popular methods due to its simplicity and promising experimental results [6].\nCC works as follows: One classifier is trained for each label. For the (i + 1)th label, each instance is augmented with the 1st, 2nd, · · · , ith label as the input to train the (i + 1)th classifier. Given a new instance to be classified, CC firstly predicts the value of the first label, then takes this instance together with the predicted value as the input to predict the value of the next label. CC proceeds in this way until the last label is predicted. However, here is the question: Does the label order affect the performance of CC? Apparently yes, because different classifier chains involve different\n∗Corresponding author\nclassifiers trained on different training sets. Thus, to reduce the influence of the label order, Read et al. [6] proposed the ensembled classifier chain (ECC) to average the multi-label predictions of CC over a set of random chain ordering. Since the performance of CC is sensitive to the choice of label order, there is another important question: Is there any globally optimal classifier chain which can achieve the optimal prediction performance for CC? If yes, how can the globally optimal classifier chain be found?\nTo answer the last two questions, we first generalize the CC model over a random label order. We then present a theoretical analysis of the generalization error for the proposed generalized model. Our results show that the upper bound of the generalization error depends on the sum of reciprocal of square of the margin over the labels. Thus, we can answer the second question: the globally optimal CC exists only when the minimization of the upper bound is achieved over this CC. To find the globally optimal CC, we can search over q! different label orders1, where q denotes the number of labels, which is computationally infeasible for a large q. In this paper, we propose the dynamic programming based classifier chain (CC-DP) algorithm to simplify the search algorithm, which requires O(q3nd) time complexity. Furthermore, to speed up the training process, a greedy classifier chain (CC-Greedy) algorithm is proposed to find a locally optimal CC, where the time complexity of the CC-Greedy algorithm is O(q2nd). Notations: Assume xt ∈ Rd is a real vector representing an input or instance (feature) for t ∈ {1, · · · , n}. n denotes the number of training samples. Yt ⊆ {λ1, λ2, · · · , λq} is the corresponding output (label). yt ∈ {0, 1}q is used to represent the label set Yt, where yt(j) = 1 if and only if λj ∈ Yt."
    }, {
      "heading" : "2 Related work and preliminaries",
      "text" : "To capture label dependency, Hsu et al. [13] first use compressed sensing technique to handle the multi-label classification problem. They project the original label space into a low dimensional label space. A regression model is trained on each transformed label. Recovering multi-labels from the regression output usually involves solving a quadratic programming problem [13], and many works have been developed in this way [7, 14, 15]. Such methods mainly aim to use different projection methods to transform the original label space into another effective label space.\nAnother important approach attempts to exploit the different orders (first-order, second-order and high-order) of label correlations [16]. Following this way, some works also try to provide a probabilistic interpretation for label correlations. For example, Guo and Gu [8] model the label correlations using a conditional dependency network; PCC [5] exploits a high-order Markov Chain model to capture the correlations between the labels and provide an accurate probabilistic interpretation of CC. Other works [6, 9, 10] focus on modeling the label correlations in a deterministic way, and CC is one of the most popular methods among them. This work will mainly focus on the deterministic high-order classifier chain."
    }, {
      "heading" : "2.1 Classifier chain",
      "text" : "Similar to BR, the classifier chain (CC) model [6] trains q binary classifiers hj (j ∈ {1, · · · , q}). Classifiers are linked along a chain where each classifier hj deals with the binary classification problem for label λj . The augmented vector {xt, yt(1), · · · , yt(j)}nt=1 is used as the input for training classifier hj+1. Given a new testing instance x, classifier h1 in the chain is responsible for predicting the value of y(1) using input x. Then, h2 predicts the value of y(2) taking x plus the predicted value of y(1) as an input. Following in this way, hj+1 predicts y(j + 1) using the predicted value of y(1), · · · , y(j) as additional input information. CC passes label information between classifiers, allowing CC to exploit the label dependence and thus overcome the label independence problem of BR. Essentially, it builds a deterministic high-order Markov Chain model to capture the label correlations.\n1! represents the factorial notation."
    }, {
      "heading" : "2.2 Ensembled classifier chain",
      "text" : "Different classifier chains involve different classifiers learned on different training sets and thus the order of the chain itself clearly affects the prediction performance. To solve the issue of selecting a chain order for CC, Read et al. [6] proposed the extension of CC, called ensembled classifier chain (ECC), to average the multi-label predictions of CC over a set of random chain ordering. ECC first randomly reorders the labels {λ1, λ2, · · · , λq} many times. Then, CC is applied to the reordered labels for each time and the performance of CC is averaged over those times to obtain the final prediction performance."
    }, {
      "heading" : "3 Proposed model and generalization error analysis",
      "text" : ""
    }, {
      "heading" : "3.1 Generalized classifier chain",
      "text" : "We generalize the CC model over a random label order, called generalized classifier chain (GCC) model. Assume the labels {λ1, λ2, · · · , λq} are randomly reordered as {ζ1, ζ2, · · · , ζq}, where ζj = λk means label λk moves to position j from k. In the GCC model, classifiers are also linked along a chain where each classifier hj deals with the binary classification problem for label ζj (λk). GCC follows the same training and testing procedures as CC, while the only difference is the label order. In the GCC model, for input xt, yt(j) = 1 if and only if ζj ∈ Yt."
    }, {
      "heading" : "3.2 Generalization error analysis",
      "text" : "In this section, we analyze the generalization error bound of the multi-label classification problem using GCC based on the techniques developed for the generalization performance of classifiers with a large margin [17] and perceptron decision tree [18].\nLet X represent the input space. Both s and s̄ are m samples drawn independently according to an unknown distribution D. We denote logarithms to base 2 by log. If S is a set, |S| denotes its cardinality. ∥ · ∥ means the l2 norm. We train a support vector machine(SVM) for each label ζj . Let {xt}nt=1 as the feature and {yt(ζj)}nt=1 as the label, the output parameter of SVM is defined as [wj , bj ] = SVM({xt, yt(ζ1), · · · , yt(ζj−1)}nt=1, {yt(ζj)}nt=1). The margin for label ζj is defined as:\nγj = 1\n||wj ||2 (1)\nWe begin with the definition of the fat shattering dimension. Definition 1 ([19]). Let H be a set of real valued functions. We say that a set of points P is γshattered by H relative to r = (rp)p∈P if there are real numbers rp indexed by p ∈ P such that for all binary vectors b indexed by P , there is a function fb ∈ H satisfying\nfb(p) = { ≥ rp + γ if bp = 1 ≤ rp − γ otherwise\nThe fat shattering dimension fat(γ) of the set H is a function from the positive real numbers to the integers which maps a value γ to the size of the largest γ-shattered set, if this is finite, or infinity otherwise.\nAssume H is the real valued function class and h ∈ H. l(y, h(x)) denotes the loss function. The expected error of h is defined as erD[h] = E(x,y)∼D[l(y, h(x))], where (x, y) drawn from the unknown distribution D. Here we select 0-1 loss function. So, erD[h] = P(x,y)∼D(h(x) ̸= y).\ners[h] is defined as ers[h] = 1n n∑\nt=1 [yt ̸= h(xt)].2\nSuppose N (ϵ,H, s) is the ϵ-covering number of H with respect to the l∞ pseudo-metric measuring the maximum discrepancy on the sample s. The notion of the covering number can be referred to the Supplementary Materials. We introduce the following general corollary regarding the bound of the covering number:\n2The expression [yt ̸= h(xt)] evaluates to 1 if yt ̸= h(xt) is true and to 0 otherwise.\nCorollary 1 ([17]). Let H be a class of functions X → [a, b] and D a distribution over X . Choose 0 < ϵ < 1 and let d = fat(ϵ/4) ≤ em. Then\nE(N (ϵ,H, s)) ≤ 2 (4m(b− a)2\nϵ2\n)d log(2em(b−a)/(dϵ)) (2)\nwhere the expectation E is over samples s ∈ Xm drawn according to Dm.\nWe study the generalization error bound of the specified GCC with the specified number of labels and margins. Let G be the set of classifiers of GCC, G = {h1, h2, · · · , hq}. ers[G] denotes the fraction of the number of errors that GCC makes on s. Define x̂ ∈ X × {0, 1}, ĥj(x̂) = hj(x)(1− y(j))−hj(x)y(j). If an instance x ∈ X is correctly classified by hj , then ĥj(x̂) < 0. Moreover, we introduce the following proposition:\nProposition 1. If an instance x ∈ X is misclassified by a GCC model, then ∃hj ∈ G, ĥj(x̂) ≥ 0. Lemma 1. Given a specified GCC model with q labels and with margins γ1, γ2, · · · , γq for each label satisfying ki = fat(γi/8), where fat is continuous from the right. If GCC has correctly classified m multi-labeled examples s generated independently according to the unknown (but fixed) distribution D and s̄ is a set of another m multi-labeled examples, then we can bound the following probability to be less than δ: P 2m{ss̄ : ∃ a GCC model, it correctly classifies s, fraction of s̄ misclassified > ϵ(m, q, δ)} < δ, where ϵ(m, q, δ) = 1m (Q log(32m)+log 2q δ ) and Q = ∑q i=1 ki log( 8em ki ).\nProof. (of Lemma 1). Suppose G is a GCC model with q labels and with margins γ1, γ2, · · · , γq , the probability event in Lemma 1 can be described as\nA = {ss̄ : ∃G, ki = fat(γi/8), ers[G] = 0, ers̄[G] > ϵ}.\nLet ŝ and ˆ̄s denote two different set of m examples, which are drawn i.i.d. from the distribution D × {0, 1}. Applying the definition of x̂, ĥ and Proposition 1, the event can also be written as A = {ŝˆ̄s : ∃G, γ̂i = γi/2, ki = fat(γ̂i/4), ers[G] = 0, ri = maxtĥi(x̂t), 2γ̂i = −ri, |{ŷ ∈ ˆ̄s : ∃hi ∈ G, ĥi(ŷ) ≥ 2γ̂i + ri}| > mϵ}. Here, −maxtĥi(x̂t) means the minimal value of |hi(x)| which represents the margin for label ζi, so 2γ̂i = −ri. Let γki = min{γ′ : fat(γ′/4) ≤ ki}, so γki ≤ γ̂i, we define the following function:\nπ(ĥ) =  0 if ĥ ≥ 0 −2γki if ĥ ≤ −2γki ĥ otherwise\nso π(ĥ) ∈ [−2γki , 0]. Let π(Ĝ) = {π(ĥ) : h ∈ G}.\nLet Bkiŝ̄̂s represent the minimal γki-cover set of π(Ĝ) in the pseudo-metric dŝ̄̂s. We have that for any hi ∈ G, there exists f̃ ∈ Bkiŝ̄̂s , |π(ĥi(ẑ)) − π(f̃(ẑ))| < γki , for all ẑ ∈ ŝˆ̄s. For all x̂ ∈ ŝ, by the definition of ri, ĥi(x̂) ≤ ri = −2γ̂i, and γki ≤ γ̂i, ĥi(x̂) ≤ −2γki , π(ĥi(x̂)) = −2γki , so π(f̃(x̂)) < −2γki + γki = −γki . However, there are at least mϵ points ŷ ∈ ˆ̄s such that ĥi(ŷ) ≥ 0, so π(f̃(ŷ)) > −γki > maxtπ(f̃(x̂t)). Since π only reduces separation between output values, we conclude that the inequality f̃(ŷ) > maxtf̃(x̂t) holds. Moreover, the mϵ points in ˆ̄s with the largest f̃ values must remain for the inequality to hold. By the permutation argument, at most 2−mϵ of the sequences obtained by swapping corresponding points satisfy the conditions for fixed f̃ .\nAs for any hi ∈ G, there exists f̃ ∈ Bkiŝ̄̂s , so there are |B ki ŝ̄̂s | possibilities of f̃ that satisfy the inequality for ki. Note that |Bkiŝ̄̂s | is a positive integer which is usually bigger than 1 and by the union bound, we get the following inequality:\nP (A) ≤ (E(|Bk1ŝ̄̂s |) + · · ·+ E(|B kq ŝ̄̂s |))2 −mϵ ≤ (E(|Bk1ŝ̄̂s |)× · · · × E(|B kq ŝ̄̂s |))2 −mϵ\nSince every set of points γ-shattered by π(Ĝ) can be γ-shattered by Ĝ, so fatπ(Ĝ)(γ) ≤ fatĜ(γ), where Ĝ = {ĥ : h ∈ G}. Hence, by Corollary 1 (setting [a, b] to [−2γki , 0], ϵ to γki and m to 2m),\nE(|Bkiŝ̄̂s |) = E(N (γki , π(Ĝ), ŝˆ̄s)) ≤ 2(32m) d log( 8emd )\nwhere d = fatπ(Ĝ)(γki/4) ≤ fatĜ(γki/4) ≤ ki. Thus E(|B ki ŝ̄̂s |) ≤ 2(32m)\nki log( 8em ki ), and we obtain\nP (A) ≤ (E(|Bk1ŝ̄̂s |)× · · · × E(|B kq ŝ̄̂s |))2\n−mϵ ≤ q∏\ni=1\n2(32m) ki log( 8em ki ) = 2q(32m)Q\nwhere Q = ∑q\ni=1 ki log( 8em ki ). And so (E(|Bk1ŝ̄̂s |)× · · · × E(|B kq ŝ̄̂s |))2 −mϵ < δ provided\nϵ(m, q, δ) ≥ 1 m\n( Q log(32m) + log 2q\nδ ) as required.\nLemma 1 applies to a particular GCC model with a specified number of labels and a specified margin for each label. In practice, we will observe the margins after running the GCC model. Thus, we must bound the probabilities uniformly over all of the possible margins that can arise to obtain a practical bound. The generalization error bound of the multi-label classification problem using GCC is shown as follows: Theorem 1. Suppose a random m multi-labeled sample can be correctly classified using a GCC model, and suppose this GCC model contains q classifiers with margins γ1, γ2, · · · , γq for each label. Then we can bound the generalization error with probability greater than 1−δ to be less than\n130R2\nm\n( Q′ log(8em) log(32m) + log 2(2m)q\nδ ) where Q′ = ∑q i=1 1 (γi)2 and R is the radius of a ball containing the support of the distribution.\nBefore proving Theorem 1, we state one key Symmetrization lemma and Theorem 2. Lemma 2 (Symmetrization). Let H be the real valued function class. s and s̄ are m samples both drawn independently according to the unknown distribution D. If mϵ2 ≥ 2, then\nPs(sup h∈H |erD[h]− ers[h]| ≥ ϵ) ≤ 2Ps̄s(sup h∈H |ers̄[h]− ers[h]| ≥ ϵ/2) (3)\nThe proof details of this lemma can be found in the Supplementary Material. Theorem 2 ([20]). Let H be restricted to points in a ball of M dimensions of radius R about the origin, then\nfatH(γ) ≤ min {R2 γ2 ,M + 1 }\n(4)\nProof. (of Theorem 1). We must bound the probabilities over different margins. We first use Lemma 2 to bound the probability of error in terms of the probability of the discrepancy between the performance on two halves of a double sample. Then we combine this result with Lemma 1. We must consider all possible patterns of ki’s for label ζi. The largest value of ki is m. Thus, for fixed q, we can bound the number of possibilities by mq . Hence, there are mq of applications of Lemma 1.\nLet ci = {γ1, γ2, · · · , γq} denote the i-th combination of margins varied in {1, · · · ,m}q . G denotes a set of GCC models. The generalization error of G can be represented as erD[G] and ers[G] is 0, where G ∈ G. The uniform convergence bound of the generalization error is\nPs(sup G∈G\n|erD[G]− ers[G]| ≥ ϵ)\nApplying Lemma 2,\nPs(sup G∈G |erD[G]−ers[G]| ≥ ϵ) ≤ 2Ps̄s(sup G∈G |ers̄[G]− ers[G]| ≥ ϵ/2)\nLet Jci = {ss̄ : ∃ a GCC model G with q labels and with margins ci : ki = fat(γi/8), ers[G] = 0, ers̄[G] ≥ ϵ/2}. Clearly,\nPs̄s(sup G∈G\n|ers̄[G]− ers[G]| ≥ ϵ/2) ≤ Pm q ( mq∪\ni=1\nJci\n)\nAs ki still satisfies ki = fat(γi/8), Lemma 1 can still be applied to each case of Pm q\n(Jci). Let δk = δ/m q . Applying Lemma 1 (replacing δ by δk/2), we get:\nPm q\n(Jci) < δk/2\nwhere ϵ(m, k, δk/2) ≥ 2/m(Q log(32m) + log 2×2 q\nδk ) and Q = ∑q i=1 ki log( 4em ki ). By the union\nbound, it suffices to show that Pm q ( ∪mq i=1 Jci) ≤ ∑mq i=1 P mq (Jci) < δk/2 ×mq = δ/2. Applying Lemma 2, Ps(sup\nG∈G |erD[G]− ers[G]| ≥ ϵ) ≤ 2Ps̄s(sup G∈G |ers̄[G]− ers[G]| ≥ ϵ/2)\n≤ 2Pm q ( mq∪\ni=1\nJci\n) < δ\nThus, Ps(supG∈G |erD[G] − ers[G]| ≤ ϵ) ≥ 1 − δ. Let R be the radius of a ball containing the support of the distribution. Applying Theorem 2, we get ki = fat(γi/8) ≤ 65R2/(γi)2. Note that we have replaced the constant 82 = 64 by 65 in order to ensure the continuity from the right required for the application of Lemma 1. We have upperbounded log(8em/ki) by log(8em). Thus,\nerD[G] ≤ 2/m ( Q log(32m) + log 2(2m)q\nδ ) ≤ 130R 2\nm\n( Q′ log(8em) log(32m) + log 2(2m)q\nδ ) where Q′ = ∑q i=1 1 (γi)2 .\nGiven the training data size and the number of labels, Theorem 1 reveals one important factor in reducing the generalization error bound for the GCC model: the minimization of the sum of reciprocal of square of the margin over the labels. Thus, we obtain the following Corollary: Corollary 2 (Globally Optimal Classifier Chain). Suppose a random m multi-labeled sample with q labels can be correctly classified using a GCC model, this GCC model is the globally optimal classifier chain if and only if the minimization of Q′ in Theorem 1 is achieved over this classifier chain.\nGiven the number of labels q, there are q! different label orders. It is very expensive to find the globally optimal CC, which can minimize Q′, by searching over all of the label orders. Next, we discuss two simple algorithms."
    }, {
      "heading" : "4 Optimal classifier chain algorithm",
      "text" : "In this section, we propose two simple algorithms for finding the optimal CC based on our result in Section 3. To clearly state the algorithms, we redefine the margins with label order information. Given label set M = {λ1, λ2, · · · , λq}, suppose a GCC model contains q classifiers. Let oi(1 ≤ oi ≤ q) denote the order of λi in the GCC model, γoii represents the margin for label λi, with previous oi − 1 labels as the augmented input. If oi = 1, then γ1i represents the margin for label λi, without augmented input. Then Q′ is redefined as Q′ = ∑q i=1 1 (γ\noi i )\n2 ."
    }, {
      "heading" : "4.1 Dynamic programming algorithm",
      "text" : "To simplify the search algorithm mentioned before, we propose the CC-DP algorithm to find the globally optimal CC. Note that Q′ = ∑q i=1 1 (γ\noi i )\n2 = 1\n(γ oq q )2\n+ · · ·+ [\n1\n(γ ok+1 k+1 )\n2 + ∑k j=1 1\n(γ oj j ) 2\n] , we\nexplore the idea of DP to iteratively optimize Q′ over a subset of M with the length of 1, 2, · · · , q. Finally, we can obtain the optimal Q′ over M. Assume i ∈ {1, · · · , q}. Let V (i, η) be the optimal Q′ over a subset of M with the length of η(1 ≤ η ≤ q), where the label order is ending by label λi. Suppose Mηi represent the corresponding label set for V (i, η). When η = q, V (i, q) be the optimal Q′ over M, where the label order is ending by label λi. The DP equation is written as:\nV (i, η + 1) = min j ̸=i,λi ̸∈Mηj\n{ 1\n(γη+1i ) 2 + V (j, η)\n} (5)\nwhere γη+1i is the margin for label λi, with M η j as the augmented input. The initial condition of DP is: V (i, 1) = 1 (γ1i )\n2 and M1i = {λi}. Then, the optimal Q′ over M can be obtained by solving mini∈{1,··· ,q} V (i, q). Assume the training of linear SVM takes O(nd). The CC-DP algorithm is shown as the following bottom-up procedure: from the bottom, we first compute V (i, 1) = 1\n(γ1i ) 2 ,\nwhich takes O(nd). Then we compute V (i, 2) = minj ̸=i,λi ̸∈M1j { 1 (γ2i ) 2 + V (j, 1)}, which requires at most O(qnd), and set M2i = M1j ∪ {λi}. Similarly, it takes at most O(q2nd) time complexity to calculate V (i, q). Last, we iteratively solve this DP Equation, and use mini∈{1,··· ,q} V (i, q) to get the optimal solution, which requires at most O(q3nd) time complexity. Theorem 3 (Correctness of CC-DP). Q′ can be minimized by CC-DP, which means this Algorithm can find the globally optimal CC.\nThe proof can be referred to in the Supplementary Materials."
    }, {
      "heading" : "4.2 Greedy algorithm",
      "text" : "We propose a CC-Greedy algorithm to find a locally optimal CC to speed up the CC-DP algorithm. To save time, we construct only one classifier chain with the locally optimal label order. Based on the training instances, we select the label from {λ1, λ2, · · · , λq} as the first label, if the maximum margin can be achieved over this label, without augmented input. The first label is denoted by ζ1. Then we select the label from the remainder as the second label, if the maximum margin can be achieved over this label with ζ1 as the augmented input. We continue in this way until the last label is selected. Finally, this algorithm will converge to the locally optimal CC. We present the details of the CC-Greedy algorithm in the Supplementary Materials, where the time complexity of this algorithm is O(q2nd)."
    }, {
      "heading" : "5 Experiment",
      "text" : "In this section, we perform experimental studies on a number of benchmark data sets from different domains to evaluate the performance of our proposed algorithms for multi-label classification. All the methods are implemented in Matlab and all experiments are conducted on a workstation with a 3.2GHZ Intel CPU and 4GB main memory running 64-bit Windows platform."
    }, {
      "heading" : "5.1 Data sets and baselines",
      "text" : "We conduct experiments on eight real-world data sets with various domains from three websites.345 Following the experimental settings in [5] and [7], we preprocess the LLog, yahoo art, eurlex sm and eurlex ed data sets. Their statistics are presented in the Supplementary Materials. We compare our algorithms with some baseline methods: BR, CC, ECC, CCA [14] and MMOC [7]. To perform a fair comparison, we use the same linear classification/regression package LIBLINEAR [21] with L2-regularized square hinge loss (primal) to train the classifiers for all the methods. ECC is averaged over several CC predictions with random order and the ensemble size in ECC is set to 10 according to [5, 6]. In our experiment, the running time of PCC and EPCC [5] on most data sets, like slashdot and yahoo art, takes more than one week. From the results in [5], ECC is comparable with EPCC and outperforms PCC, so we do not consider PCC and EPCC here. CCA and MMOC are two state-of-the-art encoding-decoding [13] methods. We cannot get the results of CCA and MMOC on yahoo art 10, eurlex sm 10 and eurlex ed 10 data sets in one week. Following [22], we consider the Example-F1, Macro-F1 and Micro-F1 measures to evaluate the prediction performance of all methods. We perform 5-fold cross-validation on each data set and report the mean and standard error of each evaluation measurement. The running time complexity comparison is reported in the Supplementary Materials.\n3http://mulan.sourceforge.net 4http://meka.sourceforge.net/#datasets 5http://cse.seu.edu.cn/people/zhangml/Resources.htm#data"
    }, {
      "heading" : "5.2 Prediction performance",
      "text" : "Example-F1 results for our method and baseline approaches in respect of the different data sets are reported in Table 1. Other measure results are reported in the Supplementary Materials. From the results, we can see that: 1) BR is much inferior to other methods in terms of Example-F1. Our experiment provides empirical evidence that the label correlations exist in many real word data sets and because BR ignores the information about the correlations between the labels, BR achieves poor performance on most data sets. 2) CC improves the performance of BR, however, it underperforms ECC. This result verifies the answer to our first question stated in Section 1: the label order does affect the performance of CC; ECC, which averages over several CC predictions with random order, improves the performance of CC. 3) CC-DP and CC-Greedy outperforms CCA and MMOC. This studies verify that optimal CC achieve competitive results compared with stateof-the-art encoding-decoding approaches. 4) Our proposed CC-DP and CC-Greedy algorithms are successful on most data sets. This empirical result also verifies the answers to the last two questions stated in Section 1: the globally optimal CC exists and CC-DP can find the globally optimal CC which achieves the best prediction performance; the CC-Greedy algorithm achieves comparable prediction performance with CC-DP, while it requires lower time complexity than CC-DP. In the experiment, our proposed algorithms are much faster than CCA and MMOC in terms of both training and testing time, and achieve the same testing time with CC. Through the training time for our algorithms is slower than BR, CC and ECC. Our extensive empirical studies show that our algorithms achieve superior performance than those baselines."
    }, {
      "heading" : "6 Conclusion",
      "text" : "To improve the performance of multi-label classification, a plethora of models have been developed to capture label correlations. Amongst them, classifier chain is one of the most popular approaches due to its simplicity and good prediction performance. Instead of proposing a new learning model, we discuss three important questions in this work regarding the optimal classifier chain stated in Section 1. To answer these questions, we first propose a generalized CC model. We then provide a theoretical analysis of the generalization error for the proposed generalized model. Based on our results, we obtain the answer to the second question: the globally optimal CC exists only if the minimization of the upper bound is achieved over this CC. It is very expensive to search over q! different label orders to find the globally optimal CC. Thus, we propose the CC-DP algorithm to simplify the search algorithm, which requires O(q3nd) complexity. To speed up the CC-DP algorithm, we propose a CC-Greedy algorithm to find a locally optimal CC, where the time complexity of the CCGreedy algorithm is O(q2nd). Comprehensive experiments on eight real-world multi-label data sets from different domains verify our theoretical studies and the effectiveness of proposed algorithms."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by the Australian Research Council Future Fellowship FT130100746."
    } ],
    "references" : [ {
      "title" : "BoosTexter: A Boosting-based System for Text Categorization",
      "author" : [ "Robert E. Schapire", "Yoram Singer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2000
    }, {
      "title" : "Hierarchical multi-label prediction of gene",
      "author" : [ "Zafer Barutçuoglu", "Robert E. Schapire", "Olga G. Troyanskaya" ],
      "venue" : "function. Bioinformatics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "Learning Multi-Label Scene Classification",
      "author" : [ "Matthew R. Boutell", "Jiebo Luo", "Xipeng Shen", "Christopher M. Brown" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Mining Multi-label Data",
      "author" : [ "Grigorios Tsoumakas", "Ioannis Katakis", "Ioannis P. Vlahavas" ],
      "venue" : "In Data Mining and Knowledge Discovery Handbook,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains",
      "author" : [ "Krzysztof Dembczynski", "Weiwei Cheng", "Eyke Hüllermeier" ],
      "venue" : "Proceedings of the 27th International Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2010
    }, {
      "title" : "Classifier Chains for Multilabel Classification",
      "author" : [ "Jesse Read", "Bernhard Pfahringer", "Geoffrey Holmes", "Eibe Frank" ],
      "venue" : "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Maximum Margin Output Coding",
      "author" : [ "Yi Zhang", "Jeff G. Schneider" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Multi-Label Classification Using Conditional Dependency Networks",
      "author" : [ "Yuhong Guo", "Suicheng Gu" ],
      "venue" : "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Multi-Label Learning by Exploiting Label Correlations Locally",
      "author" : [ "Sheng-Jun Huang", "Zhi-Hua Zhou" ],
      "venue" : "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Correlated Label Propagation with Application to Multilabel Learning",
      "author" : [ "Feng Kang", "Rong Jin", "Rahul Sukthankar" ],
      "venue" : "IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Large Margin Metric Learning for Multi-Label Prediction",
      "author" : [ "Weiwei Liu", "Ivor W. Tsang" ],
      "venue" : "Proceedings of the Twenty-Ninth Conference on Artificial Intelligence,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Learning Graph Structure for Multi-Label Image Classification via Clique Generation",
      "author" : [ "Mingkui Tan", "Qinfeng Shi", "Anton van den Hengel", "Chunhua Shen", "Junbin Gao", "Fuyuan Hu", "Zhen Zhang" ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Multi-Label Prediction via Compressed Sensing",
      "author" : [ "Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Multi-Label Output Codes using Canonical Correlation Analysis",
      "author" : [ "Yi Zhang", "Jeff G. Schneider" ],
      "venue" : "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Multilabel Classification with Principal Label Space Transformation",
      "author" : [ "Farbound Tai", "Hsuan-Tien Lin" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Multi-label learning by exploiting label dependency",
      "author" : [ "Min-Ling Zhang", "Kun Zhang" ],
      "venue" : "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Structural Risk Minimization Over Data-Dependent Hierarchies",
      "author" : [ "John Shawe-Taylor", "Peter L. Bartlett", "Robert C. Williamson", "Martin Anthony" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1940
    }, {
      "title" : "Enlarging the Margins in Perceptron Decision Trees",
      "author" : [ "Kristin P. Bennett", "Nello Cristianini", "John Shawe-Taylor", "Donghui Wu" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "Efficient Distribution-free Learning of Probabilistic Concepts",
      "author" : [ "Michael J. Kearns", "Robert E. Schapire" ],
      "venue" : "Proceedings of the 31st Symposium on the Foundations of Computer Science,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1990
    }, {
      "title" : "Generalization Performance of Support Vector Machines and Other Pattern Classifiers",
      "author" : [ "Peter L. Bartlett", "John Shawe-Taylor" ],
      "venue" : "Advances in Kernel Methods - Support Vector Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1998
    }, {
      "title" : "LIBLIN- EAR: A Library for Large Linear Classification",
      "author" : [ "Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Objective-Guided Image Annotation",
      "author" : [ "Qi Mao", "Ivor Wai-Hung Tsang", "Shenghua Gao" ],
      "venue" : "IEEE Transactions on Image Processing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For example, a document can be associated with a range of topics, such as Sports, Finance and Education [1]; a gene belongs to the functions of protein synthesis, metabolism and transcription [2]; an image may have both beach and tree tags [3].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "For example, a document can be associated with a range of topics, such as Sports, Finance and Education [1]; a gene belongs to the functions of protein synthesis, metabolism and transcription [2]; an image may have both beach and tree tags [3].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "For example, a document can be associated with a range of topics, such as Sports, Finance and Education [1]; a gene belongs to the functions of protein synthesis, metabolism and transcription [2]; an image may have both beach and tree tags [3].",
      "startOffset" : 240,
      "endOffset" : 243
    }, {
      "referenceID" : 3,
      "context" : "For example, binary relevance (BR) [4] is a simple approach for multi-label learning which independently trains a binary classifier for each label.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "[5] have shown that methods of multi-label learning which explicitly capture label dependency will usually achieve better prediction performance.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 10,
      "context" : "Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "Many multi-label learning models [5, 6, 7, 8, 9, 10, 11, 12] have been developed to capture label dependency.",
      "startOffset" : 33,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "Amongst them, the classifier chain (CC) model is one of the most popular methods due to its simplicity and promising experimental results [6].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "[6] proposed the ensembled classifier chain (ECC) to average the multi-label predictions of CC over a set of random chain ordering.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "[13] first use compressed sensing technique to handle the multi-label classification problem.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "Recovering multi-labels from the regression output usually involves solving a quadratic programming problem [13], and many works have been developed in this way [7, 14, 15].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "Recovering multi-labels from the regression output usually involves solving a quadratic programming problem [13], and many works have been developed in this way [7, 14, 15].",
      "startOffset" : 161,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "Recovering multi-labels from the regression output usually involves solving a quadratic programming problem [13], and many works have been developed in this way [7, 14, 15].",
      "startOffset" : 161,
      "endOffset" : 172
    }, {
      "referenceID" : 14,
      "context" : "Recovering multi-labels from the regression output usually involves solving a quadratic programming problem [13], and many works have been developed in this way [7, 14, 15].",
      "startOffset" : 161,
      "endOffset" : 172
    }, {
      "referenceID" : 15,
      "context" : "Another important approach attempts to exploit the different orders (first-order, second-order and high-order) of label correlations [16].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "For example, Guo and Gu [8] model the label correlations using a conditional dependency network; PCC [5] exploits a high-order Markov Chain model to capture the correlations between the labels and provide an accurate probabilistic interpretation of CC.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "For example, Guo and Gu [8] model the label correlations using a conditional dependency network; PCC [5] exploits a high-order Markov Chain model to capture the correlations between the labels and provide an accurate probabilistic interpretation of CC.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : "Other works [6, 9, 10] focus on modeling the label correlations in a deterministic way, and CC is one of the most popular methods among them.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "Other works [6, 9, 10] focus on modeling the label correlations in a deterministic way, and CC is one of the most popular methods among them.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 9,
      "context" : "Other works [6, 9, 10] focus on modeling the label correlations in a deterministic way, and CC is one of the most popular methods among them.",
      "startOffset" : 12,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "Similar to BR, the classifier chain (CC) model [6] trains q binary classifiers hj (j ∈ {1, · · · , q}).",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "[6] proposed the extension of CC, called ensembled classifier chain (ECC), to average the multi-label predictions of CC over a set of random chain ordering.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "In this section, we analyze the generalization error bound of the multi-label classification problem using GCC based on the techniques developed for the generalization performance of classifiers with a large margin [17] and perceptron decision tree [18].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 17,
      "context" : "In this section, we analyze the generalization error bound of the multi-label classification problem using GCC based on the techniques developed for the generalization performance of classifiers with a large margin [17] and perceptron decision tree [18].",
      "startOffset" : 249,
      "endOffset" : 253
    }, {
      "referenceID" : 4,
      "context" : "345 Following the experimental settings in [5] and [7], we preprocess the LLog, yahoo art, eurlex sm and eurlex ed data sets.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "345 Following the experimental settings in [5] and [7], we preprocess the LLog, yahoo art, eurlex sm and eurlex ed data sets.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 13,
      "context" : "We compare our algorithms with some baseline methods: BR, CC, ECC, CCA [14] and MMOC [7].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "We compare our algorithms with some baseline methods: BR, CC, ECC, CCA [14] and MMOC [7].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "To perform a fair comparison, we use the same linear classification/regression package LIBLINEAR [21] with L2-regularized square hinge loss (primal) to train the classifiers for all the methods.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "ECC is averaged over several CC predictions with random order and the ensemble size in ECC is set to 10 according to [5, 6].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "ECC is averaged over several CC predictions with random order and the ensemble size in ECC is set to 10 according to [5, 6].",
      "startOffset" : 117,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "In our experiment, the running time of PCC and EPCC [5] on most data sets, like slashdot and yahoo art, takes more than one week.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "From the results in [5], ECC is comparable with EPCC and outperforms PCC, so we do not consider PCC and EPCC here.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 12,
      "context" : "CCA and MMOC are two state-of-the-art encoding-decoding [13] methods.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 21,
      "context" : "Following [22], we consider the Example-F1, Macro-F1 and Micro-F1 measures to evaluate the prediction performance of all methods.",
      "startOffset" : 10,
      "endOffset" : 14
    } ],
    "year" : 2015,
    "abstractText" : "To capture the interdependencies between labels in multi-label classification problems, classifier chain (CC) tries to take the multiple labels of each instance into account under a deterministic high-order Markov Chain model. Since its performance is sensitive to the choice of label order, the key issue is how to determine the optimal label order for CC. In this work, we first generalize the CC model over a random label order. Then, we present a theoretical analysis of the generalization error for the proposed generalized model. Based on our results, we propose a dynamic programming based classifier chain (CC-DP) algorithm to search the globally optimal label order for CC and a greedy classifier chain (CC-Greedy) algorithm to find a locally optimal CC. Comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed CC-DP algorithm outperforms state-of-the-art approaches and the CCGreedy algorithm achieves comparable prediction performance with CC-DP.",
    "creator" : null
  }
}