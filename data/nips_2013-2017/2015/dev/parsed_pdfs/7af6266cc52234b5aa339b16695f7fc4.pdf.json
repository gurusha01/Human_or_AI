{
  "name" : "7af6266cc52234b5aa339b16695f7fc4.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Market Framework for Eliciting Private Data",
    "authors" : [ "Bo Waggoner", "Rafael Frongillo", "Jacob Abernethy" ],
    "emails" : [ "bwaggoner@fas.harvard.edu", "raf@colorado.edu", "jabernet@umich.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "A firm that relies on the ability to make difficult predictions can gain a lot from a large collection of data. The goal is often to estimate values y ∈ Y given observations x ∈ X according to an appropriate class of hypotheses F describing the relationship between x and y (for example, y = a · x+ b for linear regression). In classic statistical learning theory, the goal is formalized as attempting to approximately solve\nmin f∈F E x,y Loss(f ; (x, y)) (1)\nwhere Loss(·) is an appropriate inutility function and (x, y) is drawn from an unknown distribution. In the present paper we are concerned with the case in which the data are not drawn or held by a central authority but are instead inherently distributed. By this we mean that the data is (disjointly) partitioned across a set of agents, with agent i privately possessing some portion of the dataset Si, and agents have no obvious incentive to reveal this data to the firm seeking it. The vast swaths of data available in our personal email accounts could provide massive benefits to a range of companies, for example, but users are typically loathe to provide account credentials, even when asked politely.\nWe will be concerned with the design of financial mechanisms that provide a community of agents, each holding a private set of data, an incentive to contribute to the solution of a large learning or prediction task. Here we use the term ‘mechanism’ to mean an algorithmic interface that can receive and answer queries, as well as engage in monetary exchange (deposits and payouts). Our aim will be to design such a mechanism that satisfies the following three properties:\n1. The mechanism is efficient in that it approaches a solution to (1) as the amount of data and participation grows while spending a constant, fixed total budget. 2. The mechanism is incentive-compatible in the sense that agents are rewarded when their contributions provide marginal value in terms of improved hypotheses, and are not rewarded for bad or misleading information. 3. The mechanism provides reasonable privacy guarantees, so that no agent j (or outside observer) can manipulate the mechanism in order to infer the contributions of agent i 6= j.\nUltimately we would like our mechanism to approach the performance of a learning algorithm that had direct access to all the data, while only spending a constant budget to acquire data and improve predictions and while protecting participants’ privacy.\nOur construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1]. A prediction market is a mechanism designed for the purpose of information aggregation, particularly when there is some underlying future event about which many members of the population may have private and useful information. For instance, it may elicit predictions about which team will win an upcoming sporting event, or which candidate will win an election. These predictions are eventually scored on the actual outcome of the event.\nApplying these prediction market techniques allows participants to essentially “trade in a market” based on their data. (This approach is similar to prior work on crowdsourcing contests [3].) Members of the population have private information, just as with prediction markets — in this case, data points or beliefs — and the goal is to incentivize them to reveal and aggregate that information into a final hypothesis or prediction. Their final profits are tied to the outcome of a test set of data, with each participant being paid in accordance with how much their information improved the performance on the test set. Our techniques depart from the framework of [3] in two significant aspects: (a) we focus on the particular problem of data aggregation, and most of our results take advantage of kernel methods; and (b) our mechanisms are the first to combine differential privacy guarantees with data aggregation in a prediction-market framework.\nThis framework will provide efficiency and truthfulness. We will also show how to achieve privacy in many scenarios. We will give mechanisms where the prices and predictions published satisfy ( , δ)-differential privacy [10] with respect to each participant’s data. The mechanism’s output can still give reasonable predictions while no observer can infer much about any participant’s input data."
    }, {
      "heading" : "2 Mechanisms for Eliciting and Aggregating Data",
      "text" : "We now give a broad description of the mechanism we will study. In brief, we imagine a central authority (the mechanism, or market) maintaining a hypothesis f t representing the current aggregation of all the contributions made thus far. A new (or returning) participant may query f t at no cost, perhaps evaluating the quality of the predictions on a privately-held dataset, and can then propose an update df t+1 to f t that possibly requires an investment (a “bet”). Bets are evaluated at the close of the market when a true data sample is generated (analogous to a test set), and payouts are distributed according to the quality of the updates.\nAfter describing this initial framework as Mechanism 1, which is based loosely on the setting of [3], we turn our attention to the special case in which our hypotheses must lie in a Reproducing Kernel Hilbert Space (RKHS) [17] for a given kernel k(·, ·). This kernel-based “nonparametric mechanism” is particularly well-suited for the problem of data aggregation, as the betting space of the participants consists essentially of updates of the form df t = αtk(zt, ·), where zt is the data object offered by the participant and αt ∈ R is the “magnitude” of the bet. A drawback of Mechanism 1 is the lack of privacy guarantees associated with the betting protocol: utilizing one’s data to make bets or investments in the mechanism can lead to a loss of privacy for the owner of that data. When a participant submits a bet of the form df t = αtk(zt, ·), where zt could contain sensitive personal information, another participant may be able to infer zt by querying the mechanism. One of the primary contributions of the present work, detailed in Section 3, is a technique to allow for productive participation in the mechanism while maintaining a guarantee on the privacy of the data submitted."
    }, {
      "heading" : "2.1 The General Template",
      "text" : "There is a space of examplesX×Y , where x ∈ X are features and y ∈ Y are labels. The mechanism designer chooses a function space F consisting of f : X × Y → R, and assumed to have Hilbert space structure; one may view F as either the hypothesis class or the associated loss class, that is where fh(x, y) measures the loss/performance of hypothesis h on observation x and label y. In each case we will refer to f ∈ F as a hypothesis, eliding the distinction between fh and h.\nThe pricing scheme of the mechanism relies on a convex cost function Cx(·) : F → R which is parameterized by elements x ∈ X but whose domain is the set of hypotheses F . The cost function is publicly available and determined in advance. The interaction with the mechanism is a sequential process of querying and betting. On round t − 1 the mechanism publishes a hypothesis f t−1, the “state” of the market, which participants may query. Each participant arrives sequentially, and on round t a participant may place a “bet” df t ∈ F , also called a “trade” or “update”, modifying the hypothesis f t−1 → f t = f t−1 + df t. Finally participation ends and the mechanism samples (or reveals) a test example1 (x, y) from the underlying distribution and pays (or charges) each participant according to the relative performance of their marginal contributions. Precisely, the total reward for participant t’s bet df t is the value df t(x, y) minus the cost Cx(f t)− Cx(f t−1).\nMechanism 1: The Market Template MARKET announces f0 ∈ F for t = 1, 2, . . . , T do\nPARTICIPANT may query functions∇fCx(f t−1) and f t−1(x, y) for examples (x, y) PARTICIPANT t may submit a bet df t ∈ F to MARKET MARKET updates state f t = f t−1 + df t\nMARKET observes a true sample (x, y) for t = 1, 2, . . . , T do\nPARTICIPANT t receives payment df t(x, y) + Cx(f t−1)− Cx(f t)\nThe design of cost-function prediction markets has been an area of active research over the past several years, starting with [8] and many further refinements and generalizations [1, 6, 15]. The general idea is that the mechanism can efficiently provide price quotes via a function C(·) which acts as a potential on the space of outstandings shares; see [1] for a thorough review. In the present work we have added an additional twist which is that the function Cx(·) is given an additional parameterization of the observation x. We will not dive too deeply into the theoretical aspects of this generalization, but this is a straightforward extension of existing theory.\nKey special case: exponential family mechanism. For those more familiar with statistics and machine learning, there is a natural and canonical family of problems that can be cast within the general framework of Mechanism 1, which we will call the exponential family prediction mechanism following [2]. Assume that F can be parameterized as F = {fθ : θ ∈ Rd}, that we are given a sufficient statistics summary function φ : X × Y → Rd, and that function evaluation is given by fθ(x, y) = 〈θ, φ(x, y)〉. We let Cx(f) := log ∫ Y exp(f(x, y))dy so that\nCx(fθ) = log ∫ Y exp(〈θ, φ(x, y)〉dy. In other words, we have chosen our mechanism to encode a particular exponential family model, with Cx(·) chosen as the conditional log partition function over the distribution on y given x. If the market has settled on a function fθ, then one may interpret that as the aggregate market belief on the distribution of X × Y is pθ(x, y) = exp(〈θ, φ(x, y)〉 −A(θ)) where A(θ) = log ∫ X×Y exp(〈θ, φ(x, y)〉) dx dy.\nHow may we view this as a “market aggregate” belief? Notice that if a trader observes the market state of fθ and she is considering a bet of the form df = fθ − fθ′ , the eventual profit will be\nfθ′(x, y)− fθ(x, y) + Cx(fθ)− Cx(fθ′) = log pθ′(y|x) pθ(y|x) .\nI.e., the profit is precisely the conditional log likelihood ratio of the update θ → θ′.\nExample: Logistic regression. Let X = Rk, Y = {−1, 1}, and take F to be the set of functions fθ(x, y) = y · (θ>x) for θ ∈ Rk. Then by our construction, Cx(f) = log(exp(f(x, 1)) + exp(f(x,−1))) = log(exp(θ>x) + exp(−θ>x)), and we let f0 = f0 ≡ 0. The payoff of a participant placing a bet which moves the market state to f1 = fθ, upon outcome (x, y), is:\nfθ(x, y) + Cx(f0)− Cx(fθ) = yθ>x+ log(2)− log(exp(θ>x) + exp(−θ>x)) = log(2)− log(1 + exp(−2yθ>x)) ,\n1This can easily be extended to a test set by taking the average performance over the test set.\nwhich is simply negative logistic loss of the parameter choice 2θ. A participant wishing to maximize profit under a belief distribution p(x, y) should therefore choose θ via logistic regression,\nθ∗ = arg min θ E (x,y)∼p\n[ log(1− exp(2yθ>x)) ] . (2)"
    }, {
      "heading" : "2.2 Properties of the Market",
      "text" : "We next describe two nice properties of Mechanism 1: incentive-compatibility and bounded budget. Recall that, for the exponential family markets discussed above, a trader moving the market hypothesis from f t−1 to f t was compensated according to the conditional log-likelihood ratio of f t−1 and f t on the test data point. The implication is that traders are incentivized to minimize a KL divergence between the market’s estimate of the distribution and the true underlying distribution. We refer to this property as incentive-compatibility because traders’ interests are aligned with the mechanism designer’s. This property indeed holds generally for Mechanism 1, where the KL divergence is replaced with a general Bregman divergence corresponding to the Fenchel conjugate of Cx(·); see Proposition 1 in the appendix for details. Given that the mechanism must make a sequence of (possibly negative) payments to traders, a natural question is whether there is the potential for large downside for the mechanism in terms of total payment (budget). In the context of the exponential family mechanism, this question is easy to answer: after a sequence of bets moving the market state parameter θ0 → θ1 → . . . → θfinal, the total loss to the mechanism corresponds to the total payouts made to traders,∑\ni\nfθi+1(x, y)− fθi(x, y) + Cx(fθi)− Cx(fθi+1) = log pθfinal(y|x) pθ0(y|x) ;\nthat is, the worst-case loss is exactly the worst-case conditional log-likelihood ratio. In the context of logistic regression this quantity can always be guaranteed to be no more than log 2 as long as the initial parameter is set to θ = 0. For Mechanism 1 more generally, one has tight bounds on the worst-case loss following from such results from prediction markets [1, 8], and we give a more detailed statement in Proposition 2 in the appendix.\nPrice sensitivity parameter λC . In choosing the cost function family C = {Cx : x ∈ X}, an important consideration is the “scale” of each Cx, or how quickly changes in the market hypothesis f t translate to changes in the “instantaneous prices” ∇Cx(f t) (which give the marginal cost for an infinitesimal bet df t+1). Formally, this is captured by the price sensitivity λC , defined as the upper bound on the operator norm (with respect to the L1 norm) of the Hessian of the cost function Cx (over all x). A choice of small λC translates to a small worst-case budget required by the mechanism. However, it means that the market prices are sensitive in that the same update df t changes the prices much more quickly. When we consider protecting the privacy of trader updates in Section 3, we will see that privacy imposes restrictions on the price sensitivity."
    }, {
      "heading" : "2.3 A Nonparametric Mechanism via Kernel Methods",
      "text" : "The framework we have discussed thus far has involved a general function space F as the “state” of the mechanism, and the contributions by participants are in the form of modifications to these functions. One of the downsides of this generic template is that participants may not be able to reason about F , and they may have information about the optimal f only through their own privately-held dataset S ⊂ X ×Y . A more specific class of functions would be those parameterized by actual data. This brings us to a well-studied type of non-parametric hypothesis class, namely the reproducing kernel Hilbert space (RKHS). We can design a market based on an RKHS, which we will refer to as a kernel market, that brings together a number of ideas including recent work of [21] as well as kernel exponential families [4].\nWe have a positive semidefinite kernel k : Z × Z → R and associated reproducing kernel Hilbert space F , with basis {fz(·) = k(z, ·) : z ∈ Z}. The reproducing property is that for all f ∈ F , 〈f, k(z, ·)〉 = f(z). Now each hypothesis f ∈ F can be expressed as f(·) = ∑ s αsk(zs, ·) for some collection of points {(αs, zs)}. The kernel approach has several nice properties. One is a natural extension of the exponential family mechanism using an RKHS as a building block of the class of exponential family distributions [4]. A\nkey assumption in the exponential family mechanism is that evaluating f can be viewed as an inner product in some feature space; this is precisely what one has given a kernel framework. Specifically, assume we have some PSD kernel k : X × X → R, where Y = {−1, 1}. Then we can define the associated classification kernel k̂ : (X × Y) × (X × Y) → R according to k̂((x, y), (x′, y′)) := yy′k(x, x′). Under certain conditions [4], we again can take Cx(f) = log ∫ Y exp(f(x, y))dy, and for any f in the RKHS associated to k̂, we have an associated distribution of the form pf (x, y) ∝ exp(f(x, y)). And again, a participant updating the market from f t−1 to f t is rewarded by the conditional log-likelihood ratio of f t−1 and f t on the test data.\nThe second nice property mirrors one of standard kernel learning methods, namely that under certain conditions one need only search the subset of the RKHS spanned by the basis {k((xi, yi), ·) : (xi, yk) ∈ S}, where S is the set of available data; this is a direct result of the Representer Theorem [17]. In the context of the kernel market, this suggests that participants need only interact with the mechanism by pushing updates that lie in the span of their own data. In other words, we only need to consider updates of the form df = αk((x, y), ·). This naturally suggests the idea of directly purchasing data points from traders.\nBuying Data Points. So far, we have supposed that a participant knows what trade df t she prefers to make. But what if she simply has a data point (x, y) drawn from the underlying distribution? We would like to give this trader a “simple” trading interface in which she can sell her data to the mechanism without having to reason about the correct df t for this data point.\nOur proposal is to mimic the behavior of natural learning algorithms, such as stochastic gradient descent, when presented with (x, y). The market can offer the trader the purchase bundle corresponding to the update of the learning algorithm on this data point. In principle, this approach can be used with any online learning algorithm. In particular, stochastic gradient descent gives a clean update rule, which we now describe. The expected profit (which is the negative of expected loss) for trade df t is Ex [ Cx(f t−1 + df t)− Cx(f t−1)− Ey|x[df t(x, y)] ] . Given a draw (x, y), the loss\nfunction on which to take a gradient step is − ( Cx(f t−1 + df t)− Cx(f t−1)− df t(x, y) ) , whose gradient is −∇ft−1Cx + δx,y (where δx,y is the indicator on data point x, y). This suggests that the market offer the participant the trade df t = ( ∇ft−1Cx − δx,y ) , where can be chosen arbitrarily as a “learning rate”. This can be interpreted as buying a unit of shares in the participant’s data point (x, y), then “hedging” by selling a small amount of all other shares in proportion to their current prices (recall that the current prices are given by∇ftCx). In the kernel setting, the choice of stochastic gradient descent may be somewhat problematic, because it can result in non-sparse share purchases. It may instead be desirable to use algorithms that guarantee sparse updates—a modern discussion of such approaches can be found in [22, 23].\nGiven this framework, participants with access to a private set of samples from the true underlying distribution can simply opt for this “standard bundle” corresponding to their data point, which is precisely a stochastic gradient descent update. With a small enough learning rate, and assuming that the data point is truly independent of the current hypothesis (i.e. (x, y) has not been previously incorporated), the trade is guaranteed to make at least some positive profit in expectation. More sophisticated alternative strategies are also possible of course, but even the proposed simple bet type has earning potential."
    }, {
      "heading" : "3 Protecting Participants’ Privacy",
      "text" : "We now extend the mechanism to protect privacy of the participants: An adversary observing the hypotheses and prices of the mechanism, and even controlling the trades of other participants, should not be able to infer too much about any one trader’s update df t. This is especially relevant when participants sell data to the mechanism and this data can be sensitive, e.g. medical data.\nHere, privacy is formalized by ( , δ)-differential privacy, to be defined shortly. One intuitive characterization is that, for any prior distribution some adversary has about a trader’s data, the adversary’s posterior belief after observing the mechanism would be approximately the same even if the trader did not participate at all. The idea is that, rather than posting the exact prices and trades made in the market, we will publish noisy versions, with the random noise giving the above guarantee.\nA naive approach would be to add independent noise to each participant’s trade. However, this would require a prohibitively-large amount of noise; the final market hypothesis would be determined by the random noise just as much as by the data and trades. The central challenge is to add carefully correlated noise that is large enough to hide the effects of any one participant’s data point, but not so large that the prices (equivalently, hypothesis) become meaningless. We show this is possible by adjusting the “price sensitivity” λC of the mechanism, a measure of how fast prices change in response to trades defined in 2.2. It will turn out to suffice to set the price sensitivity to be O(1/polylog T ) when there are T participants. This can roughly be interpreted as saying that any one participant does not move the market price noticeably (so their privacy is protected), but just O(polylog T ) traders together can move the prices completely.\nWe now formally define differential privacy and discuss two useful tools at our disposal."
    }, {
      "heading" : "3.1 Differential Privacy and Tools",
      "text" : "Differential privacy in our context is defined as follows. Consider a randomized function M operating on inputs of the form ~f = (df1, . . . , dfT ) and having outputs of the form s. Then M is ( , δ)-differentially private if, for any coordinate t of the vector, any two distinct df t1, df t 2, and any (measurable) set of outputs S, we have Pr[M(f−t, df t1) ∈ S)] ≤ e Pr[M(f−t, df t2) ∈ S] + δ. The notation f−t means the vector ~f with the tth entry removed.\nIntuitively, M is private if modifying the tth entry in the vector to a different entry does not change the distribution on outputs too much. In our case, the data to be protected will be the trade df t of each participant t, and the space of outputs will be the entire sequence of prices/predictions published by the mechanism.\nTo preserve privacy, each trade must have a bounded size (e.g. consist only of one data point). To enforce this, we define the following parameter chosen by the mechanism designer:\n∆ = max allowed df\n√ 〈df, df〉, (3)\nwhere the maximum is over all trades df allowed by the mechanism. That is, ∆ is a scalar capturing the maximum allowed size of any one trade. For instance, if all trades are restricted to be of the form df = αk(z, ·), then we would have ∆ = maxα,z α √ k(z, z).\nWe next describe the two tools we require.\nTool 1: Private functions via Gaussian processes. Given a current market state f t = f0 +df1 + · · · + df t, where f t lies in a RKHS, we construct a “private” version f̂ t such that queries to f̂ t are “accurate” — close to the outputs of f t — but also private with respect to each df j . In fact, it will become convenient to privately output partial sums of trades, so we wish to output a f̂t1:t2 that is private and approximates ft1:t2 = ∑t2 j=t1\ndf j . This is accomplished by the following construction due to [11]. Theorem 1 ([11], Corollary 9). LetG be the sample path of a Gaussian process with mean zero and whose covariance is given by the kernel function k.2 Then\nf̂t1:t2 = ft1:t2 + ∆\n√ 2 ln(2/δ)\nG . (4)\nis ( , δ)-differentially private with respect to each df j for j ∈ {t1, . . . , t2}.\nIn general, f̂t1:t2 may be an infinite-dimensional object and thus impossible to finitely represent. In this case, the theorem implies that releasing the results of any number of queries f̂t1:t2(z) is differentially private. (Of course, the more queries that are released, the larger the chance of high error on some query.) This is computationally feasible as each sampleG(z) is simply a sample from a Gaussian having known covariance with the previous samples drawn.\nUnfortunately, it would not be sufficient to independently release f̂1:t at each time t, because the amount of noise required would be prohibitive. This leads us to our next tool.\n2Formally, each G(z) is a random variable and, for any finite subset of Z , the corresponding variables are distributed as a multivariate normal with covariance given by k.\nTool 2: Continual observation technique. The idea of this technique, pioneered by [9, 5], is to construct f̂ t = ∑t j=0 df\nt by adding together noisy partial sums of the form f̂t1:t2 as constructed in Equation 4. The idea for choosing these partial sums is pictured in Figure 1: For a function s(t) that returns an integer smaller than t, we take f̂ t = f̂s(t)+1:t + f̂s(s(t))+1:s(t) + · · ·+ f̂0:0. Specifically, s(t) is determined by writing t in binary, then flipping the rightmost “one” bit to zero. This is pictured in Figure 1. The intuition behind why this technique helps is twofold. First, the total noise in f̂ t is the sum of noises of its partial sums, and it turns out that there are at most dlog T e terms. Second, the total noise we need to add to protect privacy is governed by how many different partial sums each df j participates in, and it turns out that this number is also at most dlog T e. This allows for much better privacy and accuracy guarantees than naively treating each step independently."
    }, {
      "heading" : "3.2 Mechanism and Results",
      "text" : "Combining our market template in Mechanism 1 with the above privacy tools, we obtain Mechanism 2. There are some key differences. First, we have a bound Q on the total number of queries. (Each query x returns the instantaneous prices in the market for x.) This is because each query reveals information about the participants, so intuitively, allowing too many queries must sacrifice either privacy or accuracy. Fortunately, this bound Q can be an arbitrarily large polynomial in the number of traders without affecting the quality of the results. Second, we have PAC-style guarantees on accuracy: with probability 1− γ, all price queries return values within α of their true prices. Third, it is no longer straightforward to compute and represent the market prices∇Cx(f̂ t) unless Y is finite. We leave the more general analysis of Mechanism 2 to future work.\nEither exactly or approximately, Mechanism 2 inherits the desirable properties of Mechanism 1, such as bounded budget and incentive-compatitibility (that is, participants are incentivized to minimize the risk of the market hypothesis). In addition, we show that it preserves privacy while maintaining accuracy, for an appropriate choice of the price sensitivity λC .\nTheorem 2. Consider Mechanism 2, where ∆ is the maximimum trade size (Equation 3) and d = |Y|. Then Mechanism 2 is ( , δ) differentially private and, with T traders and Q price queries, has the following accuracy guarantee: with probability 1− γ, for each query x the returned prices satisfy ‖∇Cx(f̂ t)−∇Cx(f t)‖∞ ≤ α by setting\nλC = α 2d∆2 √\nln Qdγ ln 2 log T δ log(T )\n3 .\nIf one for example takes δ, γ = exp [−polylog(Q,T )], then except for a superpolynomially low failure probability, Mechanism 2 answers all queries to within accuracy α by setting the price sensitivity to be λC = O (α /polylog(Q,T )). We note, however, that this is a somewhat weaker guarantee than is usually desired in the differential privacy literature, where ideally δ is exponentially small.\nMechanism 2: Privacy Protected Market Parameters: , δ (privacy), α, γ (accuracy), k (kernel), ∆ (trade size 3), Q (#queries), T (#traders) MARKET announces f̂0 = f0, sets r = 0, sets C with λC = λC( , δ, α, γ,∆, Q, T ) (Theorem 2) for t = 1, 2, . . . , T do\nPARTICIPANT t proposes a bet df t MARKET updates true position f t = f t−1 + df t\nMARKET instantiates f̂s(t)+1,t as defined in Equation 4 while r ≤ Q and some OBSERVER wishes to make a query do\nOBSERVER r submits pricing query on x MARKET returns prices∇Cx(f̂ t), where f̂ t = f̂s(t)+1:t + f̂s(s(t))+1:s(t) + · · ·+ f̂0:0 MARKET sets r ← r + 1\nMARKET observes a true sample (x, y) for t = 1, 2, . . . , T do\nPARTICIPANT receives payment f t−1(x, y)− f t(x, y)− Cx(f̂ t−1 + df t) + Cx(f̂ t−1)\nComputing∇Cx(f̂ t). We have already discussed limiting to finite |Y| in order to efficiently compute the marginal prices ∇Cx(f̂ t). However, it is still not immediately clear how to compute these prices, and hence how to implement Mechanism 2. Here, we show that the problem can be solved when C comes from an exponential family, so that Cx(f) = log ∫ Y exp [f(x, y)] dy. In this case, the marginal prices given by the gradient of C have a nice exponential-weights form, namely the price of shares in (x, y) is ptx(y) = ∇yCx(f t) = e f(x,y)∑ y∈Y e f(x,y) . Thus evaluating the prices can be done by evaluating f t(x, y) for each y ∈ Y . We also note that the worst-case bound used here could be greatly improved by taking into account the structure of the kernel. For “smooth” cases such as the Gaussian kernel, querying a second point very close to the first one requires very little additional randomness and builds up very little additional error. We gave only a worst-case bound that holds for all kernels.\nAdding a transaction fee. In the appendix, we discuss the potential need for transaction fees. Adding a small Θ(α) fee suffices to deter arbitrage opportunities introduced by noisy pricing.\nDiscussion\nThe main contribution of this work was to bring together several tools to construct a mechanism for incentivized data aggregation with “contest-like” incentive properties, privacy guarantees, and limited downside for the mechanism.\nOur proposed mechanisms are also extensions of the prediction market literature. Building upon the work of Abernethy et al. [1] we introduce the following innovations:\n• Conditional markets. Our framework of Mechanism 1 can be interpreted as a prediction market for conditional predictions p(y|x) rather than a classic market which would elicit the joint distribution p(x, y), or just the marginals. (This is similar to decision markets [12, 7], but without out the associated incentive problems.) Naturally then, we couple conditional predictions with restricted hypothesis spaces, allowing F to capture, e.g., a linear relationship between x and y.\n• Nonparametric securities. We also extend to nonparametric hypothesis spaces using kernels, following the kernel-based scoring rules of [21].\n• Privacy guarantees. We provide the first private prediction market (to our knowledge), showing that information about individual trades is not revealed. Our approach for preserving privacy also holds in the classic prediction market setting with similar privacy and accuracy guarantees.\nMany directions remain for future work. These mechanisms could be made more practical and perhaps even better privacy guarantees derived, especially in nonparametric settings. One could also explore the connections to similar settings, such as when agents have costs for acquiring data.\nAcknoledgements J. Abernethy acknowledges the generous support of the US National Science Foundation under CAREER Grant IIS-1453304 and Grant IIS-1421391."
    } ],
    "references" : [ {
      "title" : "Efficient market making via convex optimization, and a connection to online learning",
      "author" : [ "Jacob Abernethy", "Yiling Chen", "Jennifer Wortman Vaughan" ],
      "venue" : "ACM Transactions on Economics and Computation,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Information aggregation in exponential family markets",
      "author" : [ "Jacob Abernethy", "Sindhu Kutty", "Sébastien Lahaie", "Rahul Sami" ],
      "venue" : "In Proceedings of the fifteenth ACM conference on Economics and computation,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "A collaborative mechanism for crowdsourcing prediction problems",
      "author" : [ "Jacob D Abernethy", "Rafael M Frongillo" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Kernel methods and the exponential family",
      "author" : [ "Stéphane Canu", "Alex Smola" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Private and continual release of statistics",
      "author" : [ "T-H Hubert Chan", "Elaine Shi", "Dawn Song" ],
      "venue" : "ACM Transactions on Information and System Security (TISSEC),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "A new understanding of prediction markets via no-regret learning",
      "author" : [ "Y. Chen", "J.W. Vaughan" ],
      "venue" : "In Proceedings of the 11th ACM Conference on Electronic Commerce (EC),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Decision markets with good incentives",
      "author" : [ "Yiling Chen", "Ian Kash", "Mike Ruberry", "Victor Shnayder" ],
      "venue" : "In Internet and Network Economics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "A utility framework for bounded-loss market makers",
      "author" : [ "Yiling Chen", "David M. Pennock" ],
      "venue" : "Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Differential privacy under continual observation",
      "author" : [ "Cynthia Dwork", "Moni Naor", "Toniann Pitassi", "Guy N Rothblum" ],
      "venue" : "In Proceedings of the forty-second ACM symposium on Theory of computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "The algorithmic foundations of differential privacy",
      "author" : [ "Cynthia Dwork", "Aaron Roth" ],
      "venue" : "Foundations and Trends in Theoretical Computer Science,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Differential privacy for functions and functional data",
      "author" : [ "Rob Hall", "Alessandro Rinaldo", "Larry Wasserman" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Decision markets. Entrepreneurial Economics: Bright Ideas from the Dismal Science, pages",
      "author" : [ "R Hanson" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "Combinatorial information market design",
      "author" : [ "R. Hanson" ],
      "venue" : "Information Systems Frontiers,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2003
    }, {
      "title" : "Logarithmic market scoring rules for modular combinatorial information aggregation",
      "author" : [ "R. Hanson" ],
      "venue" : "Journal of Prediction Markets,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Automated market makers that enable new settings: extending constant-utility cost functions",
      "author" : [ "Abraham Othman", "Tuomas Sandholm" ],
      "venue" : "In Proceedings of the Second Conference on Auctions, Market Mechanisms and their Applications (AMMA),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Computational aspects of prediction markets",
      "author" : [ "David M. Pennock", "Rahul Sami" ],
      "venue" : "Algorithmic Game Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Learning with kernels: Support vector machines, regularization, optimization, and beyond",
      "author" : [ "Bernhard Schölkopf", "Alexander J Smola" ],
      "venue" : "MIT press,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2002
    }, {
      "title" : "Machine learning markets",
      "author" : [ "Amos J. Storkey" ],
      "venue" : "In Proceedings of AI and Statistics (AISTATS),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Prediction markets",
      "author" : [ "J. Wolfers", "E. Zitzewitz" ],
      "venue" : "Journal of Economic Perspectives,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "Interpreting prediction market prices as probabilities",
      "author" : [ "Justin Wolfers", "Eric Zitzewitz" ],
      "venue" : "Technical report, National Bureau of Economic Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    }, {
      "title" : "Nonparametric scoring rules",
      "author" : [ "Erik Zawadzki", "Sébastien Lahaie" ],
      "venue" : "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Efficient online learning for large-scale sparse kernel logistic regression",
      "author" : [ "Lijun Zhang", "Rong Jin", "Chun Chen", "Jiajun Bu", "Xiaofei He" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Online kernel learning with a near optimal sparsity bound",
      "author" : [ "Lijun Zhang", "Jinfeng Yi", "Rong Jin", "Ming Lin", "Xiaofei He" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 18,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 80,
      "endOffset" : 96
    }, {
      "referenceID" : 7,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 203,
      "endOffset" : 224
    }, {
      "referenceID" : 15,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 203,
      "endOffset" : 224
    }, {
      "referenceID" : 5,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 203,
      "endOffset" : 224
    }, {
      "referenceID" : 14,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 203,
      "endOffset" : 224
    }, {
      "referenceID" : 17,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 203,
      "endOffset" : 224
    }, {
      "referenceID" : 0,
      "context" : "Our construction relies on the recent surge in literature on prediction markets [13, 14, 19, 20], popular for some time in the field of economics and recently studied in great detail in computer science [8, 16, 6, 15, 18, 1].",
      "startOffset" : 203,
      "endOffset" : 224
    }, {
      "referenceID" : 2,
      "context" : "(This approach is similar to prior work on crowdsourcing contests [3].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "Our techniques depart from the framework of [3] in two significant aspects: (a) we focus on the particular problem of data aggregation, and most of our results take advantage of kernel methods; and (b) our mechanisms are the first to combine differential privacy guarantees with data aggregation in a prediction-market framework.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "We will give mechanisms where the prices and predictions published satisfy ( , δ)-differential privacy [10] with respect to each participant’s data.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "After describing this initial framework as Mechanism 1, which is based loosely on the setting of [3], we turn our attention to the special case in which our hypotheses must lie in a Reproducing Kernel Hilbert Space (RKHS) [17] for a given kernel k(·, ·).",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "After describing this initial framework as Mechanism 1, which is based loosely on the setting of [3], we turn our attention to the special case in which our hypotheses must lie in a Reproducing Kernel Hilbert Space (RKHS) [17] for a given kernel k(·, ·).",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 7,
      "context" : "The design of cost-function prediction markets has been an area of active research over the past several years, starting with [8] and many further refinements and generalizations [1, 6, 15].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "The design of cost-function prediction markets has been an area of active research over the past several years, starting with [8] and many further refinements and generalizations [1, 6, 15].",
      "startOffset" : 179,
      "endOffset" : 189
    }, {
      "referenceID" : 5,
      "context" : "The design of cost-function prediction markets has been an area of active research over the past several years, starting with [8] and many further refinements and generalizations [1, 6, 15].",
      "startOffset" : 179,
      "endOffset" : 189
    }, {
      "referenceID" : 14,
      "context" : "The design of cost-function prediction markets has been an area of active research over the past several years, starting with [8] and many further refinements and generalizations [1, 6, 15].",
      "startOffset" : 179,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "The general idea is that the mechanism can efficiently provide price quotes via a function C(·) which acts as a potential on the space of outstandings shares; see [1] for a thorough review.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "For those more familiar with statistics and machine learning, there is a natural and canonical family of problems that can be cast within the general framework of Mechanism 1, which we will call the exponential family prediction mechanism following [2].",
      "startOffset" : 249,
      "endOffset" : 252
    }, {
      "referenceID" : 0,
      "context" : "For Mechanism 1 more generally, one has tight bounds on the worst-case loss following from such results from prediction markets [1, 8], and we give a more detailed statement in Proposition 2 in the appendix.",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 7,
      "context" : "For Mechanism 1 more generally, one has tight bounds on the worst-case loss following from such results from prediction markets [1, 8], and we give a more detailed statement in Proposition 2 in the appendix.",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "We can design a market based on an RKHS, which we will refer to as a kernel market, that brings together a number of ideas including recent work of [21] as well as kernel exponential families [4].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "We can design a market based on an RKHS, which we will refer to as a kernel market, that brings together a number of ideas including recent work of [21] as well as kernel exponential families [4].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "One is a natural extension of the exponential family mechanism using an RKHS as a building block of the class of exponential family distributions [4].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 3,
      "context" : "Under certain conditions [4], we again can take Cx(f) = log ∫ Y exp(f(x, y))dy, and for any f in the RKHS associated to k̂, we have an associated distribution of the form pf (x, y) ∝ exp(f(x, y)).",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "The second nice property mirrors one of standard kernel learning methods, namely that under certain conditions one need only search the subset of the RKHS spanned by the basis {k((xi, yi), ·) : (xi, yk) ∈ S}, where S is the set of available data; this is a direct result of the Representer Theorem [17].",
      "startOffset" : 298,
      "endOffset" : 302
    }, {
      "referenceID" : 21,
      "context" : "It may instead be desirable to use algorithms that guarantee sparse updates—a modern discussion of such approaches can be found in [22, 23].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 22,
      "context" : "It may instead be desirable to use algorithms that guarantee sparse updates—a modern discussion of such approaches can be found in [22, 23].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "This is accomplished by the following construction due to [11].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "The idea of this technique, pioneered by [9, 5], is to construct f̂ t = ∑t j=0 df t by adding together noisy partial sums of the form f̂t1:t2 as constructed in Equation 4.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "The idea of this technique, pioneered by [9, 5], is to construct f̂ t = ∑t j=0 df t by adding together noisy partial sums of the form f̂t1:t2 as constructed in Equation 4.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "[1] we introduce the following innovations:",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "(This is similar to decision markets [12, 7], but without out the associated incentive problems.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "(This is similar to decision markets [12, 7], but without out the associated incentive problems.",
      "startOffset" : 37,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "We also extend to nonparametric hypothesis spaces using kernels, following the kernel-based scoring rules of [21].",
      "startOffset" : 109,
      "endOffset" : 113
    } ],
    "year" : 2015,
    "abstractText" : "We propose a mechanism for purchasing information from a sequence of participants. The participants may simply hold data points they wish to sell, or may have more sophisticated information; either way, they are incentivized to participate as long as they believe their data points are representative or their information will improve the mechanism’s future prediction on a test set. The mechanism, which draws on the principles of prediction markets, has a bounded budget and minimizes generalization error for Bregman divergence loss functions. We then show how to modify this mechanism to preserve the privacy of participants’ information: At any given time, the current prices and predictions of the mechanism reveal almost no information about any one participant, yet in total over all participants, information is accurately aggregated.",
    "creator" : null
  }
}