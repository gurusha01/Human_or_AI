{
  "name" : "9778d5d219c5080b9a6a17bef029331c.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Color Constancy by Learning to Predict Chromaticity from Luminance",
    "authors" : [ "Ayan Chakrabarti" ],
    "emails" : [ "ayanc@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The spectral distribution of light reflected off a surface is a function of an intrinsic material property of the surface—its reflectance—and also of the spectral distribution of the light illuminating the surface. Consequently, the observed color of the same surface under different illuminants in different images will be different. To be able to reliably use color computationally for identifying materials and objects, researchers are interested in deriving an encoding of color from an observed image that is invariant to changing illumination. This task is known as color constancy, and requires resolving the ambiguity between illuminant and surface colors in an observed image. Since both of these quantities are unknown, much of color constancy research is focused on identifying models and statistical properties of natural scenes that are informative for color constancy. While pschophysical experiments have demonstrated that the human visual system is remarkably successful at achieving color constancy [1], it remains a challenging task computationally.\nEarly color constancy algorithms were based on relatively simple models for pixel colors. For example, the gray world method [2] simply assumed that the average true intensities of different color channels across all pixels in an image would be equal, while the white-patch retinex method [3]\nassumed that the true color of the brightest pixels in an image is white. Most modern color constancy methods, however, are based on more complex reasoning with higher-order image features. Many methods [4, 5, 6] use models for image derivatives instead of individual pixels. Others are based on recognizing and matching image segments to those in a training set to recover true color [7]. A recent method proposes the use of a multi-layer convolutional neural network (CNN) to regress from image patches to illuminant color. There are also many “combination-based” color constancy algorithms, that combine illuminant estimates from a number of simpler “unitary” algorithms [8, 9, 10, 11], sometimes using image features to give higher weight to the outputs of some subset of methods.\nIn this paper, we demonstrate that by appropriately modeling and reasoning with the statistics of individual pixel colors, one can computationally recover illuminant color with high accuracy. We consider individual pixels in isolation, where the color constancy task reduces to discriminating between the possible choices of true color for the pixel that are feasible given the observed color and a candidate set of illuminants. Central to our method is a function that gives us the relative likelihoods of these true colors, and therefore a distribution over the corresponding candidate illuminants. Our global estimate for the scene illuminant is then computed by simply aggregating these distributions across all pixels in the image.\nWe formulate the likelihood function as one that measures the conditional likelihood of true pixel chromaticity given observed luminance, in part to be agnostic to the scalar (i.e., color channelindependent) ambiguity in observed color intensities. Moreover, rather than committing to a parametric form, we quantize the space of possible chromaticity and luminance values, and define the function over this discrete domain. We begin by setting the conditional likelihoods purely empirically, based simply on the histograms of true color values over all pixels in all images across a training set. Even with this purely empirical approach, our estimation algorithm yields estimates with higher accuracy than current state-of-the-art methods. Then, we investigate learning the perpixel belief function by optimizing an objective based on the accuracy of the final global illuminant estimate. We carry out this optimization using stochastic gradient descent, and using a sub-sampling approach (similar to “dropout” [12]) to improve generalization beyond the training set. This further improves estimation accuracy, without adding to the computational cost of inference."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Assuming Lambertian reflection, the spectral distribution of light reflected by a material is a product of the distribution of the incident light and the material’s reflectance function. The color intensity vector v(n) ∈ R3 recorded by a tri-chromatic sensor at each pixel n is then given by\nv(n) =\n∫\nκ(n, λ)ℓ(n, λ) s(n) Π(λ) dλ, (1)\nwhere κ(n, λ) is the reflectance at n, ℓ(n, λ) is the spectral distribution of the incident illumination, s(n) is a geometry-dependent shading factor, and Π(λ) ∈ R3 denotes the spectral sensitivities of the color sensors. Color constancy is typically framed as the task of computing from v(n) the corresponding color intensities x(n) ∈ R3 that would have been observed under some canonical illuminant ℓref (typically chosen to be ℓref(λ) = 1). We will refer to x(n) as the “true color” at n.\nSince (1) involves a projection of the full incident light spectrum on to the three filters Π(λ), it is not generally possible to recover x(n) from v(n) even with knowledge of the illuminant ℓ(n, λ). However, a commonly adopted approximation (shown to be reasonable under certain assumptions [13]) is to relate the true and observed colors x(n) and v(n) by a simple per-channel adaptation:\nv(n) = m(n) ◦ x(n), (2)\nwhere ◦ refers to the element-wise Hadamard product, and m(n) ∈ R3 depends on the illuminant ℓ(n, λ) (for ℓref, m = [1, 1, 1]\nT ). With some abuse of terminology, we will refer to m(n) as the illuminant in the remainder of the paper. Moreover, we will focus on the single-illuminant case in this paper, and assume m(n) = m, ∀n in an image. Our goal during inference will be to estimate this global illuminant m from the observed image v(n). The true color image x(n) can then simply be recovered as m−1 ◦ v(n), where m−1 ∈ R3 denotes the element-wise inverse of m.\nNote that color constancy algorithms seek to resolve the ambiguity between m and x(n) in (2) only up to a channel-independent scalar factor. This is because scalar ambiguities show up in m between\nℓ and ℓref due to light attenuation, between x(n) and κ(n) due to the shading factor s(n), and in the observed image v(n) itself due to varying exposure settings. Therefore, the performance metric typically used is the angular error cos−1 ( m T m̄\n‖m‖2‖m̄‖2\n)\nbetween the true and estimated illuminant\nvectors m and m̄.\nDatabase For training and evaluation, we use the database of 568 natural indoor and outdoor images captured under various illuminants by Gehler et al. [14]. We use the version from Shi and Funt [15] that contains linear images (without gamma correction) generated from the RAW camera data. The database contains images captured with two different cameras (86 images with a Canon 1D, and 482 with a Canon 5D). Each image contains a color checker chart placed in the image, with its position manually labeled. The colors of the gray squares in the chart are taken to be the value of the true illuminant m for each image, which can then be used to correct the image to get true colors at each pixel (of course, only up to scale). The chart is masked out during evaluation. We use k-fold cross-validation over this dataset in our experiments. Each fold contains images from both cameras corresponding to one of k roughly-equal partitions of each camera’s image set (ordered by file name/order of capture). Estimates for images in each fold are based on training only with data from the remaining folds. We report results with three- and ten-fold cross-validation. These correspond to average training set sizes of 379 and 511 images respectively."
    }, {
      "heading" : "3 Color Constancy with Pixel-wise Chromaticity Statistics",
      "text" : "A color vector x ∈ R3 can be characterized in terms of (1) its luminance ‖x‖1, or absolute brightness across color channels; and (2) its chromaticity, which is a measure of the relative ratios between intensities in different channels. While there are different ways of encoding chromaticity, we will do so in terms of the unit vector x̂ = x/‖x‖2 in the direction of x. Note that since intensities can not be negative, x̂ is restricted to lie on the non-negative eighth of the unit sphere S2+. Remember from Sec. 2 that our goal is to resolve the ambiguity between the true colors x(n) and the illuminant m only up to scale. In other words, we need only estimate the illuminant chromaticity m̂ and true chromaticities x̂(n) from the observed image v(n), which we can relate from (2) as\nx̂(n) = x(n)\n‖x(n)‖2 =\nm̂ −1 ◦ v(n)\n‖m̂−1 ◦ v(n)‖2\n∆ = g(v(n), m̂). (3)\nA key property of natural illuminant chromaticities is that they are known to take a fairly restricted set of values, close to a one-dimensional locus predicted by Planck’s radiation law [16]. To be able to exploit this, we denote M = {m̂i} M i=1 as the set of possible values for illuminant chromaticity m̂, and construct it from a training set. Specifically, we quantize1 the chromaticity vectors {m̂t} T t=1 of the illuminants in the training set, and let M be the set of unique chromaticity values. Additionally, we define a “prior” bi = log(ni/T ) over this candidate set, based on the number ni of training illuminants that were quantized to m̂i.\nGiven the observed color v(n) at a single pixel n, the ambiguity in m̂ across the illuminant set M translates to a corresponding ambiguity in the true chromaticity x̂(n) over the set {g(v(n), m̂i)}i. Figure 1(a) illustrates this ambiguity for a few different observed colors v. We note that while there is significant angular deviation within the set of possible true chromaticity values for any observed color, values in each set lie close to a one dimensional locus in chromaticity space. This suggests that the illuminants in our training set are indeed a good fit to Planck’s law2.\nThe goal of our work is to investigate the extent to which we can resolve the above ambiguity in true chromaticity on a per-pixel basis, without having to reason about the pixel’s spatial neighborhood or semantic context. Our approach is based on computing a likelihood distribution over the possible values of x̂(n), given the observed luminance ‖v(n)‖1. But as mentioned in Sec. 2, there is considerable ambiguity in the scale of observed color intensities. We address this partially by applying a simple per-image global normalization to the observed luminance to define\n1Quantization is over uniformly sized bins in S2+. See supplementary material for details. 2In fact, the chromaticities appear to lie on two curves, that are slightly separated from each other. This\nseparation is likely due to differences in the sensor responses of the two cameras in the Gehler-Shi dataset.\ny(n) = ‖v(n)‖1/median{‖v(n ′)‖1}n′ . This very roughly compensates for variations across images due to exposure settings, illuminant brightness, etc. However, note that since the normalization is global, it does not compensate for variations due to shading.\nThe central component of our inference method is a function L[x̂, y] that encodes the belief that a pixel with normalized observed luminance y has true chromaticity x̂. This function is defined over a discrete domain by quantizing both chromaticity and luminance values: we clip luminance values y to four (i.e., four times the median luminance of the image) and quantize them into twenty equal sized bins; and for chromaticity x̂, we use a much finger quantization with 214 equal-sized bins in S2+ (see supplementary material for details). In this section, we adopt a purely empirical approach\nand define L[x̂, y] as L[x̂, y] = log (Nx̂,y/ ∑ x̂ ′ Nx̂′,y) , where Nx̂,y is the number of pixels across all pixels in a set of images in a training set that have true chromaticity x̂ and observed luminance y.\nWe visualize these empirical versions of L[x̂, y] for a subset of the luminance quantization levels in Fig. 1(b). We find that in general, desaturated chromaticities with similar intensity values in all color channels are most common. This is consistent with findings of statistical analysis of natural spectra [17], which shows the “DC” component (flat across wavelength) to be the one with most variance. We also note that the concentration of the likelihood mass in these chromaticities increasing for higher values of luminance y. This phenomenon is also predicted by traditional intuitions in color science: materials are brightest when they reflect most of the incident light, which typically occurs when they have a flat reflectance function with all values of κ(λ) close to one. Indeed, this is what forms the basis of the white-patch retinex method [3]. Amongst saturated colors, we find that hues which combine green with either red or blue occur more frequently than primary colors, with pure green and combinations of red and blue being the least common. This is consistent with findings that reflectance functions are usually smooth (PCA on pixel spectra in [17] revealed a Fourier-like basis). Both saturated green and red-blue combinations would require the reflectance to have either a sharp peak or crest, respectively, in the middle of the visible spectrum.\nWe now describe a method that exploits the belief function L[x̂, y] for illuminant estimation. Given the observed color v(n) at a pixel n, we can obtain a distribution {L[g(v(n), m̂i), y(n)]}i over the set of possible true chromaticity values {g(v(n), m̂i)}i, which can also be interpreted as a distribution over the corresponding illuminants m̂i. We then simply aggregate these distributions across all pixels n in the image, and define the global probability of m̂i being the scene illuminant m as pi = exp(li)/ ( ∑ i′ exp(li′)), where\nli = α\nN\n∑\nn\nL[g(v(n), m̂i), y(n)] + βbi, (4)\nN is the total number of pixels in the image, and α and β are scalar parameters. The final illuminant chromaticity estimate m̄ is then computed as\nm̄ = argmin m\n′,‖m′‖2=1\nE [cos−1(mTm′)] ≈ argmax m\n′,‖m′‖2=1\nE[mTm′] =\n∑\ni pimi ‖ ∑\ni pimi‖2 . (5)\nNote that (4) also incorporates the prior bi over illuminants. We set the parameters α and β using a grid search, to values that minimize mean illuminant estimation error over the training set. The primary computational cost of inference is in computing the values of {li}. We pre-compute values of g(x̂, m̂) using (3) over the discrete domain of quantized chromaticity values for x̂ and the candidate illuminant set M for m̂. Therefore, computing each li essentially only requires the addition of N numbers from a look-up table. We need to do this for all M = |M| illuminants, where summations for different illuminants can be carried out in parallel. Our implementation takes roughly 0.3 seconds for a 9 mega-pixel image, on a modern Intel 3.3GHz CPU with 6 cores, and is available at http://www.ttic.edu/chakrabarti/chromcc/.\nThis empirical version of our approach bears some similarity to the Bayesian method of [14] that is based on priors for illuminants, and for the likelihood of different true reflectance values being present in a scene. However, the key difference is our modeling of true chromaticity conditioned on luminance that explicitly makes estimation agnostic to the absolute scale of intensity values. We also reason with all pixels, rather than the set of unique colors in the image.\nExperimental Results. Table 1 compares the performance of illuminant estimation with our method (see rows labeled “Empirical”) to the current state-of-the-art, using different quantiles of angular error across the Gehler-Shi database [14, 15]. Results for other methods are from the survey by Li et al. [18]. (See the supplementary material for comparisons to some other recent methods).\nWe show results with both three- and ten-fold cross-validation. We find that our errors with threefold cross-validation have lower mean, median, and tri-mean values than those of the best performing state-of-the-art method from [8], which combines illuminant estimates from twelve different “unitary” color-constancy method (many of which are also listed in Table 1) using support-vector regression. The improvement in error is larger with respect to the other combination methods [8, 9, 10, 11], as well as those based the statistics of image derivatives [4, 5, 6]. Moreover, since our method has more parameters than most previous algorithms (L[x̂, y] has 214 × 20 ≈ 300k entries), it is likely\nto benefit from more training data. We find this to indeed be the case, and observe a considerable decrease in error quantiles when we switch to ten-fold cross-validation.\nFigure. 2 shows estimation results with our method for a few sample images. For each image, we show the input image (indicating the ground truth color chart being masked out) and the output image with colors corrected by the global illuminant estimate. To visualize the quality of contributions from individual pixels, we also show a map of angular errors for illuminant estimates from individual pixels. These estimates are based on values of li computed by restricting the summation in (4) to individual pixels. We find that even these pixel-wise estimates are fairly accurate for a lot of pixels, even when it’s true color is saturated (see cart in first row). Also, to evaluate the weight of these per-pixel distributions to the global li, we show a map of their variance on a per-pixel basis. As expected from Fig. 1(b), we note higher variances in relatively brighter pixels. The image in the last row represents one of the poorest estimates across the entire dataset (higher than 90%−ile). Note that much of the image is in shadow, and contain only a few distinct (and likely atypical) materials."
    }, {
      "heading" : "4 Learning L[x̂, y] End-to-end",
      "text" : "While the empirical approach in the previous section would be optimal if pixel chromaticities in a typical image were infact i.i.d., that is clearly not the case. Therefore, in this section we propose an alternate approach method to setting the beliefs in L[x̂, y], that optimizes for the accuracy of the final global illuminant estimate. However, unlike previous color constancy methods that explicitly model statistical co-dependencies between pixels—for example, by modeling spatial derivatives [4, 5, 6], or learning functions on whole-image histograms [21]—we retain the overall parametric “form” by which we compute the illuminant in (4). Therefore, even though L[x̂, y] itself is learned through knowledge of co-occurence of chromaticities in natural images, estimation of the illuminant during inference is still achieved through a simple aggregation of per-pixel distributions.\nSpecifically, we set the entries of L[x̂, y] to minimize a cost function C over a set of training images:\nC(L) =\nT ∑\nt=1\nCt(L), Ct = ∑\ni\ncos−1(m̂Ti m̂ t) pti, (6)\nwhere m̂t is the true illuminant chromaticity of the tth training image, and pti is computed from the observed colors vt(n) using (4). We augment the training data available to us by “re-lighting” each image with different illuminants from the training set. We use the original image set and six re-lit copies for training, and use a seventh copy for validation.\nWe use stochastic gradient descent to minimize (6). We initialize L to empirical values as described in the previous section (for convenience, we multiply the empirical values by α, and then set α = 1 for computing li), and then consider individual images from the training set at each iteration. We make multiple passes through the training set, and at each iteration, we randomly sub-sample the pixels from each training image. Specifically, we only retain 1/128 of the total pixels in the image by randomly sub-sampling 16 × 16 patches at a time. This approach, which can be interpreted as being similar to “dropout” [12], prevents over-fitting and improves generalization.\nDerivatives of the cost function Ct with respect to the current values of beliefs L[x̂, y] are given by\n∂Ct\n∂L[x̂, y] =\n1\nN\n∑\ni\n(\n∑\nn\nδ ( g(vt(n), m̂i) = x̂ ) δ ( yt(n) = y )\n)\n× ∂Ct\n∂lti , (7)\nwhere ∂Ct\n∂lti = pti\n(\ncos−1(m̂Ti m̂ t)− Ct\n)\n. (8)\nWe use momentum to update the values of L[x̂, y] at each iteration based on these derivative as\nL[x̂, y] = L[x̂, y]− L∇[x̂, y], L∇[x̂, y] = r ∂Ct\n∂L[x̂, y] + µL∇∗ [x̂, y], (9)\nwhere L∇∗ [x̂, y] is the previous update value, r is the learning rate, and µ is the momentum factor. In our experiments, we set µ = 0.9, run stochastic gradient descent for 20 epochs with r = 100, and another 10 epochs with r = 10. We retain the values of L from each epoch, and our final output is the version that yields the lowest mean illuminant estimation error on the validation set.\nWe show the belief values learned in this manner in Fig. 1(c). Notice that although they retain the overall biases towards desaturated colors and combined green-red and green-blue hues, they are less “smooth” than their empirical counterparts in Fig. 1(b)—in many instances, there are sharp changes in the values L[x̂, y] for small changes in chromaticity. While harder to interpret, we hypothesize that these variations result from shifting beliefs of specific (x̂, y) pairs to their neighbors, when they correspond to incorrect choices within the ambiguous set of specific observed colors.\nExperimental Results. We also report errors when using these end-to-end trained versions of the belief function L in Table 1, and find that they lead to an appreciable reduction in error in comparison to their empirical counterparts. Indeed, the errors with end-to-end training using three-fold crossvalidation begin to approach those of the empirical version with ten-fold cross-validation, which has access to much more training data. Also note that the most significant improvements (for both three- and ten-fold cross-validation) are in “outlier” performance, i.e., in the 75 and 90%-ile error values. Color constancy methods perform worst on images that are dominated by a small number of materials with ambiguous chromaticity, and our results indicate that end-to-end training increases the reliability of our estimation method in these cases.\nWe also include results for the end-to-end case for the example images in Figure. 2. For all three images, there is an improvement in the global estimation error. More interestingly, we see that the per-pixel error and variance maps now have more high-frequency variation, since L now reacts more sharply to slight chromaticity changes from pixel to pixel. Moreover, we see that a larger fraction of pixels generate fairly accurate estimates by themselves (blue shirt in row 2). There is also a higher disparity in belief variance, including within regions that visually look homogeneous in the input, indicating that the global estimate is now more heavily influenced by a smaller fraction of pixels."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we introduced a new color constancy method that is based on a conditional likelihood function for the true chromaticity of a pixel, given its luminance. We proposed two approaches to learning this function. The first was based purely on empirical pixel statistics, while the second was based on maximizing accuracy of the final illuminant estimate. Both versions were found to outperform state-of-the-art color constancy methods, including those that employed more complex features and semantic reasoning. While we assumed a single global illuminant in this paper, the underlying per-pixel reasoning can likely be extended to the multiple-illuminant case, especially since, as we saw in Fig. 2, our method was often able to extract reasonable illuminant estimates from individual pixels. Another useful direction for future research is to investigate the benefits of using likelihood functions that are conditioned on lightness—estimated using an intrinsic image decomposition method—instead of normalized luminance. This would factor out the spatially-varying scalar ambiguity caused by shading, which could lead to more informative distributions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the authors of [18] for providing estimation results of other methods for comparison. The author was supported by a gift from Adobe."
    } ],
    "references" : [ {
      "title" : "Color constancy",
      "author" : [ "D.H. Brainard", "A Radonjic" ],
      "venue" : "In The new visual neurosciences,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "A spatial processor model for object colour perception",
      "author" : [ "G. Buchsbaum" ],
      "venue" : "J. Franklin Inst,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1980
    }, {
      "title" : "The retinex theory of color vision",
      "author" : [ "E.H. Land" ],
      "venue" : "Scientific American,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1971
    }, {
      "title" : "Generalized gamut mapping using image derivative structures for color constancy",
      "author" : [ "A. Gijsenij", "T. Gevers", "J. van de Weijer" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Edge-Based Color Constancy",
      "author" : [ "J. van de Weijer", "T. Gevers", "A. Gijsenij" ],
      "venue" : "IEEE Trans. Image Proc.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Color Constancy with Spatio-spectral Statistics",
      "author" : [ "A. Chakrabarti", "K. Hirakawa", "T. Zickler" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Exemplar-based color constancy and multiple illumination",
      "author" : [ "H.R.V. Joze", "M.S. Drew" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "A supervised combination strategy for illumination chromaticity estimation",
      "author" : [ "B. Li", "W. Xiong", "D. Xu" ],
      "venue" : "ACM Trans. Appl. Percept.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Color constancy using 3D scene geometry",
      "author" : [ "R. Lu", "A. Gijsenij", "T. Gevers", "V. Nedovic", "D. Xu" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Consensus-based framework for illuminant chromaticity estimation",
      "author" : [ "S. Bianco", "F. Gasparini", "R. Schettini" ],
      "venue" : "J. Electron. Imag.,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Automatic color constancy algorithm selection and combination",
      "author" : [ "S. Bianco", "G. Ciocca", "C. Cusano", "R. Schettini" ],
      "venue" : "Pattern recognition,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "The von Kries hypothesis and a basis for color constancy",
      "author" : [ "H. Chong", "S. Gortler", "T. Zickler" ],
      "venue" : "In Proc. ICCV,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Bayesian Color Constancy Revisited",
      "author" : [ "P.V. Gehler", "C. Rother", "A. Blake", "T. Minka", "T. Sharp" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Spectral distribution of typical daylight as a function of correlated color temperature",
      "author" : [ "D.B. Judd", "D.L. MacAdam", "G. Wyszecki", "H.W. Budde", "H.R. Condit", "S.T. Henderson", "J.L. Simonds" ],
      "venue" : "JOSA,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1964
    }, {
      "title" : "Statistics of Real-World Hyperspectral Images",
      "author" : [ "A. Chakrabarti", "T. Zickler" ],
      "venue" : "In Proc. CVPR,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Evaluating combinational illumination estimation methods on real-world images",
      "author" : [ "B. Li", "W. Xiong", "W. Hu", "B. Funt" ],
      "venue" : "IEEE Trans. Imag. Proc.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Color constancy using cnns",
      "author" : [ "S. Bianco", "C. Cusano", "R. Schettini" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "A novel algorithm for color constancy",
      "author" : [ "D. Forsyth" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1990
    }, {
      "title" : "Estimating illumination chromaticity via support vector regression",
      "author" : [ "W. Xiong", "B. Funt" ],
      "venue" : "J. Imag. Sci. Technol.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2006
    }, {
      "title" : "Computational color constancy: Survey and experiments",
      "author" : [ "A. Gijsenij", "T. Gevers", "J. van de Weijer" ],
      "venue" : "IEEE Trans. Image Proc.,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "While pschophysical experiments have demonstrated that the human visual system is remarkably successful at achieving color constancy [1], it remains a challenging task computationally.",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 1,
      "context" : "For example, the gray world method [2] simply assumed that the average true intensities of different color channels across all pixels in an image would be equal, while the white-patch retinex method [3]",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "For example, the gray world method [2] simply assumed that the average true intensities of different color channels across all pixels in an image would be equal, while the white-patch retinex method [3]",
      "startOffset" : 199,
      "endOffset" : 202
    }, {
      "referenceID" : 3,
      "context" : "Many methods [4, 5, 6] use models for image derivatives instead of individual pixels.",
      "startOffset" : 13,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "Many methods [4, 5, 6] use models for image derivatives instead of individual pixels.",
      "startOffset" : 13,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "Many methods [4, 5, 6] use models for image derivatives instead of individual pixels.",
      "startOffset" : 13,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "Others are based on recognizing and matching image segments to those in a training set to recover true color [7].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 7,
      "context" : "There are also many “combination-based” color constancy algorithms, that combine illuminant estimates from a number of simpler “unitary” algorithms [8, 9, 10, 11], sometimes using image features to give higher weight to the outputs of some subset of methods.",
      "startOffset" : 148,
      "endOffset" : 162
    }, {
      "referenceID" : 8,
      "context" : "There are also many “combination-based” color constancy algorithms, that combine illuminant estimates from a number of simpler “unitary” algorithms [8, 9, 10, 11], sometimes using image features to give higher weight to the outputs of some subset of methods.",
      "startOffset" : 148,
      "endOffset" : 162
    }, {
      "referenceID" : 9,
      "context" : "There are also many “combination-based” color constancy algorithms, that combine illuminant estimates from a number of simpler “unitary” algorithms [8, 9, 10, 11], sometimes using image features to give higher weight to the outputs of some subset of methods.",
      "startOffset" : 148,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "There are also many “combination-based” color constancy algorithms, that combine illuminant estimates from a number of simpler “unitary” algorithms [8, 9, 10, 11], sometimes using image features to give higher weight to the outputs of some subset of methods.",
      "startOffset" : 148,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "We carry out this optimization using stochastic gradient descent, and using a sub-sampling approach (similar to “dropout” [12]) to improve generalization beyond the training set.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "However, a commonly adopted approximation (shown to be reasonable under certain assumptions [13]) is to relate the true and observed colors x(n) and v(n) by a simple per-channel adaptation: v(n) = m(n) ◦ x(n), (2) where ◦ refers to the element-wise Hadamard product, and m(n) ∈ R(3) depends on the illuminant l(n, λ) (for lref, m = [1, 1, 1] T ).",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "However, a commonly adopted approximation (shown to be reasonable under certain assumptions [13]) is to relate the true and observed colors x(n) and v(n) by a simple per-channel adaptation: v(n) = m(n) ◦ x(n), (2) where ◦ refers to the element-wise Hadamard product, and m(n) ∈ R(3) depends on the illuminant l(n, λ) (for lref, m = [1, 1, 1] T ).",
      "startOffset" : 332,
      "endOffset" : 341
    }, {
      "referenceID" : 0,
      "context" : "However, a commonly adopted approximation (shown to be reasonable under certain assumptions [13]) is to relate the true and observed colors x(n) and v(n) by a simple per-channel adaptation: v(n) = m(n) ◦ x(n), (2) where ◦ refers to the element-wise Hadamard product, and m(n) ∈ R(3) depends on the illuminant l(n, λ) (for lref, m = [1, 1, 1] T ).",
      "startOffset" : 332,
      "endOffset" : 341
    }, {
      "referenceID" : 0,
      "context" : "However, a commonly adopted approximation (shown to be reasonable under certain assumptions [13]) is to relate the true and observed colors x(n) and v(n) by a simple per-channel adaptation: v(n) = m(n) ◦ x(n), (2) where ◦ refers to the element-wise Hadamard product, and m(n) ∈ R(3) depends on the illuminant l(n, λ) (for lref, m = [1, 1, 1] T ).",
      "startOffset" : 332,
      "endOffset" : 341
    }, {
      "referenceID" : 14,
      "context" : "A key property of natural illuminant chromaticities is that they are known to take a fairly restricted set of values, close to a one-dimensional locus predicted by Planck’s radiation law [16].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 15,
      "context" : "This is consistent with findings of statistical analysis of natural spectra [17], which shows the “DC” component (flat across wavelength) to be the one with most variance.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "Indeed, this is what forms the basis of the white-patch retinex method [3].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 15,
      "context" : "This is consistent with findings that reflectance functions are usually smooth (PCA on pixel spectra in [17] revealed a Fourier-like basis).",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "This empirical version of our approach bears some similarity to the Bayesian method of [14] that is based on priors for illuminants, and for the likelihood of different true reflectance values being present in a scene.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 13,
      "context" : "Table 1 compares the performance of illuminant estimation with our method (see rows labeled “Empirical”) to the current state-of-the-art, using different quantiles of angular error across the Gehler-Shi database [14, 15].",
      "startOffset" : 212,
      "endOffset" : 220
    }, {
      "referenceID" : 7,
      "context" : "We find that our errors with threefold cross-validation have lower mean, median, and tri-mean values than those of the best performing state-of-the-art method from [8], which combines illuminant estimates from twelve different “unitary” color-constancy method (many of which are also listed in Table 1) using support-vector regression.",
      "startOffset" : 164,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "The improvement in error is larger with respect to the other combination methods [8, 9, 10, 11], as well as those based the statistics of image derivatives [4, 5, 6].",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : "The improvement in error is larger with respect to the other combination methods [8, 9, 10, 11], as well as those based the statistics of image derivatives [4, 5, 6].",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "The improvement in error is larger with respect to the other combination methods [8, 9, 10, 11], as well as those based the statistics of image derivatives [4, 5, 6].",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "The improvement in error is larger with respect to the other combination methods [8, 9, 10, 11], as well as those based the statistics of image derivatives [4, 5, 6].",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "The improvement in error is larger with respect to the other combination methods [8, 9, 10, 11], as well as those based the statistics of image derivatives [4, 5, 6].",
      "startOffset" : 156,
      "endOffset" : 165
    }, {
      "referenceID" : 4,
      "context" : "The improvement in error is larger with respect to the other combination methods [8, 9, 10, 11], as well as those based the statistics of image derivatives [4, 5, 6].",
      "startOffset" : 156,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "The improvement in error is larger with respect to the other combination methods [8, 9, 10, 11], as well as those based the statistics of image derivatives [4, 5, 6].",
      "startOffset" : 156,
      "endOffset" : 165
    }, {
      "referenceID" : 13,
      "context" : "Table 1: Quantiles of Angular Error for Different Methods on the Gehler-Shi Database [14, 15]",
      "startOffset" : 85,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "Method Mean Median Tri-mean 25%-ile 75%-ile 90%-ile Bayesian [14] 6.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 3,
      "context" : "However, unlike previous color constancy methods that explicitly model statistical co-dependencies between pixels—for example, by modeling spatial derivatives [4, 5, 6], or learning functions on whole-image histograms [21]—we retain the overall parametric “form” by which we compute the illuminant in (4).",
      "startOffset" : 159,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "However, unlike previous color constancy methods that explicitly model statistical co-dependencies between pixels—for example, by modeling spatial derivatives [4, 5, 6], or learning functions on whole-image histograms [21]—we retain the overall parametric “form” by which we compute the illuminant in (4).",
      "startOffset" : 159,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "However, unlike previous color constancy methods that explicitly model statistical co-dependencies between pixels—for example, by modeling spatial derivatives [4, 5, 6], or learning functions on whole-image histograms [21]—we retain the overall parametric “form” by which we compute the illuminant in (4).",
      "startOffset" : 159,
      "endOffset" : 168
    }, {
      "referenceID" : 19,
      "context" : "However, unlike previous color constancy methods that explicitly model statistical co-dependencies between pixels—for example, by modeling spatial derivatives [4, 5, 6], or learning functions on whole-image histograms [21]—we retain the overall parametric “form” by which we compute the illuminant in (4).",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 11,
      "context" : "This approach, which can be interpreted as being similar to “dropout” [12], prevents over-fitting and improves generalization.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "We thank the authors of [18] for providing estimation results of other methods for comparison.",
      "startOffset" : 24,
      "endOffset" : 28
    } ],
    "year" : 2015,
    "abstractText" : "Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes—without any spatial or semantic context—can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a “classifier” for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel’s observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminanceto-chromaticity classifier “end-to-end”. Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.",
    "creator" : null
  }
}