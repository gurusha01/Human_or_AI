{
  "name" : "81ca0262c82e712e50c580c032d99b60.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Sample Efficient Path Integral Control under Uncertainty",
    "authors" : [ "Yunpeng Pan", "Evangelos A. Theodorou" ],
    "emails" : [ "ypan37@gatech.edu", "evangelos.theodorou@gatech.edu", "kontitsis@gatech.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Stochastic optimal control (SOC) is a general and powerful framework with applications in many areas of science and engineering. However, despite the broad applicability, solving SOC problems remains challenging for systems in high-dimensional continuous state action spaces. Various function approximation approaches to optimal control are available [1, 2] but usually sensitive to model uncertainty. Over the last decade, SOC based on exponential transformation of the value function has demonstrated remarkable applicability in solving real world control and planning problems. In control theory the exponential transformation of the value function was introduced in [3, 4]. In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14]. The resulting stochastic optimal control frameworks are known as Path Integral (PI) control for continuous time, Kullback Leibler (KL) control for discrete time, or more generally Linearly Solvable Optimal Control [9, 15].\nOne of the most attractive characteristics of PI control is that optimal control problems can be solved with forward sampling of Stochastic Differential Equations (SDEs). While the process of sampling with SDEs is more scalable than numerically solving partial differential equations, it still suffers from the curse of dimensionality when performed in a naive fashion. One way to circumvent this problem is to parameterize policies [10, 11, 14] and then perform optimization with sampling. However, in this case one has to impose the structure of the policy a-priori, therefore restrict the possible optimal control solutions within the assumed parameterization. In addition, the optimized policy parameters can not be generalized to new tasks. In general, model-free PI policy search approaches\nrequire a large number of samples from trials performed on real physical systems. The issue of sample inefficiency further restricts the applicability of PI control methods on physical systems with unknown or partially known dynamics.\nMotivated by the aforementioned limitations, in this paper we introduce a sample efficient, modelbased approach to PI control. Different from existing PI control approaches, our method combines the benefits of PI control theory [5, 6, 7] and probabilistic model-based reinforcement learning methodologies [16, 17]. The main characteristics of the our approach are summarized as follows\n• It extends the PI control theory [5, 6, 7] to the case of uncertain systems. The structural constraint is enforced between the control cost and uncertainty of the learned dynamics, which can be viewed as a generalization of previous work [5, 6, 7].\n• Different from parameterized PI controllers [10, 11, 14, 8], we find analytic control law without any policy parameterization.\n• Rather than keeping a fixed control cost weight [5, 6, 7, 10, 18], or ignoring the constraint between control authority and noise level [11], in this work the control cost weight is adapted based on the explicit uncertainty of the learned dynamics model.\n• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8]. More precisely our method perform successive deterministic approximate inference and backward computation of optimal control law.\n• The proposed model-based approach is significantly more sample efficient than samplingbased PI control [5, 6, 7, 18]. In RL setting our method is comparable to the state-of-the-art RL methods [17, 19] in terms of sample and computational efficiency.\n• Thanks to the linearity of the backward Chapman-Kolmogorov PDE, the learned controllers can be generalized to new tasks without re-sampling by constructing composite controllers. In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized."
    }, {
      "heading" : "2 Iterative Path Integral Control for a Class of Uncertain Systems",
      "text" : ""
    }, {
      "heading" : "2.1 Problem formulation",
      "text" : "We consider a nonlinear stochastic system described by the following differential equation dx = ( f(x) + G(x)u ) dt+ Bdω, (1)\nwith state x ∈ Rn, control u ∈ Rm, and standard Brownian motion noise ω ∈ Rp with variance Σω . f(x) is the unknown drift term (passive dynamics), G(x) ∈ Rn×m is the control matrix and B ∈ Rn×p is the diffusion matrix. Given some previous control uold, we seek the optimal control correction term δu such that the total control u = uold + δu. The original system becomes\ndx = ( f(x) + G(x)(uold + δu) ) dt+ Bdω = ( f(x) + G(x)uold )︸ ︷︷ ︸ f̃(x,uold) dt+ G(x)δudt+ Bdω.\nIn this work we assume the dynamics based on the previous control can be represented by Gaussian processes (GP) such that\nfGP(x) = f̃(x,u old)dt+ Bdω, (2)\nwhere fGP is the GP representation of the biased drift term f̃ under the previous control. Now the original dynamical system (1) can be represented as follow\ndx = fGP + Gδudt, fGP ∼ GP(µf ,Σf ), (3) where µf ,Σf are predictive mean and covariance functions, respectively. For the GP model we use a prior of zero mean and covariance function K(xi,xj) = σ2s exp(− 12 (xi − xj)\nTW(xi − xj)) + δijσ 2 ω, with σs, σω,W the hyper-parameters. δij is the Kronecker symbol that is one iff i = j and zero otherwise. Samples over fGP can be drawn using an vector of i.i.d. Gaussian variable Ω\nf̃GP = µf + LfΩ (4)\nwhere Lf is obtained using Cholesky factorization such that Σf = LfLTf . Note that generally Ω is an infinite dimensional vector and we can use the same sample to represent uncertainty during learning [23]. Without loss of generality we assume Ω to be the standard zero-mean Brownian motion. For the rest of the paper we use simplified notations with subscripts indicating the time step. The discrete-time representation of the system is xt+dt = xt+µft+Gtδutdt+LftΩt √ dt, and the con-\nditional probability of xt+dt given xt and δut is a Gaussian p ( xt+dt|xt, δut ) = N ( µt+dt,Σt+dt ) , where µt+dt = xt + µft + Gtδut and Σt+dt = Σft. In this paper we consider a finite-horizon stochastic optimal control problem\nJ(x0) = E [ q(xT ) + ∫ T t=0 L(xt, δut)dt ] ,\nwhere the immediate cost is defined as L(xt,ut) = q(xt) + 12δu T t Rtδut, and q(xt) = (xt − xdt ) TQ(xt − xdt ) is a quadratic cost function where xdt is the desired state. Rt = R(xt) is a statedependent positive definite weight matrix. Next we show the linearized Hamilton-Jacobi-Bellman equation for this class of optimal control problems."
    }, {
      "heading" : "2.2 Linearized Hamilton-Jacobi-Bellman equation for uncertain dynamics",
      "text" : "At each iteration the goal is to find the optimal control update δut that minimizes the value function\nV (xt, t) = min δut\nE [ ∫ t+dt\nt\nL(xt, δut)dt+ V (xt + dxt, t+ dt)dt|xt ] . (5)\n(5) is the Bellman equation. By approximating the integral for a small dt and applying Itô’s rule we obtain the Hamilton-Jacobi-Bellman (HJB) equation (detailed derivation is skipped):\n−∂tVt = min δut\n(qt + 1\n2 δuTt Rtδut + (µft + Gtδut)\nT∇xVt + 1\n2 Tr(Σft∇xxVt)).\nTo find the optimal control update, we take gradient of the above expression (inside the parentheses) with respect to δut and set to 0. This yields δut = −R−1t GTt ∇xVt. Inserting this expression into the HJB equation yields the following nonlinear and second order PDE\n−∂tVt = qt + (∇xVt)Tµft − 1\n2 (∇xVt)TGtR−1GTt ∇xVt +\n1 2 Tr(Σft∇xxVt). (6)\nIn order to solve the above PDE we use the exponential transformation of the value function Vt = −λ log Ψt, where Ψt = Ψ(xt) is called the desirability of xt. The corresponding partial derivatives can be found as ∂tVt = − λΨt ∂tΨt, ∇xVt = − λ Ψt ∇xΨt and ∇xxVt =\nλ Ψ2t ∇xΨt∇xΨTt − λΨt∇xxΨt. Inserting these terms to (6) results in\nλ\nΨt ∂tΨt = qt−\nλ\nΨt (∇xΨt)Tµft−\nλ2\n2Ψ2t (∇xΨt)TGtR−1t G T t ∇xΨt+\nλ\n2Ψ2t Tr((∇xΨt)TΣft∇xΨt)−\nλ\n2Ψt Tr(∇xxΨtΣft).\nThe quadratic terms ∇xΨt will cancel out under the assumption of λGtR−1t GTt = Σft. This constraint is different from existing works in path integral control [5, 6, 7, 10, 18, 8] where the constraint is enforced between the additive noise covariance and control authority, more precisely λGtR −1 t G T t = BΣωB\nT. The new constraint enables an adaptive update of control cost weight based on explicit uncertainty of the learned dynamics. In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8]. This condition also leads to more exploration (more aggressive control) under high uncertainty and less exploration with more certain dynamics. Given the aforementioned assumption, the above PDE is simplified as\n∂tΨt = 1\nλ qtΨt − µTft∇xΨt −\n1 2 Tr(∇xxΨtΣft), (7)\nsubject to the terminal condition ΨT = exp(− 1λqT ). The resulting Chapman-Kolmogorov PDE (7) is linear. In general, solving (7) analytically is intractable for nonlinear systems and cost functions. We apply the Feynman-Kac formula which gives a probabilistic representation of the solution of the linear PDE (7)\nΨt = lim dt→0\n∫ p(τt|xt) exp ( − 1 λ ( T−dt∑ j=t qjdt) ) ΨTdτt, (8)\nwhere τt is the state trajectory from time t to T . The optimal control is obtained as Gtδût = −GtR−1t GTt (∇xVt) = λGtR−1t GTt (∇xΨt\nΨt\n) = Σft (∇xΨt Ψt ) =⇒ût = uoldt + δût = uoldt + G−1t Σft (∇xΨt Ψt ) .\n(9)\nRather than computing ∇xΨt and Ψt, the optimal control ût can be approximated based on path costs of sampled trajectories. Next we briefly review some of the existing approaches."
    }, {
      "heading" : "2.3 Related works",
      "text" : "According to the path integral control theory [5, 6, 7, 10, 18, 8], the stochastic optimal control problem becomes an approximation problem of a path integral (8). This problem can be solved by forward sampling of the uncontrolled (u = 0) SDE (1). The optimal control ût is approximated based on path costs of sampled trajectories. Therefore the computation of optimal controls becomes a forward process. More precisely, when the control and noise act in the same subspace, the optimal control can be evaluated as the weighted average of the noise ût = Ep(τt|xt) [ dωt ] , where the probability of a trajectory is p(τt|xt) = exp(− 1λS(τt|xt))∫ exp(− 1λS(τt|xt))dτ\n, and S(τt|xt) is defined as the path cost computed by performing forward sampling. However, these approaches require a large amount of samples from a given dynamics model, or extensive trials on physical systems when applied in model-free reinforcement learning settings. In order to improve sample efficiency, a nonparametric approach was developed by representing the desirability Ψt in terms of linear operators in a reproducing kernel Hilbert space (RKHS) [12]. As a model-free approach, it allows sample re-use but relies on numerical methods to estimate the gradient of desirability, i.e., ∇xΨt , which can be computationally expensive. On the other hand, computing the analytic expressions of the path integral embedding is intractable and requires exact knowledge of the system dynamics. Furthermore, the control approximation is based on samples from the uncontrolled dynamics, which is usually not sufficient for highly nonlinear or underactuated systems.\nAnother class of PI-related method is based on policy parameterization. Notable approaches include PI2 [10], PI2-CMA [11], PI-REPS[14] and recently developed state-dependent PI[8]. The limitations of these methods are: 1) They do not take into account model uncertainty in the passive dynamics f(x). 2) The imposed policy parameterizations restrict optimal control solutions. 3) The optimized policy parameters can not be generalized to new tasks. A brief comparison of some of these methods can be found in Table 1. Motivated by the challenge of combining sample efficiency and generalizability, next we introduce a probabilistic model-based approach to compute the optimal control (9) analytically."
    }, {
      "heading" : "3 Proposed Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Analytic path integral control: a forward-backward scheme",
      "text" : "In order to derive the proposed framework, firstly we learn the function fGP(xt) = f̃(x,uold)dt + Bdω from sampled data. Learning the continuous mapping from state to state transition can be viewed as an inference with the goal of inferring the state transition dx̃t = fGP(xt). The kernel function has been defined in Sec.2.1, which can be interpreted as a similarity measure of random variables. More specifically, if the training input xi and xj are close to each other in the kernel space, their outputs dxi and dxj are highly correlated. Given a sequence of states {x0, . . .xT }, and the corresponding state transition {dx̃0, . . . ,dx̃T }, the posterior distribution can be obtained by conditioning the joint prior distribution on the observations. In this work we make the standard assumption of independent outputs (no correlation between each output dimension).\nTo propagate the GP-based dynamics over a trajectory of time horizon T we employ the moment matching approach [24, 17] to compute the predictive distribution. Given an input distribution over the state N (µt,Σt), the predictive distribution over the state at t + dt can be approximated as a Gaussian p(xt+dt) ≈ N (µt+dt,Σt+dt) such that\nµt+dt = µt + µft, Σt+dt = Σt + Σft + COV[xt,dx̃t] + COV[dx̃t,xt]. (10)\nThe above formulation is used to approximate one-step transition probabilities over the trajectory. Details regarding the moment matching method can be found in [24, 17]. All mean and variance terms can be computed analytically. The hyper-parameters σs, σω,W are learned by maximizing the log-likelihood of the training outputs given the inputs [25]. Given the approximation of transition probability (10), we now introduce a Bayesian nonparametric formulation of path integral control based on probabilistic representation of the dynamics. Firstly we perform approximate inference (forward propagation) to obtain the Gaussian belief (predictive mean and covariance of the state) over the trajectory. Since the exponential transformation of the state cost exp(− 1λq(x)dt) is an unnormalized Gaussian N (xd, 2λdtQ\n−1). We can evaluate the following integral analytically∫ N ( µj ,Σj ) exp ( − 1\nλ qjdt\n) dxj = ∣∣∣I + dt 2λ ΣjQ ∣∣∣− 12 exp(− 1 2 (µj − x d j ) T dt 2λ Q(I + dt 2λ λΣjQ) −1 (µj − x d j ) ) ,\n(11)\nfor j = t+dt, ..., T . Thus given a boundary condition ΨT = exp(− 1λqT ) and predictive distribution at the final stepN (µT ,ΣT ), we can evaluate the one-step backward desirability ΨT−dt analytically using the above expression (11). More generally we use the following recursive rule\nΨj−dt = Φ(xj ,Ψj) = ∫ N ( µj ,Σj ) exp ( − 1 λ qjdt ) Ψjdxj , (12)\nfor j = t+ dt, ..., T − dt. Since we use deterministic approximate inference based on (10) instead of explicitly sampling from the corresponding SDE, we approximate the conditional distribution p(xj |xj−dt) by the Gaussian predictive distribution N (µj ,Σj). Therefore the path integral\nΨt = ∫ p ( τt|xt ) exp ( − 1 λ ( T−dt∑ j=t qjdt) ) ΨT dτt\n≈ ∫ ... ∫ N ( µT−dt,ΣT−dt ) exp ( − 1 λ qT−dtdt )∫ N ( µT ,ΣT ) exp ( − 1 λ qT )\n︸ ︷︷ ︸ ΨT dxT\n︸ ︷︷ ︸ ΨT−dt\ndxT−dt\n︸ ︷︷ ︸ ΨT−2dt\n...dxt+dt\n= ∫ N ( µt+dt,Σt+dt ) exp ( − 1 λ qt+dtdt ) Ψt+dtdxt+dt = Φ(xt+dt,Ψt+dt). (13)\nWe evaluate the desirability Ψt backward in time by successive computation using the above recursive expression. The optimal control law ût (9) requires gradients of the desirability function with respect to the state, which can be computed backward in time as well. For simplicity we denote the function Φ(xj ,Ψj) by Φj . Thus we compute the gradient of the recursive expression (13)\n∇xΨj−dt = Ψj∇xΦj + Φj∇xΨj , (14)\nwhere j = t+ dt, ..., T − dt. Given the expression in (11) we compute the gradient terms in (14) as\n∇xΦj = dΦj\ndp(xj)\ndp(xj) dxt = ∂Φj ∂µj dµj dxt + ∂Φj ∂Σj dΣj dxt , where ∂Φj ∂µj = Φj(µj − x d j ) T dt 2λ Q(I + dt 2λ λΣjQ) −1,\n∂Φj ∂Σj = Φj 2 ( dt 2λ Q(I + dt 2λ λΣjQ) −1(µj − xdj )(µj − xdj )T − I) dt2λQ(I + dt2λλΣjQ)−1, and d{µj ,Σj}\ndxt = { ∂µj ∂µj−dt dµj−dt dxt + ∂µj ∂Σj−dt dΣj−dt dxt , ∂Σj ∂µj−dt dµj−dt dxt + ∂Σj ∂Σj−dt dΣj−dt dxt } .\nThe term ∇xΨT−dt is compute similarly. The partial derivatives ∂µj\n∂µj−dt , ∂µj ∂Σj−dt , ∂Σj ∂µj−dt , ∂Σj ∂Σj−dt\ncan be computed analytically as in [17]. We compute all gradients using this scheme without any numerical method (finite differences, etc.). Given Ψt and∇xΨt, the optimal control takes a analytic\nform as in eq.(9). Since Ψt and ∇xΨt are explicit functions of xt, the resulting control law is essentially different from the feedforward control in sampling-based path integral control frameworks [5, 6, 7, 10, 18] as well as the parameterized state feedback PI control policies [14, 8]. Notice that at current time step t, we update the control sequence ût,...,T using the presented forward-backward scheme. Only ût is applied to the system to move to the next step, while the controls ût+dt,...,T are used for control update at future steps. The transition sample recorded at each time step is incorporated to update the GP model of the dynamics. A summary of the proposed algorithm is shown in Algorithm 1.\nAlgorithm 1 Sample efficient path integral control under uncertain dynamics 1: Initialization: Apply random controls û0,..,T to the physical system (1), record data. 2: repeat 3: for t=0:T do 4: Incorporate transition sample to learn GP dynamics model. 5: repeat 6: Approximate inference for predictive distributions using uoldt,..,T = ût,..,T , see (10). 7: Backward computation of optimal control updates δût,..,T , see (13)(14)(9). 8: Update optimal controls ût,..,T = uoldt,..,T + δût,..,T . 9: until Convergence. 10: Apply optimal control ût to the system. Move one step forward and record data. 11: end for 12: until Task learned."
    }, {
      "heading" : "3.2 Generalization to unlearned tasks without sampling",
      "text" : "In this section we describe how to generalize the learned controllers for new (unlearned) tasks without any interaction with the real system. The proposed approach is based on the compositionality theory [26] in linearly solvable optimal control (LSOC). We use superscripts to denote previously learned task indexes. Firstly we define a distance measure between the new target x̄d and old targets xdk, k = 1, ..,K, i.e., a Gaussian kernel\nωk = exp ( − 1\n2 (x̄d − xdk)TP(x̄d − xdk)\n) , (15)\nwhere P is a diagonal matrix (kernel width). The composite terminal cost q̄(xT ) for the new task becomes q̄(xT ) = −λ log (∑K k=1 ω k exp(− 1λq\nk(xT ))∑K k=1 ω k\n) , (16)\nwhere qk(xT ) is the terminal cost for old tasks. For conciseness we define a normalized distance measure ω̃k = ω\nk∑K k=1 ω k , which can be interpreted as a probability weight. Based on (16) we have\nthe composite terminal desirability for the new task which is a linear combination of ΨkT\nΨ̄T = exp ( − 1 λ q̄(xT ) ) = K∑ k=1 ω̃kΨkT . (17)\nSince Ψkt is the solution to the linear Chapman-Kolmogorov PDE (7), the linear combination of desirability (17) holds everywhere from t to T as long as it holds on the boundary (terminal time step). Therefore we obtain the composite control\nūt = K∑ k=1 ω̃kΨkt∑K k=1 ω̃ kΨkt ûkt . (18)\nThe composite control law in (18) is essentially different from an interpolating control law[26]. It enables sample-free controllers that constructed from learned controllers for different tasks. This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22]. Alternatively, generalization can be achieved by imposing task-dependent policies [27]. However, this approach might restrict the choice of optimal controls given the assumed structure of control policy."
    }, {
      "heading" : "4 Experiments and Analysis",
      "text" : "We consider 3 simulated RL tasks: cart-pole (CP) swing up, double pendulum on a cart (DPC) swing up, and PUMA-560 robotic arm reaching. The CP and DPC systems consist of a cart and a single/double-link pendulum. The tasks are to swing-up the single/double-link pendulum from the initial position (point down). Both CP and DPC are under-actuated systems with only one control acting on the cart. PUMA-560 is a 3D robotic arm that has 12 state dimensions, 6 degrees of freedom with 6 actuators on the joints. The task is to steer the end-effector to the desired position and orientation.\nIn order to demonstrate the performance, we compare the proposed control framework with three related methods: iterative path integral control [18] with known dynamics model, PILCO [17] and PDDP [19]. Iterative path integral control is a sampling-based stochastic control method. It is based on importance sampling using controlled diffusion process rather than passive dynamics used in standard path integral control [5, 6, 7]. Iterative PI control is used as a baseline with a given dynamics model. PILCO is a model-based policy search method that features state-of-the-art data efficiency in terms of number of trials required to learn a task. PILCO requires an extra optimizer (such as BFGS) for policy improvement. PDDP is a Gaussian belief space trajectory optimization approach. It performs dynamic programming based on local approximation of the learned dynamics and value function. Both PILCO and PDDP are applied with unknown dynamics. In this work we do not compare our method with model-free PI-related approaches such as [10, 11, 12, 14] since these methods would certainly cost more samples than model-based methods such as PILCO and PDDP. The reason for choosing these two methods for comparison is that our method adopts a similar model learning scheme while other state-of-the-art methods, such as [20] is based on a different model.\nIn experiment 1 we demonstrate the sample efficiency of our method using the CP and DPC tasks. For both tasks we choose T = 1.2 and dt = 0.02 (60 time steps per rollout). The iterative PI [18] with a given dynamics model uses 103/104 (CP/DPC) sample rollouts per iteration and 500 iterations at each time step. We initialize PILCO and the proposed method by collecting 2/6 sample rollouts (corresponding to 120/360 transition samples) for CP/DPC tasks respectively. At each trial (on the true dynamics model), we use 1 sample rollout for PILCO and our method. PDDP uses 4/5 rollouts (corresponding to 240/300 transition samples) for initialization as well as at each trial for the CP/DPC tasks. Fig. 1 shows the results in terms of ΨT and computational time. For both tasks our method shows higher desirability (lower terminal state cost) at each trial, which indicates higher sample efficiency for task learning. This is mainly because our method performs online reoptimization at each time step. In contrast, the other two methods do not use this scheme. However we assume partial information of the dynamics (G matrix) is given. PILCO and PDDP perform optimization on entirely unknown dynamics. In many robotic systems G corresponds to the inverse of the inertia matrix, which can be identified based on data as well. In terms of computational efficiency, our method outperforms PILCO since we compute the optimal control update analytically, while PILCO solves large scale nonlinear optimization problems to obtain policy parameters. Our method is more computational expensive than PDDP because PDDP seeks local optimal controls that rely on linear approximations, while our method is a global optimal control approach. Despite the relatively higher computational burden than PDDP, our method offers reasonable efficiency in terms of the time required to reach the baseline performance.\nIn experiment 2 we demonstrate the generalizability of the learned controllers to new tasks using the composite control law (18) based on the PUMA-560 system. We use T = 2 and dt = 0.02 (100 time steps per rollout). First we learn 8 independent controllers using Algorithm 1. The target postures are shown in Fig. 2. For all tasks we initialize with 3 sample rollouts and 1 sample at each trial. Blue bars in Fig. 2b shows the desirabilities ΨT after 3 trials. Next we use the composite law (18) to construct controllers without re-sampling using 7 other controllers learned using Algorithm 1. For instance the composite controller for task#1 is found as ū1t = ∑8 k=2 ω̃kΨkt∑8 k=2 ω̃ kΨkt ûkt . The performance comparison of the composite controllers with controllers learned from trials is shown in Fig. 2. It can be seen that the composite controllers give close performance as independently learned controllers. The compositionality theory [26] generally does not apply to policy search methods and trajectory optimizers such as PILCO, PDDP, and other recent methods [20, 21, 22]. Our method benefits from the compositionality of control laws that can be applied for multi-task control without re-sampling."
    }, {
      "heading" : "5 Conclusion and Discussion",
      "text" : "We presented an iterative learning control framework that can find optimal controllers under uncertain dynamics using very small number of samples. This approach is closely related to the family of path integral (PI) control algorithms. Our method is based on a forward-backward optimization scheme, which differs significantly from current PI-related approaches. Moreover, it combines the attractive characteristics of probabilistic model-based reinforcement learning and linearly solvable optimal control theory. These characteristics include sample efficiency, optimality and generalizability. By iteratively updating the control laws based on probabilistic representation of the learned dynamics, our method demonstrated encouraging performance compared to the state-ofthe-art model-based methods. In addition, our method showed promising potential in performing multi-task control based on the compositionality of learned controllers. Besides the assumed structural constraint between control cost weight and uncertainty of the passive dynamics, the major limitation is that we have not taken into account the uncertainty in the control matrix G. Future work will focus on further generalization of this framework and applications to real systems."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research is supported by NSF NRI-1426945."
    } ],
    "references" : [ {
      "title" : "Neuro-dynamic programming (optimization and neural computation series",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Handbook of learning and approximate dynamic programming",
      "author" : [ "A.G. Barto", "W. Powell", "J. Si", "D.C. Wunsch" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Exit probabilities and optimal stochastic control",
      "author" : [ "W.H. Fleming" ],
      "venue" : "Applied Math. Optim,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1971
    }, {
      "title" : "Controlled Markov processes and viscosity solutions",
      "author" : [ "W.H. Fleming", "H.M. Soner" ],
      "venue" : "Applications of mathematics. Springer,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1993
    }, {
      "title" : "Linear theory for control of nonlinear stochastic systems",
      "author" : [ "H.J. Kappen" ],
      "venue" : "Phys Rev Lett,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Path integrals and symmetry breaking for optimal control theory",
      "author" : [ "H.J. Kappen" ],
      "venue" : "Journal of Statistical Mechanics: Theory and Experiment,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "An introduction to stochastic control theory, path integrals and reinforcement learning",
      "author" : [ "H.J. Kappen" ],
      "venue" : "AIP Conference Proceedings,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Path integral control and state-dependent feedback",
      "author" : [ "S. Thijssen", "H.J. Kappen" ],
      "venue" : "Phys. Rev. E,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Efficient computation of optimal actions",
      "author" : [ "E. Todorov" ],
      "venue" : "Proceedings of the national academy of sciences,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "A generalized path integral control approach to reinforcement learning",
      "author" : [ "E. Theodorou", "J. Buchli", "S. Schaal" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Path integral policy improvement with covariance matrix adaptation",
      "author" : [ "F. Stulp", "O. Sigaud" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Path integral control by reproducing kernel hilbert space embedding",
      "author" : [ "K. Rawlik", "M. Toussaint", "S. Vijayakumar" ],
      "venue" : "In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Nonparametric infinite horizon kullback-leibler stochastic control",
      "author" : [ "Y. Pan", "E. Theodorou" ],
      "venue" : "IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Policy search for path integral control",
      "author" : [ "V. Gómez", "H.J. Kappen", "J. Peters", "G. Neumann" ],
      "venue" : "In Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Linearly solvable optimal control. Reinforcement learning and approximate dynamic programming for feedback control, pages",
      "author" : [ "K. Dvijotham", "E Todorov" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "A survey on policy search for robotics",
      "author" : [ "M.P. Deisenroth", "G. Neumann", "J. Peters" ],
      "venue" : "Foundations and Trends in Robotics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Gaussian processes for data-efficient learning in robotics and control",
      "author" : [ "M. Deisenroth", "D. Fox", "C. Rasmussen" ],
      "venue" : "IEEE Transsactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Relative entropy and free energy dualities: Connections to path integral and kl control",
      "author" : [ "E. Theodorou", "E. Todorov" ],
      "venue" : "In 51st IEEE Conference on Decision and Control,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Probabilistic differential dynamic programming",
      "author" : [ "Y. Pan", "E. Theodorou" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Learning neural network policies with guided policy search under unknown dynamics",
      "author" : [ "S. Levine", "P. Abbeel" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Learning complex neural network policies with trajectory optimization",
      "author" : [ "S. Levine", "V. Koltun" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel" ],
      "venue" : "arXiv preprint arXiv:1502.05477,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Optimal reinforcement learning for gaussian systems",
      "author" : [ "P. Hennig" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Propagation of uncertainty in bayesian kernel models-application to multiple-step ahead forecasting",
      "author" : [ "J. Quinonero Candela", "A. Girard", "J. Larsen", "C.E. Rasmussen" ],
      "venue" : "In IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2003
    }, {
      "title" : "Gaussian processes for machine learning",
      "author" : [ "C.K.I Williams", "C.E. Rasmussen" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Compositionality of optimal control laws",
      "author" : [ "E. Todorov" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "Multi-task policy search for robotics",
      "author" : [ "M.P. Deisenroth", "P. Englert", "J. Peters", "D. Fox" ],
      "venue" : "In Proceedings of 2014 IEEE International Conference on Robotics and Automation (ICRA),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Various function approximation approaches to optimal control are available [1, 2] but usually sensitive to model uncertainty.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "Various function approximation approaches to optimal control are available [1, 2] but usually sensitive to model uncertainty.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 2,
      "context" : "In control theory the exponential transformation of the value function was introduced in [3, 4].",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "In control theory the exponential transformation of the value function was introduced in [3, 4].",
      "startOffset" : 89,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 116,
      "endOffset" : 128
    }, {
      "referenceID" : 5,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 116,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 116,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 116,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 9,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 197,
      "endOffset" : 217
    }, {
      "referenceID" : 10,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 197,
      "endOffset" : 217
    }, {
      "referenceID" : 11,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 197,
      "endOffset" : 217
    }, {
      "referenceID" : 12,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 197,
      "endOffset" : 217
    }, {
      "referenceID" : 13,
      "context" : "In the recent decade it has been explored in terms of path integral interpretations and theoretical generalizations [5, 6, 7, 8], discrete time formulations [9], and scalable RL/control algorithms [10, 11, 12, 13, 14].",
      "startOffset" : 197,
      "endOffset" : 217
    }, {
      "referenceID" : 8,
      "context" : "The resulting stochastic optimal control frameworks are known as Path Integral (PI) control for continuous time, Kullback Leibler (KL) control for discrete time, or more generally Linearly Solvable Optimal Control [9, 15].",
      "startOffset" : 214,
      "endOffset" : 221
    }, {
      "referenceID" : 14,
      "context" : "The resulting stochastic optimal control frameworks are known as Path Integral (PI) control for continuous time, Kullback Leibler (KL) control for discrete time, or more generally Linearly Solvable Optimal Control [9, 15].",
      "startOffset" : 214,
      "endOffset" : 221
    }, {
      "referenceID" : 9,
      "context" : "One way to circumvent this problem is to parameterize policies [10, 11, 14] and then perform optimization with sampling.",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "One way to circumvent this problem is to parameterize policies [10, 11, 14] and then perform optimization with sampling.",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 13,
      "context" : "One way to circumvent this problem is to parameterize policies [10, 11, 14] and then perform optimization with sampling.",
      "startOffset" : 63,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "Different from existing PI control approaches, our method combines the benefits of PI control theory [5, 6, 7] and probabilistic model-based reinforcement learning methodologies [16, 17].",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "Different from existing PI control approaches, our method combines the benefits of PI control theory [5, 6, 7] and probabilistic model-based reinforcement learning methodologies [16, 17].",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "Different from existing PI control approaches, our method combines the benefits of PI control theory [5, 6, 7] and probabilistic model-based reinforcement learning methodologies [16, 17].",
      "startOffset" : 101,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "Different from existing PI control approaches, our method combines the benefits of PI control theory [5, 6, 7] and probabilistic model-based reinforcement learning methodologies [16, 17].",
      "startOffset" : 178,
      "endOffset" : 186
    }, {
      "referenceID" : 16,
      "context" : "Different from existing PI control approaches, our method combines the benefits of PI control theory [5, 6, 7] and probabilistic model-based reinforcement learning methodologies [16, 17].",
      "startOffset" : 178,
      "endOffset" : 186
    }, {
      "referenceID" : 4,
      "context" : "• It extends the PI control theory [5, 6, 7] to the case of uncertain systems.",
      "startOffset" : 35,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "• It extends the PI control theory [5, 6, 7] to the case of uncertain systems.",
      "startOffset" : 35,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "• It extends the PI control theory [5, 6, 7] to the case of uncertain systems.",
      "startOffset" : 35,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "The structural constraint is enforced between the control cost and uncertainty of the learned dynamics, which can be viewed as a generalization of previous work [5, 6, 7].",
      "startOffset" : 161,
      "endOffset" : 170
    }, {
      "referenceID" : 5,
      "context" : "The structural constraint is enforced between the control cost and uncertainty of the learned dynamics, which can be viewed as a generalization of previous work [5, 6, 7].",
      "startOffset" : 161,
      "endOffset" : 170
    }, {
      "referenceID" : 6,
      "context" : "The structural constraint is enforced between the control cost and uncertainty of the learned dynamics, which can be viewed as a generalization of previous work [5, 6, 7].",
      "startOffset" : 161,
      "endOffset" : 170
    }, {
      "referenceID" : 9,
      "context" : "• Different from parameterized PI controllers [10, 11, 14, 8], we find analytic control law without any policy parameterization.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "• Different from parameterized PI controllers [10, 11, 14, 8], we find analytic control law without any policy parameterization.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "• Different from parameterized PI controllers [10, 11, 14, 8], we find analytic control law without any policy parameterization.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "• Different from parameterized PI controllers [10, 11, 14, 8], we find analytic control law without any policy parameterization.",
      "startOffset" : 46,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "• Rather than keeping a fixed control cost weight [5, 6, 7, 10, 18], or ignoring the constraint between control authority and noise level [11], in this work the control cost weight is adapted based on the explicit uncertainty of the learned dynamics model.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "• Rather than keeping a fixed control cost weight [5, 6, 7, 10, 18], or ignoring the constraint between control authority and noise level [11], in this work the control cost weight is adapted based on the explicit uncertainty of the learned dynamics model.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "• Rather than keeping a fixed control cost weight [5, 6, 7, 10, 18], or ignoring the constraint between control authority and noise level [11], in this work the control cost weight is adapted based on the explicit uncertainty of the learned dynamics model.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "• Rather than keeping a fixed control cost weight [5, 6, 7, 10, 18], or ignoring the constraint between control authority and noise level [11], in this work the control cost weight is adapted based on the explicit uncertainty of the learned dynamics model.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "• Rather than keeping a fixed control cost weight [5, 6, 7, 10, 18], or ignoring the constraint between control authority and noise level [11], in this work the control cost weight is adapted based on the explicit uncertainty of the learned dynamics model.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "• Rather than keeping a fixed control cost weight [5, 6, 7, 10, 18], or ignoring the constraint between control authority and noise level [11], in this work the control cost weight is adapted based on the explicit uncertainty of the learned dynamics model.",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 4,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 5,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 17,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 11,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 7,
      "context" : "• The algorithm operates in a different manner compared to existing PI-related methods that perform forward sampling [5, 6, 7, 10, 18, 11, 12, 14, 8].",
      "startOffset" : 117,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "• The proposed model-based approach is significantly more sample efficient than samplingbased PI control [5, 6, 7, 18].",
      "startOffset" : 105,
      "endOffset" : 118
    }, {
      "referenceID" : 5,
      "context" : "• The proposed model-based approach is significantly more sample efficient than samplingbased PI control [5, 6, 7, 18].",
      "startOffset" : 105,
      "endOffset" : 118
    }, {
      "referenceID" : 6,
      "context" : "• The proposed model-based approach is significantly more sample efficient than samplingbased PI control [5, 6, 7, 18].",
      "startOffset" : 105,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "• The proposed model-based approach is significantly more sample efficient than samplingbased PI control [5, 6, 7, 18].",
      "startOffset" : 105,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "In RL setting our method is comparable to the state-of-the-art RL methods [17, 19] in terms of sample and computational efficiency.",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : "In RL setting our method is comparable to the state-of-the-art RL methods [17, 19] in terms of sample and computational efficiency.",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized.",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 10,
      "context" : "In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized.",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 13,
      "context" : "In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized.",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 16,
      "context" : "In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized.",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : "In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized.",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized.",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 20,
      "context" : "In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized.",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "In contrast, most policy search and trajectory optimization methods [10, 11, 14, 17, 19, 20, 21, 22] find policy parameters that can not be generalized.",
      "startOffset" : 68,
      "endOffset" : 100
    }, {
      "referenceID" : 22,
      "context" : "Note that generally Ω is an infinite dimensional vector and we can use the same sample to represent uncertainty during learning [23].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "This constraint is different from existing works in path integral control [5, 6, 7, 10, 18, 8] where the constraint is enforced between the additive noise covariance and control authority, more precisely λGtR −1 t G T t = BΣωB .",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "This constraint is different from existing works in path integral control [5, 6, 7, 10, 18, 8] where the constraint is enforced between the additive noise covariance and control authority, more precisely λGtR −1 t G T t = BΣωB .",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "This constraint is different from existing works in path integral control [5, 6, 7, 10, 18, 8] where the constraint is enforced between the additive noise covariance and control authority, more precisely λGtR −1 t G T t = BΣωB .",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "This constraint is different from existing works in path integral control [5, 6, 7, 10, 18, 8] where the constraint is enforced between the additive noise covariance and control authority, more precisely λGtR −1 t G T t = BΣωB .",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : "This constraint is different from existing works in path integral control [5, 6, 7, 10, 18, 8] where the constraint is enforced between the additive noise covariance and control authority, more precisely λGtR −1 t G T t = BΣωB .",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "This constraint is different from existing works in path integral control [5, 6, 7, 10, 18, 8] where the constraint is enforced between the additive noise covariance and control authority, more precisely λGtR −1 t G T t = BΣωB .",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8].",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8].",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8].",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 9,
      "context" : "In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8].",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : "In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8].",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8].",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8].",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 7,
      "context" : "In contrast, most existing works use a fixed control cost weight [5, 6, 7, 10, 18, 12, 14, 8].",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "According to the path integral control theory [5, 6, 7, 10, 18, 8], the stochastic optimal control problem becomes an approximation problem of a path integral (8).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "According to the path integral control theory [5, 6, 7, 10, 18, 8], the stochastic optimal control problem becomes an approximation problem of a path integral (8).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "According to the path integral control theory [5, 6, 7, 10, 18, 8], the stochastic optimal control problem becomes an approximation problem of a path integral (8).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "According to the path integral control theory [5, 6, 7, 10, 18, 8], the stochastic optimal control problem becomes an approximation problem of a path integral (8).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : "According to the path integral control theory [5, 6, 7, 10, 18, 8], the stochastic optimal control problem becomes an approximation problem of a path integral (8).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "According to the path integral control theory [5, 6, 7, 10, 18, 8], the stochastic optimal control problem becomes an approximation problem of a path integral (8).",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "In order to improve sample efficiency, a nonparametric approach was developed by representing the desirability Ψt in terms of linear operators in a reproducing kernel Hilbert space (RKHS) [12].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 9,
      "context" : "Notable approaches include PI(2) [10], PI(2)-CMA [11], PI-REPS[14] and recently developed state-dependent PI[8].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "Notable approaches include PI(2) [10], PI(2)-CMA [11], PI-REPS[14] and recently developed state-dependent PI[8].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 13,
      "context" : "Notable approaches include PI(2) [10], PI(2)-CMA [11], PI-REPS[14] and recently developed state-dependent PI[8].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "Notable approaches include PI(2) [10], PI(2)-CMA [11], PI-REPS[14] and recently developed state-dependent PI[8].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "PI [5, 6, 7], iterative PI [18] PI(2)[10], PI(2)-CMA [11] PI-REPS[14] State feedback PI[8] Our method Structural constraint λGtR −1 t G T t = BΣωB T same as PI same as PI same as PI λGR−1GT = Σf Dynamics model model-based model-free model-based model-based GP model-based Policy parameterization No Yes Yes Yes No Table 1: Comparison with some notable and recent path integral-related approaches.",
      "startOffset" : 3,
      "endOffset" : 12
    }, {
      "referenceID" : 5,
      "context" : "PI [5, 6, 7], iterative PI [18] PI(2)[10], PI(2)-CMA [11] PI-REPS[14] State feedback PI[8] Our method Structural constraint λGtR −1 t G T t = BΣωB T same as PI same as PI same as PI λGR−1GT = Σf Dynamics model model-based model-free model-based model-based GP model-based Policy parameterization No Yes Yes Yes No Table 1: Comparison with some notable and recent path integral-related approaches.",
      "startOffset" : 3,
      "endOffset" : 12
    }, {
      "referenceID" : 6,
      "context" : "PI [5, 6, 7], iterative PI [18] PI(2)[10], PI(2)-CMA [11] PI-REPS[14] State feedback PI[8] Our method Structural constraint λGtR −1 t G T t = BΣωB T same as PI same as PI same as PI λGR−1GT = Σf Dynamics model model-based model-free model-based model-based GP model-based Policy parameterization No Yes Yes Yes No Table 1: Comparison with some notable and recent path integral-related approaches.",
      "startOffset" : 3,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "PI [5, 6, 7], iterative PI [18] PI(2)[10], PI(2)-CMA [11] PI-REPS[14] State feedback PI[8] Our method Structural constraint λGtR −1 t G T t = BΣωB T same as PI same as PI same as PI λGR−1GT = Σf Dynamics model model-based model-free model-based model-based GP model-based Policy parameterization No Yes Yes Yes No Table 1: Comparison with some notable and recent path integral-related approaches.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "PI [5, 6, 7], iterative PI [18] PI(2)[10], PI(2)-CMA [11] PI-REPS[14] State feedback PI[8] Our method Structural constraint λGtR −1 t G T t = BΣωB T same as PI same as PI same as PI λGR−1GT = Σf Dynamics model model-based model-free model-based model-based GP model-based Policy parameterization No Yes Yes Yes No Table 1: Comparison with some notable and recent path integral-related approaches.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "PI [5, 6, 7], iterative PI [18] PI(2)[10], PI(2)-CMA [11] PI-REPS[14] State feedback PI[8] Our method Structural constraint λGtR −1 t G T t = BΣωB T same as PI same as PI same as PI λGR−1GT = Σf Dynamics model model-based model-free model-based model-based GP model-based Policy parameterization No Yes Yes Yes No Table 1: Comparison with some notable and recent path integral-related approaches.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 13,
      "context" : "PI [5, 6, 7], iterative PI [18] PI(2)[10], PI(2)-CMA [11] PI-REPS[14] State feedback PI[8] Our method Structural constraint λGtR −1 t G T t = BΣωB T same as PI same as PI same as PI λGR−1GT = Σf Dynamics model model-based model-free model-based model-based GP model-based Policy parameterization No Yes Yes Yes No Table 1: Comparison with some notable and recent path integral-related approaches.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "PI [5, 6, 7], iterative PI [18] PI(2)[10], PI(2)-CMA [11] PI-REPS[14] State feedback PI[8] Our method Structural constraint λGtR −1 t G T t = BΣωB T same as PI same as PI same as PI λGR−1GT = Σf Dynamics model model-based model-free model-based model-based GP model-based Policy parameterization No Yes Yes Yes No Table 1: Comparison with some notable and recent path integral-related approaches.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 23,
      "context" : "To propagate the GP-based dynamics over a trajectory of time horizon T we employ the moment matching approach [24, 17] to compute the predictive distribution.",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "To propagate the GP-based dynamics over a trajectory of time horizon T we employ the moment matching approach [24, 17] to compute the predictive distribution.",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 23,
      "context" : "Details regarding the moment matching method can be found in [24, 17].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 16,
      "context" : "Details regarding the moment matching method can be found in [24, 17].",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 24,
      "context" : "The hyper-parameters σs, σω,W are learned by maximizing the log-likelihood of the training outputs given the inputs [25].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "The partial derivatives ∂μj ∂μj−dt , ∂μj ∂Σj−dt , ∂Σj ∂μj−dt , ∂Σj ∂Σj−dt can be computed analytically as in [17].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "Since Ψt and ∇xΨt are explicit functions of xt, the resulting control law is essentially different from the feedforward control in sampling-based path integral control frameworks [5, 6, 7, 10, 18] as well as the parameterized state feedback PI control policies [14, 8].",
      "startOffset" : 179,
      "endOffset" : 196
    }, {
      "referenceID" : 5,
      "context" : "Since Ψt and ∇xΨt are explicit functions of xt, the resulting control law is essentially different from the feedforward control in sampling-based path integral control frameworks [5, 6, 7, 10, 18] as well as the parameterized state feedback PI control policies [14, 8].",
      "startOffset" : 179,
      "endOffset" : 196
    }, {
      "referenceID" : 6,
      "context" : "Since Ψt and ∇xΨt are explicit functions of xt, the resulting control law is essentially different from the feedforward control in sampling-based path integral control frameworks [5, 6, 7, 10, 18] as well as the parameterized state feedback PI control policies [14, 8].",
      "startOffset" : 179,
      "endOffset" : 196
    }, {
      "referenceID" : 9,
      "context" : "Since Ψt and ∇xΨt are explicit functions of xt, the resulting control law is essentially different from the feedforward control in sampling-based path integral control frameworks [5, 6, 7, 10, 18] as well as the parameterized state feedback PI control policies [14, 8].",
      "startOffset" : 179,
      "endOffset" : 196
    }, {
      "referenceID" : 17,
      "context" : "Since Ψt and ∇xΨt are explicit functions of xt, the resulting control law is essentially different from the feedforward control in sampling-based path integral control frameworks [5, 6, 7, 10, 18] as well as the parameterized state feedback PI control policies [14, 8].",
      "startOffset" : 179,
      "endOffset" : 196
    }, {
      "referenceID" : 13,
      "context" : "Since Ψt and ∇xΨt are explicit functions of xt, the resulting control law is essentially different from the feedforward control in sampling-based path integral control frameworks [5, 6, 7, 10, 18] as well as the parameterized state feedback PI control policies [14, 8].",
      "startOffset" : 261,
      "endOffset" : 268
    }, {
      "referenceID" : 7,
      "context" : "Since Ψt and ∇xΨt are explicit functions of xt, the resulting control law is essentially different from the feedforward control in sampling-based path integral control frameworks [5, 6, 7, 10, 18] as well as the parameterized state feedback PI control policies [14, 8].",
      "startOffset" : 261,
      "endOffset" : 268
    }, {
      "referenceID" : 25,
      "context" : "The proposed approach is based on the compositionality theory [26] in linearly solvable optimal control (LSOC).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "The composite control law in (18) is essentially different from an interpolating control law[26].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22].",
      "startOffset" : 91,
      "endOffset" : 123
    }, {
      "referenceID" : 10,
      "context" : "This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22].",
      "startOffset" : 91,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22].",
      "startOffset" : 91,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22].",
      "startOffset" : 91,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22].",
      "startOffset" : 91,
      "endOffset" : 123
    }, {
      "referenceID" : 19,
      "context" : "This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22].",
      "startOffset" : 91,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22].",
      "startOffset" : 91,
      "endOffset" : 123
    }, {
      "referenceID" : 21,
      "context" : "This scheme can not be adopted in policy search or trajectory optimization methods such as [10, 11, 14, 17, 19, 20, 21, 22].",
      "startOffset" : 91,
      "endOffset" : 123
    }, {
      "referenceID" : 26,
      "context" : "Alternatively, generalization can be achieved by imposing task-dependent policies [27].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "In order to demonstrate the performance, we compare the proposed control framework with three related methods: iterative path integral control [18] with known dynamics model, PILCO [17] and PDDP [19].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "In order to demonstrate the performance, we compare the proposed control framework with three related methods: iterative path integral control [18] with known dynamics model, PILCO [17] and PDDP [19].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "In order to demonstrate the performance, we compare the proposed control framework with three related methods: iterative path integral control [18] with known dynamics model, PILCO [17] and PDDP [19].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "It is based on importance sampling using controlled diffusion process rather than passive dynamics used in standard path integral control [5, 6, 7].",
      "startOffset" : 138,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "It is based on importance sampling using controlled diffusion process rather than passive dynamics used in standard path integral control [5, 6, 7].",
      "startOffset" : 138,
      "endOffset" : 147
    }, {
      "referenceID" : 6,
      "context" : "It is based on importance sampling using controlled diffusion process rather than passive dynamics used in standard path integral control [5, 6, 7].",
      "startOffset" : 138,
      "endOffset" : 147
    }, {
      "referenceID" : 9,
      "context" : "In this work we do not compare our method with model-free PI-related approaches such as [10, 11, 12, 14] since these methods would certainly cost more samples than model-based methods such as PILCO and PDDP.",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "In this work we do not compare our method with model-free PI-related approaches such as [10, 11, 12, 14] since these methods would certainly cost more samples than model-based methods such as PILCO and PDDP.",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "In this work we do not compare our method with model-free PI-related approaches such as [10, 11, 12, 14] since these methods would certainly cost more samples than model-based methods such as PILCO and PDDP.",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 13,
      "context" : "In this work we do not compare our method with model-free PI-related approaches such as [10, 11, 12, 14] since these methods would certainly cost more samples than model-based methods such as PILCO and PDDP.",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "The reason for choosing these two methods for comparison is that our method adopts a similar model learning scheme while other state-of-the-art methods, such as [20] is based on a different model.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "The iterative PI [18] with a given dynamics model uses 10(3)/10(4) (CP/DPC) sample rollouts per iteration and 500 iterations at each time step.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 25,
      "context" : "The compositionality theory [26] generally does not apply to policy search methods and trajectory optimizers such as PILCO, PDDP, and other recent methods [20, 21, 22].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 19,
      "context" : "The compositionality theory [26] generally does not apply to policy search methods and trajectory optimizers such as PILCO, PDDP, and other recent methods [20, 21, 22].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 20,
      "context" : "The compositionality theory [26] generally does not apply to policy search methods and trajectory optimizers such as PILCO, PDDP, and other recent methods [20, 21, 22].",
      "startOffset" : 155,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "The compositionality theory [26] generally does not apply to policy search methods and trajectory optimizers such as PILCO, PDDP, and other recent methods [20, 21, 22].",
      "startOffset" : 155,
      "endOffset" : 167
    } ],
    "year" : 2015,
    "abstractText" : "We present a data-driven optimal control framework that is derived using the path integral (PI) control approach. We find iterative control laws analytically without a priori policy parameterization based on probabilistic representation of the learned dynamics model. The proposed algorithm operates in a forward-backward manner which differentiate it from other PI-related methods that perform forward sampling to find optimal controls. Our method uses significantly less samples to find analytic control laws compared to other approaches within the PI control family that rely on extensive sampling from given dynamics models or trials on physical systems in a model-free fashion. In addition, the learned controllers can be generalized to new tasks without re-sampling based on the compositionality theory for the linearly-solvable optimal control framework. We provide experimental results on three different tasks and comparisons with state-of-the-art model-based methods to demonstrate the efficiency and generalizability of the proposed framework.",
    "creator" : null
  }
}