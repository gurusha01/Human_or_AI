{
  "name" : "f4573fc71c731d5c362f0d7860945b88.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Collaborative Filtering with Graph Information: Consistency and Scalable Methods",
    "authors" : [ "Nikhil Rao", "Hsiang-Fu Yu", "Pradeep Ravikumar", "Inderjit S. Dhillon" ],
    "emails" : [ "inderjit}@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Low rank matrix completion approaches are among the most widely used collaborative filtering methods, where a partially observed matrix is available to the practitioner, who needs to impute the missing entries. Specifically, suppose there exists a ratings matrix Y 2 Rm⇥n, and we only observe a subset of the entries Yij , 8(i, j) 2 ⌦, |⌦| = N ⌧ mn. The goal is to estimate Yi,j , 8(i, j) /2 ⌦. To this end, one typically looks to solve one of the following (equivalent) programs:\nˆZ = argmin Z\n1\n2\nkP ⌦ (Y Z)k2F + zkZk⇤ (1)\nˆW, ˆH = argmin W,H\n1\n2\nkP ⌦ (Y WHT )k2F + w 2 kWk2F + h 2 kHk2F (2)\nwhere the nuclear norm kZk⇤, given by the sum of singular values, is a tight convex relaxation of the non convex rank penalty, and is equivalent to the regularizer in (2). P\n⌦ (·) is the projection operator that only retains those entries of the matrix that lie in the set ⌦.\nIn many cases however, one not only has the partially observed ratings matrix, but also has access to additional information about the relationships between the variables involved. For example, one might have access to a social network of users. Similarly, one might have access to attributes of items, movies, etc. The nature of the attributes can be fairly arbitrary, but it is reasonable to assume that “similar” users/items share “similar” attributes. A natural question to ask then, is if one can take advantage of this additional information to make better predictions. In this paper, we assume that the row and column variables lie on graphs. The graphs may naturally be part of the data (social networks, product co-purchasing graphs) or they can be constructed from available features. The idea then is to incorporate this additional structural information into the matrix completion setting.\nWe not only require the resulting optimization program to enforce additional constraints on Z, but we also require it to admit efficient optimization algorithms. We show in the sections that follow that this in fact is indeed the case. We also perform a theoretical analysis of our problem when the observed entries of Y are corrupted by additive white Gaussian noise. To summarize, the contributions of our paper are as follows:\n• We provide a scalable algorithm for matrix completion graph with structural information. Our method relies on efficient Hessian-vector multiplication schemes, and is orders of magnitude faster than (stochastic) gradient descent based approaches. • We make connections with other structured matrix factorization frameworks. Notably, we show that our method generalizes the weighted nuclear norm [21], and methods based on Gaussian generative models [27]. • We derive consistency guarantees for graph regularized matrix completion, and empirically show that our bound is smaller than that of traditional matrix completion, where graph information is ignored. • We empirically validate our claims, and show that our method achieves comparable error rates to other methods, while being significantly more scalable.\nRelated Work and Key Differences\nFor convex methods for matrix factorization, Haeffele et al. [9] provided a framework to use regularizers with norms other than the Euclidean norm in (2). Abernethy et al. [1] considered a kernel based embedding of the data, and showed that the resulting problem can be expressed as a norm minimization scheme. Srebro and Salakhutdinov [21] introduced a weighted nuclear norm, and showed that the method enjoys superior performance as compared to standard matrix completion under a non-uniform sampling scheme. We show that the graph based framework considered in this paper is in fact a generalization of the weighted nuclear norm problem, with non-diagonal weight matrices.\nIn the context of matrix factorization with graph structural information, [5] considered a graph regularized nonnegative matrix factorization framework and proposed a gradient descent based method to solve the problem. In the context of recommendation systems in social networks, Ma et al. [14] modeled the weight of a graph edge1 explicitly in a re-weighted regularization framework. Li and Yeung [13] considered a similar setting to ours, but a key point of difference between all the aforementioned methods and our paper is that we consider the partially observed ratings case. There are some works developing algorithms for the situation with partially observations [12, 26, 27]; however, none of them provides statistical guarantees. Weighted norm minimization has been considered before ([16, 21]) in the context of low rank matrix completion. The thrust of these methods has been to show that despite suboptimal conditions (correlated data, non-uniform sampling), the sample complexity does not change. None of these methods use graph information. We are interested in a complementary question: Given variables conforming to graph information, can we obtain better guarantees under uniform sampling to those achieved by traditional methods?"
    }, {
      "heading" : "2 Graph-Structured Matrix Factorization",
      "text" : "Assume that the “true” target matrix can be factorized as Z? = W ?(H?)T , and there exist a graph (V w, Ew) whose adjacency matrix encodes the relationships between the m rows of W ? and a graph (V h, Eh) for n rows of H?. In particular, two rows (or columns) connected by an edge in the graph are “close” to each other in the Euclidean distance. In the context of graph-based embedding, [3, 4] proposed a smoothing term of the form\n1\n2\nX\ni,j\nEwij(wi wj)2 = tr(WT Lap(Ew)W ) (3)\nwhere Lap(Ew) := Dw Ew is the graph Laplacian for (V w, Ew), where Dw is the diagonal matrix with Dwii = P j⇠i E w ij . Adding (3) into the minimization problem (2) encourages solutions where wi ⇡ wj when Ewij is large. A similar argument holds for H? and the associated graph Laplacian Lap(Eh).\n1The authors call this the “trust” between links in a social network\nWe would thus not only want the target matrix to be low rank, but also want the variables W,H to be faithful to the underlying graph structure. To this end, we consider the following problem:\nmin W,H\n1\n2\nkP ⌦ Y WHT k2F+ L 2 {tr(WT Lap(Ew)W ) + tr(HT Lap(Eh)H)}+ (4)\nw 2 kWk2F + h 2 kHk2F\n⌘ min W,H 1\n2\nkP ⌦ Y WHT k2F + 1\n2\ntr(WTLwW ) + tr(H TLhH)\n(5)\nwhere Lw := L Lap(Ew) + wIm, and Lh is defined similarly. Note that we subsume the regularization parameters in the definition of Lw, Lh. Note that kWk2F = tr(WT ImW ). The regularizer in (5) encourages solutions that are smooth with respect to the corresponding graphs. However, the Laplacian matrix can be replaced by other (positive, semi-definite) matrices that encourage structure by different means. Indeed, a very general class of Laplacian based regularizers was considered in [20], where one can replace Lw by a function:\nhx, ⌧(Lap(E))xi where ⌧(Lap(E)) ⌘ |V |X\ni=1\n⌧( i)qiq T i ,\nwhere {( i, qi)} constitute the eigen-system of Lap(E) and ⌧( i) is a scalar function of the eigenvalues. Our case corresponds to ⌧(·) being the identity function. We briefly summarize other schemes that fit neatly into (5), apart from the graph regularizer we consider:\nCovariance matrices for variables: [27] proposed a kernelized probabilistic matrix factorization (KPMF), which is a generative model to incorporate covariance information of the variables into matrix factorization. They assumed that each row of W ?, H? is generated according to a multivariate Gaussian, and solving the corresponding MAP estimation procedure yields exactly (5), with Lw = C 1w and Lh = C 1 h , where Cw, Ch are the associated covariance matrices.\nFeature matrices for variables: Assume that there is a feature matrix X 2 Rm⇥d for objects associated rows. For such X , one can construct a graph (and hence a Laplacian) using various methods such as k-nearest neighbors, ✏-nearest neighbors etc. Moreover, one can assume that there exists a kernel k(xi,xj) that encodes pairwise relations, and we can use the Kernel Gram matrix as a Laplacian.\nWe can thus see that problem (5) is a very general scheme, and can incorporate information available in many different forms. In the sequel, we assume the matrices Lw, Lh are given. In the theoretical analysis in Section 5, for ease of exposition, we further assume that the minimum eigenvalues of Lw, Lh are unity. A general (nonzero) minimum eigenvalue will merely introduce multiplicative constants in our bounds."
    }, {
      "heading" : "3 GRALS: Graph Regularized Alternating Least Squares",
      "text" : "In this section, we propose efficient algorithms for (5), which is convex with respect to W or H separately. This allows us to employ alternating minimization methods [25] to solve the problem. When Y is fully observed, Li and Yeung [13] propose an alternating minimization scheme using block steepest descent. We deal with the partially observed setting, and propose to apply conjugate gradient (CG), which is known to converge faster than steepest descent, to solve each subproblem. We propose a very efficient Hessian-vector multiplication routine that results in the algorithm being highly scalable, compared to the (stochastic) gradient descent approaches in [14, 27].\nWe assume that Y 2 Rm⇥n, W 2 Rm⇥k and H 2 Rn⇥k. When optimizing H with W fixed, we obtain the following sub-problem.\nmin H f(H) =\n1\n2\nkP ⌦ Y WHT k2F + 1\n2\ntr(HTLhH). (6)\nOptimizing W while H fixed is similar, and thus we only show the details for solving (6). Since Lh is nonsingular, (6) is strongly convex.2 We first present our algorithm for the fully observed case, since it sets the groundwork for the partially observed setting.\n2In fact, a nonsingular Lh can be handled using proximal updates, and our algorithm will still apply\nAlgorithm 1 Hv-Multiplication for g(s) • Given: Matrices Lh,W • Initialization: G = WTW • Multiplication: r2g(s\n0 )s: 1 Input: S 2 Rn⇥k s.t. s = vec(S) 2 A SG+ LhS 3 Return: vec(A)\nAlgorithm 2 Hv-Multiplication for g ⌦ (s)\n• Given: Matrices Lh,W,⌦ • Multiplication: r2g(s\n0 )s: 1 Input: S 2 Rk⇥n s.t.\ns = vec(S) 2 Compute K = [k\n1 , . . . ,kn] s.t. kj P i2⌦j (w T i sj)wi\n3 A K + SLh 4 Return: vec(A)"
    }, {
      "heading" : "3.1 Fully Observed Case",
      "text" : "As in [5, 13] among others, there may be scenarios where Y is completely observed, and the goal is to find the row/column embeddings that conform to the corresponding graphs. In this case, the loss term in (6) is simply kY WHT k2F . Thus, setting rf(H) = 0 is equivalent to solving the following Sylvester equation for an n⇥ k matrix H:\nHWTW + LhH = Y TW. (7)\n(7) admits a closed form solution. However the standard Bartels-Stewart algorithm for the Sylvester equation requires transforming both WTW and Lh into Schur form (diagonal in our case where WTW and Lh are symmetric) by the QR algorithm, which is time consuming for a large Lh. Thus, we consider applying conjugate gradient (CG) to minimize f(H) directly. We define the following quadratic function:\ng(s) := 1\n2\ns TMs vec Y TW T s, s 2 Rnk, M = Ik ⌦ Lh + (WTW )⌦ In\nIt is not hard to show that f(H) = g(vec(H)) and so we apply CG to minimize g(s).\nThe most crucial step in CG is the Hessian-vector multiplication. Using the identity (BT ⌦ A) vec(X) = vec(AXB), it follows that\n(Ik ⌦ Lh) s = vec(LhS) , and (WTW )⌦ In s = vec SWTW ,\nwhere vec(S) = s. Thus the Hessian-vector multiplication can be implemented by a series of matrix multiplications as follows.\nMs = vec LhS + S(W\nTW ) ,\nwhere WTW can be pre-computed and stored in O(k2) space. The details are presented in Algorithm 1. The time complexity for a single CG iteration is O(nnz(Lh)k+ nk2), where nnz(·) is the number of non zeros. Since in most practical applications k is generally small, the complexity is essentially O(nnz(Lh)k) as long as nk  nnz(Lh)."
    }, {
      "heading" : "3.2 Partially Observed Case",
      "text" : "In this case, the loss term of (6) becomes P (i,j)2⌦(Yij wTi hj)2, where wTi is the i-th row of W and hj is the j-th column of HT . Similar to the fully observed case, we can define:\ng ⌦ (s) :=\n1\n2\ns TM ⌦ s vec WTY T s,\nwhere M ⌦ = ¯B + Lh ⌦ Ik, ¯B 2 Rnk⇥nk is a block diagonal matrix with n diagonal blocks Bj 2 Rk⇥k. Bj = P i2⌦j wiw T i , where ⌦j = {i : (i, j) 2 ⌦}. Again, we can see f(H) =\ng ⌦ (vec\nHT ). Note that the transpose HT is used here instead of H , which is used in the fully\nobserved case.\nFor a given s, let S = [s 1 , . . . sj , . . . sn] be a matrix such that vec(S) = s and K = [k\n1 , . . . ,kj , . . . ,kn] with kj = Bjsj . Then ¯Bs = vec(K). Note that since n can be very large in practice, it may not be feasible to compute and store all Bj in the beginning. Alternatively, Bjsj can be computed in O(|⌦j |k) time as follows.\nBjsj = X\ni2⌦j\n(w T i sj)wi.\nThus ¯Bs can be computed in O(|⌦|k) time, and the Hessian-vector multiplication M ⌦ s can be done in O (|⌦|k + nnz(Lh)k) time. See Algorithm 2 for a detailed procedure. As a result, each CG iteration for minimizing g\n⌦\n(s) is also very cheap.\nRemark on Convergence. In [2], it is shown that any local minimizer of (5) is a global minimizer of (5) if k is larger than the true rank of the underlying matrix.3 From [25], the alternating minimization procedure is guaranteed to globally converge to a block coordinate-wise minimum4 of (5). The converged point might not be a local minimizer, but it still yields good performance in practice. Most importantly, since the updates are cheap to perform, our algorithm scales well to large datasets."
    }, {
      "heading" : "4 Convex Connection via Generalized Weighted Nuclear Norm",
      "text" : "We now show that the regularizer in (5) can be cast as a generalized version of the weighted nuclear norm. The weights in our case will correspond to the scaling factors introduced on the matrices W,H due to the eigenvalues of the shifted graph Laplacians Lw, Lh respectively."
    }, {
      "heading" : "4.1 A weighted atomic norm:",
      "text" : "From [7], we know that the nuclear norm is the gauge function induced by the atomic set: A⇤ = {wihTi : kwik = khik = 1}. Note that all rank one matrices in A⇤ have unit Frobenius norm. Now, assume P = [p\n1 , . . . ,pm] 2 Rm⇥m is a basis of Rm and S 1/2p is a diagonal matrix with (S 1/2p )ii 0 encoding the “preference” over the space spanned by pi. The more the preference, the larger the value. Similarly, consider the basis Q and the preference S 1/2q for Rn. Let A = PS 1/2p and B = QS 1/2 q , and consider the following “preferential” atomic set:\nA := { i = wihTi : wi = Aui,hi = Bvi, kuik = kvik = 1}. (8) Clearly, each atom in A has non-unit Frobenius norm. This atomic set allows for biasing of the solutions towards certain atoms. We then define a corresponding atomic norm:\nkZkA = inf X\ni2A |ci| s.t. Z =\nX i2A ci i. (9)\nIt is not hard to verify that kZkA is a norm and {Z : kZkA  ⌧} is closed and convex."
    }, {
      "heading" : "4.2 Equivalence to Graph Regularization",
      "text" : "The graph regularization (5) can be shown to be a special case of the atomic norm (9), as a consequence of the following result:\nTheorem 1. For any A = PS 1/2p , B = QS 1/2q , and corresponding weighted atomic set A ,\nkZkA = inf W,H 1\n2\n{kA 1Wk2F + kB 1Hk2F } s.t. Z = WHT .\nWe prove this result in Appendix A. Theorem 1 immediately leads us to the following equivalence result: Corollary 1. Let Lw = UwSwUTw and Lh = UhShUTh be the eigen decomposition for Lw and Lh. We have\nTr WTLwW = kA 1Wk2F , and Tr HTLhH = kB 1Hk2F ,\nwhere A = UwS 1/2 w and B = UhS 1/2 h . As a result, kMkA with the preference pair (Uw, S 1/2 w ) for the column space and the preference pair (Uh, S 1/2 h ) for row space is a weighted atomic norm equivalent for the graph regularization using Lw and Lh.\nThe results above allow us to obtain the dual weighted atomic norm for a matrix Z\nkZk⇤A = kATZB k = kS 12 w UTwZUhS 12 h k (10)\n3The authors actually show this for a more general class of regularizers. 4Nash equilibrium is used in [25].\nwhich is a weighted spectral norm. An elementary proof of this result can be found in Appendix B. Note that we can then write\nkZkA = kA 1ZB T k⇤ = kS 1 2 wU 1w ZU T h S 1 2 h k⇤ (11)\nIn [21], the authors consider a norm similar to (11), but with A,B being diagonal matrices. In the spirit of their nomenclature, we refer to the norm in (11) as the generalized weighted nuclear norm."
    }, {
      "heading" : "5 Statistical Consistency in the Presence of Noisy Measurements",
      "text" : "In this section, we derive theoretical guarantees for the graph regularized low rank matrix estimators. We first introduce some additional notation. We assume that there is a m ⇥ n matrix Z? of rank k with kZ?kF = 1, and N = |⌦| entries of Z? are uniformly sampled5 and revealed to us (i.e., Y = P\n⌦ (Z?)). We further assume an one-to-one mapping between the set of observed indices ⌦ and {1, 2, . . . , N} so that the tth measurement is given by\nyt = Yi(t),j(t) = hei(t)eTj(t), Z?i+ p mn ⌘t ⌘t ⇠ N (0, 1). (12)\nwhere h·, ·i denotes the matrix trace inner product, and i(t), j(t) is a randomly selected coordinate pair from [m]⇥[n]. Let A,B are corresponding matrices defined in Corollary 1 for the given Lw, Lh. W.L.O.G, we assume that the minimum singular value of both Lw and Lh is 1. We then define the following graph based complexity measures:\n↵g(Z) := p mn kA 1ZB T k1 kA 1ZB T kF , g(Z) := kA 1ZB T k⇤ kA 1ZB T kF\n(13)\nwhere k · k1 is the element-wise `1 norm. Finally, we assume that the true matrix Z? can be expressed as a linear combination of atoms from (8) (we define ↵? := ↵g(Z?)):\nZ? = AU?(V ?)TBT , U? 2 Rm⇥k, V ? 2 Rn⇥k, (14)\nOur goal in this section will be to characterize the solution to the following convex program, where the constraint set precludes selection of overly complex matrices in the sense of (13):\nˆZ = argmin Z2C\n1 N kP ⌦\n(Y Z)k2F + kZkA where C := ( Z : ↵g(Z) g(Z)  c̄0 s N\nlog(m+ n)\n) ,\n(15) where c̄\n0\nis a constant depending on ↵?.\nA quick note on solving (15): since k · kA is a weighted nuclear norm, one can resort to proximal point methods [6], or greedy methods developed specifically for atomic norm constrained minimization [18, 22]. The latter are particularly attractive, since the greedy step reduces to computing the maximum singular vectors which can be efficiently computed using power methods. However, such methods will first involve computing the eigen decompositions of the graph Laplacians, and then storing the large, dense matrices A,B. We refrain from resorting to such methods in Section 6, and instead use the efficient framework derived in Section 3. We now state our main theoretical result: Theorem 2. Suppose we observe N entries of the form (12) from a matrix Z? 2 Rm⇥n, with ↵? := ↵g(Z?) and which can be represented using at most k atoms from (8). Let ˆZ be the minimizer\nof the convex problem (15) with C 1\nq (m+n) log(m+n)\nN . Then, with high probability, we have\nk ˆZ Z?k2F  C↵?2 max 1, 2 k(m+ n) log(m+ n) N +O\n✓ ↵?2\nN\n◆\nwhere C, C 1 are positive constants.\nSee Appendix C for the detailed proof. A proof sketch is as follows:\n5Our results can be generalized to non uniform sampling schemes as well.\nProof Sketch: There are three major portions of the proof: • Using the fact that Z? has unit Frobenius norm and can be expressed as a combination of\nat most k atoms, we can show kZ?kA  p k (Appendix C.1)\n• Using (10), we can derive a bound for the dual norm of the gradient of the loss L(Z), given by krL(Z)k⇤A = kS 12 w UTwrL(Z)UhS 12 h k. (Appendix C.2) • Finally, using (13), we define a notion of restricted strong convexity (RSC) that the “error” matrices Z? ˆZ lie in. The proof follows closely along the lines of the equivalent result in [16], with appropriate modifications to accommodate our generalized weighted nuclear norm. (Appendix C.3)."
    }, {
      "heading" : "5.1 Comparison to Standard Matrix Completion:",
      "text" : "It is instructive to consider our result in the context of noisy matrix completion with uniform samples. In this case, one would replace Lw, Lh by identity matrices, effectively ignoring graph information available. Specifically, the “standard” notion of spikiness (↵n := p mnkZk1kZkF ) defined in [16] will apply, and the corresponding error bound (Theorem 2) will have ↵? replaced by ↵n(Z?). In general, it is hard to quantify the relationship between ↵g and ↵n, and a detailed comparison is an interesting topic for future work. However, we show below using simulations for various scenarios that the former is much smaller than the latter. We generate m ⇥ m matrices of rank k = 10, M = U⌃V T with U, V being random orthonormal matrices and ⌃ having diagonal elements picked from a uniform[0, 1] distribution. We generate graphs at random using the schemes discussed below, and set Z = AMBT , with A,B as defined in Corollary 1. We then compute ↵n,↵g for various m.\nComparing ↵g to ↵n: Most real world graphs exhibit a power law degree distribution. We generated graphs with the ith node having degree (m ⇥ ip) with varying negative p values. Figure 1(a) shows that as p ! 0 from below, the gains received from using our norm is clear compared to the standard nuclear norm. We also observe that in general the weighted formulation is never worse then unweighted (The dotted magenta line is ↵n/↵g = 1). The same applies for random graphs, where there is an edge between each (i, j) with varying probability p (Figure 1(b)).\nSample Complexity: We tested the sample complexity needed to recover a m = n = 200, k = 20 matrix, generated from a power law distributed graph with p = 0.5. Figure 1(c) again outlines that the atomic formulation requires fewer examples to get an accurate recovery. We average the results over 10 independent runs, and we used [18] to solve the atomic norm constrained problem."
    }, {
      "heading" : "6 Experiments on Real Datasets",
      "text" : "Comparison to Related Formulations: We compare GRALS to other methods that incorporate side information for matrix completion: the ADMM method of [12] that regularizes the entire target matrix; using known features (IMC) [10, 24]; and standard matrix completion (MC). We use the MOVIELENS 100k dataset,6 that has user/movie features along with the ratings matrix. The dataset contains user features (such as age (numeric), gender (binary), and occupation), which we map\n6http://grouplens.org/datasets/movielens/\ninto a 22 dimensional feature vector per user. We then construct a 10-nearest neighbor graph using the euclidean distance metric. We do the same for the movies, except in this case we have an 18 dimensional feature vector per movie. For IMC, we use the feature vectors directly. We trained a model of rank 10, and chose optimal parameters by cross validation. Table 1 shows the RMSE obtained for the methods considered. Figure 2 shows that the ADMM method, while obtaining a reasonable RMSE does not scale well, since one has to compute an SVD at each iteration. Scalability of GRALS: We now demonstrate that the proposed GRALS method is more efficient than other state-of-the-art methods for solving the graph-regularized matrix factorization problem (5). We compare GRALS to the SGD method in [27], and GD: ALS with simple gradient descent. We consider three large-scale real-world collaborate filtering datasets with graph information: see Table 2 for details.7 We randomly select 90% of ratings as the training set and use the remaining 10% as the test set. All the experiments are performed on an Intel machine with Xeon CPU E52680 v2 Ivy Bridge and enough RAM. Figure 3 shows orders of magnitude improvement in time compared to SGD. More experimental results are provided in the supplementary material."
    }, {
      "heading" : "7 Discussion",
      "text" : "In this paper, we have considered the problem of collaborative filtering with graph information for users and/or items, and showed that it can be cast as a generalized weighted nuclear norm problem. We derived statistical consistency guarantees for our method, and developed a highly scalable alternating minimization method. Experiments on large real world datasets show that our method achieves ⇠ 2 orders of magnitude speedups over competing approaches."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This research was supported by NSF grant CCF-1320746. H.-F. Yu acknowledges support from an Intel PhD fellowship. NR was supported by an ICES fellowship.\n7See more details in Appendix D."
    } ],
    "references" : [ {
      "title" : "Low-rank matrix factorization with attributes",
      "author" : [ "Jacob Abernethy", "Francis Bach", "Theodoros Evgeniou", "Jean-Philippe Vert" ],
      "venue" : "arXiv preprint cs/0611124,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Convex sparse matrix factorizations",
      "author" : [ "Francis Bach", "Julien Mairal", "Jean Ponce" ],
      "venue" : "CoRR, abs/0812.1869,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Laplacian eigenmaps and spectral techniques for embedding and clustering",
      "author" : [ "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "Mikhail Belkin", "Partha Niyogi" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Graph regularized nonnegative matrix factorization for data representation",
      "author" : [ "Deng Cai", "Xiaofei He", "Jiawei Han", "Thomas S Huang" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "Jian-Feng Cai", "Emmanuel J Candès", "Zuowei Shen" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1956
    }, {
      "title" : "The convex geometry of linear inverse problems",
      "author" : [ "Venkat Chandrasekaran", "Benjamin Recht", "Pablo A Parrilo", "Alan S Willsky" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "The yahoo! music dataset and kdd-cup’11",
      "author" : [ "Gideon Dror", "Noam Koenigstein", "Yehuda Koren", "Markus Weimer" ],
      "venue" : "In KDD Cup,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing",
      "author" : [ "Benjamin Haeffele", "Eric Young", "Rene Vidal" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Provable inductive matrix completion",
      "author" : [ "Prateek Jain", "Inderjit S Dhillon" ],
      "venue" : "arXiv preprint arXiv:1306.0626,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "A matrix factorization technique with trust propagation for recommendation in social networks",
      "author" : [ "Mohsen Jamali", "Martin Ester" ],
      "venue" : "In Proceedings of the Fourth ACM Conference on Recommender Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Relation regularized matrix factorization",
      "author" : [ "Wu-Jun Li", "Dit-Yan Yeung" ],
      "venue" : "In 21st International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Recommender systems with social regularization",
      "author" : [ "Hao Ma", "Dengyong Zhou", "Chao Liu", "Michael R. Lyu", "Irwin King" ],
      "venue" : "In Proceedings of the fourth ACM international conference on Web search and data mining,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Trust-aware bootstrapping of recommender systems",
      "author" : [ "Paolo Massa", "Paolo Avesani" ],
      "venue" : "ECAI Workshop on Recommender Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise",
      "author" : [ "Sahand Negahban", "Martin J Wainwright" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Conditional gradient with enhancement and truncation for atomic-norm regularization",
      "author" : [ "Nikhil Rao", "Parikshit Shah", "Stephen Wright" ],
      "venue" : "NIPS workshop on Greedy Algorithms,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "Benjamin Recht" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Kernels and regularization on graphs. In Learning theory and kernel machines, pages 144–158",
      "author" : [ "Alexander J Smola", "Risi Kondor" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2003
    }, {
      "title" : "Collaborative filtering in a non-uniform world: Learning with the weighted trace norm",
      "author" : [ "Nathan Srebro", "Ruslan R Salakhutdinov" ],
      "venue" : "In Advances in Neural Information Processing",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Greedy algorithms for structurally constrained high dimensional problems",
      "author" : [ "Ambuj Tewari", "Pradeep K Ravikumar", "Inderjit S Dhillon" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "A note on sums of independent random matrices after ahlswede-winter",
      "author" : [ "Roman Vershynin" ],
      "venue" : "Lecture notes,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Speedup matrix completion with side information: Application to multi-label learning",
      "author" : [ "Miao Xu", "Rong Jin", "Zhi-Hua Zhou" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion",
      "author" : [ "Yangyang. Xu", "Wotao Yin" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Expert finding for question answering via graph regularized matrix completion",
      "author" : [ "Zhou Zhao", "Lijun Zhang", "Xiaofei He", "Wilfred Ng" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Kernelized probabilistic matrix factorization: Exploiting graphs and side information",
      "author" : [ "Tinghui Zhou", "Hanhuai Shan", "Arindam Banerjee", "Guillermo Sapiro" ],
      "venue" : "In SDM,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Notably, we show that our method generalizes the weighted nuclear norm [21], and methods based on Gaussian generative models [27].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 25,
      "context" : "Notably, we show that our method generalizes the weighted nuclear norm [21], and methods based on Gaussian generative models [27].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "[9] provided a framework to use regularizers with norms other than the Euclidean norm in (2).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] considered a kernel based embedding of the data, and showed that the resulting problem can be expressed as a norm minimization scheme.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "Srebro and Salakhutdinov [21] introduced a weighted nuclear norm, and showed that the method enjoys superior performance as compared to standard matrix completion under a non-uniform sampling scheme.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "In the context of matrix factorization with graph structural information, [5] considered a graph regularized nonnegative matrix factorization framework and proposed a gradient descent based method to solve the problem.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "[14] modeled the weight of a graph edge1 explicitly in a re-weighted regularization framework.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Li and Yeung [13] considered a similar setting to ours, but a key point of difference between all the aforementioned methods and our paper is that we consider the partially observed ratings case.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 24,
      "context" : "There are some works developing algorithms for the situation with partially observations [12, 26, 27]; however, none of them provides statistical guarantees.",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 25,
      "context" : "There are some works developing algorithms for the situation with partially observations [12, 26, 27]; however, none of them provides statistical guarantees.",
      "startOffset" : 89,
      "endOffset" : 101
    }, {
      "referenceID" : 14,
      "context" : "Weighted norm minimization has been considered before ([16, 21]) in the context of low rank matrix completion.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 19,
      "context" : "Weighted norm minimization has been considered before ([16, 21]) in the context of low rank matrix completion.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "In the context of graph-based embedding, [3, 4] proposed a smoothing term of the form",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "In the context of graph-based embedding, [3, 4] proposed a smoothing term of the form",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "Indeed, a very general class of Laplacian based regularizers was considered in [20], where one can replace Lw by a function: hx, ⌧(Lap(E))xi where ⌧(Lap(E)) ⌘ |V | X",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "We briefly summarize other schemes that fit neatly into (5), apart from the graph regularizer we consider: Covariance matrices for variables: [27] proposed a kernelized probabilistic matrix factorization (KPMF), which is a generative model to incorporate covariance information of the variables into matrix factorization.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "This allows us to employ alternating minimization methods [25] to solve the problem.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "When Y is fully observed, Li and Yeung [13] propose an alternating minimization scheme using block steepest descent.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "We propose a very efficient Hessian-vector multiplication routine that results in the algorithm being highly scalable, compared to the (stochastic) gradient descent approaches in [14, 27].",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 25,
      "context" : "We propose a very efficient Hessian-vector multiplication routine that results in the algorithm being highly scalable, compared to the (stochastic) gradient descent approaches in [14, 27].",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "1 Fully Observed Case As in [5, 13] among others, there may be scenarios where Y is completely observed, and the goal is to find the row/column embeddings that conform to the corresponding graphs.",
      "startOffset" : 28,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "1 Fully Observed Case As in [5, 13] among others, there may be scenarios where Y is completely observed, and the goal is to find the row/column embeddings that conform to the corresponding graphs.",
      "startOffset" : 28,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "In [2], it is shown that any local minimizer of (5) is a global minimizer of (5) if k is larger than the true rank of the underlying matrix.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 23,
      "context" : "3 From [25], the alternating minimization procedure is guaranteed to globally converge to a block coordinate-wise minimum4 of (5).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 6,
      "context" : "1 A weighted atomic norm: From [7], we know that the nuclear norm is the gauge function induced by the atomic set: A⇤ = {wihi : kwik = khik = 1}.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 19,
      "context" : "Note that we can then write kZkA = kA 1ZB T k⇤ = kS 1 2 wU 1 w ZU T h S 1 2 h k⇤ (11) In [21], the authors consider a norm similar to (11), but with A,B being diagonal matrices.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "A quick note on solving (15): since k · kA is a weighted nuclear norm, one can resort to proximal point methods [6], or greedy methods developed specifically for atomic norm constrained minimization [18, 22].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 16,
      "context" : "A quick note on solving (15): since k · kA is a weighted nuclear norm, one can resort to proximal point methods [6], or greedy methods developed specifically for atomic norm constrained minimization [18, 22].",
      "startOffset" : 199,
      "endOffset" : 207
    }, {
      "referenceID" : 20,
      "context" : "A quick note on solving (15): since k · kA is a weighted nuclear norm, one can resort to proximal point methods [6], or greedy methods developed specifically for atomic norm constrained minimization [18, 22].",
      "startOffset" : 199,
      "endOffset" : 207
    }, {
      "referenceID" : 14,
      "context" : "The proof follows closely along the lines of the equivalent result in [16], with appropriate modifications to accommodate our generalized weighted nuclear norm.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "Specifically, the “standard” notion of spikiness (↵n := p mnkZk1 kZkF ) defined in [16] will apply, and the corresponding error bound (Theorem 2) will have ↵? replaced by ↵n(Z).",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "We average the results over 10 independent runs, and we used [18] to solve the atomic norm constrained problem.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "6 Experiments on Real Datasets Comparison to Related Formulations: We compare GRALS to other methods that incorporate side information for matrix completion: the ADMM method of [12] that regularizes the entire target matrix; using known features (IMC) [10, 24]; and standard matrix completion (MC).",
      "startOffset" : 252,
      "endOffset" : 260
    }, {
      "referenceID" : 22,
      "context" : "6 Experiments on Real Datasets Comparison to Related Formulations: We compare GRALS to other methods that incorporate side information for matrix completion: the ADMM method of [12] that regularizes the entire target matrix; using known features (IMC) [10, 24]; and standard matrix completion (MC).",
      "startOffset" : 252,
      "endOffset" : 260
    }, {
      "referenceID" : 10,
      "context" : "Dataset # users # items # ratings # links rank used Flixster ([11]) 147,612 48,794 8,196,077 2,538,746 10 Douban ([14]) 129,490 58,541 16,830,839 1,711,802 10 YahooMusic ([8]) 249,012 296,111 55,749,965 57,248,136 20",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "Dataset # users # items # ratings # links rank used Flixster ([11]) 147,612 48,794 8,196,077 2,538,746 10 Douban ([14]) 129,490 58,541 16,830,839 1,711,802 10 YahooMusic ([8]) 249,012 296,111 55,749,965 57,248,136 20",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 7,
      "context" : "Dataset # users # items # ratings # links rank used Flixster ([11]) 147,612 48,794 8,196,077 2,538,746 10 Douban ([14]) 129,490 58,541 16,830,839 1,711,802 10 YahooMusic ([8]) 249,012 296,111 55,749,965 57,248,136 20",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 25,
      "context" : "We compare GRALS to the SGD method in [27], and GD: ALS with simple gradient descent.",
      "startOffset" : 38,
      "endOffset" : 42
    } ],
    "year" : 2015,
    "abstractText" : "Low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space. Often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions. We tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph. We formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art (stochastic) gradient-descent based methods. On the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees. We validate our results on both real and synthetic datasets.",
    "creator" : null
  }
}