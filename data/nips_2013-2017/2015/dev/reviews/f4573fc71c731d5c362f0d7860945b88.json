{"title": "Collaborative Filtering with Graph Information: Consistency and Scalable Methods", "abstract": "Low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space. Often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions. We tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph. We formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magnitude faster than state-of-the-art (stochastic) gradient-descent based methods. On the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees. We validate our results on both real and synthetic datasets.", "id": "f4573fc71c731d5c362f0d7860945b88", "authors": ["Nikhil Rao", "Hsiang-Fu Yu", "Pradeep K. Ravikumar", "Inderjit S. Dhillon"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The authors propose a method for incorporating underlying graph structure among users/items in a collaborative filtering setting. The novelty of this method is that it is the first method to both incorporate graph structure and deal with sparse data.\n\n The method itself is fairly straightforward and clearly explained. It amounts to block-coordinate ascent, solving a convex problem at each iteration. One potential methodological contribution is to solve for the the low-rank factors using conjugate-gradient methods to avoid slow matrix decompositions, but it's unclear whether this is the first time such an approach was proposed.\n\nConnections are drawn to the nuclear norm minimization of Srebro and Salakhutdinov (2010), and the authors show that their method is equivalent to employing a weighted atomic norm, where the weights are derived from the graph structure. This connection allows them to derive a consistency bound on recovering the true low-rank matrix. The bound is specified in terms of a quantity \\alpha_^* which is shown empirically to be much smaller for realistic matrices than the corresponding bound for matrix completion.\n\nFinally, the method is validated on three real datasets and is shown to outperform standard matrix completion in that it is able to achieve a given RMSE in much less time.\n\nThe work is of high quality and clearly explained. I amn less confident of its originality but if this is indeed the first time the conjugate-gradient method embedded in the coordinate-ascent scheme was emplyed in a collaborative filtering setting, then this is a reasonably novel contribution. Overall, this seems to be a signficiant advancement in how further exploiting known structure in features to improve collaborative filtering models.\n\n This paper represents a substantial contribution to the growing body of work around leveraging underlying structure among users/items for collaborative filtering. The paper is well-written and the contributions are both theoretical (consistency bounds in terms of a quantity that is empirically verified to be tighter) and practical in nature (faster method by orders of magnitude, superior performance on real data sets).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Paper 1260 tackles the problem of matrix completion with side information represented as a graph. The authors claim that this is a general framework in that the side information can come from many different kinds of sources. Major contributions of this paper are scalable learning algorithm for matrix completition with graph structures, connecting it to existing works in matrix factorization literature, and verifying the proposed method theoretically as well as experimentally.\n\nAs the authors claim, this problem is widely applicable such as recommendation systems with social network information. Considering the size of such dataset, it is important to bulid a scalable algorithm as the authors proposed.\n\nThis paper is well-structured and well-written. It is good to have both theoretical analysis of consistency and experiments with real data. Also, it is useful to connect the proposed method in the context of wider convex optimization and noisy matrix approximation.\n\nRegarding the experiment (Section 6): some baselines in Table 1 are too simple; no one might be interested in comparison with Global/User/Movie mean. I suggest comparing against more recent, state-of-the-art methods such as\n\nSalakhutdinov and Mnih, Probabilistic Matrix Factorization [NIPS 2008], Mackey et al., Divide-and-Conquer Matrix Factorization [NIPS 2011], Lee et al., Local Low-Rank Matrix Approximation [ICML 2013].\n\nIt is impressive that the proposed method runs in O(10^3) seconds with dataset in Table 2. However, to claim scalability, I recommend running the algorithm on larger dataset, such as Netflix or Yahoo Music. This way it is also possible comparing RMSE score with other methods as well.\n\nMinor comments: 1) Why Figure 1 and Figure 2 have different format of x-axis? I recommend using format of Figure 2 (10^1, 10^2, ...) instead of plotting log(time) directly. Log-scale without specifying base is ambiguous. 2) Citations are not in NIPS format.  This paper presents GRALS, an efficient way of optimizing graph-regularized matrix completion problem. With some minor issues with experiments, I see this paper is generally well-written and clear.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper considers matrix completion in a setting where the row and column variables lie on graphs. The authors\n\ndevelop a scalable alternating least squares algorithm. Further, they show that the regularizer in the optimization problem can be seen as a generalized form of weighted nuclear norm, and derive statistical consistency guarantees for the low rank matrix estimators. Experiments comparing to leading methods on a movie ratings data set shows their method achieving lowest RMSE. Further, their ALS algorithm is shown to scale orders of magnitude better than SGD.\n\na. The description of how the row/column graphs are generated from the movielens dataset is vague in the paper; please clarify.\n\nb. Sections 5 and 5.1 were difficult to follow. Terms such as \"spikiness\" are not defined, but it's key to following the main theoretical result and the comparison to standard matrix completion.\n\nc. why is there no RMSE table for the 3 large datasets? from Fig.2 it's unclear if RMSE of GRALS is equal/poorer than the other methods (it's clear that GRALS scales better) The optimization problem considered in this paper -- graph-structed matrix factorization with partial observations -- appears to be novel and is likely to be of significant interest to the collaborative filtering community. The solution is based on weighted norm minimization (similar to work in Srebro and Salakhutdinov (2010)). The alternating least squares algorithm developed by the authors is convincingly shown to be much more efficient than SGD, and performs better than leading methods that include side information. However, I found the writing in key sections difficult to follow, and I haven't checked proofs.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
