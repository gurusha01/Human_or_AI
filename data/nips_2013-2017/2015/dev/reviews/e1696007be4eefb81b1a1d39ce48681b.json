{"title": "Structured Estimation with Atomic Norms: General Bounds and Applications", "abstract": "For structured estimation problems with atomic norms, recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures, in particular Gaussian width of the unit norm ball, Gaussian width of a spherical cap induced by a tangent cone, and a restricted norm compatibility constant. However, given an atomic norm, bounding these geometric measures can be difficult. In this paper, we present general upper bounds for such geometric measures, which only require simple information of the atomic norm under consideration, and we establish tightness of these bounds by providing the corresponding lower bounds. We show applications of our analysis to certain atomic norms, especially k-support norm, for which existing result is incomplete.", "id": "e1696007be4eefb81b1a1d39ce48681b", "authors": ["Sheng Chen", "Arindam Banerjee"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "=== Summary ===\n\nIn this paper, the authors propose to bound geometric quantities that appears in statistical error bounds of structured estimation problems with atomic norms.\n\nIt was previously shown that the estimation error of Dantzig-like estimator, with random Gaussian design, could be bounded by using three key quantities: the Gaussian width of the unit ball of the regularizer, the Gaussian width of the spherical cap of the tangent cone and the restricted norm compatibility constant. The goal of this article is to provide easy to compute bounds for these three quantities, in the case of sign-invariant atomic norms.\n\nIn section 3, the authors introduce upper bounds on the three geometric quantities, which are the main contribution of the paper. The Gaussian width of the unit ball can be bounded by decomposing the set of atoms as the union of simpler sets, for which the Gaussian width are easy to compute. This result already appeared in [14]. The Gaussian width of the spherical cap of the tangent cone can be bounded using a single subgradient of the regularizer, taken at the true parameter vector w^*. Finally, the restricted norm compatibility constant can be bounded using two other \"norm compatibility constants\" which are simpler to compute (because the set on which they are defined are simpler). The authors show on two examples, the L1 and OWL norms, how these results can easily be applied.\n\nIn section 4, the authors provide lower bounds for the two last quantities. The lower bounds does not match the upper bounds, but show that the upper bound are relatively tight, and thus usefull.\n\nIn section 5, the bounds obtained in this paper are applied to the k-support norm, improving previously known results.\n\n=== Significance ===\n\nI believe this paper is addressing an important problem, providing interesting and useful new tools for studying sparse estimation problems with atomic norms. The paper is clearly written and well organized. The use of examples helps to understand the results and how to apply those in practice. Finally, the bounds derived in the paper allowed to obtain new results for the k-support norm, showing the relevance of the results. I believe this is a well written and interesting paper, providing easy to compute bounds to study the statistical error of Dantzig-like estimator with atomic norms.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper studies atomic norms used in structured sparse regression. For a given atomic set A (satisfying a sign-change invariance assumption), the paper bounds the Gaussian width of the unit norm ball, of the tangent cone, and the L2-compatibility number--quantities which are sufficient to characterize the rate of convergence of the estimation problem under certain settings.\n\nThe new technique is general. When applied to the L1 norm, both unweighted and weighted, it is shown to match bounds derived from existing techniques. The authors also apply the new technique on k-support norms and derive novel bounds; the new bounds have the interesting implication that k should be under-specified in practice.\n\n The unit norm ball analysis builds on a lemma from [14] and does not seem a significantly novel contribution. The tangent cone and the compatibility analyses look nontrivial. They build upon a simple but powerful observation that the tangent cone of an atomic norm can be upper bounded by the tangent cone of a weighted L1 norm (Lemma 3).\n\nMy one suggestion is that the authors perform numerical simulations with k-support norms and test whether the bound matches actual behavior. It would be interesting to see whether over-specifying k would hurt recovery.\n\n I find the paper interesting and the proof relatively easy to follow. The paper is well-organized and the results clearly stated and explained.\n\n [14] An inequality with application to structured sparsity and multi-task dictionary learning. This paper gives a new method of bounding the Gaussian width of various sets important for deriving the rates of consistency of predictors with structured sparsity penalties. The new method seems innovative and gives a novel convergence rate for regressions with k-support norms.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper is about sample complexity and error bounds for Dantzig-type estimators in the context of structured regression. A central ingredient of such bounds are geometric properties like the the Gaussian width of the norm ball and similar related quantities. For the class of atomic norms that are invariant under sign changes, novel bounds of these geometric properties are provided, which can be easily computed in practice. Further, tightness of the bounds is analyzed by showing that these upper bounds are close to the corresponding lower bounds.\n\nThe main focus of this work concerns a topic which is definitely relevant and interesting to the Machine Learning community. The papers is written in a clear and transparent way. In my opinion, the bounds obtained are both of theoretical and practical interest. However, I have to admit that I did not fully understood some technical details like the proof of Theorem 7.\n\n  An interesting theory paper about a highly relevant topic. Well written.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors consider sparse linear model estimation with atomic norms and provide some bounds on Gaussian widths arising in their analysis. As a special case, the authors derive bounds for l1 and ordered wighted l1 norm and provides some analysis on the recently proposed k-support norms.\n\nThe atomic norm is attractive for its generality to capture a broad class of linear inverse problems [Chandrasekaran et al, 2012 FOCM]. The present paper, however, focuses only on sparse linear models and the results, except for the recently proposed k-support norm, are already available. The proof techniques for calculating Gaussian widths are also fairly standard, although the dual norm calculation for k-support norm analysis is new.\n\nIs this technique generalizable to other instances of atomic norm analysis, e.g., nuclear norm penalized estimation of low-rank matrices?\n\nThe paper is well-written, the proofs are clear and easy to follow.\n\n  The paper is well-written and there is no gap in the mathematical proofs. However the scope of the problem is narrow: instead of providing analysis for general atomic norm analysis [Chandrasekaran et al, 2012 FOCM] unifying in a variety of linear inverse problems, the authors only consider sparse linear regression for which the key results are already available.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
