{"title": "Adaptive Stochastic Optimization: From Sets to Paths", "abstract": "Adaptive stochastic optimization optimizes an objective function adaptively under uncertainty. Adaptive stochastic optimization plays a crucial role in planning and learning under uncertainty, but is, unfortunately, computationally intractable in general.  This paper introduces two conditions on the objective function, the marginal likelihood rate bound and the marginal likelihood bound, which enable efficient approximate solution of adaptive stochastic optimization. Several interesting classes of functions satisfy these conditions naturally, e.g., the version space reduction function for hypothesis learning.  We describe Recursive Adaptive Coverage (RAC),  a new adaptive stochastic optimization algorithm that exploits these conditions, and apply it to two planning tasks under uncertainty. In constrast to the earlier submodular optimization approach, our algorithm applies to adaptive stochastic optimization algorithm over both sets and paths.", "id": "df6d2338b2b8fce1ec2f6dda0a630eb0", "authors": ["Zhan Wei Lim", "David Hsu", "Wee Sun Lee"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "*Summary*\n\nThis paper studies the adaptive stochastic optimization on paths, by imposing some constraints (namely the MLRB and the MLB conditions) on the relationship between the probability of a realization and its utility. While the notion of *adaptive submodularity* states that in expectation, no item will \"surprise\" us given a longer history, the MLRB / MLB conditions in this paper roughly characterizes the fact that such \"surprise\" only happens with small probability (i.e., realization with small probability has large utility). By leveraging such constraints, the authors propose a novel recursive algorithm with near-optimal guarantee on the expected cost needed to reach a certain coverage. Under the MLB conditions, the paper recovers the classical approximation result on bayesian pool-based active learning, up to a constant factor.\n\n*Quality*\n\nThe paper is nicely written and well structured. Regarding the experimental results: by Theorem 4, it would be very interesting to see how the proposed RAC algorithm competes with existing algorithms *empirically* on the pool-based active learning problem (e.g., [3][5][7], or [6] under the bounded noise setting), given the fact that the version space reduction function and the Gibbs error function both satisfy MLRB condition, pointwise submodularity condition, and adaptive submodularity condition. Having these results will certainly make the paper stronger.\n\n*Clarity*\n\nMost of the paper is clear. It would be helpful if the authors elaborate more on the intuitive explanations of the two conditions: why are those two conditions more natural than the adaptive submodularity condition for adaptive optimization problems on paths?\n\n*Originality and Significance*\n\nTo the best of my knowledge, the technical contribution of this contribution is novel. It provides interesting insights to developing tractable algorithms for the adaptive stochastic optimization problem. The paper proposes an interesting solution to the adaptive stochastic optimization problem on paths (and sets as a special case). The authors identify a class of problems that can be near-optimally approximated using their solution, and provides strong empirical evidence. The presentation is in general clear and easy to read.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper gives an iterative procedure to solve the Adaptive Stochastic Optimization problem. The problem defined by an objective function f, and the paper develops conditions on the function that make the problem amenable to their solution approach (recursive adaptive convergence).\n\nQuality: The ASO problem is described as a generalization of POMDP and conditions on the solution technique are given in simulation. The choice of baseline algorithms makes sense, but the paper would be much stronger if it the approach was compared more directly to similar approaches, e.g. popular POMDP solutions. Since the paper is rather technical giving more practical/concrete examples of the propositions would be helpful.\n\n Clarity: The layout of the paper, language and flow are excellent, especially given the technical nature of the material.\n\nConnecting the flow to specific problems is difficult. Having a running example, might make the paper easier to follow.\n\n Originality:\n\nThis paper seems to capitalize on theoretical developments in sub-modular function optimization and applying these results to the ASO problem. The original part comes from the particular application.\n\nSignificance: Given the generality of the setting, the results could have a significant impact.\n\nHowever, the current presentation does not make a strong case for this potential. Comparing the numerical results to alternative solution techniques or more directly arguing for the applicably of the MLRB and MLB conditions would make the case of impact stronger.\n\n This paper gives a tractablesolution algorithm and sufficient conditions for when it's applicable for a general (and generally intractable) planning problem (ASO).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper is generally clear, with a few exceptions as noted below.\n\nThe paper extends current work in a potentially useful way, although the experiments used to illustrate the method are fairly small and simple.\n\nThe theoretical work relates this work to prior work (prop 2), demonstrates applicability to a new domain (prop 1 and 3), and analyzes the algorithm.\n\nThe usefulness of the algorithm analysis is questionable (e.g. Thm 1 and 2).\n\nThe resulting bound grows with the square of the log of the number of actions (where this is actually the number of possible locations on the path, as each action is \"visit location x\") as compared to the optimal cost.\n\nThis does not seem especially tight for application with any reasonable-sized problem.\n\nA couple of clarity issues: line 110, should this inequality be reversed as A is a subset of B? line 125, what are the conditions for termination of a policy and how is that determined in practice? line between 151 and 152, I think some mistakes in notation line 216, what are alignment assumptions? experiments: the labels on the plots don't match those in the text.\n\nThe captions should be more informative and not repeat information already on the plots (Gibbs error)  This paper introduces an extension to submodular optimization that extends the method to apply to more than sets.The contributions include the development and analysis of two conditions that allow this, as well as introducing an algorithm to solve problems meeting these conditions.These contributions appear novel, well justified, and of interest to the community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
