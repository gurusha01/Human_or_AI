{"title": "Galileo: Perceiving Physical Object Properties by Integrating a Physics Engine with Deep Learning", "abstract": "Humans demonstrate remarkable abilities to predict physical events in dynamic scenes, and to infer the physical properties of objects from static images. We propose a generative model for solving these problems of physical scene understanding from real-world videos and images. At the core of our generative model is a 3D physics engine, operating on an object-based representation of physical properties, including mass, position, 3D shape, and friction. We can infer these latent properties using relatively brief runs of MCMC, which drive simulations in the physics engine to fit key features of visual observations. We further explore directly mapping visual inputs to physical properties, inverting a part of the generative process using deep learning. We name our model Galileo, and evaluate it on a video dataset with simple yet physically rich scenarios. Results show that Galileo is able to infer the physical properties of objects and predict the outcome of a variety of physical events, with an accuracy comparable to human subjects. Our study points towards an account of human vision with generative physical knowledge at its core, and various recognition models as helpers leading to efficient inference.", "id": "d09bf41544a3365a46c9077ebb5e35c3", "authors": ["Jiajun Wu", "Ilker Yildirim", "Joseph J. Lim", "Bill Freeman", "Josh Tenenbaum"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Summary: The authors introduce a novel approach for inferring hidden physical properties of objects (mass and friction), which also allows the system to make subsequent predictions that depend on these properties. They use a black-box generative model (a physics simulator), to perform sampling-based inference, and leverage a tracking algorithm to transform the data into more suitable latent variables (and reduce its dimensionality) as well as a deep model to improve the sampler. The authors assume priors over the hidden physical properties, and make point estimates of the geometry and velocities of objects using a tracking algorithm, which comprise a full specification of the scene that can be input to a physics engine to generate simulated velocities. These simulated velocities then support inference of the hidden properties within an MCMC sampler: the properties' values are proposed and their consequent simulated velocities are generated, which are then scored against the estimated velocities, similar to ABC. A deep network can be trained as a recognition model, from the inferences of the generative model, and also from the Physics 101 dataset directly. Its predictions of the mass and friction can be used to initialize the MCMC sampler. The authors compare performance to a convnet and to various human judgments.\n\nQuality: Good. The approach is well-executed, and provides good performance on an interesting task.\n\nClarity: The writing is very clear. Few (if any) typos, easy to understand.\n\nOriginality: I'm not aware of any competing approach to this problem, and the authors implement a convnet (\"oracle\" model) to compare their system against.\n\nSignificance: This work is strong, and should be of interest to both machine learning and computer vision.\n\nSuggestions: - The outcome prediction analysis (pgs 6-7) should include another comparison besides the error bar graph, such as correlations between human, POM, and uniform, because the summary \"Mean\" bars in Figure 5 give a misleading impression that human and POM are very similar, despite their patterns of errors being fairly different. - Section 4 says that after inferring physical properties, the system can be applied to novel settings, such as predicting buoyancy, which the authors say will be demoed in Section 6. There is no more talk of buoyancy, however, so this statement should be removed. The paper should be accepted because it offers a novel amalgam of deep learning and rich generative models to tackle a complex, real-world physical reasoning problem. The writing is clear, the technical work is well-executed, and the performance is thoroughly tested in various ways.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This is a high quality paper, clearly written, original, and has the potential for considerable significance.\n\n As I see it, the major contribution and innovation here is that the authors connect a symbolic physical model to an image-level recognition and tracking algorithm.\n\nAs such, I think the authors need to dedicate a bit more space to discussing the image-level recognition/tracking model, its capabilities and limitations.\n\nThe scenarios they assess are rather impoverished images; how well would this model fair with new scene configurations? with surfaces that are not smooth planes?\n\nWith objects that are not blocks?\n\nInsofar as this is indeed as general an algorithm as it might seem at first blush, this is worth advertising clearly.\n\nIf it is limited to fairly simple scenes with known geometric configurations (which would be an easy way to make something that operates over image data, but only because the domain of objects is very impoverished), this needs to be clearly stated.\n\nEither way, this reflects useful progress, but the former scenario is far more impressive.\n\nA few other comments: - In the introduction (lines ~96-98) the authors make the point that a \"computational solution asserts that the infant starts with a relatively reliable mental physics engine, or acquires it soon after birth\". This statement sounds like a nativist claim that I don't believe the authors would hold fast to - if infants developed a mature physics engine throughout the first year or two of life, the findings from this paper would still hold (and I would not count this as 'soon after birth'). In fact, much of Renee Baillaregon's work demonstrates that physical reasoning performance develops throughout the first year of life. If the authors wish to claim that mental physics engines are developed (and well formed) very early, they should give a nod to this work; however, I believe this statement distracts from the overall message of the paper and could be dropped.\n\n- On line 198 the authors fix sigma (the velocity noise) to 0.05. However, it is not clear how this value was set, or how sensitive to changes in this parameter the model is. It would be helpful to provide units for this value (is it in m/s? normalized to average velocity?) and briefly demonstrate that the model predictions do not change significantly under a couple other reasonable values for this parameter.\n\n- In the Outcome Prediction experiment (6.1), it is mentioned that the target object is either cardboard or foam, but the x-axis in Fig 5 represents only (I believe) the material of the initially moving object. Are the errors in Fig 5 averaged across both target objects, or was each initial material associated with only one of the two target objects?\n\n- It seems useful to estimate error correlations between people, the uniform model, and POM on the data in figure 5, given the wide variation in errors across all scenes.\n\n- For the other experiments (Mass Prediction & \"Will It Move\") it's not clear how the data is being analyzed. In both cases, I believe the human decision is dichotomous (obj1 heavier vs obj2 heavier for Mass Prediction and moving vs not for Will it Move), and assume that this is aggregated for each trial to get the proportion of people making each decision. Then in both cases, the model samples from the posterior a number of times to get a proportion of either choice for each model? Further clarity on this analysis would help readers understand these results\n\n- In the Mass Prediction experiment, the correlations between human and model data are calculated using Spearman's rho because the relationship is claimed to be very non-linear. However, if the POM is capturing human judgments accurately then there should be a linear, 1:1 relationship. Therefore, Pearson's correlation would be more appropriate for this relationship (unless the correlation would be skewed by outliers). Showing scatterplots of these relationships by trial would also help readers understand this relationship.\n\n- In the first sentence of the paper (line 37) it is a *Rube* Goldberg machine The paper introduces a physical reasoning system that combines a physics engine and an image-level tracking algorithm to infer latent physical properties of objects, and evaluate predictions of this system against humans in a number of experiments.This work is a useful contribution that has the potential to go far beyond current physical reasoning models by virtue of dealing directly with image-level data.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper takes on the question of whether it is possible to make predictions about the temporal evolution of objects in a scene, from watching video streams of that scene. For instance, when we see a block move a certain way, we form an expectation of where it is going, presumably because of an intuitive physics model that is part of our cognitive system. This subject has been explored in recent literature, some of which is cited by the authors.\n\nThe contribution of this paper is to utilise a deep learning tool to make predictions from visual features, which is coupled with a Bayesian generative formulation of the dynamics of the objects. The Bayesian formula in eq 1 is a standard statement of how physical parameters get translated into visual features. The authors take the observed velocity, via a tracker, as the observable of interest.\n\nThe main contribution of the paper, as I understand it, is to utilise these tools to conduct an experiment wherein the system is shown to be capable of making predictions about the evolution of the scene based on partial traces of the video. We see many variations on this experiment, to show both that the system can 'classify', i.e., make predictions about discrete labels such will or will not hit a point, as well as continuous variables such as how much motion we will see.\n\n If I were to look beyond the fact that LeNet allows one to get at visual features, the rest of the argument is not all that surprising. To the extent that the simulator acts as parameterised model, some of whose parameters are unknown at the start, what the authors are doing is exactly the same thing as inverse problems that have long been solved by meteorologists and oil industry professionals who do this with much more complex forms of dynamics (e.g., fluid flow and the estimation of viscosity). So, is it is really surprising that eq 1 can be used to estimate mass of the object?\n\n The comparison to human perception is interesting and not something that statisticians who look at inverse problems have considered before. However, we don't get much elaboration of this. Indeed, the sprinkling of cognitive hypotheses throughout the paper is of a very cursory kind without substantive claims and argument. For instance, in pp 2, we have the claim that a certain type of model would need a good physics engine to be innate - a big assumption one way or the other. However, that paragraph is left hanging without further discussion.\n\n Ultimately, beyond the timeliness of the use of a deep learning tool for getting at visual features (not something that is being advanced as an innovation within this paper), I am unsure of the novel and significant contributions of this paper.  This paper uses a generative model formulation to infer physical properties of objects in a video dataset, to show that useful predictions can be made about future evolution of the scene. The generative model itself is a standard Bayesian formula, which is coupled with a deep learning tool to make predictions from the image(s). While the results seem valid, this reviewer is unsure of its significance above and beyond what is already known from the prior work on the same topic.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
