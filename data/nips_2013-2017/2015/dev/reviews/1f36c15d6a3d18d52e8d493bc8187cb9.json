{"title": "Optimal Testing for Properties of Distributions", "abstract": "Given samples from an unknown  distribution, p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has receivedtremendous attention in Statistics, albeit focusing onasymptotic analysis, as well as in Computer Science, wherethe emphasis has been on small sample size and computationalcomplexity. Nevertheless, even for basic classes ofdistributions such as monotone, log-concave, unimodal, and monotone hazard rate, the optimal sample complexity is unknown.We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem:Given samplesfrom an unknown distribution p, and a known distribution q, are p and q close in Chi^2-distance, or far in total variation distance?The optimality of all testers is established by providing matching lower bounds. Finally, a necessary building block for our tester and important byproduct of our work are the first known computationally efficient proper learners for discretelog-concave and monotone hazard rate distributions. We exhibit the efficacy of our testers via experimental analysis.", "id": "1f36c15d6a3d18d52e8d493bc8187cb9", "authors": ["Jayadev Acharya", "Constantinos Daskalakis", "Gautam Kamath"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper has results about tests like:\n\nH0:\n\nP is monotonone\n\n versus\n\n H1: P is no monotone\n\nI like the paper.\n\nThe results are interesting. However, I have a few concerns:\n\n1. The sample space you use is a discrete cube.\n\nWhy would anyone want\n\n to test for monotonicity over this space?\n\nIn practice, people use\n\n monotinicity tests for continuous random variables.\n\nCan you cite\n\n any real data analysis problem where your setting is of scientific interest?\n\n2. You dismiss the statistics literature as concentrating on the large\n\n sample regime.\n\nThis is a bit misleading: many of the tests can be\n\n made exact by simulating the null distribution. The use of asymptotics\n\n is often to get precise theoretical results about the power of the\n\n tests. By precise I mean limits, not bounds.\n\nAnd there are\n\n statistics papers that do provide finite sample guarantees.\n\nAn\n\n example is:\n\n Dumbgen, Lutz, and Guenther Walther.\n\n \"Multiscale inference about a density.\" The Annals of Statistics (2008): 1758-1785.\n\n At any rate, your references to the vast statistical literature on this topic\n\n is too sparse. You need to have more references to the statistical\n\n literature.\n\n 3. Also, I am not convinced your test really is a finite sample test.\n\n Suppose I want to use your test and I want to make sure the type I\n\n error is less than alpha. (You take alpha = 1/3 but I assume you\n\n can change things to make alpha any user-specified level.) Your\n\n results say: there exists some N0 such that the type I error is\n\n less than alpha if the sample size N is larger than N, the type I\n\n error is less than alpha. The asymptotic statistics tests say: for\n\n large N the test has type I error close to alpha. I don't see any\n\n real difference. If I use your test with real data, I have no way\n\n of knowing if N is bigger than N0. We simply have to assume N is\n\n large enough. So it seems to me that, at a practical level the same\n\n as an asymptotic test.\n\n  Interesting paper. But I am not convinced this problem actually comes up in practice. Also, the connections to statistics are dismissed too readily.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I only give a very quick pass on the paper because it is supposed to be a \"light\" review. I found the materials interesting and very well motivated.\n\n The paper looks interesting.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "It seems to be perfect (I did not understand the details, to be honest). I am wondering why the tester is good only for monotonic, log-concave, unimodal, and monotonic hazard rate. It would be pleasing to explain why.  Considering a novel use of the chi square statistic, the paper constructs a sample-optimal and computationally efficient testers for class C and epsilon.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The problem of testing membership of an unknown distribution to the sets of monotone, log-concave, unimodal, and monotone hazard rate distributions over [n] (or [n]^d, for monotonicity) is considered. The approach to each problem follows a nice general framework -- first, an estimate q of the unknown distribution p is constructed, which effectively assumes that p belongs to the class being considered to reduce sample complexity. A modified chi-squared test is then applied to test whether p and q are close. The estimators necessary to achieve the first step are novel and unique to each class of distributions considered.\n\nThis two step procedure avoids the need for obvious alternatives such as testing equality between p and, say, a net over the class in question, which incurs large computational and statistical costs. The proposed approach gives efficient algorithms with improved (and optimal) sample complexities.\n\nIt is perhaps somewhat noteworthy that not only are the (optimal) rates for each of the four classes identical (in the case d=1, for monotone distributions), but that the construction used in the matching lower bound for each problem is identical.\n\n Including some definitions would improve clarity for readers not familiar with this particular subject, e.g. the partial order in the definition of monotone distributions over the hypergrid. (Also, line 161 promises definitions of the distances in the appendix, which seem to be missing.) The paper makes significant contributions to testing shape-constrained discrete distributions.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper derives a new method and new bounds for testing whether a distribution belongs to a certain class (eg monotone, log-concave, uni-modal) distribution. The authors provide both upper and lower bounds for the classes they consider (showing that their upper bounds are essentially tight). They also provide some simulations on synthetic data for their algorithm.\n\nThe paper is generally well written and the results seem correct. The authors also provide adequate comparisons with earlier bounds and discuss carefully in which regimes their result provides a better rate.\n\nMy main concern about this work is that it only considers discrete distributions. It would be nice if the authors could provide some motivational discussion on whether the actual testing problems they consider occur in practice. Further it would be nice to see some discussion on whether the results are extendable to non-discrete distribution (maybe under sone assumptions on the density?).\n\nMinor comments: The authors could make a stronger effort to make it a self contained read. Eg, on page 3 they refer the reader to the appendix for some basic definitions, but those definitions are not in the appendix.\n\n This paper derives a new method and new bounds for testing whether a distribution belongs to a certain class of (discrete distrbutions). The results are well presented.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This manuscript aims to the determine the number of observations required to distinguish an unknown distribution $p$ from some class of distributions $\\mathcal{C}$ that is at least $\\epsilon$ away in total variation distance. Different classes of $\\mathcal{C}$ considered include monotone, log-concave, unimodal and moonotone hazard rate distributions.\n\nBelow are a few questions and comments which will hopefully help the author(s) improve the manuscript:\n\nA. Motivations I feel that the motivations for imposing shape-related constraints on discrete distributions remain to be explained further in this manuscript. Discrete distributions are mainly useful for modelling (i) the frequencies of observed objects (e.g. IP addresses) and (ii) counting data. In (i), applying monotone constraint means that one has to impose an ordinal structure on the objects; furthermore, one needs a metric associated with the ordinal structure to use the log-concave constraint. These implications somewhat limit the use of the proposed method, since not all the objects have a natural order.\n\nB. Theory. B1. Proof of Theorem 1.\n\nIn order to have (1) and (2), one needs independence between the observations $X_i$ and the newly-drawn distribution $q$ (which satisfies Properties 1 and 2). This is not always the case, unless the observations used to estimate $q$ are discarded for the rest of the analysis. This requirement should be stated more explicitly.\n\n B2. Use of Lemma 5.\n\nIn the monotone case, Lemma 5 states that one can find $q$ such that $\\mathbb{E}[\\chi^2(p,q)]\\le \\epsilon^2/500$. However, in order to invoke Theorem~1, one requires $\\chi^2(p,q) \\le \\epsilon^2/500$. So there is a gap in between.\n\n B3. Continuous case vs discrete case.\n\nBirges (1987) was cited a few times in the development of the theory. To my knowledge, the work of Birges only deals with density estimation. Could the author(s) state the particular result from Birges (1987) that has been referred to many times in the manuscript?\n\nB4. Log-concavity vs unimodality.\n\nIn the proof of Lemma 7, as well as in Section H.2 in the appendix, it is stated that `any log-concave distribution is unimodal'. However, this is false in view of the definition given in Section~2 (i.e. $f_{i-1}f_{i+1} \\le f_i^2$). For example, consider $n=7$, $f_1 = f_4 = f_7 = 1/3$ and $f_2 = f_3 = f_5 = f_6 = 0$.\n\n B5. Rate for testing log-concavity.\n\nIt is stated in the abstract that testing log-concavity requires $O(sqrt{n}/\\epsilon^2)$ samples. However, in view of Theorem 4, this is true when $\\epsilon > n^{-1/4}$. Clearly, this statement is not precise when we are in the region of `fixing $n$ and decreasing $\\epsilon$'.\n\n C. Connections to Statistics. It is worth pointing out that here the author(s) assumed that the distribution of interest, i.e. $p$, is discrete. In addition, in the development of theory, the support of $p$ is assumed to be $[n]$ (i.e., $\\{1,2,\\ldots,n\\}$), which grows with respect to $n$. This setting is quite popular in Computer Science, though perhaps is not very well-known in Statistics. The author(s) cited work such as Hall and Van Keilegom (2005) and Cule and Samworth (2010). However, the cited work only deal with the continuous case. For more relevant work on estimating/testing a discrete monotone and log-concave distribution, see Jankowski and Wellner(2009) and Balabdaoui et.al.(2013).\n\n References\n\nJankowski, H. and Wellner, J.A. (2009). Estimation of a discrete monotone distribution. Electronic Journal of Statistics, 3, 1567--1605.\n\nBalabdaoui, F., Jankowski, H., Rufibach, K. and Pavlides, M. (2013). Asymptotic distribution of the discrete log-concave MLE and related applications, Journal of Royal Statistical Society Series B, 75, 769-790.\n\n  The main method illustrated in this manuscript is built upon Valiant and Valiant (2014), though the new settings are substantially different, which lead to the development of new theory (as well as new lemmas in the appendix).I feel that the main contribution of this manuscript is its theory (which seems interesting), though I remain to be convinced that the proofs are rigorous enough.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
