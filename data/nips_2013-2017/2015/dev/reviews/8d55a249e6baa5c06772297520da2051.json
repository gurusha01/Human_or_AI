{"title": "Learning Structured Output Representation using Deep Conditional Generative Models", "abstract": "Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.", "id": "8d55a249e6baa5c06772297520da2051", "authors": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Summary: A framework for learning complex structured output representations is presented. To this end variational auto-encoders (VAE) are extended to `conditional VAEs,' i.e., conditioned on the input data x.\n\nQuality - The paper is mostly well written, could however be improved occasionally. Clarity - The idea is clearly presented but some details are missing. Originality - Conditional VAEs seem to be a straightforward extension of standard VAEs, but certainly worth a discussion Significance - The significance could be improved by a more extensive evaluation showing results for various modifications\n\nComments: - I think the term generative is typically used when learning distributions that also involve the input data. This seems different from the proposed conditional variational auto-encoder technique that is not capable of generating data via p(x). Hence readers could get confused.\n\n- Three different methods are possible for inference, i.e., estimation of y given x. Two of them are evaluated using toy data. But which of the ones discussed in Sec. 3.1. do the authors recommend?\n\n- The authors mention a gap (l. 148) measurable by the regularization term during/after learning, i.e., the proposal q(z|x,y) and the prior p(z|x) don't exactly agree. How big is this gap?\n\n- The proposed solution to close the gap is to introduce a Gaussian stochastic neural network which models the reconstruction term directly, i.e., no regularization using a KL divergence. A weighted combination of the conditional VAE (computes regularization+reconstruction) and the newly introduced network (computes reconstruction) is then proposed as the desirable cost function. This however seems counter-intuitive to me. I would have expected the regularization gap between both distributions to be closer to zero if more weight is placed on the regularization rather than on the reconstruction term. Could the authors provide intuition? How did the authors set/cross-validate \\alpha and what was the resulting value?\n\n- There is a significant amount of semantic image segmentation techniques using CNNs. For fairness the authors might consider citing a few more recent ones.\n\n- More details regarding the noise injection into data x is useful for a reader. What exactly did the authors do?\n\n- Moreover as a reader I'm very interested in quantitative results regarding the modifications described in Sec. 3.3.2, i.e., how important is the `latent-to-output' pathway really? In addition I'm curious as to how much performance improved by using the direct output prediction as the input for the conditional prior network? Further I'd appreciate if the authors could clarify whether the conditional prior network as trained using only the direct output prediction as its input or whether both data x and prediction \\tilde{y} were used? Although some of the modifications are investigated in Tab. 3, a more careful ablation analysis would seems very useful to a reader.\n\n- Since the conditional VAE involves multiple networks I'm wondering whether its improved performance is a result of the larger amount of trainable parameters. Can the authors comment?\n\n- Since efficiency is claimed in the abstract I'm wondering about the time for training and inference. Can the authors comment?\n\n------------- As pointed out by fellow reviewers some citations could be added for completeness. Extending variational autoencoders to conditional distributions seems valuable. A more extensive evaluation and including missing details could improve the paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper extends variational auto-encoders to the structured prediction setting, where now it is used to model the probability of the structured output given the input. It adds several innovations to the training process, and shows results on pixel labeling on CUB and LFW. The paper is novel to the best of my knowledge, and using such expressive models for structured prediction is I think an important goal and something that can have a high impact. The aims of this paper and the ideas presented in this paper are therefore very welcome. However the paper falls short in two ways: - The paper is not very clearly written. It is missing several details. First, what is the architecture of all the nets? The authors mention at some point that all networks are MLPs with 2 layers, but that can't be the full story because an MLP is not a generative model. Perhaps the authors mean that the MLP predicts the parameters of a Gaussian distribution as in [24] or a mixture model in [16]? It is also not clear how p(y|x,z) is being modeled by two networks net_{z2y} and net_{x2y}. Do these two networks produce predictions that are averaged together? Multiplied together? Later researchers trying to reproduce results will have a hard time. - The results are a bit underwhelming. Tables 3 and 4 show that the CNN is remarkably competitive while also being much simpler. Is the 1 point improvement worth it? (I am also not sure of what a 1 point gain in these datasets means). Table 5 does show good results, but the setting is a bit contrived.\n\nIn total, I like the ideas described in this paper and believe they deserve exploration. However, the results are not very great, and the paper requires some rewriting to make it clearer. This paper uses recent progress in variational autoencoders to help structured prediction. While the aims are noble, the results are slightly underwhelming, and several details are missing.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In particular, the prediction of a structured output that is not the input data (ex. predicting segmentation labels) starts to make the use of the terminology of auto-encoders a bit of a stretch. Predicting the other half of an image stays within the normal conceptual framework of an autoencoding scheme.\n\nThe use of the term 'conditional *prior*' deviates a little far from what I think some members of the NIPS community would consider as appropriate and acceptable terminology.\n\nWhy not just call P(z|x) the conditional distribution of the hidden variable or the stochastic representation z or something else. Re-defining the concept of a prior as a 'conditional prior' could be quite incompatible with some already very established terminology about what it means to be a 'prior'. I very much understand the motivations here; however, I feel the terminology should be more precise in a publication.\n\nPlease give some more details on the baseline CNN, details can be important to understand if this is a strong or straw-man baseline. There are also a number of minor language errors throughout the manuscript that need to be fixed.  This paper explores a natural variation of stochastic conditional models, presented here as a conditional variational auto-encoder. The essence of the idea here is reasonable, and the experimental work is quite extensive. However, some terminology used in the paper stretches the accepted interpretations of certain concepts a little too far (see more detailed comments for specifics).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper describes a generative model for structured output variables by extending the variational auto-encoder to a conditional model . The authors propose a few different models and demonstrate on a pixel labelling tasks on toy MNIST data as well as real world CUB and LFW data, showing good results.\n\nAside from a few typos the paper is well written and clear. It is however slightly unclear how a final pixel labelling is produced, so perhaps a quick explanation illustrating the use of section 3.1 for pixel labelling would be beneficial to readers. There are some very related missing references: [Deep Structured Output Learning for Unconstrained Text Recognition, ICLR 2015] and [Learning Deep Structured Models, ICLR 2015]. Interesting paper on conditional variational autoencoder framework for pixel labelling. Other structured prediction tasks would have been interesting to see as well as a clear expose of the inference process.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
