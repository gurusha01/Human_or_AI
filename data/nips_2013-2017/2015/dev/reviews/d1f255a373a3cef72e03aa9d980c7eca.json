{"title": "Online F-Measure Optimization", "abstract": "The F-measure is an important and commonly used performance metric for binary prediction tasks. By combining precision and recall into a single score, it avoids disadvantages of simple metrics like the error rate, especially in cases of imbalanced class distributions. The problem of optimizing the F-measure, that is, of developing learning algorithms that perform optimally in the sense of this measure, has recently been tackled by several authors. In this paper, we study the problem of F-measure maximization in the setting of online learning. We propose an efficient online algorithm and provide a formal analysis of its convergence properties. Moreover, first experimental results are presented, showing that our method performs well in practice.", "id": "d1f255a373a3cef72e03aa9d980c7eca", "authors": ["R\u00f3bert Busa-Fekete", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Krzysztof Dembczynski", "Eyke H\u00fcllermeier"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper proposes an algorithm that can optimize the F-Measure in an online fashion. To do that, it relies on an online learner to estimate the posterior which is utilized at each updating. The algorithm has been proved theoretically to be able to converge to the optimal threshold computed based on batch learning. Empirical evaluation shows consistent results. The experimental results on sensitivity towards three different online learners are shown. The paper is clearly presented and organized. Also, the proposed algorithm can be of practical use. This paper proposes an algorithm that can optimize the F-Measure in an online fashion. The effectiveness of the proposed algorithm has been evaluated both theoretically and empirically with promising results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The problem of learning binary classifiers optimizing non-decomposable performance metrics has recently gained a lot of traction; this paper considers the popular F-measure specifically, and proposes an algorithm for optimizing F-measure in the online learning setting. The authors give a formal consistency analysis for the online algorithm. Experimental results on fairly large real-world datasets are presented.\n\nThe main idea in the paper -- and the online algorithm itself -- is neat and simple. It is easy to understand and more or less easy to implement (except for the caveat of estimating a model for the class-conditional probabilities in each round, which can be quite tricky). The consistency analysis is also fairly novel. Though, I'm not quite sure if the way the guarantee is presented in Theorem 3 (and 5) is precise. Perhaps it would be clearer to make the regret, which in this case is F((y_1, ... , y_t), (\\hat{y}_1,...,\\hat{y}_t)) - F(\\tau^*)), explicit in the statement (which the authors do mention later in Section 6).\n\nRecently, [9] introduced a framework for optimizing & analyzing non-decomposable measures in the online setting. It is not quite convincing from the discussion in Section 6, how exactly the analysis of OFO is stronger than that in [9]. In fact, the framework in [9] based on a certain per-round non-decomposable loss is quite intuitive. In particular, [9] also includes convergence rates for regret. It is also surprising that the method in [9] is not compared to in experiments (at least a justification is needed otherwise).\n\nThe other concern is with respect to the organization of the paper itself. The main idea/contribution which is Algorithm 1 and its analysis appear much later in the paper. Sections 2 and 3 are redundant for the most part. In particular, I fail to see the point of Section 3 in the context of this paper and its key contribution --- at best, it can be summarized in a Background section that combines the key ideas in Sections 2 and 3. This way, the readers can get to Section 4 (& 5), which is the focus of this paper, without distractions (and possibly confusions). The authors even had to move some useful results on Synthetic data to the Appendix -- I suggest including this in the main paper.\n\nResponses are satisfactory. Raising my score to Accept.\n\n  The paper considers the problem of optimizing F-measure in the online setting. An online algorithm, that is easy to understand and implement, is proposed and its consistency is analyzed. Experimental results showing conformance to theory is presented. The paper does make sufficient contributions, but I've some concerns with evaluation and presentation/development of content.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Quality: - This paper is technically sound and the supplemental file contains the detailed proofs and extensive experiments.\n\nThe discussion in Session 6 is interesting\n\nand insightful, especially the argument that F-measure itself is an aggregate measure which thus makes no sense to analyze the regret bound in general online learning.\n\n - In line 196-197, although $\\hat{h}(\\tau)$ is an unbiased estimate of $h(\\tau)$, what is the variance of $\\hat{h}(\\tau)$ w.r.t. the data distribution? Alternatively, it is suggested to add some discussions on other estimated functions of $h(\\tau)$. - Moreover, in line 268-269, it is stated that the noisy estimation of $h(.)$ depends on the labels. How can the decomposition step help to handle this undesired effect?\n\n- In line 286, it is better to clarify the assumptions that are required on the online learner.\n\n Clarity: - This paper is well written and clearly organized, although there are still some minor issues.\n\nThe overall idea of this paper is easy to follow. - For example, the second \"-\" sign in Eq.(5) should be a \"+\" sign.\n\n - There seems to be an error in the most right hand side of Eq.(1).\n\n- In line 155, what does \"P\" above the arrow denote? It should be clarified first before its usage. - In line 65-66, \"positive a negative\" should be \" positive and negative\". - In line 7 of the pseudo-code, it is better to add some notes that $a_t$ and $b_t$ can be calculated by Eq.(9). - In line 251-252, what do $F_t$-measurable and filtration mean? It is better to add more explanation on them. - In line 296, \" conference rate\" may be \" convergence rate\".\n\nOriginality: In this paper, the authors study the online optimization problem of a non-decomposable metric, the F-score, which is less explored in the community. The reviewer thinks the idea in this paper is novel.\n\nSignificance: Since there are still several nice properties of the four fundamental quantities (true/false positive/negative rate), more exploration based on this paper is expected. Moreover, since F-score function is neither convex nor concave and has the decomposition nature, there are still more challenging open problems on F-measure optimization. The reviewer thinks this paper can raise further research interest in the online learning community. In this paper, the authors propose an online algorithm to threshold a given classifier for the purpose of F-measure maximization and prove the convergence of the online F-scores obtained by the proposed algorithm to the optimal F-score.Extensive experiments are conducted to verify its convergence properties in the theoretical analysis.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper studies the problem of training classifiers that maximize the F-measure in the online setting, which is defined as the ratio of two linear functions and thus, unlike the traditional loss functions, is non-decomposable. Most existing algorithmic approaches of optimizing F-measure are in the batch setting and solve the problem by finding the optimal threshold of probabilistic classifiers on a validation set. This paper proposes an online learning algorithm to find the optimal threshold and proof consistency results under various assumptions. This authors also include comprehensive experiments (including the appendix) to demonstrate the performance of the proposed method.\n\n Generally, the paper is well written and I like the presentation of the proof the Theorem 3. The theoretical results seem to be solid and experimental section is fairly convincing.\n\n This is not my area of expertise, so I'm less certain about the novelty of the technical contribution comparing to the existing literature on F-measure maximization. That been said, I like that this paper has a balanced combination of theory/algorithm/experiments.\n\n Minor comments:\n\n In the Discussion section (Section 6), it would be nice to expand the comparisons with [9]. In terms of AUC maximization in the online setting, there are two relevant papers on the analysis of the generalization performance.\n\n 1. Wang et al., Generalization bounds for online learning algorithms with pairwise loss functions, COLT 2012\n\n 2. Kar et al., On the generalization ability of online learning algorithms for pairwise loss functions, ICML 2013\n\n Line 454: extra space in \"consistency\"\n\n  This paper proposes an online learning algorithm to optimize the F-measure with a nice combination of theory, algorithms and experiments.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
