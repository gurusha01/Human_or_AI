{"title": "Bayesian Active Model Selection with an Application to Automated Audiometry", "abstract": "We introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application. Although our method can work with arbitrary models, we focus on actively learning the appropriate structure for Gaussian process (GP) models with arbitrary observation likelihoods. We then apply this framework to rapid screening for noise-induced hearing loss (NIHL), a widespread and preventible disability, if diagnosed early. We construct a GP model for pure-tone audiometric responses of patients with NIHL. Using this and a previously published model for healthy responses, the proposed method is shown to be capable of diagnosing the presence or absence of NIHL with drastically fewer samples than existing approaches. Further, the method is extremely fast and enables the diagnosis to be performed in real time.", "id": "d9731321ef4e063ebbee79298fa36f56", "authors": ["Jacob Gardner", "Gustavo Malkomes", "Roman Garnett", "Kilian Q. Weinberger", "Dennis Barbour", "John P. Cunningham"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "# Summary\n\nThe authors introduce a new method for actively selecting the model that best fits a dataset. Contrary to active learning, where the next learning point is chosen to get a better estimate of the model hyperparameters, this methods selects the next point to better distinguish between a set of models. Similar active model selection techniques exist, but they need to retrain each model for each new data point to evaluate. The strength of the author's method is that is only requires to evaluate the predictive distributions of models, without retraining.\n\nThey propose to apply this method to detect noise-induced hearing loss. The traditional way of screening for NIHL involves testing a wide range of intensities and frequencies, which is time consuming. The authors show that with their method, the number of tests to be run could be drastically decreased, reducing the cost of large-scale screenings for NIHL.\n\n# Quality\n\nThe paper is technically sound. Although not detailed, the derivations make sense. However, there is no theoretical justification or discussion, and the empirical evaluation is weak. The authors only compare their active model selection scheme with a \"traditional\" active learning algorithm, and a random baseline, on simulated data generated from their model (possibly biasing the results). Ideally, it should also be evaluated on real data with true examples of NIHL (not generated from the model), and also compared with other active model selection methods. Moreover, some key issues, such as defining the set of candidate points or candidate models, are not discussed at all.\n\n# Clarity\n\nThe paper is very well written. The structure is clear, and the relevance and usefulness of the proposed method are well introduced. The application is particularly well explained. A pleasure to read!\n\n# Originality\n\nAs stated by the authors, the problem of active model selection is less well-studied than that of active learning. The proposed method is a variation of exiting ones (maximizing mutual information instead of expected cross entropy, for example), that relies on various existing approximations for its implementation. Its application is also original, and not well studied.\n\n# Significance\n\nThe method gives a significant complexity improvement over existing ones, as it does not require retraining the models for each new candidate. It enables real-time application of the method, which is key to some problems. It also allows to reduce significantly the number of testing points required for NIHL screening, which would have a significant impact on large-scale screening, reducing costs.\n\n# Pros\n\n* Well written and clear * New method for active model selection, that does not require retraining models * Significant gains (fewer samples required) for an important and practical application\n\n# Cons\n\n* No theoretical analysis/discussion of the new method * Weak empirical evaluation * No comparison with other active model selection techniques * Evaluation on data simulated from their own model, thus possibly biasing results * No discussion of the problem of generating the set of candidate points or models The novel active model selection approach the authors propose seems promising, and the application useful. However, the evaluation is not convincing, and their is no discussion of some key issues like selecting the candidate locations and models.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a method for active model selection using a mutual information criterion. The main technical contribution is a way to approximate the MI criterion without having to re-fit the model for each candidate. The method is evaluated on the task of detecting noise-induced hearing loss with as few sound queries as possible.\n\nOverall, this paper presents a promising and well thought out approach to the active model selection problem. The paper is readable and motivates the design choices well. The NIHL detection task is an interesting use of active model selection, and its introduction is a useful contribution of the paper.\n\nOne thing missing from the paper is experimental comparisons against prior approaches to active model selection, either on the NIHL task or on benchmarks where other methods have been successfully applied. In the experiment, the baseline [8] shows a pathology (focusing on fitting the more likely model) which other active model selection methods would presumably correct, so it would be interesting to see how the different approaches compare.\n\nThe NIHL task is a bit limited in that the positive examples are simulated from the model, which could lead model based methods to perform unrealistically well. This may be inevitable in the medical domain, though, and I don't immediately see a better way to set up the experiment.\n\n Overall, I would recommend acceptance because the paper is well written, the methods seem novel and well motivated, and the paper introduces a real-world medical task which may benefit from the proposed approach. The main thing missing is comparisons to other active model selection approaches. I'm not an expert on active learning, so I can't speak with confidence about the relationship to prior work.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I had a couple of main comments/questions.\n\nI would have liked to seen empirical evaluation of the approach on data from NIHL subjects (rather than on modified normal hearing data) for two main reasons. First, it would have been especially useful to see empirical justification for the NIHL mean function and the priors on the various hyper parameters which presumably are key to getting the method to perform sensibly e.g. so a NIHL notch is not explained by the SE fluctuations in a normal audiogram. Second, I have slight concerns about the robustness of the method and exposing it to real data would be a sensible test.\n\n Is model comparison is really the most sensible approach to diagnosis -- doesn't everyone have some degree of noise induced hearing loss and so shouldn't diagnosis correspond to identifying where on a spectrum the individual is? Would it then not be more sensible/sufficient to do inference in the NIHL model, define the mutual information gain in terms of the parameter 'd', and base diagnosis on the magnitude of this inferred parameter, rather than using a discrete mixture model?  I like the paper and vote of acceptance; the application of active learning approaches to automated audiometry is very sensible. The paper is well written, clear and the model choices and approximations well justified.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This work introduces a new criterion for sequential experimental design with the goal of model selection. The author(s) applies this algorithm to an audiometric experiment consisting of detecting noise-induced hearing loss. The experiment produces good results.\n\nQuality and clarity: This paper is very clear and very well written. It nicely outlines the steps necessary in approximating model evidence, hyperparameter posterior and predictive distributions. Minor points: on line 134, X* is mentioned before it is defined (actually, I don't believe it is ever defined) and on line 342, \"exes\" -> \"crosses.\"\n\nOriginality: The author(s)'s work applies a trick similar to that of [12], using the symmetry of mutual information to turn an intractable quantity into a one-dimensional intractable quantity that can be approximated relatively easy. They apply this technique to the problem of active model selection, which is novel.\n\nSignificance: The problem of model selection is obviously an important one. The particular application discussed in the paper of selecting among different GP priors is a prime example. Unfortunately, the example provided was simple and included only selecting among prior means and not the covariance kernel, which is often the more difficult to choose. In addition, the first model (healthy) is included in the NIHL model (as d -> 0). Therefore it begs the question of whether one could simply use the more complex model and infer the hyperparameters, and then examine the magnitude of d and/or w. Though I do not doubt the potential applications of this method, I do question how convincing this particular one is.\n\nMinor points: - Section 3: it is worth at least mentioning the hyperparameter marginalization via MCMC technique around equation (7). This is a very nice paper about a Bayesian method for active model selection. The techniques used in the method are explained at a very appropriate level of detail with useful references for further reading. Though compelling, in my opinion, the application presented failed to illustrate the strength of the method due to concerns I raise under the \"significance\" heading below.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
