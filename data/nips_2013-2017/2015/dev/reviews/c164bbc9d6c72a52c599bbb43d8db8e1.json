{"title": "A Universal Catalyst for First-Order Optimization", "abstract": "We introduce a generic scheme for accelerating first-order optimization methods in the sense of Nesterov, which builds upon a new analysis of the accelerated proximal point algorithm. Our approach consists of minimizing a convex objective by approximately solving a sequence of well-chosen auxiliary problems, leading to faster convergence. This strategy applies to a large class of algorithms, including gradient descent, block coordinate descent, SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants.  For all of these methods, we provide acceleration and explicit support for non-strongly convex objectives. In addition to theoretical speed-up, we also show that acceleration is useful in practice, especially for ill-conditioned problems where we measure significant improvements.", "id": "c164bbc9d6c72a52c599bbb43d8db8e1", "authors": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper definitely contains some new contributions from a technical point of view. However, the main idea and the theoretical results are not mature enough. Indeed, Algorithm 1 can be viewed as a simple inexact variant of Guler's fast proximal method in [9], where the authors impose the condition (5). This approach has several limitations including the choice of \\kappa, the solution of the subproblem in (5).\n\n The authors provide two convergence theorems for both the strongly convex case and the nonstrongly case. Unfortunately, the choice of the inner accuracy \\epsilon_k depends on the optimal value F* which is unknown. In addition, the method for solving (5) is elusive and its convergence guarantee is given by \\tildle{O} notion, which does not precisely reflect the real number of iterations of the inner loop. Hence, the overall complexity may be worse than existing methods. In addition, the convergence guarantees in (7) and (11) remain depending on F(x^0) - F*, which is different from existing methods.\n\nAs far as I observed, the authors fix kappa in Algorithm 1, but this is not a good idea since in the accelerated scheme, the error of the inexact oracle\n\naccumulates. Indeed, the closer we are to the true solution the more exact solution of the subproblem we require. Hence, adaptively update kappa is crucial.\n\n Clearly, extending Algorithm 1 from noncomposite to composite form has in issues, especially when the regularizer does not have a low cost proximal operator. The inner loop requires such an operator at each iteration.\n\n Overall, even though the paper contains some new technical contributions, but they are not sufficiently strong. In addition, the convergence analysis has not done rigorously. I think to overcome this limitation, one can exploit the idea of sliding gradient/conditional gradient methods from G. Lan to characterize the convergence of Algorithm 1. This paper proposes a so-called catalyst algorithm for unconstrained convex optimization. In fact, this algorithm can be viewed as an inexact variant of fast proximal method introduced by Guler in [9]. The authors analyze the convergence of their algorithm for two cases: strongly convex and nonstrongly convex. Several extensions and modifications are mentioned but are not rigorously backed up. A few numerical experiments are given to show advantages of their method.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "I think this is a very interesting and significant piece of work, with several relevant and non-trivial contributions. Not only does the paper introduce a generic scheme to accelerate several optimization methods (namely several recent incremental methods), but it also extends the domain of applicability of some of those methods. The paper is very well written and the contributions are carefully put in the context of the state of the art. The experimental results, although naturally limited by the scarcity of space, are clear and convincing. As far as this reviewer knows, the results are original.\n\nIt has been recently shown that the convergence rate of Nesterov-type acceleration methods can be significantly boosted by using adaptive restart techniques. See for example, the paper \"Adaptive Restart for Accelerated Gradient Schemes\", by O'Donoghue and Candes, and the paper \"Monotonicity and Restart in Fast Gradient Methods\", by Giselsson and Boyd. It would be interesting to consider if the method proposed in this manuscript could also benefit from some form of restarting scheme, given that it also uses Nesterov-type extrapolation.\n\n I think this is a very interesting and relevant piece of work, which contains several important and non-trivial contributions (see details below).", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "As mentioned in the paper, the algorithm in the paper is basically the same as in [9]. It combines extrapolation idea from Nesterov's acceleration scheme with proximal point operator. The algorithm in [9] used exact minimization in each iteration, which is not possible in practice. The current paper carries out the analysis with more realistic assumptions of approximate minimization. Paper [8] is quite similar in the above respects. It's stated that the current paper is independent of [8], but given that [8] has already been published, in my view the current paper should be considered only on the basis of what it has in addition to the results that overlap with [8]. There is a detailed comparison of the results here and in [8] in the appendix, but I would have also liked to see a comparison of the methods. Thus one of the main new result in the current paper is that it can also handle functions that are not strongly convex. But it seems to me---but I could be wrong as I didn't actually read the proofs---that the methods of proof are similar for the strongly convex and general convex cases. So I guess the question to answer is: Does the method of [8] also easily give the case where the function is not required to be strongly convex or is a new idea needed?\n\n The other additional result is the acceleration and other improvements of Finito/MISO. The modified algorithm here turns out to be very similar to a variant of SDCA [24], though there are differences. Given this, the experimental results should have included a comparison with SDCA, and I consider this a major omission.  The paper provides an analysis of a generic acceleration method for first-order optimization based on Nesterov's extrapolation idea and proximal point operator.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper essentially extends Neterov's first acceleration technique for proximal point algorithms, considering inexact solutions of subproblems. Nontrivial modifications to exiting analyses were made to allow for controlled inexactness, so that global convergence rate can be derived depending on it.\n\nThe paper presents technical ideas with clarity and high quality. As far as I know, this work is original, which brings a new unified framework and understanding for accelerating first order methods with inexact inner solvers.\n\n Personally, it was a bit disappointing to see in Thm 3.3 that the amount of inexactness has to decrease in a rapid rate as global iterations go on, but it is also expected.\n\n  This paper provides a unified acceleration framework and analysis for first-order optimization methods, considering carefully of inexactness of solving subproblems and showing its effect on convergence rate. Although I didn't check all the proofs, the theory looks solid and provides deeper understanding of the acclaimed acceleration techniques.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose a general scheme of wrapping an existing first order gradient method to solve the minimization of the original objective plus an iteratively sharper proximal term. The authors show that for a generic first order method this 'accelerates' the convergence by changes the depends on \\frac{L}{\\mu} to \\sqrt{\\frac{L}{\\mu}}.\n\nOverall, it is a goodpaper with a couple deficiencies in\n\n(a) experiments and\n\n(b) given the existing algorithms, when exactly would a catalyst be useful ? (especially in ML).\n\n Despite the deficiencies, I still vote for acceptance as it has some ideas which could spur future research, for e.g. the idea of doing multiple passes over data with each pass having a sharper proximal term (similar to [8])\n\nComments\n\nPage 2, line 57 - are the authors sure that 'all' the incremental methods have a O(n) cost ? Including ProxSVRG ?\n\nThe authors need to make it clear when their contributions are useful. For e.g., - when users cannot use the dual, and - ideally when the objective is not strongly convex (saying that n < L/mu seems, in my opinion, is a little weak and less useful regime)\n\n- and when data cannot be held in memory (as the inner algorithm can make a sequential passes over data).\n\nline 314, redundant 'do'\n\nline 390 - what does 'default' parameters ? default as in optimal w.r.t theory ? or just happen to be parameters in some source code ? The authors of SAG also outlined a procedure to do learning rate scheduling - was this taken into account.\n\nWas 'L' just set to the upperbound 'L' in the dataset ? was there any effort to tune this ?\n\nHow was \\eta set ? Was there any effort taken to ensure that enough tuning was performed on all the algorithms ? or was it the case that the authors picked a set of values and lucked out by getting a better performance on the proposed scheme ?\n\nOne of the benefits of using a method like SAG/SAGA/SVRG/.. is to ensure quick initial convergence which is not guaranteed by FG or Acc-FG. It would be nice to see how the proposed method does w.r.t Acc-FG.\n\nSuggestions\n\nTheorem 3.3. is misleading at a first glance, could the authors please ensure the contents of line line 256-257 are located in a more accessible place ? like the theorem statement itself.\n\nTable 1 caption \" To simplify, we only present the case where n <= L/\\mu when \\mu > 0.\" could be boldened to ensure that it is read  The authors propose a general scheme of wrapping an existing first order gradient method to solve the minimization of the original objective plus an iteratively sharper proximal term. The authors show that for a generic first order method this 'accelerates' the convergence by changes the depends on \\frac{L}{\\mu} to \\sqrt{\\frac{L}{\\mu}}. Despite multiple shortcomings, I think this work passes the bar for acceptance as it has interesting ideas that could spur future research.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
