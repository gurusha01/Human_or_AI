{"title": "Color Constancy by Learning to Predict Chromaticity from Luminance", "abstract": "Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes---without any spatial or semantic context---can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a classifier for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminance-to-chromaticity classifier end-to-end. Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.", "id": "9778d5d219c5080b9a6a17bef029331c", "authors": ["Ayan Chakrabarti"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Quality:\n\nThis is a very nice paper - an elegant approach to a long studied problem. The methodology is clear, its motivations are well founded and experiments are well performed. There is a good amount of analysis (right most columns of Figure 2 are especially interesting) but I would want a bit more analysis for the relationship between the variance of estimators and the error at specific locations. Though I would expect these to be highly correlated, it does not seem that way in the figures.\n\nClarity:\n\nThe paper is very clearly written, easy to follow and uses notation well. Minor comments about this - I would use bold face characters for all properties which are vectors (such as chromaticity, pixels etc.). This would make the distinction between scalars and vectors easier. Additionally, some important equations are inline - line 158, 216 and line 237 certainly deserve their own numbered equation (or at least a clearer separation). In line 308, \"it's\" should be \"its\".\n\nOriginality:\n\nAn original approach to an old problem.\n\nSignificance:\n\nAn old and well studied problem which is interesting for a large part of the vision community. I'm not sure NIPS is an ideal audience for this work, but in terms of quality, it is certainly suitable. This paper proposes a method for color constancy in image by learning about the conditional chromaticity distribution, conditioned on pixel luminance. This is done by simply modeling the empirical histograms in a training set, as well as globally optimizing the histograms over the training set using gradient descent on a cost function. Surprisingly enough, the luminance allows for informative predictions for the illumination chromaticity (up to the usual scaling constants) even when pixels are treated as independent. The method is quite fast to compute and results are pleasing as well as numerically impressive.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This is a well conducted study, with impressive results, summarized in an overall well written paper.\n\n [... Part about readability of the paper; Not relevant anymore; Edited out ...]\n\nI hope that the authors make their code available online should the paper be accepted.\n\n The authors describe an illuminant estimation method based on identifying the true cromaticity of a pixel from its luminance using the fact that natural illuminant chromaticities only take on a restricted set of values because of Planck's radiation law. They estimate a luminance-to-chromaticity classifier from a training set of natural images and use that in inference to obtain a distribution over illuminants that are consistent with the observed pixels. A global estimate of the scene illuminant is computed by aggregating the distributions over illuminants over all pixels.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The algorithm presented here is simple and interesting. Pixel luminance, chrominance, and illumination chrominance are all histogrammed, and then evaluation is simply each pixel's luminance voting on each pixel's true chrominance for each of the \"memorized\" illuminations. The model can be trained generative by simply counting pixels in the training set, or can be trained end-to-end for a slight performance boost. This algorithm's simplicity and speed are appealing, and additionally it seems like it may be a useful building block for a more sophisticated spatially-varying illumination model.\n\nI found the paper reasonable easy to read, though much of the math could use a revision to make things more clear and hopefully reduce the (unusually large) number of symbols which are defined for what seems to be a fairly simple algorithm. For example, why define x-hat and g()? is v(n) == v_n? Is { g(v(n), m-hat_i}_i really the easiest way to refer to that set --- and is that math even correct, given that i is used as a subscript twice? Perhaps some figures which correspond to the math would be useful, as the only variable which appears in a figure is L[].\n\nThe evaluation shows that this technique produces state-of-the-art results on the standard color checker dataset. The evaluation contains one component which I take issue with. When comparing the authors results to the results published in [19], we see that the authors very slightly outperform [19]. However, in this paper the authors have presented a modified set of error metrics, which in the supplement they explain are derived from the metrics in [7,19] subject to a correction, with the claim that this correction undoes an unfair advantage that the authors of [7,19] have achieved by using incorrectly black-leveled images. It may be true that [7,19] are evaluated incorrectly, but such an allegation should be taken very seriously, and should not be dealt with in this way. This number (as I understand it) assumes that the models of [7,19] were trained on the incorrect dataset and then evaluated on the correct dataset, which will obviously increase error metrics. But this does not mean that these numbers are indicative of how [7,19] would perform if trained *and* tested on the correct dataset. Indeed, given that incorrect black-level correction can completely ruin a color constancy algorithm, I would not be surprised in [7,19] actually performs better if trained and tested correctly. So I think the correction the authors present here is not valid.\n\nThough it is unfortunate that [7,19] contains incorrect numbers which may be cited, we should not combat such innacuracies with other innacuracies. If published, the number presented here may be taken as truthful by future work, when in fact it is even more speculative and unreliable as the numbers presented in [7,19]. So I think the authors should not present their modified error numbers. The error metrics presented here should be the same as in [7,19], but perhaps indicated with an asterix or with a grayed-out font color that they are likely not trustworthy, with an explanation of why in the supplement. Then the onus will be on [7,19] to revise their results, and we wont have two contradictory (and equally incorrect) numbers for the performance of [7,19] circulating in publications. I don't suggest this to diminish the author's work, as even if [7,19] are taken on face value the work presented here outperforms it. This is simply a matter of scientific honesty and ettiquette that we should handle as carefully as possible. The authors present an interesting color constancy algorithm which appears to outperform the state of the art on the standard color constancy dataset by a reasonable margin. The paper could be more clear and the evaluation has some issues, but otherwise the paper seems above-board.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The distribution of light reflected from a surface in a scene a function of the geometry, scene illuminance and the surface reflectance (true chromaticity). The problem of color constancy, is to estimate the surface reflectance from the observed scene, typically without being given either the geometry or scene illuminance information. This is of interest as a task that humans are relatively good at, and of importance for artificial vision systems to recognize and identify materials under variable lighting conditions.\n\nThe authors introduce a method for estimating the scene illuminance of an image (which then allows recovering the true chromaticity of all pixels). The method assumes a single, global luminance in the scene and uses a function L[x, y] encoding the belief that a pixel with an observed reflectance y has a true chromaticity x. The method also makes use of a prior on scene illuminances and an assumption of a single illuminance for the whole scene. Using a dataset containing calibrated images, they can learn L either by treating each pixel independently, or by directly minimizing the error in illuminance estimation on their training set with gradient descent. They show that their approach is generally superior to existing color constancy methods.\n\nThe manuscript is well-written and the authors compare their approach with a number of existing approaches. The method presented is pleasingly simple, makes reasonable assumptions and performs well. It is notable that this method makes no use of spatial structure in the scene and yet is able to outperform alternative approaches which do.\n\nThe approach used bears some similarities to the Bayesian estimation method used in reference 14 (the source of the image database). Although the author's compare against [14] in table 1, it would be helpful to include [14] in the discussion of existing methods on page 2.\n\nFor training of the end-to-end model the dataset is augmented by \"re-lighting\" each image with different illuminants to create 6 training images and a 7th validation image. It is unclear which hyper-parameters were chosen using this validation set. Importantly, I presume the images used for testing accuracy (table 1) are distinct (not just relit) from any used in training or validation (if not, this would be a serious flaw). This should be clarified briefly in the manuscript.\n\nA weakness of this manuscript is that it is primarily of interest only for the machine vision community. The approach used is (from a machine learning point of view) fairly straightforward and the primary contribution is specific to the problem of color constancy. End-to-end approach to the color constancy problem. Mostly of interest to the vision community.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
