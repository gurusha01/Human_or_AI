{"title": "Inference for determinantal point processes without spectral knowledge", "abstract": "", "id": "2f25f6e326adb93c5787175dda209ab6", "authors": ["R\u00e9mi Bardenet", "Michalis Titsias RC AUEB"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Determinantal point processes (DPPs) have been receiving a significant amount of recent attention in the machine learning literature, due to their ability to model repulsion within realizations of a set of points.\n\nThis repulsion is specified via a positive definite kernel function.\n\nOne challenge, however, is learning the appropriate parameters of this kernel, given example point realizations.\n\nThis estimation is difficult due to a difficult-to-compute normalization constant that, in effect, sums over all possible cardinalities and point configurations.\n\nA recent proposal [7] has provided a way to do this by observing that the spectrum can be estimated and used to provide incremental upper and lower bounds, resulting in a provably-correct scheme for \"retrospective\" Markov chain Monte Carlo (MCMC) of the kernel parameters.\n\nThe present paper proposes a different type of bound that can also be used for MCMC inference, but that does not require estimation of the eigenvalues.\n\nThis approach also lends itself to a variational approach to learning, and the paper draws connections to inducing-point methods to scalable inference in Gaussian process models.\n\nThis is a technically strong and well-written paper.\n\nI was not aware of the inequality that provides the lower bound in Proposition 1 and it seems like an excellent fit for this problem.\n\nIt is a clever way to avoid the estimation issues of [7].\n\nI love the connections to GP inducing points, as well as the ability to now perform variational inference.\n\nOverall this paper was a pleasure to read.\n\nI have two technical concerns that I would like to see addressed, however.\n\nFirst, although I like this approach very much, I do not find it all that compelling that it is a huge computational win to replace power iterations with nonlinear optimization problems of increasing dimension.\n\nSecond, it is not obvious to me that simple increasing of the cardinality is guaranteed to result in convergence of the bounds, because of the coupled optimization problem.\n\nAlthough I am not certain, I believe this optimization is highly non-convex and so one needs to increase cardinality and also ensure that a global minimum is achieved.\n\nThis is something that should be addressed directly, because it seems that a local minimum could prevent the procedure from being able to bound away the Bernoulli threshold and make it impossible to take an MCMC step.\n\nPresentation Issues:\n\n - P7 Figure 1: Please include axis labels for the figures.\n\n - The proof of the proposition is the main thing that makes this\n\n paper possible and is the central insight.\n\nI would've liked to see\n\n it in the main body of the paper and not the index.\n\n- P7 L361: \"... the variational lower is ...\"  A nice paper that uses a clever trick to bound the partition function of a determinantal point process, leading to potentially faster inference and connections to other kinds of models.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors propose novel upper and lower bounds on the matrix determinant, with applications to bounding the likelihood of a determinantal point process. The main idea is simple, but the paper is well executed and enlightening, and paves the way for several potential improvements.\n\nThe authors propose a creative use of the pseudo-inputs to bound functionals of a positive definite kernels. The proposed problem is timely, and will be of interest to several in the community.\n\nSuch an approach is quite general with potential applications beyond determinantal point processes. The experimental evaluation is well constructed to elucidate several key points, including failures of a variational approximation under certain conditions. I found the selected application area interesting and highly relevant to the studied problem.\n\n Minor comments:\n\n- It is unclear why the issue of parameterizing K (instead of L) features so prominently in the abstract and the rest of the paper, as the authors mainly focus on describing the problem, and make no efforts to address it.\n\n - If accepted, I suggest that the authors prepare a standard supplement, and not just upload an extended version of the paper as the supplement.\n\nSuggestions for future work:\n\n- On the other hand, the discussion of the interpretability of the K parameterization makes such an approach even more appealing. Could the authors consider bounding the determinant directly in terms of the K matrix? What is the scalability of such an approach if using the pseudo-input method? The authors propose novel upper and lower bounds on the matrix determinant, with applications to bounding the likelihood of a determinantal point process. The main idea is simple, but the paper is well executed and enlightening, and paves the way for several potential improvements.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes approximate inference techniques for determinantal point process (DPP) that do not require spectral knowledge of the kernel defining the DPP. Instead, inducing inputs are utilized to bound the likelihood, which reduces the computational cost. Both variational inference and approximate Markov chain Monte Carlo (MCMC) techniques are described and their efficiency is demonstrated empirically.\n\n I believe there is merit to this paper, however I found the method description rather rough and empirical evaluation not thorough enough. A few points that may help clarify:\n\n 1) How does the computational complexity and overall compute time of the two proposed methods compare?\n\n2) How do the proposed methods compare to other alternatives, both approximate and exact, in terms of quality of approximation and computation time?\n\n3) Empirical results on the toy data in Section 5.1 show that parameter estimation error using variational inference is too large, and resorts to approximate MCMC instead. In what settings does the proposed variational inference expected to work and when is it expected to fail? Why?\n\n4) A 13-point sample is generated for the toy example. How do the methods perform when the inducing inputs are actually these sampled points?\n\n5) Is there a rule of thumb to follow when selecting a) the number of inducing points, b) placing the initial inducing points?\n\nMinor comments:\n\nThe matrices K and L are referred to throughout the introduction without explaining what they are. Briefly mentioning the marginal kernel and L-ensembles may make it easier to follow for an uninformed reader.\n\n There is discussion about the advantages of directly learning the marginal kernel parameters throughout the paper, but this is not the approach taken. I believe some of it could be cut back since it does not contribute to the rest of the paper.\n\n  Inducing inputs are utilized to bound the DPP likelihood, which reduces the computational cost. Both variational inference and approximate Markov chain Monte Carlo (MCMC) techniques are described and their efficiency is demonstrated empirically.I believe there is merit to this paper, however I found the method description rather rough and empirical evaluation not thorough enough. If accepted, I strongly encourage the authors to improve the write-up.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
