{"title": "Alternating Minimization for Regression Problems with Vector-valued Outputs", "abstract": "In regression problems involving vector-valued outputs (or equivalently, multiple responses), it is well known that the maximum likelihood estimator (MLE), which takes noise covariance structure into account, can be significantly more accurate than the ordinary least squares (OLS) estimator. However, existing  literature compares OLS and MLE in terms of their asymptotic, not finite sample, guarantees. More crucially, computing the MLE in general requires solving a non-convex optimization problem and is not known to be efficiently solvable. We provide finite sample upper and lower bounds on the estimation error of OLS and MLE, in two popular models: a) Pooled model, b) Seemingly Unrelated Regression (SUR) model. We provide precise instances where the MLE is significantly more accurate than OLS. Furthermore, for both models, we show that the output of a computationally efficient alternating minimization procedure enjoys the same performance guarantee as MLE, up to universal constants. Finally, we show that for high-dimensional settings as well, the alternating minimization procedure leads to significantly more accurate solutions than the corresponding OLS solutions but with error bound that depends only logarithmically on the data dimensionality.", "id": "b1eec33c726a60554bc78518d5f9b32c", "authors": ["Prateek Jain", "Ambuj Tewari"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "SUMMARY:\n\nThis paper studies the effect of noise correlation in some models of multi-output regression. It argues that a method that does not benefit from the correlation, such as Ordinary Least Squares (OLS), may perform much worse than a method that does, such as Maximum Likelihood Estimation (MLE). For certain linear models (Pooled model and Seemingly Unrelated Regression), which are studied in the paper, the MLE estimator requires the joint optimization of the covariance and regression weights. This is a non-convex problem. Alternative Minimization (AltMin) algorithm is an approach to solve the problem by iteratively optimizing the covariance and the weights. This is not guaranteed to find the global optimum.\n\nThe contribution of the paper is a finite-sample study of AltMin and MLE and showing the effect of correlation. The results indicate that both MLE and AltMin can in fact benefit from the correlation, but OLS cannot. For certain examples, the difference can be significant. The paper provides lower bound for OLS and MLE (for the Pooled model) that up to a logarithmic factor in the number of samples is the same as the upper bound. This indicates that the upper bound of AltMin, which is the same as MLE's, is indeed tight.\n\nThe paper has some empirical studies too.\n\n EVALUATION:\n\nI believe this is a reasonably good paper. It provides intuition, has theory, and performs simple experiments that validate the theory. I have a few comments:\n\n- The eps in the experiments is very small, indicating highly correlated outputs. Would the difference between MLE/AltMin and OLS be still significant when eps is much larger (maybe in the order of 0.1-0.5)? The theory suggests that the behaviour is O(eps), but what about the effect of other constants, which might be different between OLS and AltMin/MLE?\n\n- The paper assumes that the noise model is Gaussian. How crucial is this for the conclusions of the paper? It appears that the only step that requires this Gaussian assumption is Lemma 11, which is Corollary 5.35 of [28]. It seems that one can safely use results of Section 5.3.2 of [28] to extend the results to beyond Gaussian noise models. Is this correct?\n\n- Some typos in the proof of Theorem 15 (p13):\n\n+ L650: The summation over i is missing in the RHS. + L656: zeta_1 is on top of the inequality symbol, but is never referred to. + L663: It seems that a log n term is missing after the equality.\n\n - The proof of Lemma 3, L710: The inequality indicated by zeta_1 holds with a probability at least 1 - exp(-cn), but it is not stated and taken care of.\n\n ===== UPDATE: Thank you. You addressed my concerns. I believe this is a reasonably good paper. It provides intuition, has theory, and performs simple experiments that validate the theory.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper considers the problem of multi-variable regression. The paper provides analysis of ordinary least squares and the MLE estimators for the problem and provides a characterization of when the MLE estimate is better than the OLS estimator. Further since the MLE estimator is solution to a non-convex optimization problem, for pooled model and the seemingly unrelated regression problems the paper gives finite sample analysis for alternating minimization method that can be efficiently implemented. For these problems matching upper and lower bounds are provided for OSL, MLE and upper bounds for alternative minimization approach. The analysis clearly\n\npoints out when the MLE/alternative minimization approach provides improvements for these problems.\n\nOverall the paper is well written and easy to follow. The proofs are straightforward and simple. As far as I checked the proofs seem fine. The analysis for MLE VS that for OSL seems straightforward. What really sells this work is that the theorems are proved for the AM based approach.\n\n However I do have a few concerns. 1.\n\nThe models considered are all well specified models but the applications listed as motivation are more machine learning type problems where well specified assumption is almost never true. The analysis is very heavily dependent on the well specifiedness.\n\n2. The significance of the results is not clear. On the experiments side, synthetic data is used. On the theory side, that alternative minimization (and MLE) yields better performance is clear but how significant is this in terms of applications is not clear. Can you provide concrete examples of applications where the gain is significant.\n\nSeems like for the difference between MLE and OSL to be large enough one needs the dimensionality to be large enough at the very least.\n\n3. Finally the results seem (although only a bit) out of scope for NIPS seems more classical stats econometrics type work.\n\nOverall I would like to see the paper accepted if there is space at NIPS. The results are certainly interesting but the significance of the results especially w.r.t. scope of NIPS is not clear.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper considers regression problem with multiple vector valued outputs, that are related either via sharing a common noise vector, or the same coefficient vectors, or some other attributes. This is an interesting theoretical and practical question.\n\n The paper considers two popular models for such problems. The first is the pooled model where the coefficient vector is shared across the various outcomes. The second is the SUR model which shares the noise vector across the dimensions. In both of these problems it is possible to simply ignore the additional information that can be extracted and perform the ordinary least squares regression (OLS). It can be shown that MLE outperforms this procedure in these settings. However, MLE requires solving a non-convex problem. Therefore, the authors study alternating minimization, and under these models show that its performance is within universal constant factors of MLE.\n\n 1. lines 190-200: Is the repeated use of fresh samples the reason for the additional logarithmic factor in theorem 1? It would be\n\ninteresting to see why the logarithmic factor pops up, since the lower bounds do not seem to have it.\n\n2. It would be interesting to apply these results to some real world data.\n\n3. The two models considered are in some sense restrictive. It would be nice if the authors can comment on\n\nwhat the reason for Alt-Min performing well under these models is. In particular, are there generic conditions under which Alt-Min can be shown to be within universal constant factors of MLE for regression problems?\n\n The paper shows that under two popular models for regression problems with vector valued outputs, Alternating Minimization can match the performance of MLE estimation up to universal constants. This is an interesting result.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
