{"title": "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees", "abstract": "Latent models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in [1] has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M-step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M-step using the state-of-the-art high-dimensional prescriptions (e.g., `a la [19]) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.", "id": "92977ae4d2ba21425a59afb269c2a14e", "authors": ["Xinyang Yi", "Constantine Caramanis"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This work addresses the local convergence properties of the EM algorithm in the high-dimensional setting where the number of dimensions is greater than the sample size. The paper builds on a recent work for the low-dimensional setting. The main novelty here is an iterative regularization scheme that shrinks the regularization parameter as the algorithm gets closer to the optimum. This allows the algorithm to converge at a linear rate in the local neighborhood of the optimum. There are several strong conditions on the objective function that the results rely on. As in most theoretical results on EM-type optimization, the algorithm also requires fresh samples at each step.\n\nAs this is a \"light\" review, I was unable to verify the details of this very technical contribution. If the results are correct, they represent a relevant and solid contribution to the theory of EM. The paper will primarily appeal to experts in this area.\n\n A solid (albeit very technical) contribution on the theory of EM for high-dimensional estimation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "A framework for regularized EM algorithms is proposed and analyzed. The motivation of the regularization is to allow for estimation in high dimensional settings under structural assumptions, where unconstrained estimation is statistically impossible and where ordinary EM may not even be well defined. Key is a regularized EM algorithm, where the ordinary M step is replaced by maximizing the Q function minus a regularizer on the model parameter, scaled by a regularization coefficient.\n\nThis is a refinement of previous work along similar lines. The novel contributions are as follows. First, the previous work only considered sparsity as the structural assumption, whereas the present paper allows for more general regularizers, which in particular apply to sparse and low rank structures. The assumptions on the statistical properties of the estimator of the Q function are changed in accordance to this more general regularization framework. Second, a specific choice of the regularization coefficient is given, which is updated at each iteration, and which properly controls the optimization error in relation to the statistical estimation error.\n\nIt would be helpful to provide some discussion or examples of the decomposability condition, the subspace compatibility constant, the norm ||_{R^*} (which is used in the key statistical condition 5), and the set C(S,\\bar S, R). This contribution generalizes and refines previous work on EM algorithms in high dimensions under structural assumptions. The novel results are significant for practical applicability of the theory.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "In the EM algorithm for high dimensional setting, the M step is problematic, which needs to be addressed by exploiting the structure of the high-dim paramters. The paper considers to use decompposable regularizers. The main contribution is a way to set the regularizer in the iterative process of EM. It is shown that this leads to local linear convergence, under conditions about the population and empirical estimation of log likelihood using current estimation of the parameters (the basic quantity in EM).\n\n The EM algorithm is used widely in machine learning and it is thus an important topic to identify conditions under which it leads to provable guarantees. The paper gives a priciple way to regularize the EM which is particularly interesting for the high dimensional setting. It further demonstrates the application of the theory to three well known models. The result is interesting and important.\n\n--In the abstract: the authors claim that regularzing the M-step using the state of the art prescriptions ([19]) is not guaranteed to provide the desired bound, but this needs more illustration.\n\n--Condition 5: as noted by the authors, this replaces the term that appears in previous work. It seems to me that this condition is the key to make the algorithm work in the high-dim region, while the analysis follows the previous work.\n\n minor: --line 80: Y Z should be bold face --Eqn (2.2): \"f\" is missing --line 122: | |_R* is not yet defined here and thus the sentence is a bit confusing. This paper studies the regularized EM algorithm. It proposes a way to set the regularizer, and identifies conditions under which the regularization leads to local linear convergence.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper introduces the difficulty of EM-algorithm in high-dimensional setting, where M-step can be unsolvable or unstable. Then regularization method is proposed, the key to which seems to be finding a good sequence of regularization coefficients to control the statistical error. Convergence is proved given condition on the initialization, regularization penalty and the likelihood. 3 latent variable models are then analyzed and some simulation result is provided.\n\nOverall it is a good paper which is well motivated with nice theory.\n\nA couple of thoughts: (1) more discussion about the effects of the initialization parameter $\\lambda_n^{(0)}$ and contractive factor $\\kappa$ can be helpful (2) the theory actually deals with algorithm 2, it might be good to clarify how the theory is linked to algorithm 1. It may also be interesting to show some simulation result for algorithm 2 (3) not sure about what figure 1 is trying to illustrate (probably that the algorithm does converge?). Also the font size for figures should be improved (4) some comparison (either in theory or in simulation) with competing methods (say [20]) can be helpful\n\nTypo: abstract (e.g. a la[19]) equation (2.2) y_{\\beta^*} should be f_{\\beta^*}; \\log_{\\beta'} should be \\log f_{\\beta'}  Good paper that proposes solving high-dimensional latent variable problems using regularized EM algorithm with varying regularization coefficients. Theory and examples are well explained but simulation part can be improved", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
