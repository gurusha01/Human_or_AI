{"title": "Large-scale probabilistic predictors with and without guarantees of validity", "abstract": "This paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.", "id": "a9a1d5317a33ae8cef33961c34144f84", "authors": ["Vladimir Vovk", "Ivan Petej", "Valentina Fedorova"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper tackles the very significant problem of converting arbitrary class membership scores into well-calibrated probabilities.\n\n It appears to provide a very useful technique that works well.\n\n The paper covers both theory and algorithmic issues.\n\n It is well written and easy to follow. Extends isotonic regression by using cross validation to generate multiple scoring sets and combining the scores generated on each.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper proposes two methods for turning scoring outputs into probabilities as alternatives to methods such as Platt's method and isotonic regression. It is clear and well written, and the authores are in particular applauded for giving information relevant to reproducing the experimental results. The methods proposed are original, and this is a sufficiently widespread problem that any demonstrably improved method is a significant advance. The paper is therefore very suitable for NIPS. I have some reservation regarding the experimental results in that, whereas in figure 1 CVAP seems to attain a clear advantage, the numbers presented in tables 1 and 2 are close enough to be less convincing --- can the authors provide any further justification here? A nice paper, suitable for NIPS.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes two techniques for post-processing a score-valued output of a binary classifier by converting it into an estimate of a class probability.\n\nThe techniques are simple and efficient. It is surely publishable in principle, however, in my perspective the paper in its current form has far too many weak spots to be acceptable. They are listed in the following.\n\nUntil the conclusion the distinction between contribution and background remains fuzzy.\n\nThe content of section 2 seems trivial, in particular proposition 2. All it takes is sorting and binary search. However, this seems to be one of the \"major\" contributions.\n\nThe hyperparameters of the learning machines were set by minimizing the training error?! E.g., a Gaussian kernel SVMs can easily achieve zero error. Then all we see is over-fitting. This is a clear no-go, it renders all experimental results meaningless. That alone is a reason for rejection.\n\nThe improvement over the baseline methods is consistent, but the effect size is small. So are the new methods relevant, in particular since they take more space and time than Platt's method? I am missing a discussion of this point. For acceptance I want to be convinced that somebody out there cares.\n\nMinor comments:\n\nIn contrast to what it stated in the paper, Platt's method is not invariant w.r.t. the (sigmoidal?) conversion of the scores to the unit interval. It does not become 100% clear whether the conversion is actually performed with Platt's method or not.\n\nAt some point the paper mentions that most models output values in the unit interval, which could hence be interpreted as probabilities. This point is not followed upon in the experimental evaluation. Why not?\n\nIn section 5.2 it is claimed that there is no simple ad-hoc regularization for isotonic regression. I don't see why Platt's regularization technique cannot be applied, which basically amounts to adding two virtual scores of plus and minus infinity.\n\nThe first word of the title reads \"Large-scale\". Actually, nothing in this paper is large scale, and I don't see any relation to processing of large-scale data.\n\nThe plots in figure 1 are far too small to be readable in a standard b/w printout.  This paper has too many weak spots to be publishable.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposed two algorithms for probabilistic predictions, IVAP and CVAP. At the very beginning, I need to say I am not an expert on this topic. But as for me, the idea in this paper is interesting. The methods are easy to understand and use. Theoretical results are provided to show the validity of the proposed IVAP. Experimental results justify the performance of the proposed methods.\n\nMy first concern is how this proposed method is different from and better than the Venn-Abers predictors. The authors should add some more introduction about the Venn-Abers predictors by showing its validity,predictive efficiency and computational efficiency first, then make the comparison between the original algorithm and the proposed ones.\n\nMy second concern is that the experiment used only one UCI dataset to justify the performance, which may not be very solid. The authors should try on more datasets and more challenging datasets other than UCI datasets to better confirm the superiority of the proposed method. Also, as CVAP generally obtains the best performance. The authors should try to show the sensitivity results by vary different k used in CVAP.\n\nIf the authors can well address the above concerns in the rebuttal, I would like to accept this paper.\n\nI have read the author feedback. I am happy to see some more datasets reported in the supplementary. But it is still not clearly clarified why the original Venn-Aber is not feasible on the current used dataset with this scale. So I urge the authors to reorganize the content in the introduction and move some more experimental results back into the main text. Overall, this paper shows some interesting ideas and promising results. But the novelty of this paper needs to be clarified and the experimental design needs to be improved.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
