{"title": "Matrix Manifold Optimization for Gaussian Mixtures", "abstract": "We take a new look at parameter estimation for Gaussian Mixture Model (GMMs). Specifically, we advance Riemannian manifold optimization (on the manifold of positive definite matrices) as a potential replacement for Expectation Maximization (EM), which has been the de facto standard for decades. An out-of-the-box invocation of Riemannian optimization, however, fails spectacularly: it obtains the same solution as EM, but vastly slower. Building on intuition from geometric convexity, we propose a simple reformulation that has remarkable consequences: it makes Riemannian optimization not only match EM (a nontrivial result on its own, given the poor record nonlinear programming has had against EM), but also outperform it in many settings. To bring our ideas to fruition, we develop a well-tuned Riemannian LBFGS method that proves superior to known competing methods (e.g., Riemannian conjugate gradient). We hope that our results encourage a wider consideration of manifold optimization in machine learning and statistics.", "id": "dbe272bab69f8e13f14b405e038deb64", "authors": ["Reshad Hosseini", "Suvrit Sra"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "This paper revisits fitting mixtures of Gaussians with gradient methods. A combination of manifold optimization, with a particular linesearch procedure, and a \"reparameterized\" objective, empirically beats batch EM in the experiments reported.\n\nWhile I'm not an expert in Riemannian manifolds, the work appears plausible. The work is a reported at a level that's possible to follow, if taking results from cited references on trust.\n\nPlain mixtures of Gaussians may not seem particularly exciting in this world of Bayesian nonparametrics and deep neural networks. However, GMMs are quietly plodding along as important work-horses in applications: they're useful and surprisingly powerful. People often then want to extend GMMs in various ways, e.g., by adding constraints or tying covariances together. But then EM usually doesn't apply. Gradient-based procedures would generalize fine, so ones that work well on the base model could have significant impact on statistical applications.\n\nI would previously have used an unconstrained parameterization based on the Cholesky. While the paper gives reasons to disfavour this approach, it's a shame it's not included in the empirical comparison. Are spurious stationary points an issue in practice? Is the manifold optimizer actually better? Is the main gain here from the \"representation\" in (2.3); would that also help the Cholesky approach? The paper would be stronger if it answered these questions.\n\n Minor comments:\n\nLine 081: \"open a new\" -> \"open new\"\n\nLine 138: rely *on* geodesics\n\nThe timings in Table 4 would be probably be better as a graph. A graph would give an idea of the comparison more quickly. Precise times are irrelevant, being implementation and machine dependent.\n\nI suggest replacing lines 147-149 \"At any point... specifically [4]\" with: \"Geodesics on the manifold from $\\Sigma_1$ to $\\Sigma_2$ are given by [4]:\". Riemannian metrics are not explained in the paper, and don't need to be understood or mentioned to take on trust the geodesic result. (The lack of context, and the nearby definition of `$d$', encourges a misreading of $d$ as a variable, making the trace expression evaluate to `$d^3$' (!).)\n\nI suggest adding the zero mean (\"0,\") explicitly to the normal density in (2.3) and the line below it. On a quick reading, augmenting the data with a constant in the line above (2.3) seems like a terrible idea, a Gaussian could fit these new data with infinite density. The zero-mean constraint is what stops that from happening. Also, the compact Gaussian notation is only defined explicitly including a mean term above (2.1).\n\nI wouldn't call the transformed problem (2.3) a \"reparameterization\". A different model is being fitted, one that would generate data from a different distribution than the original.\n\nI suggest numbering all equations so other people can refer to them, not just in reviews, but in reading groups and citations. Novel gradient-based procedure for fitting GMMs, which appears to beat EM. While I'm slightly unclear on some claims, I'm interested in this paper as potentially really useful in applications.I have increased my score in response to the rebuttal. The authors promise further clarification and control experiments to compare this work to the obvious Cholesky approach. I think this will be an interesting paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "summary: manifold optimization of GMMs\n\nQuality: high (i think, to be determined)\n\nClarity: high\n\nOriginality: seems so to me (though i'm unfamiliar with the manifold optimization literature)\n\nSignificance: good good.\n\ni like this paper. my comments are as follows:\n\n1) i don't like tables, and i don't like only seeing means & standard errors, especially for hard optimization problems.\n\ni also don't like seeing only performance times, because i can't tell how well the algorithms performed in terms of the objective.\n\n2) i'd like the benchmarks from UCI to be in the main text. i'd also like to see comparisons with other implementations, eg, if you are using matlab, perhaps the gmmdist set of functions or mixmod, or in R, perhaps mclust.\n\n 3) in general, saying \"EM algorithm\" as your benchmark it is unclear what implementation you are using, which matters a lot.\n\nfor example, setting the convergence criteria differently can make the run time quite different, but the estimates quite similar.\n\ntherefore, i'd like to see the following:\n\n(a) two-dimensional contour plots (or some such), showing both time and performance for each of the numerical examples.\n\n (b) same thing for a set of benchmarks.\n\n(c) some plots showing likelihood (or the surrogate objective) and time as a function of iteration.\n\none big problem with EM is that often, the loss gets nearly flat, so the last bunch of iterations are useless, and the termination criteria could be set differently to obtain vastly superior performance.\n\ni don't think it matters much what the results are, as long as they are qualitatively consistent with the text. but currently, the manuscript does not provide any evidence that the algorithms actually work.\n\n  developed a modern algorithm for a classic problem, with promising results, however, no accuracy results are provided,nor details of the benchmark algorithms, nor performance vs iteration, so more details are required for proper assessment.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Learning Gaussian Mixture Models is a very well-studied problem in statistics and ML. Expectation Maximization (EM) algorithm from 30 years ago still remains the most popular algorithm. The current paper proposes to use manifold optimization (a reasonably well-established subarea of optimization) for this problem. Very briefly, think of the optimization problem where the variables are the parameters of the mixture model (i.e., the mean, covariance and weight for each component, where the number of components is fixed), and given these one can write down the expression for the (log-) likelihood of the given data. The most important constraint on the variables---and the main source of difficulty---is that the covariance matrices are positive semi-definite. One seeks to find parameters that maximize this. As the paper observes, direct use of standard manifold optimization is not effective as the algorithms tend to be too slow. But, as the paper shows, a simple re-parametrization of the optimization program plus judicious application of standard manifold optimization algorithms with some tweaking improves the performance greatly: In the reported experiments, their algorithm (the paper has several algorithms; I will focus on the best) converges in time that's comparable and often better than EM, and the final value of the objective functions is generally the same for the two algorithms. (Note that these methods may converge to local maxima, and the question of how far the parameters thus estimated can be from the true parameters is not discussed here.) It of course remains to be seen how widely this holds. But given the importance of GMM, these results are interesting.\n\n Minor: Line 129: \"Problem (2.1) in general can require exponential time.\" This should be phrased more carefully: \"Problem (2.1) in general can require number of samples that's exponential in K.\" I think the paper can be accepted on the strength of experimental results.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
