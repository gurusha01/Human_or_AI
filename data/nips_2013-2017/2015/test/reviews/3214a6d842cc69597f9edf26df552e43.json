{"title": "Kullback-Leibler Proximal Variational Inference", "abstract": "We propose a new variational inference method based on the Kullback-Leibler (KL) proximal term. We make two contributions towards improving efficiency of variational inference. Firstly, we derive a KL proximal-point algorithm and show its equivalence to gradient descent with natural gradient in stochastic variational inference. Secondly, we use the proximal framework to derive efficient variational algorithms for non-conjugate models. We propose a splitting procedure to separate non-conjugate terms from conjugate ones. We then linearize the non-conjugate terms and show that the resulting subproblem admits a closed-form solution. Overall, our approach converts a non-conjugate model to subproblems that involve inference in well-known conjugate models. We apply our method to many models and derive generalizations for non-conjugate exponential family. Applications to real-world datasets show that our proposed algorithms are easy to implement, fast to converge, perform well, and reduce computations.", "id": "3214a6d842cc69597f9edf26df552e43", "authors": ["Mohammad Emtiyaz Khan", "Pierre Baque", "Fran\u00e7ois Fleuret", "Pascal Fua"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper presents a general method for non-conjugate variational inference based on proximal method and linearisation of the non-conjugate model. This is shown to reduce to natural gradient optimisation for conjugate exponential models. The method is shown to lead to slightly better predictive accuracy than standard approximate inference methods in a few selected problems and data sets.\n\n Quality\n\n The method relies on linearisation to handle non-conjugate models. The seems potentially problematic, as previous works have found linearisation to be unreliable in variational inference with non-conjugate models (see e.g. Honkela and Valpola, NIPS 2004). The method is evaluated empirically on a few data sets, and mostly found to perform well. The paper focuses a lot on the positive aspects of the method, and its weaknesses and limitations are not mentioned at all.\n\n Clarity\n\n The paper is mostly clearly written and well-organised, but some key details are missing (specifically: what exactly is f_n).\n\n Originality\n\n The proposed approach is novel, although it is based on combination of existing techniques from various fields. Use of references is a bit shaky: the good fundamental reference from proximal algorithms seems to be missing and furthermore previous approaches using linearisation with non-conjugate variational inference are not discussed.\n\n Significance\n\n As the reliability of the proposed method is questionable, it is difficult to judge the paper's importance until this is resolved.\n\n Other comments\n\n 1. The last sentence of the abstract contains unsubstantiated advertising: you method is not the best in every way. This must be clarified or deleted.\n\n 2. In Eqs. (2)-(3), what is eta?\n\n 3. In Eq. (3), why is it arg min and not arg max as in Eq. (2)?\n\n 4. As the proposed method seems to be a purely batch algorithm, it is unclear why it cites and links to SVI methods so much. Classic batch VB would seem much more relevant here.\n\n 5. You should clarify what f_n is precisely in the examples you study. Also, the accuracy of the linearisation procedure needs to be checked to make sure you do not suffer from similar problems as illustrated in Fig. 1 of Honkela and Valpola (NIPS 2004) for the Taylor series linearisation.\n\n Additional references\n\n Previous use of linearisation in variational inference:\n\n Lappalainen, Harri, and Antti Honkela. \"Bayesian non-linear independent component analysis by multi-layer perceptrons.\" In Advances in independent component analysis, pp. 93-121. Springer London, 2000.\n\n Analysis of problems with linearisation:\n\n Honkela, Antti, and Harri Valpola. \"Unsupervised variational Bayesian learning of nonlinear models.\" In Advances in neural information processing systems, pp. 593-600. 2004.\n\n  A novel generic framework for non-conjugate variational inference. Presented results look OK, but some previous work raises doubt on the reliability of a key approximation.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presented the use of proximal algorithms to perform variational inference.\n\nThe authors showed that natural gradient methods can be interpreted as a proximal point algorithm and also used proximal gradient methods (a.k.a mirror-descent) to non-conjugate settings.\n\nThe algorithm was demonstrated on non-conjugate models for regression and classification on real data sets.\n\nOverall, this is a very nice paper that would be a great addition to the NIPS proceedings.\n\nI think that the authors should definitely reference the recent work of Theis and Hoffman \"A trust-regions method for stochastic variational inference with applications to streaming data\" as their method is the proximal point method derived in this paper (though they do not explicitly say it's a proximal point method).\n\nAdditionally, there has been some recent work relating mirror-descent to Bayes theorem-like updates which Eqs. 11, 12, and 13 essential are (if \\beta^k = 1), so some discussion of this would be nice as well.\n\nAlso, there's a somewhat abrupt transition from the proximal point viewpoint of variational inference to using proximal gradient methods and an explanation of this transition would be very helpful.\n\nLastly, there are a good amount of typos, both grammatical and mathematical, that definitely should be fixed with a careful proof-read.\n\nAgain, I think that this is a great paper that would fit in nicely at NIPS. This is a nicely written paper that uses proximal algorithms to perform variational inference and is applicable to non-conjugate settings.Other than some small criticisms I think that the paper would be a nice addition to the NIPS proceedings.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The submission introduces a modified proximal gradient algorithm for variational Bayes. The algorithm turns out to be equivalent to natural gradient descent with a certain choice of step sizes.\n\n Comments on Quality:\n\nThe quality of the ideas in the paper is high, though not without flaws. The proximal gradient framework provides a clean way to deal with nonconjugacy and the equivalence to natural gradient descent serves as a justification and a useful link to a widely accepted method.\n\n The flaws as far as ideas go are as follows. (1) As the authors note, the algorithm is actually not a proximal gradient algorithm because the KL term goes the wrong way. The motivation for doing things this way appears to be efficiency (the optimization problem decouples across parameters in the authors' setup, but not in the true proximal gradient setup). (2) Because of issue #1, the algorithm cannot be shown to converge except in special cases (e.g. beta_k --> 0 fast enough and in fact that seems necessary and sufficient).\n\nUnfortunately, the submission does not sufficiently analyze the algorithm to demonstrate its utility. The primary argument for the algorithm appears to be potentially improved results in practice. Yet, the performance gains seem too small to justify the order of magnitude increase in runtime required to use the authors' method. Much stronger results, showing substantial improvements over the mean field baseline, would be necessary to make the submission compelling enough for acceptance.\n\nOn the whole, the reviewer believes the work has potential, but that it is too preliminary to be published in the conference.\n\nComments on Clarity:\n\nMost of the paper is presented clearly, apart from Sections 5-6, which are nearly impossible to follow. The reviewer suggests the authors revise these to make them easier to understand.\n\nComments on Originality:\n\nThe work appears original.\n\nComments on Significance:\n\nFor now, the significance appears low. The algorithm could, however, have meaningful practical significance---further experiments are required to determine that. The proximal gradient variational Bayes algorithm proposed in the submission is intriguing and warrants further investigation. The results presented in the submission, however, are not strong enough to justify accepting the paper.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
