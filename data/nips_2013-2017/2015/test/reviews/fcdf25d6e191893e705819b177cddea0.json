{"title": "Local Causal Discovery of Direct Causes and Effects", "abstract": "We focus on the discovery and identification of direct causes and effects of a target variable in a causal network. State-of-the-art algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs in order to identify the direct causes and effects of a target variable. While these algorithms are effective, it is often unnecessary and wasteful to find the global structures when we are only interested in one target variable (such as class labels). We propose a new local causal discovery algorithm, called Causal Markov Blanket (CMB), to identify the direct causes and effects of a target variable based on Markov Blanket Discovery. CMB is designed to conduct causal discovery among multiple variables, but focuses only on finding causal relationships between a specific target variable and other variables. Under standard assumptions, we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency, often by more than one order of magnitude.", "id": "fcdf25d6e191893e705819b177cddea0", "authors": ["Tian Gao", "Qiang Ji"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The goal of the paper is to identify direct causes and effects of a given variable (target variable). The main contribution stands in that the authors achieve their goal without the need of first discovering the whole causal graph before restricting attention to the neighborhood of the target variable. Instead, they directly focus on discovering the local causal structure.\n\nExisting algorithms can discover the Markov blanket (MB) of a target variable and a subset of it that consists of the set of parents and children (PC) of the target variable. The goal of this paper is to identify which of the nodes in PC are parents and which children of the target.\n\nSignificance & Originality:\n\nI consider the contribution of the paper significant. It improves on efficiency and scalability compared to previous methods which need to perform the intermediate step of global causal discovery to achieve the same goal.\n\nClarity:\n\n I think that lack of clarity and flow in several parts is a drawback of the paper. I found some descriptions confusing. In general, I would suggest more explicit explanations, motivating examples and high level explanations before going into the technical parts.\n\nSpecifically:\n\n- Last paragraph of Introduction: the authors refer to Local Causal Discovery. It took me some time to realize that they refer to existing work and not to theirs, since this term coincides with the title of the paper. I would suggest changing the title to something more specific like \"local causal discovery of direct causes and effects\". Moreover, I find the name of the proposed method \"Causal Markov Blanket\" also too general and maybe misleading. The authors could probably consider a common name for both title and method involving \"direct causes and effects\".\n\n- Section 3: Some high level explanations are necessary before going into the technical details. The authors could provide a short high level summary of all steps of the proposed method in the beginning of section 3.2.\n\n- I could be missing something in Algorithm 2: as far as I understand, ID_T should contain the causal identities of *just* the PC set of T. But in step 1 it is mentioned \"..the causal identities of *all* nodes..\" and in Algorithm 2 the size of ID_T is |V| where V is the entire variable space and no just the PC.\n\n- The output of Algorithm 2 is the ID_T. How is this converted to a CPDAG? For id_T 1 and 2 it is clear. For id_T =3 do you draw an indirect edge? Providing some visual examples of the output CPDAG of the Algorithm would be helpful.\n\n- Elaborating more on why failing to meet Lemma 3 leads to Markov equivalent structures, could be useful.\n\n- The authors could consider improving the flow in page 3.\n\n- Some parts are not very specific/explicit: e.g.: * Sec. 4, first par.: \"causal identities for a target node\": isn't it \"causal identities of nodes in the target's PC set\"? * I understand what is a CPDAG but it would be helpful to define what it is meant by the \"CPDAG of a target variable\". Is it a subset of the original CPDAG containing T and the parents and children of it? *In the proof of Theorem 3: \"CMB will identify up to a graph's CPDAG\": is this again the subset? * Footnote 3 is not clear * Sec. 3.1: what does it mean \"some V-structure children and spouses\" ? * Sec 3.1: \"PC set of a MB\" : isn't it \"PC set of a target's MB\"? * The terms \"local causal discovery of the target\" and \"local causal nodes of the target\" could be rephrased\n\nQuality:\n\nI have not checked all the technical details but the paper seems to have a well-supported theoretical analysis followed by experimental results. The results show significant improvements in efficiency compared to global methods.\n\n -Typos: * There are several typos in the references section. * Introduction: in term -> in terms *Sec. 3.1: \"... no selection bias, every..\" -> \"...no selection bias, that every...\" * Experiments: \"along its standard deviation\" -> \"along with its...\", \"As shown in in Table\"-> remove one \"in\"\n\n  I consider the paper having a significant contribution in more efficiently finding direct causes and effects of a target variable. I found the presentation sometimes confusing, it could be improved by more visual examples, motivation and more clear explanations.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Weak accept\n\nThe description of the contributions should be clearer in the introduction. Your contributions relative to LCD and BLCD are not as clear as they should be.\n\nThe paper presents a new local search algorithm for graphical relationships. Under suitable assumptions these relationships are causal. The ideas in the local search algorithm are technically interesting.\n\nI'm not convinced that comparing the efficiency to global search algorithms is fair or interesting.  light review", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers the task of learning the direct causes and effects of a predefined target variable T, instead of the entire causal structure. The algorithm introduced in the paper departs from the Markov blanket, and builds on a few rules that apply over the neighborhood of T so as to discover the directionality of the arrows. It is a clean and interesting paper, and the algorithm is shown to be equivalent to previously established ones (i.e., doesn't miss anything). In fact, the algorithm does not discover any additional arrow than already known results, but it improves on the scalability since only a subset of the variables is the target of the analysis in many real-world scenarios, not the whole graph. This paper considers the problem of learning the direct causes and effects of a predefined target variable T, instead of the entire causal structure.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper concerns learning direct causes and effects of a target variables using local learning. This paper extends existing local learning methods by taking account larger neighbourhoods and the presented method is therefore able to infer more causal relations than the previous methods. The problem is relevant and the contributions are worthwhile.\n\n Overall, technical presentation is sloppy.\n\nWhat is the definition of an \"unshielded parent\"? What is the definition of a \"set of three-fork parents\"? What is the definition of a \"sound and complete MB discovery algorithm\"?\n\nTheorems 2 and 3 do not contain all necessary assumptions. In the proof of Theorem 2, causal sufficiency is used. However, it was not assumed in the theorem. One also need to\n\nassume that conditional independence tests return always correct results (or infinite data or some other similar assumption). Also it is unclear to me why it isn't necessary that the distribution is faithful to any DAG?\n\n099: Markov condition tells that a node is independent of its non-descendants given its parents, Causal Markov condition tells that a node is independent of its non-effects given its direct causes.\n\n125, 227: R\\(S\\T) is usually different than (R\\S)\\T, so if you have two set differences in a row it is good to use parenthesis to avoid any misinterpretations.\n\n133-135: Why is this a reasonable belief? We know that, if one uses only conditional independence tests, structures can be identified only up to a Markov equivalence class.\n\n293: the PC set of a MB -> the PC set of a target\n\nThe experiments could be strengthened by adding a standard Markov blanket discovery algorithm (where you learn only the directions of arcs which are part of v-structures) as a baseline to see how much the new rules help.\n\nThe evaluation of the efficiency of different algorithms seems to give an unfair advantage to the CMB algorithm. The efficiency is measured in terms of the number of conditional tests and the number of MB discovery algorithms are invoked. The problem is that MB discovery algorithms typically use conditional independence tests to find MBs and not counting the conditional independence tests inside a call of an MB discovery algorithm favours algorithms that use such subroutines. One thing that could make evaluation more fair would be to add also the actual running times of different algorithms. An interesting topic and a worthy contribution. However, technical representation should be improved.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
