{"title": "Action-Conditional Video Prediction using Deep Networks in Atari Games", "abstract": "Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.", "id": "6ba3af5d7b2790e73f0de32e5c8c1798", "authors": ["Junhyuk Oh", "Xiaoxiao Guo", "Honglak Lee", "Richard L. Lewis", "Satinder Singh"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "The paper addresses the problem of learning a model of Atari 2600 games (a popular testbed for reinforcement learning algorithms), in other words predicting future frames conditioned on action input.\n\nThis is a challenging problem and its solution is a useful tool to build better controllers.\n\nThe paper is clear and well-structured, and has convincing experiments (and videos).\n\nThe model is a CNN (with a fully-connected layer) followed by multiplicative interactions with an action vector, followed by convolution decoding layers. The recurrent version has an LSTM layer added after the CNN.\n\nThe authors evaluate their models both on pixel accuracy (traditional way of evaluating such models) and on usefulness for control (what we really care about).\n\nIt would be desirable to include more experimental details about 1) the network architecture (especially the deconvolution part) and 2) the network training procedure. Ideally, code would be made available, but more details in the main text or supplementary would also be fine.\n\nSome comments:\n\n- It is a bit unfortunate that the authors did not try to predict rewards in addition to the next frames, that would have opened the door to using the model for planning (e.g., using UCT), instead of using a trained model-free controller to test the usefulness for control - which is a bit harder to interpret.\n\n - Baselines against which the models are compared are a bit weak, but this is fair enough since there are no obvious candidates to compare against (afaik).\n\n- About the exploration section, is the predictive model learned online to help with exploration? Or is it learned using data from a regular DQN (uninformed exploration) first, and then used to direct the exploration of a new controller. If it's the latter then it's not clear what this is achieving - since exploration has already been done to obtain the model. In any case, it is still a bit surprising that this helps in some games.\n\n- The controlled vs uncontrolled dynamics section at the end is interesting.\n\n- The authors might want to take a look at this relevant recent work \"DeepMPC: Learning Deep Latent Features for Model Predictive Control\" on learning deep predictive models (using also multiplicative interactions with the actions) for control, although this isn't in the visual domain.\n\n Minor things/typos:\n\n- \"In Seaquest, new objects appear from the left side or right side randomly, and these are hard to predict.\" I'm not sure this is completely true, but it certainly looks random.\n\n- line 430: predicing\n\n [Updated score after rebuttal. Other recent papers which learn deep dynamical model from images, though not for the Atari game: -From Pixels to Torques: Policy Learning with Deep Dynamical Models -Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images ]  A neat paper on learning the dynamics of Atari Games from data. The paper is well-written and has some convincing experiments.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper presents a system for action conditional prediction of video frames using deep convolutional and recurrent neural networks. The authors propose two deep models for encoding the video data using feedforward CNNs or a recurrent neural net, transforming the frame based on the applied action and decoding them back using up-convolution. The system is trained on four video games using the atari emulator and tested for video prediction and efficacy for control using the DQN algorithm.\n\nQuality & Originality: ------------------------ The paper is sound and logical. The proposed deep architectures are similar to previous work, with the added action dependent transformation. The main novelty comes from the new multiplicative formulation of this transformation as compared to an additive fully connected layer. Most other parts including the training has been proposed in the literature. The system does achieve good results, being able to predict multiple frames into the future.\n\n The experiments compare and contrast the two variants against two other naive baselines which do not consider the actions. A more informative baseline would have been the additive fully connected action transformation layer as this takes into account the action's effect. The authors should try to compare against this.\n\nOther experiments confirm the efficacy of the system for control, based on the DQN algorithm. The informed sampling for the DQN also improves on state-of the art results for the DQN, but that is to be expected given a decent enough generative model of the emulator. A good sanity check there would be to use the emulator as the generative model and compare the scores w.r.t the results from the learned model. Additionally, the actions separate well across the learned representations.\n\nClarity: --------- The paper is well written and clear. The figures need to be improved (especially Fig 5.a) where it is hard to see any difference between the two methods) and caption text should be added below the figures to make it easier for the reader to gain context.\n\n Significance: --------------- The paper proposes a new method for learning representations for video prediction conditioned on the actions. The paper builds on the use of deep auto-encoders for learning representations for control and is relevant to the NIPS community. There has been some prior work on action-based image data prediction in robotics that the authors may wish to cite: Boots et al., Learning Predictive Models of a Depth Camera & Manipulator from Raw Execution Traces, ICRA 2014 - This paper also does action-dependent prediction of kinect data (640 x 480), but over much shorter timeframes and simpler environments.\n\nOverall, the paper is concise and the ideas are well presented. I feel that the work is a bit incremental with not a significant theoretical contribution. Adding more experimental results with better baselines should improve the paper as a whole. I vote for a borderline accept.\n\n This paper presents two deep-architectures for predicting sequences of video frames of the Atari emulator conditioned on the agent's actions and previous frames. The system is tested on a few Atari games and is used with the DQN algorithm to evaluate its efficacy for control. The paper is well written with a few experimental results, but comes across as incremental work. I vote for a borderline accept.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The paper easy to read and easy to understand in big strokes the ideas and the message it conveys. However I found that the authors sometimes (maybe to improve readability) skip certain details. It is not obvious how important these details are , but makes a quick reproducible of the results hard.\n\nI will point out a few things that I would like the authors to be more clear about:\n\n* specify in more details the model ! Is not clear how the decoder is symmetric to\n\n the encoder. How do you upsample as you go down?\n\n * what learning procedure was used to train the generative model (SGD,\n\n SGD+momenum, RMSprop, Adam, Ada-delta, Adagrad .. to name just a few of\n\n the more popular approaches) ?\n\n* For the LSTM was there any clipping used (most LSTM models out there rely\n\n on gradient clipping for stability)\n\n* What learning rate, momentum etc. was used. Where this chosen based on\n\n intuition, or did the authors run a grid search (or random sampling of\n\n the hyper-parameters)\n\n* How is the data constructed more exactly? Was there any pruning used to\n\n make sure that you get enough frames from each stage of a game (the image\n\n statistics early on are potentially very different from later e.g.\n\n pacman and might require balancing the dataset)?\n\nWhat is the average episode length (compared to the average length of the movie you can generate)?\n\n* is the curriculum learning (going from 1,3,5 frames in the future)\n\n necessary? Does it result in more stable generation? Was this validated\n\n experimentally?\n\n* the authors claim that the action should have a multiplicative\n\n interaction, which I think is a reasonable assumption. Has this fact been\n\n verified experimentally though? Given that the task is novel, it is hard\n\n to judge how important these bits are.  The paper is well written, though occasionally lacks details of the exact experimental procedure which most probably makes the result un-reproducable. The results are impressiveand the authors make some interesting observations and analysis about the behavior of their model.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "The authors present a system for video frame prediction in ATARI games taking into account the game actions. The main novelty is a multiplicative transformation layer that selects weights based on a given action vector. To keep the system scalable, a factorization of the weight tensor is used. Encoding and decoding of images is done by a well-established convolutional network architecture.\n\nThey propose two different architectures: a feed-forward network that gets a fixed number of previous frames as input and a recurrent network that gets only one frame but has an LSTM layer before the transformation layer. (I think at least once in the paper they should say what's an LSTM and what the abbreviation stands for.)\n\nThe qualitative assessment of the generated frames as shown by the supplementary videos is convincing. A quantitative evaluation of mean squared pixel error is given in a clear way but it is not surprising that their system beats a linear and a nonlinear predictor that don't take into account the game actions.\n\nAnother experiment in which the generated frames are used instead of the real frames as input for a DQN agent also shows results as expected. The performance is worse than for real input frames, better that random play, and their system beats the no-action predictors.\n\nAn interesting application of this system is to use it during training of an agent to improve exploration. They show that if a DQN agent takes an action that leads to a predicted frame that has least similarity with previously seen frames (rather than a random action), the final performance is significantly improved on some games.\n\n Finally it is shown that the system can be used to automatically analyze some of the game dynamics. Game actions that have similar effects can be identified from similarities in the transformation weight matrix. Furthermore the system estimates which pixels of the image are controlled directly by actions and which pixels are uncontrollable game dynamics.\n\nThe authors write that \"To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.\" This may be true, because to my knowledge previous such systems did not have LSTM. I know, however, that there is an old paper by Schmidhuber and Huber from 1991 (International Journal of Neural Systems) where a similar neural predictor learns to predict the next visual input frame of a fovea, given previous input and action. The predictor is embedded in a reinforcement learning system that learns sequences of saccades that lead to desired visual objects defined in a separate goal input. Schmidhuber also had papers at IJCNN 1990 and NIPS 1991 where both the predictor and the action generator were recurrent (but no LSTM yet). Nevertheless, it would be good if the authors could point out what's different in their system, which for example also uses a different RL method.\n\nThe results are clear, the paper is well structured and suggested for acceptance after minor revisions as indicated above.\n\n A novel architecture for video frame prediction in ATARI games based on game actions is presented. There are promising results on improved exploration during agent training, as well as a simple analysis of game dynamics.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper is nice to read.\n\nThe idea is clear, the implementation looks sound, and the results are good.\n\nI also appreciate that the authors take a moment to highlight limitations (Section 4.1), and that they give both quantitative and qualitative analyses, as well as suggesting hypotheses.\n\nClearly a lot of work went into this paper, and the analysis seems good and fair.\n\nThe authors do not just consider how to do something (generative model), but also why (exploration), and then test this.\n\nThe main drawback is that the results on usefulness are a bit inconclusive, and the ultimate general impact of this line of work is a bit unclear. This is a nice empirical paper about using generative models to hallucinate futures in Atari games. The (empirical, quantitative and qualitative) analyses of the results are nice and comprehensive.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
