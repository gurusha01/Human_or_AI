{"title": "Logarithmic Time Online Multiclass prediction", "abstract": "We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.", "id": "e369853df766fa44e1ed0ff613f563bd", "authors": ["Anna E. Choromanska", "John Langford"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Heavy review\n\nSummary:\n\n The paper concerns multi-class problems with a large number of classes. It introduces a novel label tree classifier that learns and predicts in logarithmic time in the number of classes. Theoretical guarantees in terms of a boosting-like theorem have been proven. Moreover, not only node classifiers, but also the structure of the tree is trained online. Additionally, the authors show a simple subtree swapping procedure that ensures proper balancing of the tree. Empirical results show attractiveness of the introduced approach.\n\nQuality:\n\nThe paper presents both theoretical and empirical results. The method is clearly derived and theoretically justified. Some minor comments are given below.\n\nThe objective after reformulation can be stated as:\n\nJ(h) = 2 \\sum_{i=1}^k | \\Pr(i) \\Pr(h(x) > 0) - \\Pr(i, h(x) > 0) |\n\nThis is a kind of an independence test between i and h(x) > 0. I suppose that such measures have been used in rule induction (e.g., in generation of association rules) and decision tree learning. This is also similar to the so-called Carnap's confirmation measure (see, \"Comparison of confirmation measures\", Cognition, 2007, and \"Logical foundations of probability (2nd ed.).\", 1962).\n\nDecision tree learning can be introduced as a recurrent procedure of empirical risk minimization guided by a surrogate loss function, where both splits and leaf predictions are computed as empirical risk minimizers (the notion of a purity measure is not needed then). For C4.5, this is logistic loss, and for CART squared error loss. It would be interesting to check whether there exists a loss function that corresponds to J(h).\n\nThe authors do not clearly describe the difference between LOMTree and standard decision trees. Footnote 4 states that standard criteria such as Shannon or Gini entropy satisfy requirement of balanced and pure splits. Since it is possible to adapt standard decision trees to the problems with a large number of classes [16,17], it is not clear what is the difference between these two approaches. Can we substitute J(h) by Shannon entropy?\n\nI suppose that there exists a kind of a trade-off between test-time complexity and the complexity of the function class, i.e., by minimizing the number of prediction steps we should probably extend the complexity of the function class. This would explain the empirical results of OAA and LOMTree. Could you comment on this?\n\nThe prediction time results are given per test example. The overall method is also designed for improving test time for a single test example. However, when test examples are coming in mini-batches different tricks can be used to improve test time. How to use this method in such scenario?\n\n Clarity:\n\nThe paper is clearly written.\n\n Originality:\n\nThe paper is very original as it clearly states the problem of efficient learning and prediction in case of large output spaces. The introduced algorithm is also novel, however, the authors should take into account that the problem of decision tree induction was exhaustively studied in 80' and 90'. Many algorithms have already been introduced that share similarities with LOMtrees (incremental learning of trees, trees for data streams, Hoeffding trees, trees with linear splits).\n\n Significance:\n\nThe paper will be (already is) very influential for large-scale machine learning.\n\nAfter rebuttal:\n\nThank you for your responses.\n\n I gave the references to confirmation measures since this was the most similar measure I knew to your objective. It would be great if you will explore this topic in the extended (e.g., arxiv or journal) version of the article. I have found your objective very interesting, but I think that similar measures have already been considered in many settings and many interesting links can be found. I agree that such measures are not so popular and successful in decision tree learning, especially in the multi-class setting.\n\nThe same concerns incremental learning of decision trees. There are some approaches that you can easily find in Internet. The question is how they relate to your approach. One well-know approach are Very Fast Decision Trees (aka Hoeffding trees) introduced by Domingos and Hulten (2000). A detailed discussion can be postponed however to the extended version of your paper.\n\n  This is a very inspiring paper that will be (or even already is) very influential for large scale machine learning. It should be accepted for publication at NIPS.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "A nice work proposing to learn logarithmic online multiclass trees for efficient multiclass prediction. The authors introduce the principle behind LOMTree and its advantage wrt OAA classifiers. Then they propose a simple objective value for finding a partition on a given node that will promote maximally pure and balanced partition. Then the global quality of the tree is expressed as an entropy function. The proposed algorithm converges provably to a tree with log(k) complexity. Numerical experiments are performed on several datasets from 26 to 105k classes and show reasonable training and testing time.\n\n This is a good paper well written but pretty dense. The proposed approach is novel and compares favorably to other tree-based classifiers while being close to OAA performances. I just have a few comments in the remaining.\n\n- First the example for the construction of a node in the supplementary material is particularly clear. It would be nice in the paper instead of supplementary material (in parallel to algorithm?).\n\n- The linear classifiers h at each nodes are estimated online. Is the update a stochastic gradient wrt J()? A few more detail would go a long way to reproducible research.\n\n- The numerical experiments are a bit short. It would be interesting to add in table 4 the performance of OAA classification to help for the comparison (for the small datasets at least). They are given in the text but should appear in the table.\n\n Interesting paper with a novel algorithm for constructing an efficient tree based classifier. Theoretical study and numerical experiments are convincing.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper proposes a novel online algorithm for constructing a multiclass classifier that enjoys a time complexity logarithmic in the number of classes k. This is done by constructing online a decision tree which locally maximizes an appropriate novel objective function, which measures the quality of a tree according to a combined \"balancedness\" and \"purity\" score. A theoretical analysis (of a probably intractable algorithm) is provided via a boosting argument (assuming weak learnability), essentially extending the work of Kearns and Mansour (1996) to the multiclass setup. A concrete algorithm is given to a relaxed problem (but see below) without any guarantees, but quite simple, natural and interesting.\n\nThe paper is well written, very interesting and seems to be original. The theoretical analysis seems to be correct. The experimental results suggest a good improvement over other logarithmic time classifiers but fall quite short of obtaining state-of-the-art generalization performance. I see this work as a small step forward toward this goal.\n\nI do have some concerns:\n\n- Obviously, the 1-nearest neighbour classifier has train and test times which are independent of the number of classes k. This is so since with the time scale used in your time complexity analysis, reading log(k) bits is regarded as a O(1) operation. Thus I find your claim that the most efficient possible accurate approach has time complexity log(k) as misleading. I don't find the information theoretic arguments given in the introduction as relevant to your time complexity analysis of the algorithm. Can you please clarify?\n\n- If I understand correctly, the LOMtree algorithm does not quite maximize the relaxed objective given in line 263.\n\nThis is due to the fact that your estimates of the expectations m_v(y), E_v, etc. are not updated when some retrained hypothesis up the tree changes its decision on some samples along the paths. As the statistics given in algorithm 1 are additive, such changes does not reflect in your estimates. Isn't a more complex mechanism (such as message passing) is needed for accurate bookkeeping? It seems that such \"open-loop\" inaccurate statistics in the nodes can make the algorithm behave quite erratically. Can you please clarify this point?\n\n- It seems that when one goes down in the tree, the classifiers are doomed to have lower accuracy. This is so since the tree is quite balanced and lower nodes will have much less samples to train a good classifier on. Thus, I suspect that an exponential number of samples will be needed to get good generalization accuracy. This paper proposes an online multiclass classifier with sufficient novelty and interest to accept for publication.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
