{"title": "Algorithms with Logarithmic or Sublinear Regret for  Constrained Contextual Bandits", "abstract": "We study contextual bandits with budget and time constraints under discrete contexts, referred to as constrained contextual bandits. The time and budget constraints significantly complicate the exploration and exploitation tradeoff because they introduce complex coupling among contexts over time. To gain insight, we first study unit-cost systems with known context distribution. When the expected rewards are known, we develop an approximation of the oracle, referred to Adaptive-Linear-Programming(ALP), which achieves near-optimality and only requires the ordering of expected rewards. With these highly desirable features,  we  then combine ALP with the upper-confidence-bound (UCB) method in the general case where the expected rewards are unknown a priori. We show that the proposed UCB-ALP algorithm achieves logarithmic regret except in certain boundary cases.Further, we design algorithms and obtain similar regret analysis results for  more general systems with unknown context distribution or heterogeneous costs.  To the best of our knowledge, this is the  first work that shows how to achieve logarithmic regret in constrained contextual bandits. Moreover, this work also sheds light on the study of computationally efficient algorithms for general constrained contextual bandits.", "id": "310dcbbf4cce62f762a2aaa148d556bd", "authors": ["Huasen Wu", "R. Srikant", "Xin Liu", "Chong Jiang"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "Review after rebuttal : - I agree that it is not exactly the same, but it seems close enough to me : if the horizon is large enough, the coupling of the context vanishes (and given that, I don't understand why . So I am not sure why each occurrence of a context cannot be treated as a separate instance of a bandit problem). - To me, this paper would be much better with a clean (the supplementary material's bound has to be reworked) upper bound in term of arm gaps for the problem dependent case, and ideally matching lower bounds in problem dependent and independent cases.\n\n ----------------------\n\nThe authors consider the problem of a budgeted and time constrained contextual bandit. The main finding of their paper is that if the number of context and actions is finite, and if the time constraint T is sufficiently large (with respect to the problem at hand) then they prove that a regret of order \\log(T) is achievable, which is a new result for contextual bandits.\n\nThe results seem thorough, and the writing is precise. The results are interesting but not very surprising. Indeed, since the number of contexts J and arms K are finite, this bandit problem can be seen as a a time constrained and budgeted bandit problem with JK arms, which is also action constrained since at each time, not all JK arms are accessible (only the K arms corresponding to the context). Since the number of arms is finite, for T large enough, the regret is \\log(T) as in action constrained bandit problems. So I am not sure how innovative this paper is - maybe the authors can clarify this in their rebuttal.\n\nI have a few questions regarding this paper : - In Theorem 2, the regret inn case 2) is O(\\sqrt(T) + ...). But is it really O(\\sqrt(T) + ...)? Shouldn't it be O(\\sqrt(KT) + ...) at least? - Still in Theorem 2, the regret is either O(KJ \\log(T)) or O(\\sqrt(T) + KJ\\log(T)) depending on the configuration of the arms. In general, problem dependent bounds are expressed in a more refined way with the sum of inverse of arm gaps. Isn't it possible to do something similar here? I understand that you have KJ instead of a certain sum of gaps because your proof is based on events characterizing the correct orders of the UCB, and not concentration bounds on their gaps. But is it an optimal idea? - It would be interesting to have lower bounds for this problem, ideally problem dependent and problem independent. Trivial lower bounds are the ones of classical bandit which correspond to cost equals 0 and context which is always the same. Can it be refined to take into account the complexity of the context? I am still not convinced by the originality of this setting - and therefore remain neutral about this paper.-------------------------I have the impression that the setting of this paper is very close to a budgeted bandit problem with action constraint.So I am not sure how innovative this paper is - maybe the authors can clarify this in their rebuttal.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "(Light Review)\n\nThe paper considers contextual bandits with budget and time constraints. The analysis is restricted to finite discrete contexts, fixed costs and a single budget constraint. While the setting is simplified, these assumptions are still reasonable in practice. Importantly, it results in algorithms that achieve O(log T) regret by introducing the adaptive linear programming approximation of the oracle.\n\nThe paper is clear and well documented. Relevant work is cited and motivation is explicit. This is a solid theoretical paper. The practical aspects are only briefly discussed in Section 5 and 6 and no experimental results are provided. This being said, the contributions are significant: as far as I know, the authors are the first to obtain logarithmic regret in the case of constrained contextual bandits. The proofs are convincing (although I did not check them all in full detail). This is a good paper, making appropriate simplified assumptions to obtain O(log T) regret algorithms. The practical aspects might be discussed into more detail.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers a contextual bandit problem with budget constraint B and known time horizon T. For each trial, the agent observes a context j chosen according to a fixed probability distribution \\pi over a context set {1,..,J} and takes an action k from {0} \\cup {1,..,K}, where 0 means \"skip\". If k = 0, then the agents gets no reward and incurs no cost. If k > 0, then the agent gets reward y and incurs cost c_{j,k}, where y is chosen according to a fixed probability distribution over [0,1] which depends on j and k, and c_{j,k} > 0 is a parameter known to the agent. The goal of the agent is to maximize the cumulative reward under the condition that the cumulative cost never exceeds the budget B.\n\nThe main body of the paper is devoted to a special case where \\pi is known and c_{j,k} = 1 for all j and k, and the paper gives an algorithm whose regret bound is O(log T) under a mild condition. The paper extends the algorithm to the general case where \\pi is unknown and c_{j,k}'s are arbitrary.\n\nA more general setting is considered in [16], but it gives an algorithm with O(\\sqrt T) regret. So the paper is possibly the first work with O(log T) regret, although the mild condition should be satisfied. The algorithm is based on an approximation algorithm for computing the optimal action sequence when all the statistics are known. The algorithm is simple but very interesting.\n\nThe paper is well motivated and very clearly written. It would be more interesting and significant if O(log T) regret always holds, otherwise \\Omega(\\sqrt T) lower bound is proved in the boundary cases.\n\nAll the results seems to be based on Lemma 1, but I'm not convinced of its correctness. I do not think the optimal action sequence always satisfies (2) for all trials, and so I do not understand why the average constraint is a relaxed version of the hard constraint. Please give a brief explanation. ==> Now I'm convinced of Lemma 1 by the rebuttal. Thanks\n\nMinor comments: Is it NP-hard to exactly obtain the optimal action sequence? Can the results be generalized to the case where T is unknown? Can we assume u^*_1 > u^*_2 > ... > u^*_J\n\nwithout loss of generality even for the unknown context distribution case?  I strongly recommend this paper for acceptance.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Since this is a short review, I only ask one question: what is the reason that boundary cases have to be analyzed differently? It doesn't seem that rho = q_j appears anyhow special, especially for j > 1. This paper analyzes contextual bandits against a stronger baseline: one that uses adaptive linear programming to decide to pass on certain percent of suboptimal rounds (what the authors call budget). It has interesting setup and unique solutions.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "This paper considers a fairly simple budget constrained cb problem with discrete contexts and actions. The main contribution is the computational efficiency of the algorithm (via LP approximation) and the log(T) regret bound it achieves.\n\n This paper is very well written and easy to follow.\n\n Suggestions: in future work, consider using a parametric model for the reward. This is useful when the number of contexts is large. In section 4.1, when combining UCB with ALP, be more clear about the rationale using in the UCB-ALP algorithm. For example, I think you are following the optimism in the face of uncertainty. It's probably equivalent to the policy update in UCRL2. It'll be great to include some numerical experiments in extended version of the paper.\n\n you may want to this paper: http://arxiv.org/abs/1506.03374 This paper merits acceptance since it provides an efficient algorithm for constrained cb with optimal regret in most cases. It is interesting and indeed an improvement over existing work.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
