{"title": "Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing Parameter Sets", "abstract": "Inference is typically intractable in high-treewidth undirected graphical models, making maximum likelihood learning a challenge. One way to overcome this is to restrict parameters to a tractable set, most typically the set of tree-structured parameters. This paper explores an alternative notion of a tractable set, namely a set of \u201cfast-mixing parameters\u201d where Markov chain Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the stationary distribution. While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC, such procedures lack theoretical guarantees. This paper proves that for any exponential family with bounded sufficient statistics, (not just graphical models) when parameters are constrained to a fast-mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability. When unregularized, to find a solution epsilon-accurate in log-likelihood requires a total amount of effort cubic in 1/epsilon, disregarding logarithmic factors. When ridge-regularized, strong convexity allows a solution epsilon-accurate in parameter distance with an effort quadratic in 1/epsilon. Both of these provide of a fully-polynomial time randomized approximation scheme.", "id": "15de21c670ae7c3f6f3f1f37029303c9", "authors": ["Justin Domke"], "conference": "NIPS2015", "accepted": true, "reviews": [{"comments": "I think that the main issue of the manuscript is a lack of awareness of the literature on the topic. The authors' algorithm is strongly related to the Monte Carlo EM algorithm (and in fact the general class of stochastic approximation algorithms with markovian dynamic which have been proposed in various forms to optimize intractable likelihood functions). There is a well developed literature on the topic and the authors should compare what they have been doing to existing work. A theory which includes some of the authors' results already exists and covers much more general scenarios. For example the Markov chains are not required to be uniformly ergodic and there is no need to reinitialise the Markov chain at a fixed distribution r at each iteration (the authors acknowledge that they do this in order to simplify their analysis, but this is not sufficient--in addition reinitializing with the previous sample may be a better idea, as discussed at the end of the manuscript, and these algorithms have also been extensively analysed previously). The presentation of the results is far from optimal : for example for Theorem 1 and 2 it is not clear what the assumptions are? One can refer to the text before and after, but the reader should not have to do that. Note also that there is no need for the strong assumptions used in order to develop results such as those of Theorem 6 (see for example Benveniste, Metivier and Priouret for more general results). I think that the authors lack awareness of the literature and the results are only marginally novel.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Summary: The paper proposes a novel approach to computationally efficient maximum likelihood learning in exponential families. In general, finding the maximum likelihood solution is intractable. From a convex optimization perspective, the sticking point is the need to calculate an integral wrt the currently proposed EF parameter. By assuming that MCMC is fast-mixing for all allowed parameters, the author(s) are able to show that the integrals needed for proximal gradient descent can be calculated with sufficient precision that, when combined with the results of Schmidt et al. (2011), a fully-polynomial randomized approximation scheme for calculating the MLE can be obtained. Both the convex and strongly convex cases are considered, which lead to different types of guarantees: the former on the likelihood error, the latter on the parameter error. A simple experiment demonstrating the theory is also included.\n\nOverall, a high quality piece of work. The approach is well-motivated as an alternative to relying on small-treewidth assumptions. The theoretical analyses appear solid, though I haven't checked the proofs in detail. The proofs are essentially a matter of combining the MCMC mixing assumption with concentration inequalities and the results of Schmidt et al. The paper is thus a combination of existing techniques, though the combination is nontrivial.\n\n I appreciated the honesty with which the limitations of the approach were discussed. In particular, a thoughtful conclusion analyzed weaknesses of the approach, ways in which the theoretical assumptions are overly pessimistic, and directions for future work. The paper also offers some insight into CD-type algorithms, even though the algorithm that is analyzed is not quite contrastive divergence.\n\n The introduction was well-written with substantial references to previous work, but there were a few issues with the clarity of technical presentation. One small but important omission was simply the statement that the log-likelihood is in fact convex: the 2nd and 3rd terms in eq. 1 are clearly convex and the convexity of A(theta) follows from the fact that the hessian of A is the covariance of the sufficient statistics, and thus is PSD. While seemingly minor, the whole paper is premised on the the convexity of f(theta), and so discussing it will help guide the reader through the logic of the paper. A clear summary of the assumptions made in the paper. For example, one assumption never mentioned in the main paper is that the parameter set Theta must be convex.\n\n Overall, I found the paper to be an original and substantial contribution. The results to seem to be of limited practical value due to the fact that proving fast mixing of MCMC is extremely challenging. So in a sense the paper has replaced one hard problem (finding the MLE) with another (finding fast-mixing Markov chain). That said, being able to convert one problem into another is often useful, and thus having the results of the paper available to the community is valuable.\n\n Other comments: l185: \",w here\" => \", where\" l192: this paper that => this paper assumes that l207: simple example fast-mixing => simple example of a fast-mixing l281: of of => of l311-12: This sentence is nonsensical An novel approach to developing conditions under which finding the (approximate) MLE is tractable. Well-written, though of seemingly limited practical value.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "A key problem in Markov networks is weight learning, which is often defined as finding a setting of weights (attached to the features) such that the likelihood is maximized. Typically, a gradient descent algorithm is used to learn the weights and a key sub-task in this algorithm is computing the gradient. Since the latter is NP-hard in general, MCMC algorithms are often employed in lieu of exact algorithms to compute it.\n\nThe paper derives theoretical results showing that if the weights (parameters) are such that MCMC converges quickly to the stationary distribution on them, then the weight learning algorithm (that uses MCMC to compute the gradient) is a FPRAS. The paper considers two cases: unregularized case (convex case) and ridge-regularized case (non-convex case), and derives separate bounds for each.\n\nThe paper is generally well written with a few typos here and there. I was not able to verify all the proofs, however, they look plausible.\n\nAlthough, the theoretical results look impressive, they are of little to no practical relevance (I like that the authors acknowledge this). I think it is virtually impossible to ensure that the parameters satisfy the constraints required by the theorems. Have the authors given this any thought? Specifically, whether it is possible to enforce this condition artificially (the condition can be seen as a form of regularization and actually using ridge regression in addition to it may yield over-regularized models). If this latter problem can be solved, together with these theoretical results, you have a very strong paper.\n\nOverall, a reasonably well-written paper that formalizes the conditions under which using MCMC for weight learning (with and without regularization) yields a FPRAS. However, this condition is of little to no practical relevance. Non-trivial research problems need to be addressed before the results derived in the paper can yield a practical/accurate algorithm for weight learning.\n\n A reasonably well-written paper that formalizes the conditions under which using MCMC for weight learning (with and without regularization) yields a FPRAS. However, this condition is of little to no practical relevance. Non-trivial research problems need to be addressed before the results derived in the paper can yield a practical/accurate algorithm for weight learning.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}, {"comments": "Comments on Quality:\n\nThe results give rates of convergence of learning algorithms in terms of mixing times. The bounds appear meaningful and useful.\n\n The only significant complaint is that the lower bound (Theorem 7) is very difficult to interpret. An expanded discussion of the theorem and, more generally, the lower bounds applicable to the problem, would be desirable.\n\nComments on Clarity:\n\nThe presentation is clear.\n\nComments on Originality:\n\nBoth the framing of the problem (in terms of the \"fast mixing\" model class) and the theoretical results appear original.\n\n Comments on Significance:\n\nThe significance is unclear to the reviewer. It does seem that the paper fills a gap in the literature on learning intractable models using MCMC, and in this sense it does seem significant.\n\n The submission would be improved by further discussion of the work's potential impact (including future extensions and opportunities to build on it). The submission introduces a novel class of tractable models based on good MCMC-based approximability of expectations. The theoretical results giving convergence rates for learning these models appear novel and interesting, and the submission should therefore be accepted.", "IS_ANNOTATED": false, "IS_META_REVIEW": false}], "histories": []}
