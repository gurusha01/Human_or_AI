{
  "name" : "83adc9225e4deb67d7ce42d58fe5157c.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Structured Densities via Infinite Dimensional Exponential Families",
    "authors" : [ "Siqi Sun", "Mladen Kolar" ],
    "emails" : [ "siqi.sun@ttic.edu", "mkolar@chicagobooth.edu", "jinbo.xu@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17]. In an undirected graphical model, conditional independence assumptions underlying a probability distribution are encoded in the graph structure. Furthermore, the joint probability density function can be factorized according to the cliques of the graph [14]. One of the fundamental problems in the literature is learning the structure of a graphical model given an i.i.d. sample from an unknown distribution. A lot of work has been done under specific parametric assumptions on the unknown distribution. For example, in Gaussian Graphical Models the structure of the graph is encoded by the sparsity pattern of the precision matrix [6, 30]. Similarly, in the context of exponential family graphical models, where the node conditional distribution given all the other nodes is a member of an exponential family, the structure is described by the non-zero coefficients [29]. Most existing approaches to learn the structure of a high-dimensional undirected graphical model are based on minimizing a penalized loss objective, where the loss is usually a log-likelihood or a composite likelihood and the penalty induces sparsity on the resulting parameter vector [see, for example, 6, 12, 18, 22, 24, 29, 30]. In addition to sparsity inducing penalties, methods that use other structural constraints have been proposed. For example, since many real-world networks are scale-free [1], several algorithms are designed specifically to learn structure of such networks\n[5, 19]. Graphs tend to have cluster structure and learning simultaneously the structure and cluster assignment has been investigated [2, 27].\nIn this paper, we focus on learning the structure of a pairwise graphical models without assuming a parametric class of models. The main challenge in estimating nonparametric graphical models is computation of the log normalizing constant. To get around this problem, we propose to use score matching [10, 11] as a divergence, instead of the usual KL divergence, as it does not require evaluation of the log partition function. The probability density function is estimated by minimizing the expected distance between the model score function and the data score function, where the score function is defined as gradient of the corresponding probability density functions. The advantage of this measure is that the normalization constant is canceled out when computing the distance. In order to learn the underlying graph structure, we assume that the logarithm of the density is additive in node-wise and edge-wise potentials and use a sparsity inducing penalty to select non-zero edge potentials. As we will prove later, our procedure will allow us to consistently estimate the underlying graph structure.\nThe rest of paper is organized as follows. We first introduce the notations, background and related work. Then we formulate our model, establish a representer theorem and present a group lasso algorithm to optimize the objective. Next we prove that our estimator is consistent by showing that it can recover the true graph with high probability given sufficient number of samples. Finally the results for simulated data are presented to demonstrate the correctness of our algorithm empirically."
    }, {
      "heading" : "1.1 Notations",
      "text" : "Let [n] denote the set {1, 2, . . . , n}. For a vector θ = (θ1, . . . , θd)T ∈ Rd, let ‖θ‖p = ( ∑ i∈[d] |θi|p) 1 p denote its lp norm. Let column vector vec(D) denote the vectorization of matrix D, cat(a, b) denote the concatenation of two vectors a and b, and mat(aT1 , . . . , a T d ) the matrix with rows given by aT1 , . . . , a T d . For χ ⊆ Rd, let Lp(χ, p0) denote the space of function for which the p-th power of absolute value is p0 integrable; and for f ∈ Lp(χ, p0), let ‖f‖Lp(χ,p0) = ‖f‖p = ( ∫ χ |f |pdx) 1 p denote its Lp norm. Throughout the paper, we denote H (orHi,Hij) as Hilbert space and 〈·, ·〉H, ‖ · ‖H as corresponding inner product and norm. For any operator C : H1 → H2, we use ‖C‖ to denote the usual operator norm, which is defined as\n‖C‖ = inf{a ≥ 0 : ‖Cf‖H2 ≤ a‖f‖H1 for all f ∈ H1};\nand ‖C‖HS to denote its Hilbert-Schmidt norm, which is defined as ‖C‖2HS = ∑ i∈I ‖Cei‖2H2 ,\nwhere ei is an orthonormal basis ofH for an index set I . Also, we useR(C) to denote operator C’s range space. For any f ∈ H1 and g ∈ H2, let f ⊗ g denote their tensor product."
    }, {
      "heading" : "2 Background & Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Learning graphical models in exponential families",
      "text" : "Let x = (x1, x2, ..., xd) be a d-dimensional random vector from a multivariate Gaussian distribution. It is well known that the conditional independency of two variables given all the others is encoded in the zero pattern of its precision matrix Ω, that is, xi and xj are conditionally independent given x−ij if and only if Ωij = 0, where x−ij is the vector of x without xi and xj . A sparse estimate of Ω can be obtained by maximum-likelihood (joint selection) or pseudo-likelihood (neighborhood selection) optimization with an added l1 penalty [6, 22, 30]. Given n independent realizations of x (rows of X ∈ Rn×d), the penalized maximum-likelihood estimate of the precision matrix can be obtained as\nΩ̂λ = arg min Ω 0\ntr(ŜΩ)− log det Ω + λ‖Ω‖1, (1)\nwhere Ŝ = n−1XTX and λ controls the sparsity level of estimated graph.\nThe pseudo-likelihood method estimates the neighborhood of a node a by the non-zeros of the solution to a regularized linear model\nθ̂s = arg min θ\n1 n ‖Xs −X−sθ‖22 + λ‖θ‖1. (2)\nThe estimated neighborhood is then N̂(s) = {a : θsa 6= 0}. Another way to specify a parametric graphical model is by assuming that each node-conditional distributions is a part of the exponential family [29]. Specifically, the conditional distribution of xs given x−s is assumed to be\nP (xs|x−s) = exp( ∑\nt∈N(s)\nθstxsxt + C(xs)−D(x−s, θ)), (3)\nwhereC is the base measure,D is the log-normalization constant andN(s) is the neighborhood a the node s. Similar to (2), the neighborhood of each node can be estimated by minimizing the negative log-likelihood with l1 penalty on θ. The optimization is tractable when the normalization constant D can be easily computed based on the model assumption. For example, under Poisson graphical model assumptions for count data, the normalization constant is− exp( ∑ t∈N(s) θstxt). When using the neighborhood estimation, the graph can be estimated as the union of the neighborhoods of each node, which leads to consistent graph estimation [22, 29]."
    }, {
      "heading" : "2.2 Generalized Exponential Family and RKHS",
      "text" : "We say H is a RKHS associated with kernel k : χ × χ → R+ if and only if for each x ∈ χ, the following two conditions are satisfied: (1) k(·, x) ∈ H and (2) it has reproducing properties such that f(x) = 〈f, k(·, x)〉H for all f(·) ∈ H, where k is a symmetric and positive semidefinite function. Denote the RKHSH with kernel k asH(k). For any f ∈ H(k), there exists a set of xi and αi, such that f(·) = ∑∞ i=1 αik(·, xi). Similarly\nfor any g ∈ H(k), g(·) = ∑∞ j=1 βjk(·, yj), the inner product of f and g is defined as 〈f, g〉H =∑∞\ni,j=1 αiβjk(xi, yj). Therefore the norm of f simply is ‖f‖H = √∑\ni,j αiαjk(xi, xj). The summation is guaranteed to be larger than or equal to zero because the kernel k is positive semidefinite.\nWe consider the exponential family in infinite dimensions [4], where\nP = {pf (x) = ef(x)−A(f)q0(x), x ∈ χ; f ∈ F} and the function space F is defined as\nF = {f ∈ H(k) : A(f) = log ∫ χ ef(x)q0(x)dx <∞},\nwhere q0(x) is the base measure, A(f) is a generalized normalization constant such that pf (x) is a valid probability density function, and H is a RKHS [3] associated with kernel k. To see it as a generalization of the exponential family, we show some examples that can generate useful finite dimension exponential families:\n• Normal: χ = R, k(x, y) = xy + x2y2\n• Poisson: χ = N ∪ {0}, k(x, y) = xy • Exponential: χ = R+, k(x, y) = xy.\nFor more detailed information, please refer to [4].\nWhen learning structure of a graphical model, we will further impose structural conditions onH(k) in order ensure that F consists of additive functions."
    }, {
      "heading" : "2.3 Score Matching",
      "text" : "Score matching is a convenient procedure that allows for estimating a probability density without computing the normalizing constant [10, 11]. It is based on minimizing Fisher divergence\nJ(p‖p0) = 1\n2\n∫ p(x) ∥∥∥∥∂ log p(x)∂x − ∂ log p0(x)∂x ∥∥∥∥2\n2\ndx, (4)\nwhere ∂ log p(x)∂x = ( ∂ log p(x) ∂x1 , . . . , ∂ log p(x)∂xd ) is the score function. Observe that for p(x, θ) = 1 Z(θ)q(x, θ) the normalization constant Z(θ) cancels out in the gradient computation, which makes the divergence independent of Z(θ). Since the score matching objective involves the unknown oracle probability density function p0, it is typically not computable. However, under some mild conditions which we will discuss in METHODS section, (4) can be rewritten as\nJ(p‖p0) = ∫ p0(x) ∑ i∈[d] 1 2 ( ∂ log p(x) ∂xi )2 + ∂2 log p(x) ∂x2i dx. (5)\nAfter substituting the expectation with an empirical average, we get\nĴ(p‖p0) = 1\nn ∑ a∈[n] ∑ i∈[d] 1 2 ( ∂ log p(Xa) ∂xi )2 + ∂2 log p(Xa) ∂x2i . (6)\nCompared to maximum likelihood estimation, minimizing Ĵ(p‖p0) is computationally tractable. While we will be able to estimate p0 only up to a scale factor, this will be sufficient for the purpose of graph structure estimation."
    }, {
      "heading" : "3 Methods",
      "text" : ""
    }, {
      "heading" : "3.1 Model Formulation and Assumptions",
      "text" : "We assume that the true probability density function p0 is in P . Furthermore, for simplicity we assume that\nlog p0(x) = f(x) = ∑ i≤j\n(i,j)∈S\nf0,ij(xi, xj),\nwhere f0,ii(xi, xi) is a node potential and f0,ij(xi, xj) is an edge potential. The set S denotes the edge set of the graph. Extensions to models where potentials are defined over larger cliques are possible. We further assume that f0,ij ∈ Hij(kij), where Hij is a RKHS with kernel kij . To simplify the notation, we use f0,ij(x) or kij(·, x) to denote f0,ij(xi, xj) and kij(·, (xi, xj)). If the context is clear, we drop the subscript for norm or inner product. Define\nH(S) = {f = ∑\n(i,j)∈S\nfij |fij ∈ Hij} (7)\nas a set of functions that decompose as sum of bivariate functions on edge set S. Note that H(S) is also (a subset of) a RKHS with the norm ‖f‖2H(S) = ∑ (i,j)∈S ‖fij‖2Hij and kernel\nk = ∑\n(i,j)∈S kij . Let Ω(f) = ‖f‖H,1 = ∑ i≤j ‖fij‖Hij . For any edge set S (not necessarily the true edge set), we\ndenote ΩS(fS) = ∑ s∈S ‖fs‖Hs as the norm Ω reduced to S. Similarly, denote its dual norm as Ω∗S [fS ] = maxΩS(gS)≤1〈fS , gS〉 [25]. Under the assumption that the unknown f0 is additive, the loss function becomes\nJ(f) = 1\n2\n∫ p0(x) ∑ i∈[d] (∂f(x) ∂xi − ∂f0(x) ∂xi )2 dx\n= 1\n2 ∑ i∈[d] ∑ j,j′∈[d] 〈fij − f0,ij , ∫ p0(x) ∂kij(·, (xi, xj)) ∂xi ⊗ ∂kij ′(·, (xi, xj′)) ∂xi dx(fij′ − f0,ij′)〉\n= 1\n2 ∑ i∈[d] ∑ j,j′∈[d] 〈fij − f0,ij , Cijij′(fij′ − f0,ij′)〉.\nIntuitively, C can be viewed as a d2 matrix, and the operator at position (ij, ij′) is Cij,ij′ . For general (ij, i′j′), i 6= i′ the corresponding operator simply is 0. Define CSS′ as∫\np0(x) ∑\n(i,j)∈S,(i′,j′)∈S′\n∂kij(·, (xi, xj)) ∂xi ⊗ ∂ki ′j′(·, (xi′ , xj′)) ∂xi dx,\nwhich intuitively can be treated as a sub matrix of C with rows S and columns S′. We will use this notation intensively in the main theorem and its proof.\nFollowing [26], we make the following assumptions.\nA1. Each kij is twice differentiable on χ× χ. A2. For any i and x̃j ∈ χj = [aj , bj ], we assume that\nlim xi→a+i or b − i\n∂2kij(x, y)\n∂xi∂yi |y=x p20(x) = 0,\nwhere x = (xi, x̃j) and ai, bi could be −∞ or∞. A3. This condition ensures that J(p‖p0) <∞ for any p ∈ P [for more details see 26]:\n‖∂kij(·, x) ∂xi ‖Hij ∈ L2(χ, p0), ‖ ∂2kij(·, x) ∂x2i ‖Hij ∈ L2(χ, p0).\nA4. The operator CSS , is compact and the smallest eigenvalue ωmin = λmin(CSS) > 0.\nA5. Ω∗Sc [CScSC −1 SS ] ≤ 1− η, where η > 0.\nA6. f0 ∈ R(C), which means there exists γ ∈ H, such that f0 = Cγ. f0 is the oracle function. We will discuss the definition of operator C and Ω∗ in section 4. Compared with [29], A4 can be interpreted as the dependency condition and the A5 is the incoherence condition, which is a standard condition for structure learning in high dimensional statistical estimators."
    }, {
      "heading" : "3.2 Estimation Procedure",
      "text" : "We estimate f by minimizing the following penalized score matching objective\nmin f L̂µ(f) = Ĵ(f) +\nµ 2 ‖f‖H,1\ns.t. fij ∈ Hij , (8) where Ĵ(f) is given in (6). The norm ‖f‖H,1 = ∑ i≤j ‖fij‖Hij is used as a sparsity inducing\npenalty. A simplified form of Ĵ(f) is given below that will lead to efficient algorithm for solving (8).\nThe following theorem states that the score matching objective can be written as a penalized quadratic function on f .\nTheorem 3.1 (i) The score matching objective can be represented as\nLµ(f) = 1\n2 〈f − f0, C(f − f0)〉+\nµ 2 ‖f‖H,1 (9)\nwhere C = ∫ p0(x) ∑ i∈[d] ∂k(·,x) ∂xi ⊗ ∂k(·,x)∂xi dx is a trace operator.\n(ii) Given observed data Xn×d, the empirical estimation of Lµ is\nL̂µ(f) = 1\n2 〈f, Ĉf〉+ ∑ i≤j 〈fij ,−ξ̂ij〉+ µ 2 ‖f‖H,1 + const (10)\nwhere Ĉ = 1n ∑ a∈[n] ∑ i∈[d] ∂k(·,Xa) ∂xi ⊗ ∂k(·,Xa)∂xi and ξ̂ij = 1 n ∑ a∈[n] ∂2kij(·,(Xai,Xaj)) ∂x2i + ∂2kij(·,(Xai,Xaj)) ∂x2j if i 6= j, or ξ̂ij = 1n ∑ a∈n ∂2kij(·,(Xai,Xaj)) ∂x2i otherwise.\nPlease refer to our supplementary material for detailed proof 1.\nThe above theorem still requires us to minimize over F . Our next results shows that the solution is finite dimensional. That is, we establish a representer theorem for our problem.\n1Please visit ttic.uchicago.edu/∼siqi for supplementary material and code.\nTheorem 3.2 (i) The solution to (10) can be represented as\nf̂ij(·) = ∑ b∈[n] βbij ∂kij(·, (Xbi, Xbj)) ∂xi + βbji ∂kij(·, (Xbi, Xbj)) ∂xj + αij ξ̂ij , (11)\nwhere i ≤ j. (ii) Minimizing (10) is equivalent to minimizing the following quadratic function:\n1\n2n ∑ ai (∑ bj (βbijG ab ij11 + βbjiG ab ij12) + ∑ j αijh 1a ij )2 + ∑ i≤j ∑ b (βbijh 1b ij + βbjih 2b ij ) + ∑ i≤j αij‖ξ̂ij‖2 + µ 2 ‖f‖H,1\n= 1\n2n ∑ ai (DTai · θ)2 + Etθ + µ 2 ∑ i≤j √ θtijFijθij (12)\nwhere Gabijrs = ∂2kij(Xa,Xb) ∂xr∂ys , hrbij = 〈 ∂kij(·,Xb) ∂xr\n, ξ̂ij〉 are constant that only depends on X , θ = cat(vec(α), vec(β)) is the vector parameter and θij = cat(αij , vec(β·ij)) is a group of parameters. Dai, E and F are corresponding constant vectors and matrices based on G, h and the order of parameters. Then the above problem can be solved by group lasso [7, 21].\nThe first part of theorem states our representer theorem, and the second part is obtained by plugging in (11) to (10). See supplementary material for a detailed proof. Theorem 3.2 provides us with an efficient way to minimize (8), as it reduced the optimization to a group lasso problem for which many efficient solvers exist.\nLet f̂µ = arg minf∈H L̂µ(f) denote the solution to (12). We can estimate the graph as follows:\nŜµ = {(i, j) : ‖f̂µij‖ 6= 0}, (13)\nThat is, the graph is encoded in the sparsity pattern of f̂µ."
    }, {
      "heading" : "4 Statistical Guarantees",
      "text" : "In this section we study statistical properties of the proposed estimator (13). Let S denote the true edge set and Sc its complement. We prove that Ŝµ recovers S with high probability when the sample size n is sufficiently large.\nDenote D = mat(DT11, . . . , D T ai, . . . , D T nd). We will need the following result on the estimated operator Ĉ,\nProposition 4.1 (Lemma 5 in [8] or Theorem 5 in [26] ) (Properties of C)\n1. ‖Ĉ − C‖HS = Op0(n− 1 2 )\n2. ‖(C +µL)−1‖ ≤ 1µmin diag(L) , ‖C(C +µL) −1‖ ≤ 1, where µ > 0 and L is diagonal with\npositive constants.\nThe following result gives first order optimality conditions for the optimization problem (8).\nProposition 4.2 (Optimality Condition) Ĵ(f) + µ2 Ω(f) 2 achieves optimality when the following two conditions are satisfied:\n(1) ∇fs Ĵ(f) + µΩ(f) fs\n‖fs‖Hs = 0 ∀s ∈ S\n(2) Ω∗Sc [∇fSc Ĵ(f)] ≤ µΩ(f).\nWith these preliminary results, we have the following main results.\nTheorem 4.3 Assume that conditions A1-A7 are satisfied. The regularization parameter µ is selected at the order of n− 1 4 and satisfies µ ≤ ηκminωmin\n4(1−η)κmax √ |S|+ η5 , where κmin = mins∈S ‖f∗s ‖ > 0\nand κmax = maxs∈S ‖f∗s ‖ > 0. Then P (Ŝµ = S)→ 1.\nProof Idea: The theorem above is the main theoretical guarantee for our score matching estimator. We use the “witness” proof framework inspired by [23, 29]. Let f∗ denote the true density function and p∗ the probability density function. We first construct a solution f̂S on true edge set S as\nf̂S = min fSc=0\nĴ(f) + µ 2 ( ∑\n(i,j)∈S\n‖fij‖)2 (14)\nand set f̂Sc as zero. Using Proposition 4.1, we prove that ‖f̂S − f∗S‖ = Op(n− 1 4 ). Then we compute the subgradient on Sc and prove that its dual norm is upper bounded by µΩ(f) by using assumptions A4, A5 and A6. Therefore we construct a solution that satisfied the optimality condition and converges in probability to the true graph. Refer to supplementary material for detailed proof."
    }, {
      "heading" : "5 Experiments",
      "text" : "We illustrate performance of our method on two simulations. In our experiments, we use the same kernel defined as follows:\nk(x, y) = exp(−‖x− y‖ 2 2\n2σ2 ) + r(xT y + c)2, (15)\nthat is, the summation of a Gaussian kernel and a polynomial kernel. We set σ2 = 1.5, r = 0.1 and c = 0.5 for all the simulations.\nWe report the true positive rate vs false positive rate (ROC) curve to measure the performance of different procedures. Let S be the true edge set, and let Ŝµ be the estimated graph. The true positive rate is defined as TPRµ = |S=1 and Ŝµ=1| |S=1| , and false positive rate is FPRµ = |Ŝµ=1 and S=0| |S=0| , where | · | is the cardinality of the set. The curve is then plotted based on 100 uniformly-sampled regularization parameters and based on 20 independent runs.\nIn the first simulation, we apply our algorithm to data sampled from a simple chain graph-based Gaussian model (see Figure 1 for detail), and compare its performance with glasso [6]. We use the same sampling method as in [31] to generate the data: we set Ωs = 0.4 for s ∈ S and its diagonal to a constant such that Ω is positive definite. We set the dimension d to 25 and change the sample size n ∈ {20, 40, 60, 80, 100} data points. Except for the low sample size case (n = 20), the performance of our method is comparable with glasso, without utilizing the fact that the underlying distribution is of a particular parametric form. Intuitively, to capture the graph structure, the proposed nonparametric method requires more data because of much weaker assumptions.\nTo further show the strength of our algorithm, we test it on a nonparanormal (NPN) distribution ([18]). A random vector x = (x1, . . . , xp) has a nonparanormal distribution if there exist functions (f1, . . . , fp) such that (f1(x1), . . . , fd(xd)) ∼ N(µ,Σ). When f is monotone and differentiable, the probability density function is given by\nP (x) = 1\n(2π) p 2 |Σ| 12 exp{−1 2 (f(x)− µ)TΣ−1(f(x)− µ)} ∏ j |f ′j |.\nHere the graph structure is still encoded in the sparsity pattern of Ω = Σ−1, that is, xi⊥xj |x−i,j if and only if Ωij = 0 [18].\nIn our experiments we use the “Symmetric Power Transformation” [18], that is,\nfj(zj) = σj( g0(zj − µj)√∫\ng20(t− µj)φ( t−µj σj\n)dt ) + µj ,\nwhere g0(t) = sign(t)|t|α, to transform data. For comparison with graph lasso, we first use a truncation method to Gaussianize the data, and then apply graphical lasso to the transformed data. See [18, 31] for details. From figure 2, without knowing the underlying data distribution, the score matching estimator outperforms glasso, and show similar results to nonparanormal when the sample size is large."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper, we have proposed a new procedure for learning the structure of a nonparametric graphical model. Our procedure is based on minimizing a penalized score matching objective, which can be performed efficiently using existing group lasso solvers. Particularly appealing aspect of our approach is that it does not require computing the normalization constant. Therefore, our procedure can be applied to a very broad family of infinite dimensional exponential families. We have established that the procedure provably recovers the true underlying graphical structure with highprobability under mild conditions. In the future, we plan to investigate more efficient algorithms for solving (10), since it is often the case that Ĉ is well structured and can be efficiently approximated."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors are grateful to the financial support from National Institutes of Health R01GM0897532, National Science Foundation CAREER award CCF-1149811 and IBM Corporation Faculty Research Fund at the University of Chicago Booth School of Business. This work was completed in part with resources provided by the University of Chicago Research Computing Center."
    } ],
    "references" : [ {
      "title" : "Scale-free networks in cell biology",
      "author" : [ "R. Albert" ],
      "venue" : "Journal of cell science, 118(21):4947–4957",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "et al",
      "author" : [ "C. Ambroise", "J. Chiquet", "C. Matias" ],
      "venue" : "Inferring sparse gaussian graphical models with latent structure. Electronic Journal of Statistics, 3:205–238",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Theory of reproducing kernels",
      "author" : [ "N. Aronszajn" ],
      "venue" : "Transactions of the American mathematical society, pages 337–404",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1950
    }, {
      "title" : "Kernel methods and the exponential family",
      "author" : [ "S. Canu", "A. Smola" ],
      "venue" : "Neurocomputing, 69(7):714–720",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A convex formulation for learning scale-free networks via submodular relaxation",
      "author" : [ "A. Defazio", "T.S. Caetano" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1250–1258",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Sparse inverse covariance estimation with the graphical lasso",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "Biostatistics, 9(3):432–441",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A note on the group lasso and a sparse group lasso",
      "author" : [ "J. Friedman", "T. Hastie", "R. Tibshirani" ],
      "venue" : "arXiv preprint arXiv:1001.0736",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Statistical consistency of kernel canonical correlation analysis",
      "author" : [ "K. Fukumizu", "F.R. Bach", "A. Gretton" ],
      "venue" : "The Journal of Machine Learning Research, 8:361–383",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Markov random field image models and their applications to computer vision",
      "author" : [ "S. Geman", "C. Graffigne" ],
      "venue" : "Proceedings of the International Congress of Mathematicians, volume 1, page 2",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Estimation of non-normalized statistical models by score matching",
      "author" : [ "A. Hyvärinen" ],
      "venue" : "Journal of Machine Learning Research, pages 695–709",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Some extensions of score matching",
      "author" : [ "A. Hyvärinen" ],
      "venue" : "Computational statistics & data analysis, 51(5):2499– 2512",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An effective method for high-dimensional log-density anova estimation",
      "author" : [ "Y. Jeon", "Y. Lin" ],
      "venue" : "with application to nonparametric graphical model building. Statistica Sinica, 16(2):353",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "et al",
      "author" : [ "R. Kindermann", "J.L. Snell" ],
      "venue" : "Markov random fields and their applications, volume 1. American Mathematical Society Providence, RI",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Probabilistic graphical models: principles and techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : "MIT press",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "R",
      "author" : [ "Y.A. Kourmpetis", "A.D. Van Dijk", "M.C. Bink" ],
      "venue" : "C. van Ham, and C. J. ter Braak. Bayesian markov random field analysis for protein function prediction based on network data. PloS one, 5(2):e9293",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "J. Lafferty", "A. McCallum", "F.C. Pereira" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2001
    }, {
      "title" : "Markov random field modeling in Image Analysis",
      "author" : [ "S.Z. Li" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "The nonparanormal: Semiparametric estimation of high dimensional undirected graphs",
      "author" : [ "H. Liu", "J. Lafferty", "L. Wasserman" ],
      "venue" : "The Journal of Machine Learning Research, 10:2295–2328",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Learning scale free networks by reweighted l1 regularization",
      "author" : [ "Q. Liu", "A.T. Ihler" ],
      "venue" : "International Conference on Artificial Intelligence and Statistics, pages 40–48",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Foundations of statistical natural language processing",
      "author" : [ "C.D. Manning", "H. Schütze" ],
      "venue" : "MIT press",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "S",
      "author" : [ "L. Meier" ],
      "venue" : "Van De Geer, and P. Bühlmann. The group lasso for logistic regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(1):53–71",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "High-dimensional graphs and variable selection with the lasso",
      "author" : [ "N. Meinshausen", "P. Bühlmann" ],
      "venue" : "The Annals of Statistics, pages 1436–1462",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "High-dimensional graphical model selection using l1regularized logistic regression",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "J. Lafferty" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "et al",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty" ],
      "venue" : "High-dimensional ising model selection using 1-regularized logistic regression. The Annals of Statistics, 38(3):1287–1319",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convex analysis",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : "Number 28. Princeton university press",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Density estimation in infinite dimensional exponential families",
      "author" : [ "B. Sriperumbudur", "K. Fukumizu", "R. Kumar", "A. Gretton", "A. Hyvärinen" ],
      "venue" : "arXiv preprint arXiv:1312.3516",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Inferring block structure of graphical models in exponential families",
      "author" : [ "S. Sun", "H. Wang", "J. Xu" ],
      "venue" : "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 939–947",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "A markov random field model for network-based analysis of genomic data",
      "author" : [ "Z. Wei", "H. Li" ],
      "venue" : "Bioinformatics, 23(12):1537–1544",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Graphical models via generalized linear models",
      "author" : [ "E. Yang", "G. Allen", "Z. Liu", "P.K. Ravikumar" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1358–1366",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Model selection and estimation in the gaussian graphical model",
      "author" : [ "M. Yuan", "Y. Lin" ],
      "venue" : "Biometrika, 94(1):19–35",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The huge package for high-dimensional undirected graph estimation in r",
      "author" : [ "T. Zhao", "H. Liu", "K. Roeder", "J. Lafferty", "L. Wasserman" ],
      "venue" : "The Journal of Machine Learning Research, 13(1):1059–1062",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "We consider probabilities that are members of an infinite dimensional exponential family [4], which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its kernel k.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "In order to avoid this issue, our procedure minimizes the penalized score matching objective [10, 11].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "In order to avoid this issue, our procedure minimizes the penalized score matching objective [10, 11].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 27,
      "context" : "Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].",
      "startOffset" : 146,
      "endOffset" : 154
    }, {
      "referenceID" : 15,
      "context" : "Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].",
      "startOffset" : 187,
      "endOffset" : 195
    }, {
      "referenceID" : 19,
      "context" : "Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].",
      "startOffset" : 187,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].",
      "startOffset" : 216,
      "endOffset" : 223
    }, {
      "referenceID" : 16,
      "context" : "Undirected graphical models, or Markov random fields [13], have been extensively studied and applied in fields ranging from computational biology [15, 28], to natural language processing [16, 20] and computer vision [9, 17].",
      "startOffset" : 216,
      "endOffset" : 223
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, the joint probability density function can be factorized according to the cliques of the graph [14].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "For example, in Gaussian Graphical Models the structure of the graph is encoded by the sparsity pattern of the precision matrix [6, 30].",
      "startOffset" : 128,
      "endOffset" : 135
    }, {
      "referenceID" : 29,
      "context" : "For example, in Gaussian Graphical Models the structure of the graph is encoded by the sparsity pattern of the precision matrix [6, 30].",
      "startOffset" : 128,
      "endOffset" : 135
    }, {
      "referenceID" : 28,
      "context" : "Similarly, in the context of exponential family graphical models, where the node conditional distribution given all the other nodes is a member of an exponential family, the structure is described by the non-zero coefficients [29].",
      "startOffset" : 226,
      "endOffset" : 230
    }, {
      "referenceID" : 0,
      "context" : "For example, since many real-world networks are scale-free [1], several algorithms are designed specifically to learn structure of such networks",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Graphs tend to have cluster structure and learning simultaneously the structure and cluster assignment has been investigated [2, 27].",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "Graphs tend to have cluster structure and learning simultaneously the structure and cluster assignment has been investigated [2, 27].",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "To get around this problem, we propose to use score matching [10, 11] as a divergence, instead of the usual KL divergence, as it does not require evaluation of the log partition function.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "To get around this problem, we propose to use score matching [10, 11] as a divergence, instead of the usual KL divergence, as it does not require evaluation of the log partition function.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 5,
      "context" : "A sparse estimate of Ω can be obtained by maximum-likelihood (joint selection) or pseudo-likelihood (neighborhood selection) optimization with an added l1 penalty [6, 22, 30].",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 21,
      "context" : "A sparse estimate of Ω can be obtained by maximum-likelihood (joint selection) or pseudo-likelihood (neighborhood selection) optimization with an added l1 penalty [6, 22, 30].",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 29,
      "context" : "A sparse estimate of Ω can be obtained by maximum-likelihood (joint selection) or pseudo-likelihood (neighborhood selection) optimization with an added l1 penalty [6, 22, 30].",
      "startOffset" : 163,
      "endOffset" : 174
    }, {
      "referenceID" : 28,
      "context" : "Another way to specify a parametric graphical model is by assuming that each node-conditional distributions is a part of the exponential family [29].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 21,
      "context" : "When using the neighborhood estimation, the graph can be estimated as the union of the neighborhoods of each node, which leads to consistent graph estimation [22, 29].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 28,
      "context" : "When using the neighborhood estimation, the graph can be estimated as the union of the neighborhoods of each node, which leads to consistent graph estimation [22, 29].",
      "startOffset" : 158,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : "We consider the exponential family in infinite dimensions [4], where P = {pf (x) = eq0(x), x ∈ χ; f ∈ F} and the function space F is defined as F = {f ∈ H(k) : A(f) = log ∫",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "χ eq0(x)dx <∞}, where q0(x) is the base measure, A(f) is a generalized normalization constant such that pf (x) is a valid probability density function, and H is a RKHS [3] associated with kernel k.",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "For more detailed information, please refer to [4].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 9,
      "context" : "3 Score Matching Score matching is a convenient procedure that allows for estimating a probability density without computing the normalizing constant [10, 11].",
      "startOffset" : 150,
      "endOffset" : 158
    }, {
      "referenceID" : 10,
      "context" : "3 Score Matching Score matching is a convenient procedure that allows for estimating a probability density without computing the normalizing constant [10, 11].",
      "startOffset" : 150,
      "endOffset" : 158
    }, {
      "referenceID" : 24,
      "context" : "Similarly, denote its dual norm as ΩS [fS ] = maxΩS(gS)≤1〈fS , gS〉 [25].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "Following [26], we make the following assumptions.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 28,
      "context" : "Compared with [29], A4 can be interpreted as the dependency condition and the A5 is the incoherence condition, which is a standard condition for structure learning in high dimensional statistical estimators.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "Then the above problem can be solved by group lasso [7, 21].",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "Then the above problem can be solved by group lasso [7, 21].",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "1 (Lemma 5 in [8] or Theorem 5 in [26] ) (Properties of C) 1.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 25,
      "context" : "1 (Lemma 5 in [8] or Theorem 5 in [26] ) (Properties of C) 1.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "We use the “witness” proof framework inspired by [23, 29].",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 28,
      "context" : "We use the “witness” proof framework inspired by [23, 29].",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "In the first simulation, we apply our algorithm to data sampled from a simple chain graph-based Gaussian model (see Figure 1 for detail), and compare its performance with glasso [6].",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 30,
      "context" : "We use the same sampling method as in [31] to generate the data: we set Ωs = 0.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "To further show the strength of our algorithm, we test it on a nonparanormal (NPN) distribution ([18]).",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Here the graph structure is still encoded in the sparsity pattern of Ω = Σ−1, that is, xi⊥xj |x−i,j if and only if Ωij = 0 [18].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "In our experiments we use the “Symmetric Power Transformation” [18], that is,",
      "startOffset" : 63,
      "endOffset" : 67
    } ],
    "year" : 2015,
    "abstractText" : "Learning the structure of a probabilistic graphical models is a well studied problem in the machine learning community due to its importance in many applications. Current approaches are mainly focused on learning the structure under restrictive parametric assumptions, which limits the applicability of these methods. In this paper, we study the problem of estimating the structure of a probabilistic graphical model without assuming a particular parametric model. We consider probabilities that are members of an infinite dimensional exponential family [4], which is parametrized by a reproducing kernel Hilbert space (RKHS) H and its kernel k. One difficulty in learning nonparametric densities is the evaluation of the normalizing constant. In order to avoid this issue, our procedure minimizes the penalized score matching objective [10, 11]. We show how to efficiently minimize the proposed objective using existing group lasso solvers. Furthermore, we prove that our procedure recovers the graph structure with high-probability under mild conditions. Simulation studies illustrate ability of our procedure to recover the true graph structure without the knowledge of the data generating process.",
    "creator" : null
  }
}