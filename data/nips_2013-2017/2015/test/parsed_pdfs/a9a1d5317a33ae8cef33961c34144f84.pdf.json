{
  "name" : "a9a1d5317a33ae8cef33961c34144f84.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Large-scale probabilistic predictors with and without guarantees of validity",
    "authors" : [ "Vladimir Vovk", "Ivan Petej", "Valentina Fedorova" ],
    "emails" : [ "volodya.vovk@gmail.com", "ivan.petej@gmail.com", "alushaf@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Prediction algorithms studied in this paper belong to the class of Venn–Abers predictors, introduced in [1]. They are based on the method of isotonic regression [2] and prompted by the observation that when applied in machine learning the method of isotonic regression often produces miscalibrated probability predictions (see, e.g., [3, 4]); it has also been reported ([5], Section 1) that isotonic regression is more prone to overfitting than Platt’s scaling [6] when data is scarce. The advantage of Venn–Abers predictors is that they are a special case of Venn predictors ([7], Chapter 6), and so ([7], Theorem 6.6) are always well-calibrated (cf. Proposition 1 below). They can be considered to be a regularized version of the procedure used by [8], which helps them resist overfitting.\nThe main desiderata for Venn (and related conformal, [7], Chapter 2) predictors are validity, predictive efficiency, and computational efficiency. This paper introduces two computationally efficient versions of Venn–Abers predictors, which we refer to as inductive Venn–Abers predictors (IVAPs) and cross-Venn–Abers predictors (CVAPs). The ways in which they achieve the three desiderata are:\n• Validity (in the form of perfect calibration) is satisfied by IVAPs automatically, and the experimental results reported in this paper suggest that it is inherited by CVAPs.\n• Predictive efficiency is determined by the predictive efficiency of the underlying learning algorithms (so that the full arsenal of methods of modern machine learning can be brought to bear on the prediction problem at hand).\n• Computational efficiency is, again, determined by the computational efficiency of the underlying algorithm; the computational overhead of extracting probabilistic predictions consists of sorting (which takes time O(n log n), where n is the number of observations) and other computations taking time O(n).\nAn advantage of Venn prediction over conformal prediction, which also enjoys validity guarantees, is that Venn predictors output probabilities rather than p-values, and probabilities, in the spirit of Bayesian decision theory, can be easily combined with utilities to produce optimal decisions.\nIn Sections 2 and 3 we discuss IVAPs and CVAPs, respectively. Section 4 is devoted to minimax ways of merging imprecise probabilities into precise probabilities and thus making IVAPs and CVAPs precise probabilistic predictors.\nIn this paper we concentrate on binary classification problems, in which the objects to be classified are labelled as 0 or 1. Most of machine learning algorithms are scoring algorithms, in that they output a real-valued score for each test object, which is then compared with a threshold to arrive at a categorical prediction, 0 or 1. As precise probabilistic predictors, IVAPs and CVAPs are ways of converting the scores for test objects into numbers in the range [0, 1] that can serve as probabilities, or calibrating the scores. In Section 5 we briefly discuss two existing calibration methods, Platt’s [6] and the method [8] based on isotonic regression. Section 6 is devoted to experimental comparisons and shows that CVAPs consistently outperform the two existing methods (more extensive experimental studies can be found in [9])."
    }, {
      "heading" : "2 Inductive Venn–Abers predictors (IVAPs)",
      "text" : "In this paper we consider data sequences (usually loosely referred to as sets) consisting of observations z = (x, y), each observation consisting of an object x and a label y ∈ {0, 1}; we only consider binary labels. We are given a training set whose size will be denoted l.\nThis section introduces inductive Venn–Abers predictors. Our main concern is how to implement them efficiently, but as functions, an IVAP is defined in terms of a scoring algorithm (see the last paragraph of the previous section) as follows:\n• Divide the training set of size l into two subsets, the proper training set of size m and the calibration set of size k, so that l = m+ k.\n• Train the scoring algorithm on the proper training set. • Find the scores s1, . . . , sk of the calibration objects x1, . . . , xk. • When a new test object x arrives, compute its score s. Fit isotonic regression\nto (s1, y1), . . . , (sk, yk), (s, 0) obtaining a function f0. Fit isotonic regression to (s1, y1), . . . , (sk, yk), (s, 1) obtaining a function f1. The multiprobability prediction for the label y of x is the pair (p0, p1) := (f0(s), f1(s)) (intuitively, the prediction is that the probability that y = 1 is either f0(s) or f1(s)).\nNotice that the multiprobability prediction (p0, p1) output by an IVAP always satisfies p0 < p1, and so p0 and p1 can be interpreted as the lower and upper probabilities, respectively; in practice, they are close to each other for large training sets.\nFirst we state formally the property of validity of IVAPs (adapting the approach of [1] to IVAPs). A random variable P taking values in [0, 1] is perfectly calibrated (as a predictor) for a random variable Y taking values in {0, 1} if E(Y | P ) = P a.s. A selector is a random variable taking values in {0, 1}. As a general rule, in this paper random variables are denoted by capital letters (e.g., X are random objects and Y are random labels).\nProposition 1. Let (P0, P1) be an IVAP’s prediction for X based on a training sequence (X1, Y1), . . . , (Xl, Yl). There is a selector S such that PS is perfectly calibrated for Y provided the random observations (X1, Y1), . . . , (Xl, Yl), (X,Y ) are i.i.d.\nOur next proposition concerns the computational efficiency of IVAPs; Proposition 1 will be proved later in this section while Proposition 2 is proved in [9].\nProposition 2. Given the scores s1, . . . , sk of the calibration objects, the prediction rule for computing the IVAP’s predictions can be computed in time O(k log k) and space O(k). Its application to each test object takes time O(log k). Given the sorted scores of the calibration objects, the prediction rule can be computed in time and space O(k).\nProofs of both statements rely on the geometric representation of isotonic regression as the slope of the GCM (greatest convex minorant) of the CSD (cumulative sum diagram): see [10], pages 9–13 (especially Theorem 1.1). To make our exposition more self-contained, we define both GCM and CSD below.\nFirst we explain how to fit isotonic regression to (s1, y1), . . . , (sk, yk) (without necessarily assuming that si are the calibration scores and yi are the calibration labels, which will be needed to cover the use of isotonic regression in IVAPs). We start from sorting all scores s1, . . . , sk in the increasing order and removing the duplicates. (This is the most computationally expensive step in our calibration procedure, O(k log k) in the worst case.) Let k′ ≤ k be the number of distinct elements among s1, . . . , sk, i.e., the cardinality of the set {s1, . . . , sk}. Define s′j , j = 1, . . . , k′, to be the jth smallest element of {s1, . . . , sk}, so that s′1 < s′2 < · · · < s′k′ . Define wj :=\n∣∣{i = 1, . . . , k : si = s′j}∣∣ to be the number of times s′j occurs among s1, . . . , sk. Finally, define\ny′j := 1\nwj ∑ i=1,...,k:si=s′j yi\nto be the average label corresponding to si = s′j .\nThe CSD of (s1, y1), . . . , (sk, yk) is the set of points\nPi :=  i∑ j=1 wj , i∑ j=1 y′jwj  , i = 0, 1, . . . , k′; in particular, P0 = (0, 0). The GCM is the greatest convex minorant of the CSD. The value at s′i, i = 1, . . . , k′, of the isotonic regression fitted to (s1, y1), . . . , (sk, yk) is defined to be the slope of the GCM between ∑i−1 j=1 wj and ∑i j=1 wj ; the values at other s are somewhat arbitrary (namely, the value at s ∈ (s′i, s′i+1) can be set to anything between the left and right slopes of the GCM at∑i j=1 wj) but are never needed in this paper (unlike in the standard use of isotonic regression in machine learning, [8]): e.g., f1(s) is the value of the isotonic regression fitted to a sequence that already contains (s, 1).\nProof of Proposition 1. Set S := Y . The statement of the proposition even holds conditionally on knowing the values of (X1, Y1), . . . , (Xm, Ym) and the multiset *(Xm+1, Ym+1), . . . , (Xl, Yl), (X,Y )+; this knowledge allows us to compute the scores *s1, . . . , sk, s+ of the calibration objects Xm+1, . . . , Xl and the test object X . The only remaining randomness is over the equiprobable permutations of (Xm+1, Ym+1), . . . , (Xl, Yl), (X,Y ); in particular, (s, Y ) is drawn randomly from the multiset *(s1, Ym+1), . . . , (sk, Yl), (s, Y )+. It remains to notice that, according to the GCM construction, the average label of the calibration and test observations corresponding to a given value of PS is equal to PS .\nThe idea behind computing the pair (f0(s), f1(s)) efficiently is to pre-compute two vectors F 0 and F 1 storing f0(s) and f1(s), respectively, for all possible values of s. Let k′ and s′i be as defined above in the case where s1, . . . , sk are the calibration scores and y1, . . . , yk are the corresponding labels. The vectors F 0 and F 1 are of length k′, and for all i = 1, . . . , k′ and both ∈ {0, 1}, F i is the value of f (s) when s = s′i. Therefore, for all i = 1, . . . , k ′:\n• F 1i is also the value of f1(s) when s is just to the left of s′i; • F 0i is also the value of f0(s) when s is just to the right of s′i.\nSince f0 and f1 can change their values only at the points s′i, the vectors F 0 and F 1 uniquely determine the functions f0 and f1, respectively. For details of computing F 0 and F 1, see [9].\nRemark. There are several algorithms for performing isotonic regression on a partially, rather than linearly, ordered set: see, e.g., [10], Section 2.3 (although one of the algorithms described in that section, the Minimax Order Algorithm, was later shown to be defective [11, 12]). Therefore, IVAPs (and CVAPs below) can be defined in the situation where scores take values only in a partially ordered set; moreover, Proposition 1 will continue to hold. (For the reader familiar with the notion of Venn predictors we could also add that Venn–Abers predictors will continue to be Venn predictors, which follows from the isotonic regression being the average of the original function over certain equivalence classes.) The importance of partially ordered scores stems from the fact that they enable us to benefit from a possible “synergy” between two or more prediction algorithms\nAlgorithm 1 CVAP(T, x) // cross-Venn–Abers predictor for training set T 1: split the training set T into K folds T1, . . . , TK 2: for k ∈ {1, . . . ,K} 3: (pk0 , p k 1) := IVAP(T \\ Tk, Tk, x)\n4: return GM(p1)/(GM(1− p0) + GM(p1))\n[13]. Suppose, e.g., that one prediction algorithm outputs (scalar) scores s11, . . . , s 1 k for the calibration objects x1, . . . , xk and another outputs s21, . . . , s 2 k for the same calibration objects; we would like to use both sets of scores. We could merge the two sets of scores into composite vector scores, si := (s 1 i , s 2 i ), i = 1, . . . , k, and then classify a new object x as described earlier using its composite score s := (s1, s2), where s1 and s2 are the scalar scores computed by the two algorithms. Preliminary results reported in [13] in a related context suggest that the resulting predictor can outperform predictors based on the individual scalar scores. However, we will not pursue this idea further in this paper."
    }, {
      "heading" : "3 Cross Venn–Abers predictors (CVAPs)",
      "text" : "A CVAP is just a combination ofK IVAPs, whereK is the parameter of the algorithm. It is described as Algorithm 1, where IVAP(A,B, x) stands for the output of IVAP applied to A as proper training set, B as calibration set, and x as test object, and GM stands for geometric mean (so that GM(p1) is the geometric mean of p11, . . . , p K 1 and GM(1−p0) is the geometric mean of 1−p10, . . . , 1−pK0 ). The folds should be of approximately equal size, and usually the training set is split into folds at random (although we choose contiguous folds in Section 6 to facilitate reproducibility). One way to obtain a random assignment of the training observations to folds (see line 1) is to start from a regular array in which the first l1 observations are assigned to fold 1, the following l2 observations are assigned to fold 2, up to the last lK observations which are assigned to fold K, where |lk − l/K| < 1 for all k, and then to apply a random permutation. Remember that the procedure RANDOMIZE-IN-PLACE ([14], Section 5.3) can do the last step in time O(l). See the next section for a justification of the expression GM(p1)/(GM(1− p0) + GM(p1)) used for merging the IVAPs’ outputs."
    }, {
      "heading" : "4 Making probability predictions out of multiprobability ones",
      "text" : "In CVAP (Algorithm 1) we merge the K multiprobability predictions output by K IVAPs. In this section we design a minimax way for merging them, essentially following [1]. For the log-loss function the result is especially simple, GM(p1)/(GM(1− p0) + GM(p1)). Let us check that GM(p1)/(GM(1 − p0) + GM(p1)) is indeed the minimax expression under log loss. Suppose the pairs of lower and upper probabilities to be merged are (p10, p 1 1), . . . , (p K 0 , p K 1 ) and the merged probability is p. The extra cumulative loss suffered by p over the correct members p11, . . . , p K 1 of the pairs when the true label is 1 is\nlog p11 p + · · ·+ log p K 1 p , (1)\nand the extra cumulative loss of p over the correct members of the pairs when the true label is 0 is\nlog 1− p10 1− p + · · ·+ log 1− p K 0 1− p . (2)\nEqualizing the two expressions we obtain\np11 · · · pK1 pK = (1− p10) · · · (1− pK0 ) (1− p)K ,\nwhich gives the required minimax expression for the merged probability (since (1) is decreasing and (2) is increasing in p).\nFor the computations in the case of the Brier loss function, see [9].\nNotice that the argument above (“conditioned” on the proper training set) is also applicable to IVAP, in which case we need to set K := 1; the probability predictor obtained from an IVAP by replacing (p0, p1) with p := p1/(1 − p0 + p1) will be referred to as the log-minimax IVAP. (And CVAP is log-minimax by definition.)"
    }, {
      "heading" : "5 Comparison with other calibration methods",
      "text" : "The two alternative calibration methods that we consider in this paper are Platt’s [6] and isotonic regression [8]."
    }, {
      "heading" : "5.1 Platt’s method",
      "text" : "Platt’s [6] method uses sigmoids to calibrate the scores. Platt uses a regularization procedure ensuring that the predictions of his method are always in the range(\n1 k− + 2 , k+ + 1 k+ + 2\n) ,\nwhere k− is the number of calibration observations labelled 0 and k+ is the number of calibration observations labelled 1. It is interesting that the predictions output by the log-minimax IVAP are in the same range (except that the end-points are now allowed): see [9]."
    }, {
      "heading" : "5.2 Isotonic regression",
      "text" : "There are two standard uses of isotonic regression: we can train the scoring algorithm using what we call a proper training set, and then use the scores of the observations in a disjoint calibration (also called validation) set for calibrating the scores of test objects (as in [5]); alternatively, we can train the scoring algorithm on the full training set and also use the full training set for calibration (it appears that this was done in [8]). In both cases, however, we can expect to get an infinite log loss when the test set becomes large enough (see [9]).\nThe presence of regularization is an advantage of Platt’s method: e.g., it never suffers an infinite loss when using the log loss function. There is no standard method of regularization for isotonic regression, and we do not apply one."
    }, {
      "heading" : "6 Empirical studies",
      "text" : "The main loss function (cf., e.g., [15]) that we use in our empirical studies is the log loss\nλlog(p, y) := { − log p if y = 1 − log(1− p) if y = 0, (3)\nwhere log is binary logarithm, p ∈ [0, 1] is a probability prediction, and y ∈ {0, 1} is the true label. Another popular loss function is the Brier loss\nλBr(p, y) := 4(y − p)2. (4) We choose the coefficient 4 in front of (y − p)2 in (4) and the base 2 of the logarithm in (3) in order for the minimax no-information predictor that always predicts p := 1/2 to suffer loss 1. An advantage of the Brier loss function is that it still makes it possible to compare the quality of prediction in cases when prediction algorithms (such as isotonic regression) give a categorical but wrong prediction (and so are simply regarded as infinitely bad when using log loss).\nThe loss of a probability predictor on a test set will be measured by the arithmetic average of the losses it suffers on the test set, namely, by the mean log loss (MLL) and the mean Brier loss (MBL)\nMLL := 1\nn n∑ i=1 λlog(pi, yi), MBL := 1 n n∑ i=1 λBr(pi, yi), (5)\nwhere yi are the test labels and pi are the probability predictions for them. We will not be checking directly whether various calibration methods produce well-calibrated predictions, since it is well\nknown that lack of calibration increases the loss as measured by loss functions such as log loss and Brier loss (see, e.g., [16] for the most standard decomposition of the latter into the sum of the calibration error and refinement error).\nIn this section we compare log-minimax IVAPs (i.e., IVAPs whose outputs are replaced by probability predictions, as explained in Section 4) and CVAPs with Platt’s method [6] and the standard method [8] based on isotonic regression; the latter two will be referred to as “Platt” and “Isotonic” in our tables and figures. For both IVAPs and CVAPs we use the log-minimax procedure (the Brier-minimax procedure leads to virtually identical empirical results). We use the same underlying algorithms as in [1], namely J48 decision trees (abbreviated to “J48”), J48 decision trees with bagging (“J48 bagging”), logistic regression (sometimes abbreviated to “logistic”), naı̈ve Bayes, neural networks, and support vector machines (SVM), as implemented in Weka [17] (University of Waikato, New Zealand). The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as “Underlying” in our tables and figures) or can be calibrated using the methods of [6, 8] or the methods proposed in this paper (“IVAP” or “CVAP” in the tables and figures).\nFor illustrating our results in this paper we use the adult data set available from the UCI repository [18] (this is the main data set used in [6] and one of the data sets used in [8]); however, the picture that we observe is typical for other data sets as well (cf. [9]). We use the original split of the data set into a training set of Ntrain = 32, 561 observations and a test set of Ntest = 16, 281 observations. The results of applying the four calibration methods (including the vacuous one, corresponding to just using the underlying algorithm) to the six underlying algorithms for this data set are shown in Figure 1. The six top plots report results for the log loss (namely, MLL, as defined in (5)) and the six bottom plots for the Brier loss (namely, MBL). The underlying algorithms are given in the titles of the plots and the calibration methods are represented by different line styles, as explained in the legends. The horizontal axis is labelled by the ratio of the size of the proper training set to that of the calibration set (except for the label all, which will be explained later); in particular, in the case of CVAPs it is labelled by the number K − 1 one less than the number of folds. In the case of CVAPs, the training set is split into K equal (or as close to being equal as possible) contiguous folds: the first dNtrain/Ke training observations are included in the first fold, the next dNtrain/Ke (or bNtrain/Kc) in the second fold, etc. (first d·e and then b·c is used unless Ntrain is divisible by K). In the case of the other calibration methods, we used the first dK−1K Ntraine training observation as the proper training set (used for training the scoring algorithm) and the rest of the training observations are used as the calibration set.\nIn the case of log loss, isotonic regression often suffers infinite losses, which is indicated by the absence of the round marker for isotonic regression; e.g., all the log losses for SVM are infinite. We are not trying to use ad hoc solutions, such as clipping predictions to the interval [ , 1− ] for a small > 0, since we are also using the bounded Brier loss function. The CVAP lines tend to be at the bottom in all plots; experiments with other data sets also confirm this.\nThe column all in the plots of Figure 1 refers to using the full training set as both the proper training set and calibration set. (In our official definition of IVAP we require that the last two sets be disjoint, but in this section we continue to refer to IVAPs modified in this way simply as IVAPs; in [1], such prediction algorithms were referred to as SVAPs, simplified Venn–Abers predictors.) Using the full training set as both the proper training set and calibration set might appear naı̈ve (and is never used in the extensive empirical study [5]), but it often leads to good empirical results on larger data sets. However, it can also lead to very poor results, as in the case of “J48 bagging” (for IVAP, Platt, and Isotonic), the underlying algorithm that achieves the best performance in Figure 1.\nA natural question is whether CVAPs perform better than the alternative calibration methods in Figure 1 (and our other experiments) because of applying cross-over (in moving from IVAP to CVAP) or because of the extra regularization used in IVAPs. The first reason is undoubtedly important for both loss functions and the second for the log loss function. The second reason plays a smaller role for Brier loss for relatively large data sets (in the lower half of Figure 1 the curve for Isotonic and IVAP are very close to each other), but IVAPs are consistently better for smaller data sets even when using Brier loss. In Tables 1 and 2 we apply the four calibration methods and six underlying algorithms to a much smaller training set, namely to the first 5, 000 observations of the adult data set as the new training set, following [5]; the first 4, 000 training observations are used as the proper training set, the following 1, 000 training observations as the calibration set, and all other observa-\ntions (the remaining training and all test observations) are used as the new test set. The results are shown in Tables 1 for log loss and 2 for Brier loss. They are consistently better for IVAP than for IR (isotonic regression). Results for nine very small data sets are given in Tables 1 and 2 of [1], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled “SVA” in the tables in [1]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [1]).\nThe following information might help the reader in reproducing our results (in addition to our code being publicly available [9]). For each of the standard prediction algorithms within Weka that we use, we optimise the parameters by minimising the Brier loss on the calibration set, apart from the column labelled all. (We cannot use the log loss since it is often infinite in the case of isotonic regression.) We then use the trained algorithm to generate the scores for the calibration and test sets, which allows us to compute probability predictions using Platt’s method, isotonic regression, IVAP,\nand CVAP. All the scores apart from SVM are already in the [0, 1] range and can be used as probability predictions. Most of the parameters are set to their default values, and the only parameters that are optimised are C (pruning confidence) for J48 and J48 bagging, R (ridge) for logistic regression, L (learning rate) and M (momentum) for neural networks (MultilayerPerceptron), and C (complexity constant) for SVM (SMO, with the linear kernel); naı̈ve Bayes does not involve any parameters. Notice that none of these parameters are “hyperparameters”, in that they do not control the flexibility of the fitted prediction rule directly; this allows us to optimize the parameters on the training set for the all column. In the case of CVAPs, we optimise the parameters by minimising the cumulative Brier loss over all folds (so that the same parameters are used for all folds). To apply Platt’s method to calibrate the scores generated by the underlying algorithms we use logistic regression, namely the function mnrfit within MATLAB’s Statistics toolbox. For isotonic regression calibration we use the implementation of the PAVA in the R package fdrtool (namely, the function monoreg).\nFor further experimental results, see [9]."
    }, {
      "heading" : "7 Conclusion",
      "text" : "This paper introduces two new algorithms for probabilistic prediction, IVAP, which can be regarded as a regularised form of the calibration method based on isotonic regression, and CVAP, which is built on top of IVAP using the idea of cross-validation. Whereas IVAPs are automatically perfectly calibrated, the advantage of CVAPs is in their good empirical performance.\nThis paper does not study empirically upper and lower probabilities produced by IVAPs and CVAPs, whereas the distance between them provides information about the reliability of the merged probability prediction. Finding interesting ways of using this extra information is one of the directions of further research."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We are grateful to the conference reviewers for numerous helpful comments and observations, to Vladimir Vapnik for sharing his ideas about exploiting synergy between different learning algorithms, and to participants of the conference Machine Learning: Prospects and Applications (October 2015, Berlin) for their questions and comments. The first author has been partially supported by EPSRC (grant EP/K033344/1) and AFOSR (grant “Semantic Completions”). The second and third authors are grateful to their home institutions for funding their trips to Montréal."
    } ],
    "references" : [ {
      "title" : "Venn–Abers predictors",
      "author" : [ "Vladimir Vovk", "Ivan Petej" ],
      "venue" : "Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "An empirical distribution function for sampling with incomplete information",
      "author" : [ "Miriam Ayer", "H. Daniel Brunk", "George M. Ewing", "W.T. Reid", "Edward Silverman" ],
      "venue" : "Annals of Mathematical Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1955
    }, {
      "title" : "Smooth isotonic regression: a new method to calibrate predictive models",
      "author" : [ "Xiaoqian Jiang", "Melanie Osl", "Jihoon Kim", "Lucila Ohno-Machado" ],
      "venue" : "AMIA Summits on Translational Science Proceedings,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Reliable probability estimates based on support vector machines for large multiclass datasets",
      "author" : [ "Antonis Lambrou", "Harris Papadopoulos", "Ilia Nouretdinov", "Alex Gammerman" ],
      "venue" : "Proceedings of the AIAI 2012 Workshop on Conformal Prediction and its Applications,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "An empirical comparison of supervised learning algorithms",
      "author" : [ "Rich Caruana", "Alexandru Niculescu-Mizil" ],
      "venue" : "In Proceedings of the Twenty Third International Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Probabilities for SV machines",
      "author" : [ "John C. Platt" ],
      "venue" : "Advances in Large Margin Classifiers,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Algorithmic Learning in a Random World",
      "author" : [ "Vladimir Vovk", "Alex Gammerman", "Glenn Shafer" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers",
      "author" : [ "Bianca Zadrozny", "Charles Elkan" ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2001
    }, {
      "title" : "Large-scale probabilistic predictors with and without guarantees of validity",
      "author" : [ "Vladimir Vovk", "Ivan Petej", "Valentina Fedorova" ],
      "venue" : "Technical report, arXiv.org e-Print archive,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Statistical Inference under Order Restrictions: The Theory and Application of Isotonic Regression",
      "author" : [ "Richard E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H. Daniel Brunk" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1972
    }, {
      "title" : "The Min-Max algorithm and isotonic regression",
      "author" : [ "Chu-In Charles Lee" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1983
    }, {
      "title" : "Intelligent learning: Similarity control and knowledge transfer",
      "author" : [ "Vladimir N. Vapnik" ],
      "venue" : "Talk at the 2015 Yandex School of Data Analysis Conference Machine Learning: Prospects and Applications,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Introduction to Algorithms",
      "author" : [ "Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest", "Clifford Stein" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "The fundamental nature of the log loss function",
      "author" : [ "Vladimir Vovk" ],
      "venue" : "editors, Fields of Logic and Computation II: Essays Dedicated to Yuri Gurevich on the Occasion of His 75th Birthday,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "A new vector partition of the probability score",
      "author" : [ "Allan H. Murphy" ],
      "venue" : "Journal of Applied Meteorology,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1973
    }, {
      "title" : "The WEKA data mining software: an update",
      "author" : [ "Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten" ],
      "venue" : "SIGKDD Explorations,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Prediction algorithms studied in this paper belong to the class of Venn–Abers predictors, introduced in [1].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "They are based on the method of isotonic regression [2] and prompted by the observation that when applied in machine learning the method of isotonic regression often produces miscalibrated probability predictions (see, e.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : ", [3, 4]); it has also been reported ([5], Section 1) that isotonic regression is more prone to overfitting than Platt’s scaling [6] when data is scarce.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : ", [3, 4]); it has also been reported ([5], Section 1) that isotonic regression is more prone to overfitting than Platt’s scaling [6] when data is scarce.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 4,
      "context" : ", [3, 4]); it has also been reported ([5], Section 1) that isotonic regression is more prone to overfitting than Platt’s scaling [6] when data is scarce.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : ", [3, 4]); it has also been reported ([5], Section 1) that isotonic regression is more prone to overfitting than Platt’s scaling [6] when data is scarce.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : "The advantage of Venn–Abers predictors is that they are a special case of Venn predictors ([7], Chapter 6), and so ([7], Theorem 6.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "The advantage of Venn–Abers predictors is that they are a special case of Venn predictors ([7], Chapter 6), and so ([7], Theorem 6.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 7,
      "context" : "They can be considered to be a regularized version of the procedure used by [8], which helps them resist overfitting.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "The main desiderata for Venn (and related conformal, [7], Chapter 2) predictors are validity, predictive efficiency, and computational efficiency.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "In Section 5 we briefly discuss two existing calibration methods, Platt’s [6] and the method [8] based on isotonic regression.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 7,
      "context" : "In Section 5 we briefly discuss two existing calibration methods, Platt’s [6] and the method [8] based on isotonic regression.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : "Section 6 is devoted to experimental comparisons and shows that CVAPs consistently outperform the two existing methods (more extensive experimental studies can be found in [9]).",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "First we state formally the property of validity of IVAPs (adapting the approach of [1] to IVAPs).",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 8,
      "context" : "Our next proposition concerns the computational efficiency of IVAPs; Proposition 1 will be proved later in this section while Proposition 2 is proved in [9].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 9,
      "context" : "Proofs of both statements rely on the geometric representation of isotonic regression as the slope of the GCM (greatest convex minorant) of the CSD (cumulative sum diagram): see [10], pages 9–13 (especially Theorem 1.",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 7,
      "context" : ", (sk, yk) is defined to be the slope of the GCM between ∑i−1 j=1 wj and ∑i j=1 wj ; the values at other s are somewhat arbitrary (namely, the value at s ∈ (si, si+1) can be set to anything between the left and right slopes of the GCM at ∑i j=1 wj) but are never needed in this paper (unlike in the standard use of isotonic regression in machine learning, [8]): e.",
      "startOffset" : 356,
      "endOffset" : 359
    }, {
      "referenceID" : 8,
      "context" : "For details of computing F 0 and F (1), see [9].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 10,
      "context" : "3 (although one of the algorithms described in that section, the Minimax Order Algorithm, was later shown to be defective [11, 12]).",
      "startOffset" : 122,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "Preliminary results reported in [13] in a related context suggest that the resulting predictor can outperform predictors based on the individual scalar scores.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "Remember that the procedure RANDOMIZE-IN-PLACE ([14], Section 5.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "In this section we design a minimax way for merging them, essentially following [1].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "For the computations in the case of the Brier loss function, see [9].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "The two alternative calibration methods that we consider in this paper are Platt’s [6] and isotonic regression [8].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "The two alternative calibration methods that we consider in this paper are Platt’s [6] and isotonic regression [8].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "Platt’s [6] method uses sigmoids to calibrate the scores.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 8,
      "context" : "It is interesting that the predictions output by the log-minimax IVAP are in the same range (except that the end-points are now allowed): see [9].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "There are two standard uses of isotonic regression: we can train the scoring algorithm using what we call a proper training set, and then use the scores of the observations in a disjoint calibration (also called validation) set for calibrating the scores of test objects (as in [5]); alternatively, we can train the scoring algorithm on the full training set and also use the full training set for calibration (it appears that this was done in [8]).",
      "startOffset" : 278,
      "endOffset" : 281
    }, {
      "referenceID" : 7,
      "context" : "There are two standard uses of isotonic regression: we can train the scoring algorithm using what we call a proper training set, and then use the scores of the observations in a disjoint calibration (also called validation) set for calibrating the scores of test objects (as in [5]); alternatively, we can train the scoring algorithm on the full training set and also use the full training set for calibration (it appears that this was done in [8]).",
      "startOffset" : 444,
      "endOffset" : 447
    }, {
      "referenceID" : 8,
      "context" : "In both cases, however, we can expect to get an infinite log loss when the test set becomes large enough (see [9]).",
      "startOffset" : 110,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : ", [15]) that we use in our empirical studies is the log loss",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 14,
      "context" : ", [16] for the most standard decomposition of the latter into the sum of the calibration error and refinement error).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : ", IVAPs whose outputs are replaced by probability predictions, as explained in Section 4) and CVAPs with Platt’s method [6] and the standard method [8] based on isotonic regression; the latter two will be referred to as “Platt” and “Isotonic” in our tables and figures.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : ", IVAPs whose outputs are replaced by probability predictions, as explained in Section 4) and CVAPs with Platt’s method [6] and the standard method [8] based on isotonic regression; the latter two will be referred to as “Platt” and “Isotonic” in our tables and figures.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "We use the same underlying algorithms as in [1], namely J48 decision trees (abbreviated to “J48”), J48 decision trees with bagging (“J48 bagging”), logistic regression (sometimes abbreviated to “logistic”), naı̈ve Bayes, neural networks, and support vector machines (SVM), as implemented in Weka [17] (University of Waikato, New Zealand).",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "We use the same underlying algorithms as in [1], namely J48 decision trees (abbreviated to “J48”), J48 decision trees with bagging (“J48 bagging”), logistic regression (sometimes abbreviated to “logistic”), naı̈ve Bayes, neural networks, and support vector machines (SVM), as implemented in Weka [17] (University of Waikato, New Zealand).",
      "startOffset" : 296,
      "endOffset" : 300
    }, {
      "referenceID" : 5,
      "context" : "The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as “Underlying” in our tables and figures) or can be calibrated using the methods of [6, 8] or the methods proposed in this paper (“IVAP” or “CVAP” in the tables and figures).",
      "startOffset" : 234,
      "endOffset" : 240
    }, {
      "referenceID" : 7,
      "context" : "The underlying algorithms (except for SVM) produce scores in the interval [0, 1], which can be used directly as probability predictions (referred to as “Underlying” in our tables and figures) or can be calibrated using the methods of [6, 8] or the methods proposed in this paper (“IVAP” or “CVAP” in the tables and figures).",
      "startOffset" : 234,
      "endOffset" : 240
    }, {
      "referenceID" : 5,
      "context" : "For illustrating our results in this paper we use the adult data set available from the UCI repository [18] (this is the main data set used in [6] and one of the data sets used in [8]); however, the picture that we observe is typical for other data sets as well (cf.",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "For illustrating our results in this paper we use the adult data set available from the UCI repository [18] (this is the main data set used in [6] and one of the data sets used in [8]); however, the picture that we observe is typical for other data sets as well (cf.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 0,
      "context" : "(In our official definition of IVAP we require that the last two sets be disjoint, but in this section we continue to refer to IVAPs modified in this way simply as IVAPs; in [1], such prediction algorithms were referred to as SVAPs, simplified Venn–Abers predictors.",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 4,
      "context" : ") Using the full training set as both the proper training set and calibration set might appear naı̈ve (and is never used in the extensive empirical study [5]), but it often leads to good empirical results on larger data sets.",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "In Tables 1 and 2 we apply the four calibration methods and six underlying algorithms to a much smaller training set, namely to the first 5, 000 observations of the adult data set as the new training set, following [5]; the first 4, 000 training observations are used as the proper training set, the following 1, 000 training observations as the calibration set, and all other observa-",
      "startOffset" : 215,
      "endOffset" : 218
    }, {
      "referenceID" : 0,
      "context" : "Results for nine very small data sets are given in Tables 1 and 2 of [1], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled “SVA” in the tables in [1]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [1]).",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "Results for nine very small data sets are given in Tables 1 and 2 of [1], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled “SVA” in the tables in [1]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [1]).",
      "startOffset" : 212,
      "endOffset" : 215
    }, {
      "referenceID" : 0,
      "context" : "Results for nine very small data sets are given in Tables 1 and 2 of [1], where the results for IVAP (with the full training set used as both proper training and calibration sets, labelled “SVA” in the tables in [1]) are consistently (in 52 cases out of the 54 using Brier loss) better, usually significantly better, than for isotonic regression (referred to as DIR in the tables in [1]).",
      "startOffset" : 383,
      "endOffset" : 386
    }, {
      "referenceID" : 8,
      "context" : "The following information might help the reader in reproducing our results (in addition to our code being publicly available [9]).",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 8,
      "context" : "For further experimental results, see [9].",
      "startOffset" : 38,
      "endOffset" : 41
    } ],
    "year" : 2015,
    "abstractText" : "This paper studies theoretically and empirically a method of turning machinelearning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.",
    "creator" : null
  }
}