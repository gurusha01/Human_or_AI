{
  "name" : "8f19793b2671094e63a15ab883d50137.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Regularization-Free Estimation in Trace Regression with Symmetric Positive Semidefinite Matrices",
    "authors" : [ "Martin Slawski", "Ping Li", "Matthias Hein" ],
    "emails" : [ "{martin.slawski@rutgers.edu,", "pingli@stat.rutgers.edu}", "hein@cs.uni-saarland.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Trace regression models of the form\nyi = tr(X ⊤ i Σ ∗) + εi, i = 1, . . . , n, (1)\nwhere Σ∗ ∈ Rm1×m2 is the parameter of interest to be estimated given measurement matrices Xi ∈ R\nm1×m2 and observations yi contaminated by errors εi, i = 1, . . . , n, have attracted considerable interest in high-dimensional statistical inference, machine learning and signal processing over the past few years. Research in these areas has focused on a setting with few measurementsn ≪ m1 ·m2 and Σ∗ being (approximately) of low rank r ≪ min{m1,m2}. Such setting is relevant to problems such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and phase retrieval [7]. A common thread in these works is the use of the nuclear norm of a matrix as a convex surrogate for its rank [18] in regularized estimation amenable to modern optimization techniques. This approach can be seen as natural generalization of ℓ1-norm (aka lasso) regularization for the linear regression model [24] that arises as a special case of model (1) in which both Σ∗ and {Xi}ni=1 are diagonal. It is inarguable that in general regularization is essential if n < m1 ·m2. The situation is less clear if Σ∗ is known to satisfy additional constraints that can be incorporated in estimation. Specifically, in the present paper we consider the case in which m1 = m2 = m and Σ ∗ is known to be symmetric positive semidefinite (spd), i.e. Σ∗ ∈ Sm+ with Sm+ denoting the positive semidefinite cone in the space of symmetric real m×m matrices Sm. The set Sm+ deserves specific interest as it includes covariance matrices and Gram matrices in kernel-based learning [20]. It is rather common for these matrices to be of low rank (at least approximately), given the widespread use of principal components analysis and low-rank kernel approximations [28]. In the present paper, we focus on the usefulness of the spd constraint for estimation. We argue that if Σ∗ is spd and the measurement matrices {Xi}ni=1 obey certain conditions, constrained least squares estimation\nmin Σ∈Sm\n+\n1\n2n\nn∑\ni=1\n(yi − tr(X⊤i Σ))2 (2)\nmay perform similarly well in prediction and parameter estimation as approaches employing nuclear norm regularization with proper choice of the regularization parameter, including the interesting\nregime n < δm, where δm = dim(S m) = m(m+ 1)/2. Note that the objective in (2) only consists of a data fitting term and is hence convenient to work with in practice since there is no free parameter. Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for linear regression [16, 21].\nRelated work. Model (1) with Σ∗ ∈ Sm+ has been studied in several recent papers. A good deal of these papers consider the setup of compressed sensing in which the {Xi}ni=1 can be chosen by the user, with the goal to minimize the number of observations required to (approximately) recover Σ∗. For example, in [27], recovery of Σ∗ being low-rank from noiseless observations (εi = 0, i = 1, . . . , n) by solving a feasibility problem over Sm+ is considered, which is equivalent to the constrained least squares problem (1) in a noiseless setting.\nIn [3, 8], recovery from rank-one measurements is considered, i.e., for {xi}ni=1 ⊂ Rm\nyi = x ⊤ i Σ ∗xi + εi = tr(X ⊤ i Σ ∗) + εi, with Xi = xix ⊤ i , i = 1, . . . , n. (3)\nAs opposed to [3, 8], where estimation based on nuclear norm regularization is proposed, the present work is devoted to regularization-free estimation. While rank-one measurements as in (3) are also in the center of interest herein, our framework is not limited to this case. In [3] an application of (3) to covariance matrix estimation given only one-dimensional projections {x⊤i zi}ni=1 of the data points is discussed, where the {zi}ni=1 are i.i.d. from a distribution with zero mean and covariance matrix Σ∗. In fact, this fits the model under study with observations\nyi = (x ⊤ i zi) 2 = x⊤i ziz ⊤ i xi = x ⊤ i Σ ∗xi + εi, εi = x ⊤ i {ziz⊤i − Σ∗}xi, i = 1, . . . , n. (4)\nSpecializing (3) to the case in which Σ∗ = σ∗(σ∗)⊤, one obtains the quadratic model\nyi = |x⊤i σ∗|2 + εi (5) which (with complex-valued σ∗) is relevant to the problem of phase retrieval [14]. The approach of [7] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one solutions. In follow-up work [4], the authors show a refined recovery result stating that imposing an spd constraint − without regularization − suffices. A similar result has been proven independently by [10]. However, the results in [4] and [10] only concern model (5). After posting an extended version [22] of the present paper, a generalization of [4, 10] to general low-rank spd matrices has been achieved in [13]. Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].\nNotation. Md denotes the space of real d × d matrices with inner product 〈M,M ′〉 := tr(M⊤M ′). The subspace of symmetric matrices Sd has dimension δd := d(d + 1)/2. M ∈ Sd has an eigen-decomposition M = UΛU⊤ = ∑d j=1 λj(M)uju ⊤ j , where λ1(M) = λmax(M) ≥ λ2(M) ≥ . . . ≥ λd(M) = λmin(M), Λ = diag(λ1(M), . . . , λd(M)), and U = [u1 . . . ud]. For q ∈ [1,∞) and M ∈ Sd, ‖M‖q := ( ∑d j=1 |λj(M)|q)1/q denotes the Schatten-q-norm (q = 1: nuclear norm; q = 2 Frobenius norm ‖M‖F , q = ∞: spectral norm ‖M‖∞ := max1≤j≤d |λj(M)|). Let S1(d) = {M ∈ Sd : ‖M‖1 = 1} and S+1 (d) = S1(d) ∩ Sd+. The symbols , ,≻,≺ refer to the semidefinite ordering. For a set A and α ∈ R, αA := {αa, a ∈ A}. It is convenient to re-write model (1) as y = X (Σ∗) + ε, where y = (yi)ni=1, ε = (εi)ni=1 and X : Mm → Rn is a linear map defined by (X (M))i = tr(X⊤i M), i = 1, . . . , n, referred to as sampling operator. Its adjoint X ∗ : Rn → Mm is given by the map v 7→ ∑ni=1 viXi. Supplement. The appendix contains all proofs, additional experiments and figures."
    }, {
      "heading" : "2 Analysis",
      "text" : "Preliminaries. Throughout this section, we consider a special instance of model (1) in which\nyi = tr(XiΣ ∗) + εi, where Σ ∗ ∈ Sm+ , Xi ∈ Sm, and εi i.i.d.∼ N(0, σ2), i = 1, . . . , n. (6) The assumption that the errors {εi}ni=1 are Gaussian is made for convenience as it simplifies the stochastic part of our analysis, which could be extended to sub-Gaussian errors.\nNote that w.l.o.g., we may assume that {Xi}ni=1 ⊂ Sm. In fact, since Σ∗ ∈ Sm, for any M ∈ Mm we have that tr(MΣ∗) = tr(M symΣ∗), where M sym = (M +M⊤)/2.\nIn the sequel, we study the statistical performance of the constrained least squares estimator\nΣ̂ ∈ argmin Σ∈Sm\n+\n1\n2n ‖y −X (Σ)‖22 (7)\nunder model (6). More specifically, under certain conditions on X , we shall derive bounds on\n(a) 1\nn ‖X (Σ∗)−X (Σ̂)‖22, and (b) ‖Σ̂− Σ∗‖1, (8)\nwhere (a) will be referred to as “prediction error” below. The most basic method for estimating Σ∗ is ordinary least squares (ols) estimation\nΣ̂ols ∈ argmin Σ∈Sm\n1\n2n ‖y −X (Σ)‖22, (9)\nwhich is computationally simpler than (7). While (7) requires convex programming, (9) boils down to solving a linear system of equations in δm = m(m + 1)/2 variables. On the other hand, the prediction error of ols scales as OP(dim(range(X ))/n), where dim(range(X )) can be as large as min{n, δm}, in which case the prediction error vanishes only if δm/n → 0 as n → ∞. Moreover, the estimation error ‖Σ̂ols − Σ∗‖1 is unbounded unless n ≥ δm. Research conducted over the past few years has thus focused on methods dealing successfully with the case n < δm as long as the target Σ∗ has additional structure, notably low-rankedness. Indeed, if Σ∗ has rank r ≪ m, the intrinsic dimension of the problem becomes (roughly) mr ≪ δm. In a large body of work, nuclear norm regularization, which serves as a convex surrogate of rank regularization, is considered as a computationally convenient alternative for which a series of adaptivity properties to underlying lowrankedness has been established, e.g. [5, 15, 17, 18, 19]. Complementing (9) with nuclear norm regularization yields the estimator\nΣ̂1 ∈ argmin Σ∈Sm\n1\n2n ‖y −X (Σ)‖22 + λ‖Σ‖1, (10)\nwhere λ > 0 is a regularization parameter. In case an spd constraint is imposed (10) becomes\nΣ̂1+ ∈ argmin Σ∈Sm\n+\n1\n2n ‖y −X (Σ)‖22 + λ tr(Σ). (11)\nOur analysis aims at elucidating potential advantages of the spd constraint in the constrained least squares problem (7) from a statistical point of view. It turns out that depending on properties of X , the behaviour of Σ̂ can range from a performance similar to the least squares estimator Σ̂ols on the one hand to a performance similar to the nuclear norm regularized estimator Σ̂1+ with properly\nchosen/tuned λ on the other hand. The latter case appears to be remarkable: Σ̂ may enjoy similar\nadaptivity properties as nuclear norm regularized estimators even though Σ̂ is obtained from a pure data fitting problem without explicit regularization."
    }, {
      "heading" : "2.1 Negative result",
      "text" : "We first discuss a negative example of X for which the spd-constrained estimator Σ̂ does not improve (substantially) over the unconstrained estimator Σ̂ols. At the same time, this example provides clues on conditions to be imposed on X to achieve substantially better performance. Random Gaussian design. Consider the Gaussian orthogonal ensemble (GOE)\nGOE(m) = {X = (xjk), {xjj}mj=1 i.i.d.∼ N(0, 1), {xjk = xkj}1≤j<k≤m i.i.d.∼ N(0, 1/2)}.\nGaussian measurements are common in compressed sensing. It is hence of interest to study measurements {Xi}ni=1 i.i.d.∼ GOE(m) in the context of the constrained least squares problem (7). The following statement points to a serious limitation associated with such measurements.\nProposition 1. Consider Xi i.i.d.∼ GOE(m), i = 1, . . . , n. For any ε > 0, if n ≤ (1 − ε)δm/2, with probability at least 1− 32 exp(−ε2δm), there exists ∆ ∈ Sm+ , ∆ 6= 0 such that X (∆) = 0.\nProposition 1 implies that if the number of measurements drops below 1/2 of the ambient dimension δm, estimating Σ ∗ based on (7) becomes ill-posed; the estimation error ‖Σ̂ − Σ∗‖1 is unbounded, irrespective of the rank of Σ∗. Geometrically, the consequence of Proposition 1 is that the convex cone CX = {z ∈ Rn : z = X (∆), ∆ ∈ Sm+} contains 0. Unless 0 is contained in the boundary of CX (we conjecture that this event has measure zero), this means that CX = Rn, i.e. the spd constraint becomes vacuous."
    }, {
      "heading" : "2.2 Slow Rate Bound on the Prediction Error",
      "text" : "We present a positive result on the spd-constrained least squares estimator Σ̂ under an additional condition on the sampling operator X . Specifically, the prediction error will be bounded as\n1 n ‖X (Σ∗)−X (Σ̂)‖22 = O(λ0‖Σ∗‖1 + λ20), where λ0 = 1 n ‖X ∗(ε)‖∞, (12)\nwith λ0 typically being of the order O( √ m/n) (up to log factors). The rate in (12) can be a significant improvement of what is achieved by Σ̂ols if ‖Σ∗‖1 = tr(Σ∗) is small. If λ0 = o(‖Σ∗‖1) that rate coincides with those of the nuclear norm regularized estimators (10), (11) with regularization parameter λ ≥ λ0, cf. Theorem 1 in [19]. For nuclear norm regularized estimators, the rate O(λ0‖Σ∗‖1) is achieved for any choice of X and is slow in the sense that the squared prediction error only decays at the rate n−1/2 instead of n−1.\nCondition on X . In order to arrive at a suitable condition to be imposed on X so that (12) can be achieved, it makes sense to re-consider the negative example of Proposition 1, which states that as long as n is bounded away from δm/2 from above, there is a non-trivial ∆ ∈ Sm+ such that X (∆) = 0. Equivalently, dist(PX , 0) = min∆∈S+ 1 (m)‖X (∆)‖2 = 0, where\nPX := {z ∈ Rn : z = X (∆), ∆ ∈ S+1 (m)}, and S+1 (m) := {∆ ∈ Sm+ : tr(∆) = 1}. In this situation, it is impossible to derive a non-trivial bound on the prediction error as dist(PX , 0) = 0 may imply CX = Rn so that ‖X (Σ∗) − X (Σ̂)‖22 = ‖ε‖22. To rule this out, the condition dist(PX , 0) > 0 is natural. More strongly, one may ask for the following:\nThere exists a constant τ > 0 such that τ20 (X ) := min ∆∈S+\n1 (m)\n1 n ‖X (∆)‖22 ≥ τ2. (13)\nAn analogous condition is sufficient for a slow rate bound in the vector case, cf. [21]. However, the condition for the slow rate bound in Theorem 1 below is somewhat stronger than (13).\nCondition 1. There exist constants R∗ > 1, τ∗ > 0 s.t. τ 2(X , R∗) ≥ τ2∗ , where for R ∈ R\nτ2(X , R) = dist2(RPX ,PX )/n = min A∈RS+\n1 (m)\nB∈S+ 1 (m)\n1 n ‖X (A)−X (B)‖22.\nThe following condition is sufficient for Condition 1 and in some cases much easier to check.\nProposition 2. Suppose there exists a ∈ Rn, ‖a‖2 ≤ 1, and constants 0 < φmin ≤ φmax s.t.\nλmin(n −1/2X ∗(a)) ≥ φmin, and λmax(n−1/2X ∗(a)) ≤ φmax.\nThen for any ζ > 1, X satisfies Condition 1 with R∗ = ζ(φmax/φmin) and τ2∗ = (ζ − 1)2φ2max.\nThe condition of Proposition 2 can be phrased as having a positive definite matrix in the image of the unit ball under X ∗, which, after scaling by 1/√n, has its smallest eigenvalue bounded away from zero and a bounded condition number. As a simple example, suppose that X1 = √ nI . Invoking Proposition 2 with a = (1, 0, . . . , 0)⊤ and ζ = 2, we find that Condition 1 is satisfied with R∗ = 2 and τ 2 ∗ = 1. A more interesting example is random design where the {Xi}ni=1 are (sample) covariance matrices, where the underlying random vectors satisfy appropriate tail or moment conditions.\nCorollary 1. Let πm be a probability distribution on R m with second moment matrix Γ := Ez∼πm [zz ⊤] satisfying λmin(Γ) > 0. Consider the random matrix ensemble\nM(πm, q) = { 1 q ∑q k=1 zkz ⊤ k , {zk} q k=1 i.i.d.∼ πm } . (14)\nSuppose that {Xi}ni=1 i.i.d.∼ M(πm, q) and let Γ̂n := 1n ∑n i=1 Xi and 0 < ǫn < λmin(Γ). Under the event {‖Γ− Γ̂n‖∞ ≤ ǫn}, X satisfies Condition 1 with\nR∗ = 2(λmax(Γ) + ǫn)\nλmin(Γ)− ǫn and τ2∗ = (λmax(Γ) + ǫn) 2.\nIt is instructive to spell out Corollary 1 with πm as the standard Gaussian distribution on R m. The matrix Γ̂n equals the sample covariance matrix computed from N = n · q samples. It is well-known [9] that for m,N large, λmax(Γ̂n) and λmin(Γ̂n) concentrate sharply around (1+ηn)\n2 and (1−ηn)2, respectively, where ηn = √ m/N . Hence, for any γ > 0, there exists Cγ > 1 so that if N ≥ Cγm, it holds that R∗ ≤ 2 + γ. Similar though weaker concentration results for ‖Γ− Γ̂n‖∞ exist for the broader class of distributions πm with finite fourth moments [26]. Specialized to q = 1, Corollary 1 yields a statement about X made up from random rank-one measurements Xi = ziz⊤i , i = 1, . . . , n, cf. (3). The preceding discussion indicates that Condition 1 tends to be satisfied in this case.\nTheorem 1. Suppose that model (6) holds with X satisfying Condition 1 with constants R∗ and τ2∗ . We then have\n1 n ‖X (Σ∗)−X (Σ̂)‖22 ≤ max\n{ 2(1 +R∗)λ0‖Σ∗‖1, 2λ0‖Σ∗‖1 + 8 ( λ0\nR∗ τ∗\n)2}\nwhere, for any µ ≥ 0, with probability at least 1− (2m)−µ\nλ0 ≤ σ √ (1 + µ)2 log(2m) V 2 n n , where V 2 n = ∥∥ 1 n ∑n i=1 X 2 i ∥∥ ∞ .\nRemark: Under the scalings R∗ = O(1) and τ 2 ∗ = Ω(1), the bound of Theorem 1 is of the order O(λ0‖Σ∗‖1 + λ20) as announced at the beginning of this section. For given X , the quantity τ2(X , R) can be evaluated by solving a least squares problem with spd constraints. Hence it is feasible to check in practice whether Condition 1 holds. For later reference, we evaluate the term V 2n for M(πm, q) with πm as standard Gaussian distribution. As shown in the supplement, with high probability, V 2n = O(m logn) holds as long as m = O(nq)."
    }, {
      "heading" : "2.3 Bound on the Estimation Error",
      "text" : "In the previous subsection, we did not make any assumptions about Σ∗ apart from Σ∗ ∈ Sm+ . Henceforth, we suppose that Σ∗ is of low rank 1 ≤ r ≪ m and study the performance of the constrained least squares estimator (7) for prediction and estimation in such setting.\nPreliminaries. Let Σ∗ = UΛU⊤ be the eigenvalue decomposition of Σ∗, where\nU =\n[ U‖ U⊥\nm× r m× (m− r)\n] [ Λr 0r×(m−r)\n0(m−r)×r 0(m−r)×(m−r)\n]\nwhere Λr is diagonal with positive diagonal entries. Consider the linear subspace\nT ⊥ = {M ∈ Sm : M = U⊥AU⊤⊥ , A ∈ Sm−r}.\nFrom U⊤⊥Σ ∗U⊥ = 0, it follows that Σ ∗ is contained in the orthogonal complement\nT = {M ∈ Sm : M = U‖B +B⊤U⊤‖ , B ∈ Rr×m},\nof dimension mr − r(r − 1)/2 ≪ δm if r ≪ m. The image of T under X is denoted by T . Conditions on X . We introduce the key quantities the bound in this subsection depends on. Separability constant.\nτ2(T) = 1\nn dist2 (T ,PX ) , PX := {z ∈ Rn : z = X (∆), ∆ ∈ T⊥ ∩ S+1 (m)}\n= min Θ∈T, Λ∈S+\n1 (m)∩T⊥\n1 n ‖X (Θ)−X (Λ)‖22\nRestricted eigenvalue.\nφ2(T) = min 06=∆∈T ‖X (∆)‖22/n ‖∆‖21 .\nAs indicated by the following statement concerning the noiseless case, for bounding ‖Σ̂−Σ∗‖, it is inevitable to have lower bounds on the above two quantities.\nProposition 3. Consider the trace regression model (1) with εi = 0, i = 1, . . . , n. Then\nargmin Σ∈Sm\n+\n1\n2n ‖X (Σ∗)−X (Σ)‖22 = {Σ∗} for all Σ∗ ∈ T ∩ Sm+\nif and only if it holds that τ2(T) > 0 and φ2(T) > 0.\nCorrelation constant. Moreover, we use of the following the quantity. It is not clear to us if it is intrinsically required, or if its appearance in our bound is for merely technical reasons.\nµ(T) = max {\n1 n 〈X (∆),X (∆′)〉 : ‖∆‖1 ≤ 1,∆ ∈ T, ∆′ ∈ S + 1 (m) ∩ T⊥\n} .\nWe are now in position to provide a bound on ‖Σ̂− Σ∗‖1. Theorem 2. Suppose that model (6) holds with Σ∗ as considered throughout this subsection and let λ0 be defined as in Theorem 1. We then have\n‖Σ̂− Σ∗‖1 ≤ max { 8λ0\nµ(T)\nτ2(T)φ2(T)\n( 3\n2 +\nµ(T)\nφ2(T)\n) + 4λ0 ( 1\nφ2(T) +\n1\nτ2(T)\n) ,\n8λ0 φ2(T)\n( 1 + µ(T)\nφ2(T)\n) ,\n8λ0 τ2(T)\n} .\nRemark. Given Theorem 2 an improved bound on the prediction error scaling with λ20 in place of λ0 can be derived, cf. (26) in Appendix D. The quality of the bound of Theorem 2 depends on how the quantities τ2(T), φ2(T) and µ(T) scale with n, m and r, which is design-dependent. Accordingly, the estimation error in nuclear norm can be non-finite in the worst case and O(λ0r) in the best case, which matches existing bounds for nuclear norm regularization (cf. Theorem 2 in [19]).\n• The quantity τ2(T) is specific to the geometry of the constrained least squares problem (7) and hence of critical importance. For instance, it follows from Proposition 1 that for standard Gaussian measurements, τ2(T) = 0 with high probability once n < δm/2. The situation can be much better for random spd measurements (14) as exemplified for mea-\nsurements Xi = ziz ⊤ i with zi i.i.d.∼ N(0, I) in Appendix H. Specifically, it turns out that τ2(T) = Ω(1/r) as long as n = Ω(m · r).\n• It is not restrictive to assume φ2(T) is positive. Indeed, without that assumption, even an oracle estimator based on knowledge of the subspace T would fail. Reasonable sampling operators X have rank min{n, δm} so that the nullspace of X only has a trivial intersection with the subspace T as long as n ≥ dim(T) = mr − r(r − 1)/2.\n• For fixed T, computing µ(T) entails solving a biconvex (albeit non-convex) optimization problem in ∆ ∈ T and ∆′ ∈ S+1 (m)∩T⊥. Block coordinate descent is a practical approach to such optimization problems for which a globally optimal solution is out of reach. In this manner we explore the scaling of µ(T) numerically as done for τ2(T). We find that µ(T) = O(δm/n) so that µ(T) = O(1) apart from the regime n/δm → 0, without ruling out the possibility of undersampling, i.e. n < δm."
    }, {
      "heading" : "3 Numerical Results",
      "text" : "In this section, we empirically study properties of the estimator Σ̂. In particular, its performance relative to regularization-based methods is explored. We also present an application to spiked covariance estimation for the CBCL face image data set and stock prices from NASDAQ.\nComparison with regularization-based approaches. We here empirically evaluate ‖Σ̂ − Σ∗‖1 relative to well-known regularization-based methods.\nSetup. We consider rank-one Wishart measurement matrices Xi = ziz ⊤ i , zi i.i.d.∼ N(0, I), i = 1, . . . , n, fix m = 50 and let n ∈ {0.24, 0.26, . . . , 0.36, 0.4, . . . , 0.56} ·m2 and r ∈ {1, 2, . . . , 10} vary. Each configuration of (n, r) is run with 50 replications. In each of these, we generate data\nyi = tr(XiΣ ∗) + σεi, σ = 0.1, i = 1, . . . , n, (15)\nwhere Σ∗ is generated randomly as rank r Wishart matrices and the {εi}ni=1 are i.i.d. N(0, 1).\nRegularization-based approaches. We compare Σ̂ to the corresponding nuclear norm regularized estimator in (11). Regarding the choice of the regularization parameter λ, we consider the grid λ∗ · {0.01, 0.05, 0.1, 0.3, 0.5, 1, 2, 4, 8, 16}, where λ∗ = σ √\nm/n as recommended in [17] and pick λ so that the prediction error on a separate validation data set of size n generated from (15) is minimized. Note that in general, neither σ is known nor an extra validation data set is available. Our goal here is to ensure that the regularization parameter is properly tuned. In addition, we consider an oracular choice of λ where λ is picked from the above grid such that the performance measure of interest (the distance to the target in the nuclear norm) is minimized. We also compare to the constrained nuclear norm minimization approach of [8]:\nmin Σ\ntr(Σ) subject to Σ 0, and ‖y −X (Σ)‖1 ≤ λ. (16)\nFor λ, we consider the grid nσ √ 2/π · {0.2, 0.3, . . . , 1, 1.25}. This specific choice is motivated by the fact that E[‖y − X (Σ∗)‖1] = E[‖ε‖1] = nσ √\n2/π. Apart from that, tuning of λ is performed as for the nuclear norm regularized estimator. In addition, we have assessed the performance of the approach in [3], which does not impose an spd constraint but adds another constraint to (16). That additional constraint significantly complicates optimization and yields a second tuning parameter. Thus, instead of doing a 2D-grid search, we use fixed values given in [3] for known σ. The results are similar or worse than those of (16) (note in particular that positive semidefiniteness is not taken advantage of in [3]) and are hence not reported.\nDiscussion of the results. We conclude from Figure 1 that in most cases, the performance of the constrained least squares estimator does not differ much from that of the regularization-based methods with careful tuning. For larger values of r, the constrained least squares estimator seems to require slightly more measurements to achieve competitive performance.\nReal Data Examples. We now present an application to recovery of spiked covariance matrices which are of the form Σ∗ = ∑r j=1 λjuju ⊤ j + σ\n2I , where r ≪ m and λj ≫ σ2 > 0, j = 1, . . . , r. This model appears frequently in connection with principal components analysis (PCA).\nExtension to the spiked case. So far, we have assumed that Σ∗ is of low rank, but it is straightforward to extend the proposed approach to the case in which Σ∗ is spiked as long as σ2 is known or an estimate is available. A constrained least squares estimator of Σ∗ takes the form Σ̂ + σ2I , where\nΣ̂ ∈ argmin Σ∈Sm\n+\n1\n2n ‖y −X (Σ + σ2I)‖22. (17)\nThe case of σ2 unknown or general (unknown) diagonal perturbation is left for future research.\nData sets. (i) The CBCL facial image data set [1] consist of N = 2429 images of 19× 19 pixels (i.e., m = 361). We take Σ∗ as the sample covariance matrix of this data set. It turns out that Σ∗ can be well approximated by Σr, r = 50, where Σr is the best rank r approximation to Σ ∗ obtained from computing its eigendecomposition and setting to zero all but the top r eigenvalues. (ii) We construct a second data set from the daily end prices of m = 252 stocks from the technology sector in NASDAQ, starting from the beginning of the year 2000 to the end of the year 2014 (in total N = 3773 days, retrieved from finance.yahoo.com). We take Σ∗ as the resulting sample correlation matrix and choose r = 100.\nExperimental setup. As in preceding measurements, we consider n random Wishart measurements for the operator X , where n = C(mr), where C ranges from 0.25 to 12. Since ‖Σr − Σ∗‖F/‖Σ∗‖F ≈ 10−3 for both data sets, we work with σ2 = 0 in (17) for simplicity. To make recovery of Σ∗ more difficult, we make the problem noisy by using observations\nyi = tr(XiSi), i = 1, . . . , n, (18)\nwhere Si is an approximation to Σ ∗ obtained from the sample covariance respectively sample correlation matrix of βN data points randomly sampled with replacement from the entire data set, i = 1, . . . , n, where β ranges from 0.4 to 1/N (Si is computed from a single data point). For each choice of n and β, the reported results are averages over 20 replications.\nResults. For the CBCL data set, as shown in Figure 2, Σ̂ accurately approximates Σ∗ once the number of measurements crosses 2mr. Performance degrades once additional noise is introduced to the problem by using measurements (18). Even under significant perturbations (β = 0.08), reasonable reconstruction of Σ∗ remains possible, albeit the number of required measurements increases accordingly. In the extreme case β = 1/N , the error is still decreasing with n, but millions of samples seems to be required to achieve reasonable reconstruction error.\nThe general picture is similar for the NASDAQ data set, but the difference between using measurements based on the full sample correlation matrix on the one hand and approximations based on random subsampling (18) on the other hand are more pronounced."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We have investigated trace regression in the situation that the underlying matrix is symmetric positive semidefinite. Under restrictions on the design, constrained least squares enjoys similar statistical properties as methods employing nuclear norm regularization. This may come as a surprise, as regularization is widely regarded as necessary in small sample settings."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The work of Martin Slawski and Ping Li is partially supported by NSF-DMS-1444124, NSF-III1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137."
    } ],
    "references" : [ {
      "title" : "Living on the edge: phase transitions in convex programs with random data",
      "author" : [ "D. Amelunxen", "M. Lotz", "M. McCoy", "J. Tropp" ],
      "venue" : "Information and Inference,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "ROP: Matrix recovery via rank-one projections",
      "author" : [ "T. Cai", "A. Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Solving quadratic equations via PhaseLift when there are about as many equations as unknowns",
      "author" : [ "E. Candes", "X. Li" ],
      "venue" : "Foundation of Computational Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Tight oracle bounds for low-rank matrix recovery from a minimal number of noisy measurements",
      "author" : [ "E. Candes", "Y. Plan" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E. Candes", "B. Recht" ],
      "venue" : "Foundation of Computational Mathematics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming",
      "author" : [ "E. Candes", "T. Strohmer", "V. Voroninski" ],
      "venue" : "Communications on Pure and Applied Mathematics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Exact and Stable Covariance Estimation from Quadratic Sampling via Convex Programming",
      "author" : [ "Y. Chen", "Y. Chi", "A. Goldsmith" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "Handbook of the Geometry of Banach Spaces, volume 1, chapter Local operator theory, random matrices and Banach spaces, pages 317–366",
      "author" : [ "K. Davidson", "S. Szarek" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Stable optimizationless recovery from phaseless measurements",
      "author" : [ "L. Demanet", "P. Hand" ],
      "venue" : "Journal of Fourier Analysis and its Applications,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Quantum State Tomography via Compressed Sensing",
      "author" : [ "D. Gross", "Y.-K. Liu", "S. Flammia", "S. Becker", "J. Eisert" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Matrix Analysis",
      "author" : [ "R. Horn", "C. Johnson" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1985
    }, {
      "title" : "Stable low rank matrix recovery via null space properties",
      "author" : [ "M. Kabanva", "R. Kueng", "H. Rauhut und U. Terstiege" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "The phase retrieval problem",
      "author" : [ "M. Klibanov", "P. Sacks", "A. Tikhonarov" ],
      "venue" : "Inverse Problems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1995
    }, {
      "title" : "Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion",
      "author" : [ "V. Koltchinskii", "K. Lounici", "A. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Sign-constrained least squares estimation for high-dimensional regression",
      "author" : [ "N. Meinshausen" ],
      "venue" : "The Electronic Journal of Statistics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
      "author" : [ "S. Negahban", "M. Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "B. Recht", "M. Fazel", "P. Parillo" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Estimation of high-dimensional low-rank matrices",
      "author" : [ "A. Rohde", "A. Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Learning with kernels",
      "author" : [ "B. Schölkopf", "A. Smola" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization",
      "author" : [ "M. Slawski", "M. Hein" ],
      "venue" : "The Electronic Journal of Statistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Regularization-free estimation in trace regression with positive semidefinite matrices",
      "author" : [ "M. Slawski", "P. Li", "M. Hein" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2015
    }, {
      "title" : "Maximum margin matrix factorization",
      "author" : [ "N. Srebro", "J. Rennie", "T. Jaakola" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Regression shrinkage and variable selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society Series B,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1996
    }, {
      "title" : "User-friendly tools for random matrices: An introduction",
      "author" : [ "J. Tropp" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "How close is the sample covariance matrix to the actual covariance matrix ",
      "author" : [ "R. Vershynin" ],
      "venue" : "Journal of Theoretical Probability,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "A unique ’nonnegative’ solution to an underdetermined system: from vectors to matrices",
      "author" : [ "M. Wang", "W. Xu", "A. Tang" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    }, {
      "title" : "Using the Nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Such setting is relevant to problems such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and phase retrieval [7].",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "Such setting is relevant to problems such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and phase retrieval [7].",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "Such setting is relevant to problems such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and phase retrieval [7].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 15,
      "context" : "Such setting is relevant to problems such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and phase retrieval [7].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "Such setting is relevant to problems such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and phase retrieval [7].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Such setting is relevant to problems such as matrix completion [6, 23], compressed sensing [5, 17], quantum state tomography [11] and phase retrieval [7].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "A common thread in these works is the use of the nuclear norm of a matrix as a convex surrogate for its rank [18] in regularized estimation amenable to modern optimization techniques.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 22,
      "context" : "This approach can be seen as natural generalization of l1-norm (aka lasso) regularization for the linear regression model [24] that arises as a special case of model (1) in which both Σ and {Xi}i=1 are diagonal.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 18,
      "context" : "The set S+ deserves specific interest as it includes covariance matrices and Gram matrices in kernel-based learning [20].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 26,
      "context" : "It is rather common for these matrices to be of low rank (at least approximately), given the widespread use of principal components analysis and low-rank kernel approximations [28].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for linear regression [16, 21].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 19,
      "context" : "Our findings can be seen as a non-commutative extension of recent results on non-negative least squares estimation for linear regression [16, 21].",
      "startOffset" : 137,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "For example, in [27], recovery of Σ being low-rank from noiseless observations (εi = 0, i = 1, .",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "In [3, 8], recovery from rank-one measurements is considered, i.",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 6,
      "context" : "In [3, 8], recovery from rank-one measurements is considered, i.",
      "startOffset" : 3,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "As opposed to [3, 8], where estimation based on nuclear norm regularization is proposed, the present work is devoted to regularization-free estimation.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "As opposed to [3, 8], where estimation based on nuclear norm regularization is proposed, the present work is devoted to regularization-free estimation.",
      "startOffset" : 14,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "In [3] an application of (3) to covariance matrix estimation given only one-dimensional projections {xi zi}i=1 of the data points is discussed, where the {zi}i=1 are i.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : "(4) Specializing (3) to the case in which Σ = σ(σ), one obtains the quadratic model yi = |xi σ|(2) + εi (5) which (with complex-valued σ) is relevant to the problem of phase retrieval [14].",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 5,
      "context" : "The approach of [7] treats (5) as an instance of (1) and uses nuclear norm regularization to enforce rank-one solutions.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "In follow-up work [4], the authors show a refined recovery result stating that imposing an spd constraint − without regularization − suffices.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "A similar result has been proven independently by [10].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "However, the results in [4] and [10] only concern model (5).",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 8,
      "context" : "However, the results in [4] and [10] only concern model (5).",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "After posting an extended version [22] of the present paper, a generalization of [4, 10] to general low-rank spd matrices has been achieved in [13].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "After posting an extended version [22] of the present paper, a generalization of [4, 10] to general low-rank spd matrices has been achieved in [13].",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "After posting an extended version [22] of the present paper, a generalization of [4, 10] to general low-rank spd matrices has been achieved in [13].",
      "startOffset" : 81,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "After posting an extended version [22] of the present paper, a generalization of [4, 10] to general low-rank spd matrices has been achieved in [13].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].",
      "startOffset" : 6,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].",
      "startOffset" : 146,
      "endOffset" : 157
    }, {
      "referenceID" : 8,
      "context" : "Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].",
      "startOffset" : 146,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "Since [4, 10, 13] consider bounded noise, whereas the analysis herein assumes Gaussian noise, our results are not direclty comparable to those in [4, 10, 13].",
      "startOffset" : 146,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "It is well-known [9] that for m,N large, λmax(Γ̂n) and λmin(Γ̂n) concentrate sharply around (1+ηn) 2 and (1−ηn)(2), respectively, where ηn = √ m/N .",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 24,
      "context" : "Similar though weaker concentration results for ‖Γ− Γ̂n‖∞ exist for the broader class of distributions πm with finite fourth moments [26].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "5, 1, 2, 4, 8, 16}, where λ∗ = σ √ m/n as recommended in [17] and pick λ so that the prediction error on a separate validation data set of size n generated from (15) is minimized.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "We also compare to the constrained nuclear norm minimization approach of [8]: min Σ tr(Σ) subject to Σ 0, and ‖y −X (Σ)‖1 ≤ λ.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "In addition, we have assessed the performance of the approach in [3], which does not impose an spd constraint but adds another constraint to (16).",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "Thus, instead of doing a 2D-grid search, we use fixed values given in [3] for known σ.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "The results are similar or worse than those of (16) (note in particular that positive semidefiniteness is not taken advantage of in [3]) and are hence not reported.",
      "startOffset" : 132,
      "endOffset" : 135
    } ],
    "year" : 2015,
    "abstractText" : "Trace regression models have received considerable attention in the context of matrix completion, quantum state tomography, and compressed sensing. Estimation of the underlying matrix from regularization-based approaches promoting low-rankedness, notably nuclear norm regularization, have enjoyed great popularity. In this paper, we argue that such regularization may no longer be necessary if the underlying matrix is symmetric positive semidefinite (spd) and the design satisfies certain conditions. In this situation, simple least squares estimation subject to an spd constraint may perform as well as regularization-based approaches with a proper choice of regularization parameter, which entails knowledge of the noise level and/or tuning. By contrast, constrained least squares estimation comes without any tuning parameter and may hence be preferred due to its simplicity.",
    "creator" : null
  }
}