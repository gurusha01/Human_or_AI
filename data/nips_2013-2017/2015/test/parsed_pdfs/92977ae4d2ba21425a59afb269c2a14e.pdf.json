{
  "name" : "92977ae4d2ba21425a59afb269c2a14e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Regularized EM Algorithms: A Unified Framework and Statistical Guarantees",
    "authors" : [ "Xinyang Yi", "Constantine Caramanis" ],
    "emails" : [ "yixy@utexas.edu", "constantine@utexas.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We give general conditions for the convergence of the EM method for high-dimensional estimation. We specialize these conditions to several problems of interest, including high-dimensional sparse and low-rank mixed regression, sparse gaussian mixture models, and regression with missing covariates. As we explain below, the key problem in the high-dimensional setting is the M -step. A natural idea is to modify this step via appropriate regularization, yet choosing the appropriate sequence of regularizers is a critical problem. As we know from the theory of regularized M-estimators (e.g., [19]) the regularizer should be chosen proportional to the target estimation error. For EM, however, the target estimation error changes at each step.\nThe main contribution of our work is technical: we show how to perform this iterative regularization. We show that the regularization sequence must be chosen so that it converges to a quantity controlled by the ultimate estimation error. In existing work, the estimation error is given by the relationship between the population and empirical M -step operators, but this too is not well defined in the highdimensional setting. Thus a key step, related both to our algorithm and its convergence analysis, is obtaining a different characterization of statistical error for the high-dimensional setting.\nBackground and Related Work\nEM (e.g., [8, 12]) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement. On the flip side, despite a fairly long history of studying EM in theory (e.g., [12, 17, 21]),\nvery little has been understood about general statistical guarantees until recently. Very recent work in [1] establishes a general local convergence theorem (i.e., assuming initialization lies in a local region around true parameter) and statistical guarantees for EM, which is then specialized to obtain near-optimal rates for several specific low-dimensional problems – low-dimensional in the sense of the classical statistical setting where the samples outnumber the dimension. A central challenge in extending EM (and as a corollary, the analysis in [1]) to the high-dimensional regime is the M -step. On the algorithm side, the M -step will not be stable (or even well-defined in some cases) in the high-dimensional setting. To make matters worse, any analysis that relies on showing that the finite-sample M -step is somehow “close” to the M -step performed with infinite data (the population-level M -step) simply cannot apply in the high-dimensional regime. Recent work in [20] treats high-dimensional EM using a truncatedM -step. This works in some settings, but also requires specialized treatment for every different setting, precisely because of the difficulty with the M -step.\nIn contrast to work in [20], we pursue a high-dimensional extension via regularization. The central challenge, as mentioned above, is in picking the sequence of regularization coefficients, as this must control the optimization error (related to the special structure of β∗), as well as the statistical error. Finally, we note that for finite mixture regression, Städler et al.[16] consider an `1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality. However, this work doesn’t establish the theoretical properties of local optima arising from regularized EM. Our work addresses this issue from a local convergence perspective by using a novel choice of regularization."
    }, {
      "heading" : "2 Classical EM and Challenges in High Dimensions",
      "text" : "The EM algorithm is an iterative algorithm designed to combat the non-convexity of max likelihood due to latent variables. For space concerns we omit the standard derivation, and only give the definitions we need in the sequel. Let Y ,Z be random variables taking values in Y ,Z , with joint distribution fβ(y, z) depending on model parameter β ⊆ Ω ⊆ Rp. We observe samples of Y but not of the latent variable Z. EM seeks to maximize a lower bound on the maximum likelihood function for β. Letting κβ(z|y) denote the conditional distribution ofZ given Y = y, letting yβ∗(y) denote the marginal distribution of Y , and defining the function\nQn(β ′|β) := 1\nn n∑ i=1 ∫ Z κβ(z|yi) log fβ′(yi, z)dz, (2.1)\none iteration of the EM algorithm, mapping β(t) to β(t+1), consists of the following two steps:\n• E-step: Compute function Qn(β|β(t)) given β(t). • M-step: β(t+1) ←Mn(β) := arg maxβ′∈ΩQn(β′|β(t)).\nWe can define the population (infinite sample) versions of Qn andMn in a natural manner: Q(β′|β) := ∫ Y yβ∗(y) ∫ Z κβ(z|y) log fβ′(y, z)dzdy (2.2)\nM(β) = arg max β′∈Ω\nQ(β′|β). (2.3)\nThis paper is about the high-dimensional setting where the number of samples n may be far less than the dimensionality p of the parameter β, but where β exhibits some special structure, e.g., it may be a sparse vector or a low-rank matrix. In such a setting, the M -step of the EM algorithm may be highly problematic. In many settings, for example sparse mixed regression, the M -step may not even be well defined. More generally, when n p,Mn(β) may be far from the population version, M(β), and in particular, the minimum estimation error ‖Mn(β∗) −M(β∗)‖ can be much larger than the signal strength ‖β∗‖. This quantity is used in [1] as well as in follow-up work in [20], as a measure of statistical error. In the high dimensional setting, something else is needed."
    }, {
      "heading" : "3 Algorithm",
      "text" : "The basis of our algorithm is the by-now well understood concept of regularized high dimensional estimators, where the regularization is tuned to the underlying structure of β∗, thus defining a regu-\nlarized M -step via Mrn(β) := arg max\nβ′∈Ω Qn(β\n′|β)− λnR(β′), (3.1)\nwhere R(·) denotes an appropriate regularizer chosen to match the structure of β∗. The key challenge is how to choose the sequence of regularizers {λ(t)n } in the iterative process, so as to control optimization and statistical error. As detailed in Algorithm 1, our sequence of regularizers attempts to match the target estimation error at each step of the EM iteration. For an intuition of what this might look like, consider the estimation error at step t: ‖Mrn(β(t)) − β∗‖2. By the triangle inequality, we can bound this by a sum of two terms: the optimization error and the final estimation error:\n‖Mrn(β(t))− β∗‖2 ≤ ‖Mrn(β(t))−Mrn(β∗)‖2 + ‖Mrn(β∗)− β∗‖2. (3.2)\nSince we expect (and show) linear convergence of the optimization, it is natural to update λ(t)n via a recursion of the form λ(t)n = κλ (t−1) n +∆ as in (3.3), where the first term represents the optimization error, and ∆ represents the final statistical error, i.e., the last term above in (3.2). A key part of our analysis shows that this error (and hence ∆) is controlled by ‖∇Qn(β∗|β)−∇Q(β∗|β)‖R∗ , which in turn can be bounded uniformly for a variety of important applications of EM, including the three discussed in this paper (see Section 5). While a technical point, it is this key insight that enables the right choice of algorithm and its analysis. In the cases we consider, we obtain min-max optimal rates of convergence, demonstrating that no algorithm, let alone another variant of EM, can perform better. Algorithm 1 Regularized EM Algorithm Input Samples {yi}ni=1, regularizer R, number of iterations T , initial parameter β(0), initial regu-\nlarization parameter λ(0)n , estimated statistical error ∆, contractive factor κ < 1. 1: For t = 1, 2, . . . , T do 2: Regularization parameter update:\nλ(t)n ← κλ(t−1)n + ∆. (3.3)\n3: E-step: Compute function Qn(·|β(t−1)) according to (2.1). 4: Regularized M-step:\nβ(t) ←Mrn(β(t−1)) := arg max β∈Ω Qn(β|β(t−1))− λ(t)n · R(β).\n5: End For Output β(T )."
    }, {
      "heading" : "4 Statistical Guarantees",
      "text" : "We now turn to the theoretical analysis of regularized EM algorithm. We first set up a general analytical framework for regularized EM where the key ingredients are decomposable regularizer and several technical conditions on the population based Q(·|·) and the sample based Qn(·|·). In Section 4.3, we provide our main result (Theorem 1) that characterizes both computational and statistical performance of the proposed variant of regularized EM algorithm."
    }, {
      "heading" : "4.1 Decomposable Regularizers",
      "text" : "Decomposable regularizers (e.g., [3, 6, 14, 19]), have been shown to be useful both empirically and theoretically for high dimensional structural estimation, and they also play an important role in our analytical framework. Recall that for R : Rp → R+ a norm, and a pair of subspaces (S,S) in Rp such that S ⊆ S, we have the following definition: Definition 1 (Decomposability). RegularizerR : Rp → R+ is decomposable with respect to (S,S) if\nR(u + v) = R(u) +R(v), for any u ∈ S,v ∈ S⊥.\nTypically, the structure of model parameter β∗ can be characterized by specifying a subspace S such that β∗ ∈ S . The common use of a regularizer is thus to penalize the compositions of solution that\nlive outside S. We are interested in bounding the estimation error in some norm ‖ · ‖. The following quantity is critical in connectingR to ‖ · ‖. Definition 2 (Subspace Compatibility Constant). For any subspace S ⊆ Rp, a given regularizer R and some norm ‖ · ‖, the subspace compatibility constant of S with respect toR, ‖ · ‖ is given by\nΨ(S) := sup u∈S\\{0} R(u) ‖u‖ .\nAs is standard, the dual norm ofR is defined asR∗(v) := supR(u)≤1 〈 u,v 〉 . To simplify notation, we let ‖u‖R := R(u) and ‖u‖R∗ := R∗(u)."
    }, {
      "heading" : "4.2 Conditions on Q(·|·) and Qn(·|·)",
      "text" : "Next, we review three technical conditions, originally proposed by [1], on the population levelQ(·|·) function, and then we give two important conditions that the empirial function Qn(·|·) must satisfy, including one that characterizes the statistical error.\nIt is well known that performance of EM algorithm is sensitive to initialization. Following the lowdimensional development in [1], our results are local, and apply to an r-neighborhood region around β∗: B(r;β∗) := { u ∈ Ω, ‖u− β∗‖ ≤ r } .\nWe first require that Q(·|β∗) is self consistent as stated below. This is satisfied, in particular, when β∗ maximizes the population log likelihood function, as happens in most settings of interest [12]. Condition 1 (Self Consistency). Function Q(·|β∗) is self consistent, namely\nβ∗ = arg max β∈Ω Q(β|β∗).\nWe also require that the function Q(·|·) satisfies a certain strong concavity condition and is smooth over Ω. Condition 2 (Strong Concavity and Smoothness (γ, µ, r)). Q(·|β∗) is γ-strongly concave over Ω, i.e.,\nQ(β2|β∗)−Q(β1|β∗)− 〈 ∇Q(β1|β∗),β2 − β1 〉 ≤ −γ\n2 ‖β2 − β1‖2, ∀ β1,β2 ∈ Ω. (4.1)\nFor any β ∈ B(r;β∗), Q(·|β) is µ-smooth over Ω, i.e., Q(β2|β)−Q(β1|β)− 〈 ∇Q(β1|β),β2 − β1 〉 ≥ −µ\n2 ‖β2 − β1‖2, ∀ β1,β2 ∈ Ω. (4.2)\nThe next condition is key in guaranteeing the curvature of Q(·|β) is similar to that of Q(·|β∗) when β is close to β∗. It has also been called First Order Stability in [1]. Condition 3 (Gradient Stability (τ, r)). For any β ∈ B(r;β∗), we have∥∥∇Q(M(β)|β)−∇Q(M(β)|β∗)∥∥ ≤ τ‖β − β∗‖. The above condition only requires that the gradient be stable at one pointM(β). This is sufficient for our analysis. In fact, for many concrete examples, one can verify a stronger version of Condition 3 that is\n∥∥∇Q(β′|β)−∇Q(β′|β∗)∥∥ ≤ τ‖β − β∗‖, ∀ β′ ∈ B(r;β∗). Next we require two conditions on the empirical function Qn(·|·), which is computed from finite number of samples according to (2.1). Our first condition, parallel to Condition 2, imposes a curvature constraint on Qn(·|·). In order to guarantee that the estimation error ‖β(t) − β∗‖ in step t of the EM algorithm is well controlled, we would like Qn(·|β(t−1)) to be strongly concave at β∗. However, in the setting where n p, there might exist directions along which Qn(·|β(t−1)) is flat, e.g., as in mixed linear regression and missing covariate regression. In contrast with Condition 2, we only require Qn(·|·) to be strongly concave over a particular set C(S,S;R) that is defined in terms of the subspace pair (S,S) and regularizerR. This set is defined as follows:\nC(S,S;R) := { u ∈ Rp : ∥∥ΠS⊥(u)∥∥R ≤ 2 · ∥∥ΠS(u)∥∥R + 2 ·Ψ(S) · ∥∥u∥∥}, (4.3)\nwhere the projection operator ΠS : Rp → Rp is defined as ΠS(u) := arg minv∈S ‖v − u‖. The restricted strong concavity (RSC) condition is as follows.\nCondition 4 (RSC (γn,S,S, r, δ)). For any fixed β ∈ B(r;β∗), with probability at least 1− δ, we have that for all β′ − β∗ ∈ Ω ⋂ C(S,S;R),\nQn(β ′|β)−Qn(β∗|β)− 〈 ∇Qn(β∗|β),β′ − β∗ 〉 ≤ −γn\n2 ‖β′ − β∗‖2.\nThe above condition states that Qn(·|β) is strongly concave in directions β′ − β∗ that belong to C(S,S;R). It is instructive to compare Condition 4 with a related condition proposed by [14] for analyzing high dimensional M-estimators. They require the loss function to be strongly convex over the cone {u ∈ Rp : ‖ΠS⊥(u)‖R . ‖ΠS(u)‖R}. Therefore our restrictive set (4.3) is similar to the cone but has the additional term 2Ψ(S)‖u‖. The main purpose of the term 2Ψ(S)‖u‖ is to allow the regularization parameter λn to jointly control optimization and statistical error. We note that while Condition 4 is stronger than the usual RSC condition in M-estimator, in typical settings the difference is immaterial. This is because\n∥∥ΠS(u)∥∥R is within a constant factor of Ψ(S) · ∥∥u∥∥, and hence checking RSC over C amounts to checking it over ‖ΠS⊥(u)‖R . Ψ(S)‖u‖, which is indeed what is typically also done in the M-estimator setting.\nFinally, we establish the condition that characterizes the achievable statistical error. Condition 5 (Statistical Error (∆n, r, δ)). For any fixed β ∈ B(r;β∗), with probability at least 1− δ, we have ∥∥∇Qn(β∗|β)−∇Q(β∗|β)∥∥R∗ ≤ ∆n. (4.4) This quantity replaces the term ‖Mn(β)−M(β)‖which appears in [1] and [20], and which presents problems in the high dimensional regime."
    }, {
      "heading" : "4.3 Main Results",
      "text" : "In this section, we provide the theoretical guarantees for a resampled version of our regularized EM algorithm: we split the whole dataset into T pieces and use a fresh piece of data in each iteration of regularized EM. As in [1], resampling makes it possible to check that Conditions 4-5 are satisfied without requiring them to hold uniformly for all β ∈ B(r;β∗) with high probability. Our empirical results indicate that it is not in fact required and is an artifact of the analysis. We refer to this resampled version as Algorithm 2. In the sequel, we letm := n/T to denote the sample complexity in each iteration. We let α := supu∈Rp\\{0} ‖u‖∗/‖u‖, where ‖ · ‖∗ is the dual norm of ‖ · ‖.\nFor Algorithm 2, our main result is as follows. The proof is deferred to the Supplemental Material. Theorem 1. Assume the model parameter β∗ ∈ S and regularizerR is decomposable with respect to (S,S) where S ⊆ S ⊆ Rp. Assume r > 0 is such that B(r;β∗) ⊆ Ω. Further, assume function Q(·|·), defined in (2.2), is self consistent and satisfies Conditions 2-3 with parameters (γ, µ, r) and (τ, r). Given n samples and T iterations, let m := n/T . Assume Qm(·|·), computed from any m i.i.d. samples according to (2.1), satisfies Conditions 4-5 with parameters (γm,S,S, r, 0.5δ/T ) and (∆m, r, 0.5δ/T ). Let κ∗ := 5 αµτγγm , and assume 0 < τ < γ and 0 < κ\n∗ ≤ 3/4. Define ∆ := rγm/[60Ψ(S)] and assume ∆m is such that ∆m ≤ ∆.\nConsider Algorithm 2 with initialization β(0) ∈ B(r;β∗) and with regularization parameters given by\nλ(t)m = κ t γm 5Ψ(S) ‖β(0) − β∗‖+ 1− κ t 1− κ ∆, t = 1, 2, . . . , T (4.5)\nfor any ∆ ∈ [3∆m, 3∆], κ ∈ [κ∗, 3/4]. Then with probability at least 1 − δ, we have that for any t ∈ [T ],\n‖β(t) − β∗‖ ≤ κt‖β(0) − β∗‖+ 5 γm\n1− κt 1− κ Ψ(S)∆. (4.6)\nThe estimation error is bounded by a term decaying linearly with number of iterations t, which we can think of as the optimization error and a second term that characterizes the ultimate estimation error of our algorithm. With T = O(log n) and suitable choice of ∆ such that ∆ = O(∆n/T ), we bound the ultimate estimation error as\n‖β(T ) − β∗‖ . 1 (1− κ)γn/T Ψ(S)∆n/T . (4.7)\nWe note that overestimating the initial error, ‖β(0)−β∗‖ is not important, as it may slightly increase the overall number of iterations, but will not impact the ultimate estimation error.\nThe constraint ∆m . rγm/Ψ(S) ensures that β(t) is contained in B(r;β∗) for all t ∈ [T ]. This constraint is quite mild in the sense that if ∆m = Ω(rγm/Ψ(S)), β(0) is a decent estimator with estimation error O(Ψ(S)∆m/γm) that already matches our expectation."
    }, {
      "heading" : "5 Examples: Applying the Theory",
      "text" : "Now we introduce three well known latent variable models. For each model, we first review the standard EM algorithm formulations, and discuss the extensions to the high dimensional setting. Then we apply Theorem 1 to obtain the statistical guarantee of the regularized EM with data splitting (Algorithm 2). The key ingredient underlying these results is to check the technical conditions in Section 4 hold for each model. We postpone these tedious details to the Supplemental Material."
    }, {
      "heading" : "5.1 Gaussian Mixture Model",
      "text" : "We consider the balanced isotropic Gaussian mixture model (GMM) with two components where the distribution of random variables (Y, Z) ∈ Rp × {−1, 1} is characterized as\nPr (Y = y|Z = z) = φ(y; z · β∗, σ2Ip), Pr(Z = 1) = Pr(Z = −1) = 1/2. Here we use φ(·|µ,Σ) to denote the probability density function of N (µ,Σ). In this example, Z is the latent variable that indicates the cluster id of each sample. Given n i.i.d. samples {yi}ni=1, function Qn(·|·) defined in (2.1) corresponds to\nQGMMn (β ′|β) = − 1\n2n n∑ i=1 [ w(yi;β)‖yi − β′‖22 + (1− w(yi;β))‖yi + β′‖22 ] , (5.1)\nwhere w(y;β) := exp (−‖y−β‖ 2 2 2σ2 )[exp (− ‖y−β‖22 2σ2 ) + exp (− ‖y+β‖22 2σ2 )] −1. We assume β∗ ∈ B0(s; p) := {u ∈ Rp : | supp(u)| ≤ s}. Naturally, we choose the regularizer R(·) to be the `1 norm. We define the signal-to-noise ratio SNR := ‖β∗‖2/σ. Corollary 1 (Sparse Recovery in GMM). There exist constants ρ, C such that if SNR ≥ ρ, n/T ≥ [80C(‖β∗‖∞ + σ)/‖β∗‖2]2 s log p, β(0) ∈ B(‖β∗‖2/4;β∗); then with probability at least 1−T/p Algorithm 2 with parameters ∆ = C(‖β∗‖∞ + σ) √ T log p/n, λ(0)n/T = 0.2‖β (0) −β∗‖2/ √ s, any κ ∈ [1/2, 3/4] and `1 regularization generates β(t) that has estimation error\n‖β(t) − β∗‖2 ≤ κt‖β(0) − β∗‖2 + 5C(‖β∗‖∞ + σ)\n1− κ\n√ s log p\nn T , for all t ∈ [T ]. (5.2)\nNote that by setting T log(n/ log p), the order of final estimation error turns out to be (‖β∗‖∞ + δ) √ (s log p)/n) log (n/ log p). The minimax rate for estimating s-sparse vector in a\nsingle Gaussian cluster is √ s log p/n, thereby the rate is optimal on (n, p, s) up to a log factor."
    }, {
      "heading" : "5.2 Mixed Linear Regression",
      "text" : "Mixed linear regression (MLR), as considered in some recent work [5, 7, 22], is the problem of recovering two or more linear vectors from mixed linear measurements. In the case of mixed linear regression with two symmetric and balanced components, the response-covariate pair (Y,X) ∈ R× Rp is linked through Y = 〈X, Z · β∗〉+W, whereW is the noise term andZ is the latent variable that has Rademacher distribution over {−1, 1}. We assume X ∼ N (0, Ip), W ∼ N (0, σ2). In this setting, with n i.i.d. samples {yi,xi}ni=1 of pair (Y,X), function Qn(·|·) then corresponds to\nQMLRn (β ′|β) = − 1\n2n n∑ i=1 [ w(yi,xi;β)(yi − 〈xi,β′〉)2 + (1− w(yi,xi;β))(yi + 〈xi,β′〉)2 ] ,\n(5.3)\nwhere w(y,x;β) := exp (− (y−〈x,β〉) 2 2σ2 )[exp (− (y−〈x,β〉)2 2σ2 ) + exp (− (y+〈x,β〉)2 2σ2 )] −1. We consider two kinds of structure on β∗:\nSparse Recovery. Assume β∗ ∈ B0(s; p). Then let R be the `1 norm, as in the previous section. We define SNR := ‖β∗‖2/σ. Corollary 2 (Sparse recovery in MLR). There exist constant ρ, C,C ′ such that if SNR ≥ ρ, n/T ≥ C ′ [(‖β∗‖2 + δ)/‖β∗‖2]2 s log p, β(0) ∈ B(‖β∗‖2/240,β∗); then with probability at least 1−T/p Algorithm 2 with parameters ∆ = C(‖β∗‖2 + δ) √ T log p/n, λ(0)n/T = ‖β (0) − β∗‖2/(15 √ s), any κ ∈ [1/2, 3/4] and `1 regularization generates β(t) that has estimation error\n‖β(t) − β∗‖2 ≤ κt‖β(0) − β∗‖2 + 15C(‖β∗‖2 + δ)\n1− κ\n√ s log p\nn T , for all t ∈ [T ].\nPerforming T log(n/(s log p)) iterations gives us estimation rate (‖β∗‖2 + δ) √\n(s log p/n) log (n/(s log p)) which is near-optimal on (s, p, n). The dependence on ‖β∗‖2, which also appears in the analysis of EM in the classical (low dimensional) setting [1], arises from fundamental limits of EM. Removing such dependence for MLR is possible by convex relaxation [7]. It is interesting to study how to remove it in the high dimensional setting.\nLow Rank Recovery. Second we consider the setting where the model parameter is a matrix Γ∗ ∈ Rp1×p2 with rank(Γ∗) = θ min(p1, p2). We further assume X ∈ Rp1×p2 is an i.i.d. Gaussian matrix, i.e., entries of X are independent random variables with distribution N (0, 1). We apply nuclear norm regularization to serve the low rank structure, i.e, R(Γ) = ∑p1,p2 i=1 |si(Γ)|, where si(Γ) is the ith singular value of Γ. Similarly, we let SNR := ‖Γ∗‖F /σ. Corollary 3 (Low rank recovery in MLR). There exist constant ρ, C,C ′ such that if SNR ≥ ρ, n/T ≥ C ′ [(‖Γ∗‖F + σ)/‖Γ∗‖F ]2 θ(p1 + p2), Γ(0) ∈ B(‖Γ∗‖F /1600,Γ∗); then with probability at least 1 − T exp(−p1 − p2) Algorithm 2 with parameters ∆ = C(‖Γ∗‖F + σ) √ T (p1 + p2)/n, λ (0) n/T = 0.01‖Γ (0) − Γ∗‖F / √\n2θ, any κ ∈ [1/2, 3/4] and nuclear norm regularization generates Γ(t) that has estimation error\n‖Γ(t) − Γ∗‖F ≤ κt‖Γ(0) − Γ∗‖F + 100C ′(‖Γ∗‖F + σ)\n1− κ\n√ 2θ(p1 + p2)\nn T , for all t ∈ [T ].\nThe standard low rank matrix recovery with a single component, including other sensing matrix designs beyond the Gaussianity, has been studied extensively (e.g., [2, 4, 13, 15]). To the best of our knowledge, the theoretical study of the mixed low rank matrix recovery has not been considered."
    }, {
      "heading" : "5.3 Missing Covariate Regression",
      "text" : "As our last example, we consider the missing covariate regression (MCR) problem. To parallel standard linear regression, {yi,xi}ni=1 are samples of (Y,X) linked through Y = 〈X,β∗〉 + W . However, we assume each entry of xi is missing independently with probability ∈ (0, 1). Therefore, the observed covariate vector x̃i takes the form\nx̃i,j = { xi,j with probability 1− ∗ otherwise .\nWe assume the model is under Gaussian design X ∼ N (0, Ip),W ∼ N (0, σ2). We refer the reader to our Supplementary Material for the specific Qn(·|·) function. In high dimensional case, we assume β∗ ∈ B0(s; p). We define ρ := ‖β∗‖2/σ to be the SNR and ω := r/‖β∗‖2 to be the relative contractivity radius. In particular, let ζ := (1 + ω)ρ. Corollary 4 (Sparse Recovery in MCR). There exist constantsC,C ′, C0, C1 such that if (1+ω)ρ ≤ C0 < 1, < C1, n/T ≥ C ′max{σ2(ωρ)−1, 1}s log p, β(0) ∈ B(ω‖β∗‖2,β∗); then with probability at least 1 − T/p Algorithm 2 with parameters ∆ = Cσ √ T log p/n, λ(0)n/T = ‖β\n(0) − β∗‖2/(45 √ s), any κ ∈ [1/2, 3/4] and `1 regularization generates β(t) that has estimation error\n‖β(t) − β∗‖2 ≤ κt‖β(0) − β∗‖2 + 45Cσ\n1− κ\n√ s log p\nn T , for all t ∈ [T ],\nUnlike the previous two models, we require an upper bound on the signal to noise ratio. This unusual constraint is in fact unavoidable [10]. By optimizing T , the order of final estimation error turns out to be σ √ s log p/n log(n/(s log p))."
    }, {
      "heading" : "6 Simulations",
      "text" : "We now provide some simulation results to back up our theory. Note that while Theorem 1 requires resampling, we believe in practice this is unnecessary. This is validated by our results, where we apply Algorithm 1 to the four latent variable models discussed in Section 5.\nConvergence Rate. We first evaluate the convergence of Algorithm 1 assuming only that the initialization is a bounded distance from β∗. For a given error ω‖β∗‖2, the initial parameter β(0) is picked randomly from the sphere centered around β∗ with radius ω‖β∗‖2. We use Algorithm 1 with T = 7, κ = 0.7, λ(0)n in Theorem 1. The choice of the critical parameter ∆ is given in the Supplementary Material. For every single trial, we report estimation error ‖β(t) − β∗‖2 and optimization error ‖β(t) − β(T )‖2 in every iteration. We plot the log of errors over iteration t in Figure 1.\nStatistical Rate. We now evaluate the statistical rate. We set T = 7 and compute estimation error on β̂ := β(T ). In Figure 2, we plot ‖β̂−β∗‖2 over normalized sample complexity, i.e., n/(s log p) for s-sparse parameter and n/(θp) for rank θ p-by-p parameter. We refer the reader to Figure 1 for other settings. We observe that the same normalized sample complexity leads to almost identical estimation error in practice, which thus supports the corresponding statistical rate established in Section 5."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to acknowledge NSF grants 1056028, 1302435 and 1116955. This research was also partially supported by the U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center."
    } ],
    "references" : [ {
      "title" : "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
      "author" : [ "Sivaraman Balakrishnan", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "arXiv preprint arXiv:1408.2156,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Rop: Matrix recovery via rank-one projections",
      "author" : [ "T Tony Cai", "Anru Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "The Dantzig selector: statistical estimation when p is much larger than n",
      "author" : [ "Emmanuel Candes", "Terence Tao" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements",
      "author" : [ "Emmanuel J Candès", "Yaniv Plan" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Spectral experts for estimating mixtures of linear regressions",
      "author" : [ "Arun Tejasvi Chaganty", "Percy Liang" ],
      "venue" : "arXiv preprint arXiv:1306.3729,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Improved graph clustering",
      "author" : [ "Yudong Chen", "Sujay Sanghavi", "Huan Xu" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "A convex formulation for mixed regression with two components: Minimax optimal rates",
      "author" : [ "Yudong Chen", "Xinyang Yi", "Constantine Caramanis" ],
      "venue" : "In Conf. on Learning Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society",
      "author" : [ "Arthur P Dempster", "Nan M Laird", "Donald B Rubin" ],
      "venue" : "Series B (methodological),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1977
    }, {
      "title" : "High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity",
      "author" : [ "Po-Ling Loh", "Martin J Wainwright" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Corrupted and missing predictors: Minimax bounds for highdimensional linear regression",
      "author" : [ "Po-Ling Loh", "Martin J Wainwright" ],
      "venue" : "In Information Theory Proceedings (ISIT),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "Asymptotic convergence properties of the em algorithm with respect to the overlap in the mixture",
      "author" : [ "Jinwen Ma", "Lei Xu" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "The EM algorithm and extensions, volume 382",
      "author" : [ "Geoffrey McLachlan", "Thriyambakam Krishnan" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
      "author" : [ "Sahand Negahban", "Martin J Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Sahand Negahban", "Bin Yu", "Martin J Wainwright", "Pradeep K Ravikumar" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
      "author" : [ "Benjamin Recht", "Maryam Fazel", "Pablo A Parrilo" ],
      "venue" : "SIAM review,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "L1-penalization for mixture regression models",
      "author" : [ "Nicolas Städler", "Peter Bühlmann", "Sara Van De Geer" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "An analysis of the em algorithm and entropy-like proximal point methods",
      "author" : [ "Paul Tseng" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "Roman Vershynin" ],
      "venue" : "arXiv preprint arXiv:1011.3027,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Structured regularizers for high-dimensional problems: Statistical and computational issues",
      "author" : [ "Martin J Wainwright" ],
      "venue" : "Annual Review of Statistics and Its Application,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "High dimensional expectation-maximization algorithm: Statistical optimization and asymptotic normality",
      "author" : [ "Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu" ],
      "venue" : "arXiv preprint arXiv:1412.8729,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "On the convergence properties of the em algorithm",
      "author" : [ "C.F.Jeff Wu" ],
      "venue" : "The Annals of statistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1983
    }, {
      "title" : "Alternating minimization for mixed linear regression",
      "author" : [ "Xinyang Yi", "Constantine Caramanis", "Sujay Sanghavi" ],
      "venue" : "arXiv preprint arXiv:1310.3745,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recently, work in [1] has demonstrated that for an important class of problems, EM exhibits linear local convergence.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : ", à la [19]) is not guaranteed to provide this balance.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 18,
      "context" : ", [19]) the regularizer should be chosen proportional to the target estimation error.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : ", [8, 12]) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 11,
      "context" : ", [8, 12]) is a general algorithmic approach for handling latent variable models (including mixtures), popular largely because it is typically computationally highly scalable, and easy to implement.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "Very recent work in [1] establishes a general local convergence theorem (i.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "A central challenge in extending EM (and as a corollary, the analysis in [1]) to the high-dimensional regime is the M -step.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : "Recent work in [20] treats high-dimensional EM using a truncatedM -step.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 19,
      "context" : "In contrast to work in [20], we pursue a high-dimensional extension via regularization.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : "Finally, we note that for finite mixture regression, Städler et al.[16] consider an `1 regularized EM algorithm for which they develop some asymptotic analysis and oracle inequality.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "This quantity is used in [1] as well as in follow-up work in [20], as a measure of statistical error.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 19,
      "context" : "This quantity is used in [1] as well as in follow-up work in [20], as a measure of statistical error.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : ", [3, 6, 14, 19]), have been shown to be useful both empirically and theoretically for high dimensional structural estimation, and they also play an important role in our analytical framework.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : ", [3, 6, 14, 19]), have been shown to be useful both empirically and theoretically for high dimensional structural estimation, and they also play an important role in our analytical framework.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 13,
      "context" : ", [3, 6, 14, 19]), have been shown to be useful both empirically and theoretically for high dimensional structural estimation, and they also play an important role in our analytical framework.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : ", [3, 6, 14, 19]), have been shown to be useful both empirically and theoretically for high dimensional structural estimation, and they also play an important role in our analytical framework.",
      "startOffset" : 2,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "2 Conditions on Q(·|·) and Qn(·|·) Next, we review three technical conditions, originally proposed by [1], on the population levelQ(·|·) function, and then we give two important conditions that the empirial function Qn(·|·) must satisfy, including one that characterizes the statistical error.",
      "startOffset" : 102,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "Following the lowdimensional development in [1], our results are local, and apply to an r-neighborhood region around β∗: B(r;β∗) := { u ∈ Ω, ‖u− β∗‖ ≤ r } .",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "This is satisfied, in particular, when β∗ maximizes the population log likelihood function, as happens in most settings of interest [12].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "It has also been called First Order Stability in [1].",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "It is instructive to compare Condition 4 with a related condition proposed by [14] for analyzing high dimensional M-estimators.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "This quantity replaces the term ‖Mn(β)−M(β)‖which appears in [1] and [20], and which presents problems in the high dimensional regime.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 19,
      "context" : "This quantity replaces the term ‖Mn(β)−M(β)‖which appears in [1] and [20], and which presents problems in the high dimensional regime.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 0,
      "context" : "As in [1], resampling makes it possible to check that Conditions 4-5 are satisfied without requiring them to hold uniformly for all β ∈ B(r;β∗) with high probability.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : "Mixed linear regression (MLR), as considered in some recent work [5, 7, 22], is the problem of recovering two or more linear vectors from mixed linear measurements.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "Mixed linear regression (MLR), as considered in some recent work [5, 7, 22], is the problem of recovering two or more linear vectors from mixed linear measurements.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 21,
      "context" : "Mixed linear regression (MLR), as considered in some recent work [5, 7, 22], is the problem of recovering two or more linear vectors from mixed linear measurements.",
      "startOffset" : 65,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "The dependence on ‖β‖2, which also appears in the analysis of EM in the classical (low dimensional) setting [1], arises from fundamental limits of EM.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "Removing such dependence for MLR is possible by convex relaxation [7].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 9,
      "context" : "This unusual constraint is in fact unavoidable [10].",
      "startOffset" : 47,
      "endOffset" : 51
    } ],
    "year" : 2015,
    "abstractText" : "Latent models are a fundamental modeling tool in machine learning applications, but they present significant computational and analytical challenges. The popular EM algorithm and its variants, is a much used algorithmic tool; yet our rigorous understanding of its performance is highly incomplete. Recently, work in [1] has demonstrated that for an important class of problems, EM exhibits linear local convergence. In the high-dimensional setting, however, the M -step may not be well defined. We address precisely this setting through a unified treatment using regularization. While regularization for high-dimensional problems is by now well understood, the iterative EM algorithm requires a careful balancing of making progress towards the solution while identifying the right structure (e.g., sparsity or low-rank). In particular, regularizing the M -step using the state-of-the-art highdimensional prescriptions (e.g., à la [19]) is not guaranteed to provide this balance. Our algorithm and analysis are linked in a way that reveals the balance between optimization and statistical errors. We specialize our general framework to sparse gaussian mixture models, high-dimensional mixed regression, and regression with missing variables, obtaining statistical guarantees for each of these examples.",
    "creator" : null
  }
}