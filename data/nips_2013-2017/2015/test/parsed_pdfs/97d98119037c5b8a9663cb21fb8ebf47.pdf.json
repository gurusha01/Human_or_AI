{
  "name" : "97d98119037c5b8a9663cb21fb8ebf47.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Sample Complexity Bounds for Iterative Stochastic Policy Optimization",
    "authors" : [ "Marin Kobilarov" ],
    "emails" : [ "marin@jhu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider a general class of stochastic optimization problems formulated as\nξ∗ = arg min ξ Eτ∼p(·|ξ)[J(τ)], (1)\nwhere ξ defines a vector of decision variables, τ represents the system response defined through the density p(τ |ξ), and J(τ) defines a positive cost function which can be non-smooth and nonconvex. It is assumed that p(τ |ξ) is either known or can be sampled from, e.g. in a black-box manner. The objective is to obtain high-confidence sample complexity bounds on the expected cost for a given decision strategy by observing past realizations of possibly different strategies. Such bounds are useful for two reasons: 1) for providing robustness guarantees for future executions, and 2) for designing new algorithms that directly minimize the bound and therefore are expected to have built-in robustness.\nOur primary motivation arises from applications in robotics, for instance when a robot executes control policies to achieve a given task such as navigating to a desired state while perceiving the environment and avoiding obstacles. Such problems are traditionally considered in the framework of reinforcement learning and addressed using policy search algorithms, e.g. [1, 2] (see also [3] for a comprehensive overview with a focus on robotic applications [4]). When an uncertain system model is available the problem is equivalent to robust model-predictive control (MPC) [5].\nOur specific focus is on providing formal guarantees on future executions of control algorithms in terms of maximum expected cost (quantifying performance) and maximum probability of constraint violation (quantifying safety). Such bounds determine the reliability of control in the presence of process, measurement and parameter uncertainties, and contextual changes in the task. In this work we make no assumptions about nature of the system structure, such as linearity, convexity, or Gaussianity. In addition, the proposed approach applies either to a physical system without an available\nmodel, to an analytical stochastic model, or to a white-box model (e.g. from a high-fidelity opensource physics engine). In this context, PAC bounds have been rarely considered but could prove essential for system certification, by providing high-confidence guarantees for future performance and safety, for instance “with 99% chance the robot will reach the goal within 5 minutes”, or “with 99% chance the robot will not collide with obstacles”. Approach. To cope with such general conditions, we study robustness through a statistical learning viewpoint [6, 7, 8] using finite-time sample complexity bounds on performance based on empirical runs. This is accomplished using concentration-of-measure inequalities [9] which provide only probabilistic bounds , i.e. they certify the algorithm execution in terms of statements such as: “in future executions, with 99% chance the expected cost will be less than X and the probability of collision will be less than Y”. While such bounds are generally applicable to any stochastic decision making process, our focus and initial evaluation is on stochastic control problems. Randomized methods in control analysis. Our approach is also inspired by existing work on randomized algorithms in control theory originally motivated by robust linear control design [10]. For example, early work focused on probabilistic root-locus design [11] and later applied to constraint satisfaction [12] and general cost functions [13]. High-confidence bounds for decidability of linear stability were refined in [14]. These are closely related to the concepts of randomized stability robustness analysis (RSRA) and randomized performance robustness analysis (RPRA) [13]. Finite-time probabilistic bounds for system identification problems have also been obtained through a statistical learning viewpoint [15]."
    }, {
      "heading" : "2 Iterative Stochastic Policy Optimization",
      "text" : "Instead of directly searching for the optimal ξ to solve (1) a common strategy in direct policy search and global optimization [16, 17, 18, 19, 20, 21] is to iteratively construct a surrogate stochastic model π(ξ|ν) with hyper-parameters ν ∈ V , such as a Gaussian Mixture Model (GMM), where V is a vector space. The model induces a joint density p(τ, ξ|ν) = p(τ |ξ)π(ξ|ν) encoding natural stochasticity p(τ |ξ) and artificial control-exploration stochasticity π(ξ|ν). The problem is then to find ν to minimize the expected cost\nJ (v) , E τ,ξ∼p(·|ν) [J(τ)], iteratively until convergence, which in many cases also corresponds to π(·|ν) shrinking close to a delta function around the optimal ξ∗ (or to multiple peaks when multiple disparate optima exist as long as π is multi-modal).\nThe typical flow of the iterative policy optimization algorithms considered in this work is:\nIterative Stochastic Policy Optimization (ISPO) 0. Start with initial hyper-parameters ν0 (i.e. a prior), set i = 0 1. Sample M trajectories (ξj , τj) ∼ p(·|νi) for j = 1, . . . ,M 2. Compute new policy νi+1 using observed costs J(τj) 3. Compute bound on expected cost and Stop if below threshold, else set i= i+1 and Goto 1\nThe purpose of computing probably-approximate bounds is two-fold: a) to analyze the performance of such standard policy search algorithms; b) to design new algorithms by not directly minimizing an estimate of the expected cost, but by minimizing an upper confidence bound on the expected cost instead. The computed policy will thus have “built-in” robustness in the sense that, with highprobability, the resulting cost will not exceed an a-priori known value. The present paper develops bounds applicable to both (a) and (b), but only explores their application to (a), i.e. to the analysis of existing iterative policy search methods.\nCost functions. We consider two classes of cost functions J . The first class encodes system performance and is defined as a bounded real-valued function such that 0 ≤ J(τ) ≤ b for any τ . The second are binary-valued indicator functions representing constraint violation. Assume that the variable τ must satisfy the condition g(τ) ≤ 0. The cost is then defined as J(τ) = I{g(τ)>0} and its expectation can be regarded as the probability of constraint violation, i.e.\nP(g(τ) > 0) = Eτ∼p(·|ξ)I{g(τ)>0}. In this work, we will be obtain bounds for both classes of cost functions."
    }, {
      "heading" : "3 A Specific Application: Discrete-time Stochastic Control",
      "text" : "We next illustrate the general stochastic optimization setting using a classical discrete-time nonlinear optimal control problem. Specific instances of such control problems will later be used for numerical evaluation. Consider a discrete-time dynamical model with state xk ∈ X , where X is an n-dimensional manifold, and control inputs uk ∈ Rm at time tk ∈ [0, T ] where k = 0, . . . , N denotes the time stage. Assume that the system dynamics are given by\nxk+1 = fk(xk, uk, wk), subject to gk(xk, uk) ≤ 0, gN (xN ) ≤ 0,\nwhere fk and gk correspond either to the physical plant, to an analytical model, or to a “white-box” high-fidelity physics-engine update step. The terms wk denotes process noise. Equivalently, such a formulation induces the process model density p(xk+1|xk, uk). In addition, consider the cost\nJ(x0:N , u0:N−1) , N−1∑ k=0 Lk(xk, uk) + LN (xN ),\nwhere x0:N , {x0, . . . , xN} denotes the complete trajectory and Lk are given nonlinear functions. Our goal is to design feedback control policies to optimize the expected value of J . For simplicity, we will assume perfect measurements although this does not impose a limitation on the approach.\nAssume that any decision variables in the problem (such as feedforward or feedback gains, obstacle avoidance terms, mode switching variables) are encoded using a finite-dimensional vector ξ ∈ Rnξ and define the control law uk = Φk(xk)ξ using basis functions Φk(x) ∈ Rm×nξ for all k = 0, . . . , N − 1. This representation captures both static feedback control laws as well as time-varying optimal control laws of the form uk = u∗k + K LQR k (xk − x∗k) where u∗k = B(tk)ξ is an optimized feedforward control (parametrized using basis functions B(t) ∈ Rm×z such as B-splines), KLQRk is the optimal feedback gain matrix of the LQR problem based on the linearized dynamics and second-order cost expansion around the optimized nominal reference trajectory x∗, i.e. such that x∗k+1 = fk(x ∗ k, u ∗ k, 0).\nThe complete trajectory of the system is denoted by the random variable τ = (x0:N , u0:N−1) and has density p(τ |ξ) = p(x0)ΠN−1k=0 p(xk+1|xk, uk)δ(uk − Φk(xk)ξ), where δ(·) is the Dirac delta. The trajectory constraint takes the form {g(τ) ≤ 0} , ∧N−1 k=0 {gk(xk, uk) ≤ 0} ∧ {gN (xN ) ≤ 0}.\nA simple example. As an example, consider a point-mass robot modeled as a double-integrator system with state x = (p, v) where p ∈ Rd denotes position and v ∈ Rd denotes velocity with d = 2 for planar workspaces and d = 3 for 3-D workspaces. The dynamics is given, for ∆t = T/N , by\npk+1 = pk + ∆tvk + 1\n2 ∆t2(uk + wk),\nvk+1 = vk + ∆t(uk + wk),\nwhere uk are the applied controls and wk is zero-mean white noise. Imagine that the constraint gk(x, u) ≤ 0 defines circular obstacles O ⊂ Rd and control norm bounds defined as\nro − ‖p− po‖ ≤ 0, ‖u‖ ≤ umax,\nwhere ro is the radius of an obstacle at position po ∈ Rd. The cost J could be arbitrary but a typical choice is L(x, u) = 12‖u‖ 2 R + q(x) where R > 0 is a given matrix and q(x) is a nonlinear function defining a task. The final cost could force the system towards a goal state xf ∈ Rn (or a region Xf ⊂ Rn) and could be defined according to LN (x) = 12‖x− xf‖ 2 Qf\nfor some given matrix Qf ≥ 0. For such simple systems one can choose a smooth feedback control law uk = Φk(x)ξ with static positive gains ξ = (kp, kd, ko) ∈ R3 and basis function\nΦ(x) = [ pf − p vf − v ϕ(x,O) ] ,\nwhere ϕ(x,O) is an obstacle-avoidance force, e.g. defined as the gradient of a potential field or as a gyroscopic “steering” force ϕ(x,O) = s(x,O)× v that effectively rotates the velocity vector [22] . Alternatively, one could employ a time-varying optimal control law as described in §3."
    }, {
      "heading" : "4 PAC Bounds for Iterative Policy Adaptation",
      "text" : "We next compute probabilistic bounds on the expected cost J (ν) resulting from the execution of a new stochastic policy with hyper-parameters ν using observed samples from previous policies ν0, ν1, . . . . The bound is agnostic to how the policy is updated (i.e. Step 2 in the ISPO algorithm)."
    }, {
      "heading" : "4.1 A concentration-of-measure inequality for policy adaptation",
      "text" : "The stochastic optimization setting naturally allows the use of a prior belief ξ ∼ π(·|ν0) on what good control laws could be, for some known ν0 ∈ V . After observing M executions based on such prior, we wish to find a new improved policy π(·|ν) which optimizes the cost\nJ (ν) , Eτ,ξ∼p(·|ν)[J(τ)] = Eτ,ξ∼p(·|ν0) [ J(τ)\nπ(ξ|ν) π(ξ|ν0)\n] , (2)\nwhich can be approximated using samples ξj ∼ π(ξ|ν0) and τj ∼ p(τ |ξj) by the empirical cost\n1\nM M∑ j=1 [ J(τj) π(ξj |ν) π(ξj |ν0) ] . (3)\nThe goal is to compute the parameters ν using the sampled decision variables ξj and the corresponding observed costs J(τj). Obtaining practical bounds for J (ν) becomes challenging since the change-of-measure likelihood ratio π(ξ|ν)π(ξ|ν0) can be unbounded (or have very large values) [23] and a standard bound, e.g. such as Hoeffding’s or Bernstein’s becomes impractical or impossible to apply. To cope with this we will employ a recently proposed robust estimation [24] technique stipulating that instead of estimating the expectation m = E[X] of a random variable X ∈ [0,∞) using its empirical mean m̂ = 1M ∑M j=1Xj , a more robust estimate can be obtained by truncating its higher\nmoments, i.e. using m̂α , 1αM ∑M j=1 ψ(αXj) for some α > 0 where\nψ(x) = log(1 + x+ 1\n2 x2). (4)\nWhat makes this possible is the key assumption that the (possibly unbounded) random variable must have bounded second moment. We employ this idea to deal with the unboundedness of the policy adaptation ratio by showing that in fact its second moment can be bounded and corresponds to an information distance between the current and previous stochastic policies.\nTo obtain sharp bounds though it is useful to employ samples over multiple iterations of the ISPO algorithm, i.e. from policies ν0, ν1, . . . , νL−1 computed in previous iterations. To simplify notation let z = (τ, ξ) and define `i(z, ν) , J(τ)\nπ(ξ|ν) π(ξ|νi) . The cost (2) of executing ν can now be equivalently\nexpressed as\nJ (ν) ≡ 1 L L−1∑ i=0 Ez∼p(·|νi)`i(z, ν)\nusing the computed policies in previous iterations i = 0, . . . , L− 1. We next state the main result: Proposition 1. With probability 1 − δ the expected cost of executing a stochastic policy with parameters ξ ∼ π(·|ν) is bounded according to:\nJ (ν) ≤ inf α>0\n{ Ĵα(ν) + α\n2L L−1∑ i=0 b2i e D2(π(·|ν)||π(·|νi)) + 1 αLM log 1 δ\n} , (5)\nwhere Ĵα(ν) denotes a robust estimator defined by\nĴα(ν) , 1\nαLM L−1∑ i=0 M∑ j=1 ψ (α`(zij , ν)) ,\ncomputed after L iterations, with M samples zi1, . . . , ziM ∼ p(·|νi) obtained at iterations i = 0, . . . , L− 1, where Dβ(p||q) denotes the Renyii divergence between p and q defined by\nDβ(p||q) = 1\nβ − 1 log\n∫ pβ(x)\nqβ−1(x) dx.\nThe constants bi are such that 0 ≤ J(τ) ≤ bi at each iteration i = 0, . . . , L− 1.\nProof. The bound is obtained by relating the mean to its robust estimate according to P ( LM(J (ν)− Ĵα(ν)) ≥ t ) = P ( eαLM(J (ν)−Ĵα(ν)) ≥ eαt ) ,\n≤ E [ eαLM(J (ν)−Ĵα(ν)) ] e−αt, (6)\n= e−αt+αLMJ (ν)E [ e ∑L−1 i=0 ∑M j=1−ψ(α`i(zij ,ν)) ] = e−αt+αLMJE\nL−1∏ i=0 M∏ j=1 e−ψ(α`i(zij ,ν))  = e−αt+αLMJ\nL−1∏ i=0 M∏ j=1 E z∼p(·|νi) [ 1− α`i(z, ν) + α2 2 `i(z, ν) 2 ] (7)\n= e−αt+αLMJ (ν) L−1∏ i=0 M∏ j=1 ( 1− αJ (ν) + α 2 2 E z∼p(·|νi)[`i(z, ν) 2] ) ≤ e−αt+αLMJ (ν) L−1∏ i=0 M∏ j=1 e−αJ (ν)+ α2 2 Ez∼p(·|νi)[`i(z,ν) 2] (8)\n≤ e−αt+M α 2 2 ∑L−1 i=0 Ez∼p(·|νi)[`i(z,ν) 2],\nusing Markov’s inequality to obtain (6), the identities ψ(x) ≥ − log(1 − x + 12x 2) in (7) and 1 + x ≤ ex in (8). Here, we adapted the moment-truncation technique proposed by Catoni [24] for general unbounded losses to our policy adaptation setting in order to handle the possibly unbounded likelihood ratio. These results are then combined with\nE [`i(z, ν)2] ≤ b2iEπ(·|νi) [ π(ξ|ν)2\nπ(ξ|νi)2\n] = b2i e D2(π||πi),\nwhere the relationship between the likelihood ratio variance and the Renyii divergence was established in [23].\nNote that the Renyii divergence can be regarded as a distance between two distribution and can be computed in closed bounded form for various distributions such as the exponential families; it is also closely related to the Kullback-Liebler (KL) divergence, i.e. D1(p||q) = KL(p||q)."
    }, {
      "heading" : "4.2 Illustration using simple robot navigation",
      "text" : "We next illustrate the application of these bounds using the simple scenario introduced in §3. The stochasticity is modeled using a Gaussian density on the initial state p(x0), on the disturbances wk and on the goal state xf . Iterative policy optimization is performed using a stochastic model π(ξ|ν) encoding a multivariate Gaussian, i.e.\nπ(ξ|ν) = N (ξ|µ,Σ)\nwhich is updated through reward-weighted-regression (RWR) [3], i.e. in Step 2 of the ISPO algorithm we take M samples, observe their costs, and update the parameters according to\nµ = M∑ j=1 w̄(τj)ξj , Σ = M∑ j=1 w̄(τj)(ξj − µ)(ξj − µ)T , (9)\nusing the tilting weights w(τ) = e−βJ(τ) for some adaptively chosen β > 0 and where w̄(τj) , w(τj)/ ∑M `=1 w(τ`) are the normalized weights.\nAt each iteration i one can compute a bound on the expected cost using the previously computed ν0, . . . , νi−1. We have computed such bounds using (5) for both the expected cost and probability of\ncollision, denoted respectively by J + and P+ using M = 200 samples (Figure 1) at each iteration. We used a window of maximum L = 10 previous iterations to compute the bounds, i.e. to compute νi+1 all samples from densities νi−L+1, νi−L+2, . . . , νi were used. Remarkably, using our robust statistics approach the resulting bound eventually becomes close to the standard empirical estimate Ĵ . The collision probability bound P+ decreses to less than 10% which could be further improved by employing more samples and more iterations. The significance of these bounds is that one can stop the optimization (regarded as training) at any time and be able to predict expected performance in future executions using the newly updated policy before actually executing the policy, i.e. using the samples from the previous iteration.\nFinally, the Renyii divergence term used in these computations takes the simple form\nDβ (N (·|µ0,Σ0)‖N (·|µ1,Σ1)) = β\n2 ‖µ1 − µ0‖2Σ−1β +\n1\n2(1− β) log |Σβ | |Σ0|1−β |Σ1|β ,\nwhere Σβ = (1− β)Σ0 + βΣ1."
    }, {
      "heading" : "4.3 Policy Optimization Methods",
      "text" : "We do not impose any restrictions on the specific method used for optimizing the policy π(ξ|ν). When complex constraints are present such computation will involve a global motion planning step combined with local feedback control laws (we show such an example in §5). The approach can be used to either analyze such policies computed using any method of choice or to derive new algorithms based on minimizing the right-hand side of the bound. The method also applies to modelfree learning. For instance, related to recent methods in robotics one could use reward-weightedregression (RWR) or policy learning by weighted samples with returns (PoWeR) [3], stochastic optimization methods such as [25, 26], or using the related cross-entropy optimization [16, 27]."
    }, {
      "heading" : "5 Application to Aerial Vehicle Navigation",
      "text" : "Consider an aerial vehicle such as a quadrotor navigating at high speed through a cluttered environment. We are interested in minimizing a cost metric related to the total time taken and control effort required to reach a desired goal state, while maintaining low probability of collision. We employ an experimentally identified model of an AscTec quadrotor (Figure 2) with 12-dimensional state space X = SE(3) × R6 with state x = (p,R, ṗ, ω) where p ∈ R3 is the position, R ∈ SO(3) is the rotation matrix, and ω ∈ R3 is the body-fixed angular velocity. The vehicle is controlled with inputs u = (F,M) ∈ R4 including the lift force F ≥ 0 and torque moments M ∈ R3. The dynamics is\nmp̈ = Re3F +mg + δ(p, ṗ), (10)\nṘ = Rω̂, (11) Jω̇ = Jω × ω +M, (12)\nwhere m is the mass, J–the inertia tensor, e3 = (0, 0, 1) and the matrix ω̂ is such that ω̂η = ω × η for any η ∈ R3. The system is subject to initial localization errors and also to random disturbances, e.g. due to wind gusts and wall effects, defined as stochastic forces δ(p, ṗ) ∈ R3. Each component in δ is zero-mean and has standard deviation of 3 Newtons, for a vehicle with mass m ≈ 1 kg. The objective is to navigate through a given urban environment at high speed to a desired goal state. We employ a two-stage approach consisting of an A*-based global planner which produces a sequence of local sub-goals that the vehicle must pass through. A standard nonlinear feedback backstepping controller based on a “slow” position control loop and a “fast” attitude control is employed [28, 29] for local control. In addition, and obstacle avoidance controller is added to avoid collisions since the vehicle is not expected to exactly follow the A* path. At each iterationM = 200 samples are taken with 1 − δ = 0.95 confidence level. A window of L = 5 past iterations were used for the bounds. The control density π(ξ|ν) is a single Gaussian as specified in §4.2. The most sensitive gains in the controller are the position proporitional and derivative terms, and the obstacle gains, denoted by kp, kd, and ko, which we examine in the following scenarios: a) fixed goal, wind gusts disturbances, virtual environment: the system is first tested in a cluttered\nsimulated environment (Figure 2). The simulated vehicle travels at an average velocity of 20 m/s (see video in Supplement) and initially experiences more than 50% collisions. After a few iterations the total cost stabilizes and the probability of collision reduces to around 15%. The bound is close to the empirical estimate which indicates that it can be tight if more samples are taken. The collision probability bound is still too high to be practical but our goal was only to illustrate the bound behavior. It is also likely that our chosen control strategy is in fact not suitable for high-speed traversal of such tight environments.\nb) sparser campus-like environment, randomly sampled goals: a more general evaluation was performed by adding the goal location to the stochastic problem parameters so that the bound will apply to any future desired goal in that environment (Figure 3). The algorithm converges to similar values as before, but this time the collision probability is smaller due to more expansive environment. In both cases, the bounds could be reduced further by employing more than M = 200 samples or by reusing more samples from previous runs according to Proposition 1."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper considered stochastic decision problems and focused on a probably-approximate bounds on robustness of the computed decision variables. We showed how to derive bounds for fixed policies in order to predict future performance and/or constraint violation. These results could then be employed for obtaining generalization PAC bounds, e.g. through a PAC-Bayesian approach which could be consistent with the proposed notion of policy priors and policy adaptation. Future work will develop concrete algorithms by directly optimizing such PAC bounds, which are expected to have built-in robustness properties."
    } ],
    "references" : [ {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S. Sutton", "David A. McAllester", "Satinder P. Singh", "Yishay Mansour" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1999
    }, {
      "title" : "Algorithms for Reinforcement Learning",
      "author" : [ "Csaba Szepesvari" ],
      "venue" : "Morgan and Claypool Publishers,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "A survey on policy search for robotics",
      "author" : [ "M.P. Deisenroth", "G. Neumann", "J. Peters" ],
      "venue" : "Probability of Collision empirical P robust Pα PAC bound P a)",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Learning control in robotics",
      "author" : [ "S. Schaal", "C. Atkeson" ],
      "venue" : "Robotics Automation Magazine, IEEE,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "Robust model predictive control: A survey",
      "author" : [ "Alberto Bemporad", "Manfred Morari" ],
      "venue" : "Robustness in identification and control,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "The nature of statistical learning theory",
      "author" : [ "Vladimir N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1995
    }, {
      "title" : "Pac-bayesian stochastic model selection",
      "author" : [ "David A. McAllester" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Tutorial on practical prediction theory for classification",
      "author" : [ "J Langford" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Concentration inequalities : a nonasymptotic theory of independence",
      "author" : [ "Stphane Boucheron", "Gbor Lugosi", "Pascal Massart", "Michel Ledoux" ],
      "venue" : "Oxford university press,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Randomized algorithms for robust controller synthesis using statistical learning",
      "author" : [ "M. Vidyasagar" ],
      "venue" : "theory. Automatica,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "A monte carlo approach to the analysis of control system",
      "author" : [ "Laura Ryan Ray", "Robert F. Stengel" ],
      "venue" : "robustness. Automatica,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1993
    }, {
      "title" : "Probabilistic control of nonlinear uncertain systems",
      "author" : [ "Qian Wang", "RobertF. Stengel" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Randomized algorithms for analysis and control of uncertain systems",
      "author" : [ "R. Tempo", "G. Calafiore", "F. Dabbene" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "Statistical learning control of uncertain systems: theory and algorithms",
      "author" : [ "V. Koltchinskii", "C.T. Abdallah", "M. Ariola", "P. Dorato" ],
      "venue" : "Applied Mathematics and Computation,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2001
    }, {
      "title" : "A learning theory approach to system identification and stochastic adaptive control",
      "author" : [ "M. Vidyasagar", "Rajeeva L. Karandikar" ],
      "venue" : "Journal of Process Control,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "The cross-entropy method: a unified approach to combinatorial optimization",
      "author" : [ "Reuven Y. Rubinstein", "Dirk P. Kroese" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Entropy search for information-efficient global optimization",
      "author" : [ "Philipp Hennig", "Christian J. Schuler" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Covariance matrix adaptation for multi-objective optimization",
      "author" : [ "Christian Igel", "Nikolaus Hansen", "Stefan Roth" ],
      "venue" : "Evol. Comput.,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Estimation of distribution algorithms: A new tool for evolutionary computation",
      "author" : [ "Pedro Larraaga", "Jose A. Lozano", "editors" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2002
    }, {
      "title" : "A survey of optimization by building and using probabilistic models",
      "author" : [ "Martin Pelikan", "David E. Goldberg", "Fernando G. Lobo" ],
      "venue" : "Comput. Optim. Appl.,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2002
    }, {
      "title" : "Principles of Robot Motion: Theory, Algorithms, and Implementations",
      "author" : [ "Howie Choset", "Kevin M. Lynch", "Seth Hutchinson", "George A Kantor", "Wolfram Burgard", "Lydia E. Kavraki", "Sebastian Thrun" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Learning Bounds for Importance Weighting",
      "author" : [ "Corinna Cortes", "Yishay Mansour", "Mehryar Mohri" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Challenging the empirical mean and empirical variance: A deviation study",
      "author" : [ "Olivier Catoni" ],
      "venue" : "Ann. Inst. H. Poincar Probab. Statist., 48(4):1148–1185,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "A generalized path integral control approach to reinforcement learning",
      "author" : [ "E. Theodorou", "J. Buchli", "S. Schaal" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Learning neural network policies with guided policy search under unknown dynamics",
      "author" : [ "Sergey Levine", "Pieter Abbeel" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Cross-entropy motion planning",
      "author" : [ "M. Kobilarov" ],
      "venue" : "International Journal of Robotics Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Robust trajectory tracking for a scale model autonomous helicopter",
      "author" : [ "Robert Mahony", "Tarek Hamel" ],
      "venue" : "International Journal of Robust and Nonlinear Control,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2004
    }, {
      "title" : "Trajectory tracking of a class of underactuated systems with external disturbances",
      "author" : [ "Marin Kobilarov" ],
      "venue" : "In American Control Conference,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1, 2] (see also [3] for a comprehensive overview with a focus on robotic applications [4]).",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "[1, 2] (see also [3] for a comprehensive overview with a focus on robotic applications [4]).",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 2,
      "context" : "[1, 2] (see also [3] for a comprehensive overview with a focus on robotic applications [4]).",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "[1, 2] (see also [3] for a comprehensive overview with a focus on robotic applications [4]).",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 4,
      "context" : "When an uncertain system model is available the problem is equivalent to robust model-predictive control (MPC) [5].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "To cope with such general conditions, we study robustness through a statistical learning viewpoint [6, 7, 8] using finite-time sample complexity bounds on performance based on empirical runs.",
      "startOffset" : 99,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "To cope with such general conditions, we study robustness through a statistical learning viewpoint [6, 7, 8] using finite-time sample complexity bounds on performance based on empirical runs.",
      "startOffset" : 99,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "To cope with such general conditions, we study robustness through a statistical learning viewpoint [6, 7, 8] using finite-time sample complexity bounds on performance based on empirical runs.",
      "startOffset" : 99,
      "endOffset" : 108
    }, {
      "referenceID" : 8,
      "context" : "This is accomplished using concentration-of-measure inequalities [9] which provide only probabilistic bounds , i.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "Our approach is also inspired by existing work on randomized algorithms in control theory originally motivated by robust linear control design [10].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "For example, early work focused on probabilistic root-locus design [11] and later applied to constraint satisfaction [12] and general cost functions [13].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "For example, early work focused on probabilistic root-locus design [11] and later applied to constraint satisfaction [12] and general cost functions [13].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 12,
      "context" : "For example, early work focused on probabilistic root-locus design [11] and later applied to constraint satisfaction [12] and general cost functions [13].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "High-confidence bounds for decidability of linear stability were refined in [14].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "These are closely related to the concepts of randomized stability robustness analysis (RSRA) and randomized performance robustness analysis (RPRA) [13].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 14,
      "context" : "Finite-time probabilistic bounds for system identification problems have also been obtained through a statistical learning viewpoint [15].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "Instead of directly searching for the optimal ξ to solve (1) a common strategy in direct policy search and global optimization [16, 17, 18, 19, 20, 21] is to iteratively construct a surrogate stochastic model π(ξ|ν) with hyper-parameters ν ∈ V , such as a Gaussian Mixture Model (GMM), where V is a vector space.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 16,
      "context" : "Instead of directly searching for the optimal ξ to solve (1) a common strategy in direct policy search and global optimization [16, 17, 18, 19, 20, 21] is to iteratively construct a surrogate stochastic model π(ξ|ν) with hyper-parameters ν ∈ V , such as a Gaussian Mixture Model (GMM), where V is a vector space.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 17,
      "context" : "Instead of directly searching for the optimal ξ to solve (1) a common strategy in direct policy search and global optimization [16, 17, 18, 19, 20, 21] is to iteratively construct a surrogate stochastic model π(ξ|ν) with hyper-parameters ν ∈ V , such as a Gaussian Mixture Model (GMM), where V is a vector space.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : "Instead of directly searching for the optimal ξ to solve (1) a common strategy in direct policy search and global optimization [16, 17, 18, 19, 20, 21] is to iteratively construct a surrogate stochastic model π(ξ|ν) with hyper-parameters ν ∈ V , such as a Gaussian Mixture Model (GMM), where V is a vector space.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 19,
      "context" : "Instead of directly searching for the optimal ξ to solve (1) a common strategy in direct policy search and global optimization [16, 17, 18, 19, 20, 21] is to iteratively construct a surrogate stochastic model π(ξ|ν) with hyper-parameters ν ∈ V , such as a Gaussian Mixture Model (GMM), where V is a vector space.",
      "startOffset" : 127,
      "endOffset" : 151
    }, {
      "referenceID" : 20,
      "context" : "defined as the gradient of a potential field or as a gyroscopic “steering” force φ(x,O) = s(x,O)× v that effectively rotates the velocity vector [22] .",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "Obtaining practical bounds for J (ν) becomes challenging since the change-of-measure likelihood ratio π(ξ|ν) π(ξ|ν0) can be unbounded (or have very large values) [23] and a standard bound, e.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 22,
      "context" : "To cope with this we will employ a recently proposed robust estimation [24] technique stipulating that instead of estimating the expectation m = E[X] of a random variable X ∈ [0,∞) using its empirical mean m̂ = 1 M ∑M j=1Xj , a more robust estimate can be obtained by truncating its higher moments, i.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 22,
      "context" : "Here, we adapted the moment-truncation technique proposed by Catoni [24] for general unbounded losses to our policy adaptation setting in order to handle the possibly unbounded likelihood ratio.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "where the relationship between the likelihood ratio variance and the Renyii divergence was established in [23].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "π(ξ|ν) = N (ξ|μ,Σ) which is updated through reward-weighted-regression (RWR) [3], i.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "For instance, related to recent methods in robotics one could use reward-weightedregression (RWR) or policy learning by weighted samples with returns (PoWeR) [3], stochastic optimization methods such as [25, 26], or using the related cross-entropy optimization [16, 27].",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 23,
      "context" : "For instance, related to recent methods in robotics one could use reward-weightedregression (RWR) or policy learning by weighted samples with returns (PoWeR) [3], stochastic optimization methods such as [25, 26], or using the related cross-entropy optimization [16, 27].",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "For instance, related to recent methods in robotics one could use reward-weightedregression (RWR) or policy learning by weighted samples with returns (PoWeR) [3], stochastic optimization methods such as [25, 26], or using the related cross-entropy optimization [16, 27].",
      "startOffset" : 203,
      "endOffset" : 211
    }, {
      "referenceID" : 15,
      "context" : "For instance, related to recent methods in robotics one could use reward-weightedregression (RWR) or policy learning by weighted samples with returns (PoWeR) [3], stochastic optimization methods such as [25, 26], or using the related cross-entropy optimization [16, 27].",
      "startOffset" : 261,
      "endOffset" : 269
    }, {
      "referenceID" : 25,
      "context" : "For instance, related to recent methods in robotics one could use reward-weightedregression (RWR) or policy learning by weighted samples with returns (PoWeR) [3], stochastic optimization methods such as [25, 26], or using the related cross-entropy optimization [16, 27].",
      "startOffset" : 261,
      "endOffset" : 269
    }, {
      "referenceID" : 26,
      "context" : "A standard nonlinear feedback backstepping controller based on a “slow” position control loop and a “fast” attitude control is employed [28, 29] for local control.",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 27,
      "context" : "A standard nonlinear feedback backstepping controller based on a “slow” position control loop and a “fast” attitude control is employed [28, 29] for local control.",
      "startOffset" : 136,
      "endOffset" : 144
    } ],
    "year" : 2015,
    "abstractText" : "This paper is concerned with robustness analysis of decision making under uncertainty. We consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration. In particular, we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs. A novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation. The bound serves as a high-confidence certificate for providing future performance or safety guarantees. The approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented.",
    "creator" : null
  }
}