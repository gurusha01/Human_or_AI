{
  "name" : "1ee3dfcd8a0645a25a35977997223d22.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Spectral Norm Regularization of Orthonormal Representations for Graph Transduction",
    "authors" : [ "Rakesh Shivanna" ],
    "emails" : [ "rakeshshivanna@google.com", "bibaswan.chatterjee@csa.iisc.ernet.in", "ramans@csa.iisc.ernet.in", "chiru@csa.iisc.ernet.in", "francis.bach@ens.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "T ) convergence, and is generally applicable whenever a suit-\nable approximate projection is available. We use IIP to compute SPORE where the approximate projection step is computed by FISTA, an accelerated gradient descent procedure. We show that the method has a convergence rate of O( 1√\nT ).\nThe proposed algorithm easily scales to 1000’s of vertices, while the standard SDP computation does not scale beyond few hundred vertices. Furthermore, the analysis presented here easily extends to the multiple graph setting."
    }, {
      "heading" : "1 Introduction",
      "text" : "Learning problems on graph-structured data have received significant attention in recent years [11, 17, 20]. We study an instance of graph transduction, the problem of learning labels on vertices of simple graphs1. A typical example is webpage classification [20], where a very small part of the entire web is manually classified. Even for simple graphs, predicting binary labels of the unlabeled vertices is NP-complete [6].\nMore formally: let G = (V,E), V = [n] be a simple graph with unknown labels y ∈ {±1}n. Without loss of generality, let the labels of first m ∈ [n] vertices be observable, let u := n − m.\n1A simple graph is an unweighted, undirected graph with no self loops or multiple edges.\nLet yS and yS̄ be the labels of S = [m] and S̄ = V \\S. Given G and yS , the goal is to learn soft predictions ŷ ∈ Rn, such that er`\nS̄ [ŷ] := 1|S̄| ∑ j∈S̄ `(yj , ŷj) is small, where ` is any loss function. The\nfollowing formulation has been extensively used [19, 20]\nmin ŷ∈Rn\ner`S [ŷ] + λŷ >K−1ŷ, (1)\nwhere K is a graph-dependent kernel and λ > 0 is a regularizer constant. Let ŷ∗ be the solution to (1), given G and S ⊆ V, |S| = m. [1] proposed the following generalization bound\nES⊆V [ er`S̄ [ŷ ∗] ] ≤ c1 inf\nŷ∈Rn\n[ er`V [ŷ] + λŷ >K−1ŷ ] + c2 ( trp(K)\nλ|S|\n)p , (2)\nwhere c1, c2 are dependent on ` and trp(K) = ( 1 n ∑ i∈[n] K p ii )1/p , p > 0. [1] argued that trp(K) should be a constant and can be enforced by normalizing the diagonal entries of K to be 1. This is an important advice in graph transduction, however it is to be noted that the set of normalized kernels is quite large and (2) gives little insight in choosing the optimal kernel.\nNormalizing the diagonal entries of K can be viewed geometrically as embedding the graph on a unit sphere. Recently, [16] studied a rich class of unit sphere graph embeddings, called orthonormal representations [13], and find that it is statistically consistent for graph transduction. However, the choice of the optimal orthonormal embedding is not clear. We study orthonormal representations for the following equivalent [19] kernel learning formulation of (1), with C = 1λm ,\nωC(K,yS) = max α∈Rn ∑ i∈S αi− 1 2 ∑ i,j∈S αiαjyiyjKij s.t. 0 ≤ αi ≤ C ∀i ∈ S, αj = 0 ∀j /∈ S, (3)\nfrom a probably approximately correctly (PAC) learning point of view. Note that the final predictions are given by ŷi = ∑ j∈S Kijα ∗ jyj ∀i ∈ [n], where α∗ is the optimal solution to (3).\nContributions. We make the following contributions: – Using (3) we show the class of orthonormal representations are efficiently PAC learnable over a\nlarge class of graph families, including power-law and random graphs. – The above analysis suggests that spectral norm regularization could be beneficial in computing\nthe best embedding. To this end we pose the problem of SPectral norm regularized ORthonormal Embedding (SPORE) for graph Transduction, namely that of minimizing a convex function over an elliptope. One could solve such problems as SDPs which unfortunately do not scale well beyond few hundred vertices.\n– We propose an infeasible inexact proximal (IIP) method, a novel projected subgradient descent algorithm, in which the projection is approximated by an inexact proximal method. We suggest a novel approximation criteria which approximates the proximal operator for the support function of the feasible set within a given precision. One could compute an approximation to the projection from the inexact proximal point which may not be feasible hence the name IIP. We prove that IIP converges to the optimal minimum of a non-smooth convex function with rate O(1/ √ T ) in T iterations.\n– The IIP algorithm is then applied to the case when the set of interest is composed of the intersection of two convex sets. The proximal operator for the support function of the set of interest can be obtained using the FISTA algorithm, once we know the proximal operator for the support functions of the individual sets involved.\n– Our analysis paves the way for learning labels on multiple graphs by using the embedding by adopting an MKL style approach. We present both algorithmic and generalization results.\nNotations. Let ‖ · ‖, ‖ · ‖F denote the Euclidean and Frobenius norm respectively. Let Sn and S+n denote the set of n× n square symmetric and square symmetric positive semi-definite matrices respectively. Let Rn+ be a non-negative orthant. Let Sn−1 = { u ∈ Rn+ ∣∣ ‖u‖1 = 1} denote the n−1 dimensional simplex. Let [n] := {1, . . . , n}. For any M ∈ Sn, let λ1(M) ≥ . . . ≥ λn(M) denote its Eigenvalues. We denote the adjacency matrix of a graph G by A. Let Ḡ denote the complement graph of G, with the adjacency matrix Ā = 11>− I−A; where 1 is a vector of all 1’s, and I is the identity matrix. Let Y = {±1}, Ŷ = R be the label and soft-prediction spaces over V . Given y ∈ Y\nand ŷ ∈ Ŷ , we use `0-1(y, ŷ) = 1[yŷ < 0], `hng(y, ŷ) = (1 − yŷ)+2 to denote 0-1 and hinge loss respectively. The notations O, o, Ω, Θ will denote standard measures in asymptotic analysis [4].\nRelated work. [1]’s analysis was restricted to Laplacian matrices, and does not give insights in choosing the optimal unit sphere embedding. [2] studied graph transduction using PAC model, however for graph orthonormal embeddings, there is no known sample complexity estimate. [16] showed that working with orthonormal embeddings leads to consistency. However, the choice of optimal embedding and an efficient algorithm to compute the same remains an open issue. Furthermore, we show that [16]’s sample complexity estimate is sub-optimal.\nPreliminaries. An orthonormal embedding [13] of a simple graph G = (V,E), V = [n], is defined by a matrix U = [u1, . . . ,un] ∈ Rd×n such that u>i uj = 0 whenever (i, j) /∈ E and ‖ui‖ = 1 ∀i ∈ [n]. Let Lab(G) denote the set of all possible orthonormal embeddings of the graph G, Lab(G) := { U | U is an orthonormal embedding } . Recently, [8] showed an interesting connection to the set of graph kernel matrices\nK(G) := { K ∈ S+n |Kii = 1,∀i ∈ [n];Kij = 0,∀(i, j) /∈ E } .\nNote that K ∈ K(G) is positive semidefinite, and hence there exists U ∈ Rd×n such that K = U>U. Note that Kij = u>i uj where ui is the i-th column of U. Hence by inspection it is clear that U ∈ Lab(G). Using a similar argument, we can show that for any U ∈ Lab(G), the matrix K = U>U ∈ K(G). Thus, the two sets, Lab(G) and K(G) are equivalent. Furthermore, orthonormal embeddings are associated with an interesting quantity, the Lovász ϑ function [13, 7]. However, computing ϑ requires solving an SDP, which is impractical."
    }, {
      "heading" : "2 Generalization Bound for Graph Transduction using Orthonormal Embeddings",
      "text" : "In this section we derive a generalization bound, used in the sequel for PAC analysis. We derive the following error bound, valid for any orthonormal embedding (supplementary material, Section B). Theorem 1 (Generalization bound). Let G = (V,E) be a simple graph with unknown binary labels y ∈ Yn on the vertices V . Let K ∈ K(G). Given G, and labels of a randomly drawn subgraph S, let ŷ ∈ Ŷn be the predictions learnt by ωC(K,yS) in (3). Then, for m ≤ n/2, with probability ≥ 1− δ over the choice of S ⊂ V , such that |S| = m\ner0-1S̄ [ŷ] ≤ 1\nm ∑ i∈S `hng(yi, ŷi) + 2C √ 2λ1(K) +O (√ 1 m log 1 δ ) . (4)\nNote that the above is a high-probability bound, in comparison to the expected analysis in (2). Also, the above result suggests that graph embeddings with low spectral norm and empirical error lead to better generalization. [1]’s analysis in (2) suggests that we should embed a graph on a unit sphere, however, does not help to choose the optimal embedding for graph transduction. Exploiting our analysis from (4), we present a spectral norm regularized algorithm in Section 3.\nWe would also like to study PAC learnability of orthonormal embeddings, where PAC learnability is defined as follows: given G,y; does there exist an m̃ < n, such that w.p. ≥ 1 − δ over S ⊂ V, |S| ≥ m̃; the generalization error er0-1\nS̄ ≤ . The quantity m̃ is termed as labelled sample\ncomplexity [2]. Existing analysis [2] do not apply to orthonormal embeddings as discussed in related work, Section 1. Theorem 1 allows us to derive improved statistical estimates (Section 3)."
    }, {
      "heading" : "3 SPORE Formulation and PAC Analysis",
      "text" : "Theorem 1 suggests that penalizing the spectral norm of K would lead to better generalization. To this end we motivate the following formulation.\nΨC,β(G,yS) = min K∈K(G)\ng ( K )\nwhere g(K) = ωC(K,yS) + βλ1(K). (5)\n2(a)+ = max(a, 0) ∀a ∈ R\n(5) gives an optimal orthonormal embedding, the optimal K, which we will refer to as SPORE. In this section we first study the PAC learnability of SPORE, and derive a labelled sample complexity estimate. Next, we study efficient computation of SPORE. Though SPORE can be posed as an SDP, we show in Section 4 that it is possible to exploit the structure, and solve efficiently.\nGiven G and yS , the function ωC(K,yS) is convex in K as it is the maximum of affine functions of K. The spectral norm of K, λ1(K) is also convex, and hence g(K) is a convex function. Furthermore K(G) is an Elliptope [5], a convex body which can be described by the intersection of a positive semi-definite and affine constraints. It follows that hence (5) is convex. Usually these formulations are posed as SDPs which do not scale beyond few hundred vertices. In Section 4 we derive an efficient first order method which can solve for 1000’s of vertices. Let K∗ be the optimal embedding computed from (5). Note that once the kernel is fixed, the predictions are only dependent on ωC(K∗,yS). Let α∗ be the solution to ωC(K∗,yS) as in (3), then the final predictions of (5) is given by ŷi = ∑ j∈S K ∗ ijα ∗ jyj , ∀i ∈ [n].\nAt this point, we derive an interesting graph-dependent error convergence rate. We gather two important results, the proof of which appears in the supplementary material, Section C. Lemma 2. Given a simple graph G = (V,E), maxK∈K(G) λ1(K) = ϑ(Ḡ). Lemma 3. Given G and y, for any S ⊆ V and C > 0, minK∈K(G) ωC(K,yS) ≤ ϑ(G)/2.\nIn the standard PAC setting, there is a complete disconnection between the data distribution and target hypothesis. However, in the presence of unlabeled nodes, without any assumption on the data, it is impossible to learn labels. Following existing literature [1, 9], we work with similarity graphs – where presence of an edge would mean two nodes are similar; and derive the following (supplementary material, Section C). Theorem 4. Let G = (V,E), V = [n] be a simple graph with unknown binary labels y ∈ Yn on the vertices V . Given G, and labels of a randomly drawn subgraph S ⊂ V , m = |S|; let ŷ be\nthe predictions learnt by SPORE (5), for parameters C = ( ϑ(G)\nm √ ϑ(Ḡ)\n) 1 2\nand β = ϑ(G) ϑ(Ḡ) . Then, for\nm ≤ n/2, with probability ≥ 1− δ over the choice of S ⊂ V , such that |S| = m er0-1S̄ [ŷ] = O ( 1 m (√ nϑ(G) + log 1 δ )) 1 2 . (6)\nProof. (Sketch) Let K∗ be the kernel learnt by SPORE (5). Using Theorem 1 and Lemma 2 for ŷ\ner0-1S̄ [ŷ] ≤ 1\nm ∑ i∈S `hng(yi, ŷi) + 2C √ 2ϑ ( Ḡ ) +O (√ 1 m log 1 δ ) . (7)\nFrom the primal formulation of (3), using Lemma 2 and 3, we get C ∑ i∈S `hng(yi, ŷi) ≤ ωC(K∗,yS) ≤ ΨC,β(G,yS) ≤ ϑ(G) 2 + βϑ ( Ḡ ) .\nPlugging back in (7), choosing β such that βCmϑ ( Ḡ ) = 2C √ 2ϑ ( Ḡ ) and optimizing for C gives\nus the choice of parameters as stated. Finally, using ϑ(G)ϑ(Ḡ) = n [13] proves the result.\nIn the theorem above, Ḡ is the complement graph of G. The optimal orthonormal embedding K∗ tend to embed vertices to nearby regions if they have connecting edges, hence, the notion of similarity is implicitly captured in the embedding. From (6), for a fixed n and m, note that the error converges at a faster rate for a dense graph (ϑ is small), than for a sparse graph (ϑ is large). Such connections relating to graph structural properties were previously unavailable [1]. We also estimate the labelled sample complexity, by bounding (6) by > 0, to obtain m̃ = Ω ( 1 2 ( √ ϑn + log 1δ ) ) . This connection helps to reason the intuition that for a sparse graph one would need a larger number of labelled vertices, than for a dense graph. For constants , δ, we\nobtain a fractional labelled sample complexity estimate of m̃/n = Ω ( ϑ/n ) 1 2 , which is a signif-\nicant improvement over the recently proposed Ω ( ϑ/n ) 1 3 [16]. The use of stronger machinery of\nRademacher averages (supplementary material, Section C), instead of VC-dimension [2], and specializing to SPORE allows us to improve over existing analysis [1, 16]. The proposed sample complexity estimate is interesting for ϑ = o(n), examples of such graphs include: random graphs (ϑ(G(n, p)) = Θ( √ n)) and power-law graphs (ϑ̄ = O( √ n))."
    }, {
      "heading" : "4 Inexact Proximal methods for SPORE",
      "text" : "In this section, we propose an efficient algorithm to solve SPORE (see (5)). The optimization problem SPORE can be posed as an SDP. Generic SDP solvers have a runtime complexity of O(n6) and often does not scale well for large graphs. We study first-order methods, such as projected subgradient procedures, as an alternative to SDPs, for minimizing g(K). The main computational challenge in developing such procedures is that it is difficult to compute the projection on the elliptope. One could potentially use the seminal Dykstra’s algorithm [3] of finding a feasible point in the intersection of two convex sets. The algorithm asymptotically finds a point in the intersection. This asymptotic convergence is a serious disadvantage in the usage of Dykstra’s algorithm as a projection sub-routine. It would be useful to have an algorithm which after a finite number of iterations yield an approximate projection and a subsequent descent algorithm can yield a convergent algorithm.\nMotivated by SPORE, we study the problem of minimizing non-smooth convex functions where the projection onto the feasible set can be computed only approximately. Recently there has been increasing interest in studying Inexact proximal methods [15, 18]. In the sequel we design an inexact proximal method which yields an O(1/ √ T ) algorithm to solve (5). The algorithm is based on approximating the prox function by an iterative procedure which satisfies a suitably designed criterion."
    }, {
      "heading" : "4.1 An Infeasible Inexact Proximal (IIP) algorithm",
      "text" : "Let f be a convex function with properly defined sub-differential ∂f(x) at every x ∈ X . Consider the following optimization problem.\nmin x∈X⊂Rd f(x). (8)\nA subgradient projection iteration of the form\nxk+1 = PX (xk − αkhk), hk ∈ ∂f(xk) (9) is often used to arrive at an accurate solution by running the iterations O( 1 2 ) number of times, where PX (v) is the projection of v ∈ Rd on X ⊂ Rd if PX (v) = argminx∈X 12‖v − x‖ 2 F . In many situations, such asX = K(G), it is not possible to accurately compute the projection in finite amount of time and one may obtain only an approximate projection. Using the Moreau decomposition PX (v) + ProxσX (v) = v [14], one can compute the projection if one could compute proxσX , where σA(a) = maxa∈X x\n>a is the support function of X , and proxσX refers to the proximal operator for the function g′ at v as defined below3.\nproxg′(v) = argmin z∈Dom(g′)\npg′(z; v) ( = 1\n2 ‖v − z‖2 + g′(z)\n) . (10)\nWe assume that one could compute z X (v), not necessarily in X , such that pσX (z X (v); v) ≤ min\nz∈Rn pσX (z; v) + , and P X (v) = v − z X . (11)\nSee that z X is an inexact prox and the resultant estimate of the projection P X can be infeasible but hopefully not too far away. Note that = 0 recovers the exact case. The next theorem confirms that it is possible to converge to the true optimum for a non-zero (supplementary material, Section D.5). Theorem 5. Consider the optimization problem (8). Starting from any ‖x0− x∗‖ ≤ R, where x∗ is a solution of (8), for every k let us assume that we could obtain P X (yk) such that zk = yk−P X (yk) satisfy (11), where yk = xk − αkhk, αk = s‖hk‖ , ‖hk‖ ≤ L, ‖xk − x ∗‖ ≤ R, s = √ R2\nT + . Then the iterates\nxk+1 = P X (xk − αkhk), hk ∈ ∂f(xk) (12)\n3A more general definition of the proximal operator is – proxτg′(v) = argminz∈Dom(g′) 1 2τ ‖v−z‖2+g′(z)\nyield f∗T − f∗ ≤ L √ R2\nT + . (13)\nRelated Work on Inexact Proximal methods: There has been recent interest in deriving inexact proximal methods such as projected gradient descent, see [15, 18] for a comprehensive list of references. To the best of our knowledge, composite functions have been analyzed but no one has explored the case that f is non-smooth. The results presented here are thus complementary to [15, 18]. Note the subtlety in using the proper approximation criteria. Using a distance criterion between the true projection and the approximate projection, or an approximate optimality criteria on the optimal distance would lead to a worse bound; using a dual approximate optimality criterion (here through the proximal operator for the support function) is key (as noted in [15, 18] and references therein).\nAs an immediate consequence of Theorem 5, note that suppose we have an algorithm to compute proxσX which guarantees after S iterations that\npσX (zS ; v)− min z∈Rd\npσX (z; v) ≤ R̂2\nS2 , (14)\nfor a constant R̂ particular to the set over which pσX is defined. We can initialize = R̂2\nS2 in (13), that may suggest that one could use S = √ T iterations to yield\nf∗T − f∗ ≤ LR̄√ T\nwhere R̄ = √ R2 + R̂2. (15)\nRemarks: Computational efficiency dictates that the number of projection steps should be kept at a minimum. To this end we see that number of projection steps need to be at least S = √ T with the current choice of stepsizes. Let cp be the cost of one iteration of FISTA step and c0 be the cost of one outer iteration. The total computation cost can be then estimated as T 3/2 · cp + T · c0."
    }, {
      "heading" : "4.2 Applying IIP to compute SPORE",
      "text" : "The problem of computing SPORE can be posed as minimizing a non-smooth convex function over an intersection of two sets: K(G) = S+n ∩ P (G), intersection of positive semi-definite cone S+n and a polytope of equality constraints P (G) := {M ∈ Sn|Mii = 1,Mij = 0 ∀(i, j) /∈ E}. The algorithm described in Theorem 5 readily applies to the new setting if the projection can be computed efficiently. The proximal operator for σX can be derived as 4\nProxσX (v) = argmin a,b∈Rd pσX (a, b; v)\n( = 1\n2 ‖(a+ b)− v‖2 + σA(a) + σB(b)\n) . (16)\nThis means that even if we do not have an efficient procedure for computing ProxσX (v) directly, we can devise an algorithm to guarantee the approximation (11) if we can compute ProxσA(v) and ProxσB (v) efficiently. This can be done through the application of the popular FISTA algorithm for (16), which also guarantees (14). Algorithm 1 (detailed in the supplementary, named IIP FISTA), computes the following simple steps followed by the usual FISTA variable updates at each iteration t : (a) gradient descent step on a and b with respect to the smooth term 12‖(a + b) − v‖\n2 and (b) proximal step with respect to σA and σB using the expressions (14), (21) (supplementary material).\nUsing the tools discussed above, we design Algorithm 1 to solve the SPORE formulation (5) using IIP. The proposed algorithm readily applies to general convex sets. However, we confine ourselves to specific sets of interest in our problem. The following theorem states the convergence rate of the proposed procedure. Theorem 6. Consider the optimization problem (8) with X = A ⋂ B, where A and B are S+n and P (G) respectively. Starting from any K0 ∈ A the iterates Kt in Algorithm (1) satisfy\nmin t=0,...,T f(Kt)− f(K∗) ≤ L√ T\n√ R2 + R̂2.\nProof. Is an immediate extension of Theorem 5 – supplementary material, Section D.6. 4The derivation is presented in supplementary material, Claim 6.\nAlgorithm 1 IIP for SPORE 1: function APPROX-PROJ-SUBG(K0, L,R, R̂, T ) 2: s = L√ T · (√ R2 + R̂2 )\n. compute stepsize 3: Initialize t0 = 1. 4: for t = 1, . . . , T do 5: compute ht−1 . subgradient of f(K) at Kt−1 see equation (5) 6: vt = Kt−1 − s‖ht−1‖ht−1 7: K̃t = IIP FISTA(vt, √ T ) . FISTA for √ T steps. Use Algorithm 1 (supp.) 8: Kt = ProjA(K̃t) = Kt − proxσA(Kt) 9: .Kt needs to be psd for the next SVM call. Use (14) (supp.)\n10: end for 11: end function\nEquating the problem (8) with the SPORE problem (5), we have f(K) = ωC(K,yS) + βλ1(K). The set of subgradients of f at the iteration t is given by ∂f(Kt) = { − 12Yαtα > t Y + βvtv > t |αt is returned by SVM, and vt is the eigen vector corresponding to λ1(Kt) } 5, where Y be a diagonal matrix such that Yii = yi, for i ∈ S, and 0 otherwise. The step size is calculated using estimates of L,R and R̂, which can be derived as L = nC2, R = n, R̂ = n2.5 for the SPORE problem. Check the supplementary material for the derivations."
    }, {
      "heading" : "5 Multiple Graph Transduction",
      "text" : "Multiple graph transduction is of recent interest in a multi-view setting, where individual views are expressed by a graph. This includes many practical problems in bioinformatics [17], spam detection [21], etc. We propose an MKL style extension of SPORE, with improved PAC bounds.\nFormally, the problem of multiple graph transduction is stated as – let G = {G(1), . . . , G(M)} be a set of simple graphs G(k) = (V,E(k)), defined on a common vertex set V = [n]. Given G and yS as before, the goal is to accurately predict yS̄ . Following the standard technique of taking convex combination of graph kernels [16], we propose the following MKL-SPORE formulation\nΦC,β(G,yS) = min K(k)∈K(G(k))\n( min η∈SM−1 ωC ( ∑ k∈[M ] ηkK (k),yS ) + β max k∈[M ] λ1(K (k)) ) . (17)\nSimilar to Theorem 4, we can show the following (supplementary material, Theorem 8)\ner0-1S̄ [ŷ] = O ( 1 m (√ nϑ(G) + log 1 δ )) 1 2 where ϑ(G) ≤ min k∈[M ] ϑ(G(k)). (18)\nIt immediately follows that combining multiple graphs improves the error convergence rate (see (6)), and hence the labelled sample complexity. Also, the bound suggests that the presence of at least one “good” graph is sufficient for MKL-SPORE to learn accurate predictions. This motivates us to use the proposed formulation in the presence of noisy graphs (Section 6). We can also apply the IIP algorithm described in Section 4 to solve for (17) (supplementary material, Section F)."
    }, {
      "heading" : "6 Experiments",
      "text" : "We conducted experiments on both real world and synthetic graphs, to illustrate our theoretical observations. All experiments were run on CPU with 2 Xeon Quad-Core processors (2.66GHz, 12MB L2 Cache) and 16GB memory running CentOS 5.3.\n5αt = argmaxα∈Rn+, ‖α‖∞≤C αj=0 ∀j /∈S α>1− 1 2 α>YKtYα and vt = argmaxv∈Rn,‖v‖=1 v >Ktv\nTable 1: SPORE comparison.\nDataset Un-Lap N-Lap KS SPORE breast-cancer 88.22 93.33 92.77 96.67 diabetes 68.89 69.33 69.44 73.33 fourclass 70.00 70.00 70.44 78.00 heart 71.97 75.56 76.42 81.97 ionosphere 67.77 68.00 68.11 76.11 sonar 58.81 58.97 59.29 63.92 mnist-1vs2 75.55 80.55 79.66 85.77 mnist-3v8 76.88 81.88 83.33 86.11 mnist-4v9 68.44 72.00 72.22 74.88\nTable 2: Large Scale – 2000 Nodes.\nDataset Un-Lap N-Lap KS SPORE mnist-1vs2 83.80 96.23 94.95 96.72 mnist-3vs8 55.15 87.35 87.35 91.35 mnist-5vs6 96.30 94.90 92.05 97.35 mnist-1vs7 90.65 96.80 96.55 97.25 mnist-4vs9 65.55 65.05 61.30 87.40\nGraph Transduction (SPORE): We use two datasets UCI [12] and MNIST [10]. For the UCI datasets, we use the RBF kernel6 and threshold with the mean, and for the MNIST datasets we construct a similarity matrix using cosine distance for a random sample of 500 nodes, and threshold with 0.4 to obtain unweighted graphs. With 10% labelled nodes, we compare SPORE with formulation (3) using graph kernels – Unnormalized Laplacian (c1I + L)−1, Normalized Laplacian( c2I + D − 12LD− 1 2 )−1 and K-Scaling [1], where L = D − A, D being a diagonal matrix of degrees. We choose parameters c1, c2, C and β by cross validation. Table 1 summarizes the results, averaged over 5 different labelled samples, with each entry being accuracy in % w.r.t. 0-1 loss function. As expected from Section 3, SPORE significantly outperforms existing methods. We also tackle large scale graph transduction problems, Table 2 shows superior performance of Algorithm 1 for a random sample of 2000 nodes, with only 5 outer iterations and 20 inner projections.\nMultiple Graph Transduction (MKL-SPORE): We illustrate the effectiveness of combining multiple graphs, using mixture of random graphs – G(p, q), p, q ∈ [0, 1] where we fix |V | = n = 100 and the labels y ∈ Yn such that yi = 1 if i ≤ n/2; −1 otherwise. An edge (i, j) is present with probability p if yi = yj ; otherwise present with probability q. We generate three datasets to simulate homogenous, heterogenous and noisy case, shown in Table 3.\nTable 3: Synthetic multiple graphs dataset.\nGraph Homo. Heter. Noisy G(1) G(0.7, 0.3) G(0.7, 0.5) G(0.7, 0.3) G(2) G(0.7, 0.3) G(0.6, 0.4) G(0.6, 0.4) G(3) G(0.7, 0.3) G(0.5, 0.3) G(0.5, 0.5)\nTable 4: Superior performance of MKL-SPORE.\nGraph Homo. Heter. Noisy G(1) 84.4 69.2 84.4 G(2) 84.8 68.6 68.2 G(3) 86.4 72.0 54.4 Union 85.5 69.3 69.3 Intersection 83.8 67.5 69.0 Majority 93.7 76.9 76.6 Multiple Graphs 95.6 80.6 81.9\nMKL-SPORE was compared with individual graphs; and with the union, intersection and majority graphs7. We use SPORE to solve for single graph transduction, and the results were averaged over 10 random samples of 5% labelled nodes. For the comparison metric as before, Table 4 shows that combining multiple graphs improves classification accuracy. Furthermore, the noisy case illustrates the robustness of the proposed formulation, a key observation from (18)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We show that the class of orthonormal graph embeddings are efficiently PAC learnable. Our analysis motivates a Spectral Norm regularized formulation – SPORE for graph transduction. Using inexact proximal method, we design an efficient first order method to solve for the proposed formulation. The algorithm and analysis presented readily generalize to the multiple graphs setting."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We acknowledge support from a grant from Indo-French Center for Applied Mathematics (IFCAM). 6The (i, j)th entry of an RBF kernel is given by exp ( ‖xi−xj‖2\n2σ2\n) , where σ is set as the mean distance.\n7Majority graph is a graph where an edge (i, j) is present, if a majority of the graphs have the edge (i, j)."
    } ],
    "references" : [ {
      "title" : "Learning on graph with Laplacian regularization",
      "author" : [ "R.K. Ando", "T. Zhang" ],
      "venue" : "NIPS",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "An augmented PAC model for semi-supervised learning",
      "author" : [ "N. Balcan", "A. Blum" ],
      "venue" : "O. Chapelle, B. Schölkopf, and A. Zien, editors, Semi-supervised learning. MIT press Cambridge",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A Method for Finding Projections onto the Intersection of Convex Sets in Hilbert Spaces",
      "author" : [ "J.P. Boyle", "R.L. Dykstra" ],
      "venue" : "Advances in Order Restricted Statistical Inference, volume 37 of Lecture Notes in Statistics, pages 28–47. Springer New York",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Introduction to algorithms",
      "author" : [ "T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein" ],
      "venue" : "volume 2. MIT press Cambridge",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Forbidden minor characterizations for low-rank optimal solutions to semidefinite programs over the elliptope",
      "author" : [ "M. Eisenberg-Nagy", "M. Laurent", "A. Varvitsiotis" ],
      "venue" : "J. Comb. Theory, Ser. B, 108:40–80",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Graph transduction as a Non-Cooperative Game",
      "author" : [ "A. Erdem", "M. Pelillo" ],
      "venue" : "Neural Computation, 24(3):700–723",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Semidefinite programming in combinatorial optimization",
      "author" : [ "M.X. Goemans" ],
      "venue" : "Mathematical Programming, 79(1-3):143–161",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "The Lovász θ function",
      "author" : [ "V. Jethava", "A. Martinsson", "C. Bhattacharyya", "D.P. Dubhashi" ],
      "venue" : "SVMs and finding large dense subgraphs. In NIPS, pages 1169–1177",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "JMLR, 8(7):1489–1517",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Semi-supervised learning and optimization for hypergraph matching",
      "author" : [ "M. Leordeanu", "A. Zanfir", "C. Sminchisescu" ],
      "venue" : "ICCV, pages 2274–2281. IEEE",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "On the shannon capacity of a graph",
      "author" : [ "L. Lovász" ],
      "venue" : "Information Theory, IEEE Transactions on, 25(1):1–7",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Proximal algorithms",
      "author" : [ "N. Parikh", "S. Boyd" ],
      "venue" : "Foundations and Trends in optimization, 1(3):123–231",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Convergence rates of inexact proximal-gradient methods for convex optimization",
      "author" : [ "M. Schmidt", "N.L. Roux", "F.R. Bach" ],
      "venue" : "NIPS, pages 1458–1466",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning on graphs using Orthonormal Representation is Statistically Consistent",
      "author" : [ "R. Shivanna", "C. Bhattacharyya" ],
      "venue" : "NIPS, pages 3635–3643",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Application of three graph Laplacian based semi-supervised learning methods to protein function prediction problem",
      "author" : [ "L. Tran" ],
      "venue" : "IJBB",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Accelerated and Inexact Forward-Backward Algorithms",
      "author" : [ "S. Villa", "S. Salzo", "L. Baldassarre", "A. Verri" ],
      "venue" : "SIAM Journal on Optimization, 23(3):1607–1633",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Analysis of spectral kernel design based semi-supervised learning",
      "author" : [ "T. Zhang", "R.K. Ando" ],
      "venue" : "NIPS, 18:1601",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Schölkopf" ],
      "venue" : "NIPS, 16(16):321–328",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Spectral clustering and transductive learning with multiple views",
      "author" : [ "D. Zhou", "C.J.C. Burges" ],
      "venue" : "ICML, pages 1159–1166. ACM",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Recent literature [1] suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 9,
      "context" : "Learning problems on graph-structured data have received significant attention in recent years [11, 17, 20].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "Learning problems on graph-structured data have received significant attention in recent years [11, 17, 20].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "Learning problems on graph-structured data have received significant attention in recent years [11, 17, 20].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "A typical example is webpage classification [20], where a very small part of the entire web is manually classified.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "Even for simple graphs, predicting binary labels of the unlabeled vertices is NP-complete [6].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 16,
      "context" : "following formulation has been extensively used [19, 20]",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "following formulation has been extensively used [19, 20]",
      "startOffset" : 48,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "[1] proposed the following generalization bound",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1] argued that trp(K) should be a constant and can be enforced by normalizing the diagonal entries of K to be 1.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "Recently, [16] studied a rich class of unit sphere graph embeddings, called orthonormal representations [13], and find that it is statistically consistent for graph transduction.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "Recently, [16] studied a rich class of unit sphere graph embeddings, called orthonormal representations [13], and find that it is statistically consistent for graph transduction.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "We study orthonormal representations for the following equivalent [19] kernel learning formulation of (1), with C = 1 λm ,",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "The notations O, o, Ω, Θ will denote standard measures in asymptotic analysis [4].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "[1]’s analysis was restricted to Laplacian matrices, and does not give insights in choosing the optimal unit sphere embedding.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] studied graph transduction using PAC model, however for graph orthonormal embeddings, there is no known sample complexity estimate.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "[16] showed that working with orthonormal embeddings leads to consistency.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, we show that [16]’s sample complexity estimate is sub-optimal.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "An orthonormal embedding [13] of a simple graph G = (V,E), V = [n], is defined by a matrix U = [u1, .",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "Recently, [8] showed an interesting connection to the set of graph kernel matrices",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, orthonormal embeddings are associated with an interesting quantity, the Lovász θ function [13, 7].",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "Furthermore, orthonormal embeddings are associated with an interesting quantity, the Lovász θ function [13, 7].",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "[1]’s analysis in (2) suggests that we should embed a graph on a unit sphere, however, does not help to choose the optimal embedding for graph transduction.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "The quantity m̃ is termed as labelled sample complexity [2].",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "Existing analysis [2] do not apply to orthonormal embeddings as discussed in related work, Section 1.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "Furthermore K(G) is an Elliptope [5], a convex body which can be described by the intersection of a positive semi-definite and affine constraints.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Following existing literature [1, 9], we work with similarity graphs – where presence of an edge would mean two nodes are similar; and derive the following (supplementary material, Section C).",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : "Following existing literature [1, 9], we work with similarity graphs – where presence of an edge would mean two nodes are similar; and derive the following (supplementary material, Section C).",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 10,
      "context" : "Finally, using θ(G)θ(Ḡ) = n [13] proves the result.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Such connections relating to graph structural properties were previously unavailable [1].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 13,
      "context" : "icant improvement over the recently proposed Ω ( θ/n ) 1 3 [16].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Rademacher averages (supplementary material, Section C), instead of VC-dimension [2], and specializing to SPORE allows us to improve over existing analysis [1, 16].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Rademacher averages (supplementary material, Section C), instead of VC-dimension [2], and specializing to SPORE allows us to improve over existing analysis [1, 16].",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "Rademacher averages (supplementary material, Section C), instead of VC-dimension [2], and specializing to SPORE allows us to improve over existing analysis [1, 16].",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 2,
      "context" : "One could potentially use the seminal Dykstra’s algorithm [3] of finding a feasible point in the intersection of two convex sets.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Recently there has been increasing interest in studying Inexact proximal methods [15, 18].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 15,
      "context" : "Recently there has been increasing interest in studying Inexact proximal methods [15, 18].",
      "startOffset" : 81,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "Using the Moreau decomposition PX (v) + ProxσX (v) = v [14], one can compute the projection if one could compute proxσX , where σA(a) = maxa∈X x >a is the support function of X , and proxσX refers to the proximal operator for the function g′ at v as defined below3.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "Related Work on Inexact Proximal methods: There has been recent interest in deriving inexact proximal methods such as projected gradient descent, see [15, 18] for a comprehensive list of references.",
      "startOffset" : 150,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "Related Work on Inexact Proximal methods: There has been recent interest in deriving inexact proximal methods such as projected gradient descent, see [15, 18] for a comprehensive list of references.",
      "startOffset" : 150,
      "endOffset" : 158
    }, {
      "referenceID" : 12,
      "context" : "The results presented here are thus complementary to [15, 18].",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 15,
      "context" : "The results presented here are thus complementary to [15, 18].",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "Using a distance criterion between the true projection and the approximate projection, or an approximate optimality criteria on the optimal distance would lead to a worse bound; using a dual approximate optimality criterion (here through the proximal operator for the support function) is key (as noted in [15, 18] and references therein).",
      "startOffset" : 306,
      "endOffset" : 314
    }, {
      "referenceID" : 15,
      "context" : "Using a distance criterion between the true projection and the approximate projection, or an approximate optimality criteria on the optimal distance would lead to a worse bound; using a dual approximate optimality criterion (here through the proximal operator for the support function) is key (as noted in [15, 18] and references therein).",
      "startOffset" : 306,
      "endOffset" : 314
    }, {
      "referenceID" : 14,
      "context" : "This includes many practical problems in bioinformatics [17], spam detection [21], etc.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 18,
      "context" : "This includes many practical problems in bioinformatics [17], spam detection [21], etc.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "Following the standard technique of taking convex combination of graph kernels [16], we propose the following MKL-SPORE formulation",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "With 10% labelled nodes, we compare SPORE with formulation (3) using graph kernels – Unnormalized Laplacian (c1I + L)−1, Normalized Laplacian ( c2I + D − 1 2LD− 1 2 )−1 and K-Scaling [1], where L = D − A, D being a diagonal matrix of degrees.",
      "startOffset" : 183,
      "endOffset" : 186
    } ],
    "year" : 2015,
    "abstractText" : "Recent literature [1] suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction. However, the choice of optimal embedding and an efficient algorithm to compute the same remains open. In this paper, we show that orthonormal representations, a class of unit-sphere graph embeddings are PAC learnable. Existing PAC-based analysis do not apply as the VC dimension of the function class is infinite. We propose an alternative PAC-based bound, which do not depend on the VC dimension of the underlying function class, but is related to the famous Lovász θ function. The main contribution of the paper is SPORE, a SPectral regularized ORthonormal Embedding for graph transduction, derived from the PAC bound. SPORE is posed as a non-smooth convex function over an elliptope. These problems are usually solved as semi-definite programs (SDPs) with time complexity O(n). We present, Infeasible Inexact proximal (IIP): an Inexact proximal method which performs subgradient procedure on an approximate projection, not necessarily feasible. IIP is more scalable than SDP, has an O( 1 √ T ) convergence, and is generally applicable whenever a suitable approximate projection is available. We use IIP to compute SPORE where the approximate projection step is computed by FISTA, an accelerated gradient descent procedure. We show that the method has a convergence rate of O( 1 √ T ). The proposed algorithm easily scales to 1000’s of vertices, while the standard SDP computation does not scale beyond few hundred vertices. Furthermore, the analysis presented here easily extends to the multiple graph setting.",
    "creator" : null
  }
}