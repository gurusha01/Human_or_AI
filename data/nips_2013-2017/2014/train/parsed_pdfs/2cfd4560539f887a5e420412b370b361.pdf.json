{
  "name" : "2cfd4560539f887a5e420412b370b361.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deep Recursive Neural Networks for Compositionality in Language",
    "authors" : [ "Ozan İrsoy", "Claire Cardie" ],
    "emails" : [ "oirsoy@cs.cornell.edu", "cardie@cs.cornell.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep connectionist architectures involve many layers of nonlinear information processing [1]. This allows them to incorporate meaning representations such that each succeeding layer potentially has a more abstract meaning. Recent advancements in efficiently training deep neural networks enabled their application to many problems, including those in natural language processing (NLP). A key advance for application to NLP tasks was the invention of word embeddings that represent a single word as a dense, low-dimensional vector in a meaning space [2], and from which numerous problems have benefited [3, 4].\nRecursive neural networks, comprise a class of architecture that operates on structured inputs, and in particular, on directed acyclic graphs. A recursive neural network can be seen as a generalization of the recurrent neural network [5], which has a specific type of skewed tree structure (see Figure 1). They have been applied to parsing [6], sentence-level sentiment analysis [7, 8], and paraphrase detection [9]. Given the structural representation of a sentence, e.g. a parse tree, they recursively generate parent representations in a bottom-up fashion, by combining tokens to produce representations for phrases, eventually producing the whole sentence. The sentence-level representation (or, alternatively, its phrases) can then be used to make a final classification for a given input sentence — e.g. whether it conveys a positive or a negative sentiment.\nSimilar to how recurrent neural networks are deep in time, recursive neural networks are deep in structure, because of the repeated application of recursive connections. Recently, the notions of depth in time — the result of recurrent connections, and depth in space — the result of stacking\nmultiple layers on top of one another, are distinguished for recurrent neural networks. In order to combine these concepts, deep recurrent networks were proposed [10, 11, 12]. They are constructed by stacking multiple recurrent layers on top of each other, which allows this extra notion of depth to be incorporated into temporal processing. Empirical investigations showed that this results in a natural hierarchy for how the information is processed [12]. Inspired by these recent developments, we make a similar distinction between depth in structure and depth in space, and to combine these concepts, propose the deep recursive neural network, which is constructed by stacking multiple recursive layers.\nThe architecture we study in this work is essentially a deep feedforward neural network with an additional structural processing within each layer (see Figure 2). During forward propagation, information travels through the structure within each layer (because of the recursive nature of the network, weights regarding structural processing are shared). In addition, every node in the structure (i.e. in the parse tree) feeds its own hidden state to its counterpart in the next layer. This can be seen as a combination of feedforward and recursive nets. In a shallow recursive neural network, a single layer is responsible for learning a representation of composition that is both useful and sufficient for the final decision. In a deep recursive neural network, a layer can learn some parts of the composition to apply, and pass this intermediate representation to the next layer for further processing for the remaining parts of the overall composition.\nTo evaluate the performance of the architecture and make exploratory analyses, we apply deep recursive neural networks to the task of fine-grained sentiment detection on the recently published Stanford Sentiment Treebank (SST) [8]. SST includes a supervised sentiment label for every node in the binary parse tree, not just at the root (sentence) level. This is especially important for deep learning, since it allows a richer supervised error signal to be backpropagated across the network, potentially alleviating vanishing gradients associated with deep neural networks [13].\nWe show that our deep recursive neural networks outperform shallow recursive nets of the same size in the fine-grained sentiment prediction task on the Stanford Sentiment Treebank. Furthermore, our models outperform multiplicative recursive neural network variants, achieving new state-of-the-art performance on the task. We conduct qualitative experiments that suggest that each layer handles a different aspect of compositionality, and representations at each layer capture different notions of similarity."
    }, {
      "heading" : "2 Methodology",
      "text" : ""
    }, {
      "heading" : "2.1 Recursive Neural Networks",
      "text" : "Recursive neural networks (e.g. [6]) (RNNs) comprise an architecture in which the same set of weights is recursively applied within a structural setting: given a positional directed acyclic graph, it visits the nodes in topological order, and recursively applies transformations to generate further representations from previously computed representations of children. In fact, a recurrent neural network is simply a recursive neural network with a particular structure (see Figure 1c). Even though\nRNNs can be applied to any positional directed acyclic graph, we limit our attention to RNNs over positional binary trees, as in [6].\nGiven a binary tree structure with leaves having the initial representations, e.g. a parse tree with word vector representations at the leaves, a recursive neural network computes the representations at each internal node η as follows (see also Figure 1a):\nxη = f(WLxl(η) +WRxr(η) + b) (1)\nwhere l(η) and r(η) are the left and right children of η, WL and WR are the weight matrices that connect the left and right children to the parent, and b is a bias vector. Given that WL and WR are square matrices, and not distinguishing whether l(η) and r(η) are leaf or internal nodes, this definition has an interesting interpretation: initial representations at the leaves and intermediate representations at the nonterminals lie in the same space. In the parse tree example, a recursive neural network combines the representations of two subphrases to generate a representation for the larger phrase, in the same meaning space [6]. We then have a task-specific output layer above the representation layer:\nyη = g(Uxη + c) (2)\nwhere U is the output weight matrix and c is the bias vector to the output layer. In a supervised task, yη is simply the prediction (class label or response value) for the node η, and supervision occurs at this layer. As an example, for the task of sentiment classification, yη is the predicted sentiment label of the phrase given by the subtree rooted at η. Thus, during supervised learning, initial external errors are incurred on y, and backpropagated from the root, toward leaves [14]."
    }, {
      "heading" : "2.2 Untying Leaves and Internals",
      "text" : "Even though the aforementioned definition, which treats the leaf nodes and internal nodes the same, has some attractive properties (such as mapping individual words and larger phrases into the same meaning space), in this work we use an untied variant that distinguishes between a leaf and an internal node. We do this by a simple parametrization of the weights W with respect to whether the incoming edge emanates from a leaf or an internal node (see Figure 1b in contrast to 1a, color of the edges emanating from leaves and internal nodes are different):\nhη = f(W l(η) L hl(η) +W r(η) R hr(η) + b) (3)\nwhere hη = xη ∈ X if η is a leaf and hη ∈ H otherwise, and W η· = W xh· if η is a leaf and W η· =W hh · otherwise. X andH are vector spaces of words and phrases, respectively. The weights W xh· act as a transformation from word space to phrase space, and W hh as a transformation from phrase space to itself.\nWith this untying, a recursive network becomes a generalization of the Elman type recurrent neural network with h being analogous to the hidden layer of the recurrent network (memory) and x being analogous to the input layer (see Figure 1c). Benefits of this untying are twofold: (1) Now the weight matrices W xh· , and W hh · are of size |h| × |x| and |h| × |h| which means that we can use large pretrained word vectors and a small number of hidden units without a quadratic dependence on the word vector dimensionality |x|. Therefore, small but powerful models can be trained by using pretrained word vectors with a large dimensionality. (2) Since words and phrases are represented in different spaces, we can use rectifier activation units for f , which have previously been shown to yield good results when training deep neural networks [15]. Word vectors are dense and generally have positive and negative entries whereas rectifier activation causes the resulting intermediate vectors to be sparse and nonnegative. Thus, when leaves and internals are represented in the same space, a discrepancy arises, and the same weight matrix is applied to both leaves and internal nodes and is expected to handle both sparse and dense cases, which might be difficult. Therefore separating leaves and internal nodes allows the use of rectifiers in a more natural manner."
    }, {
      "heading" : "2.3 Deep Recursive Neural Networks",
      "text" : "Recursive neural networks are deep in structure: with the recursive application of the nonlinear information processing they become as deep as the depth of the tree (or in general, DAG). However, this notion of depth is unlikely to involve a hierarchical interpretation of the data. By applying\nthe same computation recursively to compute the contribution of children to their parents, and the same computation to produce an output response, we are, in fact, representing every internal node (phrase) in the same space [6, 8]. However, in the more conventional stacked deep learners (e.g. deep feedforward nets), an important benefit of depth is the hierarchy among hidden representations: every hidden layer conceptually lies in a different representation space and potentially is a more abstract representation of the input than the previous layer [1].\nTo address these observations, we propose the deep recursive neural network, which is constructed by stacking multiple layers of individual recursive nets:\nh(i)η = f(W (i) L h (i) l(η) +W (i) R h (i) r(η) + V (i)h(i−1)η + b (i)) (4)\nwhere i indexes the multiple stacked layers, W (i)L , W (i) R , and b (i) are defined as before within each layer i, and V (i) is the weight matrix that connects the (i− 1)th hidden layer to the ith hidden layer. Note that the untying that we described in Section 2.2 is only necessary for the first layer, since we can map both x ∈ X and h(1) ∈ H(1) in the first layer to h(2) ∈ H(2) in the second layer using separate V (2) for leaves and internals (V xh(2) and V hh(2)). Therefore every node is represented in the same space at layers above the first, regardless of their “leafness”. Figure 2 provides a visualization of weights that are untied or shared.\nFor prediction, we connect the output layer to only the final hidden layer:\nyη = g(Uh (`) η + c) (5)\nwhere ` is the total number of layers. Intuitively, connecting the output layer to only the last hidden layer forces the network to represent enough high level information at the final layer to support the supervised decision. Connecting the output layer to all hidden layers is another option; however, in that case multiple hidden layers can have synergistic effects on the output and make it more difficult to qualitatively analyze each layer.\nLearning a deep RNN can be conceptualized as interleaved applications of the conventional backpropagation across multiple layers, and backpropagation through structure within a single layer. During backpropagation a node η receives error terms from both its parent (through structure), and from its counterpart in the higher layer (through space). Then it further backpropagates that error signal to both of its children, as well as to its counterpart in the lower layer."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Setting",
      "text" : "Data. For experimental evaluation of our models, we use the recently published Stanford Sentiment Treebank (SST) [8], which includes labels for 215,154 phrases in the parse trees of 11,855 sentences, with an average sentence length of 19.1 tokens. Real-valued sentiment labels are converted to an integer ordinal label in {0, . . . , 4} by simple thresholding. Therefore the supervised task is posed as a 5-class classification problem. We use the single training-validation-test set partitioning provided by the authors.\nBaselines. In addition to experimenting among deep RNNs of varying width and depth, we compare our models to previous work on the same data. We use baselines from [8]: a naive bayes classifier that operates on bigram counts (BINB), shallow RNN (RNN) [6, 7] that learns the word vectors from the supervised data and uses tanh units, in contrast to our shallow RNNs, a matrix-vector RNN in which every word is assigned a matrix-vector pair instead of a vector, and composition is defined with matrix-vector multiplications (MV-RNN) [16], and the multiplicative recursive net (or the recursive neural tensor network) in which the composition is defined as a bilinear tensor product (RNTN) [8]. Additionally, we use a method that is capable of generating representations for larger pieces of text (PARAGRAPH VECTORS) [17], and the dynamic convolutional neural network (DCNN) [18]. We use the previously published results for comparison using the same trainingdevelopment-test partitioning of the data.\nActivation Units. For the output layer, we employ the standard softmax activation: g(x) = exi/ ∑ j e xj . For the hidden layers we use the rectifier linear activation: f(x) = max{0, x}. Experimentally, rectifier activation gives better performance, faster convergence, and sparse representations. Previous work with rectifier units reported good results when training deep neural networks, with no pre-training step [15].\nWord Vectors. In all of our experiments, we keep the word vectors fixed and do not finetune for simplicity of our models. We use the publicly available 300 dimensional word vectors by [19], trained on part of the Google News dataset (∼100B words).\nRegularizer. For regularization of the networks, we use the recently proposed dropout technique, in which we randomly set entries of hidden representations to 0, with a probability called the dropout rate [20]. Dropout rate is tuned over the development set out of {0, 0.1, 0.3, 0.5}. Dropout prevents learned features from co-adapting, and it has been reported to yield good results when training deep neural networks [21, 22]. Note that dropped units are shared: for a single sentence and a layer, we drop the same units of the hidden layer at each node.\nSince we are using a non-saturating activation function, intermediate representations are not bounded from above, hence, they can explode even with a strong regularization over the connections, which is confirmed by preliminary experiments. Therefore, for stability reasons, we use a small fixed additional L2 penalty (10−5) over both the connection weights and the unit activations, which resolves the explosion problem.\nNetwork Training. We use stochastic gradient descent with a fixed learning rate (.01). We use a diagonal variant of AdaGrad for parameter updates [23]. AdaGrad yields a smooth and fast convergence. Furthermore, it can be seen as a natural tuning of individual learning rates per each parameter. This is beneficial for our case since different layers have gradients at different scales because of the scale of non-saturating activations at each layer (grows bigger at higher layers). We update weights after minibatches of 20 sentences. We run 200 epochs for training. Recursive weights within a layer (Whh) are initialized as 0.5I + where I is the identity matrix and is a small uniformly random noise. This means that initially, the representation of each node is approximately the mean of its two children. All other weights are initialized as . We experiment with networks of various sizes, however we have the same number of hidden units across multiple layers of a single RNN. When we increase the depth, we keep the overall number of parameters constant, therefore deeper networks become narrower. We do not employ a pre-training step; deep architectures are trained with the supervised error signal, even when the output layer is connected to only the final hidden layer.\nAdditionally, we employ early stopping: out of all iterations, the model with the best development set performance is picked as the final model to be evaluated."
    }, {
      "heading" : "3.2 Results",
      "text" : "Quantitative Evaluation. We evaluate on both fine-grained sentiment score prediction (5-class classification) and binary (positive-negative) classification. For binary classification, we do not train a separate network, we use the network trained for fine-grained prediction, and then decode the 5 dimensional posterior probability vector into a binary decision which also effectively discards the neutral cases from the test set. This approach solves a harder problem. Therefore there might be room for improvement on binary results by separately training a binary classifier.\nExperimental results of our models and previous work are given in Table 1. Table 1a shows our models with varying depth and width (while keeping the overall number of parameters constant within each group). ` denotes the depth and |h| denotes the width of the networks (i.e. number of hidden units in a single hidden layer).\nWe observe that shallow RNNs get an improvement just by using pretrained word vectors, rectifiers, and dropout, compared to previous work (48.1 vs. 43.2 for the fine-grained task, see our shallow RNN with |h| = 340 in Table 1a and the RNN from [8] in Table 1b). This suggests a validation for untying leaves and internal nodes in the RNN as described in Section 2.2 and using pre-trained word vectors.\nResults on RNNs of various depths and sizes show that deep RNNs outperform single layer RNNs with approximately the same number of parameters, which quantitatively validates the benefits of deep networks over shallow ones (see Table 1a). We see a consistent improvement as we use deeper and narrower networks until a certain depth. The 2-layer RNN for the smaller networks and 4- layer RNN for the larger networks give the best performance with respect to the fine-grained score. Increasing the depth further starts to cause a degrade. An explanation for this might be the decrease in width dominating the gains from an increased depth.\nFurthermore, our best deep RNN outperforms previous work on both the fine-grained and binary prediction tasks, and outperforms Paragraph Vectors on the fine-grained score, achieving a new state-of-the-art (see Table 1b).\nWe attribute an important contribution of the improvement to dropouts. In a preliminary experiment with simple L2 regularization, a 3-layer RNN with 200 hidden units each achieved a fine-grained score of 46.06 (not shown here), compared to our current score of 49.5 with the dropout regularizer.\nInput Perturbation. In order to assess the scale at which different layers operate, we investigate the response of all layers to a perturbation in the input. A way of perturbing the input might be an addition of some noise, however with a large amount of noise, it is possible that the resulting noisy input vector is outside of the manifold of meaningful word vectors. Therefore, instead, we simply pick a word from the sentence that carries positive sentiment, and alter it to a set of words that have sentiment values shifting towards the negative direction.\nIn Figure 3, we give an example sentence, “Roger Dodger is one of the best variations on this theme” with its parse tree. We change the word “best” into the set of words “coolest”, “good”, “average”, “bad”, “worst”, and measure the response of this change along the path that connects the leaf to the root (labeled from 1 to 8). Note that all other nodes have the same representations, since a node is completely determined by its subtree. For each node, the response is measured as the change of its hidden representation in one-norm, for each of the three layers in the network, with respect to the hidden representations using the original word (“best”).\nIn the first layer (bottom) we observe a shared trend change as we go up in the tree. Note that “good” and “bad” are almost on top of each other, which suggests that there is not necessarily enough information captured in the first layer yet to make the correct sentiment decision. In the second layer (middle) an interesting phenomenon occurs: Paths with “coolest” and “good” start close together, as well as “worst” and “bad”. However, as we move up in the tree, paths with “worst” and “coolest” come closer together as well as the paths with “good” and “bad”. This suggests that the second layer remembers the intensity of the sentiment, rather than direction. The third layer (top) is the most consistent one as we traverse upward the tree, and correct sentiment decisions persist across the path.\nNearest Neighbor Phrases. In order to evaulate the different notions of similarity in the meaning space captured by multiple layers, we look at nearest neighbors of short phrases. For a three layer deep recursive neural network we compute hidden representations for all phrases in our data. Then, for a given phrase, we find its nearest neighbor phrases across each layer, with the one-norm distance measure. Two examples are given in Table 2.\nFor the first layer, we observe that similarity is dominated by one of the words that is composed, i.e. “charming” for the phrase “charming results” (and “appealing”, “refreshing” for some neighbors), and “great” for the phrase “not great”. This effect is so strong that it even discards the negation for the second case, “as great” and “is great” are considered similar to “not great”.\nIn the second layer, we observe a more diverse set of phrases semantically. On the other hand, this layer seems to be taking syntactic similarity more into account: in the first example, the nearest neighbors of “charming results” are comprised of adjective-noun combinations that also exhibit some similarity in meaning (e.g. “interesting results”, “riveting performances”). The account is similar for “not great”: its nearest neighbors are adverb-adjective combinations in which the adjectives exhibit some semantic overlap (e.g. “good”, “compelling”). Sentiment is still not properly captured in this layer, however, as seen with the neighbor “too great” for the phrase “not great”.\nIn the third and final layer, we see a higher level of semantic similarity, in the sense that phrases are mostly related to one another in terms of sentiment. Note that since this is a supervised task on sentiment detection, it is sufficient for the network to capture only the sentiment (and how it is composed in context) in the last layer. Therefore, it should be expected to observe an even more diverse set of neighbors with only a sentiment connection."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this work we propose the deep recursive neural network, which is constructed by stacking multiple recursive layers on top of each other. We apply this architecture to the task of fine-grained sentiment classification using binary parse trees as the structure. We empirically evaluated our models against shallow recursive nets. Additionally, we compared with previous work on the task, including a multiplicative RNN and the more recent Paragraph Vectors method. Our experiments show that deep models outperform their shallow counterparts of the same size. Furthermore, deep RNN outperforms the baselines, achieving state-of-the-art performance on the task.\nWe further investigate our models qualitatively by performing input perturbation, and examining nearest neighboring phrases of given examples. These results suggest that adding depth to a recursive net is different from adding width. Each layer captures a different aspect of compositionality. Phrase representations focus on different aspects of meaning at each layer, as seen by nearest neighbor phrase examples.\nSince our task was supervised, learned representations seemed to be focused on sentiment, as in previous work. An important future direction might be an application of the deep RNN to a broader, more general task, even an unsupervised one (e.g. as in [9]). This might provide better insights on the operation of different layers and their contribution, with a more general notion of composition. The effects of fine-tuning word vectors on the performance of deep RNN is also open to investigation."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was supported in part by NSF grant IIS-1314778 and DARPA DEFT FA8750-13-2-0015. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF, DARPA or the U.S. Government."
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Rjean Ducharme", "Pascal Vincent", "Christian Jauvin", "Jaz K", "Thomas Hofmann", "Tomaso Poggio", "John Shawe-taylor" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1990
    }, {
      "title" : "Parsing natural scenes and natural language with recursive neural networks",
      "author" : [ "Richard Socher", "Cliff C Lin", "Andrew Ng", "Chris Manning" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Semisupervised recursive autoencoders for predicting sentiment distributions",
      "author" : [ "Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2013
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "Richard Socher", "Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Learning complex, extended sequences using the principle of history compression",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1992
    }, {
      "title" : "Hierarchical recurrent neural networks for long-term dependencies",
      "author" : [ "Salah El Hihi", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1995
    }, {
      "title" : "Training and analysing deep recurrent neural networks",
      "author" : [ "Michiel Hermans", "Benjamin Schrauwen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "Neural Networks, IEEE Transactions on,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1994
    }, {
      "title" : "Learning task-dependent distributed representations by backpropagation through structure",
      "author" : [ "Christoph Goller", "Andreas Kuchler" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1996
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng" ],
      "venue" : "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V Le", "Tomas Mikolov" ],
      "venue" : "arXiv preprint arXiv:1405.4053,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom" ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2012
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Improving deep neural networks for lvcsr using rectified linear units and dropout",
      "author" : [ "George E Dahl", "Tara N Sainath", "Geoffrey E Hinton" ],
      "venue" : "In Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep connectionist architectures involve many layers of nonlinear information processing [1].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "A key advance for application to NLP tasks was the invention of word embeddings that represent a single word as a dense, low-dimensional vector in a meaning space [2], and from which numerous problems have benefited [3, 4].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "A key advance for application to NLP tasks was the invention of word embeddings that represent a single word as a dense, low-dimensional vector in a meaning space [2], and from which numerous problems have benefited [3, 4].",
      "startOffset" : 216,
      "endOffset" : 222
    }, {
      "referenceID" : 3,
      "context" : "A key advance for application to NLP tasks was the invention of word embeddings that represent a single word as a dense, low-dimensional vector in a meaning space [2], and from which numerous problems have benefited [3, 4].",
      "startOffset" : 216,
      "endOffset" : 222
    }, {
      "referenceID" : 4,
      "context" : "A recursive neural network can be seen as a generalization of the recurrent neural network [5], which has a specific type of skewed tree structure (see Figure 1).",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "They have been applied to parsing [6], sentence-level sentiment analysis [7, 8], and paraphrase detection [9].",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 6,
      "context" : "They have been applied to parsing [6], sentence-level sentiment analysis [7, 8], and paraphrase detection [9].",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "They have been applied to parsing [6], sentence-level sentiment analysis [7, 8], and paraphrase detection [9].",
      "startOffset" : 73,
      "endOffset" : 79
    }, {
      "referenceID" : 8,
      "context" : "They have been applied to parsing [6], sentence-level sentiment analysis [7, 8], and paraphrase detection [9].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "In order to combine these concepts, deep recurrent networks were proposed [10, 11, 12].",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 10,
      "context" : "In order to combine these concepts, deep recurrent networks were proposed [10, 11, 12].",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "In order to combine these concepts, deep recurrent networks were proposed [10, 11, 12].",
      "startOffset" : 74,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "Empirical investigations showed that this results in a natural hierarchy for how the information is processed [12].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "To evaluate the performance of the architecture and make exploratory analyses, we apply deep recursive neural networks to the task of fine-grained sentiment detection on the recently published Stanford Sentiment Treebank (SST) [8].",
      "startOffset" : 227,
      "endOffset" : 230
    }, {
      "referenceID" : 12,
      "context" : "This is especially important for deep learning, since it allows a richer supervised error signal to be backpropagated across the network, potentially alleviating vanishing gradients associated with deep neural networks [13].",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 5,
      "context" : "[6]) (RNNs) comprise an architecture in which the same set of weights is recursively applied within a structural setting: given a positional directed acyclic graph, it visits the nodes in topological order, and recursively applies transformations to generate further representations from previously computed representations of children.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "RNNs can be applied to any positional directed acyclic graph, we limit our attention to RNNs over positional binary trees, as in [6].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "In the parse tree example, a recursive neural network combines the representations of two subphrases to generate a representation for the larger phrase, in the same meaning space [6].",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Thus, during supervised learning, initial external errors are incurred on y, and backpropagated from the root, toward leaves [14].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "(2) Since words and phrases are represented in different spaces, we can use rectifier activation units for f , which have previously been shown to yield good results when training deep neural networks [15].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 5,
      "context" : "the same computation recursively to compute the contribution of children to their parents, and the same computation to produce an output response, we are, in fact, representing every internal node (phrase) in the same space [6, 8].",
      "startOffset" : 224,
      "endOffset" : 230
    }, {
      "referenceID" : 7,
      "context" : "the same computation recursively to compute the contribution of children to their parents, and the same computation to produce an output response, we are, in fact, representing every internal node (phrase) in the same space [6, 8].",
      "startOffset" : 224,
      "endOffset" : 230
    }, {
      "referenceID" : 0,
      "context" : "deep feedforward nets), an important benefit of depth is the hierarchy among hidden representations: every hidden layer conceptually lies in a different representation space and potentially is a more abstract representation of the input than the previous layer [1].",
      "startOffset" : 261,
      "endOffset" : 264
    }, {
      "referenceID" : 7,
      "context" : "For experimental evaluation of our models, we use the recently published Stanford Sentiment Treebank (SST) [8], which includes labels for 215,154 phrases in the parse trees of 11,855 sentences, with an average sentence length of 19.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "We use baselines from [8]: a naive bayes classifier that operates on bigram counts (BINB), shallow RNN (RNN) [6, 7] that learns the word vectors from the supervised data and uses tanh units, in contrast to our shallow RNNs, a matrix-vector RNN in which every word is assigned a matrix-vector pair instead of a vector, and composition is defined with matrix-vector multiplications (MV-RNN) [16], and the multiplicative recursive net (or the recursive neural tensor network) in which the composition is defined as a bilinear tensor product (RNTN) [8].",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "We use baselines from [8]: a naive bayes classifier that operates on bigram counts (BINB), shallow RNN (RNN) [6, 7] that learns the word vectors from the supervised data and uses tanh units, in contrast to our shallow RNNs, a matrix-vector RNN in which every word is assigned a matrix-vector pair instead of a vector, and composition is defined with matrix-vector multiplications (MV-RNN) [16], and the multiplicative recursive net (or the recursive neural tensor network) in which the composition is defined as a bilinear tensor product (RNTN) [8].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "We use baselines from [8]: a naive bayes classifier that operates on bigram counts (BINB), shallow RNN (RNN) [6, 7] that learns the word vectors from the supervised data and uses tanh units, in contrast to our shallow RNNs, a matrix-vector RNN in which every word is assigned a matrix-vector pair instead of a vector, and composition is defined with matrix-vector multiplications (MV-RNN) [16], and the multiplicative recursive net (or the recursive neural tensor network) in which the composition is defined as a bilinear tensor product (RNTN) [8].",
      "startOffset" : 109,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "We use baselines from [8]: a naive bayes classifier that operates on bigram counts (BINB), shallow RNN (RNN) [6, 7] that learns the word vectors from the supervised data and uses tanh units, in contrast to our shallow RNNs, a matrix-vector RNN in which every word is assigned a matrix-vector pair instead of a vector, and composition is defined with matrix-vector multiplications (MV-RNN) [16], and the multiplicative recursive net (or the recursive neural tensor network) in which the composition is defined as a bilinear tensor product (RNTN) [8].",
      "startOffset" : 389,
      "endOffset" : 393
    }, {
      "referenceID" : 7,
      "context" : "We use baselines from [8]: a naive bayes classifier that operates on bigram counts (BINB), shallow RNN (RNN) [6, 7] that learns the word vectors from the supervised data and uses tanh units, in contrast to our shallow RNNs, a matrix-vector RNN in which every word is assigned a matrix-vector pair instead of a vector, and composition is defined with matrix-vector multiplications (MV-RNN) [16], and the multiplicative recursive net (or the recursive neural tensor network) in which the composition is defined as a bilinear tensor product (RNTN) [8].",
      "startOffset" : 545,
      "endOffset" : 548
    }, {
      "referenceID" : 16,
      "context" : "Additionally, we use a method that is capable of generating representations for larger pieces of text (PARAGRAPH VECTORS) [17], and the dynamic convolutional neural network (DCNN) [18].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 17,
      "context" : "Additionally, we use a method that is capable of generating representations for larger pieces of text (PARAGRAPH VECTORS) [17], and the dynamic convolutional neural network (DCNN) [18].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 14,
      "context" : "Previous work with rectifier units reported good results when training deep neural networks, with no pre-training step [15].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "We use the publicly available 300 dimensional word vectors by [19], trained on part of the Google News dataset (∼100B words).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "For regularization of the networks, we use the recently proposed dropout technique, in which we randomly set entries of hidden representations to 0, with a probability called the dropout rate [20].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 20,
      "context" : "Dropout prevents learned features from co-adapting, and it has been reported to yield good results when training deep neural networks [21, 22].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "Dropout prevents learned features from co-adapting, and it has been reported to yield good results when training deep neural networks [21, 22].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 22,
      "context" : "We use a diagonal variant of AdaGrad for parameter updates [23].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "2 for the fine-grained task, see our shallow RNN with |h| = 340 in Table 1a and the RNN from [8] in Table 1b).",
      "startOffset" : 93,
      "endOffset" : 96
    } ],
    "year" : 2014,
    "abstractText" : "Recursive neural networks comprise a class of architecture that can operate on structured input. They have been previously successfully applied to model compositionality in natural language using parse-tree-based structural representations. Even though these architectures are deep in structure, they lack the capacity for hierarchical representation that exists in conventional deep feed-forward networks as well as in recently investigated deep recurrent neural networks. In this work we introduce a new architecture — a deep recursive neural network (deep RNN) — constructed by stacking multiple recursive layers. We evaluate the proposed model on the task of fine-grained sentiment classification. Our results show that deep RNNs outperform associated shallow counterparts that employ the same number of parameters. Furthermore, our approach outperforms previous baselines on the sentiment analysis task, including a multiplicative RNN variant as well as the recently introduced paragraph vectors, achieving new state-of-the-art results. We provide exploratory analyses of the effect of multiple layers and show that they capture different aspects of compositionality in language.",
    "creator" : null
  }
}