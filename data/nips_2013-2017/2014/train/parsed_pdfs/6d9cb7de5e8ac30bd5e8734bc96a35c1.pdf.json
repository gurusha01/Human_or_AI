{
  "name" : "6d9cb7de5e8ac30bd5e8734bc96a35c1.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Multitask learning meets tensor factorization: task imputation via convex optimization",
    "authors" : [ "Kishan Wimalawarne", "Masashi Sugiyama", "Ryota Tomioka" ],
    "emails" : [ "kishan@sg.cs.titech.ac.jp", "sugi@k.u-tokyo.ac.jp", "tomioka@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider supervised multitask learning problems [1, 6, 7] in which the tasks are indexed by a pair of indices known as multilinear multitask learning (MLMTL) [17, 19]. For example, when we would like to predict the ratings of different aspects (e.g., quality of service, food, etc) of restaurants by different customers, the tasks would be indexed by aspects × customers. When each task is parametrized by a weight vector over features, the goal would be to learn a features × aspects × customers tensor. Another possible task dimension would be time, since the ratings may change over time.\nThis setting is interesting, because it would allow us to exploit the similarities across different customers as well as similarities across different aspects or time-points. Furthermore this would allow us to perform task imputation, that is to learn weights for tasks for which we have no training examples. On the other hand, the conventional matrix-based multitask learning (MTL) [2, 3, 13, 16] may fail to capture the higher order structure if we consider learning a flat features × tasks matrix and would require at least r samples, where r is the rank of the matrix to be learned, for each task.\nRecently several norms that induce low-rank tensors in the sense of Tucker decomposition or multilinear singular value decomposition [8, 9, 14, 25] have been proposed. The mean squared error for recovering a n1 × · · · × nK tensor of multilinear rank (r1, . . . , rK) from its noisy version scale as O(( 1K ∑K k=1 √ rk) 2( 1K ∑K k=1 1/ √ nk)\n2) for the overlapped trace norm [23]. On the other hand, the error of the latent trace norm scales as O(mink rk/mink nk) in the same setting [21]. Thus while the latent trace norm has the better dependence in terms of the multilinear rank rk, it has the worse dependence in terms of the dimensions nk.\nTensors that arise in multitask learning typically have heterogeneous dimensions. For example, the number of aspects for a restaurant (quality of service, food, atmosphere, etc.) would be much\nsmaller than the number of customers or the number of features. In addition, it is a priori unclear which mode (or dimension) would have the most redundancy or sharing that could be exploited by multitask learning. Some of the modes may have full ranks if there is no sharing of information along them. Therefore, both the latent trace norm and the overlapped trace norm would suffer either from the heterogeneous multilinear rank or the heterogeneous dimensions in this context.\nIn this paper, we propose a modification to the latent trace norm whose mean squared error scales as O(mink(rk/nk)) in the same setting, which is better than both the previously proposed extensions of trace norm for tensors. We study the excess risk of the three norms through their Rademacher complexities in various settings including matrix completion, multitask learning, and MLMTL. The new analysis allows us to also study the tensor completion setting, which was only empirically studied in [22, 23]. Our analysis consistently shows the advantage of the proposed scaled latent trace norm in various settings in which the dimensions or ranks are heterogeneous. Experiments on both synthetic and real data sets are also consistent with our theoretical findings."
    }, {
      "heading" : "2 Norms for tensors and their denoising performance",
      "text" : "Let W ∈ Rn1×···×nK be a K-way tensor. We denote the total number of entries by N := ∏K\nk=1 nk. A mode-k fiber of W is an nk dimensional vector we obtain by fixing all but the kth index. The mode-k unfolding W (k) of W is the nk×N/nk matrix formed by concatenating all the N/nk modek fibers along columns. We say that W has multilinear rank (r1, . . . , rK) if rk = rank(W (k))."
    }, {
      "heading" : "2.1 Existing norms for tensors",
      "text" : "First we review two norms proposed in literature in order to convexify tensor decomposition.\nThe overlapped trace norm (see [12, 15, 18, 22]) is defined as the sum of the trace norms of the mode-k unfoldings as follows:\n|||W|||overlap = ∑K\nk=1 ∥W (k)∥tr, (1)\nwhere ∥ · ∥tr is the trace norm (also known as the nuclear norm) [10, 20], which is defined as the absolute sum of singular values. Romera-Paredes et al. [17] has used the overlapped trace norm in MLMTL.\nThe latent trace norm [21, 22] is defined as the infimum over K tensors as follows: |||W|||latent = infW(1)+···+W(K)=W ∑K k=1 ∥W (k)(k)∥tr. (2)\nTable 1 summarizes the denoising performance in mean squared error analyzed in Tomioka and Suzuki [21] for the above two norms. The setting is as follows: we observe a noisy version Y of a tensor W∗ with multilinear rank (r1, . . . , rK) and would like to recover W∗ by solving\nŴ = argmin W\n( 1\n2 |||W − Y|||2F + λ |||W|||⋆\n) , (3)\nwhere |||·|||⋆ is either the overlapped trace norm or the latent trace norm. We can see that while the latent trace norm has the better dependence in terms of the multilinear rank, it has the worse dependence in terms of the dimensions. Intuitively, the latent trace norm recognizes the mode with the lowest rank. However, it does not have a good control of the dimensions; in fact, the factor\n1/mink nk comes from the fact that for a random tensor X with i.i.d. Gaussian entries, the expectation of the dual norm ∥X∥latent∗ = maxk ∥X(k)∥op behaves like Op( √ maxk N/nk), where ∥ · ∥op is the operator norm."
    }, {
      "heading" : "2.2 A new norm",
      "text" : "In order to correct the unfavorable behavior of the dual norm, we propose the scaled latent trace norm. It is defined similarly to the latent trace norm with weights 1/ √ nk as follows:\n|||W|||scaled = infW(1)+···+W(K)=W K∑ k=1 1 √ nk ∥W (k)(k)∥tr. (4)\nNow the expectation of the dual norm ∥X∥scaled∗ = maxk √ nk∥X(k)∥op behaves like Op( √ N) for X with random i.i.d. Gaussian entries and combined with the following relation\n|||W|||scaled ≤ min k √ rk nk |||W|||F , (5)\nwe obtain the scaling of the mean squared error in the last column of Table 1. We can see that the scaled latent norm recognizes the mode with the lowest rank relative to its dimension."
    }, {
      "heading" : "3 Theory for multilinear multitask learning",
      "text" : "We consider T = PQ supervised learning tasks. Training samples (xipq, yipq) mpq i=1 ((p, q) ∈ S) are provided for a relatively small fraction of the task index pairs S ⊂ [P ] × [Q]. Each task is parametrized by a weight vector wpq ∈ Rd, which can be collected into a 3-way tensor W = (wpq) ∈ Rd×P×Q whose (p, q) fiber is wpq . We define the learning problem as follows:\nŴ = argmin W∈Rd×P×Q L̂(W), subject to |||W|||⋆ ≤ B0, (6)\nwhere the norm |||·|||⋆ is either the overlapped trace norm, latent trace norm, or the scaled latent trace norm, and the empirical risk L̂ is defined as follows:\nL̂(W) = 1 |S| ∑ (p,q)∈S 1 mpq mpq∑ i=1 ℓ (⟨xipq,wpq⟩ − yipq) .\nThe true risk we are interested in minimizing is defined as follows:\nL(W) = 1 PQ ∑ p,q E(x,y)∼Ppqℓ (⟨x,wpq⟩ − y) ,\nwhere Ppq is the distribution from which the samples (xipq, yipq) mpq i=1 are drawn from.\nThe next lemma relates the excess risk L(Ŵ) − L(W∗) with the expected dual norm E |||D|||⋆∗ through Rademacher complexity. Lemma 1. We assume that the output yipq is bounded as |yipq| ≤ b, and the number of samples mpq ≥ m > 0 for the observed tasks. We also assume that the loss function ℓ is Lipschitz continuous with the constant Λ, bounded in [0, c] and ℓ(0) = 0. Let W∗ be any tensor such that |||W∗|||⋆ ≤ B0. Then with probability at least 1− δ, any minimizer of (6) satisfies the following bound:\nL(Ŵ)− L(W∗) ≤ 2Λ ( 2B0 |S| E |||D|||⋆∗ + b √ ρ√\n|S|m\n) + c′ √ log(2/δ)\n2|S|m ,\nwhere c′ = c+ 1, |||·|||⋆∗ is the dual norm of |||·|||⋆, ρ := 1 |S| ∑ (p,q)∈S mpq m . The tensor D ∈ R d×P×Q\nis defined as the sum D = ∑ (p,q)∈S ∑mpq i=1 Zipq , where Zipq ∈ Rd×P×Q is defined as\n(p′, q′)th fiber ofZipq =\n{ 1\nmpq σipqxipq, if p = p′ and q = q′, 0, otherwise.\nHere σipq ∈ {−1,+1} are Rademacher random variables and the expectation in the above inequality is with respect to σipq , the random draw of tasks S, and the training samples (xipq, yipq) mpq i=1 .\nProof. The proof is a standard one following the line of [5] and it is presented in Appendix A.\nThe next theorem computes the expected dual norm E |||D|||⋆∗ for the three norms for tensors (the proof can be found in Appendix B). Theorem 1. We assume that Cpq := E[xipqxipq⊤] ⪯ κdId and there is a constant R > 0 such that ∥xipq∥ ≤ R almost surely. Let us define"
    }, {
      "heading" : "D1 := d+ PQ, D2 := P + dQ, D3 := Q+ dP.",
      "text" : "In order to simplify the presentation, we assume that maxk Dk ≥ 3 and dPQ ≥ max(d2, P 2, Q2). For the overlapped trace norm, the latent trace norm, and the scaled latent trace norm, the expectation E |||D|||⋆∗ can be bounded as follows:\n1\n|S| E |||D|||overlap∗ ≤ Cmin k\n(√ κ\nm|S|dPQ Dk logDk +\nR\nm|S| logDk\n) , (7)\n1\n|S| E |||D|||latent∗ ≤ C\n′ (√\nκ\nm|S|dPQ max k (Dk logDk) +\nR\nm|S| log(max k Dk)\n) , (8)\n1\n|S| E |||D|||scaled∗ ≤ C\n′′ (√ κ\nm|S| log(max k Dk) + R √ maxk nk m|S| log(max k Dk)\n) , (9)\nwhere C,C ′, C ′′ are constants, n1 = d, n2 = P , and n3 = Q. Furthermore, if m|S| ≥ R2(maxk nk) log(maxk Dk)/κ, the O(1/m|S|) terms in the above inequalities can be dropped.\nNote that the assumption that the norm of xipq is bounded is natural because the target yipq is also bounded. The parameter κ in the assumption Cpq ⪯ κ/dId controls the amount of correlation in the data. Since Tr(C) = E∥xipq∥2 ≤ R2, we have κ = O(1) when the features are uncorrelated; on the other hand, we have κ = O(d), if they lie in a one dimensional subspace. The number of samples m|S| = Õ(maxk nk) is enough to drop the O(1/m|S|) term even if κ = O(1). Now we state the consequences of Theorem 1 for the three norms for tensors. The common assumptions are the same as in Lemma 1 and Theorem 1. We also assume m|S| ≥ R2(maxk nk) log(maxk Dk)/κ to drop the O(1/m|S|) terms. Let W∗ be any d × P × Q tensor with multilinear-rank (r1, r2, r3) and bounded element-wise as |||W∗|||ℓ∞ ≤ B. Corollary 1 (Overlapped trace norm). With probability at least 1 − δ, any minimizer of (6) with |||W|||overlap ≤ B √ ∥r∥1/2dPQ satisfies the following inequality:\nL(Ŵ)− L(W∗) ≤ c1ΛB √ κ\nm|S| ∥r∥1/2 min k (Dk logDk) + c2Λb\n√ ρ\nm|S| + c3\n√ log(2/δ)\nm|S| ,\nwhere ∥r∥1/2 = ( ∑3\nk=1\n√ rk/3) 2 and c1, c2, c3 are constants.\nNote that Tomioka et al. [23] obtained a bound that depends on ( ∑3\nk=1\n√ Dk/3)\n2 instead of min(Dk logDk). Although the minimum may look better than the average, our bound has the worse constant K = 3 hidden in c1. The logDk factor allows us to apply the above result to the setting of tensor completion as we show below. Corollary 2 (Latent trace norm). With probability at least 1 − δ, any minimizer of (6) with |||W|||latent ≤ B √ mink rkdPQ satisfies the following inequality:\nL(Ŵ)− L(W∗) ≤ c′1ΛB √ κ\nm|S| min k rk max k (Dk logDk) + c2Λb\n√ ρ\nm|S| + c3\n√ log(2/δ)\nm|S| ,\nwhere c′1, c2, c3 are constants. Corollary 3 (Scaled latent trace norm). With probability at least 1 − δ, any minimizer of (6) with |||W|||scaled ≤ B √ mink(rk/nk)dPQ satisfies the following inequality:\nL(Ŵ)− L(W∗) ≤ c′′1ΛB\n√ κ\nm|S| min k ( rk nk ) dPQ log(max k Dk) + c2Λb √ ρ m|S| + c3 √ log(2/δ) m|S| ,\nwhere n1 = d, n2 = P , n3 = Q, and c′′1 , c2, c3 are constants.\nWe summarize the implications of the above corollaries for different settings in Table 2. We almost recover the settings for matrix completion [11] and multitask learning (MTL) [16]. Note that these simpler problems sometimes disguise themselves as the more general tensor completion or multilinear multitask learning problems. Therefore it is important that the new tensor based norms adapts to the simplicity of the problems in these cases. Matrix completion is when d = κ = m = r1 = 1, and we assume that r2 = r3 = r < P,Q. The sample complexities are the number of samples |S| that we need to make the leading term in Corollaries 1, 2, and 3 equal ϵ. We can see that the overlapped trace norm and the scaled latent trace norm recover the known result for matrix completion [11]. The plain latent trace norm requires O(PQ) samples because it recognizes the first mode as the mode with the lowest rank 1. Although the rank r of the last two modes are low relative to their dimensions, the latent trace norm fails to recognize this. In multitask learning (MTL), only the first mode corresponding to features has a low rank r and the other two modes have full rank. Note that a tensor is a matrix when its multilinear rank is full except for one mode. We also assume that all the pairs (p, q) are observed (|S| = PQ) as in [16]. The sample complexities are defined the same way as above with respect to the number of samples m because |S| is fixed. The homogeneous case is when d = P = Q. The heterogeneous case is when P ≤ r < d. Our bound for the overlapped trace norm is almost as good as the one in [16] but has an multiplicative log(PQ) factor (as oppose to their additive log(PQ) term) and ∥r∥1/2 ≥ r. Also note that the results in [16] can be applied when d is much larger than P and Q. Turning back to our bounds, both the latent trace norm and its scaled version can perform as well as knowing the mode with the lowest rank (the first mode) (see also [21]) when d = P = Q. However, when the dimensions are heterogeneous, similarly to the matrix completion case above, the plain latent trace norm fails to recognize the lowrank-ness of the first mode and\nTa bl\ne 2:\nSa m\npl e\nco m\npl ex\niti es\nof th\ne ov\ner la\npp ed\ntr ac\ne no\nrm ,l\nat en\ntt ra\nce no\nrm ,a\nnd th\ne sc\nal ed\nla te\nnt tr\nac e\nno rm\nin va\nri ou\ns se\ntti ng\ns. T\nhe co\nm m\non fa\nct or\n1/ ϵ2 is om itt ed fr om th e sa m pl e co m pl ex iti es .T he sa m pl e co m pl ex iti es ar e de fin ed w ith re sp ec tt o |S |f or m at ri x co m pl et io n, m fo rm ul tit as k le ar ni ng ,a nd m |S |f or te ns or co m pl et io n an d m ul til in ea rm ul tit as k le ar ni ng .I n th e he te ro ge ne ou s ca se s, w e as su m e P ≤ r < r′ .W e de fin e ∥r ∥ 1 / 2 = (∑ 3 k = 1 √ r k /K )2 an d N := n 1 n 2 n 3 . Sa m pl e co m pl ex iti es (p er 1 / ϵ2 ) (n 1 ,n 2 ,n 3 ) (r 1 ,r 2 ,r 3 ) (κ ,B ,| S |) O ve rl ap L at en t Sc al ed\nM at\nri x\nco m\npl et\nio n\n[1 1]\nP !\nQ !\n1 !\n(1 ,r ,r )\n(1 ,1 ,| S |)\n∥r ∥ 1\n/ 2 (P\n+ Q ) lo g (P\n+ Q )\nP Q lo g (P\nQ )\nr( P\n+ Q ) lo g (P\nQ )\nM T\nL [1\n6] (h\nom og ene ou s ca se )\nd !\nd 2 !\n(r ,d ,d )\n(d ,1 / √ d ,d\n2 )\n∥r ∥ 1\n/ 2 lo g (d\n2 )\nr lo g (d\n2 )\nr lo g (d\n2 )\nM T\nL (h\net er\nog en\neo us\nca se\n)\nd !\nP Q !\n(r ,P\n,r ′ )\n(d ,1 / √ d ,P\nQ )\n∥r ∥ 1\n/ 2 lo g (P\nQ )\nd lo g (d Q )\nr lo g (d Q )\nM L\nM T\nL [1\n7] (h\nom o-\nge ne\nou s\nca se\n)\nd ! d ! d !\n(r 1 ,r\n2 ,r\n3 )\n(κ ,1 ,| S |)\nκ ∥r\n∥ 1 / 2 d 2 lo g (d\n2 )\nκ (m\nin k r k )d\n2 lo g (d\n2 )\nκ (m\nin k r k )d\n2 lo g (d\n2 )\nM L\nM T\nL [1\n7] (h\net er oge ne ou s ca se )\nP ! Q ! d !\n(r ,P\n,r ′ )\n(κ ,1 ,| S |)\nκ ∥r\n∥ 1 / 2 P Q lo g (P\nQ )\nκ d P Q lo g (d Q )\nκ m in (r P Q ,d P r′ ) lo g (d Q )\nTe ns\nor co\nm pl\net io\nn n 2 ! n 3 ! n 1 !\n(r 1 ,r\n2 ,r\n3 )\n(1 ,1 ,| S |)\n∥r ∥ 1\n/ 2 m in k (D\nk lo g D\nk )\nm in k\nr k m a x\nk (D\nk lo g D\nk )\nm in k ( r k\nn k )N\nlo g (m\na x k D\nk )\nrequires O(d) samples, because the second mode has the lowest rank P .\nIn multilinear multitask learning (MLMTL) [17], any mode could possibly be low rank but it is a priori unknown. The sample complexities are defined the same way as above with respect to m|S|. The homogeneous case is when d = P = Q. The heterogeneous case is when the first mode or the third mode is low rank but P ≤ r < d. Similarly to the above two settings, the overlapped trace norm has a mild dependence on the dimensions but a higher dependence on the rank ∥r∥1/2 ≥ r. The latent trace norm performs as well as knowing the mode that has the lowest rank in the homogeneous case. However, it fails to recognize the mode with the lowest rank relative to its dimension. The scaled latent trace norm does this and although it has a higher logarithmic dependence, it is competitive in both cases.\nFinally, our bounds also hold for tensor completion. Although Tomioka et al. [22, 23] studied tensor completion algorithms, their analysis assumed that the inputs xipq are drawn from a Gaussian distribution, which does not hold for tensor completion. Note that in our setting xipq can be an indicator vector that has one in the jth position uniformly over 1, . . . , d. In this case, κ = 1. The sample complexities of different norms with respect to m|S| is shown in the last row of Table 2. The sample complexity for the overlapped trace norm is the same as the one in [23] with a logarithmic factor. The sample complexities for the latent and scaled latent trace norms are new. Again we can see that while the latent trace norm recognize the mode with the lowest rank, the scaled latent trace norm is able to recognize the mode with the lowest rank relative to its dimension."
    }, {
      "heading" : "4 Experiments",
      "text" : "We conducted several experiments to evaluate performances of tensor based multitask learning setting we have discussed in Section 3. In Section 4.1, we discuss simulation we conducted using synthetic data sets. In Sections 4.2 and 4.3, we discuss experiments on two real world data sets, namely the Restaurant data set [26] and School Effectiveness data set [3, 4]. Both of our real world data sets have heterogeneous dimensions (see Figure 2) and it is a priori unclear across which mode has the most amount of information sharing."
    }, {
      "heading" : "4.1 Synthetic data sets",
      "text" : "The true d× P ×Q tensor W∗ was generated by first sampling a r1 × r2 × r3 core tensor and then multiplying random orthonormal matrix to each of its modes. For each task (p, q) ∈ [P ] × [Q], we generated training set of m vectors (xipq, yipq)mi=1 by first sampling xipq from the standard normal distribution and then computing yipq = ⟨xipq,wpq⟩ + νi, where νi was drawn from a zero-mean normal distribution with variance 0.1. We used the penalty formulation of (6) with the squared loss and selected the regularization parameter λ using two-fold cross validation on the training set from the range 0.01 to 10 with the interval 0.1.\nIn addition to the three norms for tensors we discussed in the previous section, we evaluated the matrix-based multitask learning approaches that penalizes the trace norm of the unfolding of W at specific modes. The conventional convex multitask learning [2, 3, 16] corresponds to one of these approaches that penalizes the trace norm of the first unfolding ∥W (1)∥tr. The convex MLMTL in [17] corresponds to the overlapped trace norm.\nIn the first experiment, we chose d = P = Q = 10 and r1 = r2 = r3 = 3. Therefore, both the dimensions and the multilinear rank are homogeneous. The result is shown in Figure 1(a). The overlapped trace norm performed the best, the matrix-based approaches performed next, and the latent trace norm and the scaled latent trace norm were the worst. The scaling of the latent trace norm had no effect because the dimensions were homogeneous. Since the sample complexities for all the methods were the same in this setting (see Table 2), the difference in the performances could be explained by a constant factor K(= 3) that is not shown in the sample complexities.\nIn the second experiment, we chose the dimensions to be homogeneous as d = P = Q = 10, but (r1, r2, r3) = (3, 6, 8). The result is shown in Figure 1(b). In this setting, the (scaled) latent trace norm and the mode-1 regularization performed the best. The lower the rank of the corresponding mode, the lower were the error of the matrix-based MTL approaches. The overlapped trace norm was somewhat in the middle of the three matrix-based approaches.\nIn the last experiment, we chose both the dimensions and the multilinear rank to be heterogeneous as (d, P,Q) = (10, 3, 10) and (r1, r2, r3) = (3, 3, 8). The result is shown in Figure 1(c). Clearly the first mode had the lowest rank relative to its dimension. However, the latent trace norm recognizes the second mode as the mode with the lowest rank and performed similarly to the mode-2 regularization. The overlapped trace norm performed better but it was worse than the mode-1 regularization. The scaled latent trace norm performed comparably to the mode-1 regularization."
    }, {
      "heading" : "4.2 Restaurant data set",
      "text" : "The Restaurant data set contains data for a recommendation system for restaurants where different customers have given ratings to different aspects of each restaurant. Following the same approach as in [17] we modelled the problem as a MLMTL problem with d = 45 features, P = 3 aspects, and Q = 138 customers.\nThe total number of instances for all the tasks were 3483 and we randomly selected training set of sizes 400, 800, 1200, 1600, 2000, 2400, and 2800. When the size was small many tasks contained no training example. We also selected 250 instances as the validation set and the rest was used as the test set. The regularization parameter for each norm was selected by minimizing the mean squared error on the validation set from the candidate values in the interval [50, 1000] for the overlapped, [0.5, 40] for the latent, [6000, 20000] for the scaled latent norms, respectively.\nWe also evaluated matrix-based MTL approaches on different modes and ridge regression (Frobenius norm regularization; abbreviated as RR) as baselines. The convex MLMTL in [17] corresponds to the overlapped trace norm.\nThe result is shown in Figure 2(a). We found the multilinear rank of the solution obtained by the overlapped trace norm to be typically (1, 3, 3). This was consistent with the fact that the performances of the mode-1 regularization and the ridge regression were equal. In other words, the effective dimension of the first mode (features) was one instead of 45. The latent trace norm recognized the first mode as the mode with the lowest rank and it failed to take advantage of the low-rank-ness of the second and the third modes. The scaled latent trace norm was able to perform the best matching with the performances of mode-2 and mode-3 regularization. When the number of samples was above 2400, the latent trace norm caught up with other methods, probably because the effective dimension became higher in this regime."
    }, {
      "heading" : "4.3 School data set",
      "text" : "The data set comes from the inner London Education Authority (ILEA) consisting of examination records from 15362 students at 139 schools in years 1985, 1986, and 1987. We followed [4] for the preprocessing of categorical attributes and obtained 24 features. Previously Argyriou et al. [3] modeled this data set as a 27× 139 matrix-based MTL problem in which the year was modeled as a\ntrinomial attribute. Instead here we model this data set as a 24×139×3 MLMTL problem in which the third mode corresponds to the year. Following earlier papers, [3, 4], we used the percentage of explained variance, defined as 100 · (1− (test MSE)/(variance of y)), as the evaluation metric. The results are shown in Figure 2(b). First, ridge regression performed the worst because it was not able to take advantage of the low-rank-ness of any mode. Second, the plain latent trace norm performed similarly to the mode-3 regularization probably because the dimension 3 was lower than the rank of the other two modes. Clearly the scaled latent trace norm performed the best matching with the performance of the mode-2 regularization; probably the second mode had the most redundancy. The performance of the overlapped trace norm was comparable or slightly better than the mode-1 regularization. The percentage of the explained variance of the latent trace norm exceeds 30 % around sample size 4000 (around 30 samples per school), which is higher than the Hierarchical Bayes [4] (around 29.5 %) and matrix-based MTL [3] (around 26.7 %) that used around 80 samples per school."
    }, {
      "heading" : "5 Discussion",
      "text" : "Using tensors for modeling multitask learning [17, 19] is a promising direction that allows us to take advantage of similarity of tasks in multiple dimensions and even make prediction for a task with no training example. However, having multiple modes, we would have to face with more hyperparameters to choose in the conventional nonconvex tensor decomposition framework. Convex relaxation of tensor multilinear rank allows us to side-step this issue. In fact, we have shown that the sample complexity of the latent trace norm is as good as knowing the mode with the lowest rank. This is consistent with the analysis of [21] in the tensor denoising setting (see Table 1).\nIn the setting of tensor-based MTL, however, the notion of mode with the lowest rank may be vacuous because some modes may have very low dimension. In fact, the sample complexity of the latent trace norm can be as bad as not using any low-rank-ness at all if there is a mode with dimension lower than the rank of the other modes. The scaled latent trace norm we proposed in this paper recognizes the mode with the lowest rank relative to its dimension and lead to the competitive sample complexities in various settings we have shown in Table 2.\nAcknowledgment: MS acknowledges support from the JST CREST program."
    } ],
    "references" : [ {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "R.K. Ando", "T. Zhang" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "Multi-task feature learning",
      "author" : [ "A. Argyriou", "T. Evgeniou", "M. Pontil" ],
      "venue" : "Adv. Neural. Inf. Process. Syst",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "A spectral regularization framework for multi-task structure learning",
      "author" : [ "A. Argyriou", "M. Pontil", "Y. Ying", "C.A. Micchelli" ],
      "venue" : "Adv. Neural. Inf. Process. Syst",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Task clustering and gating for bayesian multitask learning",
      "author" : [ "B. Bakker", "T. Heskes" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "A model of inductive bias learning",
      "author" : [ "J. Baxter" ],
      "venue" : "J. Artif. Intell. Res.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2000
    }, {
      "title" : "Multitask learning",
      "author" : [ "R. Caruana" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "A multilinear singular value decomposition",
      "author" : [ "L. De Lathauwer", "B. De Moor", "J. Vandewalle" ],
      "venue" : "SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "On the best rank-1 and rank-(R1",
      "author" : [ "L. De Lathauwer", "B. De Moor", "J. Vandewalle" ],
      "venue" : "RN ) approximation of higher-order tensors. SIAM J. Matrix Anal. Appl.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "A Rank Minimization Heuristic with Application to Minimum Order System Approximation",
      "author" : [ "M. Fazel", "H. Hindi", "S.P. Boyd" ],
      "venue" : "In Proc. of the American Control Conference,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "Concentration-based guarantees for low-rank matrix reconstruction",
      "author" : [ "R. Foygel", "N. Srebro" ],
      "venue" : "Arxiv preprint arXiv:1102.3923,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Tensor completion and low-n-rank tensor recovery via convex optimization",
      "author" : [ "S. Gandy", "B. Recht", "I. Yamada" ],
      "venue" : "Inverse Problems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Regularization techniques for learning with matrices",
      "author" : [ "S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T.G. Kolda", "B.W. Bader" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Tensor completion for estimating missing values in visual data",
      "author" : [ "J. Liu", "P. Musialski", "P. Wonka", "J. Ye" ],
      "venue" : "In Proc. ICCV,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Excess risk bounds for multitask learning with trace norm regularization",
      "author" : [ "A. Maurer", "M. Pontil" ],
      "venue" : "Technical report,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Multilinear multitask learning",
      "author" : [ "B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Nuclear norms for tensors and their use for convex multilinear estimation",
      "author" : [ "M. Signoretto", "L. De Lathauwer", "J. Suykens" ],
      "venue" : "Technical Report 10-186,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Learning tensors in reproducing kernel hilbert spaces with multilinear spectral penalties",
      "author" : [ "M. Signoretto", "L. De Lathauwer", "J.A.K. Suykens" ],
      "venue" : "Technical report,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola" ],
      "venue" : "Adv. Neural. Inf. Process. Syst",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    }, {
      "title" : "Convex tensor decomposition via structured Schatten norm regularization",
      "author" : [ "R. Tomioka", "T. Suzuki" ],
      "venue" : "editors, Adv. Neural. Inf. Process. Syst",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Estimation of low-rank tensors via convex optimization",
      "author" : [ "R. Tomioka", "K. Hayashi", "H. Kashima" ],
      "venue" : "Technical report,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Statistical performance of convex tensor decomposition",
      "author" : [ "R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima" ],
      "venue" : "In Adv. Neural. Inf. Process. Syst",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices",
      "author" : [ "J.A. Tropp" ],
      "venue" : "Found. Comput. Math.,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Some mathematical notes on three-mode factor analysis",
      "author" : [ "L.R. Tucker" ],
      "venue" : "Psychometrika, 31(3):279–311,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1966
    }, {
      "title" : "Effects of relevant contextual features in the performance of a restaurant recommender system",
      "author" : [ "B. Vargas-Govea", "G. González-Serna", "R. Ponce-Medellın" ],
      "venue" : "In Proceedings of 3rd Workshop on Context-Aware Recommender Systems",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We consider supervised multitask learning problems [1, 6, 7] in which the tasks are indexed by a pair of indices known as multilinear multitask learning (MLMTL) [17, 19].",
      "startOffset" : 51,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "We consider supervised multitask learning problems [1, 6, 7] in which the tasks are indexed by a pair of indices known as multilinear multitask learning (MLMTL) [17, 19].",
      "startOffset" : 51,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "We consider supervised multitask learning problems [1, 6, 7] in which the tasks are indexed by a pair of indices known as multilinear multitask learning (MLMTL) [17, 19].",
      "startOffset" : 51,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "We consider supervised multitask learning problems [1, 6, 7] in which the tasks are indexed by a pair of indices known as multilinear multitask learning (MLMTL) [17, 19].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 18,
      "context" : "We consider supervised multitask learning problems [1, 6, 7] in which the tasks are indexed by a pair of indices known as multilinear multitask learning (MLMTL) [17, 19].",
      "startOffset" : 161,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "On the other hand, the conventional matrix-based multitask learning (MTL) [2, 3, 13, 16] may fail to capture the higher order structure if we consider learning a flat features × tasks matrix and would require at least r samples, where r is the rank of the matrix to be learned, for each task.",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 2,
      "context" : "On the other hand, the conventional matrix-based multitask learning (MTL) [2, 3, 13, 16] may fail to capture the higher order structure if we consider learning a flat features × tasks matrix and would require at least r samples, where r is the rank of the matrix to be learned, for each task.",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, the conventional matrix-based multitask learning (MTL) [2, 3, 13, 16] may fail to capture the higher order structure if we consider learning a flat features × tasks matrix and would require at least r samples, where r is the rank of the matrix to be learned, for each task.",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "On the other hand, the conventional matrix-based multitask learning (MTL) [2, 3, 13, 16] may fail to capture the higher order structure if we consider learning a flat features × tasks matrix and would require at least r samples, where r is the rank of the matrix to be learned, for each task.",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 7,
      "context" : "Recently several norms that induce low-rank tensors in the sense of Tucker decomposition or multilinear singular value decomposition [8, 9, 14, 25] have been proposed.",
      "startOffset" : 133,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "Recently several norms that induce low-rank tensors in the sense of Tucker decomposition or multilinear singular value decomposition [8, 9, 14, 25] have been proposed.",
      "startOffset" : 133,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "Recently several norms that induce low-rank tensors in the sense of Tucker decomposition or multilinear singular value decomposition [8, 9, 14, 25] have been proposed.",
      "startOffset" : 133,
      "endOffset" : 147
    }, {
      "referenceID" : 24,
      "context" : "Recently several norms that induce low-rank tensors in the sense of Tucker decomposition or multilinear singular value decomposition [8, 9, 14, 25] have been proposed.",
      "startOffset" : 133,
      "endOffset" : 147
    }, {
      "referenceID" : 22,
      "context" : ", rK) from its noisy version scale as O(( 1 K ∑K k=1 √ rk) (2)( 1 K ∑K k=1 1/ √ nk) (2)) for the overlapped trace norm [23].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 20,
      "context" : "On the other hand, the error of the latent trace norm scales as O(mink rk/mink nk) in the same setting [21].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "The new analysis allows us to also study the tensor completion setting, which was only empirically studied in [22, 23].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : "The new analysis allows us to also study the tensor completion setting, which was only empirically studied in [22, 23].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 11,
      "context" : "The overlapped trace norm (see [12, 15, 18, 22]) is defined as the sum of the trace norms of the mode-k unfoldings as follows:",
      "startOffset" : 31,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "The overlapped trace norm (see [12, 15, 18, 22]) is defined as the sum of the trace norms of the mode-k unfoldings as follows:",
      "startOffset" : 31,
      "endOffset" : 47
    }, {
      "referenceID" : 17,
      "context" : "The overlapped trace norm (see [12, 15, 18, 22]) is defined as the sum of the trace norms of the mode-k unfoldings as follows:",
      "startOffset" : 31,
      "endOffset" : 47
    }, {
      "referenceID" : 21,
      "context" : "The overlapped trace norm (see [12, 15, 18, 22]) is defined as the sum of the trace norms of the mode-k unfoldings as follows:",
      "startOffset" : 31,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "where ∥ · ∥tr is the trace norm (also known as the nuclear norm) [10, 20], which is defined as the absolute sum of singular values.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 19,
      "context" : "where ∥ · ∥tr is the trace norm (also known as the nuclear norm) [10, 20], which is defined as the absolute sum of singular values.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 16,
      "context" : "[17] has used the overlapped trace norm in MLMTL.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "The latent trace norm [21, 22] is defined as the infimum over K tensors as follows:",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : "The latent trace norm [21, 22] is defined as the infimum over K tensors as follows:",
      "startOffset" : 22,
      "endOffset" : 30
    }, {
      "referenceID" : 20,
      "context" : "Table 1 summarizes the denoising performance in mean squared error analyzed in Tomioka and Suzuki [21] for the above two norms.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "The proof is a standard one following the line of [5] and it is presented in Appendix A.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "[23] obtained a bound that depends on ( ∑3 k=1 √ Dk/3) 2 instead of min(Dk logDk).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "We almost recover the settings for matrix completion [11] and multitask learning (MTL) [16].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "We almost recover the settings for matrix completion [11] and multitask learning (MTL) [16].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : "We can see that the overlapped trace norm and the scaled latent trace norm recover the known result for matrix completion [11].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "We also assume that all the pairs (p, q) are observed (|S| = PQ) as in [16].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "Our bound for the overlapped trace norm is almost as good as the one in [16] but has an multiplicative log(PQ) factor (as oppose to their additive log(PQ) term) and ∥r∥1/2 ≥ r.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 15,
      "context" : "Also note that the results in [16] can be applied when d is much larger than P and Q.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "Turning back to our bounds, both the latent trace norm and its scaled version can perform as well as knowing the mode with the lowest rank (the first mode) (see also [21]) when d = P = Q.",
      "startOffset" : 166,
      "endOffset" : 170
    }, {
      "referenceID" : 16,
      "context" : "In multilinear multitask learning (MLMTL) [17], any mode could possibly be low rank but it is a priori unknown.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : "[22, 23] studied tensor completion algorithms, their analysis assumed that the inputs xipq are drawn from a Gaussian distribution, which does not hold for tensor completion.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "[22, 23] studied tensor completion algorithms, their analysis assumed that the inputs xipq are drawn from a Gaussian distribution, which does not hold for tensor completion.",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "The sample complexity for the overlapped trace norm is the same as the one in [23] with a logarithmic factor.",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : "3, we discuss experiments on two real world data sets, namely the Restaurant data set [26] and School Effectiveness data set [3, 4].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : "3, we discuss experiments on two real world data sets, namely the Restaurant data set [26] and School Effectiveness data set [3, 4].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 3,
      "context" : "3, we discuss experiments on two real world data sets, namely the Restaurant data set [26] and School Effectiveness data set [3, 4].",
      "startOffset" : 125,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "The conventional convex multitask learning [2, 3, 16] corresponds to one of these approaches that penalizes the trace norm of the first unfolding ∥W (1)∥tr.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "The conventional convex multitask learning [2, 3, 16] corresponds to one of these approaches that penalizes the trace norm of the first unfolding ∥W (1)∥tr.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 15,
      "context" : "The conventional convex multitask learning [2, 3, 16] corresponds to one of these approaches that penalizes the trace norm of the first unfolding ∥W (1)∥tr.",
      "startOffset" : 43,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : "The convex MLMTL in [17] corresponds to the overlapped trace norm.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 16,
      "context" : "Following the same approach as in [17] we modelled the problem as a MLMTL problem with d = 45 features, P = 3 aspects, and Q = 138 customers.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 16,
      "context" : "The convex MLMTL in [17] corresponds to the overlapped trace norm.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "We followed [4] for the preprocessing of categorical attributes and obtained 24 features.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "[3] modeled this data set as a 27× 139 matrix-based MTL problem in which the year was modeled as a",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "Following earlier papers, [3, 4], we used the percentage of explained variance, defined as 100 · (1− (test MSE)/(variance of y)), as the evaluation metric.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "Following earlier papers, [3, 4], we used the percentage of explained variance, defined as 100 · (1− (test MSE)/(variance of y)), as the evaluation metric.",
      "startOffset" : 26,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "The percentage of the explained variance of the latent trace norm exceeds 30 % around sample size 4000 (around 30 samples per school), which is higher than the Hierarchical Bayes [4] (around 29.",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 2,
      "context" : "5 %) and matrix-based MTL [3] (around 26.",
      "startOffset" : 26,
      "endOffset" : 29
    } ],
    "year" : 2014,
    "abstractText" : "We study a multitask learning problem in which each task is parametrized by a weight vector and indexed by a pair of indices, which can be e.g, (consumer, time). The weight vectors can be collected into a tensor and the (multilinear-)rank of the tensor controls the amount of sharing of information among tasks. Two types of convex relaxations have recently been proposed for the tensor multilinear rank. However, we argue that both of them are not optimal in the context of multitask learning in which the dimensions or multilinear rank are typically heterogeneous. We propose a new norm, which we call the scaled latent trace norm and analyze the excess risk of all the three norms. The results apply to various settings including matrix and tensor completion, multitask learning, and multilinear multitask learning. Both the theory and experiments support the advantage of the new norm when the tensor is not equal-sized and we do not a priori know which mode is low rank.",
    "creator" : null
  }
}