{
  "name" : "aa2a77371374094fe9e0bc1de3f94ed9.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Multivariate Spatio-temporal Analysis via Low Rank Tensor Learning",
    "authors" : [ "Mohammad Taha Bahadori", "Yan Liu" ],
    "emails" : [ "mohammab@usc.edu", "qiyu@usc.edu", "yanliu.cs@usc.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Spatio-temporal data provide unique information regarding “where” and “when”, which is essential to answer many important questions in scientific studies from geology, climatology to sociology. In the context of big data, we are confronted with a series of new challenges when analyzing spatiotemporal data because of the complex spatial and temporal dependencies involved.\nA plethora of excellent work has been conducted to address the challenge and achieved successes to a certain extent [8, 13]. Often times, geostatistical models use cross variogram and cross covariance functions to describe the intrinsic dependency structure. However, the parametric form of cross variogram and cross covariance functions impose strong assumptions on the spatial and temporal correlation, which requires domain knowledge and manual work. Furthermore, parameter learning of those statistical models is computationally expensive, making them infeasible for large-scale applications.\nCokriging and forecasting are two central tasks in multivariate spatio-temporal analysis. Cokriging utilizes the spatial correlations to predict the value of the variables for new locations. One widely adopted method is multitask Gaussian process (MTGP) [4], which assumes a Gaussian process prior over latent functions to directly induce correlations between tasks. However, for a cokriging task with M variables of P locations for T time stamps, the time complexity of MTGP is O(M3P 3T ) [4]. For forecasting, popular methods in multivariate time series analysis include vector autoregressive (VAR) models, autoregressive integrated moving average (ARIMA) models, and cointegration models. An alternative method for spatio-temporal analysis is Bayesian hierarchical spatio-temporal models with either separable and non-separable space-time covariance functions [6]. Rank reduced\n∗Authors have equal contributions.\nmodels have been proposed to capture the inter-dependency among variables [1]. However, very few models can directly handle the correlations among variables, space and time simultaneously in a scalable way. In this paper, we aim to address this problem by presenting a unified framework for many spatio-temporal analysis tasks that are scalable for large-scale applications.\nTensor representation provides a convenient way to capture inter-dependencies along multiple dimensions. Therefore it is natural to represent the multivariate spatio-temporal data in tensor. Recent advances in low rank learning have led to simple models that can capture the commonalities among each mode of the tensor [15, 20]. Similar argument can be found in the literature of spatial data recovery [11], neuroimaging analysis [26], and multi-task learning [20]. Our work builds upon recent advances in low rank tensor learning [15, 11, 26] and further considers the scenario where additional side information of data is available. For example, in geo-spatial applications, apart from measurements of multiple variables, geographical information is available to infer location adjacency; in social network applications, friendship network structure is collected to obtain preference similarity. To utilize the side information, we can construct a Laplacian regularizer from the similarity matrices, which favors locally smooth solutions.\nWe develop a fast greedy algorithm for learning low rank tensors based on the greedy structure learning framework [2, 24, 21]. Greedy low rank tensor learning is efficient, as it does not require full singular value decomposition of large matrices as opposed to other alternating direction methods [11]. We also provide a bound on the difference between the loss function at our greedy solution and the one at the globally optimal solution. Finally, we present experiment results on simulation datasets as well as application datasets in climate and social network analysis, which show that our algorithm is faster and achieves higher prediction accuracy than state-of-art approaches in cokriging and forecasting tasks."
    }, {
      "heading" : "2 Tensor formulation for multivariate spatio-temporal analysis",
      "text" : "The critical element in multivariate spatio-temporal analysis is an efficient way to incorporate the spatial temporal correlations into modeling and automatically capture the shared structures across variables, locations, and time. In this section, we present a unified low rank tensor learning framework that can perform various types of spatio-temporal analysis. We will use two important applications, i.e., cokriging and forecasting, to motivate and describe the framework."
    }, {
      "heading" : "2.1 Cokriging",
      "text" : "In geostatistics, cokriging is the task of interpolating the data of one variable for unknown locations by taking advantage of the observations of variables from known locations. For example, by making use of the correlations between precipitation and temperature, we can obtain more precise estimate of temperature in unknown locations than univariate kriging. Formally, denote the complete data for P locations over T time stamps with M variables as X ∈ RP×T×M . We only observe the measurements for a subset of locations Ω ⊂ {1, . . . , P} and their side information such as longitude and latitude. Given the measurements XΩ and the side information, the goal is to estimate a tensor W ∈ RP×T×M that satisfies WΩ = XΩ. Here XΩ represents the outcome of applying the index operator IΩ to X:,:,m for all variables m = 1, . . . ,M . The index operator IΩ is a diagonal matrix whose entries are one for the locations included in Ω and zero otherwise.\nTwo key consistency principles have been identified for effective cokriging [9, Chapter 6.2]: (1) Global consistency: the data on the same structure are likely to be similar. (2) Local consistency: the data in close locations are likely to be similar. The former principle is akin to the cluster assumption in semi-supervised learning [25]. We incorporate these principles in a concise and computationally efficient low-rank tensor learning framework.\nTo achieve global consistency, we constrain the tensorW to be low rank. The low rank assumption is based on the belief that high correlations exist within variables, locations and time, which leads to natural clustering of the data. Existing literature have explored the low rank structure among these three dimensions separately, e.g., multi-task learning [19] for variable correlation, fixed rank kriging [7] for spatial correlations. Low rankness assumes that the observed data can be described with a few latent factors. It enforces the commonalities along three dimensions without an explicit form for the shared structures in each dimension.\nFor local consistency, we construct a regularizer via the spatial Laplacian matrix. The Laplacian matrix is defined as L = D − A, where A is a kernel matrix constructed by pairwise similarity and diagonal matrix Di,i = ∑ j(Ai,j). Similar ideas have been explored in matrix completion [16]. In cokriging literature, the local consistency is enforced via the spatial covariance matrix. The Bayesian models often impose the Gaussian process prior on the observations with the covariance matrix K = Kv ⊗Kx where Kv is the covariance between variables and Kx is that for locations. The Laplacian regularization term corresponds to the relational Gaussian process [5] where the covariance matrix is approximated by the spatial Laplacian.\nIn summary, we can perform cokriging and find the value of tensor W by solving the following optimization problem:\nŴ = argmin W\n{ ‖WΩ −XΩ‖2F + µ M∑ m=1 tr(W>:,:,mLW:,:,m) } s.t. rank(W) ≤ ρ, (1)\nwhere the Frobenius norm of a tensor A is defined as ‖A‖F = √∑\ni,j,kA2i,j,k and µ, ρ > 0 are the parameters that make tradeoff between the local and global consistency, respectively. The low rank constraint finds the principal components of the tensor and reduces the complexity of the model while the Laplacian regularizer clusters the data using the relational information among the locations. By learning the right tradeoff between these two techniques, our method is able to benefit from both. Due to the various definitions of tensor rank, we use rank as supposition for rank complexity, which will be specified in later section."
    }, {
      "heading" : "2.2 Forecasting",
      "text" : "Forecasting estimates the future value of multivariate time series given historical observations. For ease of presentation, we use the classical VAR model with K lags and coefficient tensor W ∈ RP×KP×M as an example. Using the matrix representation, the VAR(K) process defines the following data generation process:\nX:,t,m =W:,:,mXt,m + E:,t,m, for m = 1, . . . ,M and t = K + 1, . . . , T, (2) where Xt,m = [X>:,t−1,m, . . . ,X>:,t−K,m]> denotes the concatenation ofK-lag historical data before time t. The noise tensor E is a multivariate Gaussian with zero mean and unit variance . Existing multivariate regression methods designed to capture the complex correlations, such as Tucker decomposition [20], are computationally expensive. A scalable solution requires a simpler model that also efficiently accounts for the shared structures in variables, space, and time. Similar global and local consistency principles still hold in forecasting. For global consistency, we can use low rank constraint to capture the commonalities of the variables as well as the spatial correlations on the model parameter tensor, as in [8]. For local consistency, we enforce the predicted value for close locations to be similar via spatial Laplacian regularization. Thus, we can formulate the forecasting task as the following optimization problem over the model coefficient tensorW:"
    }, {
      "heading" : "Ŵ = argmin",
      "text" : "W\n{ ‖X̂ − X‖2F + µ M∑ m=1 tr(X̂>:,:,mLX̂:,:,m) } s.t. rank(W) ≤ ρ, X̂:,t,m =W:,:,mXt,m\n(3)\nThough cokriging and forecasting are two different tasks, we can easily see that both formulations follow the global and local consistency principles and can capture the inter-correlations from spatialtemporal data."
    }, {
      "heading" : "2.3 Unified Framework",
      "text" : "We now show that both cokriging and forecasting can be formulated into the same tensor learning framework. Let us rewrite the loss function in Eq. (1) and Eq. (3) in the form of multitask regression and complete the quadratic form for the loss function. The cokriging task can be reformulated as follows:\nŴ = argmin W { M∑ m=1 ‖W:,:,mH − (H>)−1XΩ,m‖2F } s.t. rank(W) ≤ ρ (4)\nwhere we define HH> = IΩ + µL.1 For the forecasting problem, HH> = IP + µL and we have:\nŴ = argmin W { M∑ m=1 T∑ t=K+1 ‖HW:,:,mXt,m − (H−1)X:,t,m‖2F } s.t. rank(W) ≤ ρ, (5)\nBy slight change of notation (cf. Appendix D), we can easily see that the optimal solution of both problems can be obtained by the following optimization problem with appropriate choice of tensors Y and V:\nŴ = argmin W { M∑ m=1 ‖W:,:,mY:,:,m − V:,:,m‖2F } s.t. rank(W) ≤ ρ. (6)\nAfter unifying the objective function, we note that tensor rank has different notions such as CP rank, Tucker rank and mode n-rank [15, 11]. In this paper, we choose the mode-n rank, which is computationally more tractable [11, 23]. The mode-n rank of a tensorW is the rank of its mode-n unfoldingW(n).2 In particular, for a tensorW with N mode, we have the following definition:\nmode-n rank(W) = N∑ n=1 rank(W(n)). (7)\nA common practice to solve this formulation with mode n-rank constraint is to relax the rank constraint to a convex nuclear norm constraint [11, 23]. However, those methods are computationally expensive since they need full singular value decomposition of large matrices. In the next section, we present a fast greedy algorithm to tackle the problem."
    }, {
      "heading" : "3 Fast greedy low rank tensor learning",
      "text" : "To solve the non-convex problem in Eq. (6) and find its optimal solution, we propose a greedy learning algorithm by successively adding rank-1 estimation of the mode-n unfolding. The main idea of the algorithm is to unfold the tensor into a matrix, seek for its rank-1 approximation and then fold back into a tensor with same dimensionality. We describe this algorithm in three steps: (i) First, we show that we can learn rank-1 matrix estimations efficiently by solving a generalized eigenvalue problem, (ii) We use the rank-1 matrix estimation to greedily solve the original tensor rank constrained problem, and (iii) We propose an enhancement via orthogonal projections after each greedy step.\nOptimal rank-1 Matrix Learning The following lemma enables us to find such optimal rank-1 estimation of the matrices. Lemma 1. Consider the following rank constrained problem:\nÂ1 = argmin A:rank(A)=1\n{ ‖Y −AX‖2F } , (8)\nwhere Y ∈ Rq×n, X ∈ Rp×n, and A ∈ Rq×p. The optimal solution of Â1 can be written as Â1 = ûv̂\n>, ‖v̂‖2 = 1 where v̂ is the dominant eigenvector of the following generalized eigenvalue problem: (XY >Y X>)v = λ(XX>)v (9) and û can be computed as\nû = 1\nv̂>XX>v̂ Y X>v̂. (10)\nProof is deferred to Appendix A. Eq. (9) is a generalized eigenvalue problem whose dominant eigenvector can be found efficiently [12]. If XX> is full rank, as assumed in Theorem 2, the problem is simplified to a regular eigenvalue problem whose dominant eigenvector can be efficiently computed.\n1We can use Cholesky decomposition to obtain H . In the rare cases that IΩ + µL is not full rank, IP is added where is a very small positive value.\n2The mode-n unfolding of a tensor is the matrix resulting from treating n as the first mode of the matrix, and cyclically concatenating other modes. Tensor refolding is the reverse direction operation [15].\nAlgorithm 1 Greedy Low-rank Tensor Learning 1: Input: transformed data Y,V of M variables, stopping criteria η 2: Output: N mode tensorW 3: InitializeW ← 0 4: repeat 5: for n = 1 to N do 6: Bn ← argmin\nB: rank(B)=1\nL(refold(W(n) +B);Y,V)\n7: ∆n ← L(W;Y,V)− L(refold(W(n) +Bn);Y,V) 8: end for 9: n∗ ← argmax\nn {∆n}\n10: if ∆n∗ > η then 11: W ←W + refold(Bn∗ , n∗) 12: end if 13: W ← argminrow(A(1))⊆row(W(1))\ncol(A(1))⊆col(W(1)) L(A;Y,V) # Optional Orthogonal Projection Step.\n14: until ∆n∗ < η\nGreedy Low n-rank Tensor Learning The optimal rank-1 matrix learning serves as a basic element in our greedy algorithm. Using Lemma 1, we can solve the problem in Eq. (6) in the Forward Greedy Selection framework as follows: at each iteration of the greedy algorithm, it searches for the mode that gives the largest decrease in the objective function. It does so by unfolding the tensor in that mode and finding the best rank-1 estimation of the unfolded tensor. After finding the optimal mode, it adds the rank-1 estimate in that mode to the current estimation of the tensor. Algorithm 1 shows the details of this approach, where L(W;Y,V) = ∑M m=1 ‖W:,:,mY:,:,m − V:,:,m‖2F . Note that we can find the optimal rank-1 solution in only one of the modes, but it is enough to guarantee the convergence of our greedy algorithm.\nTheorem 2 bounds the difference between the loss function evaluated at each iteration of the greedy algorithm and the one at the globally optimal solution.\nTheorem 2. Suppose in Eq. (6) the matrices Y>:,:,mY:,:,m for m = 1, . . . ,M are positive definite. The solution of Algo. 1 at its kth iteration step satisfies the following inequality:\nL(Wk;Y,V)− L(W∗;Y,V) ≤ (‖Y‖2‖W∗(1)‖∗) 2\n(k + 1) , (11)\nwhereW∗ is the global minimizer of the problem in Eq. (6) and ‖Y‖2 is the largest singular value of a block diagonal matrix created by placing the matrices Y(:, :,m) on its diagonal blocks.\nThe detailed proof is given in Appendix B. The key idea of the proof is that the amount of decrease in the loss function by each step in the selected mode is not smaller than the amount of decrease if we had selected the first mode. The theorem shows that we can obtain the same rate of convergence for learning low rank tensors as achieved in [22] for learning low rank matrices. The greedy algorithm in Algorithm 1 is also connected to mixture regularization in [23]: the mixture approach decomposes the solution into a set of low rank structures while the greedy algorithm successively learns a set of rank one components.\nGreedy Algorithm with Orthogonal Projections It is well-known that the forward greedy algorithm may make steps in sub-optimal directions because of noise. A common solution to alleviate the effect of noise is to make orthogonal projections after each greedy step [2, 21]. Thus, we enhance the forward greedy algorithm by projecting the solution into the space spanned by the singular vectors of its mode-1 unfolding. The greedy algorithm with orthogonal projections performs an extra step in line 13 of Algorithm 1: It finds the top k singular vectors of the solution: [U, S, V ]← svd(W(1), k) where k is the iteration number. Then it finds the best solution in the space spanned by U and V by solving Ŝ ← minS L(USV >,Y,V) which has a closed form solution. Finally, it reconstructs the solution: W ← refold(UŜV >, 1). Note that the projection only needs to find top k singular vectors which can be computed efficiently for small values of k."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate the efficacy of our algorithms on synthetic datasets and real-world application datasets."
    }, {
      "heading" : "4.1 Low rank tensor learning on synthetic data",
      "text" : "For empirical evaluation, we compare our method with multitask learning (MTL) algorithms, which also utilize the commonalities between different prediction tasks for better performance. We use the following baselines: (1) Trace norm regularized MTL (Trace), which seeks the low rank structure only on the task dimension; (2) Multilinear MTL [20], which adapts the convex relaxation of low rank tensor learning solved with Alternating Direction Methods of Multiplier (ADMM) [10] and Tucker decomposition to describe the low rankness in multiple dimensions; (3) MTL-L1 , MTL-L21 [19], and MTL-LDirty [14], which investigate joint sparsity of the tasks with Lp norm regularization. For MTL-L1 , MTL-L21 [19] and MTL-LDirty, we use MALSAR Version 1.1 [27].\nWe construct a model coefficient tensor W of size 20 × 20 × 10 with CP rank equals to 1. Then, we generate the observations Y and V according to multivariate regression model V:,:,m = W:,:,mY:,:,m+E:,:,m form = 1, . . . ,M , where E is tensor with zero mean Gaussian noise elements. We split the synthesized data into training and testing time series and vary the length of the training time series from 10 to 200. For each training length setting, we repeat the experiments for 10 times and select the model parameters via 5-fold cross validation. We measure the prediction performance via two criteria: parameter estimation accuracy and rank complexity. For accuracy, we calculate the RMSE of the estimation versus the true model coefficient tensor. For rank complexity, we calculate the mixture rank complexity [23] as MRC = 1n ∑N n=1 rank(W(n)).\nThe results are shown in Figure 1(a) and 1(b). We omit the Tucker decomposition as the results are not comparable. We can clearly see that the proposed greedy algorithm with orthogonal projections achieves the most accurate tensor estimation. In terms of rank complexity, we make two observations: (i) Given that the tensor CP rank is 1, greedy algorithm with orthogonal projections produces the estimate with the lowest rank complexity. This can be attributed to the fact that the orthogonal projections eliminate the redundant rank-1 components that fall in the same spanned space. (ii) The rank complexity of the forward greedy algorithm increases as we enlarge the sample size. We believe that when there is a limited number of observations, most of the new rank-1 elements added to the estimate are not accurate and the cross-validation steps prevent them from being added to the model. However, as the sample size grows, the rank-1 estimates become more accurate and they are preserved during the cross-validation.\nTo showcase the scalability of our algorithm, we vary the number of variables and generate a series of tensorW ∈ R20×20×M for M from 10 to 100 and record the running time (in seconds) for three tensor learning algorithms, i.e, forward greedy, greedy with orthogonal projections and ADMM. We measure the run time on a machine with a 6-core 12-thread Intel Xenon 2.67GHz processor and 12GB memory. The results are shown in Figure 1(c). The running time of ADMM increase rapidly with the data size while the greedy algorithm stays steady, which confirms the speedup advantage of the greedy algorithm."
    }, {
      "heading" : "4.2 Spatio-temporal analysis on real world data",
      "text" : "We conduct cokriging and forecasting experiments on four real-world datasets:\nUSHCN The U.S. Historical Climatology Network Monthly (USHCN)3 dataset consists of monthly climatological data of 108 stations spanning from year 1915 to 2000. It has three climate variables: (1) daily maximum, (2) minimum temperature averaged over month, and (3) total monthly precipitation.\nCCDS The Comprehensive Climate Dataset (CCDS)4 is a collection of climate records of North America from [18]. The dataset was collected and pre-processed by five federal agencies. It contains monthly observations of 17 variables such as Carbon dioxide and temperature spanning from 1990 to 2001. The observations were interpolated on a 2.5×2.5 degree grid, with 125 observation locations.\nYelp The Yelp dataset5 contains the user rating records for 22 categories of businesses on Yelp over ten years. The processed dataset includes the rating values (1-5) binned into 500 time intervals and the corresponding social graph for 137 active users. The dataset is used for the spatio-temporal recommendation task to predict the missing user ratings across all business categories.\nFoursquare The Foursquare dataset [17] contains the users’ check-in records in Pittsburgh area from Feb 24 to May 23, 2012, categorized by different venue types such as Art & Entertainment, College & University, and Food. The dataset records the number of check-ins by 121 users in each of the 15 category of venues over 1200 time intervals, as well as their friendship network."
    }, {
      "heading" : "4.2.1 Cokriging",
      "text" : "We compare the cokriging performance of our proposed method with the classical cokriging approaches including simple kriging and ordinary cokriging with nonbias condition [13] which are applied to each variables separately. We further compare with multitask Gaussian process (MTGP) [4] which also considers the correlation among variables. We also adapt ADMM for solving the nuclear norm relaxed formulation of the cokriging formulation as a baseline (see Appendix C for more details). For USHCN and CCDS, we construct a Laplacian matrix by calculating the pairwise Haversine distance of locations. For Foursquare and Yelp, we construct the graph Laplacian from the user friendship network.\nFor each dataset, we first normalize it by removing the trend and diving by the standard deviation. Then we randomly pick 10% of locations (or users for Foursquare) and eliminate the measurements of all variables over the whole time span. Then, we produce the estimates for all variables of each timestamp. We repeat the procedure for 10 times and report the average prediction RMSE for all timestamps and 10 random sets of missing locations. We use the MATLAB Kriging Toolbox6 for the classical cokriging algorithms and the MTGP code provided by [4].\nTable 1 shows the results for the cokriging task. The greedy algorithm with orthogonal projections is significantly more accurate in all three datasets. The baseline cokriging methods can only handle the two dimensional longitude and latitude information, thus are not applicable to the Foursquare and Yelp dataset with additional friendship information. The superior performance of the greedy algorithm can be attributed to two of its properties: (1) It can obtain low rank models and achieve global consistency; (2) It usually has lower estimation bias compared to nuclear norm relaxed methods.\n3http://www.ncdc.noaa.gov/oa/climate/research/ushcn 4http://www-bcf.usc.edu/˜liu32/data/NA-1990-2002-Monthly.csv 5http://www.yelp.com/dataset_challenge 6http://globec.whoi.edu/software/kriging/V3/english.html"
    }, {
      "heading" : "4.2.2 Forecasting",
      "text" : "We present the empirical evaluation on the forecasting task by comparing with multitask regression algorithms. We split the data along the temporal dimension into 90% training set and 10% testing set. We choose VAR(3) model and during the training phase, we use 5-fold cross-validation.\nAs shown in Table 2, the greedy algorithm with orthogonal projections again achieves the best prediction accuracy. Different from the cokriging task, forecasting does not necessarily need the correlations of locations for prediction. One might raise the question as to whether the Laplacian regularizer helps. Therefore, we report the results for our formulation without Laplacian (ORTHONL) for comparison. For efficiency, we report the running time (in seconds) in Table 3 for both tasks of cokriging and forecasting. Compared with ADMM, which is a competitive baseline also capturing the commonalities among variables, space, and time, our greedy algorithm is much faster for most datasets.\nAs a qualitative study, we plot the map of most predictive regions analyzed by the greedy algorithm using CCDS dataset in Fig. 2. Based on the concept of how informative the past values of the climate measurements in a specific location are in predicting future values of other time series, we define the aggregate strength of predictiveness of each region as w(t) = ∑P p=1 ∑M m=1 |Wp,t,m|. We can see that two regions are identified as the most predictive regions: (1) The southwest region, which reflects the impact of the Pacific ocean and (2) The southeast region, which frequently experiences relative sea level rise, hurricanes, and storm surge in Gulf of Mexico. Another interesting region lies in the center of Colorado, where the Rocky mountain valleys act as a funnel for the winds from the west, providing locally divergent wind patterns."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we study the problem of multivariate spatio-temporal data analysis with an emphasis on two tasks: cokriging and forecasting. We formulate the problem into a general low rank tensor learning framework which captures both the global consistency and the local consistency principle. We develop a fast and accurate greedy solver with theoretical guarantees for its convergence. We validate the correctness and efficiency of our proposed method on both the synthetic dataset and realapplication datasets. For future work, we are interested in investigating different forms of shared structure and extending the framework to capture non-linear correlations in the data."
    }, {
      "heading" : "Acknowledgment",
      "text" : "We thank the anonymous reviewers for their helpful feedback and comments. The research was sponsored by the NSF research grants IIS-1134990, IIS- 1254206 and Okawa Foundation Research Award. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government."
    } ],
    "references" : [ {
      "title" : "Estimating linear restrictions on regression coefficients for multivariate normal distributions",
      "author" : [ "T. Anderson" ],
      "venue" : "The Annals of Mathematical Statistics, pages 327–351",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1951
    }, {
      "title" : "Approximation and learning by greedy algorithms",
      "author" : [ "A. Barron", "A. Cohen", "W. Dahmen", "R. DeVore" ],
      "venue" : "The Annals of Statistics",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Parallel and Distributed Computation: Numerical Methods",
      "author" : [ "D. Bertsekas", "J. Tsitsiklis" ],
      "venue" : "Prentice Hall Inc",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Multi-task Gaussian Process Prediction",
      "author" : [ "E. Bonilla", "K. Chai", "C. Williams" ],
      "venue" : "NIPS",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Relational learning with Gaussian processes",
      "author" : [ "W. Chu", "V. Sindhwani", "Z. Ghahramani", "S. Keerthi" ],
      "venue" : "NIPS",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Classes of nonseparable",
      "author" : [ "N. Cressie", "H. Huang" ],
      "venue" : "spatio-temporal stationary covariance functions. JASA",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Fixed rank kriging for very large spatial data sets",
      "author" : [ "N. Cressie", "G. Johannesson" ],
      "venue" : "JRSS B (Statistical Methodology), 70(1):209–226",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Fixed rank filtering for spatio-temporal data",
      "author" : [ "N. Cressie", "T. Shi", "E. Kang" ],
      "venue" : "J. Comp. Graph. Stat.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Statistics for spatio-temporal data",
      "author" : [ "N. Cressie", "C. Wikle" ],
      "venue" : "John Wiley & Sons",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A dual algorithm for the solution of nonlinear variational problems via finite element approximation",
      "author" : [ "D. Gabay", "B. Mercier" ],
      "venue" : "Computers & Mathematics with Applications, 2(1):17–40",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "Tensor completion and low-n-rank tensor recovery via convex optimization",
      "author" : [ "S. Gandy", "B. Recht", "I. Yamada" ],
      "venue" : "Inverse Problems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "H",
      "author" : [ "M. Gu", "A. Ruhe", "G. Sleijpen" ],
      "venue" : "van der Vorst, Z. Bai, and R. Li. 5. Generalized Hermitian Eigenvalue Problems. Society for Industrial and Applied Mathematics",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Applied geostatistics",
      "author" : [ "E. Isaaks", "R. Srivastava" ],
      "venue" : "London: Oxford University",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A dirty model for multi-task learning",
      "author" : [ "A. Jalali", "S. Sanghavi", "C. Ruan", "P. Ravikumar" ],
      "venue" : "NIPS",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "T. Kolda", "B. Bader" ],
      "venue" : "SIAM review",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Relation regularized matrix factorization",
      "author" : [ "W.-J. Li", "D.-Y. Yeung" ],
      "venue" : "IJCAI",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Exploring trajectory-driven local geographic topics in foursquare",
      "author" : [ "X. Long", "L. Jin", "J. Joshi" ],
      "venue" : "UbiComp",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Spatialtemporal causal modeling for climate change attribution",
      "author" : [ "A. Lozano", "H. Li", "A. Niculescu-Mizil", "Y. Liu", "C. Perlich", "J. Hosking", "N. Abe" ],
      "venue" : "KDD",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and C",
      "author" : [ "F. Nie", "H. Huang", "X. Cai" ],
      "venue" : "H. Ding. Efficient and robust feature selection via joint `2,1-norms minimization. In NIPS",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Multilinear multitask learning",
      "author" : [ "B. Romera-Paredes", "H. Aung", "N. Bianchi-Berthouze", "M. Pontil" ],
      "venue" : "ICML",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Large-scale convex minimization with a lowrank constraint",
      "author" : [ "S. Shalev-Shwartz", "A. Gonen", "O. Shamir" ],
      "venue" : "ICML",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Trading Accuracy for Sparsity in Optimization Problems with Sparsity Constraints",
      "author" : [ "S. Shalev-Shwartz", "N. Srebro", "T. Zhang" ],
      "venue" : "SIAM Journal on Optimization",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convex Tensor Decomposition via Structured Schatten Norm Regularization",
      "author" : [ "R. Tomioka", "K. Hayashi", "H. Kashima" ],
      "venue" : "NIPS",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representations",
      "author" : [ "T. Zhang" ],
      "venue" : "IEEE Trans Inf Theory, pages 4689–4708",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Learning with local and global consistency",
      "author" : [ "D. Zhou", "O. Bousquet", "T. Lal", "J. Weston", "B. Schölkopf" ],
      "venue" : "NIPS",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Tensor regression with applications in neuroimaging data analysis",
      "author" : [ "H. Zhou", "L. Li", "H. Zhu" ],
      "venue" : "JASA",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "MALSAR: Multi-tAsk Learning via StructurAl Regularization",
      "author" : [ "J. Zhou", "J. Chen", "J. Ye" ],
      "venue" : "http://www.public.asu.edu/ ̃jye02/Software/MALSAR/",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "A plethora of excellent work has been conducted to address the challenge and achieved successes to a certain extent [8, 13].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 12,
      "context" : "A plethora of excellent work has been conducted to address the challenge and achieved successes to a certain extent [8, 13].",
      "startOffset" : 116,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "One widely adopted method is multitask Gaussian process (MTGP) [4], which assumes a Gaussian process prior over latent functions to directly induce correlations between tasks.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "However, for a cokriging task with M variables of P locations for T time stamps, the time complexity of MTGP is O(M(3)P (3)T ) [4].",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 5,
      "context" : "An alternative method for spatio-temporal analysis is Bayesian hierarchical spatio-temporal models with either separable and non-separable space-time covariance functions [6].",
      "startOffset" : 171,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "models have been proposed to capture the inter-dependency among variables [1].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 14,
      "context" : "Recent advances in low rank learning have led to simple models that can capture the commonalities among each mode of the tensor [15, 20].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "Recent advances in low rank learning have led to simple models that can capture the commonalities among each mode of the tensor [15, 20].",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "Similar argument can be found in the literature of spatial data recovery [11], neuroimaging analysis [26], and multi-task learning [20].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "Similar argument can be found in the literature of spatial data recovery [11], neuroimaging analysis [26], and multi-task learning [20].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 19,
      "context" : "Similar argument can be found in the literature of spatial data recovery [11], neuroimaging analysis [26], and multi-task learning [20].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "Our work builds upon recent advances in low rank tensor learning [15, 11, 26] and further considers the scenario where additional side information of data is available.",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "Our work builds upon recent advances in low rank tensor learning [15, 11, 26] and further considers the scenario where additional side information of data is available.",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 25,
      "context" : "Our work builds upon recent advances in low rank tensor learning [15, 11, 26] and further considers the scenario where additional side information of data is available.",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "We develop a fast greedy algorithm for learning low rank tensors based on the greedy structure learning framework [2, 24, 21].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "We develop a fast greedy algorithm for learning low rank tensors based on the greedy structure learning framework [2, 24, 21].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "We develop a fast greedy algorithm for learning low rank tensors based on the greedy structure learning framework [2, 24, 21].",
      "startOffset" : 114,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : "Greedy low rank tensor learning is efficient, as it does not require full singular value decomposition of large matrices as opposed to other alternating direction methods [11].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 24,
      "context" : "The former principle is akin to the cluster assumption in semi-supervised learning [25].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : ", multi-task learning [19] for variable correlation, fixed rank kriging [7] for spatial correlations.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : ", multi-task learning [19] for variable correlation, fixed rank kriging [7] for spatial correlations.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 15,
      "context" : "Similar ideas have been explored in matrix completion [16].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "The Laplacian regularization term corresponds to the relational Gaussian process [5] where the covariance matrix is approximated by the spatial Laplacian.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "Existing multivariate regression methods designed to capture the complex correlations, such as Tucker decomposition [20], are computationally expensive.",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "For global consistency, we can use low rank constraint to capture the commonalities of the variables as well as the spatial correlations on the model parameter tensor, as in [8].",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 14,
      "context" : "After unifying the objective function, we note that tensor rank has different notions such as CP rank, Tucker rank and mode n-rank [15, 11].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "After unifying the objective function, we note that tensor rank has different notions such as CP rank, Tucker rank and mode n-rank [15, 11].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "In this paper, we choose the mode-n rank, which is computationally more tractable [11, 23].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 22,
      "context" : "In this paper, we choose the mode-n rank, which is computationally more tractable [11, 23].",
      "startOffset" : 82,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "A common practice to solve this formulation with mode n-rank constraint is to relax the rank constraint to a convex nuclear norm constraint [11, 23].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 22,
      "context" : "A common practice to solve this formulation with mode n-rank constraint is to relax the rank constraint to a convex nuclear norm constraint [11, 23].",
      "startOffset" : 140,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "(9) is a generalized eigenvalue problem whose dominant eigenvector can be found efficiently [12].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 14,
      "context" : "Tensor refolding is the reverse direction operation [15].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 21,
      "context" : "The theorem shows that we can obtain the same rate of convergence for learning low rank tensors as achieved in [22] for learning low rank matrices.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "The greedy algorithm in Algorithm 1 is also connected to mixture regularization in [23]: the mixture approach decomposes the solution into a set of low rank structures while the greedy algorithm successively learns a set of rank one components.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "A common solution to alleviate the effect of noise is to make orthogonal projections after each greedy step [2, 21].",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "A common solution to alleviate the effect of noise is to make orthogonal projections after each greedy step [2, 21].",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "We use the following baselines: (1) Trace norm regularized MTL (Trace), which seeks the low rank structure only on the task dimension; (2) Multilinear MTL [20], which adapts the convex relaxation of low rank tensor learning solved with Alternating Direction Methods of Multiplier (ADMM) [10] and Tucker decomposition to describe the low rankness in multiple dimensions; (3) MTL-L1 , MTL-L21 [19], and MTL-LDirty [14], which investigate joint sparsity of the tasks with Lp norm regularization.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 9,
      "context" : "We use the following baselines: (1) Trace norm regularized MTL (Trace), which seeks the low rank structure only on the task dimension; (2) Multilinear MTL [20], which adapts the convex relaxation of low rank tensor learning solved with Alternating Direction Methods of Multiplier (ADMM) [10] and Tucker decomposition to describe the low rankness in multiple dimensions; (3) MTL-L1 , MTL-L21 [19], and MTL-LDirty [14], which investigate joint sparsity of the tasks with Lp norm regularization.",
      "startOffset" : 287,
      "endOffset" : 291
    }, {
      "referenceID" : 18,
      "context" : "We use the following baselines: (1) Trace norm regularized MTL (Trace), which seeks the low rank structure only on the task dimension; (2) Multilinear MTL [20], which adapts the convex relaxation of low rank tensor learning solved with Alternating Direction Methods of Multiplier (ADMM) [10] and Tucker decomposition to describe the low rankness in multiple dimensions; (3) MTL-L1 , MTL-L21 [19], and MTL-LDirty [14], which investigate joint sparsity of the tasks with Lp norm regularization.",
      "startOffset" : 391,
      "endOffset" : 395
    }, {
      "referenceID" : 13,
      "context" : "We use the following baselines: (1) Trace norm regularized MTL (Trace), which seeks the low rank structure only on the task dimension; (2) Multilinear MTL [20], which adapts the convex relaxation of low rank tensor learning solved with Alternating Direction Methods of Multiplier (ADMM) [10] and Tucker decomposition to describe the low rankness in multiple dimensions; (3) MTL-L1 , MTL-L21 [19], and MTL-LDirty [14], which investigate joint sparsity of the tasks with Lp norm regularization.",
      "startOffset" : 412,
      "endOffset" : 416
    }, {
      "referenceID" : 18,
      "context" : "For MTL-L1 , MTL-L21 [19] and MTL-LDirty, we use MALSAR Version 1.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 22,
      "context" : "For rank complexity, we calculate the mixture rank complexity [23] as MRC = 1 n ∑N n=1 rank(W(n)).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : "CCDS The Comprehensive Climate Dataset (CCDS)4 is a collection of climate records of North America from [18].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 16,
      "context" : "Foursquare The Foursquare dataset [17] contains the users’ check-in records in Pittsburgh area from Feb 24 to May 23, 2012, categorized by different venue types such as Art & Entertainment, College & University, and Food.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 12,
      "context" : "We compare the cokriging performance of our proposed method with the classical cokriging approaches including simple kriging and ordinary cokriging with nonbias condition [13] which are applied to each variables separately.",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "We further compare with multitask Gaussian process (MTGP) [4] which also considers the correlation among variables.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "We use the MATLAB Kriging Toolbox6 for the classical cokriging algorithms and the MTGP code provided by [4].",
      "startOffset" : 104,
      "endOffset" : 107
    } ],
    "year" : 2014,
    "abstractText" : "Accurate and efficient analysis of multivariate spatio-temporal data is critical in climatology, geology, and sociology applications. Existing models usually assume simple inter-dependence among variables, space, and time, and are computationally expensive. We propose a unified low rank tensor learning framework for multivariate spatio-temporal analysis, which can conveniently incorporate different properties in spatio-temporal data, such as spatial clustering and shared structure among variables. We demonstrate how the general framework can be applied to cokriging and forecasting tasks, and develop an efficient greedy algorithm to solve the resulting optimization problem with convergence guarantee. We conduct experiments on both synthetic datasets and real application datasets to demonstrate that our method is not only significantly faster than existing methods but also achieves lower estimation error.",
    "creator" : "pdftk 1.44 - www.pdftk.com"
  }
}