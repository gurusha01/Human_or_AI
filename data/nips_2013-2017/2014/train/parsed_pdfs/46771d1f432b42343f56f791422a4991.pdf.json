{
  "name" : "46771d1f432b42343f56f791422a4991.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On Communication Cost of Distributed Statistical Estimation and Dimensionality",
    "authors" : [ "Ankit Garg", "Tengyu Ma", "Huy L. Nguy ̃ên" ],
    "emails" : [ "garg@cs.princeton.edu", "tengyu@cs.princeton.edu", "hlnguyen@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The last decade has witnessed a tremendous growth in the amount of data involved in machine learning tasks. In many cases, data volume has outgrown the capacity of memory of a single machine and it is increasingly common that learning tasks are performed in a distributed fashion on many machines. Communication has emerged as an important resource and sometimes the bottleneck of the whole system. A lot of recent works are devoted to understand how to solve problems distributedly with efficient communication [2, 3, 4, 1, 5].\nIn this paper, we study the relation between the dimensionality and the communication cost of statistical estimation problems. Most modern statistical problems are characterized by high dimensionality. Thus, it is natural to ask the following meta question:\nHow does the communication cost scale in the dimensionality?\nWe study this question via the problems of estimating parameters of distributions in the distributed setting. For these problems, we answer the question above by providing two complementary results:\n1. Lower bound for general case: If the distribution is a product distribution over the coordinates, then one essentially needs to estimate each dimension of the parameter individually and the information cost (a proxy for communication cost) scales linearly in the number of dimensions.\n2. Upper bound for sparse case: If the true parameter is promised to have low sparsity, then a very simple thresholding estimator gives better tradeoff between communication cost and mean-square loss.\nBefore getting into the ideas behind these results, we first define the problem more formally. We consider the case when there are m machines, each of which receives n i.i.d samples from an unknown distribution P (from a family P) over the d-dimensional Euclidean space Rd. These machines need to estimate a parameter ✓ of the distribution via communicating with each other. Each machine can do arbitrary computation on its samples and messages it receives from other machines. We regard communication (the number of bits communicated) as a resource, and therefore we not only want to optimize over the estimation error of the parameters but also the tradeoff between the estimation error and communication cost of the whole procedure. For simplicity, here we are typically interested in achieving the minimax error 1 while communicating as few bits as possible. Our main focus is the high dimensional setting where d is very large.\nCommunication Lower Bound via Direct-Sum Theorem The key idea for the lower bound is, when the unknown distribution P = P\n1 ⇥ · · · ⇥ P d is a product distribution over Rd, and each coordinate of the parameter ✓ only depends on the corresponding component of P , then we can view the d-dimensional problem as d independent copies of one dimensional problem. We show that, one unfortunately cannot do anything beyond this trivial decomposition, that is, treating each dimension independently, and solving d different estimations problems individually. In other words, the communication cost 2 must be at least d times the cost for one dimensional problem. We call this theorem “direct-sum” theorem.\nTo demonstrate our theorem, we focus on the specific case where P is a d dimensional spherical Gaussian distribution with an unknown mean and covariance 2I\nd 3 . The problem is to estimate the mean of P . The work [1] showed a lower bound on the communication cost for this problem when d = 1. Our technique when applied to their theorem immediately yields a lower bound equal to d times the lower bound for the one dimension problem for any choice of d. Note that [5] independently achieve the same bound by refining the proof in [1].\nIn the simultaneous communication setting, where all machines send one message to one machine and this machine needs to figure out the estimation, the work [1] showed that ⌦(md/ logm) bits of communication are needed to achieve the minimax squared loss. In this paper, we improve this bound to ⌦(md), by providing an improved lower bound for one-dimensional setting and then applying our direct-sum theorem.\nThe direct-sum theorem that we prove heavily uses the idea and tools from the recent developments in communication complexity and information complexity. There has been a lot of work on the paradigm of studying communication complexity via the notion of information complexity [6, 7, 8, 9, 10]. Information complexity can be thought of as a proxy for communication complexity that is especially accurate for solving multiple copies of the same problem simultaneously [8]. Proving socalled “direct-sum” results has become a standard tool, namely the fact that the amount of resources required for solving d copies of a problem (with different inputs) in parallel is equal to d times the amount required for one copy. In other words, there is no saving from solving many copies of the same problem in batch and the trivial solution of solving each of them separately is optimal. Note that this generic statement is certainly NOT true for arbitrary type of tasks and arbitrary type of resources. Actually even for distributed computing tasks, if the measure of resources is the\n1by minimax error we mean the minimum possible error that can be achieved when there is no limit on the communication\n2technically, information cost, as discussed below 3where Id denote the d⇥ d identity matrix\ncommunication cost instead of information cost, there exist examples where solving d copies of a certain problem requires less communication than d times the communication required for one copy [11]. Therefore, a direct-sum theorem, if true, could indeed capture the features and difficulties of the problems.\nOur result can be viewed as a direct sum theorem for communication complexity for statistical estimation problems: the amount of communication needed for solving an estimation problem in d dimensions is at least d times the amount of information needed for the same problem in one dimension. The proof technique is directly inspired by the notion of conditional information complexity [7], which was used to prove direct sum theorems and lower bounds for streaming algorithms. We believe this is a fruitful connection and can lead to more lower bounds in statistical machine learning.\nTo complement the above lower bounds, we also show an interactive protocol that uses a log factor less communication than the simple protocol, under which each machine sends the sample mean and the center takes the average as the estimation. Our protocol demonstrates additional power of interactive communication and potential complexity of proving lower bound for interactive protocols.\nThresholding Algorithm for Sparse Parameter Estimation In light of the strong lower bounds in the general case, a question suggests itself as a way to get around the impossibility results:\nCan we do better when the data (parameters) have more structure?\nWe study this questions by considering the sparsity structure on the parameter ✓. Specifically, we consider the case when the underlying parameter ✓ is promised to be s-sparse. We provide a simple protocol that achieves the same squared-loss O(d 2/(mn)) as in the general case, while using ˜O(sm) communications, or achieving optimal squared loss O(s 2/(mn)), with communication ˜O(dm), or any tradeoff between these cases. We even conjecture that this is the best tradeoff up to polylogarithmic factors."
    }, {
      "heading" : "2 Problem Setup, Notations and Preliminaries",
      "text" : "Classical Statistical Parameter Estimation We start by reviewing the classical framework of statistical parameter estimation problems. Let P be a family of distributions over X . Let ✓ : P ! ⇥ ⇢ R denote a function defined on P . We are given samples X1, . . . , Xn from some P 2 P , and are asked to estimate ✓(P ). Let ˆ✓ : Xn ! ⇥ be such an estimator, and ˆ✓(X1, . . . , Xn) is the corresponding estimate.\nDefine the squared loss R of the estimator to be\nR(ˆ✓, ✓) = Ê ✓,X\nh\nkˆ✓(X1, . . . , Xn) ✓(P )k2 2\ni\nIn the high-dimensional case, let Pd := {~P = P 1 ⇥ · · · ⇥ P d : P i 2 P} be the family of product distributions over X d. Let ~✓ : Pd ! ⇥d ⇢ Rd be the d-dimensional function obtained by applying ✓ point-wise ~✓ (P\n1 ⇥ · · ·⇥ P d ) = (✓(P 1 ), . . . , ✓(P d )).\nThroughout this paper, we consider the case when X = R and P = {N (✓, 2) : ✓ 2 [ 1, 1]} is Gaussian distribution with for some fixed and known . Therefore, in the high-dimensional case, Pd = {N (~✓ , 2I\nd\n) : ~✓ 2 [ 1, 1]d} is a collection of spherical Gaussian distributions. We use ˆ~✓ to denote the d-dimensional estimator. For clarity, in this paper, we always use~· to indicate a vector in high dimensions.\nDistributed Protocols and Parameter Estimation: In this paper, we are interested in the situation where there are m machines and the jth machine receives n samples ~X(j,1), . . . , ~X(j,n) 2 Rd from the distribution ~P = N (~✓ , 2I\nd ). The machines communicate via a publicly shown blackboard. That is, when a machine writes a message on the blackboard, all other machines can see the content of the message. Following [1], we usually refer to the blackboard as the fusion center or simply center. Note that this model captures both point-to-point communication as well as broadcast com-\nmunication. Therefore, our lower bounds in this model apply to both the message passing setting and the broadcast setting. We will say that a protocol is simultaneous if each machine broadcasts a single message based on its input independently of the other machine ([1] call such protocols independent).\nWe denote the collection of all the messages written on the blackboard by Y . We will refer to Y as transcript and note that Y 2 {0, 1}⇤ is written in bits and the communication cost is defined as the length of Y , denoted by |Y |. In multi-machine setting, the estimator ˆ~✓ only sees the transcript Y , and it maps Y to ˆ~✓(Y ) 4, which is the estimation of ~✓ . Let letter j be reserved for index of the machine and k for the sample and letter i for the dimension. In other words, ~X(j,k)\ni is the ith-coordinate of kth sample of machine j. We will use ~X\ni as a shorthand for the collection of the ith coordinate of all the samples: ~X\ni = { ~X(j,k) i : j 2 [m], k 2 [n]}. Also note that [n] is a shorthand for {1, . . . , n}.\nThe mean-squared loss of the protocol ⇧ with estimator ˆ~✓ is defined as\nR ⇣ (⇧, ˆ~✓), ~✓ ⌘ = sup\n~ ✓\nE ~ X,⇧\n[kˆ~✓(Y ) ~✓ k2]\nand the communication cost of ⇧ is defined as\nCC(⇧) = sup ~\n✓\nE ~ X,⇧ [|Y |]\nThe main goal of this paper is to study the tradeoff between R ⇣ (⇧, ˆ~✓), ~✓ ⌘ and CC(⇧).\nProving Minimax Lower Bound: We follow the standard way to prove minimax lower bound. We introduce a (product) distribution Vd of ~✓ over the [ 1, 1]d. Let’s define the mean-squared loss with respect to distribution Vd as\nRVd((⇧, ˆ~✓), ~✓ ) = E\n~ ✓⇠Vd\n\"\nE ~ X,⇧\n[kˆ~✓(Y ) ~✓ k2] #\nIt is easy to see that RVd((⇧, ˆ~✓), ~✓ )  R((⇧, ˆ~✓), ~✓ ) for any distribution Vd. Therefore to prove lower bound for the minimax rate, it suffices to prove the lower bound for the mean-squared loss under any distribution Vd. 5\nPrivate/Public Randomness: We allow the protocol to use both private and public randomness. Private randomness, denoted by Rpriv, refers to the random bits that each machine draws by itself. Public randomness, denoted by Rpub, is a sequence of random bits that is shared among all parties before the protocol without being counted toward the total communication. Certainly allowing these two types of randomness only makes our lower bound stronger, and public randomness is actually only introduced for convenience.\nFurthermore, we will see in the proof of Theorem 3.1, the benefit of allowing private randomness is that we can hide information using private randomness when doing the reduction from one dimension protocol to d-dimensional one. The downside is that we require a stronger theorem (that tolerates private randomness) for the one dimensional lower bound, which is not a problem in our case since technique in [1] is general enough to handle private randomness.\nInformation cost: We define information cost IC(⇧) of protocol ⇧ as mutual information between the data and the messages communicated conditioned on the mean ~✓ . 6\n4Therefore here ~̂✓ maps {0, 1}⇤ to ⇥ 5Standard minimax theorem says that actually the supVd RVd((⇧, ~̂✓), ~✓ ) = R((⇧, ~̂✓), ~✓ ) under certain compactness condition for the space of ~✓ . 6Note that here we have introduced a distribution for the choice of ~✓ , and therefore ~✓ is a random variable.\nICVd(⇧) = I( ~X;Y | ~✓ , Rpub)\nPrivate randomness doesn’t explicitly appear in the definition of information cost but it affects it. Note that the information cost is a lower bound on the communication cost:\nICVd(⇧) = I( ~X;Y | ~✓ , Rpub)  H(Y )  CC(⇧)\nThe first inequality uses the fact that I(U ;V | W )  H(V | W )  H(V ) hold for any random variable U, V,W , and the second inequality uses Shannon’s source coding theorem [13].\nWe will drop the subscript for the prior Vd of ~✓ when it is clear from the context."
    }, {
      "heading" : "3 Main Results",
      "text" : ""
    }, {
      "heading" : "3.1 High Dimensional Lower bound via Direct Sum",
      "text" : "Our main theorem roughly states that if one can solves the d-dimensional problem, then one must be able to solve the one dimensional problem with information cost and square loss reduced by a factor of d. Therefore, a lower bound for one dimensional problem will imply a lower bound for high dimensional problem, with information cost and square loss scaled up by a factor of d.\nWe first define our task formally, and then state the theorem that relates d-dimensional task with one-dimensional task.\nDefinition 1. We say a protocol and estimator pair (⇧, ˆ~✓) solves task T (d,m, n, 2,Vd) with information cost C and mean-squared loss R, if for ~✓ randomly chosen from Vd, m machines, each of which takes n samples from N (~✓ , 2I\nd ) as input, can run the protocol ⇧ and get transcript Y so that the followings are true:\nRVd((⇧, ˆ~✓), ~✓ ) = R (1)\nIVd( ~X;Y | ~✓ , Rpub) = C (2)\nTheorem 3.1. [Direct-Sum] If (⇧, ˆ~✓) solves the task T (d,m, n, 2,Vd) with information cost C and squared loss R, then there exists (⇧0, ˆ✓) that solves the task T (1,m, n, 2,V) with information cost at most 4C/d and squared loss at most 4R/d. Furthermore, if the protocol ⇧ is simultaneous, then the protocol ⇧0 is also simultaneous. Remark 1. Note that this theorem doesn’t prove directly that communication cost scales linearly with the dimension, but only information cost. However for many natural problems, communication cost and information cost are similar for one dimension (e.g. for gaussian mean estimation) and then this direct sum theorem can be applied. In this sense it is very generic tool and is widely used in communication complexity and streaming algorithms literature.\nCorollary 3.1. Suppose (⇧, ˆ~✓) estimates the mean of N (~✓ , 2I d ), for all ~✓ 2 [ 1, 1]d, with meansquared loss R, and communication cost B. Then\nR ⌦ ✓ min ⇢ d2 2\nnB logm ,\nd 2\nn logm , d\n◆\nAs a corollary, when 2  mn, to achieve the mean-squared loss R = d 2\nmn\n, the communication cost\nB is at least ⌦ ⇣ dm\nlogm\n⌘\n.\nThis lower bound is tight up to polylogarithmic factors. In most of the cases, roughly B/m machines\nsending their sample mean to the fusion center and ˆ~✓ simply outputs the mean of the sample means with O(logm) bits of precision will match the lower bound up to a multiplicative log2 m factor. 7\n7When is very large, when ✓ is known to be in [ 1, 1], ~̂✓ = 0 is a better estimator, that is essentially why the lower bounds not only have the first term we desired but also the other two."
    }, {
      "heading" : "3.2 Protocol for sparse estimation problem",
      "text" : "In this section we consider the class of gaussian distributions with sparse mean: P s = {N (~✓ , 2I d ) : | ~✓ | 0\n s, ~✓ 2 Rd}. We provide a protocol that exploits the sparse structure of ~✓ .\nInputs : Machine j gets samples X(j,1), . . . , X(j,n) distributed according to N (~✓ , 2I d ), where ~✓ 2 Rd with | ~✓ |\n0\n s.\nFor each 1  j  m0 = (Lm log d)/↵, (where L is a sufficiently large constant), machine j sends its sample mean ¯X(j) = 1\nn\nX(j,1), . . . , X(j,n) (with precision O(logm)) to the center.\nFusion center calculates the mean of the sample means ¯X = 1 m 0\n⇣ ¯X(1) + · · ·+ ¯X(m0) ⌘ .\nLet ˆ~✓ i =\n⇢\n¯X i if | ¯X i |2 ↵ 2\nmn\n0 otherwise\nOutputs ˆ~✓ Protocol 1: Protocol for P\ns\nTheorem 3.2. For any P 2 P s , for any d/s ↵ 1, Protocol 1 returns ~✓ with mean-squared loss O(↵s 2\nmn\n) with communication cost O((dm logm log d)↵).\nThe proof of the theorem is deferred to supplementary material. Note that when ↵ = 1, we have a protocol with ˜O(dm) communication cost and mean-squared loss O(s 2/(mn)), and when ↵ = d/s, the communication cost is ˜O(sm) but squared loss O(d 2/(mn)). Comparing to the case where we don’t have sparse structure, basically we either replace the d factor in the communication cost by the intrinsic dimension s or the d factor in the squared loss by s, but not both."
    }, {
      "heading" : "3.3 Improved upper bound",
      "text" : "The lower bound provided in Section 3.1 is only tight up to polylogarithmic factor. To achieve the centralized minimax rate 2 d\nmn , the best existing upper bound of O(dm log(m)) bits of communication is achieved by the simple protocol that ask each machine to send its sample mean with O(log n) bits precision . We improve the upper bound to O(dm) using the interactive protocols.\nRecall that the class of unknown distributions of our model is Pd = {N (~✓ , 2I d ) : ✓ 2 [ 1, 1]d}. Theorem 3.3. Then there is an interactive protocol ⇧ with communication O(md) and an estimator ˆ~✓ based on ⇧ which estimates ~✓ up to a squared loss of O(d 2\nmn\n).\nRemark 2. Our protocol is interactive but not simultaneous, and it is a very interesting question whether the upper bound of O(dm) could be achieved by a simultaneous protocol."
    }, {
      "heading" : "3.4 Improved lower bound for simultaneous protocols",
      "text" : "Although we are not able to prove ⌦(dm) lower bound for achieve the centralized minimax rate in the interactive model, the lower bound for simultaneous case can be improved to ⌦(dm). Again, we lowerbound the information cost for the one dimensional problem first, and applying the direct-sum theorem in Section 3.1, we got the d-dimensional lower bound.\nTheorem 3.4. Suppose simultaneous protocol (⇧, ˆ~✓) estimates the mean of N (~✓ , 2I d ), for all ~✓ 2 [ 1, 1]d, with mean-squared loss R, and communication cost B, Then\nR ⌦ ✓ min ⇢ d2 2\nnB , d\n◆\nAs a corollary, when 2  mn, to achieve mean-squared loss R = d 2\nmn , the communication cost B is at least ⌦(dm)."
    }, {
      "heading" : "4 Proof sketches",
      "text" : ""
    }, {
      "heading" : "4.1 Proof sketch of theorem 3.1 and corollary 3.1",
      "text" : "To prove a lower bound for the d dimensional problem using an existing lower bound for one dimensional problem, we demonstrate a reduction that uses the (hypothetical) protocol ⇧ for d dimensions to construct a protocol for the one dimensional problem.\nFor each fixed coordinate i 2 [d], we design a protocol ⇧ i for the one-dimensional problem by embedding the one-dimensional problem into the ith coordinate of the d-dimensional problem. We will show essentially that if the machines first collectively choose randomly a coordinate i, and run protocol ⇧\ni for the one-dimensional problem, then the information cost and mean-squared loss of this protocol will be only 1/d factor of those of the d-dimensional problem. Therefore, the information cost of the d-dimensional problem is at least d times the information cost of one-dimensional problem.\nInputs : Machine j gets samples X(j,1), . . . , X(j,n) distributed according to N (✓, 2), where ✓ ⇠ V . 1. All machines publicly sample ˘✓ i distributed according to Vd 1.\n2. Machine j privately samples ˘X(j,1) i , . . . , ˘X (j,n) i distributed according to N (˘✓ i, 2Id 1). Let ˘X(j,k) = ( ˘X(j,k)\n1 , . . . , ˘X(j,k) i 1 , X (j,k), ˘X(j,k) i+1 , . . . , ˘X(j,k) d ).\n3. All machines run protocol ⇧ on data ˘X and get transcript Y i . The estimator ˆ✓ i is ˆ✓ i (Y i ) =\nˆ~✓(Y ) i i.e. the ith coordinate of the d-dimensional estimator.\nProtocol 2: ⇧ i\nIn more detail, under protocol ⇧ i (described formally in Protocol 2) the machines prepare a ddimensional dataset as follows: First they fill the one-dimensional data that they got into the ith coordinate of the d-dimensional data. Then the machines choose publicly randomly ~✓ i from distribution Vd 1, and draw independently and privately gaussian random variables from N (~✓ i , Id 1), and fill the data into the other d 1 coordinates. Then machines then simply run the d-dimension protocol ⇧ on this tailored dataset. Finally the estimator, denoted by ˆ✓\ni , outputs the ith coordinate of the d-dimensional estimator ˆ~✓.\nWe are interested in the mean-squared loss and information cost of the protocol ⇧ i ’s that we just designed. The following lemmas relate ⇧\ni\n’s with the original protocol ⇧.\nLemma 1. Protocols ⇧ i ’s satisfy P d\ni=1\nRV ⇣ (⇧ i , ˆ✓ i ), ✓ ⌘ = RVd ⇣ (⇧, ˆ~✓), ~✓ ⌘\nLemma 2. Protocols ⇧ i ’s satisfy P d\ni=1\nICV(⇧i)  ICVd(⇧)\nNote that the counterpart of Lemma 2 with communication cost won’t be true, and actually the communication cost of each ⇧\ni is the same as that of ⇧. It turns out doing reduction in communication cost is much harder, and this is part of the reason why we use information cost as a proxy for communication cost when proving lower bound. Also note that the correctness of Lemma 2 heavily relies on the fact that ⇧\ni draws the redundant data privately independently (see Section 2 and the proof for more discussion on private versus public randomness).\nBy Lemma 1 and Lemma 2 and a Markov argument, there exists an i 2 {1, . . . , d} such that\nR ⇣ (⇧\ni , ˆ✓ i\n), ✓ ⌘\n 4 d ·R\n⇣ (⇧, ~✓ ), ~✓ ⌘ and IC(⇧ i\n)  4 d · IC(⇧)\nThen the pair (⇧0, ˆ✓) = (⇧ i , ˆ✓ i ) solves the task T (1,m, n, 2,V) with information cost at most 4C/d and squared loss 4R/d, which proves Theorem 3.1.\nCorollary 3.1 follows Theorem 3.1 and the following lower bound for one dimensional gaussian mean estimation proved in [1]. We provide complete proofs in the supplementary.\nTheorem 4.1. [1] Let V be the uniform distribution over {± }, where 2  min ⇣ 1, 2 log(m)\nn\n⌘\n.\nIf (⇧, ˆ✓) solves the task T (1,m, n, 2,V) with information cost C and squared loss R, then either C ⌦ ⇣ 2\n2\nn log(m)\n⌘\nor R 2/10."
    }, {
      "heading" : "4.2 Proof sketch of theorem 3.3",
      "text" : "The protocol is described in protocol 3 in the supplementary. We only describe the d = 1 case, while for general case we only need to run d protocols individually for each dimension.\nThe central idea is that we maintain an upper bound U and lower bound L for the target mean, and iteratively ask the machines to send their sample means to shrink the interval [L,U ]. Initially we only know that ✓ 2 [ 1, 1]. Therefore we set the upper bound U and lower bound L for ✓ to be 1 and 1. In the first iteration the machines try to determine whether ✓ < 0 or 0. This is done by letting several machines (precisely, O(logm)/ 2 machines) send whether their sample means are < 0 or 0. If the majority of the samples are < 0, ✓ is likely to be < 0. However when ✓ is very close to 0, one needs a lot of samples to determine this, but here we only ask O(logm)/ 2 machines to send their sample means. Therefore we should be more conservative and we only update the interval in which ✓ might lie to [ 1, 1/2] if the majority of samples are < 0. We repeat this until the interval (L,U) become smaller than our target squared loss. Each round, we ask a number of new machines sending 1 bits of information about whether their sample mean is large than (U + L)/2. The number of machines participated is carefully set so that the failure probability p is small. An interesting feature of the protocol is to choose the target error probability p differently at each iteration so that we have a better balance between the failure probability and communication cost. The complete the description of the protocol and proof are given in the supplementary."
    }, {
      "heading" : "4.3 Proof sketch of theorem 3.4",
      "text" : "We use a different prior on the mean N (0, 2) instead of uniform over { , } used by [1]. Gaussian prior allows us to use a strong data processing inequality for jointly gaussian random variables by [14]. Since we don’t have to truncate the gaussian, we don’t lose the factor of log(m) lost by [1]. Theorem 4.2. ([14], Theorem 7) Suppose X and V are jointly gaussian random variables with correlation ⇢. Let Y $ X $ V be a markov chain with I(Y ;X)  R. Then I(Y ;V )  ⇢2R.\nNow suppose that each machine gets n samples X1, . . . , Xn ⇠ N (V, 2), where V is the prior N (0, 2) on the mean. By an application of theorem 4.2, we prove that if Y is a B-bit message depending on X1, . . . , Xn, then Y has only n 2\n2 · B bits of information about V . Using some standard information theory arguments, this converts into the statement that if Y is the transcript of a simultaneous protocol with communication cost  B, then it has at most n 2\n2 ·B bits of information about V . Then a lower bound on the communication cost B of a simultaneous protocol estimating the mean ✓ 2 [ 1, 1] follows from proving that such a protocol must have ⌦(1) bit of information about V . Complete proof is given in the supplementary."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have lowerbounded the communication cost of estimating the mean of a d-dimensional spherical gaussian random variables in a distributed fashion. We provided a generic tool called direct-sum for relating the information cost of d-dimensional problem to one-dimensional problem, which might be of potential use for other statistical problem than gaussian mean estimation as well.\nWe also initiated the study of distributed estimation of gaussian mean with sparse structure. We provide a simple protocol that exploits the sparse structure and conjecture its tradeoff to be optimal: Conjecture 1. If some protocol estimates the mean for any distribution P 2 P\ns with mean-squared loss R and communication cost C, then C · R & sd 2\nmn , where we use & to hide log factors and potential corner cases."
    } ],
    "references" : [ {
      "title" : "Informationtheoretic lower bounds for distributed statistical estimation with communication constraints",
      "author" : [ "Yuchen Zhang", "John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Distributed learning, communication complexity and privacy",
      "author" : [ "Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour" ],
      "venue" : "In COLT, pages 26.1–26.22,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Protocols for learning classifiers on distributed data",
      "author" : [ "Hal Daumé III", "Jeff M. Phillips", "Avishek Saha", "Suresh Venkatasubramanian" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Efficient protocols for distributed classification and optimization",
      "author" : [ "Hal Daumé III", "Jeff M. Phillips", "Avishek Saha", "Suresh Venkatasubramanian" ],
      "venue" : "In ALT,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Informationtheoretic lower bounds for distributed statistical estimation with communication",
      "author" : [ "John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright", "Yuchen Zhang" ],
      "venue" : "constraints. CoRR,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Informational complexity and the direct sum problem for simultaneous message complexity",
      "author" : [ "Amit Chakrabarti", "Yaoyun Shi", "Anthony Wirth", "Andrew Chi-Chih Yao" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "An information statistics approach to data stream and communication complexity",
      "author" : [ "Ziv Bar-Yossef", "T.S. Jayram", "Ravi Kumar", "D. Sivakumar" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2004
    }, {
      "title" : "Information equals amortized communication",
      "author" : [ "Mark Braverman", "Anup Rao" ],
      "venue" : "In FOCS, pages 748–757,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "How to compress interactive communication",
      "author" : [ "Boaz Barak", "Mark Braverman", "Xi Chen", "Anup Rao" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "A tight bound for set disjointness in the message-passing model",
      "author" : [ "Mark Braverman", "Faith Ellen", "Rotem Oshman", "Toniann Pitassi", "Vinod Vaikuntanathan" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2013
    }, {
      "title" : "Exponential separation of information and communication",
      "author" : [ "Anat Ganor", "Gillat Kol", "Ran Raz" ],
      "venue" : "Electronic Colloquium on Computational Complexity (ECCC),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Communication-efficient algorithms for statistical optimization",
      "author" : [ "Yuchen Zhang", "John C. Duchi", "Martin J. Wainwright" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "A mathematical theory of communication",
      "author" : [ "Claude Shannon" ],
      "venue" : "Bell System Technical Journal,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1948
    }, {
      "title" : "The efficiency of investment information",
      "author" : [ "Elza Erkip", "Thomas M. Cover" ],
      "venue" : "IEEE Trans. Inform. Theory,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Applying this result to previous lower bounds for one dimension in the interactive setting [1] and to our improved bounds for the simultaneous setting, we prove new lower bounds of ⌦(md/ log(m)) and ⌦(md) for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 1,
      "context" : "A lot of recent works are devoted to understand how to solve problems distributedly with efficient communication [2, 3, 4, 1, 5].",
      "startOffset" : 113,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "A lot of recent works are devoted to understand how to solve problems distributedly with efficient communication [2, 3, 4, 1, 5].",
      "startOffset" : 113,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "A lot of recent works are devoted to understand how to solve problems distributedly with efficient communication [2, 3, 4, 1, 5].",
      "startOffset" : 113,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "A lot of recent works are devoted to understand how to solve problems distributedly with efficient communication [2, 3, 4, 1, 5].",
      "startOffset" : 113,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "A lot of recent works are devoted to understand how to solve problems distributedly with efficient communication [2, 3, 4, 1, 5].",
      "startOffset" : 113,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "The work [1] showed a lower bound on the communication cost for this problem when d = 1.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 4,
      "context" : "Note that [5] independently achieve the same bound by refining the proof in [1].",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Note that [5] independently achieve the same bound by refining the proof in [1].",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "In the simultaneous communication setting, where all machines send one message to one machine and this machine needs to figure out the estimation, the work [1] showed that ⌦(md/ logm) bits of communication are needed to achieve the minimax squared loss.",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "There has been a lot of work on the paradigm of studying communication complexity via the notion of information complexity [6, 7, 8, 9, 10].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "There has been a lot of work on the paradigm of studying communication complexity via the notion of information complexity [6, 7, 8, 9, 10].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "There has been a lot of work on the paradigm of studying communication complexity via the notion of information complexity [6, 7, 8, 9, 10].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : "There has been a lot of work on the paradigm of studying communication complexity via the notion of information complexity [6, 7, 8, 9, 10].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 9,
      "context" : "There has been a lot of work on the paradigm of studying communication complexity via the notion of information complexity [6, 7, 8, 9, 10].",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "Information complexity can be thought of as a proxy for communication complexity that is especially accurate for solving multiple copies of the same problem simultaneously [8].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 10,
      "context" : "communication cost instead of information cost, there exist examples where solving d copies of a certain problem requires less communication than d times the communication required for one copy [11].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 6,
      "context" : "The proof technique is directly inspired by the notion of conditional information complexity [7], which was used to prove direct sum theorems and lower bounds for streaming algorithms.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Following [1], we usually refer to the blackboard as the fusion center or simply center.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "We will say that a protocol is simultaneous if each machine broadcasts a single message based on its input independently of the other machine ([1] call such protocols independent).",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "The downside is that we require a stronger theorem (that tolerates private randomness) for the one dimensional lower bound, which is not a problem in our case since technique in [1] is general enough to handle private randomness.",
      "startOffset" : 178,
      "endOffset" : 181
    }, {
      "referenceID" : 12,
      "context" : "Note that the information cost is a lower bound on the communication cost: ICVd(⇧) = I( ~ X;Y | ~ ✓ , Rpub)  H(Y )  CC(⇧) The first inequality uses the fact that I(U ;V | W )  H(V | W )  H(V ) hold for any random variable U, V,W , and the second inequality uses Shannon’s source coding theorem [13].",
      "startOffset" : 298,
      "endOffset" : 302
    }, {
      "referenceID" : 0,
      "context" : "1 and the following lower bound for one dimensional gaussian mean estimation proved in [1].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "[1] Let V be the uniform distribution over {± }, where 2  min ⇣",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "4 We use a different prior on the mean N (0, 2) instead of uniform over { , } used by [1].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 13,
      "context" : "Gaussian prior allows us to use a strong data processing inequality for jointly gaussian random variables by [14].",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "Since we don’t have to truncate the gaussian, we don’t lose the factor of log(m) lost by [1].",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "([14], Theorem 7) Suppose X and V are jointly gaussian random variables with correlation ⇢.",
      "startOffset" : 1,
      "endOffset" : 5
    } ],
    "year" : 2014,
    "abstractText" : "We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean ~ ✓ of an unknown d dimensional gaussian distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among m different machines. The goal is to estimate the mean ~ ✓ at the optimal minimax rate while communicating as few bits as possible. We show that in this setting, the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually. Applying this result to previous lower bounds for one dimension in the interactive setting [1] and to our improved bounds for the simultaneous setting, we prove new lower bounds of ⌦(md/ log(m)) and ⌦(md) for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively. To complement, we also demonstrate an interactive protocol achieving the minimax squared loss with O(md) bits of communication, which improves upon the simple simultaneous protocol by a logarithmic factor. Given the strong lower bounds in the general setting, we initiate the study of the distributed parameter estimation problems with structured parameters. Specifically, when the parameter is promised to be s-sparse, we show a simple thresholding based protocol that achieves the same squared loss while saving a d/s factor of communication. We conjecture that the tradeoff between communication and squared loss demonstrated by this protocol is essentially optimal up to logarithmic factor.",
    "creator" : null
  }
}