{
  "name" : "07a96b1f61097ccb54be14d6a47439b0.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Recovery of Coherent Data via Low-Rank Dictionary Pursuit",
    "authors" : [ "Guangcan Liu", "Ping Li" ],
    "emails" : [ "gcliu@rutgers.edu", "pingli@rutgers.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Nowadays our data are often high-dimensional, massive and full of gross errors (e.g., corruptions, outliers and missing measurements). In the presence of gross errors, the classical Principal Component Analysis (PCA) method, which is probably the most widely used tool for data analysis and dimensionality reduction, becomes brittle — A single gross error could render the estimate produced by PCA arbitrarily far from the desired estimate. As a consequence, it is crucial to develop new statistical tools for robustifying PCA. A variety of methods have been proposed and explored in the literature over several decades, e.g., [2, 3, 4, 8, 9, 10, 11, 12, 24, 13, 16, 19, 25]. One of the most exciting methods is probably the so-called RPCA (Robust Principal Component Analysis) method [4], which was built upon the exploration of the following low-rank matrix recovery problem:\nProblem 1 (Low-Rank Matrix Recovery) Suppose we have a data matrix X ∈ Rm×n and we know it can be decomposed as\nX = L0 + S0, (1.1)\nwhere L0 ∈ Rm×n is a low-rank matrix each column of which is a data point drawn from some low-dimensional subspace, and S0 ∈ Rm×n is a sparse matrix supported on Ω ⊆ {1, · · · ,m} × {1, · · · , n}. Except these mild restrictions, both components are arbitrary. The rank of L0 is unknown, the support set Ω (i.e., the locations of the nonzero entries of S0) and its cardinality (i.e., the amount of the nonzero entries of S0) are unknown either. In particular, the magnitudes of the nonzero entries in S0 may be arbitrarily large. Given X , can we recover both L0 and S0, in a scalable and exact fashion?\nThe theory of RPCA tells us that, very generally, when the low-rank matrix L0 is meanwhile incoherent (i.e., with low coherence), both the low-rank and the sparse matrices can be exactly recovered by using the following convex, potentially scalable program:\nmin L,S\n‖L‖∗ + λ‖S‖1, s.t. X = L+ S, (1.2)\nwhere ‖ · ‖∗ is the nuclear norm [7] of a matrix, ‖ · ‖1 denotes the ℓ1 norm of a matrix seen as a long vector, and λ > 0 is a parameter. Besides of its elegance in theory, RPCA also has good empirical performance in many practical areas, e.g., image processing [26], computer vision [18], radar imaging [1], magnetic resonance imaging [17], etc.\nWhile complete in theory and powerful in reality, RPCA cannot be an ultimate solution to the lowrank matrix recovery Problem 1. Indeed, the method might not produce perfect recovery even when L0 is strictly low-rank. This is because RPCA captures only the low-rankness property, which however is not the only property of our data, but essentially ignores the extra structures (beyond low-rankness) widely existing in data: Given the low-rankness constraint that the data points (i.e., columns vectors of L0) locate on a low-dimensional subspace, it is unnecessary for the data points to locate on the subspace uniformly at random and it is quite normal that the data may have some extra structures, which specify in more detail how the data points locate on the subspace. Figure 1 demonstrates a typical example of extra structures; that is, the clustering structures which are ubiquitous in modern applications. Whenever the data are exhibiting some clustering structures, RPCA is no longer a method of perfection. Because, as will be shown in this paper, while the rank of L0 is fixed and the underlying cluster number goes large, the coherence of L0 keeps heightening and thus, arguably, the performance of RPCA drops.\nTo better handle coherent data (i.e., the cases where L0 has large coherence parameters), a seemingly straightforward idea is to avoid the coherence parameters of L0. However, as explained in [4], the coherence parameters are indeed necessary (if there is no additional condition assumed on the data). This paper shall further indicate that the coherence parameters are related in nature to some extra structures intrinsically existing in L0 and therefore cannot be discarded simply. Interestingly, we show that it is possible to avoid the coherence parameters by using some additional conditions, which are easy to obey in supervised environment and can also be approximately achieved in unsupervised environment. Our study is based on the following convex program termed Low-Rank Representation (LRR) [13]:\nmin Z,S\n‖Z‖∗ + λ‖S‖1, s.t. X = AZ + S, (1.3)\nwhere A ∈ Rm×d is a size-d dictionary matrix constructed in advance1, and λ > 0 is a parameter. In order for LRR to avoid the coherence parameters which increase with the cluster number underlying\n1It is not crucial to determine the exact value of d. Suppose Z∗ is the optimal solution with respect to Z. Then LRR uses AZ∗ to restore L0. LRR falls back to RPCA when A = I (identity matrix). Furthermore, it can be proved that the recovery produced by LRR is the same as RPCA whenever the dictionary A is orthogonal.\nL0, we prove that it is sufficient to construct in advance a dictionary A which is low-rank by itself. This gives a generic prescription to defend the possible infections raised by coherent data, providing an elementary criteria for learning the dictionary matrix A. Subsequently, we propose a simple and effective algorithm that utilizes the output of RPCA to construct the dictionary in LRR. Our extensive experiments demonstrated on randomly generated matrices and motion data show promising results. In summary, the contributions of this paper include the following:\n⋄ For the first time, this paper studies the problem of recovering low-rank, and coherent (or less incoherent as equal) matrices from their corrupted versions. We investigate the physical regime where coherent data arise. For example, the widely existing clustering structures may lead to coherent data. We prove some basic theories for resolving the problem, and also establish a practical algorithm that outperforms RPCA in our experimental study.\n⋄ Our studies help reveal the physical meaning of coherence, which is now standard and widely used in various literatures, e.g., [2, 3, 4, 25, 15]. We show that the coherence parameters are not “assumptions” for a proof, but rather some excellent quantities that relate in nature to the extra structures (beyond low-rankness) intrinsically existing in L0.\n⋄ This paper provides insights regarding the LRR model proposed by [13]. While the special case of A = X has been extensively studied, the LRR model (1.3) with general dictionaries is not fully understood yet. We show that LRR (1.3) equipped with proper dictionaries could well handle coherent data.\n⋄ The idea of replacing L with AZ is essentially related to the spirit of matrix factorization which has been explored for long, e.g., [20, 23]. In that sense, the explorations of this paper help to understand why factorization techniques are useful."
    }, {
      "heading" : "2 Summary of Main Notations",
      "text" : "Capital letters such as M are used to represent matrices, and accordingly, [M ]ij denotes its (i, j)th entry. Letters U , V , Ω and their variants (complements, subscripts, etc.) are reserved for left singular vectors, right singular vectors and support set, respectively. We shall abuse the notation U (resp. V ) to denote the linear space spanned by the columns of U (resp. V ), i.e., the column space (resp. row space). The projection onto the column space U , is denoted by PU and given by PU (M) = UUTM , and similarly for the row space PV (M) = MV V T . We shall also abuse the notation Ω to denote the linear space of matrices supported on Ω. Then PΩ and PΩ⊥ respectively denote the projections onto Ω and Ωc such that PΩ +PΩ⊥ = I, where I is the identity operator. The symbol (·)+ denotes the Moore-Penrose pseudoinverse of a matrix: M+ = VMΣ −1 M U T M for a matrix M with Singular Value Decomposition (SVD)2 UMΣMV TM .\nSix different matrix norms are used in this paper. The first three norms are functions of the singular values: 1) The operator norm (i.e., the largest singular value) denoted by ‖M‖, 2) the Frobenius norm (i.e., square root of the sum of squared singular values) denoted by ‖M‖F , and 3) the nuclear norm (i.e., the sum of singular values) denoted by ‖M‖∗. The other three are the ℓ1, ℓ∞ (i.e., sup-norm) and ℓ2,∞ norms of a matrix: ‖M‖1 = ∑\ni,j |[M ]ij |, ‖M‖∞ = maxi,j{|[M ]ij |} and ‖M‖\n2,∞ = maxj{ √ ∑ i[M ] 2 ij}, respectively.\nThe Greek letter µ and its variants (e.g., subscripts and superscripts) are reserved for the coherence parameters of a matrix. We shall also reserve two lower case letters, m and n, to respectively denote the data dimension and the number of data points, and we use the following two symbols throughout this paper:\nn1 = max(m,n) and n2 = min(m,n)."
    }, {
      "heading" : "3 On the Recovery of Coherent Data",
      "text" : "In this section, we shall firstly investigate the physical regime that raises coherent (or less incoherent) data, and then discuss the problem of recovering coherent data from corrupted observations, providing some basic principles and an algorithm for resolving the problem.\n2In this paper, SVD always refers to skinny SVD. For a rank-r matrix M ∈ Rm×n, its SVD is of the form UMΣMV T M , with UM ∈ R m×r,ΣM ∈ R r×r and VM ∈ Rn×r ."
    }, {
      "heading" : "3.1 Coherence Parameters and Their Properties",
      "text" : "As the rank function cannot fully capture all characteristics of L0, it is necessary to define some quantities to measure the effects of various extra structures (beyond low-rankness) such as the clustering structure as demonstrated in Figure 1. The coherence parameters defined in [3, 4] are excellent exemplars of such quantities."
    }, {
      "heading" : "3.1.1 Coherence Parameters: µ1, µ2, µ3",
      "text" : "For an m × n matrix L0 with rank r0 and SVD L0 = U0Σ0V T0 , some important properties can be characterized by three coherence parameters, denoted as µ1, µ2 and µ3, respectively. The first coherence parameter, 1 ≤ µ1(L0) ≤ m, which characterizes the column space identified by U0, is defined as\nµ1(L0) = m\nr0 max 1≤i≤m ‖UT0 ei‖22, (3.4)\nwhere ei denotes the ith standard basis. The second coherence parameter, 1 ≤ µ2(L0) ≤ n, which characterizes the row space identified by V0, is defined as\nµ2(L0) = n\nr0 max 1≤j≤n\n‖V T0 ej‖22. (3.5)\nThe third coherence parameter, 1 ≤ µ3(L0) ≤ mn, which characterizes the joint space identified by U0V T0 , is defined as\nµ3(L0) = mn\nr0 (‖U0V T0 ‖∞)2 =\nmn\nr0 max i,j\n(|〈UT0 ei, V T0 ej〉|)2. (3.6)\nThe analysis in RPCA [4] merges the above three parameters into a single one: µ(L0) = max{µ1(L0), µ2(L0), µ3(L0)}. As will be seen later, the behaviors of those three coherence parameters are different from each other, and hence it is more adequate to consider them individually."
    }, {
      "heading" : "3.1.2 µ2-phenomenon",
      "text" : "According to the analysis in [4], the success condition (regarding L0) of RPCA is\nrank (L0) ≤ crn2\nµ(L0)(logn1)2 , (3.7)\nwhere µ(L0) = max{µ1(L0), µ2(L0), µ3(L0)} and cr > 0 is some numerical constant. Thus, RPCA will be less successful when the coherence parameters are considerably larger. In this subsection, we shall show that the widely existing clustering structure can enlarge the coherence parameters and, accordingly, downgrades the performance of RPCA.\nGiven the restriction that rank (L0) = r0, the data points (i.e., column vectors of L0) are unnecessarily sampled from a r0-dimensional subspace uniformly at random. A more realistic interpretation is to consider the data points as samples from the union of k number of subspaces (i.e., clusters), and the sum of those multiple subspaces together has a dimension r0. That is to say, there are multiple “small” subspaces inside one r0-dimensional “large” subspace, as exemplified in Figure 1. Whenever the low-rank matrix L0 is meanwhile exhibiting such clustering behaviors, the second coherence parameter µ2(L0) (and so µ3(L0)) will increase with the number of clusters underlying L0, as shown in Figure 2. When the coherence is heightening, (3.7) suggests that the performance of RPCA will drop, as verified in Figure 2(d). Note here that the variation of µ3 is mainly due to the variation of the row space, which is characterized by µ2. We call the phenomena shown in Figure 2(b)∼(d) as the “µ2-phenomenon”. Readers can also refer to the full paper to see why the second coherence parameter increases with the cluster number underlying L0.\nInterestingly, one may have noticed that µ1 is invariant to the variation of the clustering number, as can be seen from Figure 2(a). This is because the clustering behavior of the data points can only affect the row space, while µ1 is defined on the column space. Yet, if the row vectors of L0 also own some clustering structure, µ1 could be large as well. Such kind of data can exist widely in text documents and we leave this as future work."
    }, {
      "heading" : "3.2 Avoiding µ2 by LRR",
      "text" : "The µ2-phenomenon implies that the second coherence parameter µ2 is related in nature to some intrinsic structures of L0 and thus cannot be eschewed without using additional conditions. In the following, we shall figure out under what conditions the second coherence parameter µ2 (and µ3) can be avoided such that LRR could well handle coherent data.\nMain Result: We show that, when the dictionary A itself is low-rank, LRR is able to avoid µ2. Namely, the following theorem is proved without using µ2. See the full paper for a detailed proof.\nTheorem 1 (Noiseless) Let A ∈ Rm×d with SVD A = UAΣAV TA be a column-wisely unit-normed (i.e., ‖Aei‖2 = 1, ∀i) dictionary matrix which satisfies PUA(U0) = U0 (i.e., U0 is a subspace of UA). For any 0 < ǫ < 0.5 and some numerical constant ca > 1, if\nrank (L0) ≤ rank (A) ≤ ǫ2n2\ncaµ1(A) log n1 and |Ω| ≤ (0.5− ǫ)mn, (3.8)\nthen with probability at least 1 − n−101 , the optimal solution to the LRR problem (1.3) with λ = 1/ √ n1 is unique and exact, in a sense that\nZ∗ = A+L0 and S ∗ = S0,\nwhere (Z∗, S∗) is the optimal solution to (1.3).\nIt is worth noting that the restriction rank (L0) ≤ O(n2/ logn1) is looser than that of PRCA3, which requires rank (L0) ≤ O(n2/(logn1)2). The requirement of column-wisely unit-normed dictionary (i.e., ‖Aei‖2 = 1, ∀i) is purely for complying the parameter estimate of λ = 1/ √ n1, which is consistent with RPCA. The condition PUA(U0) = U0, i.e., U0 is a subspace of UA, is indispensable if we ask for exact recovery, because PUA(U0) = U0 is implied by the equality AZ∗ = L0. This necessary condition, together with the low-rankness condition, provides an elementary criterion for learning the dictionary matrix A in LRR. Figure 3 presents an example, which further confirms our main result; that is, LRR is able to avoid µ2 as long as U0 ⊂ UA and A is low-rank. It is also worth noting that it is unnecessary for A to satisfy UA = U0, and that LRR is actually tolerant to the “errors” possibly existing in the dictionary.\nThe program (1.3) is designed for the case where the uncorrupted observations are noiseless. In reality this assumption is often not true and all entries of X can be contaminated by a small amount of noises, i.e., X = L0 + S0 +N , where N is a matrix of dense Gaussian noises. In this case, the formula of LRR (1.3) need be modified to\nmin Z,S\n‖Z‖∗ + λ‖S‖1, s.t. ‖X −AZ − S‖F ≤ ε, (3.9)\n3In terms of exact recovery, O(n2/ log n1) is probably the “finest” bound one could accomplish in theory.\nwhere ε is a parameter that measures the noise level of data. In the experiments of this paper, we consistently set ε = 10−6‖X‖F . In the presence of dense noises, the latent matrices, L0 and S0, cannot be exactly restored. Yet we have the following theorem to guarantee the near recovery property of the solution produced by the program (3.9):\nTheorem 2 (Noisy) Suppose ‖X−L0 −S0‖F ≤ ε. Let A ∈ Rm×d with SVD A = UAΣAV TA be a column-wisely unit-normed dictionary matrix which satisfies PUA(U0) = U0 (i.e., U0 is a subspace of UA). For any 0 < ǫ < 0.35 and some numerical constant ca > 1, if\nrank (L0) ≤ rank (A) ≤ ǫ2n2\ncaµ1(A) log n1 and |Ω| ≤ (0.35− ǫ)mn, (3.10)\nthen with probability at least 1−n−101 , any solution (Z∗, S∗) to (3.9) with λ = 1/ √ n1 gives a near\nrecovery to (L0, S0), in a sense that ‖AZ∗ − L0‖F ≤ 8 √ mnε and ‖S∗ − S0‖F ≤ (8 √ mn+ 2)ε."
    }, {
      "heading" : "3.3 An Unsupervised Algorithm for Matrix Recovery",
      "text" : "To handle coherent (equivalently, less incoherent) data, Theorem 1 suggests that the dictionary matrix A should be low-rank and satisfy U0 ⊂ UA. In certain supervised environment, this might not be difficult as one could potentially use clear, well processed training data to construct the dictionary. In an unsupervised environment, however, it will be challenging to identify a low-rank dictionary that obeys U0 ⊂ UA. Note that U0 ⊂ UA can be viewed as supervision information (if A is low-rank). In this paper, we will introduce a heuristic algorithm that can work distinctly better than RPCA in an unsupervised environment. As can be seen from (3.7), RPCA is actually not brittle with respect to coherent data (although its performance is depressed). Based on this observation, we propose a simple algorithm, as summarized in Algorithm 1, to achieve a solid improvement over RPCA. Our idea is straightforward: We first obtain an estimate of L0 by using RPCA and then utilize the estimate to construct the dictionary matrix A in LRR. The post-processing steps (Step 2 and Step 3) that slightly modify the solution of RPCA is to encourage well-conditioned dictionary, which is the circumstance favoring LRR.\nWhenever the recovery produced by RPCA is already exact, the claim in Theorem 1 gives that the recovery produced by our Algorithm 1 is exact as well. That is to say, in terms of exactly recovering L0 from a given X , the success probability of our Algorithm 1 is greater than or equal to that of RPCA. From the computational perspective, Algorithm 1 does not really double the work of RPCA, although there are two convex programs in our algorithm. In fact, according to our simulations, usually the computational time of Algorithm 1 is merely about 1.2 times as much as RPCA. The reason is that, as has been explored by [13], the complexity of solving the LRR problem (1.3) is O(n2rA) (assuming m = n), which is much lower than that of RPCA (which requires O(n3)) provided that the obtained dictionary matrix A is fairly low-rank (i.e., rA is small).\nOne may have noticed that the procedure of Algorithm 1 could be made iterative, i.e., one can consider ÂZ∗ as a new estimate of L0 and use it to further update the dictionary matrix A, and so on. Nevertheless, we empirically find that such an iterative procedure often converges within two iterations. Hence, for the sake of simplicity, we do not consider iterative strategies in this paper.\nAlgorithm 1 Matrix Recovery\ninput: Observed data matrix X ∈ Rm×n. adjustable parameter: λ. 1. Solve for L̂0 by optimizing the RPCA problem (1.2) with λ = 1/ √ n1.\n2. Estimate the rank of L̂0 by r̂0 = #{i : σi > 10−3σ1},\nwhere σ1, σ2, · · · , σn2 are the singular values of L̂0. 3. Form L̃0 by using the rank-r̂0 approximation of L̂0. That is,\nL̃0 = argmin L\n‖L− L̂0‖2F , s.t. rank (L) ≤ r̂0,\nwhich is solved by SVD. 4. Construct a dictionary Â from L̃0 by normalizing the column vectors of L̃0:\n[Â]:,i = [L̃0]:,i\n‖[L̃0]:,i‖2 , i = 1, · · · , n,\nwhere [·]:,i denotes the ith column of a matrix. 5. Solve for Z∗ by optimizing the LRR problem (1.3) with A = Â and λ = 1/ √ n1. output: ÂZ∗."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Results on Randomly Generated Matrices",
      "text" : "We first verify the effectiveness of our Algorithm 1 on randomly generated matrices. We generate a collection of 200 × 1000 data matrices according to the model of X = PΩ⊥(L0) + PΩ(S0): Ω is a support set chosen at random; L0 is created by sampling 200 data points from each of 5 randomly generated subspaces; S0 consists of random values from Bernoulli ±1. The dimension of each subspace varies from 1 to 20 with step size 1, and thus the rank of L0 varies from 5 to 100 with step size 5. The fraction |Ω|/(mn) varies from 2.5% to 50% with step size 2.5%. For each pair of rank and support size (r0, |Ω|), we run 10 trials, resulting in a total of 4000 (20× 20× 10) trials.\nFigure 4 compares our Algorithm 1 to RPCA, both using λ = 1/ √ n1. It can be seen that, using the learned dictionary matrix, Algorithm 1 works distinctly better than RPCA. In fact, the success area (i.e., the area of the white region) of our algorithm is 47% wider than that of RPCA! We should also mention that it is possible for RPCA to be exactly successful on coherent (or less incoherent) data, provided that the rank of L0 is low enough and/or S0 is sparse enough. Our algorithm in general improves RPCA when L0 is moderately low-rank and/or S0 is moderately sparse."
    }, {
      "heading" : "4.2 Results on Corrupted Motion Sequences",
      "text" : "We now present our experiment with 11 additional sequences attached to the Hopkins155 [21] database. In those sequences, about 10% of the entries in the data matrix of trajectories are unobserved (i.e., missed) due to vision occlusion. We replace each missed entry with a number from Bernoulli ±1, resulting in a collection of corrupted trajectory matrices for evaluating the effectiveness of matrix recovery algorithms. We perform subspace clustering on both the corrupted trajectory matrices and the recovered versions, and use the clustering error rates produced by existing subspace clustering methods as the evaluation metrics. We consider three state-of-the-art subspace clustering methods: Shape Interaction Matrix (SIM) [5], Low-Rank Representation with A = X [14] (which is referred to as “LRRx”) and Sparse Subspace Clustering (SSC) [6].\nTable 1 shows the error rates of various algorithms. Without the preprocessing of matrix recovery, all the subspace clustering methods fail to accurately categorize the trajectories of motion objects, producing error rates higher than 20%. This illustrates that it is important for motion segmentation to correct the gross corruptions possibly existing in the data matrix of trajectories. By using RPCA (λ = 1/ √ n1) to correct the corruptions, the clustering performances of all considered methods are improved dramatically. For example, the error rate of SSC is reduced from 22.81% to 9.50%. By choosing an appropriate dictionary for LRR (λ = 1/ √ n1), the error rates can be reduced again, from 9.50% to 5.74%, which is a 40% relative improvement. These results verify the effectiveness of our dictionary learning strategy in realistic environments."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "We have studied the problem of disentangling the low-rank and sparse components in a given data matrix. Whenever the low-rank component exhibits clustering structures, the state-of-the-art RPCA method could be less successful. This is because RPCA prefers incoherent data, which however may be inconsistent with data in the real world. When the number of clusters becomes large, the second and third coherence parameters enlarge and hence the performance of RPCA could be depressed. We have showed that the challenges arising from coherent (equivalently, less incoherent) data could be effectively alleviated by learning a suitable dictionary under the LRR framework. Namely, when the dictionary matrix is low-rank and contains information about the ground truth matrix, LRR can be immune to the coherence parameters that increase with the underlying cluster number. Furthermore, we have established a practical algorithm that outperforms RPCA in our extensive experiments.\nThe problem of recovering coherent data essentially concerns the robustness issues of the Generalized PCA (GPCA) [22] problem. Although the classic GPCA problem has been explored for several decades, robust GPCA is new and has not been well studied. The approach proposed in this paper is in a sense preliminary, and it is possible to develop other effective methods for learning the dictionary matrix in LRR and for handling coherent data. We leave these as future work."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Guangcan Liu was a Postdoctoral Researcher supported by NSF-DMS0808864, NSF-SES1131848, NSF-EAGER1249316, AFOSR-FA9550-13-1-0137, and ONR-N00014-13-1-0764. Ping Li is also partially supported by NSF-III1360971 and NSF-BIGDATA1419210."
    } ],
    "references" : [ {
      "title" : "Synthetic aperture radar imaging and motion estimation via robust principle component analysis",
      "author" : [ "Liliana Borcea", "Thomas Callaghan", "George Papanicolaou" ],
      "venue" : "Arxiv,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Matrix completion with noise",
      "author" : [ "Emmanuel Candès", "Yaniv Plan" ],
      "venue" : "In IEEE Proceeding,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Robust principal component analysis",
      "author" : [ "Emmanuel J. Candès", "Xiaodong Li", "Yi Ma", "John Wright" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "A multibody factorization method for independently moving objects",
      "author" : [ "Joao Costeira", "Takeo Kanade" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Sparse subspace clustering",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2009
    }, {
      "title" : "Matrix rank minimization with applications",
      "author" : [ "M. Fazel" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2002
    }, {
      "title" : "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography",
      "author" : [ "Martin Fischler", "Robert Bolles" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1981
    }, {
      "title" : "Robust estimates, residuals, and outlier detection with multiresponse data",
      "author" : [ "R. Gnanadesikan", "J.R. Kettenring" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1972
    }, {
      "title" : "Recovering low-rank matrices from few coefficients in any basis",
      "author" : [ "D. Gross" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming",
      "author" : [ "Qifa Ke", "Takeo Kanade" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "A framework for robust subspace learning",
      "author" : [ "Fernando De la Torre", "Michael J. Black" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "Robust recovery of subspace structures by low-rank representation",
      "author" : [ "Guangcan Liu", "Zhouchen Lin", "Shuicheng Yan", "Ju Sun", "Yong Yu", "Yi Ma" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Robust subspace segmentation by low-rank representation",
      "author" : [ "Guangcan Liu", "Zhouchen Lin", "Yong Yu" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Exact subspace segmentation and outlier detection by low-rank representation",
      "author" : [ "Guangcan Liu", "Huan Xu", "Shuicheng Yan" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Spectral regularization algorithms for learning large incomplete matrices",
      "author" : [ "Rahul Mazumder", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Low-rank and sparse matrix decomposition for accelerated dynamic mri with separation of background and dynamic components",
      "author" : [ "Ricardo Otazo", "Emmanuel Candès", "Daniel K. Sodickson" ],
      "venue" : "Arxiv,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Rasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images",
      "author" : [ "YiGang Peng", "Arvind Ganesh", "John Wright", "Wenli Xu", "Yi Ma" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Generalization error bounds for collaborative prediction with lowrank matrices",
      "author" : [ "Nathan Srebro", "Tommi Jaakkola" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    }, {
      "title" : "A benchmark for the comparison of 3-d motion segmentation algorithms",
      "author" : [ "Roberto Tron", "Rene Vidal" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2007
    }, {
      "title" : "Generalized Principal Component Analysis",
      "author" : [ "Rene Vidal", "Yi Ma", "S. Sastry" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Cofi rank - maximum margin matrix factorization for collaborative ranking",
      "author" : [ "Markus Weimer", "Alexandros Karatzoglou", "Quoc V. Le", "Alex J. Smola" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Outlier-robust pca: The high-dimensional case",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Shie Mannor" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2013
    }, {
      "title" : "Robust pca via outlier pursuit",
      "author" : [ "Huan Xu", "Constantine Caramanis", "Sujay Sanghavi" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2010
    }, {
      "title" : "Tilt: Transform invariant low-rank textures",
      "author" : [ "Zhengdong Zhang", "Arvind Ganesh", "Xiao Liang", "Yi Ma" ],
      "venue" : "International Journal of Computer Vision,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "The recently established RPCA [4] method provides a convenient way to restore low-rank matrices from grossly corrupted observations.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : ", data with high coherence) could be alleviated by Low-Rank Representation (LRR) [13], provided that the dictionary in LRR is configured appropriately.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "One of the most exciting methods is probably the so-called RPCA (Robust Principal Component Analysis) method [4], which was built upon the exploration of the following low-rank matrix recovery problem:",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "where ‖ · ‖∗ is the nuclear norm [7] of a matrix, ‖ · ‖1 denotes the l1 norm of a matrix seen as a long vector, and λ > 0 is a parameter.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : ", image processing [26], computer vision [18], radar imaging [1], magnetic resonance imaging [17], etc.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : ", image processing [26], computer vision [18], radar imaging [1], magnetic resonance imaging [17], etc.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : ", image processing [26], computer vision [18], radar imaging [1], magnetic resonance imaging [17], etc.",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : ", image processing [26], computer vision [18], radar imaging [1], magnetic resonance imaging [17], etc.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "However, as explained in [4], the coherence parameters are indeed necessary (if there is no additional condition assumed on the data).",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "Our study is based on the following convex program termed Low-Rank Representation (LRR) [13]:",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 12,
      "context" : "⋄ This paper provides insights regarding the LRR model proposed by [13].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "The coherence parameters defined in [3, 4] are excellent exemplars of such quantities.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "The coherence parameters defined in [3, 4] are excellent exemplars of such quantities.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "The analysis in RPCA [4] merges the above three parameters into a single one: μ(L0) = max{μ1(L0), μ2(L0), μ3(L0)}.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "2 μ2-phenomenon According to the analysis in [4], the success condition (regarding L0) of RPCA is rank (L0) ≤ crn2 μ(L0)(logn1)(2) , (3.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "The reason is that, as has been explored by [13], the complexity of solving the LRR problem (1.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "2 Results on Corrupted Motion Sequences We now present our experiment with 11 additional sequences attached to the Hopkins155 [21] database.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "We consider three state-of-the-art subspace clustering methods: Shape Interaction Matrix (SIM) [5], Low-Rank Representation with A = X [14] (which is referred to as “LRRx”) and Sparse Subspace Clustering (SSC) [6].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "We consider three state-of-the-art subspace clustering methods: Shape Interaction Matrix (SIM) [5], Low-Rank Representation with A = X [14] (which is referred to as “LRRx”) and Sparse Subspace Clustering (SSC) [6].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "We consider three state-of-the-art subspace clustering methods: Shape Interaction Matrix (SIM) [5], Low-Rank Representation with A = X [14] (which is referred to as “LRRx”) and Sparse Subspace Clustering (SSC) [6].",
      "startOffset" : 210,
      "endOffset" : 213
    }, {
      "referenceID" : 20,
      "context" : "The problem of recovering coherent data essentially concerns the robustness issues of the Generalized PCA (GPCA) [22] problem.",
      "startOffset" : 113,
      "endOffset" : 117
    } ],
    "year" : 2014,
    "abstractText" : "The recently established RPCA [4] method provides a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA is not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when data are strictly low-rank. This is because RPCA ignores clustering structures of the data which are ubiquitous in applications. As the number of cluster grows, the coherence of data keeps increasing, and accordingly, the recovery performance of RPCA degrades. We show that the challenges raised by coherent data (i.e., data with high coherence) could be alleviated by Low-Rank Representation (LRR) [13], provided that the dictionary in LRR is configured appropriately. More precisely, we mathematically prove that if the dictionary itself is low-rank then LRR is immune to the coherence parameter which increases with the underlying cluster number. This provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments. Experiments on randomly generated matrices and real motion sequences verify our claims. See the full paper at arXiv:1404.4032.",
    "creator" : null
  }
}