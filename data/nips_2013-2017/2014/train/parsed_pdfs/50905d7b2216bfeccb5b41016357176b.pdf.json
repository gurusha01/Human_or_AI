{
  "name" : "50905d7b2216bfeccb5b41016357176b.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Difference of Convex Functions Programming for Reinforcement Learning",
    "authors" : [ "Bilal Piot", "Matthieu Geist", "Olivier Pietquin" ],
    "emails" : [ "bilal.piot@lifl.fr,", "matthieu.geist@supelec.fr,", "olivier.pietquin@univ-lille1.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Large Markov Decision Processes are usually solved using Approximate Dynamic Programming methods such as Approximate Value Iteration or Approximate Policy Iteration. The main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so, we study the minimization of a norm of the Optimal Bellman Residual (OBR) T ∗Q − Q, where T ∗ is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally, we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning problem."
    }, {
      "heading" : "1 Introduction",
      "text" : "This paper addresses the problem of solving large state-space Markov Decision Processes (MDPs)[16] in an infinite time horizon and discounted reward setting. The classical methods to tackle this problem, such as Approximate Value Iteration (AVI) or Approximate Policy Iteration (API) [6, 16]1, are derived from Dynamic Programming (DP). Here, we propose an alternative path. The idea is to search directly a function Q for which T ∗Q ≈ Q, where T ∗ is the optimal Bellman operator, by minimizing a norm of the Optimal Bellman Residual (OBR) T ∗Q−Q. First, in Sec. 2.2, we show that the OBR Minimization (OBRM) is interesting, as it can serve as a proxy for the optimal action-value function estimation. Then, in Sec. 3, we prove that minimizing an empirical norm of the OBR is consistant in the Vapnick sense (this justifies working with sampled transitions). However, this empirical norm of the OBR is not convex. We hypothesize that this is why this approach is not studied in the literature (as far as we know), a notable exception being the work of Baird [5]. Therefore, our main contribution, presented in Sec. 4, is to show that this minimization can be framed as a minimization of a Difference of Convex functions (DC) [11]. Thus, a large literature on Difference of Convex functions Algorithms (DCA) [19, 20](a rather standard approach to non-convex programming) is available to solve our problem. Finally in Sec. 5, we conduct a generic experiment that compares a naive implementation of our approach to API and AVI methods, showing that it is competitive.\n1Others methods such as Approximate Linear Programming (ALP) [7, 8] or Dynamic Policy Programming (DPP) [4] address the same problem. Yet, they also rely on DP."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 MDP and ADP",
      "text" : "Before describing the framework of MDPs in the infinite-time horizon and discounted reward setting, we give some general notations. Let (R, |.|) be the real space with its canonical norm and X a finite set, RX is the set of functions from X to R. The set of probability distributions over X is noted ∆X . Let Y be a finite set, ∆YX is the set of functions from Y to ∆X . Let α ∈ RX , p ≥ 1 and ν ∈ ∆X , we define the Lp,ν-semi-norm of α, noted ‖α‖p,ν , by: ‖α‖p,ν = ( ∑ x∈X ν(x)|α(x)|p) 1 p . In addition, the infinite norm is noted ‖α‖∞ and defined as ‖α‖∞ = maxx∈X |α(x)|. Let v be a random variable which takes its values in X, v ∼ ν means that the probability that v = x is ν(x). Now, we provide a brief summary of some of the concepts from the theory of MDP and ADP [16]. Here, the agent is supposed to act in a finite MDP 2 represented by a tuple M = {S,A,R, P, γ} where S = {si}1≤i≤NS is the state space, A = {ai}1≤i≤NA is the action space, R ∈ RS×A is the reward function, γ ∈]0, 1[ is a discount factor and P ∈ ∆S×AS is the Markovian dynamics which gives the probability, P (s′|s, a), to reach s′ by choosing action a in state s. A policy π is an element of AS and defines the behavior of an agent. The quality of a policy π is defined by the action-value function. For a given policy π, the action-value function Qπ ∈ RS×A is defined as Qπ(s, a) = Eπ[ ∑+∞ t=0 γ\ntR(st, at)], where Eπ is the expectation over the distribution of the admissible trajectories (s0, a0, s1, π(s1), . . . ) obtained by executing the policy π starting from s0 = s and a0 = a. Moreover, the function Q∗ ∈ RS×A defined as Q∗ = maxπ∈AS Qπ is called the optimal action-value function. A policy π is optimal if ∀s ∈ S,Qπ(s, π(s)) = Q∗(s, π(s)). A policy π is said greedy with respect to a function Q if ∀s ∈ S, π(s) ∈ argmaxa∈AQ(s, a). Greedy policies are important because a policy π greedy with respect to Q∗ is optimal. In addition, as we work in the finite MDP setting, we define, for each policy π, the matrix Pπ of size NSNA ×NSNA with elements Pπ((s, a), (s′, a′)) = P (s′|s, a)1{π(s′)=a′}. Let ν ∈ ∆S×A, we note νPπ ∈ ∆S×A the distribution such that (νPπ)(s, a) = ∑ (s′,a′)∈S×A ν(s′, a′)Pπ((s′, a′), (s, a)). Finally, Qπ and Q∗ are known to be fixed points of the contracting operators Tπ and T ∗ respectively:\n∀Q ∈ RS×A,∀(s, a) ∈ S ×A, TπQ(s, a) = R(s, a) + γ ∑ s′∈S P (s′|s, a)Q(s, π(s′)),\n∀Q ∈ RS×A,∀(s, a) ∈ S ×A, T ∗Q(s, a) = R(s, a) + γ ∑ s′∈S P (s′|s, a) max b∈A Q(s, b).\nWhen the state space becomes large, two important problems arise to solve large MDPs. The first one, called the representation problem, is that an exact representation of the values of the action-value functions is impossible, so these functions need to be represented with a moderate number of coefficients. The second problem, called the sample problem, is that there is no direct access to the Bellman operators but only samples from them. One solution for the representation problem is to linearly parameterize the action-value functions thanks to a basis of d ∈ N∗ functions φ = (φi)di=1 where φi ∈ RS×A. In addition, we define for each state-action couple (s, a) the vector φ(s, a) ∈ Rd such that φ(s, a) = (φi(s, a))di=1. Thus, the action-value functions are characterized by a vector θ ∈ Rd and noted Qθ :\n∀θ ∈ Rd,∀(s, a) ∈ S ×A,Qθ(s, a) = d∑ i=1 θiφi(s, a) = 〈θ, φ(s, a)〉,\nwhere 〈., .〉 is the canonical dot product of Rd. The usual framework to solve large MDPs are for instance AVI and API. AVI consists in processing a sequence (QAVIθn )n∈N where θ0 ∈ R d and ∀n ∈ N, QAVIθn+1 ≈ T ∗QAVIθn . API consists in processing two sequences (QAPIθn )n∈N and (π API n )n∈N where πAPI0 ∈ AS , ∀n ∈ N, QAPIθn ≈\n2This work could be easily extended to measurable state spaces as in [9]; we choose the finite case for the ease and clarity of exposition.\nTπnQAPIθn and π API n+1 is greedy with respect to QAPIθn . The approximation steps in AVI and API generate the sequences of errors ( AVIn = T ∗QAVIθn −Q AVI θn+1\n)n∈N and ( APIn = TπnQAPIθn − QAPIθn )n∈N respectively. Those approximation errors are due to both the representation and the sample problems and can be made explicit for specific implementations of those methods [14, 1]. These ALP methods are legitimated by the following bound [15, 9]:\nlim sup n→∞\n‖Q∗ −Qπ API\\AVI n ‖p,ν ≤ 2γ (1− γ)2C2(ν, µ) 1 p API\\AVI, (1)\nwhere πAPI\\AVIn is greedy with respect to QAPI\\AVIθn , API\\AVI = supn∈N ‖ API\\AVI n ‖p,µ and C2(ν, µ) is a second order concentrability coefficient, C2(ν, µ) = (1− γ) ∑ m≥1 mγ\nm−1c(m), where c(m) = maxπ1,...,πm,(s,a)∈S×A (νPπ1Pπ2 ...Pπm )(s,a) µ(s,a) . In the next section, we compare the bound Eq. (1) with a similar bound derived from the OBR minimization approach in order to justify it."
    }, {
      "heading" : "2.2 Why minimizing the OBR?",
      "text" : "The aim of Dynamic Programming (DP) is, given an MDP M , to find Q∗ which is equivalent to minimizing a certain norm of the OBR Jp,µ(Q) = ‖T ∗Q−Q‖p,µ where µ ∈ ∆S×A is such that ∀(s, a) ∈ S × A,µ(s, a) > 0 and p ≥ 1. Indeed, it is trivial to verify that the only minimizer of Jp,µ is Q∗. Moreover, we have the following bound given by Th. 1. Theorem 1. Let ν ∈ ∆S×A, µ ∈ ∆S×A, π̂ ∈ AS and C1(ν, µ, π̂) ∈ [1,+∞[∪{+∞} the smallest constant verifying (1− γ)ν ∑ t≥0 γ tP tπ̂ ≤ C1(ν, µ, π̂)µ, then:\n∀Q ∈ RS×A, ‖Q∗ −Qπ‖p,ν ≤ 2\n1− γ\n( C1(ν, µ, π) + C1(ν, µ, π∗)\n2\n) 1 p\n‖T ∗Q−Q‖p,µ, (2)\nwhere π is greedy with respect to Q and π∗ is any optimal policy.\nProof. A proof is given in the supplementary file. Similar results exist [15].\nIn Reinforcement Leaning (RL), because of the representation and the sample problems, minimizing ‖T ∗Q − Q‖p,µ over RS×A is not possible (see Sec. 3 for details), but we can consider that our approach provides us a function Q such that T ∗Q ≈ Q and define the error OBRM = ‖T ∗Q−Q‖p,µ. Thus, via Eq. (2), we have:\n‖Q∗ −Qπ‖p,ν ≤ 2\n1− γ\n( C1(ν, µ, π) + C1(ν, µ, π∗)\n2\n) 1 p\nOBRM, (3)\nwhere π is greedy with respect to Q. This bound has the same form as the one of API and AVI described in Eq. (1) and the Tab. 1 allows comparing them. This bound has two\nadvantages over API\\AVI. First, the horizon term 21−γ is better than the horizon term 2γ (1−γ)2 as long as γ > 0.5, which is the usual case. Second, the concentrability term C1(ν,µ,π)+C1(ν,µ,π∗) 2 is considered better that C2(ν, µ), mainly because if C2(ν, µ) < +∞ then C1(ν,µ,π)+C1(ν,µ,π\n∗) 2 < +∞, the contrary being not true (see [17] for a discussion about\nthe comparison of these concentrability coefficients). Thus, the bound Eq. (3) justifies the minimization of a norm of the OBR, as long as we are able to control the error term OBRM."
    }, {
      "heading" : "3 Vapnik-Consistency of the empirical norm of the OBR",
      "text" : "When the state space is too large, it is not possible to minimize directly ‖T ∗Q − Q‖p,µ, as we need to compute T ∗Q(s, a) for each couple (s, a) (sample problem). However, we can consider the case where we choose N samples represented by N independent and identically distributed random variables (Si, Ai)1≤i≤N such that (Si, Ai) ∼ µ and minimize ‖T ∗Q − Q‖p,µN where µN is the empirical distribution µN (s, a) = 1N ∑N i=1 1{(Si,Ai)=(s,a)}. An important question (answered below) is to know if controlling the empirical norm allows controlling the true norm of interest (consistency in the Vapnik sense [22]), and at what rate convergence occurs.\nComputing ‖T ∗Q−Q‖p,µN = ( 1N ∑N i=1 |T ∗Q(Si, Ai)−Q(Si, Ai)|p) 1 p is tractable if we consider that we can compute T ∗Q(Si, Ai) which means that we have a perfect knowledge of the dynamics P and that the number of next states for the state-action couple (Si, Ai) is not too large. In Sec. 4.3, we propose different solutions to evaluate T ∗Q(Si, Ai) when the number of next states is too large or when the dynamics is not provided. Now, the natural question is to what extent minimizing ‖T ∗Q − Q‖p,µN corresponds to minimizing ‖T ∗Q − Q‖p,µ. In addition, we cannot minimize ‖T ∗Q − Q‖p,µN over RS×A as this space is too large (representation problem) but over the space {Qθ ∈ RS×A, θ ∈ Rd}. Moreover, as we are looking for a function such that Qθ = Q∗, we can limit our search to the functions satisfying ‖Qθ‖∞ ≤ ‖R‖∞1−γ . Thus, we search for a function Q in the hypothesis space Q = {Qθ ∈ RS×A, θ ∈ Rd, ‖Qθ‖∞ ≤ ‖R‖∞1−γ }, in order to minimize ‖T\n∗Q − Q‖p,µN . Let QN ∈ argminQ∈Q ‖T ∗Q − Q‖p,µN be a minimizer of the empirical norm of the OBR, we want to know to what extent the empirical error ‖T ∗QN − QN‖p,µN is related to the real error OBRM = ‖T ∗QN −QN‖p,µ. The answer for deterministic-finite MPDs relies in Th. 2 (the continuous-stochastic MDP case being discussed shortly after). Theorem 2. Let η ∈]0, 1[ and M be a finite deterministic MDP, with probability at least 1− η, we have:\n∀Q ∈ Q, ‖T ∗Q−Q‖pp,µ ≤ ‖T ∗Q−Q‖pp,µN + 2‖R‖∞ 1− γ\n√ ε(N),\nwhere ε(N) = h(ln( 2N h )+1)+ln( 4 η )\nN and h = 2NA(d+ 1). With probability at least 1− 2η:\nOBRM = ‖T ∗QN −QN‖pp,µ ≤ B + 2‖R‖∞ 1− γ\n(√ ε(N) + √ ln(1/η)\n2N\n) ,\nwhere B = minQ∈Q ‖T ∗Q−Q‖pp,µ is the error due to the choice of features.\nProof. The complete proof is provided in the supplementary file. It mainly consists in computing the Vapnik-Chervonenkis dimension of the residual.\nThus, if we were able to compute a function such as QN , we would have, thanks to Eq .(2) and Th. 2: ‖Q∗−QπN ‖p,ν ≤ ( C1(ν, µ, πN ) + C1(ν, µ, π∗)\n1− γ\n) 1 p ( B + 2‖R‖∞1− γ (√ ε(N) + √ ln(1/η) 2N )) 1 p .\nwhere πN is greedy with respect to QN . The error term OBRM is explicitly controlled by\ntwo terms B , a term of bias, and 2‖R‖∞1−γ\n(√ ε(N) + √ ln(1/η)\n2N\n) a term of variance. The\nterm B = minQ∈Q ‖T ∗Q −Q‖pp,µ is relative to the representation problem and is fixed by the choice of features. The term of variance is decreasing at the speed √ 1 N .\nA similar bound can be obtained for non-deterministic continuous-state MDPs with finite number of actions where the state space is a compact set in a metric space, the features\n(φi)di=1 are Lipschitz and for each state-action couple the next states belongs to a ball of fixed radius. The proof is a simple extension of the one given in the supplementary material. Those continuous MDPs are representative of real dynamical systems. Now that we know that minimizing ‖T ∗Q −Q‖pp,µN allows controlling ‖Q\n∗ −QπN ‖p,ν , the question is how do we frame this optimization problem. Indeed ‖T ∗Q − Q‖pp,µN is a non-convex and a nondifferentiable function with respect to Q, thus a direct minimization could lead us to bad solutions. In the next section, we propose a method to alleviate those difficulties."
    }, {
      "heading" : "4 Reduction to a DC problem",
      "text" : "Here, we frame the minimization of the empirical norm of the OBR as a DC problem and instantiate a general algorithm, DCA [20], that tries to solve it. First, we provide a short introduction to difference of convex functions."
    }, {
      "heading" : "4.1 DC background",
      "text" : "Let E be a finite dimensional Hilbert space and 〈., .〉E , ‖.‖E its dot product and norm respectively. We say that a function f ∈ RE is DC if there exists g, h ∈ RE which are convex and lower semi-continuous such that f = g − h. The set of DC functions is noted DC(E) and is stable to most of the operations that can be encountered in optimization, contrary to the set of convex functions. Indeed, let (fi)Ki=1 be a sequence of K ∈ N∗ DC functions and (αi)Ki=1 ∈ RK then ∑K i=1 αifi, ∏K i=1 fi, min1≤i≤K fi, max1≤i≤K fi and |fi| are DC functions [11]. In order to minimize a DC function f = g − h, we need to define a notion of differentiability for convex and lower semi-continuous functions. Let g be such a function and e ∈ E, we define the sub-gradient ∂eg of g in e as:\n∂eg = {δ ∈ E,∀e′ ∈ E, g(e′) ≥ g(e) + 〈e′ − e, δ〉E}. For a convex and lower semi-continuous g ∈ RE , the sub-gradient ∂eg is non empty for all e ∈ E [11]. This observation leads to a minimization method of a function f ∈ DC(E) called Difference of Convex functions Algorithm (DCA). Indeed, as f is DC, we have:\n∀(e, e′) ∈ E2, f(e′) = g(e′)− h(e′) ≤ (a) g(e′)− h(e)− 〈e′ − e, δ〉E ,\nwhere δ ∈ ∂eh and inequality (a) is true by definition of the sub-gradient. Thus, for all e ∈ E, the function f is upper bounded by a function fe ∈ RE defined for all e′ ∈ E by fe(e′) = g(e′)− h(e)− 〈e′ − e, δ〉E . The function fe is a convex and lower semi-continuous function (as it is the sum of two convex and lower semi-continuous functions which are g and the linear function ∀e′ ∈ E, 〈e − e′, δ〉E − h(e)). In addition, those functions have the particular property that ∀e ∈ E, f(e) = fe(e). The set of convex functions (fe)e∈E that upper-bound the function f plays a key role in DCA. The algorithm DCA [20] consists in constructing a sequence (en)n∈N such that the sequence (f(en))n∈N decreases. The first step is to choose a starting point e0 ∈ E, then we minimize the convex function fe0 that upper-bounds the function f . We note e1 a minimizer of fe0 , e1 ∈ argmine∈E fe0 . This minimization can be realized by any convex optimization solver. As f(e0) = fe0(e0) ≥ fe0(e1) and fe0(e1) ≥ f(e1), then f(e0) ≥ f(e1). Thus, if we construct the sequence (en)n∈N such that ∀n ∈ N, en+1 ∈ argmine∈E fen and e0 ∈ E, then we obtain a decreasing sequence (f(en))n∈N. Therefore, the algorithm DCA solves a sequence of convex optimization problems in order to solve a DC optimization problem. Three important choices can radically change the DCA performance: the first one is the explicit choice of the decomposition of f , the second one is the choice of the starting point e0 and finally the choice of the intermediate convex solver. The DCA algorithm hardly guarantee convergence to the global optima, but it usually provides good solutions. Moreover, it has some nice properties when one of the functions g or h is polyhedral. A function g is said polyhedral when ∀e ∈ E, g(e) = max1≤i≤K [〈αi, e〉H + βi], where (αi)Ki=1 ∈ EK and (βi)Ki=1 ∈ RK . If one of the function g, h is polyhedral, f is under bounded and the DCA sequence (en)n∈N is bounded, the DCA algorithm converges in finite time to a local minima. The finite time aspect is quite interesting in term of implementation. More details about DC programming and DCA are given in [20] and even conditions for convergence to the global optima."
    }, {
      "heading" : "4.2 The OBR minimization framed as a DC problem",
      "text" : "A first important result is that for any choice of p ≥ 1, the OBRM is actually a DC problem. Theorem 3. Let Jpp,µN (θ) = ‖T\n∗Qθ −Qθ‖p,µN be a function from Rd to reals, Jpp,µN (θ) is a DC functions when p ∈ N∗.\nProof. Let us write Jpp,µN as:\nJpp,µN (θ) = 1 N N∑ i=1 |〈φ(Si, Ai), θ〉 −R(Si, Ai)− γ ∑ s′∈S P (s′|Si, Ai) max a∈A 〈φ(s′, a), θ〉|p.\nFirst, as for each (Si, Ai) the linear function 〈φ(Si, Ai), .〉 is convex and continuous, the affine function gi = 〈φ(Si, Ai), .〉 + R(Si, Ai) is convex and continuous. Therefore, the function maxa∈A〈φ(s′, a), .〉 is also convex and continuous as a finite maximum of convex and continuous functions. In addition, the function hi = γ ∑ s′∈S P (s′|Si, Ai) maxa∈A〈φ(s′, a), .〉| is convex and continuous as a positively weighted finite sum of convex and continuous functions. Thus, the function fi = gi − hi is a DC function. As an absolute value of a DC function is DC, a finite product of DC functions is DC and a weighted sum of DC functions is DC, then Jpp,µN = 1 N ∑N i=1 |fi|p is a DC function.\nHowever, knowing that Jpp,µN is DC is not sufficient in order to use the DCA algorithm. Indeed, we need an explicit decomposition of Jpp,µN as a difference of two convex functions. We present two polyhedral explicit decompositions of Jpp,µN when p = 1 and when p = 2. Theorem 4. There exists explicit polyhedral decompositions of Jpp,µN when p = 1 and p = 2.\nFor p = 1: J1,µN = G1,µN − H1,µN , where G1,µN = 1N ∑N i=1 2 max(gi, hi)\nand H1,µN = 1N ∑N i=1(gi + hi), with gi = 〈φ(Si, Ai), .〉 + R(Si, Ai) and hi =\nγ ∑ s′∈S P (s′|Si, Ai) maxa∈A〈φ(s′, a), .〉.\nFor p = 2: J22,µN = G2,µN − H2,µN , where G2,µN = 1 N ∑N i=1[g2i + h 2 i ] and H2,µN = 1 N ∑N i=1[gi + hi]2 with:\ngi = max(gi, hi) + gi − ( 〈φ(Si, Ai) + γ ∑ s′∈S P (s′|Si, Ai)φ(s′, a1), .〉 −R(Si, Ai) ) ,\nhi = max(gi, hi) + hi − ( 〈φ(Si, Ai) + γ\n∑ s′∈S\nP (s′|Si, Ai)φ(s′, a1), .〉 −R(Si, Ai) ) .\nProof. The proof is provided in the supplementary material.\nUnfortunately, there is currently no guarantee that DCA applied to Jpp,µN = Gp,µN −Hp,µN outputs QN ∈ argminQ∈Q ‖T ∗Q−Q‖p,µN . The error between the output Q̂N of DCA and QN is not studied here but it is a nice theoretical perspective for future works."
    }, {
      "heading" : "4.3 The batch scenario",
      "text" : "Previously, we admit that it was possible to calculate T ∗Q(s, a) = R(s, a) + γ ∑ s′∈S P (s′|s, a) maxb∈AQ(s′, b). However, if the number of next states s′ for a given couple (s, a) is too large or if T ∗ is unknown, this can be intractable. A solution, when we have a simulator, is to generate for each couple (Si, Ai) a set of N ′ samples (S′i,j)N ′\nj=1 and provide a non-biased estimation of T ∗Q(Si, Ai): T̂ ∗Q(Si, Ai) = R(Si, Ai) + γ 1N ′ ∑N ′ j=1 maxa∈AQ(S′i,j , a). Even if |T̂ ∗Q(Si, ai) − Q(Si, Ai)|p is a biased estimator of |T ∗Q(Si, Ai)−Q(Si, Ai)|p, this biais can be controlled by the number of samples N ′. In the case where we do not have such a simulator, but only sampled transitions (Si, Ai, S′i)Ni=1 (the batch scenario), it is possible to provide a non-biased estimation of\nT ∗Q(Si, Ai) via: T̂ ∗Q(Si, Ai) = R(Si, Ai) + γmaxb∈AQ(S′i, b). However in that case, |T̂ ∗Q(Si, Ai) − Q(Si, Ai)|p is a biased estimator of |T ∗Q(Si, Ai) − Q(Si, Ai)|p and the biais is uncontrolled [2]. In order to alleviate this typical problem from the batch scenario, several techniques have been proposed in the literature to provide a better estimator |T̂ ∗Q(Si, Ai) − Q(Si, Ai)|p, such as embeddings in Reproducing Kernel Hilbert Spaces (RKHS)[13] or locally weighted averager such as Nadaraya-Watson estimators[21]. In both cases, the non-biased estimation of T ∗Q(Si, Ai) takes the form T̂ ∗Q(Si, Ai) = R(Si, Ai) + γ 1N ∑N j=1 βi(S′j) maxa∈AQ(S′j , a), where βi(S′j) represents the weight of the samples S′j in the estimation of T ∗Q(Si, Ai). To obtain an explicit DC decomposition, when p = 1 or p = 2, of Ĵpp,µN (θ) = 1 N ∑N i=1 |T̂ ∗Qθ(Si, Ai) − Qθ(Si, Ai)|p it is suffi-\ncient to replace ∑ s′∈S P (s′|Si, Ai) maxa∈A〈φ(s′, a), θ〉 by 1 N ∑N j=1 βi(S′j) maxa∈AQ(S′j , a)\n(or 1N ′ ∑N ′ j=1 maxa∈AQ(S′i,j , a) if we have a simulator) in the DC decomposition of Jpp,µN ."
    }, {
      "heading" : "5 Illustration",
      "text" : "This experiment focuses on stationary Garnet problems, which are a class of randomly constructed finite MDPs representative of the kind of finite MDPs that might be encountered in practice [3]. A stationary Garnet problem is characterized by 3 parameters: Garnet(NS , NA, NB). The parameters NS and NA are the number of states and actions respectively, and NB is a branching factor specifying the number of next states for each state-action pair. Here, we choose a particular type of Garnets which presents a topological structure relative to real dynamical systems and aims at simulating the behavior of a smooth continuous-state MDPs (as described in Sec. 3). Those systems are generally multi-dimensional state spaces MDPs where an action leads to different next states close to each other. The fact that an action leads to close next states can model the noise in a real system for instance. Thus, problems such as the highway simulator [12], the mountain car or the inverted pendulum (possibly discretized) are particular cases of this type of Garnets. For those particular Garnets, the state space is composed of d dimensions (d = 2 in this particular experiment) and each dimension i has a finite number of elements xi (xi = 10). So, a state s = [s1, s2, .., si, .., sd] is a d-uple where each composent si can take a finite value between 1 and xi. In addition, the distance between two states s, s′ is ‖s− s′‖2 = ∑i=d i=1(si− s′i)2. Thus, we obtain MDPs with a state space size of ∏d i=1 xi. The number of actions is NA = 5. For each state action couple (s, a), we choose randomly NB next states (NB = 5) via a Gaussian distribution of d dimensions centered in s where the covariance matrix is the identity matrix of size d, Id, multiply by a term σ (here σ = 1). This allows handling the smoothness of the MDP: if σ is small the next states s′ are close to s and if σ is large, the next states s′ can be very far form each other and also from s. The probability of going to each next state s′ is generated by partitioning the unit interval at NB − 1 cut points selected randomly. For each couple (s, a), the reward R(s, a) is drawn uniformly between −1 and 1. For each Garnet problem, it is possible to compute an optimal policy π∗ thanks to the policy iteration algorithm. In this experiment, we construct 50 Garnets {Gp}1≤p≤50 as explained before. For each Garnet Gp, we build 10 data sets {Dp,q}1≤q≤10 composed of N sampled transitions (si, ai, s′i)Ni=1 drawn uniformly and independently. Thus, we are in the batch scenario. The minimization of J1,N and J2,N via the DCA algorithms, where the estimation of T ∗Q(si, ai) is done via R(si, ai) + γmaxb∈AQ(s′i, b) (so uncontrolled biais), are called DCA1 and DCA2 respectively. The initialisation of DCA is θ0 = 0 and the intermediary optimization convex problems are solved by a sub-gradient descent [18]. Those two algorithms are compared with state-of the art Reinforcement Learning algorithms which are LSPI (API implementation) and Fitted-Q (AVI implementation). The four algorithms uses the tabular basis. Each algorithm outputs a function Qp,qA ∈ RS×A and the policy associated to Q p,q A is πp,qA (s) = argmaxa∈AQ p,q A (s, a). In order to quantify the performance of a given algorithm, we calculate the criterion T p,qA = Eρ[V π ∗ −V π p,q A ] Eρ[|V π∗ |] , where V π p,q A is computed via the policy\nevaluation algorithm. The mean performance criterion TA is 1500 ∑50 p=1 ∑10 q=1 T p,q A . We also\ncalculate, for each algorithm, the variance criterion stdpA = 110 ∑10 q=1(T p,q A − 1 10 ∑10 q=1 T p,q A )2\nand the resulting mean variance criterion is stdA = 150 ∑50 p=1 std p A. In Fig. 1(a), we plot the performance versus the number of samples. We observe that the 4 algorithms have similar performances, which shows that our alternative approach is competitive. In Fig. 1(b), we\nplot the standard deviation versus the number of samples. Here, we observe that DCA algorithms have less variance which is an advantage. This experiment shows us that DC programming is relevant for RL but still has to prove its efficiency on real problems."
    }, {
      "heading" : "6 Conclusion and Perspectives",
      "text" : "In this paper, we presented an alternative approach to tackle the problem of solving large MDPs by estimating the optimal action-value function via DC Programming. To do so, we first showed that minimizing a norm of the OBR is interesting. Then, we proved that the empirical norm of the OBR is consistant in the Vapnick sense (strict consistency). Finally, we framed the minimization of the empirical norm as DC minimization which allows us to rely on the literature on DCA. We conduct a generic experiment with a basic setting for DCA as we choose a canonical explicit decomposition of our DC functions criterion and a sub-gradient descent in order to minimize the intermediary convex minimization problems. We obtain similar results to AVI and API. Thus, an interesting perspective would be to have a less naive setting for DCA by choosing different explicit decompositions and find a better convex solver for the intermediary convex minimizations problems. Another interesting perspective is that our approach can be non-parametric. Indeed, as pointed in [10] a convex minimization problem can be solved via boosting techniques which avoids the choice of features. Therefore, each intermediary convex problem of DCA could be solved via a boosting technique and hence make DCA non-parametric. Thus, seeing the RL problem as a DC problem provides some interesting perspectives for future works."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The research leading to these results has received partial funding from the European Union Seventh Framework Program (FP7/2007-2013) under grant agreement number 270780 and the ANR ContInt program (MaRDi project, number ANR- 12-CORD-021 01). We also would like to thank professors Le Thi Hoai An and Pham Dinh Tao for helpful discussions about DC programming."
    } ],
    "references" : [ {
      "title" : "Fitted-Q iteration in continuous action-space MDPs",
      "author" : [ "A. Antos", "R. Munos", "C. Szepesvári" ],
      "venue" : "In Proc. of NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Learning near-optimal policies with Bellmanresidual minimization based fitted policy iteration and a single sample path",
      "author" : [ "A. Antos", "C. Szepesvári", "R. Munos" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "On the generation of Markov decision processes",
      "author" : [ "T. Archibald", "K. McKinnon", "L. Thomas" ],
      "venue" : "Journal of the Operational Research Society,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1995
    }, {
      "title" : "Dynamic policy programming",
      "author" : [ "M.G. Azar", "V. Gómez", "H.J Kappen" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Residual algorithms: reinforcement learning with function approximation",
      "author" : [ "L. Baird" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1995
    }, {
      "title" : "Dynamic programming and optimal control, volume 1",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1995
    }, {
      "title" : "The linear programming approach to approximate dynamic programming",
      "author" : [ "D.P. de Farias", "B. Van Roy" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "A smoothed approximate linear program",
      "author" : [ "Vijay Desai", "Vivek Farias", "Ciamac C Moallemi" ],
      "venue" : "In Proc. of NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Error propagation for approximate policy and value iteration",
      "author" : [ "A. Farahmand", "R. Munos", "Csaba. Szepesvári" ],
      "venue" : "Proc. of NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Generalized boosting algorithms for convex optimization",
      "author" : [ "A. Grubb", "J.A. Bagnell" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Generalized differentiability, duality and optimization for problems dealing with differences of convex functions",
      "author" : [ "J.B Hiriart-Urruty" ],
      "venue" : "In Convexity and duality in optimization. Springer,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1985
    }, {
      "title" : "Inverse reinforcement learning through structured classification",
      "author" : [ "E. Klein", "M. Geist", "B. Piot", "O. Pietquin" ],
      "venue" : "In Proc. of NIPS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Modelling transition dynamics in MDPs with RKHS embeddings",
      "author" : [ "G. Lever", "L. Baldassarre", "A. Gretton", "M. Pontil", "S. Grünewälder" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Finite-sample analysis of Bellman residual minimization",
      "author" : [ "O. Maillard", "R. Munos", "A. Lazaric", "M. Ghavamzadeh" ],
      "venue" : "In Proc. of ACML,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Performance bounds in Lp-norm for approximate value iteration",
      "author" : [ "R. Munos" ],
      "venue" : "SIAM journal on control and optimization,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Markov decision processes: discrete stochastic dynamic programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1994
    }, {
      "title" : "Approximate policy iteration schemes: a comparison",
      "author" : [ "B. Scherrer" ],
      "venue" : "In Proc. of ICML,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Minimization methods for nondifferentiable functions",
      "author" : [ "N.Z. Shor", "K.C. Kiwiel", "A. Ruszcaynski" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1985
    }, {
      "title" : "Convex analysis approach to DC programming: theory, algorithms and applications",
      "author" : [ "P.D. Tao", "L.T.H. An" ],
      "venue" : "Acta Mathematica Vietnamica,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1997
    }, {
      "title" : "The DC programming and DCA revisited with DC models of real world nonconvex optimization problems",
      "author" : [ "P.D. Tao", "L.T.H. An" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2005
    }, {
      "title" : "Value function approximation in noisy environments using locally smoothed regularized approximate linear programs",
      "author" : [ "G. Taylor", "R. Parr" ],
      "venue" : "In Proc. of UAI,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Statistical learning theory",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "This paper addresses the problem of solving large state-space Markov Decision Processes (MDPs)[16] in an infinite time horizon and discounted reward setting.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "The classical methods to tackle this problem, such as Approximate Value Iteration (AVI) or Approximate Policy Iteration (API) [6, 16]1, are derived from Dynamic Programming (DP).",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 15,
      "context" : "The classical methods to tackle this problem, such as Approximate Value Iteration (AVI) or Approximate Policy Iteration (API) [6, 16]1, are derived from Dynamic Programming (DP).",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "We hypothesize that this is why this approach is not studied in the literature (as far as we know), a notable exception being the work of Baird [5].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "4, is to show that this minimization can be framed as a minimization of a Difference of Convex functions (DC) [11].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "Thus, a large literature on Difference of Convex functions Algorithms (DCA) [19, 20](a rather standard approach to non-convex programming) is available to solve our problem.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 19,
      "context" : "Thus, a large literature on Difference of Convex functions Algorithms (DCA) [19, 20](a rather standard approach to non-convex programming) is available to solve our problem.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 6,
      "context" : "1Others methods such as Approximate Linear Programming (ALP) [7, 8] or Dynamic Policy Programming (DPP) [4] address the same problem.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "1Others methods such as Approximate Linear Programming (ALP) [7, 8] or Dynamic Policy Programming (DPP) [4] address the same problem.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : "1Others methods such as Approximate Linear Programming (ALP) [7, 8] or Dynamic Policy Programming (DPP) [4] address the same problem.",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "Now, we provide a brief summary of some of the concepts from the theory of MDP and ADP [16].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 8,
      "context" : "API consists in processing two sequences (QAPI θn )n∈N and (π API n )n∈N where πAPI 0 ∈ A , ∀n ∈ N, QAPI θn ≈ 2This work could be easily extended to measurable state spaces as in [9]; we choose the finite case for the ease and clarity of exposition.",
      "startOffset" : 179,
      "endOffset" : 182
    }, {
      "referenceID" : 13,
      "context" : "Those approximation errors are due to both the representation and the sample problems and can be made explicit for specific implementations of those methods [14, 1].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "Those approximation errors are due to both the representation and the sample problems and can be made explicit for specific implementations of those methods [14, 1].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : "These ALP methods are legitimated by the following bound [15, 9]:",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "These ALP methods are legitimated by the following bound [15, 9]:",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : "Second, the concentrability term C1(ν,μ,π)+C1(ν,μ,π) 2 is considered better that C2(ν, μ), mainly because if C2(ν, μ) < +∞ then C1(ν,μ,π)+C1(ν,μ,π ∗) 2 < +∞, the contrary being not true (see [17] for a discussion about the comparison of these concentrability coefficients).",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 21,
      "context" : "An important question (answered below) is to know if controlling the empirical norm allows controlling the true norm of interest (consistency in the Vapnik sense [22]), and at what rate convergence occurs.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 19,
      "context" : "Here, we frame the minimization of the empirical norm of the OBR as a DC problem and instantiate a general algorithm, DCA [20], that tries to solve it.",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 10,
      "context" : "Indeed, let (fi)i=1 be a sequence of K ∈ N∗ DC functions and (αi)i=1 ∈ R then ∑K i=1 αifi, ∏K i=1 fi, min1≤i≤K fi, max1≤i≤K fi and |fi| are DC functions [11].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "For a convex and lower semi-continuous g ∈ R , the sub-gradient ∂eg is non empty for all e ∈ E [11].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "The algorithm DCA [20] consists in constructing a sequence (en)n∈N such that the sequence (f(en))n∈N decreases.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 19,
      "context" : "More details about DC programming and DCA are given in [20] and even conditions for convergence to the global optima.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 1,
      "context" : "However in that case, |T̂ Q(Si, Ai) − Q(Si, Ai)| is a biased estimator of |T Q(Si, Ai) − Q(Si, Ai)| and the biais is uncontrolled [2].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : "In order to alleviate this typical problem from the batch scenario, several techniques have been proposed in the literature to provide a better estimator |T̂ Q(Si, Ai) − Q(Si, Ai)|, such as embeddings in Reproducing Kernel Hilbert Spaces (RKHS)[13] or locally weighted averager such as Nadaraya-Watson estimators[21].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 20,
      "context" : "In order to alleviate this typical problem from the batch scenario, several techniques have been proposed in the literature to provide a better estimator |T̂ Q(Si, Ai) − Q(Si, Ai)|, such as embeddings in Reproducing Kernel Hilbert Spaces (RKHS)[13] or locally weighted averager such as Nadaraya-Watson estimators[21].",
      "startOffset" : 312,
      "endOffset" : 316
    }, {
      "referenceID" : 2,
      "context" : "This experiment focuses on stationary Garnet problems, which are a class of randomly constructed finite MDPs representative of the kind of finite MDPs that might be encountered in practice [3].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "Thus, problems such as the highway simulator [12], the mountain car or the inverted pendulum (possibly discretized) are particular cases of this type of Garnets.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "The initialisation of DCA is θ0 = 0 and the intermediary optimization convex problems are solved by a sub-gradient descent [18].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : "Indeed, as pointed in [10] a convex minimization problem can be solved via boosting techniques which avoids the choice of features.",
      "startOffset" : 22,
      "endOffset" : 26
    } ],
    "year" : 2014,
    "abstractText" : "Large Markov Decision Processes are usually solved using Approximate Dynamic Programming methods such as Approximate Value Iteration or Approximate Policy Iteration. The main contribution of this paper is to show that, alternatively, the optimal state-action value function can be estimated using Difference of Convex functions (DC) Programming. To do so, we study the minimization of a norm of the Optimal Bellman Residual (OBR) T ∗Q − Q, where T ∗ is the so-called optimal Bellman operator. Controlling this residual allows controlling the distance to the optimal action-value function, and we show that minimizing an empirical norm of the OBR is consistant in the Vapnik sense. Finally, we frame this optimization problem as a DC program. That allows envisioning using the large related literature on DC Programming to address the Reinforcement Leaning problem.",
    "creator" : null
  }
}