{
  "name" : "0a0a0c8aaa00ade50f74a3f0ca981ed7.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Analog Memories in a Balanced Rate-Based Network of E-I Neurons",
    "authors" : [ "Dylan Festa", "Guillaume Hennequin", "Máté Lengyel" ],
    "emails" : [ "df325@cam.ac.uk", "gjeh2@cam.ac.uk", "m.lengyel@eng.cam.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Memories are thought to be encoded in the joint, persistent activity of groups of neurons. According to this view, memories are embedded via long-lasting modifications of the synaptic connections between neurons (storage) such that partial or noisy initialization of the network activity drives the collective dynamics of the neurons into the corresponding memory state (recall) [1]. Models of memory circuits following these principles abound in the theoretical neuroscience literature, but few respect some of the most fundamental properties of brain networks, including: i) the separation of neurons into distinct classes of excitatory (E) and inhibitory (I) cells – known as Dale’s law –, ii) the presence of recurrent and sparse synaptic connections, iii) the possibility for each neuron to sustain graded levels of activity in different memories, iv) the firing of action potentials at reasonably low rates, and v) a dynamic balance of E and I inputs.\nIn the original Hopfield network [1], connectivity must be symmetrical, which violates Dale’s law. Moreover, just as in much of the work following it up, memories are encoded in binary neuronal responses and so converge towards effectively binary recall states even if the recall dynamics formally uses graded activities [2]. Subsequent work considered non-binary pattern distributions [3, 4], and derived high theoretical capacity limits for them, but those capacities proved difficult – if not impossible – to realise in practice [5, 6], and the network dynamics therein did not explicitly model inhibitory neurons thus implicitly assuming instantaneous inhibitory feedback. More recent work\n5 Hz m em or ie s\na b c\n20 Hz exc. (prescribed distribution)\n0 10 20 30\ninh. (optimized distribution)\nexc. neurons inh. neurons firing rate [Hz]\nFigure 1: (a) Examples of analog patterns of excitatory neuronal activities, drawn from a log-normal distribution. In all our training experiments, network parameters were optimized to stabilize a set of such analog patterns and the baseline, uniform activity state (top row). For ease of visualization, only 30 of the 100 excitatory neurons are shown. (b) Optimized values of the inhibitory (auxiliary) neuronal firing rates for 5 of 30 learned memories (corresponding to those in panel a). Only 30 of the 50 auxiliary neurons are shown. (c) Empirical distributions of firing rates across neurons and memory patterns, for each population.\nincorporated Dale’s law, and described neurons using the more realistic, leaky integrate-and-fire (LIF) neuron model [7]. However, the stability of the recall states still relied critically on the saturating behavior of the LIF input-output transfer function at high rates. Although it was later shown that dynamic feedback inhibition can stabilize relatively low firing rates in subpopulations of more tightly connected neurons [8, 9], inhibitory feedback in these models is global, and calibrated for a single stereotypical level of excitation for all memories, implying effectively binary memories again. Finally, spatially connected networks are able to sustain graded activity patterns (spatial “bumps”), but make strong assumptions about the spatial structure of both the connectivity and the memory patterns, and are sensitive to ongoing noise (e.g. [10, 11]). Ref. [12] provides a rare example of spike timing-based graded memory network, but it again did not contain inhibitory units.\nHere we propose a general control-theoretic framework that overcomes all of the above limitations with minimal additional assumptions. We formalize memory storage as implying two conditions: that the desired activity states be fixed points of the dynamics, and that the dynamics be stable around those fixed points. We directly optimize the network parameters, including the synaptic connectivity, to satisfy both conditions for a collection of arbitrary, graded memory patterns (Fig. 1). The fixed point condition is achieved by minimizing the time derivative of the neural activity, such that ideally it reaches zero, at each of the desired attractor states. Stability, however, is more difficult to achieve because the fixed-point constraints tend to create strong positive feedback loops in the recurrent circuitry, and direct measures of dynamical stability (eg. the spectral abscissa) do not admit efficient, gradient-based optimization. Thus, we use recently developed methods from robust control theory, namely the minimization of the Smoothed Spectral Abscissa (SSA, [13, 14]) to perform robust stability optimization. To satisfy biological constraints, we parametrize the networks that we optimize such that they have realistic firing rate dynamics and their connectivities obey Dale’s law. We show that despite these constraints the resulting networks perform memory recall that is robust to noise in both the recall cue and the ongoing dynamics, and is stabilized through a tight dynamic balance of excitation and inhibition. This novel way of constructing structurally realistic memory networks should open new routes to the understanding of memory and its neural substrate."
    }, {
      "heading" : "2 Methods",
      "text" : "We study a network of n = nE (excitatory) +nI (inhibitory) neurons. The activity of neuron i is represented by a single scalar potential vi, which is converted into a firing rate ri via a thresholdquadratic gain function (e.g. [15]):\nri = g(vi) :=\n{ γv2i if vi > 0\n0 otherwise (1)\nWe set γ to 0.04, such that g(vi) spans a few tens of Hz when vi spans a few tens of mV, as experimentally observed in cortical areas (e.g. cat’s V1 [16]). The instantaneous state of the system can be expressed as a vector v(t) := (v1(t), . . . , vn(t)). We denote the activity of the excitatory or inhibitory subpopulation by vexc and vinh, respectively. The recurrent interactions between neurons are governed by a synaptic weight matrix W, in which the sign of each element Wij depends on the nature (excitatory or inhibitory) of the presynaptic neuron j. We enforce Dale’s law via a reparameterization of the synaptic weights:\nWij = sj log(1 + expβij) with sj = { +1 if j ≤ nE −1 otherwise (2)\nwhere the βij’s are free, unconstrained parameters. (We do not allow for autapses, i.e. we fix Wii = 0). The network dynamics are thus given by:\nτi dvi dt\n= −vi + n∑\nj=1\nWij g(vj) + hi , (3)\nwhere τi is the membrane time constant, and hi is a constant external input, independent of the memory we wish to recall.\nIt is worth noting that, since the gain function g(vi) defined in Eq (1) has no upper saturation, recurrent interactions can easily result in runaway excitation and firing rates growing unbounded. However, our optimization algorithm will naturally seek stable solutions, in which firing rates are kept within a limited range due to a fine dynamic balance of excitation and inhibition [14].\nOptimizing network parameters to embed attractor memories\nWe are going to build and study networks that have a desired set of analog activity patterns as stable fixed points of their dynamics. Let {vµexc}µ=1,...,m be a set of m target analog patterns (Fig. 1), defined in the space of excitatory neuronal activity (potentials). For a given pattern µ, the inhibitory neurons will be free to adjust their steady state firing rates vµinh to whatever pattern proves to be optimal to maintain stability. In other words, we think of the activity of inhibitory neurons as “auxiliary” variables.\nA given activity pattern vµ ≡ (vµ>exc,vµ > inh) > is a stable fixed point of the network dynamics if, and only if, it satisfies the following two conditions:\ndv\ndt ∣∣∣∣ v=vµ = 0 and α (Jµ) < 0 (4)\nwhere Jµ is the Jacobian matrix of the dynamics in Eq. 3, i.e. Jµij :=Wij g ′(vµj )− δij (Kronecker’s delta), and α(Jµ) denotes the spectral abscissa (SA), defined as the largest real part in the eigenvalue spectrum of Jµ. The first condition makes vµ a fixed point of the dynamics, while the second condition makes that fixed point asymptotically stable with respect to small local perturbations. Note that the width of the basin of attraction is not captured by the SA.\nThe two conditions in Eq. 4 depend on a set of network parameters that we will allow ourselves to optimize. These are all the synaptic weight parameters (βij , i 6= j), as well as the values of the inhibitory neurons’ firing rates in each attractor (vµinh, µ = 1, . . . ,m). Thus, we may adjust a total of n(n− 1) + nIm parameters. Using Eq. 3, the first condition in Eq. 4 can be rewritten as vµi − ∑n j=1Wijg(v µ j ) − hi = 0. Despite this equation being linear in the synaptic weights, the re-parameterization of Eq. 2 makes it nonlinear in β, and it is in any case nonlinear in vµinh. We will therefore seek to satisfy this condition by minimizing ‖ dv/dt|v=vµ ‖2, which quantifies how fast the potentials drift away when initialized in the desired attractor state vµ. When it is zero, vµ is a fixed point of the dynamics. Our optimization procedure (see below) may not be able to set this term to exactly zero, especially as we try to store a large number of memories, but in practice we find it becomes small enough that the Jacobian-based stability criterion remains valid.\nMeeting the stability condition (second condition in Eq. 4) turns out to be more involved. The SA is, in general, a non-smooth function of the matrix elements and is therefore difficult to minimize.\nA more suitable stability measure has been introduced recently in the context of robust control theory [13, 14], called the Smoothed Spectral Abscissa (SSA), which we will use here and denote by α̃ε(Jµ). The SSA, defined for some smoothness parameter ε > 0, is a differentiable relaxation of the SA, with the properties α(Jµ) < α̃ε(Jµ) and limε→0 α̃ε(Jµ) = α(Jµ). Therefore, the criterion α̃ε(J\nµ) ≤ 0 implies α(Jµ) < 0, and can therefore be used as an indication of local stability. Both the SSA and its gradient are straightforward to evaluate numerically, making it amenable to minimization through gradient descent. Note that the SSA depends on the Jacobian matrix elements {Jµij}, which in turn depend both on the connectivity parameters {βij} and on vµinh. Note also that the parameter ε > 0 controls how tightly the SSA hugs the SA. Small values make it a tight upper bound, with increasingly ill-behaved gradients. Large values imply more smoothness, but may no longer guarantee that the SSA has a negative minimum even though the SA might have one. In our system of n = 150 neurons we found ε = 0.01 to yield a good compromise. In the general case the distance between SA and SSA grows linearly with the number of dimensions.To keep it invariant, ε should be scaled accordingly. We therefore used the following heuristic rule ε = 0.01 · 150/n. We summarize the above objective into a global cost function by lumping together the fixed point and stability conditions, summing over the entire set of m target memory patterns, and adding an L2 penalty term on the synaptic weights to regularize:\nψ ({βij}, {vµinh}) := 1\nm\nm∑\nµ=1\n( 1\nn\n∥∥∥∥ dv\ndt\n∥∥∥∥ 2\nv=vµ + ηsα̃ε (J\nµ) ) + ηF n2 ‖W‖2F . (5)\nwhere ‖W‖2F is the squared Frobenius norm of W, i.e. the sum of its squared elements, and the parameters ηs and ηF control the relative importance of each component of the objective function. We set them heuristically (Table 1). We used a variant of the low-storage BFGS algorithm included in the open source library NLopt [17] to minimize ψ.\nChoice of initial parameters and attractors\nThe synaptic weights are initially drawn randomly from a Gamma distribution with a shape factor of 2 and a mean that depends only on the type of pre- and post-synaptic population. The mean synaptic weights of the four synapse types were computed using a mean-field reduction of the full network to meet the condition that the network initially exhibits a stable baseline state vµ=1exc in which all excitatory firing rates equal rbaseline = 5 Hz (Table 1, and Supplementary Material). This baseline state was included in every set of m target attractors that we used and was thus stable from the beginning, by construction. For the remaining target patterns, {vµexc}µ=2,...,m were generated by inverting (using g−1) firing rates that were sampled from a log-normal distribution with a mean matching the baseline firing rate, rbaseline (Fig. 1a) and a variance of 5 Hz. This log-normal distribution was chosen to roughly capture the skewed and heavy-tailed nature of firing rate distributions observed in vivo (see e.g. for a review [18]). The inhibitory potentials in the memory states, {vµinh}, were initialized to the baseline, g−1(5Hz), and were subsequently used as free parameters by the learning algorithm (cf. above; see also Fig. 1b)."
    }, {
      "heading" : "3 Results",
      "text" : "Example of successful storage\nFigure 2 shows an example of stability optimization: in this specific run we used 150 neurons to embed 30 graded attractors (examples of which where shown in Fig. 1), yielding a storage capacity of 0.2. Other parameters are listed in Table 1. Gradient descent gradually reduces each of the attractorspecific sub-objectives in Eq. 5, namely the SSA, the SA, and the potential velocities ‖dv/dt‖2 in each target state (Fig. 2). After convergence, the SSA has become negative for all desired states, indicating stability. Note, however, that ‖dv/dt‖ after convergence is small but non-zero in each of the target memories. Thus, strictly speaking, the target patterns haven’t become fixed points of the dynamics, but only slow points from which the system will eventually drift away. In practice though, we found that stability was robust enough that an exact, stable fixed point had in fact been created very near each target pattern. This is detailed below.\nMemory recall performance and robustness\nFor recall, we initialize neuronal activities at a noisy version of one of the target patterns, and study the subsequent evolution of the network state. The network performs well if its dynamics clean up the noise and home in on the target pattern (autoassociative behavior) and if it achieves this robustly even in the face of large amounts of noise.\nInitial cues are chosen to be linear combinations of the form r(t = 0) = σ r̃+(1−σ) rµ, where rµ is the memory we intend to recall and r̃ is an independent random vector with the same lognormal statistics used to generate the memory patterns themselves. The parameter σ regulates the noise level: σ = 0 sets the network activity directly in the desired attractor, while σ = 1 initializes it with completely random values.\nThe deviation of the momentary network state r(t) ≡ g(v(t)) from the target pattern rµ ≡ g(vµ) is measured in terms of the squared Euclidean distance, further normalized by the expected squared distance between rµ and a random pattern drawn from the same distribution (log-normal in our case). Formally:\ndµ(t) := ‖rexc(t)− rµexc‖2 〈‖r̃exc − rµexc‖2〉r̃ . (6)\nFigure 3a shows the temporal evolution of dµ(t) on a few sample recall trials, for two different noise levels σ. For σ = 0.5, recalls are always successful, as the network state converges to the right target pattern on each trial. For σ = 0.75, the network activity occasionally settles in another, well distinct attractor.\nWe used the convention that a trial is deemed successful if the distance dµ(t) falls below 0.001. (A ∼ 3 Hz deviation from the target in only one of the 100 exc. neurons, with all other 99 neurons behaving perfectly, would be sufficient to cross this threshold and fail the test.) We further measure performance as the probability of successful recall, which we estimated from many independent trials with different realizations of the noise r̃ in the initial condition (Figure 3b). The network performance is also compared to an “ideal observer” [6] that has direct access to all the stored memories (rather than just their reflection in the synaptic weights) and simply returns that pattern in the training set {rµ} to which the initial cue is closest (Fig. 3b). Thus, as an upper bound on performance, the ideal observer only produces a wrong recall when the added noise brings the initial state closer to an attractor that is different from the target. Remarkably, our network dynamics\n(continuous lines) and the ideal observer (dashed lines) have comparable performances. When trying to recall the uniform pattern of baseline activity, the performance appears much better (orange line) both for the ideal observer and the network. This is simply because the random vectors used to perturb the system have a high probability of lying closer to the mean of the log normal distribution (that is, the baseline state) than to any other memory pattern. Moreover, the network was initialized prior to learning with the baseline as the single global attractor, and this might account for the additional tendency of the network (solid orange line) to fall on such state, as compared to the ideal observer (dotted orange line).\nOnly a few strong synaptic weights contribute to memory recall\nSynaptic weights after learning (Fig. 4a) are sparse: their distribution shows the characteristic peak near zero and the long tail observed in real cortical circuits [19, 20] (Fig. 4b). This sparseness cannot be accounted for by the L2 norm regularizer in the cost function (Eq. 5) as it does not promote sparsity as an L1 term would. Thus, the observed sparsity in the trained network must be a genuine consequence of having optimized the connectivity for robust stability.\nIf we assume that weights |Wij | ≤ 0.01 correspond to functionally silent synapses, then the trained network contains 52% of silent excitatory synapses and 46% of silent inhibitory ones (Fig. 4c). We wondered if those weak, “silent” synapses are necessary for stability of memory recall, or could be removed altogether without affecting performance. To test that, we clipped those synapses {|Wij | < 0.01} to zero, and computed recall performance again (Fig. 4d). This clipping turns out to slightly shift the position of the attractors in state space, so we increased the distance threshold that defines a successful recall trial to 0.08. The test reveals that one of the attractors loses stability, reducing the average performance. However the remaining 29 attractors are robust to this removal of weak synapses and show near-equal recall performance as above. This demonstrates that small weights, though numerous, are not necessary for competent recall performance.\nBalanced state\nAs a result of the connection weight distributions and robust stability, the trained network produces a regime in which excitation and inhibition balance each other, precisely tuning each neuron to its target frequency in each attractor. Excitatory and inhibitory inputs are defined as hexci (t) =∑n j=1bWijc+ rj(t) and hinhi (t) = ∑n j=1b−Wijc+ rj(t) so that the difference hexci (t) − hinhi (t) corresponds to the total recurrent input, i.e. the second term on the r.h.s. of Eq. 3.\nFigure 5a shows the evolution of hexci (t) and h inh i (t) during a recall trial for one of the stored random attractors, for 3 different neurons. Neuron 3 has rate target of 9Hz, well above average, therefore its excitation is much higher than inhibition. Neuron 72 has a steady state firing rate of 2 Hz, below average: its inhibitory input is greater than the excitatory one, and firing is driven by the external current. Finally, neuron 101 is inhibitory and has a target rate 0, and indeed its inhibitory input is large enough to overwhelm the combined effects of the external and recurrent excitatory inputs. Notably, in all these cases, both E and I input currents are fairly large but cancel each other to leave something smaller, either positive or negative.\nFigure 5b shows the E vs. I inputs at steady-state across all the embedded attractors, for various neurons plotted in different colors. These E and I inputs tend to be correlated across attractors for every single neuron (dots in Fig. 5 tend to hug the identity line), with relative differences fine-tuned to yield the desired firing rates. These across-attractors E/I correlations are summarized in Fig. 5c as a histogram over neurons.\nRobustness to ongoing noise and reduction of across-trial variability following recall onset\nFinally, to probe the system under more realistic dynamics, we added time-varying, Gaussian white noise such that, in an excitatory neuron free from network interactions, the potential would fluctuate\nwith standard deviation 0.33. Figure 6a shows the momentary distance dµ(t) of the network state from the attractor closest to the initial cue (green), and for all other attractors (orange), during a recall trial. It is clear that the system revolves around the desired attractor, performing successful recall despite the ongoing noise. In a second experiment, we ran many trials in which the initialization at time t = 0 was random, while the same spatially patterned stimulation – aligned onto a chosen attractor – is given to the network in each trial at time t = 0.5 sec. Figure 6b shows the standard deviation of the internal state of a neuron across trials, averaged across the neural population. Following stimulus onset, neurons are always pushed towards the target attractor, and this greatly reduces trial-by-trial variability, compared to the initial spontaneous regime in which the neurons would fluctuate around any of the activity levels corresponding to its assigned attractors. Interestingly, such stimulus-induced variability reduction has been observed very broadly across sensory and motor cortical areas [21]. This extends previous work, e.g. [22] and [23], showing variability reduction in a multiple-attractor scenario with effectively binary patterns, to the case of patterns with graded activities."
    }, {
      "heading" : "4 Discussion",
      "text" : "We have provided a proof of concept that a model cortical networks of E and I neurons can embed multiple analog memories as stable fixed-points of their dynamics. Memories are stable in the face of ongoing noise and corruption of the recall cues. Neuronal activities do not saturate, and indeed, our single-neuron model did not explicitly incorporate an upper saturation mechanism: dynamic feedback inhibition, precisely matched to the level of excitation incurred by each attractor, ensures that each neuron can fire at a relatively low rate during recall. As a result, excitation and inhibition are tightly balanced.\nWe have used a rate-based formulation of the circuit dynamics, which raises the question of the applicability of our method to understanding spiking memory networks. Once the connectivity in the rate model is generated and optimized, it could still be used in a spiking model, provided the gain function we have used here matches that of the single spiking neurons. In this respect, the gain function we have used here is likely an appropriate choice: in physiological conditions, cortical neurons have input-output gain functions that are well approximated by a rectified powerlaw function over their entire dynamic range [24, 25, 26].\nAn important question for future research is how local synaptic learning rules can achieve the stabilization objective that we have approached here from an optimal, algorithmic viewpoint. Inhibitory synaptic plasticity is a promising candidate, as it has already been shown to enable self-regulation of the spontaneous, baseline activity regime, and also to promote the stable storage of binary memory patterns [27]. More work is required in this direction.\nAcknowledgements. This work was supported by the Wellcome Trust (GH, ML), the European Union Seventh Framework Programme (FP7/20072013) under grant agreement no. 269921 (BrainScaleS) (DF, ML), and the Swiss National Science Foundation (GH)."
    } ],
    "references" : [ {
      "title" : "Neural networks and physical systems with emergent collective computational abilities",
      "author" : [ "J. Hopfield" ],
      "venue" : "Proceedings of the national academy of sciences 79:2554,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1982
    }, {
      "title" : "Neurons with graded response have collective computational properties like those of two-state neurons",
      "author" : [ "J. Hopfield" ],
      "venue" : "Proceedings of the national academy of sciences 81:3088,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1984
    }, {
      "title" : "Graded-response neurons and information encodings in autoassociative memories",
      "author" : [ "A. Treves" ],
      "venue" : "Phys. Rev. A 42:2418,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1990
    }, {
      "title" : "What determines the capacity of autoassociative memories in the brain",
      "author" : [ "Treves A", "Rolls ET" ],
      "venue" : "Network: Computation in Neural Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1991
    }, {
      "title" : "Stable and rapid recurrent processing in realistic autoassociative memories",
      "author" : [ "FP Battaglia", "A. Treves" ],
      "venue" : "Neural Comput",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1998
    }, {
      "title" : "Dynamics of a recurrent network of spiking neurons before and following learning, Network: Computation",
      "author" : [ "D Amit", "N. Brunel" ],
      "venue" : "Neural Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1997
    }, {
      "title" : "Computing and stability in cortical networks, Neural computation 16:1385",
      "author" : [ "P Latham", "S. Nirenberg" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "A balanced memory network",
      "author" : [ "Roudi Y", "Latham PE" ],
      "venue" : "PLoS Computational Biology 3:e141,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Theory of orientation tuning in visual cortex",
      "author" : [ "R Ben-Yishai" ],
      "venue" : "Proc. Natl. Acad. Sci. USA",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1995
    }, {
      "title" : "Patterns of ongoing activity and the functional architecture of the primary visual cortex",
      "author" : [ "JA Goldberg" ],
      "venue" : "Neuron 42:489,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Matching storage and recall: hippocampal spike timing–dependent plasticity and phase response curves",
      "author" : [ "M Lengyel" ],
      "venue" : "Nature Neuroscience",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "The smoothed spectral abscissa for robust stability optimization",
      "author" : [ "J Vanbiervliet" ],
      "venue" : "SIAM Journal on Optimization",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Optimal control of transient dynamics in balanced networks supports generation of complex movements",
      "author" : [ "G Hennequin" ],
      "venue" : "Neuron 82:1394,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Analysis of the stabilized supralinear network",
      "author" : [ "Y Ahmadian" ],
      "venue" : "Neural Comput. 25:1994,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "The contribution of noise to contrast invariance of orientation tuning in cat visual cortex",
      "author" : [ "JS Anderson" ],
      "venue" : "Science 290:1968,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "On the distribution of firing rates in networks of cortical neurons",
      "author" : [ "A Roxin" ],
      "venue" : "The Journal of Neuroscience",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Highly nonrandom features of synaptic connectivity in local cortical circuits",
      "author" : [ "S Song" ],
      "venue" : "PLoS Biol 3:",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2005
    }, {
      "title" : "The excitatory neuronal network of the C2 barrel column in mouse primary somatosensory cortex, Neuron",
      "author" : [ "S Lefort" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "Stimulus onset quenches neural variability: a widespread cortical phenomenon",
      "author" : [ "MM Churchland" ],
      "venue" : "Nat Neurosci",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2010
    }, {
      "title" : "Slow dynamics and high variability in balanced cortical networks with clustered connections",
      "author" : [ "A Litwin-Kumar", "B. Doiron" ],
      "venue" : "Nat Neurosci",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Neural network mechanisms underlying stimulus driven variability reduction, PLoS computational biology",
      "author" : [ "G Deco", "E. Hugues" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Direction selectivity of excitation and inhibition in simple cells of the cat primary visual cortex",
      "author" : [ "NJ Priebe", "D. Ferster" ],
      "venue" : "Neuron 45:133,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2005
    }, {
      "title" : "Mechanisms underlying cross-orientation suppression in cat visual cortex",
      "author" : [ "NJ Priebe", "D. Ferster" ],
      "venue" : "Nat Neurosci",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "The emergence of contrast-invariant orientation tuning in simple cells of cat visual cortex",
      "author" : [ "IM Finn" ],
      "venue" : "Neuron 54:137,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks",
      "author" : [ "TP Vogels" ],
      "venue" : "Science 334:1569,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "According to this view, memories are embedded via long-lasting modifications of the synaptic connections between neurons (storage) such that partial or noisy initialization of the network activity drives the collective dynamics of the neurons into the corresponding memory state (recall) [1].",
      "startOffset" : 288,
      "endOffset" : 291
    }, {
      "referenceID" : 0,
      "context" : "In the original Hopfield network [1], connectivity must be symmetrical, which violates Dale’s law.",
      "startOffset" : 33,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "Moreover, just as in much of the work following it up, memories are encoded in binary neuronal responses and so converge towards effectively binary recall states even if the recall dynamics formally uses graded activities [2].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 2,
      "context" : "Subsequent work considered non-binary pattern distributions [3, 4], and derived high theoretical capacity limits for them, but those capacities proved difficult – if not impossible – to realise in practice [5, 6], and the network dynamics therein did not explicitly model inhibitory neurons thus implicitly assuming instantaneous inhibitory feedback.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "Subsequent work considered non-binary pattern distributions [3, 4], and derived high theoretical capacity limits for them, but those capacities proved difficult – if not impossible – to realise in practice [5, 6], and the network dynamics therein did not explicitly model inhibitory neurons thus implicitly assuming instantaneous inhibitory feedback.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Subsequent work considered non-binary pattern distributions [3, 4], and derived high theoretical capacity limits for them, but those capacities proved difficult – if not impossible – to realise in practice [5, 6], and the network dynamics therein did not explicitly model inhibitory neurons thus implicitly assuming instantaneous inhibitory feedback.",
      "startOffset" : 206,
      "endOffset" : 212
    }, {
      "referenceID" : 5,
      "context" : "incorporated Dale’s law, and described neurons using the more realistic, leaky integrate-and-fire (LIF) neuron model [7].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "Although it was later shown that dynamic feedback inhibition can stabilize relatively low firing rates in subpopulations of more tightly connected neurons [8, 9], inhibitory feedback in these models is global, and calibrated for a single stereotypical level of excitation for all memories, implying effectively binary memories again.",
      "startOffset" : 155,
      "endOffset" : 161
    }, {
      "referenceID" : 7,
      "context" : "Although it was later shown that dynamic feedback inhibition can stabilize relatively low firing rates in subpopulations of more tightly connected neurons [8, 9], inhibitory feedback in these models is global, and calibrated for a single stereotypical level of excitation for all memories, implying effectively binary memories again.",
      "startOffset" : 155,
      "endOffset" : 161
    }, {
      "referenceID" : 10,
      "context" : "[12] provides a rare example of spike timing-based graded memory network, but it again did not contain inhibitory units.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Thus, we use recently developed methods from robust control theory, namely the minimization of the Smoothed Spectral Abscissa (SSA, [13, 14]) to perform robust stability optimization.",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "Thus, we use recently developed methods from robust control theory, namely the minimization of the Smoothed Spectral Abscissa (SSA, [13, 14]) to perform robust stability optimization.",
      "startOffset" : 132,
      "endOffset" : 140
    }, {
      "referenceID" : 12,
      "context" : "However, our optimization algorithm will naturally seek stable solutions, in which firing rates are kept within a limited range due to a fine dynamic balance of excitation and inhibition [14].",
      "startOffset" : 187,
      "endOffset" : 191
    }, {
      "referenceID" : 11,
      "context" : "A more suitable stability measure has been introduced recently in the context of robust control theory [13, 14], called the Smoothed Spectral Abscissa (SSA), which we will use here and denote by α̃ε(J).",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "A more suitable stability measure has been introduced recently in the context of robust control theory [13, 14], called the Smoothed Spectral Abscissa (SSA), which we will use here and denote by α̃ε(J).",
      "startOffset" : 103,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "4a) are sparse: their distribution shows the characteristic peak near zero and the long tail observed in real cortical circuits [19, 20] (Fig.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "4a) are sparse: their distribution shows the characteristic peak near zero and the long tail observed in real cortical circuits [19, 20] (Fig.",
      "startOffset" : 128,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "Interestingly, such stimulus-induced variability reduction has been observed very broadly across sensory and motor cortical areas [21].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 19,
      "context" : "[22] and [23], showing variability reduction in a multiple-attractor scenario with effectively binary patterns, to the case of patterns with graded activities.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] and [23], showing variability reduction in a multiple-attractor scenario with effectively binary patterns, to the case of patterns with graded activities.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 21,
      "context" : "In this respect, the gain function we have used here is likely an appropriate choice: in physiological conditions, cortical neurons have input-output gain functions that are well approximated by a rectified powerlaw function over their entire dynamic range [24, 25, 26].",
      "startOffset" : 257,
      "endOffset" : 269
    }, {
      "referenceID" : 22,
      "context" : "In this respect, the gain function we have used here is likely an appropriate choice: in physiological conditions, cortical neurons have input-output gain functions that are well approximated by a rectified powerlaw function over their entire dynamic range [24, 25, 26].",
      "startOffset" : 257,
      "endOffset" : 269
    }, {
      "referenceID" : 23,
      "context" : "In this respect, the gain function we have used here is likely an appropriate choice: in physiological conditions, cortical neurons have input-output gain functions that are well approximated by a rectified powerlaw function over their entire dynamic range [24, 25, 26].",
      "startOffset" : 257,
      "endOffset" : 269
    }, {
      "referenceID" : 24,
      "context" : "Inhibitory synaptic plasticity is a promising candidate, as it has already been shown to enable self-regulation of the spontaneous, baseline activity regime, and also to promote the stable storage of binary memory patterns [27].",
      "startOffset" : 223,
      "endOffset" : 227
    } ],
    "year" : 2014,
    "abstractText" : "The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale’s law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics. The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.",
    "creator" : null
  }
}