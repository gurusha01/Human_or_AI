{
  "name" : "b139e104214a08ae3f2ebcce149cdf6e.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Quantized Estimation of Gaussian Sequence Models in Euclidean Balls",
    "authors" : [ "Yuancheng Zhu", "John Lafferty" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Classical statistical theory studies the rate at which the error in an estimation problem decreases as the sample size increases. Methodology for a particular problem is developed to make estimation efficient, and lower bounds establish how quickly the error can decrease in principle. Asymptotically matching upper and lower bounds together yield the minimax rate of convergence\nRn(F) = inf f̂ sup f∈F R(f̂ , f).\nThis is the worst-case error in estimating an element of a model class F , where R(f̂ , f) is the risk or expected loss, and f̂ is an estimator constructed on a data sample of size n. The corresponding sample complexity of the estimation problem is n( ,F) = min{n : Rn(F) < }. In the classical setting, the infimum is over all estimators. In contemporary settings, it is increasingly of interest to understand how error depends on computation. For instance, when the data are high dimensional and the sample size is large, constructing the estimator using standard methods may be computationally prohibitive. The use of heuristics and approximation algorithms may make computation more efficient, but it is important to understand the loss in statistical efficiency that this incurs. In the minimax framework, this can be formulated by placing computational constraints on the estimator:\nRn(F , Bn) = inf f̂ :C(f̂)≤Bn sup f∈F R(f̂ , f).\nHere C(f̂) ≤ Bn indicates that the computation C(f̂) used to construct f̂ is required to fall within a “computational budget” Bn. Minimax lower bounds on the risk as a function of the computational budget thus determine a feasible region for computation-constrained estimation, and a Paretooptimal tradeoff for error versus computation.\nOne important measure of computation is the number of floating point operations, or the running time of an algorithm. Chandrasekaran and Jordan [3] have studied upper bounds for statistical estimation with computational constraints of this form in the normal means model. However, useful lower bounds are elusive. This is due to the difficult nature of establishing tight lower bounds for\nthis model of computation in the polynomial hierarchy, apart from any statistical concerns. Another important measure of computation is storage, or the space used by a procedure. In particular, we may wish to limit the number of bits used to represent our estimator f̂ . The question then becomes, how does the excess risk depend on the budget Bn imposed on the number of bits C(f̂) used to encode the estimator?\nThis problem is naturally motivated by certain applications. For instance, the Kepler telescope collects flux data for approximately 150,000 stars [6]. The central statistical task is to estimate the lightcurve of each star nonparametrically, in order to denoise and detect planet transits. If this estimation is done on board the telescope, the estimated function values may need to be sent back to earth for further analysis. To limit communication costs, the estimates can be quantized. The fundamental question is, what is lost in terms of statistical risk in quantizing the estimates? Or, in a cloud computing environment (such as Amazon EC2), a large number of nonparametric estimates might be constructed over a cluster of compute nodes and then stored (for example in Amazon S3) for later analysis. To limit the storage costs, which could dominate the compute costs in many scenarios, it is of interest to quantize the estimates. How much is lost in terms of risk, in principle, by using different levels of quantization?\nWith such applications as motivation, we address in this paper the problem of risk-storage tradeoffs in the normal means model of nonparametric estimation. The normal means model is a centerpiece of nonparametric estimation. It arises naturally when representing an estimator in terms of an orthogonal basis [8, 11]. Our main result is a sharp characterization of the Pareto-optimal tradeoff curve for quantized estimation of a normal means vector, in the minimax sense. We consider the case of a Euclidean ball of unknown radius in Rn. This case exhibits many of the key technical challenges that arise in nonparametric estimation over richer spaces, including the Stein phenomenon and the problem of adaptivity.\nAs will be apparent to the reader, the problem we consider is intimately related to classical rate distortion theory [7]. Indeed, our results require a marriage of minimax theory and rate distortion ideas. We thus build on the fundamental connection between function estimation and lossy source coding that was elucidated in Donoho’s 1998 Wald Lectures [4]. This connection can also be used to advantage for practical estimation schemes. As we discuss further below, recent advances on computationally efficient, near-optimal lossy compression using sparse regression algorithms [12] can perhaps be leveraged for quantized nonparametric estimation.\nIn the following section, we present relevant background and give a detailed statement of our results. In Section 3 we sketch a proof of our main result on the excess risk for the Euclidean ball case. Section 4 presents simulations to illustrate our theoretical analyses. Section 5 discusses related work, and outlines future directions that our results suggest."
    }, {
      "heading" : "2 Background and problem formulation",
      "text" : "In this section we briefly review the essential elements of rate-distortion theory and minimax theory, to establish notation. We then state our main result, which bridges these classical theories.\nIn the rate-distortion setting we have a source that produces a sequence Xn = (X1, X2, . . . , Xn), each component of which is independent and identically distributed as N (0, σ2). The goal is to transmit a realization from this sequence of random variables using a fixed number of bits, in such a way that results in the minimal expected distortion with respect to the original data Xn. Suppose that we are allowed to use a total budget of nB bits, so that the average number of bits per variable is B, which is referred to as the rate. To transmit or store the data, the encoder describes the source sequence Xn by an index φn(Xn), where\nφn : Rn → {1, 2, . . . , 2nB} ≡ C(B) is the encoding function. The nB-bit index is then transmitted or stored without loss. A decoder, when receiving or retrieving the data, represents Xn by an estimate X̌n based on the index using a decoding function ψn : {1, 2, . . . , 2nB} → Rn. The image of the decoding function ψn is called the codebook, which is a discrete set in Rn with cardinality no larger than 2nB . The process is illustrated in Figure 1, and variously referred to as\nsource coding, lossy compression, or quantization. We call the pair of encoding and decoding functions Qn = (φn, ψn) an (n,B)-rate distortion code. We will also use Qn to denote the composition of the two functions, i.e., Qn(·) = ψn(φn(·)). A distortion measure, or a loss function, d : R × R → R+ is used to evaluate the performance of the above coding and transmission process. In this paper, we will use the squared loss d(Xi, X̌i) = (Xi − X̌i)2. The distortion between two sequences Xn and X̌n is then defined by dn(Xn, X̌n) = 1 n ∑n i=1(Xi−X̌i)2, the average of the per observation distortions. We drop the subscript n in dwhen it is clear from the context. The distortion, or risk, for a (n,B)-rate distortion code Qn is defined as the expected loss E d (Xn, Qn(Xn)). Denoting by Qn,B the set of all (n,B)-rate distortion codes, the distortion rate function is defined as\nR(B, σ) = lim inf n→∞ inf Qn∈Qn,B\nE d (Xn, Qn(Xn)) .\nThis distortion rate function depends on the rate B as well as the source distribution. For the i.i.d. N (0, σ2) source, according to the well-known rate distortion theorem [7],\nR(B, σ) = σ22−2B .\nWhen B is zero, meaning no information gets encoded at all, this bound becomes σ2, which is the expected loss when each random variable is represented by its mean. As B approaches infinity, the distortion goes to zero.\nThe previous discussion assumes the source random variables are independent and follow a common distribution N (0, σ2). The goal is to minimize the expected distortion in the reconstruction of Xn after transmitting or storing the data under a communication constraint. Now suppose that\nXi ind.∼ N (θi, σ2) for i = 1, 2, . . . , n.\nWe assume the variance σ2 is known and the means θn = (θ1, . . . , θn) are unknown. Suppose, furthermore, that instead of trying to minimize the recovery distortion d(Xn, X̌n), we want to estimate the means with a risk as small as possible, but again using a budget of B bits per index.\nWithout the communication constraint, this problem has been very well studied [10, 9]. Let θ̂(Xn) ≡ θ̂n = (θ̂1, . . . , θ̂n) denote an estimator of the true mean θn. For a parameter space Θn ⊂ Rn, the minimax risk over Θn is defined as\ninf θ̂n sup θn∈Θn E d(θn, θ̂n) = inf θ̂n sup θn∈Θn\nE 1\nn\nn∑\ni=1\n(θi − θ̂i)2.\nFor the L2 ball of radius c,\nΘn(c) = { (θ1, . . . , θn) : 1\nn\nn∑\ni=1\nθ2i ≤ c2 } , (1)\nPinsker’s theorem gives the exact, limiting form of the minimax risk\nlim inf n→∞ inf θ̂n sup θn∈Θn(c)\nE d(θn, θ̂n) = σ2c2\nσ2 + c2 .\nTo impose a communication constraint, we incorporate a variant of the source coding scheme described above into this minimax framework of estimation. Define a (n,B)-rate estimation code\nR(σ2, c2, B) = c2σ2 σ2 + c2 + c42−2B σ2 + c2\nCurves for five signal sizes are shown, c2 = 2, 3, 4, 5, 6. The noise level is σ2 = 1. With zero bits, the rate is c2, the highest point on the risk curve. The rate for largeB approaches the Pinsker bound σ2c2/(σ2 + c2).\nMn = (φn, ψn), as a pair of encoding and decoding functions, as before. The encoding function φn : Rn → {1, 2, . . . , 2nB} is a mapping from observations Xn to an index set. The decoding function is a mapping from indices to models θ̌n ∈ Rn. We write the composition of the encoder and decoder as Mn(Xn) = ψn(φn(Xn)) = θ̌n, which we call a quantized estimator. Denoting by Mn,B the set of all (n,B)-rate estimation codes, we then define the quantized minimax risk as\nRn(B, σ,Θn) = inf Mn∈Mn,B sup θn∈Θn\nE d(θn,Mn(Xn)).\nWe will focus on the case where our parameter space is the L2 ball defined in (1), and write\nRn(B, σ, c) = Rn(B, σ,Θn(c)).\nIn this setting, we let n go to infinity and define the asymptotic quantized minimax risk as\nR(B, σ, c) = lim inf n→∞ Rn(B, σ, c) = lim inf n→∞ inf Mn∈Mn,B sup θn∈Θn(c)\nE d(θn,Mn(Xn)). (2)\nNote that we could estimate θn based on the quantized data X̌n = Qn(Xn). Once again denoting by Qn,B the set of all (n,B)-rate distortion codes, such an estimator is written θ̌n = θ̌n(Qn(Xn)). Clearly, if the decoding functions ψn of Qn are injective, then this formulation is equivalent. The quantized minimax risk is then expressed as\nRn(B, σ,Θn) = inf θ̌n inf Qn∈Qn,B sup θn∈Θn\nE d(θn, θ̌n).\nThe many normal means problem exhibits much of the complexity and subtlety of general nonparametric regression and density estimation problems. It arises naturally in the estimation of a function expressed in terms of an orthogonal function basis [8, 13]. Our main result sharply characterizes the excess risk that communication constraints impose on minimax estimation for Θ(c)."
    }, {
      "heading" : "3 Main results",
      "text" : "Our first result gives a lower bound on the exact quantized asymptotic risk in terms of B, σ, and c.\nTheorem 1. For B ≥ 0, σ > 0 and c > 0, the asymptotic minimax risk defined in (2) satisfies\nR(B, σ, c) ≥ σ 2c2\nσ2 + c2 +\nc4\nσ2 + c2 2−2B . (3)\nThis lower bound on the limiting minimax risk can be viewed as the usual minimax risk without quantization, plus an excess risk term due to quantization. If we take B to be zero, the risk becomes c2, which is obtained by estimating all of the means simply by zero. On the other hand, letting B →∞, we recover the minimax risk in Pinsker’s theorem. This tradeoff is illustrated in Figure 2. The proof of the theorem is technical and we defer it to the supplementary material. Here we sketch the basic idea of the proof. Suppose we are able to find a prior distribution πn on θn and a random\nvector θ̃n such that for any (n,B)-rate estimation code Mn the following holds:\nσ2c2\nσ2 + c2 +\nc4\nσ2 + c2 2−2B (I) =\n∫ EXnd(θn, θ̃n)dπn(θn)\n(II) ≤ ∫ EXnd(θn,Mn(Xn))dπn(θn)\n(III) ≤ sup\nθn∈Θn(c) EXnd(θn,Mn(Xn)).\nThen taking an infimum over Mn ∈ Mn,B gives us the desired result. In fact, we can take πn, the prior on θn, to be N (0, c2In), and the model becomes θi ∼ N (0, c2) and Xi | θi ∼ N (θi, σ2). Then according to Lemma 1, inequality (II) holds with θ̃n being the minimizer to the optimization problem\nmin p(θ̃n |Xn,θn)\nE d(θn, θ̃n)\nsubject to I(Xn; θ̃n) ≤ nB, p(θ̃n |Xn, θn) = p(θ̃n |Xn).\nThe equality (I) holds due to Lemma 2. The inequality (III) can be shown by a limiting concentration argument on the prior distribution, which is included in the supplementary material. Lemma 1. Suppose that X1, . . . , Xn are independent and generated by θi ∼ π(θi) and Xi | θi ∼ p(xi | θi). Suppose Mn is an (n,B)-rate estimation code with risk E d(θn,Mn(Xn)) ≤ D. Then the rate B is lower bounded by the solution to the following problem:\nmin p(θ̃n |Xn,θn)\nI(Xn; θ̃n)\nsubject to E d(θn, θ̃n) ≤ D, (4) p(θ̃n |Xn, θn) = p(θ̃n |Xn).\nThe next lemma gives the solution to problem (4) when we have θi ∼ N (0, c2) and Xi | θi ∼ N (θi, σ2) Lemma 2. Suppose θi ∼ N (0, c2) and Xi | θi ∼ N (θi, σ2) for i = 1, . . . , n. For any random vector θ̃n satisfying E d(θn, θ̃n) ≤ D and p(θ̃n |Xn, θn) = p(θ̃n |Xn) we have\nI(Xn; θ̃n) ≥ n 2 log c4\n(σ2 + c2)(D − σ2c2σ2+c2 ) .\nCombining the above two lemmas, we obtain a lower bound of the risk assuming that θn follows the prior distribution πn: Corollary 1. Suppose Mn is a (n,B)-rate estimation code for the source θi ∼ N (0, c2) and Xi | θi ∼ N (θi, σ2), then\nE d(θn,Mn(Xn)) ≥ σ2c2\nσ2 + c2 +\nc4\nσ2 + c2 2−2B . (5)"
    }, {
      "heading" : "3.1 An adaptive source coding method",
      "text" : "We now present a source coding method, which we will show attains the minimax lower bound asymptotically with high probability.\nSuppose that the encoder is given a sequence of observations (X1, . . . , Xn), and both the encoder and the decoder know the radius c of the L2 ball in which the mean vector lies. The steps of the source coding method are outlined below:\nStep 1. Generating codebooks. The codebooks are distributed to both the encoder and the decoder.\n(a) Generate codebook B = {1/√n, 2/√n, . . . , dc2√ne/√n}. (b) Generate codebook X which consists of 2nB i.i.d. random vectors from the uniform\ndistribution on the n-dimensional unit sphere Sn−1. Step 2. Encoding.\n(a) Encode b̂2 = 1n‖X‖2 − σ2 by b̌2 = arg min{|b2 − b̂2| : b2 ∈ B}. (b) Encode Xn by X̌n = arg max{〈Xn, xn〉 : xn ∈ X}.\nStep 3. Transmit or store (b̌2, X̌n) by their corresponding indices using log c2 + 12 log n+ nB bits.\nStep 4. Decoding.\n(a) Recover (b̌2, X̌n) by the transmitted or stored indices. (b) Estimate θ by\nθ̌n =\n√ nb̌4(1− 2−2B)\nb̌2 + σ2 · X̌n.\nWe make several remarks on this quantized estimation method.\nRemark 1. The rate of this coding method is B + log c 2\nn + logn 2n , which is asymptotically B bits.\nRemark 2. The method is probabilistic; the randomness comes from the construction of the codebook X . Denoting byM∗n,B,σ,c the ensemble of such random quantizers, there is then a natural oneto-one mapping betweenM∗n,B,σ,c and (Sn−1)2 nB\nand we attach probability measure toM∗n,B,σ,c corresponding to the product uniform distribution on (Sn−1)2nB .\nRemark 3. The main idea behind this coding scheme is to encode the magnitude and the direction of the observation vector separately, in such a way that the procedure adapts to sources with different norms of the mean vectors.\nRemark 4. The computational complexity of this source coding method is exponential in n. Therefore, like the Shannon random codebook, this is a demonstration of the asymptotic achievability of the lower bound (3), rather than a practical scheme to be implemented. We discuss possible computationally efficient algorithms in Section 5.\nThe following shows that with high probability this procedure will attain the desired lower bound asymptotically.\nTheorem 2. For a sequence of vectors {θn}∞n=1 satisfying θn ∈ Rn and ‖θn‖2/n = b2 ≤ c2, as n→∞\nP ( d(θn,Mn(X n)) > σ2b2\nσ2 + b2 +\nb4\nσ2 + b2 2−2B + C\n√ log n\nn\n) −→ 0 (6)\nfor some constant C that does not depend on n (but could possibly depend on b, σ and B). The probability measure is with respect to both Mn ∈M∗n,B,σ,c and Xn ∈ Rn.\nThis theorem shows that the source coding method not only achieves the desired minimax lower bound for the L2 ball with high probability with respect to the random codebook and source distribution, but also adapts to the true magnitude of the mean vector θn. It agrees with the intuition that the hardest mean vector to estimate lies on the boundary of the L2 ball. Based on Theorem 2 we can obtain a uniform high probability bound for mean vectors in the L2 ball. Corollary 2. For any sequence of vectors {θn}∞n=1 satisfying θn ∈ Rn and ‖θn‖2/n ≤ c2, as n→∞\nP ( d(θn,Mn(X n)) > σ2c2\nσ2 + c2 +\nc4\nσ2 + c2 2−2B + C ′\n√ log n\nn\n) −→ 0\nfor some constant C ′ that does not depend on n.\nWe include the details of the proof of Theorem 2 in the supplementary material, which carefully analyzes the three terms in the following decomposition of the loss function:\nd(θn, θ̌n) = 1\nn\n∥∥θ̌n − θn ∥∥2 = 1\nn\n∥∥θ̌n − γ̂Xn + γ̂Xn − θn ∥∥2\n= 1\nn\n∥∥θ̌n − γ̂Xn ∥∥2 ︸ ︷︷ ︸ A1 + 1 n ‖γ̂Xn − θn‖2 ︸ ︷︷ ︸ A2 + 2 n 〈θ̌n − γ̂Xn, γ̂Xn − θn〉 ︸ ︷︷ ︸ A3\nwhere γ̂ = b̂ 2 b̂2+σ2 with b̂2 = ‖Xn‖2/n − σ2. Term A1 characterizes the quantization error. Term A2 does not involve the random codebook, and is the loss of a type of James-Stein estimator. The cross term A3 vanishes as n→∞."
    }, {
      "heading" : "4 Simulations",
      "text" : "In this section we present a set of simulation results showing the empirical performance of the proposed quantized estimation method. Throughout the simulation, we fix the noise level σ2 = 1, while varying the other parameters c and B.\nFirst we show in Figure 3 the effect of quantized estimation and compare it with the James-Stein estimator. Setting n = 15 and c = 2, we randomly generate a mean vector θn ∈ Rn with ‖θ‖2/n = c2. A random vector X is then drawn from N (θn, In) and quantized estimates with rates B ∈ {0.1, 0.2, 0.5, 1} are calculated; for comparison we also compute the James-Stein estimator, given by θ̂nJS = ( 1− (n−2)σ 2 ‖Xn‖2 ) Xn.We repeat this sampling and estimation procedure 100 times and report the averaged risk estimates in Figure 3. We see that the quantized estimator essentially shrinks the random vector towards zero. With small rates, the shrinkage is strong, with all the estimates close to zero. Estimates with larger rates approach the James-Stein estimator.\nIn our second set of simulations, we choose c from {0.1, 0.5, 1, 5, 10} to reflect different signal-tonoise ratios, and choose B from {0.1, 0.2, 0.5, 1}. For each combination of the values of c and B, we vary n, the dimension of the mean vector, which is also the number of observations. Given a set of parameters c, B and n, a mean vector θn is generated uniformly on the sphere ‖θn‖2/n = c2 and data Xn are generated following the distribution N (θn, σ2In). We quantize the data using the source coding method, and compute the mean squared error between the estimator and the true mean vector. The procedure is repeated 100 times for each of the parameter combinations, and the average and standard deviation of the mean squared errors are recorded. The results are shown in Figure 4. We see that as n increases, the average error decreases and approaches the theoretic lower bound in Theorem 1. Moreover, the standard deviation of the mean squared errors also decreases, confirming the result of Theorem 2 that the convergence is with high probability."
    }, {
      "heading" : "5 Discussion and future work",
      "text" : "In this paper, we establish a sharp lower bound on the asymptotic minimax risk for quantized estimators of nonparametric normal means for the case of a Euclidean ball. Similar techniques can be\napplied to the setting where the parameter space is an ellipsoid Θ = {θ : ∑∞j=1 a2jθ2j ≤ c2}. A principal case of interest is the Sobolev ellipsoid of order m where a2j ∼ (πj)2m as j → ∞. The Sobolev ellipsoid arises naturally in nonparametric function estimation and is thus of great importance. We leave this to future work.\nDonoho discusses the parallel between rate distortion theory and Pinsker’s work in his Wald Lectures [4]. Focusing on the case of the Sobolev space of order m, which we denote by Fm, it is shown that the Kolmogorov entropy H (Fm) and the rate distortion function R(D,X) satisfy H (Fm) sup{R( 2, X) : P(X ∈ Fm) = 1} as → 0. This connects the worst-case minimax analysis and least-favorable rate distortion function for the function class. Another informationtheoretic formulation of minimax rates lies in the so-called “le Cam equation” H (F) = n 2 [14, 15]. However, both are different from the direction we pursue in this paper, which is to impose communication constraints in minimax analysis.\nIn other related work, researchers in communications theory have studied estimation problems in sensor networks under communication constraints. Draper and Wornell [5] obtain a result on the so-called “one-step problem” for the quadratic-Gaussian case, which is essentially the same as the statement in our Corollary 1. In fact, they consider a similar setting, but treat the mean vector as random and generated independently from a known normal distribution. In contrast, we assume a fixed but unknown mean vector and establish a minimax lower bound as well as an adaptive source coding method that adapts to the fixed mean vector within the parameter space. Zhang et al. [16] also consider minimax bounds with communication constraints. However, the analysis in [16] is focused on distributed parametric estimation, where the data are distributed between several machines. Information is shared between the machines in order to construct a parameter estimate, and constraints are placed on the amount of communication that is allowed.\nIn addition to treating more general ellipsoids, an important direction for future work is to design computationally efficient quantized nonparametric estimators. One possible method is to divide the variables into smaller blocks and quantize them separately. A more interesting and promising approach is to adapt the recent work of Venkataramanan et al. [12] that uses sparse regression for lossy compression. We anticipate that with appropriate modifications, this scheme can be applied to quantized nonparametric estimation to yield practical algorithms, trading off a worse error exponent in the convergence rate to the optimal quantized minimax risk for reduced complexity encoders and decoders."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Research supported in part by NSF grant IIS-1116730, AFOSR grant FA9550-09-1-0373, ONR grant N000141210762, and an Amazon AWS in Education Machine Learning Research grant. The authors thank Andrew Barron, John Duchi, and Alfred Hero for valuable comments on this work."
    } ],
    "references" : [ {
      "title" : "Distributions of angles in random packing on spheres",
      "author" : [ "T. Tony Cai", "Jianqing Fan", "Tiefeng Jiang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2013
    }, {
      "title" : "Phase transition in limiting distributions of coherence of highdimensional random matrices",
      "author" : [ "T. Tony Cai", "Tiefeng Jiang" ],
      "venue" : "Journal of Multivariate Analysis,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Computational and statistical tradeoffs via convex relaxation",
      "author" : [ "Venkat Chandrasekarana", "Michael I. Jordan" ],
      "venue" : "PNAS, 110(13):E1181–E1190,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Wald lecture I: Counting bits with Kolmogorov and Shannon",
      "author" : [ "David L. Donoho" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "Side information aware coding strategies for sensor networks",
      "author" : [ "Stark C. Draper", "Gregory W. Wornell" ],
      "venue" : "Selected Areas in Communications, IEEE Journal on,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Overview of the Kepler science processing pipeline",
      "author" : [ "Jon M. Jenkins" ],
      "venue" : "The Astrophysical Journal Letters,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Information Theory and Reliable Communication",
      "author" : [ "Robert G. Gallager" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1968
    }, {
      "title" : "Function estimation and Gaussian sequence models",
      "author" : [ "Iain M. Johnstone" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2002
    }, {
      "title" : "Minimax risk: Pinsker bound",
      "author" : [ "Michael Nussbaum" ],
      "venue" : "Encyclopedia of Statistical Sciences,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Optimal filtering of square-integrable signals in Gaussian noise",
      "author" : [ "Mark Semenovich Pinsker" ],
      "venue" : "Problemy Peredachi Informatsii,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1980
    }, {
      "title" : "Introduction to Nonparametric Estimation",
      "author" : [ "Alexandre B. Tsybakov" ],
      "venue" : "Springer Series in Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Lossy compression via sparse linear regression: Computationally efficient encoding and decoding",
      "author" : [ "Ramji Venkataramanan", "Tuhin Sarkar", "Sekhar Tatikonda" ],
      "venue" : "In IEEE International Symposium on Information Theory (ISIT),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Probability inequalities for likelihood ratios and convergence rates of sieve MLEs",
      "author" : [ "Wing Hung Wong", "Xiaotong Shen" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1995
    }, {
      "title" : "Information-theoretic determination of minimax rates of convergence",
      "author" : [ "Yuhong Yang", "Andrew Barron" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1999
    }, {
      "title" : "Information-theoretic lower bounds for distributed statistical estimation with communication constraints",
      "author" : [ "Yuchen Zhang", "John Duchi", "Michael Jordan", "Martin J. Wainwright" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Chandrasekaran and Jordan [3] have studied upper bounds for statistical estimation with computational constraints of this form in the normal means model.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "For instance, the Kepler telescope collects flux data for approximately 150,000 stars [6].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 7,
      "context" : "It arises naturally when representing an estimator in terms of an orthogonal basis [8, 11].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 10,
      "context" : "It arises naturally when representing an estimator in terms of an orthogonal basis [8, 11].",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "As will be apparent to the reader, the problem we consider is intimately related to classical rate distortion theory [7].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "We thus build on the fundamental connection between function estimation and lossy source coding that was elucidated in Donoho’s 1998 Wald Lectures [4].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 11,
      "context" : "As we discuss further below, recent advances on computationally efficient, near-optimal lossy compression using sparse regression algorithms [12] can perhaps be leveraged for quantized nonparametric estimation.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : "N (0, σ(2)) source, according to the well-known rate distortion theorem [7], R(B, σ) = σ22−2B .",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "Without the communication constraint, this problem has been very well studied [10, 9].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : "Without the communication constraint, this problem has been very well studied [10, 9].",
      "startOffset" : 78,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "It arises naturally in the estimation of a function expressed in terms of an orthogonal function basis [8, 13].",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Donoho discusses the parallel between rate distortion theory and Pinsker’s work in his Wald Lectures [4].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "Another informationtheoretic formulation of minimax rates lies in the so-called “le Cam equation” H (F) = n 2 [14, 15].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 13,
      "context" : "Another informationtheoretic formulation of minimax rates lies in the so-called “le Cam equation” H (F) = n 2 [14, 15].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "Draper and Wornell [5] obtain a result on the so-called “one-step problem” for the quadratic-Gaussian case, which is essentially the same as the statement in our Corollary 1.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "[16] also consider minimax bounds with communication constraints.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "However, the analysis in [16] is focused on distributed parametric estimation, where the data are distributed between several machines.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 11,
      "context" : "[12] that uses sparse regression for lossy compression.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2014,
    "abstractText" : "A central result in statistical theory is Pinsker’s theorem, which characterizes the minimax rate in the normal means model of nonparametric estimation. In this paper, we present an extension to Pinsker’s theorem where estimation is carried out under storage or communication constraints. In particular, we place limits on the number of bits used to encode an estimator, and analyze the excess risk in terms of this constraint, the signal size, and the noise level. We give sharp upper and lower bounds for the case of a Euclidean ball, which establishes the Pareto-optimal minimax tradeoff between storage and risk in this setting.",
    "creator" : null
  }
}