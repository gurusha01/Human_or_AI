{
  "name" : "4e6cd95227cb0c280e99a195be5f6615.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Hamming Ball Auxiliary Sampling for Factorial Hidden Markov Models",
    "authors" : [ "Michalis K. Titsias", "Christopher Yau" ],
    "emails" : [ "mtitsias@aueb.gr", "cyau@well.ox.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The hidden Markov model (HMM) [1] is one of the most widely and successfully applied statistical models for the description of discrete time series data. Much of its success lies in the availability of efficient computational algorithms that allows the calculation of key quantities necessary for statistical inference [1, 2]. Importantly, the complexity of these algorithms is linear in the length of the sequence and quadratic in the number of states which allows HMMs to be used in applications that involve long data sequences and reasonably large state spaces with modern computational hardware. In particular, the HMM has seen considerable use in areas such as bioinformatics and computational biology where non-trivially sized datasets are commonplace [3, 4, 5].\nThe factorial hidden Markov model (FHMM) [6] is an extension of the HMM where multiple independent hidden chains run in parallel and cooperatively generate the observed data. In a typical setting, we have an observed sequence Y = (y1, . . . ,yN ) of length N which is generated through K binary hidden sequences represented by a K × N binary matrix X = (x1, . . . ,xN ). The interpretation of the latter binary matrix is that each row encodes for the presence or absence of a single feature across the observed sequence while each column xi represents the different features that are active when generating the observation yi. Different rows of X correspond to independent Markov chains following\np(xk,i|xk,i−1) = { 1− ρk, xk,i = xk,i−1, ρk, xk,i 6= xk,i−1,\n(1)\nand where the initial state xk,1 is drawn from a Bernoulli distribution with parameter νk. All hidden chains are parametrized by 2K parameters denoted by the vectors ρ = {ρk}Kk=1 and v = {vk}Kk=1. Furthermore, each data point yi is generated conditional on xi through a likelihood model p(yi|xi) parametrized by φ. The whole set of model parameters consists of the vector θ = (φ,ρ,v) which determines the joint probability density over (Y,X), although for notational simplicity we omit reference to it in our expressions. The joint probability density over (Y,X) is written in the form\np(Y,X) = p(Y |X)p(X) = ( N∏ i=1 p(yi|xi) )( K∏ k=1 p(xk,1) N∏ i=2 p(xk,i|xk,i−1) ) , (2)\nand it is depicted as a directed graphical model in Figure 1.\nWhile the HMM has enjoyed widespread application, the utility of the FHMM has been relatively less abundant. One considerable challenge in the adoption of FHMMs concerns the computation of the posterior distribution p(X|Y ) (conditional on observed data and model parameters) which comprises a fully dependent distribution in the space of the 2KN possible configurations of the binary matrix X . Exact Monte Carlo inference can be achieved by applying the standard forward-filteringbackward-sampling (FF-BS) algorithm to simulate a sample from p(X|Y ) in O(22KN) time (the independence of the Markov chains can be exploited to reduce this complexity toO(2K+1KN) [6]). Joint updating of X is highly desirable in time series analysis since alternative strategies involving conditional single-site, single-row or block updates can be notoriously slow due to strong coupling between successive time steps. However, although the use of FF-BS is quite feasible for even very large HMMs, it is only practical for small values of K and N in FHMMs. As a consequence, inference in FHMMs has become somewhat synonymous with approximate methods such as variational inference [6, 7].\nThe main burden of the FF-BS algorithm is the requirement to sum over all possible configurations of the binary matrix X during the forward filtering phase. The central idea in this work is to avoid this computationally expensive step by applying a restricted sampling procedure with polynomial time complexity that, when applied iteratively, gives exact samples from the true posterior distribution. Whilst regular conditional sampling procedures use locally asymmetric moves that only allow one part of X to be altered at a time, our sampling method employs locally symmetric moves that allow localized joint updating of all the constituent chains making it less prone to becoming trapped in local modes. The sampling strategy adopts the use of an auxiliary variable construction, similar to slice sampling [8] and the Swendsen-Wang algorithm [9], that allows the automatic selection of the sequence of restricted configuration spaces. The size of these restricted configuration spaces is user-defined allowing control over balance between the sampling efficiency and computational complexity. Our sampler generalizes the standard FF-BS algorithm which is a special case."
    }, {
      "heading" : "2 Standard Monte Carlo inference for the FHMM",
      "text" : "Before discussing the details of our new sampler, we first describe the limitations of standard conditional sampling procedures for the FHMM. The most sophisticated conditional sampling schemes are based on alternating between sampling one chain (or a small block of chains) at a time using the FF-BS recursion. However, as discussed in the following and illustrated experimentally in Section 4, these algorithms can easily become trapped in local modes leading to inefficient exploration of the posterior distribution.\nOne standard Gibbs sampling algorithm for the FHMM is based on simulating from the posterior conditional distribution over a single row of X given the remaining rows. Each such step can be carried out in O(4N) time using the FF-BS recursion, while a full sweep over all K rows requires O(4KN) time. A straightforward generalization of the above is to apply a block Gibbs sampling where at each step a small subset of chains is jointly sampled. For instance, when we consider pairs of chains the time complexity for sampling a pair is O(16N) while a full sweep over all possible pairs requires time O(16K(K−1)2 N).\nWhile these schemes can propose large changes toX and be efficiently implemented using forwardbackward recursions, they can still easily get trapped to local modes of the posterior distribution. For instance, suppose we sample pairs of rows and we encounter a situation where, in order to escape from a local mode, four bits in two different columns (two bits from each column) must be jointly flipped. Given that these four bits belong to more than two rows, the above Gibbs sampler will fail to move out from the local mode no matter which row-pair, from the K(K−1)2 possible ones, is jointly simulated. An illustrative example of this phenomenon is given in Figure 2(a).\nWe could describe the conditional sampling updates of block Gibbs samplers as being locally asymmetric, in the sense that, in each step, one part of X is restricted to remain unchanged while the other part is free to change. As the above example indicates, these locally asymmetric updates can cause the chain to become trapped in local modes which can result in slow mixing. This can be particularly problematic in FHMMs where the observations are jointly dependent on the underlying hidden states which induces a coupling between rows of X . Of course, locality in any possible MCMC scheme for FHMMs seems unavoidable, certainly however, such a locality does not need to be asymmetric. In the next section, we develop a symmetrically local sampling approach so that each step gives a chance to any element of X to be flipped in any single update."
    }, {
      "heading" : "3 Hamming ball auxiliary sampling",
      "text" : "Here we develop the theory of the Hamming ball sampler. Section 3.1 presents the main idea while Section 3.2 discusses several extensions."
    }, {
      "heading" : "3.1 The basic Hamming ball algorithm",
      "text" : "Recall the K-dimensional binary vector xi (the i-th column of X) that defines the hidden state at i-th location. We consider the set of all K-dimensional binary vectors ui that lie within a certain Hamming distance from xi so that each ui is such that\nh(ui,xi) ≤ m. (3)\nwhere m ≤ K. Here, h(ui,xi) = ∑K\nk=1 I(uk,i 6= xk,i) is the Hamming distance between two binary vectors and I(·) denotes the indicator function. Notice that the Hamming distance is simply the number of elements the two binary vectors disagree. We refer to the set of all uis satisfying (3) as the i-th location Hamming ball of radius m. For instance, when m = 1, the above set includes all ui vectors restricted to be the same as xi but with at most one bit flipped, when m = 2 these vectors can have at most two bits flipped and so on. For a givenm, the cardinality of the i-th location Hamming ball is\nM = m∑ j=0 ( K j ) . (4)\nFor m = 1 this number is equal to K + 1, for m = 2 is equal to K(K−1)2 + K + 1 and so on. Clearly, when m = K there is no restriction on the values of ui and the above number takes its maximum value, i.e. M = 2K . Subsequently, given a certain X we define the full path Hamming\nball or simply Hamming ball as the set Bm(X) = {U ; h(ui,xi) ≤ m, i = 1, . . . , N}, (5)\nwhere U is aK×N binary matrix such that U = (u1, . . . ,uN ). This Hamming ball, centered atX , is simply the intersection of all i-th location Hamming balls of radius m. Clearly, the Hamming ball set is such that U ∈ Bm(X) iff X ∈ Bm(U), or more concisely we can write I(U ∈ Bm(X)) = I(X ∈ Bm(U)). Furthermore, the indicator function I(U ∈ Bm(X)) factorizes as follows,\nI(U ∈ Bm(X)) = N∏ i=1 I(h(ui,xi) ≤ m). (6)\nWe wish now to consider U as an auxiliary variable generated given X uniformly inside Bm(X), i.e. we define the conditional distribution\np(U |X) = 1 Z I(U ∈ Bm(X)), (7)\nwhere crucially the normalizing constantZ simply reflects the volume of the ball and is independent from X . We can augment the initial joint model density from Eq. (2) with the auxiliary variables U and express the augmented model\np(Y,X,U) = p(Y |X)p(X)p(U |X). (8) Based on this, we can apply Gibbs sampling in the augmented space and iteratively sample U from the posterior conditional, which is just p(U |X), and then sample X given the remaining variables. Sampling p(U |X) is trivial as it requires to independently draw each ui, with i = 1, . . . , N , from the uniform distribution proportional to I(h(ui,xi) ≤ m), i.e. randomly select a ui within Hamming distance at most m from xi. Then, sampling X is carried out by simulating from the following posterior conditional distribution\np(X|Y,U) ∝ p(Y |X)p(X)p(U |X) ∝ ( N∏ i=1 p(yi|xi)I(h(xi,ui) ≤ m) ) p(X), (9)\nwhere we used Eq. (6). Exact sampling from this distribution can be done using the FF-BS algorithm in O(M2N) time where M is the size of each location-specific Hamming ball given in (4).\nThe intuition behind the above algorithm is the following. Sampling p(U |X) given the current state X can be thought of as an exploration step where X is randomly perturbed to produce an auxiliary matrix U . We can imagine this as moving the Hamming ball that initially is centered at X to a new location centered at U . Subsequently, we take a slice of the model by considering only the binary matrices that exist inside this new Hamming ball, centered at U , and draw an new state for X by performing exact sampling in this sliced part of the model. Exact sampling is possible using the FF-BS recursion and it has an user-controllable time complexity that depends on the volume of the Hamming ball. An illustrative example of how the algorithm operates is given in Figure 2(b).\nTo be ergodic the above sampling scheme (under standard conditions) the auxiliary variable U must be allowed to move away from the current X(t) (the value of X at the t-th iteration) which implies that the radiusmmust be strictly larger than zero. Furthermore, the maximum distance a newX(t+1) can travel away from the current X(t) in a single iteration is 2mN bits (assuming m ≤ K/2). This is because resampling a U given the current X(t) can select a U that differs at most mN bits from X(t), while subsequently sampling X(t+1) given U further adds at most other mN bits."
    }, {
      "heading" : "3.2 Extensions",
      "text" : "So far we have defined Hamming ball sampling assuming binary factor chains in the FHMM. It is possible to generalize the whole approach to deal with factor chains that can take values in general finite discrete state spaces. Suppose that each hidden variable takes P values so that the matrix X ∈ {1, . . . , P}K×N . Exactly as in the binary case, the Hamming distance between the auxiliary vector ui ∈ {1, . . . , P}K and the corresponding i-th column xi of X is the number of elements these two vectors disagree. Based on this we can define the i-th location Hamming ball of radius m as the set of all uis satisfying Eq. (3) which has cardinality\nM = m∑ j=0 (P − 1)j ( K j ) . (10)\nThis, for m = 1 is equal (P − 1)K + 1, for m = 2 it is equal to (P − 1)2K(K−1)2 + (P − 1)K + 1 and so forth. Notice that for the binary case, where P = 2, all these expressions reduce to the ones from Section 3.1. Then, the sampling scheme from the previous section can be applied unchanged where in one step we sample U given the current X and in the second step we sample X given U using the FF-BS recursion.\nAnother direction of extending the method is to vary the structure of the uniform distribution p(U |X) which essentially determines the exploration area around the current value of X . We can even add randomness in the structure of this distribution by further expanding the joint density in Eq. (8) with random variables that determine this structure. For instance, we can consider a distribution p(m) over the radius m that covers a range of possible values and then sample iteratively (U,m) from p(U |X,m)p(m) and X from p(X|Y,U,m) ∝ p(Y |X)p(X)p(U |X,m). This scheme remains valid since essentially it is Gibbs sampling in an augmented probability model where we added the auxiliary variables (U,m). In practical implementation, such a scheme would place high prior probability on small values of m where sampling iterations would be fast to compute and enable efficient exploration of local structure but, with non-zero probabilities on larger values on m, the sampler could still periodically consider larger portions of the model space that would allow more significant changes to the configuration of X .\nMore generally, we can determine the structure of p(U |X) through a set of radius constraints m = (m1, . . . ,mQ) and base our sampling on the augmented density\np(Y,X,U,m) = p(Y |X)p(X)p(U |X,m)p(m). (11)\nFor instance, we can choose m = (m1, . . . ,mN ) and consider mi as determining the radius of the i-location Hamming ball (for the column xi) so that the corresponding uniform distribution over ui becomes p(ui|xi,mi) ∝ I(h(ui,xi) ≤ mi). This could allow for asymmetric local moves where in some part of the hidden sequence (where mis are large) we allow for greater exploration compared to others where the exploration can be more constrained. This could lead to more efficient variations of the Hamming Ball sampler where the vector m could be automatically tuned during sampling to focus computational effort in regions of the sequence where there is most uncertainty in the underlying latent structure of X .\nIn a different direction, we could introduce the constraints m = (m1, . . . ,mK) associated with the rows of X instead of the columns. This can lead to obtain regular Gibbs sampling as a special case. In particular, if p(m) is chosen so that in a random draw we pick a single k such that mk = N and the rest mk′ = 0, then we essentially freeze all rows of X apart from the k-th row1 and thus allowing the subsequent step of sampling X to reduce to exact sampling the k-th row of X using the FF-BS recursion. Under this perspective, block Gibbs sampling for FHMMs can be seen as a special case of Hamming ball sampling.\nFinally, there maybe utility in developing other proposals for sampling U based on distributions other than the uniform approach used here. For example, a local exponentially weighted proposal of the form p(U |X) ∝ ∏N i=1 exp(−λh(ui,xi))I(h(ui,xi) ≤ m), would keep the centre of the proposed Hamming ball closer to its current location enabling more efficient exploration of local configurations. However, in developing alternative proposals, it is crucial that the normalizing constant of p(U |X) is computed efficiently so that the overall time complexity remains O(M2N)."
    }, {
      "heading" : "4 Experiments",
      "text" : "To demonstrate Hamming ball (HB) sampling we consider an additive FHMM as the one used in [6] and popularized recently for energy disaggregation applications [7, 10, 11]. In this model, each k-th factor chain interacts with the data through an associated mean vector wk ∈ RD so that each observed output yi is taken to be a noisy version of the sum of all factor vectors activated at time i:\nyi = w0 + K∑ k=1 wkxk,i + ηi, (12)\n1In particular, for the rows k′ 6= k the corresponding uniform distribution over uk′,is collapses to a point delta mass centred at the previous states xk′,is.\nwhere w0 is an extra bias term while ηi is white noise that typically follows a Gaussian: ηi ∼ N (0, σ2I). Using this model we demonstrate the proposed method using an artificial dataset in Section 4.1 and a real dataset [11] in energy disaggregation in Section 4.2. In all examples, we compare HB with block Gibbs (BG) sampling."
    }, {
      "heading" : "4.1 Simulated dataset",
      "text" : "Here, we wish to investigate the ability of HB and BG sampling schemes to efficient escape from local modes of the posterior distribution. We consider an artificial data sequence of length N = 200 generated as follows. We simulated K = 5 factor chains (with vk = 0.5 , ρk = 0.05, k = 1, . . . , 5) which subsequently generated observations in the 25-dimensional space according to the additive FHMM from Eq. (12) assuming Gaussian noise with variance σ2 = 0.05. The associated factor vector where selected to be wk = wk ∗Maskk where wk = 0.8 + 0.05 ∗ (k − 1), k = 1, . . . , 5 and Maskk denotes a 25-dimensional binary vector or a mask. All binary masks are displayed as 5 × 5 binary images in Figure 1(a) in the supplementary file together with few examples of generated data points. Finally, the bias term w0 was set to zero.\nWe assume that the ground-truth model parameters θ = ({vk, ρk,wk, }Kk=1,w0, σ2) that generated the data are known and our objective is to do posterior inference over the latent factors X ∈ {0, 1}5×200, i.e. to draw samples from the conditional posterior distribution p(X|Y,θ). Since the data have been produced with small noise variance, this exact posterior is highly picked with most all the probability mass concentrated on the single configuration Xtrue that generated the data. So the question is whether BG and HB schemes will able to discover the “unknown” Xtrue from a random initialization. We tested three block Gibbs sampling schemes: BG1, BG2 and BG3 that jointly sample blocks of rows of size one, two or three respectively. For each algorithm a full iteration is chosen to be a complete pass over all possible combinations of rows so that the time complexity per iteration for BG1 is O(20N), for BG2 is O(160N) and for BG3 is O(640N). Regarding HB sampling we considered three schemes: HB1, HB2 and HB3 with radius m = 1, 2 and 3 respectively. The time complexities for these HB algorithms were O(36N), O(256N) and O(676N). Notice that an exact sample from the posterior distribution can be drawn in O(1024N) time.\nWe run all algorithms assuming the same random initialization X(0) so that each bit was chosen from the uniform distribution. Figure 3(a) shows the evolution of the error of misclassified bits in X , i.e. the number of bits the state X(t) disagrees with the ground-truth Xtrue. Clearly, HB2 and HB3 discover quickly the optimal solution with HB3 being slightly faster. HB1 is unable to discover the ground-truth but it outperforms BG1 and BG2. All the block Gibbs sampling schemes, including the most expensive BG3 one, failed to reach Xtrue."
    }, {
      "heading" : "4.2 Energy disaggregation",
      "text" : "Here, we consider a real-world example from the field of energy disaggregation where the objective is to determine the component devices from an aggregated electricity signal. This technology is use-\nful because having a decomposition, into components for each device, of the total electricity usage in a household or building can be very informative to consumers and increase awareness of energy consumption which subsequently can lead to possibly energy savings. For full details regarding the energy disaggregation application see [7, 10, 11]. Next we consider a publicly available data set2, called the Reference Energy Disaggregation Data Set (REDD) [11], to test the HB and BG sampling algorithms. The REDD data set contains several types of home electricity data for many different houses recorded during several weeks. Next, we will consider the main signal power of house_1 for seven days which is a temporal signal of length 604, 800 since power was recorded every second. We further downsampled this signal to every 9 seconds to obtain a sequence of 67, 200 size in which we applied the FHMM described below.\nEnergy disaggregation can be naturally tackled by an additive FHMM framework, as realized in [10, 11], where an observed total electricity power yi at time instant i is the sum of individual powers for all devices that are “on” at that time. Therefore, the observation model from Eq. (12) can be used to model this situation with the constraint that each device contribution wk (which is a scalar) is restricted to be non-negative. We assume an FHMM with K = 10 factors and we follow a Bayesian framework where each wk is parametrized by the exponential transformation, i.e. wk = e\nw̃k , and a vague zero-mean Gaussian prior is assigned on w̃k. To learn these factors we apply unsupervised learning using as training data the first day of recorded data. This involves applying an Metropolis-within-Gibbs type of MCMC algorithm that iterates between the following three steps: i) sampling X , ii) sampling each w̃k individually using its own Gaussian proposal distribution and accepting or rejecting based on the M-H step and iii) sampling the noise variance σ2 based on its conjugate Gamma posterior distribution. Notice that the step ii) involves adapting the variance of the Gaussian proposal to achieve an acceptance ratio between 20 and 40 percent following standard ideas from adaptive MCMC. For the first step we consider one of the following four algorithms: BG1, BG2, HB1 and HB2 defined in the previous section. Once the FHMM has been trained then we would like to do predictions and infer the posterior distribution over the hidden factors for a test sequence, that will consist of the remaining six days, according to\np(X∗|Y∗, Y ) = ∫ p(X∗|Y∗,W, σ2)p(W,σ2|Y )dWdσ2 ≈ 1\nT T∑ t=1 p(X∗|Y∗,W (t), (σ2)(t)), (13)\nwhere Y∗ denotes the test observations and X∗ the corresponding hidden sequence we wish to infer3. This computation requires to be able to simulate from p(X∗|Y∗,W, σ2) for a given fixed setting for the parameters (W,σ2). Such prediction step will tell us which factors are “on” at each time. Such factors could directly correspond to devices in the household, such as Electronics, Lighting, Refrigerator etc, however since our learning approach is purely unsupervised we will not attempt to establish correspondences between the inferred factors and the household appliances and, instead, we will focus on comparing the ability of the sampling algorithms to escape from local modes of the posterior distribution. To quantify such ability we will consider the mean squared error (MSE) between the model mean predictions and the actual data. Clearly, MSE for the test data can measure how well the model predicts the unseen electricity powers, while MSE at the training phase can indicate how well the chain mixes and reaches areas with high probability mass (where training data are reconstructed with small error). Figure 3(b) shows the evolution of MSE through the sampling iterations for the four MCMC algorithms used for training. Figure 3(c) shows the corresponding curves for the prediction phase, i.e. when sampling from p(X∗|Y∗,W, σ2) given a representative sample from the posterior p(W,σ2|Y ). All four MSE curves in Figure 3(c) are produced by assuming the same setting for (W,σ2) so that any difference observed between the algorithms depends solely on the ability to sample from p(X∗|Y∗,W, σ2). Finally, Figure 4 shows illustrative plots on how we fit the data for all seven days (first row) and how we predict the test data on the second day (second row) together with corresponding inferred factors for the six most dominant hidden states (having the largest inferred wk values). The plots in Figure 4 were produced based on the HB2 output.\nSome conclusions we can draw are the following. Firstly, Figure 3(c) clearly indicate that both HB algorithms for the prediction phase, where the factor weightswk are fixed and given, are much better than block Gibbs samplers in escaping from local modes and discovering hidden state configurations\n2Available from http://redd.csail.mit.edu/. 3Notice that we have also assumed that the training and test sequences are conditionally independent given\nthe model parameters (W,σ2).\nthat explain more efficiently the data. Moreover, HB2 is clearly better than HB1, as expected, since it considers larger global moves. When we are jointly sampling weights wk and their interacting latent binary states (as done in the training MCMC phase), then, as Figure 3(b) shows, block Gibbs samplers can move faster towards fitting the data and exploring local modes while HB schemes are slower in terms of that. Nevertheless, the HB2 algorithm eventually reaches an area with smaller MSE error than the block Gibbs samplers."
    }, {
      "heading" : "5 Discussion",
      "text" : "Exact sampling using FF-BS over the entire model space for the FHMM is intractable. Alternative solutions based on conditional updating approaches that use locally asymmetric moves will lead to poor mixing due to the sampler becoming trapped in local modes. We have shown that the Hamming ball sampler gives a relative improvement over conditional approaches through the use of locally symmetric moves that permits joint updating of hidden chains and improves mixing.\nWhilst we have presented the Hamming ball sampler applied to the factorial hidden Markov model, it is applicable to any statistical model where the observed data vector yi depends only on the i-th column of a binary latent variable matrix X and observed data Y and hence the joint density can be factored as p(X,Y ) ∝ p(X) ∏N i=1 p(yi|xi). Examples include the spike and slab variable selection models in Bayesian linear regression [12] and multiple membership models including Bayesian nonparametric models that utilize the Indian buffet process [13, 14]. While, in standard versions of these models, the columns of X are independent and posterior inference is trivially parallelizable, the utility of the Hamming ball sampler arises where K is large and sampling individual columns of X is itself computationally very demanding. Other suitable models that might be applicable include more complex dependence structures that involve coupling between Markov chains and undirected dependencies."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the reviewers for insightful comments. MKT greatly acknowledges support from “Research Funding at AUEB for Excellence and Extroversion, Action 1: 2012-2014”. CY acknowledges the support of a UK Medical Research Council New Investigator Research Grant (Ref No. MR/L001411/1). CY is also affiliated with the Department of Statistics, University of Oxford."
    } ],
    "references" : [ {
      "title" : "A tutorial on hidden Markov models and selected applications in speech recognition",
      "author" : [ "Lawrence Rabiner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1989
    }, {
      "title" : "Bayesian methods for hidden Markov models",
      "author" : [ "Steven L Scott" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Modeling linkage disequilibrium and identifying recombination hotspots using single-nucleotide polymorphism",
      "author" : [ "Na Li", "Matthew Stephens" ],
      "venue" : "data. Genetics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Genotype imputation for genome-wide association studies",
      "author" : [ "Jonathan Marchini", "Bryan Howie" ],
      "venue" : "Nature Reviews Genetics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2010
    }, {
      "title" : "OncoSNP-SEQ: a statistical approach for the identification of somatic copy number alterations from next-generation sequencing of cancer",
      "author" : [ "Christopher Yau" ],
      "venue" : "genomes. Bioinformatics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Factorial hidden Markov models",
      "author" : [ "Zoubin Ghahramani", "Michael I. Jordan" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1997
    }, {
      "title" : "Approximate inference in additive factorial HMMs with application to energy disaggregation",
      "author" : [ "J Zico Kolter", "Tommi Jaakkola" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Nonuniversal critical dynamics in Monte Carlo simulations",
      "author" : [ "Robert H Swendsen", "Jian-Sheng Wang" ],
      "venue" : "Physical review letters,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1987
    }, {
      "title" : "Unsupervised disaggregation of low frequency power measurements",
      "author" : [ "Hyungsul Kim", "Manish Marwah", "Martin F. Arlitt", "Geoff Lyon", "Jiawei Han" ],
      "venue" : "In SDM,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "REDD: a public data set for energy disaggregation research",
      "author" : [ "J. Zico Kolter", "Matthew J. Johnson" ],
      "venue" : "In SustKDD Workshop on Data Mining Applications in Sustainability,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Bayesian variable selection in linear regression",
      "author" : [ "Toby J Mitchell", "John J Beauchamp" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1988
    }, {
      "title" : "Infinite latent feature models and the Indian buffet process",
      "author" : [ "Thomas L Griffiths", "Zoubin Ghahramani" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "The infinite factorial hidden Markov model",
      "author" : [ "J. Van Gael", "Y.W. Teh", "Z. Ghahramani" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The hidden Markov model (HMM) [1] is one of the most widely and successfully applied statistical models for the description of discrete time series data.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Much of its success lies in the availability of efficient computational algorithms that allows the calculation of key quantities necessary for statistical inference [1, 2].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "Much of its success lies in the availability of efficient computational algorithms that allows the calculation of key quantities necessary for statistical inference [1, 2].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 2,
      "context" : "In particular, the HMM has seen considerable use in areas such as bioinformatics and computational biology where non-trivially sized datasets are commonplace [3, 4, 5].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "In particular, the HMM has seen considerable use in areas such as bioinformatics and computational biology where non-trivially sized datasets are commonplace [3, 4, 5].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "In particular, the HMM has seen considerable use in areas such as bioinformatics and computational biology where non-trivially sized datasets are commonplace [3, 4, 5].",
      "startOffset" : 158,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : "The factorial hidden Markov model (FHMM) [6] is an extension of the HMM where multiple independent hidden chains run in parallel and cooperatively generate the observed data.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 5,
      "context" : "Exact Monte Carlo inference can be achieved by applying the standard forward-filteringbackward-sampling (FF-BS) algorithm to simulate a sample from p(X|Y ) in O(2N) time (the independence of the Markov chains can be exploited to reduce this complexity toO(2KN) [6]).",
      "startOffset" : 261,
      "endOffset" : 264
    }, {
      "referenceID" : 5,
      "context" : "As a consequence, inference in FHMMs has become somewhat synonymous with approximate methods such as variational inference [6, 7].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "As a consequence, inference in FHMMs has become somewhat synonymous with approximate methods such as variational inference [6, 7].",
      "startOffset" : 123,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "The sampling strategy adopts the use of an auxiliary variable construction, similar to slice sampling [8] and the Swendsen-Wang algorithm [9], that allows the automatic selection of the sequence of restricted configuration spaces.",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "To demonstrate Hamming ball (HB) sampling we consider an additive FHMM as the one used in [6] and popularized recently for energy disaggregation applications [7, 10, 11].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "To demonstrate Hamming ball (HB) sampling we consider an additive FHMM as the one used in [6] and popularized recently for energy disaggregation applications [7, 10, 11].",
      "startOffset" : 158,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : "To demonstrate Hamming ball (HB) sampling we consider an additive FHMM as the one used in [6] and popularized recently for energy disaggregation applications [7, 10, 11].",
      "startOffset" : 158,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "To demonstrate Hamming ball (HB) sampling we consider an additive FHMM as the one used in [6] and popularized recently for energy disaggregation applications [7, 10, 11].",
      "startOffset" : 158,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "1 and a real dataset [11] in energy disaggregation in Section 4.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : "For full details regarding the energy disaggregation application see [7, 10, 11].",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 8,
      "context" : "For full details regarding the energy disaggregation application see [7, 10, 11].",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "For full details regarding the energy disaggregation application see [7, 10, 11].",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "Next we consider a publicly available data set2, called the Reference Energy Disaggregation Data Set (REDD) [11], to test the HB and BG sampling algorithms.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 8,
      "context" : "Energy disaggregation can be naturally tackled by an additive FHMM framework, as realized in [10, 11], where an observed total electricity power yi at time instant i is the sum of individual powers for all devices that are “on” at that time.",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "Energy disaggregation can be naturally tackled by an additive FHMM framework, as realized in [10, 11], where an observed total electricity power yi at time instant i is the sum of individual powers for all devices that are “on” at that time.",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Examples include the spike and slab variable selection models in Bayesian linear regression [12] and multiple membership models including Bayesian nonparametric models that utilize the Indian buffet process [13, 14].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 11,
      "context" : "Examples include the spike and slab variable selection models in Bayesian linear regression [12] and multiple membership models including Bayesian nonparametric models that utilize the Indian buffet process [13, 14].",
      "startOffset" : 207,
      "endOffset" : 215
    }, {
      "referenceID" : 12,
      "context" : "Examples include the spike and slab variable selection models in Bayesian linear regression [12] and multiple membership models including Bayesian nonparametric models that utilize the Indian buffet process [13, 14].",
      "startOffset" : 207,
      "endOffset" : 215
    } ],
    "year" : 2014,
    "abstractText" : "We introduce a novel sampling algorithm for Markov chain Monte Carlo-based Bayesian inference for factorial hidden Markov models. This algorithm is based on an auxiliary variable construction that restricts the model space allowing iterative exploration in polynomial time. The sampling approach overcomes limitations with common conditional Gibbs samplers that use asymmetric updates and become easily trapped in local modes. Instead, our method uses symmetric moves that allows joint updating of the latent sequences and improves mixing. We illustrate the application of the approach with simulated and a real data example.",
    "creator" : null
  }
}