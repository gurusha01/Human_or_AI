{
  "name" : "6cdd60ea0045eb7a6ec44c54d29ed402.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Robust Logistic Regression and Classification",
    "authors" : [ "Jiashi Feng", "Huan Xu", "Shie Mannor", "Shuicheng Yan" ],
    "emails" : [ "jshfeng@berkeley.edu", "mpexuh@nus.edu.sg", "shie@ee.technion.ac.il", "eleyans@nus.edu.sg" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Logistic regression (LR) is a standard probabilistic statistical classification model that has been extensively used across disciplines such as computer vision, marketing, social sciences, to name a few. Different from linear regression, the outcome of LR on one sample is the probability that it is positive or negative, where the probability depends on a linear measure of the sample. Therefore, LR is actually widely used for classification. More formally, for a sample xi ∈ Rp whose label is denoted as yi, the probability of yi being positive is predicted to be P{yi = +1} = 1\n1+e−β >xi , given the LR model parameter β. In order to obtain a parameter that performs well, often a set of labeled samples {(x1, y1), . . . , (xn, yn)} are collected to learn the LR parameter β which maximizes the induced likelihood function over the training samples.\nHowever, in practice, the training samples x1, . . . , xn are usually noisy and some of them may even contain adversarial corruptions. Here by “adversarial”, we mean that the corruptions can be arbitrary, unbounded and are not from any specific distribution. For example, in the image/video classification task, some images or videos may be corrupted unexpectedly due to the error of sensors or the severe occlusions on the contained objects. Those corrupted samples, which are called outliers, can skew the parameter estimation severely and hence destroy the performance of LR.\nTo see the sensitiveness of LR to outliers more intuitively, consider a simple example where all the samples xi’s are from one-dimensional space R, as shown in Figure 1. Only using the inlier samples provides a correct LR parameter (we here show the induced function curve) which explains the inliers well. However, when only one sample is corrupted (which is originally negative but now closer to the positive samples), the resulted regression curve is distracted far away from the ground truth one and the label predictions on the concerned inliers are completely wrong. This demonstrates that LR is indeed fragile to sample corruptions. More rigorously, the non-robustness of LR can be shown via calculating its influence function [7] (detailed in the supplementary material).\nAs Figure 1 demonstrates, the maximal-likelihood estimate of LR is extremely sensitive to the presence of anomalous data in the sample. Pregibon also observed this non-robustness of LR in [14]. To solve this important issue of LR, Pregibon [14], Cook and Weisberg [4] and Johnson [9] proposed procedures to identify observations which are influential for estimating β based on certain outlyingness measure. Stefanski et al. [16, 10] and Bianco et al. [2] also proposed robust estimators which, however, require to robustly estimating the covariate matrix or boundedness on the outliers. Moreover, the breakdown point1 of those methods is generally inversely proportional to the sample dimensionality and diminishes rapidly for high-dimensional samples.\nWe propose a new robust logistic regression algorithm, called RoLR, which optimizes a robustified linear correlation between response y and linear measure 〈β, x〉 via an efficient linear programmingbased procedure. We demonstrate that the proposed RoLR achieves robustness to arbitrarily covariate corruptions. Even when a constant fraction of the training samples are corrupted, RoLR is still able to learn the LR parameter with a non-trivial upper bound on the error. Besides this theoretical guarantee of RoLR on the parameter estimation, we also provide the empirical and population risks bounds for RoLR. Moreover, RoLR only needs to solve a linear programming problem and thus is scalable to large-scale data sets, in sharp contrast to previous LR optimization algorithms which typically resort to (computationally expensive) iterative reweighted method [11]. The proposed RoLR can be easily adapted to solving binary classification problems where corrupted training samples are present. We also provide theoretical classification performance guarantee for RoLR. Due to the space limitation, we defer all the proofs to the supplementary material."
    }, {
      "heading" : "2 Related Works",
      "text" : "Several previous works have investigated multiple approaches to robustify the logistic regression (LR) [15, 13, 17, 16, 10]. The majority of them are M-estimator based: minimizing a complicated and more robust loss function than the standard loss function (negative log-likelihood) of LR. For example, Pregiobon [15] proposed the following M-estimator:\nβ̂ = arg min β n∑ i=1 ρ(`i(β)),\nwhere `i(·) is the negative log-likelihood of the ith sample xi and ρ(·) is a Huber type function [8] such as\nρ(t) = { t, if t ≤ c, 2 √ tc− c, if t > c,\nwith c a positive parameter. However, the result from such estimator is not robust to outliers with high leverage covariates as shown in [5].\n1It is defined as the percentage of corrupted points that can make the output of an algorithm arbitrarily bad.\nRecently, Ding et al [6] introduced the T -logistic regression as a robust alternative to the standard LR, which replaces the exponential distribution in LR by t-exponential distribution family. However, T -logistic regression only guarantees that the output parameter converges to a local optimum of the loss function instead of converging to the ground truth parameter.\nOur work is largely inspired by following two recent works [3, 13] on robust sparse regression. In [3], Chen et al. proposed to replace the standard vector inner product by a trimmed one, and obtained a novel linear regression algorithm which is robust to unbounded covariate corruptions. In this work, we also utilize this simple yet powerful operation to achieve robustness. In [13], a convex programming method for estimating the sparse parameters of logistic regression model is proposed:\nmax β m∑ i=1 yi〈xi, β〉, s.t. ‖β‖1 ≤ √ s, ‖β‖ ≤ 1,\nwhere s is the sparseness prior parameter on β. However, this method is not robust to corrupted covariate matrix. Few or even one corrupted sample may dominate the correlation in the objective function and yield arbitrarily bad estimations. In this work, we propose a robust algorithm to remedy this issue."
    }, {
      "heading" : "3 Robust Logistic Regression",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Setup",
      "text" : "We consider the problem of logistic regression (LR). Let Sp−1 denote the unit sphere andBp2 denote the Euclidean unit ball in Rp. Let β∗ be the groundtruth parameter of the LR model. We assume the training samples are covariate-response pairs {(xi, yi)}n+n1i=1 ⊂ Rp × {−1,+1}, which, if not corrupted, would obey the following LR model:\nP{yi = +1} = τ(〈β∗, xi〉+ vi), (1) where the function τ(·) is defined as: τ(z) = 11+e−z . The additive noise vi ∼ N (0, σ 2 e) is an i.i.d. Gaussian random variable with zero mean and variance of σ2e . In particular, when we consider the noiseless case, we assume σ2e = 0. Since LR only depends on 〈β∗, xi〉, we can always scale the samples xi to make the magnitude of β∗ less than 1. Thus, without loss of generality, we assume that β∗ ∈ Sp−1. Out of the n + n1 samples, a constant number (n1) of the samples may be adversarially corrupted, and we make no assumptions on these outliers. Throughout the paper, we use λ , n1n to denote the outlier fraction. We call the remaining n non-corrupted samples “authentic” samples, which obey the following standard sub-Gaussian design [12, 3]. Definition 1 (Sub-Gaussian design). We say that a random matrix X = [x1, . . . , xn] ∈ Rp×n is sub-Gaussian with parameter ( 1nΣx, 1 nσ 2 x) if: (1) each column xi ∈ Rp is sampled independently from a zero-mean distribution with covariance 1nΣx, and (2) for any unit vector u ∈ R p, the random variable u>xi is sub-Gaussian with parameter2 1√nσx.\nThe above sub-Gaussian random variables have several nice concentration properties, one of which is stated in the following Lemma [12]. Lemma 1 (Sub-Gaussian Concentration [12]). Let X1, . . . , Xn be n i.i.d. zero-mean subGaussian random variables with parameter σx/\n√ n and variance at most σ2x/n. Then we have∣∣∑n\ni=1X 2 i − σ2x ∣∣ ≤ c1σ2x√ log pn , with probability of at least 1− p−2 for some absolute constant c1. Based on the above concentration property, we can obtain following bound on the magnitude of a collection of sub-Gaussian random variables [3]. Lemma 2. SupposeX1, . . . , Xn are n independent sub-Gaussian random variables with parameter σx/ √ n. Then we have maxi=1,...,n|Xi| ≤ 4σx √ (log n+ log p)/n with probability of at least 1− p−2. 2Here, the parameter means the sub-Gaussian norm of the random variable Y , ‖Y ‖ψ2 = supq≥1 q −1/2(E|Y |q)1/q .\nAlso, this lemma provides a rough bound on the magnitude of inlier samples, and this bound serves as a threshold for pre-processing the samples in the following RoLR algorithm."
    }, {
      "heading" : "3.2 RoLR Algorithm",
      "text" : "We now proceed to introduce the details of the proposed Robust Logistic Regression (RoLR) algorithm. Basically, RoLR first removes the samples with overly large magnitude and then maximizes a trimmed correlation of the remained samples with the estimated LR model. The intuition behind the RoLR maximizing the trimmed correlation is: if the outliers have too large magnitude, they will not contribute to the correlation and thus not affect the LR parameter learning. Otherwise, they have bounded affect on the LR learning (which actually can be bounded by the inlier samples due to our adopting the trimmed statistic). Algorithm 1 gives the implementation details of RoLR.\nAlgorithm 1 RoLR Input: Contaminated training samples {(x1, y1), . . . , (xn+n1 , yn+n1)}, an upper bound on the number of outliers n1, number of inliers n and sample dimension p. Initialization: Set T = 4 √ log p/n+ log n/n.\nPreprocessing: Remove samples (xi, yi) whose magnitude satisfies ‖xi‖ ≥ T . Solve the following linear programming problem (see Eqn. (3)):\nβ̂ = arg max β∈Bp2 n∑ i=1 [y〈β, x〉](i).\nOutput: β̂.\nNote that, within the RoLR algorithm, we need to optimize the following sorted statistic:\nmax β∈Bp2 n∑ i=1 [y〈β, x〉](i). (2)\nwhere [·](i) is a sorted statistic such that [z](1) ≤ [z](2) ≤ . . . ≤ [z](n), and z denotes the involved variable. The problem in Eqn. (2) is equivalent to minimizing the summation of top n variables, which is a convex one and can be solved by an off-the-shelf solver (such as CVX). Here, we note that it can also be converted to the following linear programming problem (with a quadratic constraint), which enjoys higher computational efficiency. To see this, we first introduce auxiliary variables ti ∈ {0, 1} as indicators of whether the corresponding terms yi〈β,−xi〉 fall in the smallest n ones. Then, we write the problem in Eqn. (2) as\nmax β∈Bp2 min ti n+n1∑ i=1 ti · yi〈β, xi〉, s.t. n+n1∑ i=1 ti ≤ n, 0 ≤ ti ≤ 1.\nHere the constraints of ∑n+n1 i=1 ti ≤ n, 0 ≤ ti ≤ 1 are from standard reformulation of ∑n+n1 i=1 ti = n, ti ∈ {0, 1}. Now, the above problem becomes a max-min linear programming. To decouple the variables β and ti, we turn to solving the dual form of the inner minimization problem. Let ν, and ξi be the Lagrange multipliers for the constraints ∑n+n1 i=1 ti ≤ n and ti ≤ 1 respectively. Then the dual form w.r.t. ti of the above problem is:\nmax β,ν,ξi −ν · n− n+n1∑ i=1 ξi, s.t. yi〈β, xi〉+ ν + ξi ≥ 0, β ∈ Bp2 , ν ≥ 0, ξi ≥ 0. (3)\nReformulating logistic regression into a linear programming problem as above significantly enhances the scalability of LR in handling large-scale datasets, a property very appealing in practice, since linear programming is known to be computationally efficient and has no problem dealing with up to 1× 106 variables in a standard PC."
    }, {
      "heading" : "3.3 Performance Guarantee for RoLR",
      "text" : "In contrast to traditional LR algorithms, RoLR does not perform a maximal likelihood estimation. Instead, RoLR maximizes the correlation yi〈β, xi〉 . This strategy reduces the computational complexity of LR, and more importantly enhances the robustness of the parameter estimation, using\nthe fact that the authentic samples usually have positive correlation between the yi and 〈β, xi〉, as described in the following lemma. Lemma 3. Fix β ∈ Sp−1. Suppose that the sample (x, y) is generated by the model described in (1). The expectation of the product y〈β, x〉 is computed as:\nEy〈β, x〉 = E sech2(g/2), where g ∈ N (0, σ2x + σ2e) is a Gaussian random variable and σ2e is the noise level in (1). Furthermore, the above expectation can be bounded as follows,\nϕ+(σ2e , σ 2 x) ≤ Ey〈β, x〉 ≤ ϕ−(σ2e , σ2x).\nwhere ϕ+(σ2e , σ 2 x) and ϕ −(σ2e , σ 2 x) are positive. In particular, they can take the form of ϕ+(σ2e , σ 2 x) = σ2x 3 sech\n2 (\n1+σ2e 2 ) and ϕ−(σ2e , σ 2 x) = σ2x 3 + σ2x 6 sech 2 ( 1+σ2e 2 ) .\nThe following lemma shows the difference of correlations is an effective surrogate for the difference of the LR parameters. Thus we can always minimize the difference of ‖β̂−β∗‖ through maximizing∑ i yi〈β̂, xi〉. Lemma 4. Fix β ∈ Sp−1 as the groundtruth parameter in (1) and β′ ∈ Bp2 . Denote η = Ey〈β, x〉. Then Ey〈β′, x〉 = η〈β, β′〉, and thus,\nE [y〈β, x〉 − y〈β′, x〉] = η(1− 〈β, β′〉) ≥ η 2 ‖β − β′‖22.\nBased on these two lemmas, along with some concentration properties of the inlier samples (shown in the supplementary material), we have the following performance guarantee of RoLR on LR model parameter recovery.\nTheorem 1 (RoLR for recovering LR parameter). Let λ , n1n be the outlier fraction, β̂ be the output of Algorithm 1, and β∗ be the ground truth parameter. Suppose that there are n authentic samples generated by the model described in (1). Then we have, with probability larger than 1 − 4 exp(−c2n/8),\n‖β̂ − β∗‖ ≤ 2λϕ −(σ2e , σ 2 x)\nϕ+(σ2e , σ 2 x)\n+ 2(λ+ 4 + 5\n√ λ)\nϕ+(σ2e , σ 2 x)\n√ p\nn +\n8λ\nϕ+(σ2e , σ 2 x) σ2x\n√ log p\nn +\nlog n\nn .\nHere c2 is an absolute constant. Remark 1. To make the above results more explicit, we consider the asymptotic case where p/n→ 0. Thus the above bounds become\n‖β̂ − β∗‖ ≤ 2λϕ −(σ2e , σ 2 x)\nϕ+(σ2e , σ 2 x) ,\nwhich holds with probability larger than 1−4 exp(−c2n/8). In the noiseless case, i.e., σe = 0, and assuming σ2x = 1, we have ϕ +(σ2e) = 1 3 sech 2 ( 1 2 ) ≈ 0.2622 and ϕ−(σ2e +1) = 13 + 1 6 sech 2 ( 1 2 ) ≈ 0.4644. The ratio is ϕ−/ϕ+ ≈ 1.7715. Thus the bound is simplified to:\n‖β̂ − β∗‖ . 3.54λ.\nRecall that β̂, β∗ ∈ Sp−1 and the maximal value of ‖β̂ − β∗‖ is 2. Thus, for the above result to be non-trivial, we need 3.54λ ≤ 2, namely λ ≤ 0.56. In other words, in the noiseless case, the RoLR is able to estimate the LR parameter with a non-trivial error bound (also known as a “breakdown point”) with up to 0.56/1.56× 100% = 36% of the samples being outliers."
    }, {
      "heading" : "4 Empirical and Population Risk Bounds of RoLR",
      "text" : "Besides the parameter recovery, we are also concerned about the prediction performance of the estimated LR model in practice. The standard prediction loss function `(·, ·) of LR is a non-negative and bounded function, and is defined as:\n`((xi, yi), β) = 1\n1 + exp{−yiβ>xi} . (4)\nThe goodness of an LR predictor β is measured by its population risk: R(β) = EP (X,Y )`((x, y), β),\nwhere P (X,Y ) describes the joint distribution of covariate X and response Y . However, the population risk rarely can be calculated directly as the distribution P (X,Y ) is usually unknown. In practice, we often consider the empirical risk, which is calculated over the provided training samples as follows:\nRemp(β) = 1\nn n∑ i=1 `((xi, yi), β).\nNote that the empirical risk is computed only over the authentic samples, hence cannot be directly optimized when outliers exist.\nBased on the bound of ‖β̂−β∗‖ provided in Theorem 1, we can easily obtain the following empirical risk bound for RoLR as the LR loss function given in Eqn. (4) is Lipschitz continuous.\nCorollary 1 (Bound on the empirical risk). Let β̂ be the output of Algorithm 1, and β∗ be the optimal parameter minimizing the empirical risk. Suppose that there are n authentic samples generated by the model described in (1). Define X , 4σx √ (log n+ log p)/n. Then we have, with probability larger than 1− 4 exp(−c2n/8), the empirical risk of β̂ is bounded by,\nRemp(β̂)−Remp(β∗) ≤ X { 2λ ϕ−(σ2e , σ 2 x)\nϕ+(σ2e , σ 2 x)\n+ 2(λ+ 4 + 5\n√ λ)\nϕ+(σ2e , σ 2 x)\n√ p\nn\n+ 8λσ2x\nϕ+(σ2e , σ 2 x)\n√ log p\nn +\nlog n\nn\n} .\nGiven the empirical risk bound, we can readily obtain the bound on the population risk by referring to standard generalization results in terms of various function class complexities. Some widely used complexity measures include the VC-dimension [18] and the Rademacher and Gaussian complexity [1]. Compared with the Rademacher complexity which is data dependent, the VC-dimension is more universal although the resulting generalization bound can be slightly loose. Here, we adopt the VC-dimension to measure the function complexity and obtain the following population risk bound.\nCorollary 2 (Bound on the population risk). Let β̂ be the output of Algorithm 1, and β∗ be the optimal parameter. Suppose the parameter space Sp−1 3 β has finite VC dimension d. There are n authentic samples are generated by the model described in (1). Define X , 4σx √ (log n+ log p)/n. Then we have, with high probability larger larger than 1− 4 exp(−c2n/8)− δ, the population risk of β̂ is bounded by,\nR(β̂)−R(β∗) ≤ X { 2λ ϕ−(σ2e , σ 2 x)\nϕ+(σ2e , σ 2 x)\n+ 2(λ+ 4 + 5\n√ λ)\nϕ+(σ2e , σ 2 x)\n√ p\nn + 8λσ2x ϕ+(σ2e , σ 2 x)\n√ log p\nn +\nlog n\nn\n+2c3\n√ d+ ln(1/δ)\nn\n} .\nHere both c2 and c3 are absolute constants."
    }, {
      "heading" : "5 Robust Binary Classification",
      "text" : ""
    }, {
      "heading" : "5.1 Problem Setup",
      "text" : "Different from the sample generation model for LR, in the standard binary classification setting, the label yi of a sample xi is deterministically determined by the sign of the linear measure of the sample 〈β∗, xi〉. Namely, the samples are generated by the following model: yi = sign (〈β∗, xi〉+ vi) . (5) Here vi is a Gaussian noise as in Eqn. (1). Since yi is deterministically related to 〈β∗, xi〉, the expected correlation Ey〈β, x〉 achieves the maximal value in this setup (ref. Lemma 5), which ensures that the RoLR also performs well for classification. We again assume that the training samples contain n authentic samples and at most n1 outliers."
    }, {
      "heading" : "5.2 Performance Guarantee for Robust Classification",
      "text" : "Lemma 5. Fix β ∈ Sp−1. Suppose the sample (x, y) is generated by the model described in (5). The expectation of the product y〈β, x〉 is computed as:\nEy〈β, x〉 =\n√ 2σ4x\nπ(σ2x + σ 2 v) .\nComparing the above result with the one in Lemma 3, here for the binary classification, we can exactly calculate the expectation of the correlation, and this expectation is always larger than that of the LR setting. The correlation depends on the signal-noise ratio σx/σe. In the noiseless case, σe = 0 and the expected correlation is σx √ 2/π, which is well known as the half-normal distribution. Similarly to analyzing RoLR for LR, based on Lemma 5, we can obtain the following performance guarantee for RoLR in solving classification problems.\nTheorem 2. Let β̂ be the output of Algorithm 1, and β∗ be the optimal parameter minimizing the empirical risk. Suppose there are n authentic samples generated by the model described by (5). Then we have, with large probability larger than 1− 4 exp(−c2n/8),\n‖β̂ − β∗‖2 ≤ 2λ+ 2(λ+ 4 + 5 √ λ)\n√ (σ2e + σ 2 x)πp\n2σ4xn + 8λ\n√ (σ2e + σ 2 x)π\n2\n√ log p\nn +\nlog n\nn .\nThe proof of Theorem 2 is similar to that of Theorem 1. Also, similar to the LR case, based on the above parameter error bound, it is straightforward to obtain the empirical and population risk bounds of RoLR for classification. Due to the space limitation, here we only sketch how to obtain the risk bounds.\nFor the classification problem, the most natural loss function is the 0 − 1 loss. However, 0 − 1 loss function is non-convex, non-smooth, and we cannot get a non-trivial function value bound in terms of ‖β̂ − β∗‖ as we did for the logistic loss function. Fortunately, several convex surrogate loss functions for 0−1 loss have been proposed and achieve good classification performance, which include the hinge loss, exponential loss and logistic loss. These loss functions are all Lipschitz continuous and thus we can bound their empirical and then population risks as for logistic regression."
    }, {
      "heading" : "6 Simulations",
      "text" : "In this section, we conduct simulations to verify the robustness of RoLR along with its applicability for robust binary classification. We compare RoLR with standard logistic regression which estimates the model parameter through maximizing the log-likelihood function.\nWe randomly generated the samples according to the model in Eqn. (1) for the logistic regression problem. In particular, we first sample the model parameter β ∼ N (0, Ip) and normalize it as β := β/‖β‖2. Here p is the dimension of the parameter, which is also the dimension of samples. The samples are drawn i.i.d. from xi ∼ N (0,Σx) with Σx = Ip, and the Gaussian noise is sampled as vi ∼ N (0, σe). Then, the sample label yi is generated according to P{yi = +1} = τ(〈β, xi〉+vi) for the LR case. For the classification case, the sample labels are generated by yi = sign(〈β, xi〉+vi) and additional nt = 1, 000 authentic samples are generated for testing. The entries of outliers xo are i.i.d. random variables from uniform distribution [−σo, σo] with σo = 10. The labels of outliers are generated by yo = sign(〈−β, xo〉). That is, outliers follow the model having opposite sign as inliers, which according to our experiment, is the most adversarial outlier model. The ratio of outliers over inliers is denoted as λ = n1/n, where n1 is the number of outliers and n is the number of inliers. We fix n = 1, 000 and the λ varies from 0 to 1.2, with a step of 0.1.\nWe repeat the simulations under each outlier fraction setting for 10 times and plot the performance (including the average and the variance) of RoLR and ordinary LR versus the ratio of outliers to inliers in Figure 2. In particular, for the task of logistic regression, we measure the performance by the parameter prediction error ‖β̂ − β∗‖. For classification, we use the classification error rate on test samples – #(ŷi 6= yi)/nt – as the performance measure. Here ŷi = sign(β̂>xi) is the predicted label for sample xi and yi is the ground truth sample label. The results, shown in Figure 2,\nclearly demonstrate that RoLR performs much better than standard LR for both tasks. Even when the outlier fraction is small (λ = 0.1), RoLR already outperforms LR with a large margin. From Figure 2(a), we observe that when λ ≥ 0.3, the parameter estimation error of LR reaches around 1.3, which is pretty unsatisfactory since simply outputting a trivial solution β̂ = 0 has an error of 1 (recall ‖β∗‖2 = 1). In contrast, RoLR guarantees the estimation error to be around 0.5, even though λ = 0.8, i.e., around 45% of the samples are outliers. To see the role of preprocessing in RoLR, we also apply such preprocessing to LR and plot its performance as “LR+P” in the figure. It can be seen that the preprocessing step indeed helps remove certain outliers with large magnitudes. However, when the fraction of outliers increases to λ = 0.5, more outliers with smaller magnitudes than the pre-defined threshold enter the remained samples and increase the error of “LR+P” to be larger than 1. This demonstrates maximizing the correlation is more essential than the thresholding for the robustness gain of RoLR. From results for classification, shown in Figure 2(b), we observe that again from λ = 0.2, LR starts to breakdown. The classification error rate of LR achieves 0.8, which is even worse than random guess. In contrast, RoLR still achieves satisfactory classification performance with classification error rate around 0.4 even with λ→ 1. But when λ > 1, RoLR also breaks down as outliers dominate in the training samples.\nWhen there is no outliers, with the same inliers (n = 1×103 and p = 20), the error of LR in logistic regression estimation is 0.06 while the error of RoLR is 0.13. Such performance degradation in RoLR is due to that RoLR maximizes the linear correlation statistics instead of the likelihood as in LR in inferring the regression parameter. This is the price RoLR needs to pay for the robustness. We provide more investigations and also results for real large data in the supplementary material."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We investigated the problem of logistic regression (LR) under a practical case where the covariate matrix is adversarially corrupted. Standard LR methods were shown to fail in this case. We proposed a novel LR method, RoLR, to solve this issue. We theoretically and experimentally demonstrated that RoLR is robust to the covariate corruptions. Moreover, we devised a linear programming algorithm to solve RoLR, which is computationally efficient and can scale to large problems. We further applied RoLR to successfully learn classifiers from corrupted training samples."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The work of H. Xu was partially supported by the Ministry of Education of Singapore through AcRF Tier Two grant R-265-000-443-112. The work of S. Mannor was partially funded by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and by the Israel Science Foundation (ISF under contract 920/12)."
    } ],
    "references" : [ {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L Bartlett", "Shahar Mendelson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Robust estimation in the logistic regression model",
      "author" : [ "Ana M Bianco", "Vı́ctor J Yohai" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1996
    }, {
      "title" : "Robust sparse regression under adversarial corruption",
      "author" : [ "Yudong Chen", "Constantine Caramanis", "Shie Mannor" ],
      "venue" : "In ICML,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Residuals and influence in regression",
      "author" : [ "R Dennis Cook", "Sanford Weisberg" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1982
    }, {
      "title" : "Binary regression models for contaminated data",
      "author" : [ "JB Copas" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1988
    }, {
      "title" : "T-logistic regression for binary and multiclass classification",
      "author" : [ "Nan Ding", "SVN Vishwanathan", "Manfred Warmuth", "Vasil S Denchev" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "The influence curve and its role in robust estimation",
      "author" : [ "Frank R Hampel" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1974
    }, {
      "title" : "Influence measures for logistic regression",
      "author" : [ "Wesley Johnson" ],
      "venue" : "Another point of view. Biometrika,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1985
    }, {
      "title" : "Conditionally unbiased bounded-influence estimation in general regression models, with applications to generalized linear models",
      "author" : [ "Hans R Künsch", "Leonard A Stefanski", "Raymond J Carroll" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1989
    }, {
      "title" : "Efficient L1 regularized logistic regression",
      "author" : [ "Su-In Lee", "Honglak Lee", "Pieter Abbeel", "Andrew Y Ng" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity",
      "author" : [ "Po-Ling Loh", "Martin J Wainwright" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach",
      "author" : [ "Yaniv Plan", "Roman Vershynin" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Logistic regression diagnostics",
      "author" : [ "Daryl Pregibon" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1981
    }, {
      "title" : "Resistant fits for some commonly used logistic models with medical applications",
      "author" : [ "Daryl Pregibon" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1982
    }, {
      "title" : "Optimally hounded score functions for generalized linear models with applications to logistic regression",
      "author" : [ "Leonard A Stefanski", "Raymond J Carroll", "David Ruppert" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1986
    }, {
      "title" : "Robust logistic regression using shift parameters",
      "author" : [ "Julie Tibshirani", "Christopher D Manning" ],
      "venue" : "arXiv preprint arXiv:1305.4987,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "Vladimir N Vapnik", "A Ya Chervonenkis" ],
      "venue" : "Theory of Probability & Its Applications,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1971
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "More rigorously, the non-robustness of LR can be shown via calculating its influence function [7] (detailed in the supplementary material).",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 12,
      "context" : "Pregibon also observed this non-robustness of LR in [14].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "To solve this important issue of LR, Pregibon [14], Cook and Weisberg [4] and Johnson [9] proposed procedures to identify observations which are influential for estimating β based on certain outlyingness measure.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 3,
      "context" : "To solve this important issue of LR, Pregibon [14], Cook and Weisberg [4] and Johnson [9] proposed procedures to identify observations which are influential for estimating β based on certain outlyingness measure.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "To solve this important issue of LR, Pregibon [14], Cook and Weisberg [4] and Johnson [9] proposed procedures to identify observations which are influential for estimating β based on certain outlyingness measure.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "[2] also proposed robust estimators which, however, require to robustly estimating the covariate matrix or boundedness on the outliers.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "Moreover, RoLR only needs to solve a linear programming problem and thus is scalable to large-scale data sets, in sharp contrast to previous LR optimization algorithms which typically resort to (computationally expensive) iterative reweighted method [11].",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 13,
      "context" : "Several previous works have investigated multiple approaches to robustify the logistic regression (LR) [15, 13, 17, 16, 10].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "Several previous works have investigated multiple approaches to robustify the logistic regression (LR) [15, 13, 17, 16, 10].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 15,
      "context" : "Several previous works have investigated multiple approaches to robustify the logistic regression (LR) [15, 13, 17, 16, 10].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "Several previous works have investigated multiple approaches to robustify the logistic regression (LR) [15, 13, 17, 16, 10].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "Several previous works have investigated multiple approaches to robustify the logistic regression (LR) [15, 13, 17, 16, 10].",
      "startOffset" : 103,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : "For example, Pregiobon [15] proposed the following M-estimator:",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "However, the result from such estimator is not robust to outliers with high leverage covariates as shown in [5].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "Recently, Ding et al [6] introduced the T -logistic regression as a robust alternative to the standard LR, which replaces the exponential distribution in LR by t-exponential distribution family.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "Our work is largely inspired by following two recent works [3, 13] on robust sparse regression.",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "Our work is largely inspired by following two recent works [3, 13] on robust sparse regression.",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "In [13], a convex programming method for estimating the sparse parameters of logistic regression model is proposed:",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "We call the remaining n non-corrupted samples “authentic” samples, which obey the following standard sub-Gaussian design [12, 3].",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "We call the remaining n non-corrupted samples “authentic” samples, which obey the following standard sub-Gaussian design [12, 3].",
      "startOffset" : 121,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "The above sub-Gaussian random variables have several nice concentration properties, one of which is stated in the following Lemma [12].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 10,
      "context" : "Lemma 1 (Sub-Gaussian Concentration [12]).",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "Based on the above concentration property, we can obtain following bound on the magnitude of a collection of sub-Gaussian random variables [3].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 16,
      "context" : "Some widely used complexity measures include the VC-dimension [18] and the Rademacher and Gaussian complexity [1].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "Some widely used complexity measures include the VC-dimension [18] and the Rademacher and Gaussian complexity [1].",
      "startOffset" : 110,
      "endOffset" : 113
    } ],
    "year" : 2014,
    "abstractText" : "We consider logistic regression with arbitrary outliers in the covariate matrix. We propose a new robust logistic regression algorithm, called RoLR, that estimates the parameter through a simple linear programming procedure. We prove that RoLR is robust to a constant fraction of adversarial outliers. To the best of our knowledge, this is the first result on estimating logistic regression model when the covariate matrix is corrupted with any performance guarantees. Besides regression, we apply RoLR to solving binary classification problems where a fraction of training samples are corrupted.",
    "creator" : null
  }
}