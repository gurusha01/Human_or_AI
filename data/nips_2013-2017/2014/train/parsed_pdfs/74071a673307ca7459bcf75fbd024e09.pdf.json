{
  "name" : "74071a673307ca7459bcf75fbd024e09.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On Prior Distributions and Approximate Inference for Structured Variables",
    "authors" : [ "Oluwasanmi Koyejo", "Rajiv Khanna", "Joydeep Ghosh" ],
    "emails" : [ "sanmi@stanford.edu", "rajivak@utexas.edu", "ghosh@ece.utexas.edu", "poldrack@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Data in scientific and commercial disciplines are increasingly characterized by high dimensions and relatively few samples. For such cases, a-priori knowledge gleaned from expertise and experimental evidence are invaluable for recovering meaningful models. In particular, knowledge of restricted degrees of freedom such as sparsity or low rank has become an important design paradigm, enabling the recovery of parsimonious and interpretable results, and improving storage and prediction efficiency for high dimensional problems. In Bayesian models, such restricted degrees of freedom can be captured by incorporating structural constraints on the design of the prior distribution. Prior distributions for structured variables can be designed by combining conditional distributions - each capturing portions of the problem structure, into a hierarchical model. In other cases, researchers design special purpose prior distributions to match the application at hand. In the case of sparsity, an example of the former approach is the spike and slab prior [1, 2], and an example of the latter approach is the horseshoe prior [3].\nWe describe a framework for designing prior distributions when the a-priori information include structural constraints. Our framework follows the maximum entropy principle [4, 5]. The distribution is chosen as one that incorporates known information, but is as difficult as possible to discriminate from the base distribution with respect to relative entropy. The maximum entropy approach\nhas been especially successful with domain knowledge expressed as expectation constraints. In such cases, the solution is given by a member of the exponential family [6, 7] e.g. quadratic constraints result in the Gaussian distribution. Our work extends this framework to the design of prior distributions when a-priori information include domain constraints.\nOur main technical contributions are as follows:\n• We show that under standard assumptions, the information projection of a base density to domain constraints is given by its restriction (Section 2).\n• We show the equivalence between relative entropy inference with data observation constraints and Bayes rule for continuous variables\n• When such restriction is intractable, we propose a family of parameterized approximations indexed by subsets of the domain (Section 2.1).\nWe consider approximate inference in the special case of sparse structure:\n• We characterize the restriction precisely, showing that it is given by a conditional distribution (Section 3).\n• We show that the approximate sparse support estimation problem is submodular. As a result, greedy forward selection is efficient and guarantees (1- 1e ) factor optimality (Section 3.1).\nOur work is motivated by the predictive modeling of high-dimensional functional neuroimaging data, measured by cognitive neuroscientists for analyzing the human brain. The data are represented using hundreds of thousands of variables. Yet due to real world constraints, most experimental datasets contain only a few data samples [8]. The proposed approach is applied to predictive modeling of simulated data and high-dimensional neuroimaging data, and is compared to Bayesian hierarchical models and non-probabilistic sparse predictive models, showing superior support recovery and predictive accuracy (Section 4). Due to space constraints, all proofs are provided in the supplement."
    }, {
      "heading" : "1.1 Preliminaries",
      "text" : "This section includes notation and a few basic definitions. Vectors are denoted by lower case x and matrices by capital X. xi,j denotes the (i, j)th entry of the matrix X. xi,: denotes the ith row of X and x:,j denotes the jth column. Let |X| denote the determinant of X. Sets are denoted by sans serif e.g. S. The reals are denoted by R. [n] denotes the set of integers {1, . . . , n}, and ℘(n) denotes the power set of [n]. Let X be either a countable set, or a complete separable metric space equipped with the standard Borel σ-algebra of measurable set. Let P denote the set of probability densities on X i.e. positive functions P = {p : X 7→ [0, 1] , ∫ X p(x) = 1}. For the remainder of this paper, we make the following assumption: Assumption 1. All distributions P are absolutely continuous with respect to the dominating measure ν so there exists a density p ∈ P that satisfies dP = pdν.\nTo simplify notation, we use use the standard dν = dx. As a consequence of Assumption 1, the relative entropy is given in terms of the densities as:\nKL(q‖p) = ∫ X q(x) log q(x) p(x) dx.\nThe relative entropy is strictly convex with respect to its first argument. The information projection of a probability density p to a constraint set A is given by the solution of:\ninf q∈P\nKL(q‖p) s.t. q ∈ A.\nWe will only consider projections where A is a closed convex set so the infimum is achieved. The delta function, denoted by δ(·), is a generalized set function that satisfies ∫ X δA(x)f(x)dx =∫\nA f(x)dx, and ∫ X δA(x)dx = 1, for some some A ⊆ X. The set of domain restricted densities, denoted by FA for A ⊂ X, is the set of probability density functions supported on A i.e.\nFA = {q ∈ P | q(x) = 0 ∀ x /∈ A} ∪ {δ{x} ∀ x ∈ A} ⊂ FA ⊂ P = FX. Further, note that FA is closed and convex for any A ⊆ X (including nonconvex A). Restriction is a standard approach for defining distributions on subsets A ⊆ X. An important special case we will consider is when A is a measure zero subset of X. The common conditional density is one such example, the existence of which follows from the disintegration theorem [9]. Restrictions of measure require extensive technical tools in the general case [10]. We will employ the following simplifying condition for the remainder of this manuscript: Condition 2. The sample space X is a subset of Euclidean space with ν given by the Lebesgue measure. Alternatively, X is a countable set with ν given by the counting measure.\nLet P be a probability distribution on X. Under Assumption 1 and Condition 2, the restriction of the density p to the set A ⊂ X is given by:\nq(x) =\n{ p(x)∫\nA p(x)dx x ∈ A, 0 otherwise."
    }, {
      "heading" : "2 Priors for structured variables",
      "text" : "We assume a-priori information identifying the structure of X via the sub-domain A ⊂ X. We also assume a pre-defined base distribution P with associated density p. Without loss of generality, let p have support everywhere1 on X i.e. p(x) > 0 ∀ x ∈ X. Following the principle of minimum discrimination information, we select the prior as the information projection of the base density p to FA. Our first result identifies the equivalence between information projection subject to domain constraints and density restriction. Theorem 3. Under Condition 2, the information projection of the density p to the constraint set FA is the restriction of p to the domain A.\nTheorem 3 gives principled justification for the domain restriction approach to structured prior design. Examples of density restriction in the literature include the truncated Gaussian, Beta and Gamma densities [11], and the restriction of the matrix-variate Gaussian to the manifold of low rank matrices [12]. Various properties of the restriction, such as its shape, and tail behavior (up to re-scaling) follow directly from the base density. Thus the properties of the resulting prior are more amenable to analysis when the base measure is well understood. Next, we consider a corollary of Theorem 3 that was introduced by Williams [13]. Corollary 4. Consider the product space X = W× Y. Let domain constraint be given by W× {ŷ} for some ŷ ∈ Y. Under Condition 2, the information projection of p to FW×{ŷ} is given by p(w|ŷ)δŷ.\nIn the Bayesian literature, p(w) is known as the prior, p(y|w) is the likelihood and p(w|ŷ) is the posterior density given the observation y = ŷ. Corollary 4 considers the information projection of the joint density p(w, y) given observed data, and shows that the solution recovers the Bayesian posterior. Williams [13] considered a generalization of Corollary 4, but did not consider projection to data constraints2. While Corollary 4 has been widely applied in the literature e.g. [14], to the best of our knowledge, the presented result is the first formal proof."
    }, {
      "heading" : "2.1 Approximate inference for structured variables via tractable subsets",
      "text" : "For many structural constraints of interest, restriction requires the computation of an intractable normalization constant. In theory, rejection sampling and Markov Chain Monte Carlo (MCMC) inference methods [15] do not require normalized probabilities. However, as many structured subdomains are measure zero sets with respect to the dominating measure, randomly generated samples generated from the base distribution are unlikely to lie in the constrained domains e.g. random samples from a multivariate Gaussian are not sparse. Hence rejection sampling fails, and MCMC suffers from low acceptance probabilities. As a result, inference on such structured sub-domains\n1When this condition is violated, we simply redefine X as the subdomain supporting p. 2Specifically, Williams [13] noted “Relative information has been defined only for unconditional distribu-\ntions, which say nothing about the relative probabilities of events of probability zero.“\ntypically requires specialized methods e.g. [11, 12]. In the following, we propose a class of variational approximations based on an inner representation of the structured subdomain. Let {Si ∈ A} represent a (possibly overlapping) partitioning of A into subsets. We define the domain restricted density sets generated by these partitions as FSi , and their union D = ⋃ FSi . Note that by definition each FSi ⊆ D ⊆ FA ⊆ FX. Our approach is to approximate the optimization over densities in FA by optimizing over D - a smaller subset of tractable densities. Approximate inference is generally most successful when the approximation accounts for observed data. Inspired by the results of Corollary 4, we consider such a projection. Let pA(w, y) be the information projection of the joint distribution p(x, y) to the set FA×{ŷ}. We propose approximate inference via the following rule:\npS∗,ŷ = arg min q∈D×F{ŷ} KL(q(w, y)‖pA(w, y)) = arg min S\n[ min\nq∈FS×{ŷ} KL(q(w, y)‖pA(w, y))\n] . (1)\nOur proposed approach may be decomposed into two steps. The inner step is solved by estimating a parameterized set of prior densities {qS} corresponding to choices of S, and the outer step is solved by the selection of the optimal subset S∗. The solution is given by pS∗,ŷ(w, y) = pS∗(w|ŷ)δŷ (Corollary 4) with the associated approximate posterior given by pS∗(w|ŷ). The following theorem considers the effect of a sequence of domain constrained information projections (see Fig. 1b), which will useful for subsequent results. Theorem 5. Let π : [n] 7→ [n] be a permutation function and {Cπ(i) | Cπ(i) ⊂ X} represent a sequence of sets with non empty intersection B = ⋂ Ci 6= ∅. Given a base density p, let q0 = p, and define the sequence of information projections:\nqi = arg min q∈FCπ(i)\nKL(q‖qi−1).\nUnder Condition 2, q∗ = qN is independent of π. Further q∗ = min q∈FB KL(q‖p).\nWe apply Theorem 5 to formulate equivalent solutions of (1) that may be simpler to solve. Corollary 6. Let pS∗,ŷ(w, y) be the solution of (1), then the posterior distribution pS∗(w|ŷ) is given by:\npS∗(w|ŷ) = arg min q∈D KL(q(w)‖pA(w|ŷ)) = arg min q∈D KL(q(w)‖p(w|ŷ)). (2)\nCorollary 6 implies that we can estimate the approximate structured posterior directly as the information projection of the unstructured posterior distribution p(w|ŷ). Upon further examination, Corollary 6 also suggests that the proposed approximation is most useful when there exist subsets of A such that the restriction of the base density to each subset leads to tractable inference. Further, the result is most accurate when one of the subsets S∗ ∈ A captures most of the posterior probability mass. When the optimal subset S∗ is known, the structured prior density associated with the structured posterior can be computed as shown in the following corollary.\nCorollary 7. Let pS∗,ŷ(w, y) be the solution of (1). Define the density pS∗(w) as:\npS∗(w) = arg min q∈FS∗ KL(q(w)‖pA(w)) = arg min q∈FS∗ KL(q(w)‖p(w)). (3)\nthen pS∗(w) is the prior distribution corresponding to the Bayesian posterior pS∗(w|ŷ)."
    }, {
      "heading" : "3 Priors for sparse structure",
      "text" : "We now consider a special case of the proposed framework for sparse structured variables. A d dimensional variable x ∈ X is k-sparse if d− k of its entries take a default value of ci i.e |{i | xi = ci}| = d−k. In Euclidean space X = Rd and in most cases, ci = 0 ∀ i. Similarly, the distribution P on the domain X is k-sparse if all random variablesX ∼ P are at most k-sparse. The support of x ∈ X is the set supp(x) = {i | xi 6= ci} ∈ ℘(d). Let S ⊂ X denote the set of variables with support s i.e. S = {x ∈ X s.t. supp(x) = s}. We will use the notation xS = {xi | i ∈ s}, and its complement xS′ = {xi | i ∈ s′}, where s′ = [d]\\s. The domain of k sparse vectors is given by the union of all possible d!(d−k)!k! sparse support sets as A = ⋃ Si. While the sparse domain A is non-convex, each subset S is a convex set, in fact given by linear subspaces with basis {ei | i ∈ s}. Further, while the information projection of a base density p to A is generally intractable, the information projection to its convex subsets S turn out to be computationally tractable. We investigate the application of the proposed approximation scheme using these subsets.\nConsider the information projection of an arbitrary probability measure P with density3 p to the set D = ⋃ FSi given by:\nmin q∈D KL(q‖p) = min S∈{Si} [ min q∈FS KL(q‖p) ] = min S∈{Si} KL(pS‖p).\nApplying Theorem 3, we can compute that pS = p(x)δS(x)/Z, where Z is a normalization factor:\nZ = ∫ S p(x) = ∫ X p(xS,xS′)δS(x) = ∫ X p(xS|xS′)p(xS′)δS(x) = p(xS′ = cS′).\nThus, the normalization factor is a marginal density at xS′ = cS′ . We may now compute the restriction explicitly:\npS(x) = p(xS|xS′)p(xS′)δS(x)\np(xS′ = cS′) = p(xS|xS′ = cS′)δS(x). (4)\nIn other words, the information projection to a sparse support domain is the density of xS conditioned on xS′ = cS′ . The resulting gap is:\nKL(pS‖p) = ∫ S pS(x) log pS(x) p(x) = ∫ S pS(x) log p(x) p(x)p(xS′ = cS′) = − log p(xS′ = cS′).\nThus, for a given target sparsity k, we solve:\ns∗ = arg max |s|=k J(s), where J(s) = log p(xS′ = cS′). (5)"
    }, {
      "heading" : "3.1 Submodularity and Efficient Inference",
      "text" : "In this section, we show that the cost function J(s) is monotone submodular, and describe the greedy forward selection algorithm for efficient inference. Let F : ℘(d) 7→ R represent a set function. F is normalized if F (∅) = 0. A bounded F can be normalized as F̃ (s) = F (s) − F (∅) with no effect on optimization. F is monotonic, if for all subsets u ⊂ v ⊆ ℘(d) it holds that F (u) ≤ F (v). F is submodular, if for all subsets u, v ⊆ m it holds that F (u ∪ v) + F (u ∩ v) ≤ F (u) + F (v). Submodular functions have a diminishing returns property [16] i.e. the marginal gain of adding elements decreases with the size of the set. Theorem 8. Let J : ℘(d) 7→ R, J(s) = log p(xS′ = cS′), and define J̃(s) = J(s) − J(∅), then J̃(s) is normalized and monotone submodular.\n3Where p may represent the conditional densities as in Section 2.1. To simplify the discussion, we suppress the dependence on ŷ.\nWhile constrained maximization of submodular functions is generally NP-hard, a simple greedy forward selection heuristic has been shown to perform almost as well as the optimal in practice, and is known to have strong theoretical guarantees.\nTheorem 9 (Nemhauser et al. [16]). In the case of any normalized, monotonic submodular function F, the set s∗ obtained by the greedy algorithm achieves at least a constant fraction ( 1− 1e ) of the\nobjective value obtained by the optimal solution i.e. F (s∗) = ( 1− 1e ) max |s|≤k F (s).\nIn addition, no polynomial time algorithm can provide a better approximation guarantee unless P = NP [17]. An additional benefit of the greedy approach is that it does not require the decision of the support size k to be made at training time. As an anytime algorithm, training can be stopped at any k based on computational constraints, while still returning meaningful results. An interesting special case occurs when the base density takes a product form.\nCorollary 10. Let J(s) be defined as in Theorem 8 and suppose the base density is product form i.e. p(x) = ∏d i=1 p(xi), then J(s) is linear.\nIn particular, define h = {p(xi = 0) ∀ i ∈ [d]}, then the solution of (5) is given by set of dimensions associated with the smallest k values of h."
    }, {
      "heading" : "4 Experiments",
      "text" : "We present experimental results comparing the proposed sparse approximate inference projection to other sparsity inducing models. We performed experiments to test the models ability to estimate the support of the reconstructed targets and the predictive regression accuracy. The regression accuracy was measured using the coefficient of determination R2 = 1 − ∑ (ŷ − y)2/ ∑ (y − ȳ)2 where y is the target response with sample mean ȳ and ŷ is the predicted response. R2 measures the gain in predictive accuracy compared to a mean model and has a maximum value of 1. The support recovery was measured using the AUC of the recovered support with respect to the true s∗.\nThe baseline models are: (i) regularized least squares (Ridge), (ii) least absolute shrinkage and selection (Lasso) [18], (iii) automatic relevance determination (ARD) [19], (iv) Spike and Slab [1, 2]. Ridge and Lasso were optimized using implementations from the scikit-learn python package [20]. While Ridge does not return sparse weights, it was included as a baseline for regression performance. We implemented ARD using iterative re-weighted Lasso as suggested by Wipf and Nagarajan [19]. The noise variance hyperparameter for Ridge and ARD were selected from the set 10{−4,−3,...,4}. Lasso was evaluated using the default scikit-learn implementation where the hyperparameter is selected from 100 logarithmically spaced values based on the maximum correlation between the features and the response. For each of these models, the hyperparameter was selected in an inner 5-fold cross validation loop. For speed and scalability, we used a publicly available implementation of Spike and Slab [21], which uses a mean field variational approximation. In addition to the weights, Spike and Slab estimates the probability that each dimension is non zero. As Spike and Slab does not return sparse estimates, sparsity was estimated by thresholding this posterior at 0.5 for each dimension (SpikeSlab0.5 ), we also tested the full spike and slab posterior prediction for regression performance alone (SpikeSlabFull).\nThe proposed projection approach is designed to be applicable to any probabilistic model. Thus, we applied the projection approach as additional post-processing for the two Bayesian model baselines. The first method is a projection of the standard Gaussian regression posterior (Sparse-G ) (more details in supplement). The second is a projection of the spike and spike and slab approximate posterior (SpikeSlabKL). We note that since the spike and slab approximate posterior uses the mean field approximation, the posterior distribution is in product form and the projection is straightforward using Corollary 10. Support size selection: The selection of the hyperparameter k - specifying the sparsity, can be solved by standard model selection routines such as cross-validation. We found that support size selection using sequential Bayes factors [22] was particularly effective, thus the support size was selected as the first k where log p(y|Sk+1)− log p(y|Sk) < ."
    }, {
      "heading" : "4.1 Simulated Data",
      "text" : "We generated random high dimensional feature vectors ai ∈ Rd with ai,j ∼ N (0, 1). The response was generated as yi = w>ai + νi where νi represents independent additive noise with νi ∼ N ( 0, σ2 ) for all i ∈ [n]. We set σ2 implicitly via the signal to noise ration (SNR) as SNR = var(y)/σ2, where var(y) is the variance of y. In each experiment, we sampled a sparse weight vector w by sampling k dimensions at random with from [d], then we sampled values wi ∼ N (0, 1) and set other dimensions to zero. We performed a series of tests to investigate the performance of the model in different scenarios. Each experiment was run 10 times with separate training and test sets. We present the average results on the test set.\nOur first experiment tested the performance of all models with limited samples. Here we set k = 20, d = 10, 000 and an SNR of 20dB. The number of training values was varied from n = 100, . . . , 400 with 200 test samples. Fig. 2a shows the model performance in terms of support recovery. With limited training samples, Sparse-G outperformed all the baselines including Lasso. We also found that SpikeSlabKL consistently outperformed SpikeSlab0.5. We speculate that the significant gap between Sparse-G and SpikeSlabKL may be partly due to the mean field assumption in the underlying Spike and Slab. Fig. 2b shows the corresponding regression performance. Again, we found that Sparse-G outperformed all other baselines, with Ridge achieving the worst performance.\nOur second experiment tested the performance of all models with high levels of noise. Here we set k = 20, d = 10, 000 and n = 200 with 200 test samples. We varied the SNR from 40dB to −10dB (note that σ2 increases as SNR is decreased). Fig. 2c shows the support recovery performance of the different models. We found a performance gap between Sparse-G and Lasso, more pronounced than in the small sample test. The SpikeSlab0.5 was the worst performing model, but the performance was improved by SpikeSlabKL . Only Sparse-G achieved perfect support recovery at low noise (high SNR ) levels. The regression performance is shown in Fig. 2d. While ARD and Lasso matched Sparse-G at low noise levels (high SNR), their performance degraded much faster at higher noise levels (low SNR)."
    }, {
      "heading" : "4.2 Functional Neuroimaging Data",
      "text" : "Functional magnetic resonance imaging (fMRI) is an important tool for non-invasive study of brain activity. fMRI studies involve measurements of blood oxygenation (which are sensitive to the\namount of local neuronal activity) while the participant is presented with a stimulus or cognitive task. Neuroimaging signals are then analyzed to identify which brain regions which exhibit a systematic response to the stimulation, and thus to infer the functional properties of those brain regions [23]. Functional neuroimaging datasets typically consist of a relatively small number of correlated high dimensional brain images. Hence, capturing the inherent structural properties of the imaging data is critical for robust inference.\nFMRI data were collected from 126 participants while the subjects performed a stop-signal task [24]. For each subject, contrast images were computed for “go” trials and successful “stop” trials using a general linear model with FMRIB Software Library (FSL), and these contrast images were used for regression against estimated stop-signal reaction times. We used the normalized Laplacian of the 3- dimensional spatial graph of the brain image voxels to define the precision matrix. This corresponds to the observation that nearby voxels tend to have similar functional activation. We present the 10- fold cross validation performance of all models tested on this data. We tested all models using the high dimensional 100,000 voxel brain image and measured average predictive R2 . The results are: Sparse-G (0.051), Lasso (-0.271), Ridge (-0.473), ARD (-0.478). The negative test R2 for baseline models show worse predictive performance than the test mean predictor, and indicate the difficulty of this task. Even with the mean field variational inference, the Spike and Slab models did not scale to this dataset. Only Sparse-G achieved a positive R2 . The support selected by Sparse-G with all 100,000 voxels is shown in Fig. 3, sliced across the vertical dimension. The recovered voxels show biologically plausible brain locations including the orbitofrontal cortex, dorsolateral prefrontal cortex, putamen, anterior cingulate, and parietal cortex, which are correlated with the observed response. Further neuroscientific interpretation and validation will be included in an extended version of the paper."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We present a principled approach for enforcing structure in Bayesian models via structured prior selection based on the maximum entropy principle. The prior is defined by the information projection of the base measure to the set of distributions supported on the constraint domain. We focus on the case of sparse structure. While the optimal prior is intractable in general, we show that approximate inference using selected convex subsets is equivalent to maximizing a submodular function subject to cardinality constraints, and propose an efficient greedy forward selection procedure which is guaranteed to achieve within a (1− 1e ) factor of the global optimum. For future work, we plan to explore applications of our approach with other structural constraints such as low rank and structured sparsity for matrix-variate sample spaces. We also plan to explore more complicated base distributions on other samples spaces.\nAcknowledgments: fMRI data was provided by the Consortium for Neuropsychiatric Phenomics (NIH Roadmap for Medical Research grants UL1-DE019580, RL1MH083269, RL1DA024853, PL1MH083271)."
    } ],
    "references" : [ {
      "title" : "Bayesian variable selection in linear regression",
      "author" : [ "T.J. Mitchell", "J.J. Beauchamp" ],
      "venue" : "JASA, 83(404):1023–",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1988
    }, {
      "title" : "Spike and slab variable selection: frequentist and bayesian strategies",
      "author" : [ "H. Ishwaran", "J.S. Rao" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "The horseshoe estimator for sparse signals",
      "author" : [ "C. M Carvalho", "N.G. Polson", "J.G. Scott" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Information Theory and Statistical Mechanics",
      "author" : [ "E.T. Jaynes" ],
      "venue" : "Physical Review Online Archive,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1957
    }, {
      "title" : "Information Theory and Statistics",
      "author" : [ "S. Kullback" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1959
    }, {
      "title" : "Information Theory, Inference and Learning Algorithms",
      "author" : [ "D. MacKay" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "A representation approach for relative entropy minimization with expectation constraints",
      "author" : [ "O. Koyejo", "J. Ghosh" ],
      "venue" : "In ICML WDDL workshop,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Inferring mental states from neuroimaging data: From reverse inference to large-scale decoding",
      "author" : [ "R.A. Poldrack" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Conditioning as disintegration",
      "author" : [ "J.T. Chang", "D. Pollard" ],
      "venue" : "Statistica Neerlandica,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "Foundations of the theory of probability. Chelsea",
      "author" : [ "A.N. Kolmogorov" ],
      "venue" : "New York,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1933
    }, {
      "title" : "Sampling truncated normal, beta, and gamma densities",
      "author" : [ "P. Damien", "S.G. Walker" ],
      "venue" : "J. of Computational and Graphical Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Bayesian inference for low rank spatiotemporal neural receptive fields",
      "author" : [ "M. Park", "J. Pillow" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Bayesian conditionalisation and the principle of minimum information",
      "author" : [ "P. Williams" ],
      "venue" : "The British Journal for the Philosophy of Science,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1980
    }, {
      "title" : "Constrained Bayesian inference for low rank multitask learning",
      "author" : [ "O. Koyejo", "J. Ghosh" ],
      "venue" : "In UAI,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Monte Carlo statistical methods, volume 58",
      "author" : [ "C.P. Robert", "G. Casella" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1999
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functions",
      "author" : [ "G.L. Nemhauser", "L.A. Wolsey", "M.L. Fisher" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1978
    }, {
      "title" : "A threshold of ln n for approximating set cover",
      "author" : [ "U. Feige" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1998
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "R. Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1996
    }, {
      "title" : "A new view of automatic relevance determination",
      "author" : [ "D. Wipf", "S. Nagarajan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Pedregosa. Scikit-learn: Machine learning",
      "author" : [ "F. et. al" ],
      "venue" : "in Python. JMLR,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Spike and slab variational inference for multi-task and multiple kernel learning",
      "author" : [ "Michalis K Titsias", "Miguel Lázaro-Gredilla" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Learning to decode cognitive states from brain",
      "author" : [ "T.M. Mitchell", "R. Hutchinson", "R.S. Niculescu", "F. Pereira", "X. Wang", "M. Just", "S. Newman" ],
      "venue" : "images. Mach. Learn.,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2004
    }, {
      "title" : "Decomposing decision components in the stop-signal task: A model-based approach to individual differences in inhibitory control",
      "author" : [ "Corey N White", "Eliza Congdon", "Jeanette A Mumford", "Katherine H Karlsgodt", "Fred W Sabb", "Nelson B Freimer", "Edythe D London", "Tyrone D Cannon", "Robert M Bilder", "Russell A Poldrack" ],
      "venue" : "Journal of Cognitive Neuroscience,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In the case of sparsity, an example of the former approach is the spike and slab prior [1, 2], and an example of the latter approach is the horseshoe prior [3].",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "In the case of sparsity, an example of the former approach is the spike and slab prior [1, 2], and an example of the latter approach is the horseshoe prior [3].",
      "startOffset" : 87,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "In the case of sparsity, an example of the former approach is the spike and slab prior [1, 2], and an example of the latter approach is the horseshoe prior [3].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "Our framework follows the maximum entropy principle [4, 5].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "Our framework follows the maximum entropy principle [4, 5].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "In such cases, the solution is given by a member of the exponential family [6, 7] e.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "In such cases, the solution is given by a member of the exponential family [6, 7] e.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "Yet due to real world constraints, most experimental datasets contain only a few data samples [8].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "The common conditional density is one such example, the existence of which follows from the disintegration theorem [9].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 9,
      "context" : "Restrictions of measure require extensive technical tools in the general case [10].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "Examples of density restriction in the literature include the truncated Gaussian, Beta and Gamma densities [11], and the restriction of the matrix-variate Gaussian to the manifold of low rank matrices [12].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : "Examples of density restriction in the literature include the truncated Gaussian, Beta and Gamma densities [11], and the restriction of the matrix-variate Gaussian to the manifold of low rank matrices [12].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 12,
      "context" : "Next, we consider a corollary of Theorem 3 that was introduced by Williams [13].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 12,
      "context" : "Williams [13] considered a generalization of Corollary 4, but did not consider projection to data constraints2.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "[14], to the best of our knowledge, the presented result is the first formal proof.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "In theory, rejection sampling and Markov Chain Monte Carlo (MCMC) inference methods [15] do not require normalized probabilities.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "(2)Specifically, Williams [13] noted “Relative information has been defined only for unconditional distributions, which say nothing about the relative probabilities of events of probability zero.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 15,
      "context" : "Submodular functions have a diminishing returns property [16] i.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "In addition, no polynomial time algorithm can provide a better approximation guarantee unless P = NP [17].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "The baseline models are: (i) regularized least squares (Ridge), (ii) least absolute shrinkage and selection (Lasso) [18], (iii) automatic relevance determination (ARD) [19], (iv) Spike and Slab [1, 2].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 18,
      "context" : "The baseline models are: (i) regularized least squares (Ridge), (ii) least absolute shrinkage and selection (Lasso) [18], (iii) automatic relevance determination (ARD) [19], (iv) Spike and Slab [1, 2].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "The baseline models are: (i) regularized least squares (Ridge), (ii) least absolute shrinkage and selection (Lasso) [18], (iii) automatic relevance determination (ARD) [19], (iv) Spike and Slab [1, 2].",
      "startOffset" : 194,
      "endOffset" : 200
    }, {
      "referenceID" : 1,
      "context" : "The baseline models are: (i) regularized least squares (Ridge), (ii) least absolute shrinkage and selection (Lasso) [18], (iii) automatic relevance determination (ARD) [19], (iv) Spike and Slab [1, 2].",
      "startOffset" : 194,
      "endOffset" : 200
    }, {
      "referenceID" : 19,
      "context" : "Ridge and Lasso were optimized using implementations from the scikit-learn python package [20].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 18,
      "context" : "We implemented ARD using iterative re-weighted Lasso as suggested by Wipf and Nagarajan [19].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "For speed and scalability, we used a publicly available implementation of Spike and Slab [21], which uses a mean field variational approximation.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : "Neuroimaging signals are then analyzed to identify which brain regions which exhibit a systematic response to the stimulation, and thus to infer the functional properties of those brain regions [23].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 22,
      "context" : "FMRI data were collected from 126 participants while the subjects performed a stop-signal task [24].",
      "startOffset" : 95,
      "endOffset" : 99
    } ],
    "year" : 2014,
    "abstractText" : "We present a general framework for constructing prior distributions with structured variables. The prior is defined as the information projection of a base distribution onto distributions supported on the constraint set of interest. In cases where this projection is intractable, we propose a family of parameterized approximations indexed by subsets of the domain. We further analyze the special case of sparse structure. While the optimal prior is intractable in general, we show that approximate inference using convex subsets is tractable, and is equivalent to maximizing a submodular function subject to cardinality constraints. As a result, inference using greedy forward selection provably achieves within a factor of (1-1/e) of the optimal objective value. Our work is motivated by the predictive modeling of high-dimensional functional neuroimaging data. For this task, we employ the Gaussian base distribution induced by local partial correlations and consider the design of priors to capture the domain knowledge of sparse support. Experimental results on simulated data and high dimensional neuroimaging data show the effectiveness of our approach in terms of support recovery and predictive accuracy.",
    "creator" : null
  }
}