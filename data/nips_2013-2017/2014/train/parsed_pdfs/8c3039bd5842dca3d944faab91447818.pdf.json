{
  "name" : "8c3039bd5842dca3d944faab91447818.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Factoring Variations in Natural Images with Deep Gaussian Mixture Models",
    "authors" : [ "Aäron van den Oord", "Benjamin Schrauwen" ],
    "emails" : [ "benjamin.schrauwen}@ugent.be" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6]. Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their recent results in supervised learning [7]. Although not a universal recipe for success, the merits of deep learning are well-established [8]. Because of their multilayered nature, these methods provide ways to efficiently represent increasingly complex relationships as the number of layers increases. “Shallow” methods will often require a very large number of units to represent the same functions, and may therefore overfit more.\nLooking at real-valued data, one of the current problems with deep unsupervised learning methods, is that they are often hard to scale to large datasets. This is especially a problem for unsupervised learning, because there is usually a lot of data available, as it does not have to be labeled (e.g. images, videos, text). As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model (GMM) and the Student-t Mixture Model (STM), that remain surprisingly competitive [2]. Of course, the disadvantage of these mixture models is that they have less representational power than deep models.\nIn this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model (Deep GMM). The Deep GMM is a straightforward but powerful generalization of Gaussian Mixture Models to multiple layers. It is constructed by stacking multiple GMM-layers on\ntop of each other, which is similar to many other Deep Learning techniques. Although for every deep GMM, one could construct a shallow GMM with the same density function, it would require an exponential number of mixture components to do so.\nThe multilayer architecture of the Deep GMM gives rise to a specific kind of parameter tying. The parameterization is most interpretable in the case of images: the layers in the architecture are able to efficiently factorize the different variations that are present in natural images: changes in brightness, contrast, color and even translations or rotations of the objects in the image. Because each of these variations will affect the image separately, a traditional mixture model would need an exponential number of components to model each combination of variations, whereas a Deep GMM can factor these variations and model them individually.\nThe proposed training algorithm for the Deep GMM is based on the most popular principle for training GMMs: Expectation Maximization (EM). Although stochastic gradient (SGD) is also a possible option, we suggest the use of EM, as it is inherently more parallelizable. As we will show later, both the Expectation and the Maximization steps can easily be distributed on multiple computation units or machines, with only limited communication between compute nodes. Although there has been a lot of effort in scaling up SGD for deep networks [9], the Deep GMM is parallelizable by design.\nThe remainder of this paper is organized as follows. We start by introducing the design of deep GMMs before explaining the EM algorithm for training them. Next, we discuss the experiments where we examine the density estimation performance of the deep GMM, as a function of the number of layers, and in comparison with other methods. We conclude in Section 5, where also discuss some unsolved problems for future work."
    }, {
      "heading" : "2 Stacking Gaussian Mixture layers",
      "text" : "Deep GMMs are best introduced by looking at some special cases: the multivariate normal distribution and the Gaussian Mixture model.\nOne way to define a multivariate normal variable x is as a standard normal variable z ⇠ N (0, In) that has been transformed with a certain linear transformation: x = Az + b, so that\np (x) = N x|b, AAT .\nThis is visualized in Figure 1(a). The same interpretation can be applied to Gaussian Mixture Models, see Figure 1(b). A transformation is chosen from set of (square) transformations Ai, i = 1 . . . N (each having a bias term bi) with probabilities ⇡i, i = 1 . . . N , such that the resulting distribution becomes:\np (x) =\nNX\ni=1\n⇡iN x|bi, AiATi .\nWith this in mind, it is easy to generalize GMMs in a multi-layered fashion. Instead of sampling one transformation from a set, we can sample a path of transformations in a network of k layers, see Figure 1(c). The standard normal variable z is now successively transformed with a transformation from each layer of the network. Let be the set of all possible paths through the network. Each path p = (p1, p2, . . . , pk) 2 has a probability ⇡p of being sampled, with\nX\np2 ⇡p =\nX\np1,p2,...,pk\n⇡(p1,p2,...,pk) = 1.\nHere Nj is the number of components in layer j. The density function of x is:\np (x) =\nX p2 ⇡pN x| p,⌦p⌦Tp , (1)\nwith p = bk,pk +Ak,ik (. . . (b2,p2 +A2,p2b1,p1)) (2)\n⌦p =\n1Y\nj=k\nAj,pj . (3)\nHere Am,n and bm,n are the n’th transformation matrix and bias of the m’th layer. Notice that one can also factorize ⇡p as follows: ⇡(p1,p2,...,pk) = ⇡p1⇡p2 . . .⇡pk , so that each layer has its own set of parameters associated with it. In our experiments, however, this had very little difference on the log likelihood. This would mainly be useful for very large networks.\nThe GMM is a special case of the deep GMM having only one layer. Moreover, each deep GMM can be constructed by a GMM with Qk j Nj components, where every path in the network represents one component in the GMM. The parameters of these components are tied to each other in the way the deep GMM is defined. Because of this tying, the number of parameters to train is proportional toPk\nj Nj . Still, the density estimator is quite expressive as it can represent a large number of Gaussian mixture components. This is often the case with deep learning methods: Shallow architectures can often theoretically learn the same functions, but will require a much larger number of parameters [8]. When the kind of compound functions that a deep learning method is able to model are appropriate for the type of data, their performance will often be better than their shallow equivalents, because of the smaller risk of overfitting.\nIn the case of images, but also for other types of data, we can imagine why this network structure might be useful. A lot of images share the same variations such as rotations, translations, brightness changes, etc.. These deformations can be represented by a linear transformation in the pixel space. When learning a deep GMM, the model may pick up on these variations in the data that are shared amongst images by factoring and describing them with the transformations in the network.\nThe hypothesis of this paper is that Deep GMMs overfit less than normal GMMs as the complexity of their density functions increase because the parameter tying of the Deep GMM will force it to learn more useful functions. Note that this is one of the reasons why other deep learning methods are so successful. The only difference is that the parameter tying in deep GMMs is more explicit and interpretable.\nA closely related method is the deep mixture of factor analyzers (DMFA) model [10], which is an extension of the Mixture of Factor Analyzers (MFA) model [11]. The DMFA model has a tree structure in which every node is a factor analyzer that inherits the low-dimensional latent factors\nfrom its parent. Training is performed layer by layer, where the dataset is hierarchically clustered and the children of each node are trained as a MFA on a different subset of the data using the MFA EM algorithm. The parents nodes are kept constant when training its children. The main difference with the proposed method is that in the Deep GMM the nodes of each layer are connected to all nodes of the layer above. The layers are trained jointly and the higher level nodes will adapt to the lower level nodes."
    }, {
      "heading" : "3 Training deep GMMs with EM",
      "text" : "The algorithm we propose for training Deep GMMs is based on Expectation Maximization (EM). The optimization is similar to that of a GMM: in the E-step we will compute the posterior probabilities np that a path p was responsible for generating xn, also called the responsibilities. In the maximization step, the parameters of the model will be optimized given those responsibilities."
    }, {
      "heading" : "3.1 Expectation",
      "text" : "From Equation 1 we get the the log-likelihood given the data:\nX\nn\nlog p (xn) =\nX\nn\nlog\n2 4 X\np2 ⇡pN xn| p,⌦p⌦Tp\n3\n5 .\nThis is the global objective for the Deep GMM to optimize. When taking the derivative with respect to a parameter ✓ we get:\nr✓ X\nn\nlog p (xn) =\nX\nn,p\n⇡pN xn| p,⌦p⌦Tp ⇥ r✓ logN xn| p,⌦p⌦Tp ⇤ P q ⇡qN xn| q,⌦q⌦Tq\n=\nX\nn,p\nnpr✓ logN xn| p,⌦p⌦Tp ,\nwith\nnp = ⇡pN xn| p,⌦p⌦Tp\nP q2 ⇡qN xn| q,⌦q⌦Tq ,\nthe equation for the responsibilities. Although np generally depend on the parameter ✓, in the EM algorithm the responsibilities are assumed to remain constant when optimizing the model parameters in the M-step.\nThe E-step is very similar to that of a standard GMM, but instead of computing the responsibilities nk for every component k, one needs to compute them for every path p = (p1, p2, . . . , pk) 2 . This is because every path represents a Gaussian mixture component in the equivalent shallow GMM. Because np needs to be computed for each datapoint independently, the E-step is very easy to parallelize. Often a simple way to increase the speed of convergence and to reduce computation time is to use an EM-variant with “hard” assignments. Here only one of the responsibilities of each datapoint is set to 1:\nnp =\n⇢ 1 p = argmaxq ⇡qN xn| q,⌦q⌦Tq\n0 otherwise\n(4)\nHeuristic Because the number of paths is the product of the number of components per layer ( Qk\nj Nj), computing the responsibilities can become intractable for big Deep GMM networks. However, when using hard-EM variant (eq. 4), this problem reduces to finding the best path for each datapoint, for which we can use efficient heuristics. Here we introduce such a heuristic that does not hurt the performance significantly, while allowing us to train much larger networks.\nWe optimize the path p = (p1, p2, . . . , pk), which is a multivariate discrete variable, with a coordinate ascent algorithm. This means we change the parameters pi layer per layer, while keeping the\nparameter values of the other layers constant. After we have changed all the variables one time (one pass), we can repeat. The heuristic described above only requires Pk\nj Nj path evaluations per pass. In Figure 2 we compare the heuristic with the full search. On the left we see that after 3 passes the heuristic converges to a local optimum. In the middle we see that when repeating the heuristic algorithm a couple of times with different random initializations, and keeping the best path after each iteration, the loglikelihood converges to the optimum.\nIn our experiments we initialized the heuristic with the optimal path from the previous E-step (warm start) and performed the heuristic algorithm for 1 pass. Subsequently we ran the algorithm for a second time with a random initialization for two passes for the possibility of finding a better optimum for each datapoint. Each E-step thus required 3 ⇣Pk j Nj ⌘ path evaluations. In Figure 2(c) we\nshow an example of the percentage of data points (called the switch-rate) that had a better optimum with this second initialization for each EM-iteration. We can see from this Figure that the switchrate quickly becomes very small, which means that using the responsibilities from the previous E-step is an efficient initialization for the current one. Although the number of path evaluations with the heuristic is substantially smaller than with the full search, we saw in our experiments that the performance of the resulting trained Deep GMMs was ultimately similar."
    }, {
      "heading" : "3.2 Maximization",
      "text" : "In the maximization step, the parameters are updated to maximize the log likelihood of the data, given the responsibilities. Although standard optimization techniques for training deep networks can be used (such as SGD), Deep GMMs have some interesting properties that allow us to train them more efficiently. Because these properties are not obvious at first sight, we will derive the objective and gradient for the transformation matrices Ai,j in a Deep GMM. After that we will discuss various ways for optimizing them. For convenience, the derivations in this section are based on the hard-EM variant and with omission of the bias-terms parameters. Equations without these simplifications can be obtained in a similar manner.\nIn the hard-EM variant, it is assumed that each datapoint in the dataset was generated by a path p, for which n,p = 1. The likelihood of x given the parameters of the transformations on this path is\np (x) = A 1 1,p1 . . . A 1k,pk N\n⇣ A\n1 1,p1 . . . A 1 k,pk\nx|0, In ⌘ , (5)\nwhere we use |·| to denote the absolute value of the determinant. Now let’s rewrite:\nz = A 1 i+1,pi+1 . . . A 1 k,pk x (6)\nQ = A 1 i,pi\n(7)\nRp = A 1 1,p1 . . . A 1 i 1,pi 1 , (8)\nz\nso that we get (omitting the constant term w.r.t. Q): log p (x) / log |Q|+ logN (RpQz|0, In) . (9)\nFigure 3 gives a visual overview. We have “folded” the layers above the current layer into one. This means that each path p through the network above the current layer is equivalent to a transformation Rp in the folded version. The transformation matrix for which we will derive the objective and gradient is called Q. The average log-likelihood of all the data points that are generated by paths that pass through Q is:\n1\nN\nX\ni\nlog p (xi) / log |Q|+ 1\nN\nX\np\nX\ni2 p\nlogN (RpQzi|0, I) (10)\n= log |Q| 1 2\nX\np\n⇡pTr ⇥ pQ T ⌦pQ ⇤ , (11)\nwhere ⇡p = Np N , p = 1 Np P i2 p ziz T i and ⌦p = RTp Rp. For the gradient we get:\n1\nN\nrQ X\ni\nlog p (xi) = Q T\nX\np\n⇡p pQ T ⌦p. (12)\nOptimization\nNotice how in Equation 11 the summation over the data points has been converted to a summation over covariance matrices: one for each path1. If the number of paths is small enough, this means we can use full gradient updates instead of mini-batched updates (e.g. SGD). The computation of the covariance matrices is fairly efficient and can be done in parallel. This formulation also allows us to use more advanced optimization methods, such as LBFGS-B [12].\nIn the setup described above, we need to keep the transformation Rp constant while optimizing Q. This is why in each M-step the Deep GMM is optimized layer-wise from top to bottom, updating one layer at a time. It is possible to go over this process multiple times for each M-step. Important to note is that this way the optimization of Q does not depend on any other parameters in the same layer. So for each layer, the optimization of the different nodes can be done in parallel on multiple cores or machines. Moreover, nodes in the same layer do not share data points when using the EMvariant with hard-assignments. Another advantage is that this method is easy to control, as there are no learning rates or other optimization parameters to be tuned, when using L-BFGS-B “out of the box”. A disadvantage is that one needs to sum over all possible paths above the current node in the gradient computation. For deeper networks, this may become problematic when optimizing the lower-level nodes.\nAlternatively, one can also evaluate (11) using Kronecker products as\n· · · = log |Q|+ vec (Q)T ( X\np\n⇡p (⌦p ⌦ p) ) vec (Q) (13)\n1Actually we only need to sum over the number of possible transformations Rp above the node Q.\nand Equation 12 as\n· · · = Q T + 2mat ( X\np\n⇡p (⌦p ⌦ p) ) vec (Q) ! . (14)\nHere vec is the vectorization operator and mat its inverse. With these formulations we don’t have to loop over the number of paths anymore during the optimization. This makes the inner optimization with LBFGS-B even faster. We only have to construct\nP p ⇡p (⌦p ⌦ p) once, which is also easy to\nparallelize. These equation thus allow us to train even bigger Deep GMM architectures. A disadvantage, however, is that it requires the dimensionality of the data to be small enough to efficiently construct the Kronecker products.\nWhen the aforementioned formulations are intractable because there are too number layers in the Deep GMM and the data dimensionality is to high, we can also optimize the parameters using backpropagation with a minibatch algorithm, such as Stochastic Gradient Descent (SGD). This approach works for much deeper networks, because we don’t need to sum over the number of paths. From Equation 9 we see that this is basically the same as minimizing the L2 norm of RpQz, with log |Q| as regularization term. Disadvantages include the use of learning rates and other parameters such as momentum, which requires more engineering and fine-tuning.\nThe most naive way is to optimize the deep GMM with SGD is by simultaneously optimizing all parameters, as is common in neural networks. When doing this it is important that the parameters of all nodes are converged enough in each M-step, otherwise nodes that are not optimized enough may have very low responsibilities in the following E-step(s). This results in whole parts of the network becoming unused, which is the equivalent of empty clusters during GMM or k-means training. An alternative way of using SGD is again by optimizing the Deep GMM layer by layer. This has the advantage that we have more control over the optimization, which prevents the aforementioned problem of unused paths. But more importantly, we can now again parallelize over the number of nodes per layer."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "For our experiments we used the Berkeley Segmentation Dataset (BSDS300) [13], which is a commonly used benchmark for density modeling of image patches and the tiny images dataset [14]. For BSDS300 we follow the same setup of Uria et al. [15], which is best practice for this dataset. 8 by 8 grayscale patches are drawn from images of the dataset. The train and test sets consists of 200 and 100 images respectively. Because each pixel is quantized, it can only contain integer values between 0 and 255. To make the integer pixel values continuous, uniform noise (between 0 and 1) is added. Afterwards, the images are divided by 256 so that the pixel values lie in the range [0, 1]. Next, the patches are preprocessed by removing the mean pixel value of every image patch. Because this reduces the implicit dimensionality of the data, the last pixel value is removed. This results in the data points having 63 dimensions. For the tiny images dataset we rescale the images to 8 by 8 and then follow the same setup. This way we also have low resolution image data to evaluate on.\nIn all the experiments described in this section, we used the following setup for training Deep GMMs. We used the hard-EM variant, with the aforementioned heuristic in the E-step. For each M-step we used LBFGS-B for 1000 iterations by using equations (13) and (14) for the objective and gradient. The total number of iterations we used for EM was fixed to 100, although fewer iterations were usually sufficient. The only hyperparameters were the number of components for each layer, which were optimized on a validation set.\nBecause GMMs are in theory able to represent the same probability density functions as a Deep GMM, we first need to assess wether using multiple layers with a deep GMM improves performance. The results of a GMM (one layer) and Deep GMMs with two or three layers are given in 4(a). As we increase the complexity and number of parameters of the model by changing the number of components in the top layer, a plateau is reached and the models ultimately start overfitting. For the deep GMMs, the number of components in the other layers was kept constant (5 components). The Deep GMMs seem to generalize better. Although they have a similar number of parameters, they are able to model more complex relationships, without overfitting. We also tried this experiment on a more difficult dataset by using highly downscaled images from the tiny images dataset, see Figure\n4(b). Because there are less correlations between the pixels of a downscaled image than between those of an image patch, the average log likelihood values are lower. Overall we can see that the Deep GMM performs well on both low and high resolution natural images.\nNext we will compare the deep GMM with other published methods on this task. Results are shown in Table 1. The first method is the RNADE model, a new deep density estimation technique which is an extension of the NADE model for real valued data [16, 15]. EoRNADE, which stands for ensemble of RNADE models, is currently the state of the art. We also report the log-likelihood results of two mixture models: the GMM and the Student-T Mixture model, from [2]. Overall we see that the Deep GMM has a strong performance. It scores better than other single models (RNADE, STM), but not as well as the ensemble of RNADE models."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this work we introduced the deep Gaussian Mixture Model: a novel density estimation technique for modeling real valued data. we show that the Deep GMM is on par with the current state of the art in image patch modeling, and surpasses other mixture models. We conclude that the Deep GMM is a viable and scalable alternative for unsupervised learning. The deep GMM tackles unsupervised learning from a different angle than other recent deep unsupervised learning techniques [17, 18, 19], which makes it very interesting for future research.\nIn follow-up work, we would like to make Deep GMMs suitable for larger images and other highdimensional data. Locally connected filters, such as convolutions would be useful for this. We would also like to extend our method to modeling discrete data. Deep GMMs are currently only designed for continuous real-valued data, but our approach of reparametrizing the model into layers of successive transformations can also be applied to other types of mixture distributions. We would also like to compare this extension to other discrete density estimators such as Restricted Boltzmann Machines, Deep Belief Networks and the NADE model [15]."
    } ],
    "references" : [ {
      "title" : "From learning models of natural image patches to whole image restoration",
      "author" : [ "Daniel Zoran", "Yair Weiss" ],
      "venue" : "In International Conference on Computer Vision,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "The student-t mixture model as a natural image patch prior with application to image compression",
      "author" : [ "Aäron van den Oord", "Benjamin Schrauwen" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Neural probabilistic language models",
      "author" : [ "Yoshua Bengio", "Holger Schwenk", "Jean-Sbastien Sencal", "Frderic Morin", "Jean-Luc Gauvain" ],
      "venue" : "In Innovations in Machine Learning. Springer,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "In proceedings of Workshop at ICLR,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "One-shot learning by inverting a compositional causal process",
      "author" : [ "Brenden M. Lake", "Ruslan Salakhutdinov", "Joshua B. Tenenbaum" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "How to construct deep recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Çaglar Gülçehre", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "Foundations and Trends  R  in Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "One weird trick for parallelizing convolutional neural networks",
      "author" : [ "Alex Krizhevsky" ],
      "venue" : "In Proceedings of the International Conference on Learning Representations,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Deep mixtures of factor analysers",
      "author" : [ "Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "The em algorithm for mixtures of factor analyzers",
      "author" : [ "Zoubin Ghahramani", "Geoffrey E Hinton" ],
      "venue" : "Technical report, University of Toronto,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1996
    }, {
      "title" : "A limited memory algorithm for bound constrained optimization",
      "author" : [ "Richard H Byrd", "Peihuang Lu", "Jorge Nocedal", "Ciyou Zhu" ],
      "venue" : "SIAM Journal on Scientific Computing,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1995
    }, {
      "title" : "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
      "author" : [ "David Martin", "Charless Fowlkes", "Doron Tal", "Jitendra Malik" ],
      "venue" : "In Proceedings of the International Conference on Computer Vision. IEEE,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2001
    }, {
      "title" : "80 million tiny images: A large data set for nonparametric object and scene recognition",
      "author" : [ "Antonio Torralba", "Robert Fergus", "William T Freeman" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "A deep and tractable density estimator",
      "author" : [ "Benigno Uria", "Iain Murray", "Hugo Larochelle" ],
      "venue" : "In Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "RNADE: The real-valued neural autoregressive density-estimator",
      "author" : [ "Benigno Uria", "Iain Murray", "Hugo Larochelle" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Deep autoregressive networks",
      "author" : [ "Karol Gregor", "Andriy Mnih", "Daan Wierstra" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Stochastic back-propagation and variational inference in deep latent gaussian models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Deep generative stochastic networks trainable by backprop",
      "author" : [ "Yoshua Bengio", "Eric Thibodeau-Laufer", "Jason Yosinski" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6].",
      "startOffset" : 144,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "1 Introduction There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6].",
      "startOffset" : 180,
      "endOffset" : 186
    }, {
      "referenceID" : 3,
      "context" : "1 Introduction There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6].",
      "startOffset" : 180,
      "endOffset" : 186
    }, {
      "referenceID" : 4,
      "context" : "1 Introduction There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "1 Introduction There has been an increasing interest in generative models for unsupervised learning, with many applications in Image processing [1, 2], natural language processing [3, 4], vision [5] and audio [6].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 6,
      "context" : "One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their recent results in supervised learning [7].",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "Although not a universal recipe for success, the merits of deep learning are well-established [8].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model (GMM) and the Student-t Mixture Model (STM), that remain surprisingly competitive [2].",
      "startOffset" : 183,
      "endOffset" : 186
    }, {
      "referenceID" : 8,
      "context" : "Although there has been a lot of effort in scaling up SGD for deep networks [9], the Deep GMM is parallelizable by design.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 7,
      "context" : "This is often the case with deep learning methods: Shallow architectures can often theoretically learn the same functions, but will require a much larger number of parameters [8].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 9,
      "context" : "A closely related method is the deep mixture of factor analyzers (DMFA) model [10], which is an extension of the Mixture of Factor Analyzers (MFA) model [11].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 10,
      "context" : "A closely related method is the deep mixture of factor analyzers (DMFA) model [10], which is an extension of the Mixture of Factor Analyzers (MFA) model [11].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "This formulation also allows us to use more advanced optimization methods, such as LBFGS-B [12].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "4 Experiments and Results For our experiments we used the Berkeley Segmentation Dataset (BSDS300) [13], which is a commonly used benchmark for density modeling of image patches and the tiny images dataset [14].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 13,
      "context" : "4 Experiments and Results For our experiments we used the Berkeley Segmentation Dataset (BSDS300) [13], which is a commonly used benchmark for density modeling of image patches and the tiny images dataset [14].",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 14,
      "context" : "[15], which is best practice for this dataset.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "The first method is the RNADE model, a new deep density estimation technique which is an extension of the NADE model for real valued data [16, 15].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 14,
      "context" : "The first method is the RNADE model, a new deep density estimation technique which is an extension of the NADE model for real valued data [16, 15].",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "We also report the log-likelihood results of two mixture models: the GMM and the Student-T Mixture model, from [2].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : "The deep GMM tackles unsupervised learning from a different angle than other recent deep unsupervised learning techniques [17, 18, 19], which makes it very interesting for future research.",
      "startOffset" : 122,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "The deep GMM tackles unsupervised learning from a different angle than other recent deep unsupervised learning techniques [17, 18, 19], which makes it very interesting for future research.",
      "startOffset" : 122,
      "endOffset" : 134
    }, {
      "referenceID" : 18,
      "context" : "The deep GMM tackles unsupervised learning from a different angle than other recent deep unsupervised learning techniques [17, 18, 19], which makes it very interesting for future research.",
      "startOffset" : 122,
      "endOffset" : 134
    }, {
      "referenceID" : 14,
      "context" : "We would also like to compare this extension to other discrete density estimators such as Restricted Boltzmann Machines, Deep Belief Networks and the NADE model [15].",
      "startOffset" : 161,
      "endOffset" : 165
    } ],
    "year" : 2014,
    "abstractText" : "Generative models can be seen as the swiss army knives of machine learning, as many problems can be written probabilistically in terms of the distribution of the data, including prediction, reconstruction, imputation and simulation. One of the most promising directions for unsupervised learning may lie in Deep Learning methods, given their success in supervised learning. However, one of the current problems with deep unsupervised learning methods, is that they often are harder to scale. As a result there are some easier, more scalable shallow methods, such as the Gaussian Mixture Model and the Student-t Mixture Model, that remain surprisingly competitive. In this paper we propose a new scalable deep generative model for images, called the Deep Gaussian Mixture Model, that is a straightforward but powerful generalization of GMMs to multiple layers. The parametrization of a Deep GMM allows it to efficiently capture products of variations in natural images. We propose a new EM-based algorithm that scales well to large datasets, and we show that both the Expectation and the Maximization steps can easily be distributed over multiple machines. In our density estimation experiments we show that deeper GMM architectures generalize better than more shallow ones, with results in the same ballpark as the state of the art.",
    "creator" : null
  }
}