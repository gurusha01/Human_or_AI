{
  "name" : "3fab5890d8113d0b5a4178201dc842ad.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Beta-Negative Binomial Process and Exchangeable ￼Random Partitions for Mixed-Membership Modeling",
    "authors" : [ "Mingyuan Zhou" ],
    "emails" : [ "mingyuan.zhou@mccombs.utexas.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "For mixture modeling, there is a wide selection of nonparametric Bayesian priors, such as the Dirichlet process [1] and the more general family of normalized random measures with independent increments (NRMIs) [2, 3]. Although a draw from an NRMI usually consists of countably infinite atoms that are impossible to instantiate in practice, one may transform the infinite-dimensional problem into a finite one by marginalizing out the NRMI. For instance, it is well known that the marginalization of the Dirichlet process random probability measure under multinomial sampling leads to the Chinese restaurant process [4, 5]. The general structure of the Chinese restaurant process is broadened by [5] to the so called exchangeable partition probability function (EPPF) model, leading to fully collapsed inference and providing a unified view of the characteristics of various nonparametric Bayesian mixture-modeling priors. Despite significant progress on EPPF models in the past decade, their use in mixture modeling (clustering) is usually limited to a single set of data points.\nMoving beyond mixture modeling of a single set, there has been significant recent interest in mixedmembership modeling, i.e., mixture modeling of grouped data x1, . . . ,xJ , where each group xj = {xji}i=1,mj consists of mj data points that are exchangeable within the group. To cluster the mj data points in each group into a random, potentially unbounded number of partitions, which are exchangeable and shared across all the groups, is a much more challenging statistical problem. While the hierarchical Dirichlet process (HDP) [6] is a popular choice, it is shown in [7] that a wide variety of integer-valued stochastic processes, including the gamma-Poisson process [8, 9], betanegative binomial process (BNBP) [10, 11], and gamma-negative binomial process (GNBP), can all be applied to mixed-membership modeling. However, none of these stochastic processes are able to describe their marginal distributions that govern the exchangeable random partitions of grouped data. Without these marginal distributions, the HDP exploits an alternative representation known as the Chinese restaurant franchise [6] to derive collapsed inference, while fully collapsed inference is available for neither the BNBP nor the GNBP.\nThe EPPF provides a unified treatment to mixture modeling, but there is hardly a unified treatment to mixed-membership modeling. As the first step to fill that gap, this paper thoroughly investigates the law of the BNBP that governs its exchangeable random partitions of grouped data. As directly deriving the BNBP’s EPPF for mixed-membership modeling is difficult, we first randomize the group sizes {mj}j and derive the joint distribution of {mj}j and their random partitions on a shared list of exchangeable clusters; we then derive the marginal distribution of the group-size count vector m = (m1, . . . ,mJ)\nT , and use Bayes’ rule to further arrive at the BNBP’s EPPF that describes the prior distribution of a latent column-exchangeable random count matrix, whose jth row sums tomj .\nThe general method to arrive at an EPPF for mixed-membership modeling using an integer-valued stochastic process is an important contribution. We make several additional contributions: 1) We derive a prediction rule for the BNBP to simulate exchangeable random partitions of grouped data governed by its EPPF. 2) We construct a BNBP topic model, derive a fully collapsed Gibbs sampler that analytically marginalizes out not only the topics and topic weights, but also the infinitedimensional beta process, and provide closed-form update equations for model parameters. 3) The straightforward to implement BNBP topic model sampling algorithm converges fast, mixes well, and produces state-of-the-art predictive performance with a compact representation of the corpus."
    }, {
      "heading" : "1.1 Exchangeable Partition Probability Function",
      "text" : "Let Πm = {A1, . . . , Al} denote a random partition of the set [m] = {1, 2, . . . ,m}, where there are l partitions and each element i ∈ [m] belongs to one and only one set Ak from Πm. If P (Πm = {A1, . . . , Al}|m) depends only on the number and sizes of the Ak’s, regardless of their order, then it is called an exchangeable partition probability function (EPPF) of Πm. An EPPF of Πm is an EPPF of Π := (Π1,Π2, . . .) if P (Πm|n) = P (Πm|m) does not depend on n, where P (Πm|n) denotes the marginal partition probability for [m] when it is known the sample size is n. Such a constraint can also be expressed as an addition rule for the EPPF [5]. In this paper, the addition rule is not required and the proposed EPPF is allowed to be dependent on the group sizes (or sample size if the number of groups is one). Detailed discussions about sample size dependent EPPFs can be found in [12]. We generalize the work of [12] to model the partition of a count vector into a latent column-exchangeable random count matrix. A marginal sampler for σ-stable Poisson-Kigman mixture models (but not mixed-membership models) is proposed in [13], encompassing a large class of random probability measures and their corresponding EPPFs of Π. Note that the BNBP is not within that class and both the BNBP’s EPPF and perdition rule are dependent on the group sizes."
    }, {
      "heading" : "1.2 Beta Process",
      "text" : "The beta process B ∼ BP(c,B0) is a completely random measure defined on the product space [0, 1]× Ω, with a concentration parameter c > 0 and a finite and continuous base measure B0 over a complete separable metric space Ω [14, 15] . We define the Lévy measure of the beta process as\nν(dpdω) = p−1(1− p)c−1dpB0(dω). (1)\nA draw from B ∼ BP(c,B0) can be represented as a countably infinite sum as B =∑∞ k=1 pkδωk , ωk ∼ g0, where γ0 = B0(Ω) is the mass parameter and g0(dω) = B0(dω)/γ0 is the base distribution. The beta process is unique in that the beta distribution is not infinitely divisible, and its measure on a Borel set A ⊂ Ω, expressed as B(A) = ∑ k:ωk∈A pk, could be larger than one and hence clearly not a beta random variable. In this paper we will work with Q(A) = − ∑ k:ωk∈A ln(1− pk), defined as a logbeta random variable, to analyze model properties and derive closed-form Gibbs sampling update equations. We provide these details in the Appendix."
    }, {
      "heading" : "2 Exchangeable Cluster/Partition Probability Functions for the BNBP",
      "text" : "The integer-valued beta-negative binomial process (BNBP) is defined as\nXj |B ∼ NBP(rj , B), B ∼ BP(c,B0), (2)\nwhere for the jth group rj is the negative binomial dispersion parameter and Xj |B ∼ NBP(rj , B) is a negative binomial process such that Xj(A) = ∑ k:ωk∈A njk, njk ∼ NB(rj , pk) for each Borel set A ⊂ Ω. The negative binomial distribution n ∼ NB(r, p) has probability mass function (PMF) fN (n) = Γ(n+r) n!Γ(r) p n(1− p)r for n ∈ Z, where Z = {0, 1, . . .}. Our definition of the BNBP follows\nthose of [10, 7, 11], where for inference [10, 7] used finite truncation and [11] used slice sampling. There are two recent papers [16, 17] that both marginalize out the beta process from the negative binomial process, with the predictive structures of the BNBP described as the negative binomial Indian buffet process (IBP) [16] and “ice cream” buffet process [17], respectively. Both processes are also related to the “multi-scoop” IBP of [10], and they all generalize the binary-valued IBP [18]. Different from these two papers on infinite random count matrices, this paper focuses on generating a latent column-exchangeable random count matrix, each of whose row sums to a fixed observed integer. This paper generalizes the techniques developed in [17, 12] to define an EPPF for mixedmembership modeling and derive truncation-free fully collapsed inference.\nThe BNBP by nature is an integer-valued stochastic process as Xj(A) is a random count for each Borel set A ⊂ Ω. As the negative binomial process is also a gamma-Poisson mixture process, we can augment (2) as a beta-gamma-Poisson process as\nXj |Θj ∼ PP(Θj), Θj |rj , B ∼ ΓP[rj , B/(1−B)], B ∼ BP(c,B0),\nwhere Xj |Θj ∼ PP(Θj) is a Poisson process such that Xj(A) ∼ Pois[Θj(A)], and Θj |B ∼ ΓP[rj , B/(1−B)] is a gamma process such that Θj(A) = ∑ k:ωk∈A θjk, θjk ∼ Gamma[rj , pk/(1− pk)], for each Borel set A ⊂ Ω. The mixed-membership-modeling potentials of the BNBP become clear under this augmented representation. The Poisson process provides a bridge to link count modeling and mixture modeling [7], since Xj ∼ PP(Θj) can be equivalently generated by first drawing a total random count mj := Xj(Ω) ∼ Pois[Θj(Ω)] and then assigning this random count to disjoint disjoint Borel sets of Ω using a multinomial distribution."
    }, {
      "heading" : "2.1 Exchangeable Cluster Probability Function",
      "text" : "In mixed-membership modeling, the size of each group is observed rather being random, thus although the BNBP’s augmented representation is instructive, it is still unclear how exactly the integervalued stochastic process leads to a prior distribution on exchangeable random partitions of grouped data. The first step for us to arrive at such a prior distribution is to build a sample size dependent model that treats the number of data points to be clustered (partitioned) in each group as random. Below we will first derive an exchangeable cluster probability function (ECPF) governed by the BNBP to describe the joint distribution of the random group sizes and their random partitions over a random, potentially unbounded number of exchangeable clusters shared across all the groups. Later we will show how to derive the EPPF from the ECPF using Bayes’ rule. Lemma 1. Denote δk(zji) as a unit point mass at zji = k, njk = ∑mj i=1 δk(zji), and Xj(A) =∑\nk:ωk∈A njk as the number of data points in group j assigned to the atoms within the Borel set A ⊂ Ω. The Xj’s generated via the group size dependent model as\nzji ∼ ∑∞ k=1 θjk Θj(Ω)\nδk, mj ∼ Pois(Θj(Ω)), Θj ∼ ΓP[rj , B/(1−B)], B ∼ BP(c,B0) (3)\nis equivalent in distribution to the Xj’s generated from a BNBP as in (2). Proof. With B = ∑∞ k=1 pkδωk and Θj = ∑∞ k=1 θjkδωk , the joint distribution of the cluster indices\nzj = (zj1, . . . , zjmj ) given Θj and mj can be expressed as p(zj |Θj ,mj) = ∏mj i=1 θjzji∑∞ k′=1 θjk′\n= 1\n( ∑∞ k=1 θjk) mj ∏∞ k=1 θ njk jk ,which is not in a fully factorized form. Asmj is linked to the total random mass Θj(Ω) with a Poisson distribution, we have the joint likelihood of zj and mj given Θj as\nf(zj ,mj |Θj) = f(zj |Θj ,mj)Pois(mj ,Θj(Ω)) = 1mj ! ∏∞ k=1 θ njk jk e −θjk , (4)\nwhich is fully factorized and hence amenable to marginalization. Since θjk ∼ Gamma[rj , pk/(1 − pk)], we can marginalize θjk out analytically as f(zj ,mj |rj , B) = EΘj [f(zj ,mj |Θj)], leading to\nf(zj ,mj |rj , B) = 1mj ! ∏∞ k=1 Γ(njk+rj) Γ(rj) p njk k (1− pk)rj . (5)\nMultiplying the above equation with a multinomial coefficient transforms the prior distribution for the categorical random variables zj to the prior distribution for a random count vector as f(nj1, . . . , nj∞|rj , B) = mj !∏∞\nk=1 njk! f(zj ,mj |rj , B) =\n∏∞ k=1 NB(njk; rj , pk). Thus in the prior,\nfor each group, the sample size dependent model in ( 3) draws njk ∼ NB(rj , pk) random number of data points independently at each atom. With Xj := ∑∞ k=1 njkδωk , we have Xj |B ∼ NBP(rj , B)\nsuch that Xj(A) = ∑ k:ωk∈A njk, njk ∼ NB(rj , pk).\nThe Lemma below provides a finite-dimensional distribution obtained by marginalizing out the infinite-dimensional beta process from the BNBP. The proof is provided in the Appendix. Lemma 2 (ECPF). The exchangeable cluster probability function (ECPF) of the BNBP, which describes the joint distribution of the random count vectorm := (m1, . . . ,mJ)T and its exchangeable random partitions z = (z11, . . . , zJmJ ), can be expressed as\nf(z,m|r, γ0, c) = γ KJ 0 e −γ0[ψ(c+r·)−ψ(c)]∏J j=1mj ! ∏KJ k=1 [ Γ(n·k)Γ(c+r·) Γ(c+n·k+r·) ∏J j=1 Γ(njk+rj) Γ(rj) ] , (6)\nwhere KJ is the number of observed points of discontinuity for which n·k > 0, r := (r1, . . . , rJ)T , r· := ∑J j=1 rj , n·k := ∑J j=1 njk, and mj ∈ Z is the random size of group j."
    }, {
      "heading" : "2.2 Exchangeable Partition Probability Function and Prediction Rule",
      "text" : "Having the ECPF does not directly lead to the EPPF for the BNBP, as an EPPF describes the distribution of the exchangeable random partitions of the data groups whose sizes are observed rather than being random. To arrive at the EPPF, first we organize z into a random count matrix NJ ∈ ZJ×KJ , whose jth row represents the partition of the mj data points into the KJ shared exchangeable clusters and whose order of these KJ nonzero columns is chosen uniformly at random from one of the KJ ! possible permutations, then we obtain a prior distribution on a BNBP random count matrix as\nf(NJ |r, γ0, c) = 1KJ ! ∏J j=1 mj !∏KJ k=1 njk! f(z,m|r, γ0, c)\n= γ KJ 0 e −γ0[ψ(c+r·)−ψ(c)]\nKJ !\n∏KJ k=1 Γ(n·k)Γ(c+r·) Γ(c+n·k+r·) ∏J j=1 Γ(njk+rj) njk!Γ(rj) . (7)\nAs described in detail in [17], although the matrix prior does not appear to be simple, direct calculation shows that this random count matrix has KJ ∼ Pois {γ0 [ψ(c+ r·)− ψ(c)]} independent and identically distributed (i.i.d.) columns that can be generated via\nn:k ∼ DirMult(n·k, r1, . . . , rJ), n·k ∼ Digam(r·, c), (8) where n:k := (n1k, . . . , nJk)T is the count vector for the kth column (cluster), the Dirichlet-multinomial (DirMult) distribution [19] has PMF DirMult(n:k|n·k, r) = n·k!∏J j=1 njk! Γ(r·) Γ(n·k+r·) ∏J j=1 Γ(njk+rj) Γ(rj)\n, and the digamma distribution [20] has PMF Digam(n|r, c) = 1\nψ(c+r)−ψ(c) Γ(r+n)Γ(c+r) nΓ(c+n+r)Γ(r) , where n = 1, 2, . . .. Thus in the prior, the BNBP generates a Poisson random number of clusters, the size of each cluster follows a digamma distribution, and each cluster is further partitioned into the J groups using a Dirichlet-multinomial distribution [17].\nWith both the ECPF and random count matrix prior governed by the BNBP, we are ready to derive both the EPPF and prediction rule, given in the following two Lemmas, with proofs in the Appendix. Lemma 3 (EPPF). Let ∑∑K k=1 n:k=m\ndenote a summation over all sets of count vectors with∑K k=1 n:k = m, where m· = ∑J j=1mj and n·k ≥ 1. The group-size dependent exchangeable partition probability function (EPPF) governed by the BNBP can be expressed as\nf(z|m, r, γ0, c) = γ KJ 0∏J j=1mj !\n∏KJ k=1 [ Γ(n·k)Γ(c+r·) Γ(c+n·k+r·) ∏J j=1 Γ(njk+rj) Γ(rj) ] ∑m· K′=1 γK ′ 0 K′! ∑∑K′ k′=1 n:k′=m ∏K′ k′=1 Γ(n·k′ )Γ(c+r·) Γ(c+n·k′+r·) ∏J j=1 Γ(njk′+rj) njk′ !Γ(rj) , (9)\nwhich is a function of the cluster sizes {njk}k=1,KJ , regardless of the orders of the indices k’s.\nAlthough the EPPF is fairly complicated, one may derive a simple prediction rule, as shown below, to simulate exchangeable random partitions of grouped data governed by this EPPF. Lemma 4 (Prediction Rule). With y−ji representing the variable y that excludes the contribution of xji, the prediction rule of the BNBP group size dependent model in (3) can be expressed as\nP (zji = k|z−ji,m, r, γ0, c) ∝  n−ji·k c+n−ji·k +r· (n−jijk + rj), for k = 1, . . . ,K −ji J ;\nγ0 c+r· rj , if k = K −ji J + 1.\n(10)\nψ(c+ ∑ j rj)−ψ(c)\n, and (a) rj = 1, (b) rj = 10, or (c) rj = 100 for all the 10 groups. The jth row\nof each matrix, which sums to 50, represents the partition of the mj = 50 data points of the jth group over a random number of exchangeable clusters, and the kth column of each matrix represents the kth nonempty cluster in order of appearance in Gibbs sampling (the empty clusters are deleted)."
    }, {
      "heading" : "2.3 Simulating Exchangeable Random Partitions of Grouped Data",
      "text" : "While the EPPF (9) is not simple, the prediction rule (10) clearly shows that the probability of selecting k is proportional to the product of two terms: one is related to the kth cluster’s overall popularity across groups, and the other is only related to the kth cluster’s popularity at that group and that group’s dispersion parameter; and the probability of creating a new cluster is related to γ0, c, r· and rj . The BNBP’s exchangeable random partitions of the group-size count vectorm, whose prior distribution is governed by (9), can be easily simulated via Gibbs sampling using (10).\nRunning Gibbs sampling using (10) for 2500 iterations and displaying the last sample, we show in Figure 1 (a)-(c) three distinct exchangeable random partitions of the same group-size count vector, under three different parameter settings. It is clear that the dispersion parameters {rj}j play a critical role in controlling how overdispersed the counts are: the smaller the {rj}j are, the more overdispersed the counts in each row and those in each column are. This is unsurprising as in the BNBP’s prior, we have njk ∼ NB(rj , pk) and n:k ∼ DirMult(n·k, r1, . . . , rJ). Figure 1 suggests that it is important to infer rj rather than setting them in a heuristic manner or fixing them."
    }, {
      "heading" : "3 Beta-Negative Binomial Process Topic Model",
      "text" : "With the BNBP’s EPPF derived, it becomes evident that the integer-valued BNBP also governs a prior distribution for exchangeable random partitions of grouped data. To demonstrate the use of the BNBP, we apply it to topic modeling [21] of a document corpus, a special case of mixture modeling of grouped data, where the words of the jth document xj1, . . . , xjmj constitute a group xj (mj words in document j), each word xji is an exchangeable group member indexed by vji in a vocabulary with V unique terms. We choose the base distribution as a V dimensional Dirichlet distribution as g0(φ) = Dir(φ; η, . . . , η), and choose a multinomial distribution to link a word to a topic (atom). We express the hierarchical construction of the BNBP topic model as\nxji ∼ Mult(φzji), φk ∼ Dir(η, . . . , η), zji ∼ ∑∞ k=1 θjk Θj(Ω) δk, mj ∼ Pois(Θj(Ω)),\nΘj ∼ ΓP ( rj , B 1−B ) , rj ∼ Gamma(a0, 1/b0), B ∼ BP(c,B0), γ0 ∼ Gamma(e0, 1/f0). (11)\nLet nvjk := ∑mj i=1 δv(xji)δk(zji). Multiplying (4) and the data likelihood f(xj |zj ,Φ) =∏V\nv=1 ∏∞ k=1(φvk)\nnvjk , where Φ = (φ1, . . . ,φ∞), we have f(xj , zj ,mj |Φ,Θj) =∏∞ k=1 ∏V v=1 nvjk!\nmj !\n∏∞ k=1 ∏V v=1 Pois(nvjk;φvkθjk). Thus the BNBP topic model can also be con-\nsidered as an infinite Poisson factor model [10], where the term-document word count matrix (mvj)v=1:V, j=1:J is factorized under the Poisson likelihood as mvj = ∑∞ k=1 nvjk, nvjk ∼ Pois(φvkθjk), whose likelihood f({nvjk}v,k|Φ,Θj) is different from f(xj , zj ,mj |Φ,Θj) up to a multinomial coefficient.\nThe full conditional likelihood f(x, z,m|Φ,Θ) = ∏J j=1 f(xj , zj ,mj |Φ,Θj) can be further\nexpressed as f(x, z,m|Φ,Θ) = {∏∞\nk=1 ∏V v=1 φ nv·k vk } · {∏∞ k=1 ∏J j=1 θ njk jk e −θjk∏J j=1mj ! } , where the\nmarginalization of Φ from the first right-hand-side term is the product of Dirichlet-multinomial distributions and the second right-hand-side term leads to the ECPF. Thus we have a fully marginalized\nlikelihood as f(x, z,m|γ0, c, r) = f(z,m|γ0, c, r) ∏KJ k=1 [ Γ(V η)\nΓ(n·k+V η)\n∏V v=1 Γ(nv·k+η) Γ(η) ] . Directly\napplying Bayes’ rule to this fully marginalized likelihood, we construct a nonparametric Bayesian fully collapsed Gibbs sampler for the BNBP topic model as\nP (zji = k|x,z−ji, γ0,m, c, r)∝  η+n−jivji·k V η+n−ji·k · n −ji ·k c+n−ji·k +r· · (n−jijk + rj), for k = 1, . . . ,K −ji J ;\n1 V · γ0 c+r· · rj , if k = K−jiJ + 1. (12)\nIn the Appendix we include all the other closed-form Gibbs sampling update equations."
    }, {
      "heading" : "3.1 Comparison with Other Collapsed Gibbs Samplers",
      "text" : "One may compare the collapsed Gibbs sampler of the BNBP topic model with that of latent Dirichlet allocation (LDA) [22], which, in our notation, can be expressed as\nP (zji = k|x, z−ji,m, α,K) ∝ η+n−jivji·k\nV η+n−ji·k · (n−jijk + α), for k = 1, . . . ,K, (13)\nwhere the number of topics K and the topic proportion Dirichlet smoothing parameter α are both tuning parameters. The BNBP topic model is a nonparametric Bayesian algorithm that removes the need to tune these parameters. One may also compare the BNBP topic model with the HDP-LDA [6, 23], whose direct assignment sampler in our notation can be expressed as\nP (zji = k|x, z−ji,m, α, r̃) ∝  η+n−jivji·k V η+n−ji·k · (n−jijk + αr̃k), for k = 1, . . . ,K −ji J ;\n1 V · (αr̃∗), if k = K −ji J + 1;\n(14)\nwhere α is the concentration parameter for the group-specific Dirichlet processes Θ̃j ∼ DP(α, G̃), and r̃k = G̃(ωk) and r̃∗ = G̃(Ω\\DJ) are the measures of the globally shared Dirichlet process G̃ ∼ DP(γ0, G̃0) over the observed points of discontinuity and absolutely continuous space, respectively.\nComparison between (14) and (12) shows that distinct from the HDP-LDA that combines a topic’s global and local popularities in an additive manner as (n−jijk + αr̃k), the BNBP topic model combines them in a multiplicative manner as n −ji ·k\nc+n−ji·k +r· · (n−jijk + rj). This term can also be rewritten\nas the product of n−ji·k and n−jijk +rj\nc+n−ji·k +r· , the latter of which represents how much the jth document\ncontributes to the overall popularity of the kth topic. Therefore, the BNBP and HDP-LDA have distinct mechanisms to automatically shrink the number of topics. Note that while the BNBP sampler in (12) is fully collapsed, the direct assignment sampler of the HDP-LDA in (14) is only partially collapsed as neither the globally shared Dirichlet process G̃ nor the concentration parameter α are marginalized out. To derive a collapsed sampler for the HDP-LDA that marginalizes out G̃ (but still not α), one has to use the Chinese restaurant franchise [6], which has cumbersome book-keeping as each word is indirectly linked to its topic via a latent table index."
    }, {
      "heading" : "4 Example Results",
      "text" : "We consider the JACM1, PsyReview2, and NIPS123 corpora, restricting the vocabulary to terms that occur in five or more documents. The JACM corpus includes 536 documents, with V = 1539 unique terms and 68,055 total word counts. The PsyReview corpus includes 1281 documents, with V = 2566 and 71,279 total word counts. The NIPS12 corpus includes 1740 documents, with V = 13, 649 and 2,301,375 total word counts. To evaluate the BNBP topic model4 and its performance relative to that of the HDP-LDA, which are both nonparametric Bayesian algorithms, we randomly choose 50%\n1http://www.cs.princeton.edu/∼blei/downloads/ 2http://psiexp.ss.uci.edu/research/programs data/toolbox.htm 3http://www.cs.nyu.edu/∼roweis/data.html 4Matlab code available in http://mingyuanzhou.github.io/\nof the words in each document as training, and use the remaining ones to calculate per-word heldout perplexity. We set the hyperparameters as a0 = b0 = e0 = f0 = 0.01. We consider 2500 Gibbs sampling iterations and collect the last 1500 samples. In each iteration, we randomize the ordering of the words. For each collected sample, we draw the topics (φk|−) ∼ Dir(η + n1·k, . . . , η + nJ·k), and the topics weights (θjk|−) ∼ Gamma(njk + rj , pk) for the BNBP and topic proportions (θk|−) ∼ Dir(nj1 + αr̃1, . . . , njKJ + αr̃KJ ) for the HDP, with which the per-word perplexity is\ncomputed as exp ( − 1mtest·· ∑ v ∑ jm test vj ln ∑ s ∑ k φ (s) vk θ (s) jk∑\ns ∑ v ∑ k φ (s) vk θ (s) jk\n) , where s ∈ {1, . . . , S} is the index\nof a collected MCMC sample, mtestvj is the number of test words at term v in document j, and mtest = ∑ v ∑ jm test vj . The final results are averaged over five random training/testing partitions. The evaluation method is similar to those used in [24, 25, 26, 10]. Similar to [26, 10], we set the topic Dirichlet smoothing parameter as η = 0.01, 0.02, 0.05, 0.10, 0.25, or 0.50. To test how the algorithms perform in more extreme settings, we also consider η = 0.001, 0.002, and 0.005. All algorithms are implemented with unoptimized Matlab code. On a 3.4 GHz CPU, the fully collapsed Gibbs sampler of the BNBP topic model takes about 2.5 seconds per iteration on the NIPS12 corpus when the inferred number of topics is around 180. The direct assignment sampler of the HDP-LDA has comparable computational complexity when the inferred number of topics is similar. Note that when the inferred number of topics KJ is large, the sparse computation technique for LDA [27, 28] may also be used to considerably speed up the sampling algorithm of the BNBP topic model.\nWe first diagnose the convergence and mixing of the collapsed Gibbs samplers for the HDP-LDA and BNBP topic model via the trace plots of their samples. The three plots in the left column of Figures 2 show that the HDP-LDA travels relatively slowly to the target distributions of the number of topics, often reaching them in more than 300 iterations, whereas the three plots in the right column show that the BNBP topic model travels quickly to the target distributions, usually reaching them in less than 100 iterations. Moreover, Figures 2 shows that the chains of the HDP-LDA are taking in small steps and do not traverse their distributions quickly, whereas the chains of the BNBP topic models mix very well locally and traverse their distributions relatively quickly.\nA smaller topic Dirichlet smoothing parameter η generally supports a larger number of topics, as shown in the left column of Figure 3, and hence often leads to lower perplexities, as shown in the middle column of Figure 3; however, an η that is as small as 0.001 (not commonly used in practice) may lead to more than a thousand topics and consequently overfit the corpus, which is particularly evident for the HDP-LDA on both the JACM and PsyReview corpora. Similar trends are also likely to be observed on the larger NIPS2012 corpus if we allow the values of η to be even smaller than 0.001. As shown in the middle column of Figure 3, for the same η, the BNBP topic model, usually representing the corpus with a smaller number of topics, often have higher perplexities than those of the HDP-LDA, which is unsurprising as the BNBP topic model has a multiplicative control mechanism to more strongly shrink the number of topics, whereas the HDP has a softer additive shrinkage mechanism. Similar performance differences have also been observed\nin [7], where the HDP and BNBP are inferred under finite approximations with truncated block Gibbs sampling. However, it does not necessarily mean that the HDP-LDA has better predictive performance than the BNBP topic model. In fact, as shown in the right column of Figure 3, the BNBP topic model’s perplexity tends to be lower than that of the HDP-LDA if their inferred number of topics are comparable and the η is not overly small, which implies that the BNBP topic model is able to achieve the same predictive power as the HDP-LDA, but with a more compact representation of the corpus under common experimental settings. While it is interesting to understand the ultimate potentials of the HDP-LDA and BNBP topic model for out-of-sample prediction by setting the η to be very small, a moderate η that supports a moderate number of topics is usually preferred in practice, for which the BNBP topic model could be a preferred choice over the HDP-LDA, as our experimental results on three different corpora all suggest that the BNBP topic model could achieve lower-perplexity using the same number of topics. To further understand why the BNBP topic model and HDP-LDA have distinct characteristics, one may view them from a count-modeling perspective [7] and examine how they differently control the relationship between the variances and means of the latent topic usage count vectors {(n1k, . . . , nJk)}k. We also find that the BNBP collapsed Gibbs sampler clearly outperforms the blocked Gibbs sampler of [7] in terms of convergence speed, computational complexity and memory requirement. But a blocked Gibbs sampler based on finite truncation [7] or adaptive truncation [11] does have a clear advantage that it is easy to parallelize. The heuristics used to parallelize an HDP collapsed sampler [24] may also be modified to parallelize the proposed BNBP collapsed sampler."
    }, {
      "heading" : "5 Conclusions",
      "text" : "A group size dependent exchangeable partition probability function (EPPF) for mixed-membership modeling is developed using the integer-valued beta-negative binomial process (BNBP). The exchangeable random partitions of grouped data, governed by the EPPF of the BNBP, are strongly influenced by the group-specific dispersion parameters. We construct a BNBP nonparametric Bayesian topic model that is distinct from existing ones, intuitive to interpret, and straightforward to implement. The fully collapsed Gibbs sampler converges fast, mixes well, and has state-of-the-art predictive performance when a compact representation of the corpus is desired. The method to derive the EPPF for the BNBP via a group size dependent model is unique, and it is of interest to further investigate whether this method can be generalized to derive new EPPFs for mixed-membership modeling that could be introduced by other integer-valued stochastic processes, including the gamma-Poisson and gamma-negative binomial processes."
    } ],
    "references" : [ {
      "title" : "A Bayesian analysis of some nonparametric problems",
      "author" : [ "T.S. Ferguson" ],
      "venue" : "Ann. Statist.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Distributional results for means of normalized random measures with independent increments",
      "author" : [ "E. Regazzini", "A. Lijoi", "I. Prünster" ],
      "venue" : "Annals of Statistics",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Models beyond the Dirichlet process",
      "author" : [ "A. Lijoi", "I. Prünster" ],
      "venue" : "N. L. Hjort, C. Holmes, P. Müller, and S. G. Walker, editors, Bayesian nonparametrics. Cambridge Univ. Press",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Ferguson distributions via Pólya urn schemes",
      "author" : [ "D. Blackwell", "J. MacQueen" ],
      "venue" : "The Annals of Statistics",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Combinatorial stochastic processes",
      "author" : [ "J. Pitman" ],
      "venue" : "Lecture Notes in Mathematics. Springer- Verlag",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Hierarchical Dirichlet processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "JASA",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Negative binomial process count and mixture modeling",
      "author" : [ "M. Zhou", "L. Carin" ],
      "venue" : "To appear in IEEE Trans. Pattern Anal. Mach. Intelligence",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Bayesian nonparametric statistical inference for Poisson point processes",
      "author" : [ "A.Y. Lo" ],
      "venue" : "Zeitschrift fur, pages 55–66",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "The infinite gamma-Poisson feature model",
      "author" : [ "M.K. Titsias" ],
      "venue" : "NIPS",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Beta-negative binomial process and Poisson factor analysis",
      "author" : [ "M. Zhou", "L. Hannah", "D. Dunson", "L. Carin" ],
      "venue" : "AISTATS",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Combinatorial clustering and the beta negative binomial process",
      "author" : [ "T. Broderick", "L. Mackey", "J. Paisley", "M.I. Jordan" ],
      "venue" : "To appear in IEEE Trans. Pattern Anal. Mach. Intelligence",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sample size dependent species models",
      "author" : [ "M. Zhou", "S.G. Walker" ],
      "venue" : "arXiv:1410.3155",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A marginal sampler for σ-Stable Poisson-Kingman mixture models",
      "author" : [ "M. Lomelı", "S. Favaro", "Y.W. Teh" ],
      "venue" : "arXiv preprint arXiv:1407.4211,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Nonparametric Bayes estimators based on beta processes in models for life history data",
      "author" : [ "N.L. Hjort" ],
      "venue" : "Ann. Statist.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Hierarchical beta processes and the Indian buffet process",
      "author" : [ "R. Thibaux", "M.I. Jordan" ],
      "venue" : "AISTATS",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The combinatorial structure of beta negative binomial processes",
      "author" : [ "C. Heaukulani", "D.M. Roy" ],
      "venue" : "arXiv:1401.0062",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Priors for random count matrices derived from a family of negative binomial processes",
      "author" : [ "M. Zhou", "O.-H. Madrid-Padilla", "J.G. Scott" ],
      "venue" : "arXiv:1404.3331v2",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Infinite latent feature models and the Indian buffet process",
      "author" : [ "T.L. Griffiths", "Z. Ghahramani" ],
      "venue" : "NIPS",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Modeling word burstiness using the Dirichlet distribution",
      "author" : [ "R.E. Madsen", "D. Kauchak", "C. Elkan" ],
      "venue" : "ICML",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Generalized hypergeometric",
      "author" : [ "M. Sibuya" ],
      "venue" : "digamma and trigamma distributions. Annals of the Institute of Statistical Mathematics, pages 373–390",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "J. Mach. Learn. Res.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "T.L. Griffiths", "M. Steyvers" ],
      "venue" : "PNAS",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Online variational inference for the hierarchical Dirichlet process",
      "author" : [ "C. Wang", "J. Paisley", "D.M. Blei" ],
      "venue" : "AISTATS",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distributed algorithms for topic models",
      "author" : [ "D. Newman", "A. Asuncion", "P. Smyth", "M. Welling" ],
      "venue" : "JMLR",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Evaluation methods for topic models",
      "author" : [ "H.M. Wallach", "I. Murray", "R. Salakhutdinov", "D. Mimno" ],
      "venue" : "ICML",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The discrete infinite logistic normal distribution for mixedmembership modeling",
      "author" : [ "J. Paisley", "C. Wang", "D. Blei" ],
      "venue" : "AISTATS",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Fast collapsed Gibbs sampling for latent Dirichlet allocation",
      "author" : [ "I. Porteous", "D. Newman", "A. Ihler", "A. Asuncion", "P. Smyth", "M. Welling" ],
      "venue" : "SIGKDD",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Sparse stochastic inference for latent Dirichlet allocation",
      "author" : [ "D. Mimno", "M. Hoffman", "D. Blei" ],
      "venue" : "ICML",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For mixture modeling, there is a wide selection of nonparametric Bayesian priors, such as the Dirichlet process [1] and the more general family of normalized random measures with independent increments (NRMIs) [2, 3].",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 1,
      "context" : "For mixture modeling, there is a wide selection of nonparametric Bayesian priors, such as the Dirichlet process [1] and the more general family of normalized random measures with independent increments (NRMIs) [2, 3].",
      "startOffset" : 210,
      "endOffset" : 216
    }, {
      "referenceID" : 2,
      "context" : "For mixture modeling, there is a wide selection of nonparametric Bayesian priors, such as the Dirichlet process [1] and the more general family of normalized random measures with independent increments (NRMIs) [2, 3].",
      "startOffset" : 210,
      "endOffset" : 216
    }, {
      "referenceID" : 3,
      "context" : "For instance, it is well known that the marginalization of the Dirichlet process random probability measure under multinomial sampling leads to the Chinese restaurant process [4, 5].",
      "startOffset" : 175,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "For instance, it is well known that the marginalization of the Dirichlet process random probability measure under multinomial sampling leads to the Chinese restaurant process [4, 5].",
      "startOffset" : 175,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "The general structure of the Chinese restaurant process is broadened by [5] to the so called exchangeable partition probability function (EPPF) model, leading to fully collapsed inference and providing a unified view of the characteristics of various nonparametric Bayesian mixture-modeling priors.",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "While the hierarchical Dirichlet process (HDP) [6] is a popular choice, it is shown in [7] that a wide variety of integer-valued stochastic processes, including the gamma-Poisson process [8, 9], betanegative binomial process (BNBP) [10, 11], and gamma-negative binomial process (GNBP), can all be applied to mixed-membership modeling.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "While the hierarchical Dirichlet process (HDP) [6] is a popular choice, it is shown in [7] that a wide variety of integer-valued stochastic processes, including the gamma-Poisson process [8, 9], betanegative binomial process (BNBP) [10, 11], and gamma-negative binomial process (GNBP), can all be applied to mixed-membership modeling.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 7,
      "context" : "While the hierarchical Dirichlet process (HDP) [6] is a popular choice, it is shown in [7] that a wide variety of integer-valued stochastic processes, including the gamma-Poisson process [8, 9], betanegative binomial process (BNBP) [10, 11], and gamma-negative binomial process (GNBP), can all be applied to mixed-membership modeling.",
      "startOffset" : 187,
      "endOffset" : 193
    }, {
      "referenceID" : 8,
      "context" : "While the hierarchical Dirichlet process (HDP) [6] is a popular choice, it is shown in [7] that a wide variety of integer-valued stochastic processes, including the gamma-Poisson process [8, 9], betanegative binomial process (BNBP) [10, 11], and gamma-negative binomial process (GNBP), can all be applied to mixed-membership modeling.",
      "startOffset" : 187,
      "endOffset" : 193
    }, {
      "referenceID" : 9,
      "context" : "While the hierarchical Dirichlet process (HDP) [6] is a popular choice, it is shown in [7] that a wide variety of integer-valued stochastic processes, including the gamma-Poisson process [8, 9], betanegative binomial process (BNBP) [10, 11], and gamma-negative binomial process (GNBP), can all be applied to mixed-membership modeling.",
      "startOffset" : 232,
      "endOffset" : 240
    }, {
      "referenceID" : 10,
      "context" : "While the hierarchical Dirichlet process (HDP) [6] is a popular choice, it is shown in [7] that a wide variety of integer-valued stochastic processes, including the gamma-Poisson process [8, 9], betanegative binomial process (BNBP) [10, 11], and gamma-negative binomial process (GNBP), can all be applied to mixed-membership modeling.",
      "startOffset" : 232,
      "endOffset" : 240
    }, {
      "referenceID" : 5,
      "context" : "Without these marginal distributions, the HDP exploits an alternative representation known as the Chinese restaurant franchise [6] to derive collapsed inference, while fully collapsed inference is available for neither the BNBP nor the GNBP.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : "Such a constraint can also be expressed as an addition rule for the EPPF [5].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "Detailed discussions about sample size dependent EPPFs can be found in [12].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "We generalize the work of [12] to model the partition of a count vector into a latent column-exchangeable random count matrix.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "A marginal sampler for σ-stable Poisson-Kigman mixture models (but not mixed-membership models) is proposed in [13], encompassing a large class of random probability measures and their corresponding EPPFs of Π.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "2 Beta Process The beta process B ∼ BP(c,B0) is a completely random measure defined on the product space [0, 1]× Ω, with a concentration parameter c > 0 and a finite and continuous base measure B0 over a complete separable metric space Ω [14, 15] .",
      "startOffset" : 238,
      "endOffset" : 246
    }, {
      "referenceID" : 14,
      "context" : "2 Beta Process The beta process B ∼ BP(c,B0) is a completely random measure defined on the product space [0, 1]× Ω, with a concentration parameter c > 0 and a finite and continuous base measure B0 over a complete separable metric space Ω [14, 15] .",
      "startOffset" : 238,
      "endOffset" : 246
    }, {
      "referenceID" : 9,
      "context" : "those of [10, 7, 11], where for inference [10, 7] used finite truncation and [11] used slice sampling.",
      "startOffset" : 9,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "those of [10, 7, 11], where for inference [10, 7] used finite truncation and [11] used slice sampling.",
      "startOffset" : 9,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "those of [10, 7, 11], where for inference [10, 7] used finite truncation and [11] used slice sampling.",
      "startOffset" : 9,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "those of [10, 7, 11], where for inference [10, 7] used finite truncation and [11] used slice sampling.",
      "startOffset" : 42,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "those of [10, 7, 11], where for inference [10, 7] used finite truncation and [11] used slice sampling.",
      "startOffset" : 42,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "those of [10, 7, 11], where for inference [10, 7] used finite truncation and [11] used slice sampling.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "There are two recent papers [16, 17] that both marginalize out the beta process from the negative binomial process, with the predictive structures of the BNBP described as the negative binomial Indian buffet process (IBP) [16] and “ice cream” buffet process [17], respectively.",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 16,
      "context" : "There are two recent papers [16, 17] that both marginalize out the beta process from the negative binomial process, with the predictive structures of the BNBP described as the negative binomial Indian buffet process (IBP) [16] and “ice cream” buffet process [17], respectively.",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 15,
      "context" : "There are two recent papers [16, 17] that both marginalize out the beta process from the negative binomial process, with the predictive structures of the BNBP described as the negative binomial Indian buffet process (IBP) [16] and “ice cream” buffet process [17], respectively.",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 16,
      "context" : "There are two recent papers [16, 17] that both marginalize out the beta process from the negative binomial process, with the predictive structures of the BNBP described as the negative binomial Indian buffet process (IBP) [16] and “ice cream” buffet process [17], respectively.",
      "startOffset" : 258,
      "endOffset" : 262
    }, {
      "referenceID" : 9,
      "context" : "Both processes are also related to the “multi-scoop” IBP of [10], and they all generalize the binary-valued IBP [18].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "Both processes are also related to the “multi-scoop” IBP of [10], and they all generalize the binary-valued IBP [18].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 16,
      "context" : "This paper generalizes the techniques developed in [17, 12] to define an EPPF for mixedmembership modeling and derive truncation-free fully collapsed inference.",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : "This paper generalizes the techniques developed in [17, 12] to define an EPPF for mixedmembership modeling and derive truncation-free fully collapsed inference.",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "The Poisson process provides a bridge to link count modeling and mixture modeling [7], since Xj ∼ PP(Θj) can be equivalently generated by first drawing a total random count mj := Xj(Ω) ∼ Pois[Θj(Ω)] and then assigning this random count to disjoint disjoint Borel sets of Ω using a multinomial distribution.",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 16,
      "context" : "(7) As described in detail in [17], although the matrix prior does not appear to be simple, direct calculation shows that this random count matrix has KJ ∼ Pois {γ0 [ψ(c+ r·)− ψ(c)]} independent and identically distributed (i.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : ", nJk) is the count vector for the kth column (cluster), the Dirichlet-multinomial (DirMult) distribution [19] has PMF DirMult(n:k|n·k, r) = n·k! ∏J j=1 njk! Γ(r·) Γ(n·k+r·) ∏J j=1 Γ(njk+rj) Γ(rj) , and the digamma distribution [20] has PMF Digam(n|r, c) =",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 19,
      "context" : ", nJk) is the count vector for the kth column (cluster), the Dirichlet-multinomial (DirMult) distribution [19] has PMF DirMult(n:k|n·k, r) = n·k! ∏J j=1 njk! Γ(r·) Γ(n·k+r·) ∏J j=1 Γ(njk+rj) Γ(rj) , and the digamma distribution [20] has PMF Digam(n|r, c) =",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 16,
      "context" : "Thus in the prior, the BNBP generates a Poisson random number of clusters, the size of each cluster follows a digamma distribution, and each cluster is further partitioned into the J groups using a Dirichlet-multinomial distribution [17].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 20,
      "context" : "To demonstrate the use of the BNBP, we apply it to topic modeling [21] of a document corpus, a special case of mixture modeling of grouped data, where the words of the jth document xj1, .",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 9,
      "context" : "Thus the BNBP topic model can also be considered as an infinite Poisson factor model [10], where the term-document word count matrix (mvj)v=1:V, j=1:J is factorized under the Poisson likelihood as mvj = ∑∞ k=1 nvjk, nvjk ∼ Pois(φvkθjk), whose likelihood f({nvjk}v,k|Φ,Θj) is different from f(xj , zj ,mj |Φ,Θj) up to a multinomial coefficient.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 21,
      "context" : "One may compare the collapsed Gibbs sampler of the BNBP topic model with that of latent Dirichlet allocation (LDA) [22], which, in our notation, can be expressed as",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "One may also compare the BNBP topic model with the HDP-LDA [6, 23], whose direct assignment sampler in our notation can be expressed as",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 22,
      "context" : "One may also compare the BNBP topic model with the HDP-LDA [6, 23], whose direct assignment sampler in our notation can be expressed as",
      "startOffset" : 59,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "To derive a collapsed sampler for the HDP-LDA that marginalizes out G̃ (but still not α), one has to use the Chinese restaurant franchise [6], which has cumbersome book-keeping as each word is indirectly linked to its topic via a latent table index.",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "The evaluation method is similar to those used in [24, 25, 26, 10].",
      "startOffset" : 50,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "The evaluation method is similar to those used in [24, 25, 26, 10].",
      "startOffset" : 50,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "The evaluation method is similar to those used in [24, 25, 26, 10].",
      "startOffset" : 50,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "The evaluation method is similar to those used in [24, 25, 26, 10].",
      "startOffset" : 50,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "Similar to [26, 10], we set the topic Dirichlet smoothing parameter as η = 0.",
      "startOffset" : 11,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "Similar to [26, 10], we set the topic Dirichlet smoothing parameter as η = 0.",
      "startOffset" : 11,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "Note that when the inferred number of topics KJ is large, the sparse computation technique for LDA [27, 28] may also be used to considerably speed up the sampling algorithm of the BNBP topic model.",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 27,
      "context" : "Note that when the inferred number of topics KJ is large, the sparse computation technique for LDA [27, 28] may also be used to considerably speed up the sampling algorithm of the BNBP topic model.",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "in [7], where the HDP and BNBP are inferred under finite approximations with truncated block Gibbs sampling.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "To further understand why the BNBP topic model and HDP-LDA have distinct characteristics, one may view them from a count-modeling perspective [7] and examine how they differently control the relationship between the variances and means of the latent topic usage count vectors {(n1k, .",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 6,
      "context" : "We also find that the BNBP collapsed Gibbs sampler clearly outperforms the blocked Gibbs sampler of [7] in terms of convergence speed, computational complexity and memory requirement.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "But a blocked Gibbs sampler based on finite truncation [7] or adaptive truncation [11] does have a clear advantage that it is easy to parallelize.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "But a blocked Gibbs sampler based on finite truncation [7] or adaptive truncation [11] does have a clear advantage that it is easy to parallelize.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 23,
      "context" : "The heuristics used to parallelize an HDP collapsed sampler [24] may also be modified to parallelize the proposed BNBP collapsed sampler.",
      "startOffset" : 60,
      "endOffset" : 64
    } ],
    "year" : 2014,
    "abstractText" : "The beta-negative binomial process (BNBP), an integer-valued stochastic process, is employed to partition a count vector into a latent random count matrix. As the marginal probability distribution of the BNBP that governs the exchangeable random partitions of grouped data has not yet been developed, current inference for the BNBP has to truncate the number of atoms of the beta process. This paper introduces an exchangeable partition probability function to explicitly describe how the BNBP clusters the data points of each group into a random number of exchangeable partitions, which are shared across all the groups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a novel nonparametric Bayesian topic model that is distinct from existing ones, with simple implementation, fast convergence, good mixing, and state-of-the-art predictive performance.",
    "creator" : null
  }
}