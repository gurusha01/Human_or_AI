{
  "name" : "b7892fb3c2f009c65f686f6355c895b5.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian Nonlinear Support Vector Machines and Discriminative Factor Modeling",
    "authors" : [ "Ricardo Henao", "Xin Yuan" ],
    "emails" : [ "r.henao@duke.edu", "xin.yuan@duke.edu", "lcarin@duke.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A new Bayesian formulation is developed for nonlinear support vector machines (SVMs), based on a Gaussian process and with the SVM hinge loss expressed as a scaled mixture of normals. We then integrate the Bayesian SVM into a factor model, in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier. Inference is performed with expectation conditional maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive set of experiments demonstrate the utility of using a nonlinear Bayesian SVM within discriminative feature learning and factor modeling, from the standpoints of accuracy and interpretability."
    }, {
      "heading" : "1 Introduction",
      "text" : "There has been significant interest recently in developing discriminative feature-learning models, in which the labels are utilized within a max-margin classifier. For example, such models have been employed in the context of topic modeling [1], where features are the proportion of topics associated with a given document. Such topic models may be viewed as a stochastic matrix factorization of a matrix of counts. The max-margin idea has also been extended to factorization of more general matrices, in the context of collaborative prediction [2, 3]. These studies have demonstrated that the use of the max-margin idea, which is closely related to support vector machines (SVMs) [4], often yields better results than designing discriminative feature-learning models via a probit or logit link. This is particularly true for high-dimensional data (e.g., a corpus characterized by a large dictionary of words), as in that case the features extracted from the high-dimensional data may significantly outweigh the importance of the small number of labels in the likelihood. Margin-based classifiers appear to be attractive in mitigating this challenge [1].\nJoint matrix factorization, feature learning and classifier design are well aligned with hierarchical models. The Bayesian formalism is well suited to such models, and much of the aforementioned research has been constituted in a Bayesian setting. An important aspect of this prior work utilizes the recent recognition that the SVM loss function may be expressed as a location-scale mixture of normals [5]. This is attractive for joint feature learning and classifier design, which is leveraged in this paper. However, the Bayesian SVM setup developed in [5] assumed a linear classifier decision function, which is limiting for sophisticated data, for which a nonlinear classifier is more effective.\nThe first contribution of this paper concerns the extension of the work in [5] for consideration of a kernel-based, nonlinear SVM, and to place this within a Bayesian scaled-mixture-of-normals construction, via a Gaussian process (GP) prior. The second contribution is a generalized formulation of this mixture model, for both the linear and nonlinear SVM, which is important within the context of Markov Chain Monte Carlo (MCMC) inference, yielding improved mixing. This new construction generalizes the form of the SVM loss function.\nThe manner we employ a GP in this paper is distinct from previous work [6, 7, 8], in that we explicitly impose a max-margin-based SVM cost function. In the previous GP-based classifier design, all data contributed to the learned classification function, while here a relatively small set of support vectors play a dominant role. This identification of support vectors is of interest when the number of training samples is large (simplifying subsequent prediction). The key reason to invoke a Bayesian form of the SVM [5], instead of applying the widely studied optimization-based SVM [4], is that the former may be readily integrated into sophisticated hierarchical models. As an example of that, we here consider discriminative factor modeling, in which the factor scores are employed within a nonlinear SVM. We demonstrate the advantage of this in our experiments, with nonlinear discriminative factor modeling for high-dimensional gene-expression data.\nWe present MCMC and expectation conditional maximization inference for the model. Conditional conjugacy of the hierarchical model yields simple and efficient computations. Hence, while the nonlinear SVM is significantly more flexible than its linear counterpart, computations are only modestly more complicated. Details on the computational approaches, insights on the characteristics of the model, and demonstration on real data constitute a third contribution of this paper."
    }, {
      "heading" : "2 Mixture Representation for SVMs",
      "text" : "Previous model for linear SVM Assume N observations {xn, yn}Nn=1, where xn ∈ Rd is a feature vector and yn ∈ {−1, 1} is its label. The support vector machine (SVM) seeks to find a classification function f(x) by solving a regularized learning problem\nargminf(x)\n{\nγ ∑N n=1 max(1− ynf(xn), 0) +R(f(x))\n}\n, (1)\nwhere max(1 − ynf(xn), 0) is the hinge loss, R(f(x)) is a regularization term that controls the complexity of f(x), and γ is a tuning parameter controlling the tradeoff between error penalization and the complexity of the classification function. The decision boundary is defined as {x : f(x) = 0} and sign(f(x)) is the decision rule, classifying x as either −1 or 1 [4]. Recently, [5] showed that for the linear classifier f(x) = β⊤x, minimizing (1) is equivalent to estimating the mode of the pseudo-posterior of β\np(β|X,y, γ) ∝ ∏Nn=1 L(yn|xn,β, γ)p(β|·) , (2) where y = [y1 . . . yN ]\n⊤, X = [x1 . . . xN ], L(yn|xn,β, γ) is the pseudo-likelihood function, and p(β|·) is the prior distribution for the vector of coefficients β. Choosing β to maximize the log of (2) corresponds to (1), where the prior is associated with R(f(x)). In [5] it was shown that L(yn|xn,β, γ) admits a location-scale mixture of normals representation by introducing latent variables λn, such that\nL(yn|xn,β, γ) = e−2γmax(1−ynβ ⊤ xn,0) =\n∫ ∞\n0\n√ γ√\n2πλn exp\n( − (1 + λn − ynβ ⊤xn) 2\n2γ−1λn\n)\ndλn . (3)\nExpression (2) is termed a pseudo-posterior because its likelihood term is unnormalized with respect to yn. Note that an improper flat prior is imposed on λn.\nThe original formulation of [5] has the tuning parameter γ as part of the prior distribution of β, while here in (3) it is included instead in the likelihood. This is done because (i) it puts λn and the regularization term γ together, and (ii) it allows more freedom in the choice of the prior for β. Additionally, it has an interesting interpretation, in that the SVM loss function behaves like a globallocal shrinkage distribution [9]. Specifically, γ−1 corresponds to a “global” scaling of the variance, and λn represents the “local” scaling for component n. The {λn} define the relative variances for each of the N data, and γ−1 provides a global scaling.\nOne of the benefits of a Bayesian formulation for SVMs is that we can flexibly specify the behavior of β while being able to adaptively regularize it by specifying a prior p(γ) as well. For instance, [5] gave three examples of prior distributions for β: Gaussian, Laplace, and spike-slab.\nWe can extend the results of [5] to a slightly more general loss function, by imposing a proper prior for the latent variables λn. In particular, by specifying λn ∼ Exp(γ0) and letting un = 1−ynβ⊤xn,\nL(yn|xn,β, γ) = ∫ ∞\n0\nγ0 √ γ√\n2πλ e−\nγ 2\n(un+λn) 2\nλn e−γ0λndλn = γ0\nc e−γ(c|un|+un) , (4)\nwhere c = √\n1 + 2γ0γ−1 > 1. The proof relies (see Supplementary Material) on the identity, ∫∞\n0 a(2πλ)−1/2 exp{− 12 (a2λ + b2λ−1)}dλ = e−|ab| [10]. From (4) we see that as γ0 → 0 we recover (3) by noting that 2max(un, 0) = |un| + un. In general we may use the prior λn ∼ Ga(aλ, γ0), with aλ = 1 for the exponential distribution. In the next section we discuss other choices for aλ. This means that the proposed likelihood is no longer equivalent to the hinge loss but to a more general loss, termed below a skewed Laplace distribution.\nSkewed Laplace distribution We can write the likelihood function in (4) in terms of un as\nL(un|γ, γ0) = ∫ ∞\n0\nN (un| − λn, γ−1λn)Exp(λn|γ0)dλn = γ0\nc\n{\ne−γ(c+1)un , if un ≥ 0 e−γ(c−1)|un| , if un < 0 , (5)\nwhich corresponds to a Laplace distribution, with negative skewness, denoted as sLa(un|γ, γ0). Unlike the density derived from the hinge loss (γ0 → 0), this density is properly normalized, thus it corresponds to a valid probability density function. For the special case γ0 = 0, the integral diverges, hence the normalization constant does not exist, which stems from exp(−2γmax(un, 0)) being constant for −∞ < un < 0. From (5) we see that sLa(un|γ, γ0) can be represented either as mixture of normals or mixture of exponentials. Other properties of the distribution, such as its moments, can be obtained using the results for general asymmetric Laplace distributions in [11]. Examining (5) we can gain some intuition about the behavior of the likelihood function for the classification problem: (i) When ynβ ⊤xn = 1, λn = 0 and xn lies on the margin boundary. (ii) When ynβ ⊤xn > 1, xn is correctly classified, outside the margin and |1 − ynβ⊤xn| is exponential with rate γ(c − 1). (iii) xn is correctly classified but lies inside the margin when 0 < ynβ\n⊤xn < 1, and xn is misclassified when ynβ\n⊤xn < 0. In both cases, 1 − ynβ⊤xn is exponential with rate γ(c + 1). (iv) Finally, if ynβ ⊤xn = 0, xn lies on the decision boundary.\nSince c+1 > c− 1 for every c > 1, the distribution for case (ii) decays slower than the distribution for case (iii). Alternatively, in terms of the loss function, observations satisfying (iii) get more penalized than those satisfying (ii). In the limiting case, γ0 → 0 we have c → 1, and case (ii) is not penalized at all, recovering the behavior of the hinge loss. In the SVM literature, an observation xn is called a support vector if it satisfies cases (i) or (iii). In the latter case, λn is the distance from ynβ\n⊤xn to the margin boundary [4]. The key thing that the Exp(λ0) prior imposes on λn, relative to the flat prior on λn ∈ [0,∞), is that it constrains that λn not be too large (discouraging ynβ\n⊤xn ≫ 1 for correct classifications, which is even more relevant for nonlinear SVMs); we discuss this further below.\nExtension to nonlinear SVM We now assume that the decision function f(x) is drawn from a zero-mean Gaussian process GP(0, k(x, ·,θ)), with kernel parameters θ. Evaluated at the N points at which we have data, f ∼ N (0,K), where K is a N × N covariance matrix with entries kij = k(xi,xj ,θ) for i, j ∈ {1, . . . , N} [7]; f = [f1 . . . fN ]⊤ ∈ RN corresponds to the continuous f(x) evaluated at {xn}Nn=1. Together with (5), for un = 1−ynfn, where fn = f(xn), the full prior specification for the nonlinear SVM is f ∼ N (0,K) , λn ∼ Exp(γ0) , γ ∼ Ga(a0, b0) . (6) It is straightforward to prove the equality in (5) holds for fn in place of β ⊤xn, as in (6).\nFor nonlinear SVMs as above, being able to set γ0 > 0 is particularly beneficial. It prevents fn from being arbitrarily large (hence preventing 1 − ynfn ≪ 0). This implies that isolated observations far away from linear decision boundary (even when correctly classified when learning) tend to be support vectors in a nonlinear SVM, yielding more conservative learned nonlinear decision boundaries. Figure 1 shows examples of logN (1 − ynfn;−λn, γ−1λn) Exp(λn; γ0) for γ = 100 and γ0 = {0.01, 100}. The vertical lines denote the margin boundary (ynfn = 1) and the decision boundary (ynfn = 0). We see that when γ0 is small, the density has a very pronounced negative skewness (like in the hinge loss of the original SVM) whereas when γ0 is large, the density tends to be more of a symmetric shape."
    }, {
      "heading" : "3 Inference",
      "text" : "We wish to compute the posterior p(f ,λ, γ|y,X), where λ = [λ1 . . . λN ]⊤. We describe and have implemented three inference procedures: Markov chain Monte Carlo (MCMC), a point estimate via expectation-conditional maximization (ECM) and a GP approximation for fast inference.\nMCMC Inference is implemented by repeatedly sampling from the conditional posterior of parameters in (6). Conditional conjugacy allows us to express the following distributions in closed form:\nf |y,λ, γ ∼ N (m,S) , m = γSYΛ−1(1 + λ) , S = γ−1K(K+ γ−1Λ)−1Λ ,\nλ −1\nn |fn, yn, γ ∼ IG\n(\n√\n1 + 2γ0γ−1\n|1− ynfn| , γ + 2γ0\n)\n, γ|y, f ,λ ∼ Ga\n(\na0 + 1\n2 N, b0 +\n1 2 ǫ ⊤ Λ −1 ǫ\n)\n, (7)\nwhere Λ = diag(λ), Y = diag(y), ǫ = 1 + λ − Yf , and IG(µ, γ) is the inverse Gaussian distribution with parameters µ and γ [10].\nIn MCMC γ0 plays a crucial role, because it controls the prior variance of the latent variables λn, thus greatly improving mixing, particularly that of γ. We also verified empirically that for small values of γ0, γ is consistently underestimated. In practice we fix γ0 = 0.1, however, a conjugate prior (gamma) exists, and sampling from its conditional posterior is straightforward if desired.\nThe parameters of the covariance function θ in the GP require Metropolis-Hastings type algorithms, as in most cases no closed form for their conditional posterior is available. However, the problem is relatively well studied. We have found that slice sampling methods [12], in particular the surrogate data sampler of [13], work well in practice, and are employed here.\nFor the case of SVMs, MCMC is naturally important as a way of quantifying the uncertainty of the parameters of the model. Further, it allows us to use the hierarchy in (6) as a building block in more sophisticated models, or to bring more flexibility to f through specialized prior specifications. As an example of this, Section 5 describes a specification for a nonlinear discriminative factor model.\nECM The expectation-conditional maximization algorithm is a generalization of the expectationmaximization (EM) algorithm. It can be used when there are multiple parameters that need to be estimated [14]. From (6) we identify f and γ as the parameters to be estimated, and λn as the latent variables. The Q function in EM-style algorithms is the complete data log-posterior, where expectations are taken w.r.t. the posterior distribution evaluated at the current value of the parameter of interest. From (7) we see that λn appears in the conditional posterior p(f |y,K,λ, γ) as first order terms, thus we can write\n〈λ−1n 〉 = E[λ−1n |yn, f (i)n , γ(i)] = √ 1 + 2γ0(γ(i))−1|u(i)n |−1 , (8)\nwhere f (i) n and γ (i) are the estimates of fn and γ at the i-th iteration, and u (i) n = 1 − ynf (i)n . From (7) and (8) we can obtain the EM updates: f (i+1) = K(K+ (γ(i))−1〈Λ〉)−1Y(1 + 〈λ〉) and\nγ(i+1) = ( a0 − 1 + 12N )\n(\nb0 + 1 2 ∑N n=1〈λ−1n 〉(u (i+1) n )2 + 2u (i+1) n + 〈λn〉\n)−1\n.\nIn the ECM setting, learning the parameters of the covariance function is not as straightforward as in MCMC. However, we can borrow from the GP literature [7] and use the fact that we can marginalize f while conditioning on λ and γ:\nZ(y,X,λ, γ,θ) = N (Y(1 + λ),K+ γ−1Λ) . (9) Note that K is a function of X and θ. Estimation of θ is done by maximizing logZ(y,X,λ, γ,θ). For this we need only compute the partial derivatives of (9) w.r.t. θ, and then use a gradient-based\noptimizer. This is commonly known as Type II maximum likelihood (ML-II) [7]. In practice we alternate between EM updates for {f , γ} and θ updates for a pre-specified number of iterations (typically the model converges after 20 iterations).\nSpeeding up inference Perhaps one of the most well known shortcomings of GP is that its cubic complexity is prohibitive for large scale problems. However there is an extensive literature about approximations for fast GP models [15]. Here we use the Fully Independent Training Conditional (FITC) approximation [16], as it offers an attractive balance between complexity and performance [15]. The basic idea behind FITC is to assume that f is generated i.i.d. from pseudo-inputs {vm}Mm=1 via fm ∈ RM such that fm ∼ N (0,Kmm), where Kmm is aM×M covariance matrix. Specifically, from (5) we have\np(u|fm) = ∏N n=1 p(un|fm) = N (KnmK−1mmfm, diag(K−Qnn) + γ−1Λ) ,\nwhere u = 1 − Yf , Kmn is the cross-covariance matrix between {vm}Mm=1 and {xn}Nn=1, and Qnn = KnmK −1 mmKmn. If we marginalize out fm thus Z(y,X,λ, γ,θ) = N (Y(1 + λ),Qnn + diag(K−Qnn) + γ−1Λ) . (10) Note that if we drop the diag(·) term in (10) due to the i.i.d. assumption for f , we recover the full GP marginal from (9). Similar to the ML-II approach previously described, for a fixed M we can maximize logZ(y,X,λ, γ,θ) w.r.t. θ and {vm}Mm=1 using a gradient-based optimizer but with the added benefit of having decreased the computational cost from O(N3) to O(NM2) [16].\nPredictions Making predictions under the model in (6), with conditional posterior distributions in (7), can be achieved using standard results of the multivariate normal distribution. The predictive distribution of f⋆ for a new observation x⋆ given the dataset {X,y} can be written as f⋆|x⋆,X,y ∼ N (k⋆ΣY(1 + λ), k⋆ − k⊤⋆ Σk⋆) , (11) where Σ = (K + γ−1Λ)−1, k⋆ = k(x⋆,x⋆,θ) and k⋆ = [k(x⋆,x1,θ) . . . k(x⋆,xN ,θ)]\n⊤. Furthermore, we can directly use the probit link Φ(f⋆) to compute\np(y⋆ = 1|x⋆,X,y) = ∫ Φ(f⋆)p(f⋆|x⋆,X,y)df⋆ = Φ ( k⋆ΣY(1 + λ)(1 + k⋆ − k⊤⋆ Σk⋆)−1 ) ,\nwhich follows from [7]. Computing the class membership probability is not possible in standard SVMs, because in such optimization-based methods one does not obtain the variance of the predictive distribution; this variance is an attractive component of the Bayesian construction.\nThe mean of the predictive distribution (11) is tightly related to the predictor in standard SVMs, in the sense that both are manifestations of the representer theorem. In particular\nE[f⋆|x⋆,X,y] = ∑N n=1 αnk(x⋆,xn,θ) , (12)\nwhere α = (K + γ−1Λ)−1Y(1 + λ). From the expectations of λn and f conditioned on γ and γ0 it is possible to show that α is a vector with elements γ(1 − c) ≤ αn ≤ γ(1 + c), where c = √ 1 + 2γ0γ−1. We differentiate three types of elements in α as follows\nα =\n\n\n\nynγ(1 + c), if ynfn < 1 α0n , if ynfn = 1 (λn = 0)\nynγ(1− c) , if ynfn > 1 , (13)\nwith α0 = K −1 0,0 (y0 − γ(1 + c)K0,aya − γ(1− c)K0,byb), where α0n is an element of α0, and 0, a and b are subsets of {1, . . . , N} for which λn = 0, ynfn < 1 and ynfn > 1, respectively. This implies α and so the prediction rule in (12) depend on data for which λn > 0 only through γ and γ0. Note also that we do not need the values of λ but whether or not they are different than zero. When γ0 → 0 then c → 1 and α becomes a sparse vector bounded above by 2γ. This result for standard SVMs can be found independently from the Karush-Kuhn-Tucker conditions for its objective function [4].\nFor ECM and variational Bayes EM inference (the latter discussed below in Section 5), we set γ0 = 0 and therefore α is sparse, with αn = 0 when ynfn > 1, as in traditional SVMs. This property of the proposed use of GPs within the Bayesian SVM formulation is a significant advantage relative to traditional classifier design based directly on GPs, for which we do not have such sparsity in general. For MCMC inference, we find the sampler mixes better when γ0 6= 0. Details on the derivations of (13) and the concavity of the problem may be found in Supplementary Material."
    }, {
      "heading" : "4 Related Work",
      "text" : "A key contribution of this paper concerns extension of the linear Bayesian SVM developed in [5] to a nonlinear Bayesian SVM. This has been implemented by replacing the linear f(x) = β⊤x considered in [5] with an f(x) drawn from a GP. The most relevant previous work is that for which a classifier is directly implemented via a GP, without an explicit connection to the margin associated with the SVM [7]. Specifically, GP-based classifiers have been developed by [17]. In [7] the f is drawn from a GP, as in (6), but f is used directly with a probit or logit link function, to estimate class membership probability. Previous GP-based classifiers did not use f within a margin-based classifier as in (6), implemented here via p(un) = N (−λn, γ−1λn), where un = 1−ynfn. It has been shown empirically that nonlinear SVMs and GP classifiers often perform similarly [8]. However, for the latter, inference can be challenging due to the non-conjugacy of multivariate normal distribution to the link function. Common inference strategies employ iterative approximate inference schemes, such as the Laplace approximation [17] or expectation propagation (EP) [18]. The model we propose here is locally fully conjugate (except for the GP kernel parameters) and inference can be easily implemented using EM style algorithms, or via MCMC. Besides, the prediction rule of the GP classifier, which has a form almost identical to (12), is generally not sparse and therefore lacks the interpretation that may be provided by the relatively few support vectors."
    }, {
      "heading" : "5 Discriminative Factor Models",
      "text" : "Combinations of factor models and linear classifiers have been widely used in many applications, such as gene expression, proteomics and image analysis, as a way to perform classification and feature selection simultaneously [19, 20]. One of the most common modeling approaches can be written as xn = Awn + ǫn, ǫn ∼ N (0, ψ−1I) , L(yn|β,wn, ·) , where A is a d×K matrix of factor loadings, wn ∈ RK is a vector of factor scores, ǫn is observation noise (and/or model residual), β is a vector ofK linear classifier coefficients and L(·) is for instance but not limited to the linear SVM likelihood in (5) (a logit or probit link may also be used). One of many possible prior specification for the above model is ak ∼ N (0,Φk) , wn ∼ N (0, I) , ψ ∼ Ga(aψ, bψ) , β ∼ N (0,G) , where ak is a column of A, Φk = diag(φ1k, . . . , φdk), φik ∼ Exp(ν), G = diag(g1, . . . , gK) and each element of A is distributed aik ∼ Laplace(ν) after marginalizing out {φik} [10]. Shrinkage in A is typically a requirement when N ≪ d or when its columns, ak, need to be interpreted. For simplicity, we can set G = I, however a shrinkage prior for the elements gk of G might be useful in some applications, as a mechanism for factor score selection. Although the described model usually works well in practice, it assumes that there is a linear mapping from Rd to RK , such that K ≪ d, in which the classes {−1, 1} are linearly separable. We can relax this assumption by imposing the hierarchical model in (6) in place of β. This implies that matrix K from (6) has now entries kij = k(wi,wj ,θ). Inference using MCMC is straightforward except for the conditional posterior of the factor scores. This model is related to latent-variable GP models (GP-LVM) [21], in that we infer the latent {wi} that reside within a GP kernel. However, here {wi} are also factor scores in a factor model, and the GP is used within the context of a Bayesian SVM classifier; neither of latter two have been considered previously.\nFor the nonlinear Bayesian SVM classifier we no longer have a closed form for the conditional of wn, due to the covariance function of the GP prior. Thus, we require a Metropolis-Hastings type algorithm. Here we use elliptical slice sampling [22]. Specifically, we sample wn from p(wn|A,W\\n, ψ,y,λ, γ,θ) ∝ p(wn|xn,A, ψ)Z(y,wn,W\\n,λ, γ,θ) , (14) where p(wn|xn,A, ψ) ∼ N (SNψAxn,SN), W = [w1 . . . wN ], W\\n is matrix W without column n, S−1N = ψA\n⊤A + I, and we have marginalized out f as in (9) with W in place of X. The elliptical slice sampler proposes samples from p(wn|xn,A, ψ) while biasing them towards more likely configurations of λ. Provided that λ ultimately controls the predictive distribution of the classifier in (11), samples of wn will at the same time attempt to fit the data and to improve classification performance. From (14), note that we sample one column of W at a time, while keeping the others fixed. Details of the elliptical slice sampler are found in [22]. In applications in which sampling from (14) is time prohibitive, we can use instead a variational Bayes EM (VB-EM) approach. In the E-step, we approximate the posterior of A, {Φk}, ψ, f , λ and γ by a factorized distribution q(A) ∏\nk q(Φk)q(ψ)q(f)q(λ)q(γ) and in the M-step we optimize W and θ, using LBFGS [23]. Details of the implementation can be found in the Supplementary Material."
    }, {
      "heading" : "6 Experiments",
      "text" : "In all experiments we set the covariance function to (i) either the square exponential (SE), which has the form k(xi,xj , θ) = exp ( −‖xi − xj‖2 /\nθ2), where θ2 is known as the characteristic length scale; or (ii) the automatic relevance determination (ARD) SE in which each dimension of x has its own length scale [7]. All code used in the experiments was written in Matlab and executed on a 2.8GHz workstation with 4Gb RAM.\nBenchmark data We first compare the performance of the proposed Bayesian hierarchy for nonlinear SVM (BSVM) against EP-based GP classification (GPC) and an optimization-based SVM, on six well known benchmark datasets. In particular, we use the same data and settings as [8], specifically 10-fold cross-validation and SE covariance function. The parameters of the SVM {γ, θ} are obtained by grid search using\nan internal 5-fold cross-validation. GPC uses ML-II and a modified SE function k(xi,xj ,θ) = θ21 exp ( −‖xi − xj‖2 /\nθ22), where θ1 acts as regularization trade-off similar to γ in our formulation [7]. For our model we set 200 as the maximum number of iterations of the ECM algorithm and run ML-II every 20 iterations. Table 1 shows mean errors for the methods under consideration. We see that all three perform similarly as one might expect thus error bars are not showed, however BSVM slightly outperforms the others in 4 out of 6 datasets. From the three methods, the SVM is clearly faster than the others. GP classification and our model essentially scale cubically with N , however, ours is relatively faster mainly due to overhead computations needed by the EP algorithm. More specifically, running times for the larger dataset (USPS 3 vs 5) were approximately 1000, 1200 and 5000 seconds for SVM, BSVM and GPC, respectively.\nTable 2: FITC results (mean % error) for USPS data.\n3 vs. 5 (N = 767) 4 vs. non-4 (N = 7291) FITC-GPC FITC-BSVM FITC-GPC FITC-BSVM\nError 3.69± 0.26 3.49± 0.29 2.59± 0.17 2.44± 0.17 Time 102 46 604 116\nIn order to test the approximation introduced in Section 3 (to accelerate GP inference) we use the traditional splitting of USPS, 7291 for model fitting and the remaining 2007 for testing, on two different tasks: 3 vs. 5 and 4 vs. non-4. Table 2\nshows mean error rates and standard deviations for FITC versions of BSVM and GPC, forM = 100 pseudo-inputs and 10 repetitions. We see that FITC-BSVM slightly outperforms FITC-GPC in both tasks while being relatively faster. As baselines, full BSVM and GPC on the 3 vs. 5 task perform roughly similar at 2.46% error. We also verified (results not shown) that increasing M consistently decreases error rates for both FITC-BSVM and FITC-GPC.\nUSPS data We applied the model proposed in Section 5 to the well known 3 vs. 5 subset of the USPS handwritten digits dataset, consisting of 1540 gray scale 16 × 16 images, rescaled within [−1, 1]. We use the resampled version, this is, 767 for model fitting and the remaining 773 for testing. As baselines, we also perform inference as a two step procedure, first fitting the factor model (FM), followed by a linear (L) or a nonlinear (N) SVM classifier. We also consider learning jointly the factor model but with a linear SVM (LDFM), and a two step procedure consisting of LDFM followed by a nonlinear SVM. Our proposed nonlinear discriminative factor model is denoted NDFM. VB-EM versions of LDFM and NDFM are denoted as VLDFM and VNDFM, respectively. MCMC details for the linear SVM part can be found in [5]. For inference, we set K = 10, a SE covariance function and run the sampler for 1200 iterations, from which we discard the first 600 and keep every 10-th for posterior summaries. We observed in general good mixing regardless of random initialization, and results remained very similar for different Markov chains.\nTable 3 shows classification results for the eight classifiers considered; we see that the nonlinear classifiers perform substantially better than the linear counterparts. In addition, the proposed nonlinear joint model (NDFM) is the best of all five. The nonlinear classifier is powerful enough to perform well in both two step procedures. We found that VNDFM is not performing as good as NDFM because the data likelihood is dominating over the labels likelihood in the updates for the factor scores, which is not surprising considering the marked size differences between the two. On the positive side, runtime for VNDFM is approximately two orders of magnitude smaller than that of NDFM. We also tried a joint nonlinear model with a probit link as in GP classification and we\nfound its classification performance (a mean error rate of 3.10%) being slightly worse than that for NDFM. In addition, we found that using ARD SE covariance functions to automatically select for features of A and larger values of K did not substantial changed the results.\nGene expression data The dataset originally introduced in [24] consists of gene expression measurements from primary breast tumor samples for a study focused towards finding expression patterns potentially related to mutations of the p53 gene. The original data were normalized using RMA and filtered to exclude genes showing trivial variation. The final dataset consists of 251 samples and 2995 normalized gene expression values. The labeling variable indicates whether or not a sample exhibits the mutation. We use the same baseline and inference settings from our previous experiment, but validation is done by 10-fold cross-validation. In preliminary results we found that factor score selection improves results, hence for the linear classifier (L) we used an exponential prior for the variances of β, gk ∼ Exp(ρ), and for the nonlinear case (N) we set an ARD SE covariance function for K. Table 3 summarizes the results, the nonlinear variants outperform their linear counterparts and our joint model perform slightly better than the others. Additionally, the joint nonlinear model with GP and probit link yielded an error rate of 19.52%.\nAs a way of quantifying whether the features (factor loadings) produced by FM, LDFM and NDFM are meaningful from a biological point of view, we performed Gene Ontology (GO) searches for the gene lists encoded by each column of A. In order to quantify the strength of the association between GO annotations and our gene lists we obtained Bonferroni corrected p-values [25]. We thresholded the elements of matrix A such that |aik| > 0.1. Using the 10 lists from each model we found that FM, LDFM and NDFM produced respectively 5, 5 and 8 factors significantly associated to GO terms relevant to breast cancer. The GO terms are: fatty acid metabolism, induction of programmed cell death (apoptosis), anti-apoptosis, regulation of cell cycle, positive regulation of cell cycle, cell cycle and Wnt signaling pathway. The strongest associations in all models are unsurprisingly apoptosis and positive regulation of cell cycle, however, only NDFM produced a significant association to anti-apoptosis which we believe is responsible for the edge in performance of NDFM in Table 3."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have introduced a fully Bayesian version of nonlinear SVMs, extending the previous restriction to linear SVMs [5]. Almost all of the existing joint feature-learning and classifier-design models assumed linear classifiers [2, 3, 26]. We have demonstrated in our experiments that there is a substantial performance improvement manifested by the nonlinear classifier. In addition, we have extended the Bayesian equivalent of the hinge loss to a more general loss function, for both linear and nonlinear classifiers. We have demonstrated that this approach enhances modeling flexibility, and yields improved MCMC mixing. The Bayesian setup allows one to directly compute class membership probabilities. We showed how to use the nonlinear SVM as a module in a larger model, and presented compelling results to highlight its potential. Point estimate inference using ECM is conceptually simpler and easier to implement than MCMC or GP classification, although MCMC is attractive for integrating the factor model and classifier (for example). We showed how FITC and VB-EM based approximations can be used in conjunction with the SVM nonlinear classifier and discriminative factor modeling, respectively, as a way to scale inference in a principled way."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR."
    } ],
    "references" : [ {
      "title" : "MedLDA: maximum margin supervised topic models for regression and classification",
      "author" : [ "J. Zhu", "A. Ahmed", "E.P. Xing" ],
      "venue" : "ICML, pages 1257–1264",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Fast max-margin matrix factorization with data augmentation",
      "author" : [ "M. Xu", "J. Zhu", "B. Zhang" ],
      "venue" : "ICML, pages 978–986",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Nonparametric max-margin matrix factorization for collaborative prediction",
      "author" : [ "M. Xu", "J. Zhu", "B. Zhang" ],
      "venue" : "NIPS 25, pages 64–72",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine Learning, 20(3):273–297",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Data augmentation for support vector machines",
      "author" : [ "N.G. Polson", "S.L. Scott" ],
      "venue" : "Bayesian Analysis, 6(1):1– 23",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Gaussian processes for classification: Mean-field algorithms",
      "author" : [ "M. Opper", "O. Winther" ],
      "venue" : "Neural Computation, 12(11):2655–2684",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : "The MIT Press",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Assessing approximate inference for binary Gaussian process classification",
      "author" : [ "M. Kuss", "C.E. Rasmussen" ],
      "venue" : "JMLR, 6:1679–1704",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Shrink globally",
      "author" : [ "N.G. Polson", "J.G. Scott" ],
      "venue" : "act locally: sparse Bayesian regularization and prediction. Bayesian Statistics, 9:501–538",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Scale mixtures of normal distributions",
      "author" : [ "D.F. Andrews", "C.L. Mallows" ],
      "venue" : "JRSSB, 36(1):99–102",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "A class of asymmetric distributions",
      "author" : [ "T.J. Kozubowski", "K. Podgorski" ],
      "venue" : "Actuarial Research Clearing House, 1:113–134",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Slice sampling",
      "author" : [ "R.M. Neal" ],
      "venue" : "AOS, 31(3):705–741",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Slice sampling covariance hyperparameters of latent Gaussian models",
      "author" : [ "I. Murray", "R.P. Adams" ],
      "venue" : "NIPS 23, pages 1723–1731",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Maximum likelihood estimation via the ECM algorithm: A general framework",
      "author" : [ "X.-L. Meng", "D.B. Rubin" ],
      "venue" : "Biometrika, 80(2):267–278",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "A unifying view of sparse approximate Gaussian process regression",
      "author" : [ "J Quiñonero-Candela", "C.E. Rasmussen" ],
      "venue" : "JMLR, 6:1939–1959",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Sparse Gaussian processes using pseudo-inputs",
      "author" : [ "E. Snelson", "Z. Ghahramani" ],
      "venue" : "NIPS 18, pages 1257– 1264",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Bayesian classification with Gaussian processes",
      "author" : [ "C.K.I. Williams", "D. Barber" ],
      "venue" : "PAMI, 20(12):1342– 1351",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A family of algorithms for approximate Bayesian inference",
      "author" : [ "Thomas P. Minka" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2001
    }, {
      "title" : "High-dimensional sparse factor modeling: Applications in gene expression genomics",
      "author" : [ "C.M. Carvalho", "J. Chang", "J.E. Lucas", "J.R. Nevins", "Q. Wang", "M. West" ],
      "venue" : "JASA, 103(484):1438–1456",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Non-parametric Bayesian dictionary learning for sparse image representations",
      "author" : [ "M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "G. Sapiro", "L. Carin" ],
      "venue" : "NIPS 22, pages 2295–2303",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Gaussian process latent variable models for visualisation of high dimensional data",
      "author" : [ "N.D. Lawrence" ],
      "venue" : "NIPS 16",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Elliptical slice sampling",
      "author" : [ "I. Murray", "R.P. Adams", "D.J.C. MacKay" ],
      "venue" : "AISTATS, pages 541–548",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the limited memory method for large scale optimization",
      "author" : [ "D.C. Liu", "J. Nocedal" ],
      "venue" : "Mathematical Programming B, pages 503–528",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "et al",
      "author" : [ "L.D. Miller", "J. Smeds", "J. George", "V.B. Vega", "L. Vergara", "A. Ploner", "Y. Pawitan", "P. Hall", "S. Klaar", "E.T. Liu" ],
      "venue" : "An expression signature for p53 status in human breast cancer predicts mutation status, transcriptional effects, and patient survival. PNAS, 102(38):13550–13555",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "GATHER: a systems approach to interpreting genomic signatures",
      "author" : [ "J.T. Chang", "J.R. Nevins" ],
      "venue" : "Bioinformatics, 22(23):2926–2933",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Supervised dictionary learning",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman" ],
      "venue" : "NIPS 21, pages 1033–1040",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "For example, such models have been employed in the context of topic modeling [1], where features are the proportion of topics associated with a given document.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 1,
      "context" : "The max-margin idea has also been extended to factorization of more general matrices, in the context of collaborative prediction [2, 3].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "The max-margin idea has also been extended to factorization of more general matrices, in the context of collaborative prediction [2, 3].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "These studies have demonstrated that the use of the max-margin idea, which is closely related to support vector machines (SVMs) [4], often yields better results than designing discriminative feature-learning models via a probit or logit link.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Margin-based classifiers appear to be attractive in mitigating this challenge [1].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "An important aspect of this prior work utilizes the recent recognition that the SVM loss function may be expressed as a location-scale mixture of normals [5].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 4,
      "context" : "However, the Bayesian SVM setup developed in [5] assumed a linear classifier decision function, which is limiting for sophisticated data, for which a nonlinear classifier is more effective.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 4,
      "context" : "The first contribution of this paper concerns the extension of the work in [5] for consideration of a kernel-based, nonlinear SVM, and to place this within a Bayesian scaled-mixture-of-normals construction, via a Gaussian process (GP) prior.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "The manner we employ a GP in this paper is distinct from previous work [6, 7, 8], in that we explicitly impose a max-margin-based SVM cost function.",
      "startOffset" : 71,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "The manner we employ a GP in this paper is distinct from previous work [6, 7, 8], in that we explicitly impose a max-margin-based SVM cost function.",
      "startOffset" : 71,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "The manner we employ a GP in this paper is distinct from previous work [6, 7, 8], in that we explicitly impose a max-margin-based SVM cost function.",
      "startOffset" : 71,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "The key reason to invoke a Bayesian form of the SVM [5], instead of applying the widely studied optimization-based SVM [4], is that the former may be readily integrated into sophisticated hierarchical models.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "The key reason to invoke a Bayesian form of the SVM [5], instead of applying the widely studied optimization-based SVM [4], is that the former may be readily integrated into sophisticated hierarchical models.",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "The decision boundary is defined as {x : f(x) = 0} and sign(f(x)) is the decision rule, classifying x as either −1 or 1 [4].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Recently, [5] showed that for the linear classifier f(x) = βx, minimizing (1) is equivalent to estimating the mode of the pseudo-posterior of β p(β|X,y, γ) ∝ ∏Nn=1 L(yn|xn,β, γ)p(β|·) , (2)",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "In [5] it was shown that L(yn|xn,β, γ) admits a location-scale mixture of normals representation by introducing latent variables λn, such that L(yn|xn,β, γ) = en ⊤ xn,0) = ∫ ∞",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "The original formulation of [5] has the tuning parameter γ as part of the prior distribution of β, while here in (3) it is included instead in the likelihood.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "Additionally, it has an interesting interpretation, in that the SVM loss function behaves like a globallocal shrinkage distribution [9].",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "For instance, [5] gave three examples of prior distributions for β: Gaussian, Laplace, and spike-slab.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 4,
      "context" : "We can extend the results of [5] to a slightly more general loss function, by imposing a proper prior for the latent variables λn.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "The proof relies (see Supplementary Material) on the identity, ∫∞ 0 a(2πλ) exp{− 12 (a(2)λ + b2λ−1)}dλ = e [10].",
      "startOffset" : 107,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "Other properties of the distribution, such as its moments, can be obtained using the results for general asymmetric Laplace distributions in [11].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 3,
      "context" : "In the latter case, λn is the distance from ynβ xn to the margin boundary [4].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "where Λ = diag(λ), Y = diag(y), ǫ = 1 + λ − Yf , and IG(μ, γ) is the inverse Gaussian distribution with parameters μ and γ [10].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 11,
      "context" : "We have found that slice sampling methods [12], in particular the surrogate data sampler of [13], work well in practice, and are employed here.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 12,
      "context" : "We have found that slice sampling methods [12], in particular the surrogate data sampler of [13], work well in practice, and are employed here.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 13,
      "context" : "It can be used when there are multiple parameters that need to be estimated [14].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 6,
      "context" : "However, we can borrow from the GP literature [7] and use the fact that we can marginalize f while conditioning on λ and γ: Z(y,X,λ, γ,θ) = N (Y(1 + λ),K+ γΛ) .",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "This is commonly known as Type II maximum likelihood (ML-II) [7].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "However there is an extensive literature about approximations for fast GP models [15].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Here we use the Fully Independent Training Conditional (FITC) approximation [16], as it offers an attractive balance between complexity and performance [15].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "Here we use the Fully Independent Training Conditional (FITC) approximation [16], as it offers an attractive balance between complexity and performance [15].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 15,
      "context" : "θ and {vm}m=1 using a gradient-based optimizer but with the added benefit of having decreased the computational cost from O(N3) to O(NM2) [16].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "This result for standard SVMs can be found independently from the Karush-Kuhn-Tucker conditions for its objective function [4].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "4 Related Work A key contribution of this paper concerns extension of the linear Bayesian SVM developed in [5] to a nonlinear Bayesian SVM.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "This has been implemented by replacing the linear f(x) = βx considered in [5] with an f(x) drawn from a GP.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "The most relevant previous work is that for which a classifier is directly implemented via a GP, without an explicit connection to the margin associated with the SVM [7].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 16,
      "context" : "Specifically, GP-based classifiers have been developed by [17].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 6,
      "context" : "In [7] the f is drawn from a GP, as in (6), but f is used directly with a probit or logit link function, to estimate class membership probability.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 7,
      "context" : "It has been shown empirically that nonlinear SVMs and GP classifiers often perform similarly [8].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "Common inference strategies employ iterative approximate inference schemes, such as the Laplace approximation [17] or expectation propagation (EP) [18].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 17,
      "context" : "Common inference strategies employ iterative approximate inference schemes, such as the Laplace approximation [17] or expectation propagation (EP) [18].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 18,
      "context" : "5 Discriminative Factor Models Combinations of factor models and linear classifiers have been widely used in many applications, such as gene expression, proteomics and image analysis, as a way to perform classification and feature selection simultaneously [19, 20].",
      "startOffset" : 256,
      "endOffset" : 264
    }, {
      "referenceID" : 19,
      "context" : "5 Discriminative Factor Models Combinations of factor models and linear classifiers have been widely used in many applications, such as gene expression, proteomics and image analysis, as a way to perform classification and feature selection simultaneously [19, 20].",
      "startOffset" : 256,
      "endOffset" : 264
    }, {
      "referenceID" : 9,
      "context" : ", gK) and each element of A is distributed aik ∼ Laplace(ν) after marginalizing out {φik} [10].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "This model is related to latent-variable GP models (GP-LVM) [21], in that we infer the latent {wi} that reside within a GP kernel.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 21,
      "context" : "Here we use elliptical slice sampling [22].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 21,
      "context" : "Details of the elliptical slice sampler are found in [22].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "k q(Φk)q(ψ)q(f)q(λ)q(γ) and in the M-step we optimize W and θ, using LBFGS [23].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "θ(2)), where θ(2) is known as the characteristic length scale; or (ii) the automatic relevance determination (ARD) SE in which each dimension of x has its own length scale [7].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "In particular, we use the same data and settings as [8], specifically 10-fold cross-validation and SE covariance function.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "θ(2) 2), where θ1 acts as regularization trade-off similar to γ in our formulation [7].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 4,
      "context" : "MCMC details for the linear SVM part can be found in [5].",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 23,
      "context" : "Gene expression data The dataset originally introduced in [24] consists of gene expression measurements from primary breast tumor samples for a study focused towards finding expression patterns potentially related to mutations of the p53 gene.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 24,
      "context" : "In order to quantify the strength of the association between GO annotations and our gene lists we obtained Bonferroni corrected p-values [25].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "7 Conclusion We have introduced a fully Bayesian version of nonlinear SVMs, extending the previous restriction to linear SVMs [5].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "Almost all of the existing joint feature-learning and classifier-design models assumed linear classifiers [2, 3, 26].",
      "startOffset" : 106,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "Almost all of the existing joint feature-learning and classifier-design models assumed linear classifiers [2, 3, 26].",
      "startOffset" : 106,
      "endOffset" : 116
    }, {
      "referenceID" : 25,
      "context" : "Almost all of the existing joint feature-learning and classifier-design models assumed linear classifiers [2, 3, 26].",
      "startOffset" : 106,
      "endOffset" : 116
    } ],
    "year" : 2014,
    "abstractText" : "A new Bayesian formulation is developed for nonlinear support vector machines (SVMs), based on a Gaussian process and with the SVM hinge loss expressed as a scaled mixture of normals. We then integrate the Bayesian SVM into a factor model, in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier. Inference is performed with expectation conditional maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive set of experiments demonstrate the utility of using a nonlinear Bayesian SVM within discriminative feature learning and factor modeling, from the standpoints of accuracy and interpretability.",
    "creator" : null
  }
}