{
  "name" : "08419be897405321542838d77f855226.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning to Discover Efficient Mathematical Identities",
    "authors" : [ "Wojciech Zaremba", "Karol Kurach" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a grammar of math operators, we build trees that combine them in different ways, looking for compositions that are analytically equivalent to a target expression but of lower computational complexity. However, as the space of trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neuralnetwork. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation."
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine learning approaches have proven highly effective for statistical pattern recognition problems, such as those encountered in speech or vision. However, their use in symbolic settings has been limited. In this paper, we explore how learning can be applied to the discovery of mathematical identities. Specifically, we propose methods for finding computationally efficient versions of a given target expression. That is, finding a new expression which computes an identical result to the target, but has a lower complexity (in time and/or space).\nWe introduce a framework based on attribute grammars [14] that allows symbolic expressions to be expressed as a sequence of grammar rules. Brute-force enumeration of all valid rule combinations allows us to discover efficient versions of the target, including those too intricate to be discovered by human manipulation. But for complex target expressions this strategy quickly becomes intractable, due to the exponential number of combinations that must be explored. In practice, a random search within the grammar tree is used to avoid memory problems, but the chance of finding a matching solution becomes vanishingly small for complex targets.\nTo overcome this limitation, we use machine learning to produce a search strategy for the grammar trees that selectively explores branches likely (under the model) to yield a solution. The training data for the model comes from solutions discovered for simpler target expressions. We investigate several different learning approaches. The first group are n-gram models, which learn pairs, triples etc. of expressions that were part of previously discovered solutions, thus hopefully might be part of the solution for the current target. We also train a recursive neural network (RNN) that operates within the grammar trees. This model is first pretrained to learn a continuous representation for symbolic expressions. Then, using this representation we learn to predict the next grammar rule to add to the current expression to yield an efficient version of the target.\nThrough the use of learning, we are able to dramatically widen the complexity and scope of expressions that can be handled in our framework. We show examples of (i) O ( n3 )\ntarget expressions which can be computed in O ( n2 ) time (e.g. see Examples 1 & 2), and (ii) cases where naive eval-\nuation of the target would require exponential time, but can be computed in O ( n2 ) or O ( n3 )\ntime. The majority of these examples are too complex to be found manually or by exhaustive search and, as far as we are aware, are previously undiscovered. All code and evaluation data can be found at https://github.com/kkurach/math_learning.\nIn summary our contributions are:\n• A novel grammar framework for finding efficient versions of symbolic expressions. • Showing how machine learning techniques can be integrated into this framework, and\ndemonstrating how training models on simpler expressions can help which the discovery of more complex ones. • A novel application of a recursive neural-network to learn a continuous representation of mathematical structures, making the symbolic domain accessible to many other learning approaches. • The discovery of many new mathematical identities which offer a significant reduction in computational complexity for certain expressions.\nExample 1: Assume we are given matrices A ∈ Rn×m, B ∈ Rm×p. We wish to compute the target expression: sum(sum(A*B)), i.e. : ∑ n,pAB = ∑n i=1 ∑m j=1 ∑p k=1Ai,jBj,k which naively takes O(nmp) time. Our framework is able to discover an efficient version of the formula, that computes the same result in O(n(m+ p)) time: sum((sum(A, 1) * B)’, 1). Our framework builds grammar trees that explore valid compositions of expressions from the grammar, using a search strategy. In this example, the naive strategy of randomly choosing permissible rules suffices and we can find another tree which matches the target expression in reasonable time. Below, we show trees for (i) the original expression and (ii) the efficient formula which avoids the use of a matrix-matrix multiply operation, hence is efficient to compute.\n——————— Example 2: Consider the target expression: sum(sum((A*B)k)), where k = 6. For an expression of this degree, there are 9785 possible grammar trees and the naive strategy used in Example 1 breaks down. We therefore learn a search strategy, training a model on successful trees from simpler expressions, such as those for k = 2, 3, 4, 5. Our learning approaches capture the common structure within the solutions, evident below, so can find an efficient O(nm) expression for this target: k = 2: sum((((((sum(A, 1)) * B) * A) * B)’), 1)\nk = 3: sum((((((((sum(A, 1)) * B) * A) * B) * A) * B)’), 1)\nk = 4: sum((((((((((sum(A, 1)) * B) * A) * B) * A) * B) * A) * B)’), 1)\nk = 5: sum((((((((((((sum(A, 1)) * B) * A) * B) * A) * B) * A) * B) * A) * B)’), 1)\nk = 6: sum(((((((((((((sum(A, 1) * B) * A) * B) *A) * B) * A) * B)* A) * B) * A) * B)’), 1)"
    }, {
      "heading" : "1.1 Related work",
      "text" : "The problem addressed in this paper overlaps with the areas of theorem proving [5, 9, 11], program induction [18, 28] and probabilistic programming [12, 20]. These domains involve the challenging issues of undecidability, the halting problem, and a massive space of potential computation. However, we limit our domain to computation of polynomials with fixed degree k, where undecidability and the halting problem are not present, and the space of computation is manageable (i.e. it grows exponentially, but not super-exponentially). Symbolic computation engines, such as Maple [6] and Mathematica [27] are capable of simplifying expressions by collecting terms but do not explicitly seek versions of lower complexity. Furthermore, these systems are rule based and do not use learning approaches, the major focus of this paper. In general, there has been very little exploration of statistical machine learning techniques in these fields, one of the few attempts being the recent work of Bridge et al. [4] who use learning to select between different heuristics for 1st order reasoning. In contrast, our approach does not use hand-designed heuristics, instead learning them automatically from the results of simpler expressions.\nThe attribute grammar, originally developed in 1968 by Knuth [14] in context of compiler construction, has been successfully used as a tool for design and formal specification. In our work, we apply attribute grammars to a search and optimization problem. This has previously been explored in a range of domains: from well-known algorithmic problems like knapsack packing [19], through bioinformatics [26] to music [10]. However, we are not aware of any previous work related to discovering mathematical formulas using grammars, and learning in such framework. The closest work to ours can be found in [7] which involves searching over the space of algorithms and the grammar attributes also represent computational complexity.\nClassical techniques in natural language processing make extensive use of grammars, for example to parse sentences and translate between languages. In this paper, we borrow techniques from NLP and apply them to symbolic computation. In particular, we make use of an n-gram model over mathematical operations, inspired by n-gram language models. Recursive neural networks have also been recently used in NLP, for example by Luong et al. [15] and Socher et al. [22, 23], as well as generic knowledge representation Bottou [2]. In particular, Socher et al. [23], apply them to parse trees for sentiment analysis. By contrast, we apply them to trees of symbolic expressions. Our work also has similarities to Bowman [3] who shows that a recursive network can learn simple logical predicates.\nOur demonstration of continuous embeddings for symbolic expressions has parallels with the embeddings used in NLP for words and sentence structure, for example, Collobert & Weston [8], Mnih & Hinton [17], Turian et al. [25] and Mikolov et al. [16]."
    }, {
      "heading" : "2 Problem Statement",
      "text" : "Problem Definition: We are given a symbolic target expression T that combines a set of variables V to produce an output O, i.e. O = T(V). We seek an alternate expression S, such that S(V) = T(V), but has lower computational complexity, i.e. O(S) < O(T).\nIn this paper we consider the restricted setting where: (i) T is a homogeneous polynomial of degree k∗, (ii) V contains a single matrix or vector A and (iii) O is a scalar. While these assumptions may seem quite restrictive, they still permit a rich family of expressions for our algorithm to explore. For example, by combining multiple polynomial terms, an efficient Taylor series approximation can be found for expressions involving trigonometric or exponential operators. Regarding (ii), our framework can easily handle multiple variables, e.g. Figure 1, which shows expressions using two matrices, A and B. However, the rest of the paper considers targets based on a single variable. In Section 8, we discuss these restrictions further.\nNotation: We adopt Matlab-style syntax for expressions."
    }, {
      "heading" : "3 Attribute Grammar",
      "text" : "We first define an attribute grammar G, which contains a set of mathematical operations, each with an associated complexity (the attribute). Since T contains exclusively polynomials, we use the grammar rules listed in Table 1.\nUsing these rules we can develop trees that combine rules to form expressions involving V , which for the purposes of this paper is a single matrix A. Since we know T involves expressions of degree\n∗I.e. It only contains terms of degree k. E.g. ab + a2 + ac is a homogeneous polynomial of degree 2, but a2 + b is not homogeneous (b is of degree 1, but a2 is of degree 2).\nk, each tree must use A exactly k times. Furthermore, since the output is a scalar, each tree must also compute a scalar quantity. These two constraints limit the depth of each tree. For some targets T whose complexity is only O(()n3), we remove the matrix-matrix multiply rule, thus ensuring that if any solution is found its complexity is at most O(()n2) (see Section 7.2 for more details). Examples of trees are shown in Fig. 1. The search strategy for determining which rules to combine is addressed in Section 6."
    }, {
      "heading" : "4 Representation of Symbolic Expressions",
      "text" : "We need an efficient way to check if the expression produced by a given tree, or combination of trees (see Section 5), matches T. The conventional approach would be to perform this check symbolically, but this is too slow for our purposes and is not amenable to integration with learning methods. We therefore explore two alternate approaches."
    }, {
      "heading" : "4.1 Numerical Representation",
      "text" : "In this representation, each expression is represented by its evaluation of a randomly drawn set of N points, where N is large (typically 1000). More precisely, for each variable in V , N different copies are made, each populated with randomly drawn elements. The target expression evaluates each of these copies, producing a scalar value for each, so yielding a vector t of length N which uniquely characterizes T. Formally, tn = T(Vn). We call this numerical vector t the descriptor of the symbolic expression T. The size of the descriptor N , must be sufficiently large to ensure that different expressions are not mapped to the same descriptor. Furthermore, when the descriptors are used in the linear system of Eqn. 5 below, N must also be greater than the number of linear equations. Any expression S formed by the grammar can be used to evaluate each Vn to produce another N -length descriptor vector s, which can then be compared to t. If the two match, then S(V) = T(V). In practice, using floating point values can result in numerical issues that prevent t and s matching, even if the two expressions are equivalent. We therefore use an integer-based descriptor in the form of Zp†, where p is a large prime number. This prevents both rounding issues as well as numerical overflow."
    }, {
      "heading" : "4.2 Learned Representation",
      "text" : "We now consider how to learn a continuous representation for symbolic expressions, that is learn a projection φ which maps expressions S to l-dimensional vectors: φ(S) → Rl. We use a recursive neural network (RNN) to do this, in a similar fashion to Socher et al. [23] for natural language and Bowman et al. [3] for logical expressions. This potentially allows many symbolic tasks to be performed by machine learning techniques, in the same way that the word-vectors (e.g.[8] and [16]) enable many NLP tasks to be posed a learning problems.\nWe first create a dataset of symbolic expressions, spanning the space of all valid expressions up to degree k. We then group them into clusters of equivalent expressions (using the numerical representation to check for equality), and give each cluster a discrete label 1 . . . C. For example, A, (AT )T might have label 1, and ∑ i ∑ j Ai,j , ∑ j ∑ iAi,j might have label 2 and so on. For k = 6, the dataset consists of C = 1687 classes, examples of which are show in Fig. 1. Each class is split 80/20 into train/test sets.\nWe then train a recursive neural network (RNN) to classify a grammar tree into one of theC clusters. Instead of representing each grammar rule by its underlying arithmetic, we parameterize it by a weight matrix or tensor (for operations with one or two inputs, respectively) and use this to learn the concept of each operation, as part of the network. A vector a ∈ Rl, where l = 30‡ is used to represent each input variable. Working along the grammar tree, each operation in S evolves this vector via matrix/tensor multiplications (preserving its length) until the entire expression is parsed, resulting in a single vector φ(S) of length l, which is passed to the classifier to determine the class of the expression, and hence which other expressions it is equivalent to.\nFig. 2 shows this procedure for two different expressions. Consider the first expression S = (A. ∗ A)′ ∗ sum(A, 2). The first operation here is .∗, which is implemented in the RNN by taking the\n†Integers modulo p ‡This was selected by cross-validation to control the capacity of the RNN, since it directly controls the\nnumber of parameters in the model.\ntwo (identical) vectors a and applies a weight tensor W3 (of size l × l × l, so that the output is also size l), followed by a rectified-linear non-linearity. The output of this stage is this max((W3 ∗ a) ∗ a, 0). This vector is presented to the next operation, a matrix transpose, whose output is thus max(W2 ∗ max((W3 ∗ a) ∗ a, 0), 0). Applying the remaining operations produces a final output: φ(S) = max((W4 ∗max(W2 ∗max((W3 ∗ a) ∗ a, 0), 0)) ∗max(W1 ∗ a, 0)). This is presented to a C-way softmax classifier to predict the class of the expression. The weights W are trained using a cross-entropy loss and backpropagation.\nWhen training the RNN, there are several important details that are crucial to obtaining high classification accuracy:\n• The weights should be initialized to the identity, plus a small amount of Gaussian noise added to all elements. The identity allows information to flow the full length of the network, up to the classifier regardless of its depth [21]. Without this, the RNN overfits badly, producing test accuracies of ∼ 1%. • Rectified linear units work much better in this setting than tanh activation functions. • We learn using a curriculum [1], starting with the simplest expressions of low degree and\nslowly increasing k. • The weight matrix in the softmax classifier has much larger (×100) learning rate than the\nrest of the layers. This encourages the representation to stay still even when targets are replaced, for example, as we move to harder examples. • As well as updating the weights of the RNN, we also update the initial value of a (i.e we backpropagate to the input also).\nWhen the RNN-based representation is employed for identity discovery (see Section 6.3), the vector φ(S) is used directly (i.e. the C-way softmax used in training is removed from the network)."
    }, {
      "heading" : "5 Linear Combinations of Trees",
      "text" : "For simple targets, an expression that matches the target may be contained within a single grammar tree. But more complex expressions typically require a linear combination of expressions from different trees.\nTo handle this, we can use the integer-based descriptors for each tree in a linear system and solve for a match to the target descriptor (if one exists). Given a set of M trees, each with its own integer descriptor vector f , we form an M by N linear system of equations and solve it:"
    }, {
      "heading" : "Fw = t mod Zp",
      "text" : "where F = [f1, . . . , fM ] holds the tree representations, w is the weighting on each of the trees and t is the target representation. The system is solved using Gaussian elimination, where addition and multiplication is performed modulo p. The number of solutions can vary: (a) there can be no solution, which means that no linear combination of the current set of trees can match the target expression. If all possible trees have been enumerated, then this implies the target expression is outside the scope of the grammar. (b) There can be one or more solutions, meaning that some combination of the current set of trees yields a match to the target expression."
    }, {
      "heading" : "6 Search Strategy",
      "text" : "So far, we have proposed a grammar which defines the computations that are permitted (like a programming language grammar), but it gives no guidance as to how explore the space of possible expressions. Neither do the representations we introduced help – they simply allow us to determine if an expression matches or not. We now describe how to efficiently explore the space by learning which paths are likely to yield a match.\nOur framework uses two components: a scheduler, and a strategy. The scheduler is fixed, and traverses space of expressions according to recommendations given by the selected strategy (e.g. “Random” or “n-gram” or “RNN”). The strategy assesses which of the possible grammar rules is likely to lead to a solution, given the current expression. Starting with the variables V (in our case a single element A, or more generally, the elements A, B etc.), at each step the scheduler receives scores for each rule from the strategy and picks the one with the highest score. This continues until the expression reaches degree k and the tree is complete. We then run the linear solver to see if a linear combination of the existing set of trees matches the target. If not, the scheduler starts again with a new tree, initialized with the set of variables V . The n-gram and RNN strategies are learned in an incremental fashion, starting with simple target expressions (i.e. those of low degree k, such as∑\nij AA T ). Once solutions to these are found, they become training examples used to improve the strategy, needed for tackling harder targets (e.g. ∑\nij AA TA)."
    }, {
      "heading" : "6.1 Random Strategy",
      "text" : "The random strategy involves no learning, thus assigns equal scores to all valid grammar rules, hence the scheduler randomly picks which expression to try at each step. For simple targets, this strategy may succeed as the scheduler may stumble upon a match to the target within a reasonable time-frame. But for complex target expressions of high degree k, the search space is huge and the approach fails. 6.2 n-gram In this strategy, we simply count how often subtrees of depth n occur in solutions to previously solved targets. As the number of different subtrees of depth n is large, the counts become very sparse as n grows. Due to this, we use a weighted linear combination of the score from all depths up to n. We found an effective weighting to be 10k, where k is the depth of the tree."
    }, {
      "heading" : "6.3 Recursive Neural Network",
      "text" : "Section 4.2 showed how to use an RNN to learn a continuous representation of grammar trees. Recall that the RNN φmaps expressions to continuous vectors: φ(S)→ Rl. To build a search strategy from this, we train a softmax layer on top of the RNN to predict which rule should be applied to the current expression (or expressions, since some rules have two inputs), so that we match the target.\nFormally, we have two current branches b1 and b2 (each corresponding to an expression) and wish to predict the root operation r that joins them (e.g. .∗) from among the valid grammar rules (|r| in total). We first use the previously trained RNN to compute φ(b1) and φ(b2). These are then presented to a |r|-way softmax layer (whose weight matrix U is of size 2l× |r|). If only one branch exists, then b2 is set to a fixed random vector. The training data for U comes from trees that give efficient solutions to targets of lower degree k (i.e. simpler targets). Training of the softmax layer is performed by stochastic gradient descent. We use dropout [13] as the network has a tendency to overfit and repeat exactly the same expressions for the next value of k. Thus, instead of training on exactly φ(b1) and φ(b2), we drop activations as we propagate toward the top of the tree (the same\nfraction for each depth), which encourages the RNN to capture more local structures. At test time, the probabilities from the softmax become the scores used by the scheduler."
    }, {
      "heading" : "7 Experiments",
      "text" : "We first show results relating to the learned representation for symbolic expressions (Section 4.2). Then we demonstrate our framework discovering efficient identities. For brevity, the identities discovered are listed in the supplementary material [29]."
    }, {
      "heading" : "7.1 Expression Classification using Learned Representation",
      "text" : "Table 2 shows the accuracy of the RNN model on expressions of varying degree, ranging from k = 3 to k = 6. The difficulty of the task can be appreciated by looking at the examples in Fig. 1. The low error rate of ≤ 5%, despite the use of a simple softmax classifier, demonstrates the effectiveness of our learned representation."
    }, {
      "heading" : "7.2 Efficient Identity Discovery",
      "text" : "In our experiments we consider 5 different families of expressions, chosen to fall within the scope of our grammar rules:\n1. ( ∑ AAT)k: A is an Rn×n matrix. The k-th term is ∑ i,j(AA T )bk/2c for even k\nand ∑\ni,j(AA T )bk/2cA , for odd k. E.g. for k = 2 : ∑ i,j AA T ; for k = 3 : ∑ i,j AA TA;\nfor k = 4 : ∑\ni,j AA TAAT etc. Naive evaluation is O\n( kn3 ) .\n2. ( ∑\n(A. ∗A)AT)k: A is an Rn×n matrix and let B = A. ∗ A. The k-th term is∑ i,j(BA T )bk/2c for even k and ∑ i,j(BA TB)bk/2c , for odd k. E.g. for k = 2 : ∑ i,j(A.∗\nA)AT ; for k = 3 : ∑ i,j(A. ∗A)AT (A. ∗A); for k = 4 : ∑\ni,j(A. ∗A)AT (A. ∗A)AT etc. Naive evaluation is O ( kn3 ) .\n3. Symk: Elementary symmetric polynomials. A is a vector in Rn×1. For k = 1 : ∑\niAi, for k = 2 : ∑ i<j AiAj , for k = 3 : ∑ i<j<k AiAjAk, etc. Naive evaluation is O ( nk ) . 4. (RBM-1)k: A is a vector in Rn×1. v is a binary n-vector. The k-th term is:∑ v∈{0,1}n(v\nTA)k. Naive evaluation is O(2n). 5. (RBM-2)k: Taylor series terms for the partition function of an RBM. A is a matrix in\nRn×n. v and h are a binary n-vectors. The k-th term is ∑\nv∈{0,1}n,h∈{0,1}n(v TAh)k. Naive evaluation is O ( 22n ) .\nNote that (i) for all families, the expressions yield a scalar output; (ii) the families are ordered in rough order of “difficulty”; (iii) we are not aware of any previous exploration of these expressions, except for Symk, which is well studied [24]. For the ( ∑ AAT)k and ( ∑ (A. ∗A)AT)k families we remove the matrix-multiply rule from the grammar, thus ensuring that if any solution is found it will be efficient since the remaining rules are at most O ( kn2 ) , rather than O ( kn3 ) . The other families use the full grammar, given in Table 1. However, the limited set of rules means that if any solution is found, it can at most be O ( n3 ) , rather than exponential in n, as the naive evaluations would be. For each family, we apply our framework, using the three different search strategies introduced in Section 6. For each run we impose a fixed cut-off time of 10 minutes§ beyond which we terminate the search. At each value of k, we repeat the experiments 10 times with different random initializations and count the number of runs that find an efficient solution. Any non-zero count is deemed a success, since each identity only needs to be discovered once. However, in Fig. 3, we show the fraction of successful runs, which gives a sense of how quickly the identity was found.\n§Running on a 3Ghz 16-core Intel Xeon. Changing the cut-off has little effect on the plots, since the search space grows exponentially fast.\nWe start with k = 2 and increase up to k = 15, using the solutions from previous values of k as training data for the current degree. The search space quickly grows with k, as shown in Table 3. Fig. 3 shows results for four of the families. We use n-grams for n = 1 . . . 5, as well as the RNN with two different dropout rates (0.125 and 0.3). The learning approaches generally do much better than the random strategy for large values of k, with the 3-gram, 4-gram and 5-gram models outperforming the RNN.\nFor the first two families, the 3-gram model reliably finds solutions. These solutions involve repetition of a local patterns (e.g. Example 2), which can easily be captured with n-gram models. However, patterns that don’t have a simple repetitive structure are much more difficult to generalize. The (RBM-2)k family is the most challenging, involving a double exponential sum, and the solutions have highly complex trees (see supplementary material [29]). In this case, none of our approaches performed better than the random strategy and no solutions were discovered for k > 5. However, the k = 5 solution was found by the RNN consistently faster than the random strategy (100± 12 vs 438± 77 secs)."
    }, {
      "heading" : "8 Discussion",
      "text" : "We have introduced a framework based on a grammar of symbolic operations for discovering mathematical identities. Through the novel application of learning methods, we have shown how the exploration of the search space can be learned from previously successful solutions to simpler expressions. This allows us to discover complex expressions that random or brute-force strategies cannot find (the identities are given in the supplementary material [29]).\nSome of the families considered in this paper are close to expressions often encountered in machine learning. For example, dropout involves an exponential sum over binary masks, which is related to the RBM-1 family. Also, the partition function of an RBM can be approximated by the RBM-2 family. Hence the identities we have discovered could potentially be used to give a closed-form version of dropout, or compute the RBM partition function efficiently (i.e. in polynomial time). Additionally, the automatic nature of our system naturally lends itself to integration with compilers, or other optimization tools, where it could replace computations with efficient versions thereof.\nOur framework could potentially be applied to more general settings, to discover novel formulae in broader areas of mathematics. To realize this, additional grammar rules, e.g. involving recursion or trigonometric functions would be needed. However, this would require a more complex scheduler to determine when to terminate a given grammar tree. Also, it is surprising that a recursive neural network can generate an effective continuous representation for symbolic expressions. This could have broad applicability in allowing machine learning tools to be applied to symbolic computation.\nThe problem addressed in this paper involves discrete search within a combinatorially large space – a core problem with AI. Our successful use of machine learning to guide the search gives hope that similar techniques might be effective in other AI tasks where combinatorial explosions are encountered."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank Facebook and Microsoft Research for their support."
    } ],
    "references" : [ {
      "title" : "Curriculum learning",
      "author" : [ "Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston" ],
      "venue" : "ICML,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "From machine learning to machine reasoning",
      "author" : [ "L. Bottou" ],
      "venue" : "Machine Learning, 94(2):133–149,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Can recursive neural tensor networks learn logical reasoning",
      "author" : [ "S.R. Bowman" ],
      "venue" : "arXiv preprint arXiv:1312.6192,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Machine learning for first-order theorem proving",
      "author" : [ "J.P. Bridge", "S.B. Holden", "L.C. Paulson" ],
      "venue" : "Journal of Automated Reasoning, 53:141–172, August",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Symbolic logic and mechanical theorem proving",
      "author" : [ "C.-L. Chang" ],
      "venue" : "Academic Press,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Maple V library reference manual, volume 199",
      "author" : [ "B.W. Char", "K.O. Geddes", "G.H. Gonnet", "B.L. Leong", "M.B. Monagan", "S.M. Watt" ],
      "venue" : "Springer-verlag New York,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "An attribute grammar based framework for machine-dependent computational optimization of media processing algorithms",
      "author" : [ "G. Cheung", "S. McCanne" ],
      "venue" : "ICIP, volume 2, pages 797–801. IEEE,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A unified architecture for natural language processing: deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "ICML,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The complexity of theorem-proving procedures",
      "author" : [ "S.A. Cook" ],
      "venue" : "Proceedings of the third annual ACM symposium on Theory of computing, pages 151–158. ACM,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Using attribute grammars to find solutions for musical equational programs",
      "author" : [ "M. Desainte-Catherine", "K. Barbar" ],
      "venue" : "ACM SIGPLAN Notices, 29(9):56–63,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "First-order logic and automated theorem proving",
      "author" : [ "M. Fitting" ],
      "venue" : "Springer,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Church: a language for generative models",
      "author" : [ "N. Goodman", "V. Mansinghka", "D. Roy", "K. Bonawitz", "D. Tarlow" ],
      "venue" : "arXiv:1206.3255,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov" ],
      "venue" : "arXiv:1207.0580,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Semantics of context-free languages",
      "author" : [ "D.E. Knuth" ],
      "venue" : "Mathematical systems theory, 2(2):127–145,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "M.-T. Luong", "R. Socher", "C.D. Manning" ],
      "venue" : "CoNLL,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "arXiv:1301.3781,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "A. Mnih", "G.E. Hinton" ],
      "venue" : "NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Evolutionary program induction of binary machine code and its applications",
      "author" : [ "P. Nordin" ],
      "venue" : "Krehl Munster,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Solving knapsack problems with attribute grammars",
      "author" : [ "M. ONeill", "R. Cleary", "N. Nikolov" ],
      "venue" : "Proceedings of the Third Grammatical Evolution Workshop (GEWS04). Citeseer,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Practical probabilistic programming",
      "author" : [ "A. Pfeffer" ],
      "venue" : "Inductive Logic Programming, pages 2–3. Springer,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "A.M. Saxe", "J.L. McClelland", "S. Ganguli" ],
      "venue" : "arXiv:1312.6120,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
      "author" : [ "R. Socher", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, pages 1–9,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C.P. Potts" ],
      "venue" : "EMNLP,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Enumerative combinatorics",
      "author" : [ "R.P. Stanley" ],
      "venue" : "Number 49. Cambridge university press,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Word representations: a simple and general method for semisupervised learning",
      "author" : [ "J. Turian", "L. Ratinov", "Y. Bengio" ],
      "venue" : "ACL,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "An approximate matching algorithm for finding (sub-) optimal sequences in s-attributed grammars",
      "author" : [ "J. Waldispühl", "B. Behzadi", "J.-M. Steyaert" ],
      "venue" : "Bioinformatics, 18(suppl 2):S250–S259,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The mathematica book, volume 221",
      "author" : [ "S. Wolfram" ],
      "venue" : "Wolfram Media Champaign, Illinois,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Evolutionary program induction directed by logic grammars",
      "author" : [ "M.L. Wong", "K.S. Leung" ],
      "venue" : "Evolutionary Computation, 5(2):143–180,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning to discover efficient mathematical identities",
      "author" : [ "W. Zaremba", "K. Kurach", "R. Fergus" ],
      "venue" : "arXiv preprint arXiv:1406.1584 (http://arxiv.org/abs/1406.1584),",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "We introduce a framework based on attribute grammars [14] that allows symbolic expressions to be expressed as a sequence of grammar rules.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "The problem addressed in this paper overlaps with the areas of theorem proving [5, 9, 11], program induction [18, 28] and probabilistic programming [12, 20].",
      "startOffset" : 79,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "The problem addressed in this paper overlaps with the areas of theorem proving [5, 9, 11], program induction [18, 28] and probabilistic programming [12, 20].",
      "startOffset" : 79,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "The problem addressed in this paper overlaps with the areas of theorem proving [5, 9, 11], program induction [18, 28] and probabilistic programming [12, 20].",
      "startOffset" : 79,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "The problem addressed in this paper overlaps with the areas of theorem proving [5, 9, 11], program induction [18, 28] and probabilistic programming [12, 20].",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 27,
      "context" : "The problem addressed in this paper overlaps with the areas of theorem proving [5, 9, 11], program induction [18, 28] and probabilistic programming [12, 20].",
      "startOffset" : 109,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "The problem addressed in this paper overlaps with the areas of theorem proving [5, 9, 11], program induction [18, 28] and probabilistic programming [12, 20].",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 19,
      "context" : "The problem addressed in this paper overlaps with the areas of theorem proving [5, 9, 11], program induction [18, 28] and probabilistic programming [12, 20].",
      "startOffset" : 148,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "Symbolic computation engines, such as Maple [6] and Mathematica [27] are capable of simplifying expressions by collecting terms but do not explicitly seek versions of lower complexity.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "Symbolic computation engines, such as Maple [6] and Mathematica [27] are capable of simplifying expressions by collecting terms but do not explicitly seek versions of lower complexity.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "[4] who use learning to select between different heuristics for 1st order reasoning.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "The attribute grammar, originally developed in 1968 by Knuth [14] in context of compiler construction, has been successfully used as a tool for design and formal specification.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 18,
      "context" : "This has previously been explored in a range of domains: from well-known algorithmic problems like knapsack packing [19], through bioinformatics [26] to music [10].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : "This has previously been explored in a range of domains: from well-known algorithmic problems like knapsack packing [19], through bioinformatics [26] to music [10].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "This has previously been explored in a range of domains: from well-known algorithmic problems like knapsack packing [19], through bioinformatics [26] to music [10].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "The closest work to ours can be found in [7] which involves searching over the space of algorithms and the grammar attributes also represent computational complexity.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 21,
      "context" : "[22, 23], as well as generic knowledge representation Bottou [2].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "[22, 23], as well as generic knowledge representation Bottou [2].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "[22, 23], as well as generic knowledge representation Bottou [2].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "[23], apply them to parse trees for sentiment analysis.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "Our work also has similarities to Bowman [3] who shows that a recursive network can learn simple logical predicates.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "Our demonstration of continuous embeddings for symbolic expressions has parallels with the embeddings used in NLP for words and sentence structure, for example, Collobert & Weston [8], Mnih & Hinton [17], Turian et al.",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 16,
      "context" : "Our demonstration of continuous embeddings for symbolic expressions has parallels with the embeddings used in NLP for words and sentence structure, for example, Collobert & Weston [8], Mnih & Hinton [17], Turian et al.",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 22,
      "context" : "[23] for natural language and Bowman et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "g.[8] and [16]) enable many NLP tasks to be posed a learning problems.",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 15,
      "context" : "[8] and [16]) enable many NLP tasks to be posed a learning problems.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 20,
      "context" : "The identity allows information to flow the full length of the network, up to the classifier regardless of its depth [21].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "• We learn using a curriculum [1], starting with the simplest expressions of low degree and slowly increasing k.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : "We use dropout [13] as the network has a tendency to overfit and repeat exactly the same expressions for the next value of k.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 28,
      "context" : "For brevity, the identities discovered are listed in the supplementary material [29].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 23,
      "context" : "Note that (i) for all families, the expressions yield a scalar output; (ii) the families are ordered in rough order of “difficulty”; (iii) we are not aware of any previous exploration of these expressions, except for Symk, which is well studied [24].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 28,
      "context" : "The (RBM-2)k family is the most challenging, involving a double exponential sum, and the solutions have highly complex trees (see supplementary material [29]).",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 28,
      "context" : "This allows us to discover complex expressions that random or brute-force strategies cannot find (the identities are given in the supplementary material [29]).",
      "startOffset" : 153,
      "endOffset" : 157
    } ],
    "year" : 2014,
    "abstractText" : "In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a grammar of math operators, we build trees that combine them in different ways, looking for compositions that are analytically equivalent to a target expression but of lower computational complexity. However, as the space of trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neuralnetwork. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation.",
    "creator" : null
  }
}